Two of my three 2012R2 machines exhibited this behavior last April. They would hang at Checking for updates... forever. I never learned exactly what caused the problem, but I did get it resolved by doing the following: 

Use a Debian live CD to rescue the system. Many specialized live CDs such as those you mentioned have kernels with only a very limited set of drivers. Since it's a Debian system, the Debian installation media should have the necessary drivers. 

You don't do it that way. First, you automate your Ubuntu installation using preseeding. Then, you use a configuration management tool such as puppet or chef to manage the machines on an ongoing basis. As for deploying the web application itself, doing this right tends to be highly app-specific, and you really haven't given enough detail to give a good recommendation. 

You asked for the URL path to be appended to the redirected URL, by using . Thus, if you go to you're redirected to , because the requested URI is . Further, if you go to then you are redirected to , because it's . If you just want people to be redirected to the same URL, regardless, then you should not have here. 

So this would rewrite to . It probably doesn't exist. You said you simply wanted to change to , so try something like this: 

In addition to CIDR ranges, you can specify single IP addresses or ipset names prefixed with . After this, all traffic from the specified addresses will be allowed on any port. Remember to make it permanent, either by repeating the command with appended, or by running . 

Active Desktop itself may remain disabled, but the wallpaper also has to be set here in order for Set Desktop Background to not work. Keep in mind that the option won't be grayed out, but will stop working. 

For you should be able to label the filesystem and then refer to it by label. Indeed, by default RHEL already labels as . 

Use to see recent yum transactions. Use to view the details of a transaction. Use to revert the most recent transaction, or to revert a specific transaction. 

Some older versions of libvirtd get confused after some internal bug and start throwing errors of this type. I never bothered tracking down the exact bug as it eventually got fixed after a number of updates. Two things to do: 

You're running CentOS 6, but you've installed third party repositories and , possibly among others, which are designed for CentOS 5. Remove them (and any other inappropriate repos) and replace them with the correct repositories. After doing so, you will need to clean old metadata from your system and replace existing inappropriate packages with correct packages. 

Of course, with such a long display as might generate, the headings may scroll off your terminal. As you use the commands more, you'll eventually learn which columns are which without having to refer to the headings. 

You get a web designer to produce a document which looks like you want it to look. Then you configure the load balancer to serve this document for HTTP 500, 502, 503 and 504 errors. Exactly how you do that varies; check your documentation. Some load balancers are just "dumb" TCP balancers which forward TCP connections and do nothing at layer 7. Others can act as full reverse proxies (e.g. nginx) and these are capable of doing what you need. From a quick scan of the Brocade ServerIron ADX documentation, it doesn't seem capable of serving error documents on HTTP requests. While it's definitely smarter than your typical "dumb" TCP forwarder, it's probably not going to do what you want here. A reverse proxy such as nginx would be capable of this, though if you set that up, you may as well just replace the load balancer (since nginx can do HTTP/HTTPS load balancing as well). 

(If not, you'll need to remove the rpmdb entries for the rest of the packages, and then try again.) If the packages for which rpmdb entries were removed aren't reinstalled or updated by , reinstall them. 

This answer comes almost certainly too late to help the original poster, but perhaps it will help someone else. The problem is that that howto is plain wrong. To upgrade Fedora online using yum, follow Fedora's guide for doing so (if you like living dangerously). The process goes generally like this: 

You installed PHP from the repository. It is disabled by default. You should re-enable this repository if you want to install further PHP packages from it. 

Assuming you use the default squid logfile format for your access.log, to see the slowest requests, try something like: 

Assign as many vCPUs as you wish to any virtual guest, up to the number of physical CPU cores (or hyperthreads). You can overcommit CPU by assigning more vCPUs among your running virtual machines than exist in the server, subject to the limit that no single virtual guest may have more vCPUs than physical CPU cores. Keep in mind that if you have CPU intensive tasks, that overcommitting CPU won't buy you anything and may slow down the rest of your guests. 

should be sufficient to install it on any system with Ruby available, if it's not in your system packages already. 

This is meant to be set to the Ruby binary itself. But you have set it to a directory. Change it to the location of your interpreter. 

You are only allowing return input traffic on the eth1 interface. Remove this restriction and you will find that your outgoing traffic on eth2 starts working. 

The easiest way to do this is to place all the files in shared storage (i.e. an NFS mount), which is then mounted to the same mount point on all of the servers. Then, you do not need to do any sort of rsync or other copying. 

The parameter was added in nginx 1.1.11, so your version of nginx is too old to use it. Since you are on CentOS, you should run the latest stable nginx provided by nginx.org's own CentOS repository. Updating to the latest stable nginx via this method will resolve the problem. 

Aside from the ideas you already had, you could add more RAM, though that seems like an expensive short-term band-aid. Long-term your best bet is probably going to be to switch away from OpenVZ. 

Build your RPMs using . This builds within a chroot containing only the necessary packages for building the RPM and helps to prevent situations like this. 

You can't create a custom error page for "ssl connection error" because, if this happens, then there is no connection! The browser could not even send an HTTP request for the server to respond to, if SSL/TLS negotiation fails. The browser's built-in message is the only thing you will get. Fortunately, when this happens because of a missing or invalid client certificate, the browser's message will state this. 

This comes out of the dim recesses of my memory, as I haven't had to script command line FTP in this way since, oh, the 90's... In an FTP protocol session, after sending the username to the FTP server, the server may then ask for a password, even if the username indicates that anonymous FTP should be used (i.e. the username sent is or ). In this case, the server requests the password but ignores it. So in a script that drives the command line FTP client, a blank line would be used to send an empty password. If the server doesn't ask for a password, the blank line does nothing and is harmless. 

Use to copy your keys; it will create any necessary files/directories and fix any broken permissions along the way. 

We'll generate a local policy module containing the necessary permissions. (You need to use the option with here so that the output can be processed by other scripts.) If you saw clearly irrelevant entries, redirect the output to a file, and then edit it to remove them. Then use the file here instead. 

An empty bridge is one that is not connected to any other network. So you have placed your VM on a completely isolated network, not connected to anything else, and thus not capable of communicating with anything. It may as well be unplugged. And the reason your bridge is empty is that you haven't placed either of your NICs in . Rather, it says , which is not the name of a NIC that you have in your system. To fix the problem, add the correct NIC to your bridge (and configure IP on the bridge instead of the NIC, if necessary). (Of course, you aren't supposed to be doing anything with Windows XP anymore...) 

You created your guest with (or its equivalent). This creates transient domains, which are deleted when they power off. To create persistent domains, use instead. These remain defined after they are powered off or destroyed, and can be started again at any time. 

Only if you make that change to the TTL more than three hours in advance of the IP address change. Remember that the TTL tells other DNS servers how long to cache records. So you must reduce it at least that long in advance of your desired change. 

You also have the IUS community repo installed and enabled. Disable this repo, as it conflicts with remi (and many other repositories). 

Finally, it's best practice to use instead of unless you really absolutely need to pass the command through a shell. 

Your firewall rules reject all incoming traffic. You tried to deal with this by manually appending rules to allow HTTP, HTTPS and MySQL connections, but this does not work since they are already rejected by a previous rule. Further, your system is running firewalld. To resolve the problem, you should use firewalld to manage your firewall rules. For example: 

Those package names are for Debian-based systems, and you aren't on such a system. Use the equivalents for Red Hat-based systems instead. (Which they really should have provided for you so that I wouldn't have to look them up.) 

The void lookup limit was introduced in RFC 7208 and refers to DNS lookups which either return an empty response (NOERROR with no answers) or an NXDOMAIN response. This is a separate count from the 10 DNS lookup overall count. 

Also note that, strangely enough, Microsoft doesn't actually own that domain name, and the actual owner is unknown. While Windows shouldn't send these queries out to the network, there is still the possibility of leakage if this sort of literal gets reused in other contexts. You would be well advised to set up DNS for your network. 

Another way you can do this is with a map. An example; off the top of my head so check for syntax errors first...: 

You installed PHP 5.5 from the repository, but you don't seem to have this repo enabled anymore. This is why the wrong older version of is being proposed for installation. Try enabling the repo. 

I've been running LUKS encrypted filesystems for over a decade, with ext2/3/4, XFS, ZFS and maybe some other filesystems I've forgotten about. While I don't have any benchmarks handy, I do have a few notes to share: The only real performance issue you have with LUKS is the encryption and decryption itself. This introduces some latency to the process and has the potential to make disk I/O CPU-bound. On older systems without hardware AES acceleration on-chip, this was a significant issue. Today, as long as you have AES-NI in your processor and a kernel from this decade, it's almost unnoticeable for moderate workloads. Better still if you have a recent (3.x+?) kernel which can do crypto in multiple kernel threads. 

I don't think is what you really want here. That just disables sending the and headers entirely, and you get browser-default behavior (which is usually RFC compliant, resulting in the resource being cached). If you mean to have the resource not cached at all by user agents, use instead. 

In addition, consider making a separate repo for this package if deploying it this way is truly time-sensitive and doesn't help enough. 

Heroku doesn't support receiving email directly to your application, and you can't really run an SMTP server on Heroku anyway. You'll need to use a third party addon such as CloudMailin or Sendgrid. 

This gets a little unwieldy. However, it does have its advantages. If you inspect this carefully you'll notice that the CentOS version is different; it enables a repository while installing the package. Of course there's a reason for doing it this way. On different systems you might even have different package names, so you may end up having to have different tasks for them anyway. But that's something you can shove in an OS-specific variable and then do something like: 

Load the unique_id_module. This provides the UNIQUE_ID environment variable which serves this purpose. 

It's redundant. If you reach the end and haven't matched anything, the default is anyway. But having it there makes that explicit for people who don't know what the default is. 

But, that's not how you should fix the problem. This has a further problem, in that the http to https redirect is inefficient. This has to be evaluated on every request, and the has a gratuitous regex. See Taxing Rewrites for more information. Instead, you should have a completely separate block for HTTP versus HTTPS, and remove the / from the HTTPS block entirely. 

Generally, cron jobs that do things like "backup files" and "send email" need to do so within the context of the user whose files they are manipulating. This ensures that they have permission to read and write the files, and that actions taken can be audited as corresponding to that user. So you will want to run the cron jobs as the user on whose behalf they act. 

Here we see that this IP is in AS209 (which you can store instead of the IP, or in addition to it, if you wish) and is registered to Qwest Communications, which you can then display. 

Thus, since that directory doesn't have search permssions, Apache can't descend into it. You will need to set at minimum a+x (or 711) on this directory. 

The problem I see here is that this isn't treated as a regex match, but is being treated literally, because it doesn't start with . Further, with and a regex match, the alias must include a variable for the captured part of the . This should resolve the problem. 

The second set of rules has a similar problem. I don't know who originally wrote this ruleset, but it's been widely copied all over the Internet. It seems to have been written by someone who was unfamiliar with stateful firewalls in general, or iptables in particular. In this ruleset, the input rule allows new and established incoming traffic to destination port 22 from anywhere. Then, the output rule allows established traffic from source port 22 to anywhere. This is essentially a mirror of the first rule and something like it is required if your default policy on outgoing traffic is to drop or reject it. The problem is that these outbound rules become redundant very fast, which leads to performance issues as well as comprehension problems for any humans who have to read the rules later. If you are dropping outgoing traffic, only one rule is needed for established outgoing traffic (matching allowed incoming traffic) on any port, regardless of how many incoming ports you allow.