I've written some code to parse the names and phone numbers from craigslist. It starts from the link in "m_url" then goes one layer deep to parse the name and then again another layer deep to parse the phone number. Note that it goes 2 layer deep only when it sees "show contact button" on that page so that it can unveil the phone number from that link to scrape. It only prints the result when it sees the button on that page. That's because there are around 120 names on that page but it prints only those containing that specific button. Sometimes when I come across such "show contact button" link within a page from where I am supposed to harvest data, I get frightened. That's why I tried to work on it. It works smoothly now. Any improvement on this script will be very helpful. 

I've written a script in python scrapy to parse "name" and "price" of different products from a website. Firstly, it scrapes the links of different categories from the upper sided bar located in the main page then it tracks down each categories and reach their pages and then parse the links of different sub-categories from there and finally gets to the target page and parse the aforementioned data from there. I tried to do the whole thing slightly differently from the conventional method in which it is necessary to set rules. However, I got it working the way I expected using the logic I applied here. If any improvement is to be made, I'll be very glad to comply with. Here is what I've tried with: "sth.py" aka spider contains: 

is defined twice. First as a string ( and the second time as a label. I would rename the first one to and use it in 

Other things that might be good to fix You have the same loop for scanning the characters. Why not enclose it as another proc and just use in those two places? You can make assumption that for example will point to the buffer that has to be filled. Also in this scanning you could handle for example backspace character and delete the characters. 

Using those instead of bare -ing will make your code more readable. Having those it's clear what you try to do with this code High-level functions 

You have a bit of code duplication that could and should be removed to avoid mistakes. FASM supports macros that you can use to remove duplicated parts. One can define a macro: 

You have a bit of code duplication that could be extracted and consolidated. But first with some error fixes (at least the code did not compile on my TASM). Compilation issues 

The actual string starts on index 2 and before that you have max string length, and bytes read. Also there's no at the end. You have to put it there. 

Buffered input I don't know why you chose buffered input as your method of entering the hint, but in case of this command the data has a specific format. 

I have written a crawler in python with the combination of class and function. Few days back I saw a scraper in a tutorial more or less similar to what I did here. I found it hard the necessity of using class here. However, I decided to create one. My scraper is able to traverse all the next pages and print the collected results errorlesly. If there is any suggestion or input to give this scraper a better look, I'm ready to comply with that. Thanks in advance. Here is what I've written: 

I've written a script in python to scrape e-mail addresses from different pizza shops located in los-angeles available in yellowpage traversing multiple pages. It is able to go one-layer deep and dig out email addresses. I believe, this crawler has got the ability to parse all the emails from any link no matter how many pages it has spread across. Just needed to adjust the last page number in the crawler. Here is what I've written: 

After a long try I've been able to create a script in vba which can successfully handle webpages with lazy-load. It can reach the bottom of a slow loading webpage if the hardcoded number of the loop is set accurately. I tried with few such pages and found it working flawlessly. The one I'm pasting below is created using site. It can parse the title of different news after going down to a certain level of that page according to the loop I've defined. Now, what I wanna expect to have is do the same thing without using hardcoded delay what I've already used in my script. Thanks in advance for any guidance to the improvement. Here is what I've written: 

You are calling on a source basically retrieving the whole collection with all the properties and only later you are taking just the two fields. If you want to reduce time and space - try taking only what's needed from the underlying source, but also it greatly depends on the typo of source you are using underneath. Consider dropping the LINQ at all. LINQ is great and compact but it has it's 'dark side' in term of hidden allocations. Also think abount some old-hasioned ways like 'paging'? For the dropdown that might not be the best way - but maybe it is. If for most cases only the top 20 repair issues are accessed then here's your optimization - and for few cases user would need to load more items to the list - that might be ok. 

Without the actual model and some numbers (how long does it take to execute/ how much 'space' it takes?) might be hard to optimize, but what I can tell is that you call quite a lot in such short method. When you do call it you basically iterate over a source (is it DB source? in memory source? other?) and grab it to the memory of your process. Try reducing the number of calls - it will improve your performance. Take a look at this fragment: 

I've written a script in VBA which is able to parse image links from a website, download and store them in a local folder and finally set those images beside each link in an excel file. As the script is a bit big, I tried to make it clean so that it can serve the purpose errorlessly. However, it can do successfully what I mentioned above. There are always rooms for improvement, though. Thanks in advance. Here is the full code: 

I've written a script in python using requests module in combination with selenium along with regex to parse email address (if any exists) from any website. I tried to create it in such a way so that it can traverse javascript enabled sites as well. My crawler is supposed to track any website link (given in it's list storage) then find or etc keywords from that page and parsing the matching link it will go to the target page and using regular expression it will finally parse the email address from that page. It scrapes the email address along with the link address where it parses the email from. I tried with several links and most of the cases it succeeds. I know it's very hard to create a full-fledged one but I tried and it is not despairing at all. Any suggestion to improve this crawler will be vastly appreciated. Here is what I have written: