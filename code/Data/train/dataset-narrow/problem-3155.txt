Emails from the SpamAssassin corpus -- note that both "ham" (non-spam) and spam datasets are available microblogPCU data set from UCI, which is data scraped from the microblogs of Sina Weibo users -- note, the raw text data is a mix of Chinese and English (you could perform machine translation of the Chinese, filter to only English, or use it as-is) Amazon Commerce reviews dataset from UCI Within the bag-o-words dataset, try using the Enron emails The Twenty Newsgroups dataset This nice collection of SMS spam You can always scrape (extract) your own text data from the Internet; I'm not sure which language or statistical package you're using, but XPath-based packages are available in R (, , etc) and Python to accomplish this 

I'm trying out a Kaggle competition, which puts me in the unusual position of being able to get feedback on my models' "true" performance (you can submit several predictions per day and they give you your score on a performance metric -- in this case MSE -- each time). As such, I've been observing a situation I can't explain -- my cross-validation results for my first stage Random Forest model, constructed with sklearn in Python, are consistently accurate within 1 percentage point of the true MSE. For example, my first stage model will give a CV MSE of about .22 and the "ground truth" (actual) MSE will turn out to be about .23. OTOH, my 2nd stage Random Forest, which is a different type of RF constructed in R, gives way "better" (smaller) training and cross validation MSE errors, like 0.05, but consistently turns out to have a much higher true MSE, like .23. In the 2nd stage Random Forest I have the predictions from the first stage and a few additional engineered features as regressors. Is there a statistical explanation for this? For example is it the result of some sort of recursion from stacking the models? If so, given that most winners of these Machine Learning contests are stacking many models, what's the appropriate way to do so that would avoid this statistical problem? 

My question is if there is an statistical or logical methodological framework that can guide the testing of various blends of your models? That is, can some algorithm or methodology be applied to the process of deciding how to alter the blend of predictions? Ideally, such a framework would also guide the decision of when to add or remove a set predictions, though even a framework which assumes a given number of models would be highly useful. 

Redundant features can be features that are multicolinear (i.e. highly correlated), but more importantly they're measuring the same thing without a unique contribution. For instance, age and income might be highly correlated, but in some analyses they still have a unique effect in your model and may have conceptual differences that you want to captured for interpretation. OTOH, age and birth date are purely redundant in most use cases I can think of (though there are always exceptions, such as if season of birth is important). Can PCA help reduce redundancy? Sure. It's one of at least dozens of techniques you could use for this. One way you use PCA for feature selection is to look at the factor loading on the principal components and determine which correlated variables are measuring the same principal component then pick the top 1 or few variables to represent that latent variable, eliminating highly correlated non-distinct features. Should you eliminate redundant features before PCA? If you're going to use the principal components for prediction rather than feature elimination, then yes. You can do one round of feature analysis involving PCA or other techniques and a second round to create latent variables for your model if you want to do both. Some additional tools for feature selection: 

Someone I know has been using the Flesch-Kincaide readability score algorithm to try to gauge how well students write as a predictor of their academic success. They've been mostly unsuccessful. While taking a UCSD Coursera course I recently learned about this algorithm and I believe that their lack of success is just a basic misunderstanding of how the algorithm works. A low score does not imply poor writing -- anything written for college level reading ability, such as U.S. law and classic literature, receives a low readability score. High readability scores imply more simplistic writing, but not that the writing is more or less correct. So it's not really measuring much in terms of proficiency. All that is to ask this -- is there some other algorithm or metric to gauge writing quality, which would be more useful? Something that perhaps measures proper grammar and/or punctuation? I have been and will continue to search for this myself, but if anyone has any knowledge they can share it would be greatly appreciated. 

Unlike with Google Cardboard, most of the businesses that might clone and sell their versions of it have not yet gotten into the game (to my knowledge). Is it possible to piece together a list of what you'd need to buy to make your own? Here's what I have so far. Some of this (the Pi) was meant to be purchased separately anyway: 

When training an XGboost model some of the information printed regards "extra nodes". I can't find an explanation of these anywhere in the documentation. What exactly are extra nodes? 

I'm facing some indecision when choosing how to allocate my scarce learning time for the next few months between Scala and Java. I would like help objectively understanding the practical tradeoffs. The reason I am interested in Java is that I think some of my production, frequently refreshed, forecasts and analyses at work would run much faster in Java (compared to R or Python) and by becoming more proficient in Java I would enable myself to work on interesting side projects, such as non-Data-Science apps I'd like to develop. Currently I've taken a couple of Java courses, but I need much more education and practice to master it. The reason I started considering learning Scala is very similar -- I figured that the statistical/ML orientation would make it good for my work as a Data Scientist, and since it is based on Java I would be getting practice at work that helps me with my side-interest in Java, even though there are a few major differences such as functional vs. imperative and traits vs. interfaces. It seems like a lot of the advantages of Scala revolve around integration with Spark. I am thinking that this should be the tipping point for my decision, because my team is not currently using Spark and I don't have a good enough reason to request it. However, I thought I should ask here so that I don't waste too much time if Scala is still a better choice. For the purposes of this question please ignore alternatives such as Python, R, Julia, etc (I've eliminated those from consideration for other reasons, such as already being sufficiently familiar with them for my use cases). 

This is a fairly common task, especially in fields like economics, but the choice of using a Neural Network makes your approach different from most. Having said that, I think you can still use common validation metrics: 

Raspberry Pi Zero W board Raspberry Pi Camera 2 Raspberry Pi Zero W compatible power supply (SKU: 001917) Micro SD card (available everywhere offline and online) Cardboard (I have questions on this below) Piezo buzzer 

in addition to the dozens of paradigms and sub-paradigms outside of logic programming. Within Functional Logic Programming for instance, there exists extensions of ILP called Inductive Functional Logic Programming, which is based on inversion narrowing (i.e. inversion of the narrowing mechanism). This approach overcomes several limitations of ILP and (according to some scholars, at least) is as suitable for application in terms of representation and has the benefit of allowing problems to be expressed in a more natural way. Without knowing more about the specifics of your database and the barriers you face to using ILP, I can't know if this solves your problem or suffers from the same problems. As such, I'll throw out a completely different approach as well. ILP is contrasted with "classical" or "propositional" approaches to data mining. Those approaches include the meat and bones of Machine Learning like decision trees, neural networks, regression, bagging and other statistical methods. Rather than give up on these approaches due to the size of your data, you can join the ranks of many Data Scientists, Big Data engineers and statisticians who utilize High Performance Computing (HPC) to employ these methods on with massive data sets (there are also sampling and other statistical techniques you may choose to utilize to reduce the computational resources and time required to analyze the Big Data in your relational database). HPC includes things like utilizing multiple CPU cores, scaling up your analysis with elastic use of servers with high memory and large numbers of fast CPU cores, using high-performance data warehouse appliances, employing clusters or other forms of parallel computing, etc. I'm not sure what language or statistical suite you're analyzing your data with, but as an example this CRAN Task View lists many HPC resources for the R language which would allow you to scale up a propositional algorithm.