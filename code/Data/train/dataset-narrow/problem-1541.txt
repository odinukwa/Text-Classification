RFC2821 status is PROPOSED STANDARD. This RFC obsoletes RFC974 and, in the scope of MX record handling, it slightly differs from RFC1123. While the former REQUIRES a random selection of a SMTP server among multiple MX records with an equal preference value, the latter just RECOMMENDS it. 

In Ubuntu, FFmpeg headers are distributed among several packages, which are built from the source package. I guess you should install all of them (list here): 

The method you use for remote script execution generates a conflict between code and data flows. I mean... During the execution of , you allocates the standard input (STDIN) for the code that will execute remotely. But if the executed code needs a data input, it will be fed by the next code line (because the STDIN is transmitting lines of code to the remote process). If you want to use STDIN to supply input data to a script that runs remotely, you may have to use another stream for the code. OpenSSH seems not to have a native file descriptor forwarding feature yet, that is, STDIN (file descriptor number 0) seems to be the only file descriptor which could be used for data forwarding from the local terminal to the remote session. As a workaround, you could copy the script file in advance, for example: 

In the server that eventually produces kernel oops, forward messages to a "monitor" server using the netconsole module. 

=> User didn't authenticate, so this restriction is ignored. => User doesn't belong to the internal network, so this restriction is ignored. => Postfix seems to have deprecated this restriction, so it may also be ignored. => Postfix detects that it is the final destination of the message, so the restriction is ignored. An implicit takes place, so Postfix finally accepts and delivers the message. 

I have uploaded a PFX file to the Azure Automation account - it is now a certificate asset over there. Now, I would like to have a DSC configuration that ensures this certificate is present in the LOCAL_MACHINE\My store on the target machine and its private key is accessible by a certain domain user. I found the resource, but it is unclear to me how I can use it. Any ideas? EDIT 1 My current solution is as follows: 

The ISO file itself is not small - over 7GB. But the folder is huge - 19GB, which are added to the same content of the ISO. My question - do I really need to keep this 26GB if I want to have a silent install of VS2015? Is it safe to delete the folder? And if it is, 19GB is still an awful lot. Is there a more space efficient way to arrange the silent install? 

Suppose I have a product which I want to install on a non SSD drive, if exists. Otherwise, I want it to go on the drive C. For example, the following configuration is supposed to install the Sql Server 2016 in the default location: 

I am trying to grant a read-only access to my temp directory to another user from the command line using icacls.exe. Tried the following (PowerShell): 

Part of configuration is to run all the Windows Updates. I am trying to figure out how to express it with Powershell DSC. Seems like the best is to ensure that the windows updates are scheduled regularly. I found xWindowsUpdateAgent DSC resource, but it does not allow to specify the schedule itself, only to ensure that the updates are scheduled. So, is it possible to ensure a concrete windows update schedule? 

According to the tutorials I have read, the Active Directory replication subsystem does not handle synchronization of SYSVOL and NETLOGON folder contents between domain controllers. Instead, it handles topology discovery (KCC, ISTG) and synchronization of changes committed against the LDAP databases. Replication of SYSVOL and NETLOGON shares are handled by either FRS (File Replication Service) or DFS-R (Distributed File System Replication) subsystem. 

Your firewall rules seem correct. However, redirection of packets that enters the AP should be configured in the chain. Rules added into the chain affect locally-generated packets only. So, the following rule should be added: 

EDIT: I see that the setting contains a and no other additional restrictions. Also, neither nor restriction is applied, so an implicit prevails. Let me write how your Postfix daemon behaves when it receives a command in a SMTP transaction. Consider that a remote client started it and intends to send a fake message from and to . 

I suggest you to verify the DFS Replication health between the servers, by following this article: $URL$ : 

EDIT: Actually, my executable comes from s-nail package. The argument is not supported by GNU mailutils, so my solution doesn't work. You can use heirloom-mailx instead of GNU Mailutils. Install the package and modify the crontab line to: 

The line above tells to authorize user to run any commands as any user that is member of the group. The related sections from are: 

I guess a modern mail transfer agent follows at least RFC2821 or RFC5321 procedures, so all three DNS setups provide failover capabilities. However, only the first setup may provide a better load balancing. If you give a try to the second or the third setup, you will have to make sure your DNS server delivers responses in a random order. Besides, DNS records may be cached either by MTA's themselves or by recursive DNS servers, so the randomness can't be guaranteed. I think will receive most of the messages. Another reason that directs my opinion against the second and third setups is the reference of multiple names to one IP address. Mail servers in the internet commonly rejects messages from hosts whose mapping doesn't match (as does the Postfix restriction reject_unknown_client_hostname), so you will have to take special care on setting PTR records. Clients that don't try MX records in a random order are already violating the RFC2821 and RFC5321 standards. So, I think there is no guarantee that these clients will also try the secondary IP address automatically. Because of that, I prefer the simplest DNS configuration: 

The following article explains how to generate a silent installation of VS 2015 - $URL$ The essential command is vs_enterprise.exe /Layout SomeSharedDirectory In the end it created the following folder structure: 

Renamed SSMS-Setup-ENU.exe to SSMS-Setup-ENU.zip Peeked inside. The only text file is named "0" and this is an XML file. The very first element is from which I concluded that the exe was created with WIX burn. Renamed the file back. Found $URL$ Downloaded and extracted wix311-binaries.zip from $URL$ Ran , which extracted a lot of msi files from SSMS-Setup-ENU.exe to . Found $URL$ Saved the PowerShell script from the answer and modified it a bit to ease the piping (the script code is below). Piped all the msis through the script and identified the two msis with the Product Codes in question. 

Server Manager → Manage → Server Manager Properties "Do not Start manager automatically at logon" Server Manager → Local Server → IE Enhanced Security Configuration → Off 

I found the following post - $URL$ Great, all I need to know now is the Product Id of SSMS-Setup-ENU.exe, right? But how, on Earth, am I supposed to do it? When I installed it on another machine and tried the approach described in $URL$ I got two Product Ids: 

Next, I have no idea how to push to a different branch. For instance, a developer works in the default branch and wishes to backup its local commits to the special backup repository shared by all the other devs. Of course, this backup repository is not the one from which developers pull the new changes. Anyway, the backup needs to go to a branch dedicated to that particular dev, as stated per options 2. How do I do it? How can I tell push that the changes are pushed to a different branch? 

The custom script may restart the machine via the out-of-band management interface (iDRAC on Dell servers): 

A Squid server configured as a forward proxy is able to receive plain HTTP requests from clients and forward HTTPS requests to upstream servers transparently. However, an external URL rewrite program is needed. Write the following URL rewriting program into : 

RFC1123 status is INTERNET STANDARD. Section 5.3.4 aims to "refine" the RFC974 procedures about how to handle MX records. It now requires MTA's to try all SMTP servers in ascending order of preference until one succeeds. However it still allows a configurable limit on the number of tries. If there are multiple MX records with an equal preference value, the RFC recommends (and doesn't require) MTA's to select one record at random. However, if a MX record references multiple A records (IPv4 addresses), the RFC requires the MTA's to contact all these addresses until one succeeds, in the order given by the DNS server. 

EDIT: Actually, I forgot to remove the closing brace in a second test. In fact, keepalived startup process ignores the missing brace, but the failover doesn't work during the runtime. 

RFC5321 status is DRAFT STANDARD. This RFC obsoletes RFC2821 and, in the context of DNS resolution, it basically rewrites the same server lookup procedure and presents a new section that slightly discuss handling of MX records that references IPv6 addresses. 

Note: I admit I am proposing a weird and possibly unsupported solution, which may get broken on future versions. Nonetheless, it will certainly work on a CentOS 7 server. I suggest you to directly add such port forwarding rule into the / stack. You can achieve that by using 's direct rules. For example: 

The problem is that the system temp directory is required when machine installs updates first thing after a restart. My RAM disk, however, starts through , which is probably read later in the process. My question is this - is there a way to redirect the system temp directory to a RAM disk in a way that this redirection is realized before windows installs the updates after restart? Use case: I have installed a RAM disk that gets initialized on startup. Everything works fine, including rebooting the server (it is possible to save the RAM disk image, making it survive the reboot). However, when I reboot the server as a result of Windows Update the update is undone, because Windows is unable to access the TEMP directory - the RAM disk has not been loaded yet. Truth to be told the problem is only with the system TEMP directory and there is not much harm not to redirect the system TEMP and leave it as is. Still, if I could load the RAM disk before the Windows Update resumes then I could place the system TEMP on the RAM disk as well. My question - is it possible? 

However, I would like to check first if there is a non SSD drive and if present install it there. As I understand it, I cannot use Powershell code, because that code runs during compilation. I need it to run during the configuration. How do I do it? 

There is, however, a problem - the certificate is left on the target machine. I would like to avoid it. One idea could be instead of copying the certificate to share the Azure File Storage container having the PFX file and install it from the share, but then the share remains permanently mounted on the target machine - essentially the same problem. I am looking for a way to ensure the certificate is present without leaving a trace. I thought I could add a resource with Ensure = "Absent" and make it depend on the PfxImport resource, but then how will it work? Will it constantly download the certificate, ensure it is installed (it would be) and then delete it again? Doing this every 30 minutes? Is this the right approach? 

You can add a or a restriction at the end of if you intend to receive messages from authenticated users only. However, if you are going to receive messages from domains that you don't own (i.e. a public mail server), you have to follow other alternatives, as an explicit or will reject those messages. Apply the resolution pointed by @Jenny D, that is, either set restriction or configure Sender Policy Framework. Also, you can apply additional anti-spam techniques: 

Or you could use Bash hackings in order not to write local code into the remote filesystem. For example: 

shows the process that reads the file is a process whose PID is and owner is . shows the socket that sends the file data to the remote client is a process whose PID is and owner is . 

Add the argument to the command, which means "do not send any messages if the body is empty". Therefore, change the crontab line to: 

I figured that CentOS's creates a chain in table for each active zone, which gets evaluated when packets that matches zone definitions arrive. So / rules can be added there. 

I could not find any documentation that explains the meaning of . I usually define the initial state to and let the election process to choose the master instance. I copied your configuration files to a lab environment and have found that a closing brace for is missing in Server2's keepalived.conf . However, the failover seemed to work well despite that character absence. Please, check using wether unrelated VRRP packets with are present. Or try to change the to another number. As VRRP packets are sent to the multicast address , each virtual IP in a network must use an unique . Also, if you intend to let Server1 take over the virtual IP, I suggest you to set in its . RFC5798 section 6.4.3. Master says that if Server1's IP address is greater than Server2's IP address and both servers have the same priority, Server1 wins the election and gets the virtual IP. However, seems to compare priorities only. 

I used ssh-host-config both on vm and srv to configure the ssh to run as a windows service. Besides that I did nothing else. Can anyone help me troubleshoot this issue? Thank you very much. EDIT The virtual machine software is VMWare Workstation 7.1.4. I think the problem is in its settings, but I have no idea where exactly. The Network Adapter is set to Bridged. EDIT2 All the machines are located in the company lab, I think all of them are on the same segment, but I may be wrong. Below is the output for each machine (skipping the linux server). I have deleted the Tunnel adapters to keep the output minimal. If anyone thinks they matter, do tell so and I will post them as well. In addition ping output is given to show that DNS is correct. Something else, may be relevant, may be not. Doing to srv works OK, whereas to vm failes with Access Denied. srv: 

It is unclear to me how Azure Automation DSC can allow for such coupling. Sometimes just the account name is needed (to configure Sql Server), but sometimes the resource must be run as the owner, i.e. the owner credentials are needed. Unless there is a way for a DSC resource to run as the workstation owner without knowing owner's credentials. In short, it seems to me that Azure Automation DSC cannot be used to configure developer machines. And I am not even talking about shortage of out of the box and tested DSC resources like: 

It should be added to the Sql Server as a login account in sysadmin role. Visual Studio extensions that I want every developer to have must be installed from a session started by the owner account (not the SYSTEM account used to apply the DSC configurations) The TFS workspaces must be associated with the owner account.