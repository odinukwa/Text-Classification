I want to store an integer data which represents a pixel type in channel A. Here is part of the fragment shader: 

I develop 3d menu and sometimes I need to blur only part of the screen. I use a forward rendering. I create a frame buffer object with 3 color attachments. Rendering looks like this: 

I found a interesting technique - Exponential Variance Shadow Mapping which should provides better light bleeding fix. Here is saving depth for that technique: 

What are differences between them ? Are both ways correct ? For the standard shadow mapping with software pcf a shadow test will depend on the linear depth format. What about variance shadow mapping ? I implemented omnidirectional shadow mapping for points light using a non-linear depth and hardware pcf. In that case a shadow test looks like this: 

Usually in 3d drawing you want to ideally have fewer larger textures rather than lots of smaller ones, and make sure you render the same kinds of textures at the same time. So there is less texture swapping going on behind the scenes. This is not a difficult thing to do as most textures are purely used for models or terrains/structures. So you may have a terrain texture, a structures texture and a couple of player/enemy textures. Then just draw the terrain, the structures and any enemies and the player, only requiring a few texture swaps there, and if you are also using some sort of model instancing for the enemies you can get away with some other improvements there. Anyway now enter the 2d drawing world, rather than applying a texture and then drawing lots of vertices using it you are giving a texture to each draw command. So I am wondering how XNA handles this behind the scenes. Take for example a top down 2d game, you have a tileset for the level containing many ground sprites, an enemy sprite sheet which may contain anywhere from 10-20 enemies sprites and animations, then possibly a player/equipment sheet which contains anything for them. Now in this instance if I were to draw my level which was lets say 64x64 tiles (a tile being an x,y,tileSetIndex), and I have a tileset with 8x8 sprites (lets say 32x32 each): 

but I have no idea how to do that for the first format of linear depth. Is it possible ? Edit 2: For non-linear depth I used glPolygonOffset to fix shadow acne. For linear depth and distance to the light some offset should be add in the shader. I'm trying to implement standard shadow mapping without pcf using a linear depth (-viewSpace.z * linearDepthConstant + offset) but following shadow test doesn't produce correct results: 

I use Reinhard tone mapping. I calculate an average luminance and max luminance in screen space. Based on those parameters I get an eye adaptation, for example when a camera is moved from a bright outdoor area to the dark room. How an eye adaptation process is achieved in Uncharted 2 tone mapping ? In my engine I render results of the shading to the floating point texture. I assume that I don’t need hardcoded exposure adjustment (texColor *= 16) and exposure bias = 2.0f. Or maybe I should interpret those parameters based on the average luminance ? What about “w” parameter ? Can I assign to “w” a maximum luminance from the screen space ? 

Basically, this means that for models with relatively few special materials (i.e. no mirror, subsurf, etc.), the computation time will be large determined by the number of poligons. This is usually the case for very simple video game graphics. In practice, however, in newer high-end games and especially movies, the culprit is "usually" ray tracing. Why? Well for two reasons. I'll give the mathy reason first, and then my opinion of the real reason at the end. 

Adding to the other great answers that were already posted, it is worth noting that in order to achieve the fast processing times that games need, game developers need to bake many of their visual effects as simple textures. This means that great care must be taken in avoiding effect that won't bake well. One important effect that's hard to bake for video games is Subsurface Scattering (SSS). Unfortunately, this effect is really important in generating realistic-looking human skin. That's why many "realistic" video game characters come out looking plastic. One way developers avoid this issue is by deliberately making the characters brightly colored to detract from the plastic look, or by adding a lot of shadows and textural details to the face (like beards etc.) to break up the otherwise large, continuous sections of skin. 

The code doesn't work properly and I don't know how to explain that. What is wrong ? I found other solution which works perfectly, but it require extra integer texture in the gbuffer: 

I can use subroutines to reduce overhead during switching shaders. I also found the following article . Do you have any other ideas how to optimize above rendering ? 

I use a variance shadow mapping with a "standard" light bleeding fix in my graphics engine which is based on deferred rendering.. I have a single shadow map for a directional light beacause a test scene is relatively small. Saving depth looks like this: 

Now in this example you could inherit, but your Update may become a bit tricky then without changing your base class to alert you if you had changed, but it is up to each scenario to choose if its inheritance/implementation or composition. Also the above implementation will re-cache within the rendering cycle, which may cause performance stutters but its just an example of the scenario... Ignoring those facts as you can see that in this example you could use a cache-able component or a non cache-able one, the rest of the framework needs not know. The problem here is that if lets say this component is drawn mid way through the game rendering, other items will already be within the default drawing buffer, so me doing this would discard them, unless I set it to be persisted, which I hear is a big no no on the Xbox. So is there a way to have my cake and eat it here? One simple solution to this is make an ICacheable interface which exposes a cache method, but then to make any use of this interface you would need the rest of the framework to be cache aware, and check if it can cache, and to then do so. Which then means you are polluting and changing your main implementations to account for and deal with this cache... I am also employing Dependency Injection for alot of high level components so these new cache-able objects would be spat out from that, meaning no where in the actual game would they know they are caching... if that makes sense. Just incase anyone asked how I expected to keep it cache aware when I would need to new up a cachable entity. 

gives better results but light bleeding is still quite visible. Edit2: Light bleeding: EVSM with minVariance which is calculated as above gives better result than VSM without standard light bleeding fix, but to get the best result I also use standard light bleeding fix with EVSM. A light bleeding fix is performed in Chebyshev function but maybe I should add it after following calculation: shadow = min(posResult, negResult) ? EVSM causes small artifacts on the illuminated side of the objects so I add some glPolygonOffset when saving EVSM depth. 

I'm implementing ominidirectional shadow mapping for point lights. I want to use a linear depth which will be stored in the color textures (cube map). A program will contain two filtering techniques: software pcf (because hardware pcf works only with depth textures) and variance shadow mapping. I found two ways of storing linear depth: 

This question is more difficult than it looks. I think a good rule of thumb is the following equation (which I made up by the way): 

For these settings we can guesstimate that, in the best case scenario, this super-simple reflection will double the computational cost of your model. But like I said before, this example is very-oversimplified, and you can find a great many effects (besides the ones I mentioned already) that will shoot your rendering time through the roof. Case and point: try to render reflections with (the default in Blender), then turn the gloss down to 0.01 and compare the two rendering times. You'll find that the one with 0.01 gloss will be much slower, but the model complexity hadn't been changed at all. 

Is it possible ? How to decode a pixel type when reading from texture ? Edit I use a pixel type to choose a shading function (standard shading - int 1, shading with normal maps - int 2, shading with uniform color - int 3). I need to use additive blending because of multiple light sources (point lights). I changed the code as you suggested: 

I also implemented standard shadow mapping without pcf which using second format of linear depth: (Edit 1: i.e. distance to the light + some offset to fix shadow acne) 

This is a slightly vague one but I am currently looking at a couple of areas of my current framework, such as spatial partitioning and UI based menus, and with UI menus it makes sense to have an event based system so you can tell if the user has clicked on it etc... its like your own small version of winforms. However for some other areas it got me thinking that I could expose a lot of evens for other things, such as when an object moves so it could be re-allocated in a spatial tree of some kind if needed, or if it needed to calculate a collision it could return an event. Anyway I quite like using events everywhere as it decouples you from having to know about who needs to know what, however I was wondering if there were any hidden gotchas with using events on the Xbox/Phone. I read a few older posts that mentioned that there are performance penalties with events on non PC platform (i.e $URL$ So is this still an issue or has XNA and the .net implementation on these machines improved in this area, as it would be nice to add events of all kinds to high level entities, like Player.HealthChanged, Creature.AnimationStateChanged etc, but I dont want to go exposing these for usage if there is a major performance overhead with doing so... 

My qualitative but more realistic reason: Increasing the mesh complexity will only improve the model quality to a point. After going past a couple million faces, there really isn't a whole lot that adding more faces will do. I'll even go so far as that say that if you use smoothing, you'll be able to get away with just a couple hundred faces for most general purposes. But the things that almost always make a difference are the lighting, material effects, and ray tracing. That's why movies will tend to use a great number of these in an attempt to approximate the beautiful complexities of the real world.