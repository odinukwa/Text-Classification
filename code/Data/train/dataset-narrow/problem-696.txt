I have been able to update statistics on the underlying system tables. This has worked in my SQL 2008 R2 or newer environments: 

Marcello, if you absolutely have to be able to achieve your goal with a single script, you could try iterating through the databases and running TSQL in the context of each database. An example is below with sp_executesql running sp_executesql from/in each database in the cursor loop. Note that @Tsql is static here, so it's declared outside of the cursor loop. However it could just as easily be built dynamically within the loop. This approach isn't easy. It might even seem a little convoluted. I'll let you be the judge. 

Background: I have numerous databases with a large number of VIEW's, and an extremely large number of SYNONYM's. For example, one db has more than 10k VIEW's and 2+ million SYNONYM's. General Problem: Queries involving (and system tables in general) tend to be slow. Queries involving are glacial. I am wondering what I can do to improve performance. Specific Example This command is run by a third party tool. It is slow in both the app, and in SSMS: 

You would need to specify if you are referring to (PaaS) or (IaaS). As already noted TDE with Azure SQL Database is only available with V12 which is only available in certain regions. If you an Azure VM is what you are working with you can implement TDE in SQL Server after the VM is built, even after SQL Server is installed. An Azure VM built from Azure's catalog with SQL Server pre-installed you are paying for the VM resources plus SQL Server licensing. With just a built Azure VM you have to provide the license for SQL Server (and the media). 

Then if you absolutely needed to keep a particular separator between the columns you could just modify your query a bit: 

If you used the IDE to perform the restore and not TSQL then you should get in the habit of verifying what server you are connected to before hitting ok. If you use TSQL then you can utilize registered servers in SSMS that will let you color code you query tabs. So you can use something like red for your production servers to indicate you need to be careful. 

The function is documented here: $URL$ ATTENTION Be aware that those two queries can give different results for periods that are exactly . The following returns : 

I'm currently running a OS X Lion Server system which ships with a built-in and not-upgradable PostgreSQL version. After years of usage I've finnaly decided to leave the built-in version and install an indipendent version. I disabled the built-in installation and downloaded the installer from EDB and followed the wizard. After many issues reguarding encoding and locales, I've finally managed how to setup a DB with no locale and UTF8 encoding. I issued the following command: 

It's clear that the issue is related to . If you have control over the dump process, a possible solution is to do not execute the file from . To do so, you simply have to run the following two queries: 

I hope this post will help other people in the future. After reading about a lot of digressions about kernel, virtual memory, RAID controllers, disk cache, WAL and other tech stuff I never found someone talking about collations. 

Remember that must be owned by user, so run . This way you can use wathever to analyze the table (if you need to). 

I'm trying to tweak the "Server Activity" Data Collection set as outlined in this blog post (I wrote the post, btw. Sorry--it's kinda long). It is working in SQL 2008 R2 and also in SQL 2014. However, when I run on SQL 2012, I get this error: 

I'm passing in a non-NULL value for , so I fully understand why I'm getting the error on SQL 2012. Questions: 

Here are two queries I have used to compare permissions between database users. The first shows the database roles (if any) to which the database user has membership. The second shows individual GRANT and DENY permissions. 

Here's an example that can be run manually in a single batch from SSMS. Just make sure to replace all occurrences of "2016" in the script with your SPID. 

2nd Attempt Here I opted for a variable to store the total number of users (instead of a sub-query). The scan count increased from 1 to 17 compared to the 1st attempt. Logical reads stayed the same. However, elapsed time dropped considerably. 

Although I don't have a local copy of the Stack Overflow database, I was able to try out a couple of queries. My thought was to get a count of users from a system catalog view (as opposed to directly getting a count of rows from the underlying table). Then get a count of rows that do (or maybe do not) match Erik's criteria, and do some simple math. I used the Stack Exchange Data Explorer (Along with and ) to test the queries. For a point of reference, here are some queries and the CPU/IO statistics: QUERY 1 

Now you would need to create a custom message using (BOL), so you can use it with the . Then your SQL Agent Alert would just check for that message text. When the alert is fired (meaning it caught the message) you configure it to notify you by email. 

You can also use that will provide much more information if you can run it during the hours noted. Returns CPU, Memory, and I/O usage of each query. You can even have it pull the execution plan of the offending query as well. Do a basic Bing/Google search for SQL Server DMV and high CPU queries or something will likely bring up some other good scripts/blog post on researching your particular situation. Just a note on your comment regarding moving the scheduled reports, I would also look at ad-hoc reports on those that users may run on their own. Especially if you are running SSRS on the same server as the database engine. I usually take a look at the ExecutionLog table in the SSRS database (default name ReportServer). There is a wealth of information in there on execution time and rendering time of the reports. 

What I get from a transaction like the one above using the previous bash command are three outputs: , and bash variable. I'll post some extract of them to make it clear what's their content. plog.log 

As @a_horse_with_no_name said, without a plan made on the 100.000.000 rows it's difficult to give other advices. Try the previous query and tell us if it's faster (and correct too). 

That time is spent by pgAdmin to pack and render data and is not the time spent by Postgres to complete the query execution. Why are you fetching 250.000 rows into pgAdmin? If you need to export the table to a plain-text file (like a CSV with header) you can execute this query: 

The problem is that I can't real-time access the content of variable. I can instead access the content of and , so the usage of doesn't make sense at all because it doesn't appear in both and but only in . Should I stick to parse or in order to find the output value from the function (removing the statement in that function because of it's uselessness) or there's an alternative method? Am I missing something? Thank you 

No, it shouldn't because that is not a server level event that the cluster would be aware of when it happens. I seem to recall coming across an article that referenced specific events that could cause a failover but can't find it right now. 

You should never try to update system tables directly, in most cases it is not going to let you as you have found. In your case you will want to build out a dynamic statement for the without knowing how many there are on the instance. However, you will need to be cautious in doing this to ensure you do not touch logins you shouldn't. This query should give you those logins that are "non-default" or installed via the installation process: 

You could likely take what Chrissy did in this PowerShell script to import large CSV files into SQL Server, then either translate it into C# or have the C# code just call this script. Dump the data into a staging table, and then split out as needed. You do not state that you need to import into multiple tables. I would expect if you have to do this all from C# then getting the initial data into SQL Server table first will make it easier to manage. 

You can play with to obtain the merging concept you have in mind. Here's a link to the official documentation (PostgreSQL 9.4): UNION clause. I think that you would like to remove duplicates (if there are duplicate entries in both tables), so probably the is right for you. 

If I connect using pgAdminIII I get no problems. The command displays as the encoding used by pgAdminIII (the default installation gave me a encoding and that's why I run the command). The problem is that I'm not able to connect to PostgreSQL using . Whatever I pass to it, I get the following error: 

Anyone can help me to figure it out? Many thanks to all Pietro UPDATE I forgot to logout and then login after editing the bash profile. The suggestion made by Daniel Vérité was right. I just edited the env variable in in order to make it visible at a global level and not only from the interactive shell. I added the following line to : 

where and are 50% and 75% of total RAM size respectively. should lay between and for your use case.Test it and give us a feedback. There are also two other moves you should make: