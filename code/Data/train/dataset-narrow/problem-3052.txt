b) Sparse types that support efficient access, arithmetic operations, column or row slicing, and matrix-vector products: 

At first sight, the total acumulated energy consumption seems to have a linear relation with time, so I suggest to try a linear regression at first. There are several libraries you can use to code it. I recommend you do it with pandas and sklearn, here is an answer related to this: answer. If the relation is not linear, so I could try with a more complex model (but I suggest to keep simplicity at first). Since you are trying to predict a temporal serie, I would try with an LSTM model. Here is a tutorial to implement an LSTM neural network with keras. 

I recommend you to use tensorflow which is under strong development and supports deep learning. You can use the high-level neural networks API Keras that runs on top of tensorflow and is very simply to use, just try a tutorial and you are going to love it. 

Not really, no. Sort of. It depends on how complex your model/data is. It's entirely possible to have a situation where a feature taken in isolation will not be correlated with the target variable, but multiple features considered together will. This is why univariate correlation is unreliable for feature selection. A trivial case that demonstrates this is a bivariate model performing a binary classification where the positive class is bounded by the right upper and left lower quadrants, and the negative class is bounded by the left upper and right lower quadranta (i.e. the "XOR" pattern): 

EDIT: Regarding your first question, both C4.5 and CART are depth-first examples, not best-first. Here's some relevant content from the reference above: 

Generate a dense grid of coordinates that fill your plotting area Score each point on your grid Plot your results, coloring observations based on your model's predictions. This serves as the background. Overlay your test data as a scatter plot 

If you have a hugh dataset you can cluster it (for example using KMeans from scikit learn) after obtaining the representation, and before predicting on new data. This code perform all these steps. You can check it on my github repo. 

Using text documents in different languages you are going to have different vector representations, unless you translate the documents previously. For example, house and maison are going to be related to different features. So a cluster algorithm is not going to recognize them as synonymous. You should try a previous translation of your reviews. The quality of that translation is going to affect the clustering algorithm depending on the algorithms you are using. If you tell me the steps you are performing in your cluster I could help you better. 

Since you know the quantity of labels (6) you can use k-means algorithm to cluster your data into 6 groups. I recommend you to represent each using the tfidf method. You can implement your code using sklearn functions. 

If you omit the indexing at the end there, the call to will concretely show you what values appear with what frequency in the column, which might help you debug what's going on if this still doesn't work. 

If you have a probabilistic cost function (e.g log-loss), I'm pretty sure backprop is an estimator for the MLE. Consider the derivation here: $URL$ If you have articles on hand, I'd be interested to see an example of an experiment where they discussed fitting a network via MLE but explicitly were not using backprop. Otherwise, I think you can assume that the authors were describing backprop when they said "MLE". Maybe they thought "MLE" sounded more academic? 

I can't find any literature discussing setting a fixed learning rate differently for the bias specifically than for the other weights, but if you asked the researcher responsible for that model their answer would probably be something like: 

It's not so much a machine learning term as it is a control theory term. A "control policy" is a heuristic that suggests a particular set of actions in response to the current state of the agent (in your case, a robot) and the environment. In the case of reinforcement learning, a policy is parameterized by the network weights. Changing the weights changes the policy, so the distribution of weights comprises a distribution over policies, hence why fitting models in this context is often referred to as "policy search". It's not uncommon to use an ensemble for these kinds of problems, in which case each component of the ensemble comprises a different policy which recommends some action, and then the ensemblification mechanism selects an action from one of these distinct policies (e.g. by vote or highest score) or combines their recommendations into an action (e.g. by taking an average). The whole ensemble can also be described as representing a policy. 

I am ploting a figure using matplotlib. In this figure I have a simple interactive feature: when you hover the mouse over some data, some annotation appears. How can I plot this and continue with my script? I am recomputing the data in a for loop (so I need to update the plot once per 5 minutes, for example). This is a minimum temple of my code: 

This is exactly why. If you use your test dataset to select the best architecture that can predict, then you are not using it as a test data, you are using your test data as a validation data. And, in the end, you run out of data to test. To make it clear: 

Once the matrices are build using one of the a) types, to perform manipulations such as multiplication or inversion, we should convert the matrix to either CSC or CSR format. 

You can also choose the method used to calculate the correlation between this: -pearson -kendall -spearman 

Parse an edgelist out of your data, load it into a dataframe with two columns (source, target), then feed that to 

I don't know what "VDM" stands for, but a simple solution for tie breaking is to randomly pick one of the tied options. 

As @Emre mentioned, RNN is a good option. It's worth noting that if the number of possible nodes in each tree is the same or at least has the same upper bound, you could use literally any architecture you want and just pass in the adjacency matrix. Alternatively, you could build an intermediate model to convert your graph into a graph embedding and then once again, you can do basically whatever you want with that. A pretty big piece of potentially important information here is what you are trying to accomplish, which could have significant consequences for how you want to represent your inputs. 

No matter the model, you can always use the non-parametric bootstrap to construct a confidence interval for any parameter, including predictions (which are actually random variables themselves but are reported as expectations). Here's the general procedure: 

Ok, I was looking for an answer and now I have it clearer: Scipy documentation does not elaborate too much on the explanation, buy wikipedia article is much more clear. For those who are looking for an answer, there are two major groups of sparse matrices: a) Sparse types used to construct the matrices: 

You have to use unsupervised learning. After that, in order to measure accuracy of your model, you should use cluster quality intrinsic and extrinsic metrics. Compute the similarity of the data in each cluster (intrinsic metric), and the dissimilarity between the data of different clusters (extrinsic metric). A good clustering throws data with great similarity in each cluster and great dissimilarity between clusters. 

If you are trying to add weights to rare or infrequent terms, which appear only in few texts, definetly you should use the tf-idf technique, which computes the frequency of each word on all the data set and after that computes a weight of each word in each text. Another case, if you want to add weights to specific words, you just can modify the tf-idf technique. 

RMSE doesn't work that way. An RMSE of 13 might actually great, it completely depends on how your target variable is scaled. For example, if your target variable was in the range [0,1e9], than an RMSE of 13 is spectacular. On the other hand, if your target is in the range [0,1], an RMSE of 0.5 is terrible. If you want to try a metric that can be more readily interpretable as having a "good" or "bad" score, try Mean Average Percent Error (MAPE). As far as why you get a lower MSE when you cross validate: you don't show us how you constructed your training and test sets, but my guess is that you basically just got unlucky and ended up with a training/test split that performed poorly on your holdout set. Your CV-MSE is clearly better than your single holdout MSE, but you should also check the spread of CV scores as well. In any event, for a dataset as small as yours I'd recommend using bootstrap cross validation instead of k-fold.