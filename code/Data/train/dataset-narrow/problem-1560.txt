Make sure you have enough free space and that you will not be destroying data. You've been warned. The problem with your setup is that the /var partition doesn't have LVM, it's a just a regular partition. Therefore you will not be able to reclaim space on that. What I suggest is that you create a new LVM partition out of the space you got from your LVM, transfer the data and use that as your new /var. 

As a matter of fact, Amazon S3 is based on Xen. When you set up your Xen server, you get the same hypervisor features but none of the "shiny" features such as on-demand provisionning and hypervisor load balancing. These tools make the difference between "cloud computing" and "my hypervisor". 

You really can't do anything. Even if you could claim unused sectors to do this, it would be a very bad idea. Unused sectors are used to replace failed sectors so using them right away would strongly increase the risk of data loss and decrease the life of your hard drive. 

We currently have a NAC server set up to authenticate against a Samba4 AD using the ntlm_auth utility and would like to make it more tolerant to network outages. Currently, when the NAC loses connectivity to the Samba4 Active Directory, every login attempt fails. This situation used to be acceptable but has become more problematic now that our network topology has changed. I have added "winbind offline logon = true" in the smb.conf of the NAC and of the Samba4 AD as per the Samba documentation. In order to test offline authentication, I added two iptables rules that drop all traffic to the Samba4 Active Directory server. When I try to authenticate using winbind, it works as expected : 

I think the main difference between TCP/IP and OSI model is that one is protocol-specific and one is as-generic-as-can-be. The question is not TCP/IP or OSI as they are not incompatible. TCP is a layer-4 protocol in the OSI model and IP is a layer-3 protocol in the OSI model. But there are many other protocols that can be adapted in these layers. For example IPX, IGMP and ICMP are other layer-3 protocols and UDP is another layer-4 protocol. Also the OSI models covers more as it includes lower layers which are very important to the networking world. One problem with the OSI layer is that it is a little bit too "extensive" as layers 5 through 7 are often merged into one. Not contradictory. Just different. 

You can get the multifunction printers that just send you the scanned document via email. Simple and efficient... 

If al else fails, you can always compile from source. I've done it before on CentOS and it actually works quite smoothly. 

This behavior is a very good way of finding a potential duplicate IP. If your computer gets no answer, then it is the only one with that IP. If your computer gets an answer, there is another computer with the same IP which is a problem obviously. Concerning RFCs, I find them horrible to read. I only use them for reference concerning specific problems. I have probably read just one from start to finish. The rest I read bits by bits. IMO, I find that the best way to learn about something is to pick up the O'Reilly or similar paper book and read it. There can be more then one RFC for a single protocol. For example IPv6 has 10 different RFC just concerning transition mechanisms from IPv4 to v6. There are many others for such things as neighbor discovery. SCTP is covered by 4 RFC also. 

Also, Windows web server also have some pretty advanced features. Thinking that LAMP is necessarily more customizable by a large margin is not always true. I'm a Linux person myself but I have seen Windows people do pretty advanced things with IIS. Having a GUI kills the 1337-command-line-mad-skills impression but remember that it may only be an impression. Just as an example, AFAIK the Stack Exchange network (so this site included) is run on Windows web servers. That goes to show that you can do great things with Windows. 

You should maybe consider DRBD with OCFS so that you can have master/master nodes. This creates no SPOF because each node has a local copy. You can also make two NFS node servers (DRBD master/standby or DRBD master/master with LB). If you have many nodes, this is the best option. 

First of all, if the owner of a file doesn't need to have write access, don't give it to them. You can do this by setting the first number of the UNIX permissions. This means that only root will have write access to their files. This will be very annoying when they will need to upgrade files but you have said that they shouldn't have write access.... Secondly, they execute their script as a specific user. If they are locked in a file using open_basedir, it will not be a problem if they can write their files. If there is a hack, it will destroy only that user's files. Even without open_basedir, they should only be able to write in their files unless you have files with 777 rights which is another problem. Thirdly, many websites need write access to, at least, certains files/folders. Per example, Wordpress needs to write in case there is a file upload. Write access for websites is not necessarily a security hole if you manage it well. suexec is a very good start and open_basedir will make sure your users with write access stay in the defined directory. 

In order to avoid having to setup a NAS system, I am considering the possibility of setting up a dual-primary with DRBD to handle RAID1 over IP. Concerning filesystems, I know of OCFS and GFS as distributed filesystem. I am leaning towards OCFS as that seems like the more reliable option. Do you have any production experience with such a system ? Am I heading towards trouble ? What should I expect/plan ? 

If you want to go the open source route, you have Convirt and Enomalism If you go the Citrix way, you have the XenCenter which looks very much like VMWare infrastructure. If you need entreprise level features, this is what you really should be using. 

Twitter displays the fail whale in reaction to HTTP Error 503. Therefore all you need is a customized error page for this error code. 

You create a "L2 Loop". Basically what happens is that broadcasts go out of one end of the cable and in the other looping back into the network. You therefore create a broadcast storm that will most likely render your network either really slow or unusable. Nothing happens hardware-wise I reassure you. You can protect yourself of such problems with Spanning Tree or Etherchannel. 

You're definently taking this the wrong way. What you're trying to be is most likely complicated and will definently add no extra security whatsoever. You should consider filtering the IPs of the people trying to connect to the proxy instead. Although it's bad idea, I think this is not possible in Squid. 

With Citrix XenServer, you have the control interface called XenCenter with which you can connect to your hypervisor and do such things. With OSS Xen, you can use the xm console command. 

The kernel will apply different priorities to specific processes. You can see this by running "top" and checking the "nice" column. The time critical OS processes will therefore have a higher priority. Therefore, the OS will run these processes before your specific applications. This will make sure that these processes happen at the right moment. This mechanism makes a core reservation pretty much useless. 

When you log in from a TACACS account, you have to type your own password twice (once at connection and once at enable). I imagine this works as well for RADIUS accounts. 

Hyper-V is a hypervisor solution only available with Windows Server 2008. It puts an hypervisor in between your hardware and the OSes which leads to modification of the kernel of the guests OSes. With a hypervisor solution, everything is virtualized except the hypervisor itself. This results in far better solution in exchange of a higher complexity. Virtual PC is a standard virtualization that virtualizes an OS in an application. It's more simple but performance is not as good. Hyper-V is only for Windows 2008 server on the "host OS" 

Very bad idea for the least. I have had a setup before where we had Apache+MySQL on the same server and it would regularly go over 10 load average and the pages would take forever to load. We split both services and put MySQL on a smaller server. Both servers would never go above 0.35 of load average. Do the math... There is a lot of loss into putting Web server + DB server on the same server. Evan also has a good point. Merging things into a machine tends to lead to many shortcuts which make scaling out a lot harder. One service = one OS is a good rule of thumb (would have said one service = one machine but now there is virtualization). 

I think this example should clear things up. This solution allows you to separate your applications using the HTTP Host used to connect to it. To sum up, if the "works" have the same code base all you can use this IMHO clean solution. EDIT : In terms of performance, Apache/Passenger needs 512MB of RAM. I found that anything below that will lead to bad performance but YMMV. I don't believe it requires more RAM if you add more virtual host. 

DRBD only works with two nodes. If you use it in a primary/primary setup, it will indeed allow you to have some sort of high availability. I wouldn't recommend GFS, it is really slow. DRBD is slow because it needs to replicate data over the network, if you add on top of that a slow FS, you're looking for trouble. By slow, I mean slower then DAS. The replication features included with MySQL are a mess. It's already quite complicated with two servers but with circular replication, it's just crazy. For DB servers, I find that the best solution is to use the replication features included with the DB system. Using block replication could potentially lead to data corruption which would most likely crash the server. Imagine this, you have a master/master setup using DRBD/OCFS. What happens if you do an insert in the same table of both hosts at the same time (+/- 1ms) ? How is DRDB/OCFS supposed to know how to order the tuples ? The replication features of the DB software will know how to handle this while DRBD/OCFS is not meant to handle such things. Therefore, use the MySQL replication features and crash test them. Try to pull the cord on one server and see what happens when it comes back online. Try to pull the cord on both and see what happens when they come back online. Do many simultaneous insert and updates on both at the same time. For more then two nodes, consider MySQL Proxy.