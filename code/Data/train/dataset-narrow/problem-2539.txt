texelFetch() exists to access texture data with texture coordinates in "image dimensions", but texelFetch skips filtering. In case of 2D textures, it's possible to use a rectangle texture sampler to access textures without normalized coordinates with filtering, but the same equivalent doesn't seem to exist for 3D textures. Is this somehow possible with 3D textures? 

where cameraSpaceAxis.w = 0; but that produces wrong results, e.g. if I give the algorithm the axis (1,0,0) to rotate around, continuously rotation around it in 1 degree steps just creates a wobbly circular motion around y and x axis; 

I've got a 2D-map, largely consisting of rectangular tiles, but with some none-rectangular objects mixed in as well (tilted lines on corners for example). Take this image as an example: 

I'm interested in how the market of casual games is split up between different platforms, both in terms of revenue as well as "amount of games published". For example, I think that Facebook and iPhone games should make a quite large percentage of the market, Facebook alone due to the *Ville games. Does anyone of you know of one or several articles on market research in that area? Or some simple web page which lists statistics? 

I can't find anything comprehensive using Google. I'm wondering what the core concepts of physically correct lighting are, and where I could read up on it. What's physically correct lighting all about? Is Phong illumination generally physically incorrect? 

I've recently implemented both versions in OpenGL (4.3) and using the (Crytek) Sponza scene as a test render scene, I had shaders that used either only a subset or the complete set of all vertex attributes defined. There was one difference to your setup: In the non-interleaved case, the attributes were inside the same vertex buffer, too. It easy to do this with OpenGL, you can just bind byte offsets to a certain attribute with glVertexAttribPointer(...) with the last parameter. An equivalent option probably exists for DirectX. Specifically, while rendering a shadow map, which only needs vertex positions (in my case) went from ~210 to ~220 FPS when using non-interleaved buffers, which is a difference of about 0.2ms. In other words, you couldn't even go from 60 FPS to 61 FPS with that speedup. When rendering the scene from the shadowing light's perspective, but using all attributes (in my case, a deferred shading geometry pass), the performance didn't change at all between the two versions. I can only guess, but I think the reason the shadow map rendering improved is because GPUs probably (like CPUs) dont just pull single bytes, words or dwords out of memory with one memory access, but instead pull more bytes (on most modern CPUs this would be 64 bytes per access) and cache it somewhere. When data is interleaved, the GPU might pull one set of position/normal/texcoord/whatever per memory access at once, of which all but the position will be wasted memory bandwidth if you only use position. One caveat: I was not memory bandwidth bound in my test case, so if memory throughput is your bottleneck, the measurements might come out differently. Overall I'd suggest you leave it the way you have it now and change it later once you notice that some optimization is necessary. Changing it doesn't take long, you only need to change how the data is handed over to GPU memory and change how attributes are bound using your graphics API. 

You need the view matrix of the class (looks like you can retrieve it via GetViewMatrix()). Take a vec4 that points in the "default direction" (0 0 -1 most likely), e.g. (0 0 -1 0) and multiply it by the view matrix. This should give you the direction of the spot light in the x y and z components of the vec4. This is assuming, though, that the view matrix here actually does what I assume it does, which is to shift the coordinate system such that its new z axis through the origin lines up with the "middle ray" of the projection cone. 

I've implemented the Cascaded Light Propagation Volumes algorithm (no indirect shadowing yet) for real-time diffuse global illumination detailed here and here. It works fine but I'm still trying to fix one artifact in particular. Short summary You may skip this if you already know how the algorithm works. The algorithm works by storing lighting information in the form of spherical harmonics in a 3D grid, where initially the data in each cell of the grid comes from rendering an extended shadow map (reflective shadow map) that also includes color and normal information, besides depth. The idea is that essentially all pixels seen by a light source are the cause of the first bounce of indirect illumination, so you store the required information alongside the ordinary depth buffer you use for shadow mapping, and sample all the data to initialize the 3D grid. The information in the 3D grid is then propagated iteratively by (for each iteration) propagating the information in one cell to all of its 6 direct neighbours (above, below, left,right, top, bottom). To light the scene using the information in the grid, you apply a full screen pass over your scene, and for each rasterized pixel you have the world space position of the rasterized surface available (e.g. from G-Buffers in deferred shading), so you know which cell of the grid a certain pixel on screen belongs to. This is working fine for the most part, here are two images without simulated GI and just a hardcoded ambient term, and next to it an image with the LPV algorithm. Notice colored reflections on surfaces, better depth detail, etc. 

I'd use these short scripts to do simple things like change the position of objects during runtime (for debugging or other purposes), create simple config files for all kinds of entities which would go like: bomb.script: 

For intentional reasons, certain units in the game I'm currently programming don't have any collision detection and response among each other. This enables them to clutter right on top of each other. This is a wanted behavior, since there will be situations in the game when the player does want them to stack like that. However, I want to make the process of uncluttering them easy for the player, so that they just have to press a hotkey or click some button on the screen and have the units disperse just enough so it's easy to select a group of them with the mouse (if they stand on top of each other one mouseclick selects all units). How could I do this without running a brute force N^2 nearest neighbor search on all units? 

I have a simple model that uses a diffuse and a normal map - in this case just two textures applied to the same material, where for the normal map I uncheck all the options in the "Influence" section in Blender and check the option for "Normals". Inside Blender the normal map is used correctly, but when I export the file to collada and try to load it with OpenAssimp, the normal map doesn't show up in the material. Also, the Collada file doesn't seem to mark the normal map as something special, it's just listed as another image under (there's no reference to it being a normal map, and there is no bump effect in library_effects). Do I need to activate some special option in blender to make sure the normal shows up in the collada file? Notes: AssimpView doesn't load the normal map either, but when I export as 3DS it shows the normal map correctly - too bad I can't use 3DS since it doesn't support bones. 

and I'm getting really poor performance. I only average about 15FPS with my Radeon HD 6850 graphics card and i5 2500k CPU. This is very surprising to me, since I average about 70 FPS doing exactly the same thing with my own custom OpenGL engine, which uses old OpenGL rendering (meaning one draw call for every single sprite). I actually expected XNAs rendering to be a ton faster, since it draws everything in one draw call. Am I rendering this the wrong way? Playing around with SpriteSortMode settings has made almost no difference, some of them just make it a tiny bit slower (depth sorting etc.).