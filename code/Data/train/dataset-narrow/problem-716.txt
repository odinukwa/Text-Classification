Or create 1 index (a,b,c,d,e) (include columns) these are the counts of distinct values for each column. total rows 1446631 , a = 366279 , b= 96 , c = 6 , e = 2 , d= 11098 Thank you 

I need some clarification regarding Always on avilabilty group in SQL SERVE 2016 connection to the secondary DB. Now i have availability group with 2 nodes. 

Update I find this Delete in the Query Store , there were 3 plans and I forced SQL to use different plan. I am wondering Why SQL choose to use the most expensive execution plan? 

when the application is busy and there are many deletes coming, the performance become so bad, My questions are. How to make this delete perform better? why the granted memory is so high, it steals the memory from the buffer pool ? Note: there is on delete cascade on the 2 tables Highschoolcourse and highschoolgrade. 

In the research group where I work, we must solve following problem hundreds, maybe thousands of times every day: given a set of putative gene names (typically a few hundred of them), flag those that are not in our (MySQL) database. This problem is solved in a number of ways by our various applications and scripts. I would like to optimize the process. The simplest approach, of course, is to iterate over the list of gene names (after removing any duplicates, of course), and for each gene name perform something like 

1 Another way to express the same constraint would be to say that the following two queries should always produce identical outputs: 

to clarify the situation described in the question above; to provide table-initialization code that responders can use to test their proposals if they so wish (it's in fact the code I used to generate Listing 2); to give an example of the sort of hard-to-maintain hack that I'm trying to avoid. 

SQL Server 2016 Enterprise Edition SP1 Database Mail was working fine with no problem until a server patching was performed. I tried to do all the troubleshooting available but still all mails got queued and no records in the log. Finally when I checked the database mail program I didn't find the executable file in the Binn folder or any other folders! My questions are how this happened and how to solve it ? 

However the trigger is preventing the user from log although the IP exists in the IPAddress table. Any idea where in the problem in the trigger 

i would like your help in the best way to tune and re-write a stored procedure. it is consuming a lot of time and don't know what should i do to make it faster 

If I am going to set an Alwayson Availability group with 3 nodes, with read-only routing. Do I need to configure a Quorum ? 

You can delete databases with DBCA which takes care of most of it. Or you can do as below, but this will do the same as removing the datafiles, redo logs, controlfiles manually. 

You can not do that, you need dynamic SQL, and be careful with that. Below is a simple example (without any error handling, for educational purpose only). 

Thats is not what you should execute. is the procedure, is simply a cursor in it. You do not execute the cursor local to a procedure, you can not even access it. Execute the below: 

counts all rows in the group where is not null. counts all rows in the group. returns the ratio of the above in percent. 

Notice how the optimizer completetely eliminated the DISTINCTs in the first and second queries, but not the GROUP BYs. Unfortunately in these cases, since the DISTINCT elimination happens in subqueries ("views"), this information is not present in the optimizer trace, just like for the original queries in the question. So now we know that DISTINCT and GROUP BY are handled indeed differently, lets go back to question 1. To be continued in the next post... (Both answers together exceed the 30000 characters limit.) 

we are going to upgrade from SQL server 2012 Standard edition to SQL Server 2016 Enterprise edition. we can't have a downtime more than 30 mins .The new SQL server will use always on availably group 1 primary and 2 read-only secondaries . I am looking for the fastest way to do this upgrade. I am thinking of creating mirroring between the old server and new primary server , then shut down the application update the connection string , fail over the DB to the new server, stop mirroring, start the application. After the application is up, start in configuring the Availability group. IS this a correct way to use 

note : the counts for IX_CoursePrerequisiteAssignment is not accurate because i just rebuild it These columns are the predicts for the top executed queries 

I don't know much about the feature, my question is as now we can send read-only workload to secondary replica . My question is what will perform better single instance with 32 cpu 1 primary and 1 secondary with 16 cpu each or 1 primary and 2 secondaries 12 cpu each Or in other way is the number of worker threads affected by the number of cpu in the primary only or the number of cpu of all members in Availability Groups 

First of all, people often fall for this, but high waits do not necessarily mean I/O problem. Second, this could be really troublesome, if you have many physical devices in the ASM diskgroup where your redo logs are, because the extents of your files will be evenly distributed on several disks. Anyway, you need to find the ASM diskgroup number and file number for your redo logs. For example my redo logs: 

PMON registers to addresses defined in and parameters. If you have an invalid address defined there, then it will not be able to register the instance in the listener. If you have nothing defined there, then it tries to register on the local machine using the default 1521 port. You could also try to set your as: 

The source of stored procedures is stored in the tablespace in the table. You need not to restore any user data at all, simply restore the , and tablespaces and skip the rest. You can then open the database with the other tablespaces offlined, and you can access the stored procedures as usual, or query or directly. 

Now, suppose that instead of the three constraints above we had the following:   1'. no two rows may have the same non-empty -value but different -values;   2'. the -value is never empty; (Constraint 2' is the same as constraint 2; constraint 1' entails adding the qualifier "non-empty" to constraint 1.) With these new constraints the following listing, where the missing -values are , would become valid: Listing 2 

...but it made no difference: the output of remains unchanged. Is there some other way to optimize this query? 

no two rows may have the same -value but different -values; the -value is never empty; the -value is never emtpy; 

...because there's no way to determine the appropriate value of for those rows where is . I'm looking for a normalized schema that would enforce the new set of constraints (and thus allow data like that shown in Listing 2). 

I'm looking for a way to enforce this constraint. (In case it matters, I'm particularly interested in solutions applicable to SQLite3 and PostgreSQL.) EDIT: Just to be clear, the description above does not preclude the existence of rows in table whose value of is not mentioned at all in table . For such values of there is no value of at all, principal or otherwise. It is only for those values of that appear in table that there must be one and only one row in table having . 

I see that i don't need all these indexes as they overlapping. What is your recommendation for best performance ? 

If I have database db1 with schema sch1. I want to create another schema sch2 which will be used by different application and the tables in sch2 will be populated from tables in sch1. So my approach is to create triggers on insert, update, delete perform this task So my questions are 1- Is this a good way to do this or there is a better way? 2- I don’t want to affect the performance on db1 so I am want to replicate it to different server and create the triggers on the subscriber DB, so replicated server will have Sch1 and Sch2 on it, Can I do that? This is on SQL Server 2012 Standard Edition Note: tables in sch2 have different names and different columns name. also more than one table in sch2 may get populated from data from sch1 

It will transfer all datafiles again. The previous duplicate already changed all your datafiles. You have a new database incarnation, with a new name, a new dbid, which was written in the datafile headers. RMAN duplicate skips existing datafiles in case of re-running a failed duplicate so it does not perform the same work again, but that is a different use case. 

Obviously, if you want, you can just stop the listener, take care of the listener log, then start the listener again. 

+2. In-Memory is an extra cost option for the database that needs to be licensed. Use it only if you have the appropriate license for it. 

Without Grid Infrastructure, you can start your databases manually, or with a custom script. You need to modify these scripts and update ORACLE_HOME in them. Also, you need to update /etc/oratab if you are on a UNIX/Linux platform. This is also needed if you use the scripts provided by Oracle (dbstart and dbshut). If you do not have any scripts for starting and stopping the databases, all you need to do is: