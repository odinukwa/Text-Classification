Docker gets lots of press and blog mentions which leads to developers getting interested in using it. For some people it is the interest in playing with a new technology or understanding how things work. For others it is a desire to add keywords to their resume. Either way, the more developers know about how things work and how they get deployed the less surprised they will be later. From what I've seen there's a decent amount of pre-existing interest in this so it shouldn't be that hard to encourage it further. 

a playbook that automates your build using . a git hook to invoke your ansible playbook, probably post-commit. 

This is totally possible. Of course you can achieve it without jenkins or similar tools being required. We're always free to reinvent the wheel. The question becomes is it worth the effort? When it helps you avoid jenkins I'd be inclined to put the effort in. (I'd also suggest looking at Concourse before doing jenkins again, but that's not what this question is about...) For your case you need two things: 

Having a CasC setup means that you are in a better position to deal with any scalability issues that should arise later. It should take you a while to max out a nagios box. As long as you haven't starved it of memory or something you should be able to get thousands of nodes on a single nagios box. How many depends on how many checks you're doing and how expensive those checks are. But if you get to the point that a single nagios box is not enough being in a CasC situation gives you the power to shard things in a convenient way and go on with life. There would be much less manual intervention than if you had to split your configs by hand. You might not reach the maximum for a single instance, but want the reliability of having local nagios boxes in each data center. Having CasC lets you do this more easily and transparently as well. 

busybox docs excerpt: In a comment the OP asked about why this wouldn't work in Alpine Linux. Since alpine is based on I looked up the busybox docs. According to the docs busybox doesn't support sending additional arguments to : 

If the system recently rebooted and your proxy server is giving 502/503s, it's most likely that the backend service failed to start. Using whatever tools are most appropriate for your OS/distro (e.g. , /, ///etc.), do the following things: 

As you can see, the invocation of can get relatively complex. See the documentation linked above for more complete information. 

You cannot "Build with Parameters" on the first build of a Pipeline job. This is a long-standing known bug with Pipelines. 

Or this example builds a job with parameters and also triggers the build asynchronously (parent job won't wait for child job to complete before moving on to the next step): 

Unfortunately I had to disable the Groovy sandbox because I ran into so many situations where methods I wanted to use in my jobs did not appear available for whitelist on the script security page. Instead of adding a bunch of individual method calls to a whitelist or disabling the sandbox, you can also use a global shared library, since global shared libraries are automatically whitelisted in the sandbox. (The shared library approach worked well for me at first, but eventually I ran into situations where I did not feel that the code I was writing was appropriate for a shared library, so I just disabled the sandbox as it had never provided any benefit to me anyway. Just as a warning, disabling the sandbox is usually fine in single-tenancy situations, but not in multi-tenancy Jenkins instances.) As for your code, unfortunately doesn't appear to have any properties pointing to the actual URL of the SCM source (I can't confirm this as I don't use SCM polling on my Jenkins instance). Instead you could try something like this: 

doesn't persist environment variables. Instead, you need to tell what steps to run with that environment by passing it a block. Here is what I think you want your code to look like: 

I know this isn't the solution you want to hear, but you're probably going to have to switch to scripted pipelines. Generally speaking, scripted pipelines are more flexible and powerful than declarative pipelines, whereas declarative pipelines are best used for simple, straightforward builds. Once you have some complexity in your build, such as these requirements you're describing here, scripted pipelines become not just superior but necessary. From the official Pipeline docs: 

centralized While the other answers are smarter and wiser for the long term I think the quick hacky CLI solution is worth mentioning. Run on one server that can reach all of the others. A good place for this would be a jump box or some other place that folks are commonly logged in anyway. Within this "central" ssh to each box in a different pane and tail whatever log files are necessary. You can use ctrl-b " to get more panes in one tab within . Now all someone has to do to check things is attach to the "central" session and they can see the whole cluster at a glance. I spent a lot of time building the web UI solutions that you are working toward, but if you need it today hacking together something with can save the day. 

Whether you allow remote or remote access to something that does you have a pretty similar attack surface. I would keep in the chain because it lets you limit the commands easily and has logging that will be vital if you need to audit things later. also has a much longer history in production. Doing something else will have less history and higher changes of unpleasant surprises. There are other things you can do to make this more secure though: 

If you are using gitlab or github you should look at their specific documentation for creating hooks, but otherwise the generic help I linked to should do it. To answer your specific questions: 

For more on the importance of information passing in IT projects see Fred Brook's Mythical Man Month. 

Yes, use the git hook. Sure. You probably want ansible to do a fresh first. Eventually you will want to add any prerequisites to be verified too. You don't have to assume anything, just add these steps to the end of your playbook. If the build fails, it will stop before wasting effort on testing anything. You could split build, deploy, and validate into their own roles within one larger playbook. This will help make each of the sets of tasks easier to manage and you could call them manually if needed for debugging or emergencies. You could do this in a myriad of ways. I'd create a text file with a date/time stamp and the results inside. You could write it to a database or K/V store. I'm not sure what you mean. 

I believe this may be a result of running Jenkins behind a proxy, which can cause legitimate requests to perhaps appear to Jenkins as cross-site requests. From the official wiki: 

It's much easier to use scripted Pipelines to do this since you can use arbitrary Groovy, but you should still be able to do this with declarative Pipelines using the step. 

where is a variable containing the name of the repository you wish to build. Things can get a little more complicated, such as if you're using folders or Multibranch Pipelines: 

According to the official GitHub Branch Source Plugin documentation, the plugin can automatically configure webhooks for you if you have your GitHub API token configured in Jenkins global settings: 

This is a known bug. See JENKINS-42878 and JENKINS-41996. This bug has been resolved upstream, which means you should be able to fix the bug by upgrading the plugin to the latest version. 

Yes, this is pretty easy with Jenkinsfiles with no need for any third-party plugins or anything along those lines: use the built-in Pipeline build step. I use this to trigger builds of projects in a dependency chain, so that after one project builds successfully, other projects that depend on it will pull in the updated dependency and build against it. Here is what this looks like in a Jenkinsfile: 

If you just need generic post-receive hooks, you can just do a regular web hook and hit the build API endpoint. For instance, if the name of your Jenkins server is and the name of the Pipeline job to trigger is , then you can fire off a POST request to . Depending on your security model, this request may or may not need to be authenticated and authorized. If you do need authentication and authorization, you can use HTTP Basic auth (unfortunately I haven't found a reasonable way to do keypair auth). If you need to trigger builds on pull requests, then you'll need a plugin. I've been using the pull-request-notifier plugin for Bitbucket to trigger builds from Bitbucket and it's worked pretty well. It works especially well in combination with the Bitbucket Branch Source plugin for Jenkins, which automatically creates Jenkins jobs for every branch and open PR of every single repository in a Bitbucket project. The combination of these two plugins allows me to automatically trigger builds as soon as a PR is opened and prevent merging the PR until the build passes (this is a pretty common workflow so I figure it's worth mentioning even if you didn't explicitly ask about it).