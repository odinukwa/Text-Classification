The command allows you to specify options which will be passed to the service's function. I don't know offhand if any of the existing "run an application as a service" solutions will pass these arguments to the application, but it shouldn't be too hard to implement. Make sure you validate the folder name carefully. You don't want someone being able to reset permissions on , for example. 

The simplest solution is probably to use a UDP broadcast. That way, Windows will have to put the data on the wire, since it has no way of knowing that there are no other nodes on the network. I'm not absolutely certain that Windows won't drop the incoming broadcast packets but it seems unlikely. You might conceivably see two copies of each packet (one sent directly and one sent over the wire) but that seems unlikely too. 

Note: the problem has been resolved, the admin of the parent domain managed to remove the DC object. I gather he used ADSIEdit. I'm working with an AD forest (in an isolated test environment, luckily, so failing all else we can blow everything away) and accidentally tried to create a subdomain using a cloned machine as the new domain controller. Naturally enough, this didn't work. But the new subdomain got created, even though AD was not successfully installed on the putative DC, and now I can't get rid of it. I started with this article. That didn't work because there was still metadata for the DC for the orphaned domain. So I found this. Unfortunately this doesn't work either. I opened AD Site and Services, opened the site in question, opened the Servers container, selected the DC in question, right-clicked on NTDS Settings and selected Delete. Clicked through "Are you sure", selected "This Domain Controller is permanently offline ...", confirmed that I wanted to do this even though the DC isn't in the domain I'm connected to, and confirmed that I wanted to do this even though this was the last DC in the domain. Then it said "Windows cannot delete object LDAP://... because: A referral was returned from the server." Anyone seen this before? Any ideas? 

OK, since you only want to reduce your access rights (rather than actually running as Peter per se) you may have some options. In Windows 7 (and Windows Server 2008 R2) the task scheduler supports this directly (via the "Do not store password" option) but I don't think there is any built-in equivalent for Windows Server 2003. Running the task this way on a different machine probably won't help because you don't get network access. It can be done in software, though, even on Windows 2003, via the CreateRestrictedToken Win32 function. A Google search found a piece of software called ulimitnt which appears be able to do what you want (via the option). Note that I've never used this program so I can't vouch for its reliability. Using this approach, the script only has access to files that grant access to both local system and to Peter. (Note that the local system account implicitly belongs to the Administrators group, so if Administrators has access that will be sufficient.) 

You can also do it from the GUI if you prefer. You don't need to turn off "Include inheritable permissions from this object's parent" but I recommend you select the "Apply to files only" option. 

The approval setting for the child groups are not actually changing in this scenario; they remain "Same as Parent". It is just that when the approval is "Same as Parent" the console shows the parent setting with a note that the setting is inherited. If you have explicitly changed the setting for a child group to anything other than "Same as Parent" then changing the setting for the parent group will not affect the child group. The "Apply to Children" option changes the setting for all child groups to "Same as Parent". 

I suggest you look into the Sysinternals tools (now part of Microsoft) in particular AccessChk and AccessEnum. I haven't used them myself, but they sound appropriate to your needs. 

I have a large set of data (about nine million files of various sizes and types; mostly student home directories) and when I run chkdsk on the volume it spends a long time on a particular index. By "a long time" I mean several hours, a substantial fraction of the total time the chkdsk takes. During most of the chkdsk you can see progress every second, but at a particular index number it just stops. If I do a chkdsk again (without any changes to the data) it stops at the same number. I've moved the data from one volume to a newly formatted one, and the same thing happens there. As I delete chunks of the data, the chkdsk time becomes shorter, but there is still a single index that takes a sizable fraction of the total time right up to the point where the disk is nearly empty. The index number in question changes sometimes when I delete a chunk of data. Is this normal behaviour? Can anyone explain it? Is there a special index that contains all files, or something along those lines? 

Replace /home/git with whatever your home directory for the 'git' user is, if it was different in the tutorial. If it's not permissions, then please let comment and we'll see what else might be the issue. 

If you change your custom cookbooks, you need to update the cookbooks on each instance. You can do this under the "Deployments" area by clicking "Run Command", selecting "Update Custom Cookbooks" from the drop-down and then pressing the "Update Custom Cookbooks" button. You can usually leave all instances checked, unless you don't want to update one for some reason. 

This is not possible in the current OpsWorks system. Even if you could, it would require the server to have the OpsWorks agent and Chef to be installed, which is handled already in the OpsWorks AMIs provided by Amazon. A bit late, but I thought I'd give it an answer. 

The Amazon free tier is only valid for 12 months since creation of your account and it has quite strict guidelines in terms of what you can do - if you've stuck by these and still being charged, it's best to contact Amazon billing and ask them why. It really depends what instances you're running, what AMI you've used to launch the instances, how much disk space you're using, how much I/O activity you have going on the servers, how much bandwidth you're using and if you're using other Amazon services. Without being able to see the activity statement, it's hard to analyse it. Usually if you give the Amazon billing department an e-mail, they will be able to help you out quite well. If you have anything more specific about the usage though, I'm happy to help you answer it. 

It's probably not running because it's unable to find the binary. Without any error log though, that's just an assumption I'm making. Try adjusting the part in your script to use its full path. I'm not sure how you installed (either via yum or manually), but you can find out its full path by running . 

If it's not set before it reaches this location block, then it will set it to a blank string. You can just as easily add a string between the quotes. I do not get any errors when doing a configuration test using this. Please let me know if you're seeing otherwise. 

You can't map a port on a public IP address to more than one private IP address, it just won't work because how is the router supposed to know which one to go to? If you're using name-based virtual hosts, you could achieve this by sitting a HAProxy instance in-front of the web instances and direct all traffic from the router to the HAProxy instance. On the HAProxy instance, you create a front-end and specify the domains and the backend to use. Then, depending on which domain is accessed via HTTP, it forwards the request to the appropriate back-end to serve the request. I do it all the time when I want to conserve server or IP resources. 

By default, when you create an account in WHM, it will configure local mail accounts and it will set the local DNS to use the local MX records. However, if you do not wish to rely on the local DNS for the MX records, then you can force WHM to always use a remote mail exchanger. In WHM, goto: DNS Functions -> Edit DNS Zone -> (select domain name) At the bottom, you will see something called Email Routing. Set this to Remote Mail Exchanger and then ensure the DNS server is restarted once saved. 

The and variables are not required as they have default values, so you do not need to include either if you don't want to. Hopefully this solves your problem. 

Below is a sample configuration for the HAProxy frontend and backends which I just wrote up. This is not the full configuration file, so please use the /etc/haproxy/haproxy.cfg as a guide to having a complete configuration. 

Rather than having a location block for each redirect, you can always just add rewrite rules into an existing location block: 

I do not believe this is possible as the name servers stored in the domain name do not have any given priority. The client will use the name server that it is given. 

In common cPanel you can find the Simple DNS Zone Editor ! It allows you to enter Autoritative records for your domain name. Redirects mysubdomain.mydomain.net to 128.128.12.12 : alt text $URL$ 

Apparently there is a bug in Apache 2.4.17. Disabling the module auto index (which is the cause of the wrong behaviour, will prevent the error. 

Use the ServerPath like you want (doc in your example) and then use RewriteRule to bind the ServerPath to your new location. Don't forget the $1 (matched part). Don't forget to load mod_rewrite in apache. 

There is a useful tool that test the different DNS nameservers available (your ISP, current configuration, DynDNS, Google Public DNS and other one). From my point of view Google DNS are pretty fast but depending on the load GoogleDNS supports my ISP Dns is sometimes faster. NameBench (Linux/Windows/Mac OS X) Output : alt text $URL$ 

Since the last Fusion Passenger update, all my Sinatra applications have stopped working on the following env: Apache (2.4.17), Phusion Passenger (5.0.21). Everything was working as expected before updating the passenger middleware. The problem is, apache directory is trying to proceed to a listing of the public directory when I'm requesting the url ($URL$ of my vhost. The ODD part: If a route defined in the Sinatra controller is requested (i.e: $URL$ passenger is started and the requested page is served as expected. Here is the Virtual Host part: 

Phusion will address the issue in the realase of Passenger 5.0.22 before Apache 2.5.0 will be released. 

I'm trying to enable DNSSEC on my authoritative dns Bind machine. So far I've done the following Tutorial : 

Context I have two linux based servers which are remotely located. It has already happened twice: after a system or kernel update, the system wasn't responding/reachable over ssh anymore (configuration errors or disk failure, ...). I had travel to the location to rescue the server. When the boot process fails, the systems lands in the emergency/rescue shell which need to be administrated locally. Question: Is there a way or a feature of the bootloader to monitor the booting process (i.e. a watchdog), if the system is stuck after some time, a timeout triggers a the system reboot with a different image with network capabilities, ssh, ... (stored on a dedicated media, i.e USB key) in order to be administrated over ssh ? Thanks 

Apache is trying to do directory listing (which has been disabled by configuration). Has someone experience the same issue ? How to make apache launch passager and serves the root of the Sinatra app ? 

you don't have to worry ! If you MAC address has really changed the only direct issues I can immagine are wireless authentication based on MAC Adresses. If so, go to te admin interface and update mac addresses you allow to join the network. May be you can also have dhcp / fixed ip issues. But if you don't use any of them, it's ok. 

I've seen you were using an alternative port for ssh so, replace the yyyy be the port the sshd deamon is running. Then restart fail2ban. 

You can't do this by indicating multiple the MX records for the same (sub)domain. The mail will be delivered to the server(s) defined by your MX record regardin only the (sub)domain, without any look at the user@... Mail routing is done on the (sub)domain part only. Possible solutions : 

Does anyone know how to solve this ? Is there any online DNSSEC tool that display more infos about the dnssec status ? 

I don't know what's wrong... Apache has the right to read the passwd file. In addition if I comment the AuthDigest... lines and uncomment the Order and Allow, apache serves the folder like a charm. Apache responds me with a 403 and doesn't prompt my browser for user/pass .. Any help ? 

I want to share file from my linux box to OSX clients. I don't want to use SMB ! I'm using netatalk 2.1.2 but apparently there is a filname length limitation : I've tried "123456789012345678901234567.avi" (31 characters long), and it works but when the filename is >31 char long. Some programs like Quicktime can't open the file. I've started the share with :