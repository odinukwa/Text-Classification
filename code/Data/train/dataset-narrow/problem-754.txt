Folks, Is it required to turn off huge pages in Mysql 5.6 server under /sys/kernel/mm/transparent_hugepage/enabled -> To never? We have a heavy transactional database and calling the data segments quite often for a few particular tables by its index. How does a memory foot-print work here? Would it help or degrade the performance. In AnonHugePages: 109242368 kB ( I see entire innodb buffer pool size is occupied under AnonHugepages). No specific issues found so far due to memory. But need an advise on when to keep it on and when to turn it off. 

Restart mysqld. Every four hours let cron to exec a script by doing below steps one by one. 1). "flush logs" on the server. Get file names from binary logs generated from restart to latest - 1. (Make a note of latest - 1 file name for later reuse). 2). Convert those logs to .sql .ie., mysqlbinlog mysql.0000021 > mysql.0000021.sql ; mysqlbinlog mysql.0000022 > mysql.0000022.sql ... so on until latest - 1. 3). Now replace string "use A" to "use B" like. sed -i 's/use A/use B/g' mysql.0000021.sql 4). Now apply them to your DB instance. mysql -uwill -psmith << EOF source mysql.0000021.sql ; EOF 5). Capture errors in the below manner at the starting of the script. exec 7>&2 exec 2> ERR_FILENAME.txt 6). And now if [ -s ERR_FILENAME.txt ] then sendmail ; fi 7). For the next iteration use the binary log file that you took a note at step 1 as a starting file name. IMPORTANT 

1. Will it mess up the sync in any way - On a high availability architectural view "YES". As long you have Master up and steady you might not end up problems. 2. Will the changes get overwritten from the master during the next replication event 

Ah! This one could be application framework has the value passed at the time of creating connection to DBs to set wait_timeout or interactive_timeout for their session level, this overrides default mysql settings. If you enable general log on any test environment and have their application to point to that DB. You will find wait_timeout or interactive_timeout setting such values. Short Term Fix: 

Now the sync is broken on Slave. Your SQL_SLAVE_THREAD stops. Or anyother conflicting statements tries to append it will get broken, above is a better example for that. But if you get updates like below on Master, for sure your Slave data is consistent with Master. 

SQL Server 2014 SP2. As the titles says, we have converted one of our database tables to be in-memory. After we did this, the corresponding memory-optimized filegroup takes up 1GB on the disk, but on a larger server it's up to 4GB). I suspect it has to do with the number of CPUs. The table is EMPTY! The structure of the table is nothing special, something like this: 

This seems a browser problem to me. When I render a reporting services report in IE11, the "Export", "Refresh" and "Export to Data Feed" buttons show up under each other taking up a lot of extra screen space. Does anybody know what could be causing this? I am running Reporting Services 2012: 

I am trying to deploy my 2012 SSIS project to the Integration Services catalog on the SSIS server (also 2012). I am getting the error below. I checked that there is plenty of space in SSISDB and msdb databases as well as on the disk. Any ideas what might be causing this? 

OK, here is how I ended up addressing this. I basically wrote a script to compare row counts between tables on publisher and the distributor. One per-requisite is to have a linked server between the two servers. In my case it is called "distributor_ls" (since distributor is on the same server as the subscriber). First part of the script gather information about published databases into a temporary table called #tmp_replcationInfo. The credit for this first part of the script belongs here Second part of the script then uses this information to gather the table row counts into a temporary table called #result for those tables where row counts differ. The differences are then displayed. I know it may not be not super-reliable because it relies on row counts stored in sys.partitions table, but it does what I need. I hope somebody finds it useful. 

Initially everything seems to be ok and the results indicate that all of the tables match. Next I go over to the subscriber and manually delete a few rows from some of the tables. I manually verify that the row counts in my tables are now different between the publisher and the subscriber. Finally I run the sp_publication_validation procedure again and .... it says that everything is still OK. This is wrong! I also tried to return both rowcnt and checksum and it still doesn't detect the fact that there are differences between the publisher and subscriber. I appreciate any ideas. Thank you! 

[difference this time is you are granting with the password for REPLICATION SLAVE GRANT] Show Grants: If you do show grants; after you give GRANT REPLICATION command. You should get below grants from same slave host via CLI. 

Table will be partitioned by RANGE weekly. On Master keep latest week's partition. And drop earlier partitions in below way. 

Depends upon the criticality of Master and your wallet. If reporting queries are bothering much to Master, you may add a slave to it. If your business grows then chances of reporting might require different indexes created on main data hence you might need to handle different backup policies for Master and Analytics slave. 

Now I want to retain the structure by removing 4th week prior DATA ONLY . But I want to see the structure there WITHOUT data for that partition. 

Kill all processes for mysql in ps -ef | grep -i mysql If you see any any ibdata file generated under your default data directory you may remove them also iblogfile1,2 since its a fresh installation no need to safe backup them. chown -R mysql:root /var/lib/mysql chmod -R 775 /var/lib/mysql also the same permissions to /etc/my.cnf. Now try to start the instance by issuing /etc/init.d/mysql start and tail -f /var/log/mysqld.log. It should start without any issues if you get any error message . 

If usage means you can only connect to the DB Server and not anything else. Whereas in this case REPLICATION SLAVE will require to read and pull binlog events from Master. To Fix this: On Master Execute: 

Since you are using to take a consistent value of master status. The internals of mysqldump will issue below commands to mysql server. 

Doing by this way the SELECT statements are performed in a nonlocking fashion, but a possible earlier version of a row might be used. Thus, using this isolation level, such reads are not consistent.When you say not-consistent it means recently changing records i.e.. DML transactions that are currently in process will not be read. I assume which is in your case it is acceptable. This is also called a “dirty read.” Otherwise, this isolation level works like READ COMMITTED. If I were to be you, the below order is what I follow. 

I guess you are under a wrong impression that it is a OS related issue. Check your server bit if it is 32 bit and if you try to install 64bit package percona's xtrabackup . It would end up throwing such error messages.