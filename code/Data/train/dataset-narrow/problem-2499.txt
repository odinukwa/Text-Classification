First: all your inputs are either 0 or 180, and the midpoint gate always gives a point on the arc between its two inputs. So any intermediate value stays on the arc between 0 and 180, there is never wrap-around, and we may just assume that each input is a bit and the gates return $(a+b)/2$. Then there is a gate at the top that tests whether its input is in a range $[c, d]$. Notice that any such circuit recognizes a set of the form $S(a, c, d)=\{x: c \leq \langle a, x\rangle \leq d\}$ where $a$ is $n$-dimensional and $c$ and $d$ are scalar constants. The way to see that is that the input to the top gate is a linear function of the input. In fact your setup is significantly more restricted, but nevermind that. The family of sets $S(a, c, d)$ has VC-dimension at most $2n+2$: halfspaces have VC-dimension $n+1$, and taking all pairwise intersections of sets in family can at most double the VC-dimension. Therefore the family of sets computed by your circuits cannot possibly shatter all $2^n$ possible binary inputs, even for $n=4$. There are boolean functions on $4$ bits that a circuit of this type does not compute. 

This is the Min-Disagreements version of the correlation clustering problem (on complete graphs), defined by Bansal, Blum, and Chawla (full version). They give a (huge) constant factor approximation for the problem and prove it's NP-hard. Charikar, Guruswami, and Wirth show the problem is APX-hard, and improve the approximation factor to 4 via region growing. There is a beautiful and simple combinatorial algorithm (a variant of QuickSort), due to Ailon, Charikar, and Newman, that gives a factor 3 approximation. I believe the current best known approximation ratio is due to Chawla, Makarychev, Schramm, and Yaroslavtsev, and is around 2.06. 

Problem $P$ is easy to solve fairly well, since we have an algorithm that finds a good approximation. Algorithm $A$ is good, since it finds a good approximation. 

And the entropy? 2.7 bits per pixel. File size in practice? 344923 bytes for 1M pixels. In a truly best-case scenario, with some cheating, we pushed the compression ratio to 1:3. 

Motivation: A coauthor edits a manuscript and I would like to see a clear summary of the edits. All "diff"-like tools tend to be useless if you are both moving text around (e.g., re-organising the structure) and doing local edits. Is it really so hard to get it right? 

Just allocating memory without touching it is very cheap. The cost should be negligible in any kind of theoretical algorithm analysis. You pay the serious penalty the first time you touch each page. The CPU will generate page faults and the operating system will have to do the one-time setup for each memory page. This is a non-trivial cost. Large constant per memory page. In general, you should make sure that your algorithm touches each memory page a large number of times; otherwise the memory management cost will dominate. 

Conversely, if we can get the benefit of at least $nM + m - k$, then we have at most $k$ nodes in $\{S_1, S_2, ..., S_m\}$ and all of the nodes in $U$ must have outgoing edges; this gives a set cover of size at most $k$. 

(Disclaimer: I am not an expert, feel free to suggest corrections, or write a more comprehensive answer if you are.) Extending computability and complexity to the real numbers (which is a first step in doing analysis) is tricky and has been done in several inequivalent ways. One is the Blum-Shub-Smale (BSS) model, which augments Turing machines with the ability to store and perform algebraic operations with real numbers. The resulting theory is algebraic in flavor, e.g. all computable functions are piecewise semi-algebraic. The model is interesting but has some strange features that make it seem unrealistic, at least as a model of how computers actually deal with real number computation. For example it allows computation with uncomputable constants: the constant function with value Chaitin's constant is computable in the BSS model. On the other hand, $e^x$ is not computable in the BSS model. Another approach can be found in the field of computable analysis, and I think that's what you are looking for. Check the book by Weihrauch for an introduction (the introduction and the chapter on computable real numbers are available on the linked page, and will give you a good idea of what is going on). There still are several not quite equivalent models here, but the rough idea is that rational numbers have finite representation, and then you construct the computable reals the same way you construct the reals as a completion of the rationals. So, analogously to defining a real as a (equivalence class of) a Cauchy sequence of rationals, a computable real is given by a Turing machine that computes arbitrarily good approximations to it. Then a function $f: \mathbb{R} \to \mathbb{R}$ is computable if a Turing machine can compute arbitrarily good approximations of $f(x)$ given (as an oracle) a machine that computes arbitrarily good approximations of $x$. There are fascinating connections between computable analysis and classical/modern analysis and many other fields, for example algorithmic randomness. One simple example of a theorem is that all computable functions are continuous. To give a more sophisticated example (without actually going into the details), there are interesting counterparts to classical theorems in analysis, e.g. an analogue of Rademacher's theorem is that all computable Lipschitz functions $f:[0,1] \to [0,1]$ are differentiable at all algorithmically random points (for the right notion of algorithmic randomness). Formulating a complexity theory for real functions is, AFAIK, even trickier. This is related to the fact that computing a real function is a higher-order computation (since it takes a Turing machine as input) so the bit size of the input is not usually the right thing to measure the runtime against. Check this paper by Mark Braverman for one approach to defining efficient real computation. At this point I am way out of my depth to say more, so I will stop. 

Given two strings $x$ and $y$ and integers $k$ and $K$, I would like to solve the following problem: 

That is, can be expressed in the "compressed" notation using, for example, any of the following forms: or or . The compressed notation is unambiguous in the sense that you can recover a semantically equivalent expression by simply adding a sufficient number of leading and trailing parentheses to make everything balanced. Of course, might represent or , but adding extra parentheses does not change the language defined by the expression. The "compressed" notation is a regular language. And yes, this is cheating. 

There are two interpretations of the claim "algorithm $A$ finds an $\alpha$-approximation of problem $P$": 

(source) For example, here we can read the following approximations of the pairwise distances between the four stations (using A, B, C, D for brevity): 

Hence it is NP-hard to find an optimal $s$. The same idea can be used to show that the problem is hard for any $0 < p < 1$. 

However, something to keep in mind with any approach. As you can't identify an arbitrary set of missing events with 32-bit labels, then you might have different kinds of weaker goals like these: 

You can add randomness to your algorithm, and combine it with all of the above. Then you will get, e.g., worst-case expected running time (worst-case instance, but averaged over all possible sequences of random coin flips in the algorithm) and worst-case running time with high probability (again, worst-case instance, but probability over the random coin flips in the algorithm). 

Clearly $P$ has an integer point if and only if $\phi$ is satisfiable. Also the number of integer points in $P$ equals the number of solutions of $\phi$. So as long as the ratio between the diameter and the width of $P$ is polynomial, this answers both your questions. Let us then compute its diameter and width. $P \subseteq [0,1]^n$, so the diameter of $P$ is at most $\sqrt{n}$. By width, I am assuming you mean the standard $$ \min_{\theta: \|\theta\|_2 = 1} \max_{x, y \in P} \langle \theta, x-y\rangle, $$ i.e. the smallest distance between two parallel hyperplanes that sandwich $P$. Notice that any $x$ which satisfies $\frac13 \le x_i \le \frac23$ for all $i$ is in $P$. So, for a $\theta$, pick $x_i$ to be $\frac23$ if $\theta_i > 0$ and $\frac13$ otherwise, and pick $y_i$ to be $\frac13$ if $\theta_i > 0$ and $\frac23$ otherwise. Then: $$ \langle \theta, x - y \rangle = \sum_{i = 1}^n \frac{|\theta_i|}{3} = \frac13 \|\theta\|_1 \ge \frac13 \|\theta\|_2. $$ So the width is at least $\frac13$. 

The Batson-Spielman-Srivastava barrier function method has had a number of applications to geometry and functional analysis, arose in computer science, and is a very original form of potential function argument, reminiscent of the method of pessimistic estimators. Moreover, it goes against the conventional wisdom that analyzing the characteristic polynomial of random matrices is intractable, and one is better off looking at matrix moments instead. The barrier function method was first developed to prove the existence of (and construct in deterministic polynomial time) sparsifiers of graphs that preserve their spectral properties. Such sparsifiers were motivated by algorithmic applications: essentially any algorithm that needs to compute cuts approximately can be sped up by being given as input a sparsified version of the original input. Beyond sparsifiers however, the method has had numerous applications, many of which are surveyed by Assaf Naor in this paper. Some prominent examples are construction of weighted expander graphs, approximate John decompositions of the identity with fewer points, dimension reduction of subsets/subspaces of $\ell_1^n$, a tight version of Bourgain and Tzafriri's restricted invertibility principle. For all of the above applications, the barrier function method yields essentially tight bounds, gives an efficient deterministic algorithm in addition to an existence proof, and often provides a more elementary proof than prior methods (although not without some hairy calculations). Fast forward to 2013, and the barrier function method, on steroids, and augmented with the machinery of interlacing polynomials, was used by Marcus, Srivastava, and Spielman, to solve one of the most notorious problems in functional analysis, the Kadison-Singer problem. This problem arises from fundamental questions in mathematical physics, but it goes much further - it is known to be equivalent to dozens of problems all over mathematics. Not to mention that many analysts (including Kadison and Singer) did not even think the problem had a positive resolution (the cited survey by Cassaza et al. speculates on possible counterexamples). 

Therefore solving the problem optimally in a certain graph family $F$ is exactly as hard as finding a minimum dominating set in the same graph family $F$. In particular, the problem is NP-hard even in the case of bipartite graphs. (However, (in)approximability results related to dominating sets cannot be directly applied here. In essence, we have changed the objective function from $\min |D|$ to $\max n-|D|$.) 

Now each pair $ab$ with $a \ne b$ occurs exactly once. Similarly, each pair $a$-above-$b$ with $a \ne b$ occurs exactly once. Hence this is a valid construction; alphabet size $n$ and an $n \times n$ square. Moreover, it is optimal. In an $n \times n$ square there are $n(n-1)$ horizontal pairs, and each of them must be different. If we had an alphabet of size $n-1$, we could only construct $(n-1)^2 < n(n-1)$ different horizontal pairs. 

I am here interested in relatively small and inexpensive details – something that conference organisers could have easily done if only they had thought about it in time. For example, it might be a useful piece of information that could be put on the conference web page well in advance; a five-dollar gadget that may save the day; something to consider when choosing the restaurant for the banquet; the best timing of the coffee breaks; or your ideal design of the conference badges. We can cover here all aspects of conference arrangements (including paper submissions, program committees, reviews, local arrangements, etc.). 

There are enough connections between information theory and computational complexity to merit a graduate course, e.g. this one: $URL$ 

To get the obvious out of the way first, to show that the $n\times n$ matrix $M$ is PSD, it is enough to show that it is the Gram matrix of $n$ vectors, i.e. $m_{ij} = \langle u_i, u_j\rangle$ where $u_1, \ldots, u_n \in \mathbb{R}^n$. (This is a "sum of squares" proof of positive semi-definiteness, because it shows that the bi-linear form $x^T M x$ can be written as a sum of squares of linear functions of $x$. I am wondering if you meant something more sophisticated by "sum of squares".) You get a useful class of PSD matrices from positive definite functions. A function $f: \mathbb{R}^d \to \mathbb{R}$ is positive definite if for all sequences $u_1, \ldots, u_n \in \mathbb{R}^d$, the $n\times n$ matrix $M$ defined by $m_{ij} = f(u_i - u_j)$ is PSD. It is not very hard to see that if $f$ is the Fourier transform of a finite measure, then $f$ is positive definite. I.e. it is sufficient that there is a finite Borel measure $\mu$ on $\mathbb{R}^d$ such that for all $x$ $$ f(x) = \int{e^{i\langle x, y\rangle} d\mu(y)}. $$ A classical theorem of Bochner shows that this condition is necessary as well. A special class of positive definite functions are the functions $f(x) = g(\|x\|_2)$, for which the matrix $M$ becomes $m_{ij} = g(\|u_i - u_j\|_2)$. Another classical result of Schoenberg shows that such an $f$ is positive definite if and only if there exists a finite Borel measure $\mu$ on $[0, \infty)$ so that for every $t \geq 0$ $$ g(t) = \int_{0}^\infty{e^{-t^2s}d\mu(s)}. $$ The original motivation for Schoenberg's theorem was to determine what functions $g$ have the property that for any $u_1, \ldots, u_n \in \mathbb{R}^d$, the metric space with metric $d(u_i, u_j) = g(\|u_i - u_j\|_2)$ embeds isometrically into Euclidean space. This blog post has a simple proof of Schoenberg's theorem and many links. 

This is a non-answer, but it might help to understand the question (assuming that I understood it correctly). Here is a simple but slightly non-trivial example: 

Spoiler: If this does not sound that much like a "false belief", I suggest that you have a look at conferences such as PODC and DISC, and see what kind of work people are really doing when they study theoretical aspects of distributed computing. A typical problem setting is the following: We have a cycle with $n$ nodes; the nodes are labelled with unique identifiers from the set $\{1,2,...,\text{poly}(n)\}$; the nodes are deterministic and they exchange messages with each other in a synchronous manner. How many synchronous communication rounds (as a function of $n$) are needed to find a maximal independent set? How many rounds are needed to find an independent set with at least $n/1000$ nodes? [The answer to both of these questions is exactly $\Theta(\log^* n)$, discovered in 1986–2008.] That is, people often study problems that are completely trivial from the perspective of centralised algorithms, and have very little in common with any kind of supercomputing or high-performance computing. The point certainly is not speeding up centralised computation by using more processors, or anything like that. The goal is to build a complexity theory by classifying fundamental graph problems according to their computational complexity (e.g., how many synchronous rounds are needed; how many bits are transmitted). Problems like independent sets in cycles may seem pointless, but they serve a role similar to 3-SAT in centralised computing: a very useful starting point in reductions. For concrete real-world applications, it makes more sense to have a look at devices such as routers and switches in communication networks, instead of computers in grids and clusters. This false belief is not entirely harmless. It actually makes it fairly difficult to sell work related to theory of distributed algorithms to the general TCS audience. I have received hilarious referee reports from TCS conferences... 

assigns high complexity to a random function does not assign high complexity to an easy function can be easily computed from the truth table of a function 

I think $S_n$ can be written in terms of inequalities in the obvious way. Let $$ Q_n = \{(x, y): x = \sum_{i = 0}^{n-1}{2^i y_i}, \forall i: 0 \leq y_i \leq 1\}. $$ I claim that $Q_n = S_n$. First, obviously all $(x, y) \in S_n$ are also in $Q_n$, so $S_n \subseteq Q_n$. Second, fix a point $(x^*, y^*) \in Q_n$. Consider the probability distribution over $\{0, 1\}^n$ induced by picking $y_i = 1$ with probability $y^*_i$, independently for each $i$. If $y$ is sampled from that distribution, then, by linearity of expectation, $$ \mathbb{E}\ x = \mathbb{E} \sum_{i = 0}^{n-1}{2^i y_i} = \sum_{i = 0}^{n-1}{2^i y^*_i} = x^*. $$ Therefore $(x^*, y^*)$ is in the convex hull of $S_n$, which proves $Q_n \subseteq S_n$. BTW, this didn't really use anything special about $S_n$. Whenever you have the convex hull of the set $\{(x, y): y \in S, x = Ay\}$, and the convex hull of $S$ has a concise extended formulation, the same thing will work. The main point is that $x$ is a linear function of $y$. Here we used the fact that the cube $[0, 1]^n$ has a very easy formulation in terms of inequalities. 

Edit: There are some natural variants of the question that have been mentioned in the comments; let's give them some names: 

(Moved from a comment.) Of course you can solve any LP by using a general-purpose LP solver, but specialised algorithms typically have a much better performance. It is not only about theoretical asymptotic performance guarantees, it is also about practical real-world performance. Algorithms such as the Hungarian method can be extremely streamlined and they are relatively easy to implement correctly and efficiently. You can also often avoid issues like using exact rational numbers vs. floating point numbers; everything can be done easily with integers. 

Some background: A simple $k$-matching is a subset $M \subseteq E$ such that each node is incident to at most $k$ edges in $M$. A simple $1$-matching is a matching in the usual sense. For more information on (simple) $k$-matchings and algorithms for finding them, see, for example: 

Yes, just think how you would use a SAT solver to solve the independent set problem. If you have one variable per node, it should be clear how to write constraints that guarantee that variables that are true indeed form an independent set. I think the only non-trivial part is counting: how to make sure that your independent set is sufficiently large. For that you can use BDD-based techniques; see e.g. Section 5.3 of this paper.