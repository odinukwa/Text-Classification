We are working a data center migration and are to the SSRS boxes and I am having trouble. SSRS is installed on a server alone pointing to the ReportServer and ReportServerTempDB on a different SQL Engine. We migrated the SSRS server to a new data center last night and removed from old domain and added to new domain. I have installed SQL Engine on the SSRS server itself and restored the ReportServer and ReportServerTempDB from the old SQL Engine location in the old data center. My issue is I cannot get the SSRS service to start as I believe it is trying to use a service account that is in a domain it cannot see anymore and a ReportServer database it no longer can access. My question is, can I somehow change the pointer for the service account being used and the pointer to the SQL Engine being used. The Report Services Manager GUI only gives me an option to change service account and everything else is greyed out. I changed the service account there, but the service will still not start. Any ideas will be helpful. 

By all accounts this may be a bugged behavior in the sys.stats_columns DMV. This appears to be causing problems when a statistic is updated by way of the parent index. I believe this to be due to the mechanism with which the statistics are being updated in a constraint change. Should you create a statistic manually and then wish to change the columns you must first drop and re-create which forces the meta-data to be updated in the DMV in question. In the operation you have demonstrated there appears to be a situation where the metadata is not updated under any circumstances (DBCC *, CHECKPOINT, server restart, statistics update through parent index change, etc) once the change has been made. From my initial testing I can find only one case when the metadata is properly updated which is the drop and re-create scenario. You can take a look at the Connect item on the issue and up-vote as appropriate. There is a work around query posted there but its mechanism is based on matching the index name to the statistic name and utilizing the index meta-data. 

We are trying to determine our best MAXDOP and Cost Threshold settings for an EDW box. I setup a job to clear the wait stats before the build started and then capture the wait stats after the build was complete. Our server is Windows 2008 and SQL 2008 Enterprise. (Yeah I know, it is are oldest box and we are working on that). There are 8 CPU and 132GB of memory for the box and 118GB max memory for SQL. The wait stats after the run are below. Would you tweak cost threshold (Current 50) or change maxdop (Current 4). 

I am looking for some assistance with this code. This code builds the proper command line syntax and executes fine. However, when running via TSQL I get Incorrect Syntax near '/'. 

We currently have a 4 Node cluster used for our Availability Groups. 2 nodes in local data center and 2 nodes in remote data center. Currently the 2 nodes local are Synchronous and the 2 remote nodes are ASynchronous. They are setup in a multi-subnet network. We are wanting to make one of the remote nodes Synchronous so that we can manually failover to the remote data center to run production traffic. Our testing turned up a network issue that we did not think about. We use DNS aliases to connect to the listener. (ex. product.company.com) that resolves to the primary through the listener IP address. When we failover manually to the remote node the listener IP stays in the local subnet IP. My question is how can we configure our DNS Alias to automatically resolve to the remote listener to hit the remote as primary with either CNAME or other DNS options. Below is a crude diagram I tried to work up. 

Digging into the mechanics of this wait you have the log blocks being transmitted and hardened but recovery not completed on the remote servers. With this being the case and given that you added additional replicas it stands to reason that your HADR_SYNC_COMMIT may increase due to the increase in bandwidth requirements. In this case Aaron Bertrand is exactly correct in his comments on the question. Source: $URL$ Digging into the second part of your question about how this wait could be related to application slowdowns. This I believe is a causality issue. You are looking at your waits increasing and a recent user complaint and drawing the conclusion potentially incorrectly that the two have a relationship when this may not be the case at all. The fact that you added tempdb files and your application became more responsive to me indicates that you may have had some underlying contention issues that could have been exacerbated by the additional overhead of the implicit snapshot isolation level overhead when a database is in an availability group. This may have had little or nothing to do with your HADR_SYNC_COMMIT waits. If you wanted to test this you could utilize an extended event trace that looks at the hadr_db_commit_mgr_update_harden XEvent on your primary replica and get a baseline. Once you have your baseline you can then add your replicas back in one at a time and see how the trace changes. I would strongly encourage you to use a file that resides on a volume that does not contain any databases and set a rollover and maximum size. Please adjust the duration filter as needed to gather events that match up with your waits so that you can further troubleshoot and correlate this with any other teams that need to be involved. 

We restore a database daily that we download from an FTP site. I would like to build a history table that tracks when the database is restored on our SQL Server. This is easy. My issue is sometime the download is incomplete and therefore the restore does not work and throws an error. I would like to add logic to my history process to insert failed if the restore fails. Not sure how to handle this logic and am looking for assistance. This is the error we get most frequently when the failure occurs. 

I have recently automated the SQL patching of ~200 development servers. Currently we have 6 servers that run the Microsoft Master Data Services feature. After a SQL patch is applied there is a manual step to go in and open Master Data Services Configuration Manager and do an upgrade of the schema. Since this kills my automation for those 6 servers I am trying to find out if there is a programmatic way to run this upgrade process after applying the SQL patch. 

To answer the question of will you get the same results running an analysis such as Brent's sp_Blitz scripts in a non-production environment the answer will lie in what information you are trying to gather and what that information depends on. Take the two examples below as a means of showing the difference in results (or no difference) based on the type information sought. 1) Suppose I am interested in analyzing the indexes in the database to determine if I have duplicate indexes. To do this I would look at the database metadata which would be part of the database structure itself. In this case the results should be the same regardless of environment because the data that is being queried to draw the conclusion is part of the physical database structure. Brent's sp_IndexBlitz does this as one of the steps in its analysis, among others. 2) Suppose I am interested in analyzing an index to find out if the index is utilized. To do this I would examine the DMV (sys.dm_db_index_usage_stats) to determine if the index in question has any scans, seeks, lookups or updates. Using a combination of this data I could then determine if this index is making inserts run slower or is a benefit to select performance in a way that justifies its overhead. In this case though the data will be different in results between production and the non-production environment unless the exact same workload and configuration is running in both environments. Brent's sp_IndexBlitz also performs this same check and will provide the usage details based on the settings specified. To further clarify on the "why would data be different" which seems to be a sticking point here lets dig in to what DMVs are. A DMV is at high level just an abstraction layer that provides a view of the instrumentation that SQL Server has running. With this being said as mentioned by Tony when SQL Server restarts the data that is within these DMVs is not persisted. For this reason when you perform a backup and restore it is functionally equivalent to a server restart for the purposes of the discussion around DMVs. Once the database has been decoupled from the original host of this instrumentation data the data would be lost unless it was persisted elsewhere. This is mentioned in Microsoft's documentation as shown below: 

My company is currently going through a data center migration. We are now down to the SQL Servers and this includes SSRS. The bad thing that is going to hit us is the current SSRS server is being migrated to a new domain. So all users/groups that have access to SSRS are in DomainA and after the migration the SSRS server will be in DomainB. All user and groups were migrated from DomainA to DomainB, supposidly using the preserve SIDs. During a test we could not get this to work. So DomainA\User1 does not work when server is changed to DomainB. We have over 70,000 combinations of user permissions and group permissions across all of our reports. Does anyone have any suggestions or ideas on how we can properly migrate this server from DomainA to DomainB without having to manually duplicate the security settings. Note: This is a virtual machine and is being replicated from one VMWare cluster to one on the new network and then will be dis-joined from DomainA and joined to DomainB. Service accounts will be changed to DomainB and backup of encryption key will be restored after service account is changed. 

Under the circumstances that you have indicated have you looked at VSS backups through a VSS provider that is either 3rd party or Microsoft based? You can perform a COPY_ONLY backup that will not break your production recovery chain and you should end up with a backup that of all of the databases that you can then recover elsewhere to within your reasonable margins. Keep in mind that a VSS backup has some of the same mechanisms and downfalls as database snapshots in that a very active database could cause a disk space issue due to the sparse files used. Take a look at the TechNet resources on the SQL Writer service here and VSS backups of SQL Server here. To do this through Windows Server Backup you will follow the wizard steps for a manual backup ensuring that you select VSS copy backup on the custom configuration settings under VSS Settings. This will allow your Windows Server backup to not interfere with any other backups taken on the server. See Windows Server Backup reference for details.