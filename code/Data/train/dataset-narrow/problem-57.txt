Hence there is no need to generate 1 more buffer object for the texture, just pass the same buffer but change the binding points to However do note that you need to use appropriate Memory barrier calls as SSBO reads and writes are incoherent memory accesses. Don't know much about these but you can find more information here 

To be honest, terms like these are very confusing as they aren't clear cut and on one side of the border. They are more grayish. I'm gonna tell you how I convinced myself, as I too had this confusion as soon as I read your question. But I managed to convince myself through this argument. First of all we are gonna clear up 4 terms, Radiance, Irradiance, Differential radiance and Differential Irradiance. "Radiance" is what you say associated with a certain direction. To be more formal and according to wikipedia, 

It was getting a little big to fit in the comments so posting it as an answer instead. Might not be a solution to your problem but the concept is related. People usually forget that whenever you define a transformation matrix by placing respective basis vectors in respective columns, you are specifying that with respect to another basis usually the right-handed or left handed world coordinate space. For example your above matrix \begin{bmatrix} r_x & u_x & -l_x & 0 \\ r_y & u_y & -l_y & 0 \\ r_z & u_z & -l_z & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix} Reason why you put sign in the third column is you are working with a right handed coordinate system and the camera is looking in the negative direction (the -Z axis). So all these basis vectors are also with respect to another basis vector. Let's make this matrix simple so we can understand what happens when we multiply a matrix with a vector. Let the matrix $M$ be \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & -1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix} This is your camera initially without any rotation/translation etc. This means your camera space $+Z$ axis maps to world space $-Z$ axis. Now consider a camera space coordinate $[2,3,6,0]^T$. Multiplying this vector by the matrix gives you $[2,3,-6,0]^T$. This is the world space representation of $[2,3,6,0]^T$. So as Nathan pointed your matrix transforms from camera space to world space. To do the inverse we just take the inverse of the matrix which is the transpose if the basis is ortho-normal. One of the reasons the transformation matrix is built this way is because it's much easier. It's easier to think about the transformed basis vectors of camera or any other space with respect to the world, write the matrix then invert it to go from the world space to that target space. 

This means a lot less calls and the driver can do some optimizations because it knows which vertex formats will be used. (some hardware has to do software vertex pulling so suddenly having a new format will require a patch of the vertex shader). In version 4.3 (or with extension ARB_separate_attrib_format) there is a new way to bind the vertex attributes where the vao holds the format separately from the buffer binding to help exactly the above mentioned hardware: 

Hard shadows are simple that only needs a point light. How it's done is by rendering the scene from the point of view of the light and only keep only the depth information. This is the shadow map. Then when doing the actual rendering you calculate the point on the triangle in world space and find where it would be on the shadow map. Then you sample the depth from the shadowmap at that point and compare it to the distance to the light. Soft shadows are harder. They require figuring out how close you are to the edge of what would be a hard shadow and adjusting the light value based on that. There are several techniques for that. One of which is simply sampling multiple times. 

Technically any file format where you can dump the entire file into a VBO and then render from that will work for the .bin files. Unfortunately those formats are less well known than they should be. Then you only need to adjust the bufferview elements to reference the proper subset of the file 

The simplest solution is to use a lookat matrix. This is a matrix calculated from a "eye" point, a "target" point and an up vector. The resulting matrix will then make the Z axis point from the eye to the target and the y axis in the general direction of up. 

If you are talking about an oblique perspective projection in which the line joining the eye and center of the projection plane is not perpendicular to the plane like here (upper left), 

I Recently posted this question on SO but didn't got any response so i thought to post it here since it's somewhat related to Raytracing. I am making a real time ray tracer in OpenGL using Compute Shaders for my project and was following this link as a reference. The link tells to first draw a full screen quad, then store all the individual pixel colors gotten through intersections in a texture and render the texture to the quad. However i was thinking can't we use Frame Buffer Objects to display the texture image instead of rendering the quad and save the over head? Like I save all the colors using ImageStore and GlBindImageTexture in a texture, then attach it to a FBO to display it. And since I won't be using any rendering commands I won't be causing a Feedback loop as in writing and reading the same texture? Here is the snippet 

First of all we need to understand why do we need 4x4 matrices in the first place. With 3x3, we couldn't represent translation as it wasn't a linear transformation (it displaces the origin). So in order to avoid extra work, homogeneous coordinates and affine transformation was introduced. Now instead of doing $v' = Lv + t$ where is a linear transform and is the translation, we can do $v' = Av$ Where is the affine matrix. This makes it cleaner. So 4x4 matrices are a real necessity, we just can't work without them. In order to distinguish between vectors and points we use for points and for vectors. So you are suggesting to make this 4th dimension implicit and don't store it as it'll actually use space/memory. 

Light that is blocked will mean that $L(p*, -w_i)$ under the integral is 0 plus how much light the blocking object itself reflects. In other words the shadowing is embedded in the incoming light function. 

welcome to the world of padding. Doubles are 8 byte aligned so when a 4 byte float gets inserted between 2 doubles some padding gets inserted to keep the next double aligned to a 8 byte boundary. You can make sure the offset is always correct (even when alignment changes in future versions) by using the method. 

Instead of having a binary on/off per pixel you can instead make a heat map. The more points fall within the pixel the brighter the pixel is. Essentially create a 2D histogram which you then renormalize to get the grayscale for the output. If you want to blur the points then you can move a portion of the brightness to the next pixel based on the remaining fraction after multiplying to the resolution. If it the coordinate is and your resolution is 1000x1000 then goes to , goes to , goes to and goes to . This is distributed linearly but there are other options. 

Take a point $P$ and it's rotated point $P'$. Find the plan that runs through the middle between them $C = \frac{P+P'}{2}$ and is perpendicular to the line connecting them. Do this for all 3 of them and find the line of the planes' intersection. That will be the rotation axis. If all mid-planes are coplanar then you can use the planes of the triangles themselves. To get the angle you can take a $P$ and $P'$ again and project them onto the axis. Take a point $A$ on the axis and the direction $v$ of the axis. The projected point is $P_p=A+\frac{v \cdot (P-A)}{v\cdot v} v$. And then the angle is $acos(\frac{dot(P-P_p, P'-P_p)}{|P-P_p|* |P'-P_p|})$ 

As you can see in the Y-Z-X order, the Y axis remains there as in 3rd picture causing the axes to coincide... 

Next is differential radiance. We can think of it as an infinitesimal quantity of radiance emitted or recieved in a very small solid angle $d\omega$. Next is Irradiance. Irradiance isn't normally associated with a direction. According to Wikipedia it's 

So i searched a lot after this and I think it was my confusion on FrameBuffer Objects. I thought you could use FBO's just like a default FrameBuffer and display the texture image attached to it but you can't. It's only used for offscreen rendering. So while you can use rendering commands to draw something to a "texture image" attached to it, you can't "display the image" by making it default framebuffer or something like that. 

If we rotate around the X-axis we can see that the plane will either roll left or right. Suppose we rotate a little around the X-axis and then rotate around so that it is pointing straight up like this 

This is the simple case for just a projective transformation. Consider the vector going through 5 or 6 transformations and in the last comes the projective transformation. If we pre-multiply all these transformation to create a single matrix, you will notice that now when we multiply the vector with this combined transformation matrix the division factor isn't just a simple value. The 4th row of the matrix won't be as in the standard projection matrix. It might have changed due to multiplying all the transformations together. Now when you multiply that 4th row with the 4D vector, you will get your value by which you need to divide now. 

memory leaks, both normal ones and opengl resources. lack of precision after incrementing a float to a big number. After 4 weeks your millisecond counter will be at 2.4 billion which is well beyond the point where integers cannot be expressed exactly in a single precision floating point number. Make sure that those numbers are wrapped down to a smaller value explicitly. 

The color on hit should depend on the object's material (the color) and the relation between the normal vector the incoming eye ray and the outgoing light ray. The light sky box is hardcoded to be a gradiant. only a single recursive sample on each hit. This gives very grainy images and requires a lot of samples per pixel to converge to a proper image. 

A gpu only cares about the full transform used for a particular object. It's much better that the CPU (webworker) collates the transformation hierarchy into a single matrix. What you can do is use UBO and create a block where the final transform is stored and as you traverse the scenegraph update the data on the gpu. In the vertex shader you then index into it to get the data that applies to the vertexes. But you shouldn't store graph data in there only the final data needed for rendering. 

If you would do all lights in a single pass then you would need to loop over all lights in the shader. You would also need to get all information to the GPU at the same time. GPUs are limited in the number of uniforms you can pass into an invocation. This limitation has lessened with UBOs and SSBOs. However you are still limited with how many textures you can bind for shadow mapping. Lights often only affect a small portion of the screen so the engine may only render geometry that is close to the light or adjust the viewport to only render the area the light would affect.