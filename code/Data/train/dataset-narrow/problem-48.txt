This illustrates the compression process: The average and the principal axis of the colours are calculated. A best fit is then performed to find two end points that 'lie on' the axis which, along with the two derived 1:2 and 2:1 blends (or in some cases a 50:50 blend) of those end points, that minimises the error. Each original pixel is then mapped to one of those colours to produce the result. If, as in this case, the colours are reasonable approximated by the principal axis, the error will be relatively low. However if, like in the neighbouring 4x4 block shown below, the colours are more diverse, the error will be higher. The example image, compressed with the AMD Compressonator produces: Since the colours are determined independently per-block, there can be discontinuities at block boundaries but, as long as the resolution is kept sufficiently high, these block artefacts may go unnoticed: 

run principal component analysis on your image data to generate "the" main axis Project your pixels onto that axis (i.e. compute the dot product of each colour value against the axis) Using the dot prods, sort the the colour values from smallest to largest and Find the "partitioning" point, i.e. that splits smaller values in one set, larger in the other, that minimises, say, the mean squared error. i.e. each partition is represented by the average of its members and you compute the total error. 

To align vectors, you are applying a rotation. If you consider a simple rotation matrix, e.g. rotation of angle $\theta$ around Z axis, then you will trivially see that its inverse matrix, a rotation of $-\theta$, is the transpose, since $sin(-\theta)=-sin(\theta)$ and $cos(-\theta)=cos(\theta)$. An arbitrary rotation matrix, R, can be constructed by multiplication of other rotations, e.g $R = A \cdot B$. If the inverse of A and B exist, then $R^{-1} = B^{-1} \cdot A^{-1}$. Similarly we know $(A \cdot B)^T = (B^T \cdot A^T)$. Put these together and you have your answer. 

After this is the time of applying the actual lighting. During the lighting pass for each light you want to draw a light volume that depends on the type of light: 

In the pixel shader of this pass you pass your GBuffers and perform your lighting and shading using the information in them. This way you process only the pixels affected by each of the lights having a sensible speed-up if compared to the classical forward rendering. It has also various disadvantages, most notably the handling of transparent objects and higher consumption of bandwidth and video memory. But also it is trickier to handle various models for materials. You have other side-advantages (as having lots of info ready for post-processing) and is also pretty easy to implement. But this is not the coolest thing around for lots of lights anymore. Newer techniques are for example Tiled rendering ones. The main idea of those is to subdivide the scene in screen space "tiles" and assign to each tile the lights affecting it. This exists both in a deferred and forward fashion. These techniques lead to some problems when you have various depth discontinuities in a tile, but is generally faster than the classical deferred and it solves various problems of it. For example, among the advantages, with tiled deferred you read the GBuffers once per lit fragment and pixels in the same tile coherently process the same lights. Further evolution on this side is the Clustered shading which is conceptually similar to the tiled based approaches, having instead of screen space tiles, clusters with a 3D extent. This method handles better the depth discontinuities problem and generally performs better than the tiled methods. IMPORTANT NOTE: I have described the basics of the deferred shading. There are multiple variations, optimizations and improvements around so I urge you to experiment with a simple version and then do some researches on other techniques such the one I mentioned above. 

Sample gaps will occur if you don't do things "properly". In OpenGL or D3D, assuming a consistent winding order, if you have triangles ABC and CBD, then when a sample point - that is a test during scan conversion to determine if a point (read pixel) is inside a triangle - lies exactly on the shared edge BC, then that sample will belong to exactly one of those two triangles. This avoids both gaps and double filling. The latter would be problematic with, say, translucency or stencil operations. 

Originally, it wouldn't have been perspective correct, but on (hardware) systems these days it will be. FWIW Dreamcast had perspective correct Gouraud shading because, once you are doing perspective correct texturing, it is relatively little additional cost to do Gouraud "correctly". 

(since the SBox function is quite non-linear) Having said that, (and please forgive me if I've got some of the details wrong) in a past life I implemented Perlin noise using a relatively simple RNG/Hash function but found the correlation in X/Y/Z due to my simple mapping of 2 or 3 dimensions to a scalar value was problematic. I found that a very simple fix was just to use a CRC, eg. something like 

It could be that you have to overcome a different bottleneck first. Have you ever read Jim Blinn's "The Truth About Texture Mapping"? (I had a quick search to see if I could find a non-paywalled version but you may have better luck than me. Alternatively you might find a dead tree version of "Jim Blinn's Corner" in a library). Though this article is old and describes paging of texture data, it is still very relevant today. Essentially, if your textures are large (i.e. too large to fit in the cache), in scan order, and they have been rotated when displayed on the polygons, you are very likely to be thrashing your cache and, as memory is an order or two of magnitude slower than the CPU, this will hurt performance. To avoid the cache thrashing, textures are often stored in twiddled-order (that's what we called it in the early 90s but, more correctly, it'll be some variant of Morton order) or in a block order, which is what Blinn describes. This then makes texel/memory accesses far more coherent and the cache more effective. 

Just to an add another way to the excellent @NathanReed answer, you can use mean and gaussian curvature that can be obtained with a discrete Laplace-Beltrami. So suppose that the 1-ring neighbourhood of $v_i$ in your mesh looks like this                                          $A(v_i)$ can be simply a $\frac{1}{3}$ of the areas of the triangles that form this ring and the indicated $v_j$ is one of the neighbouring vertices. Now let's call $f(v_i)$ the function defined by your mesh (must be a differentiable manifold) at a certain point. The most popular discretization of the Laplace-Beltrami operator that I know is the cotangent discretization and is given by: $$\Delta_S f(v_i) = \frac{1}{2A(v_i)} \sum_{v_j \in N_1(v_i)} (cot \alpha_{ij} + cot \beta_{ij}) (f(v_j) - f(v_i)) $$ Where $v_j \in N_1(v_i)$ means every vertex in the one ring neighbourhood of $v_i$. With this is pretty simple to compute the mean curvature (now for simplicity let's call the function of your mesh at the vertex of interest simply $v$ ) is $$H = \frac{1}{2} || \Delta_S v || $$ Now let's introduce the angle $\theta_j$ as                                          The Gaussian curvature is: $$K = (2\pi - \sum_j \theta_j) / A$$ After all of this pain, the principal discrete curvatures are given by: $$k_1 = H + \sqrt{H^2 - K} \ \ \text{and} \ \ k_2 = H - \sqrt{H^2 - K}$$ 

$$ O + 1 \times (L_p - O ) = L_p $$ So if you trace your shadow ray with an unnormalized direction $\vec{D}$, you can just set your maximum $t$ to be just under $1$. This is in fact what PBRT does, and as far as I am aware many other renderers. Black dots (or NaNs) Nothing to add over @Dan Hulme comment, so I'll just add it here for completeness of the answer: 

Decoding Speed: You don't want texture compression to be slower (at least not noticeably so) than using uncompressed textures. It should also be relatively simple to decompress since that can help achieve fast decompression without excessive hardware and power costs. Random Access: You can't easily predict which texels will be required during a given render. If some subset, M, of the accessed texels come from, say, the middle of the image, it's essential that you don't have to decode all of the 'previous' lines of the texture in order to determine M; with JPEG and PNG this is necessary as pixel decoding depends on the previously decoded data. Note, having said this, just because you have "random" access, doesn't mean you should try to sample completely arbitrarily Compression Rate and Visual Quality: Beers et al argue (convincingly) that losing some quality in the compressed result in order to improve compression rate is a worthwhile trade-off. In 3D rendering, the data is probably going to be manipulated (e.g. filtered & shaded etc) and so some loss of quality may well be masked. Asymmetric encoding/decoding: Though perhaps slightly more contentious, they argue that it is acceptable to have the encoding process much slower than the decoding. Given that the decoding needs to be at HW fill rates, this is generally acceptable. (I will admit that compression of PVRTC, ETC2 and some others at maximum quality could be faster) 

Apologies in advance for the poor quality of this answer, but this sounds a little like what an ex-colleague was doing as part of his PhD. The "Free-Viewpoint video" papers listed at the bottom of $URL$ might be useful, or at least a starting point to find related work. 

It is a well known "standard" to use bilateral upscaling when it comes to comes to combine a low resolution target and an higher res one. I have personally noticed that using the basic algorithm (with weights based on depth differences between high and low res depth values) is far from perfect in situations where high res and low res are blended, say for example an high res object inside a low res effect. 

There are several techniques used. A simple, but limited, post-process approach that is not really used any more consists in reconstructing the world space position of a pixel using both the view projection matrix from current and previous frame. Using these two values you can compute the velocity at a pixel and blur accordingly, sampling along the velocity vector with how many samples you want ( the more sample the blurrier, but the more expensive as well). This method unfortunately takes into account just the camera movement and therefore is not accurate enough if you have a dynamic scene with fast moving objects. As you mentioned, other more modern techniques use a velocity buffer. This velocity buffer can be created during the base pass in the deferred pipeline transforming each vertex using the world-view-projection matrix of the previous frame and then computing in the pixel shader the velocity vector using the screen space position of both frames. With this velocity buffer you can, in a post-process step, sample across the per-pixel velocity direction like in the approach I described above. The motion blur that you will get is obviously not camera-only; however you get an overhead in terms of memory (you need to keep an extra buffer) and time (you need to render additional info per pixel). One technique based on velocity buffer that I particularly like is McGuire et al. that IIRC, is used with some variations in Unreal Engine 4 and many other games. 

I'm fairly certain your problem lies in not handling so-called degenerate Bezier patches correctly, in particular (as Joojaa noted), the computation of the surface normal. I say "so-called degenerate" because, geometrically, the surfaces are often perfectly well behaved. It's just that some assumptions people frequently make regarding the parametric equations may not hold. Books such as Gerald Farin's, snappily titled, "Curves and Surfaces for CAGD. A Practical Guide" will give more details, but I'll try to summarise two simple cases. Now assuming your Bezier is defined as $\bar{B}(u,v)$ the usual two causes of problems are: Zero derivatives: To compute the normal at $(a,b)$ one normally (pardon the pun) computes the two derivatives, $\frac{\partial }{\partial v}\bar{B}(a,b)$ and $\frac{\partial }{\partial u}\bar{B}(a,b)$ or scaled versions thereof, to obtain two tangents and then take the cross product. (Implementation note: Since we can use a scaled version of the tangent in the calculation, we really don't have to calculate the actual derivative. For example, especially at the corners, differences of control points can yield a scaled tangent. However for brevity in this discussion we will assume the actual derivatives). A common occurrence, at least at the corners, is that the 1st partial derivatives at the location can be zero, which leads to an incorrect normal, i.e. a zero vector. In the case of the tops and bottoms of the teapot, one whole edge of a number of the (bicubic) Bezier patches has been collapsed to a point, i.e. all 4 control points are the same, and thus, say $\bar{B}(0,v)==\bar{B}(1,v)$ and $\frac{\partial }{\partial u}\bar{B}(0,0)=\bar{0}$. The surface, however, is completely well behaved so you can simply choose another derivative that starts at that collapsed point. In this case, say, choosing $\frac{\partial }{\partial v}\bar{B}(1,0)$ for the second tangent. Having said this, you still have to check that your first derivatives are not zero for another reason (e.g 2 or 3 coincident control points), in which case, you can fall back (thanks to L'Hopital's rule) on the second (or if that's zero, even the third!) derivative(s) to obtain valid tangents. Parallel tangents: Another similar problem can arise if your two tangents, are parallel. - Farin has a good example in his book. In this case, I think you may need to look at using something like $\frac{\partial^2 }{\partial u \partial v}\bar{B}$ or, possibly, just fall back to using a small UV offset to approximate a vector. 

With an high threshold you will find coarser edges and you might miss some, conversely, with a low threshold you might detect false edges. You have to experiment to find the threshold that better suits your needs. The reason why these functions work is worth mentioning but I don't have time for it now, I am likely to update this answer later on :) Screen space post-process You could go fancier than this, now the field of Edge detection in image processing is immense. I could cite you tens of good ways to detect edge detection according to your needs, but let's keep it simple for now, if you are interested I can cite you more options! So the idea would be similar to the one above, with the difference that you could look at a wider neighbourhood and use a set of weights on sorrounding samples if you want. Typically, you run a convolution over your image with a kernel that gives you as a result a good gradient info. A very common choice is the Sobel kernel                                    Which respectively give you gradients in x and y directions:                                    You can get the single value out of the gradient as $ GradientMagnitude = \sqrt{ (Gradient_x) ^ 2 + (Gradient_y) ^ 2 } $ Then you can threshold as the same way I mentioned above. This kernel as you can see give more weight to the central pixel, so effectively is computing the gradient + a bit of smoothing which traditionally helps (often the image is gaussian blurred to eliminate small edges). The above works quite well, but if you don't like the smoothing you can use the Prewitt kernels:                                                     (Note I am in a rush, will write proper formatted text instead of images soon! ) Really there are plenty more kernels and techniques to find edge detection in an image process-y way rather than real time graphics, so I have excluded more convoluted (pun not intended) methods as probably you'd be just fine with dFdx/y functions. 

With compression we are after the best visual representation per bit and, as the eye is non-linear in approximately this way, it makes sense to use non-linear representations. 

"How (hardware) texture compression works" is a large topic. Hopefully I can provide some insights without duplicating the content of Nathan's answer. Requirements Texture compression typically differs from 'standard' image compression techniques e.g. JPEG/PNG in four main ways, as outlined in Beers et al's Rendering from Compressed Textures: 

Case 2 If the case 1 test fails, you could then check for the "trivial" existence of an intersection. Now there are probably better ways to do this, but the following, relatively cheap, approach occurred to me: Consider just curve A: 

You might find the 1983 paper that introduced this**, i.e. Lance Williams' "Pyramidal Parametrics" informative. You can ignore the scheme, in Figure 1, he used for the layout of the MIP maps as I doubt any hardware, at least in the last 20+ years, used that approach. ** (Actually, Williams *may* have described the technique at an earlier SIGGRAPH (1981) as part of tutorial/course but I've never been able to get hold of a copy) 

The area around the beak also illustrates the horizontal and vertical partitioning of the 4x4 blocks. Global+Local There are some texture compression systems that are a cross between global and local schemes, such as that of the distributed palettes of Ivanov and Kuzmin or the method of PVRTC. PVRTC: 4 & 2 bpp RGBA PVRTC assumes that an (in practice, bilinearly) upscaled image is a good approximation to the full-resolution target and that the difference between the approximation and the target, i.e. the delta image, is locally monochromatic, i.e. has a dominant principal axis. Further, it assumes the local principal axis can be interpolated across the image. (to do: Add images showing breakdown) The example texture, compressed with PVRTC1 4bpp produces: