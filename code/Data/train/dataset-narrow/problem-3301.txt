SlySven's answer is good, but I'd suggest this might be more easily accomplished with the new gpiozero library too. For example: 

OpenCV uses V4L to communicate with web-cams by default. When using the Pi's camera module this means you need to load a V4L driver to expose the camera module as a V4L device (like ). This forum post contains details of the foundation's official V4L driver but the short version is: 

This last one effectively says "set the state of the relay to the state of the button" (various functions are available in gpiozero.tools to do things like inverting state, combining states of multiple devices, etc.) I'd strongly recommend reading the Recipes chapter in the docs which we're always adding new stuff too. 

As to the rest of your code - you've got some good stuff there, but some can be simplified with a few new toys introduced in picamera 1.11. Specifically, have a look at the new PiCameraCircularIO.copy_to method, and the clear method (just above it in the docs). Those can be used to greatly simplify (or even eliminate) your function. 

Bad news first: only one process can access the camera at a given time, and raspivid is (for the most part) non-interactive, so I'm afraid you won't be able to use a second raspivid process to control the first one (the second process will attempt to open the camera and find it already locked by the first process). Now the good news: it's certainly possible to manipulate the brightness and contrast of the camera (and many other things!) while it's running (and recording), so it shouldn't be terribly difficult to create a script which allows such manipulation from the initial SSH session. Here's a little script which uses curses (a venerable console interface library) to provide a simple keystroke based interface to the camera. It allows adjusting brightness, contrast, saturation while the camera is running, and permits recording to be started and stopped (and being text based obviously runs happily in an SSH session): 

So, I'm getting a little over 1fps, but we can immediately see that the capture time is a tiny proportion of the loop's overall time, and the vast majority is being spent in findBlobs and displaying the preview. If we simply cut out the preview display I get the following results: 

This is intentional as it allows the AWB loop (which runs in the camera firmware in the background) several frames with which to calculate the white balance. If you skip this you need to be prepared to specify the white balance manually or you'll just wind up with green (because the red and blue gains are zero). The capturing consistent images recipe has more information on calculating and specify the white balance manually. I'd also recommend having a read of the camera hardware chapter for a better overview of all the stuff that happens in the background with the camera firmware. 

If the screen is operating from the GPIO pins (I must admit I'm not familiar with the Kookye 3.5" LCD) then the short answer is: you don't. From the picamera FAQ: 

Pure Python is indeed very very slow (unsurprising given it's an interpreted language). One trick to getting performance out of it (when required, premature optimization being the root of evil) is to push tight loops down to a point where compiled C can deal with them. Numpy provides exactly that with its vectorized operations. For example, to subtract two arrays you can simply subtract the two array objects which will yield a new array object with the same dimensions: 

I don't think I've compiled a complete list of the attributes that can definitely be used when recording because I don't know them myself. I can say that and both can't be used because they involve re-initializing the camera. Almost everything else seems to be okay but there are odd exceptions (e.g. mostly seems to work, but some effects don't operate on the video output). I'm a bit hesitant to create a definitive list because it's probably yet another list that would become incomplete as things evolved ("this new feature isn't on the list - so it can't be used when recording, or it can and you just forgot to add it?"). However, I can say that every attribute that definitely can't be used when recording is active, has that fact clearly documented in the API reference. For example the following sentence appears in the description of both and : 

Ah, the perfect excuse to dabble in some source/values stuff in GPIO Zero! As others have commented, doing this on a Pi won't give you precise timing but this might be good enough (and the code's quite simple). Component "values" One of the concepts we've tried to introduce in GPIO Zero is that all components have a which for single components is represented as a single number. For simple things like an LED that'll be 0 (off) and 1 (on). Same thing for a button. For more complex things like a Servo that might would be a floating point value from -1 (all the way counter-clockwise) to 0 (mid-position) to 1 (all the clockwise). Naturally, a pot (via some ADC like an MCP3008) would be a float from 0 to 1. So, all components have a property which we can read to obtain this number. They also have a property which provides an infinite iterator of these numbers (every time you request another value from it, it reads the property). This is useful in combination with the property. This exists on components which can be manipulated from the Pi (as opposed to externally), so LEDs have a property, servos have a property, but buttons don't (because the Pi can't manipulate the state of the button, only something external can). When the property is assigned an iterator of values it continually reads that iterator (with a defined delay between each read) and assigns the value read to the component. LEDs and Buttons Hence we can make an LED flash in response to a Button simply by doing: 

This still won't be absolutely precise - it's the time of the first write from the firmware to the output object after capture, rather than the time of the capture itself but I suspect it's as close as you can easily get without measuring things like the time it takes the firmware to do de-mosaic, lens shading, etc. etc. (which you could apply as a negative offset to the answer of the script above). 

The code doesn't look like "normal" picamera code, but read through the introduction in the link above and you should get a reasonable feel for the style of doing things. In the code below I assume you've got a 1080p sized display, and I'm using a 960x720 capture size for a 4:3 ratio display in each eye (centered vertically); you'll need to modify things if your display is a different size: 

Note that the first entry is blank (which means search the current directory). So, the chain of events that occurred above was as follows: 

However, we might be able to go further depending on whether the luminance channel in a YUV capture (the Y bit) is close enough to the V values in the HSV representation (this is something you'll have to decide for yourself). If it is, then we can dispense with OpenCV entirely: 

So, the simple solution is to get rid of the line setting the framerate. Alternatively, if you want to ensure the framerate is explicitly set, I generally recommend setting it next to the resolution so that all the sensor resetting gets done together. If you don't like the idea of all that sensor resetting (which is quite slow - although that's generally not a problem if it's a one-off thing at script start-up), you can override the initial resolution and framerate in the PiCamera() initializer: 

Note that at this point you can push your button on GPIO18 as much as you like and the circuit will momentarily drop low for as long as you're holding the button, but it doesn't matter because you're not reading GPIO18 - the script is waiting on keyboard input (specifically until the Enter key is pressed). By the time you get back to reading GPIO18 you may not be pushing the button anymore. What I suspect you want is to read two things simultaneously (or as near as possible that you don't notice the difference): the keyboard, and the GPIO18 pin. There's a couple of ways of going about this: Non-blocking keyboard input Instead of using a blocking function like that waits around for an entire string of characters until Enter is pressed, use non-blocking input which reads a character at a time (or nothing if no key is pressed) and returns immediately. That way you can go whizzing round your loop quickly and from a human's perspective it'll appear you're reading the two things more or less simultaneously. Unfortunately non-blocking input is non-trivial (you need to learn new ways to print , but here goes: 

As mentioned in the answer, you can approximate the camera's preview but be aware that it generally lacks the "smooth" framerate that the "real" preview system manages (simply because there's a lot more work involved). 

So, back to the capture method. Why is it throwing such a confusing error? When you pass the method the tuple it assumes "it's not a string, so I'll try and use it as a file-like object". What is a file like object? It's any object with a method. The tuple doesn't have a method but that doesn't matter yet because first the capture method needs to know what format (JPEG, PNG, etc) it should use. To figure this out it looks at the format parameter, which you haven't passed so it defaults to meaning "figure it out from the filename". So the capture method tries to look at the attribute of the supposed file like object... Unfortunately the tuple doesn't have one of those either so at this point the capture method blows up and complains that a format must be specified. This is relatively typical of programming: the error is fairly early on but the actual exception that gets raised is somewhere further down the line and it takes a certain amount of "under the hood" knowledge to track back to the actual issue. It's rather compounded in this case due to the dynamic typing in python although I could improve the checks at the start of that method to test for a method on the file-like object up front (which would make the error message a little more useful) 

At this point you should have a nice, smooth preview in both eyes (and all the work will be occurring on the GPU, so your CPU is entirely free for other tasks). Controlling the camera via mmalobj requires a bit more effort than with picamera (unsurprising given that picamera is built upon mmalobj, so picamera is a simpler abstraction on top of the lower level mmalobj). For example, to find out how to change the brightness, take a look at PiCamera._set_brightness: 

The Pi's compute module with the I/O driver board is capable of driving two Pi camera modules and in the last few days the firmware has been enhanced to provide stereoscopic capabilities too. As that post makes clear though, this is for the compute module only, it'll never work on the Pi as only one CSI interface is exposed on the Pi. 

There are several APIs available for direct camera interaction. In C you have a choice of the MMAL API, which is what the official demo apps (raspistill and raspivid) are based upon (their source code is a good intro to using MMAL), or OpenMAX. I've heard the latter is a bit harder to work with, but it is an open API while MMAL is defined by Broadcom. And finally, there's the V4L interface which you've already been playing with (but as you noted it's rather limited compared to the direct APIs). For Python, there's the picamera library (full disclosure: I'm the author of that one). For JavaScript, there's a set of node.js bindings. On the other hand, if you're looking for a web interface there's this project. I'm not sure it has a RESTful interface (I haven't read through that enormous thread fully yet!) but given its web based it probably wouldn't be too hard to stick one on top. 

P.S. the UNIX shell absolutely is a programming language; it's got variables, branching, loops, even functions. It's a programming language, no two ways about it. P.P.S. I didn't include C above because obviously there's an API for that already, but I'm not sure I'd class it "high level". 

The short version is: you're passing the capture method a tuple instead of a string. You probably want something like the following for the name: 

open a couple of console windows side by side in one of the windows start htop (with "Hide userland threads" disabled, the default) in the other start an interactive python session, "import picamera", then initialize the camera with "camera = picamera.PiCamera()" 

So, your regular Pi camera (or any old CCD with an IR filter) will have a response curve limited by the cyan (hot mirror) line in that graph. However, the PiNoIR (or any old CCD without an IR filter) will have a response curve limited by the black line. If you want it to look "normal" you need to stick an IR filter over it (or get a camera with an IR filter installed in it). 

I can't say I've tried this personally, so what follows is all just an educated guess, but try comparing the Pi's hardware to a home router (which should handle 10-20 connections happily): I've got a little Netgear WNR1000 as a home router; looking at the attached devices there's currently 7 wired devices, and another 8 wireless devices (a good half of those are Pi's :). So, not quite 20 wireless connections, but close to that number of total connections and it's doing fine. What hardware has the WNR1000 got? According to the OpenWRT pages it's got a Broadcom BCM5356 processor (presumably an ARM architecture) running at 333Mhz, and a whopping 14Mb of RAM ... so the Pi's got it beat hands down there. However, the one place you might find the Pi wanting compared to a "proper" router is in the bandwidth available to its network interfaces. The Pi's Ethernet port and of course any WiFi dongles you plug into it will all be using the USB bus - which is not exactly fantastic for overall bandwidth (although it's still better than the SD card!). I'm not sure how Ethernet or WiFi are attached on "proper" routers, but I'd be rather surprised if they weren't simply integrated directly into the SoC which would be considerably more efficient. So, my educated guess would be: the Pi's got more than enough processing power and RAM to handle being an "industrial strength" router, but you could probably saturate its IO bandwidth quite easily with 20 simultaneous connections. In other words, if your 20 clients are all doing low-bandwidth stuff (e-mail, browsing web-pages, etc.) you'd be fine but if a few of them started streaming video simultaneously you might find things stuttering!