These don't answer all of your questions. I'm pretty sure both of these studies covered most of them. I don't see anything about tool selection specifically. I imagine sales reps had a lot to do with getting the overall product in the door, but the data scientists themselves leveraged the tools they were most comfortable with. I don't have a lot of insight into that area in the big data space. 

Google does not publish the models they use, but they specifically do not support models from the PMML specification. If you look closely at the documentation on this page, you will notice that the model selection within the schema is greyed out indicating that it is an unsupported feature of the schema. The documentation does spell out that by default it will use a regression model for training data that has numeric answers, and an unspecified categorization model for training data that results in text based answers. The Google Prediction API also supports hosted models (although only a few demo models are currently available), and models specified with a PMML transform. The documentation does contain an example of a model defined by a PMML transform. (There is also a note on that page stating that PMML ...Model elements are not supported). The PMML standard that google partially supports is version 4.0.1. 

Use consecutive search strings from users (short-to-long-tail searches) as training data for a machine-learning algorithm. Run searches that occur > N amount of times a week against that recommendation algorithm. Validate and clean results. Push them out to the general population in rotation/A-B testing. Track click-throughs. Refine results over time. 

2. Document proposed changes Any changes you want to make to data should be documented so that they can be replicated moving forward. Version control and timestamp the document every time you change it. That will help in troubleshooting at a later date. Be explicit. Do not simply state "correct capitalization problems", state "ensure that the first letter of each city name begins with a capital letter and the remaining letters are lower-case." Update the document with references to any automation routines that have been built to manage data cleansing. 3. Decide on a standard data cleansing technology Whether you use perl, python, java, a particular utility, a manual process or something else is not the issue. The issue is that in the future you want to hand the data cleansing process to somebody else. If they have to know 12 different data cleansing technologies, delegating the cleansing procedure will be very difficult. 4. Standardize the workflow There should be a standard way to handle new data. Ideally, it will be as simple as dropping a file in a specific location and a predictable automated process cleanses it and hands off a cleansed set of data to the next processing step. 5. Make as few changes as is absolutely necessary It's always better to have a fault tolerant analysis than one that makes assumptions about the data. 6. Avoid manual updates It's always tempting, but people are error-prone and again it makes delegation difficult. Notes on manual processing To more completely address the original question as to whether there's a "good" way to do manual processing, I would say no, there is not. My answer is based on experience and is not one that I make arbitrarily. I have had more than one project lose days of time due to a client insisting that a manual data cleansing process was just fine and could be handled internally. You do not want your projects to be dependent on a single individual accomplishing a judgement based task of varying scale. It's much better to have that individual build and document a rule set based on what they would do than to have them manually cleanse data. (And then automating that rule set) If automation fails you in the end or is simply not possible, the ability to delegate that rule set to others without domain specific knowledge is vital. In the end, routines to do something like correct city names can be applied to other data sets. 

You can code certain simple rules like the ones you have mentioned in the question. Additionally, you can use knowledge bases like Freebase and WordNet to enrich your language model. Note that this will not necessarily "noisify" your data but would have effect similar to the effect on data augmentation on say images for downstream tasks. 

Predicting "future" especially in the context of economics is a well-researched domain. For example, you can check out the research involving "predicting stock market using social networking". I am sure you would find such results for other domains as well. 

Another way would be to create your own dataset by downloading say Github user's graph, defining a network by adding edges between user A and user B if A follows B and so on. Then you can take snapshots of this graph at different time instances and see if you can use last snapshots to predict the edges in the th snapshot. Of course that would be a lot more extra effort and the links I posted may suffice. Please let me know if it helps. 

I understand that your time series are unevenly spaced. In this case, why not simply use a library like traces and transform them to evenly spaced time series. 

You should refer this survey paper on Anomaly Detection (from University of Minnesota). Please let me know if this helps you. 

The question asks about the and I would answer in terms of space and time complexity. Let us say that number of input transactions are N(=20) and the number of unique elements be R(approx 900). Assuming your threshold count is quite small (which means very few candidates are pruned), the time and space complexity for size i candidates would be . So you see, if very few candidates are pruned, the space (and time) requirements become exponential. It might seem unintutive at first, given that you have only 20 transactions. But the bottleneck is the number of candidates which increases exponentially with number of items. 

I've now seen two data science certification programs - the John Hopkins one available at Coursera and the Cloudera one. I'm sure there are others out there. The John Hopkins set of classes is focused on R as a toolset, but covers a range of topics: 

You should be looking towards Natural Language Processing, specifically at Sentiment Analysis. The link I provided is a good starting point for learning about sentiment analysis. If this is what you are looking for, it is available as part of Stanford's Core NLP. 

News outlets tend to use "Big Data" pretty loosely. Vendors usually provide case studies surrounding their specific products. There aren't a lot out there for open source implementations, but they do get mentioned. For instance, Apache isn't going to spend a lot of time building a case study on hadoop, but vendors like Cloudera and Hortonworks probably will. Here's an example case study from Cloudera in the finance sector. Quoting the study: 

I've seen a few similar systems over the years. I remember a company called ClickTrax which if I'm not mistaken got bought by Google and some of their features are now part of Google Analytics. Their purpose was marketing, but the same concept can be applied to user experience analytics. The beauty of their system was that what was tracked was defined by the webmaster - in your case the application developer. I can imagine as an application developer I would want to be able to see statistical data on two things - task accomplishment, and general feature usage. As an example of task accomplishment, I might have 3 ways to print a page - Ctrl+P, File->Print, and a toolbar button. I would want to be able to compare usage to see if the screenspace utilized by the toolbar button was actually worth it. As an example of general feature usage, I would want to define a set of features within my application and focus my development efforts on expanding the features used most by my end users. Some features that take maybe 5 clicks and are popular, I might want to provide a hotkey for, or slim down the number of clicks to activate that feature. There is also event timing. Depending on the application, I might want to know the average amount of time spent on a particular feature. Another thing I would want to look at are click streams. How are people getting from point A to point B in my application? What are the most popular point B's? What are the most popular starting points? 

In Image Processing, this task is known as localization. You basically want to localize each digit in the image and then use your digit recognizer over the digits. A cursory google search for digit localization in images gives me following papers which seem to be very helpful. 

Your use case boils down to categorizing news feed on an online forum and then finding out top-n categories. I would suggest you look at this Hacker News Categorizer developed by MonekyLearn. This way you can understand how to get started with such projects. PS : I am not affiliated with MonkeyLearn. 

I have a dataset which contains information about when do people enter and leave a premise. I have the following information in the dataset : 

Spark support for Mahout came from Mahout 0.10 release while you are using 0.9 release. So this should explain why you get the error. I would suggest using a higher version of Mahout. 

The dataset has around 50 unique persons. Each person will have multiple entries corresponding to multiple visits. The data spans over a year so I have quite a lot of entries (around 1 million). These people can be classified on the basis of the department they work under (2 departments - mutually exclusive) or on basis of their role (4 possible roles - all mutually exclusive) I was wondering what kind of data analysis can be done with this kind of dataset. I am not looking for straight-forward things like "who spent the most time in building". However things like finding correlation between visits of 2 people would be interesting. So if person A visits the premise, what is the probability that person B would also visit. Since I have only around 50 unique visitors, I think such an analysis is feasible. Another line of thought was to apply some interval-pattern mining techniques but I am not much familiar with them. Can someone give me some pointers/ideas about what kind of data products can be build using this or what kind of techniques can be used with such data. Edit : As discussed in comments, I call it a product in the sense that I do not want some simple or trivial analysis. And I am not looking for any commercially viable idea - just some cool fun idea :) 

Just about any ETL tool can manage fixed width, CSV, TSV, or PSV input, and just about any tool should be able to manage 100B records. The limiting part of the question really has to do with what your destination format is, and what disk throughput you need. Expected throughput on an i2.4xLarge is 250mb/s. If an 8xLarge is double that, times 32 machines, you are looking at the ability to write a petabyte in ~138 hours. Not to mention the time and bandwidth of bringing in the source data in the first place. Unless my math is completely off, that means 30 Petabytes can get written to disk in about 6 months. It seems odd that you are looking to either normalize or turn into human readable format that much data (it's only going to get bigger), and even odder that you'd want to leverage machine learning as part of a transformation/load of that size. Your solution will need to be on local hardware in order to keep costs reasonable. I couldn't recommend a system (commercial or open source) that would scale to the degree necessary to perform this kind of ETL on 30 Petabytes in a matter of days. At that scale, I'd be looking into lots of memory, ram backed/fronted SSDs, and custom development on FPGAs for the actual transformations. Of course, if my math on the write timing is wrong this whole answer is invalid. 

I am not looking for a recommendation on a program or a quality comparison. I am curious about other certifications out there, the topics they cover, and how seriously DS certifications are viewed at this point by the community. EDIT: These are all great answers. I'm choosing the correct answer by votes. 

I am not very sure how effective NN would be for this problem. The way I see it is that you have 48 entries in a time series and now you are trying to predict the next 4. Keeping aside the correlations for a second, it seems like there is very little data to "learn" from. I mean the power consumption is a seasonal thing (much like temperature) but you have only 1 years data so it should be difficult for NN to capture this seasonality. I would like to raise another point about the way you are using correlations. If you are taking correlations using the actual temperature value then I do not think that to be a good idea. Think of it in this way, I would be using a lot of power at both very high and very low temperature. So instead of using the raw temperature values, I should normalize them with respect to some "comfortable" temperature value by taking the absolute difference between the actual temperature and the "comfortable" temperature. For instance, let us say that "comfortable" temperature is 25C, then both 5C and 45C would be normalized to 20 and this looks more plausible as my power consumption should be high in both the cases. This might also explain the poor correlation that you observe for 3 months of data. What should be the "comfortable" temperature is an entirely different story altogether. Do let me know if this line of thought makes sense. 

This does not mean there is nothing to learn from the data. In the worst case, there is nothing to learn from data, with the given architecture, hyperparameters and the time you are willing to let the network learn.