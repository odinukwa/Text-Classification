Instead of using embeddings and doc2vec, maybe you should start by an easier and more straightforward information retrieval approach. Start by building a bag-of-words representation of your long document. Given its vocabulary $V$ of size $|V|$, you represent it as well as the short texts for the second document in a space $\mathbb{R}^{|V|}$. This will give a vector for the first document and several vectors, one for each of the lines, for the second document. Proceed by calculating the similarity (euclidean distance) of the vectors of the lines of the second document with the vector of the first document. There are many ways in this line of though that would enable you to use word embeddings if you want, but I would suggest starting with the simplest approach first. 

The purpose of topic modeling methods is to discover the latent themes (topics) assumed to have generated the documents of a corpus. Topic modeling methods are built on the distributional hypothesis, suggesting that similar words occur in similar contexts. To this end, they assume a generative process (a sequence of steps), which is a set of assumptions that describe how the documents are generated. Given the assumptions of the generative process, inference is done, which results in learning the latent variables of the model. For instance, for Latent Dirichlet Allocation, this is the per-topic document distributions and the per-word document distributions. In this sense, a document can be represented by its per-topic distribution (doc$_1$ = 0.3$\times$Sports + 0.7$\times$Cinema). This later can be seen as a soft clustering approach, i.e., doc$_1$ belongs 30% in cluster Sports and 70% in Cinema. But topic models are not solely clustering methods, as can also been used for understanding, exploring, visualizing a collection. On the other hand, clustering methods aim at partitioning data into coherent groups. Of course, what is coherent and how the partitioning is performed differs between the various clustering algorithms. The distance between the data instances is central for clustering methods and for this the instances can be represented in various ways: for documents this can be term frequencies (tf), tf-idf, and even with the per-document topic distributions learned by topic models. 

Linear models like Logistic Regression and Support Vector Machines can also handle such feature dimensionalities. Often in text mining problems like text classification the dimensianality of the feature space equals the vocabulary size which is high. 

Increasing the number of epochs usually benefits the quality of the word representations. In experiments I have performed where the goal was to use the word embeddings as features for text classification setting the epochs to 15 instead of 5, increased the performance. 

What you describe uses the data that the paths can offer. You can easily generate features from the data and time. For instance, given the date, you can generate a categorical variable denoting the weekday (Monday, Tuesday, etc..). Given the timestamp, you can generate binary variables to partition the day in four or more partitions: is_morning, is_afternoon etc.. Somebody may only read in the morning or at night, and the aim of these features is to capture this. Further, you can get the interactions between weekdays and day partitions. Such features may help to distinguish users that in the Sunday mornings read about sports while Monday mornings they are at work and read financial news. Be careful of the overfitting though. Note that trees have been shown to capture such complex interactions; given them explicitly is beneficial though. 

Here, keys becomes a string variable while I guess you would expect it to take the value of the list you defined in the first lines.. Try instead with a dictionary: 

Your target y can be whatever you need. If you want to do sequence to sequence for example, y will have the same number of time steps than the inputs (you predict something for each timestep). But you can also define an output with one timestep only, for text classification for example. Your input data has 10 timesteps, but the output is only a prediction at the last timestep. It depends on the problem you’re trying to solve. 

The classification rule being $\underset{c_j}{argmax} \; p(c_j)p(w|c_j)$ with $p(w|c_j) = \prod\limits_{w_i}p(w_i|c_j)$ thanks to the independence assumption. To prevent underflow errors, we often use the log : $log \;\;p(w|c_j) = \sum\limits_{w_i}log\;\;p(w_i|c_j)$ Therefore, the final word is : to classify a document (or paragraph or whatever piece of text), you take each word $i$ from it, and for each class $j$, see the fraction of times it appears in it (in documents from class $j$), in the training data (which is a probability), and add up the log. It will give you $j$ probabilities (one for each class), and you take the class corresponding to the maximum. What is not clear in your question is the relation between labels and sentiment analysis. See this for a detailed explanation and examples. 

In fact, you use 1D convolution. Given that the dimension of the output of embedding layer is 100, that the kernel size is 5, and that the number of filters is 128, You have 100x5x128 = 64000 weights. Add to this 128 biases and you get 64128 parameters. Note that parameter sharing is used, so that there is only one set of weights and biases per filter, in depth. 

First of all, you should use to get you predictions vector, so that you followed approximately the same validation scheme to get them : 

Okay, I got it. If anyone interested, they use 5x5 filter but with padding 2 and striding 1 so that with bias it doesn't change the 2D dimension of the output when applied on the result of max-pooling. The info on padding isn't present on the original paper... 

When using Conv2D , the input_shape does not have to be (1,68,2). The number of samples does not have anything to do with the convolution, one sample is given to the layer at each time anyway. What changes is the number of spatial dimensions of your input that is convolved: 

you trained and tested on different folds which logically will lead to worse results (but closer to real condition experiment). Besides, try to use and then in order to have a detailed report of metrics for each class. 

In effect it is ordinal regression/classification. I suggest you to go for Mean Absolute Error to take into account the missclassification that play a bigger role when far from the ground truth : $MAE = \frac{\sum_{i=1}^n|h(x_i) - y_i|}{n}$, creating your own scoring function in python. Given the number of different outputs you have, I recommend you to go for regression. Anyway, rounding any hypothesis output would only play on the score, it's up to you to decide if it makes more sense to see wine quality scores as classes or not. 

If you are considering time series only, you may have an the option to run a linear regression model, considering one variable as dependent. If you can get a good R² and residual plot, with whatever transformation to make a linear model, then you may be able to assess there is a correlation between both. 

Go over list comprehension if you want a more pythonic way to do it, here is the most understandable to begin. 

In order to train some supervised learning algorithm to identify 'Problem' and 'Solution', you need to somehow generate some data that has labels of these things, which may be your best bet. So you would need pieces of article text that you know are 'Problems' and 'Solutions'. Then the trained algorithm could identify new articles based on the training set. But I imagine this is a lot of work to generate if you don't have this training data to begin with. You can also try some unsupervised method, for example you may split each article into multiple overlapping parts, and compute TFIDF vectors of each part. Do this for many articles, then cluster the TFIDF vectors. You can then look at your clusters and hope one of them is generally capturing the 'Problem', and another the 'Solution'. It will probably be very messy, and you'll have to clean it up with some post-hoc heuristic, depending on what the result looks like. I think this approach would be hard to actually get to work well, but it would avoid having to obtain (or create) a labeled data set. Finally, if you look at articles you may be able to devise generally good heuristics (e.g. the solution might be the first paragraph in a section titled 'Discussion' 70% of the time) to bolster both of these approaches, and if the article patterns are repeatable enough you may be able to just use heuristics to do a generally decent job. Since articles are often very similarly structured, this is where I would start to see how far I can get, and how much I can simplify the problem using heuristics before passing the simplified problem to the other methods. 

The tutorial you are looking at is attempting to model only the of a time-series. They first look at power of different frequencies, in the second figure in the tutorial. They find a lot of power for the 1/12 frequency, as well as a few other harmonics, and also the 1/72 frequency. But they decide that since the 1/12 frequency has the most power, and most of the others are harmonics of this, they will only model the 1/12 frequency. You can see this from this quote: 

In the specific case where the grouping variable is categorical, it is only very slightly clunky but it makes sense and is just fine. It sounds a bit smoother to use something like 'greater' in that case. For example you might want to say: 

If the grouping variable in question is numeric and you use 'increase' or 'decrease' to describe it, then describing the metric with 'increase' or 'decrease' also sounds just fine. E.g.: 

You can see the resulting plot that follows now better models the time series since it includes more of the frequencies that showed increased power. 

So you therefore only need one frequency bin, and its negative, since you are only modeling one frequency. In other words, in this part, they are just using essentially the equivalent of a single sine wave (at the 1/12 frequency) as their model to fit the data. All the frequency bin variable does is to act as an index and select the component of the fft() that contains the 1/12 frequency. They then analyze their result (the autocorrelation plots) and find that they have indeed missed some of the cyclical patterns at other frequencies they did not model. So then they add three more frequencies to the model. Rather than using frequency bins this time to do the indexing, they simply ask for the 6 highest in power (6 because each of the 3 needs both positive and negative frequencies) components of the fft() with these lines: 

It will search for ipynb files. Otherwise try to search for ipynb files from the root folder of your OS. 

You add a bicycle class or other classes that can be problematic in your opinion due to close features with your base classes, and train again. Complicated to be exhaustive with all situations. You define a threshold of confidence of your model: for example if there is no probability more than 95% then the model is not confident in its choice and you should not trust him. Good and larged models should stay at equiprobable choices at the softmax mater when the features are not recognized well. 

Named-Entity Recognition (NER) is one of the techniques you could look at. Different techniques exist, you could first look at pre-trained models of Spacy in Python here. Otherwise, you could train a model on your own training data if you have. Also, more systematic approaches can be efficient. For exemple, looking at POS tags values, do regex identification... More generally, this is the field of Information Extraction (IE) in text mining. Some good resources can be found here and here. Sorry, everything is in Python as I work mainly with it. Good luck! 

tokeninzing (nltk tokenizer, custom regex tokenizer) to identify words. Depending on the application, you can work on special cases such as english contractions, negations (sentiment analysis especially) and others normalizing urls and/or mentions, transforming each of them to the same version to reduce voacbulary size ($URL$ @mention) or totally removing them. removing of numbers, punctuation or others. Note that this is highly domain dependent, for example "!" has been shown to express a lot in sentiment analysis. 

When doing GridSearchCv, the best model is already scored. You can access it with the attribute and get the model with . You do not need to re-score it in a cross validation. Also, yes, the pipeline is entirely fitted when doing each split during the cv. 

It has been a long I am confused on understanding some of the AlexNet architecture : The output of the first conv layer is 55x55x48 (96 considering the division between GPUs but let's stick to 1 GPU so depth 48). Then max pooling is applied and there come my problem. When applying max pooling, the result is 27x27x48 right ? If so, how is applied the next convolution over this result (with 5x5x48 filters) to output 27x27x128 ? I finally don't see how and when to apply max-pooling in between convolutions. I must miss something here... 

1) Supervised learning is most of the time the process of learning a mapping, e.g relation, of input features x (sample) to an output y (often labels). Unsupervised learning doesn’t not use labels /output y to learn a relation between the samples and possible labels (ex: clustering). 2) Classification and regression are two types of supervised learning (discrete output labels vs continuous). Very good resources exist on the forum and web to go deeper with it if you’d like, don’t hesitate. 

I think you are referring to a king of image auto captioning, and why not sentence alignement ! I suggest you to go over the paper Deep Visual-Semantic Alignments for Generating Image Descriptions which is one of the references today (by M. Karpathy). The first step is about recognizing objects in an image and align sentences to them (done with R-CNN and BRNN) and the second is about generating new captions (CNN + RNN) Even though it may be quite difficult to deeply go through, there is some code available on GitHub and it has been remade by a a lot of people with various implementations. Here is the summary. Good luck !