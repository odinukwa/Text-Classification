A SIGMOD 2014 paper from Microsoft Research states that the "importance of sorting almost sorted data quickly has just emerged over the last decade", and goes on to propose variants of Patience sort and Linear Merge, and measure their performance on synthetic "close to sorted" data. It seems to me that this description matches the theme of "Adaptive Sorting", covering algorithms taking advantage of existing preorder in sequences to be sorted, which has been the topic of various publications (albeit in the community of theoretical computer science rather than databases) from as early as 1979: 

It took a few years (five!), but here is a partial answer to the question: $URL$ Optimal Prefix Free Codes With Partial Sorting Jérémy Barbay (Submitted on 29 Jan 2016) We describe an algorithm computing an optimal prefix free code for n unsorted positive weights in time within O(n(1+lgα))⊆O(nlgn), where the alternation α∈[1..n−1] measures the amount of sorting required by the computation. This asymptotical complexity is within a constant factor of the optimal in the algebraic decision tree computational model, in the worst case over all instances of size n and alternation α. Such results refine the state of the art complexity of Θ(nlgn) in the worst case over instances of size n in the same computational model, a landmark in compression and coding since 1952, by the mere combination of van Leeuwen's algorithm to compute optimal prefix free codes from sorted weights (known since 1976), with Deferred Data Structures to partially sort a multiset depending on the queries on it (known since 1988). 

Given a graph $G$, and a vertex-induced subgraph $H$ of $G$, there are two superficially-similar definitions ways to define the density of $H$: (1) Average degree of vertices in $H$ (2) What I will refer to as the "edge density" of $H$, defined as $\frac{ E(H) }{ \binom{V(H)}{2}}$. I'm interested in understanding the complexity of finding dense subgraphs, subject to constraints on the number of nodes, using the second definition of density. Unfortunately, most of the relevant work I can find, most notably on variants of the $k$-densest subgraph problem, uses the first definition. I'm looking for pointers to work on the second definition. First, I will elaborate on the similarities and differences between the two definitions before asking two specific questions. If one is interested in finding the densest subgraph with a given number of nodes $k$, it doesn't matter which definition you use. The two definitions coincide when comparing subgraphs with the same number of nodes. The result is the NP-hard $k$-densest subgraph problem, which is well studied, but who's complexity of approximation remains poorly understood. However,when looking for the "densest at-most-k subgraph" or "densest at-least-k" subgraph, the two definitions of density appear importantly different. Densest at-most-k subgraph: Using the average-degree definition, this problem is NP-hard, and its approximability is essentially equivalent to that of the classic "k-densest subgraph" problem [Anderson and Chellapilla '09]. Using the edge density definition, however, this problem is trivial: any two nodes connected by an edge are an optimal choice of $H$. Densest at-least-k subgraph: Using the average-degree definition, this problem has been studied before, and admits a constant factor approximation [Anderson and Chellapilla '09]. However, using the edge density definition, I can't seem to find any related work. This get more dicey when one considers both upper and lower bounds on $k$, the number of nodes in the sought induced subgraph. My questions: (a) Can anyone point me to work on approximation algorithms for densest at-least-k subgraph using the edge-density definition of density? (b) What is the proper name of what I'm calling "edge density"? Why does this definition appear more esoteric, as far as the approximation algorithms community is concerned, than the average degree definition? (Or is it?) 

Element distinctness can be solved deterministically in the RAM model in time within $O(n\log\log n)\subset o(n\log n)$ time: Sort in time within $O(n\log\log n)$ your $n$ numbers of $w$ bits using the sorting algorithm described by Han in STOC 2002 ("Deterministic sorting in $O(n\log\log n)$ time and linear space"), then scan in linear time for collisions. As far as I know, that is the best result known to this day. 

I am not sure I understood well your question, but if all weights are positive, I think that you can support weights updates in time within $O(n)$ (better than $O(n^3)$ per update, which I understand is what you asked?). Consider 

Have a look at my (modest) proposal of "fun" problem below. If you work on it, get in touch with me! 

A Hufman code associates an "integer" number of bits to each message: on some distributions this can sum up to a linear cost in the number of messages being sent (hence the $1$ in the $n(1+H)$ upper bound for the size of the sequence of bits produced. My understanding is that arithmetic codes overcome this weakness, at the cost of a more complex algorithm to compute the codes, and encoding tables (disclaimer: I am rusty on this, anyone should feel free to correct or complete), reaching $nH+O(1)$. The difference between $n(1+H)$ and $nH+O(1)$ is more sensible for $H$ small of course, so arithmetical codes are prefered for low entropy distribution, if there are no constaints on the cost of encoding/decoding. 

In the planted clique problem, one must recover a $k$-clique planted in an Erdos-Renyi random graph $G(n,p)$. This has mostly been looked at for $p=\frac{1}{2}$, in which case it is known to be polynomial-time solvable if $k > \sqrt{n}$ and conjectured hard for $k< \sqrt{n}$. My question is: what is known/believed about other values of $p$? Specifically, when $p$ is a constant in $[0,1]$? Is there evidence that, for every such value of $p$, there exists some $k=n^{\alpha}$ for which the problem is computationally hard? References would be particularly helpful, as I haven't succeeded in finding any literature which looks at the problem for values other than $p=\frac{1}{2}$. 

I have a basic question regarding the best known polynomial-time "distinguishing advantage" for the planted clique problem. By this, I'm referring to the problem of distinguishing the distribution $G(n,\frac{1}{2})$ of Erdos Renyi Random graphs from $G_k(n,\frac{1}{2})$ -- the distribution of graphs formed by planting a clique of size $k$ in $G(n,\frac{1}{2})$. This problem (as well as its natural bipartite variant) is conjectured hard for $k=n^{\frac{1}{2} - \delta}$ when $\delta >0$ is an absolute constant. In this post, you can think of $k=n^{\frac{1}{4}}$. It is often assumed as a hardness assumption that no polynomial time algorithm can distinguish $G(n,\frac{1}{2})$ from $G_k(n,\frac{1}{2})$ with "non-negligible" advantage. Formally, this means that for any "non-negligible" advantage $\epsilon>0$, there is no polynomial time algorithm $f$ from graphs to $\{0,1\}$ such that $|\Pr[ f(G)=1 ] - \Pr[f(G_k)=1] | > \epsilon$ for $G \sim G(n,\frac{1}{2})$ and $G_k \sim G_k(n,\frac{1}{2})$. In applications of this hardness assumption which I have seen, "non-negligible" is often taken to mean an absolute constant. In other words, it is believed that no polynomial-time algorithm can distinguish between the two distributions with constant advantage. My question is whether this extends to sub-constant $\epsilon$ which are not too small? i.e., is it believed hard to distinguish the two distributions with advantage, say, $\epsilon = \frac{1}{n}$ or $\epsilon = \frac{1}{\sqrt{n}}$ ? Clearly, there is some advantage $\epsilon$ more than exponentially small in $n$ but less than a constant where this problem crosses from being easy to being hard, and I'm interested in what is known about this "threshold". The only reference to sub-constant distinguishability I could find is in this paper, which (assuming I understand the paper correctly) seems to rule out polynomial-time distinguishability with $\epsilon > \frac{1}{o(n)}$ via "statistical algorithms." Anybody know what else is known / believed? 

I can answer your question with 100% certainty for a time period. If we consider the period from the seminal paper of Hartmanis and Stearns to any point in the future, the oldest problem in TCS which is still open is: 

I assume that talking with someone about $\mathbb{P/poly}$ and $\mathbb{NP}$ means that person is familiar with the $\mathbb{P}$ vs $\mathbb{NP}$ question and the verifying-solving duality. Then, I would try to explain that $\mathbb{P/poly}$ is so powerful because for each different length, the TM is given advice that it can trust completely. Then I would mention that we can devise hard (non-TM-computable actually) languages that have 1 word per input length (i.e. unary), so they are in P/poly ! But maybe a polynomial long advice isn't enough to solve all languages in $\mathbb{NP}$, since there we are allowed a different hint for every different input. On the other hand, I would remind that person that $\mathbb{NP}$ has to verify the answer, not trust it completely. So, we cannot use the same advice for each input length, it may not be verifiable! Finally, I would mention that complexity theorists believe that there are languages in $\mathbb{NP}$ that require more than polynomial many hints for some input length, and thus cannot be in $\mathbb{P/poly}$. A critical point for giving a good understanding, which I think is also common when teaching the subject for the first time, is making clear that advice and "hint" (i.e. certificate) are different things, and how they differ. 

The total precomputation time is within $O(1+2+4+...+n)\subseteq O(n)$ Answer a query for the $k$ smallest elements in $A$ in time $O(k)$: 

I was assuming that those two problems were equivalent, and I was using the second one as a pedagogical example for some parameterization technique (while there is an algorithm running in $O(n^2)$ for both, the second one can take advantage of variations in the size of the sets to solve the problem much faster, down to $O(n_1 n_3+\sum n_i\lg n_i)$ and $O(n_1 n_2 \lg \frac{n_3}{n_2}+\sum n_i\lg n_i)$). But I realized today that I did not know how to prove this equivalence, and I might even start to think that they are not. 

I named this problem "Binary Sorted Min Sum" for lack of a better name. The computational complexity of this problem in the worst case over instances of size $n$ is clearly $2n$ evaluations, $n-1$ comparisons, and $n$ additions, all within $O(n)$. A linear number of instances of this problem occur in the core of my colleague's solution, which complexity he analyzes as $O(n^2)$. Inspired by the fact that many instances of this problem can be solved in sublinear time (e.g. when $f$ is increasing much faster than $g$ is decreasing, one can certify that the minimum is at index $1$ in one single comparison, 4 accesses and 2 sums), I got some interesting adaptive results, showing tight bounds within $O(\delta\lg(n/\delta)$ for some parameter $\delta\in[1..n]$ measuring the difficulty of the instance, implying both the $\Theta(n)$ bound in the worst case and the $\Theta(\lg n)$ bound in easy instances: it is actually quite similar (but not reducible, it seems) to the "Binary Sorted Intersection" problem which I studied previously. A quick search on "Binary Sorted Min Sum" did not yield anything meaningful, yet the problem is quite simple, so I am wondering if someone introduced it before under another name? 

The general (non-bernoulli) problem is #P hard, via a reduction from #Knapsack. #Knapsack is the problem of counting the solutions to an instance of the knapsack problem. This problem is known to be #P complete. An equivalent way to think of the #Knapsack problem is the following: You are given a set of integers $a_1,\ldots,a_n$ and a threshold $t$. Let $x_i$ be a random variable which is either $0$ or $a_i$ with equal probability, and assume those random variables are independent. Compute the probability that $\sum_i x_i \leq t$. It is not too hard to see that the #Knapsack problem could be equivalently defined as the problem of computing the probability that $\sum_i x_i \geq t$ (simply flip the sign of all the integers and add a large constant). Therefore, had I stated my problem with an arbitrary probability $p$ rather than $0.5$, the problem stated in the question can be interpreted as the decision version of #knapsack. A reduction of #knapsack to its decision version via binary search would then complete the #P hardness proof. The way I defined the problem, however, fixed a particular threshold 0.5. It's not too hard to see that this doesn't make the problem easier. We can reduce the decision problem with probability $p\leq 0.5$ to the problem with $p=0.5$ by simply adding an additional random variable $x_0$ which is equal to the threshold $t$ with probability $\frac{0.5-p}{1-p}$ and $0$ the rest of the time. For $p > 0.5$, a similar reductions lets $x_0$ be $-M$ for a sufficiently large $M$ with probability $\frac{p-0.5}{p}$ and $0$ otherwise; if you don't like negative numbers, simply shift all random variables and the threshold $t$ up by a suitable constant. 

For more details, see the section on "Trans-dichotomous algorithms" from the wikipedia page for "Integer Sorting". 

Given the spanning tree structure of the solution of the all pair minimum path and the fact that the weights are always decreased, I think that this gives linear time per update. I think you could easily get the same result for directed weighted graphs, at the cost of some extra space and a more complicated structure. 

Let $u$ denote the size of the union of the sets in $S$, $n\leq 2^u$ denote the size of $S$, $h\geq n$ denote the size of the output, and $S_i$ the set of sets resulting of the intersection of $i$ sets from $S$ (in this sense, $S=S_1$), maintained in lexicographical order. For all $i\in[1..n]$, $|S_i|\leq h$. $S_2$ can be computed in time $un(n-1)/2\leq uh(h-1)/2$: each binary intersection takes at most time $u$, and there are at most $n(n-1)/2$ of them. $S_4, S_8, \ldots$ can be computed in time $uh(h-1)/2$ each in a similar way. Other $S_j$ which are not power of two can be obtained by combining those results, up to $S_n$. The total running time would be within $O(unh^2)$? 

where $S_{K}$ is the complexity class of all problems computable in $O( K(n) )$ . T(n) must obey $ T(n) \geq n/k$ for some integer $k$ and all $n$ and $T(n) \leq T(n+1)$ , but it doesn't have to be time-constructible. Your function obeys the first rule for $k = 1$ but fails to obey the second rule. Corollary 2.2 is the reciproral of the above and uses limit supremum, but still has these requirements. I guess as algorithms got more complex by the years, it is possible that the requirements have been relaxed. 

The idea is to traverse those subgraphs that will give as much gain as possible first, in order to be able to bear the cost of the negative weight subgraphs later. 

It's good to see a fellow undergrad in pursue of this great problem, with such an enthusiasm. Allow me to offer you a piece of advice from my own experiences. $ P \neq NP $ is a very interesting problem. The implications of the answer are immense, especially in the case that the two classes are equal. The reward is great in many levels, from the altruistic scientific one to the materialistic money award. That leads many young people that encounter the problem in trying to solve it, with no or limited knowledge about it. Perhaps most theory students go through that phase. You will have an idea and think it is right, but it is almost certain that you are wrong. Some people never get through that phase and embarrass themselves by being too stubborn to admit their errors. In FOCS 2010, Rahul Santhanam compared the $ P \neq NP $ question to a mythical monster. It would take many sacrifices and courage to even try to defeat this monster. After all, it may be the most difficult problem ever. To have a fighting chance, you will have to study a lot about this problem and complexity in general. You'll never know what the "monster's weakness" will be. So my advice is this: Take your time in knowing the problem. Every time you figure out a solution, assume you are wrong somehow and try to find the problem with it. That way you'll learn much. As for references, I would recommend Sipser's book as well. After finishing it, I would recommend "Computational Complexity:A modern approach" by Arora and Barak, a more complexity-oriented book, that requires a good understanding of the concept of computation. 

Say I have $n$ independent Bernoulli random variables, with parameters $p_1,\ldots,p_n$. Say, also, that I wish to decide whether their sum exceeds some given threshold $t$ with probability at least $0.5$. What is the computational complexity of this decision problem, when $p_1,\ldots,p_n$ and $t$ are represented in binary and given as input? More generally, I'm interested in the generalization of this problem to (non-Bernoulli) discrete distributions. Specifically, there are $n$ independent random variables, each supported on at most $m$ rational numbers, with each variable's probability histogram given explicitly in the input. In this case, also, I want to decide whether the sum of these variables exceeds $t$ with probability at least $0.5$. I have a feeling this problem is PP-hard, though I can't quite prove it. I wonder what the answer is, and whether it's already known. Note that I'm not looking for approximation algorithms for this problem. It's clear that monte carlo methods yield positive answers to approximate versions of this decision problem. I'm interested in the exact decision problem as stated above. 

By using the adversary method, the last generated time could be the previous object in the ordering of your preference. Therefore, in order to distinguish between any two numbers (and thus be able to order them) you would have to use information equivalent to the length of the full representation. I believe that this question is related to issues in databases and relational algebra such as primary key selection. I don't think you need to refrain to pure information-theory arguments in order to solve this problem. 

When considering variations of a computation model, let alone one that is not natural and might confuse our intuition, definitions are of the essense. Your oracle might be in #P but that doesn't mean that is representative of that class. Since you have changed the definition of the function the oracle computes, you will have to either prove the new problem #P-complete or find another appropriate class for which the problem is complete. Fooling around the complexity zoo, it seems that such a class has already been defined: $URL$ and $URL$ , depending on how you choose N.