As you can see when ran naturally or the code is selected the output is returned as expected, however once it is passed through sp_executesql it returns as ? characters. Its worth noting if you attempt to send a varchar instead of an nvarchar it errors out so it defiantly uses unicode. Have tried a few different collations for the input but that doesn't seem to change anything 

So we're working now, I'm not 100% sure which part of this fixed the issue but I'm putting it down for if anyone else has this issue in the future where there doesn't seem a logical reason for the distribution growth. Our distributor is on a windows failover cluster across two machines along with a Reporting Instance (and a MSDTC). 

Am trying to use to run commands but I've come across an issue where in non-english characters (aka Chinese & Japanese) are changed to ??? when ran. sql server 2008R2 & 2012 Example Code 

TLDR; if you can do your backups on your secondary, run them there, but system databases need to be done per server. Our main consideration for this is licensing, if your secondary node is passive then you can't take backups on it, if you're using it as a read replica then that's not an issue. Second consideration is what synchronisation method are you using if your actually in synchronous mode then your secondary node is up to date as the data is written at the same time. So if you're on an active secondary with synchronous commit then you can happily run the backup on the secondary node to release pressure on the primary node. The AG group is sensible enough to tell the Primary replica that the backup has been taken and will clear the transaction log out on the primary accordingly (all of our backups are done on secondary) hereby no backups never need to be done on the primary node. Any databases that aren't in the AG (aka master, model, msdb and any other locals) need to be backed up independently still 

to see where it is attempting to place the database files (observe the PhysicalName column). if that location (not the file the location) doesn't exist then you will need to move the file This can be done by 

The Page / Buffer life expectancy is simply how long the data is held in memory for See: $URL$ for a more detailed view Basically its a gauge on how volatile your data is in RAM, the lower the time, the more re-writing of data there is going on, quite often if you perform a large query that has had no data in active memory (like a lot of nightly reporting jobs often do) you'll see a large drop off on the life expectancy of the database, (I personally see this at 2.32 every morning when a certain report kicks in) If your low time is over two hours then you're doing alight still and wouldn't be worried, if it comes down to less than 30 minutes and stays there that's the point I'd start considering looking at what's taking the memory. Ultimately unless you're seeing a performance hit from disk reads (writing to memory wont be taking time unless you've saturated your memory 

Your best bet is to delete on cascade but that's not always an option Making a procedure to dynamically work out what columns exist and which relate to one another is not an easy task even if you do have all primary / foreign key relationships setup. I'd advise sitting down looking at where everything is and how it all relates, looking from the main table then each one of them in turn and delete based on the content of the first table to the second, second to the third and so on, you'll need to get the data from the first -> second -> third level in that order, but delete in the order third -> second -> first Its painstaking but it seems to be the best way rather than trying to essentially write an AI program in sql. (unless someone knows a really neat trick that I'm unaware of) I've got a similar process scheduled on my list of things to do next week, not going to lie, not looking forward to it. Ste psudo code for what I'm doing 

TLDR version: the availability group listener points to the instance where the database sits, not the database itself. Right now that that's out the way. If I'm reading this right you have multiple databases set up with Always On High Availability(AOHA) across two (or more)instances There are a few settings to consider here. Firstly the unchangeable, you never connect to a database directly, you connect to an instance and then to the database within that instance, if you want to be restricted to only access the one database you need to do that with user/group level permissions and have different accounts to access each database separately. The setting you have control over is what happens with the non primary instance, there are a few options: Connections in Primary Role (Allow all, Allow read/write) Readable Secondary (Yes, Read Intent Only, No) If you turn readable secondary to No, then you'll only ever be able to use the database if it is the primary node (note this will save on licensing as a server with all of the databases in this mode does not require to be licensed(few conditions)) If this is set to 'yes' or 'read intent only' then users connected to that instance will be able to see and read data from that database (obviously read intent only will only allow read intent connections). The connections in primary role is used to re-direct read intent connections to the secondary node without having to know about the read-replica On the note of separate listeners that depends on how you are using the databases, if they're completely separate and can exist within different instances (or their operations can be done with linked server connections) then being able to move the primary node to separate servers may allow you to unburden a server under heavy load. If the databases are dependant on being active on the same box and instance then there isn't much point in having multiple listeners as they will all always point to the same place which is ultimately the instance itself, it would be better to use different users (if possible) to lock off different databases (unless you wanted to do some really really complicated stuff with login triggers granting / removing permissions on the fly any time someone connects (which I would heavily not recommend and don't know if its actually possible) Hope that helps 

You want a pivot table (which isn't a inbuilt function in MySQL, but there are workarounds) A rough example (I cant test this here): 

Firstly this needs to pre-mentioned that only shrink files if its actually needed, as you'll cause a lot of index fragmentation which you'll need to clear up afterwards. With that said, if you're worried about the time it takes to run the entire shrinkfile then you can do it in stages (takes longer overall but technically can be stopped and resumed with a lot more ease), personally I use this to drop it in 2GB stages (also useful on high transactional dbs to reduce contention for the operation to shrink in smaller chunks I've found) 

note that the size of these files is quite large (depending on what you're doing, Its advisable to have your data and log files on separate drives and I've added in a separate Primary and Data file where DATA is the default file where new tables will be added, (its easier for DR if you there's a problem with the master database and its all in one file without having to worry about the rest of the database). 

The simplest way to view the differential (although this isn't exactly what it is) is a bulk transaction log You'll usually keep all the things in between to do a point in time restore if a problem was introduced at some point along the way you can restore the system to before it happened. So while you may have - 

Rather urgent one. Have created an index on one of our tables and for some reason all data going into that table has stopped. Can't see any blocking, but when I'm trying to drop or disable the index with either 

Something isn't right in this Your using the case in the where clause but I don't think that's where you want it. if seems like you just want to format the date Your not actually comparing the end date to anything which is where you're missing something (it is expecting there result of your case statement to be compared to something) I think this is what you're trying to do 

I notice this every now and again after I've had SSMS (2012) open for over a week or more and I've been running things like activity monitor / replication monitor / AO Dashboards Check opening another instance of SSMS to see if it takes the same length of time, if it is significantly shorter then have both closed (I'm not talking instant, just 2-3 second instead of 9-10) Then save out and scripts you need and close all copies of SSMS running on that machine and open them, should be back to a normal time. This has worked for me a few times in the past. 

I'm sure this is possible but I have no idea how or where to find the information on how to do it. Scenario: I have a reporting database (on SQL2008R2) which is split up into multiple file groups and multiple files. We're reaching the data limit of the current SAN drive and have reclaimed space from another SAN to provide the space we need. Problem: The 'new' SAN is on a 1GB connection whereas the current SAN is a 10GB connection, the intention is to keep all current data on the current SAN and put all old data or data for inactive users onto the slower SAN as it will not be used for anything other than very specific reports. Pseudo Solution 1 

For clarification from gbn's answer, if you want to designate the locations when you first create the database do it with the CREATE DATABASE command eg. 

Warning, make sure that the procedure is secured away and locked so nobody can use it by accident, we're having the procedure content commented out and everyone denyed access to it so someone must go in, edit the proc and then run it (we're only going to run it possibly twice in the next year, I don't know how often you intend to use yours) 

Replace with the current size of your file in MB and the with what size you want to shrink down to and with the name of the file you wish to be shrinking 

Presumably your connection is secured so only your office IP can access the database. With that in mind a secure VPN tunnel that only directs out to the database area would provide security from them accessing any other systems And from the database perspective his access rights should be secured down to read-only which will prevent him from modifying anything. See $URL$ for more details on creating users, note the GRANT command, GRANT SELECT will allow them select access, so they cannot update, delete, insert or perform any other such actions. See $URL$ for more details on RDS authentication if that is required As for VPN that depends on the infrastructure you have setup, speak to your IT Admins about this 

Our current implementation has a data coming into the system onto a Service broker queue a small amount of processing is performed in case more pertaining data comes in, then a SQL Agent job running every two minutes looks for any data that has reaches its 'time to process' and then adds it to another service broker queue. Is there a way to instead to add something to a service broker queue but not receive it till 5 minutes time (or 20 minutes, or 1 day depending on the data) There's the message_enqueue_time so you could technically delay everything in the queue, but there's things that need processing at different times from introduction to the queue. So I'd like to say this entry doesn't want to be picked up till x time and then this doesn't want to be done till y time(which could technically be a week from now). Is there any way to do this purely within Service Broker or any other way to make the service (next to) real time instead a pulse of data processing every 2 minutes (and not just run the Agent job more often) 

It means that if something is wrong with the Latest diff backup you can restore up to Trans 1.1.2 without loosing too much data if you see the above table think of your restore order as the point you want to restore to (so for the latest in that list 1.2.2 you require Full backup 1, Differential Backup 2 (remember you need all transactional logs though) Hope that helps clear it up Ste 

The other simple way is to put in a sub query as @dnoeth suggests in his answer The slightly more complex way is to instead of using a sub query putting the data you want into a view and then querying from that: 

I get a report of deadlock preventing it, but nothing is being reported to sp_readerrorlog Any clues as to what's going on? 

This will grab you the information from the active database[DB1], and then information from the other database [DB2] because you specified to go there to find it 

Table for each user would be wildly inefficient as whenever a user leaves or joins you need to re-arrange your table structure. having an field in the table and then allowing the general users to be identifiable meaning that when someone is logged in the query can run Be worth having a table to link the ID to the logged in user, and then you can also set up managers (Table User_ID, Manager_ID,name,detail...) which means you can allow managers to view all data of their underlings (if this is needed) Its worth noting doing a structure like this to consider indexing the table in a structure that organises the data according to the field to improve performance for each user as they will unlikely overlap 

Then any queries notice that the linked server is there, but has no access to anything within it. you'll find quite often that the queries for linked servers(as far as I'm aware) just check for the linked server (since you're declaring it as not as SQL database it doesn't seem to check in as much detail it presumes you know what you're doing (If you do it in the GUI you will get an warning but can still create the linked server without it registering that it can connect. Not sure if this is an acceptable workaround for what you're after since you are actually creating a linked server, but it is essentially a dummy as it goes nowhere, if you ever tried to access it you'd get a login error 

Problem solved Very hacky but got the system back Set the deadlock priority to high to stop it being auto killed there are a lot of external apps that access the DB which ended up blocking the drop index (information from the master..sysprocesses table) (but not reporting as blocking through all the checks we have), so I just killed until there was nothing more in its way, dropped straight away cheers for the quick eyes over