I am completely new to analyze cluster texts, I'm using Goodreads API to get Books synopsis. My goal is to group similar books, for example: 

Sundar Pichai is correctly identified as a person, but also the word users. How can I differentiate real names vs words which refer to persons? I have seen that for popular people there is metadata like Knowledge Graph mid or Wikipedia articles, but for others there is no reference, (Example: from "Susan Fowler" recent Uber scandal) Any ideas/pointers will be greatly appreciated. 

I'm downloading Stackoverflow questions & answers for a specific tag using Stackoverflow API and Python. The goal is to perform document clustering to find relevant terms across the documents and find similarity among them. Example: 

Im new to ML. I'm trying to predict if a new Music Album will exceed X amount of dollars in Sales. I'm looking to build a model to go only after potential best sellers. I do have historic data for Music Sales from 2010 till 2016. I have many signals: 

Some other signals like company url exists, but in terms of name matching wondering if text similarity is a good approach for this grouping problem. 

Not sure if this is the right forum, but currently i have a dataset which contains a list of TV shows. Each record contains pricing between competitors (price in provider 1. Example: Itunes) TV show cover image, synopsis, country of origin, language, etc. Looking for ideas what project is suggested that i can prototype it, i want to learn ML and this may be a useful dataset. 

Since questions and answers contain HTML, URLs, special characters, quotes, single quotes, commas, etc in body property. I'm looking to convert this text into structured data which can be represented as a single text chunk and be analyzed as tokens and then I use TF-IDF. Details: 2 Objects: Question and Answers Question object has an array of answers objects as properties. Each Question and Answer objects contain a body property which is a single string containing each text. What's the recommended way to store this information? 

I have a dataset which contains various columns: numerical and categorical. Dataset here: I was able to process the categorical data using and features in Pandas dataframe as explained here in Approach #2. 

I would recommend you to start from reading the draft of the introductory book "Sentiment analysis and opinion mining" by Bing Liu. The draft in a PDF document format is available for free here. More details about the new upcoming book of this author, as well as comprehensive information on the topic of aspect-based sentiment analysis, with references and links to data sets, are available at this page: $URL$ Another interesting resource is a survey book "Opinion mining and sentiment analysis by Bo Pang and Lillian Lee. The book is available in print and as a downloadable PDF e-book in a published version or an author-formatted version, which are almost identical in terms of contents. 

So far, so good. However, let's scroll down a bit further to see the the books ratings in relevant categories. We should expect Amazon to figure out categories, relevant to the book's discipline, topic and contents. How surprised was I (and that's an understatement!) to see the following result of Amazon.com's sophisticated ML engine and algorithms: 

Knowledge is a general term and I don't think that there exist definitions of knowledge for specific disciplines, domains and areas of study. Therefore, in my opinion, knowledge, for a particular subject domain, can be defined just as a domain-specific (or context-specific, as mentioned by @JGreenwell +1) perspective (projection) of a general concept of knowledge. 

Instead of exporting your models, consider creating an R-based interoperable environment for your modeling needs. Such environment would consists of R environment proper as well as integration layers for your third-party libraries. In particular, for the OpenCV project, consider either using open source project ($URL$ or integration via OpenCV C++ APIs and R package ($URL$ Finally, if you want to add PMML support to the mix and create a deployable-to-cloud solution, take a look at the following excellent blog post with relevant examples: $URL$ 

I have a very limited knowledge of game theory, but hope to learn more. However, I think that potential applications of Nash equilibrium in the context of big data environments, implies the need of analyzing a large number of features (representing various strategic pathways or traits) as well as large number of cases (representing significant number of actors). Considering these points, I would think that complexity and, consequently, performance requirements for Nash equilibrium in big data applications grow exponentially. For some examples from the Internet load-balancing domain, see paper by Even-Dar, Kesselman and Mansour (n.d.). The above-mentioned points touch only the volume aspect of 4V big data model (an update of Gartner's original 3V model). If you add to that other aspects (variety, velocity and veracity), the situation seems to become even more complex. Perhaps, people with econometrics background and experience will have some of the most comprehensive opinions on this interesting question. A lot of such people are active on Cross Validated, so I will let them know about this question - hopefully, some of them will be interested to share their view by answering this question. References Even-Dar, E., Kesselman, A., & Mansour, Y. (n.d.). Convergence time to Nash equilibria. Retrieved from $URL$ 

Hi I'm currently trying to predict if an item will be successful in my store, this means (How much is going to sale in USD) My training dataset contains many features: 

My goal is to create a Model which I can help me understand which Music may generate a lot of revenue (Boolean). I created a field AverageRevenueGenerated which is the average of all Revenue generated for all artists. Im looking for a tool that can help me associate or generate insights based on input signals above. This cold be automatically or a specific guide that allows me to say for example if: 

I'm able to train my model and test data. When I generate my training dataset where categorical features were encoded. (Using ), this numerical data is different than my predictions. Example: 

I'm currently using Levenshtein distance, based on that I can find out if Book is in remote system or not. In this case if Book1 vs Book2 result score exceeds X, I assume is the same Book. 

I'm using a rich dataset of Movies and I currently need to group if a Movie is the same across different Retailers. Example: Movie: Beauty and the Beast Platforms: Google, Netflix, iTunes, Amazon. I have access to signals like: Studio, Movie Name, Runtime, Language, Release Year, etc. But in the case some Movies which are not the same and signals mentioned before are not capable to find right match I need to do what a human would do: Check Movie cover. Example: 

return ~0.5 similarity. hardwired metallica and metallica hardwire return ~0.433 Other docs with more words return higher values. (Im using cosine_similarity from sklearn.metrics.pairwise) I iterate over each document and get the similarity among all docs, after that I extract the highest values. (cosine similarity > 0.55) So far is working fine but there are cases in which I can't find similar sentences unless I reduce my coefficient, doing so it may associate other values to non-related items. I want to know what is the best technique to group common sentences from a list of sentences. Not sure if that would be semantic similarity. 

I'm using Cosine similarity to find common documents and group them. I have realized that similar docs: 

The dataset in use is PIMA indians which contains exactly 768 records. In Keras when I pass it runs fine. In my model, since at epoch 78 there is no more data (78 * 10), we start feeding empty information. What would be the correct behavior when training a model and number of epochs already consumed all data for model. The obvious answer would be stop training or continue feeding data to model and restart counter and start passing information from beginning. Any feedback is appreciated. 

While @Ben's answer is nice and partially introduces what should be done first with a newly cleaned data set, I feel that the approach is important enough to have its name presented loud and clear: exploratory data analysis (EDA). Therefore, the short answer to your question is that the first step should be EDA. This suggestion is supported by most researchers, regardless of their knowledge domain or type of study. Here is how the father of EDA presents his thoughts on the subject (Tukey, 1977, p. 1-3; emphasis mine): 

Another idea is to combine OpenStreetMap project map data, for example, using corresponding nice R package ($URL$ with census data (population census data, such as the US data: $URL$ as well as census data in other categories: $URL$ to analyze temporal patterns of geosocial trends. HTH. 

I would suggest to consider using latent variable modeling (LVM) or similar structural equation modeling (SEM) as an approach to this problem. Using this approach is based on recognizing and analyzing latent variables - constructs (factors), measured not directly, but through sets of measured variables (indicators). Note that a closely related term latent feature is frequently used within the machine learning domain. It seems to me that latent variables resemble what you call "combinations/subsets/segments of the IVs". By hypothesizing - usually, based on theory or domain knowledge - the latent structure of factors, LVM or SEM are able to automatically confirm or decline those hypotheses. This is done by using a combination of exploratory factor analysis (EFA) and confirmatory factor analysis (CFA) (see my answer ). While EFA is frequently performed independently (and maybe that's enough for your purposes), doing it along with CFA represents a large part of LVM/SEM methodology, which is usually completed by performing path analysis, which is concerned about relationships between latent variables. The ecosystem offers a variety of packages for performing LVM/SEM in its entirety or for performing EFA, CFA and path analysis. The most popular ones for EFA are , and . The most popular packages for CFA, path analysis and LVM are (the first R package for SEM), , , , . Various supplementary SEM-focused packages are also available. 

Check the Stanford NLP Group's open source software ($URL$ in particular, Stanford Classifier ($URL$ The software is written in , which will likely delight you, but also has bindings for some other languages. Note, the licensing - if you plan to use their code in commercial products, you have to acquire commercial license. Another interesting set of open source libraries, IMHO suitable for this task and much more, is parallel framework for machine learning GraphLab ($URL$ which includes clustering library, implementing various clustering algorithms ($URL$ It is especially suitable for very large volume of data (like you have), as it implements model and, thus, supports multicore and multiprocessor parallel processing. You most likely are aware of the following, but I will mention it just in case. Natural Language Toolkit (NLTK) for ($URL$ contains modules for clustering/classifying/categorizing text. Check the relevant chapter in the : $URL$ UPDATE: Speaking of algorithms, it seems that you've tried most of the ones from , such as illustrated in this topic extraction example: $URL$ However, you may find useful other libraries, which implement a wide variety of clustering algorithms, including Non-Negative Matrix Factorization (NMF). One of such libraries is Python Matrix Factorization (PyMF) with home page at $URL$ and source code at $URL$ Another, even more interesting, library, also Python-based, is NIMFA, which implements various NMF algorithms: $URL$ Here's a research paper, describing : $URL$ Here's an example from its documentation, which presents the solution for very similar text processing problem of topic clustering: $URL$ 

I tried this example and works fine for a specific number of clusters (K) where K < N | K <= N. But since searches are unpredictable need to find a way to automate the number of clusters: my goal is to cluster 2 or more similar items and let alone single searches in independent clusters, example: Cluster 1: 

I do have access to the art image. I'm using Python to do this comparison. Is there a library that can help me compare 2 images and determine if they are similar? 

Will be potential high revenues. I found house prices example: $URL$ is it the same type of problem? I'm looking which input signals may be the highest revenue. Any insights or pointers will be helpful. 

Most of the examples I have found online for LSTMs refer to "random" text generation. One of the problems that I'm trying to solve is to generate a "summary" of many docs into 1 doc. For example: 

In case of Toll fraud and client being insecure, the attacker can send calls via original agent hence the Service Provider won't reject calls immediately at least based from IP information. (Toll fraud). I'm exploring which approach is the best to implement an ML model, instead of static rules to be able to detect Toll fraud in a live system: 

I'm new to RL, does this scenario makes sense for live systems? Can I use offline data to train an RL model? Offline data includes Fraud cases detected historically. Reference. 

When I want to do a prediction and I pass the original record which looks like this: (Header just for reference) 

I'm building a function to pass training information for my Tensorflow model. It is similar to Keras: 

I read about Word Embeddings and before I start coding want to check if converting my text information to embeddings may improve my results. 

Not all the features present in training data/test data will be present when I will be making predictions. Is this normal in ML ? What is the rule of thumb when doing feature engineering for this type of cases. 

I provide an offline library of Music to my users. My goal is to understand what my users are looking for, which means translate raw user searches to: Music Artists, Songs, Albums and then add music to the company library. What are the suggested clustering algorithms to group common short sentences into a single entity. Example: 

I also suggest software ($URL$ which seems to be quite powerful. Some additional information on using with large networks can be found here and, more generally, here. ($URL$ is an alternative to , being an another popular platform for complex network analysis and visualization. If you'd like to work with networks programmatically (including visualization) in R, Python or C/C++, you can check collection of libraries. Speaking of R, you may find interesting the following blog posts: on using R with Cytoscape ($URL$ and on using R with Gephi ($URL$ For extensive lists of network analysis and visualization software, including some comparison and reviews, you might want to check the following pages: 1) $URL$ 2) $URL$ 3) $URL$ 

No, you cannot. This is the corollary from my above-mentioned thoughts. Data science does not automatically imply big data - there is plenty of data science work that Excel can handle quite well. Having said that, if a data scientist (even experienced one) does not have knowledge (at least, basic) of modern data science tools, including big data-focused ones, it is somewhat disturbing. This is because experimentation is deeply ingrained into the nature of data science due to exploratory data analysis being a essential and, even, a crucial part of it. Therefore, a person, who does not have an urge to explore other tools within their domain, could rank lower among candidates in the overall fit for a data science position (of course, this is quite fuzzy, as some people are very quick in learning new material, plus, people might have not had an opportunity to satisfy their interest in other tools due to various personal or workplace reasons). Therefore, in conclusion, I think that the best answer an experienced data scientist might have to a question in regard to their preferred tool is the following: My preferred tool is the optimal one, that is the one that best fits the task at hand. 

I think that it is impossible to answer this question comprehensively, at least for the following reasons: 

There are tons of materials on financial (big) data analysis that you can read and peruse. I'm not an expert in finance, but am curious about the field, especially in the context of data science and R. Therefore, the following are selected relevant resource suggestions that I have for you. I hope that they will be useful. Books: Financial analysis (general / non-R)