It looks like based on your error that the user you are trying to create a login for already has a login in that database with another name, so you are effectively "double mapping" them. If you have access to SSMS you can see what their mapping is by using the Object Explorer and navigating the following path: Security -> Logins -> Double click on the username This should pop up a GUI, you can see the "User Mapping" in the top left hand corner, this should show you to whom the user is mapped. 

The easiest method is (probably) to uninstall and this time run the command line installer again with a different set of flags and it will install to a different location unless you are talking about components not listed below. From: $URL$ Proper Use of Setup Parameters Use the following guidelines to develop installation commands that have correct syntax: 

Yes, SQL Server can report how long it took to do any of those actions (though you may have to run it to get some additional details such as actual row counts returned) Statistics Time 

At first when I tried, I was told that the IP was not allowed. So I added the IP to the server firewall settings. But now, I get this: 

What makes me wonder if the database is actually inside the elastic pool is what's circled in red in the above screenshot. 

Another clue that the DB is not in the pool is that the pool shows no usage. So, I'm pretty sure I did something wrong. But I can't seem to figure out HOW to put the database inside the pool. Help!?! 

I am in the process of using the Data Migration Assistant to move a local DB to Aszure SQL. The schema transfer went well, and 243 of the 246 tables all were able to transfer their data with no errors. However, the transfer has been running for 13+ hours, and over the last three hours, three of the tables have not budged on their percent complete. One is at 92.3%, one is at 93.9%, and the last is at 98.5%. Some questions: 

SQL Server Setup Control /INSTALLSHAREDDIR Specifies a nondefault installation directory for 64-bit shared components. Default is %Program Files%\Microsoft SQL Server Cannot be set to %Program Files(x86)%\Microsoft SQL Server /INSTALLSHAREDWOWDIR Specifies a nondefault installation directory for 32-bit shared components. Supported only on a 64-bit system. /INSTANCEDIR Specifies a nondefault installation directory for instance-specific components. 

To explore a bit more about the question and comment, I want to note that generally when normalizing your goal (among others) is to reduce duplication. A naive example of your table might be everything you have but adding things like the CSV list you mention, this ends with a very wide table that has multiple areas of duplication and which will require you to manage and clean your data (and is generally bad for performance.) A solution to this problem is to simply project the data into another table and provide the keys to join back to the original. Eg: 

For an example of windowing functions and subqueries in T-SQL(if you assume g1 is your table and not some subquery I just invented): 

Service still would not start (although it looks like nothing changed, so I wasn't surprised). I've tried to start it from within the normal Windows Services, and from within SQL Server Configuration Manager. In the Properties, I have tried to logon as all of the options on the Built-in account dropdown. I've also tried the admin account I use to connect to the DB from within SSMS. The service is set to Start Type automatic. I'm not sure what else to try. Any suggestions? 

I am trying to set up an Azure SQL Database inside of an elastic pool. I'm not sure if I did it right. Here's my dashboard: 

in the server name when connecting via SSMS. Also, the username is an email address. Let's say it's: 

Not very helpful. I found the SQLAGENT.OUT file, but there is nothing recent in it. The last entry is from over a month ago. None the less, this is the contents of the file: 

I am trying to connect to an Azure SQL database via SSMS. The connection string supplied by Azure is: 

If you find yourself saying "nullable columns" before you have written code I generally think you need to normalize more(as always, it depends.) If you want one system to maintain, the last option seems the most relevant, but "splitting them into two tables" is not really where you would want to take it. if we want to track basic things like Players, Games, and Sport type you would simply add intersection tables between the relevant things you want to store. 

You are grouping by the thing you are counting, not by the department name. Change your group by to: 

Simple answer, you grouped by your sum. Solution is simply to remove that from your group by statement, eg: $URL$ 

Edit:Adding a little flavor text as requested. Basically what is happening is you are choosing to aggregate one of the values (in this case the counts of the salary) so that it "rolls up", and the group by generally indicates which value you want to do the rolling up by. It makes some sense to say "I want to group by the number of employees" but you are actually trying to express "I want to return the number of employees grouped by department." 

Looking at SQL Server maximum capacity limits on here: $URL$ Database size can be as big as 524,272 terabytes. Here, you could see VM size limits on Azure: $URL$ If you look at the table on the second page, the maximum number of data disks that could be attached is 16 (1 TB each). Does that mean on Azure, SQL Server cannot house more than 16 TBs and anything more than that would require a new VM? 

I don't have expertise in architecting databases, and I've been teaching myself new stuff every day. I'd like to make an Internet-scale application using SQL Server as the data store. I haven't found any good information online with regards to scaling out SQL Server. My understanding is that scaling out is great for write throughput, but it doesn't necessarily scale reads. A simple example (which is relevant in my case) is, if data is sharded by posting user id, status 1 posted by user X living in shard A will have all its likes and comments across the whole federation. So, if I need to fetch the comments on this status, I need to hit every database and merge and sort/filter results in application memory. This is bad for the databases because they are kept busy and bad for the web servers because I will be using CPU and RAM for post processing the objects. Ideally, I'd like to write to one database and read from one database for maximum scalability. Now, what I'm thinking of doing is, instead of sharding by posting user id, shard by receiving user id. So, if user X posts status 1, user Y living in shard B can insert a comment in shard A, and I can enforce a parent-child relationship between the status and the comment. User Z living in shard C can insert a like in shard A for the comment, so the comment and the like can constitute a parent-child relationship. The benefit of this approach is I query only one database to get all the comments and likes for a specific status rather than naively querying every single shard. However, I need to get results like "comments on status 1 by people who are male or 18+ years old". This is a crucial functionality I want to implement. I still have to hit other databases to get information about the users. In order to eliminate this, I'm thinking of creating a sync group where one database (hub) syncs all user deltas to all shards (every 5 minutes). I'm okay with eventual consistency though it has its own problems for example, if a user deletes their account, from the time the account is deleted to the time the delta is persisted to a shard, other users will not see the change potentially adding child objects to objects created by that user. This seems to me a data integrity issue. I'm also aware of replication and caching to increase read throughput. My question is, which approach should I pursue? If I choose the second one, will I have trouble syncing data across potentially hundreds or thousands of servers? Not to mention the hub is essentially a single point of failure. 

This means we could count all of Michael Jordan's games by sport(as he played multiple) with a query something like: 

This table would store the ids for both the previous tables and any information that is specific to that exam's instance of that question. This would allow you to write queries such as "How many questions belong to one exam" 

I dont think so except for the fact that there is a 5NF, which describes a design where your joins are only on the candidate keys. Many "4NF" designs meet this criteria, but not all, and it is definitely something you can change a 4NF "into" to be be more normalized. 

As a fairly newly minted DBA under the gun, I have run the gamut of free tools and done some experimentation in the paid space (DPA, SQL Sentry, and Foglight) and it really depends on what you want the tool for. In my experience the most important thing was not just communicating performance baselines (management vastly didn't care unless there was someone to yell at), but produce something in an easy to consume format that made the priorities clear and was able to track down performance issues in production. You can absolutely build up your skills by going the free route, and the tools for SQL Server are great. 

Bah. This server and elastic pool was created by my boss. I got permission from him to wipe it out and start over. When I created it myself, I was able to log in from SSMS no problem. 

On my localhost, I am running 2014 (not express). I need to create a job, and for that, I need the Agent service to run. But I can't get it started. I have been googling the snot out of this, but so far no luck. In the Windows Logs, under System, I have only this: 

(The was the name of the former developer who used to use this laptop. I am his replacement. For his sake, I removed his name.) Following some advice I found on other threads, I ran this against the master database: 

I am using the same UserID and Password that I use to log into the Azure portal. And it's the UserID that is specified in the Access Control window. I am working from home, but I have also tried while on my company's VPN. Neither work. Is there something else I'm doing wrong? 

With these and some additional databases/tables and jobs and time you can build out a basic monitoring system (but it isn't pretty) these are tools for DBAs; unless you are good at BI stuff you will struggle to find time to produce useful business friendly stuff from it, though the Ozar sp_blitz app is pretty dang cool. After spending around a year doing the free thing and resolving plenty of issues (but not getting much buy in) I was able to make it clear, after a major issue, that perf monitoring software was a priority, and we were going to buy it come hell or high water. After demoing the previously mentioned clients, I chose DPA because management could easily consume the results, though I definitely have client licenses for SQL Sentry Plan Explorer Pro (1000% worth the money) and really liked using the server version, it just didnt grab them the same way. I also tried getting SQLNexus working at one point but I ended up working a lot than I was interested in, it may suit your needs.