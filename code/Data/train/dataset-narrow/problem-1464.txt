You have the data set apparently, profile and see where the time is spent. Determine the time complexity of your algorithm. These types of problems almost never require you to "optimize" your code to make it faster, they need you to nail the correct algorithm. Let me rephrase that, even a shitty, most sub-optimal implementation of the correct algorithm with the right time complexity will likely pass the time requirement (in fact this is typically one of the design goals when making the data sets: A poor implementation of the right algorithm must pass while a super optimized version of the wrong algorithm must fail). If you're getting time limit exceeded, you most likely have the wrong algorithm and time complexity or you have a bug (infinite loop or w/e). 

And implement the other methods by calling . I'm also not sure if your detail is correct. I think (without testing) that: 

OpenMP is not a magic box that will automatically make everything faster. You still need to think about synchronisation and proper algorithms. Also starting a thread does have some overhead and if the work done by the thread is small in comparison to the overhead, then you're slowing the program down. There are plenty of resources on parallel programming on the internet, I recommend starting with doing your threading manually until you know how it works and then start using tools like OpenMP. 

Hash value collisions Your hash function might be causing your problems. As you are only using the hash value () in the closed and open set (as opposed to a that implements and to resolve hash value collisions), a hash value collision will appear as the node has already been visited even though it might not have been. Meaning that you skip parts of the search space, parts that could contain the solution. This could result in that you simply miss the easy solutions and keep on searching for a long time. For a 5x5 problem there exists 25 unique tiles (the empty tile counted as a tile), so you have 25! ~= 1.55E+25 unique board combinations. Using a 32bit integer as a hash code, at most 2^32 ~= 4.29E+9 unique board combinations can be represented without a collision. So for each hash value there exists ~= 3.61E+15 (=1.55E25/4.29E9) boards that have collisions. Or put the other way, if you pick two random boards the likely hood of a collision is: 

The code above is less efficient performance-wise since it does more - yet it is safer and much more readable (and IMO elegant :-)). Simplicity and readability reduces errors and thus testing and maintenance costs. If you refactor processing of the input parameters (validation, conversion into a map) and creating the String array out of the method it gets very short, very straightforward and more efficient. Since the input validation should be done anyway its cost shouldn't actually be taken into account. Now the command line processing is pretty ad-hoc, there are better ways to do it - for example using . 

to spare on the call. At the end I’d suggest reverting the keys and values of the mapping hash (because you start from roman characters), using symbols instead of strings as keys and replacing recursion with a loop: like this: 

meaning "find all groups consisting of one or more apostrophes and replace each of them a with starting apostrophe followed by the found group and an ending apostrophe". Now try to do this without a regular expression - it would require much more complicated code. All this will reduce the very act of processing the to a mere one liner if you exclude checking for null: 

Now this means "find all groups of single or double apostrophes and replace each of them a with starting apostrophe followed by the found group and an ending apostrophe". This works only for single or double apostrophes - so does probably the right thing for your use case. To enclose any number of input apostrophes in a pair of the same, you'd use: 

I assume you get the arguments as a array - given the above you should first transform it into a - the most efficient would be an , but I would suggest using a which will preserve the order of the arguments. This will also validate the input and only accept valid arguments, thus preventing the program to crash because of an invalid input. Now what you actually do to the is basically enclosing a single or double apostrophe within a pair of apostrophes. There is a way to do it without iterating over the string variable character by character - use a regular expression, all you need is: 

White spaces are free, use them Your code is hard to read as all operators are cramped together. Avoid branches A missed branch prediction can force your CPU to discard all speculative execution it has done. Although modern CPU's are pretty good at branch prediction, it's a good practice to avoid branches when possible. And in your case it will reduce iterations in the loop. 

is undefined behaviour. If the object has been destroyed, there is nothing that says that will have been set to in fact I'd wager it won't. At any rate as your waiters are reading from you need to inhibit destruction of until all waiters have returned. Otherwise you risk reading freed memory. You should initialize all variables when they are declared: 

To me the implementations look identical. But one thing jumps out at me: If is big then you will do a lot of string concatenations here: which may look inconspicuous but if n=399, then you will do around 160k useless string concatenations. Try to use a and build all your output into it and then dump it in one go when you're done. Or simply change to: 

So you are very unlikely to have correct closed/open sets after 300 000 iterations. Remember with A* you're searching for the optimal solution and you will expand a lot of nodes to find it; As opposed to if you're just looking for a solution then you'll take the first solution you find (greedy best-first search will do this quickly). The closed set is not necessary (and neither is the open) The purpose of the closed set is to guarantee that you do not expand the same state twice as the consistency of the heuristic guarantees that the first time you expand a node is the best way to expand it. It is just an optimization. If you get rid of the closed set you will still find the optimal solution but you may expand each state multiple times. However as A* remembers the cost to get to a specific state you're going to expand better (less costly) paths first. If you don't have the closed set, you can simply remove the open set as it is essentially the same as your fringe of nodes to expand. This should help you on larger problems, but running out of memory after 300k nodes sounds odd, which brings the next point: Adjust your JVM memory limits If I'm not mistaken the default memory limit on some JVM's is quite low, you can increase this by passing for example as command line argument to java. 

Web Client implementation doesn't need the actual implementation, if the service is hit able, it's client can surely be written. 

Naming Convention: , doesn't make sense to me. If I were you, I will be making them more readable e.g. etc.. But this is just my opinion. Your default constructor , you don't need to do as , and are object members so these will be auto initialized as , so I don't see any good reason to do . 

As far as OOP paradigm is to be considered, I do not see any problem with the implementation and as per my knowledge and experience of OOP, I can not find any mistake or make any suggestion for any improvement. Well, here are few pointers that I will consider if I will implement the same thing: 

As far as I see, your code is well written but just my two cents. Appending @Malachi answers as you don't need the nesting. May be he forgot to tell about: 

Other than that, if you are using Java 7 or 8, you should be using try with resources instead of try-catch as that is less error prone. And instead of returning null, you should be returning an empty collection . As far as I know, returning null is not a very good approach. 

I want to know the problems that I might not see at the moment but can occur in future? Also, is there much better way to improve this piece of code with respect to multi-threading? If someone here can help me cloning or downloading the whole project and suggest about the overall architecture of application and multi-threading, I will be very very thankful. But since codereview is only to review some particular piece of code, so here it is, I want to review. 

Yes, other end might fail, so you can implement a timeout to get the response and if it doesn't, push your item back in the queue. And BlockingQueue is threadsafe already so you don't need to worry about using ThreadPoolExecutor instead just to make it thread safe. As far as efficiency is concerned, I am not very sure about it, may be someone else could provide suggestion about that. 

I think that conceptually you're fine. There are some details that need attention. Hide the implementation Your creation syntax: 

requires the user to know that is a this isn't ideal. I believe that it's a good idea to hide the implementation class. You should put your class in an implementation namespace: 

Overall I don't see any major problems with the code. There are some things I would change though: Write unit tests. Whenever I deal with mathematical classes, I think that it is obligatory to provide unit tests to verify correct computation and computational precision. Add JavaDoc Please add a JavaDoc to clarify whether it computes sample or population statistics. Add comments The use instead of in the look ups to has a significant but subtle difference that is easy to miss if you're not an expert. I would add comments to clarify this (you are correctly avoiding and getting different counts). Use Since Java 8, which we have to assume is wide spread by this point, you can replace: 

Represent 2D matrix with one vector Conventional wisdom is that 2D and 3D (or nD for the matter) volumes should be linearised to 1D arrays by computing a 1D index from nD coordinates. I.e. replace: 

I have not benchmarked this, so take this with a grain of salt. In the merge step you are iterating over the lists repeatedly. If you consider a pathological case that results in \$\frac{n}{2}\$ lists of \$2\$ elements each. Then on the first step you reduce to \$\frac{n}{4}\$ lists of \$4\$ elements each if I read your code correctly, then \$\frac{n}{8}\$ lists and so forth. This means you will do \$\log_2\left(n\right)\$ passes and in each pass you will iterate once over all \$n\$ elements. Giving you \$\mathcal{O}\left(n\cdot\log_2\left(n\right)\right)\$ steps through the linked list. Iterating through all elements in a linked list is \$\mathcal{O}\left(n\right)\$ just as it is for a vector. But in my experience the constant factor for iterating through a list is is a lot larger than for a vector, I'm not talking about 10-20% but more like 300-2000% larger. This is due to the difference in locality of reference, for a vector it is contiguous memory accesses and the CPU's prefetcher is going to ❤❤❤ you for it. However for a list you have to chase the pointers and worst case is you get a cache miss on every pointer (can happen if your list size exceeds the CPU cache size) in the list resulting in a \$\approx200\$ cycle delay on each node. A custom allocation strategy for list nodes can mitigate this to some degree but it is like putting a band-aid on a stab wound, you need stitches. My gut feeling is that the \$\mathcal{O}\left(n\right)\$ insertion time at the head of a vector in the encroaching phase is going to be recovered and then some in the merge phase due to higher locality of reference. But as always you should benchmark to find out. In fact I think you might be able to use that has \$\mathcal{O}\left(1\right)\$ insertion at the front and random access. However I believe that is implemented as a list of vectors, in which case the size of the vectors is important, this is one of these cases where a custom data structure might benefit you. 

Next you have a hash map but do not use it as such, just iterating over values. Even then you could use: 

The organization of your code seems strange to me: should be class or instance method of the String class or maybe neither? The method lies outside of the String class bracket, yet references private hash and . My throws a runtime error while trying to execute it. Then the code doesn’t properly handle invalid (non-roman) characters, it stops recursion because for the first invalid character the mapping will fail, in this case not the result of the canversion up to that point but the array of the ’s values will be returned, which is supposedly not what you want to achieve. After putting within the class frame and adding public before it for the code to execute: 

I admit it is longer, but behaves better raising error on erroneous input and is faster – on my machine benchmarked against your code was regularely almost 3 times faster. UPDATE @200_success Thanks for asking - besides the performance gain there are two more reasons for using symbols instead of strings as keys. Never use mutable objects as keys in hash tables This is reason enough - mutable objects as keys are evil, if they change unpredictable things can happen, since values are placed in buckets of a hash table based on hashcode of the keys, now you can imagine that changing the keys of pair after it has been placed into a hash map is never a good idea. And strings in Ruby are mutable! I admit though, that this will rather not happen in our case, yet it would be a bad practice anyway and should be avoided just to keep the code clean. And it seems that in fact Ruby copies and freezes the strings if used as keys in hash maps in case we forget about this rule :-). Nevertheless we have our performance gain by using symbols. roman_mapping is in fact an immutable constant Neither keys nor values would ever change. So it is logical to use an immutable class for the keys - like symbols. Now numbers are immutable, too, but the hash maps are not. So while we are talking this, we should make also immutable using , and while doing this we can also replace the method with a frozen constant. To make immutable all these steps are necessary - simple freezing the hash map wouldn't be enough. If a reference to its keys leaks into the outside word it still could be changed. I've just updated the code above. A new benchmark on my machine shows now even more performance gain: the new code is more than 10 times faster than the original: