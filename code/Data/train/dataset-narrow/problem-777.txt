This trivializes finding the last day of the month. It is one day before the first day of the next month. No need to calculate leap years, lengths of months, et cetera. 

My experience is that a useful index gets used almost immediately. It does change the information available to the optimizer after all. However, a new index may never get used if the optimizer rejects its value. You should note that a stored procedure may hold onto an existing plan for some time. If so, you can use to cause a stored procedure to (yes) recompile and assess the value of your new index. If you want to know much more about the optimizer and indexes, you should read through this article by Benjamin Nevarez: Index Selection and the Query Optimizer $URL$ This should give you some insight into how things work, at least enough to get you started on testing a potentially better index. 

If you that only returns one row (the count), is relatively light, and is the way to get that datum. And is not a physical no-no, in that it is legal and allowed. However, the problem with is that you can cause a lot more data movement. You operate on every column in the table. If your only includes a few columns, you might be able to get your answer from an index or indexes, which reduces the I/O and also the impact on the server cache. So, Yes it is recommended against as a general practice because it is wasteful of your resources. The only real benefit of is not typing all the column names. But from SSMS you can use drag and drop to get the column names in your query and delete those that you do not need. An analogy: If someone uses when they do not need every column, would they also use without a (or some other limiting clause) when they do not need every row? 

One thing missing from the sample tables above is and (or whatever you want to call them). Since you are tracking history, the dates are obviously valuable and can be used when querying older entries. Having a and a table does split out the information, but that is a mixed blessing, since the student's full information is split across two different tables. You might consider implementing like this: 

The MSDN Blog "SQL Server SSIS and Replication" has a post from a few years ago. $URL$ How these old snapshot folders are purged depends on the type of replication you are using: 

This will cause the BULK INSERT to process 1000 rows per transaction, commit the transaction, then start the next transaction until the file is processed. Of course, you can change the 1000 to any number you choose. If you are importing data from another database (either on the same server or across a linked server) then you may need to write some looping code to insert 1000 rows (or 1 row, if that is what you want) at a time. 

Your problem is apparently all about storage space. Your SSD array is not large enough to hold all your history on SSD. Creating another filegroup is a good way to separate the use of storage space. That way you control what goes onto the historic data disk array. (When creating objects on a filegroup other than default, you must be sure to specify the filegroup you are using of course. ) Depending on how massive the historic data is, you may not need to split the data by quarters, etc. For example, you were running well on the SSDs until space became a problem. So, remembering that space is the problem, you could ask yourself questions like: 

Well you can use , write (or find and use) a CLR routine to do file copies and things like that. For example, the unrecomnended use of . 

In this Oracle page entitled Database High Availability Best Practices it discusses Data Guard Deployment Options: Requirement of Zero data loss protection and availability for Oracle Database: 

If one database successfully restored, it certainly does not mean that the next database has no problems. (E.g. I once, many years ago now, had some disk array problems that resulted in about 20% of the databases on a server manifesting corruption.) I run a weekly process that and runs on all databases from over 70 servers. This is done one database at a time and for me takes most of a week. For fully logged databases we also test the latest full backup by restoring all the logs as well. I have a dedicated machine for doing this, so that there is no conflict with anything happening on the standard landscape of servers. The truth is that most everything works fine, but occasionally there is a corruption and that is followed as quickly as possible with a resolution. Statistically you can test fewer databases and feel pretty safe, but checking them all gives you a quicker heads up. 

These are Severity 16 errors which are correctable errors. You might need to fix these first to avoid a later problem. 

Then to grant access to the CustomConnection endpoint to the SQLSupport group (as an example), you could use something like this: 

Certainly adding more cores will change the amount of pressure each CPU must face. So, it seems quite reasonable that more processors will reduce the pressure on each CPU, which in turn provides better CPU throughput. 

If you have a serious problem that requires lowering or raising the minimum, you can reconfigure the setting (without a restart being needed) and it will change up or down in fairly short order. The actual SQL Server memory use, if my memory serves me, can be a little higher than the minimum value, because of some OS overhead. 

So this is about what you are trying to accomplish with each server, not how fast the data is processed. 

For speed of your query as shown, the primary impact on performance would be whether or not you have an index (which should probably be either a or else ) on . That index would allow a seek to the row that needs to be updated. With regard to column order or for the two updates in your sample, there is no difference. You should view the two column changes on a row as happening all at once. Furthermore, just for clarity, if your update changed many rows, you still should view the changes as happening all at once. (Though it could take a longer time, of course.) 

An administrator of the SQL Server can change the amount of cache memory available to the server. So, that that extent you can influence the query cache size. However, there is not a configuration to say "this much to the plan cache and that much to the buffer cache". SQL Server dynamically manages the use of cache as needed to balance the pressure of plans and buffers. There are ways that you can influence the reuse of cached files, such as 'forced parameterization', but that assumes that you really know your data. (And that you trust your optimization over the SQL Server optimizer.) 

This allows you the chance to cobble things back together, but it can still have issues. So, please, make sure you maintain good and complete backups. 

Actually there is a way to wildcard full text search using both prefixes and suffixes combined with operator in SQL Server. This is a little bit more expensive since it requires some preprocessing of the query. You can selecting the needed individual words from the full text index views or . After you have all the words you will need to use a to create the query. Here is some sample code to show how it works: 

To decide how to manage you blocking problems first read: SET TRANSACTION ISOLATION LEVEL (Transact-SQL) If you read this MSDN post you will see that there are several different configurations of how to manage how your data and throughput can be used. On a highly-active server I have used Read Committed Snapshot Isolation, but usually Read Committed isolation is sufficient for less stressed systems. And there are side-effects of every decision that you make. 

You might find it interesting to check Thomas Kejser's take on indexes in SQL Server. Although clustered indexes are very useful, there can be reasons to keep a heap. For example, read this post: $URL$ Particularly look at the topic: Fragmentation Prone tables with lots of INSERT activity This topic seems to exactly describe the issue that you are facing with fragmentation. So... 

Yes, you can separate tables into individual files and spread those files across different disks. One example of documentation in MySQL (in this case for 5.7) is: $URL$ A partial quote says: "This mode is controlled by the innodb_file_per_table configuration option, and is the default in MySQL 5.6.6 and higher." Spreading the load can be useful for many applications. (Using slower disks is really only for economic reasons of course.) 

If you have defined the Foreign Key constraints as then the Primary Key value that was changed should cascade down to all the Foreign Keys with that constraint. If you do not have the constraint, then you will need create scripts to complete the update. EDIT: Since you do not have the constraint, but you want to have that set up, it is a bit of work. SQL Server does not support altering the constraints to a new setting. It is necessary to iterate through each table that has a FK constraint to the PK table. For each table with the FK: 

Some years ago we implemented READ_COMMITTED_SNAPSHOT isolation on a database that was severely suffering from blocking. But once we changed the isolation level we began getting deadlocks in a couple of critical areas. Why did this happen? Because the previous isolation level caused heavy blocking, the code could "never" reach the point of deadlocking. However, with READ_COMMITTED_SNAPSHOT isolation, the queries could keep moving forward. However, some percentage of the no-longer waiting transactions began deadlocking. Fortunately our case was resolved quickly by determining the deadlock points and adjusting the indexes on a couple of tables to have a more rational column order. This greatly reduced our locking problems. 

Since you have determined that the two database approach is not right for you, you want to merge the two databases. (And I agree for all the little value that is) (1) One suggestion, the simplest, is to rename one of the tables and make the needed changes to the code to use the proper table instead of the proper database and table. (2) Another suggestion would be to merge the two tables into a single table. You would need to add a column to differentiate between the "Quotes" rows and the "Jobs" rows. Also, you would need to make sure that your primary keys are unique between the two tables. If these rows were generated with Access ascending keys it is likely that you have duplicate keys between the two tables. If this is the case, you would need to change the Primary Keys in, let us say "Jobs", to avoid any PK conflicts with "Quotes" rows. Then also change all the referencing tables to have Foreign Key values that reference the new Primary Key values. If there are many duplicate tables, then repeat as needed to bring everything into the new united database schema. This will be a bit of work to set up, but once completed you should not need to think about that any more. 

First of all, the Table Designer is not the wisest of all software and probably does not choose the approach to change the table that you would expect. The designer usually creates a new table, moves the data, then drops the original table. However, based on your error the designer also may not be savvy. But if you try it yourself through commands, you will see that SQL Server cannot alter a column to remove the rowguidcol setting. See what happens: 

SSRS has a number of configuration files, including RSReportServer.config. Both Shared Schedules and Shared Data Sources are good things to create since they help you manage your configuration without giving each and every report it own schedule. View this as saving you some work. Of course, independent Schedules and Data Sources will likely be needed for some reports. Since you are concerned with overall load, note that the MaxQueueThreads configuration will control how many parallel reports can run at once. 0 means run as many as possible, but some other number (e.g. 3) will limit Reporting Services to run that number reports at one time. If you have 10 reports on the same schedule the other reports will run as Queue Threads open up for use. To configure appropriately need to know your resources available and configure appropriately. Probably there will be a little trial and error until you get things balanced. 

The Maintenance Plan task can reorganize indexes and that should have some benefit for you. Likely, however, you will reorganize more than necessary using that approach. You should look at Ola Hallengren's solution for Maintenance at: $URL$ 

You must write code to populate the materialized view and to refresh the data as needed. If you must have the most current data, then you will need to write triggers to provide the needed updates. 

You can try to address the problem by increasing the timeout periods. Increase the standard settings on your server for and . Or in your connection string you could include "Connect Timeout=120;General Timeout=120;" to increase the time allowed for the connections. 

It is asynchronous, but I have never experienced a dramatic change. As the buffers are flushed, the memory is released to the OS. Of course, this means that the data cache is smaller and less data is kept in cache which will affect performance since it may result in more storage I/O. Jonathan's post at $URL$ makes the general default recommendation: 

If Kerberos is needed because of multiple hops between computers: Microsoft® Kerberos Configuration Manager for SQL Server® has some guidance on setting up and configuring in your environment. 

If you have the default trace running, and you should have it running, it may be able to help you see what happened. The default trace is lightweight and among other things does track Server Stop and Server Start. See Feodor Georgiev's article: $URL$ The trace logs recycle fairly quickly on a busy server, so you would need to check as soon as possible after a Server Stop and Server Start. Toward the bottom of the heading Security Audit Events there is code to ‘Audit Server Starts and Stops’. (Tweaked only slightly to include the default count of trace logs.) 

If you like that solution then you should be able to keep all the configuration inside the SQL Server. 

There should be no real performance problems related to using the system objects to build your T-SQL. Consider also that building your dynamic T-SQL is probably a much lighter load on your server than actually running the T-SQL statements that you generate. There are also INFORMATION_SCHEMA views, but these only partially cover the system objects. I mention this only to warn you that, although once recommended, it has many limitations and I do not use them any more. Likely the biggest issue for your code is that system objects do change over time. Probably most changes would not affect your code generation, but you should plan in advance before upgrading to a new version of SQL Server. SQL Server documentation usually details out the changes for the next upgrade. For example, here is a link for SQL Server 2014's breaking changes. $URL$ 

In addition, you can create an audit table and use a DDL trigger to track the changes being made to the database by tracing the details you want to know about. You can also filter out nuisance changes, such as disabling and enabling indexes and so forth. Sample DDL Audit Trigger: 

If you need a free and much more functional version of SQL Server, consider using SQL Server Express. Its limits are considerable above the Compact edition. You can lookup its limitations. 

If you RECOVER the database at any point between the Database restore and a transaction log, the transaction logs that follow are no longer in the same recovery path. Recovery of the database starts a new Recovery Path. If you are using Enterprise Edition (likely what Shanky was wondering) instead of recovering the database in order to read from it, you can create a SNAPSHOT DATABASE that you can read from. Or, you could and can also use to allow you to read from a database still in RECOVERY. Both of these approaches should permit you to restore further LOG backups in order to reach a new point of recovery. There are instructions for a warm standby at: $URL$ From your description I am unclear on exactly when you get the backup files and what your recovery points are intended to be. But either a SNAPSHOT database or STANDBY mode may help you. 

I believe that if you run native SQL Server backups (using T-SQL BACKUP) instead of DPM protection agent-based SQL backups, the SQL Server VSS Writer service must not be used on your SQL Server database folders. (The same problem is true for other third-party backup tools.) This DPM/VSS service is not used by native SQL backups. If you prevent the DPM protection agent backups from triggering SQL Server VSS snapshots, this will protect your chain of log backups. If DPM/VSS does continue to backup the SQL Server files, then the log chain is broken, because there are two tools (T-SQL BACKUP and DPM/VSS) backing up the log files.