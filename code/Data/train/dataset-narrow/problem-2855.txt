The magic in that screenshot is the Fresnel effect at the border. A plain transparent sphere with a texture is not sufficient. There is a demo in the Ogre programming book, but I don't know if sources are available. 

What data structure would you use to represent meshes that are to be altered (e.g. adding or removing new faces, vertices and edges), and that have to be "studied" in different ways (e.g. finding all the triangles intersecting a certain ray, or finding all the triangles "visible" from a given point in the space)? I need to consider multiple aspects of the mesh: their geometry, their topology and spatial information. The meshes are rather big, say 500k triangles, so I am going to use the GPU when computations are heavy. I tried using arrays with vertices and arrays with indices, but I do not love adding and removing vertices from them. Also, using arrays totally ignore spatial and topological information, which I may need studying the mesh. So, I thought about using custom double-linked list data structures, but I believe doing so will require me to copy the data to array buffers before going on the GPU. I also thought about using BST, but not sure it fits. Any help is appreciated. If I have been too fuzzy and you require other information feel free to ask. 

There is a good documentation on Wikipedia. The format is made by Wavefront, you'll find lot more information online with this name. This is the Wikipedia article. Basically, means vertex position, means texture coordinate and means vertex normal. defines indices of a face. It isn't that easy to draw models in a modern way, since their indexing works different from how OpenGL buffers work. In an file, indices (faces) point to a position, a normal and a texture coordinate separately. In OpenGL instead, an index is expected to point at at a single position in all those attributes at once. I struggled with the format for some weeks and then decided to use the great loader Assimp which supports and among many others. You may consider to use that library from the beginning. 

Though similar issues are often caused by clipping, the near plane is not the issue here. If it would be, the disappearance would be per pixel and not per triangle. In your animation the triangles exactly disappear at the moment all of its three vertices get outside of the screen. Your algorithm may be based on the false assumption that triangles are hidden when all their vertices are hidden. Here is an article talking about a good frustum culling implementation. 

This is mostly a documentation question. I hope this is the proper place to ask and not, for example, programmers or stackoverflow. I did not read the OpenGL specification, but from books (Red Book) and various tutorial, I cannot properly grasp how different objects and parts of the API interact and relate. For example, I know that if a VAO is bound, then binding a VBO and setting an attribute will result in the VBO being bound to the VAO. But there are many objects, their relation may vary. For example, the VAO is completely unrelated to the current shading program, but the outcome of drawing operations may depend on both of them. Among all the various OpenGL components, I can see some relations like A-binds-to-B or A-and-B-are-used-by-C. All these relations concur in determining how one can alter the state of the context, how can change things, how can save things together and optimize. I am looking for resources explaining just these relations. Like an UML diagram, a graph, or even a text document, which does not focus about the calls, but describes in a compact way how the various OpenGL components interact and relate one to each other. Do you know where to find such a resource? 

If your renderer is to slow to handle "a couple of 2D images", it won't help to simply take the game loop into another thread. But I assume that you are facing another problem. For a smooth frame rate you have around 16 milliseconds per frame. According to the code example you posted, your game loop includes a fixed the delay of 10 milliseconds. 

I suggest automatically generating, and maybe caching, the convex collision meshes instead of creating them by an external tool. However, I am not sure which of the following ideas is what you want. Do you want to end up with a single convex shape for a single model? That would be a convex hull around all vertices. Of course that simplification results in not that accurate collisions but on the other hand it is very fast compute them. Moreover, you have only one physics body per object which is faster than the alternative below. 

I lately integrated Bullet Physics into my little game engine, but for now I only use basic shapes as spheres or boxes for collision checks. For more realistic physics I need collision meshes for all models in the scene. They should be much simpler and in lower detail than the actual model for drawing to speed up computations. Since I want adding new asserts to the engine as easy as possible, I am looking for a way to automatically generate collision meshes. I could just use the actual mesh loaded from file and that would result in very realistic physics calculations but that won't be fast enough. Therefore, how can I generate low detail collision meshes from a given set of vertices? This task doesn't seem trivial since there many different cases to handle. For example a model of a house with furniture must retain details of floor and door frames while ornaments should be removed. I believe there already was research on this topic, but I couldn't find anything useful yet. 

This works fine for basic static tile levels, but when the player is standing on a descending platform, or moving down a downward slope for instance, it becomes very difficult to jump. The reason for this (in the case of the descending platform) is that the platform has moved from underneath him in that frame, so for that moment he is not actually colliding with the ground, and therefore the condition to be able to jump is not fulfilled. Is there a general way to add some leniency into the jump mechanic so that the player can jump even if they are not technically on the ground for that exact frame? This seems like it should be quite a common problem but I cannot seem to find any similar questions on stackexchange or google. 

I have written some jumping code for my player in a platformer game. At the moment it has some basic logic which says that the player can jump if he is on the ground. The pseudocode looks something like this: 

I am currently making a game with the java library LibGDX and want to add XBox 360 controller support using the GDX-Controllers extension. At the moment I have a setup where I create a listener to listen for particular button presses, then attach that listener to the static class using the method: This method works fine for listening for button press events from controllers which were connected at the time the program started but, if I try connecting a controller after the game has started, events for this controller are not registered. I cannot see from the source code a way to update the listener for new controllers, and it appears from this blog post that disconnects and re-connects may not be supported on the desktop. My question is: Is there a solution to my problem using GDX-controllers and, if not, what are my options for getting full Xbox 360 controller support (Including controller connects and disconnects) If I want functionality which integrates with the libGDX framework? 

I'm totally not experienced, but I think that a good solution is based on heuristics, not on a complete checking of the known map. Heuristics I can think of are locally based and experience based. Local controls can be based on local terrain check and obstacles, keeping moving toward the required direction. I think that most maps don't require complex maze-like movements, but are pretty connected. Another heuristic is to use previous known paths (explored by other units or explicitly by the user) to move units to known or near-known positions. But I'm talking about moving on big maps, not really in closed spaces like ZorbaTHut said. In crowded cases the algorithm may be more complex, requiring sorts of "prediction", coordination among units of the same team or just semaphore-like waiting strategies. Also, note that continuous or discrete terrain and unit size calculation are really important when working on this case. I think heuristic algorithms are good because they usually provide a good solution on big spaces with a reasonable computation time (which does matter, when you're moving many units). Sorry if this a generic answer: I worked with crowds, but the space was pretty peculiar and I can't explain exactly how the algorithm worked (was agent based, anyway, not globally defined). I hope you can get some useful ideas from my answer. 

Before integrating the Bullet Physics, the camera worked fine. But instead of using my own simple struct for transform data, I use rigid body structs provided by the physics library now. There are two issue with the first person camera at the moment. First, it is stuck in the ground and when I try to move it, it slips back to it's position. Second, When I get the camera out of the ground, which sometimes happens, The applied rotation isn't in the way I move the mouse. When I move the mouse straight, the camera is moved in circles. This is how the rigid body it set up. 

I am developing a game with a Minecraft-like terrain made out of blocks. Since basic rendering and chunk loading is done now, I want to implement block selecting. Therefore I need to find out what block the first person camera is facing. I already heard of unprojecting the whole scene but I decided against that because it sounds hacky and isn't accurate. Maybe I could somehow cast a ray in view direction but I do not know how to check the collision with a block in my voxel data. Of course this calculations must be done on the CPU since I need the results to perform game logic operations. So how could I find out which block is in front of the camera? If it is preferable, how could I cast a ray and check collisions? 

The reason for this seems to be that collisions are detected after the player has been pushed through the shape by its movement or gravity. When the system resolves the collision, it resolves them in an order that doesn't make sense (for example, when the player is moving from one flat rectangle to another, gravity pushes them below the ground, but the collision with the left hand side of the second block is resolved before the collision with the top of the block, meaning the player is pushed back left before being pushed back up). Other similar posts have resolved this problem by having a strict rule on which axes to resolve first. For example, always resolve the collision on the y axis, then if the object is still colliding with things, resolve on the x axis. This solution only works in the case of a completely axis oriented box world, and doesn't solve the problem if the player is stuck moving along a series of angled shapes or sliding down a wall. Does any one have any ideas of how I could alter my collision system to prevent these situations from happening? 

I disagree with the first answer. It is possible to procedurally generate seamless images, it is described in Michael Goodfellow's blog post Part 84: Seamless Textures. Although the article shows an example for an animated lava texture, the technique can of course be adapted for water easily. 

Traditionally, models consist of lots of vertices connected by triangles. That forces the use of a high amount of vertices for detailed organic shapes or makes models kind of blocky. Even though polygon models can be smoothed by subdivision algorithms, this slow, not that exact or still needs a lot of vertices. Instead, models could be represented by 3 dimensional splines, bezier patches, nurbs or something similar. This way, we would gain perfectly detailed models instead of blocky one with much less vertices. I am not an expert but I assume that a spline could be even rendered faster than its polygon representation. To make the such mathematically perfect shapes more detailed, we could use displacement mapping. The only big challenge I can see is projecting those splines in the correct perspective. We can simply multiply all vertices with a projection matrix since they are not connected by straight lines. Instead the parameters of a curve must be modified to fit another perspective. I am not sure if there are mathematical straight forward ways to do this. Are there games using the latter approach?