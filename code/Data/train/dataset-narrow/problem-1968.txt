To combine that with an output stream, consider this dedicated script which adds a timestamp to each input line: 

Instead of , you need to use or with the timestamp and the name of field to extract, such as 'hour' or 'day'. Calling on the results is not necessary since these functions produce numbers, not strings. See the doc at $URL$ 

It appears that the date exposed to the user by Windows Mail does not come from the header field, but rather from the latest field ( fields are primarily meant to trace a message through delivery gateways, as mentioned in RFC5321 and RFC5322). When fetchmail delivers a message, even with the option (not using SMTP), by default it adds its own field with the current date and time. Fortunately it provides an option to disable this: 

The quotes around are Unicode fancy quotes as opposed to normal ASCII quotes. That's wrong and it explains why the query fails if you have copy-pasted that query like I just did above. They must be replaced by normal single quotes as in: 

From the result of you don't have a PostgreSQL 10 server actually installed. The name of the package is exactly : $URL$ . You should install that package. The lack of error of and the fact that a service does exist are confusing: it's because is an "umbrella" service that launches every postgresql instance installed and configured. In your case you currently have zero such instance, but that's fine as far as the service is concerned. In the most general case, you may have several different versions of PostgreSQL running concurrently (from different packages ), and several instances of the same version too (from the same package). I'd suggest to check your PostgreSQL instances with rather than . See also to control them. 

The purpose of this map is for to log in as the db user. In , the keyword should apply to the user. Also in the psql invocation, as a user must be explicitly given as argument. This works for me: pg_hba.conf: 

I believe you want to combine this option with a non-SMTP delivery, for instance with . Otherwise the SMTP server would probably add its own field after fetchmail. 

is invalid because the domain name lacks a trailing dot. This is mentioned in RFC1537 (Common DNS Data File Configuration Errors): 

The environment variable is considered when the option is not set. So you may use a batch file essentially doing: 

If that file doesn't have any specified encoding, you might want to import it as such. If the target is a dedicated database, in case of PostgreSQL, you may create it without enforcing any encoding. As a SQL command: 

(combined my being set to ) tells that non-local connection attempts must be rejected. If you want to accept mail from the outside, don't do that, leave to its empty default value. 

This could be refined to timestamp only the output lines that signal the start of a sub-command, assuming they can be grep'ed: 

is a command that has to connect to the server just like any other client command (even when run on the server itself), and as such requires a database user to connect with. If you don't specify a database user with option, it takes your OS username, hence the reference to in the error message. The password that is asked is the password of a non-existing database user. There are also other options such as the host defaulting to and the port defaulting to , see its manual page for all of them. Since you mention that you know the postgres superuser password, the simplest solution would be to use the user to initiate the command. It makes sense anyway, because on a fresh install this user would the only one that has the permission to create a database. Try It should ask for a password, and that will be the password of the database user. If it doesn't ask for a password, it means that the security policy in is configured that way. If the password is lost (or no password has ever been set, which is the case for some automated installs, especially on Unix), has to be changed and the service reloaded to authorize passwordless connections, at least temporarily to set a new password. 

You might be right if all the updates to browsers were just bug fixes and tweaks to the original version. However, each new version adds more features which introduce more potential for bugs and security holes. Also, security and usability are often at opposite ends of a trade-off, and features sell more software than the lack of security issues. 

This seems like it should be so basic, but it apparently isn't very intuitive. The scenario: (1) I've inserted a table into a new blank MS-Word document. (2) I want to add some text outside and above the table. Problem: The table is butted right up against the top of the document and I can't move the cursor to a point before the table to insert any new text. I've found that you can do this by cutting the table to the clipboard, type the new text and paste the table back, but there just has to be a more straightforward way to put the cursor at a point before a table that is at the top of a document. 

In case it helps, I am running FF 3.5 on that exact same OS and patch level and it seems to be working. If you indicate the error you are getting I might be able to help further. 

Another key reason I think it is so popular is that it is considered a "Green" way of operating your data center, because it has the potential to use less electricity. And Greenwashing is a big thing for corporate PR departments as of late. In a typical non-virtualized environment you you have build each server with excess capacity to handle the peak load, which means that you have a lot of extra horsepower suckling on a power outlet just in case everyone decides they need to kick off an expensive request at the same time. In a virtualized environment, multiple logical servers can share that excess capacity under the assumption that the logical servers co-located on a physical machine aren't all going to get maxed out simultaneously. A second reason it is gaining steam is that it is riding the coat-tails of cloud computing. Virtualized servers are a core technology that makes it possible to offer many of the features of cloud computing, that not-coincidentally mirror those of virtualization. Cloud computing is a hot trend right now and chances are that if you are putting servers in the cloud they are virtualized servers. 

I have a Dell laptop computer issued to me by my employer with an Internal network card. I am considering upgrading my home wifi network from 802.11g to 802.11n, but want to make sure the laptop will benefit from the upgrade. Is there an easy way to tell if this internal modem supports the 802.11n standard, or maybe is there a way in the OS (Windows 7) to get the make/model of the card so I can look this information up? 

I have a wireless network in my home that consists of a 802.11g/b router connected directly to my broadband provider and several devices (Ipod, Xbox, Laptops, etc) that access the network over the wireless connection. For a couple of corners of my house I have a really weak and sometimes intermittent connection that I'd like to improve and am trying to determine the best/cheapest way to accomplish this. The question is this, with an 802.11n router do I get the benefit of the longer range when talking to an 802.11b device? It is well documented that I won't get the full 802.11n bandwidth, but I am only concerned about getting a few more bars at further distances. If this won't work, what is the best way to increase the reach of my wireless network with the constraint that the router can't move to another physical location. 

I store all of my data on an external 5TB USB 3.0 HDD. I also have an external 3TB USB 3.0 HDD that I periodically backup my 5TB to (I only have about 2TB of data). The hard-drives used to be used by a Mac in the house, and so every file on the drives had a hidden .dstore file associated with them. When I moved to Windows, I formatted the 3TB drive as NTFS, copied the data over to it, then formatted the 5TB as NTFS and copied the data back over. That got rid of the Apple Partitions but I still had the .dstore files to deal with (in hindsight I could have fixed that while copying, instead of using the steps below. Live and learn) To solve for that, I installed Beyond Compare and synchronized the 5TB over to the 3TB. I setup my filter so that the .dstore files were not included. That took a long time. I went to bed and slept for 6 hours, got up and it was still copying. I left for work and it finished sometime during the day. When I got home, I then formatted the 5TB and had planned on just copying all of the data over to it like I did when I moved off the Apple Partition. This however is where my issues started. It doesn't matter if I use Beyond Compare or Windows Explorer, copying my data from the 3TB over to the 5TB fails with error code . Both Windows and BC tell me it's a fatal hardware issue, but it doesn't tell me on which device. Initially, the copy starts and it tells me it'll be done in 1 hour and 15 minutes. Then it hits the same location on the disk every time and falls to an ETA of 27 days, then fails. At first glance I would assume the 3TB drive now hosting my data is failing, and not the 5TB. Since it happens at the same location (usually stops at the exact same file), I'm thinking the OS can't read data from the 3TB at that location. I've tried using different USB hubs, and I've tried it with different combinations of USB ports on my machine but it seems to not make a difference. Unfortunately, this is now the only place I have all of this data (lesson learned, have more than 1 backup). Does this sound like hardware failure to you guys? If so what steps can I take to get that data off safely? I have the super important stuff, like all of my family photos and videos, on the drive synced to Google Drive. So worse case is I can download my 600gb Google Drive directory again. The other 1TB of data is my iTunes media and my Plex media. I can re-download the iTunes library and I still have all of the physical discs to re-create my Plex library. I just have a limit of 500GB of download per month at my ISP so getting that data back will take me a couple months of incremental downloads. The Plex library will take weeks to re-create (DVD/Blu-Rays take forever to convert!). I'm really hoping you guys can give me some guidance on tools or approach I can take to try and get the data off onto my 5TB drive safely. Note: I typically keep a complete backup on BackBlaze, but when I moved the license to my Windows machine the backup was deleted from BackBlaze. BackBlaze needed 93 days to fully backup my data and this happened 3 days into that 93. Another lesson learned, don't touch your data until it's backed up off site. EDIT The Health tab in HD Tune showed the following messages: 

I am planning a wireless network that will involve a wireless radio modem that must be mounted pretty far from my house on a post (Needs line of sight to work) and then getting the signal back over to the house using an outdoor wireless access point(something like this, for example). Both the modem and the WAP can be powered by Power over Ethernet and I was wondering if I can chain them. For example, can I replace this configuration... Cat5 -> PoE Injector -> Cat5 -> WAP -> Cat5 --> PoE Injector-> Cat5 -> Modem With this configuration... Cat5 -> PoE Injector -> Cat5 -> WAP -> Cat5 -> Modem Is this possible? Also, Is one of these two configurations a bad idea for some reason? 

This seems like it would be obvious, but I can't find it anywhere on the screens provided when I log into my wireless router. Maybe I'm just missing something obvious here, but I thought I'd take the chance of looking stupid and ask the community anyway. Is there a simple way to see all the current devices connected to my wireless router? It is a LinkSys WRT54-G router if it matters. 

There are a number of console type applications, e.g. ipconfig, that output information to the standard output. The problem is that if I create a windows shortcut to them it runs the command and closes the console window before I can read the output. I realize that I could create a batch file with a pause command and then make the shortcut go to that batch file, but I was wondering if there was any generic way I could configure a shortcut to a console/command-line type program to leave the window open until the user explicitly closes it. 

Sure. Just open two instances of access by clicking the icon to start Access instead of clicking the database file itself. Then within each instance of Access open the Database from the file menu. 

MS Access is actually a combination of a rapid development UI tool and file system based relational database (JET). Pros: - Easy deployment, just copy the file out to the network and tell people the path to it. - Rapid prototyping and very good UI development tools for database type applications - Generally much cheaper to deploy, especially for small installations. Cons - Maintenance - You have to lock out all the users when you want to do DB compaction, repair or other maintenance. - As a result of using a file based DB, it is more prone to data corruption with a large number of users or with flaky network connections. - Although the number is debated, you are going to max out the number of users you can support with a single Access DB much sooner than with SQL. MS SQL Server is a Client-server relational database system, with no UI development tools built in. Pros: - Maintenance - Lots of tools for maintenance, can do most of it with users in DB. Thus, you get higher uptime. - Enterprise scale - it is designed to support a lot more users and scales better to handle them. Cons: - Expensive - For your 5 user app to keep track of a few hundred thousand items, it can be overkill. - More complex - All the extra features introduce a learning curve. - No built-in UI development tools - You will need another dev platform to build a front-end and (arguably) reporting. Actually, Access can work as a front end for SQL, but for a number of reasons DBAs tend to hate Access users connecting to their databases. Miscellany - Although you may hear otherwise, don't make the decision based on the size of the DB, make the call based on features and size of your user base. - Access actually is a pretty good UI tool for databases hosted in SQL Server. So the right answer might be "Both" depending on your needs.