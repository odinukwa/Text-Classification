Yes, the wrong SQL Server installation files (for the CAL-based setup) were used. The 20-core-limit version is designed for people upgrading from older versions of SQL Server, as explained by Aaron Bertrand. Check with the team who got your installation files for SQL Server (ISO, CD, ZIP, etc) to get the core-based installation files instead of CAL-based installation files. Microsoft's instructions for Linux don't include CentOS, but if you read through the comments on that page, they tell you how to pick your repo. If you're still using the latest CU repo and you're still getting the issue, then contact Microsoft support - they may have deployed the wrong binaries to the repo. (This wouldn't be something an outsider could fix.) 

Oh, goodness, I have some bad news here. On a 32-bit OS, SQL Server only uses the first 4GB of memory for things like query workspace. (And it's fighting the OS for that 4GB, too - any other running apps will also compete for that memory.) 4GB might sound like a lot, but it's relatively easy to write a query that needs several GB of memory in order to run. When enough queries demand enough memory, SQL Server throws RESOURCE_SEMAPHORE waits because queries can't get enough memory in order to start. RESOURCE_SEMAPHORE_QUERY_COMPILE means they can't even get enough memory to compile an execution plan - and yeah, that's pretty bad. So how do you fix it? 

Occam's razor suggests starting with the obvious: If your script sometimes leaves a database in restoring state, then debug the script. Start by logging what you're doing in a table or a file. Then, when you end up with the database in restoring state, step back through your logs to see what went wrong. (If you want a second set of eyes from the community, try uploading your script as a Github Gist, but keep in mind that the bigger it is, the harder it might be for folks to spot bugs.) If you don't wanna do that, then try running a Profiler or Extended Events trace to track the restore events, but be warned - it's way harder than it looks. (Read the comments on that post for even more reader ideas that they've tried and failed.) 

Here's my Perfmon tutorial for SQL Server: $URL$ For more counters and thresholds, here's a poster we did when I was at Quest: $URL$ 

No. Resource Governor is for throttling CPU, query workspace memory, and storage throughput, but there is no feature in SQL Server that restricts which files/filegroups a database can access in TempDB. Plus, think about cross-database queries - if you had a cross-database query, how would that even work? 

In reality, I've been on several projects where managers wanted to accomplish something like this, but once we even white-boarded out the changes to just one table, they realized we couldn't keep them both in sync at a price that the customers were willing to pay. I have been on one project where the customers did pay, and here's how they did it: 

You've got two questions in here: 1. Does the nonclustered index have a sorted B-tree? Yes, because you have to get to the right page of the index to find the data you're looking for. 2. Why do you want a nonclustered index when you also have a clustered index? Think of the white pages of the phone book as your clustered index. If you wanted to find all of the people in your city with a first name of "Brent", then the clustered index isn't going to do you much good - the clustered index is based on last name, first name, middle initial. Wouldn't it be helpful to have a separate phone book sorted by first name? Just like in real life, the answer to that might be yes or no - but it depends on how often you search for people by first name, and how often people move in/out of your city, or how often people change names. 

In SQL Server 2005 and prior, you could specify the filegroup for full text. In SQL Server 2008 and newer, full text is completely different, and the ON FILEGROUP stuff doesn't matter. However, Microsoft doesn't just rip out syntax - they deprecate it, and a few versions later, it'll disappear. (Backup log with truncate_only is a good example of this.) This gives you time to clean up your scripts before the syntax disappears. 

First, check the Windows event log. Most backup software logs there. (It's likely taking a whole-OS backup, not just SQL Server.) Generally, when you see something taking a backup, it's got sysadmin-level permissions. (You can also use the db_backupoperator role, but that's fairly unusual.) So to catch the thing doing the backups: 

OPTIMIZE FOR UNKNOWN doesn't use a value - instead, it uses the density vector. If you run DBCC SHOWSTATISTICS, it's the value listed in the "All density" column of the second result set: 

But be aware that it can produce some stunningly bad clustered index recommendations. The DTA isn't a replacement for basic data modeling for how to store your data. Generally, your clustered indexes should follow the SUN-E method: 

This is a known issue: mapping volumes on Macs isn't supported yet. You can follow that Github issue for more news, like when it's fixed. Until then, no, people aren't doing anything more than simple testing with that combination of tools (Mac, Docker, Linux, SQL Server). If you need to do real, productive development on a platform that's supported today, use: 

To keep life easy, I'd keep the same usernames on both servers and then you should be fine with sp_change_users_login. 

If you have a table with 1.9 billion rows, I'm guessing it has a fairly low change rate as a percentage. Check how many rows you're actually inserting/updating on a daily basis - it's likely less than 1%. In that case, it doesn't make sense to reorganize the whole table every 2-3 days (especially given that it takes 10+ hours.) I'd start by only doing index maintenance weekly, if not MONTHLY on a table of that size. Start by taking a step back and asking, "What's the problem I'm trying to solve by doing index reorganizations?" If the answer is slow select statements, then index maintenance on a 1.9bn row table isn't going to be the answer. 

Takes a longer sample, and tells your waits over that time range. Wait stats can be kind of cryptic, so next to every wait type, I link to the SQLskills wait stats repository for that wait type. You can just copy/paste out the name of your top wait type, go to their site, and learn more about what causes that wait and how to fix it. If PLE is dropping due to queries reading a lot of data pages from disk, for example, you might see PAGEIOLATCH% wait types. If it's dropping due to queries getting huge memory grants, you might see RESOURCE_SEMAPHORE. If PLE isn't the problem, then you'll see different wait types altogether. 

If you have the full and transaction log backups, you can use a log reading tool like Quest Litespeed or ApexSQL Log Reader. (Disclaimer: I used to work for Quest.) Those read the full backup, then the logs, in order to build a chain of what happened. You can search for transactions by syntax, table name, etc, and generate undo scripts. Here's the catch, though: if it was a shared login, like a SQL login where multiple people knew the password, you're going to have a tough time pinning down who did it. 

Use the Extended Events technique described here: From the DMVs, can you tell if a connection used ApplicationIntent=ReadOnly? Not necessarily an exact duplicate of this question, but the same techniques will be used to produce the answer. (I wish it was easier.) 

Putting the comments into an answer so the question can be marked answered: @crummel4 says, "See the third section from the bottom of $URL$ titled How "initialize with backup" Works, and How to avoid Pitfalls" @Kin says, "For step by step instructions, please refer to $URL$ or $URL$ 

I know that you're going to say, "But I need to write it this way." Thing is, the reason you're not finding any results for your question out there is that people don't write it that way. As your data grows, your server grows, and your career grows, you're going to need to extract data from multiple sources and you won't be able to take locks across all of them at once to accomplish the load. That's why the ETL (extract/transform/load) design is so popular. 

You didn't include the exact query to reproduce the issue (it doesn't happen every time), so here's a few different ways it'll happen: Using WITH (NOLOCK) - this hint in your query can cause records to be read twice, to be skipped altogether, and for your query to fail outright. (The same thing can happen with the SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED hint.) An error in the SELECT query - for example, if you're joining multiple tables together to get the data for the INSERT, and you have an error in the join (or a legitimate join that produces multiple rows of results), you can get duplicates. Concurrency - say that while you're working, someone else is inserting and removing records from the source table. They might have temporarily had duplicates in the source table, and when you go to look later, they're gone. (I've hit issues where people would insert a new version of a row, then delete the old version of a row, so some selects out of that table would see duplicate rows. They couldn't implement unique constraints in the database because of that code pattern.) To get an exact answer for your exact situation, post your exact code, or code that reproduces the issue you're having, and we'd be glad to dig deeper. And welcome to Stack! 

First, there's no SQL Server 2013, but the good news is that the version number of SQL Server doesn't matter. It's more based on Windows failover cluster networking, which is explained here in a 3-part series by the Microsoft folks: $URL$ And then additional info in part 4: $URL$ Short story - you need some network redundancy, but you don't have to get it with multiple IP addresses. Instead, you get it with network teaming software.