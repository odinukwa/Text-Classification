BAD NEWS You cannot use mysqldump to do this. GOOD NEWS You can use the mysql client. First, find out if the Stored Procedure exists. If it does, then fetch the type of Procedure it is ( or ). Then, issue the for the Stored Procedure. You will have to strip the top 3 and bottom 3 lines: 

I guess some developer just gave up putting it in the code and slapped up a configure file as some demented shortcut. If you have such a file, please comment out the last line and restart mysqld. MySQL 5.7's default value for is as mentioned 

CAVEAT Your original proposal is actually just as good. Most of my answer looks like your anyway. The only difference is that I introduce an outage to guarantee no new inserts starting at . An outage comes a little earlier in your plan (). I added concurrent_insert to speed up inserts into the MyISAM table duing the mysqldump. You can either go with your plan or mine. Just add to the mix. Give it a Try !!! 

SUMMARY Look at the table definition again. There is 18 bytes for an index entry on just the index. That's 18G per billion rows. Same goes with . You just don't have enough room. It is imperative to try one of my newest suggestion to bypass the need to sort all these keys. UPDATE 2013-05-22 12:15 EDT YoU asked 

CAVEAT #1 Running is the same as running followed by . Therefore, there is no need to run a separate . CAVEAT #2 Please note I use instead of . This ensures that the is not recorded in the binary logs because it can replicate to Slaves. If you want the to be replicated all Slaves, is fine. CAVEAT #3 Note that I ordered all the tables by the biggest table first. If you want to optimize all tables starting with the smallest table, change this line 

Basically your database has stalled because it has ran out of space to store archive logs, and needs to switch one. To remedy, delete the generated logs (assuming you don't need them for backup and recovery purposes), then execute (as SYSDBA): 

You have 2 options to fix this. Option 1 - create the intermediate table without a primary key (best option!!): 

Oracle 11g doesn't support the clause, though the impending 12c release is rumored to support it. Anyway, you can do this using an analytic windowing function: 

They are separate things, but it's best to read this White Paper to get a better understanding of how each work both individually, and together: Oracle Active Data Guard Real-Time Data Protection and Availability. Downloads available here. Note that Golden Gate has replaced Oracle Streams as of Oracle 12. 

What you are experiencing is called caching. The database doesn't have to go to disk the 2nd time because it can either get the data from its own buffer cache, or the operating system/disk array can also provide the data faster from its own cache. In order to see whether Oracle fetched the data from disk, or used its cache you can enable autotrace in SQL Developer. You'll get something like the following: 

The value used in the DDL for the datatype (eg: tinyint(1)) is, as you suspected, the display width. However, it is optional and clients don't have to use it. The standard MySQL client doesn't use it, for example. 

Your question's not brilliantly clear, but if you want the 2nd, 3rd and 4th characters in the column , in the table , then use : 

The answer is very simple. The prefix in executes the following text as a command in the local shell. In your case it's executing the Unix command locally. Query to check the name of the machine you're connected to. There are ways to execute local Unix commands via the database, but it's seen as a huge security hole, so I won't document them here. ssh (assuming Unix) into the database host instead. 

I have table that has fields, and another table called , which has a one-to-one relationship to the table. stores information about the message, including its text, and subject, and the recipient, etc. 

And it has non-clustered indexes on and fields. I want to find what a has sent us in a specified period. Thus I run this query: 

This works fine when runs sequentially. But when I run multiple instances of my application (concurrency), I see that some messages are processed twice or thrice. Here's what happens: 

Of course the overall design and query is much more complex and more details are in action in selecting next new word for a given learner. Now, imagine that words list contains 100K words, and a learner has already learnt more than 5K words. Using the given query, this gets slower and slower and slower by more learners learning more words. Is there a better design for these types of business requirements? How to design for scale in this case? 

Now that I've made sure that SAN is up and running, and permission are OK, how can I tell SQL Server to continue recovering? Since this database is very large, I don't want to interrupt the course of recovering and start from the beginning. And also, any backup would take hours to complete. 

How string comparison works in SQL Server Why comparison doesn't behave the same on one machine, and one platform, but different environments These 4 characters represent one human-understandable character. Why they are so abundant in Unicode character map? 

And one hundred times it worked. Now I'm stuck. I've done all the steps, and client can't connect to the engine. It simply times out. Here's what I've done: 

Instance A gets some unprocessed messages While instance A is setting the to true in RAM, instance B gets some messages, and chances are that it fetches one or more of the messages which are already fetched by instance A 

They're not all using the same port. The main mysqld process will have ed its children off (you can see the child<>parent relationship in the 2nd and 3rd columns of the output - pid 26308 looks to be the master "thread". Only one of the processes will actually be listening on the TCP port. You can verify this using . I'll add that a ed process will inherit the commandline from its parent, unless has been explicitly altered before the call, hence the confusion with - it means nothing. 

To do this, you'll be wanting to use the 11.1 Data Pump (/) rather than /. With Data Pump you export using the higher version export utility with the parameter. For example: 

Oracle won't do this natively. You'll have to use an external utility to do this. On Unix systems this is usually achieved with logrotate. 

Basically, there's a one-to-many relationship between and . You don't say what RDBMS you'd want to use, so the data types will need changing accordingly (bigger s/ columns etc will be needed for long articles/comments etc). Constraint DDL is also largely RDBMS-dependent, so you'll have to look the syntax up for yourself once you've made a decision. I can recommend Postgres. 

Basically, Oracle is assuming that all of the data in the source table may take up 3 bytes per character, due to characterset conversion. 

This page explains how to deal with string lengths when Unicode characters are involved. is the function you need. 

The clause in the DDL statement causes the actual population to be deferred until the first refresh. While Oracle parses the actual SQL used to populate the view, it does not execute it & will therefore not pick up "runtime" problems. This is easily demonstrated. Parsing error, due to not being a datatype: 

The command erases all relay logs and starts downloading from scratch. Once you , if you get the corrupt error log message again, then network transmission was not the issue. The binary log on M1 may just be corrupt after all. You must go to M1, make copy of the the suspected log, run against that copied binary log and redirect to a text file. Read the text file. If the text files contains gibberish or indiscernable characters, then you must perform a full sync of the slave. 

If you ever decide to query for deleted records, you will get an full index scan because of the order of trhe columns in this index. 

or you could simply make a backup of the binlogs folder and extract the incremental changes at yourt own convenience. 

I loaded you sample data into a local database on my laptop. Then, I ran the query PROPOSED QUERY EXECUTED 

You may want to create a separate column with the underscores replaced with blanks. You will have to perform the following conversion 

OMG 220G out of 240G ??? WAY TOO BIG !!! Take a look at this Pictorial Representation of InnoDB from Percona CTO Vadim Tkachenko 

That's a pass through (1 million) rows to inject a column. Then, you have to perform a join of the ugliest kind 

Running clears all relay logs and starts with a new one. You will be replicating from the Last Master BinLog Event (BinLog,Position) that executed on the Slave. Give it a Try !!! 

See the InnoDB Buffer Pool on the Left ? It has an Insert Section responsible for index changes. InnoDB would automatically flush index changes for you. No such mechanism exists for MyISAM. So, please convert to InnoDB ASAP. UPDATE 2017-08-07 15:25 EDT If you must go on using MyISAM, you should look into using Dedicated MyISAM Key Caches. You can create a key buffer for specific set of MyISAM tables. In your case, you would create a dedicated key cache for the one table with the FULLTEXT indexes. You would have to judge how big or how small to make the cache. Suppose your table with the FULLTEXT indexes is . You could then run this to find the proper size rounded up to the nearest MB 

Use with the or parameters, along with to export the schema at a consistent point in time. This is the same as using in the legacy utility. Examples here. If the data for the schema in question is in a tablespace of its own with no other objects from other schemas, you can use to do a tablespace point in time recovery. 

No, you have to upgrade in stages, which wouldn't actually be possible now because Oracle 8.x and 9.x are now unsupported and the software is unavailable (unless you have an Oracle support contract and ask them for copies of the software). Read the OracleÂ® Database Upgrade Guide 11g Release 2 (11.2) for more information. Your best bet, assuming you have the 7.x database up and running, is to use to export the schemas you require, then import them into a fresh 11.2.x database. 

First, I'll cover the SQL row-limiting side of things. In Oracle 11.2.x and lower, you have to use and a subquery, as it doesn't support the or clauses: 

The ISO SQL 2008 Standard document ISO/IEC 9075-1:2008 Information technology -- Database languages -- SQL -- Part 1: Framework (SQL/Framework) is now freely available from the ISO website. 

Name the columns so they're understood in the context that they appear in, which is the table they reside in. If you do that for each table, then the relationships will be obvious, as each column will reference a foreign column with a similar name. If the columns referencing each other don't have a similar name, then ensure that you name the FK appropriately to make it immediately obvious what the relationship is between the columns. It all ends up looking more readable when writing SQL (eg: , rather than ). 

I think you'd be better off using the text version of the explain plan output rather than XML, as it gives you a pre-formatted readable ascii representation of the query plan. eg: 

With the MERGE storage engine, there is no long migration path. The mapping takes place in 2 seconds. The maintenance of each individual table could affect any query against the MERGE engine if there is no primary key to unique identify one MyISAM table from another MyISAM table. With Table Partition, the individual tables has a partition map built in. Mapping may include a migration path. Maintenance is just a mixed bag as it would be with any other table. In either case, a well-designed indexing scheme needs to be in place. Why? The query's WHERE, ORDER BY and GROUP BY clauses should dictate what indexes are really needed to support the query. 

The problem here is just a misunderstanding Using --flush-logs with mysqldump simply closes and opens all files handles against the following: 

Note: In option #2, you use --dump-slave. This tells a slave to publish its master's binary log file/pos to the output of mysqldump instead of its own. STEP 03 : Get the Replication Log File and Log Position 

The whole folder () needs to be restored from the same moment in time it was being backed up. If there are no physical copies of from the same moment in time, then log sequence numbers for future transactions can never be referenced correctly. If you do not trust your host in this matter, perhaps you can get MySQL started with innodb_force_recovery set to an appropriate value. Here are the values from the MySQL Documentation 

This tell me that you have read up to , but you are currently processing things from the . This also tell me you have about 80 binary logs on the Master, each about 100 MB. The is simply the NOW() minus the TIMESTAMP set at (Relay_Master_Log_File) position (Exec_Master_Log_Pos). As long as Slave_SQL_Thread is Yes, the relay logs get processed 

But when I look at the results, instead of seeing a column that starts from 0 and goes up to 100, I see a column that starts from 0 and goes up to 37.xxxx. Though BOL does not explicitly mention that the result is distributed over 0-100 scale, my understanding from the word made me use this ranking function. What do I miss here? 

But frankly, that doesn't seem the proper way for natural locking detection, because I've manipulated the default behavior using transaction statements. Do we have a tool, like to show us some information about locking of a given query? If not, how can we find out what type of lock a given query can apply on a database, because they get executed TOO fast to be detected. 

Of course it can be solved in a minute of Googling around, via some tricks and scripts. However, the more we search the less we find out about the reason behind this. Why it has happened in the first place at all? What possible reasons caused it, and might cause it again? Was it because of an attack? We have no clue, and any help is appreciated. Update: in this post it's been argued that a policy check can make this happen. We haven't set a policy so far, and we do not even know where are the policies. Update2: We realized that some of the threads in some applications kept working, even though other threads and other applications were encountering message. Could it be that doesn't require re-authentication? How is that possible? 

And this query works lightening-speed fast for past 2 years. But when I change the part to a closer date, it freezes out and takes more than 2 minutes to complete. In other words, based on different inputs, it behaves differently, sometimes even hanging out and not returning for more than 10 minutes. I expected a consistent behavior. What do I miss about indexing? What can cause this inconsistent performance? Update: I changed names of columns and table, so I can't attach execution plan as a picture. But here's the issue. Thanks for guiding me. when I change value of date parameter, SQL changes index seek from to . I never thought that SQL creates execution plan based on the value of parameters. How that could be?