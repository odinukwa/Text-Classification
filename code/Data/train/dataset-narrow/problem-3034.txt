Is my MULTI SVHN model too small for this problem/dataset? If not, how do I improve its performance? Is there a way to generate better candidate sliding windows, as my single digit SVHN seems to be the best model I have among the three. Is my reasoning correct that the MNIST model didn't work well on my images as the digits are somewhat different (handwritten vs computer generated) ? Are there other preprocessing techniques that I could use to improve my performance espeically for images like the 2nd one (18458882)? Is there any other approach that I could try to solve this? 

The risk of overfitting increases with the number of parameters, but there are techniques to limit overall model complexity so that the number of parameters can still be higher than the number of samples. In general they are called regularization. Implementation depends on the type of model - for neural nets, commonly used methods are weight decay, L1/L2 regularization and dropout. In fact most non-linear models are slightly overfitted in practice. As long as you cross-validate properly and choose the model only based on out-of-sample performance, this is not a major concern. If your model overfits badly (train error is significantly lower than test error), increase regularization to get a potential performance gain. 

You can test with higher values of Dropout (0.5,0.7,0.9) and/or try L1/L2 Regularization to combat overfitting : Keras Regularizers. Update: You can play with a combination of l1/l2 regularization and dropout for your convolutional and FC layers. Start with low values of lambda (0.001) and increase it thereon. A common practice is to have dropout only in the FC layers, see if it helps your problem. Also, from the looks of your loss curve, your model probably hasn't converged.You can train it until your validation loss starts going up/becomes stable. 

It will be great if someone who has worked on VAE and autoencoders for text data can provide their inputs regarding the questions mentioned above. Note: the attached code is a simplified version of the code in THIS LINK 

I have several thousand text documents and I am currently working on obtaining the latent feature representations of words and generate sentences using variational auto encoder. The main obstacle I am facing is “how to input such large textual vectors into VAE (or say even in a simple auto encoder)". I know there are other techniques such as adding layers in the form of LSTMs, but I want to keep it simple and keep the model as just VAE. One way is to use one-hot-encoded vectors or bag of words, but again, this is not the most efficient way since for a vocabulary of 100K unique words, each document will have a 100K input vector. Additionally, we loose the sentence structure. For small datasets however, there is no problem in training an autoencoder using this type of input. Another method is to use word embeddings using a pre-trained Word2Vec. This is what I have been trying to do and the python notebook which can be DOWNLOADED HERE uses this technique. The code is too long and has multiple pre-processing steps, so I am unable to embed this code in my post. The following are my questions: 

The images are not of a fixed resolution but are mostly in the range (80*20 to 130 *40). Due to lack of enough labelled data(~3K rows), I had to go for an open source digit dataset. I tried both MNIST and SVHN with not much luck. I've detailed both my approaches below.. Note : The only preprocessing that I've used is converting to grayscale ,resizing the images to a fixed size and doing a mean subtraction on the image. 1. SINGLE DIGIT MNIST and SLIDING WINDOW FOR TESTING : I trained a Convnet on the MNIST set with ~99% on the validation set.Then I used a sliding window to move over my test image and predict for each. This approach had two problems : First, as my images are not of a fixed resolution often I couldn't get the right sliding window (one which contains just one digit completely). Second, I cropped out individual digits from my test images, and tested my MNIST model on these, the results were 'wildly off', out of the ~15 samples that I had cropped out, my MNIST model couldn't get even one right. Here's a cropped out image for reference. 

Your problem is called multi-label classification. If you search for this term you will find a lot of literature. In short, these are the most common approaches: 

If you are using Python, you can use pandas. It comes with an expanding standard deviation function. 

Unless the label distribution is balanced, stratified sampling of folds will give you a better estimate of performance than random sampling. Also, try to avoid that correlated samples end up in different folds. Otherwise your models are likely overfitted and the error is underestimated. For example, if your data contains temporal correlation, always split by time. 

Currently, I am writing a Variational Auto Encoder model for capturing the latent representations of text in documents. I played with VAE for MNIST dataset and it works well and I can see the error decreasing, but when I recreate this model for text data, I get weird loss values. Also, since I am inputting text I had to use the embedding layer in order to avoid the bag-of-words type input, which will be extremely large. I would highly appreciate if someone can me with this. Here is the structure of model: 

Now you can train a binary classifier on your labeled pairs dataset. For example, start with Logistic Regression or Random Forests. Maybe you can already reach the performance you are looking for. For further reference, search relevance classification is a very similar problem. You can check out the solutions from the Kaggle competitions Homedepot and Crowdflower to get more inspiration for feature engineering. 

You can use any base learner for boosting (Adaboost requires sample weighting though). Keep in mind however that the original idea is to use weak learners with strong bias and reducing that bias through boosting. If it is a classification problem, usually logarithmic loss is used to calculating the residual/gradient for boosting. For Python, there is a nice AdaBoost wrapper in scikit-learn (AdaBoostClassifier) which can take for example a Random Forest as base learner. 

Now, each sentence (or document) will have different number of words. So the number of word embeddings for each document will have different lengths. Unfortunately, keras requires all the inputs to be of same length ( if I am right). So, how to handle such cases of varying input lengths?. Currently, in the fifth block of the python notebook, you can see the statement . That is, I only consider documents that have exactly 10 words to overcome this problem. Ofcourse, this is not practical. Can we pad 0 vectors? The VAE example shown in Keras blog uses MNIST data as exmple. Therefore, they use a sigmoid activation in the final reconstruction layer; consequently, “binary_crossentropy” as the loss function (along with the KL divergence). Since my inputs are word embeddings, where there are even negative values in the embeded vectors, I believe I should not use the activation as “sigmoid” in the final reconstruction layer. Is that right?. Additionally, I have also changed the loss as “mean_squared_error” instead of “binary_crossentropy” in the attached code. 

Depending on the nature of the data, one or the other approach will work best. Start with a binary classifier per label to get a baseline. If the labels are correlated (some labels frequently occur together), there is potential for improvement using one of the other methods. Using one of the existing implementations of multi-label classifiers is probably the easiest next step. The power set approach only makes sense if the number of possible combinations is reasonable. I am not sure if classifier chains are worth the effort, if you are not doing this for a Kaggle competition :) As always, setup a proper cross-validation strategy, define the evaluation metric (e.g. micro/macro f-measure or cross-entropy loss) so that you are able to identify the best model.