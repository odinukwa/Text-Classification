Consider the class of functions computable in time $(b+o(1))^n = 2^{\log_2{(b)} \times n + o(n)}$ on a $2$-tape Turing machine. By the Hennie-Stearns theorem, the same functions are computable in time $(b+o(1))^n$ on a $k$-tape Turing machine for any $k \geq 2$. And we can extend this equivalence to any model of computation that has an essentially linear time translation to and from $2$-tape machines. But this isn't a very natural class, even given that we believe in the extended Church-Turing thesis. If we account for the space usage as well, more models are time-translatable with a blowup of the form $O(t \times s^k)$. For example, when emulating a $2$-tape machine on a $1$-tape machine, it only has to scan the space used each time step, so the time blowup is $O(t \times s)$. Similarly, it seems like for RAM models there is a small $k$ that works: use one tape as an associative memory, a list of key-value pairs, which the emulator searches to find a key written on a different tape. Now it seems sensible to say something like this: $F$ is computable in $(b+o(1))^n$ time and $2^{o(n)} = (1+o(1))^n$ space. I don't have to say what model I'm using because all the obvious ones work. There is sort of a trade-off here, in exchange for a subexponential space restriction, we can specify a particular base $b$ for the time complexity of our function that's invariant in a larger context. That's about as far as I understand this issue right now. And I could be completely mistaken about something so partly I'm looking for some reassurance that the above is roughly correct. But I wonder if that's all there is to it? Is there some other set of assumptions under which the exponential time base of a function can be uniquely specified? EXAMPLE: Rubinstein's theorem says that we can factor with base $2^\frac{1}{3}$. Since the algorithm uses subexponential space, this result is robust across models. EXAMPLE: The square-root barrier for finding primes. If it were broken by exhibiting an algorithm that required exponential space on a Turing machine, would that really be a natural result, or possibly just an artifact of the machine model? EXAMPLE: The Strong Exponential-Time Hypothesis. Might it be the case that a RAM program can solve general $\text{CNF-SAT}$ in exponential time with a base less than $2$, but also requiring exponential space, so that when translated to a TM the algorithm runs in exponential time with a base at least $2$ and (a slightly-weakened version of) SETH is true in the TM model only? Is there a variant of SETH where only subexponential space algorithms are considered in order to make it applicable to more models? Or is there some reason I'm not understanding that no generality is lost without this assumption? 

The problem of comparing the lengths of two paths of line segments connecting points in $\mathbb{Q}^2$ is not known to be in $\text{P}$, nor even in $\text{NP}$. Does requiring that the paths begin and end on the same point change anything about our state of knowledge? What if additionally the paths are not permitted to self-intersect, that is, they are polygons? Finally, what if the polygons are required to be convex? My understanding of the obstruction is that the length of a path with $n$ segments is the root of a $2^n$-degree polynomial and we can't rule out that those might end up being so close together that we have to compute exponentially many digits. Superficially, we're in the same situation with bumpy wheels as with wild scribbles. 

First of all, contrary to some sources, I claim that the $\text{ECTT}$ can absolutely be understood as a mathematical axiom, or at least as a mathematical proposition if we doubt its truth. Introduce into our working language a new predicate symbol defined on models of computation with the intended meaning that a model is reasonable. This is essentially the same situation Peano and others faced: we already have an intended meaning for the symbols $\{0,1,+,\times\}$, even prior to writing the axioms involving them. At least until we axiomatize it, our theory remains sound under the interpretation of the new symbol, whatever it means, because the only facts about it that we can prove are tautologies. What is reasonable is reasonable, for example. Now add an axiom, the $\text{ECTT}$, which says that this predicate of reasonableness is satisfied by exactly those models that have a polynomial time-translation to a Turing machine. As an axiom it's not falsifiable in the sense of our theory being able to contradict it, as long as the theory was consistent to begin with, but the soundness of our theory is falsifiable: conceivably there is a reasonable model of computation that's not related to Turing machines by a polynomial time-translation. Allowing that this hypothetical discovery might involve a shift in thinking about what is reasonable, that is how I see the formal side. It seems trivial in retrospect but I think it's an important point to delineate the mathematics from everything else. Overall, I view the $\text{ECTT}$ as a solid principle and axiom. But we have working computers that are well-described by $\text{BPP}$, and there are problems like prime-finding and polynomial identity-testing that are not known to be in $\text{P}$, so why doesn't this violate the $\text{ECTT}$? It doesn't until we can actually prove $\text{P} \neq \text{BPP}$: in the meantime, instead of shifting our focus to $\text{BPP}$, we're no worse off keeping the $\text{ECTT}$ as-is and saying what-if polynomial identity-testing is actually in $\text{P}$. This approach also lets us isolate particular problems we are interested in such as factoring. It's a subtly different assumption than equipping our model with an oracle, since we don't actually change the model, but the effect is the same. From this utilitarian point of view, the $\text{ECTT}$ is sufficient until we can prove any separations. The situation is the same for quantum computing, except we have to build a working quantum computer and prove $\text{P} \neq \text{BQP}$ to really take the wind out of the $\text{ECTT}$. If we just build one without the proof, maybe the universe is a simulation running on a classical computer and the $\text{ECTT}$ still holds, or if we prove it without building one, maybe it's not really a reasonable model. To make the argument really tight, we need problems that are complete for $\text{BPP}$ and $\text{BQP}$ with respect to $\text{P}$, but we can make do with choosing whatever problems we know how to solve. For example, suppose I claim to have built a machine that factors numbers and that its runtime satisfies a particular polynomial bound. The machine is in a box, you feed in the number written on a paper tape, and it prints out the factors. There is no doubt that it works, since I've used it to win the RSA challenges, confiscate cryptocurrency, factor large numbers of your choice, etc. What's in the box? Is it some amazing new type of computer, or is it an ordinary computer running some amazing new type of software? By assuming the $\text{ECTT}$, we're saying it must be software, or at least that the same task could be accomplished by software. And until we can open the box by proving complexity class separations, no generality is lost under this assumption. That's because even if the operation of the machine is explained well by some reasonable non-classical or non-deterministic model and not explained by the classical deterministic one we would still need to prove those models are actually different in order to break our interpretation of the $\text{ECTT}$ and make our theory unsound. To challenge the $\text{ECTT}$ from an entirely extra-mathematical direction, it seems that we'll need a machine or at least a plausible physical principle for solving an $\text{EXPTIME}$-complete problem in polynomial time. Even a time machine implementing $\text{P}_\text{CTC} = \text{PSPACE}$ is not powerful enough to defeat the $\text{ECTT}$ without a proof of $\text{P} \neq \text{PSPACE}$, although it might help us produce one. To illustrate, Doctor Who has strung his telephone wires through a wormhole and built a contraption that he uses to discover a gigabyte-long formal proof of $\text{P} \neq \text{NP}$. He wins the Millenium Prize, and he has also invalidated the $\text{ECTT}$, because the result implies $\text{P} \neq \text{P}_\text{CTC}$. If his contraption finds a proof of $\text{P} = \text{NP}$ instead, or a proof of the Riemann hypothesis, he still wins the Prize, but that's it — no $\text{ECTT}$ violation. However, the Doctor's contraption seems like a better tool for attacking the $\text{ECTT}$ than my amazing factoring box, since I don't know how being able to magically factor numbers in polynomial time can help me prove that it isn't possible to do the same thing without magic. To be on equal footing it would have to be the case that factoring is $\text{NP}$-complete and also that I (somehow) know a reduction to it from $\text{3SAT}$ — then I could encode the search for a proof that factoring is not in $\text{P}$ as a series of factoring problems and have a chance at finding it before the wormhole reopens. In the other corner towers Deep Blue, a giant robot designed by a corporation to solve $\text{EXPTIME}$-complete problems. Its challenge is to play perfect chess quickly on all board sizes and convince us all that it can really do that with an unlimited marketing budget. But it doesn't have to justify the uniqueness of its methods to make us rewrite the $\text{ECTT}$, since we already know that $\text{EXPTIME} \neq \text{P}$. This is more trivial than it may appear: if the robot is reasonably constructed, and what the robot does is amazing, then the reasonable model describing it is capable of amazing things and we can repurpose the $\text{ECTT}$ to polish its gears. In my view, Scott Aaronson's answer is mathematically incoherent, because it's not compatible with any formalization of the $\text{ECTT}$ that I can identify. We are supposed to weigh evidence for and against $\text{P} = \text{BPP}$, but I think we should demand proof not just evidence before we drop the whole idea of the $\text{ECTT}$ or modify it for no practical benefit (nevermind the nasty business of extending the concept of time-translations to non-deterministic models). And as I've argued above the discussion of whether or not quantum computing is real is a red herring without a proof of $\text{P} \neq \text{BQP}$. Here is a summary of the situation. For any given model of computation, it is inconsistent to simultaneously believe these three statements: the $\text{ECTT}$; that the model is reasonable or physically possible; and that the model is more powerful than a Turing machine. Only the last statement is in the language of our original theory, $\{\in\}$. If it's not already settled, then we're taking a gamble with consistency by assuming it as an axiom, or by assuming the first two statements together which imply its negation. So our only choice to incorporate any of these ideas which is sure to preserve consistency is between a definition of what reasonable means, and a statement that this particular model is reasonable (which by itself, without the definition, doesn't give us much to work with). Of course, we can have both and still be consistent if we change the $\text{ECTT}$ to something else, but this will have been wasted effort if the class separation is settled opposite the way we expected. Regardless, by axiomatizing our reasonability predicate symbol under such a nebulous interpretation, we're taking a gamble with soundness. Before, with our language equal to $\{\in\}$, we only had arithmetical soundness to worry about, and now we are expected to agree about what is reasonable as well. Having browsed the linked paper by Dershowitz and Falkovich, I believe that its authors also hold an incoherent or maybe just tautological view of the $\text{ECTT}$.