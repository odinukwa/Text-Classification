Obviously, a lot of this devolves to simple personal choice. Here are my own, personal, rationalizations. I've been using Powershell with SQL SQL since PSH v 1.0, and before SQL Server started officially integrating it. (When I started with PSH, I was administering SQL Server 2000 and 2005 servers.) So, I learned with SMO (or it's slightly older incarnation, the name of which escapes me at the moment) and .Net and I'm used to them. I'd generally lean towards SMO, since it makes some things a lot easier, like scripting out objects. My own code uses SMO some times and .Net some times. I think it's handier to use .Net to get simple result sets, for instance. I think that Invoke-SQLCMD makes more sense if you have lots of existing TSQL scripts. If you are creating strings and executing them through -Query, that's going to be messy. If you have a good grasp of how Powershell works with .Net and SMO, using Invoke-SQLCMD occasionally, when you have a script file to run, is easy. I've always found the PSDrive thing clunky and felt that they implemented it because they got caught up in the "everything can look like a file system" idea. I know that the *nix guys love \proc and such, but I feel that this implmentation feels sort of forced. I think that PSDrive is OK, maybe even good if you hate the UI, for exploring things but I've never written a script that uses it. I have never seen anyone use the WMI provider. So, that would be my last choice. So, I'd lead with SMO and fall back to .Net when it's handier to. 

Breaking the schemas into two separate databases will make backup and restore more difficult, particularly if you are trying to do a point-in-time restore. You can just put the tables into appropriate file groups and put them on different disks. It might be easier to move the frontend tables onto a different disk than the back end because they are smaller and more easily moved. OTOH, you might get better performance by adding additional drives to your RAID set and making sure that the data is effectively spread around the RAID. Assuming that your RAID hardware supports the addition of new drives and volume expansion. If the batch processing relies on data from the front end you may still run into blocking problems. It's also possible that the backend tables aren't indexed very well, or that the statistics for existing indexes have gone stale. As a few people have pointed out, it would be best to understand the problem before trying to fix it. There is a big difference between I/O timeouts caused by slow storage and query timeouts caused by a 60 second query timeout value. 

UPDATE STATISTICS does not have any sort of internal parallelism. It does not matter if you are running with either FULLSCAN or SAMPLING. Of course, you can run several UPDATE STATISTICS commands at once, each on a different connection, through multiple SQL Agent jobs or some other contrivance. Depending on your exact situation with hardware and data, you may find that simply reindexing the tables is faster than UPDATE STATISTICS with FULLSCAN and possibly a better option. 

A common problem when developers involves the schema that SQL Server assumes when creating or altering an object. This is especially likely in the following circumstances: 

Log shipping might be "crude" (I prefer to call it straightforward), but replication is complex and, in my experience, brittle. You don't want to add complexity if you can avoid it. You don't want to have to go through some sort of heroics to fix a broken replication setup. Managing the replication configuration of all of those database would be a bear of a problem. You can't use the forms of replication that would alter database tables, as I suspect testing those changes on 500 databases is a very large project, if not a career, in and of itself. You can start to have performance problems on the subscriber side that require you to start looking at fillfactor, maintenance plans and other, similar complications. What happens if you need to flip over from your primary servers to your "local" secondary servers? Are you going to re-do all of the replication configuration? That's likely to be a big task. If you automate it, that is time spent creating and testing the automation, and then it needs to be looked at when you change those 500 databases. Recall that all we had for DR was log shipping up until SQL Server 2005. It's not glamorous, but it works. TL;DR- Simpler is better. Use log shipping. 

How long does it take your code to perform a transaction? Assume that the resolution of GETDATE is one millisecond. If a transaction takes less than a millisecond, then you should expect to have more than one transaction marked with the same time. If you cause take more than a millisecond most of time, then you are playing the odds that you won't get two sub- millisecond transactions back to back. The longer your code runs (months), the more likely that you will get two sub- millisecond transactions in a row. BUT, it is worse than that. The resolution of GETDATE is not one millisecond. It is three milliseconds, IIRC. This makes it more likely that you will get transactions with the same time. Relying on Date time values for keys is unreliable. It will often break when moving to faster hardware or after improving algorithm ice efficiency in some way, like adding an index. If you are using that value as part of a (natural) key, you should look into using a surrogate key. 

The syscomments compatibility view contains a copy of the code for all stored procedures, triggers, functions and similar objects. You can easily search it using "LIKE" criteria to find a list of object IDs that need to be updated. 

Each job step has a setting that allows it's output to be sent to a text file. This can be cumbersome because it's a text file located in the file system and not in a table and might be hard to get to. There is also an option to send the output of a job step to a table. You can query the table, but it's still clunky. (It is hard to write a query to pick the "X rows affected" string out of a blob of text, then pull the number out so that you can aggregate it, etc.) I would recommend that you build a simple table in your user database (not in msdb or master) to keep that information. IMO, this is the best way to keep close track of the number of rows affected by each command, particularly if you want to be able to write simple queries to do historical reporting. This way, you can decide to log at the job step level, or if the step calls a stored procedure with multiple commands, you can log a audit record after each command. As always, @@ROWCOUNT is your friend. 

As an aside, note that your table is literally named "IR.TimesheetHoursProjectTask" and not "TimesheetHoursProjectTask" under the "IR" schema. To specify the later, using the [] notation, it would be [IR].[TimesheetHoursProjectTask]. 

SQL Server's replication feature is intended to replicate data, not objects like tables. When configuring replication you may find that the slave table may not be a perfect copy of the master table; constraints and indexes may not be recreated on the slave side. If you need the same table at the slave side, I believe that you should just run the script to create a table and forget about replication. 

While there is no limit on the number of simultaneous connections by a single login, other than the usual @@MAX_CONNECTIONS value, using one login for multiple users or developers is generally frowned on because doing so makes it difficult or impossible to limit permissions on a per-person basis. (You may still be able to identify/contain people based on something like a workstation name, but this would require much hackery on the DBA's part, while simply using seperete logins addresses the issue.) Such "well-known" logins also have a way of getting hard-coded into things, along with the well-known password, and then developers and users become very resistant to ever changing that password. With so many ways to get onto a corporate network these days (a rogue LogMeIn running somewhere, for example) being able to turn database access off via an AD group is beneficial. Using a single login for a web application, which can have many simultaneous users, is more common. It might not be feasible to give out SQL Server logins or AD logins to individuals for large sites (for example, facebook). In that case, there is a benefit in that IIS can use connection pooling. I haven't worked in an environment where it was standard operating procedure to use a shared login like that for many years, although there were some smaller/older projects where a shared login was used (and sometimes abused). 

Assuming that your indexes are up to date, dropping from 100% cpu to something less, while having timeout problems, implies that the app is now either waiting for locks or for disk. The IO latch waits implicate the disk. I would use perfmon to look at the disks to see if there seems to be an unusually high amount of reads, look at the query plans for exact queries that are running slow and re-think what I did about the indexing. Another thought is, if I've made a change and things seem worse afterwards, the first thing I would do is undo that change. IOW, put the old indexes back. 

I would not trust rsync to copy the ldf and mdf files for user or system databases, nor would I trust anything I hacked together (VSS or otherwise) in a production environment. SQL Server is very fussy about when (and if) things get written to the ldf and the mdf files. Software (rsync) that isn't designed with that in mind might not get it right, especially if it doesn't understand that the ldf files and mdf files need to be treated as an interrelated system of files. If that software doesn't get things right, nothing might be noticed until you failover, try to go live and have your databases flagged as suspect due to what SQL Server sees as data corruption. Even worse, "getting it right" might be dependant on how much load is on the system and you might not find the problem in a lightly-loaded test environment. I have seen enough examples of people who thought that they were replicating their files but were not. They were left with corrupt files on the recovery site and with inaccessible backup files at the primary site. So, that meant no database for them. In short, you are making an appointment for trouble. If you had some sort of block-level replication technology like EMC's SRDF or were looking at shared-nothing clusters, that might be different. Those technologies interface with SQL Server and the clustering services that Windows provides in a way that your writes will be safe and your files will/should be consistent. If my only disaster recovery option was a remote site that was normally down, I'd use log shipping and make sure that I had all of the pieces to restore the database(s) on the remote site. If you can't make the built-in log shipping do that, writing your own isn't that hard. I've probably written three or four (simplistic) log shipping systems from scratch in the last 14 years. At a minimum, the key things you need are: A full backup needs to be taken and copied over to the remote site. Your tlog backups need to be taken and copied over to the remote site. You need an automated method to restore that full backup and any relevant tlog backups. Ideally, this should be simple enough for someone else to do and/or simple enough for you to figure out at 3AM when your primary server fails and you are half asleep. When you have an event, bringing up the other server will take longer because you would have so much manual stuff to do. That means that this isn't as good as simply implementing regular log shipping. You will need to test this periodically, as well. (Of course, you need to worry about other things too, like jobs, packages, login and user synchronization, changing DSNs on the web servers during a failure, etc. If you have a large environment and a serious disaster, like the loss of your primary data center, you will be trying to do this when IIS guys, file server guys, network guys and whatever else guys are trying to bring their stuff up too.) If it were me, I would be agitating for a warm standby server at the remote site. That would make (standard, out-of-the box) log shipping easier and database mirroring possible. It sounds like you have tried that. 

a) Pretty much anything takes a schema stability lock. You don't want something else changing the structure of the table while you are updating your statistics. According to this, update statistics takes schema stability and modification locks. b) If something tries to change the table's structure, it will be blocked. IIRC, update stats does dirty reads, so it shouldn't block connections that are merely reading or writing. c) If you use FULLSCAN, it will read the entire table because that is what you told it to do. I don't see how that can be seen as anything but 'causing heavy i/o'. Normally the default of 'sampling' works well enough, but I have seen it cause problems with data with non-homogenous distributions. Often, it's also easier to just reindex the whole table (especially if you can do it online) because reindexing is parallelizable where as update statistics isn't. (AFAIK, MS did not fix that in sql 2008.) 

Within reason (I mean "less than thousands", and you are no where near that) the number of databases should not affect the performance of the server. I have seen people claim to run thousands of databases on a single instance of SQL Server, and the performance of the user databases hasn't been a problem. (On the other hand, looking at those databases with SSMS is a problem, as is trying to manually manage all of them.) Very roughly speaking the performance of the server is governed by the amount of data that is accessed in those databases and how much memory the server has. Assuming that you have enough disk space to store both databases and that you never access the "test" database, the production database performance should be unaffected. If you "sometimes" access the test database, the performance of the server could be affected at those times. You can mitigate that by testing when the server isn't busy, like in the evenings or on weekends. All of the data in a database might not be regularly accessed. It is very common to have old records, "archives" and other sorts of stuff which remains in the production database by isn't regularly returned by queries on a day-to-day basis. In many databases, this "cold" data might be much larger than the amount of "hot" data that is regularly accessed. If the amount of hot data is very small, you might not notice any performance problems when using the "testing" database even when the production database is being used full-blast. To make things a bit more complex, poorly designed queries and database tables might be inefficient, and return data that really should be "cold", or cause table scans. In short, if the server can hold all of the hot data (from production and testing) in RAM, then you will probably be ok, or at least not much worse off than you are with only the production database in use. Back to your direct question: Will adding more load be a problem? The first thing to do would be to look at the performance of the server now. Do you have performance problems? Are those problems caused by a lack of RAM, a lack of disk I/O capacity or a lack of cpu power? There are many, many guides on the internet and StackExchange to help diagnose where your currently bottlenecks are. 

The maximum batch size for SQL Server 2005 is 65,536 * Network Packet Size (NPS), where NPS is usually 4KB. That works out to 256 MB. That would mean that your insert statements would average 5.8 KB each. That doesn't seem right, but maybe there are extraneous spaces or something unusual in there. My first suggestion would be to put a "GO" statement after every INSERT statement. This will break your single batch of 45,000 INSERT statements into 45,000 separate batches. This should be easier to digest. Be careful, if one of those inserts fails you may have a hard time finding the culprit. You might want to protect yourself with a transaction. You can add those statements quickly if your editor has a good search-and-replace (that will let you search on and replace return characters like \r\n) or a macro facility. The second suggestion is to use a Wizard to import the data straight from Excel. The wizard builds a little SSIS package for you, behind the scenes, and then runs that. It won't have this problem.