I gave up resolving this issue for a while now. Could someone please assist me on this? I need to create this Linked server!! What am I doing wrong all these days? 

However, if the Domain1/User1 and Domain2\User1 are added as individual accounts then we could Login without issues. 

Since the command running indexoptimize was a deadlock victim, I am wondering the job succeeded instead of an error!! 

Server Config: RAM: 24GB - 21.5GB for SQL Server Processors: 8 Not much activity in this server - hardly 50 people access this SQL Server via Sharepoint. 

There is AD group on created on . The domains each other (per IT team). Now, we have a which is also trusted for both Domain1 and 2. We added AD group to . When either of Domain1\User1 or Domain2\User1 try to Login to SQL Server on Domain3 - we get Login Failed message. 

I am working with some old jobs and I found this code snippet. I am just wondering why would someone dump transaction log files from databases in to one file ? 

Is there a way to resolve this to make work on Domain3 SQL Server? Also, creating a group and adding the individual userID's from other domains worked too. We don't want a new group just for this. 

There is a SQL job scheduled with number of steps. Few steps are throwing Query timeout expired error but still the job step is being reported as Succeeded. I confirm that the steps are set to "Quit the job reporting failure" on failure. What could be the cause for the step to succeed? 

Full Backup started on 4/21 12 AM. While backing up one of the database, it is stuck. sp_WhoIsActive shows following information 

I just by chance added alert for Error 825 and ran sp_Blitz again. And the message for Finding "No Alert for Corruption" was not displayed for Error 823 and 824 !! After Alert setup: 

I have added all available actions for the login event (SELECT * FROM sys.dm_xe_objects WHERE package_guid = '655FD93F-3364-40D5-B2BA-330F7FFB6491' AND object_type = 'action' ORDER BY name) but none appear to give the network protocol. It may be of course that Login is not the correct event to give this information, but I can't see a connection event (or similar) within the XE DMV. To confirm, I want an extended event session to expose the net_transport information that is returned by dm_exec_connections: - 

I don't believe the issue is concerning the encryption. You are doing two sub-queries either of which could contain multiple rows, hence the error. If both source tables are the same, you can re-write as: - 

And yet the .Net connection is hard-coded to set this off by default? As another point, you have to be very careful when diagnosing performance issues with this setting. Different set options will result in different query plans for the same query. Your .Net code could experience a performance issue (SET ARITHABORT OFF) and yet when you run the same TSQL query in SSMS (SET ARITHABORT ON by default) it could be fine. This is because the .Net query plan will not be reused and a new plan generated. This could potentially eliminate a parameter sniffing problem for example and give much better performance. 

Note that the filter on cal.[name] is not sargable. This means that even if that column is indexed, the index will not be used for a seek due to the leading "%" in the predicate. 

Take a look at sys.dm_db_partition_stats ($URL$ There are some sample queries at that link and also at $URL$ You can modify to suit your needs i.e. filter in/out non-clustered indexes. Multiply page count by 8 to get size in KB, then divide by 2^10 (=1024) to convert to MB if required. sp_spaceused ($URL$ will also give table size information. 

The Cardinality Estimation logic was updated for SQL Server 2014 and could potentially be a reason. You would have to test the same queries with the old cardinality estimator and compare performance metrics. You could do this by lowering the compatibility level to <120. I would perform all this testing on a test server and not in production. $URL$ $URL$ 

P.S. The Service Account was granted rights and the old files are being cleaned up. Version Info: SQL Server 2012 SP3 CU8 / Windows Server 2012 

I have setup a SQL Agent job to delete files older than 7 days. The script does it job when run through windows powershell window. However the same script does not work from SQL Agent job 

It turns out that the person migrated the jobs from old server to new server edited the jobs manually. One of the edit was to this was set to default log location on old server. On the migrated server it was edited to where the directory did not exist. (that person missed deleting the text in the path) On the other jobs (FULL and DIFF) the text had been removed so it was set like this and these both were working fine! 

Here is the scenario which I am working with IT team to get it right with no luck. Following are the users who work on a SQL Server. 

Is there any way to find who used Dedicated Admin Connection? Not active connection but the previous one which is already closed? 

And I verified by only setting up 823 and ignoring 824 and 825, still the sp_Blitz does not report for other 2 missing (824 and 825) !! 

We recently migrated to a new SQL Server and the SQL Services are running under a Service Account. I did observe that the Service Account do not have rights on G drive and the old files are not being cleaned up. Question: Why doesn't the procedure when it cannot delete a file OR simply move on instead of waiting for around 9.5 minutes? Also, doesn't the job error if it cannot delete the old backups? We will not know until we receive alert !! Here is the log details. 

I had a situation where the Native Backups were being made on a Server. I happened to see in that there was a third party backup tool () that was also taking VSS (kind-of) . At some interval, the AppAssure (backup being made to ) was doing a and at some other interval it was doing a breaking the log chain. Is there any way() to know when a backup log chain is broken? Here is a screenshot of the situation from February. 

I found this out be chance by creating a session from the "Connection Tracking" Microsoft shipped extended events template. SSMS | Management | Extended Events | Sessions | Right click | New session | General page | Template = Connection Tracking. That template includes "SET collect_options_text=(1)" and I wasn't previously aware such an option existed. An example is also given in this sql-server-performance article. The last screenshot shows options_text populated with "network protocol: LPC.." LPC is shared memory. 

If this is a production database, you should remove developer's access rights to create tables. This can be achieved via the REVOKE command. You may have further work to do if the developers are adminstrators. Developers should then create new tables in a developement database and these are migrated to production via the DBA team following a code review and test cycle. Alternatively, you could create a DDL trigger that could stop users creating tables via a ROLLBACK and create a warning message for the developers as you suggest. Finally, to clean up test tables, you will need some method to identify them (table name pattern, owner etc). You can then automate a script to drop such tables. 

You can drop all views on the instance via the following PowerShell script. Edit YourInstanceName before running. This will drop all views with DoNotNeed_ in the name from all databases on the instance. This script could easily be extended to do the same against hundreds of instances by adding another level of iteration. 

Take a look at tSQLt ($URL$ which is a great free tool to create unit tests in SQL Server. You can fake tables (by executing tSQLt.FakeTable) within your tests and then create mock data as you describe. Tests are themselves stored procedures and any changes made as part of your testing suite are rolled back post test. You run your tests by executing the tSQLt.RunAll stored procedure and it gives you visual feedback summary of your test results. To get started, navigate to $URL$ and download. Running Example.sql will create an example database called [tSQLt_Example], create the testing framework and some example tests to see what is possible. Also available is the Redgate paid product SQL Test ($URL$ This is a GUI wrapper around tSQLt but is not required to use tSQLt. 

How Do I See the Full Error Message? The SQL Server error log on the instance you are trying to connect to will have any errors that SQL Server generated about the logon failure. If SQL rejected the connection/failed the logon for some reason it will generate an 18456 error typically. On the SQL side, this error will have an error state that gives clues to the cause of the error. On the client side, however, you may or may not see the useful details - it likely that you'll just see an error state of 1. To see the full error message it is best to look at the SQL Server that you are trying to connect to and look in the SQL Server Error Log. You should see the error for the connection attempt if the connection is making it to SQL Server. From there the information is more useful than some other error messages if you know where to look. What If There is no Error in the SQL Server Error Log? This could be caused by several factors but it is quite likely that you are not dealing with a logon failure. Logon failures do a good job of presenting themselves in the error log. In this case it is quite possible that you are dealing with a failure to connect to SQL Server. You can troubleshoot this problem first and at least verify that you are connecting to SQL Server in the first place and that there are no issues here. Rather than get into all the potential steps for troubleshooting that here in this answer, this Technet Twiki article is an excellent resource to at least get you started. How do I Read the Error Messages in the Log? There are several resources for the meaning of the various error states and error messages. The below is pasted from this msdn link to get you started with some of the more common states of the 18456 error. These states may change in future versions or in the version you are on so it is a good idea to pay attention to both the message and the state and to dig deeper to understand the issue. State ---- Meaning 

Have you considered creating nonclustered columnstore indexes on your subscription. Columnstore indexes will give you the benefit of massive compression rates. I have just tested this with SQL Server 2016 Developer edition and it allowed me to create nonclustered columnstore indexes on my subscription. 

An alternative would be to use virtualisation and maintain separate instances on the same physical host. The benefit in doing this is each instance maintains its own dedicated allocation of CPU and memory that can be scaled up or down as necessary. If you merge all instances as you describe, then all databases will be competing for the same CPU and memory. In addition, a shared plan cache will now mean plans from one database can age out plans from another. If you maintain separate instances in VM, then you can retain the same linked servers and not have to make any database or software changes. Performance monitoring is also far easier with multiple instances. It is simple to see which VM are contributing to peaks in resource usage. In order to use maximum virtualisation described, each core of the physical host must be licensed for SQL Server enterprise edition. Valid software assurance is also a condition of this licensing model. 

See the article "Dynamic Search Conditions in Tâ€‘SQL" from Erland Sommarskog at $URL$ This explains in detail your options for dynamic searching and the performance implications of doing so. 

In an OLTP system, they should be kept as two separate tables with appropriate indexing on the ID columns. In a data warehouse star schema design it would be perfectly legitimate to denormalise Color into the Car dimension. I am assuming here that Car would be a dimension, supprting facts such as Journey or CarSale etc. The denormalisation would be handled by your ETL process during import to the warehouse from your OLTP system. This would usually be done via intermediate staging tables. 

A last point is members of db_owner can drop your database. It is usually good practice to grant the minimum permissions a user requires to perform their role and nothing more. 

This command is described here. If you want to understand the mechanics deeper, I wrote a blog post a few years ago that discusses transaction log growth. Proper management is best because then you won't have to worry about finding an automatic way to shrink it. 

The only thing I'll add is that I find it easier to split later than combine later. Depending on what type of information you are looking at about Boats or Cars and how many points you care about and what you are doing with the data (To @JackDouglas's point about needing more specifics to give specific advice) maybe you won't need them combined, maybe you will. If you aren't sure and can design with them combined and are leaning that way, it is worth a try and testing knowing that someday you may have to split them out. Won't make refactoring easy, but IMO it would be easi*er* than trying to combine the data, if only by a little. 

Unfortunately, no. You are installing a program and making service and registry changes, etc. You need to run setup as an account that has local administrative rights. No workaround to this that wouldn't involve some back door into administrative rights. Best bet is to ask for someone to temporarily give you the rights or install it for you. Source: The Microsoft SQL Server books online article for how to install SQL Server 2008 R2Express 

So I can say for sure in my limited testing on SQL Server 2012 the traces of the DB were gone from the buffer pool and the plan cache completely. Which makes sense because the DB that they related to disappeared, that is the expected behavior. That said - after a restore one thing you definitely should consider doing is updating your statistics. I like to start with clean stats build up and I really like to see people using production like data in dev and I like starting with clean statistics. So good on you for using real data in dev. (Caveat - sometimes you need to deidentify your data before doing this, but yay for testing with real data distributions and sizes.)