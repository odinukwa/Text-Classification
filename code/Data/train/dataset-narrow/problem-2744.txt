One popular method is to use parent-children hierarchies to store matrix transformations in relative space. If you are moving the table and everything along with it, the bowls are the children of the table as far as their location is concerned. This can be done as simply as storing and pushing matrices in an array, and multiplying them. This could also be a simple use of a scene graph (note that scene graphs are not strictly defined as a spatial hierarchy though, as they can be used for many purposes). and are examples in OpenGL for stack local transformations. However, these operations are expensive if you frequently look up their world position this way. It's a better to store an extra matrix for each object for its absolute position in space. You access this matrix when the object in question is not being moved at the moment, to save performance. Multiplication of local matrices then would happen only when the object is moved, and then its world (absolute location) matrix will be updated. 

In Blender 2.5 and later, select the face or edge you want to measure in Edit Mode, and turn on the Properties shelf by pressing 'N'. In here, scroll to Mesh Display > Numerics. You can select to display the edge length and the face area of the faces. To get the size of an entire object, the Properties shelf in Object Mode will list the X, Y and Z dimensions. It's right in the Transform section. 

Usually the metric used is node overhead. You can take it to the pathological case (if you can subdivide you do), but at some point the extra effort stops gaining you better results. For normal spatial partitioning octrees, that's processing overhead and memory overhead. Marching Cubes is an odd one, because the subdivision is gaining you more accuracy. The finer grained the nodes, the smaller the polygons generated, and the closer the surface generated is to the actual surface shape. But the same principles still apply - you're choosing to subdivide if the smaller nodes will get you substantially better results. The heuristic is basically just a decision on how much better things will be if you subdivide. The threshold is just a number below which you consider the improvement in results to be trivial / insufficient to warrant the extra cost. In marching cubes, the items (polygons) don't exist when the partitioning tree is being generated. Only the density and surface threshold is available. The triangles representing that surface are still to be generated, but the assumption is that lumpier parts of the surface will need more triangles to represent them. So the metric being used here is "how lumpy is the surface within this node". If it's flat/planar, it doesn't need subdivided, because the triangles in that node will be large and relatively flat. I can't follow the maths exactly in your linked paper but I presume the logic is the same as any error metric. For each node you can calculate an error - if you stopped subdividing at that node and just generated the surface polygons at that resolution, how bad would the result be (in terms of error difference against the real surface). You'd be calculating 'error improvement' from subdividing, where the improvement in the error is: Reading further into the heuristic, it seems like they are comparing the interpolated values between the 8 corners with the actual values at the subdivided node corners. Not sure why they're interpolating trilinearly. But this seems to me like a good way of estimating lumpiness. If the surface was relatively flat, the surface values at the midpoints used for the sub-divided nodes would be very close to the values you'd get if you just interpolated from the corners of the parent nodes. If the surface was very lumpy, the actual and interpolated mid-points would be way off. How far off can be calculated - and that looks like what that graph is. v represents the distance from the estimated surface to the actual surface at that point. Higher values of v mean that the actual surface has diverged a lot from the estimated surface that you'd get if you didn't subdivide. That gives you an error metric that you can use to threshold whether or not to subdivide. If it were that naive though, it would fall down in the case where sinusoidal variations in the surface neatly aligned with node boundaries, and happened to result in the mid-point being accurate (imagine a case where in the left side of the node, the surface bulges outwards, but on the right side the surface bulges inwards by the same amount - at the midpoint, the surface is at the same position as it would be if it were flat). I presume that's why the tangent of the surface is also considered - it's much harder for the surface to be in the right place with the right tangent and not also be flat. 

You are on the right track about keeping the HUD out of the ECS. HUDs and game screens in general benefit greatly from inheritance, as the flow of gameplay is essentially one tree where you can branch out in different paths or backtrack in a manner that makes sense to the player. I use a variation of state pattern, where However, the HudLayer and GameScene can be condensed (in the programmatic sense) into the same type of object. I consider HUDs to be GameScenes too, in that they can contain entities and a update->display loop that may await user input. Entities in HUDs are the background, icons and text, in a similar way that characters and backgrounds on the battle screen are entities composed of graphics and locations. While this means HUDs contain elements that are part of the ECS, the HUD screens themselves are not in the ECS. Going back to inheritance, if the battle screen is considered the primary GameScene that is active, all the on-screen HUDs and menus can be children screens of the battle screen. When you open the inventory screen from the battle screen, the battle screen stops taking input and creates the inventory screen. The battle screen keeps track of the active character and what cell it is on, and any character-specific menus are loaded with this information. After making a selection and closing the menu, a value based on the selection is returned back to its parent, the battle screen, and the ECS would take care of making the character entity respond appropriately. 

For an in-depth study of latency in games, Mick West is the undisputed master. Read these two articles first, and it will make everything else make so much more sense. There are two aspects to this. The first, which you've picked up on, is the latency between an instantaneous control input (e.g. a button press), and the time at which the user perceives the effects of that action. The firing of a gun is usually the best example of this as it's easy to measure. There are many contributing factors to this, most of which are completely out of your control. Worse, it's a cumulative effect, so if each stage is introducing a bit of latency, it can all mount up to lead to severe overall latency. Different types of latency you'll see: 

There's another nuance here about simulating too far ahead of time, meaning the user's inputs might be ignored even though they happened before the frame was actually rendered, but don't worry about that until you're confident that the loop is simulating smoothly. 

Streaming assets from disk dynamically, whether in the background or synchronously during a loading screen, allows you to be far more efficient in your use of memory, with negligible effort and only a minor downside for the user. But it's an optimisation, and like any other optimisation it has diminishing returns the more you try to apply it. At some point the increase in memory footprint for keeping something resident is outweighed by the advantages gained from having it there without the need to load. But until that point is reached, it's better to load only what you need, when you need it. 

Exception handling is a cleaner way to handle unexpected errors from any point in the game. But the beauty of it is, in your codebase, it won't matter how deep you are in the code to handle it. Without exceptions, some kind of error handling routine would be returning a bool or int for some kind of status, then unwinding the nested subroutines till you got to a top-level area (your game class) and exit from there. It will get very cumbersome to write your code to error check from any possible point, wheras using exceptions requires little to no refactoring. Since your question is XNA specific, it will probably be of good use to know how to handle exceptions in an Xbox game, because when a game in there doesn't catch one, you are not left with a clue as to why it happened. This article shows you a clever way to work around that by running your entire game in a try/catch block. It will launch a new "ExceptionGame" if something happens to go wrong, passing the details of the error to display in the ExceptionGame. 

These techniques don't super-sample or multi-sample, so lines that appear less than 1 pixel in thickness will appear with gaps and not be anti-aliased correctly. This is the downside to using a non-MSAA approach. Since you're only working with a raster image at full resolution, you can't create additional information from these empty gaps. Take notice that all of these techniques are dependent on sampling adjacent luma (brightness) or chroma (color) values. Calculating luma and optional gamma correction requires additional instructions on the AA shader, though it's pretty straightforward. You can offload this by calculating the luma in the previous shader that provides the un-retouched image, storing the luma in the alpha channel. Then in the AA shader, you will simply sample the alpha. 

All of these I found in 15 minutes just looking at the OpenGL function reference. I think you'd be better served by stepping back and going through a few tutorials which illustrate the use of vertex / index arrays and other basic GL functions before you jump to trying to write a .OBJ renderer. Alternatively I'd recommend Googling for other existing OpenGL .OBJ implementations (of which there must be dozens) to look at for reference. 

Fundamentally I think this is a problem with the grid-based ray casting being a pretty inaccurate and granular approximation of the 'real' thing you're trying to simulate. No matter what you'll do you'll run into problems. Instead of thinking about it in terms of the specifics of the iteration and implementation, think of it as if your world was actually made of blocks which are the size of your grid squares. You are casting rays as if they were going through the centre of each block. And you're modelling visibility drop as if a partially opaque block was made of some material which acted like depth fog (i.e. the visibility drop from passing through the block was proportional to the amount of block the ray passed through). Let's assume you are standing and looking at a block 2 north and 2 east of your position. If you look at the corner of the block closest to you (the SW corner), you actually have ~1.414u of block to look through. If you look at the NW or SE corners, your vision ray passes through no block at all. It's a fundamentally flawed approximation to model visibility loss in this way. The correct way to model it would I think be to raytrace to each corner of each cell instead, and use the maximum of the 4 values. I'm guessing that is rather too expensive, and if you're stuck using an integral/Bresenhams raytrace then it simply won't be possible. I'm also assuming that you're setting the cell visibility (i.e. how bright the cell is) based on the visibility level of the ray as it enters that cell. So a ray passing through fully transparent air sets all the cells to 100% visible, then when it hits an opaque wall the entry visibility is 100% (so the wall is fully visible) but the exit visibility is 0% (so no cells afterwards are visible). If this case is indicative of your problem: 

There are several alternatives to native MSAA in OpenGL. With post-processing effects, the best thing about them is that you can usually just throw in the shader to the final, unprocessed image and it does the rest. Here are three methods worth taking a look: 

This makes the NodeRotation editable in the Properties window, so now you can set the rotation (in degrees) on a per-model basis. 

Disclaimer: This solution really is simplified for your particular problem, because your camera is looking straight down the Z axis of the world's coordinate system. Also, the center point at 50 units away is already given as (0, 0, 50). If your camera's viewing direction was an arbitrary vector, there would be more multiplications involving the distance with a cross product of the viewing vector and the camera's Up vector. Determining the borders of a plane at a given distance is dependent on the FOV angle in which the view is projected. Usually, the FOV angle is measured in the Y axis for rectangular viewports. For any given distance Z from the camera, the shortest distance D from the center point of a plane perpendicular to the viewing vector at Z to one of its borders above or below the center (really, the intersection of the plane and frustum) is . Add and subtract D from the center point's Y component to get the maximum and minimum Y extents. To get the minimum and maximum X extents, add and subtract . Now getting the location of the plane's corners is simply plugging in the mix/max X and Y in its four possible combinations along with the Z distance. 

Well, as a basic bit of advice, on PC I'd say "don't assume your user wants to run full screen". And in windowed mode, pick your ideal ratio and just use that directly. Users I think are generally accepting of black bars when presented with full screen content. So strategies 3 and 4 are acceptable, if not ideal. They have the advantage that you always know how much content you are rendering: i.e. no sneaky bugs which only occur when running in wide-screen. If you are trying to be adaptive, and detecting the users ratio via screen resolution and showing as much content as possible, then you have to take account of high and low priority content differently. High priority content is stuff that the user absolutely has to see onscreen, if it is offscreen the game is failing. So this is things like HUD and UI elements, and the player avatar and anything they are interacting with. Low priority content is stuff that if it's onscreen then that's good, but if it is offscreen it's no big deal. E.g.:Background graphics and things that are reasonably far away from the avatar. Assuming you have a UI/HUD which is being superimposed over the top of some 'physical' 2D world, then this is straightforward enough. Low priority items are easy, you just make sure that the 4:3 viewport is centred on the interesting things, then draw as much low priority stuff as you can to the left or right. High priority things in the 2D world (e.g. your character, enemies your character is directly fighting) should always be kept in the 4:3 viewport. I.e. don't have your game code zoom the camera in to take advantage of the extra screen real estate, because then you will have game code acting differently in widescreen vs. not. Have the game code assume that the world is being rendered in 4:3, and only let your rendering code be aware that there is actually more than that being visible. Laying out UI/HUD elements can be approached in one of two ways: 

This is definitely possible. I would look into this book or ebook online and it teaches you how to write effective sound classes so that you can use them in the future, with situations like you are speaking of. You can iterate through samples in any sound file and throw flags on high/low pitches, amplitudes, frequencies, etc.. Here is the book: $URL$ Once you have the sound class down, you can store the samples into an obj, then use that obj on the fly. Instead of trying to iterate through the sound file as it is playing. 

I am aware of the AfflineTransform's getTranslateX and Y() methods. I have tried assigning these returned values to my x and y weapon values in the update method, but that is definitely not the solution. Main issue: weapon coordinates aren't rotating, but the drawing is successful. If anyone has any advice on where to start, based on my problem, it would be greatly appreciated. 

In the process of developing a 2D game and am having some issues with translating coordinates on rotation. The object being rotated is a sword that is attached to the player. The player currently points towards a Point (mouse pointer). I am having no problems drawing the rotation, but am a bit confused on how to update the local coordinates so that the bounding box will follow also, around the player. The weapon (coordinates) is just stuck on the left side of the player(invisible), but the drawing of the weapon is rotating around the player. The bounding box is working fine, but who wants invisible collision detection. It basically just isn't following the draw rotation. Here is my basic draw implementation for rotating this weapon around the player. WEAPON CLASS: 

If you don't need the bullet firing to happen within the animation, but only once it has finished, you can get away with a much less complex system without the notion of events. But you'll still want a system where animations play back on their own, know how long they are, and can call back to the game code when an animation completes. 

Great answers so far from James and Ricket, I only answer to add a note of caution. Message passing / event driven communication is certainly an important tool in the developer's arsenal, but it can easily be overused. When you have a hammer, everything looks like a nail, and so on. You absolutely, 100%, should be thinking about the flow of data around your title, and be fully aware of how information is passing from one subsystem to another. In some cases message passing is clearly best; in others, it may be more appropriate for one subsystem to operate over a list of shared objects, allowing other subsystems to also operate on the same shared list; and many more methods besides. At all times you should be aware of which subsystems need access to which data, and when they need to access it. This affects your ability to parallelise and optimise, as well as helping you avoid the more insidious problems when different parts of your engine become too closely entwined. You need to be thinking about minimising unnecessary copying around of data, and how your data layout affects your cache usage. All of which will guide you to the best solution in each case. That being said, pretty much every game I've ever worked on has had a solid asynchronous message passing / event notification system. It allows you to write succinct, efficient maintainable code, even in the face of complex and quickly changing design needs.