In my experience when using GBM and xgboost while training large datasets (5 million+ records), I've experienced significantly reduced memory utilization (in R) for the same dataset and found it easier to use multiple cores to reduce training time. 

Decision trees are by nature immune to multi-collinearity. For example, if you have 2 features which are 99% correlated, when deciding upon a split the tree will choose only one of them. Other models such as Logistic regression would use both the features. Since boosted trees use individual decision trees, they also are unaffected by multi-collinearity. However, its a good practice to remove any redundant features from any dataset used for training, irrespective of the model's algorithm. In your case since you're deriving new features, you could use this approach, evaluate each feature's importance and retain only the best features for your final model. The importance matrix of an xgboost model is actually a data.table object with the first column listing the names of all the features actually used in the boosted trees. The second column is the Gain metric which implies the relative contribution of the corresponding feature to the model calculated by taking each feature's contribution for each tree in the model. A higher value of this metric when compared to another feature implies it is more important for generating a prediction. 

Refer to the authors' paper on xgboost here - XGBoost: A Scalable Tree Boosting System. Equation 3 on page 2 mentions that in each step all the predictors are used to greedily fit the next additive tree. However, since column sub-sampling is also employed to prevent over-fitting, not all features may actually be used for each step, see section 2.3 of the same paper. Hence, the specific answer to your query is that the algorithm uses all features to fit the residual. 

The paper cited in the question "FaceNet: A Unified Embedding for Face Recognition and Clustering" is available at $URL$ . Page no 5 of this paper lists the reference [16] for the Inception module architecture. The original paper introducing and describing this Inception architecture is - "Going Deeper With Convolutions", it is accessible at $URL$ . An overview of inception modules is given in the diagram on page 4, its included here - 

Reduce the features step-by-step until you end up with using just 1 feature and see whether the accuracy changes or not. Add a sine-wave and a random noise to the feature set and see whether it effects any of these optimization algorithms. Re-evaluate how you selected or derived these features, check if these are highly correlated. Are your classification targets highly imbalanced? If so, then under/over sample them to achieve a more balanced training set. Then check the performance of you algorithm after training over this balanced dataset. 

darch: Package for Deep Architectures and Restricted Boltzmann Machines. The darch package is built on the basis of the code from G. E. Hinton and R. R. Salakhutdinov (available under Matlab Code for deep belief nets). This package is for generating neural networks with many layers (deep architectures), train them and fine tuning with common known training algorithms like backpropagation or conjugate gradients. Additionally, supervised fine-tuning can be enhanced with maxout and dropout, two recently developed techniques to improve fine-tuning for deep learning. CRAN link: $URL$ deepnet: deep learning toolkit in R. Implement some deep learning architectures and neural network algorithms, including BP,RBM,DBN,Deep autoencoder and so on. CRAN link: $URL$ 

The 4 charts represent time on x-axis and value of a random variable on y-axis. You want to understand whether the underlying process can be decomposed into erratic components and periodic component, and while doing so be able to predict the value at the next point in time. 

Divide up the day into various parts - early-morning, morning, noon, afternoon, evening, late evening, night, etc. Assign time boundaries to each part of the day, e.g. noon could be 12 pm to 1 pm. Create 3 new labels - "part of the day to call the customer", for each positive case (status of call=true) assign it the corresponding label (morning/noon/evening). These labels will be in one-hot encoded format e.g. prefer_morning=0/1, prefer_noon, prefer_evening, etc. Build 3 models to predict whether the lead prefers morning/noon/or evening time of the day for a call to be successful. 

Identify a client using their ISN: There is a certain logic in generating random initial sequence numbers by each OS, and if you have a large sample of ISN nos available, then you could identify the attributes of the random number generator to pinpoint the client. This ISN generation process has been exploited for DoS spoofing attacks, and many OS's have strengthened it since then, but this may still provide some clues to identify machines uniquely. Analyse SSL handshakes to identify clients via their attributes such as supported cipher suites information exchanged during the SSL setup handshake, etc. See this paper for details about this technique. The source port nos. are usually assigned in increasing order, and it is unlikely that every machine started at the same time and had the same TCP traffic volume; this helps identify clients uniquely. TCP/IP timestamps: Most OS's will increment the timestamps in TCP headers and this keeps increasing as long as the system is up. Since systems may be switched on at different times, it helps segregate clients even if they have exactly the same OS and browser versions. See this RFC for mode details. Some proxies my use extra HTTP headers (Via, X-Forwarded-For) to indicate the exact client hostname or address requesting the data. This provides a straightforward way to identify the client, but only if you can access the HTTP header information from the packet. 

Tim Dettmers has an excellent (frequently updated) blog where he's compared different cards, near the end he's also given a simple speed-up comparison. Using that as a guide, it can be estimated that Titan X pascal would be upto 5 times faster than a GTX 970 for Deep Learning. 

Clients reaching the server without intervening firewall/proxy: In this case each client will have a unique source ip-address which should help segregate the traffic without further analysis. Clients behind a proxy which filters traffic and changes the ip-address to set a unique ip-address of the gateway for all the traffic coming to the server. In this case, you could use a combination of the following: a) Segregate clients by TCP/IP packet attributes: Attributes such as TTL, SYN packet size, TCP window size, etc, can identify client OS. For example - Linux and Mac-OSX have default starting TTL values of 64, whereas windows has a default of 128, others like Cisco/Solaris 2.x have 255. So you can roughly segregate the traffic by OS type. b) Since the TTL is decremented by 1 for each router placed between the client and the server, so even if you have 2 windows machines, but placed in a network segment separated by one more router, then the TTL number will be one lesser than the other windows clients. c) Further identify unique sessions by the sequence number and acknowledgement number in the TCP headers. Each acknowledgement number of the destination server can be correlated by the seq number of subsequent packets from the client for that session. This will let you segregate users connecting to the same server at the same time e.g. 10 different sessions to stackexchange.com. d) In case you can get further web-server log details, then attributes such as user-agent strings and HTTP cookies can help you identify a client. e) In some cases, the random source port no. used by clients for the TCP session might overlap for different clients accessing different dest. ip-addresses at the same time, you can then safely assume these are traffic form two different clients. 

So let me clarify your query: you have trained a random forest model to classify a dataset into multiple classes, e.g. A, B, C, or D. Now, you want to understand for each different class label (e.g. A), which features contributed to it being classified as class A vs. not class A, and so on. To find the importance of a variable in random forest, each variable is permuted among all trees and the difference in out of sample error of before and after permutation is calculated. The variables with highest difference are considered most important, and ones with lower values are less important. this method gives importance for the entire multinomial classification, i.e. A vs. B/C/D, B vs. A/C/D, etc. and not just for one label. So you can't claim that feature1 is more important to identify class A, but feature 2 is more important to identify Class B. I've been using the random forest package implemented in R and it does not seem to provide any way to decipher the internal details on the one-vs-all categorization. If you're really interested in doing just that, then you should fit a set of random forest models each for classifying class A vs. the rest, class B vs. the rest, etc. The variable importance of these separate models would give you the feature importance for classifying each label. You can use the same library to fit these models using scikit-learn. You may refer to their help page here - $URL$ 

Find out the most important predictors used by your model; the methods used to identify importance depend on the model used. For each of the major pedictors try to find out how does each predictor segment the customer population of will-churn vs. will-not-churn. That would be the starting point for framing hypothesis of why these set of customers are churning. Generate reports/info-graphics showing how these predictors impact the tendency to churn. Sometimes predictors are not causes but side-effects of the hidden cause showing up elsewhere. So be open to skepticism about the predictors. Involve the business/users and have brainstorming sessions to go through each of these hypothesis and evolve the most practical and economic actions that could be taken to reduce churn. Before you get into a discussion, plan for a sufficient amount of time, come up with your best thought actions which you feel will address the situation and have all the details gleaned from your analysis available for sharing in the session. 

The random forest algorithm fits multiple trees, each tree in the forest is built by randomly selecting different features from the dataset. The nodes of each tree are built up by choosing and splitting to achieve maximum variance reduction. While predicting on the test dataset, the individual trees output is averaged to obtain the final output. Each variable is permuted among all trees and the difference in out of sample error of before and after permutation is calculated. The variables with highest difference are considered most important, and ones with lower values are less important. The method by which the model is fit on the training data is very different for a linear regression model as compared to random forest model. But both models don't contain any structural relationships between the variables. Regarding your query about non-linearity of the dependent variable: The lasso is essentially a linear model which will not be able to give good predictions for an underlying non-linear processes, as compared to tree based models. You should be able to check this by verifying the models performance over a set-aside test set, if the random forest performs better, the underlying process may be non-linear. Alternatively, you could include variable interaction effects and higher order variables created using a, b, and c in the lasso model and verify if this model performs better as compared to a lasso with only a linear combination of a, b and c. If it does, then the underlying process might be non-linear. References: 

Before deciding on the model, I would recommend to re-formulate the dataset to best suit your problem. You could approach this problem as follows: 

Yes, that is the standard approach to convert categorical variables for fitting a model. In this case it would be used to train a neural network. So each category of a categorical variable is represented as a separate vector. Note that you do not need to do this for binary variables such as Male/Female as the presence of one category implies absence of the other category, so instead of using a variable such as ; you could convert it into a variable called . If this dataset is used to fit a regression model, the proper nomenclature should be multiple linear regression. 

In addition to the answer given by Icyblade, the developers of xgboost have made a number of important performance enhancements to different parts of the implementation which make a big difference in speed and memory utilization: 

So your query is a comparison of linear regression vs. random forest's model-derived importance of variables. The lasso finds linear regression model coefficients by applying regularization. A popular approach to rank a variable's importance in a linear regression model is to decompose $R^2$ into contributions attributed to each variable. But variable importance is not straightforward in linear regression due to correlations between variables. Refer to the document describing the PMD method (Feldman, 2005) in the references below. Another popular approach is averaging over orderings (LMG, 1980). The LMG works like this: 

As a first step, to segregate the messages that appear to be a bot, you could first try binning by message size. For example, if messages sent by bots are likely to be around 128 bytes to 140 bytes, assign these to a unique bin. Next, create a time series based on this bin. Try to decompose the time series using an additive or multiplicative method such as Holt Winters. A strong seasonal component would help you identify regular and repetitive messages which are being generated automatically.