You wouldn't have root privileges if you did this. Perhaps you need to discuss what specific privileges you all need, and then acheive these without root. Of course, someone will need to have the root privilege - that could be the VPS admin at the hosting company I suppose. 

So to get 12 hours between notifications, you could set notification_interval to 720, and leave interval_length alone. But I still think the acknowledgement setting is better because it allows Nagios to keep nagging your team till they take some sort of action. Note that, either way, Nagios may still send notifications more frequently depending on what is going on. I've had alerts relating to CPU use, where it oscillated between just above and just below the critical threshold - no matter what I did, every time it went over the Critical threshold value, an alert went out. The flapping detection in Nagios is used to handle these situations. Or you might want to look at your alert thresholds. 

It would entirely depend on how confident you are about your PHP install. If you think it is rock solid, even if an attacker knows everything about your PHP install, then you could leave it in place. But really, why would you leave this in place on a production system anyway? There may be exploits you are not aware of in your version of PHP - people may now or in future scan for your version of PHP, or particular options you have enabled, because they know how to carry out these exploits. So by keeping this up publically, you added yourself to their hitlist. If you want to keep it up, you can put it in a password protected directory, or just switch it on when you need it. Given the small cost of these options, I wouldn't take the risk of keeping it public. 

Are you trying to do some kind of hot copy, or are you shutting down the VMs before taking the image? I think the converter may work best on machines that are shutdown, so it isn't having to snapshot memroy etc. Also, check any snapshots you might have of those VMs. That may make the process more complex. It is possible that converting to VMWare Server is working as it isn't trying to convert any ESXi specific features. 

I don't think you can access the OS directly from an ESX host, and until you network those machines you aren't going to be able to run commands on them through SSH. Since you are switching IP tables off, I am assuming that you don't have a security policy in place that requires IP tables (or any firewall) is switched on before connecting them to the network. So I suggest switching the network on, allowing DHCP to configure the IP address and DNS entries for each host and then use SSH (via a script) to do your configuration tasks. Functionality similar to what you are thinking of exists in Xen, although this also requires a network connection to get the commands through. 

R-studio got me out of that hole too. I got all my files back quickly and easily. Make sure to stop using the physical disk immediately, if you can, and install whatever tool you use on a different disk. Tricky if it is the system drive - you'd need to attach the disk to a different machine if that is the case. The longer you use it the greater chance your data has of being overwritten. It is less of a risk (zero?) if you haven't created any new partitions, but if the data is valuable I wouldn't take any chances. 

I want to set up various infrastructure in MS Azure that will then be available to multiple locations that are equipped with Cisco Meraki MX Security Appliances. Unfortunately, the MXs don't yet support route based VPNs, and Azure only supports multiple site to site networks when using route based VPN. I think similar challenges may exist with AWS and other cloud service providers. I think I may be able to work around this limitation using a virtual firewall, such as Cisco ASAv, but I haven't been able to find any documentation or marketing material that makes it clear this is suitable. I know I have done hub/spoke VPN with physical ASAs in the past, but I have no experience with ASAv. Has anyone got any experience doing cloud provider hub with ASAv (or any other virtual firewall) and branch office spoke using firewalls that don't support IKEv2 or route based VPNs, such as Meraki MX, Cisco ASA etc? 

Interfaces show as down when you don't have any active devices connected to them. When testing you tend not to have things connected to all the interfaces right? 

There may be benefits for you, but almost none for your customers. Maybe you can make it cost less, depending on your requirements, but I think that is the only possible benefit and it is questionable. Actually - there is a benefit that isn't insignficant - you will learn a lot by setting up and maintaining your own server, but your customers suffer from all the mistakes and trial and error. Downsides for you are: 

If these are going to be critical tools for your users, then you need to build something resilient. A single SBS 2003 Server isn't going to be either resilient or powerful enough, so you are looking at something that clusters. You have a lot of choice. But before you start down this route, with only 25 clients and running SBS you seem like a fairly small shop. Think long and hard about whether you should go down this route. What is really wrong with what you are already doing? Have you got any budget? Because while maybe getting cheaper hardware for user, you will have additional server costs, licensing issues and a learning curve to breach. Is this of value to your employers, yourself and your users? 

This works fine in my own environment, but I am planning on releasing it for wider use, and want to make sure it validates correctly for others. 

Cisco discuss some cases where you might want to manually configure port speed and duplex rather than using autonegotiate, when using PIX/ASA security devices: $URL$ 

I think you may find that a less technical solution meets your needs. Give permission to access the 4 day weekers mailbox to the person you would have forwarded their mail to. You can set up the user account to send replies in their own name, on behalf of, or sent as the 4 day weeker. Sure, you still have to make sure that the other member of staff checks the mailbox, which may be an issue if this is sales related. Or you could just use Out of Office to forward the mail on to the appropriate member of staff. Your 4 dayer could then just switch on Out of Office every Thursday. She'll get prompted to switch it off when she logs back in. Once Out of Office is configured, it will have the same config each time she opens Outlook. Again, it requires someone to take a manual action - but if she is that good, switching on Out of Office every Thursday before she leaves isn't that hard to remember (and you could set a Task with a reminder in Outlook for 4:30pm every Thursday). It might also be easier to script switching on Out of Office, with whatever rules have already been configured for the user. 

One of our remote offices has given a security contract to a company that came in and set up IP security cameras and a server in our office. They clearly didn't know anything about integration of their system into an existing network, as they completed the job without talking to anyone in our team. Our internal network is running on 10.6.n.0/24. They set up their equipment to use 192.168.1.0/24. It's all plugged into the same network infrastructure - the same broadcast domain. Of course, all their equipment can talk to each other, so the security system works, internally at least. If we have no requirement for external access to or from the security system, are there any issues that would necessitate proper integration with our network? Or can I safely leave the equipment set up as it is? 

When you talk about keeping your costs down make sure you are considering the cost to yourself or your staff of maintaining an email server. As previously mentioned there is a very steep learning curve including getting the initial setup right and over the first year or two of running the server dealing with spam, user requests, handling outages, redundancy etc etc. Compared to hosting a webserver, a mail server is much more demanding on your time. As a very small service provider, I regret setting up an email server - it has been the biggest pain, and not rewarded me in line with the effort put in. Consider the options of outsourcing the email and passing on the cost to your customers (by which I mean whoever is paying you - which might be advertisers rather than end users). Or set up your system so that your users can use an existing mail account of their choice. Do any of your customers not already have an email address/service? Do any of your customers really need another one? Of course, maybe you really want to add the mail admin string to your bow, in which case go for it! If you opt for Postfix as your MTA, then The Book of Postfix is a worthwhile investment, and there is a pretty good website out there for asking questions when you encounter problems or don't understand something... Can't quite recall what it is right now :-) 

I think you will need to migrate all the content to a new team site based on one of the WSS templates. This new team site needs to be in a Site Collection that is based on WSS template, and using a separate Content Database - you'll then join that Content Database to your WSS Farm. Once you have your new Team Site, you can save your Document Libraries as templates, with the content included, then create new Document libraries based on these templates. With any luck WSS will handle the MOSS columns gracefully. If not, perhaps you can just delete them. Any content based on Pages you will probably have to manually recreate as web part pages in the WSS team site. Once you have this working ok, you detach the Content DB from the WSS farm, and attach it to the MOSS farm. I think this is worth trying. It may also be possible to change the template a site is based on, which may work too, but I think requires some programming skill. 

You can have a single repository or multiple - it doesn't really matter from a technical point of view as you can check out part of the repository or all of it depending on your needs. However, if you work with other people on different sites you probably want one repo per site. One repo per site also works if you want to give each client a disk quota. I create a user folder for each user, and store the repo, staging site, and live site files in the user folder like this: