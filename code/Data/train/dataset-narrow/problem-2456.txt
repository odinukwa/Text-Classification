I am looking for interesting streaming algorithms that would be suitable for presentation in an undergraduate algorithms course. Good choices should probably satisfy the following requirements: 

Randomly pick a variable to eliminate, say $x$. Pick values for $y,z,\ldots$ randomly. If $\Delta(y,z,\cdots)$ is a square in $\mathbb{F}$, then the equation $p(x,y,z,\cdots)=0$ has a solution for $x$, namely $x=(-q(y,z,\cdots) \pm \sqrt{\Delta(y,z,\cdots)})/(2c)$ (assuming $\mathbb{F}$ does not have characteristic 2). This gives us an assignment to the variables $x,y,z,\ldots$ that makes $p(x,y,z,\cdots)=0$, so we're done. If $\Delta(y,z,\cdots)$ is not a square, go back to step 1. 

You can solve this in $O(n \lg^2 n)$ time. Build a (balanced) binary tree with $n$ nodes, where the leaves are annotated with the values $x_1,x_2,\dots,x_n$, and $x_i$ is placed on the $i$th leaf from the left. Annotate each internal node $v$ in the tree with the maximum value over all the leaves in the subtree rooted at $v$. For instance, the root is annotated with $\max(x_1,x_2,\dots,x_n)$. The tree and the annotations can be built in $O(n)$ time. Next, note that, for any $j,k$, a sequence $x_j, x_{j+1}, \dots, x_k$ of consecutive leaves can be covered by some union of $O(\lg n)$ subtrees. In particular, given any $j,k$, we can compute $\max(x_j,x_{j+1},\dots,x_k)$ in $O(\lg n)$ time, by traversing the tree and using the annotations on the internal nodes at the top of those $O(\lg n)$ subtrees. Now, to compute $y_k$, we do a binary search to find the largest $j$ such that $\max(x_j,x_{j+1},\dots,x_{k-1})<x_k$ but $\max(x_{j-1},x_j,x_{j+1},\dots,x_{k-1})\ge x_k$. Each iteration of the binary search takes $O(\lg n)$ time, and the binary search takes at most $O(\lg n)$ iterations, so this allows us to compute each $y_k$ in $O(\lg^2 n)$ time. Finally, there are $n$ $y$-values that we want to compute, so the total running time is $O(n \lg^2 n)$. This can be adjusted to work in an incremental/online/streaming fashion, with $O(\lg^2 n)$ running time per value of $x$ received, by tweaking the structure of the binary tree appropriately. 

If you read the paper carefully, you will find a discussion about using entropy as a metric for the degree of anonymity (see Section 3). In particular, if the conditional entropy of a certain random variable is too low (conditioned on the information available to the adversary), then the adversary has narrowed down the value of the supposedly-anonymized information to a small set of plausible values -- so anonymity is poor. The paper makes this formal. For instance, see Definition 4 and the surrounding discussion. Read the paper. Carefully. The whole paper. 

Both problems are easy to solve using standard methods. To solve the first (minimize the maximum weight of the edges in the matching): This problem is known as bottleneck matching, and you can find plenty of literature on it by searching for that phrase (thanks to David Eppstein for pointing this out). One simple approach is to sort the edge weights, then use binary search to find the smallest threshold $t$ such that a perfect matching exists when you keep only the edges whose weight is $\le t$; the running time is $O(\lg |V|)$ times the running time for unweighted bipartite matching, and there are multiple algorithms for that. To solve the second (minimize the total weight of the edges in the matching): This is known as the assignment problem; just negate all the edge weights first, then maximize the total weight of the edges in the matching. There are standard algorithms for the assignment problem. 

If the problem is well-defined, I suspect it should be achievable using the following method. Pick a $m$ values uniformly at random from the stream. For each value, the expected value of the number of times it is included in the sample will be proportion to its frequency in the stream. If the frequency of every item is small compared to $1/m$, the probability that an item appears in the sample will be approximately proportional to its frequency in the stream. If some items appear more often than that, use a sketch (e.g., a CountMin sketch) to estimate the frequency of frequently-occurring items. You don't need to estimate the frequency of all items, only those whose frequency is higher than $1/(1000m)$ (say), so the sketch can be very efficient. Then, you select $m$ values uniformly at random from the stream, removing duplicates; if any of the selected values is in the CountMin sketch and has probability close to $1/m$ or larger, then you fix things up (e.g., by dropping it with some probability, as needed). I'll let you work out the exact arithmetic for how to make the probabilities work out however you want (as it's not clear to me exactly what you want to have happen), but this should work and be very efficient, as the sketch only needs to track frequencies for a small number of items. 

I don't think they're very similar, once you look under the hood at the techniques that are used. GANs are typically about continuous optimization; program sketches are discrete combinatorial problems. As far as your lattice formulation, optimization over $\mathbb{R}$ is pretty different from optimization over the boolean lattice $\{\top,\bot\}^n$ -- the algorithms look quite different (e.g., gradient descent vs SAT solvers). It's not clear how you'd apply methods for continuous optimization (like gradient descent) to combinatorial optimization problems (like SAT), or how you'd apply methods for discrete combinatorial problems (like SAT solvers) to continuous optimization. 

New answer (10/24): I think the following paper provides an elegant and efficient solution to your problem: 

There is of course a formulation of this as an integer linear program (ILP). There's no reason to expect it to run in polynomial time, but off-the-shelf ILP servers are pretty good so it might yield decent solutions in practice if your graph isn't too large. In particular, let $x_{i,v}$ be a 0-or-1 integer variable, with the intended meaning that $x_{i,v}=1$ means that the $i$th clique contains vertex $v$. We obtain the linear equations $$\sum_{v \in V} x_{i,v} = M$$ $$\sum_i x_{i,v} = 1$$ and the inequalities $0 \le x_{i,v} \le 1$. Next, let $y_{i,u,v}$ be a 0-or-1 integer variable, with the intended meaning that $y_{i,u,v}=1$ if and only if $x_{i,u}=x_{i,v}=1$. We obtain the linear inequalities $$y_{i,u,v} \ge x_{i,u} + x_{i,v}-1, y_{i,u,v} \le x_{i,u}, y_{i,u,v} \le x_{i,v}, 0 \le y_{i,u,v} \le 1.$$ Finally, we wish to maximize/minimize $$\sum_{i,u,v} y_{i,u,v} wt(u,v).$$ If you wish, you could add some symmetry-breaking by requiring that $\sum_{u,v} y_{i,u,v} wt(u,v) \le \sum_{u,v} y_{i+1,u,v} wt(u,v)$ for all $i$: it is possible that this might help the ILP solver a little bit. 

Linear regression Yes. For linear regression, you can do both updates in $O(1)$ time. Recall that for ordinary least squares estimation, we estimate the parameter vector $\hat\beta$ using the equation $$\hat{\beta} = (X^T X)^{-1} X^T y.$$ Here $X$ is a $n \times 2$ matrix and $y$ is a $n$-vector. Adding a point amounts to adding a row to $X$ and $y$; deleting a point corresponds to deleting a row. So, here is the technique. When you do the initial estimate, remember the value of $X^T X$ (which is a $2 \times 2$ matrix) and of $X^T y$ (which is a $2$-vector). This only requires $O(1)$ storage. Now let's say you add a point. This involves adding a row to $X,y$, to obtain new values, call them $X',y'$. We now need to compute $(X'^T X')^{-1} X'^T y'$. Fortunately, this can computed efficiently. It is easy to compute $X'^T X'$ in $O(1)$ time, given the value of $X^T X$ and the row that was added to $X$. It is also easy to compute $X'^T y'$ in $O(1)$, given the value of $X^T y$ and the row that was added to $X,y$. Once you know the values of $X'^T X'$ and $X'^T y'$, you can simply multiply them (takes $O(1)$ time) and update the stored values. A similar procedure can be used to remove a point, if you've stored all of the points and all of $X,y$. Quadratic regression The same techniques work for quadratic regression. Now instead of a $n\times 2$ matrix, we have a $n \times 3$ matrix. Basically, instead of the simple linear model $y \sim \beta_1 x + \beta_2$, we obtain the model $y \sim \beta_1 x^2 + \beta_2 x + \beta_3$, which has three parameters instead of two. Everything else transfers over, and all updates can be done in $O(1)$ time. Gradient descent Finally, one last method. Some variations on linear regression work by using gradient descent (or some other iterative mathematical optimization algorithm) to find the best model that maximizes some objective function (likelihood, or loss, or whatever). Heuristically, if you update the set of points, you can often update the existing model much more efficiently than computing a new one from scratch. In particular, you simply use grade descent with the new objective function -- but you use the old model as the starting point / initial point for the gradient descent. Gradient descent is usually much faster when the starting point is close to the final optimum value, and when you make a small change (like adding or removing one point) we can expect this to be the case. I don't think there are any complexity-theoretic guarantees with this approach, so it's entirely heuristic -- but it might work in practice if you are using gradient descent based methods for estimation instead of ordinary least squares. 

Your problem is in $P$. In fact, it can be solved in $O(n^2)$ time. Given a tree, you can find a label (a binary string) that is a canonical form for the tree (i.e., all isomorphic trees will share the same label). The algorithm also computes a label for each of its subtrees along the way. The algorithm uses $O(n)$ space and $O(n^2)$ time. Compute these labels for $T_1$ and $T_2$ and all of their subtrees. Store the labels of all subtrees of $T_2$ in a hashtable. There are only $O(n)$ candidates for $T$: since we want $T$ to be isomorphic to some subtree of $T_1$, each subtree of $T_1$ is a candidate for $T$. Given a candidate for $T$, you can test whether it is an acceptable solution in $O(n)$ time: check whether its label matches any of the labels of the subtrees of $T_2$. Out of all the acceptable candidates, keep the smallest one. The total running time to check all candidates is $O(n^2)$ time. Thus, we obtain an algorithm that solves your problem in $O(n^2)$ time. 

One challenge is that if you remove the "monotone" restriction, we do know how to compute such things efficiently. You can compute the value of all $S_0^n,\dots,S_n^n$ (evaluate all $n+1$ elementary symmetric polynomials) in $O(n \log^2 n)$ time, using FFT-based polynomial multiplication. So, proving a $\Omega(nk)$ lower bound in the monotone circuit model would require proving a $\Omega(n^2)$ lower bound on polynomial multiplication. Here's how. Introduce a formal unknown $y$, and consider the polynomial $$P(y) = \prod_{i=1}^n (1 + x_i y).$$ Note that since the $x_i$'s are known constants, this is a univariate polynomial with unknown $y$ and with degree $n$. Now you can note that the coefficient of $y^k$ in $P(y)$ is exactly $S_k^n$, so to evaluate all the $S_0^n,\dots,S_n^n$, it suffices to compute $P(y)$. This makes it possible to compute $P(y)$ in $O(n \lg^2 n)$ time: build a balanced binary tree of polynomials with the $(1+x_i y)$'s at the leaves, and multiply the polynomials. Multiplying two polynomials of degree $d$ takes $O(d \lg d)$ time using FFT techniques, so we get the recurrence $T(n) = 2 T(n/2) + O(n \lg n)$, which solves to $T(n) = O(n \lg^2 n)$. For convenience, I am ignoring $\text{poly}(\lg \lg n)$ factors. If you care about the case where $k$ is very small, you can compute $S_0^n,\dots,S_k^n$ in $O(n \lg^2 k)$ time using similar tricks, keeping in mind that you only care about $P(x) \bmod y^{k+1}$ (i.e., throwing away all terms of $y^{k+1}$ or higher powers of $y$). Of course, the FFT uses subtraction, so naively it's not expressible in a monotone circuit. I don't know whether there's some other way to multiply polynomials efficiently with monotone arithmetic circuits, but any efficient monotone method for polynomial multiplication immediately leads to an algorithm for your problem as well. So, lower bounds on your problem require/imply lower bounds for polynomial multiplication. 

Analysis. Will this work? Will it eventually converge on $A=\{a_1,\dots,a_6\}$ and $B=\{b_1,\dots,b_6\}$, or will it get stuck without completely solving the problem? The best way to find out is probably to test it. However, for your parameters, yes, I expect it will be effective. If we use method #1, as long as $|A|,|B|$ are not too large, heuristically I expect the sizes of the sets to monotonically shrink. Consider deriving $A^*$ from $A,B$. Each difference $d$ suggests $|B|$ values; one of them correct, and the other $|B|-1$ can be treated (heuristically) as random numbers. If $x$ is a number that does not appear among the $a$'s, what is the probability that it survives the filtering and is added to $A^*$? Well, we expect $a$ to be suggested about $(|B|-1) \times 36/251$ times in total (on average, with standard deviation about the square root of that). If $|B|\le 36$, the probability that a wrong $x$ survives the filtering should be about $p=0.4$ or so (using the normal approximation for the binomial, with continuity correction). (The probability is smaller if $|B|$ is smaller; e.g., for $|B|=30$, I expect $p\approx 0.25$.) I expect the size of $A^*$ to be about $p (|A|-6) + 6$, which will strictly improve the over-approximation since it is strictly smaller than $|A|$. For instance, if $|A|=|B|=36$, then based upon these heuristics I expect $|A^*|\approx 18$, which is a big improvement over $|A|$. Therefore, I predict that the running time will be very fast. I expect about 3-5 iterations of refinement to be enough for convergence, typically, and about 6 guesses at $z$ should probably be enough. Each refinement operation involves maybe a few thousand memory reads/writes, and we do that maybe 20-30 times. So, I expect this to be very fast, for the parameters you specified. However, the only way to find out for sure is to try it and see if it works well or not.