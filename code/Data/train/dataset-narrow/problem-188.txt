I have 3 different networks and I'd like users from network A to be able to wake up pc network B (ofc with certain password) so I guess it could be good idea to allow them to use WOL capabilities built in router. 

When I'm configuring wireless interface in RouterOS or in fact most of APs in general there are basically 2 options for controlling traffic between stations connected to AP - forward all, and don't forward (AP isolation) it's kind of binary... I'd like to be able to apply some at least a bit more advanced filtering between stations connected to AP. Just like in some routers it's possible to apply some basic firewall on bridge interface level between bridge ports. So lets say I could allow stations to connect with each other only on ssh port, nothing else 

Puts a lot of centralized load on router Requires a lot of VLANs making it hard to manage Bridging is usually full CPU based operation -> inefficient 

Long time ago I heard from friend that on really short distance using optical links makes no much sense because comparing to passive connection optical interfaces introduce another layer of hardware converting electrical signal to light. Considering 0.5m 10g passive DAC vs 0.5m 10g AOC - which one provides lower latency if any? 

I've seen several switch products advertising port isolation feature as "way to limit ports communication without configuring VLANs" and it sounds quite... sketchy? If i understand it correctly port isolation allows you to literally specify which ports can each port talk to. And it sounds like really really error prone example of clumsy VLAN/ACL combo implementation. So what are actual correct use cases (not ones coming from lazyness and misunderstanding) where this feature is necessary? Can we look at port isolation as kind of simplified ACL? I don't quite see any real value coming from such feature if we have switch with ACL support and VLANs support. It sounds to me like incorrect network/security design. 

TCP Syn-segments are always sent uncompressed. After each host has received a SYN-segment, they know the other side's IP address. They can then compress the following segments' headers, because the IP address is remembered and re-entered in the header by the decompressor. 

You can use all loopback addresses from 127.0.0.1 upto 127.255.255.254 however you like. According to RFC 3330, they are all "looped back inside the host". Some operating systems respond to all loopback addresses out of the box, on others you have to explicitly define extra addresses on the loopback interface. For most testing, 127.0.0.1/32 is sufficient, but sometimes multiple IPs can be useful or necessary. Examples include: 

As a side note, it is not about who answers first. If you give your computer an address in the same subnet, the traffic will go straight to the machine, no routers involved. Even if you would use routing, entering a route to 143.166.0.0/16 on some internal router(s) in your network, they will prefer this route over their (default?) route to the internet. This happens because the "longest prefix match" is preferred, ie. the most specific route is chosen. Your correct about the net result, the 143.166.0.0/16 part of the internet will be unreachable for you or your network should you install the route in your internal routers. /16 seems a bit large for this issue, the smaller the subnet you route to, the smaller the chance of getting bitten. 

I think RFC 1583 is good reading material, and it seems to answer your question (section A.3, emphasis mine): 

A router with "LAN ports" (and probably one "WAN port") sounds like a router designed for a specific use case: providing NATted internet access to multiple devices over a connection with one public IP address. These are often consumer or SOHO products. In this case, the LAN ports will most likely behave as an unmanaged switch. There are of course exceptions. Please be specific if you have a particular device in mind. (Note: consumer grade products are explicitly off-topic here) A more generic router would just have interfaces, which can be configured in many ways and connected to multiple LANs, WANs and other networks. 

I would approach it this way: fetch sysuptime, then calculate the boot date/time and the predicted next wrap date/time. Write both to the log. Calculate the next poll time and if it is after the predicted next wrap date/time by a little margin, write a 'wrap expected' bit to the log. Next time you fetch, look at the 'wrap expected' bit and the predicted next wrap time from the log, and if the bit is 1 and if the predicted wrap time and current boot times are pretty close (extremely close if your server and router are both using NTP) then you know it has wrapped and not rebooted. If not, you know it rebooted. If the wrap expected bit wasn't set, simply go back to the main script logic and calculate the new boot time and predicted wrap time, make your wrap prediction bit, and write it all to the log. You are trying to guard against a reboot false positive by not anticipating the uptime wrap, AND a reboot true negative by assuming the counter wrapped when it actually rebooted. To do both you need pretty careful timing, and even then it just reduces the probability (down past 1 in 1,000,000), but it doesn't eliminate them. If you want to go full tilt, you can do something like adding a second detection layer on top. For example look at the UDP traffic counter: since you are polling via snmp you will be constantly incrementing it a tiny bit. Since there probably arent many other SNMP polls taking place, it will not likely wrap very often (if at all compared to reboots for other reasons) so if you looked at sysuptime going down AND udp traffic count going down you can increase the confidence that you caught a reboot. 

What happens at the time-out is actually pretty clear from the drawing... The congestion window size drops back to its original value of 1 and slow start is run again. The specifics of how a TCP stack will handle congestion events depend on what variant you are using. This drawing looks like an example of the TCP Reno algorithm. When seeing 3 duplicate ACKs, TCP Reno concludes there is congestion, but the network is still working since some segments were ack-ed. In case of a time-out, the situation is worse: the network seems completely unresponsive. Actually, the fact that duplicate acks are being received before a retransmit timer expires means segments are still being received by the other side, even though some may have been lost (or re-ordered). So, in case of the 3 duplicate acks, the congestion window is cut in half and then increased linearly. This is known as fast recovery, and its goal is actually to avoid waiting for retransmission timeouts. When a retransmit time-out does occur, the reaction is more drastic. TCP Reno starts over with slow start from a congestion window of size 1, and a slow start threshold of half of the value of the congestion window when the time-out occurred. When the threshold is reached, increase becomes linear again (additive increase). TCP Tahoe did not include fast recovery, and would react the same way in both cases, resetting the congestion window to its initial value and executing slow start. TCP Reno's fast recovery basically skips the slow start, immediately setting the congestion window to the threshold value and starting the linear increase. Note that many more variants exist and actual implemantations can be more complex. Also observing these algorithms at work is not easy because other TCP mechanisms can interfere. I do not know whether what you had in mind (using fast recovery in both situations) exists as a known and implemented congestion avoidance algorithm. It was probably tested and discarded when Reno was implemented. Feel free to do some digging around in the scientific papers in this area. 

Back all the way up to the premise of the question. A collision, as it were, can only happen in a half duplex situation when "the wire" is common to both receive and transmit, and both sides try to transmit at once making the output from the wire unusable. So, even between two PCs attached to the same switch that communicate with each other, the two ports are not a collision domain as long as both hosts are linked at full duplex. If two hosts do happen to be both half duplex, then there is a collision domain between the switch and the host, which will be arbitrated by the efficacy of the carrier sense logic of each. 

The Ethertype bytes will get altered to something unique if additional VLAN tagging is in use on the link. This might also explain your lack of connectivity. Perhaps the vendor requires a specific VLAN for normal traffic? 

Look at the actual hardware destination of the packet. Just because the IP was not a broadcast doesn't mean the hardware destination can't be. Functions like failover often operate via broadcast traffic (sent to ff:ff:ff:ff:ff:ff) which will be seen by every port on the subnet. If the destination is a mac address that is not your PC, and the switch should have learned it (i.e. that host is active) then the packet should not have been sent to you for the reason you stated. If you have the capture file, open it with a tool like Wireshark and you will be able to drill into a lot of detail. 

Some more sophisticated DHCP solutions will use ping or other activity metrics to gauge actual lease usage, instead of always reserving the address for the entire lease (counting on poorly behaved clients, like you pose). This would allow them to verify and maintain leases still in use and reclaim leases that were issued to a malicious actor. Most (especially basic ones) will simply get to the end of the reservation block and stop answering requests until the leases expire. So, yes a DOS based on lease exhaustion is quite possible on most networks. But, other forms of DOS are more mischievous (such as sucking up all uplink bandwidth, or attacking actual hosts) so most attackers don't bother causing problems for DHCP, and therefore mitigation countermeasures are rarely ever considered. 

For IPv4, IP-packets sent to the duplicate address will be sent to the MAC address currently in the ARP cache. Flapping will only occur if both hosts send ARP replies to the sender, flapping the ARP entry. If no ARP replies are received, connectivity will be fine with one of the owners of the duplicate IP. So the last host to answer (or have answered) an ARP request will "win". This competition will be held for every new host that wants to communicate with the duplicate IP. You could rig the competition by regularly sending gratuitous ARPs. 

I don't know if you can fix the SSH directly through SNMP. However, a lot of Cisco's suppport SNMP triggered config downloading or uploading from/to a TFTP server. You should be able to upload a new config containing the SSH stuff. You need the CISCO-CONFIG-COPY-MIB, available on IOS >= 12.0. If you have the device config, you can create a new version including the SSH stuff and upload it. If not, first download the running config from the device. Important note: uploading to the running config merges your upload into the existing config, but uploading to the startup config overwrites what's already there. Cisco's instructions can be found here 

This is normal behavior for a layer 3 switch, management traffic can be sent to any active Switch Virtual Interface (SVI). If you only want to allow management traffic from a specific vlan/subnet, you could set up ACLs. A layer 2 switch can be assigned a management IP address, attached to one vlan, called the management vlan. It will only be reachable inside this vlan. In the case of the SF300, I believe you have to define a management ACL instead of a normal IP ACL if you want to filter management traffic into the switch. Use the command, and then apply the ACL using .