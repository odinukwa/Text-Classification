I think visualizing convolutional neural networks is what you want. CS231n is a good place to start, here is a step-by-step guide about how to visualize the activations, first-layer weights, and shows how to retrieve images which maximally activate a neuron. Also there's a demo which visualized multiple convnets (including ResNet50, Inception v3, etc). You may learn a lot from this demo (at least I do). If you're interesting in details, this well-known paper may help you. 

Here's a detailed table about different machine learning libraries on different languages: $URL$ Checkout C version here and C++ version here. Personally speaking, try OpenCV ! OpenCV provides multiple machine learning implementations including KMeans, kNN, SVM, etc. 

As @oW_ mentioned, Orange seems to be a good choice. Data Source Orange3-spark provides Hive/Spark support, cool! Interface A little laggy, but acceptable. Algorithms Full algorithm supports (based on scikit-learn). I can implement deep learning stuff myself. Customization Highly supported. Widgets & addons can be developed. Visualization Not bad. 

As I've never used wrappers for scikit-learn API, I'm not very sure why you have integer output, my best guess is because of and (in scikit-learn APIs, returns integer, basically the predicted class and returns float, indicating the probability of each class). Try to use , it would help. BTW, you should really use . As you have mentioned, there's so many classes to predict. In fact, it's a regression problem. Could you please add the links of these 3 papers? Neural networks having only 1 neuron in output layer seem to be a regression NN to me. Regression NN usually uses as the output layer. 

In this case: validation data = hold out data = Public LB (if my understanding is correct) The first figure of page 12 is the normal condition, we shouldn't check our Public LB to tune our model(or we're training model on it). So the answer is no. The second figure is the small data condition, we have to train model using the feedback from Public LB, because of the data limitaion. So the answer is yes. The third figure shows that Public Data is the same as Private Data, they both represent something under same time period, sharing the same "reality", thus we can use the feedback from Public LB. The forth figure shows that Public Data is reality, and Private Data is more "realistic" than Public Data. It's the "hold out data should be out-of-time" case, the answer is yes. 

Interesting question! I'm assuming you have a bunch of points $(X, Y, Z)$ plotting the lung you want to measure. So your question is actually the measure of a convex hull. Checkout scipy.spatial.ConvexHull. The object has a attribute which indicates the column of the convex hull you want. It supports N dimensions, which your question is about three dimensions. 

Actually it's $49C*C$, the first $C$ is the number of input channels, and the second $C$ is the number of filters. Quote from CS231n: 

Shift humidity for days, which makes Pearson correlation coefficient minimize. Extract frequencies using Fourier Transform etc. Do power spectral density estimation using Welch's method etc. 

$x$ you want is the exact data point you want to predict, in your case, $x$ is a tuple. Assuming you want to find a $f(r, s)$, where $f(roomnum_i, size_i) = price_i, \forall i$. You have a dataset, and an algorithm to fit $f(r,s)$. The most interesting part of locally weighted linear regression is that, the model changes when $x$ changes (keep in mind $x$ is the data point you want to query). Assume $x=(R,S)=(3,30)$, the algorithm becomes: Find $\theta$ to minimize $\sum_{i} exp(-\frac{|x^{(i)}-x|^2}{2\tau^2})(y^{(i)}-\theta^T x^{(i)})$ where $x^{(i)}$ and $y^{(i)}$ is your dataset, $x=(3, 30)$ is the point you want to query. The article shows that clearly: 

Here's my understanding: I think it's the explanation of page 10, and time is the same as the "time" in page 10, which means the time axis in real life. In page 10, it shows that: 

You can try to set different weights on each samples. For example, you can set 0.05 weight on those who bought other products, and 0.95 weight on those who bought product X. In sklearn, there's a parameter which provides this method. In R, checkout this guide. If you're using XGBoost, here is a guide which shows how to deal with imbalanced class in XGBoost. 

I'm confused. As you have said, the targets are ratings. It's definitely a regression problem to me, instead of a classification problem. There's several problems in your code: 

It seems that someone has managed to implement a Poisson sampling in SQL Server. Checkout this link: $URL$ 

When using pandas, try to avoid performing operations in a loop, including , , etc. That's slow! If you want to count the missing values in each column, try: or On the other hand, you can count in each row (which is your question) by: 

If you want to do prediction using 2011 features, the answer is yes, you can do that. However, as you don't want to use these features, the answer might be no. Without using 2011 features, your dataset will have only 2 samples(2009 and 2010) under the assumption that every is different. Prediction from two samples is neither reliable nor feasible. 

10 epoches may be too few. In my practice, 100+ epoches will be applied. I think 100 epoches will be a good start for you. Assuming your data is fully-prepared, some suggestions of the CNN: 

(someone denoted ) means average mAP over different IoU thresholds, from 0.5 to 0.95, step 0.05 (0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95). 

Actually, after you've completed your training, the weights of all these convolution layers are the feature maps you've extracted. You may try to visualize these weights as well as activations to get the full feature maps. Here's a guide you may refer to. References $URL$ $URL$ 

Generally memory cost increases linearly while mini-batch size increases. If batch size 32 costs you 2GB memory, then batch size 64 will cost you 4GB memory. In practice, deep learning framework(for instance, keras) will load the entire full-batch into your memory, thus you cannot observe apparent memory increment or decrement during mini-batch tuning. References $URL$ 

It depends on which algorithm you're using. If you're using tree-based algorithms like random forest, just pass this question. Categorical encoding isn't necessary for tree-based algorithms. For other algorithms like neural network, I suggest trying both method(continuous & categorical). The effect differs between different situations. 

Sample weights change the probability of each sample to be fed into model training. For example, we have 4 samples with weights $1, 2, 3, 4$, it means our model has two times the probability to fit on sample than sample , and 3 times the probability to fit on sample than sample . As you wonder what's the purpose of doing that, let's imagine what our model will become when it's fed with different weights of samples. Example: Durian World If you're born in a country full of durian, when you grow up, the word "fruit" means something smells awful, but we all know that most fruit smells tasty. So is machine learning models, when our model doesn't recognize durian, just feed more durian(increase weight as there are few durian in our life compared to apple or banana). That's the situation called "unbalanced class". When(and if) you hate durian very very much (assume you would die when you smelt durian), but you can't recognize durian well, how to keep you survived? Just feed more durian and non-durian samples, you might loss the capability to distinguish apple from banana, but you shall survive. That's the situation called "cost sensitive". 

You have to specify a Python>=3.4 version (with orange3 itself installed) while installing . I'm not a Mac user, however, in Windows, orange3 installer will automatically install a Python 3.4 if there's no compatible Python available. 

There are several changes between current version of cifar10_cnn.py and the code in your question. However, both version is not that bad (10% accuracy) in my environment. Result of your code: 

In my practice, I am more willing to concern the scoring gap between TRAIN, VALIDATION(DEVELOP) and TEST dataset. Personally speaking, TRAIN>VALIDATION=TEST is better than TRAIN=VALIDATION>TEST. Edit: For class imbalance problem, there are some resources: 

After reading the source code, I find out that the problem is nothing related to your memory, but the limit of integer. In the source code, it shows both of $n\min(2k,n)$ and $\min(2k,n)*(\min(2k,n)+8)$ should be less than , which is $2^{31}-1=2147483647$. In your case, $n\min(2k,n)=36176793968>2147483647$, and $\min(2k,n)*(\min(2k,n)+8)=6781851888>2147483647$. I'm no spark expert nor java export so maybe I can't give you a solution. But personally speaking your dataset is truly big! References $URL$ 

Transfer learning is what you want to need. Checkout this note from CS231n, it provides some general advices on transfer learning and model fine-tuning. In the meanwhile, this blog from keras shows how to use pre-trained VGG16 network to classify dogs&cats.