As shown, women chose CS relatively more often until the peaks, then relatively chose other majors after the peaks. Not a bad thing So women like being medical professionals more than programmers - awesome! As long as people are freely choosing what they want to do, then that's exactly what we should be striving for. 

Follow the rabbit hole down as far as you can go without losing sight of the light of day. Understand where you've ended up, and build a solid foundation there. Build back up 'til you're out of the rabbit hole, standing on more solid ground. Rinse and repeat, iteratively falling down further and building back up more. The more you do this, the greater your understanding grows. 

is actually perfectly legal code! Since it omits a condition and doesn't do anything, it's basically an infinite loop that runs forever without doing anything. In C#, it doesn't consume CPU time, but it also blocks the thread from progressing. For example, 

Date everything Everything should be dated; ideally, you'd prefix all filenames with a time stamp of when they were created (originally - not just when the actual file was transferred/copied/downloaded/etc.). As always, use ISO 8601 for time stamps and other dates. 

They help you trace through your notes to find something that you recall but need to reread. Especially in research, understandings constantly change. 

Definitely, Philosophy is exactly where you want to end up! Anyone who can't trace their field back to Philosophy hasn't understood it so much as memorized it. If you know exactly how to get from basic philosophical principles to the current state-of-the-art, then you can really claim to understand the field. Questions 

Absorb non-electronic notes If you do have non-electronic note materials; e.g., hand-written notes, images, lab book notes, or handouts from a class; if they're potentially important, try to get them on the computer. Scanning them or snapping a picture with your phone can be better-than-nothing. Optical character recognition (OCR) can help make some hand-written notes into electronic copies. It's not particularly reliable or stress-free just yet, but as long as you have something for future OCR to work on, it remains an increasingly useful tool. 

Using physical addresses is misleading - it would be clearer to use an email address, or a phone number. If people still had POTS (telephone) in the house, you have segmented (area code) dynamic addresses, and 'mum, dad, youngest child etc.' which could represent ports - which are ways of reaching the same type of person in each house. With this analogy, you can explain that people's phone number sometimes changes (but doesn't have to, even if they move location), and there are many different ways of addressing a set of people who might be in a house. Depending on who is at home, a different person might actually handle any specific call. When someone places an outgoing call, you'd see which house the call was coming from, but not who (until they identify themselves). 

In the context of an undergraduate course on microprocessors and architecture, I'm looking at how to introduce a VGA display peripheral. This is with the specific goal that they implement the peripheral in verilog on an FPGA (they're given a microprocessor and the rest of the system). It's VGA because of the current hardware used for the supporting labs - and I kind of like the example peripheral because it has a useful set of physical layer constraints. An LCD interface might seem like a more relevant, but then we're looking at a SPI peripheral or similar, and that defeats the object of the module. The VGA physical layer is clearly an evolution of analogue TV and CRT technology, so it's quite tempting to start the description of how pixels need to map onto the screen by talking about the CRT and how an electron beam scans across the phosphor, line by line. My question: how relevant is this analogy going to be to current school leavers, and will it work as a starting point for describing horizontal and vertical sync? I've thought of trying to update the module by describing how an LCD panel would de-serialise the VGA signal, or switch to another output interface altogether, but it seems that the example I have at the moment might be worth keeping alive for a few years longer. 

I think there are two phases here: the first is just being aware of what the correct idioms and best practices are, and second is attempting to apply what you learned and getting feedback. For the first phase, you can get a lot of mileage by doing research and casting a wide net. In particular, I personally find the following Google queries to be pretty useful at giving me an overview of the best practices and general "feel" of a language: 

Something I've often observed is that many students find it challenging to figure out how to use full-fledged IDEs such as Eclipse, Netbeans, Visual Studios, IntelliJ, and PyCharm. From what I can tell, this is due to a variety of factors: 

Same sort of thing as before -- if you're a busy student, then it makes sense that you'd do only the bare minimum needed to pass the class. In particular, the students probably aren't thinking of the work they need to turn in as the "bare minimum": they're probably thinking they're turning in exactly what's expected. In that case, one thing you can do is to raise your expectations. Make the assignments harder, or (if there's room for creativity on the assignments) show students examples of good projects other students have completed in the past. Maybe tell students that they're required to complete at least one item from the "above and beyond" section (though they're free to chose which one they want to do). You can also perhaps try motivating the students in other ways by explicitly telling your students that the purpose of this class is to help them build a portfolio they can show employers so they should put their best foot forward. Making the assignments more interesting, as ctrl-alt-delor suggests, would also help. Making your assignments group-work might also help (and help partially compensate for the added difficulty). 

Speak the language, e.g. know about quantum computing terminology. Know the background, e.g. quantum physics and computational theory. 

Lots of profiling. For all anyone knows, that random gibberish full of typos and conspiracy theories on Reddit is the deepest, most correct theory of physics ever. But, since it's probably not and there's only so much time in the day, your bets are better hedged by focusing on content that appears to be of higher quality. To that end, you should develop a bunch of prejudices over what's likely to be better content, then selectively focus on it. Sure, if you're truly out of stuff to read, then maybe you'll end up going through that weird Reddit post; or, if you've got too much good stuff to read, you might not even be able to make time for Einstein's words. It's all relative. Examples of common prejudices: 

Optimal note-taking depends on the goal. For example, if all you want is to pass a quiz next week, then a quick, flimsy style's more appropriate. Here, I'll write about taking notes that a life-long learner intends to keep-and-extend into life as a researcher. Always typed; never written Electronic notes are vastly superior to hand-written notes because: 

They never are. Whenever you read a textbook - even the best-known ones in well-developed fields, e.g. introductory Calculus - never just trust it. Put each word on trial, and absorb only the ones that you can't tear down. You often need to memorize large amounts of content without first thoroughly scrutinizing it. All of this content should carry a red flag in your head that labels it as unscrutinized claims. If there should come a time where you really need that information, or want to build upon it, you should really scrutinize it first. 

Something I wasn't sure about when I was reading your post was if the problem is whether the issue is if students lack motivation or if they lack the ability to succeed in your course. If 30% of your students are consistently failing your course, then I genuinely do have to wonder if the latter is the case. If so, I'm not sure this is a problem you can easily fix within just within the context of your course. For example, I'm assuming the students are expected to complete a few pre-reqs before they can take your course. Why aren't those pre-reqs adequately preparing your students? Etc. If the students are genuinely under-prepared and have "given up" and are just going through the motions so they can graduate, then the best fix is probably to try and systematically repair your entire department. Perhaps that might mean finding more ways to add in support (especially in the intro courses), making sure students know where to find support, raising the bar on core classes... It could also be the case that the pre-reqs listed for your course aren't actually enough, and that you need to add another pre-req class or two to make sure your students are prepared. 

Details: Changes in Java 10 The changes landed in Java 10 are comparatively smaller (which makes sense -- Java 9 was released about two years after Java 8; Java 10 was released about 6 months after Java 9). You can find a full list of changes at the following two documents: 

Version control isn't that challenging to learn (assuming you know how to use the command line). It's something you can trivially understand by following a few tutorials online -- I'd argue the hardest part is actually finding the right tutorial (there are tons of garbage "how to use Git" tutorials out there, and surprisingly few good ones, for example). Despite that, it can be hard to find time to fit in the command line and git. You'll probably need to spend at least 30 minutes talking about both things, and if you have a lot of material you need to cover, it can be hard to squeeze that in without compromising on something else. In particular, it's worth noting that you can't necessarily just add in new material somewhere and shift everything over. You need to make sure your change maintains the overall tempo of the course, make sure no topic "overlaps" oddly over a weekend or a holiday, need to make sure students are still being taught the material they need a sufficient amount of time before you make related HW due... If the original course was competently designed, it's probably a well-tuned engine where every minute matters, so adjusting it will require some degree of effort. If your university didn't find time to squeeze in version control, you'll be told to learn it on at least day 1 of your first internship or job. (And they'll probably do a better job of teaching it! It's hard to teach the value of things/give students the opportunity to practice things like branching, rebasing (if you're using git), using specific workflows, etc. in a university setting -- those sorts of more advanced operations are really only useful in larger teams working on a long-term project.) You mentioned that your intro course covered git on day 1. I can see an instructor considering against that approach if they want to prioritize moving to programming as soon as possible to try and "hook" the students in with instant gratification. (If you're completing new to programming, I can imagine learning to use tools like version control would be relatively dull and sort of a bother -- you don't really get to see the payoff until you start tackling more complex projects later on.) 

The big disadvantage seems to be that you're re-enforcing their (or maybe your own) preconceptions about barriers to entry, and taking them further from the 'normal' programming environment. Yes, you can twist a spreadsheet to demonstrate some topics, but the biggest misconception you will teach by starting like this is 'everything executes in parallel'. Having moved from coding software to verilog (hardware) I can vouch for this being a major mindset change. You're also teaching a non-flat code layout, have zero scope for sensible comments, and an unusual syntax. A better approach is probably to find an online simulator (i.e. a web browser, which everyone has or can access in many countries). These will handle the code storage (in the browser or in the cloud) and avoids any assumptions about how familiar they are with specific types of software. I really think you're better identifying this as a new journey, and not expecting your cohort to have anything installed on their machines. Having supported a public drop-in class, I'd also opt for an environment which allows both 'block' coding or text based views like the micro:bit javascript blocks editor. This removes the barrier of needing to remember syntax and seems just as relevant with older introductory level students as very young ones. Once you have introduced the fundamentals (keeping the first contact easy and low stress), you can move to using text based entry. Returning to the micro:bit example, I think having a physical device is less important with an older cohort (particularly if they are in your class by choice). It gives you scope to use a demo, and show some context about how close to invisible/trivial computers are (something else that your spreadsheet doesn't expose). To summarise my objections to the proposed approach: 

Test to see if students understand what implementation detail is -- that is, do the recognize when something "breaks" an abstraction? For example, you could give students a description of a car, and ask them to identify from a list which element is implementation detail. (For example, "hitting the brakes causes a moving car to slow down or stop" is part of the interface; "a combustion engine powers the car" or "the car moves via wheels" would both be implementation detail). If you want something more directly related to code, give them a header comment for some method or class, and ask them to do the same. Test to see if students understand how to find similarities and find a viable abstraction between multiple items. (After all, if you have only one of something, why would you bother abstracting it?) For example, you could give students a description of several different queue-like objects (a FIFO queue, a priority queue, etc) and ask them which interface description would work for all of them. (This probably isn't the best example, but hopefully you get the idea). I would also add an option that says "none of these are good abstractions" and make that the right answer some of the time. After all, if students are taught that they should always be abstracting, then you'll end up with a bunch of architecture astronauts, which seems suboptimal. Draw inspiration from category theory. After all, there's a argument to be made that constructs from category theory and design patterns are more or less the same thing (except that category theory is applicable to things beyond code and design patterns are more handwavy). For example, you could perhaps describe something like monoid (e.g. if the set M is a monoid, then it must be the case that that the set is closed under some associative binary operation and there must exist some identity element I in the set such that A op I = A and I op A = A). You could then explain how strings + string concatenation form a monoid as a case example (the empty string is the identity; concatenation is the associative binary operation), then ask them to correctly identify how lists or numbers or something are a monoid. You could also ask them which entry from a list wouldn't count as a monoid. This basically lets you test if students can understand how to use some (relatively basic) abstraction/adapt existing code to work with unfamiliar abstractions. (And tests to see if students know how to adapt to dealing with new definitions and such). Give students a snippet of code that breaks an abstraction, and ask them to identify the correct reason why the code is wrong (and probably add an option for "no, everything's fine" to make it a little harder.) 

tl;dr- A loop is just a loop that has extra slots for a pre-loop statement and a post-loop-run statement. Since they're pretty much the same thing just written differently, a loop is best understood as syntactic sugar that's useful when it increases the readability of the code. Sometimes new programmers are worried about performance, thinking that syntactic sugar might be less efficient. That's not the case; the compiler really doesn't care how you write these loops, it's all the same to it. So, students should be told to just write whichever's cleaner in the current context. Homework/test question idea: Have students convert a loop into an equivalent loop, or a loop into an equivalent loop. Mapping between and The loop: 

From a systems approach, capacity issues do make some sense for explaining a peak like the one here. The rapid inflow of students could be allowed since they were filling available capacity; then, once that capacity was taxed and admissions was allowing too many students into the classroom, they'd have had to cut back on allowing new students in. Helps explain the difference in interest-vs.-achievement The mid-1980's peak is the basically one place where Freshman interest is higher than degree completion. 

Once you have some basis for understanding the discussion, then you focus on getting your own ideas straight. In the end, it's really all about you; sharing research results really isn't a goal so much as a duty to be performed when/if you become the foremost authority on some particular understanding. If you're a younger student, you've probably got a lot to learn and develop before you can really enter that arena. But, if you're interested in it anyway, it seems like you could do two things: 

Seems like a developing field is a discussion that a community of researchers engages in. To participate, you'd need to: