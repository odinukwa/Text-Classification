3) The Binding is added to the spacial index inside of the level at the appropriate point. When you are preparing to render. 1) Get the camera (It will just be another object, except it's location will be tracking the players character and your renderer will have a special reference to it, in fact that's all it really needs). 2) Get the camera's SpacialBinding. 3) Get the spacial index from the binding. 4) Query the objects that are (possibly) visible to the camera. 5A) You need to have the visual information processed. Textures uploaded to the GPU and so on. This would be best done in advance (such as on level load) but perhaps could be done at runtime (for an open world, you could load stuff when you are nearing a chunk but should still be done in advance). 5B) Optionally build a cached render tree, if you want to depth/material sort or keep track of nearby objects the might be visible at a later time. Otherwise you can just query the spacial index everytime it will depend on your game/performance requirements. Your renderer will likely need a RenderBinding object that will link between the Object, the coordinates 

This will depend a lot on what you want to acomplish and the type of game you have. You would have a 'game world' time that advances based on the cpu clock. Generally at a much faster pace so it can be seen. And you will need to take into account pausing, alt-tabbing and so on. You can also advance it manually such as when the player sleeps, uses some kind of fast travel system (a less simple one would be based on the distance travelled) or from mission to mission. More complex game time keeping systems might keep a record of the date and potentially even have their own calendar systems (Like the Elder Scrolls series). This could influence weather and so on. You need to adjust your visuals based on the game time. This normally involves lighting and the sky. For a simple system, outside will normally have a light that represents the sun. You would need to adjust the brightness/color based on the time of the day and set it to a much darker color at night. And the sky texture itself will need to switch between day, night and dawn/dusk textures (preferably with some kind of smooth fade). More advanced visual systems will track a path for the sun and moon across the sky. A simpler system many just put them at polar opposites to each other and have them go directly overhead, east to west. In reality both bodies have their own paths. The moon can be up during the day at the same time the sun is. The moon has cycles. And both will often not be directly over head but on an arc the angle of which changes depending on the time of the year. The stars them selves will change slightly too. Skyrim was even more advanced and had 3 moons. The original Theif games actually had star charts rather than just using a texture. You will also want to use the suns position for shadows is you have realtime ones. 

I am a mac developer using Unity and I hardly use a PC. When you build a Unity game for Windows, does it use Direct3D or OpenGL? P.S. I'm not sure if it's called Direct3D or DirectX 

I'm following the GPU Gems 3 chapter 1 (Generating Complex Procedural Terrains Using the GPU). The main difference is that I'm using the CPU for generating the mesh(es) based off of marching cubes. And I have this problem that I'm not sure how to solve or even debug. In Figure 1-8, they start adding noise to the plane at y=0: 

Yes. I've used box2d before and right now I'm using unity. I'd say it's just as fast box2d and good enough for Angry Birds. 

I managed to get the code working in unity. But I think I might have done something wrong. When I turn up the , All it does is add more vertices and polygons without moving them around. Did I forget something? 

It's not that easy. Since you want it 3d, you will have to make your whole game with OpenGL ES, not just simple 2d cocoa touch frameworks. But before you re-make your game with OpenGL, you will have to learn 3d modeling, animation, cloth simulation, etc. A word for the wise: start small. I suggest you start with 3d modeling. Check out blender. It's free and open-source. 

After doing some research, I found out about the marching cubes algorithm and "metaballs". How exactly do they differ? 

The short answer is it depends, or use a bit of both. Now days I don't think you even really do textures on objects, instead you do 'materials' or 'effects' on objects. A material can have multiple textures (1 or more basic image textures possibly with transparency, another for normals/bump mapping, another for specular reflections (The shiny part of metal will reflect light more than the scratched dirty bits) another texture for parts that should 'glow'. Another for ambient light (although that will often be the glow texture). They will have inputs (so you can swap between multiple different textures for the 1 material, modify the colour (maybe depending on the team the model is on), change animation speeds. You're likely to use bother depending on what kind of object you are dealing with. And you can also do hybrids between 1 texture over the whole object and individual textures and so on. If you are talking about limited hardware (mobile/web). Or a game with simpler graphics (although in that case optimizing is less important then anyway), I think it really depends. Firstly, how close are you going to get to the object. Something like a house will need multiple textures if you are going to be walking up close to the walls, standing on the ceiling etc... but if it's a top down game where the house never gets closer than a specific distance then it's fine to use 1 texture. Maybe you use both methods depending on which LOD model is loaded. It will depend on the kinds of objects you are going to be using. Some more 'organic' things can only really be done with 1 big texture (think of a tree trunk). Of course something like a face will It will also depend on how many objects you are going to be seeing on the screen at once. If you have a RTS with armies of people vs a smaller number. For the point of view of which is easiest to implement programmer wise, there about the same. In fact if you implement the multiple texture system, that will implement the single texture option. It should also be possible for you to use multiple different materials/textures on one continuous mesh via extra vertex attributes. One of those can be a texture id. This is much easier on modern OpenGL where you can share uniform variables between shaders (I don't know about DirectX but I assume it's possible there too). Of course if you have different properties for your shaders then this will be harder. It also takes more memory. And you have to make the meshes have that metadata (although that can be done automatically). It's also about the content creation pipeline rather than the programming side of things. Are you (or your artist) happier making a model, unwrapping the mesh then paining on it. This will mean each model is individually textured and unique, kind of hand crafted. A Blizzard game might do something like this, it allows fast creation of simple content, that's low performance, just knock together single mesh models then paint on them with a tablet. They use a simple cartoonish painting style. Or are you likely to share textures between various models. A game that uses tiles might want the stone brick texture on 90% of them but mix in a few extra bits on most of them. And unwrapping tile objects is going to be harder in may ways due to the geometry. 

is a 3-dimensional float array of pink (Perlin) noise whose mean is 0. It has a frequency of 2 and 3 octaves with half persistence. Its standard deviation is ~ 0.2. And it varies around Â±0.65. The isolevel of the marching cubes algorithm is set to 0. What could be causing this undesired effect? 

What would be the easiest way to make a bump map of the dimples on a citrus fruit such as an orange or lemon in gimp/photoshop without using pictures of it? e.g: 

How did they make the terrain in trine? I think they took a polygon approach. It doesn't seem that they used a heightmap because it is in all three vectors, or a voxel method because it doesn't have overhangs and cliffs. If it is polygon-based (I assume it is), how in the world would you texture the whole thing? 

Here is a good tutorial. He used an empty transform to control the position and scale of the projection. 

I am currently working on a voxel terrain that uses the marching cubes algorithm for polygonizing the scalar field of voxels. I am using a triplanar texturing shader for texturing. say I have a grass texture set to the Y axis and a dirt texture for both the X and Z axes. Now, when my player digs downwards, it still appears as grass. How would I make it to appear as dirt? I have been thinking about this for a while, and the only thing I can think of to make this effect, would be to mark vertices that have been dug with a certain vertex color. When it has that vertex color, the shader would apply that dirt texture to the vertices marked. Is there a better method? 

The problem with games is they need things to be done in a specific order. Physics must be done before graphics because otherwise what would be the point of drawing the picture if nothing changed. Scripting needs to be done before physics since if you move an object the physic engine needs to process it. The only alternative I could think of would be a distributed Actor model. Basically each object is given control of it's own resources and everything must talk to it through some communication channel such as a message que. It could be done with shared memory across threads, over IPC for separate processes or networked for separate systems. It can be massively parallel (each object could potentially have it's own thread). There is all kinds of fancy shenanigans you can pull, your objects could be on different servers, with IPv6 you could even give each object it's own ip address, of course it makes sense to keep objects likely to interact together on a thread to cut out overhead but maybe each level/chunk has it's own server process and the player is transferred from process to process. That would result in a lot of overhead. But for some game types it might be ok, a SNES style MMO JRPG might be fine but not a FPS. Physics might be kind of complex in such a situation. It would be non-deterministic since you can guarantee the order of operations, that would make saving/loading/replays unstable and mean more network overhead since you can't just transfer the state and let the client simulate it. But not everything needs physics other than basic collisions. Although even that can still have issues, if you step into a space at the same time as another character, who gets right of way? Different systems might see a different result depending on where they are located. Rendering the graphics would be a pain since it would have to 'ask' each object to report its position each time (or request a subscription to be told when the location of any visible objects is changed and also request to be informed when a object becomes visible to the camera or is created). This could end up with a lot of chatter. Another idea is an event loop. Probably not as a replacement for a main game loop but more for things like resource management. Rather than loading a directory of textures you could spawn a thread and tell it to 'watch' that directory. Any time it sees a file that hasn't been loaded it can spawn off another thread to handle the I/O. It can return 'dummy' objects for placeholders that then load the real object into place when ready. A nice feature is your game can be edited live. It can watch the filesystem for changes and when you edit a texture the game can pull it in straight away. You could also have things 'pop' into existence which might be useful for something like a virtual world (ie Second Life). Or you could have low detail ones that then resolve into clearer images. 

Check out Unity3d. It compiles to Mac, PC, Web, iOS, Android, Wii, XBox, and PS3. It doesn't use java at all, but it has a visual editor, three scripting languages (javascript, c#, and python), physics, particle systems, image effects, 3d sound, networking, and more. 

Part 1 (doesn't work!) When you buy the game, they give you serial codes (the old school, serial codes). Obviously, only after you input the codes, you can play the game. Now, there's a problem with this method. The ever wretched crack. If you can crack it you can play it. Part 2 (works!) Now that the hacker cracked the game, they can play it, but only to a certain degree. You can't play online without having to re-input your serial codes to make a username & password. But here's the gist, it contacts the servers and checks if the serial code has been used yet! If AOE III would have used "Part 2" from the beginning of installation, the problem would be solved! No playing the game without contacting the servers. This of course can be a slight problem to some players that don't have internet. 

Figured it out. What you have to do is raycast from your point in any direction you want. Then, count how many times the raycast intersects with your mesh. If the count is an odd number, it is inside of the mesh. If it is even it is outside of it. Since unity doesn't raycast on inside sides, you have to flip the faces in your 3d authoring tool. Then you have to raycast in the opposite direction to make up for the other side. (I haven't figured out how to get the count of hits from the raycast yet. But I am using a cube right now, which only has 2 perpendicular faces, so it works) Unity C# code: 

These are probably fairly different. SDL draws graphics (at a fairly low level) and handles input but things like asset loading and game loops will be your stuff, in particular for 3D graphics you will be looking at raw OpenGL. You could have a look at MonoGame for an XNA clone. Unity builds for Linux but I'm not sure that the tools work on it. SFML would be an alternative to SDL. 

This will depend a lot on your game. Counter-Strike, or any source engine game or any Quake derivative, and most other FPSs will have custom editing tools for the actual act of crating a level. That tool will bake the specified static meshes into the format by applying some algorithm. That format will be a spacial index. In the example case, BSP stands for binary space partition. You will basically be separating (and in some cases splitting) polygons into groups based on their location so it will be quick to query what polygons are visible from a location in a direction. Now days there will be various BSP implementations of with different optimizations since the initial BSP design from Quake. Now days you can probably scrap custom editing tools for a general purpose 3D editor and just parse the file it outputs. In addition to that you will have dynamic objects that can move about that won't work in a BSP (the structure just doesn't support making alterations at runtime with any decent levels of efficiency). The objects will just link their position to their object informations (such as the 3D mesh to draw at the location). BSPs date back to the original Quake engine. Now days it might be a better idea to use an object based approach rather than a giant polygon mesh. For example Skyrim uses a modular level design. Basically rather than keeping everything as a giant mesh of polygons, you will just have a list of objects and their positions, the actual mesh will be stored separately. BSP isn't the only spacial index around. It's good for indoor environments such as FPS shooters. For an outdoor game like GTA you might do better with an Octree (or Quadtree for flatter game worlds). Those 2 are able to be used for both lowlevel polygon level storage or higher level object level storage (off hand I'm not sure if BSP would be useful for objects). In addition to the raw mesh data, there are extra things you can bake in. For example if you know your mesh is static and your lights are static then you can prerender the lights into a lightmap. Basically it's an extra texture that is applied to the polygons of the level for light the lights brightness so you don't have to do it at runtime. You increase your memory but save runtime processing. You could even look at hooking up a whole raytracer so you get raytrace quality lighting (although the generation would be very slow, but the resulting quality would be great). 

I am trying to write an ice shader in Unity that looks good and at least semi-realistic. If the following shot (found on Google) was CG, what would its shader include? (the foreground cave). I might be wrong but it looks like it even has a different lighting model than the default diffuse. 

Does anyone have an algorithm for creating a sphere proceduraly with amount of latitude lines, amount of longitude lines, and a radius of ? I need it to work with Unity, so the vertex positions need to be defined and then, the triangles defined via indexes (more info). 

How can I find if a point (Vector3) is inside of a mesh? Would this work for both concave and convex objects? I read somewhere that if you raycast in both directions of every axis (X, -X, Y, -Y, Z, -Z), take the count of the hits, and if it is even it is outside, if it is odd it is inside. I tried this and it didn't work. 

Is it possible to append a blur shader to a standard (diffuse) shader ? I am looking for a way to do this as Unity indie doesn't allow render textures. 

I've been researching processors and graphics cards, and I discovered that GPUs are way faster than CPUs. I read in this one article, a 2-year-old Nvidia GPU outperformed a 3.2GHz Core I7 Intel processor by 14 times in certain circumstances. If GPUs are that fast, why don't developers use them for every function in a game? Is it possible for GPUs to do anything other than graphics? 

An alternative idea would be to include a Depth/Z Buffer image with your sprites similar to how 3D APIs (OpenGL) ensure polygons don't overlap. For a basic sprite game it could just be a bunch of 0/1 bits (black and white bitmap) (although you could use more levels of depth if you want). If you have a combined 'deskchair' sprite and want the chair to appear in front of any other sprites (ie people) on that square you would make the Z-Buffer version of the char 'black' while the desk would be 'white'. When you render the dynamic sprite (a person) on that tile it would check to see what color is in the depth buffer. For pixels that are 0 (black) you skip drawing the person sprites pixels and just leave the chair ones there. Rather than check every pixel 1 at a time, a simpler way to do the check would be to multiply the sprite pixels by the depth buffer value since the closer depth would be 0 and cancel out the. Then of course you have to colorkey (unless you have a alpha color channel). This of course adds overhead (although you could skip it for anything that doesn't need multiple depth levels like a deskchair). You also need to sync your animations (ie the person would have to match up with the desk chair anim). It doesn't give you the same flexibility of just having a whole bunch of separate sprites.