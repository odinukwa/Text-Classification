The client specs call for PoE+ / at for a reason - it needs to pull more than the 15.4W that an af switch is supposed to provide. If things go wrong you could be looking at damaged equipment. Even if it comes up you're pushing your switch past specified tolerances, which pretty much means a postponed failure. If you want to run PoE+ on a PoE switch then get a PoE+ injector. 

1W is the limit at the transmitter, but higher EIRP amounts are allowed for certain kinds of antennae. A 6 dBi multipoint antenna connected to a 30dBm (1W) source yields 4W EIRP - which is within the legal limit. Much higher limits are specified for point-to-point setups. The question isn't so much the claim of 2W but rather where- and how- that 2W is measured. 

Physical locations don't correlate cleanly with network locations. One carrier might take you from Oregon to Kansas via a single POP on each side and a dedicated circuit that happens to physically run directly between the two sites while another carrier might hand off to an intermediary transit provider that backhauls the traffic to Virginia to hand off to a carrier that routes through Texas to go back up to Kansas. Best of all, either path could be valid at a given time depending on fault and traffic conditions. 

On the commercial side there's Cisco Security Manager that can handle ACL's on IOS boxes, ASA, etc. There's a 90 day free eval and it runs in a VM. That might be worth looking at. There's also fwbuilder that offers multi-platform ACL management (including IOS), but I haven't spent much time with it. 

Start with the basics - you've got two different series of machines that likely have two different series of NIC's. Are both sides set for autonegotiation and, if so, are they agreeing on the appropriate speed? Try hard-coding both sides as an experiment to see if it improves at all (..or if it's hard-coded on either side currently then let both sides negotiate). 

Worth noting - to catch the private address range (RFC1918) you probably want 192.168.0.0/16, -not- /8. 

The switches don't necessarily have to be physically connected to one another but the ports connecting the bond members need to be in the same broadcast domain (usually synonymous with VLAN). Keep in mind that the IP for the interface can potentially show up on either switch. If the connected ports are discontiguous how would the rest of the network know where to send packets? So - say, for example, you had a common pair of aggregation switches where the L3 gateway resides and runs HSRP/VRRP. Two access switches independently connect to these aggregation switches and are passed the same VLAN. This would be fine. In contrast, if you hook up to two random switches that each are configured with the same subnet but are otherwise disconnected then it's not only not going to work during failover but would likely be broken under normal circumstances as the rest of the network has no way of knowing which network is currently active vs passive (unless you start dealing with custom tuning metrics and dynamically signaling state somehow - which is definitely gilding the lily for simple NIC failover). 

There is a particular problem with cooling gear in small spaces: airflow. If you can simply cycle a decent volume of outside air through the overall enclosure (or put the gear in a much larger overall room) then the actual amount of refrigeration required is substantially reduced. It is a fairly common trend now to use fresh air cooling in large data centers (Intel was an early adopter in NM). Focus on moving lots of air (even if it is warm outside) through your racks and into your cold aisle and then on pulling as much air as possible out of the hot aisle. The gear will be somewhat warmer but still well within rated norms if appropriately designed. A lot of $$$ can be saved both on chillers and the electricity to run them.. 

There isn't really an accepted standard for icons. The Cisco icons themselves have changed substantially over the years based on rebranding, acquisitions, new product lines, etc (and will absolutely change again). Being consistent with icon usage, including legends and clearly labeling are each as- or more- important than the icon itself. A well thought out selection of boxes and circles with appropriate labels and some kind of consistent flow is infinitely more useful than a jumbled collection of images from a stencil. 

The 80% rule is there for a reason. The best case overloading a circuit is going to yield circuit breakers popping, the worst a fire. That said, there's a certain art to estimating how much capacity you're actually using (short of a clamp probe or similar). At first blush most folks add up the wattage of each piece of gear being plugged into a circuit and compare it to the 80% value... So 8 servers with 300W power supplies = 2400W = 20A @ 120V which, of course, is a no-go unless you've wired in a 30A circuit. The thing is, though, that the plate ratings on equipment are generally the maximum rating. In practice most equipment never even approaches this number - so those 300W servers might pull, say, 200W momentarily at startup when drives are spinning up and such and then drop back to 80W when fans slow down, speedstep (or equivalent) kicks in, etc. For a modular network device the number may assume a full complement of ports all running optics with the highest power demand - which is also unrealistic. Various vendors will publish so-called "typical" power draws, but these are often marketing numbers (...a quick way to appear more efficient). Other vendors may be a lot more conservative and publish a more realistic estimate. The only way to truly know what's going on is to hook up some kind of meter to get an actual empirical view of the power being drawn. Sources might include something as simple as a Kill-A-Watt to clamp on probes to smart PDU's providing graphed measurement on a per-socket basis. So... if you want to be extremely conservative (not a terrible thing) add up the plate ratings until you get to 80%. Some electricians make recommendations around dropping 30% off of the plate rating of equipment, but I've always suspected they were working from experience with motors and such rather than electronic equipment. This is probably fine and could save you some pole capacity but ultimately you shouldn't take anyone's word for it. Measure the consumption of actual gear under something like actual conditions and then plan accordingly. Remember: Being careful will cost you an extra circuit breaker, or two. Not being careful can cost a whole lot more. 

While dd can be a useful tool it's also important to remember the possibility that you're testing a file that has already been cached. In NFS environments we've gone so far as to unmount and remount partitions between test iterations to make sure we were definitively hitting the server rather than relying on something local. 

The 4548 line cards have just hit end of life, but have on the order of 5 more years of supportability. The 4306 blades hit EOL in 2010, so have about 3 years of support left. As syneticon-dj points out, you need a supervisor intended for L3 applications (i.e. 6E, etc). Simply replacing the supervisors will allow you to support L3 and OSPF. This should be considerably less expensive than replacing the entire switch and leaves you a simple fallback.