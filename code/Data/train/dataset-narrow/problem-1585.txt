We have an FMS Origin server that is outputting HDS(Also RTMP but we don't want to distribute this). We want to put Vanish on another box and have it act as an intermediary for our CDNs so their edge servers don't hammer our Origin. Documentation is lack 

There are a lot of facets to this question. Firstly, how are you defining 'squatting'? Maybe they're just parking it. Did they set out to poach the domain in an attempt to cash in on it when your group/company/project needed it? If they're TRULY squatting, you have some legal options you may want to explore. Barring that, don't make the first offer. You'll either set it WAY too low and be working hard to recover, or WAY too high and they got what they want. Secondly, what is your plan if you don't secure the domain? Have you considered alternative domains or are you tied to this? If tied, see my first point. Legal options might not be terrible but it'll probably be cheaper to buy them off. Let me know how it goes. 

Any answer you'll get here is purely anecdotal. I prefer Debian or Ubuntu. Make sure you don't use the 64 bit version if you're using less than 4Gb of ram. Also, check out Linode for a better deal(assuming you're using Slicehost). 

I'm assuming you mean block access for incoming users and not for YOUR users connecting to the site(external)? If the former: Look into apache mod_rewrite. That'll do what you want. If you're not using apache, you may need to consult with someone else. If the latter: Look into setting up a proxy server. It'll depend on what OS you're using. 

Sounds like your ssh config is blocking remote access for root and su is probably doing something similar locally. Double check all your configs. 

The cluster gives you a little more power and some redundancy(if it's feasible for your software). The big 'workstation' has the advantage of being simpler to deploy and won't be bottlenecked by your switch. I can't say for certain if the switch will bottleneck since it will depend on your transfer sizes, etc. 

You should probably clarify what web server you're using but I'll assume Apache. If so, look into vhost files. What you want to do is very common and quite easy. $URL$ 

I just spun up two EC2 instances and got an elastic IP for each. I can't seem to get it to connect. Here is the bulk of my config: 

While the AZ is fixed over the lifetime of the gear, it is quasi-unique. Those AZs are randomized on a per account basis so your us-east-1b might be my us-east-1d for example. It is arbitrary and not worth worrying about. 

I have been offered a job as an 'engineer' for a large tech company - already working as a systems admin for the government. I wanted your take on how you step into a new job and learn how all the pieces work together. They have approximately 2 racks of gear and some very complicated relationships. What is your strategy for getting a grasp on things? Any preferred tools/methods? 

KVM sounds like it's the best option in this case. We use ESXi here but that's overkill. Xen can be tricky to setup while KVM is relatively simple and stable. I've seen it used in production environments. 

I've setup IPSEC tunnels between 3 management VPCs in 3 distinct AWS regions. Each of those regions has additional VPCs (dev/prod) that are peered to the management VPCs. It's setup in a hub/spoke like this: 

Why not run a standalone system for MySQL, like a VPS. I used to run my dev and production servers on a couple of these, with replicated MySQL DBs on separate nodes. 

You're going to be doing something called bonding. Check out this article for the specifics. One term to research NLB-Network Load Balancing for Windows. 

I have a list of values (ipOutList). I want to repeat a step N number of times, in parallel based on the size of that list. Can someone show me what I'm missing in this process? Thanks! 

I've seen this done before. It's only as insecure as your network/destination servers make it. Only you know that. Are you transmitting these over a secure network? If so, you SHOULD be fine. But we can't possibly guarantee that. Why not write a simple ssh script to distribute them? That's what I would recommend. Or write a script to download the cert from a central server and distribute the script via puppet. Just an idea. EDIT: Since there is some confusion. I'm NOT saying Puppet/SSH are anymore secure. But if you're worried about unauthorized access, ensure everything is secure. This is most easily done with a custom SSH script YOU distribute. 

Moving those files shouldn't be an issue. This article is what I used. Its a little sparse on details but it will basically let you mount Amazon S3 to migrate those files. Best of luck! 

First of all, check out Denyhosts. Add his(and your IP) to it and that will prevent others from connecting. Also, set 'AllowUser hisname yourname in sshd_config. I wouldn't recommend ftp. Check out sftp. If you're using ssh already, you might as well use sftp too. Blocking http is going to be trickier. Check out .htaccess but even that isn't ideal. Most of this is for naught if you're using dynamic ips(most home connections). 

You can probably leave them alone in respects to response time. However, it can't hurt to disable them if you know how. Also, consider looking at your apache and mysql configs. That could be part of the problem if you have some nasty queries running. 

I would actually recommend a VPS if you're willing to take on the technical requirements. A VPS gives you root access to a server of your choosing. AWS and Linode are two great options(AWS is free for a micro). If you're not technically inclined enough to take on a server; look for a host that offers php/mysql(most do) and that should do you. I would shy away from a hosting provider that offers anything 'unlimited' though. 

Can I offer what may seem like a silly idea? Check the type of cable they're using. Also, check the cabling on that switch(unless it's in common use by others already). I'm saying all this because it sounds like a cable issue. Have the user try another cable too and another switch(if possible). 

You haven't setup a standard user. As such, you're connecting with root. Consider adding a regular user and specifying that users login credentials when connecting. 

I wouldn't recommend it. There ARE still methods of cracking these quite easily. I personally recommend a Truecrypt volume that contains a Keepass database. It servers me well and is extremely portable. And I'm using it in an environment with thousands of passwords. EDIT: And Keepass is already well laid out for password management. With a nice GUI(i.e., easy to see what password is which type) and built-in password generators...can't go wrong. 

This is very doable and a great idea if you have the server hardware but not the routing stuff and don't want to make the full switch to AWS. Check out this guide. Just tweak some of it to your needs/what you already have. 

This is standard practice. You wouldn't want your users trying to submit changes to a database that is being backed up. Also, stop doing backups during production hours. Schedule them for after-hours or something. It's very poor practice to do otherwise. 

I think you're going about this the wrong way. Look into tuning Apache first of all. Then, research Linux memory management. You WANT your server using the ram, otherwise why do you have it? 

I would recommend adding this code to your script $URL$ Or use screen. Edit: This is a little more modern but I haven't used it(I use something custom) $URL$ 

You could try: ps aux | grep -v grep | grep program name That'll check the running processes, grep for the program name, but exclude the grep itself. 

Regardless of whether you can reduce the overhead, I would recommend replication of MySQl to a secondary server. With some load-balancers, you can dramatically decrease your downtime AND ease the load on the server. Just a thought. Message me if you want some guidance on setting up replication. 

Check out webmin It should allow you to do a lot of this. There are some great guides on configuring it. 

You've really over complicated your configs. First of all. Look into $URL$ Then, trim down those files to the bare minimum. Make sure to activate the sites and ensure the directories exist. That should do the trick. Also, don't forget to reload apache after all of that. 

I'd suggest using Google apps for this and just setting up forwarding. If you insist on setting up a mail server, check out this guide: $URL$ 

Slicehost vs. Amazon EC2 is not really fair. I'd check out Linode vs. Amazon EC2. Anyways, my company recently decided to use Linode due to the simplicity of use and the appeal of better I/O rates. Amazon is better if your requirements are minimal or excessive in any sense. Linode/Slicehost win out if you need quality I/O rates. There are more advantages/disadvantages but I still prefer Linode. 

Without getting into too much detail, why not off-set that common IP to a load-balancer and then put your work-horses behind it? That'll help you scale a bit without losing that traffic. Let me know if you need more explanation. Edit: If your clients are always typing in that IP from memory, there won't be a lot you can do to switch them to a domain. As I mentioned, your best bet will to be to incorporate that IP into a load-balancer. Should be the 'easiest' way. 

Ultimately, my goal is to be able to iterate over a list of values 'N' times and repeat a stage and pass parameters to each stage and if any of the stage 'fail', fail the whole stage. 

I have a few dozen Solaris(5.10) boxes. By default, they are using ksh as the shell. Only a few users have sudo privileges. The rest do not. I don't want my regular users changing to another shell. Here is what I've done: Minimal sudoers so /etc/passwd is out of the question for them. Minimal sudoers so usermod -s is out of the question for them. I have NOT disabled access to the shells in /bin or /usr/bin yet, but it will be done - so please disregard the fact that it hasn't been done yet. So, in theory they could write startup scripts to execute them, right? Am I missing anything else? 

There is DEFINITELY going to be some consequences of doing this. The primary one is going to be IO read/writes. Beyond that, it's just a very scary way of dealing with that type of data(at that scale). 

We're presently using googledocs(word processing, spreadsheet and email) for everything. We'd like to move away from remotely hosted software. We have a large internal infrastructure to support this sort of thing so technical resources won't be an issue. Are there any decent alternatives to googledocs that meet the following requirements? Open-source Local Hosted option Collaboration/multi-user support Word processing Spreadsheet support Privacy features Importing/Exporting Minimal installation footprint I've been leaning towards a collaboration suite, however, I was also wondering if there is some way to collaborate within Openoffice? Thanks! 

I have a Cloudformation stack that I create through Jenkins in various Regions. I have a Chef server in one Region with a separate security group. I need new instances created via Cloudformation to register/be created and add themselves to the Chef SG in us-west-1 regardless of their region. Is this feasible? Edit: I need to do this via the Cloudformation script as opposed to other methods for a multitude of reasons that are lengthy/convoluted. Edit2: For clarity, I don't want the instance to be part of the SG, but rather for that the EIP of the new instance to be added as an ingress in the SG. 

This is pretty easy to do so I'll keep the answer a little high-level. You can use Cygwin and to get the current IP of your Windows box. Forward rsync(ssh presumably) to your Windows machine. Simple script on the Windows/Cygwin box to then rsync the tar'd file(you're taring it, right?) that needs to be backed up. I.e., That should do it. 

You really won't notice a major performance difference unless you're getting a massive surge of traffic. If so, put it on the disk with the fastest RPM. However, you may want to put all this on the larger disk if you imagine it'll grow beyond the size of the smaller disk. 

We have a bunch of existing servers in EC2. Future servers are created with Cloudformation with Cloudwatch integration. However, I need to setup Cloudwatch for servers that weren't created with Cloudformation. I have been asked to create a Cloudwatch Cloudformation stack. Is it possible to just create alarms in Cloudformation? If so, how do I specify which servers to monitor? Thanks! 

You didn't specify your ssh key by the looks of it so it's assuming you want to use a password. Most AMIs are configured to NOT allow ssh access for root with a password. Fix your parameters to ensure the key is being specified. 

I've seen this before. It's something to do with Ubuntu 10.10. I'd look around on the bug tracker as its been posted a few times. To be sure, take a snapshot of the disk, wipe it then drop it in a secondary system to see if the bug repeats itself(to rule out the disk - unlikely culprit). 

Try another server in the same region. It sounds like it's probably the destination server. You can't expect everyone to have 1Gbps links. 

First of all, don't try uploading straight into a web folder. I would recommend uploading to your home folder, doing the necessary permission changes THEN copying(NOT moving) the files to that destination(will probably require sudo). That should do it. 

It generally comes down to poor planning on the former description. Reactive security resolution/putting out fires. I.e., "Oh shit, they're hammering us on 123, lock it down!" or more appropriately, blacklisting. The latter is a form of whitelisting(best practice) and usually comes down to locking things down through implicit exclusion and opening up what you need as required. Which one you chose usually comes down to your environment/style. 

I don't know what to do at this point... EDIT: I've fixed my settings to match the following: MASTER1: 

I have thousands of web files(*.php, *.htm, *.html, etc) that contain an absolute path($URL$ I need to scan a directory and all subdirectories for the various file types and within those files, modify my absolute path to a relative one of my choosing and then leave the rest of the file as is. It must not change the path of any other absolute URLs though. I've checked the similar questions already posted on the matter but none seem to address this directly. Any suggestions? Thanks! 

I have approximately 30 users on a box. Those users are in overlapping groups(about 6-10 groups). I need them to be able to land in a specific folder based on their group assignment when they FTP in. I.e., group1 -> /tmp/site1 group2 -> /tmp/site2 Is this at all possible with VSFTP on a SuSE box? Using SFTP isn't an option unfortunately. Thanks! EDIT: And in the event of a user being in several groups, just dumping them to the highest-level folder necessary to view the various folders they have access to. 

Why not run that same command again with the proper permissions then apply your desired permissions to sites/default? 

Ignore the Ubuntu 10.10 suggestion. If you're rolling a server, avoid the non LTS release. I'd suggest Debian if you're comfortable with the package system as it has a smaller foot-print than Ubuntu 10.04. 

My favourite is to deploy a simple FTP/NAS device to the network with a public folder. Push scripts to all the hosts to download from the FTP folder and execute the .exe's. 

MGMT1 is the main VPC. We're able to ping other MGMT VPCs. However, the route from MGMT1 -> MGMT3 -> DEV doesn't seem to be available. I can't ping. I can ping from MGMT3 -> DEV however. That fact leads me to believe all the routes are correct. However, MGMT1 isn't aware of DEV despite being connected to MGMT3 and it knowing about DEV through the peering route table. Am I missing something? Thanks! EDIT: All security groups are very permissive as indicated by the ability to ping across the MGMT layer and from MGMT to DEV within the same region.