Generally it is best to hold data in a normalised structure. If testing at scale, on production-grade hardware, proves there is a problem, and that the problem is because of an over-large table, and index tuning and SQL re-factoring do not bring the problem withing acceptable bounds, then splitting the table is a recognised technique. Moving some rows to a different table is called horizontal partitioning or sharding. Moving some columns to a different table is call vertical partitioning. Both entail additional design and maintenance work. There will be run-time implications if values from several partitions are required by a single query. There is some further discussion at this answer. 

To be able to values into the shape of your target table you need a set of (parameter_name, parameter_value) tuples. I assume the design is able to distinguish one parameter value from another, either because of its position in the file (row 1 is Parameter A etc.) or due to a label in the row (ParamA=ValueA etc.). For a positional design, create a reference table with two columns - and and populate it accordingly. Create a staging table with an identity column and . Import the parameter rows from a single source file into this staging table. Values will be generated by the identity in the order that rows appear in the source file so the first parameter value will be ID=1 and the last will be ID=40. Join the staging and reference tables to get the tuple. For the embedded labels use string functions to separate the name and value on the delimiter. This will be simpler with two staging tables - one with a single column to hold the raw records from file and the second with two columns to hold and separately. I'd suggest you import each file in two steps. In step 1 bring in the 40 rows that represent your parameters. In step 2 bring in the remaining rows which are data. has and parameters which can control this. bcp utility has similar switches. Now you have a set of 40 (parameter_name, parameter_value) tuples that can be ed into your table's schema. Use this to to table, which presumably will have a surrogate primary key which you can capture (an will help). Use this value in the of the remaining data points. This is why I suggest doing it in two steps. Package the whole thing as a stored procedure. Use a Powershell script to itterate over your files and call the SP for each. You will need to use dynamic SQL for the . If you use temporary tables for the staging table you can run any number of processing streams in parallel without worrying about tidy-up. Before each file is processed the staging table. This will reset the idenity as well as remove the previous file's values. Of course the whole file could be read into a staging table and the parameters separated from sensor data afterwards. This works equally well, it just represents additional run-time work for little value in my opinion. 

For the given user and time find the most recent activity which is a 46 or a 47. If it is 46 the user is "automatic". If it is 47 the user is not "automatic". There will be users who do not have any 46 or 47 records. These will return and empty resultset and the application will have to interpret these appropriately. If activities can arrive so quickly that the 46 and 47 rows both have the same values this won't work. You will have to try a different approach, increase the resolution of or use a sequential integer column as a proxy. 

You may have to involve , too, if the SPs are spread across multiple schemas. Take the output of the above and run it against the staging DB. You can do something similar for functions, triggers, other executables and any type of object. Once done, backup the staging DB and send that to the customer. The whole thing can be automated. 

The phrase refers to different columns; in the first it will be , in the second . Since is the key it will be indexed and the will be a trivial amount of work. To , however, the system will have to retrieve every row, sort the complete table by , then choose just one of those rows. Change both queries to and I think your execution times will be almost identical. 

Since first starting to type up an anser I have argued myself into, and out of, a set position. I think this question steps over the border from the science of normalisation into the craft of data modelling. The answer will be determined by how you (or, more importantly, your users) understand the semantics of the situation. Is "extra cheese" a new "thing" which is sold, just like "burger" is a thing which is sold, or do they see "extra cheese" as a simple modification of the component list and "burger" is still the product? I'll take a different business area to show the difference more clearly. If two people each bought a Ford Focus but with different engine, radio, wheels, paint etc., would you think they had bought esentially the same "thing"? Chances are you would. Even though different specifications go into the factory, the item that emerges is thereafter treated as a single unit for sales, transport, handling, support and all other purposes. However, if one customer orders, say, a tow bar that would be a post-factory option and would be handled through different sales, distribution, stock keeping, and maintenance channels, so is likely to be seen by the business as a separate product. So to the burger. If a burger with extra cheese is still a burger then model it as variations to the composition. If "extra cheese" and "no cheese" are seen by the users as products in their own right model them thus, with their own composition and price, which may be negative. In term of line items, treating the variations as separate products is the easiest. Once set up in and they will flow naturally through to the invoice and stock totals just like the standard products will. You may need to add a product dependency table to ensure the variations make sense ("no cheese" only applies to cheese burgers; can't order "cola with extra cheese"). For the scenario where there is a single product with variaitons I would model it thus: 

Your approach is reasonable. Put a check constraint on the table to ensure exactly one of your foreign keys is ever set at any point in time. The flag is redundant since the same information is captured by which of the foreign keys is . Keeping it just adds maintenance overhead to the application. Eventually it will become inconsistent and then you'll have a mess to sort out. An alternative design is to combine some of the information from and into a table. The schema then becomes 

Will have the obvious cost of the write to and read from #T1. There may be a benefit, however, if you can index #T1 in a way which helps the second query, perhaps if complex manipulation was performed on Table1's values. Stored procedures can suffer from parameter sniffing. This can sometimes be rectified by recompiling the procedure at each execution. Recompiles can be expensive. By splitting a complex query into several simpler ones you can put the recompile hint on just the statement(s) that benefits from it. An optimizer will not search indefinitely for the best execution plan. It will eventually time out and run with the best available at that point in time. The work the optimiser must do is exponentially larger with the complexity of the query. Having simpler queries may allow the optimizer to find the best plan for each individual query, providing a lower-cost execution overall. 

Generally speaking, I find it less confusing to use positive terms in design. Here it would mean storing "can call" days rather than "no call" days. Up to you, of course, but it may be worth considering. If you were to normalise fully you would have a table and a table with values "Monday", "Tuesday" .. "Sunday" and a table with values , for example. I think the risk of new weekdays appearing is sufficiently small that you can denormalise the days into withont worrying. Going that route, however, complicates your query as you have to compare each "day" column explicity against today's date. If you use the fully normalised form you can employ the syntax in a generic way. Your query then becomes (in pseudo code, sorry I don't have a copy of Access to test with) 

then that part could be handled by the value-storing software and the GIS eliminated from the architecture. I have not implemented such a system. I'm really just thinking out loud here. At the petabyte scale there are no off-the-shelf solutions. There are, however, many satellite data providers so your problem is tractable. Good luck. 

Having numbered columns is an anti pattern. Your Game table shows it. The relationship between Game and Player is many-to-many, even though the cardinality is fixed from a game's perspective. Resolve the problem by adding a new table ("Participant"?), with foreign keys to both Game and Player. To see how it helps, try writing a query to count the number of games an individual was in using each design. The relationships to Player Round Result should be to this new table, not to Game and Player separately. This ensures a Player must necessarily have been in a Game to have a result recorded. The foreign key columns in Player Round Result will have the same names and types, but the foreign key constraint declaration will reference a different target. Further, I'd suggest the table currently called Player would be better as Person, since the values it holds are for the human per se. The new table I suggest above would be well-named as Player as it is all about a person participating in a game. 

Reading this table will be efficient with an index on business_name and one on group_identifier. Trying to maintain a minimal set of groupings may be tricky if the bussiness often change groupings. 

You could declare a table variable, insert to that rather than printing and dump it in toto at the end of your process: 

The usual design is to relocate each column with repeating values into its own table. You don't say, but I'll call your current table , and I'll assume the is the primary key. Next create a table for each repeating-value column and link them back to . For example 

This becomes ugly real quick. My suggestion is to implement it one table per entity. Write the queries and test them. They can be written incrementally, checking first for, say, country validity, then country and index, then country, index and counterparty and so on. If this approach surfaces a particular problem, pause and re-evaluate in the light of the new circumstances.