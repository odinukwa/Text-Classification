I really like PhysFS for this. It allows you to access either folders or zip archives with the same code. It works well for all stages of a Games lifetime. 

The MMO terminology for "remain within a single game world" is single shard. EVE online is the only major MMO to attempt stuffing every player into a single shard. Lucky for you they published a very informative article on how they do it. $URL$ The bad news. You cannot apply EVE online's techniques generally. Their solutions are absolutely tailored to their particular genre and implementation. NOTE: For all of EVE online's super fancy single shard network they use one database. They were unable to design a scalable, consistent, moderately real-time solution for distributed databases. Either way reading how they did it ought to help you design you ownr solution. Beware however, you are attempting to solve a very difficult problem. Instead of distributing your game server I would suggest explore your other avenues first. 

I would suggest having a repeating sound effect of a spinning flute. Input the player position and the flute's position/velocity. Skip the directional stuff for now. You could record the sound that comes from each end of a flute as the air flows by and have the flute use two sound sources. But I doubt it would be worth the effort unless the whole game mechanic revolved around thrown and spinning flutes. 

So what is common practice, use the same language for tools, or use a more RAD friendly one? What if you use a language that differs from the language of the game which do you use? How do tools interact with the game engine link to it directly or fake it? 

In X Men 2, Magneto shatters his glass prison with several marble sized metal balls, youtube. In the DVD commentary the designers explian how they did the breaking effect. It was too computationally intensive to fully simulate it, instead they had an artist determine where the breaks would be (and where they'd look good). Then they scripted when the pieces would detach, the falling glass shards were physically simulated. 

Assume a client-server game where there server manages all state and the clients are simply rendering / input. How much should I bake user input from the client before sending it off to the server? Does it make sense to send "jump" or "space keydown"? I'm inclined to bake the input as much as possible. Am I setting myself up for pain later? 

Squirrel's primary disadvantage is it's not Lua. Lua is much more widely used. But if that's not an issue Squirrel is an easy win. However, often the language's popularity is useful feature in itself, so the decision is not so clear cut. 

update: I used the tutorial included with the source and the doxygen documentation to learn PhysFS. The API really is fairly simple, you'll spend more time learning how to load images into SFML via memory buffers instead of by file path. 

Two examples of evil patents that are being up held and effecting the game industry. Direction arrows above cars are patented in racing games. IE you're racing you get knocked off course, the arrow points the way towards the course/goal. Every game that uses this mechanic needs to pay royalties to Sega of America. A ghost racer is time trials. IE you're trying to beat your best time and you race against a ghost version of the current fastest time. Every game that has this mechanic must pay royalties to Midway games. I repeat software patents are sick and evil. 

In Glen fielder's Fix your time step he says to "Free the Physics". That means your physics update rate should not be tied to your frame rate. 

I was under the impression that in a components based design the entities are essentially components containers (with possibly some message thrown in). Viewed from this perspective the each components would store a little of the state. For instance if the ghost-behavior-components decides it needs to enter the intangible mode it also sends a message to the physics component telling it to enable no-clip. It would probably also send a message to the ghost-model-components telling to kick up the alpha. 

I was thinking of how to implement overriding of behaviors in a component based entity system. A concrete example, an entity has a heath component that can be damaged, healed, killed etc. The entity also has an armor component that limits the amount of damage a character receives. Has anyone implemented behaviors like this in a component based system before? How did you do it? If no one has ever done this before why do you think that is. Is there anything particularly wrong headed about overriding component behaviors? Below is rough sketch up of how I imagine it would work. Components in an entity are ordered. Those at the front get a chance to service an interface first. I don't detail how that is done, just assume it uses evil s (it doesn't but the end effect is the same without the need for RTTI). 

I wrote a little proof of concept that used CSG to make scorched earth/worms style game play. I implemented it with gluTessellate. I then mapped the tessellated triangles onto Box2D polygons. I could both add and subtract dirt from the simulation as well as make and fill holes. The largest problem I found using gluTessallate is that it has no problem returning degenerate triangles. I had to filter those out before moving the tessellated triangles into the physics engine. One of the nice things about gluTessallate is that it is possible determine adjacency from the callback information. I never took it further but in theory you could use the adjacency and SCC to accurately detect islanding. 

My understanding of the Dynamic Tree's algorithm is this. The Dynamic tree is the cross between a classic avl binary tree and a quadtree. The end effect is a quadtree that that only splits each node in half, and the split line isn't fixed (the two halves aren't equal sized like a quad tree). AVL comes in because quadree with dynamic splits can degenerate to essentially a list (O(n) lookup speed). The AVL is used to rebalance subtrees so to ensure O lg(N) lookup speed. Best of all the code is MIT so feel free to copy / derived / shamelessly-steal / etc. 

Build a framework. I have experience with building both. I prefer to use libraries over frameworks. Frameworks feel very restrictive and "bossy". So when I built my first game I wanted to build an engine that was composed of many libraries that could be easily reused in other projects. It was a disaster. It spent half my time writing glue code between my various libraries. My libraries also tended to be bloated with extra abstractions, so that it became a huge chore to add features to the libraries/components. Now, however my engines are built with the game I'm writing in mind. I still keep my eyes open for abstractions that will allow me to reuse it or the new features I'm adding in later games. I'm much more happy and productive. For me the turning point was to decide to not publish my engine. I may still publish it, but for now knowing I don't need to justify my game specific hacks to other users has let me build an engine better suited to my games. 

While I was a student, I successfully petitioned to get an intro to game programming taught at my university, WSU. The course was taught by Scott Wallace, an associate professor at the Vancouver campus. Most of the students myself included took the class at WSU Pullman (the main campus) via video streaming. The web pages from the classes are still up. I believe 2006 was the only year the class was taught on both campuses. It's been taught off and on at the Vancouver campus since 2004. Do some url hacking, their sites are still up. Computer Game Design 2006 I'm sure Dr. Wallace would be willing to answer some questions and give some pointers about your own class. He may be willing to share his basic course template as a starting point. Dr. Wallace did his graduate work at UMich under John Laird. His course is based on a similar course Dr. Laird teaches: Computer Game Design and Implementation. 

UCS is not omnipotent. It cannot model the benefits of protection spells. For that you'll have to upgrade to alpha-beta search or minimax. Also it doesn't handle area-of-affect and group fights very well. UCS can be tweaked to give reasonable solutions in these situations, it is not guaranteed to find the optimal solution. 

There has been a lot of discussion on this very topic on the Box2D forums. You'll find most of the info under the guise of breakable bodies, which is your problem in reverse. Here are my suggestions in order of easiness: 

OpenGL ES 2.0 is not a new revision of OpenGL ES 1.1. Or in other words, OpenGL ES 2.0 is not a better OpenGL ES 1.1. Well no duh, they changed everything. The import of that simple statement is easy to overlook. Consider: OpenGl 2.0 is a new revision of OpenGL 1.5. OpenGL 2.0 is a better OpenGL 1.5. This means If you're using OpenGl 1.5 and want to maintain forward compatibility you should move to 2.0. The same is not true of OpenGL ES. 1.1 was not superseded by 2.0. They are different standards. The confusion might have been eliminated if they had been named: 

OpenAL I really like OpenAL as a cross-platform audio library. The API is a bit low level but you can easily wrap it into a nicer engine friendly interface. It's truly cross platform with first class support for: iPhone, Android, MacOSX, Windows, and Linux. Take a look at Djinn Engine sound code. Two files, maybe 300 lines between them (counting comments etc) and you'vd got a workable sound system. I've never build an audio centric game, so OpenAL might not fit the bill if extreme control is needed. 

Which is more efficient using OpenGL's transformation stack or applying the transformations by hand. I've often heard that you should minimize the number of state transitions in your graphics pipeline. Pushing and popping translation matrices seem like a big change. However, I wonder if the graphics card might be able to more than make up for pipeline hiccup by using its parallel execution hardware to bulk multiply the vertices. My specific case. I have font rendered to a sprite sheet. The coordinates of each character or a string are calculated and added to a vertex buffer. Now I need to move that string. Would it be better to iterate through the vertex buffer and adjust each of the vertices by hand or temporarily push a new translation matrix? 

Update: By 'game-centric GUI' I mean a GUI framework that doesn't own the window or the main loop. There are other features I'd add but the main loop is the only critical one. This usually involves various rendering backends, input injection, and an update method. 

I've been fiddling with both QuickGUI and MyGUI. They're both Ogre GUI libraries that recent dropped the ogre requirement. I don't have too much experience with either yet, but it looks like I'll be sticking with QuickGUI, as it is much smaller and simpler. I'm not sure if the QuickGUI wiki has been updated yet to point to the newest Ogre-free version. Here is the latest release. $URL$ There repo has seen a few cross-platform updates, so you'll be better off using the svn directly. $URL$ $URL$ $URL$ Username: Anonymous Password: QuickGUI Lastly here is the brief wiki on how to add diferent rendering backends. $URL$ MyGUI seems to have more features at the cost of getting bloated. And honestly if simplicity isn't a requirement CEGUI blows MyGUI out of the water. 

What tool do you use to convert ttf fonts to bitmap fonts? Following is a list of features I'm looking for. I'd be happy with any tool really, but one that had these features would be better. 

Imagine a 1 on 1 (not teams) competition between AI bots, like the Google AI Challenge. The various bots are assigned an ELO rating based on the outcome of the various versus matches. The reason I specify AI bots as they can compete 24/7 without regard for player fatigue, geolocation, etc. Given limited server resources only so many bouts can be run per day. I'm looking for a heuristic (or an optimal algorithm) to choose which two bots should compete next. All the past competitions have been tracked. By this I mean that the algorithm has more to work with than just the ELO ratings. The use cases I'm partitularly interested in: 

I'm looking into scripting languages to embed in my game. I've always assumed Lua was the best choice, but I've read some recent news about embedding V8 as was considering using it instead. My question is two fold: Does anyone with experience embedding v8 (or another javascript engine) recommend it? How does it compare with embedding Lua? I like that v8 has a c++ embedding API. However Lua API has had lots of time to be refined (newer isn't always better and all that). Note: At this point I'm not too concerned with which is better language or which library has better performance. I'm only asking about ease of embedding. 

My solution far less elegant/complicated than most. I'm using Box2D as my physics engine so keeping more than one copy of the system state isn't manageable (clone the physics system then try to keep them in sync, there might be a better way but I couldn't come up with one). Instead I keep a running counter of the physics generation. Each update increments the physics generation, when the physics system double updates, the generation counter double updates as well. The rendering system keeps track of the last rendered generation and the delta since that generation. When rendering objects that wish to interpolate their position can use these values along with their position and velocity to guess where the object should be rendered. I didn't address what to do if the physics engine was too fast. I'd almost argue that you shouldn't interpolate for fast movement. If you did both, you'd need to be careful to not cause the sprites to jump around by guessing too slow then guessing too fast. When I wrote the interpolation stuff I was running the graphics at 60Hz and the physics at 30Hz. It turns out that Box2D is much more stable when it is run at 120Hz. Because of this my interpolation code gets very little use. By doubling the target framerate the physics on average updates twice per frame. With jitter that could be 1 or 3 times as well, but almost never 0 or 4+. The higher physics rate kinda fixes the interpolation problem by itself. When running both the physics and framerate at 60hz you might get 0-2 updates per frame. The visual difference between 0 and 2 is huge compared to 1 and 3. 

CEGUI's coordinate system seems really well thought out. It's unified coordinate system blends absolute positioning with relative positioning. You can specify locations like: 

Behavior and logic are executed on the server. The clients are mostly for rendering, audio, and gathering input. It looks as though most of the a components architecture benefits are only realized on the server. Is it still beneficial to design the client with an component based architecture? Or is there a different approach that better fits a client. 

I've been working with a few game oriented gui libraries trying to find a good fit. More specifically, a GUI that will work on Windows,Linux,iPhone, and Android with minimal special code. In my tinkering I've noticed a few libraries like to render the their GUI's to an off screen texture that just display the texture instead. Is this a common GUI technique? Isn't there a performance hit with rapidly changing GUIs? Were I the implementer I would have just rendered the GUI directly each frame. Is there anything particularly wrong or inefficient direct rendering?