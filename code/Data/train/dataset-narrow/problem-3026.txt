If the week ID is given as you state, calculate a bucket variable $w_x=int(weekID/x)$. Then use a SQL statement to summarize the volume to levels of $w_x$. 

The R language is the best place to start. Grab some open datasets, and start programming with r. R has many different analytical functions that you can learn a lot with it. 

With this kind of general problem there are many possible approaches, and I can't list them all. In general, you want to go with the simplest model that creates value over a more naive approach. So we can look at the different approaches, ranked by complexity, and see what you need for each. This is the order I would try for this analysis: (1) The simplest approach is just contact a flat 20%, drawn randomly. This is very easy to do, but hopefully it can be improved upon. (2) Next, you can try an approach based on the basic statistics of the problem at hand. For example, you can calculate for those employers which previously placed ads, how many applications* did they get. And then start with the ones that got the most, and work your way down. (3) Now that you have baselines sets from doing a blind and then a basic descriptive analytics approach, you can try something a bit more complicated. Assuming you have the text from the past ads, you can mine that text for features which might be descriptive of what they are looking for. If the employers placed multiple ads, you can combine those and look for features in the aggregate. Once you do the same with the resume data, you can create a recommendation engine to match potential employers with employees. You can then use that data to determine which possible employer is most likely to have potential matches in your client pool. As a side effect, this engine can also guide your clients to which jobs might be the best matches. There are many variations which you can use here. For example, for the recommendation engine, you may want to weight your client sample towards clients which have logged in the most recently, or most frequently, so or you can just filter out the "stale" clients who haven't really been looking recently. *An alarm bell went off when you said you know who applied. Are you sure? Or do you just have clickthrough statistics to know whether they started an application? How do you know that they applied? The first rule of real world data analytics is that you need to be skeptical of your data and especially of what you think your data represents. 

It is possible and common to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights. More details here. 

Typically they specify somewhere whether they talk about the forward (a.k.a. inference a.k.a. test) time, e.g. from the page you mentioned in your question: 

I am looking for a program that would allow me to fine-tune pre-trained word embeddings on my data set. Ideally, open source and working on Linux or Windows. 

Just to complement the question, regarding the overview of the algorithm, I found this slide to be very clear (but also failed to mention the table update mechanism): 

a question, and then some comments. First, what kind of market analyst do you want to be? This field is rapidly becoming specialized, and the answer will affect the decision. If you want to specialize in the pharma or especially the medical field, the biostat background could be an advantage. Generally speaking, you need to think outside the box of what you will learn purely as defined by the program, either stats or biostats. You will need to branch out into areas not defined by either program. This could include (1) software language, (2) data science problems (Kaggle or external projects run through a campus department), or (3) courses not expressly given in either department, whether it's in "hard" skills like data mining or "soft" skills like marketing. Ask for exceptions to do things that don't fit the program, but fit your career goals, because being a market analyst doesn't fit squarely into either field. 

The statsmodels documentation describes a method that can be used to make predictions on inputs from a design matrix. You should be able to use this method on your test data once you have fit the model. 

Note: $D(x)$ should be updated as more centroids are added. It should be set to be the distance between a data point and the nearest centroid. You may also be interested to read this paper that proposes the method and describes its overall expected performance. 

A quick glance at the docs for LogisticRegressionWithLBFGS indicates that it uses feature scaling and L2-Regularization by default. I suspect that R's is returning a maximum likelihood estimate of the model while Spark's is returning a regularized model estimate. Note how the estimated model weights of the Spark model are all smaller in magnitude than those in the R model. I'm not sure whether or not in R is implementing feature scaling, but this would also contribute to different model values. 

Is there any Python library that provides ready-to-use metrics to analyze the performance of a classifier for a multioutput-multiclass classification task? scikit-learn doesn't have this option yet (as stated in the documentation and in the corresponding feature request on GitHub). 

You can look at nolearn/lasagne/util.py to see how learning capacity and image coverage are computed for each layer: 

which in the code corresponds to: . In the method, I fail to see where $\frac{\partial Q^{\theta}(s,a)}{\partial a}$ gets multiplied by $\frac{\partial \mu(s|\theta)}{\partial \theta}$. I did read: 

As Dawny33 says, TensorFlow is just getting started, but it is interesting to note that the number of questions on TensorFlow (244) on Stack Overflow already surpasses Torch (166) and will probably catch up with Theano (672) in 2016. 

It really comes down to exactly what you want to do and how much control you want to have over the map. Additionally you should consider what sort of interactivity you need. Google Maps is probably the most user friendly for very basic map functions but is somewhat limited in what you can do stylistically. Mapbox and CartoDB are both user friendly and offer good options for styling and displaying different varieties of data. However, they also both tend to require subscription fees for anything other than small maps. Also, the last time I checked, CartoDB explicitly handles animation and time-series data where Mapbox does not. D3 will probably give you the most control over display, animation, and interactivity but also has a long learning curve. Even if you don't want the map to be available on the internet, this is still a very good tool for making interactive visualizations that run in the browser. If you don't care as much about the visualizations being online, you can get a lot of work done in open GIS software like QGIS or GrassGIS, though I don't know if user interactivity is really an option there. As I said though, it really comes down to the specifics of exactly what you're trying to do and how comfortable you are with various aspects of mapping and coding. 

However, on $URL$ I don't see any entities that existing services. How can I create a new project? I want to use the speech to text API. 

I want to train word embeddings using word2vec. My corpus is split into several documents (it's a large set of patient notes). Should I just concatenate all documents into one before running word2vec on it, or is there a better way? 

The second advantage, which is also very important for large databases, is that column-based storage allows better compression, since the data in one specific column is indeed homogeneous than across all the columns. The main drawback of a column-oriented approach is that manipulating (lookup, update or delete) an entire given row is inefficient. However the situation should occur rarely in databases for analytics (“warehousing”), which means most operations are read-only, rarely read many attributes in the same table and writes are only appends. Some RDMS offer a column-oriented storage engine option. For example, PostgreSQL has natively no option to store tables in a column-based fashion, but Greenplum has created a closed-source one (DBMS2, 2009). Interestingly, Greenplum is also behind the open-source library for scalable in-database analytics, MADlib (Hellerstein et al., 2012), which is no coincidence. More recently, CitusDB, a startup working on high speed, analytic database, released their own open-source columnar store extension for PostgreSQL, CSTORE (Miller, 2014). Google’s system for large scale machine learning Sibyl also uses column-oriented data format (Chandra et al., 2010). This trend reflects the growing interest around column-oriented storage for large-scale analytics. Stonebraker et al. (2005) further discuss the advantages of column-oriented DBMS. Two concrete use cases: How are most datasets for large-scale machine learning stored? (most of the answer comes from Appendix C of: BeatDB: An end-to-end approach to unveil saliencies from massive signal data sets. Franck Dernoncourt, S.M, thesis, MIT Dept of EECS) 

A good place to start would be to look at the Spearman's Rank Correlation between your dependent variable (academic level) and your individual independent variables (other columns). This should give you a basic indication of whether any of your columns are correlated with academic level. This should be straightforward to implement in both Excel and SPSS. If you wanted to go a step further you could set the problem up as one of Multinomial Logistic Regression. This would allow you to build a model to directly attempt to predict academic level from your other variables. I'm certain there's probably some way to do this in Excel, but SPSS can definitely handle it. 

where corresponds to $\frac{\partial Q^{\theta}(s,a)}{\partial a}$, and corresponds to $\frac{\partial \mu(s|\theta)}{\partial \theta}$, but I see no multiplication. Where does $\frac{\partial Q^{\theta}(s,a)}{\partial a}$ get multiplied by $\frac{\partial \mu(s|\theta)}{\partial \theta}$? 

I agree that the current trend is to use Python/R and to bind it to some C/C++ extensions for computationally expensive tasks. However, if you want to stay in C/C++, you might want to have a look at Dlib: 

I am looking for benchmarks based on neural networks libraries (Theano/TensorFlow/Torch/Caffe/…) to compare the performance between different GPUs. I am aware of: 

I am trying to understand the training phase of the tutorial Using Keras and Deep Deterministic Policy Gradient to play TORCS (mirror, code) by Ben Lau published on October 11, 2016. The tutorial says: