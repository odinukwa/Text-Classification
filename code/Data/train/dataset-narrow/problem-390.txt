I am only updating the AGE column, so will MySQL still update the indexes even though non of the indexed columns were modified or will it leave the indexes alone? 

This did not work either. I then tried to change the password for root to make sure I was using the correct password: 

Additional "would like to have feature, but can live without it requirement if it effects performance considerably" would be to have a user view their messages with another user: 

I have a very simple requirement. I have an application where users log in, add/remove/view friends, and can send/receive (also view the message you sent and received) a message (message is more like an email, not a real time chat) from a friend. When they receive the message, the application needs to mark the message as 'read'. The messages must also come ordered by newest to oldest. My thoughts: KINDS: 

Say I have a table with 3 columns: Say ID has an index on it, name has an index on it, and there is another index which combines id and name. Lets say I now have an update statement which looks like this: 

1) User would login using their username and password 2) User could get friends by getting their USER entity based on their username 3) User could add/remove friends by getting the entity of user1 and user2 and either adding or removing friend via transaction to make sure they are consistent. 4) User could get all the message they have sent by using indexing the the 'from' attribute (limit of 10 message per request). The same could be done to view all the messages they have received by using the 'to' attribute. When the message has been seen for the first time I would go to the message (1 get) and update the entity (1 write + (4 writes x 3) = 13 writes to update the entity). My major concern - If a user gets 10 messages, this will require 10 get requests plus if all 10 messages are new I will need to update each entity and if all of them are new that is (10 x 14) 140 writes. Then if I get another 10 message for this user the same process and this could all add up very quickly. I then thought of creating an entity to store all the sent/received messages in a string for a user inside of a single entity: 

Using my sysadmin account, I get a row for every database. Most rows are all NULL because they are not mirrored databases, but the two mirrors report their status. If you only want to check running mirrors you could use: 

A is an overriding authority on a SQL Server. You can try auditing to let you know what has happened, but a can override that as well. Server-Level Roles In addition to the Fixed Server Roles, such as , there are more granular permission settings, if they prove valuable to you. See the graphic at Server Level Roles and Permissions. You may be able to satisfy some needs with 26 roles that may have lower permissions. 

Do not use the /// keys at all to select your code to run. Assuming that a computer mouse can be used, do this: 

If your goal is simply to create a Flat File have you considered using BCP.EXE to Bulk Copy your data out. Bulk Import and Export of Data (SQL Server) gives a link to the bcp utility. You do need to give directions on how to interpret the data. Here at Create a Format File (SQL Server) you can examine the two Format File type used to define the data format. (Naturally.) The format file that I use is a Non-XML Format File. This is just a text file and is fairly easy to type and make changes within. For me, this is a quick way to export a flat file. (Of course, if you read further down the page you will see an XML Format File, which might be more your style.) It is also possible to use BCP to copy data back into a SQL Server and it is pretty fast. (However BULK INSERT tends to be a bit faster for importing data.) So BCP is fast, fairly easy, and comes in two flavors. 

I have a fresh install of MySQL 5.7 on my Windows machine. When I make a connection using root@localhost, I am able to connect, but when i try to make a connection root@192.168.1.10 (the private IP address of my server) I get the following error: mysql 5.7 access denied for user 'root'@'192.168.1.10'. I opened my my.ini file and added the line bind-address=0.0.0.0. I saved the file and restart my MySQL instance, but still not luck the same error appears. I then tried: 

Without the LIMIT there would be 170 rows returned where fr_user = 1; I am now trying to understand how efficient this query (not trying to improve the query, just to see how efficient it CURRENTLY is) is. I have ran the query and looked at the execution plan: 

I have two kinds: USER and MESSAGES. A user can send/receive many messages to/from another user. Thinking in a relational way the relationship would suggest a user can have many messages, and a message can only be created by one user. So if user A sends a message to User B, both user A should be able to see the message they sent, and user B should be able to see the message they received. I was thinking first of storing all sent/received messages inside of each user object, but since there is a 1mb limit for entities this is not an option as some users can retrieve many more messages than this. Second I was thinking of creating two kinds, USER and MESSAGES.The messages kind would have a to and from property both which would be indexed so that I can get all the messages a user has sent and received. As a result each message would be its own entity and this is where it is problematic. Lets say a User is deleted, I know have to delete all the message sent and received by this user. If a user has sent though sands of messages, the cost of removing each message will be very expensive. I was wondering what would be an efficient way to model this. I am be to Google Datatore and would be open to any ideas as I have nothing implemented yet. 

As with most things SQL, it all depends, there is no correct autogrowth setting, although bigger tends to be better. Each grow event takes a lot of effort for SQL server, writing to logs increasing the DB size, so ideally you want to have as few as possible events. You need to find the middle ground between 1MB on a 20MB DB and 50% growth on a 500GB DB. It's all about finding your own balance. The best is to evaluate what you have and what growth you expect over the next 30 days or 90 days, punch those numbers in is as a % or fixed size and check it again in a month. Rinse and repeat. As a rule of thumb our default is 10% autogrowth and that works fine for most of our DBs, from 10MB up to the 0.7TB ones. You just need to manage it. 

We have a linked server on SQL pointing to a MySQL server. Creating the linked server is a pretty straightforward thing. The tricky part is getting the permissions sorted out on MySQL side. If memory serves my right you need to create a local account on the SQL server and create an login on the MySQL server for the full computer name where the linked server will run. On the SQL server I have a login for Sqltest. This is a SQL account On the linked server properties I have "Security" > Be made using this security context > MySQL login username and password On the MySQL box I have a login for Sqltest@myserver.com. This is where my linked server failed for quite a long time. 

The post that Roni Vered referred to in 2013 did not say simply say: "change the connection pooling in the application's connection string." The text said to that there was a problem with using "an Application Role ... with connection pooling." The answer was: Turn Off Connection Pooling. This could be done in the application's connection string by including "; Pooling=False". This worked back in the mid-2000's. Likely it will still work for you if you are still using SQL Server 2008. The no longer correct path to the original post was: $URL$ A suggested workaround from 2007: On each call if the approle is already in use, then skip the setting of the approle again. Sample code: 

Assuming that the [PhoneField] is 10 digits, such as '1234567890' then you could use STUFF to put the proper values in. E.g. 

Any time the Restore Job recovers a series of LOG files, it will disconnect any STANDBY users that are still connected. Also see Kendra Little's post: Reporting From a Log Shipping Secondary in STANDBY mode Contained in here post is a mention of the problem that you have run into with restoring a 2008 database to a 2008 R2 Secondary. 

As JamesZ mentioned, the memory is intended for the SQL Server to use, particularly for the data cache. Therefore SQL Server will attempt to use everything that has been made available to the SQL Server process. (Plus some operating overhead.) If you are concerned about reserving memory for Windows Operating System and any other processes that you run on the server you can adjust your maxservermemory to a lower number. I have had good success with Jonathan Kehayias's "formula" that he posted here: How Much Memory Does My SQL Server Actually Need? The first paragraph has a good outline of how Kehayias would by default configure the Maximum Memory. Of course, this is not absolute and you will need to observe your SQL Server and determine whether more or less memory needs to be reserved. Brent Ozar has a sanity check post at: Memory Dangerously Low or Max Memory Too High So, you should not have a script to tell you that there is too little memory available. You should configure appropriately for what your system needs. Brent Ozar's post for a 512 GB Server suggests: 

We believe this will work for, but we currently have an active community of users and have estimated when we roll out the feature it will be heavily used. Thus we would also like to plan for the future to be able to scale (premature now, but we think this is a very simple feature and are hoping to design it well now to save us time in the future). If in the future we need to scale horizontally we do not think our design will scale very well. We don't believe an auto-incremented message_id pk will work in a multi node environment. We looked into setting a UUID for this column ($URL$ but have read that this can really hurt performance since the indexes will be large. Reading this article we see paging can be an issue too. $URL$ In our current design we don't really see a great shard key which can be used for all our queries. We would like our queries to reach one shared server if possible. So my question is what would be a an efficient way to implement this basic messaging feature so that in the future it scales well with the queries we need. I have only ever worked with a single instance of MySQL so I am not an expert on scale out design with MySQL by any means and am open to ANY ideas (complete redesign too)! We believe sharding will be inevitable since our instance types are not very large. PS: We know some may say NoSQL is a great option for this scenario, but we looked into NoSQL options for this feature (Cassandra,DynmoDB,Google Datastore, Azure DocumentDB, FileSystem like AWS S3 or Azure storage) for a few months but due to costs for performance (indexes are very expensive in managed NoSQL environments), lack of ACID compliance (we have other ideas which will need true transactions), and more we decided on MySQL. 

The lifespan of a login with enabled is set by Windows. I understand that in a Domain, the property is set at the domain level then propogated to the Windows account. I do not believe that you can directly change it. 

Or you might suffix the numbers so that all from Server1 end with a 1 and all from Server2 end with a 2. Or anything else that helps you avoid or limit the need for juggling values constantly or keeping the two machines aware of each other's state. 

create an for the fulfillment of a subscription instance. The subscription could have a different scheme (e.g. SUB-Number-Date) for identifying the subscription than you would use for an for an (e.g INV:InvoiceNumber). [I would guess that the subscription would be marked as already paid.] Create a log with a header for the subscription and a log to track each fulfillment. That allows you to track the details of each time have their fulfilled. In the case that a subscription is cancelled early, it could be used to refund the customer the unused sum. 

MySQL - At $URL$ discusses how to use EXPLAIN to determine the usage of indexes. Postgresql - At $URL$ outlines the usage of statistics in views such as . Microsoft SQL Server - At $URL$ in the sys.dm_db_index_usage_stats view which reports statistics such as seeks, scans, updates, and latest usage. 

EDIT There is no simple function to to what you want, unless you don't mind (0 row(s) affected) all the time. Let me explain more. You want to look at 'Semarang' for this. 

Maybe this will be a good starting point, just click on the blue XML to show the execution paths that have been cached. 

You want to only update 1 value at a time right? In order to UPDATE existing values, you will need to generate a table with the possible values and a selectable value, something like a counter, which I added into the @table. The reason for this is you might not always have sequential IDs with no prizes. Take 'Semarang' as an example of this. So we force a sequence by using IDENTITY. We will then select a random number against the IDENTITY column in order to always select an existing value. 

It is not really a question of the index, it is more a badly written query. You have a only 100 unique values of name, this leaves a unique count of 5000 per name. So for each line in table 1 you are joining 5000 from table 2. Can you say 25020004 lines. Try this, note this is with only 1 index, the one you listed. 

Ask your SQL Server DMV's about what indexes it might think you need, pick the biggest impact and have a look at what columns are used. If you have tons of inserts only feel free to add a fillfactor of 80-90, of course your mileage may vary 

Perhaps creating some denormalized data in advance would speed up the process. That, where possible, could reduce the number of rows involved in your ultimate query. This could mean a persistent table that you maintain. But it could also be implemented as a temp table (e.g. #criteria) that could then be joined more simply to get your final results. There is nothing special about those; it just reduces the complexity of the queries, giving the optimizer easier job steps to work on. Of course, if you are still suffering with the performance, you would need to analyze whether this is useful in your case. 

As you can see if you run the script, the contents of that you inserted will be displayed. Actually if you change fields from to then the data will be of the proper data type for your calculation. Here is my calculation: Code: 

Creating a Scale Out database at Internet scale is pretty huge step. You will face a lot of issues that are not critical on a single big database. From your notes I see that you understand some of the basic issues you face. Since Microsoft has papers on using SQL Server for scale out, I suggest that you study those first. Your scale out strategy will need to take into account the database server you choose. For Microsoft SQL Server you should first study: $URL$ This paper discusses the decisions that you need to make and why they are important. It offers 5 SQL Server strategies for scaleout: