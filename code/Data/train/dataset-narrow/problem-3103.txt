Let me present you with a toy example and a reasoning on image normalisation I had: Suppose we have a CNN architecture to classify NxN grayscale images in two categories. Pixel values range from 0 (black) to 255 (white). 

In (4), doesn't $\lambda(\textbf{x}^T \textbf{v}) = \textbf{x}^T C \textbf{v}$ hold for $\textbf{any}$ value of $\textbf{x}$? Why does (4) only hold when $\textbf{x} \in D$? I do not understand how their end up with (4). Thanks. 

Note: The images show basically binary values: white/black, however, assume that black values could be in the range of [0, 30] and white ones [230-255] with some random distribution. Prior to feeding the network with the images, we pre-process and normalize the data. This basically means (i) centring the data and (ii) scaling the data. A typical and simple approach could be to subtract 127.5 to all pixels and scale by 255 so as to have pixel values ranging [-1, 1]. Alternatively, another approach consists of computing the "mean image" and the "dynamic-range image" and centre and scale the images pixel-wise. Let us focus on the latter. Computing the "mean image" mainly means summing all images pixel-wise and taking the average pixel-wise. As a result, we obtain a NxN average image. This can sometimes shed some light on our task and help us understand the data we are working with. Likewise, the "dynamic range image" is obtained by computing the maximum and minimum values pixel-wise. The "dynamic range image" shows us which range of values each pixel can take. Hypothesis 1 During training, the kernels of the different layers in the CNN will be trying to detect the circle. Since the task is rather simple, the kernels basically would have to learn that there is a strong pixel-value-transition from "empty region" to "circle region" when a circle is present. Hypothesis 2 After normalising the images pixel-wise, we have that all pixels have the same range and same mean, namely [-1, 1] and 0, respectively. Final Hypothesis Consider a class 0 image (i.e. image has a circle). Before normalizing, pixel (i, j) has value 30 (dark), which happens to be the maximum that pixel (i, j) ever takes. Pixel (i, j+1) has a value 255 (white), also the maximum value that pixel (i, j+1) ever takes. So we have a strong transition that a kernel from first layers should be able to learn. But, if we normalise the data with "mean image" and "dynamic range image" isn't this strong transition going to be softened? In the particular, the transition would be from value 1 to value 1, since both were the maximums at their respective positions. From a clear and hard transition we have ended with a transitioned from similarly valued pixels. Conclusion Using this pixel-wise normalization we end up losing global pixel information, which might make the job of the kernels more difficult. I believe same conclusions might be drawn if "variance image" is used instead of "dynamic range image". Question Am I missing some point or is this normalisation no longer used? Bonus I also thought of Batch Normalisation, which sometimes is used at the first layer so that normalisation is done within the network. I thought that maybe the parameters "beta" and "gamma" used in BN might be able to fix this issue if it ever appears while training. 

I have been reading several papers and articles related to Principal Component Analysis (PCA) and in some of them, there is one step which is quite unclear to me (in particular (3) in [Sch√∂lkopf 1996]). Let me reproduce their reasoning below. 

Consider the centered data set $D = \{\textbf{x}_k\}_{k=1}^M$ with $\textbf{x}_k \in \textbf{R}^N$ and $ \sum_{k=1}^M \textbf{x}_k = 0$. PCA diagonalizes the (sample) covariance matrix $$ C = \frac{1}{M} \sum_{j=1}^M \textbf{x}_j \textbf{x}_j^T. \tag{1} $$ To do this we find the solution to the eigenvector equation $$ \lambda \textbf{v} = C \textbf{v} \tag{2} $$ for eigenvalues $\lambda \geq 0$ and eigenvectors $\textbf{v} \in \textbf{R}^N\backslash \{{0}\}$. As $$ \lambda \textbf{v} = C \textbf{v} = \frac{1}{M} \sum_{j=1}^M (\textbf{x}_j^T \textbf{v}) \textbf{x}_j, \tag{3} $$ all solutions $\textbf{v}$ with $\lambda \neq 0$ must lie in the span of $\textbf{x}_1, \dots, \textbf{x}_M$, hence (2) is equivalent to $$ \lambda(\textbf{x}_k^T \textbf{v}) = \textbf{x}_k^T C \textbf{v}, \qquad \text{for } k = 1, \dots, M \tag{4} $$ 

The question asks about the and I would answer in terms of space and time complexity. Let us say that number of input transactions are N(=20) and the number of unique elements be R(approx 900). Assuming your threshold count is quite small (which means very few candidates are pruned), the time and space complexity for size i candidates would be . So you see, if very few candidates are pruned, the space (and time) requirements become exponential. It might seem unintutive at first, given that you have only 20 transactions. But the bottleneck is the number of candidates which increases exponentially with number of items. 

I understand that your time series are unevenly spaced. In this case, why not simply use a library like traces and transform them to evenly spaced time series. 

Spark support for Mahout came from Mahout 0.10 release while you are using 0.9 release. So this should explain why you get the error. I would suggest using a higher version of Mahout. 

This does not mean there is nothing to learn from the data. In the worst case, there is nothing to learn from data, with the given architecture, hyperparameters and the time you are willing to let the network learn. 

This means that is in fact an RDD and not a dataframe (which you are assuming it to be). Either you convert it to a dataframe and then apply or do a operation over the RDD. Please let me know if you need any help around this. 

You should refer this survey paper on Anomaly Detection (from University of Minnesota). Please let me know if this helps you. 

I am not very sure how effective NN would be for this problem. The way I see it is that you have 48 entries in a time series and now you are trying to predict the next 4. Keeping aside the correlations for a second, it seems like there is very little data to "learn" from. I mean the power consumption is a seasonal thing (much like temperature) but you have only 1 years data so it should be difficult for NN to capture this seasonality. I would like to raise another point about the way you are using correlations. If you are taking correlations using the actual temperature value then I do not think that to be a good idea. Think of it in this way, I would be using a lot of power at both very high and very low temperature. So instead of using the raw temperature values, I should normalize them with respect to some "comfortable" temperature value by taking the absolute difference between the actual temperature and the "comfortable" temperature. For instance, let us say that "comfortable" temperature is 25C, then both 5C and 45C would be normalized to 20 and this looks more plausible as my power consumption should be high in both the cases. This might also explain the poor correlation that you observe for 3 months of data. What should be the "comfortable" temperature is an entirely different story altogether. Do let me know if this line of thought makes sense. 

Your use case boils down to categorizing news feed on an online forum and then finding out top-n categories. I would suggest you look at this Hacker News Categorizer developed by MonekyLearn. This way you can understand how to get started with such projects. PS : I am not affiliated with MonkeyLearn. 

Predicting "future" especially in the context of economics is a well-researched domain. For example, you can check out the research involving "predicting stock market using social networking". I am sure you would find such results for other domains as well. 

The Enron Corpus: A New Dataset for Email Classification Research paper describes the kind of data set you want. The paper mentions the following link to download the data set: $URL$ Additionally, the paper also mentions various other papers which have used smaller data sets related to email classification which may not be of much use given this larger dataset.