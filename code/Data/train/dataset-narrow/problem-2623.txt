Since any 3D vector can be written as a combination of , if we know how a matrix changes these three vectors, we know how a matrix changes any vector. So, to start off, let's examine the first matrix you have: 

In order to retrieve the projected coordinate of , you can simply bilinearly interpolate between each of the corners of the trapezoid: 

The pros and cons for animation blending are more or less the inverse of the pros and cons for sprite usage. In general, you want to use animated models for 

As the link suggests, a BVH is a data structure that is used to quickly compute collision queries for a set of polygons. It doesn't usually provide information about the underlying geometry itself other than possibly maintaining references to triangles in the leaves of the tree. In terms of your problem of finding out whether or not a point is within a set of geometry, a BVH is a good way to go. How to construct a BVH from a set of particles is a bit more tricky. In general, your BVH won't be a tight fit, and might result in some false positives for points that are extremely close to your point set. In order to reconstruct a 3D surface from your points you will likely need to use some implicit representation to generate triangles. This work was done back in 2001 with a very famous model: $URL$ Once you get the idea for how to think about these kinds of problems, solutions are available via the Computational Geometry Algorithms Library. More specifically, they have a solution to the problem of generating surfaces from 3D point sets: $URL$ 

In traditional Blinn/Phong shading, you calculate the diffuse term for a pixel by measuring the cosine of the angle between the normal at the surface and the direction of the light. So, in this case, a simple shader would look something like this: 

Set up a quad whose vertices (clockwise) are . Assign texture coordinates to the vertices with , respectively. In the vertex shader, define a uniform which is set to the frame of the sprite animation you wish to draw (between zero and seven for this example). When you draw the sprite at frame , for each vertex with texture coordinate and position , calculate the final texture coordinate with 

According to the MSDN documentation, the BoundingFrustum constructor only takes Left-Handed projection matrices: $URL$ 

If you want a more academic description of the problem, you can read What Every Computer Scientist Should Know about Floating Point Arithmetic 

we see that and do not change, but changes drastically. This means that every vector in the plane will remain the same, but every vector that has a nonzero coordinate will become distorted! So, to answer your question, neither of these are translation matrices. As your intuition might tell you by this point, you cannot translate a 3D vector using only 3x3 matrices. For that you need to use homogeneous coordinates. I would suggest you go through these steps with the last matrix you have written. Spoiler alert: it is indeed a rotation about the z-axis. 

With these ideas in mind, we used (roughly) the following system. Each save game file was saved as plaintext. The beginning of each save game file always started with a line that looked like: 

If all of the agents are running the same algorithm, then they will choose velocities that mutually complement each other and will avoid other agents. In some situations, you can cause oscillations like that awkward thing that happens when you walk directly into someone in the hall and you both try to move out of the way in the same direction, but the papers cover how to avoid that. For computing the two stages of the algorithm above, you can use Minkowski Sums to determine what the velocity obstacle is, and then use a linear programming model (such as the Simplex Algorithm) to determine the closest point to that avoids the velocity obstacle. Also, code for doing collision avoidance is available for your perusal and has been ported to C# to be used in game engines like Unity. This technique has been used at least in Warhammer 40,000: Space Marine, and maybe other games. 

In order to add a colored "light" around your character, you can do a similar trick. First, render an image that's black and transparent everywhere except your "light" area, where you give a pixel value of whatever color you'd like your "light" to be, and an alpha value of, say, 0.5. Then, you can render this picture over your scene using in exactly the same way as in the code above. After you read the wikipedia article, you can find a lot of information regarding the different ways to play with alpha in Java by reading the documentation 

You can use the same idea to generate different effects for the specular lighting as well. "Ramp" shading really only refers to the idea of modulating a shading value by a texture. EDIT: Team Fortress 2 uses a ramp shader in their rendering. You can check out the details in this paper: $URL$ Look under Section 5, at the subsection entitled "Diffuse Warping Function" 

As far as I know, most game engines don't do dynamic batching the way you've described it because of the problems you're running into. The rule of thumb is to make sure that everything is grouped so that you have as little actual processing (generating vertex buffers, loading textures, etc) during your render loop as possible, and to do this you can usually get away with having one or two state changes to switch a material or have a few extra draw calls. 

It depends what you mean by tile-based. Most 2D games that involve a level that you view from the top-down are tile based. In this way different tiles usually have different properties, and pathfinding such as A* is easy to implement. A lot of level editors use a tile-based sprite-sheet to define all of the aspects of a level, but then use them in a tiled way to construct a platformer. (See Tile Studio) Games that involve real physics, like Angry Birds, generally are not tile-based in that sense. Most of the characters are created as sprites and then their bounding boxes are used as physical objects to perform collision detection and response. It is hard to discuss the merits of "tiles" without knowing exactly what the game you're trying to develop is. You should start developing the game and see whether or not a tile-based approach makes sense for you. 

Yes, there are many performance implications to consider even when two objects share the same amount of geometry. 

Physically, adding a colored light to a scene without any of that color will not illuminate the scene at all. Think about it in science terms: what you "see" in the real world is light reflecting off of various objects. In real-life, the light spectrum contains all colors... so the corresponding wavelength of light gets reflected off of objects and you "see" their color. However, if you restrict this spectrum, objects that absorb the corresponding colors won't reflect any back! A better way to think about this is that if you shine a pure blue light in a pure green room, you won't be able to see the walls! That being said, I understand what you're trying to achieve in the game, and the best way to get what you're looking for is to use alpha compositing. Ideally, you can use this to generate the ambient effects that you want as well, and it will likely be faster than going through and modifying pixel values one by one. For example, one way to darken your scene is to generate a black image with alpha 0.3, and composite it using : 

The 2D coordinate system that describes your field can be parameterized the same way as your trapezoid. Think about it this way... First let's label the corners of the field in the unprojected space: 

To build upon Sean's answer, you don't need to make a separate FBO in order to render the effect you're looking for. Unfortunately, you cannot do it in a single pass because you're asking for two different blending operations for the pixels depending on whether or not your orange triangle is overlapping the light blue one. Fortunately, this is an excellent example of a place where the stencil buffer can be used. In order to generate the effect that you want you need to do the following: 

Let's rephrase your question: Given a rotation , and a position , we would like the rotated point to lie along the Z-axis (also known as the center of the camera). For this, we can use linear algebra: 

Fragments that fail a Z-buffer test will not invoke fragment shaders. The amount of screen-space that the objects occupy will impact performance, as fill-rate is a big deal especially on mobile devices. If you have large triangles that are close to parallel with the camera view direction then filtering textures on them will not be very cache-friendly. 

Animation blending: For example, if you want to be able to move and shoot a gun in a particular direction at the same time and want it to look good, you can use animation blending. This is a little tricky to implement, but Unity has these features built in for you so you don't need to worry too much about it. Smooth animations: Everything is interpolated, meaning that everything looks smooth and nice. Transitioning from one animation to another is fluid and looks good. 

Clear the stencil and color bits for your framebuffer. Render the blue triangle into both the color buffer and the stencil buffer. Render the orange triangle once with regular alpha blending testing whether or not the stencil value is non-zero Render the orange triangle again without alpha blending (painter's algorithm) wherever the stencil value is zero. 

You will need to make sure that your OpenGL context has a stencil buffer, which you can usually do during creation of the context. I believe that WebGL asks for it explicitly. Here is some (untested) pseudocode as well: 

When you detect a tap, set a timer () On each frame, decrease the timer by dt If the timer reaches zero, move the sprite a tile and reset the timer If the user releases their finger before the first cycle of the timer, move the sprite one tile 

There are two supported GLSL editors for OS X. The first, known as Shader Builder, comes with the Graphics Tools available from the Apple developer website. The second is the open source Shader Maker. Neither of these tools are strictly for OpenGL ES 2.0, but the best workflow would probably be to get the look you want before you determine how to get the look you can have. In terms of debugging shaders, it is very difficult to send debugging information back to the CPU. The only strategy that I have ever used is to use conditionals to output a specific color to the screen and look at the output. Tools such as glslDevil also exist, but not for OS X.