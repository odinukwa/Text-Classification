The table it is referring to used to exist there, but was renamed into a different schema and then dropped, or possibly just dropped (foggy memory) in order to get all references to it to follow into the other schema. Ultimately, the table was drop-swapped for one intentionally established as a copy via replication. It doesn't appear in information schema, doesn't appear in the SHOW TABLES list, can't be queried with SELECT statements, and DROP/ALTER TABLE for that name in that schema (after getting a view out of the way) just gets us: 

It's been too long to remember how we got here accurately, but we get the following error on MySQL start (though it does still start the service with InnoDB): 

...exactly as we expect. There is, however, now a view by the same name in its place (which refers to the new table in the other schema), and it has a corresponding .frm file, and of course, no .ibd file, as we would expect. Further, the table in the new schema works just as expected, as does the view in the old schema that aliases it. In fact, as far as we can tell, this is causing no problems...we just want to stop worrying whoever sees the log. We can even create and drop a table in its place (after moving the view and before putting it back), and still get that error. So why does MySQL expect an ibd file by that name in the old schema? How do we convince it to forget? 

If I were you, I would use a view to transform the date with the DATE_FORMAT function: $URL$ $URL$ To avoid refactoring your SELECTs, rename the table out of the way, and use the original name for the view. As long as you keep the view simple like that, it will perform about as well as the physical table (negligible overhead) on anything except for anything that might enjoy an index on that date field. Because the DATE_FORMAT function make the view non-writeable, you still have to refactor your write statements to use DATE_FORMAT to go the other way. 

If you want to store it as a hierarchical structure, the pattern that would best serve you is the "Closure Table" pattern, which allows for subtree operations (select/update/delete all descendants of x). A generic example (closure table and data table ) of what flagging an entry and all children at all levels below it would look like this: 

If 3072 bytes will be enough, you can enable innodb_large_prefix, or upgrade to a recent version of 5.7 to have it by default: $URL$ For a URL, it will help to use ASCII as the character set if the characters will be truly limited to that set. One byte per char. 

Without knowing the query set that's going to hit this structure, I'll give a generalized answer: I try to use the TIMESTAMP type for this, which works well with time/date functions, and is timezone agnostic (under the hood it is storing a unix timestamp). By default, this will only be a 4 byte wide key (instead of an 8 byte bigint), so if you index other columns (which always tacks on the primary key at the end...albeit hidden), those structures can stay skinny and quick as well...and also not eat your RAM ;-). TIMESTAMP by default has a resolution of 1 second, which is overkill for your purposes, but the easy use of date functions on it makes that worth it. Main downside: TIMESTAMP is based on a 32-bit unix timestamp integer (it stores that way, but displays as a date/time), so it does eventually run out. At that time (decades from now, when RAM is cheap) you can adjust it, and the date/time display/handling nature of the type will allow you to make the change transparently to your software. If you want to use it as an integer, you can use FROM_UNIXTIME(your_integer_here) to insert, and UNIX_TIMESTAMP() in queries that need that. I'm going to guess though that the date/time format is easier to work with for you though. Also, you do not require a second UNIQUE key, as any PRIMARY key implies this already, and is included for free in the storage/RAM price of your data table (which is sorted by PK as a B-Tree...your table is an index, and can/will be used as such). 

This is an alternative solution that is similar to Rick James' answer, but with a way to avoid app walking, and enjoy a fixed number (1 or 3) of statements to UPDATE the entries in all 3 applicable levels: Create a table for each of your well-defined levels of event (I will call them , , and to keep this generic), so that has field ., and middle has field .. If your case remains simple, you may be able to enjoy MySQL's ability to update more than one table simultaneously, and require only ONE statement: 

Here are some well written resources on how to implement that: $URL$ $URL$ $URL$ The best place to read about it is Bill Karwin's book "SQL Anti-patterns", which details it as an alternative to less savory patterns. 

For the purposes of diagnosing performance issues, what kinds of operations might wait their turn for this mutex, and what configuration options and resources might influence contention on it? 

It is part of the CREATE TABLE syntax. The manual explicitly recommends this as a supported alternative to symbolic links. Also, there may be a list of bad reasons to do this, and even a list of reasons that look good but bear gotchas, but there's also a perfectly legitimate list of good reasons for suitable cases. There are cases where this can and should be leveraged for performance when your storage hardware is tiered, and faster devices are limited in capacity. 

I can think of a perfect case for it, and we have tested thoroughly and run it in production...I call it the "fast lane" clustering strategy: If you do read-write splitting with a proxy like MaxScale, or your application is capable, you can send some of the reads for those seldom invalidated tables only to slaves that have the query cache turned on, and the rest to other slaves with it turned off. We do this and handle 4M calls per minute to the cluster during our load tests (not benchmark...the real deal) as a result. The app does wait on master_pos_wait() for some things, so it is throttled by the replication thread, and although we have seen it with a status of waiting on Qcache invalidation at very high throughput, those throughput levels are higher than the cluster is even capable of without Qcache. This works because there's rarely anything relevant in the tiny query cache on those machines to invalidate (those queries are only relevant to infrequently updated tables). These boxes are our "fast lane". For the rest of the queries that the application does, they don't have to contend with Qcache since they go to boxes without it turned on. 

You can look in the system dmv for os performance counters. With many of the counters, you have to collect the value, wait some time, then collect it again to get the number of transactions that have happened during that time frame. Something like this will work: 

I don't know PHP or MySql but the answer is definitely not creating new tables for each room. I'd have one table for rooms and one for messages and have the messages refer to the rooms (via foreign key). You probably want to consider archiving and/or purging as the data will probably grow quickly. Look into MySql's ability to partition tables and if there is an ability to SWITCH (this is possible in Sql Server) data in a partition from one table to another VERY quickly - it's just a meta data change. This could move Gbs in seconds vs having to physically copy and delete data from one table to the other. 

I doubt seriously that the entire deployment happens in one batch and therefore variables wouldn't persist throughout the deploy (but you could test it). What I think makes more sense is to just store any "pre" values in a temp table (maybe a global temp table) and then retrieve them during the post deploy. 

From this link: $URL$ Cardinality and Modality are the indicators of the business rules around a relationship. Cardinality refers to the maximum number of times an instance in one entity can be associated with instances in the related entity. Modality refers to the minimum number of times an instance in one entity can be associated with an instance in the related entity. Cardinality can be 1 or Many and the symbol is placed on the outside ends of the relationship line, closest to the entity, Modality can be 1 or 0 and the symbol is placed on the inside, next to the cardinality symbol. For a cardinality of 1 a straight line is drawn. For a cardinality of Many a foot with three toes is drawn. For a modality of 1 a straight line is drawn. For a modality of 0 a circle is drawn. zero or more [b4] 1 or more [b5] 1 and only 1 (exactly 1) [b6] zero or 1 [b7] Cardinality and modality are indicated at both ends of the relationship line. Once this has been done, the relationships are read as being 1 to 1 (1:1), 1 to many (1:M), or many to many (M:M). 

In your query above, it looks like you've answered your own question. Use a CASE statement and compare the "substring'd" value to zero and use THEN to return a "No" or "Yes". You're already doing this for the column "hasUlcers", except you're returning True and False. 

You feel like they should be separated because that makes perfect sense in the relational world. But as you said in the NoSql world and in MongoDB in particular, you want to group like-items together. I've not done extensive research on Mongo, but have spent some time with the online classes and I believe the answer is to store them together. You probably realize there is no such thing as a join in Mongo and therefore if you wanted to get 100 rows and get their corresponding images, you'd have to get the IDs for the 100 rows and then get 100 rows by their identifier (object_id or whatever). In other words, you have to do manual joins. 

I think source1 has the answer. He (Aaron) states that even when you get an "accurate" count, you could be blocking users trying to write to the table. As soon as you put your eyes on the result, it could be immediately inaccurate as writers were trying to put data in your table while you blocked them for the accurate count. So, I believe using partition_stats works fine for what you want. I wouldn't be afraid to test it though and it might be an interesting test at that. You could create a db of 1000 empty tables. Then a routine that constantly inserts and deletes data in random tables. Then run your partition query and run your count query and compare. I think what might work best is to use partitions and accumulate your data in a table and over periods of days or weeks, any table with consistently zero results is unused or abandoned. 

There is an excellent blog post $URL$ that explains what's happening. SQL Server allows for a set number of compilations based on their complexity. It groups them into small, medium, and large. For large compilations, there can be onlyi one compiled at a time, so let's say all of your procs are considered large, then each one has to be compiled serially. That could account for the blocking. I think there may be several approaches to the problem - consider more resources (more CPUs will allow more small and medium queries to be concurrent or may up the threshold for what is considered medium). Also, more memory may solve the problem. If you're like most of us, that might not be possible. Another option might be to review the ADO calls and see if the number of calls can be reduced or spread out so that not all calls happen at the same time. Reducing the number at any given time should reduce your wait time. If that doesn't work, consider fixing the 'compilability' of the stored procs. Maybe break them down into smaller chunks which might reduce them to the small or medium buckets and allow more parallel compilations. Or determine why the procs need to be recompiled each time. See if they can be rewritten such that they don't need to be recompiled. Finally, I'd consider using Plan Guides. These will allow the procs to be precompiled and may save some time. Hope that helps 

@JulienVavasseur is correct in that you need to format your queries using Ansi92 join syntax. If nothing else, it will make it easier for us to read and help you with your questions. However, looking at the problem, it seems as easy as adding a Distinct to the count of session.id to reduce it from 7882 to 752. When you ask for a count, you're going to get a count of the rows, not a distinct count of the sessionid (unless you ask for distinct).