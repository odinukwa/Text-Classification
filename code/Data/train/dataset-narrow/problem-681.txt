Considering the volumes you are discussing moving it to the cloud isn't going to be magic fix. "There is no such thing as a free lunch" A computer still has to process that volume, and that comes at a cost. If you are asking a 3rd party vendor to provide that theyre not just covering costs, but trying to make a profit too. Changing to a different db platform can also have a lot of hidden costs. Even though licensing might be "free", there can be additional costs in maintenance, integration and compatibility. Even learning a new system is an overhead. The spec you mention is not a big machine by server standards. I would be confident saying thats undersized for the volumes you describe. What performance tuning have you done already? What are the bottlenecks? RAM/CPU/IO? How much faster does it need to be? How fast is fast enough? Do you need a 20% improvement? 50% 100% You might easily get that with more RAM and some SSD's. If you cant add more resources, is there anyway you can make the server do less work? Archiving: Could you move old data off to a separate db. Or even better could you aggregate the data and purge the raw records? 

In SSMS right click on Database, select Reports, Standard Reports, Disk Usage by Top Tables. The report will give you number of rows and kilobytes used per table. 

Doing this via a macro seems to be an odd and unnecesary step. In theory you should only need to create the new schema once. I would create the new schema in postgress and use an ETL tool to map between them. I would export the access schema and convert the names to lower case using an editor or as part of the export process. This link gives details on how that can be done with vba $URL$ Another option is to import the schema and then use an alter table statement to rename the columns using lower case. It shouldnt be too hard to write something that can be reused. 

Jynus's answer is very good, but i would also add that in some cases more than one table in a query can have the same field name which is ambiguous and must be resolved. Although I guess you could use a full table name as a prefix. In my experience when dealing with extremely long table names that can have very similar spelling, using aliases makes the code much more compact, tidier and more readable. 

Another alternative is to combine the teachers and students in a single table. Add a flag or attribute to identify who is a teacher or a student. This has the advantage of allowing a single user to change from one to the other or even be both at the same time. 

Yes an index on the table you are selecting on will improve query performance (for the select). However as the table you are creating is huge the query is going to take a long time anyway. Especially as a single insert. Personally i would be looking for a way to break the insert into blocks or batches. As pararazzi has suggested dropping the index on the target table will improve insert performance. 

I'd like to create a SQL agent job to restart SSRS each day. In the past i've used a Powershell script executed by a Scheduled task taken from here. This has worked fine, but I would prefer it to be a single job in SQL agent to simplify deployment and admin maintenance. I have taken the powershell code and added a job with a single step. 

The 2nd and 3rd option make more sense if you spend more time working with tables as groups or with original and processed as a pair. 

SQL authentication: means using a user id and password which is not the same as your domain credentials. 

If you execute a use database statement, subsequent queries will only execute against that db. However: this is only for the current tab. In SSMS to see the connection context look in the bottom right corner of the current tab. You should see the current database name there. Or you could query the current db with: 

SELECT REPLACE(value,'apple','orange') from table1 Or to update: Update table1 set value = REPLACE(value, 'apple', 'orange') If you have multiple substitutions you could use a cursor and variables. 

The answer will depend upon what tools you have available. What database system are you trying to get the data into? Oracle? Sql server? Windows machine or unix box? One alternative could be install software on a 3rd machine. (your PC?) Extract the data and convert it into a suitable format to import. Another option could be to export the data to a .csv file and then import that. You could use Excel, MS Access or even sql express. However, this is going to become part of a regular process. I suggest you try again to have the required drivers installed. That would be the cleanest and most straight forward option. Everything extra step you have to take costs time and introduces risk. 

As each of the various workarounds I have tried so far don't seem to work... How can I force SSRS into doing this before a real user attempts to run the report? 

It seems the problem is that SSRS somehow treats these connections differently than if a real user opens the report through a web browser. Even though the report is run it doesn't appear to be handled in the same way as if a user is hitting it via the portal. And so.. the first user experiences a big delay. Using the F12 debugging features in internet explorer, I managed to reproduce and capture what the browser is seeing. SSRS only took 4 seconds to produce and return the report, however in the browser it took 122 seconds from trying to open the URL to the screen being rendered. 2 minutes!!! No wonder users are complaining. Looking at my SSRS log files ( Located in: C:\Program Files\Microsoft SQL Server\MSRS13.MSSQLSERVER\Reporting Services\LogFiles) it appears that the SSRS appdomain is restarting or reloading. The Log excerpt below shows 

Is anybody else experiencing this? Is this something I can fix or is it a bug? SSMS version 13.0.16106.4 It's a bit frustrating as I can't view the end of the create index recommendation. 

You could query before and after looking for the particular conditions you are wanting to update. For adhoc queries it's a good idea to run the query as a select statement using the same where clause before running as an update statement. The update should update exactly the same rows select returned unless the data has changed. 

Given the answers above. If this is a one-off bulk insert, I'd create a temporary table with an identity column. Insert the data to it. And then select the data and insert to your target table. You could set the identity start value, or start at 1 and add a fixed value when you do the final insert. I can't check atmo but a believe data writer privileges should allow you to create your own #tables 

There are few different methods. Assuming: you are logged into windows, on the same network, and the same domain: You can connect using the server name or you can use the ip address of the server. From sql server management studio: click connect > data engine. Enter the server name or ipaddress. Make sure windows authentication is selected and click login. Provided that you have been granted login privileges on the sql server it should allow you to connect. This microsoft guide has instructions including screen shots There are some variations on the theme above such as if the data is a named instance rather than using the default name. But generally if you have been setup to use windows authentication you just need to know the server. If you are using other products and connecting via ODBC you may need to configure a connection first. 

I have a an update statement which uses a subquery to filter records. My input table contains dates in a varchar field, and some of them are invalid. 

I would add an Identity column as a primary key to uniquely identify a single row. This makes life easier for applications. I would add a Question Id column. So that you can see Student A, for Question B selected Answer C. Optional: If you added a unique index on Student ID & Question id, Students could only select 1 answer per question. 

Right click on the database in question, select Properties> Options> set Auto Update Statistics = True. click Ok. 

Personally i have staging tables for extract, transform and persistent data storage. Whether you do full exports or incremental loads will depend on what tools you have, your strategy and whether your app schema and data support it. Sometimes you cant avoid full exports. Adding a column to a dimension isnt a big deal, but backfilling historic data could be very difficult or may not be possible at all. Trying to reconstruct how an app looked at a point in time retrospectively would be a major undertaking. You would need a very good case to justify that. All of the things you mention are possible, but only you can decide if the cost/benefit is worth it. 

It looks like there is an odbc driver for project You could try attaching access directly via a linked table. 

If no attributes apply the XML field is NULL. If any of those attributes do apply then the application inserts an XML document and relevant nodes. A valid example looks like this: 

Use a different schema i.e [Processed] and keep the table name the same. Add a prefix so that all processed tables appear together. Add a suffix to the table name. So that when you sort by name the original and processed appear together. 

I have an SSIS package deployed to the SSIS catalog on my Data warehouse which has run successfully for months. The package is executed by a SQL agent job. Now it fails intermittently with the following error: SQL agent history says says: 

Once you have a clear requirement, it will allow you to shortlist viable solutions. Within your shortlist you can start to express preferences and pros and cons for the evaluation process. 

Why would it only fail sometimes? Why is it failing now? What is the root cause? How can I fix this? 

As i understand your question you have 2 unique ids in your table. If they are both truly unique and have no other purpose (ie no business meaning) you have a redundant field. If you're developing a system that will merge data later and you want to assign a persistent unique id now then use a GUID. If you want performance and simplicity use an int. Other than enforcing uniqueness... Putting an index on a column that you won't search by is pointless. Searching on columns that aren't indexed will perform poorly. Ideally if you can enforce uniqueness and improve query performance with a single column you will have a more efficient design. 

A select on a single table should always be faster. As soon as you have found your vehicle you already have all the details. However you lose the efficiency of normalization. For example if 1 car had many models with different options. Is this a reference db of all cars? Or a list of second hand vehicles? Would there be many examples of the same make/model with the same options? Edit: i should qualify my answer as being generic rdbms rather than postgres specific. I defer to @Erwin's detailed answer specific to postgres 

Start by checking max memory. Instructions here: $URL$ You should set the max to be low enough to leave space for the os and any other apps. Which in your case is going to be very low. Maybe 500mb-1gb 

Indexes don't have to be unique. Primary keys do. The purpose of a Primary Key is to uniquely identify a single row of data. If you don't have something that is naturally unique then add a column such as an identity column and define it as the primary key. It's standard practice, and will help you later if you need to update a row. If the table is being queried for analysis or reporting a single table is fine. Do some analysis on the types of queries you are performing and add indexes on relevant key columns. Good indexes will significantly improve query performance. For example if you are looking for results for a single store, by adding an index on the store id, you could exclude the other 299 stores from the select. This reduces IO and speeds up the query. If you have years worth of data but are only looking for things that happened in the last week then adding an index to a date column may be a big help. Look at your queries and see if there fields you are regularly filtering on. Start with those. Ensure the db has updated statistics for the table and see if performance improves. The results you get will vary depending upon what you are trying to do. If you are trying to aggregate records (i.e total sales for each store) then the query may still have to read the whole table. Below I've added a script with a basic test example. 

But it provides no further details about how to install it. It mentions an ODBC Driver Manager but not much more than that. So.. How do I install RODBC? Do I unzip it to a specific path? Is there a utility or tool required to install it? Do I run a script? 

I have an intermittent fault in my SSIS ETL process when I try and process my OLAP cube. There are no error messages in the SQL log, and there's nothing else significant running at the time. The only error message I can find in the event log is: 

Your results may vary depending upon machine setup and load, But I think you should resonably expect to get your query down to ~1 sec or less. 

If you want to use that as a naming convention, yes you can do that. If you want the parameters to show on the report you can do that too. You can add them to the title, a header, footer or anywhere you want on the report as an expression. If you are talking about saving the report output using parameters in tge name... i don't believe you could enforce the behavior but you may be able to do it programmatically. But if you want the report name (.rdl file name) to be dynamic and change as you select different parameters, then no, you can't do that. 

Firstly: SSRS PowerBI on premise is not an update to the original SSRS service. Instead it installs its own service. The original still exists. When i installed it i used the existing reportserver db and same URL. In hindsight that wasnt such a great idea. The install changes the db and hogs the URL crippling the original SSRS service. We found the reporting services configuration manager for the new service allows you to create a new db and assign a new url base. To get the original service working again we had to restore our reportserver db. We had to play around with the urls and ports to get them back to normal and it now works again. I would highly reccomend if you are installing PowerBI on premise that you consider giving it its own db and URL. 

I'm building an SSAS OLAP cube on Meteorological data. Some of this data can be totaled (eg rainfall) and some can't (like wind direction). I want totals, but only on data sources where it makes sense. (Where the interpolation method = 3.) I've tried adding SCOPE with this: 

Domain groups are a nice way of granting access without having to grant access to individual users each time. For example if a department or team should all have access to a system (e.g HR or Payroll) you can grant access to the team as a whole. As users are added to the team in AD they automatically inherit access to the db via their membership of the group. This can be overridden by denying individuals access if required. Eg all of HR have access except user xyz. If SQL is configured to use both modes this referred to as "mixed mode" authentication. A login gives you access to the SQL instance. You also need to be granted access to individual dbs. Once you have access to sql the method becomes irrelevant. Once you're in, you're in. Technically there is third method related to contained databases. But in my experience this is rare.