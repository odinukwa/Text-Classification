The kind of artefacts you're seeing - the squares - are often caused by not sampling some pixels within the blur range. Since you're only using 9 samples, and your blur is quite soft, this is likely the case here. You're effectively blurring a lower-resolution image, but the next fragment over is blurring a slightly different lower-resolution image, and so on. There are many approaches to blurring, but since you specifically mention Gaussian blur, I present two options without changing the blurring algorithm: 

You'll want to do it right at the beginning. As you probably know, if the target is moving at the same velocity as the tower, it should aim as if neither are moving. If the tower is unmoving, it should be the same calculation you already have there. So, right at the beginning: 

(...according to the manual under Mobile Optimisations; desktop doesn't seem to need this) If you're worried about the performance implications of updating a small mesh every frame, perhaps it'll be of comfort that I use a similar technique to have thousands of GPU-simulated particles and decals. Also, while it might seem like a kind of hacky way to do things, this is probably how I'd do it in OpenGL, too, though I'd look into instancing or perhaps a geometry shader. One way or another, using a mesh to map from space on one texture to space on another is the best way to do this I can think of, assuming I understood your question. 

will give you an answer between 0 and 1 (inclusive), but the result is more likely to be close to zero, following a quadratic curve. 

Shadows match the shape of the casting mesh. This is very limiting, and is probably the biggest technical reason not to use stencil shadows. Tree leaves, bushes, wire fences, rips in cloth, and other common shadow-casting game elements use alpha transparency to define the shape of the object. But since stencil shadows match the mesh and not strictly the visible parts of the texture on the mesh, they can't be used for such objects. For example, a cluster of leaves in a tree will usually be a texture of a cluster of leaves on a quad. Stencil shadows would only be able to show the shadow of the quad itself (and it'll have problems with that since a quad isn't a closed mesh, but this is a minor detail I'll skip over for this answer, as it's generally able to be worked around). Can't do accurate soft shadows. You've pointed out that Doom 3 has been modded to allow soft shadows. This version here, at least, uses shadow mapping for soft shadows. I've also seen screen-space techniques for softening hard shadows, but they're prone to all sorts of problems. Can't do shadows on transparent objects. There are workarounds for this -- rendering shadows for transparent objects in separate passes and compositing them afterwards. But besides difficult workarounds, the nature of stencil shadows is such that each pixel on-screen is either in shadow or not. This is also the reason for no soft shadows as mentioned before. The generally preferred and most robust implementation of stencil shadows, often called "Carmack's Reverse", is actually patented, as Angew pointed out in the comments. (Not patented by Carmack; he had to work around it for the open source version of Doom 3) 

Let me list some general compared optimizations related to Blitting pixels to a surface(from my experience). 1)Usually palette images(indexed images) when blitted, will under go one extra level of redirection (to get the color).So they will be slow when blitting when compared to true color images. 2)True color pixel data (assume With out Alpha - say24 bit data) can be blitted very fast as we can do a memcpy for each scanline of the image(if we are copying a part of the image) on to the device frame buffer.If the data to be blitted is a full image, then we can directly memcpy the whole data which is much faster! 3)Blitting Alpha pixel data will be the costliest as it will include calculating the each component resultant and we need to pack it again to RGB data. In simple terms more operations for each pixel for getting final color! 

I gave a quick try(infact took some time to get used to shader lab syntax ;) as i was trying after quite some time). Let me know if this shader works for you. Basically checking the frag color by sign value of dot product of view direction and normal of the vertex. Btw, I set the culling to off. Thanks for letting me trying this out. Very happy with the output ^_^ . I'm aware of doing without shader but thought of giving this a try with shader. Sorry for replying abit late to your post. Link : $URL$ Update: Here is the explanation of the code. This is infact how Back-face culling works. The Dot product between The camera view direction and the vertex normal will tell which way the vertex is oriented. Whether its away from camera or towards the camera. Dot product value of those two vectors goes negative if the vectors are away from each other. So , the same math is used to find out which texture we need to show. In the code, I took two texture samples (_FirstTex,_SecondTex) as properties. Vertex Shader 1) First we need to convert vertex co-ordinate which is in object space to Clip Space. 2)Next we use world View space direction(_WorldSpaceCameraPos) provided by unity cg includes(UnityCG.cginc,UnityShaderVariables.cginc) and normalize it. (Infact there is no real importance of normalising here as we just need sign of dot product.) 3)We then need to convert vertex normal to world space.(Please check the comment there for converting to same co-ordinate system). We can do the other way as well(converting view direction to object space). 4)Find the dot product and save the value in color.w to pass it to fragment shader. Fragment Shader: 1)We find out the sign of the dot product value and based on that decide which texel to fetch. 2)Return the texel color. Let me know if you need any further clarification. 

So, here if you see, we are limited not to use memcpy as we need to check each pixel's color validity! 

HCY is described in the Wikipedia page as "luma/chroma/hue". This page, on the other hand, appears to use the "HCY" shorthand and shows how to convert it to and from RGB. 

If you're drawing your terrain before the water tile, the terrain is writing to the z buffer (glDepthMask), and the water tile has depth testing enabled (glEnable(GL_DEPTH_TEST)) with only fragments closer to or equal to what's already in the depth buffer passing the test (glDepthFunc(GL_LESS or GL_LEQUAL)), and your water shader doesn't modify its depth output, then most drivers (perhaps all, but I can't recall if this is a requirement) will discard fragments that are behind what's already been rendered without having to fully process them. If you're adding the water texture in a post-process stage, and thus the depth buffer or the depth of the surface of the water are not available to you, then you could mask out areas of the frame buffer using the stencil buffer. This could be done in a post-process stage after rendering the 3D world but before doing the water-stage, or you could create the stencil buffer mask as you draw the 3D world in the first place. 

will give you an answer between 0 and 1 (inclusive), but the result is more likely to be close to one, following a square root curve. And since the min and max of the output are still 0 and 1, this distribution can be mapped to any range (given min, max, and the exponent p) like so: 

I don't believe there's an elegant solution to that problem. It's something I put a lot of time into investigating a few years ago writing my own collision detection. The solution I ended up using was iterative: 

For a one-off change of velocity, I'd recommend changing velocity yourself. So rather than applying a force to the object, try something like after you create the shot and figure out the regular shooting velocity. Secondly, it'd probably be best not to count on the speed difference to avoid the shooter being destroyed by its own bullet. When you create the shot, you could use Physics2D.IgnoreCollision to make it impossible for the shot to hit the entity that created it. If you do need the shot to hit its creator (such as with a ricochet), you could use the same function to let them hit each other again after the shot has already travelled a certain distance. Let me know if any of that is unclear :) 

With any asset with UV coordinates, it's good practice to have parts that need more detail take up more area than parts that don't (usually, anyway). So you can have the UV coordinates for the detailed parts of your normal map scaled quite large, and the UVs for the less detailed parts scaled quite small. You can also have two different sets of UV coordinates -- the first used to sample the diffuse texture and the second used to sample the normal map. The second UV set could be laid out to make more efficient use of the texture space based on what needs more detail. 

I haven't used Corona.It needs a license if you need to publish.Cocos2d on the other hand is really flexible and stable 'graphics' engine right now. I would suggest Cocos2d-x (C++ version) as objective-c version has some performance issues when your update cycle is Overloaded.This performance issues is ONLY because of the message passing System in Objective-C. As others said,You need to depend on the support of 'third party Engine' if any thing NEW comes up!But cocos2d is flexible at that moment! If you are targeting multiple platforms,Cocos2d-X is ready for that as well! Check this link - $URL$ 

I'm learning OpenGL and really like to know how the interaction with the Graphics card will be. I feel understanding how it was implemented in the Graphics driver, will let me know complete internals of the opengl(With this I can know what stages/factors influence my decisions regarding performance in opengl). Are there any ways for this path to proceed.Does exploring the 'Mesa lib' will help me in this aspect? Am I in the right path? [I posted this question in SOF but it seems here is the right place for this Question.] 

I haven't worked on pyGame before.But , A quick look on souce code of it, lead me to assume that it is using 'sdl' Blit functions under the hood. Usually Sdl blit will be very faster and optimized so, just make a not of the above points and profile once again! Good luck! *Update:*Setting color key is like adding one extra check when blitting each pixel to the surface.Some thing like this - 

Check these frameworks I found out related to this architecture... www.burgerengine.com PushButtonEngine Arthemis Framework - $URL$ Having a look at Unity Api. You can find a lot of stuff regarding the Component based architecture. (Will update the list as soon as I find somemore...) Update: 

I was earlier in the same confusion and finally found out there is no proper solution for this. My consideration for Google play was not for ranking but for using the leaderboards and multiplayer features. And again If I go with Google Play Services alone, I miss a-lot with Facebook login. So you decide which has more weightage and pick one of it. If you are targeting only for Android I suggest going alone for Google Play services as most of the users will have an account. I picked Parse.com and implemented my own leaderboards and used Facebook for logging in (as I targeted iOS as well, I see FB has more reach). In between, I tried OpenKit and dropped as its getting closed soon. Now i'm checking NextPeer which is a bit close enough to solve my problem. Will update once I play with NextPeer. Hope this helps! 

To my knowledge the best project i have seen is Burger Engine. jst download the code and check how well they have implemented.The whole thing is data-driven from xml and they used very well entity based architecture.worth looking at it. $URL$ 

OR go for one of the formulae in that Wikipedia article I linked that emulates black and white film. 

Using different values of p can bias you towards the minimum or maximum value depending on your needs: 

One problem I see is that your timer only decreases on OnTriggerExit. OnTriggerExit only happens as an object leaves the trigger area. On subsequent frames, OnTriggerExit isn't being called again, so the timer is no longer going down. You want your timer to go down as long as it's not touching the trigger, right? So I'd put in another bool (call it, touchingTrigger, for example). Set it to true in OnTriggerEnter and false in OnTriggerExit. Then have your countdown go down in Update. Something like this: 

The lack of an elegant, exact way to calculate what you're looking for is at least part of the reason most physics engines have a skin width or skin thickness parameter, defining how close you want to get to something and call it a collision. 

Scalability. While shadow mapping is neither cheap nor perfect (ed: but stencil shadows aren't cheap either), it's relatively easy to trade fidelity for performance by changing the resolution of the shadow maps or changing the filtering for shadows. This makes it relatively easy to tune the shadow quality / performance for different systems. As you've pointed out, this often isn't perfect, but a lot of this will depend on the perceptiveness of the player, and the resolution they're playing at (higher screen resolution will make it easier to see the blockiness of shadow mapping). Some AAA games (like Fracture) do an amazing job with shadow mapping just by picking the right filters and tuning the right knobs, while many just slap it in there and say "good enough". Cookies (textured shadows): Add the texture of a spotlight, or stained glass, and you can emulate all sorts of real-world light sources. This is doable with stencil shadows, too, by the way, but involves almost all the setup of shadow mapping anyway (the same projection matrices used for projecting the light's texture on objects are the ones used for shadow mapping). Transparent shadows: Most games don't attempt much with this, but transparent shadows are possible with shadow mapping. I don't just mean alpha cutouts like trees and fences like I mentioned before, but I mean partially transparent shadows, like from smoke and dust. 

EDIT: In case anyone's wondering how we can be sure A->C->D is longer than A->B->D by just eyeballing it: angle B is wider than angle C (also eye-balling, unfortunately, but B is closer to A than C is to D, so hopefully that helps). So A->B->D is closer to a straight line than A->C->D. The shortest distance between two points is a straight line, so A->B->D is the shortest. By now you're probably wondering why I didn't just give each edge a length and be done with it -- and so am I.