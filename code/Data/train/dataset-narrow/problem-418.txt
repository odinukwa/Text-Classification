The only time I would ever consider a delimted list is if there is no chance you will want to look at the data separately. Then indeed it is faster. This is a very rare case, however. And in what you are doing, there is an approximately 100% chance you will want to look at individual payments separately. Delimited lists are harder to query (and updating if someone made a typo, ugh) for individual elements (and generally slower for this than a related table would be) and harder to maintain the data integrity. Databases can easily handle many millions of records in tables that join with correct indexing and design. If you need more performance at that point typically, you partition the database. Sit down and read some books on database design and performance tuning. The rules for good design are different in databases than applications. Please take the time to learn them. 

You should get some significant performance improvement using the FetchSize connection string property. The Oracle documentation for the max value is here. 

You are correct that currently (up through SSAS 2017 Tabular) is is not possible to have mixed mode partitions. The whole model can operate in DirectQuery or In-Memory. The DirectQueryMode connection strong property can specify whether SSAS should use DirectQuery or In-Memory for answering the current query. But that's not honestly too much better than just deploying the model to two different databases, one DirectQuery and one In-Memory. There is a similar connection string DataView=Sample which has SSAS answer the query using a special cached partition marked as the sample partition. 

The Customer dimension is joined to the measure group at a non-key granularity. If the Customer Name attribute isn't related to the attribute you join on then data repeats in reports. You need to do one of two things: 

Update 2: I was hoping to not do this, but maybe the exception handling is part of the problem. Here is the create procedure for this, and also another stored procedure being called in the exception handler. Sorry for the length: 

I have two reports from the same customer, where they claim that an error occurred that implies a SQL Statement didn't execute. These are very simple statements, setting a value in a single column. I can see in my application logs that the command executes, and no errors occur. In one case, the records affected value is even verified to make sure that exactly one row was affected. And yet, when they view the data elsewhere, the value is unchanged. What would cause SQL Server to respond successfully to an update statement, and yet when that data is read within the next minute or two, that change is not reflected in the returned data? I'm kind of at a loss of where to even look. SQL Server logs? This is an extremely rare occurrence. It's not reproducible. Customer care informs me that this behavior occurs once every 6 to 9 months, with tens of thousands of statements working in between occurrences. But they know immediately after it happens, and could look at logs. This customer is using SQL Server 2008 (R2 I believe), my application is using ADO.NET (.NET 4.0) and written in C#. These statements do not have an explicit transaction, and are not occurring as part of a batch. One area where this has occurred is setting a single bit column to 1, for exactly one row. The other is updating a single record with an integer value. Sample query: 

And for SQL 2016 Tabular models using the latest 1200 compatibility level, use the following script. Just change the name to the desired database name and the DOMAIN\GroupName to your administrators group. 

My recommendation is not to attempt to do the sequence logic in MDX. You will be disappointed in performance. I would recommend the following approach: First create a view (or a physical table if you prefer) which returns one row per part/customer/person per day showing the currently effective value. The view would look something like this: 

The only time you need to use Schema Generation is if you did top down development and have never generated the schema before. Now that you have previously generated the SQL schema, you can just script out all the objects in SSMS and provide that script along with the SSAS project to other developers. Then the new developer would deploy the SQL scripts and edit the data source in the SSAS project to point to their database. This is the most straightforward approach I believe. 

I can be reasonably sure there are not conflicting updates to this record (record is owned by a single user). There would be a lot more wrong than this if that weren't the case. Newer versions use a stored procedure here, but it would not be possible for me to change that in this version. 

I have a query, that as far as I know has failed exactly one time. It's a simple select count(*) from one table, no joins. But at least this once, executing that query resulted in no data read from SqlDataReader. Not even null, just nothing. First call to Read returns false. No exception was raised. Has any one ever heard of that before? Any scenarios you can think of that would cause it? I'm not even sure what to ask for to look at beyond SQL server logs. It's not something we can duplicate. I am assuming I'll have to chalk it up to a fluke and move on if/until it becomes a chronic problem. Here's a similar query: 

But mainly I would recommend upgrading to SSIS Enterprise edition and using this connector. Here is the one for SSIS 2014 but you can search for other versions easily. The performance of that connector is much improved. 

What happened is that developers added new data sources instead of adding tables from an existing connection. In the future go to the Model menu... Existing Connections... Open. Then choose another table or query to import. This process is documented here. I don't believe there is a UI way of switching an existing table to use a different data source. The best approach is probably to just delete and recreate that table with the process above. You should be able to delete the extra unused data sources from the Existing Connections dialog. 

/netonly tells it to just use those alternate credentials when talking over the network. Change the OTHERDOMAIN\OtherUser to match the username needed to connect to the remote system. And then the last part is the path to Visual Studio 2015. Once that's done you should be able to deploy without supplying alternate credentials again. By the way, I would highly recommend you install the free BIDS Helper in order to leverage the Deploy SSIS Packages feature which makes it easier to deploy single packages. You still need runas.exe though. 

I'm not sure if there is a named pattern for this, or if there isn't because it's a terrible idea. But I need my service to operate in an active/active load balanced environment. This is the applicaiton server only. The database will be on a separate server. I have a service that will need to run through a process for each record in a table. This process can take a minute or two, and will repeat every n minutes (configurable, usually 15 minutes). With a table of 1000 records that needs this processing, and two services running against this same data set, I would like to have each service "check out" a record to process. I need to make sure that only one service/thread is processing each record at a time. I have colleagues that have used a "lock table" in the past. Where a record is written to this table to logically lock the record in the other table (that other table is pretty static btw, and with a very occassional new record added), and then deleted to release the lock. I'm wondering if it wouldn't be better for the new table have a column that indicates when it was locked, and that it is currently locked, instead of inserting an deleting constantly. Does anyone have an tips for this kind of thing? Is there an established pattern for long(ish) term logical locking? Any tips for how to ensure only one service grabs the lock at a time? (My colleague uses TABLOCKX to lock the entire table.) 

Employees can have more than one address, you should have a join table to address for the many to many relationship. The phone table is designed incorrectly. You do not want to add a column when you get a new phone type. The whole leave thing makes no sense. Please explain if this is a system to manage leave requests? You should have individual records for leave accumulated and leave taken. Do you need an approvals table? The balance shoudl be figured out at the time of query. The pormotions table makes no sense at all. You want a table to store the organizational position (And it should be a history table so it should have start and end dates). This table should be updated everytime the postion title changes (they are not always promotions). Same with salary, you want a start and stop date. You seem to be missing data on who the person reports to. Generally reporting will need to be able to sort through the reporting hierarchy. YOu also seem to have designed this solely on the basis of the data entry GUI. This is HUGE mistake of epic proportions. With this kind of information, reporting is a far larger problem and you need to consider reporting and how you will need to see the data over time. For someone who works for the comany for ten years, what type of information do you need to call up ablout his history? This is a business critical database and should have been designed by a professional database programmer. There are legal implications to this data, there are security concerns. How are you planning to protect this information? Pretty much all of this should be unavaliable to most users and admins. It should be encrypted. This is critical privacy data. Employee records typically include benefits information. They also typically include information concenring awards and performance appraisals and performance warnings. The use of ID as the PK is a SQL Antipattern ($URL$ You should use tablenameId. 

I would study this white paper and the AsPartitionProcessing sample for ideas. Ignore that it targets Azure Analysis Services as it should work against your SSAS instance if you use that servername instead of asazure:// as the servername. It uses the Tabular Object Model (TOM). 

In SSAS 2014 Tabular partitions inside a table process serially. In SSAS 2016 Tabular partitions inside a table can process in parallel. So in your version partitioning won't speed up processing if you process the whole table. However partitioning is great for use in incremental processing. If you partition by year and only 2017 rows changed then you can just process the 2017 partition. Also to archive off data older than 10 years old you can drop the oldest partition. I don't know where the 2 partition rule you mentioned came from but don't believe it makes sense. If you have a reference for that assertion with some context then feel free to share. 

P.S. I did edit this a little, changing names of the procedures, the name of the table only, and removed comments only. 

These images show the two elements with the highest cost. They have identical properties between both the query plans. The difference in the final row totals is the result of joins with other tables. But those joins all have a low relative cost. 

This may be a terrible question, because I'm not sure how information I can include to help. We have data segregated by customer. One customer apparently has higher volume of data. The same query ran for a small customer returns in 2 seconds, and the result is 11 rows. The larger customer takes 47 seconds, and the result is 6600 rows. This is a complicated query with 11 joins. This is just for a report, but the report is timing out and the operator is complaining. It's possible the difference is just the volume of data, but I want to investigate. When I look at the query plan, the percentages for the query cost are exactly the same between the two. There are no suggestions for indexes. In both query plans, I can see the highest cost is from a join to a table that has 3.8 million rows. However, both sets of data are joining with that same table. In both cases the resulting "actual number of rows" is 3.8 million. In both cases this join has a clustered index scan that is 39% of the query "cost". So my question is this: Do the percentages and query cost even mean anything? are they referring to the estimated data from the query plan, and do not reflect the real cost? Otherwise how could the same join make up 39% of the "cost" for both, but one takes 2 seconds and one takes 47 seconds? Is SQL server lying to me and the actual difference in cost is the "nested loop" for the inner join that produces the final row count even though it lists that as 0% of the cost? 

If your drillthrough command includes a dimension attribute from a many-to-many dimension then it behaves as described: showing a row zero to many times. The easiest solution is to create a drillthrough action and mark that action as the default drillthrough action and specify the columns you want to include in the drillthrough results. If you don't include the many-to-many dimensions then it will stop skipping or duplicating rows. 

In SQL Server Management Studio you can connect Object Explorer to SSAS, expand to the table, right click, choose Partitions, then click to process one or more partitions and do a Process Clear: 

For some reason, SSMS doesn't provide this New Database menu option for SSAS Tabular. But you can easily create a new empty or shell database with the following XMLA script. Just click the XMLA button in the toolbar and then paste in the following. Edit the ID and Name property to the desired database name. And edit the DOMAIN\GroupName to set the group that has admin permission. This will allow members of that group to deploy over this database without being SSAS admins for the whole instance. The following script is for SSAS 2012 SP1 or 2014 Tabular models: