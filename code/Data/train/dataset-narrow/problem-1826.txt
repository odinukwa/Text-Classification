In nginx folder, I am seeing this folder with 777 permissions which I can't access in SFTP. Is this normal? $URL$ I don't understand that that little arrow on top of the folder means. Sorry for being a noob! 

W3 total cache is not performing very well with this setup so I am thinking of abandoning it all together. That leaves me with no caching, no CDN at all at this point. Any help pointing to the right direction with regards to my caching needs, would be much appreciated. 

May work for you. All ssh connections (or port 22 connections rather) get tagged as "sshbrute", second command drops any new ssh connection if there are more than 4 new connections in one minute from a particular IP. Don't mess up on your password, or if you do, do it slowly. 

is set to only accept incoming SMTP connections from the 3 other servers (think firewall and connecting to port 25). The server is set to only accept mail from the 2 virusco.com servers. the two virusco.com servers are configured to accept mail from the world as MX servers for example.com My server - foo.org - can't get to mail.example or backupmx.example (by configuration design) so it delivers to mail.virusco, which is configured to forward the mail to mail.example via normal SMTP MX decision. This means that if mail.example is down, AND backupmx.example is down, mail.virusco simply holds the mail and tries again "later" If mail.virusco is down for a bit, mail goes to backupmx.virusco, gets scanned, and then backupmx.virusco follows the MX decision tree and tries delivery to mail.example, backupmx.example, mail.virusco in that order, and if none are successful it too will sit on the mail for "a while" and then retry delivery. You can even take backupmx.example out of the system entirely and it will still work in that fashion. A lower priority (higher value) MX server will try to deliver to the highest priority (lowest number) server, until it reaches itself, at which point it will sit on it and try again later. 

I got into recovery mode and ran which allowed me to get rid of my bad NFS mounts in (and I prevented further issues by adding the option to the NFS entries), but then my boot (in non-recovery) seemingly got worse -- I got no output on the screen after selecting normal boot from the GRUB menu! I then added to the boot option list and everything worked. I have no idea how those two were related, but that's what fixed the issue. 

Say I'm debugging an Ansible playbook and want to quit after a given task (and not run through all of the following tasks). Is there any one-line magic command available, or do I have to manually create an exit/assert task? From the manual, I see that there is a flag, but I don't see anything like an 'end-at' counterpart. 

Running the should return a error. However, some ISPs will return the IP of an ad server, a search server, etc. instead of a proper NXDOMAIN. What service provider are you using? What DNS servers are you using? What happens if you use a known-NX-issuing server for DNS requests? I'll bet that if you try (ie, ask Google's public server about $URL$ you'll get a proper NXDOMAIN return. 

You would need to edit the webserver config and specify the index document. Your attempt to modify that directive was correct, you just used the wrong syntax - %20 is the URL code for a space - so instead of it would be Or, if you are trying to get to it for testing purposes, then ought to get it (or it does on my local apache install) 

is there a similar rule to rewrite the FROM name ? UPDATE After reading the postfix manual, it is desirable not to alter the message headers. However, how can I tell postfix not to write any message headers and use whatever names i have set up in Google Aps ? 

Using Postfix on Ubuntu 16 with google smtp server. Everything works fine. The only problem is that I can't seem to find out how to change the FROM name in postfix. I have rewritten the email address successfully in using: 

User1 is the owning user, a group that User2 is a member of is the owning group, set permissions to 750 

the option sets a umask for the internal-sftp program/subsystem and any files uploaded through it IF the user is a member of the group. Personally I also the users so that they can only access their directories - check the option as it applies to a directive in the file. 

You need to set up a mail server that is driven by db tables, or arrange to export your db tables in an appropriate format every so often. Lots of ways to do this, but the way I use and seems to be fairly common is a Postfix+Dovecot+Mysql solution. If you need/want a nice admin interface for it all, look at ISPConfig. Overkill - maybe - but it works well. If nothing else, the server setup pre-requisites will get you 3/4 of the way to a roll your own solution. 

I'm comparing a fresh install of Ubuntu 16.04.1 LTS with its equivalent Docker image/container and I'm noticing many typical binaries like , , , and so on are missing from the Docker version. Just counting binaries in some key folders from the Docker container: 

This is on Ubuntu Server 12.04.1 LTS. I don't even know which email server I'm using or how to find out. My understanding of the directory must be off, because when I do (for both root and my regular user) it says there is no crontab. Maybe it's an issue with and not email or cron? EDIT: I've killed off the and processes and removed the lock, now I'll just have to wait to see if I get the error again. 

You can do this on the webserver level using (name based) virtual hosts. Set DNS for both names to the same IP. Configure apache with 2 (name based) virtual hosts. domain1 has a document root of and contains some directories, including one named /download - so the file system path to that folder is Set up domain2 so that its document root is What domain1 sees as $URL$ domain2 will serve up at $URL$ Another lazy way is to use symbolic links, if you tell the webserver to follow them. You can also alias various directories, so if you install phpMyAdmin it can be available on $URL$ but the files all live in /usr/lib/phpMyAdmin and that directory is simply aliased in. 

I have a VMDK (with several partitions) file which I need to modify. It is not attached to any particular virtual machine. I've tried using (from the VDDK toolset) in on Ubuntu server 12.04 which allows me to mount a particular partition from the VMDK to my local machine. I can successfully view the files and, after changing some permissions, I can write changes to the files. The problem is that they don't persist after I unmount the vmdk (then remount it to check for the changes). Do I need to do anything before unmounting? I'm currently using to unmount after making my changes. I haven't tried to convert the VMDK to a different format, make changes, then convert back. I'm skeptical about going that route. 

Edit to include an example of a host definition wtih SSL directives pointing to a letsencrypt certificate with a redirect for the same site on http to bounce them to https - 

If it reports loading OK (with maybe a "out of zone info ignored" for your glue record servers) then edit again, increase your serial, and restart bind. 

Go to your Linode control panel, open the 'node in question, go to the "Remote Access" tab, and at the top next to the first part of the "Public IPs" area with the IP/gateway info where the "ssh root@your.ip" text is there is a link that says "Reverse DNS". Click that, change the value to match your , save it. Wait for up to 24 hours for the caches to clear and things to repropogate. Happy user of Linode since the days of 24mb/ram and 6gb or so of disk being $20 per month :) 

Right now we're hosting our website on an Azure web app. We've created a new repo for a complete overhaul of the site (as opposed to a new branch or something within the existing repo). The switchover plan was to have two webapps - the exist one and a new one which hosts the new site - and use Azure Traffic Manager to do switch traffic from the old to new site. We tried this and got a "multiple domains point to region" error. We could easily work around this (new app in different datacenter, then clone of new app in original datacenter) but it seems like there should be a better way. What is the best practice for achieving our goal of switching traffic from one app to another within the same datacenter?