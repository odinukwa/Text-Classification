We have had problems a couple times now where the links to the CDN in our components don't get properly updated between the team, qa, and staging environments. Is there a way to force an error if the QA environment tries to access the team CDN or vice-versa? Or some other well-known solution to this problem? 

Hack your Team Bringing about change in your organisation is hard. People have habits, they resist change, and they are often comfortable with the status quo. To bring about change, in no particular order, here are a few tools you can use. 

In this case, "Fire and Forget" doesn't mean what you think it means. It isn't the case that you fire the build and then forget about the outcome. What actually happens is that you fire the event, and then forget about what the process is doing up until the point where the process returns feedback to you and reminds you about what was fired. As an example, the old way of doing things might be to trigger a build and then let it run while you watch the output. You sit there watching the results of the build as they occur and don't work on anything else productive during that time. Or you do work on something productive, but you have one eye on the build process. When it is done, you need to either be paying attention, or remember to check on it to see the results and then continue based on that. In Jenkins model of "Fire and Forget", you have some automated process do the build for you, and your mind is not focused on the build process until something goes wrong, or the build completes. At that point, you get a message from Jenkins, either as an email or in a program like slack, which now reminds you of the build process and tells you all the information you need to know to move on. In the meantime, you were working on some other task with your full focus, because you knew that you didn't have to keep an eye on it. The automated system would alert you to anything you needed to know. 

I've been evaluating Netflix Ice - as a billing tool - but it appears to only work at the Machine instance level - not at the container level. I'm looking for a tool to help generate billing reports for using docker containers across a cluster of docker servers on EC2. (Non ECS) My question is: How to track (non ECS) container costs on EC2? EDIT: I have different groups each running their container on the same host. So I want a way to split instance costs by container usage. 

My coworker is trying to attach IAM roles to EC2 instances and doesn’t have permissions. I’m trying to work out which permissions to give him. My question is: What is the AWS user permission that allows attaching and detaching IAM Roles to instances? 

In an environment of Secured Virtual Private Clouds, in a Cluster of Docker Containers, we have to setup some routes. We can do that with the Ambassador pattern, which is simple and easy to maintain. (Docker does have some usage of IPTables under the hood - but from what I can see the Ambassador pattern uses socat, and not IPTables to achieve its forwarding. We can setup NAT rules with IPTables to achieve a similar goal. My question is: When would I choose IPTables over an Ambassador pattern for port forwarding? 

Cloud services hosted by Amazon Web Services, Azure, Google and most others publish the Service Level Agreement, or SLA, for the individual services they provide. Architects, Platform Engineers and Developers are then responsible for putting these together to create an architecture that provides the hosting for an application. Taken in isolation, these services usually provide something in the range of three to four nine's of availability: 

Publish Custom Metrics. Create an AutoScaling Launch Configuration. Create an AutoScaling Group and Policies based upon your Custom Metric. 

If all of that wasn't enough for you most of the cloud providers will compensate you in credits if you blog about their services in detail, you do have to put in a substantial effort up front and "appear" on their radar by writing blog posts, presenting at meetup and conferences but once they see you they will gladly put $100 a month towards your further learning. 

Where is the index, i.e. , or in the above example. If you were to construct this from scratch it would be in the following format. Get the list of statuses from the commit 

We require a review process using Pull requests in github onto our main dev or master branches. During that review process, we will mark pull requests as requiring changes if many files have white space or line ending differences, and insist that they follow the formatting of the dev or master branch they are making the pull request for. There are also some nice tools depending on which languages you use and which CI tools you use, which can, during the build process or pull request step auto format the code based on rules that you set up. That also helps with keeping code looking consistent and minimizing formatting issues when committing code. 

If you have docker compose installed on the Jenkins Build machine you can use to run docker-compose from a new shell. In general, you can run any command from your JenkinsFile using the command. 

It is actually very simple to convert a multi-branched hydra repository into a single branched model. First, you want to start with the branches which have the least difference between itself and master or trunk. Examine their age and relevance. If they are still relevant, start merging them together and resolving conflicts. If they are no longer relevant, then delete them. Continue this process until you have managed to merge all your branches, resolved all conflicts, and you have only a single branch remaining. You can follow this simple outline to get started: 

Now to me, a server application written in C++ (not memory managed) vs Java (memory managed) has no impact on the utility of a unikernel. In both you get the isolated protected way to manage your application lifecycle. Perhaps I'm missing something. My question is: What does it mean that "outside of memory-managed code execution runtimes (JVM, Go, etc.) the usefulness of unikernels starts to rapidly decline"? 

I'm having a discussion with a friend about use cases for Docker. One guy in the team wants to use Docker for everything - like a kind of universal unix process wrapper. The other thinks that Docker should only be used for stateless applications like Microservices and AWS Lambda style apps. We've engineered proof of concepts for both. On our docker cluster we have a shared drive that gets mounted when the Docker host is mounted, and if a Database in a container is mounted, it simply mounts a volume to the shared drive. My friend still sticks to his position, despite being shown the contrary evidence. (He also argues that Docker adds unnecessary risk by adding complexity to the stack.) I'm trying to listen and understand his point of view, both in an act of empathy, but also to better reason with him. (We all get on quite well - so this is a mix of in-jest and serious discussion). Kind of the question behind the question is: are databases cattle? This comment suggests that a good automated backup and retrieval strategy for your database is indistinguishable from a cattle server. My question is: What are the reasons Docker should not be used for databases? EDIT: People have asked me to clarify my terminology. I was assuming that the database application was in the container, and the storage was in the volume. What I meant was, the RDBMS is in the container, and the database storage is in the volume. Some commentators have suggested that the docker volume drivers aren't going to work with database writes very well. (Or something to that effect). Could you please expand on that? 

As far as I was able to tell it's not possible. We solved this by using Pipeline putting the custom messages in the Jenkinsfile. The flexibility gained by having the Jenkinsfile in the git repo really helped our developers have a better understanding of the build process, and allowed them to make necessary changes to the build commands without the communications headache. It's a nice excuse to move over to Pipeline. It also happens to be very easy to do: 

If any tests fail, go back, find out why, and fix them, and repeat the process. If tests still fail, pick two different branches to work with. 

This article explains the trouble with monitoring, but it doesn't provide any good examples of how to actually monitor a microservice inside of the docker container. We are currently using PM2 monit to monitor our microservices, but as we put them into docker containers we lose the ability to access this data within one screen for all the various microservices which each run in their own docker container. Dockerswarm monitoring will tell us the state of the containers, but not the microservice running inside of them. What's a solid proven way of solving this problem? 

Prevent a deployment from Master overwriting the Hotfix, which could result in an regression. Prevent a Hotfix sitting in the Hotfix branch languishing without being merged in. 

I can think of two architectures that would support the answering of these questions, however, the enormity of the problem could well be clouding my judgement: Approach #1: Walled Garden Effectively firewall off the sources of these open source packages, i.e. npm, Docker Registry, nuget, etc., then create an internal repository of approved packages, implementing some process to whitelist packages. 

So having gone backwards and forward over this for a couple of weeks, Azure has confirmed to me in-person that the only way to utilise FIPS-140 Level 2 certified hardware security modules in Microsoft Azure is to use Azure Key Vault. 

I have to admit to never having asked, or been asked, the question if it is possible to have a Hardware Security Module in a public cloud, by which I mean Google, Amazon or Azure. Has anyone found any techniques for enabling organizations to use HSMs that they fully manage? It seems to me that the two concepts, Cloud and HSMs, are fundamentally at odds with each other - because cloud generally involves "outsourcing" or transferring the risk of operating hardware to the cloud service provider. There is clearly a middle ground in terms of fully managed HSMs as you find in Azure and AWS: 

Repeat steps 2 - 7 until you have only two branches, your master/trunk and . Finally, merge into master/trunk and live with your new single-branch model. 

Repeat step 2 to find the next branch with the least divergence. Merge the two branches found in step 2 and step 3, resolving all conflicts. Merge your these two branches into your branch. Test the code in temp_master code to see if it compiles, and builds, and run any other automated tests you have for sanity. 

Our team has two separate repositories for a frontend/backend system. They would like to have these two repositories deployed to the shared team environment together. My understanding of the JenkinsFile is that it will only work on the repository it committed to and only run when the SCM system sends a request to Jenkins informing it of new changes. Is it possible to have the JenkinsFile in the two separate repositories communicate with each other so that when one is built the other is built as well and only after both are built, they will then be deployed? My main goal here is to avoid creating a separate Jenkins job within the Jenkins UI. 

The benefit of unikernels is managing a large number of applications in a protected space. (Some might say all the benefits of Docker without all the overhead). (Please don't get me wrong - I'm a huge fan of docker and use it 20 times a day in my work - I'm using this question as a a way to explore an idea). The following commentator writes: 

I've got an ec2 autoscaling group that starts in the morning and finishes in the evening. I'd like to create a 'button' that people can click to warm up the autoscaling group to run jobs in the middle of the night (only on an on-demand basis). My question is: What is the ec2 cli command to modify the minimum number of nodes in a scaling group? 

There is a great discussion of the Cattle vs Pets distinction from Randy Bias here. Martin Fowler talks about a SnowFlakeServer. In a series of talks, Adrian Cockcroft talks about how they moved toward Cattle Servers for solving a scalability problem at Netflix. The challenge with this distinction is always managing persistent state. Does it make sense to treat your database servers as Cattle? It does if you (a) manage the state outside of your cattle model (external volumes for your docker containers), or (b) use a distributed database like Cassandra that allows for individual nodes to fail, but still maintain state in the cluster. I get that you can get very close to the 'disposability with persistent state' of Docker containers mounting a shared volume, with launching AMIs to machine instances mounting a shared drive. You can get closer this this idea of scheduled cluster management by having an autoscaling group that recreates machines that you've blown away. To me - the machine instances lack the granularity of a docker container. They gravitate more towards the 'pets' end of the spectrum. My question is: Does the "cattle not pets" distinction apply as equally to machine instances as to containers? 

Potentially could create the above from scratch, or transition from a partially deployed state to the above desired state. I am aware that Terraform splits its work into the execution plan stage and the application phase which actually makes changes to the target architecture. Can this be used to write tests against the execution plan, if so are there frameworks to help write these? 

To be absolutely clear, these are functions that used to belong to the operations organization and are now owned by the Agile/DevOps organization. There are existing KPIs that drive bad behaviors are: 

If you were to apply IT Service Management (ITSM) or ITIL language to the same situation you would likely call it an IT Service Continuity Plan or Recovery Plan: 

I would set up a base taxonomy to start with, you can use automation across your estate to ensure that all resources follow the taxonomy. You have a choice between simply deleting resources that don't comply or reporting on it to a central team who are responsible for chasing down and educating the owners. I have followed the Best Practice set out by Microsoft for both Azure and Amazon: