My first question is whether an interactive proof system characterisation is known for all the classic complexity classes. I would call P, NP, PSPACE, EXP, NEXP,EXPSPACE, recursive and recursively enumerable functions classic (among others). Specifically,is an interactive proof system characterisation known for recursive and recursively enumerable functions? I only know that IP = PSPACE and that MIP = NEXPTIME. By `know' means I understand the definitions of objects on both sides of the equality and possibly understand the equality. My second question is whether there is a graphical summary of different types of interactive proof systems and the complexity classes they characterise. Specifically, I would like a reference to a figure similar to Immerman's picture of description complexity characterisations. 

This fragment is also called difference logic and was, for a brief time, unfortunately called separation logic (because $x$ and $y$ are separated by a constant). The following paper provides a practical view of solving the quantifier-free fragment of the problem. 

Complexity In terms of complexity, it is known that temporal logic with past-time modalities is exponentially more succinct than LTL with only future-time modalities. Nonetheless, a result of Sistla and Clarke (1985) shows that the model checking problem for both is in PSpace. The actual complexity issues are a bit more complicated because one can ask for lower-bounds for expressiveness in both logics involved in a comparison. I recommend consulting the references below for more on this. 

I believe it would be beneficial for you to look up the theory of abstract interpretation, which provides very thorough answers to similar questions in the slightly different area of lattice-based program analysis. It appears to me that you are using a framework based on algebras. I'm using the word algebra here in the sense of universal algebra, where I assume that constraints on the structure of the algebra are given by equalities between terms. There are two different senses in which abstractions (or hierarchies) enter the picture. 

[Comment space was too short] I think it depends on what you mean by behaviour. Probabilistic automata follow in the tradition of finite automata, so their behaviour is defined in terms of their language or traces. Labelled Markov Processes follow in the tradition of process algebra, where it is known that processes can be compared using a variety of preorders typically starting with bisimulation. You do have notions of bisimulation defined for LMPs and in that paper. Bisimulation is stronger than trace equivalence, as you may know, so the behaviour you are interested in is the tree unfolding rooted at a given state. Definition 3.1 of the paper you have linked gives approximations of this tree. 

I cannot tell from your question how deep your understanding of syntax and semantics is, but I see some pitfalls there. A programming language has an indispensable syntactic component. It also has a semantic component. And there is the connection between syntax and semantics. There are many semantic structures that have been proposed to give meaning to programming languages. These vary from operational semantics to domain theoretic structures. If you want to start with semantics, you can identify a space of domains and then try to define a language that can express properties of these domains. Samson Abramsky, in his PhD thesis, showed that this process can be made very systematic. 

It is over two years since this question was asked, but in that time, there have been more papers published about algorithms for computing Craig interpolants. This is a very active research area and it is not feasible to give a comprehensive list here. I have chosen articles rather arbitrarily below. I would suggest following articles that reference them and reading their related work sections to get a clear picture of the landscape. 

You could have invented spectral sequences, Timothy Chow, Notices of the AMS Forcing for dummies, Timothy Chow 

A casual introduction to Abstract Interpretation, Patrick Cousot (Joint work with Radhia Cousot), Workshop on Systems Biology and Formals Methods (SBFM'12) A gentle introduction to formal verification of computer systems by abstract interpretation, Patrick and Radhia Cousot, Marktoberdorf Summer School 2010. Lecture 13: Abstraction Part I, Patrick Cousot, Abstract interpretation, MIT Course. Introduction to Abstract Interpretation, Samson Abramsky and Chris Hankin, Abstract Interpretation of Declarative Languages, 1987. Abstract interpretation and application to logic programs, Patrick and Radhia Cousot, 1992. The first two sections have a general, high-level overview with several examples. 

This is slightly tangential, since Neil's lovely answer already addresses your question. The difference between programming a Turing machine and what is usually considered a programming language is a topic that can be discussed at various levels of sophistication. Usually, we find it convenient to structure programs using variables, functions, function composition, etc. This is largely how we structure our mathematics. We tend to focus on the result of the computation rather than the mechanical process of how those results are derived. There are situations when thinking in terms of state machines is convenient. For example when designing simple protocols, or describing access and locking policies for resources such as files, printers or memory. In such situations, it is easier to take a standard programming language with variables and functions and add support or explicitly encode a little state-machine behaviour. The word "behavioural" is often used in such contexts. You will find explicit state-based programming more often in Verilog/VHDL than in high-level software. There are also certain object oriented design patterns that enable state-based programming. 

Rather than ask how we can strengthen/weaken the notion of isomorphism, another possibility is to ask: What is the right notion of equivalence between computational structures, and what is the mathematical structure underlying this notion. One large family of structures is coalgebras. Structures such as lists, trees, automata, both of the finite and infinite variety can be described as coalgebras. We can then study homomorphism or isomorphism between coalgebras. However, even homomorphisms between coalgebras don't tell the whole story. You may find it helpful to look up simulations, bisimulations and other logical relations. If you strictly prefer an algebraic approach (as opposed to a relational one) Galois connections are one option. Here are some starting points. 

The reduction hinted at above treats languages as abstract mathematical objects. To apply these ideas in practice, we need a data structure to represent languages and algorithms to manipulate these data structures. Enter automata. Automata allow us to reduce questions about abstract mathematical objects like languages to concrete, algorithmic questions about labelled graphs. Languages and automata theory, besides an insane number of practical applications, provide a very significant intellectual service. We can think about problems ranging from formatting zip codes to decision procedures for monadic second order logic in uniform and uncluttered conceptual space. How amazing is that! I have said nothing about logic and decision procedures. (Yes, they have practical applications.) See Kaveh's answer for an authoritative overview. 

I appreciate the desire to know about historical origins, but it is very important to appreciate that this is a nuanced topic and you cannot always find simple answers. Often, we do not even know when or why a concept was developed. Regarding your last paragraph, the historical origins are, in my opinion, not a good source of motivation for or understanding about the essence of computation. In fact, they are a pretty bad source for undergrads. There are many contemporary essays, books, talks and the like about why computation is important, universal, beautiful, fascinating, and deep. That's where you should be looking. Here are a few random starting points in no particular order. 

The coarsest bisimulation has a well known greatest fixed point characterisation. This greatest fixed point characterisation gives you a simple algorithm to calculate bisimulation quotients. There are quite a few immediate improvements you can make to this algorithm. A reference that contains most of these details is Joost Pieter Katoen's lecture on bisimulation minimization. If you need more detail, I suggest looking at the book 

Modal Correspondence Theory, van Benthem Modal Logic, Blackburn, de Rijke, Venema Model Theory of Modal Logic, Goranko and Otto 

Note that we have not only changed the problem but also strictly generalized it because a solution to the original problem is still a solution to the modified problem. The big unanswered question now is: How can we find an approximate solution? Abstract Interpretation Idea 2: Fixed Point Characterization of the Original Solutions The second big idea is to observe that the set of solutions to many problems has a characterization as a fixed point in a lattice of candidate solutions. As an example, suppose you have a graph and you want to know if a vertex $t$ is reachable from a vertex $s$. We can break this down into finding the set $\mathit{Reach}(s)$ of all vertices reachable from $s$ and then checking if $t$ is in this set. We can further observe that $\mathit{Reach}(s)$ is the least solution to the equation: 

These are a small and non-representative sample of the questions that one may ask. As in any field, answers to some questions generate new questions and drive enquiry about other questions. You can find a rather dated view of the field by browsing the articles in the Handbook of Theoretical Computer Science. 

One of my favourite family of results is that various problems of a seemingly infinite nature are decidable. 

The problem as you state it is extremely general and has been around in some form since the early days of computer science. For example, language equivalence of various kinds of machines is a kind of equivalence of computational devices. That said, your question indicates you are interested in the equivalence of imperative programs. There are several ways and approaches to do this. One specific application of such work is to prove the correctness of compiler optimizations. Translation validation is another technical term you may want to look up. The equivalence of two programs is a semantic property. It is difficult to check directly. A core idea behind many techniques is to find a structural property of programs (or the transition graphs they generate) that is easier to check and implies equivalence. For example, one may use bisimulation, simulation or related notions in a proof. The challenge is to lift these ideas from transition systems to the program text. Here are a few recent proposals in this area. 

There are lots of courses where people teach temporal logic and those notes and exercises have lots of examples. I would also suggest Manna and Pnueli's book for examples. 

The abstraction function makes explicit the idea that if the structure over $N$ is an abstraction of the structure over $M$, then evaluating a term in $N$ cannot produce more precise results (with respect to the notion of approximation in $N$) than evaluating the same term in $M$ and then mapping it to $N$. Now we can ask if it is necessary to approach the problem in terms of abstraction as opposed to refinement. Meaning, can we not say that $M$ is a refinement of $N$ and formulate conditions in terms terms. This is exactly what a concretisation function does. 

I briefly reviewed some areas here, trying to focus on ideas that would appeal to someone with a background in advanced mathematical logic. Finite Model Theory The simplest restriction of classical model theory from the viewpoint of computer science is to study structures over a finite universe. These structures occur in the form of relational databases, graphs, and other combinatorial objects arising everywhere in computer science. The first observation is that several fundamental theorems of first-order model theory fail when restricted to finite models. These include the compactness theorem, Godel's completeness theorem, and ultraproduct constructions. Trakhtenbrot showed that unlike classical first order logic, satisfiability over finite models is undecidable. The fundamental tools in this area are Hanf locality, Gaifman locality, and numerous variations on Ehrenfeucht-Fraisse games. The topics studied include infinitary logics, logics with counting, fixed point logics, etc. always with a focus on finite models. There is work focusing on expressivity in finite-variable fragments of first-order logic and these logics have characterisations via pebble-games. Another direction of enquiry is to identify properties of classical logics that survive the restriction to finite models. A recent result in that direction from Rossman shows that certain homomorphism preservation theorems still hold over finite models. 

There are different questions being asked here. It is important to keep in mind that most courses on this topic are designed to clearly communicate and deepen a student's understanding of the concept of computation. Historical motivation and precedence is not usually required to communicate the current understanding of the concepts or even their current relevance. Moreover, historical and philosophical issues are often subtle and doing them academic justice requires time and training that is not usually part of a computer science curriculum. 

Decidability of Second-Order Theories and Automata on Infinite Trees, Rabin, 1969 Automata on infinite objects, Thomas, 1988 Automata: From Logics to Algorithms, Vardi, 2007 

Automata on Infinite Words Where there are languages, computer scientists will have automata. Enter the theory of automata over infinite words and infinite trees. It is extremely sad that although automata over infinite words appeared within two years of automata on finite words, this fundamental topic is rarely covered in standard computer science curricula. Automata over infinite words and trees provide a very robust approach to prove decidability of satisfiability for a very rich family of logics. A fundamental result is that the different acceptance criteria for infinite word automata are all equivalent. The basic problems of union, intersection, and complement for $\omega$-automata are more involved than their finite word counterparts and the details differ with the acceptance criterion used. Safra gave a famously complex determinization algorithm for Buchi automata and significant work has been devoted to deriving a new and simpler construction. Rabin famously proved that the monadic second order theory of the binary tree is decidable. His proof uses automata and the cornerstone result is the determinization of Rabin automata. I have heard tell that Rabin's theorem is the mother of all decidability results in program verification. 

De Nicola and Vaandrager have a few papers on translating between Kripke structures and LTSes and how this affects logical properties. They introduce doubly labelled transition systems to move between the two types of representations and semantic interpretations. Action versus state based logics for transition systems , De Nicola and Vaandrager, 1990 Three logics for branching bisimulation, De Nicola and Vaandrager, 1995 If the goal is verification of event-based systems, one option is to translate the LTS into a Kripke structure and use a Kripke structure model checker. The translation has to preserve the interpretation of CTL over the LTS. An action-based framework for veryfying logical and behavioural properties of concurrent systems, De Nicola, Fantesi, Gnesi, Ristori Translating between representations involves a blowup. For the case of LTL, there are cases when this blowup can be avoided. Here is a recent paper on the topic. State/event-based software model checking, Chaki, Clarke, Ouaknine, Sharygina, Sinha