I applied the instructions on Ask VG to the classic theme to permanently hide the command bar. This is the resulting . I hope this saves some of you some time. 

There's O&O ShutUp10. It ranks really highly on duckduckgo and I've seen it mentioned in a few other places like thewindowsclub.com. Right now, though, there doesn't seem to be a clear frontrunner in this space. So, maybe just try a bunch of the suggestions in the second link? 

I like navigating forms in web browsers with the keyboard, but chrome is making this very difficult. Normally when you hit tab to select the next form item, a dotted rectangle or something like that highlights it, so you know which item you're editing. Chrome isn't doing that consistently in all web sites. Is there a way to force checkbox highlighting? 

How much space do Windows 7-10 actually occupy on the UEFI System Partition (ESP)? my reasons: I'm installing Linux on a new computer with a blank hard drive. I would like the option of installing Windows on it later if I need to, and I would like to know now while I am formatting the partition table how big I need to make the ESP in case I do decide to drink that kool-aid. I know there are recommendations to allocate ~200MiB of space, but I can safely assume that there is a massive safety margin I know I won't need. I do only have a small SSD so it's not like I'm made of gigabytes here. As a pre-emptive stfu to all the people who say 200 megs isn't a lot, think of it as a 50% savings and you'll see why I care. The entire system is comprised of little pieces of 200MiB or less, and a 50% reduction in each of these components across the board is huge. My own purposes notwithstanding, this would be a good thing to know in general and I'm sure there are others who require this information for all kinds of reasons. 

I'd recommend Opus as the codec for low bandwidth at good quality. You should be able to find a quality sweet spot that will be on average at maybe even half the 64kbps that you quote. You can use avconv/ffmpeg for the live transcoding. As streaming server I personally prefer Icecast. All three have support for sending the encoded stream to it. It's as easy as and answering the setup questions. (If it doesn't start by itself double check and the config in 

While I agree with the answer of piping output through STDOUT and through SSH, I'd like to offer a variation of perspective: You can use SSHfs (a FUSE filesystem) on the remote machine to mount a directory from your local machine and then write to it. If necessary this can be tunneled back through a connection from your local machine to the remote machine. Example A: 

You should verify, that the command you are using really sends that header. Using cURL it works just fine (see last line of output): 

The Chrome web browser offers a "Copy video address" context menu item on that page, if you right click on the video. Please note, that the link you will get in return seems to contain an authentication token. This means, that most likely it will only work for you and only for a limited time. Accessing the URL returns a file, the signalled file name happens to end in . So while the URL doesn't point to a file ending in , the file that eventually is downloaded from that URL does. Please note that different sites will have different structures and will use different technologies. There is no universal answer to this question. 

There are those HDMI extenders that are network driven. Lenkeng is one of them. They've also been reverse engineered, for those fearless that want to replace either the source or the sink with a computer: $URL$ I haven't seen them with WiFi, so you might need to add a WiFi bridge with sufficient bandwidth. As those use MJPEG under the hood, there might be a detectable quality loss. HDMI uses much higher bandwidth than simple Ethernet or WiFi can supply, so all those network based technologies use compression. edit: Turns out they have a dedicated PC→network→HDMI device: $URL$ Price seems to be in the $80-90 region. 

edit: You can print $$ to a file in the else, prior to the exec if you want to capture the pid. You can also just grep through PS output to kill based on pid. 

It really has nothing to do with the number of "users" or wireless clients. If I'm downloading a 1gig file at 1mbs that would leave 53mbs (on a 54mb link) for other applications to do stuff. However, if I'm able to get that 1gig file with a rate of 45mbs that would leave 9mbs for other applications. The speed of the flow depends on bandwidth, routers, network conditions between the two devices involved in the flow. Note: due to header overhead etc, you will never get 54mbs of transfer rate out of a 54mbs network. And it's really not that straight forward, but it's a more accurate way of thinking about the problem. 

Try running firefox from the commandline, what happens? If that fails, try running the following from the command line: 

QoS is the right choice. I don't know about the D-Link software, but I know that dd-wrt handles this just fine. I don't know if it's available in the D-Link firmware, but you may be able to set your QoS stuff using a mac address rather than an IP. that way you would still be able to use automatic address assignment. The only reason I could think that would give you connection problems would be using a duplicate IP address. Try setting the static IP of your xbox to be on the upper end of your range. i.e. if your home network is numbered out of 192.68.1.0/24 space, try setting the xbox to be 192.168.1.200. This should take it out of the range of addresses normally used for DHCP. Someone will un-doubtedly complain that this is a gaming question. I think it's a networking question. 

from the command line will (at least for me) open a new tab to superuser.com in firefox (which is my default browser) 

Streaming Proxying a stream is well possible and shouldn't require expensive hardware. In case of a continuous HTTP stream (like e.g. WebM over Icecast) a simple Icecast instance set to relay the original stream will take care of things. One stream to the Internet, local availability with capacity only limited by network. In case of progressive HTTP (DASH, HLS, etc) you could resort to a simple HTTP proxy (unless the origin server is trying hard to prevent proxying). In all other cases it will be more involved, but if all else fails you can always run a transcoder and have that send a stream to a local server (e.g. avconv/ffmpeg → Icecast). Network This is going to be by far the larger challenge and will require some engineering and considerable testing. An ordinary off the shelf AP can handle somewhere between 10 and 30 client devices on the wireless side. Enterprise hardware may come closer to 100. That doesn't cover bandwidth though, with increasing number of devices you have a non-linear decrease of available bandwidth (A theoretical 100MBit/s AP with 100 clients would have considerably less than 1MBit/s per client of effective bandwidth), this is further compounded by legacy devices that force the AP to switch between fast and slower modes. You'll need to deploy a fleet of enterprise Access points with intelligent network control if you plan to serve 1000 concurrent client devices at considerable bandwidth. Stream format You may want to consider the format of streaming and its parameters you'll choose, as this can reduce your bandwidth needs by orders of magnitude. Even if you don't control the original stream, you should consider serving a modified local stream to accommodate the networking challenges you face. An example: If you choose to stream WebM with carefully tuned parameters and the Video signal being encoded is 'simple' (e.g. a conference speaker and slides, as opposed to a sports game with lots of movement in the picture). Then you'll likely have a resulting stream bandwidth well below 1 MBit/s for most of the time. Added benefit: plays straight in most web browsers (excluding iDevices, but there are apps). Conclusion Good luck, this is going to be neither cheap, nor easy, but certainly is feasible. 

Download and install Easy Display Manager before running the BIOS Updater. I'm not exactly sure why, but the Easy Display Manager is a prerequisite to updating the BIOS. When it's installed, run the BIOS updater under windows 7 compatibility mode. It should automatically download and install the BIOS problem-free from here. If something goes wrong after the new BIOS is successfully downloaded, try finding the downloaded BIOS (in the same folder as the updater. It has a name like ITEM_20130321_500_WIN_06UU.exe.) and running it directly, also in windows 7 compatibility mode. After the system is restarted, as a sanity check, run the BIOS updater again to check and make sure the latest BIOS is indeed installed on your system. After the BIOS is updated, Easy Display Manager may be uninstalled. 

I just found a friend running windows 7. Its /boot/efi/Microsoft partition is 17.2MiB. Refind occupies another 7.1 MiB, making the total well under 32 MiB. It is unlikely Windows 8 or 10 uses much more than that, but it would be nice to know anyway. A 64MiB partition should be plenty. 

in .screenrc. I'd like this kind of behaviour. I am not using GNU screen because it handles terminal resizing extremely poorly. 

I want a way for all three to work within tmux. I know 3 won't work in split screens. I'm OK with that. I won't be using split screens. I know there's . That only does number 2 in the above list. I really really want all three in that list. GNU screen actually has this ability. You use the line 

paprefs configures pulseaudio in some way, ultimately probably by modifying some configuration files, but which ones? I want to know so that I can do the same kind of configuration but manually using my text editor. 

GUI terminal emulators like xfce4-terminal have several ways of scrolling through the terminal's buffer. Primarily: 

I've recently had a linux live USB completely fail to boot on a mac running El Capitan, which makes me suspect secure boot is rearing its ugly head in the Apple world, as well as the PC world. So my question is thus: does, or has Apple ever used EFI secure boot? 

The UID of the files on the mount need to be the same as the user's UID on the local box If the user has local root you'll want to make sure root-squash is active on the NFS server. This ensures that access to files as the local root user is blocked. That said, if the user does have local root, it's pretty trivial for that user to create a local account with the same UID as the files he wants to get access to, and su to the new local user account, thus giving access to any file with the same UID as the newly created local user. 

I'd give handbrake a shot. It has worked great for most of what I've thrown at it. You may have to tweak settings, but the defaults should be a good starting point. The big thing to check is how it handles the audio stream. I've noticed that some of the defaults want to convert everything to stereo. It's easy enough to change, just check. 

i2p is another anonymous proxy system. It works differently than Tor but it still quite slow. It essentially creates a virtual encrypted/anonymous network on top of regular IP. There are web servers that are only accessible via the i2p network. The i2p network provides things like out proxies and dns proxies that allow you to get to normal (i.e. non i2p) websites. You need to run an i2p "router" (java software) on your local device to connect to the i2p network. 

Clarification: I've installed the Lion-Server package on my desktop. This isn't headless server hardware. 

Try removing your profile (you will lose your bookmarks, passwords, browsing history, etc). Your profile is usually located in ~/.mozilla or ~/.firefox, it's a sub-directory in there, remove the subdir only. This is a fixable problem (or at least should be). The issue is figuring out why firefox isn't starting. That said, it's possible that firefox not starting is the least of your problems. Who knows what else is busted. . . getting some idea as to why firefox is failing may help you figure out if anything else is broken. If all else fails: 

I only want to sync bookmarks. Unless I'm mistaken, though, the moment I sign into chrome for the first time, it immediately syncs everything, including history and passwords. This is a profoundly stupid thing to do. I should be able to choose what to sync BEFORE syncing. Is this possible? If it isn't, then I'm just going to have to avoid chrome sync altogether. 

My own solution ended up being a switch to the mplayer fork called mpv, which handles the realtime terminal time display much more sensibly by default. It seems serious mplayer development has completely stagnated, thus mpv is now better in every way imaginable. It has become my primary media player. 

If I understand the situation, modern hard drives are responsible for bad sector detection and avoidance. When a sector is written, it is immediately read back to ensure that sector is good, and if it isn't, the physical sector is marked bad and the data is written somewhere else, changing the mapping between that virtual sector and its physical sector. That all makes sense except for one thing: where are all of these extra sectors coming from? Are there just more physical sectors than virtual sectors? That's the only thing that makes sense to me. Otherwise, a command like would fail eventually once it inevitably hits the bad sectors, which are sure to exist, even on the best and newest of drives. Also, after running the above command, as far as the hard drive knows, every virtual sector on it is being used. Yet there isn't a sudden increase in write errors on the software end, like you would expect there to be if the hard drive had run out of room to remap bad sectors. Therefore the hard drive must have somewhere else to map any new bad sectors it finds, even though as far as the hard drive knows, the entire virtual address space is being used. Assuming there are more physical sectors than virtual ones, how can you figure out how many extra sectors there are, and how many are left? That way, when the extra sector supply is running low, I could get some kind of warning so I can replace my hard drive before it dies. EDIT: It turns out I didn't exactly understand the situation right. Reading after every write operation isn't necessary when error correction codes are used. That makes me wonder, though: does a sector get marked bad and remapped every time an ECC has to be used to recover from a write error to a sector?