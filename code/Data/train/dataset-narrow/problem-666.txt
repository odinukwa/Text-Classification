PID 9593 is the most problematic one which other users get blocked by this one. As far as the user is admitting to, he truncated his table, then did inserts in batches of 1,000 committing after each batches. Currently this PID shows the following locks: 

Your best place would be pgfoundry. But you won't find much. Doesn't look like many plugins are available. 

You could write a procedure with the EXECUTE and build the statement in the procedure by looping over the list of schema which match your query from the information_schema tables. Then you can call your procedure from psql or other passing it your criteria such as 'ceu_shard_test_merge_%'. You could have a parameter to do it or just dry run and instead of execute then it could output the statements or something along those lines. 

Yes, it is possible and it is covered in documentation and in lot of FAQs like the one at iBase.ru For the example at my development box I have co-installed FB 2.1.5 Win32 SuperServer (at default port 3050/tcp) and FB 2.5.2 Win64 Super-Classic (at a custom 3064/tcp) There might be troubles with FB 1.x as it was using registry, but FB 2.x was made isolated and self-dependent. One option is to download ZIPs and unpack them to different folders. Then you have to run text window of Windows Command Prompt "As Administrator", go into "bin" subfolder and there are all those executables like server itself. There also is "inst_svc" tool. Running it with an option like "-?" would show you brief help. Focus on installing main service, not installing Guardian (only needed on Win98, only shipped for legacy uniformity) and giving non-default "instance name". Then go outside "bin" and open "firebird.Conf" with any text editor like notepad. The documentation is within that file how to set non-default TCP port. That's all. Do the same for your another FB folder and you've done it. Another option is to run two installers. The 1st one would do all the described above things automatically. The second one would unpack files - and ask you to do those configuring operations for the second copy manually. Just do it like described above. 

A database server running Postgres 9.4 shows no statistics when running - well only the is non-zero. The is null. I haven't been able to find much as to what should be checked in this case, to find out why statistics aren't reported. and are both set to . Per the manual: 

Code generators are great! Code generators are evil! 10-15 years ago, I would have said that having a code generator for quickly creating the boiler plate code for database driven applications would have been a great gift to mankind. 5-10 years ago, I would have said code generator sucks, they generate too much duplicate code and that having a data-driven user interface (where the fields on the screen, validation, etc is driven by meta-data in a database instead of coding the screens one by one) would have been a great gift to mankind that supplanted the code generators. Today I would say - write each screen individually. Use existing framework that wire fields and model objects and possibly ORM when doing simple CRUD. But do design each screen to the exact purpose of the screen. Application screens that mirror a RDMS table too much is only good for managing lookup tables. Don't annoy the user with geeky interface that are designed against a computer storage model (RDMS)... make a screen that has only what they need. That may mean that it will save to multiple tables, etc. Who cares. The user isn't a database. So my thought? Don't waste your time making a code generator. 

PS. additionally you have to learn the difference between SuperServer (targeted at small installations) and Classic/SuperClassic servers. For running 24/7 the second options would be preferable, since frequently the server instances would be shut down after user disconnects. So while "server" as a concept keeps running 24/7, the actual executable programs of it get closed and restarted, easening at potential problems like memory leaks in server or UDFs. OTOH Classic server is more vulnerable to cache synchronization issues like in case of crash during Garbage Collection or attempts at metadata (scheme) changes while users are working. In FB 3.x they promise to integrate those two approaches to make it a kind of sliding scale options in firebird.conf 

Yes, that would be fine if your design goal is that Persons have only 1 Addresses. With this design, each Persons can have 1 Addresses but two or more Persons can have the same Addresses. It's all a matter of your business needs. If the above is what you are trying to get, then yes, it is correct. However, I think it's most common the other way around where the Addresses would have a foreign key to the Persons because a Persons could have more than one Addresses. As for your constraint to check the postal code - well first off you are missing the space and it's lower case. Whether that will work will depend on which database system you are using. I tested it with PostgreSQL and it does not work. I don't think you can really have such a simple constraint to fully validate a Canadian postal code. For example, there are some letters and some numbers which are never used. I'm a bit rusty on my Canadian postal office codes but I seem to recall that number 5 is never used as it's too similar to S, etc. 

Back-up is very fast (just dumping of changing pages), can use cascading (database -> monthly large snapshots -> daily deltas from last monthly -> hourls delta form last daily). However it does not optimize (by recreating) the database. And it does not detect database errors other than top-level, errors of pages allocation (like "orphane page") OTOH the pages snapshotted are mostly intact, so in case of partially corrupt database they might still have a manually salvageable data. If the corruption would be noticed quickly. In a sense, that amounts to safe and incremental copying of he database file(s), wit hall the cons and pros. 

Time for the backup to run is a reason for doing incremental backup on Progress. My backup is running fast enough that I did not need to use this function and I'm still doing only full backups. It also depends on your requirements. For example, if you have heavy financial transactions and you want to keep a backup every hour or something like that, you would need to do incremental (or real time). But unless you have something forcing you to do incremental, I would do full backup, I find that easier to restore. In terms of performance impact, if you have a fast disk array, I haven't seen much impact of doing a backup even during moderately heavy usage. Obviously it depends also on the size of your system. I'm talking about a 37Gb DB. 

Reads data as a usual SNAPSHOT transaction, thus db errors would effect it. Some db erors would manifest it in read errors (like if DBA changed column type in a way incompatible with data), but other might result in some data being "invisible" and skipped. Backup file is stripped of fast-access metadata and geta a lot smaller, which is good for archives (example: 1.1GB of raw database -> 380 MB FBK -> ~700 MB after restore). In FB 2.x GBak is known to work considerably slower via TCP/IP than via Firebird Service Manager connection. It is told to be amended in forecoming FB 3. Restore is basically recreating database, so it is slow. But it optimizes the database internal layout and saves some space (no more half-empty pages in the middle of the file). Due to Firebird being very liberal in online (during active operations of users) scheme change (the safe approach was enforced in 2.0 but undone in 2.1 after uproar), the traditional backup might be "unrestorable", so the attempt at restoring a FBK file into a spare free disk is a must. Until you proven you can restore that backup, you may consider you don't have it. 

Not an elegant solution but after installing the package using (which fails creating the cluster but installs PostgreSQL), I switched to the user and created the database using . Then back to , I created the cluster using the command. This moved the configurations to and set it all up. 

To answer your question on those 2 options, neither seem right to me. A) will lock you in and B) is a lot of work. The current schema you describe is not too bad (except for having the information name ("first name", "square foot", etc) as string instead of an ID referenced to a lookup table. However, this seems to me like a good candidate for a NoSQL database ( $URL$ ). While I never worked with such database, what you describe is a typical scenario that this solves.