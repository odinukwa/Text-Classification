YES, your plan can work since your structure is an exact copy. No recovery needed nor possible since your DB is currently running in no archive mode. Don't forget the listener.ora, tnsnames.ora, sqlnet.ora and orapw{$ORACLE_SID} files. Also check your dba_directories view for any references to your local server filesystem from within the database. If this really is a migration, remove the source database when all is up and running. If it is intended as a copy, change the DBID to prevent an accidental mixup of databases in backup/recovery operations. If this is a prod database, change to ARCHIVELOG mode and use RMAN for regular backups. 

Should do the trick. If you can use reference partitioning, you don't have to add the redundant data in the table because the relation is already defined in the partition definition. an example: 

In short I would say: NO. If you want to invest lot's of time ... you could generate SQL's and usage frequencies from awr sql history tables but the parameters will be the same for all occurences of an SQL since the bind variables are not recorded for every execution. I am not sure about hammerora but from swingbench I know you can create your own transactions and for that you could use awr sql history as a guide on how many tx/s to generate. It still remains a synthetic load. Smarter would be to capture the databases input from a proxy between the application server and the database. Oracle Replay has a price, for a reason. 

Chances are that your table contains user defined types or is created in a tablespace for which you have no quota in your local database. You can trace the session that runs the script in your local database to find where it crashes. An other option is to get the ddl for the problamatic table and apply that to your local database for better error messages. To get the ddl run: 

In Oracle you could do this using just the transactions table with a fast refreshable Materialized View on it that does the aggregation to form the balance. You define the trigger on the Materialized View. If the Materialized View is defined with 'ON COMMIT', it effectively prevents adding/modifying data in the base tables. The trigger detects the [in]valid data and raises an exception, where it rollback the transaction. A nice example is here $URL$ I don't know sqlserver but maybe it has a similar option? 

Be happy that your users seem to be going to use SQL to do the analysis, instead of pumping to excel. Make sure you have separated tasks and don't give privs to update the source data. Resource manager is your friend to prevent runaway queries. give them resources that they need, make sure other users don't get pushed out. 

One of the first things Oracle does at restore time is pre-create the data files to the defined size. So you do need a way to store the full size of the files. A compressed filesystem could be used for that and with a little luck it works. It just does not solve the problem. What you do want is to re organize the source database. The quickest way is to prepare a new database with correct sized tablespaces and datafiles. Next use datapump to transport the schemas to the new database. If needed, this can even be done online by using smart tools like dbvisit replicate (or goldengate if money is of no importance). Trash the old database when the two are in sync. 

Without a valid database backup: forget it. If you do have a valid backup you could use that to restore the database. If your database is also running in archivelog mode and you have all archives created since that backup, you can also roll forward the transactions in them and doing so recover all data. It all starts with a valid backup, or a standby database. No magic involved, just common sense. You could take a look on jdul a tool that has 'reads datafiles when db is down or corrupted' in it's description. I never used it. Kurt has made more nice tools. 

The best way to find out will be to run some known code samples with and without using the profiler. The differences normally should not be much but indeed can vary. Even dbms_application_info can hurt when put inside a high frequent loop. 

This caused that even the simplest of all projects took months to even get started and in a time where time to market is extremely important, this is killing. For some reason this is missed by many, often even neglected for political reasons. The solution for this is quite simple. Create one serious database, give every project/application their own schema[s] and access user[s] and get running in hours, instead of months. If you are going to do something like this, it could prove beneficial to combine applications that have similar up-time requirements. Oracle gets more and more on-line maintenance options but for sometimes, getting a few hours downtime is a lot easier. Having time windows for this defined beforehand could prevent a lot of problems. You are going to need some downtime. Don't allow applications to connect to the database, make them connect to services dedicated to the application using their own tns-aliasses. Doing so enables you to move the application to an other database, without having to re-configure the application. BTW: the companies that used this way of consolidation saved a lot of cash yearly, more than the licenses required to start rolling. 

your db_block_size is 8k and your db_16k_cache_size = 0. Normally we don't build databases with mulitple block sizes because the optimizer does not take the different block sizes into account. If you still want to do this, start with something like 

create resource consumer groups create a resource manager plan using the created groups somehow arrange that sessions are mapped to a resource consumer group in the resource directives specify if a request should be estimated first or not make sure users have switch privileges to the required resource consumer groups 

zfs will use the available memory to cache. Your database will be better because you can dedicate that to exactly the purpose where you want to spend your memory on. zfs cache can be a good second. 90 seconds for a single insert looks like ages. Is it a video that is inserted? Is something like a physical standby database in place in synchronous mode? Does the table happen to be a remote table? Do you have indexes or triggers on that table that cause a lot of extra work? How many disk are in the raid 10 set? Do you have a transaction log active? Is that log location full? I don't know how to tune innodb so I can not give you any guarantee but if this was oracle, for sure, this is no memory problem, there is something more going on in the database that slows the insert to an amazing 'speed' unless you happen to be inserting something like a video with 'Rambo' or something with a similar size. ZFS is in development; how up to date is your Solaris and zfs installation? 

A simple example is shown here Running ASMCMD commands in a script Change this example slightly to get it to copy all files in a local directory, also using this Using the ASMCMD cp command as input: 

Seems like a good reason to visit the Oracle Documentation site. The 2-day dba documents are very good. If your database is running in ARCHIVELOG mode, it copies all transactions to the archivelog destination. The transactions are always written to the redolog files but when they are full, they are only saved when running in archivelog mode. This enables you to restore your database to any point in time. This restore operation starts with restoring a full backup where you can apply the archives until you reach the point in time where you want to stop the recovery. For example, close before a table was dropped. Normally production backups are made with the database online. Again, a reason to run your databases in archivelog mode. IF not running archivelog mode, in a disaster scenario you might loose all transactions made since the last backup. If your transactions are really important it could be smart to copy the archivedlog files to a second DC on a very regularly basis. How regularly depends on the cost of loosing transactions. If you can handle loosing one day, you copy daily, most sites copy a few times/hour or even use standby databases that receive the transactions in a near sync way. In general you start restoring the database and perform a recovery until time. Normally we use rman to do this, something like 

You have 32G real memory on your system, use amm and have almost 25G for memory_max_target. That does not leave a lot of memory available for other tasks. Chances are that you SGA is swapped out of memory. Also the connection pool is a bit big. Can your system handle 350 concurrent running sessions? I would start with a number that is close to the number of CPU's of your system. AMM is OK, as long as your application is a perfect Oracle citizen. This means, it does use bind variables where possible and only uses literals in SQL for class selections. If your application is not such a perfect Oracle citizen, start with setting a fixed db_buffer_cache size to protect the database cache by being pushed down in size by the shared pool. If your app uses many literals, it won't re-use cursors -but still tries to cache them- and waste valuable shared pool space. Check v$sql .... do your cursors have high 'executions' or do you have many app SQL that have executions = 1? To prevent swapping out the SGA, use LOCK_SGA=true and use larger memory pages, say like 16MB page size. This gives better use of the memory and reduces CPU usage. As mentioned before .... the swapping in itself does not need to be a problem, if your app keeps running as intended it could be OK. It could run quicker if it would not happen. best tip: hire a real DBA to do some serious diagnosis. It is a job that requires a little more than just being able to install software and create an empty database. 

It looks like the hardening was a little to hard. Oracle does need some access to the server. You can revert back the chmods for ping, Oracle rdbms does not need it. /tmp/ is not required, a usable location for tmp is, mostly for scripting. The rdbms has it's own tricks and storage for sorts. Never install Oracle as root; use the root.sh at the end of the installation to do the system tweeks required. On some platforms there is a root pre script that should run before installation. The Oracle installation scripts refuse to run using the root account and can be run using any other account. The access to some server config files has been made a little too tight for the install user to check the prerequisites. If you know they are ok, it's ok. Proving they are ok is a little easier when support issues arise. Check the group ownership of the critical config files and add the Oracle installation user to that group to give it read access. 

Use the tools that you are most comfortable with. Your situation seems simple and lasts only for a short time. If you have to keep this up and running for a long time, you might end up by using more advanced tooling, where you define rules for conversions and synchronizations to be performed for you. Doing so is easier to maintain and to document. The learning curve might be a little steep. On the other hand, you could use this case to train yourself. There is no urgent deadline and when it gets more urgent, you probably build the home cooked version within a day. 

As long as you have your database organized in such a way that there is an account that is the owner of the tables and other objects, and your application connects using a separate account you can do what you want. If you have tables and procedures organized in separate owners, you fist have to grant the required tables to the procedure owner (directly granted). Next you can grant the execute to a role or a user. As soon as you allow connections to an object owner your security is broken since in that case you can always use all the objects of the owner (you are owner so you can use it). 

This depends on the flavor of *nix where you run on and whether the installation was customized or not. If something of Oracle was installed on your system, the directory /etc/oracle or /var/opt/oracle normally would have to exist. Normally they contain a file /etc/oraInst.loc or /var/opt/oracle/oraInst.log that points to the inventory that contains the central registry of the installations done on the host, if it has been done in a standard way. If you found the oraInstloc you know at least that some installation has been taken place. The inventory contains the details about what was installed and where. Normally this directory is protected. If the /etc/oratab or /var/opt/oracle/oratab file has been maintained, it contains a list of all instances running on your system, including the software locations. This is the file that is used by the oraenv utility that sets the minimal environment variables you need to be able to use that software for the specified ORACLE_SID. The oratab, if maintained shows all defined ORACLE_SID's, also when they are not running. But again, asking your dba might save a lot of time. 

put the code in the action or in the procedure definition of aud_clear_fun, that really needs to be a procedure to work with Oracle Scheduler. The Scheduler is not going to read the returned value ... 

It is fairly easy to consolidate multiple databases into one real database. In Oracle, a database is the collection of files. Users connect to the database by connecting to an instance. A database can be served by multiple instances, in which case you are running RAC. So for simply consolidating database into one single database you don't need RAC but you could, if you want/need to do this. If you are consolidating into one database there are a few things to take into account: 

should do the trick. If you work with Oracle, make sure that the environment is setup correctly. Also note that if you have multiple Oracle installations on the same Windows machine, the PATH is modified such that the last installed Oracle installation is selected first from PATH. That gives quite a few side effects. 

In order to find a specific session running a certain script, it is easiest if the scripts makes itself identifiable by using dbms_application_info and dbms_session. See Morgans Library for details. Also see track the parts of my application that are in use for sample code. Doing so enables you to select on v$session and filter not only on username and machine but also on module, action, client_info and client_id. Username and machine are not mutable but the other columns are controlled by dbms_application_info and dbms_session. Using those makes your script recognizable. This of course, only works when the other session has select privileges on v_$session. dbms_application_info is also great for finding where your code is running; a way to instrumentate your code to make performance analysis easier. your code in SQLdeveloper would be as simple as