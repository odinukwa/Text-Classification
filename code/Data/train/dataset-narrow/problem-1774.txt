I need to monitor a single process (e.g. be warned when there are more than 3000 connections established) and collect statistics on it (e.g. to determine how many connections were established today 01:20 AM, when the server worked too slow, as client said). What tools should I use? 

to Log into the remote system and type xclock &. This starts a X clock program that can be used for testing the forwarding connection. If the X clock window is displayed properly, you have X11 forwarding working. 

I've loaded this files to document root and opened in Firefox 3.5. I see the first frame, see controls, but can't play video. This video plays good even from my server. How should I encode video to view it in browser? UPD: If I start playing video from the middle, everything works fine. 

You need to use non-standard ssh-ports for your VMs. I use openVZ on my server and have three virtual machines with VIDs 101,102,201. I thought, that it would be convinient to ssh into 101 VM through 101 port, into 102 through 102 and so on. To connect to the hardware node you can use 22 port. So I added this routing rules to iptables on hardware node: 

I have hosts A,B and C. From host A I can access through ssh only B. From B I can access C. I want to be able to run X11 programs on C and forward display to A. I tried this: 

Account will expire at the end of 2009-07-03. Logged in user won't be kicked out. He won't be able to log in after he logout. 

What you probably want to do is a reverse proxy. Assuming the other websites on the linux server are running under Apache. Then using mod_proxy your config would be something like this: 

Another option is if you're willing to run your own DNS server, you can use includes to easily have the same config for all domains. I do this and then have my DNS Provider replicate the settings from there. Doing this adds a bit of overhead on the maintenance side though. 

You really need to talk to the Network and/or Host Server administrator about this. If for example you're in a corporate environment they might not be too happy with you if you did something like this. That said, you could probably use an SSH Tunnel, though I don't know how practical that would end up being. 

It looks like your rule is getting you to a PHP file. I'm not sure what you mean by "downloading a php file" if you mean its downloading the code as plain text then you've likely got a problem with you php handler and not the routing. Otherwise it probably means that the PHP code is using the original URL to do routing and not your rewritten URL. What it sounds like you really want to do is have your final rule be a redirect. So your rules should be something like this. 

Personal I've been happy with FreeBSD. For some reason it's always made a little more sense to me than other *nix OS (It has great documentation). The ports system is great for applying updates, and I've had little problem with updates breaking things. Pkg_updating especially helps. With any distro, if you're using packages that come with the os or even through 3rd party repo's, you're going to have to wait a least a little bit for them to build the latest version of the software you want, and sometimes that's a good thing because there may be stability or dependency issues you might not know about. Of course, you can always build directly from source, then you never have to wait. PS: Upgrading the base os is also quite easy and stable using freebsd-update even between major versions. 

You need user account on the server (login/password may be different, than for your ftp-account). This user must be or have permissions to run . 

While installing package I get some additional packages on dependency. But when I remove installed package with yum, the dependency packages are not removed, though I don't need them anymore. How can I remove them automatically? 

I tried to measure network performance by soon discovered that terminal was fine. What has happened? We have a load balancing between two Internet channels router. Sometimes it routes my ssh traffic through wan1 and sometimes through wan2. I proposed, that there is something wrong with only one channel. So I measured network performance with mtr (great tool!) for two channels separately. yeah! wan2 has 21 hops with 110 ms and wan1 has 15 with only 21 ms! wan2 latency is the problem. 

I ssh on remote host but terminal performance is poor. Symbols I am typing are not shown immediately, but with some delay. Sometimes two symbols are shown at one time after delay. 

This asks foo.bar 100 times for URI /test with fixed rate 10 connection per second. If you want to ask different URI's, run many httperf instances in parallel mode with different --uri parameters. 

You can login to EC2 instance with the same access key you use for default user. Suppose, you are running Ubuntu and default username is . Then your public key is located in Create new user as described in AWS documentation and add public key to its . Now you should be able to login to you instance with original key pair. 

Let's assume I know nothing about mailservers or Linux/Ubuntu. I want to set up a mail server that will easily allow me to add emails with different domains and connect with various email clients via pop/imap. What is the best way to set this up? I believe I have postfix running, but that's as much as I know. :/ I'm a noob at this. :) 

I want apache to automatically recognize subdomains based on the filename that I have placed in /var/www/subdomains/ I'm a complete apache noob. P.S. I'm using Ubuntu 10.10 

I just set up Invision Powerboard on my server (Ubuntu 10.10). We have the name server pointing to my IP via dns record (outside that hosting). When you access the url in Safari, everything works. Using any other browser, it will redirect you after login to domain.com/domain.com/... This seems really weird. Details removed to protect security/privacy. Using no .htaccess rewrite The one generated by IPB, if I chose to enable it would be this: 

I would like apache to send 10.23.45.67/subdomain to /var/www/subdomain What is the easiest way to do this? 

I'm a total noob when it comes to Servers. I've been designing websites for about four years, and I'm completely self-tought (22 years old)... I want to get away from Mediatemple because they don't support rails. I'm trying Rackspace cloud hosting right now, and I've got a beginner's grasp on Ubuntu, but only slightly. I'm probably going to start my config over...so if I stick with Rackspace, will I have to monitor stuff all the time, or can I set it up, and check back like once a month? If I can set it and forget it, which platform should I use? (Ubuntu, Debian, Fedora)...And what are some good things to keep in mind? P.S. I'm currently having trouble setting up a basic mail server. I'm a bit frustrated with the learning curve, but I still like the versatility. Should I just stick with managed hosting? 

You'll probably need to tweak these because I don't understand all of what you're trying to accomplish. For example the RewriteCond below seemed redundent so I left it out. If you really are trying to rewrite everything under the guide directory you'll have to incorporate it back in. 

Instead of linking to the static files directly use your controller to read in the files and return them. Then in order to avoid the extra overhead on every call, cache the files in a separate cache location. You can then use the same process to cache your dynamic content and you it could also allow you to build things like your css dynamically. For example you could merge the css from different modules into one file or change the human readable JavaScript into a highly minimized version. 

Another way to do it would be to have a public folder inside each module. Then use alias' and/or rewrite rules to direct requests to the appropriate location. Using the directive you can set explicit permissions for folders outside of the document root. Doing it this way though would likely require you to reconfigure Apache every time you added a module. Something like this might work: 

With this configuration if the requested URI isn't an actual location it'll get routed to and from there to the backend. The part is there to prevent unauthorized code execution, so if the requested URI is which doesn't exist then a simple nginx is generated. What would be a better way to configure this (& remain 'secure') so that a request for a nonexistant php file also get passed to the backend via ? 

I have HAProxy 1.5 running on Ubuntu 14.04 (modified). It accepts connections on http and https ports. Two backend applications process requests using persistent connection. When I create around 2200 client connections haproxy stops accepting additional connections. But I want this system to accept at least 10K simultaneous connections. Here is connection statistics: 

First, you must be sure, that there is your ISP fault. Try utility. It is combination of and which can provide you more information about latency bottlenecks. From its output you will find out, where the problem occurs. Simple usage example: 

I think, this potentional issue is called load-balancing router. It routes your request sometimes to the 1st interface and sometimes to the 2nd. They have different latency. For example, we have optics on wan1 and ADSL on wan2. For one particular server ping through wan1 was 17 ms and through ADSL - 112 ms. 

I want all the files in directory be owned by nginx.devel. I have performed once and update these files using . But if I create a new file and then it, it will be owned by user.user and I need to run with privileges on it. 

In this case you will be asked for password which travel the network through encrypted ssh-channel and it will be not displayed in console. 

You should place it somewhere outside webserver root, for example. It has security considerations: webserver code must not be exposed to uploaded code, and it can happen, if you mix these two things and set wrong permissions. And you keep your content between upgrades. If users upload small files, the best way is to store them in database. 

If I'm following your setup correctly you have your SPF record set on the root, but you are sending email from the sub-domain if the 'envelope from' of the email is root@mail.myapp.com then you need to change the "@" txt record to "mail". if you were sending mail from root@myapp.com then your setup would be correct. So something like this 

For a reverse proxy, fixing the links in your content output (as mentioned in the other answer) is the better way to go, however mod_proxy_html is another possibility. That said, if you're serving both applications out of the same Apache server why bother with the two virtual hosts? You should be able to serve two different languages from the same virtual host, that way you can skip the reverse proxy step completely. 

Unison might work, I believe it meets all your criteria. It can work peer-to-peer, but if this is for more than 2 users it'll be easier to have a central location to sync to. 

Many of the guides for setting up Wordpress (or similar applications) on nginx with php-fpm use a configuration that in part looks something like this: 

You'll probably also want to do a search on drupal and pretty or clean urls which will give you more info on this and will also let you drop the index.php. 

It's called a Reverse Proxy. You can do it with Apache as well as other web servers. There are also dedicated proxy servers that you could use as well. 

EDIT: It should work without the RewriteCond as the first two conditions should cover it unless you're doing more rewrites in that directory or something. Are you using custom error pages? Something is changing the Request_URI so that it no longer matches any of the conditions. If you're not on a shared host or if your host supports it try setting up a rewrite log this will tell you exactly what is going on.