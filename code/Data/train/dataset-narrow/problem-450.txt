We use partition switches to quickly load data, switching entire tables. We load an empty Table A with INSERTs and then switch it with empty Table B. These tables have identical indexes. What I don't understand why Table A's index as no fragmentation but Table B's index has fragmentation (16% in one example). I assume because both tables had to have the same indexing, the indexes also got switched, but clearly that's not the case. I am looking for an explanation as to how this works and why this happens. 

The sum of the days the subscription is set to run is the number in this column. Thus subscriptions that run Monday through Friday have a value in this column of . The end result I want is a view that derives T/F flags for each day based on this number so that I have a column for each day. The method I am currently exploring to get me there is converting this integer to base-2 so then I can convert that to a varchar and parse out the days. In this example, the result would be 111110. The final twist - I do not have the ability to create functions or stored procedures in this database, so my strong preference is to solve this within a statement... (If push comes to shove, I will move the raw data and use a function in a separate database - and have found a number of those online.) 

Performing those subqueries repeatedly for each distinct ID value is what's killing you. I ran a test of your method on a sample table of 10,000 rows and it took ~2.5 minutes to execute on my notebook. By comparison, the following code consistently returned the same results in less than 0.5 seconds: 

Note that the query uses syntax, which is generally preferred over the older method of putting the join conditions in the clause. 

Edit I just ran another test on a sample table of 400,000 rows and the code above completed in under 7 seconds 

HSQLDB stores text data as Unicode. You can verify that by opening the file for the database and looking at the data. For example, for a table with a row containing "Montréal" the file contains 

The most straightforward way to have Access VBA interact with a web service would be to use a object like this: 

I am using the PIVOT function in Oracle and am curious if I can replace the null values with zeroes? I know I can wrap the entire query in another SELECT and then use COALESCE on the values, but I am curious if there is a shortcut. 

Our SQL Sever 2008 application database is replicated from Server A to Server B (push replication). We use the copy, let's call it database_b, on Server B to create reports and run other queries so that our reports do not interfere with the application. Currently we leverage inefficient views to combine data across multiple tables in database_b so that report writing is simplified for our report writers (who have basic SQL skills). 99.9% of the database activity is INSERTS, so we are exploring a way to replace the inefficient views with tables we can optimize. Here is a simplified example: There is an table and a (lookup) table. Every time a new appointment is scheduled, a row is added to the table. Every time this INSERT happens, I want to take that and insert it and its corresponding location name (join on from both tables) into a reporting table. I have accomplished this with a trigger on the appointment table in database_b on Server B. My question is - are there any particular considerations given that database_b is a replicated copy? Do I need to worry about a failing trigger mucking up the entire (push) replication process? Anything else I am missing? Unfortunately it's difficult to test this in our development environment, so I don't have the opportunity for a lot of trial and error. 

I recently had to do essentially the same thing, except that I wanted to migrate MySQL tables and data to SQL Server. I tried: 

Unfortunately, no. The basic Find (Ctrl+F) and Replace (Ctrl+H) functions in the Access UI search the resultset of the form row-by-row. For an ODBC linked table (any ODBC linked table, not just PostgreSQL) once Access has gone past the end of the rows that it pre-fetched when it displayed the form it retrieves one row at a time until it finds a match or reaches the end. (Find and Replace is often considerably faster for Access linked tables - as opposed to ODBC linked tables - because the Access Database Engine works directly with the database file, retrieving 4KB pages of data at a time. If consecutive rows happen to be on the same page, or if the 4KB page for a given row has recently been cached, then Access can "get" that row without another round-trip to the database file.) However, there is one workaround that may be helpful: If a Filter has been applied to a Form or Datasheet view then Find and Replace operations are restricted to the filtered rows. Access can take advantage of indexes and (at least some) server-side processing when applying a Filter to a form bound to an ODBC linked table, so you might be able to get the job done faster by Filtering the records first (even a non-sargable filter such as ), and then performing the Find and Replace on the filtered set. 

We are going to be implementing a mechanism to track changes in our SQL Server 2008 application database. The programmers are concerned about INSERT speed being negatively impacted, so I have set out to test three cases: no mechanism to track changes, change tracking, and Change Data Capture (CDC). In all of my tests, there appears to be no significant difference in the speed of INSERTs. Does this make sense? I want to make sure I am not overlooking anything before I remove this concern from our list of requirements. (Note: my testing has shown differences with UPDATES, but we are only concerned about INSERTS) EDIT: My tests were relatively simple INSERTS into a production-like table, but 250K of them. The processes ran over 5 minutes, so I'm comfortable it was enough for our environment. 

I have a number of Oracle stored procedures whose activity I want to be able to audit, so I am having them INSERT some data into an auditing table. It would be swell if I could list the variables and their values as well. Is it possible to get this list in an automated way? I am already using to get my procedure name. 

As you have discovered, the column type in Access does not support fractional seconds. If you need to preserve the precision (and not round to the nearest second) then you'll need to import the time values as and then populate another column with the times converted to tenths-of-milliseconds (which I have abbreviated to "tms"): After the initial import of the times as you will have something like this 

Yes. The Access Database Engine is designed to work with "real" Windows file sharing and Samba – or at least some versions of Samba – apparently do not implement all of the low-level features of the SMB protocol that are needed for the Access Database Engine to work reliably. (ref: Corrupt Microsoft Access MDB Causes - Samba) 

Instead of trying to insert directly into the table you could insert your data into a temporary table ... 

It is a bit kludgey, but here's an idea sketch... instead of merging the cells, could you fake it by placing an A in the left cell aligned to the right and a D in the right cell aligned to the left? You could then use expressions to control value, alignment, color, and borders of the two separate cells. Let me know if you need more details. I think this would give you the visual you want. Good luck! 

I am looking for the most efficient way to solve this situation: We are an Oracle shop. I need to load data from one table to another. My source table, which contains 30 million rows, has a person identifier that is supposed to be unique, but it is not enforced so there are cases where it is not - about .1% of the time. There is logic I can use to resolve the non-unique cases. I want my target table to "enforce" the uniqueness of the identifier - so I have defined this identifier as my primary key. (Note that the data that is loaded isn't a straight source table -> target table; there are a few look-up tables that are used too) My goal: load this table as quickly as possible. It's a lot of rows, so efficiency is key. I have thought of a number of ways to address this, but I'm not sure what would be the most efficient. My ideas include: 

They both do what is essentially a "Compact and Repair" into a new database file. (The original database file is not compacted, only the resulting copy.) 

An "After Insert" data macro can certainly modify records in other tables. You can use the LookupRecord, EditRecord, and CreateRecord Data Blocks as required. However, a data macro cannot run a VBA script. Your actions are limited to those available in the data macro environment itself. 

Your query is missing If you are just getting started with SQL then you might consider building simple queries like this in the Access query designer and then switch to SQL view to see what the query designer has generated. In your case we have 

Otherwise the characters really are converted to question marks if SQL Server is unable to map them to corresponding non-Unicode characters, even in an column. 

I would like to know more about the data-driven subscriptions on our ReportServer. I am specifically looking for what shared data sources are being used, but the connection string would also suffice. I have poked around the and tables, but nothing seems to give me the information I need. Any ideas? 

(I fear what I wish to accomplish shall not be easy, but a lot of my googling around about it yielded rather dated results, so here it goes...) I have a Postgres 8.4.x database with over 100 tables. I need to recreate these tables in SQL Server 2008. I'm not concerned about the data, just the structure. Are there any slick tools, shortcuts or suggestions to accomplish this? Heck, I'll even take the find-and-replace values to run on Postgres CREATE TABLE scripts! Thanks! 

Here's a bit of a challenge. I am working with the table and there is a column called . I may not be using the proper terminology here, but this is an integer from which one can derive the days of the week an SSRS subscription is set up to run. Each day is assigned a number as follows: 

Finally, even though you will be working with SQL Server 2005 I highly recommend that you get a copy of SQL Server 2008 (or 2012) Express for learning, mainly because of the IDE enhancements in SQL Server Management Studio (namely, auto-completion and interactive debugging). Just be aware of any newer (2008+) features you may encounter and don't rely on them for your production code. (Or, use them as bullet points for your pitch to upgrade from SQL Server 2005....) 

If you want to insert Unicode text in your statement you need to supply a Unicode string of the form , as in 

The "Back Up Database" and "Save Database As" options do the same thing. The only real difference is that "Back Up Database" will provide a default file name like 

If the web service returns a relatively short and straightforward response then you could just parse it yourself. 

I need to make a decision about deployment and security of some SSRS reports and would like some advice on what the most appropriate method is. We have 40+ sites that have access to information for only their site. Each site has access to a number of reports, say Report A, Report B, etc. For each site, the report name is Report A - Site 1, Report B - Site 1, etc. For each report (Report A), there is a Master report (Report A - Master). The Master report is the report itself - dataset and formatting - and includes a hidden parameter for site name. Each sites' report links to this Master report and passes in the site name. Thus if the report changes, the change is made in one place. Please note that limiting the reports to site-specific information on the database is not possible. Our data source uses a service account and all users access to the information is handled in the Web Portal. Now it is time to grant users access to these reports. Currently it is set up that all of the reports and the master reports are in a single folder. Access is controlled on each report individually and users must have access to the Master report as well as their site reports. I would strongly prefer to set up a folder for each site and control access that way. Obviously that would make it easier as site-specific reports are added, and it's superusers who will be controlling access. My concern is this will make deployment of reports incredibly difficult. Because of the linked mechanism, all 40+ versions (plus Master) are in the same BIDS project - and it seems like a nightmare to deploy to 40 different locations for every change. I would like to know if anyone else has had a similar challenge and found a good way to solve it. I have played around with linked reports, but then the site report can't "find" the Master report, even if a linked copy is placed in the same folder. EDIT: To clarify, and use the exact terminology, the Master report is a sub-report of each site report.