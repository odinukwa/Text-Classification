It is somewhat misleading to say that Haskell's typing system is "the hinley-milner type system". Haskell's types are much more powerful, including, among others, higher-kinded types. Indeed the typing system is so powerful that you can embed Turing-complete programming languages in the typing system, see here. This is not the only reason for Haskell's power, Cody has mentioned some others. 

So the general setup you are interested in is this: you are given 3 language $(A, \cong_A)$, $(B, \cong_B)$ and $(T, \cong_T)$, each comprising of a set of programs ($A$, $B$ and $T$) with associated notions of program equivalence ($\cong_A$, ...). In addition, you have two mappings: 

The question you ask has been widely and acrimoniously discussed under the heading "checked exceptions or unchecked exceptions?". Here "checked exceptions" means the Java model where all possible exceptions a method can throw have to be declared explicitly. I think it's mostly agreed that while checked exceptions might seem a good idea when looked at from the point of view of a conventional type-theorist, they are terrible for large scale software engineering, because simple changes to code in one place may trigger changes in the exception interface of vast amounts of seemingly unrelated code. As Neel points out, with modern typing systems (typically extensions of Fω), more expressive than Java's, some of these problems can be alleviated, although not always without introducing new problems (such as making type-inference harder). This is the main reason why modern, and otherwise strongly typed languages such as Scala and Ocaml don't have checked exceptions. In my opinion, the last word on this issue has not been spoken, because with increasing use of logic-based specification and verification techniques, it's increasingly useful to let the type-checker infer as many (basic) program properties as possible. OTOH, most exception usage is simple (when something goes wrong, throw an exception, and catch it in the outermost loop, printing "sorry, please restart" or some variant thereof), using unchecked exceptions is not that big a software engineering problem in practise. 

Some probability distributions are easier to work with than others. Consider the following two problems. 

K. Honda, V. Vasconcelos, N. Yoshida: Secure Information Flow as Typed Process Behaviour. K. Honda, N. Yoshida, A uniform type structure for secure information flow. M. Berger, K. Honda, N. Yoshida: Sequentiality and the Pi-Calculus N. Yoshida, K. Honda, M. Berger: Strong Normalisation in the Pi-Calculus M. Berger, K. Honda, N. Yoshida: Genericity and the Pi-Calculus K. Honda, N. Yoshida, M. Berger: Control in the Pi-Calculus K. Takeuchi, K. Honda, M. Kubo, An Interaction-based Language and its Typing System. K. Honda, V. T. Vasconcelos, M. Kubo, Language Primitives and Type Disciplines for Structured Communication-based Programming. D. Orchard, N. Yoshida, Effects as sessions, sessions as effects. D. Sangiorgi, D. Walker: The π-Calculus: A Theory of Mobile Processes C. Hewitt, Viewing Control Structures as Patterns of Passing Messages. S. Fowler, S. Lindley, P. Wadler, Mixing Metaphors: Actors as Channels and Channels as Actors. 

* There is a subtle issue in that if you want to define higher-order functions by recursion in nominal logic, the recursion operator requires some kind of freshness. See the conclusion of Pitt's Nominal logic, a first order theory of names and binding, which points out that "Nominal Logic does not give a complete axiomatisation of the notion of finite support that underlies the notion of freshness in FM-sets. Nevertheless, the first-order properties of a notion of freshness of names presented in this paper seem sufficient to develop a useful theory, independent of any particular object-level language involving binders". 

H. Schwichtenberg, Definierbare Funktionen im λ-Kalkül mit Typen. M. Zakrzewski, Definable Functions in the Simply Typed λ-Calculus. M. Zaionc, Word operation definable in the typed lambda-calculus. M. Zaionc, λ-Definability on free algebras. 

I think the overall goal of PL theory is to lower the cost of large-scale programming by way of improving programming languages and the techincal ecosystem wherein languages are used. Here are some high-level, somewhat vague descriptions of PL research areas that have received sustained attention, and will probably continue to do so for a while. 

One may ask at this point why the two dimensions are so seemingly different and how they nevertheless relate. The key insight is that programming language research and development has multiple dimensions: technical, social and economic. Almost by definition, industry is interested in the economic payoff of programming languages. Microsoft et al don't develop languages out of the goodness of their hearts but because they believe programming languages give them an economic advantage. And they have investigated deeply why some programming languages succeed, and others, seemingly similar or with more advanced features, don't. And they found that there isn't a single reason. Programming languages and their environments are complex, and so are the reasons for adopting or ignoring any specific language. But the single biggest factor for the success of a programming language is the preferential attachment of programmers to languages that are already widely used: the more people use a language, the more libraries, tools, teaching material are available, and the more productive a programmer can be using that language. This is also called the network effect. Another reason is the the high cost switching languages for individuals and organisation: mastering language, especially to a not-so-experienced programmer, and when the semantic distance to familiar languages is large, is a serious, time-consuming effort. Given these facts, one may ask why do new languages get traction at all? Why do companies develop new languages at all? Why don't we just stay with Java or Cobol? I think there are several key reasons for whey a language does succeed, and displaces the originally more popular incumbents. 

I suggest not to give up on the operational intuition. Operational is primary, all semantics are derived, and are but proof techniques for operational semantics. The key ideas are as follows. A program $P$ uses a variable $x$ universal-polymorphically, provided $P$ doesn't do anything with $x$ that requires knowledge of $x$'s type. For example in the $\lambda$-calculus the following operations could be polymorphic (whether they are or not depends on the precise language semantics). 

Formal semantics is useful primarily when you want to reason about programs. In the past this was mainly done in programming language development (and to a lesser degree in compiler construction). Increasingly formal semantics is also used in automatic and interactive program verification, which is beginning to see industrial use. Since you are interested in compiler design, verified and certifying compilation is a hot research topic now that is based heavily on formal semantics. Check out the following systems. 

For analytical purposes it's useful to keep them apart. I will focus on the former. The ability of a programming languages to represent, manipulate (and run) its programs as data goes under terms such as meta-programming or homoiconicity. In an (awkward) way, all well-known programming languages can do meta-programming, namely by using the string data type together with the ability of invoking external programs (compiler, linker etc) on strings (e.g. by writing them to the file system first). However, that's probably not what you mean. You probably have nice syntax in mind. Strings are not nice syntax for program representation because almost all strings don't represent programs, i.e. the string data type contains a lot of 'junk' when seen as a program representation mechanism. To make matters worse, the algebra of string operations has essentially no connection with the algebra of program construction. What you probably have in mind is something much nicer. E.g. if $P$ is a program, then $\newcommand{\QUOTE}[1]{\langle #1\rangle}\QUOTE{P}$ is $P$, but as data, at hand for manipulation and analysis. This is often called quotation. In practise, quotation is inflexible, so we use quasi-quotation instead, which is a generalisation of quotation where the quote can have 'holes' in which programs can be run that provide data to 'fill' the holes. For example $$\newcommand{\SYNTAX}[1]{\mathsf{#1}} \newcommand{\IF}[3]{\SYNTAX{if}\; #1 \; \SYNTAX{then}\; #2\; \SYNTAX{else}\; #3}\QUOTE{\IF{[\cdot]}{7}{8+9}}$$ is a quasi-quote representing a conditional where instead of a condition we have a hole $[\cdot]$. If the program $M$ evaluates to the data $\QUOTE{x > 0}$, then the quasi-quote $$\QUOTE{\IF{[M]}{7}{8+9}}$$ evaluates to the data $$\QUOTE{\IF{x > 0}{7}{8+9}}.$$ (Note that $M$ is a normal program (not a program as data) that returns a quoted program, i.e. program as data.) In order for this to work, you need a data-type to represent programs. Typically that data-type is called AST (abstract syntax tree), and you can see (quasi-)quotes as abbreviation mechanisms for ASTs. Several programming languages offer quasi-quotes and other feature for meta-programming. It was Lisp with its macroing functionality that pioneered this ability to treat programs as data. Perhaps unfortunately, the power of Lisp-based macros was long seen to rest largely on Lisp’s minimalistic syntax; it was not until MetaML (1) that a modern, syntactically rich language was shown to be capable of meta-programming. Since then, MetaOCaml (2) (a descendant of MetaML, important for its breakthrough in the still ongoing quest to solve the problem of how to type programs as data), Template Haskell (3) and Converge (4) (the first language to get all key meta-programming features right in my opinion) have shown that a variety of modern programming languages can house meta-programming. It is important to realise that we can take any programming language $L$ and turn it into a meta-programming language $L_{mp}$ that is $L$ together with the ability of representing (and evaluating) its own programs as data. Representing the outcome of running program, given as data, is achieved by adding an $\newcommand{\EVAL}{\mathsf{eval}}\EVAL(\cdot)$ function that takes a program (given as data) as input and runs it, returning its result. E.g. if $P$ is a program evaluating to 17 and $\QUOTE{P}$, the (quasi-)quoted version of $P$, i.e. $P$ as data, then $\EVAL(\QUOTE{P})$ also returns 17. There are all manner of subtleties here that I'm ignoring here such as the question when meta-programmed programs are being evaluated (giving rise to the distinction between compile-time and run-time meta-programmed), what to do with types or failing evaluations, what happens to bound and free variables in the process of going from $P$ to $\QUOTE{P}$ or vice versa. As to the second dimension, reasoning about programs given as data. As soon as you can convert programs into data, they are 'normal' data and can be reasoned about as data. You can use all manner of prover technology, e.g. dependent types or contracts or interactive theorem provers or automated tools, as Joshua has pointed out. However you will have to represent the semantics of your language in the reasoning process. If that language, as you require, have meta-programming abilities, things can become a bit tricky and not much work has been done in this direction, with (5) being the only program logic for this purpose. There is also Curry-Howard based work on reasoning about meta-programming (6, 7, 8). Note that these logic-based approaches, and the type-based approach (2) can indeed express properties that hold for all future meta-programming stages. Apart from (2) none of those papers have been implemented. In summary: what you asked for has been implemented, but it's quite subtle, and there are still open questions, in particular to do with types and streamlined reasoning. 

There are several ways to learn about type theory. For a working programmer, Types and Programming Languages by B. Pierce is a good start. Practical Foundations for Programming Languages by R. Harper might also be good. If you want a bit of easy to read background on operational semantics, I recommend G. Winskel's, The Formal Semantics of Programming Languages: An Introduction. With T. Nipkow, G. Klein, Concrete Semantics, a variant of Winskel's book has been formalised for the Isabelle/HOL interactive proof assistant. I suspect it's really difficult to get to grips with a prover just from this (or any) book, you'd want an expert nearby to ask questions. If you want a more mathematical approach to type-theory, you could look at J. R. Hindley, J. P. Seldin, Lambda-Calculus and Combinators: An Introduction, or H. Barendregt's, Lambda Calculi with Types. Although I wouldn't recommend starting from Barendregt. If you want a single recommendation, I'd say read all of Pierce except Part VI (Higher-Order Systems), and implement the toy languages the book discusses. You'll end up with a strong grounding in type theory, and probably a better programmer too. 

N. de Bruijn: The Mathematical Language AUTOMATH, Its Usage, and Some of Its Extensions. R. F. Harper, F. Honsell, G. Plotkin: A Framework for Defining Logics. F. Pfenning: Logical frameworks. F. Pfenning: Logical frameworks -- A Brief Introduction. 

Here is a simple encoding: $ (\nu a)(\overline{a}\langle x\rangle | !a(x).\big( in(y).\overline{a}\langle y \rangle\ +\ \overline{out}\langle x \rangle.\overline{a}\langle x \rangle\big) $. 

Quite a bit is know about this. The concept of Pure Type Systems (PTS) is useful for showing Church-Rosser (CR) for large classes of typed $\lambda$-calculi. Paraphrasing (1): 

The term program inversion has multiple shades of meaning, but probably got started with J. McCarthy's 1956 work The Inversion of Functions Defined by Turing Machines in the context of AI. By now many connections between program inversion and other fields have been discovered, e.g. reversible programming (physical and logical), partial evaluation, verification, bidirectional programming, logic programming, and machine learning. What is program inversion? In first approximation it's something like this: Given a program $P : A \rightarrow B$ taking arguments of type $A$ and returning results of type $B$, produce a program $P^{-1}$ that is "somehow" the inverse of $P$. I'm deliberately being vague here, since the concept can be (and is) clarified in various ways: e.g. is $P$ required to be injective? Should $P^{-1}(b)$ return all or just some $a$ such that $P(a) = b$? There are generic ways of inverting a program, e.g. using diagonalisation as already pointed out by McCarthy, or using partial evaluation, but they tend not to be efficient. Also most work on program inversion I'm familiar with does not seem to deal with full higher-order programming languages (i.e. $\lambda$-calculi). Reference request. What is the state-of-the-art in explicit algorithms for program inversion of $\lambda$-calculi (with no restriction on higher-orderness)? 

The latter typically takes place in industry with the aim of providing programming languages as a product. The teams developing Java at Oracle and C# at Microsoft are examples. In contrast, pure research is not tied to products. Its aim is to understand programming languages as objects of intrinsic interest and to explore the mathematical structures underlying all programming languages. Because of divergent goals, Different aspects of programming language theory are relevant in pure research and in product focused R&D The picture below may give an indication what's important where. 

PTS are very general formalisms and include System F, Fω, LF as well as the calculus of constructions. The last two are dependently typed. Both (1, 2) are quite old papers, and I imagine that more is known in 2015. 

If you are interested in concurrency theory, programming languages or interactive theorem proving, I warmly recommend the videos of the recent Milner Symposium. For example J. Parrow's talk The pi-calculus: Origin and recent developments tells the beautiful story of the early development of $\pi$-calculus, and B. Pierce's talk Types à la Milner is a lucid overview of work on types for process calculi. 

Extensional equality in Turing complete programming languages is undecidable in general, but that shouldn't stop you from being able to verify or falsify that any two specific functions are extensionally equal. Verification can proceed in many forms, you could for example reason in ZFC set-theory using the operational semantics. However, that would be painful. If denotational semantics exist, they could also be used, but good denotational semantics exist only for a few languages. Usually one uses program logics, e.g. Hoare logic, for showing the extensional equality of programs. In order to be able to do this, Hoare logics for languages with functions typically require an axiom stating that $f = g \Leftrightarrow \forall x^{\alpha}. f(x) = g(x)$, assuming that $f$ and $g$ are functions of type $\alpha \rightarrow \beta$ (details of the axiom variy with the details of the chosen approach to Hoare logics). 

Maybe slightly tangential to the original question, but the blog entry "How recursion got into programming: a comedy of errors" describes an interesting part of early computing history. 

You would be well-qualified to dive into more specialised subfields of programming language research with this background. None of the above would be very useful for the algorithms / complexity track of theoretical computer science. 

Dualisation $(.)^{\bot}$ (which is linear logic's negation) switches input and output. Hence the dual of $A \otimes B$ is $$ (A \otimes B)^{\bot} \quad = \quad A^{\bot} ⅋ B^{\bot} $$ In this reading $A^{\bot} ⅋ B^{\bot}$ is the process that communicates with $A \otimes B$. Linear logic's equivalent of disjunction can be given a similar process-theoretic reading. The formula $$ A\ \&\ B $$ should also be seen as two processes $A$ and $B$ in parallel, but rather than actively sending messages, they wait for the environment to decide which to run. So $A \& B$ sits there, waiting on its channel for a bit of information which decides if $A \& B$ should run as $A$ or as $B$. This is a 'parallel' version of the $if/then/else$ in sequential programmaning languages. The dual $(A \& B)^{\bot}$ of $A \& B$ is $$ (A \& B)^{\bot} \quad = \quad A^{\bot} \oplus B^{\bot} $$ can be seen as a process sending 1 bit of information to $A \& B$, namely: "continue as $A$" or "continue as $B$". This is similar to in $if\ true\ then\ P\ else\ Q$ evaluating to $P$ while $if\ false\ then\ P\ else\ Q$ evaluating to $Q$, except that the choice between $A$ and $B$ is now made by the environment.