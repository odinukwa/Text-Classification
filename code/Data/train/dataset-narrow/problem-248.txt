When servers are physically remote to each other there might still be good reasons to arrange them very tightly logically. Without virtual network connections this means running separate network links for each zone. This increases cost and can be prohibitively expensive. 

In a nutshell, you basically build two independent network and connect them with a router. This router becomes the default gateway for both networks and decides whether to route to the other side or towards the Internet routers. A fast yet inexpensive choice for this central router would be a layer-3 switch. Given geographic proximity of both departments, you can even hook up all clients to this switch and use VLANs to separate the network logically. 

There is no inter-VLAN communication without a router. Multicasts are not usually routed - unless your router is specifically configured to do so. 

one ACL name is probably misspelled the ACL entries are sorted by the sequence number at the beginning of a line - if you like a on the very bottom put it on or 

The interface is part of the policy. When you use , the rule applies to all interfaces, ie. it doesn't matter where the packet enters or is bound to exit the firewall for the policy to be applied. FGs apply RPF - in order to accept a source address on an interface there must be an appropriate route out of that interface. So, essentially you're not required to use interfaces in the policies at all times. RPF can be disabled by turning on asymmetric routing (), disabling stateful inspection on the way. A better way may be to set a route with a high metric that won't ever get used. 

Sounds like a definite mess. The solution is quite easy really: have an independent certifier check the installation, take the measurement protocols and hit the installer square on the head. There's no way this will pass. 

Without further information, the encircled links indicate channel bonding/trunking. Bonded channels (or LAG trunks, Etherchannel) are regarded as a single, logical link. STP doesn't block any of the physical links. Even if the physical links to a server are not bonded, there's only half a bridge loop: the switch forwards frames solely based on the destination MAC address and which port this MAC is - currently - located on from the switch's perspective. The server is not likely to speak STP, so no ports are blocked. When the ports are bonded on the server side, the switch may see the MACs flapping back and forth. Due to the nature of broadcasts, the server will also receive its own broadcasts back on the other port. Only static trunks can be active on one side without the other. LACP protocol trunks always come up on both sides or not at all (STP will block the spare links between switches) - they should be preferred. However, if there's some kind of switching software active on the server it behaves like a switch. With neither bonding nor STP active on the dual link you might have a bridge loop, depending on the nature of the switch software. For instance, an ESXi vSwitch never forwards from one physical link to another physical link, so it'll never cause a bridge loop. An alternative way to use multiple links between a server and a switch is to use them as separate links. This may accomplish better load balancing but doesn't provide full redundancy without further measures. 

The router's MAC addresses are associated with the Ethernet ports - since these are not used with the LTE modem your ISP won't see them. 

The MTU is the maximum IP packet size that can be transported on a given network link unfragmented. The IPv4 header and the TCP header (20 bytes each) eat into this packet size - the MSS should always be 40 bytes less than the MTU. When a TCP segment size causes the packet exceed the link's allowed frame size it causes a high degree of fragmentation which is very inefficient - the router needs to do a lot of work fragmenting and the header overhead is increased, decreasing the usable bandwidth. 

This depends on how the wireless and the Ethernet parts are connected. When they are transparently bridged (a common case) and are thus in the same segment and subnet, the bridge will forward all broadcast traffic, so an ARP request from one NIC - sent as a broadcast - will reach the other. If both belong to different subnets and segments then they won't get each other's ARP requests. Proxy ARP is when (usually) a router answers on behalf of a device that's not on the segment, so even then they won't hear the other interfaces ARP requests. 

On some devices, a trunk port implicitly encompasses all VLANs defined on the device, ie. a trunk port automatically becomes a member of each VLAN, unless configured otherwise. On other devices, there are no global trunk ports and each port's VLAN membership has to be either explicitly configured or learned through a registration protocol e.g. MVRP or GVRP. You'll need to check the manual for a given device's definition of trunk port or general VLAN trunking and how VLAN membership works. 

Auto negotiation is performed on the physical layer - the NIC driver may report what has been negotiated or it may not (only speed). If you want to look deeper into this I see two variants: 

Classes are long obsolete (since 1993!), replaced by CIDR in RFC 1518 and RFC 1519. What class B used to be back in the days is now known as /16, which you can subnet into four /18 subnets, two of which you can rejoin into a /17 subnet. 

Daisy chaining generally is not a good idea. Any problem with an intermediate switch will cause problems across the network. Additionally, security zoning with VLANs will get awkward at best. Best practice is to select a 'core' switch you connect all other switches and infrastructure devices to. Your router isn't well equipped as a core device with just two GE ports. If you need more bandwidtch between core and edge switch you can connect more than one link, provided you configure an aggregation group (LAG) for these ports on both sides (preferrably using LACP) - do not connect multiple links without configuring LAG or spanning tree. Running redundant connections between edge switches creates a physical mesh. This in turn requires you to configure spanning tree: select the core switch as root bridge and set it to a very low STP priority (0). The redundant links won't be used for traffic in normal use but they'll kick in automatically when an active link is disconnected. 

(Only classful in RIP? - had to look that up) RIPv1 uses classful routes which is one of the reasons it shouldn't be used any more. RIPv2 (RFC 2080) uses CIDR, doesn't transmit the complete routing table over and over again, leverages multicast (v1 uses broadcasts) and should be preferred over RIPv1 at all times. On a somewhat larger network you should consider to use a link-state routing protocol like OSPF instead of the aged distance-vector concept. 

The router checks the routes in its routing table and the most specific one (longest network mask) that matches the IP address is used. Depending on the egress interface, additional actions like source or destination NAT may be used (especially on the edge between private and public address space). 

Generally, this depends on context. Usually, virtual means that the IP address is not assigned on a permanent basis but may move around. A virtual IP address can be a public address or a private address. One example is a content delivery web cluster. On the public side it's got a pool of virtual IPs that one or more load balancers map to the servers' DMZ addresses. Under normal load there is a 1:1 mapping, so that a virtual address is always forwarded to the same DMZ address. However, when a server crashes or exceeds normal load the load balancer moves the mapping over to another server. Some vendors also call addresses that are mapped over destination NAT virtual IPs (e.g. Fortinet). This is the same as port forwarding. 

Additionally, you could e.g. examine LLDP packets or STP BPDUs to find out more about the infrastructure but you don't seem to be looking for that. 

Ethernet, IP, and UDP (with optional checksum) each only transport the packet while it is intact - when the checksum doesn't match, the packet is dropped. The only guarantee is that a received packet is as sent. TCP tracks which packets have been successfully received - they are acknowledged by the receiver - and automatically resends those that haven't. Ethernet may use dynamic "routing" but since this can cause unexpected side effects in higher layers it's rarely done. However, in higher layers it is not uncommon for packets to take different routes. TCP provides for this by resorting the packets into the original order before handing over the data to the application. Ethernet frames don't need to arrive in-order, broadcasts can actually get duplicated in certain conditions (e.g. SPB may cause this). All Ethernet specifications can be found at IEEE 802 - layer 1 is 802.3, layer 2 802.1: $URL$ 

Usually, a specific VLAN is associated with a specific IP subnet. So essentially, changing the VLAN association changes the IP address. Assuming you require routing between the VLANs/subnet, there's no other way. However, the IP doesn't change itself and is not assigned by the VLAN. It's either statically assigned or issued by a DHCP server serving that VLAN. 

Yes. Port mapping/destination NAT may also use a public destination IP. Imagine a load-balancing/fail-over scenario where you map virtual/balanced IP addresses to dedicated server addresses. Often this is done with private addresses but there's no reason not to use public addresses. 

I don't think there is a way. Both on the client and on the server side, sessions are tied to the client's IP address. This in turn is tied to the client NIC's MAC address which is likely to change when switching hardware (from wireless to wired or vice versa). So, we've got two essential requirements: neither IP address nor MAC address must change. For the server side this could be worked around with a single LAA MAC that is assigned to both wi-fi and Ethernet NICs. Sending a frame out from the client after switching NICs would update the switches to the new port. (Both wired and wireless connections would need to use the same segment.) For the client side this is much harder. You'd need to somehow hand over the open sessions from one NIC to the other which most probably isn't possible with any OS. The only imaginable solution would be a virtual NIC that all sessions run through. Its traffic would need to be bridged to the physical NIC. That way you could work around the IP and MAC address restrictions. Possibly there's a VPN solution out there that could accomplish this. A VPN (or other) tunnel could additionally solve the problem with needing to bridge the wired and the wireless network - the underlying, physical connection would have little importance.