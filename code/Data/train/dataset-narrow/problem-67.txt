Then there can be one or two cases depending on how much you want to optimise. If the tail extends outside of the end position circle, you draw the green-dashed nonagon with your decision-based shader. This contains vastly fewer pixels. If the tail doesn't extend outside, then you'll need to mirror this polygon to get the other side of the circle. Alternatively, rather than make the decision, you could always mirror it and let the Z-test eliminate unneeded fills. Your examples also have the have the capsules shown as opaque. If this is the case, you can also make gains with, say, TBDR-systems (a blatant PowerVR plug :-) ) or those with early-Z, provided you supply all your opaque "tail" portions first and then follow with all parts requiring decisions. The shapes of course were just examples. You may find it's better to use more segments in your outlines in order to reduce the regions requiring the complex shader. 

This last step, i.e. finding the best partition, can be done in linear time by starting with the "least" item in partition "A", all the rest in partition "B", and progressively shifting the next smaller over from "B" to "A, one at a time. You just need to keep separate sums of the values and sums of squares of the values for each partition and then you can compute the total MSE relatively cheaply. Note that the function should follow a quadratic curve so it (should) begin decreasing until it reaches the minimum. Once it starts increasing again you can stop the search. A cheap way to find the principal axis is to first generate the 3x3 covariance matrix and then use the "repeated matrix vector multiplication" trick to find the principal eigenvector (i.e. the principal axis) 

I'm sorry, I've no idea what you mean by "to crush z", but sometimes, if chosen carefully, after the projection transform you may be able to get by with just 3 terms, e.g. (X/W, Y/W, 1/W) assuming you also have the post-projection texture coordinates, (U/W, V/W). 

One possibility might be to borrow from the cryptographic community and, in particular, the 8-bit to 8-bit substitution used in the AES/Rijndael cipher. The table and code to generate it can be found on wikipedia. I'd guess that, in order to generate up to 256 additional tables, you could just do something like: 

"Downscaling by skipping rows and columns â€“ any example images?" You asked for example images: The following is a sequence of 50 frames from a well known (and hopefully freely available) sequence, subsampled as you suggested. I think you might find simply subsampling/decimating to be a bit too noisy. Are you sure you can't at least do some box filtering? 

Would Diego Nehab et al's "Accelerating Real-Time Shading with Reverse Reprojection Caching" be the sort of thing you are looking for? Paper Link and/or Slide presentation from Graphics Hardware 2007 IIRC, the idea is to reuse a previous frame's rendered image results in the current frame rather than running a, potentially expensive, shader for every pixel. At the conference, I did ask Diego if he'd considered doing 'bidirectional' reuse (i.e. rendering frames out of order somewhat akin to h.264) but I can't recall his answer. I think in your proposed use case it might be of value. 

Compared to BTC-schemes, the block artefacts typically are eliminated but there can sometimes be "overshoot" if there are strong discontinuities in the source image, for example around the silhouette of the lorikeet's head. The 2bpp variant has, naturally, higher error than the 4bpp (note loss of precision around the blue, high frequency areas near the neck) but arguably still of reasonably quality: 

As per Russ's answer, I'd also suggest using scissor rectangles but, on the off-chance that is also slow, is it possible to use the Z buffer to set the Z to "near" in the regions you don't want drawn? BTW, it would be useful to have an example image. 

I strongly suspect that you are simply running out of floating point precision (at least in "U" dimension). With this sort of mathematics, the LSBs will soon become noisy resulting in the jagged appearance when you are (effectively) doing yes/no comparisons against known constants. May I suggest instead that, rather than doing point sampling of your texture, try doing (at the very least) bilinear filtering which will hide the precision artefacts and also IMHO look better than having little squares. UPDATE It just occurred to me that one of the possible causes of inaccuracy is that you are doing increments for each pixel X step, e.g. these two 

For the moment we'll ignore 3 and 4, but come back to them later. Case 1 As you hint in your question, if the respective bounding boxes of the control points of A and B), don't intersect, then the curves can't intersect. Obviously this is a quick reject test but it's overly conservative. As you probably know, with a Bezier curve, the convex hull of its control points forms a (tighter) bound on the curve. We can thus use the separating axis technique to decide if the hulls of A and B don't intersect. (e.g. as shown in Wikipedia:) 

Placing a vertex at "infinity", e.g. for shadow volumes Representing directions rather than absolute positions Doing rational curves. e.g. $\frac{Quadratic Bezier()}{Quadratic Bezier()}$ to represent, say, conic sections. 

When generating your smaller mip map levels try to avoid using a simple 2x2 box filter because, though cheap and cheerful, they do a really poor job of removing high frequency information (that exceeds the Nyquist limit) as well as over filtering some of the lower frequency information you need to keep. (Also, as an aside, you need to perform the MIP map generation in linear space. If your source data is sRGB, you thus need to map to and from linear). FWIW, In Williams' original paper on MIP mapping, he said he used a "box (Fourier) window" to generate the prefiltered levels, which would be a sinc function in image space. Standard Trilinear filtering will also over- and under-filter parts of the texture. If you don't like the aliasing you may have to a) adjust the bias to increase the filtering (which will increase blurring) and/or b) try using anisotropic filtering. 

If your aim is just to fill the region and since you say you're using opengl, an alternative to Nathan's suggestion of using a triangulation algorithm++, is to use the stencil buffer. Assuming you want odd/even fill, clear the stencil buffer, dice up your polygon as before but have your triangles just (IIRC) invert the stencil. When all are sent, draw again but only where the stencil is non-zero. (It's been a while but I think you can clear the stencil buffer at the same time as this second pass to save time on the next complex polygon.) The stencil buffer approach should also work with self-intersecting polygons. One final thing, I think it is more fill-efficient if you use a triangle strip rather than a fan when you chop up your polygon. You just need to access your vertices in something like 0, 1, N-1, 2, N-2 etc. order More information on the stencil buffer can be found in OpenGL Stencil Tests and in Drawing Filled, Concave Polygons Using the Stencil Buffer ++ Though, if you do want to use a triangulation algorithm but have a very large number of vertices, you could try Seidel's method as it's 'relatively' easy to implement but has nearly O(n) time complexity. However if you do a search for Seidel, note that the code in Narkhede & Manocha is not actually Seidel's method as theirs is only O(n log n) not the faster O(n log* n) you should expect, where, if you're not familiar with it, $$log^*n =\begin{cases} 1+ log^*(log(n)) & \text{if $n$ > 1} \\ 0 & \text{otherwise} \end{cases}$$ In practical terms, you can consider it to be constant as it grows so slowly. 

If it's of interest, the Dreamcast had a "racing the beam" rendering mode, whereby it was able to dedicate a relatively small fraction of memory to framebuffer pixels (e.g. 64 scan lines) and it would render rows of 32 in turn, synchronised to the display update. This however, was only used to save memory. I doubt anyone was generating "modified" geometry for latter parts of the display. 

If you are in a hurry to get your renderer working and you already have the filled polygonal routine functioning correctly, can I suggest an alternative, possibly easier approach? Though I'm not familiar with Lua, it seems you are solving for the exact intersection of a scan line with the quadratic Bezier which, though admirable, is possibly overkill. Instead, tessellate your Beziers into line segments and then throw those into the polygon scan converter. I suggest just using (recursive) binary subdivision: i.e. the quadratic Bezier with control points, $(\overline {A} , \overline {B} , \overline {C})$ can be split into two Beziers, $(\overline {A} , \overline {D} , \overline {E})$ and $(\overline {E} , \overline {F} , \overline {C})$ where $$ \begin{align*} & \overline {D}=\dfrac {\overline {A}+\overline {B}} {2}\\ & \overline {E} =\dfrac {\overline {A}+2\overline {B}+\overline {C}}{4}\\ & \overline {F}=\dfrac {\overline {B}+\overline {C}} {2} \end{align*} $$ (which is also great if you only have fixed point maths). IIRC, each time you subdivide, the error between the Bezier and just a straight line segment joining the end points goes down by a factor of ~4x, so it doesn't take many subdivisions before the piecewise linear approximation will be indistinguishable from the true curve. You can also use the bounding box of the control points to decide if you can skip out of the subdivision process early since that will also be a conservative bound on the curve. 

Disclaimer: I haven't actually tested this but it seems feasible. I agree with trichoplax that what you want could possibly be expressed more clearly but, assuming I've understood correctly, would the following do the job? First I assume you can supply the opaque and translucent geometry separately, and that the opaque is sent first. I'm also going to assume there may be translucent geometry with exactly the same pixel Z values.(If this isn't the case, the stencil requirements could probably be dropped.) To handle the "single translucent layer", clear the stencil buffer and send the translucent polygons to update the Z-buffer and set the stencil buffer on each pixel, but don't touch the frame buffer. Submit the translucent data a second time (to actually update the framebuffer), but change the Z test to 'equal' and only allow drawing on the pixels that are currently set in the the stencil buffer. Also clear the stencil for any pixels that pass the tests. This, I think, should result in only the closest translucency layer being applied. 

Alan, Jim Blinn's compositing article also explains why premultiplied is the correct way to do any filtering of transparent images. 

"Go not to the CG Elves for counsel for they will say both no and yes". Yes and No. As noted in another post, these issues don't exist with ray tracing but I'll assume you are interested in standard rasterisation. 

I apologise for only skimming through your question/answer and the links you gave but I thought it would be a good idea to post what needs to be done to correctly do any blending and/or filtering on images with an alpha channel. The first thing to note is that all calculations should be done in a linear colour space, but the image data in PNGs and JPEGs is most likely to be in sRGB which is non-linear. You should convert this data to a linear form before doing any calculations. For example, the average of black (0,0,0) and white (255,255,255) in sRGB is not (127,127,127) but (off the top of my head) something like (186,186,186). Images, especially filtering of saturated colours can look quite odd if you don't take gamma into account. The second part of this theory dates back to Porter and Duff's seminal 1984 paper, "Compositing Digital Images", which introduces the concept of pre-multiplied alpha. Pre-multiplied alpha basically means that you multiply the RGBs by the alpha value before doing calculations. I get the feeling this was initially done just to reduce computation costs but in his article "Compositing-Theory", Jim Blinn shows that the correct way to do any filtering or compositing operations is to first convert everything to premultipled. Once you have linearised and then pre-multiplied your pixel data, do the filtering/blending and, finally, undo the pre-multiply and reapply the gamma function to map back to sRGB. 

Early History & Techniques It may surprise some to learn that texture compression has been around for over three decades. Flight simulators from the 70s and 80s needed access to relatively large amounts of texture data and given that 1MB of RAM in 1980 was > $6000, reducing the texture footprint was essential. As another example, in the mid 70s, even a small amount of high speed memory and logic, e.g. enough for a modest 512x512 RGB frame buffer) could set you back the price of small house. Though, AFAIK, not explicitly referred to as texture compression, in the literature and patents you can find references to techniques including: a. simple forms of mathematical/procedural texture synthesis, b. use of a single channel texture (e.g. 4bpp) that is then multiplied by a per-texture RGB value, c. YUV, and d. palettes (the literature suggesting use of Heckbert's approach to do the compression) Modelling Image Data As noted above, texture compression is nearly always lossy and thus the problem becomes one of trying to represent the important data in a compact way whilst disposing of the less significant information. The various schemes that will be described below all have an implicit 'parameterised' model that approximates the typical behaviour of texture data and of the eye's response. Further, since texture compression tends to use fixed-rate encoding, the compression process usually includes a search step to find the set of parameters which, when fed in to the model, will generate a good approximation of the original texture. That search step, however, can be time consuming. (With the possible exception of tools such as optipng, this is another area where typical use of PNG & JPEG differ from texture compression schemes) Before progressing further, to help with further understanding of TC it's worth taking a look at Principal Component Analysis (PCA) - a very useful mathematical tool for data compression. Example texture To compare the various methods, we'll use the following image: 

I don't think you'll get terribly far with, AFAICS, a single linear approximation to a rather non-linear interpolation, but perhaps this paper / presentation by Gribel et al on motion blur in rasterisation may help. 

These will soon cause errors to accumulate. Remove them and replace with multiplies by the X value and I expect things should improve. UPDATE2 It will also be affected by how far away from the origin of the texture you are when you sample it. If, for example, your source texture is defined over [0..1]x[0..1] and then repeats outside of that range, you might expect sampling at uv = (1000000.125, 100000.125) to be the same as (0.125,0.125) but, because the precision of FP decreases with increasing magnitude (i.e. the steps between floats also increases), you might end up with something quite different. 

Typically, despite the name, the Z-buffer contains an interpolated 1/W, 1/Z, or some linear function of one of these. - 

lhf's answer is good from the perspective of tessellation, but these can occur with simpler triangle mesh use cases. Take this trivial example of three, screen-space triangles, ABC, ADE and DBE... Although point E was, mathematically, intended to be exactly on the line segment AB, the pipeline won't be using fully precise values, such as rational numbers (e.g. $URL$ Instead, it will likely be using floats, and so some approximation/error will be introduced. The result is probably going to be something like: 

Note again that this is a relatively tough image for VQ schemes. VQ with larger vectors (e.g. 2bpp ARGB) Inspired by Beers et al, the Dreamcast console used VQ to encode 2x2 or even 2x4 pixel blocks with single bytes. While the "vectors" in the palette textures are 3 or 4 dimensional, the 2x2 pixel blocks can be considered to be 16 dimensional. The compression scheme assumes there is sufficient, approximate repetition of these vectors. Even though VQ can achieve satisfactory quality with ~2bpp, the problem with these schemes is that it requires dependent memory reads: An initial read from the index map to determine the code for the pixel is followed by a second to actually fetch the pixel data associated with that code. Additional caches can help alleviate some of the incurred latency, but adds complexity to the hardware. The example image compressed with the 2bpp Dreamcast scheme is . The index map is: Compression of VQ data can be done in various ways however, IIRC, the above was done using PCA to derive and then partition the 16D vectors along the principal vector into 2 sets such that two representative vectors minimised the mean squared error. The process then recursed until 256 candidate vectors were produced. A global k-means/Lloyd's algorithm approach was then applied to improve the representatives. Colour Space Transformations Colour space transformations also make use of PCA noting that the global distribution of colour is often spread along a major axis with far less spread along the other axes. For YUV representations, the assumptions are that a) the major axis is often in the luma direction and that b) the eye is more sensitive to changes in this direction. The 3dfx Voodoo system provided "YAB", an 8bpp, "Narrow Channel" compression system that split each 8 bit texel into a 322 format, and applied a user selected colour transform to that data to map it into RGB. The main axis thus had 8 levels and the smaller axes, 4 each. The S3 Virge chip had a slightly simpler, 4bpp, scheme that allowed the user to specify, for the entire texture, two end colours, which should lie on the principal axis, along with a 4bpp monochrome texture. The per-pixel value then blended the end colours with appropriate weights to produce the RGB result. BTC-based schemes Rewinding some number of years, Delp and Mitchell designed a simple (monochrome) image compression scheme called Block Truncation Coding, (BTC). This paper also included a compression algorithm but, for our purposes, we are mainly interested in the resulting compressed data and of the decompression process. In this scheme, images are broken into, typically, 4x4 pixel blocks, which can be compressed independently with, in effect, a localised VQ algorithm. Each block is represented by two "values", a and b, and a 4x4 set of index bits, which identify which of the two values to use for each pixel. S3TC: 4bpp RGB (+1bit alpha) Although several colour-variants of BTC for image compression were proposed, of interest to us is Iourcha et al's S3TC, some of which appears to be a rediscovery of the somewhat forgotten work of Hoffert et al that was used in Apple's Quicktime. The original S3TC, without the DirectX variants, compresses blocks of either RGB or RGB+1bit Alpha to 4bpp. Each 4x4 block in the texture is replaced by two end colours, A and B, from which up to two other colours are derived by fixed-weight, linear blends. Further, each texel in the block has a 2-bit index that determines how to select one of these four colours. For example the following is a 4x4 pixel section of the test image compressed with the AMD/ATI Compressenator tool. (Technically it's taken from a 512x512 version of the test image but forgive my lack of time to update the examples).