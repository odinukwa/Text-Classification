Use it where you need it, it's just another tool in the collection. You may find that connecting directly to the secondary is better for your use cases and scenarios that using ROR... or you may write your own health checks for the external load balancer (if it supports it) to only route to secondary servers that take into account locality of the user. 

This is a problem with how the password is being stored, not a problem with SQL Server TBH. However, you can overcome this by encrypting the password at rest and decrypting on demand then freeing the memory - for example. The password is not sent over the wire in plain text, this encryption when connecting to SQL Server through SQL Authentication is completed for you automatically. 

It may show the old collation, but it's been changed. Please note that changing the collation at the database level does not change it on any of the items underneath until it is specifically changed, however all new items will use that by default. If you don't trust me (I'm a random internet person!) and you have a test system you'll see that this data is cached. Change the collation, then restart the secondary replica. Now check the collation. I do not know, though, why the metadata cache is not refreshed. 

The scenario is called out and supported on the link you've provided. Availability Group with One Remote Secondary Replica If you have deployed an availability group only for disaster recovery, you may need to fail over the availability group to an asynchronous-commit secondary replica. Such configuration is illustrated by the following figure: Availability Group Upgrade in DR Scenario In this situation, you must fail over the availability group to the asynchronous-commit secondary replica during the rolling upgrade/update. To prevent data loss, change the commit mode to synchronous commit and wait for the secondary replica to be synchronized before you fail over the availability group. Therefore, the rolling upgrade/update process may look as follows: 1.Upgrade/update the remote server 2.Change the commit mode to synchronous commit 3.Wait until synchronization state is SYNCHRONIZED 4.Fail over the availability group to the remote site 5.Upgrade/update the local (primary site) server 6.Fail over the availability group to the primary site 7.Change the commit mode to asynchronous commit 

However, SA is just a login. The server roles of sysadmins gives full privileges over the system. The only way to secure a symmetric key using SQL Server against a sysadmins is to use multiple encryption techniques involving the users and/or the application. Note that always encrypted and HSMs use Asymmetric keys. 

Surely there must be, as to create a distributed availability group the secondary availability groups must be void of all databases. How to add a database to an existing distributed availability group: 

This insert should be extremely fast if it were only involving the in memory table. It, however, also involves a disk based table and is subject to all of the locking and blocking associated with that. Thus, the real time waste here is on the disk based table. When I did a quick test against 100,000 row insert from the disk based table after loading the data into memory - it was sub-second response times. However, most of your data is only kept for a very short amount of time, less than 20 seconds. This doesn't give it much time to really live in cache. Additionally I'm unsure how large really is and don't know if the values are being read off of disk or not. We have to rely on you for these answers. With the Select query: 

So what is the fix? Unfortunately it is very hard to say why it happened - and even if my assumption was correct, we'd need to know what error was being swallowed. Since we have neither of those, and just a wait type, I'm afraid the answer is "I don't know". Having said that, since it has to deal with Query Store (and if my assumption is correct) then turning off Query Store - without digging into the issue further - would be the only good route to go. Disclaimer: I work for Microsoft If you run into this again (or anyone for that matter) I would highly advise you open up a support case with Microsoft. 

During the time the secondary replicas are removed from the AG, continue to take log backups as normal. This will facilitate the reuse of the log so that it doesn't grow out of control. Keep these log backups handy and ready for action. Once the affected secondary replicas are no longer affected, copy all of the log backups taken while the secondary replicas were out of the AG and apply the log backups to those databases. When applying the log backups make sure to keep the databases in a restoring state by choosing on each log restore. Finally, suspend the log backups and restore any final ones that were taken while restoring the older ones. This will bring the databases on the previously removed secondary replicas to the same time frame as the primary and any other secondary replicas. Once the final log backup has been applied and the databases still left in a restoring state, add the replicas back into the AG. When this happens, since the databases are still in a restoring state and have been restored to the last log backup the AG will be able to join the replicas and databases without issue. There will be a short period of time where the replicas will need to catch up. Once the secondary replicas and databases are rejoined, resume log backups as normal. This would be the ideal process as it keeps your AG intact (for any unaffected secondary replicas), continues to leverage the listener for your applications, can still provide HA and to some extent DR depending upon the replicas available, continues to allow for log backups and re-use, stays transparent to the end user. 

Give the CNO Create computer objects, list properties, read properties, write properties over the OU it resides in. Create the listener through SSMS/TSQL/Powershell 

Distributed AGs don't hold databases, they hold availability groups. Availability Groups hold databases, that's where we need to go. 

When SQL Server cleanly shuts down, it flushes all of the buffers and asks internal systems to shut themselves down. So, how do you make this faster? First, this is only really the case when you are cleanly shutting down - which most likely won't happen during a real failure. Secondly, you don't let SQL Server shut down - I would look into using Availability Groups and testing your manual failover times to compare and contrast the differences. Additionally, no matter what you use, indirect checkpoints are much more effective than traditional. 

How do you have an "extra AOAG listener" just chilling out? I don't understand that part of your question. MSDTC is for distributed transactions, which aren't supported in 2012/2014 and only supported with certain restrictions (as of this moment) in 2016. Thus this is not required in a supported scenario. If you're going unsupported then this is still a moot point as the local MSDTC will be used. Unless I'm missing something (completely possible) or not understanding, this is not needed. 

You're correct. Changes in behavior between CUs (much like also were involved in SPs) can and do happen given the newer model (even in the SP + CU days this happened). I'd be interested to see if the configuration replica solves your issue since it was specifically added in CU1 for metadata safety as metadata issues for the replicas can and did happen since Read-Scale again wasn't made for HADR. 

No. you can make many, many mount points. In fact, you'll generally have an issue with your device interfaces before you hit any appreciable limit inside of Windows Server (Assuming you're not using a version of Windows Server that is over 17 years old...). 

This may not be an exhaustive list, but should give you an idea and the most critical areas to look at. 

There is a trace flag for each log item The trace flags are publically documented The trace flags don't differ through versions 

In this case, no they generally wouldn't be able to get to the decrypted data. There are, however, edge cases that won't stop someone with administrative access and a decent working knowledge of encryption from possibly getting the data. It's all about "reasonable protection" wherein you've given a reasonable enough protection to the data. If you wanted it to be extremely secure then you'd need to use an HSM, which another team owns and audits on an extremely frequent basis. No one would have administrative access to the server, and no one would have sysadmin access to SQL Server. That's probably not going to fly - so "reasonable" protection should be sufficed. 

You have much more control over the options used to create the self-signed certificate through Powershell. There is still the minimum options the certificate needs, but one example you've already mentioned is the valid date. Other than that, the way it is done doesn't really matter. It's the output (certificate) that's needed. 

AFAIK there is no way in TSQL to deal with this other than the way you currently are with XACT_ABORT. There are no structures that TSQL knows (or even cares about) as everything this deals with lives outside of TSQL. What is actually happening is called an "Attention" event which could range from a connection closing to an actual cancel (for example the sqlcommand.cancel() method). Since this lives outside of the actual query, it just tells SQL Server that something happened and that it needs to clean up some stuff. The actual query has no idea what is going on. Try/Catch is for query execution errors such as diving by 0, but not for any other type of errors. For example, compile/parse errors are not caught as the try/catch had never run. Since attention signals are not part of query execution (again, it's outside of that scope) it doesn't know anything about it and thus is not called as no execution exception has happened (according to the query itself). 

There isn't a one sized fits all answer, so assuming that the reporting workload doesn't run the secondary replica out of CPU/Memory/Disk/Network resources and there isn't any contention for said resources then the primary should not be impacted but log growth will occur, which in the end could impact the performance of the AG/SQL.