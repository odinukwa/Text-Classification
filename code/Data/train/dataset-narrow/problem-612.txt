I presume you have substantial data volumes if you have separate staging and data warehouse servers. From your posting It sounds like your staging server is doing a bit more than just staging the data. You describe merging data into the fact table on the warehouse server, so I presume that the staging server actually has ETL processing on it as well. I would not generally have recommended a transform-push approach like this; running ETL processing from the warehouse and pulling from a staging area is generally better. The merge happens on the warehouse. If you need to scale out you can replicate the warehouse, and the same operations will happen on the source and targets. However, I guess you're stuck with the transform-push architecture, so you've got a couple of options: 

Although Access has data bound controls, by and large you don't really want to use them for any sort of non-trivial application - partiuclarly if you want to do significant data validation before you save records. You can use ADO through VBA on Access, much as the same as through VB. The code to populate a form from a record set is pretty straightforward, and Access comes with a usable set of controls that can be used on a form. If you're using ADO, the life cycle is that you read the record by issuing a query to the database. Then you can populate the form from the recordset - this is straightforward to do with VBA. The 'save' button onclick method calls a validate and save procedure. You can add whatever other UI code you need to populate dropdowns, enable/disable controls etc. You may also want to separate data access into a separate class, depending on the complexity of the application. This lets you have a M:M relationship between formns and data items with the data access modularised into one place (i.e. one form can gather data from two or more queries by using the appropriate modules). Beyond this we're getting into the realms of application architecture, which is probably a bigger scope than you had in mind for this question. There are plenty of books on the subject, including ones specifically about application development with MS-Access. 

You could do this with an OLAP system - some of the benefits of SSAS for this type of application include: 

The export process will be quite expensive, though. On SQL Server, you could create a temporary table with the last names using a query along the lines of: 

Intuitively, if I was doing an OLAP solution for a retail chain, I'd say your infrastructure is really inappropriate for a system with substantial data volumes. This sort of kit has trouble with the data volumes you get in insurance, which is probably a couple of orders of magnitude smaller than I would expect to see in retail. As gbn states in the comments, SSRS is just a web application. You can set up a farm - start with one server with a few GB of RAM and a couple of virtual CPUs. Monitor it, and expand it if it's overloaded. The amount of disk space used by SSAS depends on the volume and the aggregations you put on the cubes. The storage format is quite compact - more so than SQL Server, but if you have large volumes of data it will start to get quite big. If it's getting into the 100+GB range then you should also look at partitioning. A surprisingly applicable generic solution Now, your client probably doesn't want to hear this, but VMs are not my recommended hardware configuration for any business intelligence solution. They work OK for transactional applications or anything that is not too computationally intensive. BI for a retail chain is probably a bit aggressive for this sort of infrastrucutre. As a generic 'starter for 10', My recommended configuration is a bare metal 2-4 socket server like a HP DL380 or DL580, and direct attach SAS storage. The principal reason for this is that a machine of this sort is by far the best bang for buck as a B.I. platform and has a relatively modest entry price. If you put a HBA on the machine then you can mount a LUN off the SAN for backups. IOPS for IOPS, this sort of kit is an order of magnitude cheaper than any SAN-based virutal solution, particularly on sequential workloads like B.I. The entry level for a setup of this configuration is peanuts - maybe Â£10-20,000 - and it's a lot cheaper and easier to get performance out of something like this than a VM based solution. For a first approximation, the only situation where this kit is inappropriate for B.I. work is when the data volumes get too large for it, and you need something like Teradata or Netezza. What can you do with your VMs, though? 

Unfortunately I don't have a running instance of PostgreSQL to hand, so this was on SQL Server, but the same basic structure should work on PostgreSQL. 

DB locks can exist on rows, pages or whole tables or indexes. When a transaction is in progress, the locks held by the transaction take up resources. Lock escalation is where the system consolidates multiple locks into a higher level one (for example consolidating multiple row locks to a page or multiple pages to a whole table) typically to recover resources taken up by large numbers of fine-grained locks. It will do this automatically, although you can set flags on the tables (see ALTER TABLE in the books on line) to control the policy for lock escalation on that particular table. In particular, premature or overly eager lock escalation used to be a problem on older versions of Sybase and SQL Server when you had two processes writing separate rows into the same page concurrently. If you go back far enough (IIRC SQL Server 6.5) SQL Server didn't actually have row locking but could only lock tables or pages. Where this happened, you could get contention between inserts of records in the same page; often you would put a clustered index on the table so new inserts went to different pages. 

This is not a technical problem, it's a contract management problem. Don't frig with the turnkey application. It gives the vendor a get-out-of-jail-free card that allows them to ignore their service level agreement. Either, 

Sparx Enterprise Architect is a UML tool with a database modelling function. It's not terribly expensive and pretty UML centric. It also has some pretty good extensibility features if you're that way inclined. 

Entity-Attribute-Value An entity-attribute-value structure has a 1:M relationship against the base table. The relationship has the attribute type (from a reference table) and the value against the parent entity's primary key. You have a base table, a reference table for the attribute names and types and a link table that records the value for each attribute against the entity. It is commonly used on software products. The view has to calculate a pivot or similar function to flatten the data out and can be generated from the attribute list for the entity type. 

The only significant reference to the notation I've ever read is the Elmasri and Navathe textbook - EER gets taught in database papers from time to time, but I don't think it's widely used in practice.. I'm not aware of any data modelling tools with support for the notation. 

Pros: Better database modelling and schema management than VS2010, better version control system, easier to customise the build and project work flow, better management of specs and documentation. Cons: More effort to integrate tools, limited DB integration into build process. Assumptions: Assumes that control of change/release process in more important than automated or tightly integrated release management for DB schemas. Also assumes that automated DB schema management is not 100% reliable. Not terribly high tech or slickly integrated, but for a complex project you're probably more interested in control than cute automated features. I'd argue that you're better off with a set of best of breed tools and whatever home brew build and test scripting is necessary to integrate them. Just try to keep the build (a) relatively simple to understand and (b) fully automated. 

If your query is returning a lot of rows across a slow WAN link then you might be seeing a slowdown due to network traffic. I have that class of problem where I'm working now. If you report server is on the same fast LAN as the database server it would be quicker under these circumstances. This might explain the difference in performance. Also, if you have virus scanning software running on your PC it could interfere with the local processing in a variety of ways - scanning the .data file etc. Often real-time scanning also hooks into memory allocation and scans data segments of applications. This really slows down managed applications. Those are a couple of possibilities that come to mind. There may be other causes as well. 

Then create one or more filegroups to allocate the partitions to. For a large data set, these file groups could be set up on different physical volumes. Note that direct attach storage will be much faster than a SAN for this in almost all cases. In the example below, we would have created 6 filegroups called PartVol1-PartVol6. One or more partition schemes can be created to allocate table partitions to filegroups based on the value of the partition function, e.g. 

Put the indicators on the customer dimension. This means that any fact table that joins against the customer dimension has access to all of the indicators. If they are on the dimension then you can trivially make them available to any fact table that links to the customer dimension. If you just need a fact table with counts of customers that roll up by the dimension attributes, then you can create a 'factless fact table' that just has a single fact - a 'QTY' column with a value of 1. This allows counts of customers to be grouped by any of the attributes. If Customer is a slowly changing dimension, consider putting an additional row with -1 in the QTY column into the fact table every time a type 2 change is made. This should link to the previous version of the dimension, with the current version having an additional row with a 'QTY' of 1. This allows you to track statistics on changes in customer attributes over time. A cube can consume this change over time for the counts by implementing a calculated measure that does a running sum in 'QTY' from beginning up to the selected date. If necessary you could also build a snapshot table. If the number of attributes and volume of changes becomes unwieldy then you have what Kimball calls a 'Rapidly Changing Monster Dimension' in his first book. In this case, consider pulling the attributes out into a separate junk dimension that has a row for each distinct combination of values. You will still want some scaffolding to link this to the actual dimension rows so you can copy the junk dimension key onto any fact tables you have the customer dimension key on. A cube can consume either structure, so it is less likely to be an issue for the cube so much as an issue for the ETL processing. 

Replicate your database onto another server and move the reporting sprocs onto it. Reports are run off the replicated server. This is the least effort and can re-use your existing reports and stored procedures. Build a Data Warehouse that consolidates the data from your production systems and transforms it into a form that is much friendlier for reporting. If you have a lot of ad-hoc statistical reporting that could be done acceptably from a snapshot as of 'close of business yesterday' a data warehouse might be the better approach. 

If you have lost the historical data (i.e. it gets overwritten rather than stored as a transaction history) then you can't reconstruct the historical state - full stop. However, there are a few approaches to dealing with this. 

SQL Server developer edition is pretty cheap (and comes with Visual Studio Pro and up anyway) and is intended that you can install it on a desktop PC. It's essentially the same software as SQL Server Enterprise edition, so it comes with all the tools, and even SSAS. I've done this on many occasions, even to the extent of building some big HP workstations with fast SCSI disks to develop ETL processes on. Absolutely you can install it on your laptop. 

This allows an application to do the update without holding locks open. This is necessary for n-tier systems working through a connection pool, and prevents a class of deadlocks that used to be common on two-tier client-server systems. There is nothing enforced in the database about this. It's all done explicitly by the application. 

This is configurable through SSMS. However if you have a 32 bit build and are not booting with the /3GB switch the figures shown look suspiciously like the hard limits that will be imposed by the operating system anyway. You can configure it from SSMS by right-clicking on the database server in the explorer, and you can see the memory usage policy under the 'memory' set of properties. If you have a 32 bit server you may be able to use AWE to get more than your 2GB or so, although older versions of Std edition Windows (2003R1 and earlier) still restrict you to fairly small memory sizes. If you have a more recent version of the operating system and a 64 bit build you should be able to use up to 32GB of memory. 

The point of zIIP processors is that you can't run z/OS code on them. Your COBOL code won't run on them. However, according to this article DB/2 for z/OS is an eligible workload to run on a zIIP processor. I presume your COBOL code has embedded SQL. Embedded SQL architectures are actually preprocessors that generate code that sends the query off to the database behind the scenes, so your SQL code is going to run on the DB server. If the COBOL programme is sending SQL to the DB/2 server via TCP/IP (DRDA) then it should run on the zIIP if the DB/2 server is configured to run on it. If the COBOL code is doing a lot of client-side processing or it's not using this then this will not be eligible. Chances are the COBOL app is not using DRDA, though. 

. . . So help me Codd In your example we can assume 1NF to begin with as the relational structure doesn't imply any repeating groups within the row (i.e. no D1, D2, D3 etc.). 

Scenario 2: M:M relationship between dimensions: Harder to think of a use case, but one could envisage something out of healthcare with ICD codes again. On a cost analysis system, the inpatient visit may become a dimension, and will have M:M relationships between the visit (or consultant-episode in NHS-speak) and the codings. In this case you can set up the M:M relationships, and possibly codify a human-readable rendering of them on the base dimension. The relationships can be done through straight M:M link tables or through a bridging 'combinations' table as before. This data structure can be queried correctly through Business Objects or better quality ROLAP tools. Off the top of my head, I can't see SSAS being able to consume this without taking the relationship right down to the fact table, so you would need to present a view of the M:M relationship between the coding and the fact table rows to use SSAS with this data. 

This is essentially a speed for space tradeoff, and temporal databases are really just abstractions on a data structure that is semantically equivalent to one of the schemes described above. If you want speed, calculate and persist periodic snapshots of your position. This will use more space, but if your DBMS platform supports partitioning then the snapshots can be managed and disposed quite efficiently. The snapshot date forms the partition key, so any queries that specify dates will give the optimiser enough information to ignore partitions that are not relevant to the query. Frequent snapshots will use lots of disk space, so you trade off granularity against disk space usage. If you want to save space then store changes and calculate running sums for your as-at positions. You can also use periodic (though less frequent) snapshots in combination with the deltas for a period to optimise the query by starting with balance figures from the appropriate snapshot and adding the deltas to your as-at date. This technique is commonly used with accounting systems such as Oracle Financials. Hybrid approaches Essentially you have a continuum from a pure running sum to a pure snapshot. Snapshotting is faster to query, and can be used to speed up running sum calculations by allowing you to start the running sum calculation at the snapshot date. A hybrid approach is more complex as you have to implement both mechanisms, but it does allow you to tune your speed/disk usage tradeoff to whatever is most appropriate to your application. Some thoughts on what might be applicable when It's far easier to retrofit snapshots to a base transactional model than to go the other way around. However, some models - things where you have time-dependent state - only make sense when viewed as a snapshot that shows the state at a given point in time. An example of this is aged debt reporting where you want to see the invoices with money that's been owing for more than (for example) 180 days. If you want to retain a reconcilable record of aged debt positions then you may wish to snapshot this periodically. In the first instance a transactional fact table with the base movements (deltas) gives you the raw data from which you can calculate your snapshotted positions. It's best to have that available if at all possible, unless recording the data in that format is made infeasible by some limitation of the upstream data source. I've done an aged debt reporting system and a claims reporting system based on snapshots before. The aged debt report was an operational report run at the beginning of each week, and the requirement for the claims snapshot table was for a monthly report where the business wished to see counts and outstanding values of claims that were open at the time. Some things (e.g. claim reserving) are done on a periodic basis, so (for example) ultimates and IBNR1 are only calculated on a monthly or quarterly basis and it only makes sense to report those on a periodic basis. In this case, snapshotting the figures makes sense due to the nature of the underlying process that generates the data. 1Ultimates are insurance-speak for forecasts of the total claims expected on policies incepting in a given year. IBNR (Incurred but not Reported) is an estimate of the figures on claims that have not occurred or been reported yet.