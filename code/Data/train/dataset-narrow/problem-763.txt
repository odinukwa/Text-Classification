I'm running a MySQL 5.5 server on my workstation for scientific data analysis and wonder how to configure MySQL in order to get the most out of it performance-wise. The types of query that I typically run involve joins of 10-20 tables and can run for quite long, one to several minutes being no exception at all. Only very few users access the database at the same time (5 being the maximum). I moved the server from a Lenovo Thinkpad T61 with a 2.2 GHz Dual Core and 4 GB of RAM to the following brand-new machine with hand-selected components: 

recently my PC powered off abruptly for no apparent reason, leaving my Windows 7 installation severly damaged. I can still log onto the machine, but tons of Windows system files have been corrupted and I have no other choice than performing a repair install of Windows. At the same time, my MySQL 5.5.31 instance running on this system has been damaged. I am not sure if it's just the MySQL installation that got damaged, or the data itself. What I can say is that while the serve was certainly running during the shutdown, it did not perform any tasks at that time. What happens now is that when I try to run mysqldump from MySQL Workbench, it aborts after a couple of tables saying that it has lost the connection to the server. Most tables are InnoDB, a few MyISAM. How can I repair the installation and recover my data? Should I uninstall and re-install? Appreciate your help. EDIT: These are the two errors I get from the error event log in MySQL Workbench: "InnoDB: Page checksum 3072680322, prior-to-4.0.14-form checksum 1925954319 InnoDB: stored checksum 24388168, prior-to-4.0.14-form stored checksum 1077079361 InnoDB: Page lsn 3225980244 611338381, low 4 bytes of lsn at page end 2304517124 InnoDB: Page number (if stored to page already) 2370756659, InnoDB: space id (if created with >= MySQL-4.1.1 and stored already) 4293596229" "InnoDB: Assertion failure in thread 3988 in file page0page.ic line 745 InnoDB: We intentionally generate a memory trap. InnoDB: Submit a detailed bug report to $URL$ InnoDB: If you get repeated assertion failures or crashes, even InnoDB: immediately after the mysqld startup, there may be InnoDB: corruption in the InnoDB tablespace. Please refer to InnoDB: $URL$ InnoDB: about forcing recovery." 

I need to rebuild some big indexes and I'm doing some tests with the various options (sort_in_tempdb, maxdop, online) of the statement on an test index with 4 levels and 800000 pages on leaf level. I noticed when I'm running the statement with the intermediate pages (level 1) of my index are higher fragmented as before (89% in stead of 3%). The intermediate pages only get defragmented when I'm setting . With the options the level 2 fragmentation jumps from 0 to 100. This are the statements that caused an increase of fragmentation on level 1: 

The second method is twice as fast as the first one. But the disadvantage is that after a few years the archive disk will contain a lof of folders/partitions containing the filestream data + database needs to be offline. As with the first method there is only 1 folder/partitions (or I can split that big partition by year). I hope my explanation is clear enough because it was hard to put all this in writing. What is the best method to accomplish the archiving or am I missing something? 

I've setup an AlwaysOn AG on SQL Server 2016 SP1 Standard. Then I created an AG and added a database with autoseeding (synchronous mode). I used SSMS 2017 to create my AG and to add the database. Everything works fine. But when I check the wait stats I get waits of type VDI_CLIENT_OTHER (80%) on the primary with an average resource time of 42 seconds. After some research I found out that the waits are generated by 4 sessions that execute the command VDI_CLIENT_WORKER. As I understand the wait means that a thread is waiting for work when seeding a new AG. But what I do not understand is why I have those waits because my AG is ready and why do I have 4 sessions that execute the VDI_CLIENT_WORKER command? I found out that each scheduler has one VDI_CLIENT_WORKER Can somebody try to explain what VDI_CLIENT_WORKER-command does and how can I solve the problem of the many VDI_CLIENT_OTHER waits? 

My issue here was that the accounts had an Active Directory policy that was set to expire the password at first log in. In other words, the user was meant to reset their password when they first use the account. Taking this off the password policy made the accounts work. 

Okay, everything is good and the system is put in to use. Now, two year after the system is in use, the client wants to compare not just with {, } but also with . So I change the structure of to: 

Neither of the above solutions are ideal. I was wondering if there is anything I can do at the query level to on 2 or 3 columns based on whether IS . 

The problem I now face is that all old data in doesn't successfully since they have a for as it was a newly added column. In other words, I have working queries but a disconnect in the data sets. 

I have an instance of Analysis Services set up with my cube and am attempting to access it through MS Excel. This works for one of my accounts but not for another. My goal is to get the second account to be able to access the cube just like the first. However, I am not sure what permissions I am missing for the second. I have added the second user as an administrator on Analysis services and even on the SQL Server instance and yet can't get past the below error. How does my second account need to be set up to access the cube? The server name looks like: $URL$ 

I'm trying to setup an AlwaysOn AG with T-SQL commands but my secondary is always in a disconnected state (when I failover, my old primary becomes disconnected). This is not an firewall/network issue, I'm able to send messages from SQL1 to SQL2 with a TCP-sender/receiver. The option New Availability Group... in SMSS also generates the same problem. However when I'm creating an AG with New Availability Group Wizard... it works perfect. This is the T-SQL statement I'm using: 

We have a SQL Server 2016 SP1 CU7 Enterprise server where we enabled the query store on a database. This is a highly used database.The has been set to 200MB and is 1 day. I've used to check the properties of the query store and everything works perfect as long as stays under the 200MB. I've set to Auto but no cleanup happens, normally a cleanup should be triggered when the query store is 90% full. What happens next is that the query store goes into readonly mode because there is no available space left. I've tried changing to Auto but nothings changes.If I then change to 30 days the changes to READ_WRITE and the query store starts capturing again. The just becomes higher than the and after a while the goes back to READ_ONLY. Can anybody explain this behavior? 

I'd like to know whether somebody would suggest changes to the above numbers or even further settings that I do not know of. I'd appreciate any helpful remark. Steve EDIT: I have two queries involving joins across 10-20 tables and ran them on my Lenovo notebook and the new PC. Query #1 took 3m36s on the new machine vs 9m11s on the laptop; Query #2 took 22.5s on the workstation vs 48.5s on the laptop. So the execution speed was improved by roughly the factor 2-2.5. On the workstation, not even 50% of the RAM was used. The average CPU load across the four cores (as reported by Windows Task Manager) was only about 13%. The load on a per-core basis (as reported by Core Temp) was about 25-40% for ONE core, while it was <=10% for the others, indicating that MySQL does not make use of multiple cores for a single query. 

First tests (running the same query on both machines) showed a definitive improvement in speed for the new one, but the queries still take a lot of time and I had expected more of a boost. The queries in question are fairly well optimized, i.e. all tables have proper key that are also being used as of "explain extended". Now to my current MySQL settings: First I should mention that I moved from MyISAM to Innodb long time ago. Some of my my.ini tweaks (i.e. departures from default settings): 

I'm a newbie regarding the query store and have some problems understanding what I'm seeing. We have a third-party application (running on SQL Server 2016 Enterprise SP1 CU7) that uses the query-hints and . When I do some monitoring in the querystore I see that some queries has multiple plan ids in the plan summary window. Why can a query that uses and have multiple plans (sometimes completely different, sometimes the same)? And the second question, if there are plans that look exactly the same (same physical operators, same set options, same queryhash), how can one plan have a missing index and the second plan not? This are two anonymised plans: Plan 1 Plan 2 

A while ago a consultant said that I should set processor affinity when I'm running SQL Server on VMWare. The advice was to disable CPU0 so it was free for the OS. When I read the "Architecting Microsoft SQL Server on VMware vSphere"-PDF on the site of VMWare I find following: 

Is fragmentation on intermediate pages something to worry about and what is causing the increase in fragmentation? 

On the secondary dm_hadr_database_replica_states is empty and the database doesn't show up in sys.databases. When I look in SSMS the database is shown in 'Availability Databases' Is this a bug in SQL Server 2016 SP1 Standard CU2 or is there something wrong with my T-SQL statement?