Or you can use sql sentry's plan explorer tool and see the tab that will list the and for actual execution plan. If you cannot turn on the actual plan then you can look into plan cache as described below. 

This is true up-to certain extent only. Its a best practice, but depending on how much data you are replicating and how busy (in terms of activity or transactions) is your publication database and the network latency between publisher-distributor-subscriber, this setup will affect you. In your scenario, if your database is -- 

In both cases, you will have data loss (provided you have good full and T-log backups before that guy pushed green button instead of blue). I would go for option 1 => Restore from a know latest good backup ! Refer to IAM pages, IAM chains, and allocation units and IAM page corruption examples 

Backup compression uses CPU cycles to compress the data before it leaves the server, and that’s why in the vast majority of scenarios, compressed backups are faster than uncompressed backups. Note that when you use Open source tools you need to uncompress the database backup file before you can start the restore process it self. e.g: When you receive a SQL database backup of 50 Gb which is compressed to 5 GB. To restore this database, you need much more diskspace: 

Obviously, Enterprise edition can take advantage of Parallel execution of DBCC statements, but look out for MAXDOP setting as it might end up taking all your CPU. This can be hard limited by Resource Governor. Note: If you are having SPARSE column, then your CHECKDB will be dead slow as described here. Finally, its how to prevent database corruption by utilizing all available tool set + your faith in your database server hardware system and most importantly the value of your data. Some excellent references : 

From, Amazon RDS FAQs, the "transaction log capture" will allow you to do a point-in-time recovery of your database. 

ANSI_NULL_DEFAULT { ON | OFF } : When you dont specify in or statements, this database option will determine the default value, or for a column or CLR user-defined type. When SET to , the DEFAULT value is . When SET to , the DEFAULT value is . 

Apart from the fact that you cant change the recovery option for tempdb, You dont need a loop for what you are doing : Run in SSMS by pressing CTRL+T 

This will eventually depend on the frequency of log reader agent. If you set it to run continuously then you will see your changes immediately on the subscribers. The latency depends on the amount of activity at publisher (server 1), network latency, distribution database - if it is on the same server as publisher or is on its own dedicated server. There are many factors to consider when you choose replication as Disaster recovery (a stand by server when your primary is down). As a side note: What you are looking is a good HA - case when server 1 is down .. do we have a standby that can serve the application ? .. something along that line. You should consider moving to a more recent version - sql server 2014 and look into using AlwaysON Availability groups. 

Note: Test below script upfront in your environment. I have assumed that xp_cmdshell is enabled. If not, it needs to be enabled before you run this script. Edit: As per Aaron's suggestion, you can think of doing backup restore as a good and safe option (the script can be changed to do that). 

turn off CPU affinity first and then bind the ports to see if it works. This works Fine, and I am able to see NUMA 0 listening on one port and NUMA 1 listening on second port. This is what I want, but I dont want to turn off CPU Affinity. Am I in the right direction ? CPU Affinity ON. Try NUMA assigning to ports. This does not work in the sense that Instance1 is bind to NUMA 0 and Instance 2 does not show in error log that it is listening on NUMA 1. 

is a deprecated. You should use . If you change your server name after installing sql server then you have to make the entry in which can be done by and 

Oh Boy! System Databases on C Drive are recipe for disaster ! Change the default database location away from C:\ drive. You can do it using 

The answers above suggest to use query hint or switch. Since you know a specific query that you are running and you dont want to cache the plan (we will come to missing index DMV detail later), you can use 

Logshipping is a death tested technique that has been around since many ages. Looking at your specific error sequence on secondary server 

Install and configure fresh SQL Server standard 2008R2 instance (same configuration as like your 2008R2 ). Run below script replacing the @destserver with the NEW Server Name on the Old Enterprise Edition instance. 

The basic concept is to use GRANT/DENY Schema Permissions. You can efficiently manage permissions by creating a role and then adding members to it. Below is an example that will explain you in detail 

This way, you have a very less downtime and the risk of deployment failure on a live system (which is in maintenance window, since you are doing upgrade) will be highly minimized. Also, following the Blue-Green approach, you will be oscillating between LIVE and PREVIOUS version which will be staging for the next version. Again, this will require more hardware/licensing as well as planning and testing. Most of the steps can be automated using DACPACs and PowerShell. Also, if you are installing multiple instances on one server, make sure to re-balance the Memory settings when switching between Blue and Green. The LIVE environment gets more memory than the Passive environment. In my current environment, we have implemented Blue/Green Model for Agile Code Deployment that allows us to promote code every 2 weeks with ample amount of time for testing and business sign-off. Also, its a breeze to rollback in case something goes horribly wrong. We have automated majority of the deployment stuff using Dacpacs and PowerShell. 

UPDATE: I see the problem now ... that auto_close option is turned ON for report server databases. According to BOL: 

The algorithm for a REBUILD vs REORG is different. A REORG will NOT allocate new extents as opposed to a REBUILD. A REORG will work with currently allocated pages (allocates one 8Kb random page so that it can move the pages around) and moves them around and then deallocate the pages if needed. From my SQLSkills internals (formerly IE0) notes .... For REBUILD : 

Note : Below solution can be automated using tsql or Powershell as described here You can restore the databases with the MOVE and REPLACE option if the folder paths are different and to overwrite the existing ones: 

No this does not make sense, unless you have selected it to delete destination data when creating SSIS package using Import/Export Wizard. You must have selected : Delete rows in an existing destination table. - See step 3 in BOL. 

As per Installation documentation the correct version of each available plugin is always provided in the SQuirreL SQL Client install jar. 

The DMV column will tell you Depending on what SP + CU you are running, you may want to look into - KB 3012182 FIX: Log_Send_Rate column cannot reflect the rate accurately in SQL Server 2012 or in SQL Server 2014 Below PERFMON counter will tell you 

is at the transaction level or at the session level while is a query hint. You mentioned that you fully understand dirty reads, so using at the transaction level is what I would recommend. if you want dirty reads on some tables only, then NOLOCK hint will help you. SQL server 2005 and up allows you to use SNAPSHOT isolation where readers don't block writers and it uses row versioning with some penalty on tempdb. essentially SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED and NOLOCK are same thing but the use depends on the context of what you are using. 

Perform backup and restore of PROD database to dev environments Once above is completed, kick off the scrubbing script to purge old data. 

1) Use while using the command. 2) the older database which is conflicting and restore again using command. There is no problem with the SQL Server version. As Aaron pointed out, I am also able to restore the database from 2008 to 2012 and same versions as well. 

I would suggest that they are equally important on physical or VMs. suggest you to create all critical alerts as highlighted by Glenn Berry. 

Use Import/Export wizard and instead of exporting the entire table, choose sql to export specific rows and columns. You can write sql like : or if you don't care about specific data then you can use top command to extract any number of rows that you require. Edit : Below are screenshots for Import export Wizard (Launch it from SSMS): 

As a side note: Do refer to below post from Paul Randall about sizing tempdb : On all later versions, including SQL Server 2012, that recommendation persists, but because of some optimizations (see my blog post) you usually do not need one-to-one – you may be fine with the number of tempdb data files equal to 1/4 to 1/2 the number of logical processor cores – which is what everyone apart from the official Microsoft guidance recommends. 

Since you want to move the database to a different disk, it should not be that big of a downtime. Below are the steps that you should follow : 

Best is to load the data into a staging database on premise and then incrementally load the data to the main database. You can use : SQL Database Migration Wizard found on codeplex. Even though it is for Azure to onpremise sql server and vice versa, it will work for Amazon RDS as well. Other alternative is to use SSIS as mentioned by @sqlreader. 

Taking into consideration what @MikeWalsh and @KookieMonster, I would suggest you to monitor your autogrowth events overtime and then go with a number that suits your environment - Applies to both Data and Log files (By getting a number through monitoring and keeping a 10% buffer, it will help you minimize the number of autogrowth events that kicks in due to poor autogrowth settings). Auto-growth events are expensive operations that slow down the performance of your database. Always, pre-Size your databases and establish appropriate autogrowth settings for your databases based on their growth profile. One more time to stress, enable Instant File Initialization so Data files can leverage its awesomeness. Below is how you can check the autogrowth events using the default Trace (which is running by default - unless you have turned it off. You can adjust the script by changing ) 

You cannot restore a database that is part of AG. You have to remove it from AG and then restore it and add it back. As I said above, corruption wont trigger failover. 

Note: You need to transaction log backups available or the transaction should not have been cleared from the active portion of the log Excellent reading at : Using fn_dblog, fn_dump_dblog, and restoring with STOPBEFOREMARK to an LSN and Joining sys.dm_tran_database_transactions to fn_dblog Results 

Backup operations do not take locks on user objects. ... see my answer on Backup internals - What happens when a backup job is running - in terms of locking and performance overhead in SQL Server? 

This is not correct. You just have to create SERVER LEVEL / DATABASE LEVEL TRIGGER that will take care of the database events that occur on the server instance or database. You can even filter out the databases that you need to audit for schema change programatically if you are using server level trigger. Refer to : DDL Event Groups Code Project has a working example - Send Email Alert on Database Schema Change in SQL Server Be careful as this can generate a lot of data as the trigger will fire for every alter, create, drop events. My preferred way of doing this is using Event Notification using - creating a notification on an individual databases. 

All the above steps you can script out and automate. You can refer to the script that I wrote to rename the database - both logical and physical and adapt as per your needs. 

SO I guess, we just have to follow what Microsoft says - SQL server will manage them for us :-) ONLY FOR EDUCATIONAL PURPOSE : I managed to clean up the old files by 

For the sake of completeness, I would add using method : I will use the same fiddle written by Leigh Riffel. 

I would suggest you to suspend data movement prior to the move (and resume it once the secondary server is up). This will have a side-effect of primary database accumulating the unsent transaction log records in the send queue. This means that for your maintenance time, transaction log on primary database cannot be truncated. Remember that primary database is exposed - if it goes down, there will be data loss. Make sure you have a good tested full backup + T-log backups. 

It can be the case where the index from your perspective is not useful, but there might be queries that require that index or part of that Index. I agree that caching such a huge index and if it is really not useful - is a waste of Buffer Pool and you might run into performance issues like more I/O's or excessive plan recompilations if the plan cache is too constrained. 

Transaction log is your ldf file. Writes to the transaction log are sequential in nature and benefit by being isolated on to separate physical devices. Read up on : Diagnosing Transaction Log Performance Issues and Limits of the Log Manager and Optimizing Transaction Log Throughput 

You can use TSQL script task in SSIS and use (if you only have SD and JK). Note: There may be better ways for complex scenarios, but in this scenario, using TSQL does the job. 

So next time you insert into TableName, the identity value inserted will be 1. When you delete rows from the table, it will not reset the Identity value, but it will keep increasing it. Just like what happened in your case. Now when you truncate the table, it will reset the Identity value to its original Seed value of the table. Refer to : SQL SERVER – DELETE, TRUNCATE and RESEED Identity for a detailed example and some good explanation of Difference between Truncate and Delete 

drop any foreign keys in other tables that references this index. drop the index change the column datatype rebuild the index Put back any foreign keys that you dropped in step 1. 

The above commmand will ask your confirmation if you want to really evict the node or not -- unless you use use switch. 

From above results we can see that the we cannot directly compare values on columns with different collations, you have to use to compare the column values. TEST 2 : The major difference is performance, as Erland Sommarskog points out at this discussion on msdn. 

I believe you are writing your own scripts to detect fragmentation and address it based on some criteria. Remember - there is a cost to reinvent the wheel (courtesy of Aaron Bertrand) I would suggest you to look into - a more intelligent way of addressing your index fragmentation issues. (hint: SQL Server Index and Statistics Maintenance by Ola hallengren) The benefit of using Ola's script is that it has logging functionality [logs to table] which over time will become a gold mine and you can get real value out of it. e.g. 

Make sure you have error logs set to manageable size. Check MSDN : Configure SQL Server Error Logs and kb 2199578 for managing SQL Server error log. EDIT: Below is what we do in our PROD environments: 

Another cool powershell utility that I use everyday for server migrations is Start-SqlServerMigration.ps1 written by Chrissy LeMaire It is very adaptive with different options. Eg. In your scenario for migrating jobs with schedule, you can use 

The memory allocations for AWE in 32bit use the AllocateUserPhysicalPages() API. The memory allocations for lock pages in 64bit use the same API, but they aren't using AWE because the VAS in a 64-bit process is 8TB for user mode space and it doesn't require special mappings through AWE. Queries timing out has least to do with AWE. There might be missing indexes, outdated stats, poorly configured memory settings and many other stuff that can lead to queries timing out. What specific version of sql server are you running (along with patch level) ?