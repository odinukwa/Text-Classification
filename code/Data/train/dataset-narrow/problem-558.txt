Their cannot be a comparison drawn as Snapshot is "photo copy" of data file. The difference lies in amount of activity both does and consistency of product arising out of both the operations. Full backup "Is Always" more reliable than snapshot backup Full backup includes all committed and uncommitted transaction when full backup has finished. Read more about this in Understanding SQL Server Backups full backup also includes few amount of transaction log, if required, this is not the case with snapshot backups. Backing of transaction log is required so that recovery can run successfully when backup is restored. Read about Database snapshot A database snapshot provides a read-only, static view of a source database as it existed at snapshot creation, minus any uncommitted transactions. Uncommitted transactions are rolled back in a newly created database snapshot because the Database Engine runs recovery after the snapshot has been created (transactions in the database are not affected). 

If percentage is > 60 you can keep that index If percentage is between 50 and 60 you yourself need to figure out whether this index is helpful or not. If it is less than 50 the index is causing more of I/O utilization in updates than it is helping in making query faster. If it is around 20-30% you can first disabled and then removed. 

Consider this as answer SP3 is new and thoroughly tested its highly unlikely it would create any issue but again I would recommend you not to proceed without any testing and that too when Cluster is involved. Also asking whether anybody faced issue will lead you in problem, what if somebody writes 'Yes I have done and its working fine' yes they are correct it worked in there environment but I am sure your and his environment are not same and after applying SP if something stops working who would you catch other than Microsoft. I have applied SP3 in my environment and every thing is going good as of now. But if you ask my plan of action I have now deployed in DEV then I will deploy in UAT and then after a month, because couple of SSIS packages run monthly, I would apply in production. Please create an environment and test first believe me it will save you lot of hassle. If still you want to proceed 1.. Backup system and user databases. Use TSQL backup command [This link is external to TechNet Wiki. It will open in a new window.] or SQL Server Management studio GUI [This link is external to TechNet Wiki. It will open in a new window.] to backup system and user databases. Since you backup system and user databases you don't need to backup jobs, SSIS packages, mail profiles, linked servers and logins as all of these would be included in MSDB and Master database backups. 

This error message is for clustered index (index ID 1), the corruption is in clustered index which basically means the table. Running repair is most likely to give you data loss. Cause: Corruption doesnt comes out of blue, the most likely cause is bad hardware or may be bad memory stick. Your task is to find out from SQL Server errorlog and windows event viewer what is that. Unless you find it and fix it your issue is not solved. Advise: As I see here restoring from good backup is the most preferred option. Hope you have one 

What you are seeing is called binaries of SQL Server and as per rule binaries always goes on system drive and you have no control over it. If you read SQL Server file location of default and named instance you can get various default locations. 

Restore verifyonly(as per BOL)Verifies the backup but does not restore it, and checks to see that the backup set is complete and the entire backup is readable. However, RESTORE VERIFYONLY does not attempt to verify the structure of the data contained in the backup volumes. So even if backup set comes as verified clean by Verifyonly its not 100% guaranteed that backup set is consistent only a successful restore of a backup can guarantee that backup set is valid. I have not see an scenario where verifyonly fails but restores succeeds(unless you succeed with using continue_after_error) it can only happen when verifyonly failed saying there is not enough space to check backup consistency because verifyonly also checks that enough space is there to restore the database Checks performed by RESTORE VERIFYONLY include: •That the backup set is complete and all volumes are readable. •Some header fields of database pages, such as the page ID (as if it were about to write the data). •Checksum (if present on the media). •Checking for sufficient space on destination devices. Please show me the message which you got when verifyonly failed After Verifyonly fails I would run a checkdb on my database to check for any inconsistency but point here to note is checkdb does not do consistency check for log file only when recovery on snapshot is run during checkdb log file is used so we 'can' say that it checks for log file as well but not complete like it does for data file. If checkdb comes out clean I would say my database is consistent. 

You can use standard edition in SQL server database mirroring but you cannot use Snapshot functionality to get report as Mirror server is always restoring and you need to create database snapshot on mirror to run reports but snapshot is enterprise only feature. If you use transaction log shipping with Secondary database in standby mode you can run reports although when log backups will be restored users will be disconnected. 

Starting from 2008 onwards Index rebuild is fully logged in full recovery model. If you rebuild huge index in full recovery model its bound to produce too much logs. So in this table you have how many indexes? Are you rebuilding all such indexes in one go. If you are doing this you must consider doing it piece meal. One index at a time Of course you can switch to bulk logged recovery model when rebuilding index but you would loose point in time recovery for period the index rebuild operation is going. After index rebuild is done and if you still continue with bulk logged recovery model and some transactions are done which are not defined as minimally logged in BL recovery model they would be fully logged Consider below before switching to Bulk Logged Recovery Model 

You should call a Microsoft licensing expert and talk to them. They would be best person to resolve all your queries. Microsoft licensing is bit comlex and varies per environment. 

Recovery model controls the logging and there by how much data you can recover. Only certain transactions which would be minimally logged would be minimally logged all other transactions would be fully logged in bulk logged recovery model. So it depends on how much data you can loose keeping that in mind choose recovery model. You should read Backup under BL recovery model. If you are performing a bulk logged operation and dont really care about point in time recovery during the period the BL operation runs go ahead change recovery model to bulk logged. But read considerations before changing to BL recovery Model Risk. If you perform operation that are minimally logged then you loose point in time recovery for the time period this transaction is running. After this transaction finishes and other transaction starts which is fully logged you can perform point in time recovery. You must read Understanding Logging and recovery in SQL Server 

This is because some tool or windows server native backup feature is taking a Snapshot backup of SQL Server. Such backups normally use and to connect to perform Snapshot backup. Please note since you said there is no third party backup scheduled I have guessed here that it might be windows native backup. If you want to find more about such backup start the profiler and capture the events when backup is running. Or you can schedule the profiler. I must say profiler can create load so be aware about that so be selective in events when configuring the profiler. Workaround: If you are very much certain that you don't need this backup, which IMO is useless as compared to native TSQL backup, you can . You can go to Services.msc and you would find this service there and then disable it. 

There are basically two types of splits good page split or sequential page split which occurs when leaf level of index page is full and when new record is inserted page split occurs which results in allocation of new page and data is written in sequential order because new page is added at last of all pages. The other one is bad page split or non sequential page split which occurs due to insert or update operation on a page resulting in page split in between. What happens is when record is updated a space for new record is created and this might splits page and move some of record on new page now this new page would be just after page that was updated hence severely creating a mismatch in order by which Logical clustering keys are arranged and how data pages are arranged. In fact depending on how fast new page can be allocated for page split it can also cause performance issue. Fragmentation exists when indexes have pages in which the logical ordering, based on the key value, does not match the physical ordering inside the data file. This article aptly demonstrates how page split can occur. You can also read this article by Paul about how to see bad page splits. Both of these use undocumented commands if you have SQL Server 2012 and above you can use Extended events trace to track nasty page splits One more drawback associated is when huge number of page splits occur they could saturate I/O by utilizing it. Consider a scenario where update causes page split now one I/O would be required for update to index key one for update to page(normally) it would also require additional I/O for new page addition and updating index after new page is created, on a busy system with not so upto date hardware this could cause performance issue There is other way using DMV to find number of page splits happening in database but it wont tell you which one is good and which one is bad 

No this is a temporary solution. I guess you posted same question before could you please tell what is total size of database you have in your SQL Server instance. Size of tempdb depends on how much your queries are using it. It cannot grow by its own unless you use it. Links shared by Aron would help but you need to tune queries if they are heavily using temdbb or may be its the default requirement of your environment. I have seen few env. where 200 G of tempdb was acceptable because queries required that much amount of tempdb space. 

No not the data flow as such. You can run cluster validation of all the resources but storage with no downtime. Although few of MS blogs suggest you can run all cluster validation test but storage on WSFC without downtime, still I would suggest you to run the validation when load is relatively very less. Please note that when you run validation of shared disks cluster manually failover the disks to each node to check the failover and availability and hence it will cause downtime apart from this all other failover tests are online. I assume your AG is one having local disks attached to each node so I do not think you have to worry about downtime but again run validation when load is relatively very less. I would suggest you to read Validating a Cluster with Zero Downtime. Quoting from the blog 

Other blunder you are doing is deleting the files after backup, how are you going to recover database if disaster strikes. Please stop that as well. Please define proper RPO and RTO and make a backup retention policy after which you may delete the files which exceeds the retention date. And NO taking log backup and deleting the files do not break log chain but you actually lost the ability to do point in time recovery as files which stored backup is deleted Further to our conversation you asked whether taking manual backup would break the log chain and answer it yes. Even though you take and delete the files the changes are done to the LSN when you take manual backup. 

If table on other hand has records you would get tabular output. For more understanding I suggest you read following blogs from Paul Randal 

Let me answer your question point wise 1.SQL Server express has SQL server agent code built in but disabled. When you go to sp_configure you can see Agent XP enable/disable option this is so because if any point of time you are planning to upgrade SQL Server Express Microsoft upgrade process does not have to do much change but just a minimum to enable SQL server agent functionality. So before upgrading to Standard it wants you to use proper service account as a start up for SQL server agent so that after upgrade when database engine tries to bring agent services up it can do it eaisly. By default NT Authority/System account has admin privileges so it was able to bring Agent services up. Also please make sure that you always change SQL Server/Agent service account using SQL Server configuration manager it preserves the ACL's 2.As a good security practice SQL server should not run with Built in account instead a domain account should be created and granted least privileges to run it. Below Microsoft web resource will tell you what rights are required. The article is bit lengthy but beautifully written $URL$ 

You dont need to to give db_datareader and db_datawriter permission if you have given db_owner permission. As per BOL 

SQL Server 2005 SP2 is not supported at all. As a fact SQL Server 2005 even with SP4 does not falls under category of normal support however it does falls under category of extended support. As per MSDN Blog We would like to remind all customers that Mainstream Support for SQL Server 2005 Service Pack 3 and SQL Server 2005 Service Pack 4 will end on April 12, 2011, and Service Pack Support for SQL Server 2008 Service Pack 1 will end on October 11, 2011. Microsoft is ending support for these products as part of our Support Lifecycle policy, found in $URL$ Please read MSDN blog for more details PS: Even if you have Microsoft Premier support and you are at SQL Server 2005 SP2 and raise a case with Microsoft for help they would first ask you to apply SP4 and ask you to see if issue subsides. If not they would take (After applying SP4) your case under extended support If you want to know difference between mainstream and extended support This MSDN Blog would give you more details 

Yes you are correct taking multiple backup does not fixes corruption in log backup but taking multiple log backups does not break log chain. Log files are inked internally with LSN number and taking multiple log backup does not breaks chain if log backups are restored in sequential manner 

Stop looking at Buffer Cache hit ratio to determine memory pressure. This is because with read ahead mechanism in SQL Server more than enough pages are alredy in buffer pool to satisfy the query so BCHR does not gives accurate figure about memory pressure. You might even see that BCHR even would not drop when there is memory pressure. All these has been explained in Great SQL Server Debate about BCHR PLE output you posted seems really low but we cannot just use one counter to gauge memory pressure. PLE is more indication of I/O activity on server. It could be possible that due to heavy I/O activity PLE plummeted. If you note Target and Total server memory still remains same. Which is good sign. For . You can use below counters 

The answer is you cannot shrink log file if there is no free space available. Shrinking can occur only while the database is online and, also, while at least one virtual log file is free. In some cases, shrinking the log may not be possible until after the next log truncation. Can you run and see what is last value in status column. If value is 2 you cannot shrink if value is 0 you can because 0 means VLF is truncated and no active transaction is using it. You can read how shrinking of Transaction Log works A log file can only be shrunk to a virtual log file boundary, shrinking a log file to a size smaller than the size of a virtual log file might not be possible, even if it is not being used. The size of the virtual log file is chosen dynamically by the Database Engine when log files are created or extended. It wont give you failure message but wont even shrink. I also think there is no free space in log file which can be freed and you are just forcing it to free it. What is the query you are using to shrink database log file can you please show it. You must read This BOL article on shrinking a log file. What is output of below query