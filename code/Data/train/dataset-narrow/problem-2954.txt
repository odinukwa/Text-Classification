Version control isn't that challenging to learn (assuming you know how to use the command line). It's something you can trivially understand by following a few tutorials online -- I'd argue the hardest part is actually finding the right tutorial (there are tons of garbage "how to use Git" tutorials out there, and surprisingly few good ones, for example). Despite that, it can be hard to find time to fit in the command line and git. You'll probably need to spend at least 30 minutes talking about both things, and if you have a lot of material you need to cover, it can be hard to squeeze that in without compromising on something else. In particular, it's worth noting that you can't necessarily just add in new material somewhere and shift everything over. You need to make sure your change maintains the overall tempo of the course, make sure no topic "overlaps" oddly over a weekend or a holiday, need to make sure students are still being taught the material they need a sufficient amount of time before you make related HW due... If the original course was competently designed, it's probably a well-tuned engine where every minute matters, so adjusting it will require some degree of effort. If your university didn't find time to squeeze in version control, you'll be told to learn it on at least day 1 of your first internship or job. (And they'll probably do a better job of teaching it! It's hard to teach the value of things/give students the opportunity to practice things like branching, rebasing (if you're using git), using specific workflows, etc. in a university setting -- those sorts of more advanced operations are really only useful in larger teams working on a long-term project.) You mentioned that your intro course covered git on day 1. I can see an instructor considering against that approach if they want to prioritize moving to programming as soon as possible to try and "hook" the students in with instant gratification. (If you're completing new to programming, I can imagine learning to use tools like version control would be relatively dull and sort of a bother -- you don't really get to see the payoff until you start tackling more complex projects later on.) 

Tell the students they have seven decimal digits of precision in a float, and that the decimal point is placed "somewhere" within those seven digits, depending on the magnitude of the number. Ask them to add 0.01 to 9,999,999 and express the result to seven digits. Ask what the answer would be if they performed the addition a thousand times. Now ask them to do the same calculation using 15 digits of precision. 

In a previous question, I asked about the appropriate notation when teaching Boolean logic to students ages about 11â€“14. I selected the notation of engineering and computer science in part because I believe (without any real research) that's what the students are most likely to encounter later. I'm still not sure how to explain + as OR other than by saying that the same symbol is used for different, but similar, things. Is there a better way to explain that? Aside: I am going to use center-dot for AND because most of these students have not had an algebra course and implicit multiplication confuses them. They won't encounter the inner product concept until college. 

A really good question as arguably MOOCs these days are preferred/easily accessible over conventional textbooks in CS. I had the experience of supplementing my curriculum with MOOCs in 2015 and since then, in every course, there are some supplements from them. Here are my observations about this experience: Clear Advantages 

Now I would talk on general terms, @Buffy has a good idea of giving them projects. I like it but I would like to add little bit more: A complete/successful project doesn't always guarantee that a student has covered all the basics of even OOP (assuming he develops project in an OO language/paradigm) or SQL, etc. (I am coming from a Pakistani background where OOP and SQL still have lion's share in market, this suggestion may vary depending upon the industry you have) I think if I were to give any student a suggestion, it would be to firstly cover OOP and SQL properly and then complement it by project I agree that now SW industry has much more technologies and not everyone gets away with such a luck, but my experience still tells me that OOP/basic algorithm development and SQL are almost necessary for lot of jobs. So my conclusion is, both OOP, SQL, etc. (Theory) and Project (Practical) are necessary for you even if you are Final year student and water has reached near your throat. P.S: @ctrl-alt-delor's answer has a detailed analysis of technologies road-maps and I would like to refer to it for choosing a specific tool(s). 

So, for that reason, I recommend taking a more "organic" approach. Let students write code, and push them to write it in the simplest way possible. The code initially is probably going to be messy, but that's fine -- if your FRC team is anything like mine, there's probably a fair bit of prototyping in the first week or two (or even up to the last week...). As you see redundancy, point it out, and push students to refactor it. Try and emphasize this sort of procedural refactoring first. Next, encourage students to create objects to encapsulate behavior and state. Somebody introduces a new subsystem? Create an object for it. Somebody wants to play with a new UI? Another object. Don't worry about having these objects conform to any kind of interface/extend any object -- getting them to think about subsystems as distinct units/distinct objects would be a good step. Finally, near the endgame, as the design as settled and people are iterating, start introducing the idea of interfaces. The overall design of your subsystems are probably fixed at this point, and the team is likely focused on iterating. In that case, you can probably now create an interface per each subsystem and create a new subclass per each distinct variation or something. That way, if the team wants to trial-run one particular variation, it's easy to swap out the corresponding code. You can maybe facilitate this whole process by having code reviews, perhaps on a weekly basis or something. This lets students still retain ownership of the code while still giving you a mechanism to propose changes or give advice. This does mean, however, that your advice will (at least initially) be reactive, rather then proactive. However, I don't think that's necessarily a bad thing -- it's good for the students to have ownership of their code, make mistakes, learn from them, etc... And if the team never really does get to the point where using interfaces and such makes sense, don't bother bringing it up. Realistically, the code they're writing is likely going to be simple, in the grand scheme of things. Don't over-complicate it. Edit: A little more specifically, I'm fond of the "rule of three" (link 1, link 2). I would try and drill in this principle into your students: when you see something repeated three or more times, that's a good point to think about refactoring and abstracting. Basically, my opinion is that the hard part is learning when it's appropriate to refactor or abstract. If you can teach them to identify when to abstract, it's comparatively easier to teach them how. 

I got a chance to teach Operating Systems to students of category 2 and needless to mention how badly they failed at comprehending basic system programs written in C++ (not even C). It took me more than 3 lectures to explain them only the syntax of since they had no background of pointers, function pointers and even passing by reference and how could they given they begun programming with Java. Recently, I taught Introduction to Biological Computing and used Python in the course and that experience prompted me to think a bit about why not use it in PF for CS students, but No! This experiment, thankfully I stopped it before implementation. Obviously doing Python after C is very easy, but its not other way around. No wonder that as per TIOBE indexing, C language has made a whooping 6.62% growth in last 12 months. 1 

So my personal suggestion is to purchase Paper books for important topics (e.g. some de facto books like Elmasri, Gonzalez, Silberchatz, etc.) and use E-books for rest/unavailable in physical format or ones available freely. 

The second phase is actually getting feedback on the code you wrote. This can be tricky to do as a self-learner, since you have no direct way of getting feedback, but there are a few things you can try. One resource to try is CodeReview.SE, though it's probably best to submit relatively small programs for review. Another idea is to contribute to an open-source project. The more popular projects tend to be relatively picky about following best practices and idioms and will code review your submissions before accepting them. As before, start small: try and find a small and easy bug in their issue tracker, and start with that. It can also be valuable to try googling "open source for beginners" -- that reveals plenty of resources and websites that attempt to connect projects looking for help with beginners. 

Details: Changes in Java 9 Here is the document describing the full list of new features in Java 9: $URL$ If you skim through, the majority of the features should have no real impact for both the average Java programmer and for students -- they add additional options for tweaking the compilation and deployment process, and some API refinements. There are, however, a few features worth talking about in more detail: 

I am a college teacher developing an introduction to digital logic for grades 8-12. This is a special topic that will be presented in three 50-minute periods with in-class exercises. It won't go beyond basic combinational logic. Provisionally, module 1 starts with gates and truth tables and ends with a half-adder; module 2 extends to a four-bit ripple-carry adder, and module 3 extends the ripple-carry adder to make a two's complement adder/subtractor. These are not set in stone, but will be rather soon. I am struggling with how to explain the importance of a knowledge of digital logic to high school and upper middle school students. I want something better than, "To help you understand that computers are not magic." Ideally, the explanation would be only a short paragraph, and memorable. So, how can I explain the importance of comprehension of simple digital logic to pre-college students? That is, how do I explain why students should care about this topic? Edit: Well, I'm teaching this to eighth-graders beginning Wednesday! Day one is about what I planned last fall. Day 2 has a ripple-carry adder and something else. I'm still dithering between adder-subtractor and S-R latch which becomes a D-latch. (I'm not sure the kids have enough grounding in binary arithmetic for the adder-subtractor.) Day 3 now introduces a little bit of Boolean algebra. The kids are introduced to Kat, who is heading for Hogwarts, and must design Kat's cat checker. There are five inputs and three possible "OK" results from among the combinations. For the explanation that everything computers do comes from a few simple circuits, as suggested by Ben I. below, I'm going to show this video at the beginning of day 1: $URL$ 

But what if you want to implement something like a video game? Mutation is an integral part of programs like those. Of course, there are ways to restructure your code to avoid mutation, but most existing literature, tools, libraries, game engines, etc... related to gamedev don't take such an approach. Or what if you want to go into embedded systems or security and end up needing to mess around with a lot of assembly or C? For better or for worse, you're going to end up spending a lot of time working with lower-level languages: functional languages won't necessarily end up being relevant to your work. Or what if you're interested in scientific and mathematical computing? Again, I don't think functional languages are really widely-used here: languages like Python, R, Julia, C, or maybe even Matlab and Fortran are more ubiquitous for a variety of reasons. Basically, a CS curriculum is supposed to prepare students to excel in a wide variety of backgrounds, not one specific area. For that reason, I don't know if it's necessarily correct to assume that all (or even most!) students will move into working on enterprise software and the like. In particular, I think many "intro to CS" courses are targeted to be useful for people who may not necessarily want to move into computer science but still want to learn coding as a skill. In that case, you'd probably want to teach them a more mainstream language. (You could also split up your "intro to CS" course into a "intro for non-majors" and "intro for majors", but that would introduce some overhead, both for students and teachers.) 

Save the Trees Easy/Quick Transportation Multiple copies enable you to comment out etc. Searching, etc. is very easy. Very helpful in finding those books who are unavailable/unheard of/banned in a given jurisdiction. A course/science may have lot of books and one can't purchase all of them (E-books are relatively inexpensive), for example, there are heaps of books on OOP and we can't designate any single one as the Only one, so it would be better to designate one as Textbook (physical form) and use others in E-books form. 

Since the OP asked about functional programming, so I would highly recommend you to read "Learning Concurrent Programming in Scala" by Aleksandar Prokopec, (2014). All the examples for this book are available on GitHub to give you some idea of the book before you purchase it. C# I would recommend you to read "Professional Parallel Programming with C#" - its based on .Net 4.0 and I have no update about following editions (if any) C++ I think Pacheco's book would be my recommendation for sure Best of luck! 

I would maybe flip the order in which students approach the problem -- rather then going from harder to easier, start from easier to harder. That is, start by having your students solve a simpler version of the problem for partial points, then guide them steadily towards a more and more generalized solution until they have a finished product. This would probably be easier for them to grasp, for starters, since it mirrors how the material itself was presented. Going from simple to complex would also align incentives correctly -- currently, your assignment is set up such that students get the impression that they are in effect punished for admitting they don't understand and need extra assistance. Naturally then, they're going to want to avoid that hit, either by refusing to read ahead, or by cheating. In this particular case, I would also take the allegations of cheating seriously. If your students are intelligent, it would be pretty easy for them to read ahead, take the gist of your solution, and re-write it in their own style to avoid accusations of plagiarism. It costs them very little to do so (even if they don't carefully follow your hints, just skimming them would be useful), and there's relatively low risk they'll be caught. Of course, there's the honor code, but not all students will follow it (and the students who do will feel punished for being honest) -- IMO it's better to just set the assignment up so cheating is difficult to begin with. In any case, a few more ideas for lowering the difficulty curve without compromising on material: