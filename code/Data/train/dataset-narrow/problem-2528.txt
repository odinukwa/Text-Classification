Personally I still use RenderMonkey. However it's not entirely suitable for this exact question because it's only a facade of a real app and therefore has some odd limitations. It's well out of support but I've just not found anything better for debugging GLSL. 

In an FPS context, fixed-point values might actually be a liability. Close to zero floating-point is more accurate. It is only over large distances fixed-point becomes more preferable. The answer is simply that its dependent on context. In something like a galaxy you can use frames of reference. Use a huge scale for the solar systems and then use the center of the Sun (or similar point) as the point of origin for anything inside the system. Using this system you can have your cake and eat it, so to speak, and its not difficult to envision. IIRC, the dev on Infinity stated that he was continuously iterating around scale issues in one of his interviews. 

From what I can gather from the paper, the code is correct. The current solution is deferred, reconstructing position using the depth stencil texture and sampling normals from a RGBA32F texture (trying to eliminate precision issues). A directional light also seems to produce some level of banding, although not as visible as with the area light, which can be seen in the following images. 

specPower equals 9 in this image Is this as simple as an HDR/LDR issue or is it somehow related to normal precision? I believe I have encountered this issue before in a deferred renderer where normals were of low precision and improperly packed/unpacked but in this case normals are generated on the fly and the whole thing is rendered directly to the backbuffer. Update 1 I would like to add to the above that triangles suffer the same artifact and they have their normal currently generated in the following way: 

I'm working on a front-to-back renderer for a 2D engine using an orthographic projection. I want to use the depth buffer to avoid overdraw. I have a 16-bit depth buffer, a camera at Z=100 looking at Z=0, zNear is 1, and zFar is 1000. Each sprite rendered sets its Z co-ordinates to increasingly distant values, allowing depth test to skip rendering anything which is underneath. However I'm aware the way Z positions end up with Z buffer values is non-linear. I want to make use of the full resolution of the 16-bit depth buffer, i.e. allowing 65536 unique values. So for every sprite rendered, I want to increment the Z position to the next position to correlate to the next unique depth buffer value. In other words I want to turn an incrementing index (0, 1, 2, 3...) of the sprite being drawn in to the appropriate Z position for each sprite to have a unique depth buffer value. I'm not sure of the maths behind this. What is the calculation to do this? Note I'm working in WebGL (basically OpenGL ES 2), and I need to support a wide range of hardware, so while extensions like gl_FragDepth might make this easier, I can't use it for compatibility reasons. 

We're having some issues with our ray tracing in DirectX, especially with some serious banding issues with specular. With high specular power (above 8) banding starts. I'm wondering if this is an HDR/LDR issue or could be related to something else, such as normals or other vectors? UPDATE Look below for updates. Here's the relevant shader code for Blinn-Phong on a sphere: 

I'm trying to hook lua-scripts to my entities, where several entities of the same type want to use a separate instance of the same script. Problem is, when I run two or more scripts and use any C-api functionality, I get an access violation after some time. I'm guessing some kind of stack corruption occurs but I'm not sure. The application is single-threaded but uses lua_newthread for each entity, and I then use lua_resume on each new thread for when I want the scripts to run. The code looks like this. Script 1 

If it's so straight forward, why can't you solve it? No need to be rude. Anyway, the first hit on Google is the spec that tells you exactly how you can perform the conversion: $URL$ Alternatively, a Google for tools that will do this brings up plenty of hits. Straight-forward indeed. 

Facebook and Twitter provide cross-system APIs accessible in a variety of ways, typically a simple HTTP request (or a few). 

Always, always spend that little bit extra on consulting a lawyer. Build up a good relationship with your lawyer if you can. I've never taken anyone to court but it's amazing how often running a tiny studio throws minor legal issues your way. How to handle trademark infringements, publishing contracts, hiring contractors... people will screw you if you give them the chance when there's money involved. 

I've got multiple buffers in OpenGL holding data on position, normals and texcoords. I also have an equal number of buffers holding distinct index data for each of those buffers. I quite like this format (indvidual indexes for each buffer) utilised by COLLADA since it strikes me as optimally efficient at accessing each buffer. I've set up pointers to the relevant data arrays using VertexPointer, NormalPointer, etc however I have no way to assign pointers to the index buffers since DrawElements appear to only look at one ElementArrayBuffer. Can I utilise multiple indices some way or will I be better off using a different technique which can support this? I'd prefer to keep the distinct indices if at all possible. 

I have a 2D game engine that draws tilemaps by drawing tiles from a tileset image. Because by default OpenGL can only wrap the entire texture (), and not just part of it, each tile is split off in to a separate texture. Then regions of the same tile are rendered adjacent to each other. Here's what it looks like when it's working as intended: 

I have two object-aligned bounding boxes (i.e. not axis aligned, they rotate with the object). I'd like to know if two object-aligned boxes overlap. (Edit: note - I'm using an axis-aligned bounding box test to quickly discard distant objects, so it doesn't matter if the quad routine is a little slower.) My boxes are stored as four x,y points. I've searched around for answers, but I can't make sense of the variable names and algorithms in examples to apply them to my particular case. Can someone help show me how this would be done, in a clear and simple way? Thanks. (The particular language isn't important, C-style pseudo code is OK.) 

The paper mentions some level of banding but it should not be on the level which I am currently getting, which can be seen in motion in this video: $URL$ . These images show the illuminance without shadows or anything. 

So maybe it's still just a precision issue? The code for handling my depth and position reconstruction looks like this: 

Looking at the problem further, it just seems to be an issue in darker areas. Adding just a low level of ambient light, such as in the attached image, resolves the issue. Maybe someone else can elaborate on the cause of the problem? I consider the issue resolved for the moment, as I always have some level of ambient lighting present, but further explanation would be much appreciated. 

I'd say this makes it even more unlikely that the normal of the surface is the problem. The following image shows what happens when specPower reaches 2048. 

If you're using C++0x, use . It has no performance overhead, unlike which has reference counting overhead. A unique_ptr owns its pointer, and you can transfer ownership around with C++0x's move semantics. You can't copy them - only move them. It can also be used in containers, e.g. , which is binary-compatible and identical in performance to , but will not leak memory if you erase elements or clear the vector. This also has better compatibility with STL algorithms than . IMO for a lot of purposes this is an ideal container: random access, exception safe, prevents memory leaks, low overhead for vector reallocation (just shuffles around pointers behind the scenes). Very useful for many purposes. 

Why does this happen? I thought it was due to linear filtering blending the borders of the quads, but it still happens with point filtering. The only solution I've found so far is to ensure all positioning and scaling only happens at integer values, and use point filtering. This can degrade the visual quality of the game (particularly that sub-pixel positioning no longer works so motion is not so smooth). Things I have tried/considered: 

I'm currently working on an implementation of rectangular area lights but I am having some issues with the illuminance calculation, which gives me serious colour banding across the entire lit area. I'm using the paper published by DICE, "Moving Frostbite to Physically Based Rendering" ($URL$ as my base for implementation (Listing 12). This is the relevant code used: Solid angle of the rectangle 

If I remove the call to GetPosition from Script 1, the application runs without issue. Am I missing some kind of required clean-up or garbage collection problems? I'd like to try and avoid creating a new state for each entity, as that seems excessive and sharing globals via the core state is nice. lua_pcall doesn't produce any results. The application eventually stops running Script 1 and after a few more cycles it gets the access violation on 

Which to me looks correct, but I might be missing something. Any other ideas or suggestions? Maybe a matter of tonemapping/HDR problems?