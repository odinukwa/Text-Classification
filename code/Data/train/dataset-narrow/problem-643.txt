The decision of whether to merge these tables or not should depend on whether the separated tables provide useful information. Use the rules of database normalization. What are the functional dependencies? What are the candidate keys? Season_League doesn't seem to have any information in it other than which leagues exist in which seasons (pure intersection) League_Division just adds division to this pure intersection. Division_Schedule doesn't seem to add any information at all. One question I would have is whether it is useful to know that a league exists in a season. If you combine the tables, then what you have is a division exists in a league in a season. There is no "independent life" of a league except that it has a division. There are other subtleties too, like whether a division can exist in more than one league within a given season. If you combine the tables into one then this type of business rule could not be enforced with declarative referential integrity. Does this matter to you? It depends on what other manual or automated controls you're willing to put in place to manage this risk. Without any more information about your intended business rules, I would say these three tables should definitely be combined. 

It isn't too hard to create table-driven access rules if you can base the rules on something that is in the data already, like a country code. You want to beware the power of multiplication. Don't try to create access rules based on the cross-product of two or three or more factors. The management of such a rule set will quickly become impractical due to the shear amount of data. Instead, keep each access rule separate (i.e. country, department) and treat each as a gate that the user has to get through before they get access. This will be a bit of work when setting up your data access code, but it is the best way to operate the system in the long term. 

If you wanted to enforce this business rule through application logic instead of through declarative constraints in the database, you could use the first schema noted above instead. 

You can use a correlated subquery to solve this problem. Consider the following SQL Fiddle (which includes a lot of assumptions about your table and data): Working SQL Fiddle With this table definition and sample data: 

I would think the thing to do is to store the property or properties that are used to calculate the rank and then build an index over them. Rather than trying to force the database to physically store the data in ranked order or using a manually managed linked list, why not let the database engine do what it was designed to do? 

EDIT: Something Else to Consider: Something else you should consider is that you should NOT try to model (and maintain data for) things which aren't important to your system. That is just making work for yourself now and down the road. Your system will be more complicated to build, maintain, upgrade and use. Consider the following ERD: 

The way you've modelled it is fine. Your model ensures that the business rules are enforced by the database. There are a couple of things you could do as an alternative. One would be to eliminate the surrogate key on . As an intersection table, you could use the two foreign keys together as a composite primary key. If you did this, that would propagate and down to your table. This would have the advantage of giving you more of a "one stop shop" for your application permission data (i.e. fewer joins) without compromising your normalization in any way. Another way you could go would be to simplify slightly and define a default permission for each application. You could call it whatever you like, such as "default access" or "basic access" or "user" or whatever makes sense to you. This would allow you to flatten your model and essentially drop the table and join straight to . This would change the nature of the query that you would use to ask "which roles can access which applications?" but your business rules would still be enforced by the schema. 

This design assumes the simplification suggested under Issue 1, above. One potential drawback of this simplistic solution is that there is no constraint against assigning a teacher to teach a lesson for which they are unqualified. This constraint, if it is important, could be accommodated by replacing the and the tables with new tables as follows: 

If you still have their attention by this point, you could try to show them the power of binary searching. For this you might be better finding a YouTube clip of the game show "Price is Right" and their "high/low game" which smart players play using binary searching. 

If you are running a video rental store, then whether or not your children eat depends very much on a video appearing in as many reservations as possible. In your example, is a weak entity which relies on both the FK to and the FK to in addition to a further attribute, perhaps , to comprise its primary key. It is not possible for a M:N relationship to be supporting for a weak entity (although the intersection table which resolves a M:N relationship can be a weak entity). This is because a supporting relationship needs to be to a particular entity object. A set of entity objects can't identify something in relational algebra. It is possible for a 1:1 relationship to be supporting for a weak entity, although this would be rare "in the wild". I could imagine it coming up in the context of situations that would typically be 1:M, but where there is a business rule that restricts the child to one at a time. 

Assuming that you are prepared to compromise on the absolute correctness of the answer in order to obtain practical performance, you could do the following: 

If you can tell us more about how you plan to use the events, from the perspective of how the information is meant to be helpful, I might be able to give you more concrete advice. 

The best first strategy is always to keep your transactional data all together in one, normalized data store. This minimizes the risk of data integrity problems and keeps the amount of code you have to write to a minimum. Often, people will find that there is some kind of concern around performance. This leads them to consider denormalization. The best thing is to wait until your volume testing proves that there is a real problem with performance, rather than assuming that there will be one because your table is "big". Given that you have a large amount of historical data that you don't use most of the time, it is possible that you may benefit from denormalization. You really should test this out before assuming it to be the case though. If it is the case, then one approach which might work well for you is to keep two data stores: (a) one where your current values are kept and (b) one where all values, including current and historical are kept. If this makes sense to do (for performance reasons), you should build some automation to track all changes to the current data and record them in your history table. This is best done with triggers in DBMS that support them. What almost never makes sense is keeping current values in one place and historical values (excluding current values) in another place. I have never come across a situation where you need to query only historical records, excluding current values. This means that your queries would have to be twice as complex in order to use current + historical values if you segregate them in this way. This is what I would recommend to you: 

Splitting given names from surnames is not normalisation. What would be normalisation would be if you created a series of tables that contained additional details about some of the columns in your table. For example, you could create a table that contains information about the house other than the name, such as it's catch phrase or home town etc. You might do something similar with and . Don't worry about nulls in your case. Some people are concerned about too many nulls because it can be a sign that your design is not correct or because they are worried about wasting disk space. In your case neither of these concerns is germane. 

No native RDBMS date data type is going to do for applications that require very old (and for some, even distant future) dates. If I were you, I'd use a string type for the native storage and stick with a place-significant format like: +YYYY-MM-DD to accomodate BC/AD and any foreseeable historical or reasonable future date. If it might help, you could build a library class that converts your internal storage format into a more presentable one for the UI layer. You might even include library functions that convert to a native date type, if your language of choice supports the dates that you will have in your database. 

There are a few potential advantages of using an intermediary staging database, which may or may not apply to your situation. There is no perfect, one-size fits all solution. Some of the potential advantages include: 

The first thing you want to do to keep your thinking straight is to choose some clear terminology. You are using the word "assessment" in two different contexts: (i) a Test and (ii) the results for a student taking the test. I understand that "test" may not be the best term, since you might be assessing based on lots of different kinds of work. The important thing is that you need to clearly differentiate between a measuring instrument and the results of an individual student against that measuring instrument. Keeping these concepts separate will help you to keep your thinking straight when designing your database. I'd recommend that all of your RESULTS be kept in a single table. That will make things like report cards easier to handle. Whether the result is a percentage, a letter grade or a pass/fail, you probably want to have a common representation (percentage is a good lowest common denominator) and a weight to help you aggregate marks across assessments. 

Your data model may be high level (entities/relationships only) or lower level (add in attributes with details of data types) and it may be logical or physical. It all depends on what stage of your design process you're at. The point of a data model is to record your database design, which is just an expression of your business requirements for persisting data. Crows foot and ERD etc. are just graphical conventions for showing your data model pictorially. The great thing about standards is that there's so many to choose from, and this is also true of graphical conventions. Read this wiki article for a sampling. Which conventions you choose may depend on what you need to communicate about your model, or it might just depend on what conventions make sense to you. Don't get too wound up about all the different terminology. It's all just your database design and how you choose to record and communicate it. 

Entity-Relationship diagrams show entities and relationships, including the cardinality of the relationships. Some people and tools expand this to show attributes, primary keys and foreign keys. I've even seen some attempts at visually representing some contraints like attribute nullability and basic data domains. Some of the classic graphical conventions, like IDEF1X also include information about the nature of the relationships (whether they are determinants of the children or not). I'm not aware of any graphical conventions for showing access rules in an ER diagram. I would argue that trying to pack too much information into a visual representation of the schema will make the diagram so difficult to understand that it becomes unhelpful. I would say that this applies to something like access rules. In order to show access rules, you really need to show the accessors (i.e. the users) since different accessors will generally have different access rules. There is no established convention for including users in an ER diagram so I wouldn't try to do it that way. As an alternative, I would suggest using tables or charts to indicate which users have access to which entities. You could include a column that shows data-level rules that expand on the table-level rules. A data level rule could be something like "users can only access sub categories which belong to categories that they own". It won't result in a pretty picture, but I believe that understandable documentation will beat a pretty picture when it comes to trying to explain your design to a developer or customer. 

You should never assume that a data point which is outside of the control of your system will never change. This means you shouldn't assume student names won't change. There are lots of reasons in the real world why names might change. Anything that is at reasonable risk of changing is a bad candidate for a primary key. Also, names are very unlikely to be unique over a student population of any reasonable size. Some exceptions to this might be things which are controlled by an external standards body which can be reasonably well trusted to maintain consistency. This could include something the the IATA code for an airport or the symbol for an atomic element. Regarding the efficiency of textual (natural) keys vs. integer (surrogate) keys, there is no easy answer to this as it depends on many factors. On balance, it is fair to say that surrogate integer keys are more efficient than natural textual keys - especially if your textual values are much larger than a few characters. Nevertheless, there are advantages to natural keys beyond raw file I/O and CPU cycles, as long as you can trust the natural keys to be stable. A school's student ID is probably a pretty good candidate because you can institute an internal policy that says student IDs are granted for life and never change. 

There are some things you want to think about based on the nature of your items and how you store and track them. Are your items discrete or are they a commodity? The way you would track television sets, each of which has its own distinct serial number is different from how you might track boxes of nails. If your SKUs are discrete you don't have a "quantity" field, instead you track individual items with an intersection table (as suggested by Joe). Otherwise whether you need an intersection table depends on how many different SKUs can be on a shelf (see below). Is it important to track inventory movements? Do you need to see stock deliveries and shipments? If so you might want to take a kind of double entry accounting approach, treating shelves like GL numbers. If not, a simpler quantity per shelf may be sufficient. Can multiple SKUs share a shelf? If so, you need an intersection table. If not, you can get by with a foreign key from to . Does every bin have at least one shelf? Your application (queries) will be much simpler if every bin has at least one shelf, such that you don't need to track inventory which is in a bin but has no applicable shelf information. 

Notice that this is different from your normalized table picture in two important respects: First, states and districts should be separated into two tables so you don't repeat the state names. Second, and more importantly, prices depend on district and product. Therefore the price should be in the intersection table between district and product, not in the same table as states and districts. 

Ask yourself what is the difference between a and a ? Are they just organizational objects that may contain either employees or other, smaller organizational objects? Do they have any other differences in terms of their relationships and attributes? If the answer is no, then you might want to consider replacing both with a single organization table containing an unleveled hierarchy. This can be represented with an involuted (self-referencing) foreign key (i.e. or etc.) Of course, in a relational database, unleveled hierarchies can be awkward to work with. If you decide to go this route, you should do some research on hierarchical data management techniques, like visitation numbers or adjacency lists. If you really need to keep and separate because they have markedly different definitions, then you will need to consider implementing a dummy or null for each to handle the situation where there is no analogue in reality. If you go this route, you could consider adding a flag or some other indicator to differentiate between real and pro-forma .