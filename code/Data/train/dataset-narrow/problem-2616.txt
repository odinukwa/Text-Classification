On 3D graphics hardware, the edges of adjacent polygons are only guaranteed to align 100% if their vertices are 100% the same. So if you generate the vertices for each cube individually, and you use an algoritm that introduces tiny floating point errors... *boom* there could be tiny cracks all over the place at any time. Make sure to re-use vertices where possible, or at least re-use their floating point position. Are the textures used for each cube subtextures? In that case, try setting the AddresU/AddressV to . Are the textures used for each cube subtextures? Do the u,v coordinates match the exact corners of the subtexture without any additional padding? In that case, mipmapping is biting you. Especially the further-away tiles are sampled from the 2x downscaled mipmap, so your neatly fitting u,v coordinates now include part of the pixels from the adjacent area's (i.e. bleeding). As a quick test, assign a new sampler state and set 

$URL$ has a nice approach (at least I think so, I'm using it) to both 1. hierarchical storage of a world map comprised of smaller regional maps, and 2. sparse storage of upper floors/basements/... so you're not storing megabytes and megabytes of "empty air" or "solid rock". It's designed to work with procedural generation, and personally I'm aiming to do the procedural gen of new regions in a second thread to cut down on stalling the main thread, but there's no reason you couldn't use it for storing a hand-generated world to disk. 

Note that I've replaced the variables in the code above with constants, but allocated the D3D11_SUBRESOURCE_DATA array dynamically because its size is unknown at compile time. Why would you ever have different slices with different pitches? Is that even really supported? That seems really strange. 

With full source (or even just a thorough explanation) of a perspective correct textured triangle rasterizer being too long for an answer, I'll gladly refer you to Chris Hecker's classic series of articles on the topic, including source: $URL$ From your link I assume you are using Flash as a platform, so I sure hope you know how to efficiently turn on/off individual pixels there. 

I don't generally recommend Bourg's Physics for Game Programmers, but he talks about this a bit in Chapter 10 (around page 171), and might give you a starting point. Unfortunately, the vehicle code in PhysX is still 'sample' and not well documented, so you can't easily figure out how that works. I believe I've seen code derived from their sample display the kind of behavior you're looking for in 3D, but it's a lower-level simulation than I think you want. 

What you want to look for are techniques for doing Constructive Solid Geometry, CSG for short. CSG isn't particularly hard (although numerical border cases might cause some trouble), but doing it in realtime isn't trivial. Sander van Rossen writes interesting stuff about the datastructures he used for his particular implementation. His approach might be more approriate for animated character models than the standard BSP representation often used in non-realtime CSG. A solid open implementation of rendering CSG objects is OpenCSG. 

Google Euler angle conversion for more info. To get a direction vector from a pitch and a yaw (pitch is what you have now and yaw is around the vertical axis) you want this: 

From what you describe I suspect that it has something to do with your timer. I think the timer is calculating the delta time from the last time you pressed a key and not from and display update. It's pretty simple keeping your own delta time (in a display or update function): 

Any physics library like ODE will work. Most, if not all, physics libraries will offer a way to divide objects into different 'subspaces', where only objects in the same subspace will collide. How exactly you handle things will mostly depend on how you do all the other 'large world handling'. Let's put this differently: unless you want to do some extreme world-spanning physics effects, physics are going to be the least of your troubles in doing a big game world. 

The DoBoxesIntersect above is a good pairwise solution. However, if you have a lot of boxes, you still have an O(N^2) problem, and you might find you need to do something on top of that like what Kaj refers to. (In the 3D collision detection literature, this is known as having both a broad-phase and a narrow-phase algorithm. We'll do something really fast to find all possible pairs of overlaps, and then something more expensive to see if our possible pairs are actual pairs.) The broad-phase algorithm I've used before is "sweep-and-prune"; for 2D, you'd maintain two sorted lists of the start and end of each box. As long as box movement is not >> box scale from frame to frame, the order of these lists isn't going to change much, and so you can use bubble or insertion sort to maintain it. The book "Real-Time Rendering" has a nice writeup on optimizations you can do, but it boils down to O(N+K) time in the broad phase, for N boxes, K of which overlap, and with excellent real-world performance if you can afford N^2 booleans to keep track of which pairs of boxes are intersecting from frame-to-frame. You then have O(N+K^2) time overall, which is << O(N^2) if you have many boxes but only a few overlaps. 

(Note: This is the answer originally provided in the question body by the OP). Problem solved. I needed to create an array of D3D11_SUBRESOURCE_DATA elements, one for each element in the Texture2DArray. Like this: 

You copied too much from the suggested command line, the apple-crunch-read-only bit is the local path in the example. Just leave it out, use only the URL, and you will be fine. So to recap, right click on your folder with explorer, select SVN Checkout, and use the url 

If you're looking for responsiveness then you'll have to blend the animation, their lot's of tricks in making your blends looks clean. Foot placement is the biggest culprit for weird blends (if two feet slide it looks really off). One trick is that at least one foot needs to be planted in the same spot as the first frame of your next animation. If it's the back foot then you have you more time for a smooth blend. You can also transition to left foot or right foot forward first. You can have a intermediate animation that is synced with your idle animation the basically has a planted foot, then you blend to that one then blend to your walk. It's always easier to figure out some clever animations then it is to create some crazy rules for your animation system. 

My former employer shifted from using a robust set of custom container classes to STL. Build times went up and ease of debugging went down, both pretty significantly. If we'd been starting from scratch, STL (perhaps better used) would likely have made sense, but it was never clear to me that we gained anything in switching to STL that would justify throwing out working, fast, debuggable code. For my personal projects, whether STL fits or not depends on the project. If I'm trying to do some Mike Acton-style data-driven, memory-and-cache-access optimized work, I'll at least think about rolling my own custom data structures. If I'm prototyping some algorithms or gameplay and don't care about performance, scalability, target platform, etc. I'll automatically grab STL.