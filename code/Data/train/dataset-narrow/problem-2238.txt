We've just put up a paper on arXiv ($URL$ that proves generic lower bounds for "statistical algorithms" for optimization problems, with each "problem" having its own lower bound depending on its various properties. Coordinate descent (and pretty much anything else we can think of) can be seen as a statistical algorithm in our framework, so hopefully this paper has some results that will be of interest to you. 

One purpose of writing down a solution to a problem in the first place is that it needs to be utterly convincing to your readers. If you think you have an argument, try writing it down fully, first in whatever language / notation you wish. Beware of any instances where you tell yourself things like "It is clear that..." or "The following just has to be true." Then, ask yourself: does it have any holes in it? Am I hand-waving/cheating anywhere? Can I fully explain every detail if asked? If you find that your work cannot withstand such questions, then you still have some more to figure out. For example, I find that I am not completely sure of my proofs until I try writing them down, and sometimes in the process of writing, I discover problems that I wouldn't have imagined. If I leave writing until last minute, I often find myself in danger. On the other hand, if you can address every issue, then you can go back and rewrite your work up formally, using the proper notation and language so that your peers will be willing to read your work. Unfortunately, there are no shortcuts here either. It takes reading/presenting a lot of papers until you are familiar with the proper style. One advantage of getting comfortable with writing in proper style is that after a while, you'll be writing proofs "properly" from the first attempt and won't have to do the rewriting step. 

Unless I misunderstood, your problem is NP Hard in general. You have different tracks of different lengths, and you need them to add up to a given distance. Subset sum reduces to your problem, so I am afraid there's little hope for an efficient algorithm in the worst case. However, it does not mean the cases you care about are hopeless. For example, if you don't have too many track sizes, you can even do brute-force search. If you don't need a very small margin, then dynamic programming should work. 

One major task of computational learning theory is to try to handle questions like this for specific classes. In general, if you assume no structure on the hypothesis space, you basically have no choice but to look at each hypotheses individually. On the other hand, if there is a lot of structure, eg. if you assume the hypotheses are linear separators, say in $\mathcal{R}^2$, you can see that it's possible to be quite efficient even though there are infinitely many linear separators! The same thing happens for trying to find the ERM (empirical risk minimizing) hypothesis in a class. This is important for PAC learning. Sometimes it's easy, sometimes it's not. 

This recent paper finally proves that edge contractions do not preserve the property that a set of graphs has bounded clique-width. 

I cannot beat David in the elegance of his answer. But after spending al lot of time on thinking about this problem I would like to betray my solution to you though ;) Let $k \ge 2$ be a fixed interger. Given $G$, construct $H$ as follows:Take two copies $G_1$, $G_2$ and a clique $Q$ on $k$ vertices $x, x_1, x_2, \ldots, x_{k-1}$, a new vertex $y$, fix a vertex $v_1 \in G_1$ and a vertex $v_2 \in G_2$. $H$ is obtained from $G_1, G_2, Q$ and $y$ by joining $x$ to $v_1$, joining $x_1, x_2, \ldots, x_{k-1}$ to $v_2$ and joining all neighbors of $v_1$ in $G_1$ and all neighbor of $v_2$ in $G_2$ to $y$. Then it can be easily seen that $G$ has a Hamiltonian cycle if and only if $H$ has a completeness tree with at most $k$ leaves. 

Given a tree $T$, a partition of $V(T)$ in $k$ levels $\phi: V(T)\to \{1,\ldots, k\}$ (i.e., edges of $T$ connect vertices of neighbouring levels $i$ and $i+1$), and an integer $K$. Can you permute the vertices inside the levels such that the crossing number is at most $K$? This problem is NP-complete, proved by Martin Harrigan and Patrick Healy, $k$-Level Crossing Minimization Is NP-Hard for Trees, WALCOM 2011, LNCS 6552, pp. 70â€“76. 

In this paper graphs in which the size of every minimal separator $|S| \le k$ are called graphs of separability at most $k$. Theorem 1 of that paper implies that chordal graphs of separability at most $2$ are obtained from complete graphs by "gluing" along a vertex or an edge. 

While the problem "is the crossing number of a graph at most $k$?" is trivially in NP, the NP-membership of the related problems for the rectilinear crossing number and the pair crossing number are highly not obvious; cf. Bienstock, Some probably hard crossing number problems, Discrete Comput. Geometry 6 (1991) 443-459, and Schaefer et al., Recognizing string graphs in NP, J. Comput. System Sci. 67 (2003) 365-380. 

This is not a new answer but rather a clarification the first and easy-to-obtain reference for hardness of INDEPENDENT SET in triangle-free cubic planar graphs: The note by Owen Murphy, Computing independent sets in graphs with large girth, Discrete Applied Mathematics 35 (1992) 167-170 proves that 

I like this textbook very much: Sanjoy Dasgupta, Christos Papadimitriou, and Umesh Vazirani: Algorithms Published by McGraw-Hill 2007. I don't calculate your suggested ratio but I think you will also like it :) 

Let $L_G$ be the Laplacian matrix of an undirected weighted graph $G = (V,E,w)$. A sparsifier of a graph $G$ is a graph $H$ such that for every $x \in R^V$ the following holds: $$ x^T L_H x (1-\epsilon) \le x^T L_G x \le x^T L_H x (1+\epsilon).$$ Using spectral methods, Batson et al. show how to construct these sparisifiers using $O(n/\epsilon^2)$ edges. Even though the statement of the theorem is debatably not "inherently spectral," I do not think it is known how one can get this result or a result like it without using spectral techniques. 

This question is inspired by the Georgia Tech Algorithms and Randomness Center's t-shirt, which asks "Randomize or not?!" There are many examples where randomizing helps, especially when operating in adversarial environments. There are also some settings where randomizing doesn't help or hurt. My question is: 

Sanjeev Arora has a nice document for a grad course (for 1st year students) he taught called the "theorist's toolkit," which has a lot of the basic material a theory student should know. A lot of this stuff you can wait until grad school to learn, but it will give you a good idea of what you'll need to know and some of the prerequisites. 

Yes - Satyen Kale, Rob Schapire, and I have a recent paper (NIPS '10) on this very problem, where we consider choosing "slates" of arms instead of individual arms. We consider the cases when position matters in the slate and when it doesn't. We also analyze both the experts MAB "contextual" model and the expertless setting. 

I don't know if you'll consider the following a non-trivial bound, but here I go. First, to be clear so that we're not confusing $c$-DNF with $k$-term DNF (which I often do), an $c$-DNF formula over variables $x_1, \ldots, x_n$ is of the form $\vee_{i=1}^{k}(\ell_{i,1} \wedge \ell_{i,2} ... \ell_{i,c})$ where $\forall 1 \le i \le k$ and $1 \le j \le c$, $\ell_{i,j} \in \{x_1, \ldots, x_n, \bar{x}_1, \ldots, \bar{x}_n \}$. We can first ask how many distinct terms can exist in an $c$-DNF. Each term will have $c$ of the $n$ variables, each either negated or not -- making for $2^c\binom{n}{c}$ different possible terms. In a 2-DNF instance, each term will either appear or not, making for $|\mathcal{H}| = 2^{2^c\binom{n}{c}}$ possible "targets," where $\mathcal{H}$ is the hypothesis space. Imagine an algorithm that takes $m$ samples and then tries all of the $|\mathcal{H}|$ hypotheses until it finds one that predicts perfectly on the samples. Occam's Razor theorem says that you only need to take about $m = O(\frac{1}{\epsilon}|(\mathcal{H}|+\frac{1}{\delta})$ samples for this algorithm to find a target with error $\le \epsilon$ with probability $\ge 1-\delta$. In our case, for $c=2$, $\lg|\mathcal{H}| = O(n^2)$, which means you'll need about $n^2$ samples to do the (proper) learning. But the whole game in learning is not really sample complexity (though that's part of the game, especially in attribute-efficient learning), but rather in trying to design polynomial-time algorithms. If you don't care about efficiency, then $n^2$ is the simplest answer for PAC sample complexity. UPDATE (given the changed question): Because you explicitly stated that you only cared about sample complexity, I presented the brute-force Occam Algorithm, which is the probably the simplest argument. However, my answer was a bit coy. $2$-DNF are actually learnable in polynomial time! This is a result from Valiant's original paper, "A Theory of the Learnable." In fact $c$-DNF are learnable for any $c = O(1)$. The argument goes as follows. You can view a $c$-DNF as a disjunction of $\approx n^c$ "meta-variables" and try to learn the disjunction by eliminating the meta-variables inconsistent with the examples. Such a solution can be easily translated back to a "proper" solution, and takes $O(n^c)$ time. As a side-note, it is still open whether there is polynomial-time algorithm for $c = \omega(1)$. As to whether the $n^2$ sample complexity is also a lower bound, the answer is pretty much yes. This paper by Ehrenfeucht et al. shows that the Occam bound is almost tight. 

Another problem with this phenomenon is the MINIMUM $t$-SPANNER problem on split graphs. For a constant $t$, a $t$-spanner of a connected graph $G$ is a connected spanning subgraph $H$ of $G$ such that for every pair of vertices $x$ and $y$, the distance between $x$ and $y$ in $H$ is at most $t$ times their distance in $G$. The MINIMUM $t$-SPANNER problem asks for a $t$-spanner with minimum number of edges of a given graph. A split graph is a graph whose vertex set can be partitioned into a clique and an independent set. In this paper it was shown that MINIMUM 2-SPANNER on split graphs is NP-hard while for each $t \ge 3$, MINIMUM $t$-SPANNER is easy on split graphs. 

What is known about "generalized outerplanar graphs" or (2,3)-sparse graphs? Some additional facts to DavidEppstein's answer: In 1982, in this paper (Corollaries 1 and 2), LovÃ¡sz and Yemini characterized generalized outerplanar graphs (in their notation, generic independent graphs) as those graphs $G$ having the property that doubling any edge of $G$ results in a graph which is the edge-disjoint union of two forests. Very previously, in 1970, Henneberg and Laman proved that generalized outerplanar graphs can be recursively obtained from $K_2$ by three so-called Henneberg moves (adding a degree-1 vertex, adding a degree-2 vertex, and a certain kind of adding a degree-3 vertex). These characterizations give the first polynomial recognitions for generalized outerplanar graphs. Some remarks related to generalized planar graphs can be found in the last section of this paper. I think, characterizing and recognizing generalized planar graphs still remain very interesting open questions. 

(In particular, INDEPENDEN SET is NP-complete for cubic planar graphs without cycles of length less than $c$ for any constant $c>0$) The reduction indicated by @BartJansen is a special case in Murphy's proof of his theorem. For the opposite property, line graphs seem to be more natural than circle graphs as addressed by @DavidEppstein. For line graphs, COLOURING is NP-complete but INDEPENDENT SET is easy. 

Another answer to the first question: Line graphs form another natural class of independent node degree 2. More general, claw-free graphs if they are natural enough for you. 

A subset $U\subseteq V(G)$ of a graph $G$ is a disconnected cutset if $G[U]$ and $G-U$ are disconnected. Deciding if a graph of diameter 1 has a disconnected cutset is trivial. The problem becomes NP-hard on graphs of diameter 2 see this paper and is again easy on graphs of diameter at least 3 see this paper. 

Imagine having to predict 0 or 1 on every round, and we are counting the number of correct predictions. Every deterministic strategy has an adversarial sequence that makes it always predict incorrectly. Hence, no competitive ratio is possible. However, the randomized strategy of flipping a coin on every round will be $2$-competitive with the best constant prediction (in expectation). 

One recent model trying to capture such a notion is by Balcan, Blum, and Gupta '09. They give algorithms for various clustering objectives when the data satisfies a certain assumption: namely that if the data is such that any $c$-approximation for the clustering objective is $\epsilon$-close to the optimal clustering, then they can give efficient algorithms for finding an almost-optimal clustering, even for values of $c$ for which finding the $c$-approximation is NP-Hard. This is an assumption about the data being somehow "nice" or "separable." Lipton has a nice blog post on this. Another similar type of condition about data given in a paper by Bilu and Linial '10 is perturbation-stability. Basically, they show that if the data is such that the optimal clustering doesn't change when the data is perturbed (by some parameter $\alpha$) for large enough values of $\alpha$, one can efficiently find the optimal clustering for the original data, even when the problem is NP-Hard in general. This is another notion of stability or separability of the data. I'm sure there is earlier work and earlier relevant notions, but these are some recent theoretical results related to your question. 

$R^n$ "stands for" the fact that the (data) points to be classified lie in a $n$-dimensional Euclidean space. For example, points in our familiar $2$ dimensions (with, say, latitude and longitude coordinates) lie in $R^2$ -- an $n$-dimensional space generalizes this notion. A hyperplane (in $R^n$) is a flat of dimension $n-1$. It is what an SVN uses to classify the data (after they have been mapped to the higher dimensional space by the chosen Kernel). For example, if your space is $R^2$ (or a plane) then a hyperplane is line, and so on. How to transform the space? This is done via the Kernel function. A big part of SVM design is choosing a proper Kernel for the problem at hand. 

Let me give a partial answer from a learning theory perspective. As your question isn't well specified, this answer won't be either. In my answer, I'm assuming your question was inspired by your blog post, linked from your profile. Say that you are thinking about programs that are just functions (so they have to halt, etc.). You can ask whether certain classes of such functions can appear randomly by, perhaps, looking at the probability a random program (from some distribution that you think is likely) lands in that class or not, with the hope that probability is polynomially large. I haven't really thought this argument through. You can also ask whether such a class is efficiently evolvable according to Valiant's model of evolution (also in @Artem's pointer in comments): luckily what is efficiently evolvable is known to be the class learnable by correlational statistical queries; taking "crossover" into account, you get parallel correlational statistical queries. One thing to note is that just because evolvability is characterized, it is still a separate and sometimes difficult task to determine whether a particular class is evolvable (learnable with CSQs) or not. If you find a class of "programs" that is neither randomly occurring nor evolvable, perhaps you can conclue it has a "creator/programmer," though that conclusion may still take a leap of faith.