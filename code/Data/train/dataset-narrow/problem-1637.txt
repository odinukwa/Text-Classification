I don't use debian but if it can help you, RHEL6 (2.6.32 kernel) uses ext4 as the default fs even for the boot partition since 2010, and I never had any issue with it. Not sure you'll get big performances boosts, sure it got some better algorithms and some tweaked functions but it honestly depends on the kind of usage of the disk. 

The first part of the initial code fails because the condition is verified again the resource Arn, so you're essentially denying access to all queues. The second part should work, but an explicit access denyal always has precedence. The "policy 2" is wrong as again you'd give access to everybody but the user you need to give access to. But the global deny you used above still has precedence so no effect at all. I think your problem is at the very base of your current IAM settings - you should have never given such a wide access to your systems to all of your users. IAM works on an "allow what you strictly need" basis which is best for security. Instead you got youself tied into a "deny everything but what you need" which is hardly maintenable in IAM because of the "deny" statements always winning. The workaround for now is what you did - apply a deny policy to a group, and apply that group to all the users who shouldn't have access. But, as more queues might be created, you'll find yourself into a never ending loop of deny policies where you'll need to explicity set the names of all of your queues in the deny policies. And still - will that be secure or make sense? You said You've given to many users an administrator role - if they can modify their IAM policies, they can do everything - cloudtrail might tell you what happened, but won't bring your data back. I'd seriously suggest you to take a different approach instead, and define much more limited policies for your users. Create extra policies as needed and make them so what they are cumulative, attach them to groups and from the groups to the users. Take advantage of the wildcard on the arn values to eventually create "reserved" namespaces for your resources with more restricted access. 

That functionality needs to be supported both by the hardware platform, and by the OS. In your case, the HP Proliant DL385 g7 manual clearly states that a full power off is needed to install new memory modules, and as such I wouldn't even care to check for ESXI support to the functionality. 

RAM/CPU? Clustering? Load balancing? Only you can know. I've seen RAM occupation vary from a few megabytes to several hundreds, even for what seemed to be simple applications. A lot depends on what libraries you'll be loading. Same for CPU. There is no substiture for profiling a web application's performances. Start from a sample configuration, get some workload test software like JMeter (but there are many, the choice might depend on what your webservice does), create a decently average weight testing script and launch 50, 100, 150 threads. Keep your java virtual machine monitored (one free tool might be javamelody, and is both free and lightweight, but there are others) and check your logs if it crashes. Then make your proper calculations based on the response times and the machine's status when under load. 

How is it supposed to resolve the domain debian70.vm? looks to me you're using bookings@debian70.vm as the sender address. The spam check is done over debian70.vm, which can't get resolved. 

The "log" agent item checks the whole log by default, you must tell it to use the "skip" mode to avoid processing of older data. Simply add a ",skip" to the end of your parameters to the log item. Check carefully this page in the documententation. 

You might also want to try to lower the maximum number of threads and spare threads in Apache first, this might make your website react a little slower but if you tweak it a little bit and you don't have sudden usage spikes you should be fine. 

Look at that last file and you'll find everything needed to bypass the default options, the file is decently commented. About tomcat6 being ok with the JDK8 instead, that's for you to discover :) 

This is an idea using iptable's string module to check the http header content, of course it must be adapted to your special case: $URL$ However I'm not really sure about what would be faster between inspecting all of the incoming packets and having your webserver handle them. I'd eventually instead just add a line in the server configuration to immediately return a 404 on that specific url; traffic will self decrease with time, as webmasters and search engines correct their links (and they will, as 404 errors would cause a bad positioning in modern web search engines). 

Indeed the problem that you speak of exists and makes up for a lot of questions, like "for how long should I cache an answer? What if I have two projects whose commits rates are very different?". There are some proprietary solutions which do what you are looking form i.e. if you use Atlassian Stash, it has a built-in plugin which manages checkout answers caching in order to lower the load on the server. The best solution anyway is different from what you want to do. The best and recommended solution is to use post-commit hooks, they exist in git, svn and I think in other vcs as well. Just have your repository trigger the build on your CI system, rather than the CI jobs polling continuosly. As you mentioned Jenkins (Hudson), the Git plugin for example already provides urls to perform this kind of activity. 

No need to boot up with a live cd as suggested by @Mr Shunz, since the system is working. Just restart the server, in the boot manager select to edit the boot parameters and add an to the end. The system will boot in runlevel 1 with no network and the root user already logged in. Eventually disable system management programs from starting up automatically at startup, so that things like puppet or chef won't eventually enforce particular users or passwords on the system. Change the root password, restart and then do your investigation on what's on the system using the root user. 

Where can't you find servers with good memory amounts? Of course manufacturers propose low ram solutions, to keep initial prices low. They all propose upgrades, at a price. Manufacturers certified memory is expensive though. Even most economic to mainstream servers nowadays will accept over 32GB of ram. Dual socket servers work well with just one cpu too. It's unlikely that with all that memory busy with data you'll want just a few cores to serve a limited number of clients. If you have so little clients, it's unlikely that you need the extra speed of keeping so much inside your memory. Microsoft doesn't only sell per-core license. It also got Server+CALs licenses. Please see this link. You choice depends on your conditions. The future is virtual machines, sometimes even cloud based. A per-core license adapts very well to it, as in virtual machines you allocate cores, not cpus. Nowadays servers tend to have plenty of processing power; put a virtualization hypervisor on your machine and allocate just the resources (disk,cores,ram) you need. Upgrade in minutes if needed. Or use the same machine to host other stuff. 

They seem to be both indicating a RAID 10 to me. Please have a look at $URL$ from page 13 for a nice description of what you got as output from megacli. 

Sounds like you simply need to transfer some files. If you have a correct username and password in the domain you should be able to connect and upload your files. Solution 2, connect with a remote desktop and upload your files through it (slower). Or, if you got any troubles or want some piece of mind, why can't you install some ftp server like filezilla... you can set it up in 5 minutes and remove it in even less time when you're done. 

Speaking about virtualhosts, we had the same problem for a project of ours. The Apache's "ServerName" directive isn't optional, it's mandatory to use. Apache has no such thing as a "default" virtualhost to load when it has no ServerName match. And if virtualhost servernames overlap, well, that's just going to give you problems. In the end in those cases Apache just decides what to do depending on its configuration files loading order. And if you try to contact the server through an unspecified address (ip address, or something not mapped on the virtualhost) it will load the first virtualhost he got a configuration loaded for that address. The correct way to do it all is to have virtualhosts detailing all of the possible servernames in ServerName and ServerAlias directive. See name based virtual hosting on apache, and consider carefully the evidenced note "Main host goes away". 

With Apache you can do that with a reverse proxy. In your case with Apache you could use this kind of approach in the configuration file: 

There is no real solution. The best things you can do for now is to enable both SPF and DKIM for your domain/mail server, but google's filter is still pretty strict. 

You forgot the burst capacity. For both EBS magnetic and General purpose SSD, the "base" speed is as you said. Base speed is the minimuum guaranteed speed in any moment. But, while with magnetic disks you have a burst capacity of a "few hundreds", with SSD you can go up to 3000 iops. It then depends on what use you make of your machine: need stable constant i/o performance on a small disk through the whole day? Go magnetic. Have the casual I/O spike (reboots, a few minutes of heavy traffic, your night backup process)? Perfect scenary for the general purpose SSD. 

Caching would use memory anyway (and even Apache can do that), so what's the advantage with that? At most to improve performances you can set nginx to serve the static content, while proxying instead to Apache for dynamic content. Of course if: 

Does it make more sense now? For all addresses in 10.*, including those you want to exclude, both the first and the second part always evaluate to True. Just changing the first OR into an AND should do the trick. 

KeepAlive normally makes a lot of sense, requires more memory but lowers the number of connections, CPU usage and connections overhead. MaxClients and other stuff must be tuned to your situation instead. Normally you'll want Apache to have a number of idle instances big enough to serve all of your users without having it to spawn new child processes all the time; at the same time you want to avoid excessive values for maxclients in order to keep memory usage under control. Unless you have sudden usage spikes though, the default Apache settings are usually adequate and will self adapt well enough to your environment. If you serve content through https and since you're not using php you might want to give a try to mod_spdy module too, many browsers support that already (waiting for a global support for http 2.0). Finally another option is to use caching/precaching. In that regard some functionality is already available in Apache thanks to several modules (please see $URL$ ), or you can put something else in front of the server (Varnish is a popular option). 

Rsync+rsync or snapshot+rsync won't really make much difference - with rsync maybe being more handy, since you're able to eventuall compress/encrypt data during the transfer without the hassle of having to use extra commands. In both cases you're going to forever try to catch up with what your users might have copied on the share since the last rsync, including partial file still in transit. Honestly what I would recommend to you is to do a first copy with rsync in a period of low usage. Then, warn your users that there will be a small outage due to needed maintenance. Stop the services writing on the disk. Remount the old share in read only mode, do a final rsync and then completely replace the old nfs share with the new one. If you want/can, you can give customers read only access during that period. 100% availability is a pure dream, and it's better to stop your customers for 1 hour than to chase after possible endless complaints of lost/corrupt data and application crashes. 

Simple way, if you're sure of your security settings: set the suid bit on the main folder. But I don't see what permission troubles you'd have anyway, since the group is already set on www-data. 

(Of course you should have mod_proxy_http enabled and loaded in your Apache) If your application makes use of cookies, you might also have to properly set the ProxyPassReverseCookieDomain and ProxyPassReverseCookiePath directives. You can read the related documentation here. Also, you forgot to mention which application server you're running, even if I see you're using some java application server. Mind that often application servers won't like at all if you change the application's context, which you should keep even on the proxy to stay on the safe side. Finally, regarding your url rewriting, I'm sorry but if you don't post your exact configuration I guess that no one would be able to help you. Mind that anyway slashes do actually make a difference to web/application servers. Some add them automatically, others won't. 

Items are "entries". The "path" to an item is its DN, or Distinguished Name. References: Relative article on Microsoft's MDSN Link from RHEL7 documentation 

The RDP connection should come back shortly after the network disconnects, so you will just have to reconnect, I don't see the problem, your desktop session will remain open. Actually if the blackout is as quick as it should, RD will probbaly reconnect for you. If you want anyway you can also install Virtualbox as an unattended installation, as said here on the virtualbox manual following Microsoft's instructions. 

You can modify directly the /etc/sysconfig/iptables file. Reload the iptables service to reload the rules from that file. Yet, as you were told already, firewalld is the new default firewall system for Centos, and this is a good chance to learn how to use it, don't you think?