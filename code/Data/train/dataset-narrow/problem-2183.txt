Apologies in advance for this a soft question which has no closed, correct answer. This is probably the best forum to ask my question. I am a third year graduate student in theory group of a top-15 school in US. So far, I've been doing decently well. I have a first author theory paper and a first author practical paper so far. My advisor has been superb in helping me honing my skills but I feel stuck (and helpless). So far, my advisor has been a helping hand and a guiding force to always point me right directions and pose the right questions (not too abstract, not too concrete) which have been instrumental in me finding the answers to research problem. As I try to be more "independent", i.e, ask the right questions and prove results, I feel I have failed miserably and quickly feel overwhelmed. I feel that I don't have the maturity of a theory PhD student yet where I can ask myself the right questions and come with answers. In other words, given the right definitions and some hand holding, I am able to do things but otherwise, it becomes very hard. My sloppiness when it comes to writing proofs doesn't help either. I am looking for advice on how can I sharpen my skills and be more "theory-minded" where I can grasp things quicker and require less hand holding. While one answer is simply to keep working and hope that experience makes me wiser, I am not sure how this will work out. The only solution I have is to actually go through come recent papers in my area and write down proofs by hand in as much detail as possible to help me nail proof writing skills and build intuition. Any advice would be extremely helpful. Please let me know if question is unclear or needs more detail. 

Update: The paper is now available online: Explicit relation between all lower bound techniques for quantum query complexity by Loïck Magnin and Jérémie Roland 

EDIT (v2): Added a section at the end on what I know about the problem. EDIT (v3): Added discussion on threshold degree at the end. Question This question is mainly a reference request. I don't know much about the problem. I want to know if there has been previous work on this problem, and if so, can someone point me to any papers that talk about this problem? I'd also like to know the current best bounds on the approximate degree of $\textrm{AC}^0$. Any other information would also be appreciated (e.g., historical information, motivation, relation to other problems, etc.). Definitions Let $f:\{0,1\}^n \to \{0,1\}$ be a Boolean function. Let $p$ be a polynomial over the variables $x_1$ to $x_n$ with real coefficients. The degree of a polynomial is the maximum degree over all monomials. The degree of a monomial is the sum of exponents of the various $x_i$ that appear in that monomial. For example $\textrm{deg}(x_1^7x_3^2) = 9$. A polynomial $p$ is said to $\epsilon$-approximate $f$ if $|f(x)-p(x)|<\epsilon$ for all $x$. The $\epsilon$-approximate degree of a Boolean function $f$, denoted as $\widetilde{\textrm{deg}}_{\epsilon}(f)$, is the minimum degree of a polynomial that $\epsilon$-approximates $f$. For a set of functions, $F$, $\widetilde{\textrm{deg}}_{\epsilon}(F)$ is the minimum degree $d$ such that every function in $F$ can be $\epsilon$-approximated by a polynomial of degree at most $d$. Note that every function can be represented with no error by a degree $n$ polynomial. Some functions really do need a degree $n$ polynomial to approximate to any constant error. Parity is an example of such a function. Problem statement What is $\widetilde{\textrm{deg}}_{1/3}(\textrm{AC}^0)$? (The constant 1/3 is arbitrary.) Notes I encountered this problem in the paper The Quantum Query Complexity of AC0 by Paul Beame and Widad Machmouchi. They say 

Consider the following linear program, $$\min y \\ xc_1 \leq c_2 + yz,\\ x = x_1 + \dots + x_n,\\ z \leq x_1 + x_2, \\ z \leq x_2 + x_3, \\ \vdots\\ z \leq x_{n-1} + x_n, \\ x,x_1, \dots, x_n,y,z \geq 0 $$ where $c_1, c_2$ are constants. This is an example of quadratically constrained linear program where I have 1 quadratic constraint. I wish to find out if this problem is NP-Hard or not. The quadratic constraint can be expressed in the form $\vec{y}M\vec{y}^T$ where $M$ for my problem is not positive semidefinite (and thus, non-convex) which is perhaps evidence of hardness Listing specific questions below: 

Can this problem be transformed into a linear program by taking logarithms? Is there any literature reference or reduction showing that linear programs with non-convex quadratic constraints is an NP-Hard problem? 

I am confused whether $\mathsf{APX-hard} \subseteq \mathsf{NP-hard}$. My confusion stems from a result on graph pricing from this paper which says the following : "Unlike the general case of the graph pricing problem, the bipartite case was not even known to be NP-hard. We show that it is in fact APX-hard by a reduction from MAX CUT". It seems like the authors are implying that APX-hardness is a stronger property than NP-hardness Since $\textsf{APX} \subseteq \textsf{NP}$ by definition, the above statement is true for MAXCUT as it is in $\textsf{APX}$ and also in $\textsf{APX-hard}$ but I am not sure if $\mathsf{APX-hard} \subseteq \mathsf{NP-hard}$ in general. Any pointers/counter example problem is appreciated! 

Obs: Note that sampling uniformly from X is not the same as sampling uniformly from the circuits representing function from X, since many circuits may represent to the same function. 

Group isomorphism can be solved in time $n^{O(\log n)}$ Graph isomorphism can be solved in time $n^{\log^{O(1)} n}$ Isomorphism of linear codes can be solved in time $2^{O(n)}$ ... 

A famous theorem due to Impagliazzo and Wigderson states that if some function in $E=DTIME[2^{O(n)}]$ requires circuits of size $2^{\Omega(n)}$ then P=BPP. When can we change $P$ with some complexity class $C$ such that the same result works? In other words, for which complexity classes the following statement is true? Statement: If some function in $E$ requires circuits of size $2^{\Omega(n)}$ then $C= BP-C$. Here $BP-C$ is the bounded probabilistic version of $C$. In particular does it work for formulas? I.e. for $C$ equal to logspace uniform $NC_1$ ? In this case the statement would be: If some function in E requires formulas of size $2^{\Omega(n)}$ then $NC_1 = BP-NC_1$ 

It is well known that each resolution refutation $\Pi$ for an unsatisfiable CNF formula $F = C_1\wedge C_2 \wedge ... \wedge C_m$ over variables $X$ can be translated in polynomial time (in the size of $\Pi$) into a deterministic branching program $P$ solving the following search problem: 1) $P$ has one source node and one sink node for each clause $C_i$. 2) For each assingment $\alpha:X\rightarrow \{0,1\}$ there is a consistent path in $P$ from the source node to some sink node associated with a clause that is falsified by $\alpha$. Question: Is there a proof system strictly stronger than resolution where each proof $\Pi$ can be translated in polynomial time (in the size of $\Pi$) into a not necessarily deterministic branching program $P$ solving the search problem above? 

First note that the sum $O\left(\sqrt{\frac{N}{t}}+\sqrt{\frac{N-1}{t-1}}+\dots+\sqrt{\frac{N-t+1}{1}}\right) = O(\sqrt{Nt})$. The quantum query complexity of this problem is indeed $\Theta(\sqrt{Nt})$. The lower bound can be shown by reduction from the problem of deciding whether the input has $t$ marked elements or $t+1$ marked elements. This problem is very similar to $t$-threshold, and has a lower bound of $\Omega(\sqrt{Nt})$. This can be shown using the polynomial method or the adversary method. 

I had a vague recollection that I knew an excellent reference for such oracle separations. I finally found it. A great reference for oracle separations (for classes between P and PSPACE) is the following paper: 

What you want is a good reference to understand the exponential lower bounds for $AC^0$ circuits computing the PARITY function. Now you haven't stated whether you actually want to understand the proof, or just understand things at a high level, the way a survey article would explain things. A survey article I recently read and liked is "The complexity of finite functions" by Boppana and Sipser. If you really want to sit down and understand the proof, then you can either read proofs based on the Switching lemma (which appear in the papers you cited - [FSS], [Y] and [H1]), or the Razborov-Smolensky proof. For proofs using the Switching Lemma, Håstad's Ph.D. thesis is a good read, if a bit hard to follow if you're new to the area. A better exposition of the proof is in "Introduction to circuit complexity and a guide to Håstad's proof" by Allan Heydon. The only problem with it is that I can't find it online, and I have a hard copy. I really recommend it if you're new to circuit complexity. For the Razborov-Smolensky approach, just google for it and you'll get a bunch of lecture notes. I understood the lower bound from these three lecture notes: Sanjeev Arora, Madhu Sudan and Kristoﬀer Arnsfelt Hansen. 

Where for each $n$ we assume that an isomorphism is a permutation of the set $\{1,...,n\}$. Is there a well studied variant of isomorphism problem that is known to be solvable in time $2^{O(n\log n)}$ but not in time $2^{O(n)}$? Obs: Note that $n!\cdot n^{O(1)} = 2^{O(n\log n)}$ is roughly the time necessary to test all permutations. 

Has some notion similar to the OR-Weft hierarchy been studied in parameterized complexity theory? What kind of functions can be computed by circuits of constant OR-weft? 

Proof assistants are a valuable tool for verifying the correctness of proofs of mathematical theorems. When dealing with proofs of correctness of algorithms, one is not only interested on showing that the algorithm indeed works correctly, but also on showing some upper bound on the execution of the algorithm. Can proof assistants based on dependent type theory be used in a natural way to prove upper bounds on the running time of algorithms (besides of course of proving their correctness)? What is the standard approach to do so? If such assistants are not appropriate, is there some other kind of proof assistant that can do the job? Obs: When I say time, I mean the complexity theorist notion of time, in the sense of number of steps in an idealized RAM machine. I don't care about formalization of actual hardware and physical time. Therefore, although Neel Krishnaswami is interesting, I'm not accepting it because it is far more complicated than what I'm looking for. 

but it was a major open problem to show that either $Pn(f) = \Theta(D(f))$ or that there exists a function for which $Pn(f) = o(D(f))$. A few days ago this was resolved by Mika Göös, Toniann Pitassi, Thomas Watson ($URL$ They show that there exists a function $f$ which satisfies 

Reposting my comment as an answer: AC0[p] is strict subset of NP, where AC0[p] is AC0 with mod p gates, and p is a prime. As for whether we can define NP-completeness to make it stricter, you can define it w.r.t., say, logspace reductions or even ACC reductions. As far as I know, it is not known if this changes the class NPC. What is known, however, is that AC0 reductions are not powerful enough. (Ref: Reducing the complexity of reductions by M. Agrawal, E. Allender, R. Impagliazzo, T. Pitassi and S. Rudich) 

They also show an optimal result for the one-sided version of $Pn(f)$, which I'll denote by $Pn_1(f)$, where you only need to cover the 1-inputs with rectangles. $Pn_1(f)$ also satisfies 

It is contained in DP: Difference Polynomial-Time, which is also BH$_2$, the second level of the Boolean hierarchy. This class is itself contained in $\Delta^\textrm{P}$, but that is believed to be a bigger class. A language L is in DP if it is the intersection of a language L1 in NP with a language L2 in coNP, and so your example is clearly in DP. 

Consider a set of $k$ continuous variables. Each variable $x_k$ is associated with a hidden distribution from which its value is sampled independently of other variables. I am given a set of observations of the sum of $k$ variables, i.e, $\sum_{i = 1}^{k} x_i$. The challenge is to learn the hidden distribution from the given observations. This is the general variant of the problem I am trying to study. To simplify the problem, I can make the assumption that the hidden distribution is uniform and of the form $[0, u_k]$. Then, the challenge is to find $u_k$ for each variable $x_k$. I am not very familiar with learning style algorithms but I am sure variants of this problem have been studied in great detail. Are there any references that I can look at to find out more about this problem? 

I will begin by linking a previous post where I asked a general question for a stochastic setting which I describe below. It turns out that my "proof" for a restricted case had a mistake and there is a much simpler setting where showing hardness should be easier. Please let me know if I should amend the original question instead. Consider a graph $G = (V, E)$ with $n$ vertices and $m$ edges. Each vertex $v_i$ can take positive value $a_i$ with probability $p_i$ and value $0$ with probability $1-p_i$. We will restrict $G$ to be a cycle where every vertex has degree $2$ (and $m = n$). The challenge is to assign weights $w_e$ to each edge to maximize the objective function $E = \sum_{e = \{i,j\}} w_e \Pr[X_i + X_j \geq w_e]$ where $\Pr[X_i + X_j \geq w_e]$ denotes the probability that that the sum of values taken by vertex $i$ and $j$ is greater than $w_e$. The additional constraint is that the weights $w_e$ need to be sub-additive, i.e., for any two edges $e'$ and $e''$ that "cover" edge $e$ meaning $e'$ and $e''$ include the vertices that make $e$, it holds that $w_e \leq w_{e'} + w_{e''}$. Observe that the deterministic version where $p_i = 1$ is trivial. Any suggestions on possible directions for hardness or PTIME algorithm would be very helpful! 

What is the simplest machine model accepting the following language? $$ L = \{(w\# )^{k}\;|\; w\in \Sigma^{*},\;k\in \mathbb{N}\}$$ In other words, $L$ is obtained by taking each string $w$ in $\Sigma^*$, and then creating the strings $w\#\;,\; w\#w\#\;,\; w\#w\#w\#\;,...$ Here $\#$ is a separator symbol which is not in $\Sigma$. The simplest I could think of was queue automata with $2$-queues. But these are already Turing complete. Is there some well studied class of machines that is not Turing complete and accepts $L$? 

Say that a node of a circuit is small if it has fan-in at most 2 and large if it has fan-in greater than 2. The weft of a circuit is the maximum number large nodes in any path from an input node to an output node. Let $C_{t,d}$ be the class of circuits of weft at most $t$ and depth at most $d$. The notion of weft is used fundamentally in parameterized complexity theory to define the W hierarchy. Namely, a parameterized problem P belongs to $W[t]$ if there is a parameterized reduction from P to $WCS[C_{t,d}]$ for some $d>1$, where $WCS[C_{t,d}]$ is the problem of determining whether a circuit in $C_{t,d}$ has a satisfying assignment of Hamming weigth exactly $k$. I'm interested in circuits in which only OR gates are allowed to be large. More precisely, say that a circuit $C$ has OR-weft at most $t$ if the following conditions are satisfied.