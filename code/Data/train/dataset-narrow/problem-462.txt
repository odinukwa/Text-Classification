The link that Thomas provided in his comment on the question is a good resource of some scenarios to test. Bob also provided some tests that are good, many of which are included in the blog post linked. I would say in addition to those great lists of "what" to check, you also want to look at various application scenarios to test failover during. I've seen a lot of clusters get built and then get tested from the server team/DBA team side - but the application teams were never involved. What happens to your applications during that failover? Now it really mostly looks like a restart to the application (effectively that is what the failover is.. Service goes down on Node A.. Service goes up on Node B.. SQL does what it does when SQL is shut down and restarted or when it crashes and comes back up.. DBs go through recovery on the other side of the restart, connections are all dropped where they are, etc.) So it may seem pointless to test, but it is good to see what kind of process the users will experience and to understand what processes the application owners and helpdesk folks, etc. need to do when that failover happens. You should ask questions like: 

So I can say for sure in my limited testing on SQL Server 2012 the traces of the DB were gone from the buffer pool and the plan cache completely. Which makes sense because the DB that they related to disappeared, that is the expected behavior. That said - after a restore one thing you definitely should consider doing is updating your statistics. I like to start with clean stats build up and I really like to see people using production like data in dev and I like starting with clean statistics. So good on you for using real data in dev. (Caveat - sometimes you need to deidentify your data before doing this, but yay for testing with real data distributions and sizes.) 

This command is described here. If you want to understand the mechanics deeper, I wrote a blog post a few years ago that discusses transaction log growth. Proper management is best because then you won't have to worry about finding an automatic way to shrink it. 

The first principle is - you will never secure your MDF file from being looked into by someone with SQL savvy. There are ways around most of what you can do. If you are deploying this application to a customer, and that customer has enough SQL knowledge and you've given them a data file - they can do what they like with it ultimately.. This principle rules all of the others. Support contracts and license agreements protect you legally here - that is outside the scope of this format. But you have to trust the folks buying your app to honor their support agreements and EULAs. Every vendor out there has to do that same thing. You can, however, do things like encrypt data in tables and encrypt/decrypt it in and out from your application. This can help protect data, but it isn't foolproof. Someone can try and attach a debugger, dig into the traffic and try and get a sense for what is going on. You can encrypt your stored procedures to protect the stored procedure code - again this isn't full proof (remember Principle #1 rules them all here) 

Not sure exactly what you want to grab out of the referenced system view. Much of the data you care about is likely still exposed to you, but through the Database related DMVs which have more or less been around in some fashion since SQL Server 2005 with additions in versions since. You might have some luck with querying a few of these DMVs. You can explore with 

I'm trying to implement a hierarchy using closure tables maintained by triggers. I'm using MySQL 5.6. I know we should move out of the 90s and use a database that actually supports CTEs, but this is what we're stuck with for now. I've written this simple test database and procedures for now that seems to work: 

Will this work as intended? DO I need to change the transaction isolation level? Is there a better way to do this in MySQL? Note I'm using MySQL 5.6 in one environment and Amazon RDS in another environment, so it should be as compatible as possible. 

Multiple upstream Kapacitor servers are sending notifications to a load-balanced django app. If all the upstream servers are working correctly, the app will always receive duplicates of these notifications (since all the upstream notifiers should send the same notifications; it's just for redundancy). I want to be able to filter out these duplicates. However, since the python app is load-balanced, the only place we can check for these duplicates is in the database. This means I'm using this stored procedure to control application logic, just inserting the data with the hash into the database and ignoring duplicates is not an option (the application might do something like send someone an SMS message based on the contents of the alert, so we definitely don't want dupes) To do this, I'm hashing the messages, then invoking a stored procedure in the database that checks if a message received in the last 10 seconds had the same hash. I want to be 99% sure the stored procedure is safe against race conditions. Here's some SQL code that seems to work: 

However, I'm very worried about the synchronization of all this and making sure we don't end up in a bad state if multiple queries are running concurrently. Because we have multiple nodes that could be accessing the database, concurrency at the application level is very tricky (we would need to use redis or something for locks, which I'd rather not do). I can use GET_LOCK and RELEASE_LOCK with a named lock, or I can use table locking. My fear with these methods is: what will happen if something goes wrong? The lock is held for the length of the session, right? We're using connection pooling (some services using Apache DBCP and others using Hikari), which reuses connections, and even without that, we might have transactions that do multiple operations. Since MySQL lacks any mechanism, I'm afraid we might get into some sort of deadlock situation. What's the best way to implement concurrency with this approach? Note that I only care about concurrency within the triggers/update process itself. It would be nice if other clients don't read an intermediate state, but it's not the end of the world. As long as it's eventually consistent, I can handle phantom/stale/whatever reads from clients that aren't doing updates. 

CONTEXT: this isn't exactly a closure table. We need to have the full hierarchy in these other tables for quick access. For example, we need to be able to do joins on the complete set of parents (since stuff like permissions are inherited by sub-folders, so if a user has access to a parent, they have access to all subfolders). Therefore, the and (sorry about the naming conventions; I don't control that part) need to have a record of every item and every child of that item (transitively). Basically, we need to be able to check if a user tries to access a folder 5 levels deep what level of access that user has access to it, which can be granted at any level of the hierarchy. We also need to do some aggregations/sorting on stuff that joining the subfolders table would be helpful for (ie "sort on total number of items in this folder and all its subfolders"). The reason there are two tables is that we have one table for folders, and one for items. Folders and items have different ID spaces. Think of it like a filesystem with directories and files treated separately. A file ("watchlist") can't have any children, but a folder can have either folders or files as children. The code I posted above works for this. My concern is just with the concurrency and correctness in the face of multiple simultaneous clients. 

That should tell you what time zone your SQL Server thinks it is in. I would suggest that the issue is either the client on which you are running the SharePoint application session - or more likely a setting in SharePoint itself or the server time zone on the SharePoint server itself. You can check the regional settings for a site and see what it reports. This link should help get you started there. 

So the issue here is a path issue as you see in the comments on the question. There was an issue with the path being used in the restore script that u123 was using. The path was not correctly typed and this led to the error as there was no file there. This is a case of the error message being quite useful but missing some helpful text (namely - the actual error message from windows) To understand the troubleshooting approach that I used here to investigate this one: 1.) Find the error message(s).. In this case: 

So ALTER is the minimum permissions required. You can get that as DB Owner, you can get that as DB_DDLAdmin. Or just grant alter. If you think about what truncate does and how it works, this makes sense, it is a pretty "severe" command and empties the table of data and does it quickly. 

Short Answer: Nothing at all is wrong with a domain account. When all the computers participating are in the same domain, go that way. Easier to manage and maintain and secure. Your confusion here lies in the way the documents you are looking at are structured. In the document you reference in the comments above you see this section: 

You are misreading that document. In the document and image you are referring to - there are actually two distributed Availability Groups in the image you refer to. each one containing two Availability Groups. A distributed Availability Group is distributed between two (and as far as I now - only two) Availability Groups. You can have multiple secondaries in an Availability Group and they can span data centers, so depending on what you are trying to do, you may not need a Distributed Availability Group to do it. 

I would use tinyint for the in this case they told us the possible values are 1 or 0. may also work. You could also try . But is an unsigned 64 Byte integer. BIGINT is signed, so the max value is lower. So technically speaking a DECIMAL(20,0) or greater precision would be used here. But in later versions of that same article this is a BIGINT (For SQL Server 2008 R2 and SQL Server 2012) so I am sure you are fine with BIGINT here. If you get enough disk space and time to create a database big enough to compress to a value that blows BIGINT you can test this theory out someday ;-) No undesired behavior if you go with / I am not sure I understand your question, but I believe the answer is conversion if you are asking what I think you are asking but this is potentially just an oops I'm not sure why those datatypes are in the documentation but you've chosen good logical approximations. 

The best two books here are both from Itzik Ben Gan - Inside SQL Server T-SQL Querying and Inside SQL Server T-SQL Programming. Read the Querying one first. Covers all the important information on how queries are processed, thinking in terms of sets, etc. Covers all aspects of querying. The second book goes into programming constructs when working with T-SQL. Amazing books. 

If I understand you properly - you want to have a second reporting instance and you are asking how you can keep that updated. It also appears you are on SQL Server 2005 so your options are limited more so than, say, if you were on SQL Server 2012 with Availability Groups. Your options are really going to be in the spirit of: Log Shipping And here you can actually restore your logs in mode - this means that you can effectively query your log shipped database for read only queries anytime. You can read more about that here and at the related links on that article. You cannot add indexes, add statistics, do any writes, etc. But you can avoid creating a snapshot. But you still require the blip of connectivity when the standby restore happens - so you haven't saved much. Mirroring Same idea of log shipping (at a very basic level that is) - and here you can read off of a standby. Still ned that downtime of the reporting users to rebuild your snapshot. You can do A/B switching and effectively abstract some of that through views, content switching/load balancing but you still need a downtime for the snapshot change. Replication You can use transactional replication and replicate data. This is near real time and updates just happen as they happen on the primary and get replicated (or you can control the frequency). This has more of an overhead on the source, latency really matters, requires a bit more DBA overhead, but it allows you to report on the secondary with no need to worry about standby restores, or new snapshots. Benefit here is you can choose what comes across and what doesn't if the reports only care about certain tables. ETL You could always roll your own process to extract, transform and load just the data you care about. This way you can create a reporting optimized version of the database, index it appropriately and stream data in at the frequency you want. You need to worry about concurrency and blocking risk during your loads if it needs to be updated throughout the day, but there are ways of dealing with that. More overhead and developer time required. And I would really encourage you to think about SQL Server 2012/2014 - AlwaysOn Availability Group Active Secondaries are a benefit here potentially. The main benefit being the read only secondary database. There are some optimizations here including the ability to have temporary statistics created and stored in the TempDB database on the secondary instance. The read only doesn't require snapshots, with standby, etc. It is truly read only - so you cannot write into it - but for reporting this can be a big win. Granted the move from 2005 to 2012 is a bigger conversation, requires licensing conversations, etc. but it is an option here for sure. If you are talking about HA for SSRS that is a separate conversation.