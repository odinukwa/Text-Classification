More generally, the competitive ratio is $\min\{\frac{a+c}{a+b} , \frac{a+b+c}{a+c}\}$, where $a\le b\le c$ are the three comparison costs. (There are more cases to consider here, because there are optimal algorithms that do not perform the cheapest comparison first, but the case analysis is still elementary.) Tedious calculations imply that the expression $\min\{\frac{a+c}{a+b} , \frac{a+b+c}{a+c}\}$ is maximized when $a=0$, $b=1$, and $c=\phi$. In particular, if $\frac{a+c}{a+b} > \frac{a+b+c}{a+c}$, the best possible algorithm can be forced to perform all three comparisons. 

1 Branko Grünbaum. Polygons: Meister was right and Poinsot was wrong but prevailed. Beiträge zur Algebra und Geometrie 53(1):57–71, 2012. 2 See, for example: Erik D. Demaine and Joseph O'Rourke. Geometric Folding Algorithms: Linkages, Origami, Polyhedra. Cambridge University Press, 2007. 

The first point is both the hardest and the most important. If you try to cover your ass, you will lose the respect and attention of your students (who are not stupid), which means they won't try as hard, which means they won't learn as well, which means you haven't done your job. I don't think it's fair to let students twist in the wind with questions I honestly don't think they can answer without some advance warning. (I regularly include open questions as homework problems in my advanced grad classes, but I warn the students at the start of the semester.) Educational, sure, but not fair. It's sometimes useful to give hints or an outline (as @james and @Martin suggest) to make the problem more approachable; otherwise, almost nobody will even try. Obviously, this is only possible if you figure out the solution first. On the other hand, sometimes it's appropriate for nobody to even try. (For example, "Describe a polynomial-time algorithm for X" when X is NP-hard, or if the setting is a timed exam.) If you still can't solve the problem yourself after sweating buckets over it, relax. Probably none of the students will solve it either, but if you're lucky, you'll owe someone a LOT of extra credit and a recommendation letter. And if you later realize the solution is easy after all, well, I guess you screwed up twice. Go to step 1. 

Yes. Let me write $\Sigma$ for the surface on which $G$ and $G^*$ are embedded. Because the cycles $C_1$ and $C_2$ are homotopic, they are also in the same $\mathbb{Z}_2$-homology class. So by definition, the symmetric difference $C_1\oplus C_2$ is the boundary of the union of some subset of faces of $G^*$; call this union of faces $U$. (In fact, either $U$ or its complement $\Sigma\setminus U$ must be an annulus, but this isn't important.) Because $C_1$ and $C_2$ are disjoint, the symmetric difference $C_1\oplus C_2$ is equal to the union $C_1\cup C_2$. In particular, we have $C_1\oplus C_2\ne \varnothing$, which implies that both $U$ and its complement $\Sigma\setminus U$ are non-empty. In other words, the subsurface $\Sigma \setminus (C_1\cup C_2)$ is disconnected. Any path in $G$ can be viewed as a path in $\Sigma$ that avoids the vertices of $G^*$, and vice versa (up to homotopy). Thus, the (graph) components of $G\setminus (E_1\cup E_2)$ correspond bijectively to the (surface) components of $\Sigma \setminus (C_1\cup C_2)$. We conclude that $G\setminus (E_1\cup E_2)$ is disconnected. The assumption that $\Sigma$ is orientable is never used. 

Label the edges of $F$ alternately clockwise and counterclockwise. Edelsbrunner and Preparata further prove that if a separating triangle exists, then there is a separating triangle whose edges are collinear with clockwise edges of $F$. In $O(n)$ additional time, we can find the (necessarily clockwise) edge of $F$ first hit by a ray from each clockwise edge $e$; call this edge the "successor" of $e$. The successor pointers partition the clockwise edges into cycles; if there is a separating triangle, one of these successor cycles has length 3 (and none have length more than 4). See the original paper for more details: 

The definitions of "polynomial time" and "exponential time" describe the limiting behavior of the running time as the input size grows to infinity. On the other hand, any physical experiment necessarily considers only inputs of bounded size. Thus, there is absolutely no way to determine experimentally whether a given algorithm runs in polynomial time, exponential time, or something else. Or in other words: what Robin said. 

You're describing the classical Bentley-Saxe logarithmic method, applied to static sorted arrays. The same idea can be used to add support for insertions to any static data structure (no insertions or deletions) for any decomposable searching problem. (A search problem is decomposable if the answer for any union $A\cup B$ can be computed easily from the answers for the sets $A$ and $B$.) The transformation increases the amortized query time by a factor of $O(\log n)$ (unless it was already bigger than some polynomial in $n$), but increases the space by only a constant factor. Yes, it can be deamortized, thanks to Overmars and van Leeuwen, but you really don't want to do that if you don't have to. These notes cover the basics. Cache-oblivious lookahead arrays are the mutant offspring of Bentley-Saxe and van Emde Boas trees on steroids. 

Now replace every vertex of degree $k$ with a hoard, connected to a horizontal $k$-splitter and a vertical $k$-splitter. For each edge $(i,j)$, place constant number of corners aligned with the splitters for hoard $i$ and hoard $j$. It's easy to arrange for the corners for different edges to lie at distinct $x$- and $y$-coordinates. Place the white king inside one of the hoards, in the position shown by the white circle above. 

I think Guibas and Stolfi's “edge algebra” formalism is a bit unnecessary. All that's really necessary is to remember the distinction between primal and dual graphs. Each face $f$ of the primal graph has a corresponding dual vertex $f^*$; each edge $e$ of the primal graph has a corresponding dual edge $e^*$; and each vertex $v$ of the primal graph has a corresponding dual face $v^*$. Primal edges connect primal vertices and separate primal faces; dual edges connect dual vertices and separate dual faces. The dual of the dual of anything is the original thing. See Figure 4 in Guibas and Stolfi's paper: 

Well, sure, it's possible. But since we basically don't know how to prove that anything is hard, it doesn't look easy. In the mid 90s I proved an $\Omega(n^2)$ lower bound for 3SUM in a variant of the linear decision tree model; my proof was later simplified by Ailon and Chazelle. This model of computation has nothing corresponding to "memory"; the running time of an algorithm is defined simply as the worst-case number of (generalized) comparisons. So adding a cache won't break these lower bounds; to beat $O(n^2)$ time, you have to do some non-obvious computation with the data in the cache. 

My lecture notes on computational topology include some basic material on topological graph theory. They don't come anywhere close to the completeness of Archeacon's survey (or Giblin's book, Mohar and Thomassen's book, or Gross and Tucker's book), but you may still find them useful. (Start with "Curves on Surfaces".) If you're a student, you may also find your local university library useful. Most mathematics libraries have copies of all the books I mentioned, and they will actually let you borrow them for free, possibly for several months at a stretch. Sadly, these books may only be available in a legacy format that is difficult to search and copy from, but hey, you get what you pay for. 

My half-baked idea was a little too ambitious. I'm including it below for reference, but the distance condition I specified is not actually sufficient to guarantee large girth. There are arbitrarily large highly symmetric surface maps with large girth, but published existence proofs are largely based on group theory rather than topology or geometry per se. Specifically, for any integers $g$, $d$, and $r$ such that $1/g + 1/d < 1/2$, there is a regular surface map in which every face has $g$ edges, every vertex has degree $d$, and every non-contractible cycle on the surface crosses at least $r$ edges. Here "regular" means both that every vertex has the same degree and that for any pair of directed edges, there is an automorphism of the embedding that sends directed edge to the other. Setting $r$ large enough in this construction guarantees that the girth of the graph is $g$. See, for example: 

I'm pretty sure no such book exists. I drew up an annotated bibliography for my recent course, which was loosely based on Erik's course at MIT. It's definitely incomplete—I covered very few geometric data structures and no text data structures, for example—but you might still find it useful. 

Joseph Y-T. Leung, Tommy W. Tam, C. S. Wong, Gilbert H. Young, and Francis Y. L. Chin. Packing squares into a square. J. Parallel and Distributed Computing 10(3):271–275, 1990. 

Most people cite Euler's 1741 "Bridges of Königsburg" paper as the oldest graph algorithm. Unfortuantely, Euler doesn't actually describe his algorithm in detail, but only gives a half-hearted example: 

If you work on a university campus, there's likely a large building nearby full of study carrels and magazine racks called a "LIE-BURY". Deep in the bowels of such buildings, one can often find images of old research papers inscribed onto thin sheets of cellulose fiber extracted from wood and cotton, called "paper", and then bound into larger brick-sized objects called "books". (At least according to Wikipedia, this is the reason we call our research reports "papers". Who knew?!) Surprisingly, despite the crudeness of the technology that produced them, these so-called "books" are often significantly easier to read than their more modern electronic counterparts. One serious disadvantage of this approach is that one must actually walk all the way across campus, but we all must occasionally suffer for our art. 

If you are restricted to equality tests, an $\Omega(n^2)$ lower bound follows from a straightforward adversary argument. 

Although it's true that Galois' algorithm doesn't run in polynomial time, Galois clearly meant something much less precise. This is also the oldest reference I know of that considers the mere existence of an algorithm significant in its own right. 

This sounds like a perfect problem for a GPU. For each site (a,b), render the paraboloid z^2 = (x-a)^2 + (y-b)^2 into an nxn Z-buffer. At the end of the loop, the z-buffer contains the results you want. 

Finally, knowing these functions tell you absolutely everything about the topology of the subdivision, and any polygonal subdivision of any surface (orientable or not) can be encoded using these three functions. The quad-edge data structure is a particularly convenient representation of a surface graph that provides access to all these functions, along with several other constant-time operations like inserting, deleting, contracting, expanding, and flipping edges; splitting or merging vertices or faces; and adding or deleting handles or cross-caps. Have fun! 

The "convex skull" problem is to find the maximum-area convex polygon inside a given simple polygon. The fastest algorithm known for this problem runs in $O(n^7)$ time [Chang and Yap, DCG 1986]. 

Cycle cancellation is a standard textbook algorithm for computing minimum-cost circulations: As long as the residual graph of the current circulation contains a negative cycle, push as much flow along that cycle as possible. I'm trying to understand the worst-case behavior of the generic algorithm. For purposes of analysis, I want to assume that at each iteration, the negative cycle to augment is chosen adversarially. If all capacities and costs are integers, each iteration decreases the cost of the current circulation by at least 1. Thus, the algorithm halts after augmenting at most $-\$(f^*)$ cycles, where $\$(f^*)$ is the cost of the final circulation. Moreover, standard bad examples for Ford-Fulkerson and the standard reduction from maximum flow to minimum-cost circulation imply that $-\$(f^*)$ iterations are necessary in the worst case. If we allow irrational capacities, then standard bad examples of Ford-Fulkerson and the standard reduction from maximum flow to minimum-cost circulation imply that the algorithm may never terminate, and worse, may not even converge to an approximate minimum-cost circulation in the limit. What is known about the case of integer capacities and irrational costs? I have found nothing in the literature about this case. Cycle cancelling must terminate after a finite number of iterations in this case, because there are only a finite number of feasible integer circulations, but that argument only implies the upper bound $O(U^E)$, where U is the maximum edge capacity. Zadeh's classical bad examples for network simplex (and I suspect similar bad examples for Ford's generic shortest-path algorithm) imply an $\Omega(2^V)$ lower bound, even if the most negative cycle is canceled at each iteration. Are any tighter bounds known? In particular: What is known about the case of unit capacities and irrational costs? 

Ziegler defines the fatness and complexity of a cell decomposition $X$ of the 3-sphere (for example, a 4-dimensional convex polytope) as follows: $$ F(X) = \frac{f_1 + f_2 - 20}{f_0 + f_3 - 10} \quad\text{and}\quad C(X) = \frac{f_{03} - 20}{f_0 + f_3 - 10}. $$ Here, $f_i$ denotes the number of $i$-dimensional cells (0=vertices, 1=edges, 2=polygons, 3=chambers), and $f_{03}$ is the number of incidences between vertices and chambers. (The -10 and -20 terms simplify some inequalities.) Your question is equivalent to asking whether $C(X)$ is always less than some fixed constant. Ziegler observes that the generalized Dehn-Sommerville relations (a particular generalization of a generalization of Euler's formula) imply that $$ C(X) \le 2F(X) -2 \quad\text{and}\quad F(X) \le 2C(X) - 2. $$ So equivalently, you are asking whether there is an upper bound on the maximum possible fatness of a 3d cell complex. In fact, there is no such upper bound. Eppstein, Kuperberg, and Ziegler construct, for any integer $n$, a cellular 3-sphere $X_n$ with $O(n^{12})$ vertices and chambers but $\Omega(n^{13})$ edges and polygons, so $C(X_n) = \Omega(n)$. Moreover, it is an open question whether $C(X)$ is bounded for convex 4-polytopes. Eppstein, Kuperberg, and Ziegler also construct an infinite family of 4-polytopes $X$ such that $F(X)>5$ and thus $C(X)>3.5$; these are the fattest 4-polytopes known. On the other hand, it is known that fatness cannot grow arbitrarily quickly; specifically, any $n$-vertex cell decomposition has fatness $O(n^{2/3})$, and any $n$-vertex convex 4-polytope has fatness $O(n^{1/3})$. 

Here I-I means you should move the middle brick from any 3-layer the top, II- means you should move a side brick from a 3-layer to the top, -I- means you should move a side brick from a 2-layer to the top, and the bob-omb means you should think about death and get sad and stuff. If there's more than one suggested move in a box, you can choose any one of them. It's trivial to execute this strategy in $O(1)$ time if you already know the triple $(m,n,t)$, or in $O(N)$ time if you don't. Moral: Jenga is only fun if everyone is clumsy and/or drunk.