First of all, learning other programming languages is always a good thing. There are many reasons for this: 

Different languages have different libraries and different communities of users that use them for different purposes. Most people doing machine learning aren't working primarily in JavaScript. Some are working in Python, some are working in R, some are working with Spark using Scala or Python, etc. 

HDP is an extension of LDA, designed to address the case where the number of mixture components (the number of "topics" in document-modeling terms) is not known a priori. So that's the reason why there's a difference. Using LDA for document modeling, one treats each "topic" as a distribution of words in some known vocabulary. For each document a mixture of topics is drawn from a Dirichlet distribution, and then each word in the document is an independent draw from that mixture (that is, selecting a topic and then using it to generate a word). For HDP (applied to document modeling), one also uses a Dirichlet process to capture the uncertainty in the number of topics. So a common base distribution is selected which represents the countably-infinite set of possible topics for the corpus, and then the finite distribution of topics for each document is sampled from this base distribution. As far as pros and cons, HDP has the advantage that the maximum number of topics can be unbounded and learned from the data rather than specified in advance. I suppose though it is more complicated to implement, and unnecessary in the case where a bounded number of topics is acceptable. 

If you already know JavaScript, then learning Python shouldn't be too hard for you, and it has some great machine learning libraries like scikit-learn, with lots of great resources online to help you learn. So I'd recommend starting there. But I also recommend that you eventually branch out and learn some languages that are further outside your comfort zone. 

Event without the other variables, it's already going to be able to fit the data better than because, instead of forcing the price of the single-floor houses to $0, it would be able to fit it to the average price of all the single-floor houses in your data set - which is the best you can do for those houses until you add other variables besides . (Actually, it wouldn't be forced to $0 if you were to include a constant term in your model, but the constant term that best lets you fit the linear portion of the data still probably isn't what you want for the single-floor houses, some of which - as you see - are quite expensive). 

Learning new languages makes it easier for you to learn other new languages in the future - and sooner or later in your career you will almost certainly want to or need to. Learning new languages helps you understand the language you're already working in. If I told you JavaScript is a dynamically-typed language, would that mean anything to you? Probably not so much if you've never used a statically-typed language. If I told you it has first-class functions, would that mean anything to you? It's like telling someone "night is dark" when they've never seen daylight. Learning new languages teaches you about ideas from other languages that may eventually make it into your language. ECMAScript 6 added "arrow functions" to JavaScript, these are also known as lambdas and would already be familiar to you if you've used C# (from version 3 onwards), Java (from version 8 onwards), Scala, Haskell, or many other languages. Learning new languages teaches you different ways to think about code. And this can only make you a better coder. Learning new languages increases your employment opportunities. Learning new languages is fun! 

The IDF part of TF-IDF gives less weight to a word if it occurs in a large fraction of the documents in your corpus. However, this doesn't necessarily mean that the word is unimportant for distinguishing your two classes. A word which is common in your corpus, but which also occurs substantially more often in one class than the other, could very well be quite valuable in distinguishing the classes. This can especially be true if your set is not balanced between the two classes. For example: Suppose that 95% of negative reviews contained the term "boring", while only 5% of the positive reviews contained "boring". If this occurred in a balanced set of negative and positive reviews, that means that half the total reviews would contain "boring", and this might be one of the terms whose weight is more strongly suppressed by IDF, despite the fact that it's obviously a valuable term for distinguishing the classes. If instead 90% of the reviews are negative (with "boring" again occurring in 95% of negative reviews and 5% of positive ones), then a full 86% of your documents contained the term, and its weight might be highly suppressed by IDF despite its obvious importance. In essence, the problem is that TF-IDF is agnostic to the class labels in your training set. One alternative to TF-IDF for text classification that doesn't have this flaw is Bi-Normal Separation (BNS), see for example this paper: (link to PDF) 

Yes, this is guaranteed by the Moore–Aronszajn theorem. $K(\mathbf{x},\mathbf{y}) = e^{-\|\mathbf{x} - \mathbf{y}\|}$ is a positive definite kernel. This means it is a symmetric function satisfying $$\sum_{i,j=1}^{n} c_i c_j K(\mathbf{x_i},\mathbf{x_j}) \ge 0$$ for all $n \in \mathbb{N}$, all $\mathbf{x_1},\dotsb ,\mathbf{x_n} \in \mathbb{R}^n$, and all $c_1, \dotsb , c_n \in \mathbb{R}$. The Moore–Aronszajn theorem says that for each such function there exists a unique reproducing kernel Hilbert space with reproducing kernel $K(\mathbf{x},\mathbf{y})$. That is, there must be a unique Hilbert space $H$ of functions $f: \mathbb{R}^n \to \mathbb{R}$ with a mapping $\phi: \mathbb{R}^n \to H$ such that $K(\mathbf{x},\mathbf{y}) = \langle \phi(\mathbf{x}), \phi(\mathbf{y}) \rangle$ for all $\mathbf{x}, \mathbf{y} \in \mathbb{R}^n$, and such that $K(\mathbf{x},\mathbf{y})$ satisfies the reproducing property: $$\langle f, K(\cdot, \mathbf{x}) \rangle = f(\mathbf{x}) \qquad \forall \mathbf{x} \in \mathbb{R}^n, \enspace \forall f \in H$$ While the reproducing kernel Hilbert space is unique for a given positive definite kernel $K(\mathbf{x}, \mathbf{y})$, there can be other mappings $\phi$ to other Hilbert spaces which also satisfy $K(\mathbf{x},\mathbf{y}) = \langle \phi(\mathbf{x}), \phi(\mathbf{y}) \rangle$ but without the reproducing property. 

You could try including "Does not have a 2nd floor" as a separate categorical variable, which you could encode as 1 or 0 in your linear model. Something like: 

The columns of your matrix are a basis of orthonormal eigenvectors. This is an important concept from linear algebra, and well worth learning about in detail if you're not familiar. But for the purposes of this answer it can be understood as defining a system of coordinates. For each student, we can define a point in a four-dimensional space (specifically, in $\mathbb{R}^4$) which represents their grades (after centering and normalization). Or to put it another way, you can imagine the set of all students' grades as a scatterplot in four dimensions, with four perpendicular axes. We can orient these axes in various directions (just as we can in two or three dimensions). The most obvious choice is to have one axis for each subject, so the axis which is collinear with the unit vector pointing from the origin to the point $(1,0,0,0)$ represents their grade in German, and likewise the axis which is collinear with the vector $(0,1,0,0)$ represents their grade in Philosophy, the axis which is collinear with the vector $(0,0,1,0)$ represents their grade in Math, and the axis which is collinear with the vector $(0,0,0,1)$ represents their grade in Physics. However, there's no reason to expect that the direction in which our scatterplot is most spread out (the direction of greatest variance in the data) will align with one of these axes. PCA picks out a new set of axes so that one axis aligns with the direction of greatest variance, and another aligns with the direction of the greatest remaining variance after the first direction is projected out, and so forth. The unit vectors (expressed in the original coordinate system) which point along these new axes are the columns in your matrix. In the case of this particular example, the loading vector for the first principal component is along an axis that basically expresses whether they're better at Math and Physics, or better at German and Philosophy. The loading vector for the second principal component is along an axis that basically expresses how good or bad a student they are over all (hence all the components of the vector have the same sign and similar magnitude). You wondered about the negative sign on all four components - if you're familiar with eigenvectors you'll know that changing all components of the vector by an overall sign is irrelevant. Basically, it's the same as just swapping which end of the axis we call positive and which we call negative. So in this case the first two loading vectors are fairly close to what many of us might have expected to see. But even in this fairly intuitive example, you shouldn't be surprised that the loading vectors for the later principal components don't seem as obvious to you. That's because these are only addressing the variance that remains after we project out the variance that's explained by the first two factors. We all probably know that students who are good at Physics tend to be good at Math, but how many of us know (for example) if, after controlling for how good they are at Physics, the ones who are also better at Philosophy than German will be better at Math? These subtler effects will be less obvious to a causal observer than the dominant effects. Once you get to the loading vector for the fourth principal component (out of four), you really don't need to wonder at all about why it has the particular value that it has. In fact, this vector was entirely determined by the previous three (up to the irrelevant overall sign) . This can be understood by remembering that PCA picked out four perpendicular axes in a four-dimensional space - once the first three are specified, there's only one remaining possible choice that's perpendicular to all of them.