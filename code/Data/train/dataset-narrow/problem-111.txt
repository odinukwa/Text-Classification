'monitor start messages' is roughly equivalent to 'term mon. 'messages' is the name of the log file you want to monitor. If you don't see the output you expect, you need to change what is logged to given file from 'set configuration system syslog file X ...'. 

I think your configuration is quite dangerous and you seem indecisive if you are using 'enable/line' or 'local' as fallback, correct answer is local, never use 'enable' and especially never 'line' for anything (line is two-way 'encrypted' not one-way hashed). I would recommend this configuration instead: 

But this probably is not even doing what you want to do, are you really interested in the order? I don't think BGP enforces any particular ordering in communities, RFC1997 just allows manipulation without specifying order. And Cisco seems to order communities in numerical order in presentation (neighbor sends me community 1:42 3:42, I add 2:42 and communities shown are '1:42 2:42 3:42' instead of what you maybe expect '1:42 3:42 2:42'. That is, your denying of community 65000 in the beginning probably does not work. If you explain your application, maybe better solution can be found. When it comes to communities in different platform, what @Teun implied is that they are quite different. Juniper had brilliant idea to use ASN as atom in their as-path regexp, normally atom is character (or byte, if you don't support UTF8). Juniper solution is clearly superior solution, if you think about character classes, say you want to match private ASN in JunOS, simple as [64512-65534]. If atom is normal character, like in IOS and Quagga, it's much more awkward. 

You should run 'port-fast' (in standard terms edge port) in every port not part of your switch core. Even if it is switch. You should NOT have L2 loop through customer switches. You should run BPDUGuard and BUM policers all interfaces, customer facing interfaces should be 1/5th or less of core facing limits. Unfortunately limiting unknown unicast often is not supported. Why running 'port-fast' or edge is crucial is performance of RSTP (and by extension MST) rely on it. How RSTP works is it asks downstream if it can go to forwarding mode, and downstream asks its downstreams until there are no more ports to ask frmo, then the permission propagates back up. Port-fast or edge port is implicit permission from RSTP point-of-view, if you remove this implicit permission, explicit permission must be gotten otherwise it'll fall back to classic STP timers. Which means even one non-portfast port will kill your subsecond RSTP converge. 

How to maximize available buffers Cisco Catalyst 3750 QoS Configuration Examples Configuration Guide 

multipath-relax is needed as normally you'll only multipath with same as-path routes. I'm personally strongly against carrying default routes via dynamic routing-protocols, there simply isn't any need, as long as you operate platform which supports recursive static routes. Consider your operator router gets disconnect from their core, then you still receive the default route and your traffic is blackholed until you manually intervene. However if they'd send you some candidate route, perhaps their own PA block or maybe few PA blocks crucial to you, you could add static default routes towards these networks, if edge box is disconnected from core, the aggregate network is gone, and you can converge without manual intervention. 

Remember to set 'accept-remote-nexthop' under BGP settings, so that the next-hop can be changed to something else than link address. I also strongly support that providers start to support different blackhole communities than just 'full blackhole'. Especially if you operate in more than one country usually attack is from transit and often customer actually mostly wants to access domestic peerings, so it's useful to implement several levels of blackhhole: 

There is no reason to limit customer to blackhole only /32, allow them to blackhole anything from them. Something like this: 

B will distribute the prefix+label to A, using LDP A will allocate local label for this, say 400 (could be anything) 

Ethernet standard officially allows 10^-12 bit-error-rate, while in practice the hardware meet much better BER than which standard demands. You should also be able to bing for 'SQA' (Service Quality Assurance) or 'SLA' (Service Level Agreement), some companies publish them, you could use them to check what your competitors are offering and offer something to that level. Our SQA states to customers that 0.02% is minor fault (we will fix if ticket is opened), which I think is quite large packet loss for fibre connection, but same SQA covers also DSL so we didn't want to be too aggressive with it. So far this has been sufficient to customers, but we are prepared to reduce the number if it is hurting sales. There are several bingable tools online, where you can check how much packet loss hurts TCP, which can be useful information when deciding what is acceptable loss for your application/product: 

When gc_thresh3 tries to exceed, it tries to force garbage collection run, unless it has been already ran recently. Garbage collect appears to delete entries which are not referred to anymore, so does not mean oldest or newest, however gc_staletime exceeding seems to be one way of dereferencing entry, which again translates to oldest entry. If garbage collect cannot be ran, new entry is simply not added. All these gc_threshN and periodic garbage collect intervals can be tuned. The code is address family (ipv4, ipv6) agnostic, so IPv6 ND and IPv4 ARP tables are handled by exact same code-path, not duplicate path. 

For IOS and JunOS and other commercial stacks you just have to test, it's not very hard luckily. But for linux, freebsd, netbsd, openbsd, uIP, lwIP and probably many other implementations you can just check their source code for the behavior. In Linux you need to check 'net/core/neighbour.c' (start with line 'if (entries >= tbl->gc_thresh3' ||') and 'net/ipv4/arp.c'. In Linux you seem to have three full levels 

You could create dummy VRF and associate the pool to that VRF. Not exactly clean solution, but seems to satisfy your requirement. 

I would never run anything without CEF and it is probably only because of legacy reasons choice of disabling it even exists. Consider Juniper, they don't have specific term they use to describe same concept, as they do not need to differentiate it from some alternative method, as such inferior alternative does not exist. 

This is purely random example, you should decide how many classes you need (I recommend as few as possible, and only add classes when you absolutely you know you must, start with 2) and you should decide how to distribute the guaranteed share of them. 'bandwidth percent X' is burstable, 'priority percent X' usually is not (should be accompanied with policer). Good staring point would be two classes, both burstable (bandwidth percent X), some high priority traffic class and rest (class-default). Then you'd only need to decide what traffic to put in high priority class, and what their guaranteed share of the 300Mbps would be. 

This should display routes passing the policy There isn't direct way to ask the opposite, but you could create policy which calls 'PS-Filter-Prefix-Size' and then rejects and accepts rest. Testing this policy would yield opposite results, showing routes not passing 'PS-Filter-Prefix-Size'. As far as I understand your 'PS-Filter-Prefix-Size' does nothing, as there is no action, so existing 'default-action' is honored. You should have 'then accept' or 'then reject', and preferably after that explicit catch-all rule. In your situation I'd do something like this 

You shouldn't rely on default metrics/cost, not in IGP or STP. My one question review if LAN was designed is to ask which link is blocking, if engineer can't answer without looking, it's strong indication the network was not designed. You should design how you want traffic to flow under normal operation and under fault scenarios and then design metric standard with satisfies those requirements. I prefer to do this with pen and paper add coffee if so inclined. I greatly prefer role based metric standard, where P-P, P-PE, PE-PE etc each have same metric value, regardless of bandwidth or latency. That is, I know where I want my traffic to go, to me it does not matter what the latency is or what the capacity is, if I don't have enough capacity where I want the traffic to flow, I'll upgrade the links. Role based certainly isn't only way to do it, if you want to base your metric standard on link speed or measured latency, go ahead. Regardless how you design your metric standard it will be fully static and unaware of actual link conditions such as packet loss, latency, jitter, capacity etc. If you want network to more dynamically respond (demand which I tend to think often rises from poor planning) to changing network conditions, you need to look into RSVP or even SDN.