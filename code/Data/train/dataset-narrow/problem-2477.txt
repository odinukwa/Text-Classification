I found another reference that goes through a detailed proof of the decidability of typechecking for systems of dependent types up to the CIC: Chapter 2 of Advanced Topics in Types and Programming Languages: Dependent Types, David Aspinall & Martin Hofmann. As you probably know, the proof of decidability is conditional on decidability of $\beta$-equality, which itself is implied by the normalization of the calculus. The proof of that statement is significantly more difficult, partly because it implies consistency of the logical system. 

This is because you need the property of $\Pi$-injectivity $$ \Pi x:A.B=_{\beta\eta}\Pi x:A'.B'\ \Leftrightarrow\ A=_{\beta\eta}A'\wedge B=_{\beta\eta} B'$$ in order to prove inversion, which is required to prove preservation/subject reduction. So you can't even prove that $\beta\eta$-reductions preserve types without confluence, but confluence doesn't even hold on untyped/ill-typed terms! Breaking out of this vicious circle requires some technical tricks, which are hard to summarize here, but arguably the simplest to understand is to simply stop being interested in $\eta$-reductions, but instead concentrate on $\eta$-expansions: $t\rightarrow_{\eta*}\lambda x:A.t\ x$ Of course, you need to restrict this rule to non-$\lambda$ and non-applied terms to even hope to get termination, but with these restrictions it seems that the reduction behavior is much better behaved, and the meta-theory works out without too many problems. A good reference seems to be Neil Ghani, Eta-Expansions in Dependent Type Theory. A different, and recently quite popular approach, is described by Abel, Untyped Algorithmic Equality for Martin-Löf's Logical Framework with Surjective Pairs. 

There have been recent developments in dependent type theory which relate type systems to homotopy types. This is now a relatively small field, but there is a lot of exciting work being done right now, and potentially a lot of low hanging fruit, most notably in porting results from algebraic topology and homological algebra and formalizing the notion of higher inductive types. 

I'm not sure there is a name for this specific property, though I would say "All right-hand sides are in head-normal form". To be honest, this seems like a very strange property to request, especially since an inner reduction may provoke a head reduction, like so: $$ {\cal R} = \{a \rightarrow b, f(b)\rightarrow f(a)\}$$ (I'm using letters for function symbols instead of numbers, as is traditional). In this case $f(a)$, the right hand side of the second rule, does not match any rule at it's head, but it may match the second rule if you apply the $a\rightarrow b$ reduction inside. 

This result is probably a bit recent to qualify as fundamental, but I believe that the types-as-homotopy-types interpretation qualifies. This view allows interpreting types from constructive type theory as sets with certain geometric properties, in this case homotopy. I find this point of view to be particularly beautiful as it makes certain previously complex observations about type theory simple, for instance the fact that"axiom K" is not derivable. An overview of this budding field by Steve Awodey can be found here. 

A first observation in the direction of decidability of ODEs is this paper by Avigad, Clarke and Gao, which classifies the complexity of $\delta$-decidability, in which solutions are to be found within a certain bounded error (the "delta") in one direction. one of the main results is that $\delta$-solvability of (Lipschitz-continuous) ODEs is $\mathrm{PSPACE}$-complete. 

In your second example, things are a bit more tricky, as you have somethings along the lines of $$ \mathrm{Bad} = \mathrm{Bad}' \rightarrow A$$ where $\mathrm{Bad}'$ is related, but not equal, to $\mathrm{Bad}$ (in your case they are equal to $\mathrm{Bad}\ a$ and $\mathrm{Bad}\ (\mathrm{Not}\ a)$ respectively). I'll admit that I could not build a straightforward isomorphism between the two. The same problem is present if you replace 

And that's it! The fact that $\lambda x y. x+y$ and $\lambda x y. x\cdot y$ are two very computationally meaningful proofs of that statement does not factor at all into the statement as it stands. 

Proof: Exercise. Note that this nice property is not true for well-founded orders in general! In this sense, well-quasi orders are much more "stable". It's often the case that a stronger property is better behaved, even if the weaker property is what we want to prove at the end (well-quasi orderdness vs well-foundedness in this case). Since every simplification order contains the homeomorphic embedding, it is indeed a WPO. Your more precise statement is correct: we could directly prove that a simplification ordering is a WPO by re-producing the proof for the homeomorphic embedding, but this is not necessary because of the above statement, which shows that it suffices to prove the statement for the "minimal" such ordering. 

There is a really interesting approach to a set theory-like foundational system that I am rather fond of: Grue's Map Theory. The basic idea is to take the (untyped!) $\lambda$-calculus as a base foundation, and to represent a set $S$ as a term $f$ such that $$ S=\{f(x)\mid x\in\Phi\}$$ where $\Phi$ represents the well founded functions ($x\neq\bot$ in the Scott semantics). It doesn't look like there is any proof assistant based on it though, so I have no idea how it would work in practice. 

This is going to be a somewhat incomplete answer, since you are asking some pretty broad questions about the applications of the techniques. First let me start by saying that while the research in the field of equational logic and completion hasn't seen a complete revolution since 1980 (as compared to, say SMT) there have been substantial improvements, so bear in mind that techniques and tools have changed. My first recommendation would be to take a look at Term Rewriting and All That by Baader and Nipkow, which is slightly more up to date (but still almost 20 years old!). You might also want to check CiME as an implementation of some of these techniques. The canonical set from the first example of the appendix is actually $\begin{eqnarray}R1 &:& 0+X\rightarrow X\\R2&:&I(X)+X\rightarrow 0\\R3&:&(X+Y)+Z\rightarrow X+(Y+Z)\\R8&:&I(0)\rightarrow 0\\R11&:&X+0\rightarrow X\\R12&:&I(I(X))\rightarrow X\\R13&:&X+I(X)\rightarrow 0\\R14&:&X+(I(X)+Y)\rightarrow Y\\R17&:&I(X+Y)\rightarrow I(Y)+I(X)\end{eqnarray}$ For the life of me I can't understand why they adopted additive notation for non-commutative groups. Note that the set $\{R1,R2,R3\}$ is not canonical, since e.g. $X+0$ does not reduce to $0$ (but $X+0=0$ is derivable in the equationnal system). The canonical system was derived from the first 3 rules by application of Kuth-Bendix completion, and nothing else, $\varepsilon$-unification is not needed in this case. There are many papers that expand on this example, including, I think, the original paper which is sadly tough to find online. The wikipedia page is a good place to fish for references. I'm not sure what advice to recommend for using these techniques in practice. I gave a reference to an existing tool, there are many others in various states of maintenance, see e.g. here. I would also suggest taking a look at first-order theorem provers with equality and equality logic provers, e.g. Otter & Mace since they integrate these kinds of techniques and might be more suited to your needs, if you're trying to prove basic theorems in some algebraic theory. I really suspect you might want some more specialized tools for Gröbner bases if you're trying to prove theorems about fields, since these are much more specialized (and much more powerful). Perhaps some people more familiar with computer algebra can point you in the right direction for this. 

Undecidability of FOL follows, as proving $$ \vdash \phi_1\wedge\ldots\wedge\phi_n\rightarrow \exists y,\psi(\overline{n},y)$$ is equivalent to determining wether the machine with index $n$ halts. A detailed description of this proof is given in chapter 4 of Avigad's lecture notes on computability. 

In contradiction with Gurkenglas' answer, there actually is a community of scientists who work on proving non-termination of programs in various language and formalisms. An obvious approach would be to check for looping non-termination: for a given program $w$, pick an input $x$ and check to see if the same state is reached twice with the same data. Non-looping non-termination is obviously more complex, but has been studied, e.g. in Emmes & al Proving Non-Looping Non-Termination Automatically or Endrullis & al Proving Looping and Non-Looping Termination by Finite Automata. 

In this answer I mention a paper by Geuvers in which he describes a class of models for a type theory $\lambda P_2$ which is a sub-system of the CoC and roughly corresponds to 2nd order predicate logic. He goes on to describe a class of realizability models, which are based on Weakly Extensional Combinatory Algebras (WECAs). In a couple of lines, the model involves a WECA $\cal A$ and a set ${\cal P}\subset {\mathscr P}({\cal A})$, closed under arbitrary intersections and closed under the realizability product: $$\prod_{t\in X}F(t):= \{a\in{\cal A}\mid \forall t\in X,\ a\cdot t\in F(t)\} $$ where $F:X\rightarrow {\cal A}$ is an arbitrary function. One can then define the interpretation $[\![\_]\!]$ from types in $\lambda P_2$ to elements of $\cal P$: $$[\![\alpha]\!]_{\xi\rho}:= \xi(\alpha)$$ $$[\![\Pi\alpha.\tau]\!]_{\xi\rho}:=\bigcap_{A\in{\cal P}}[\![\tau]\!]_{\xi[\alpha:=A]\rho} $$ $$[\![\Pi x:\sigma.\tau]\!]_{\xi\rho}:= \prod_{a\in[\![\sigma]\!]}[\![\tau]\!]_{\xi\rho[x:=a]}$$ $$[\![P\ t]\!]_{\xi\rho}:= [\![P]\!]_{\xi\rho}((\!|t|\!)_{\xi\rho})$$ $$[\![\lambda x:\sigma. P]\!]_{\xi\rho}:= a\in[\![\sigma]\!] \mapsto [\![P]\!]_{\xi\rho[x:=a]}$$ Where $\xi$ and $\rho$ are appropriate valuations, and $(\!|\_|\!)$ is the interpretation of terms into elements of the WECA $\cal A$. I'm omitting a fair number of details, e.g. the interpretation is actually over type constructors and not only types. My question is pretty straightforward: is this class of models complete for $\lambda P_2$? Is the "obvious" generalization complete for CoC? For CIC? Are there any references for this? The interpretation of the "large" product is restricted to be intersections, which may make it too weak a notion of models to be complete, but the choice of WECAs seems to make it a powerful notion of models. In Pitts' paper Polymorphism is Set-Theoretic, Constructively, it is shown that every model of system $F$ can be embedded in a topos, and so a rather general notion of topos models is indeed complete for system $F$. So my follow up questions are: Do these topos models generalize the class of realizability models described above? Does the theorem generalize to $\lambda P_2$? I suspect the answer is yes, but I'd kind of like to see the details worked out. 

The question of truth in Presburger Arithmetic with bounded quantifier alternation has been answered with quite some precision by Reddy and Loveland: C.R. Reddy & D.W. Loveland: Presburger Arithmetic with Bounded Quantifier Alternation. The paper may be found here (sorry for the ugly link). Their main result is stated as follows: 

As a direct answer to your question, the "statement" $\mathbb{N}\Rightarrow\mathbb{N}\Rightarrow\mathbb{N}$ should be read in English as something like 

The ACL2 theorem prover seems quite close to what you want, though it is hard to know exactly what the strength of the system is. From the logical description here it looks like the base is a quantifier-free equational logic with induction principles, which include principles allowing the definition of PR functions. This gives exactly PRA, but in addition, the standard library defines induction up to $\varepsilon_0$, which brings the strength up to full Peano Arithmetic. You might therefore want to work in ACL2 with a restricted version of the standard induction principle (I don't know how hard that is). 

There are absolutely some relationships between the semantics and practice of OOP and category theory. This is somewhat unsurprising since both fields attempt to give a principled generic account of structure and behavior in a synthetic manner. The most apparent work I am aware of is the categorical semantics of UML, which is admittedly different from OOP in the large, but I think captures much of the crux of the debate on the semantics of objects themselves. One example is Zinovy Diskin's Mathematics of UML. 

A proof that is computationally intensive or untrusted for some reason (long, complex). A result that is relevant to undergraduate mathematics. A result that is useful to program verification. 

Where and are inductive types with 1 and 0 constructors, respectively. Notably, this allows you to prove the proposition $0 \not= 1$, which cannot be proven without this ability. As noted in another answer ($URL$ a good resource that explains this is the following github repo: $URL$ 

In this case, you could build a looping combinator in exactly the same manner as before. I suspect you can carry a similar (but more complex) construction using 

Finally, references for the proof theory of type systems: there's really a gap in the literature here I think, and I would relish a clean treatment of all these subjects (in fact, I dream of writing it myself some day!). In the meantime: 

Now define $I=\forall X.(F(X)\rightarrow X)\rightarrow X$. It is pretty clear how to build $fold$: just take $$fold=\lambda i:I. i\ A\ \alpha : I\rightarrow A$$ Building $in$ is a bit more tricky, Wadler explains it, so I won't try to. Note however that it requires the fact that $F$ is a functor, which can be seen as a positivity requirement. Now in category theory, we often want to consider the situation in which all arrows are reversed. In this case, given $F$, we can consider the category of $F$-coalgebras with 

This theorem can be proven in $\mathrm{PA}$, and so we have $$\mathrm{PA}\vdash F\mbox{ is normalizing}\Rightarrow\mbox{$\mathrm{PA}_2$ is consistent} $$ and Gödel's argument applies (and $\mathrm{PA}$ cannot prove normalization of system $F$). It's useful to note that the reverse implication holds as well, so we have an exact characterization of the proof-theoretic power needed to prove normalization of system $F$. There is a similar story for system $F_\omega$, which, I believe, corresponds to higher arithmetic $\mathrm{PA}_\omega$. 

Now for $I$ to be the recursive type represented by $F$, $I$ needs to be initial in this category: we need 

The formula $A$ is provable, this is almost exactly the content of Löb's Theorem. The proof (which can be found on the Wikipedia page) is a bit contorted, as it requires an additional application of diagonalization to build and analyze the formula: $$ \Psi \equiv Pr(\lceil\Psi\rceil)\rightarrow A $$ 

You are incorrect about the definition of large elimination: it refers to the ability to build values of type $\mathrm{Type}$ by eliminating an inductive value. The canonical example: 

Finally, it's of academic interest to note that any definition in a total language like Coq or Agda must have an intrinsic run-time upper bound: any program of size $n$ must fully evaluate in time $O(f(n))$ where $f$ is some computable function to be described by some clever proof theorist. Sadly, that bound is so enormous so as to be effectively useless; for example it is easy to describe the Ackermann function in both systems, so $f$ must grow faster than any given tower of exponentials (much, much, much faster). 

Finally, we have the tricky case of MLTT with inductive types. Here again a somewhat subtle issue arises. Certainly here we can express the consistency of $\mathrm{PA}$, so that isn't an issue, and there is no proof-irrelevant model, as we can prove that the type $\mathrm{Nat}$ has at least 2 elements (an infinite amount of distinct elements, in fact). However we run into a surprising fact of higher-order intuitionistic theories: $\mathrm{HA}_\omega$, the higher-order version of Heyting Arithmetic is conservative over $\mathrm{HA}$! In particular, we cannot prove consistency of $\mathrm{PA}$, (which is equivalent to that of $\mathrm{HA}$). An intuitive reason for this is that intuitionistic function spaces do not allow you to quantify over arbitrary subset of $\mathbb{N}$, since all definable functions $\mathbb{N}\rightarrow \mathbb{N}$ must be computable. In particular, I don't think you can prove consistency of $\mathrm{PA}$ if you add only natural numbers to MLTT without universes. I do believe adding either universes or "stronger" inductive types (like ordinal types) will give you enough power though, but I'm afraid I have no reference for this. With universes, the argument seems quite simple though, since you have enough set theory to build a model of $\mathrm{HA}$.