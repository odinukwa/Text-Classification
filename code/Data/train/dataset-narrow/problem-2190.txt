Is anything known about the second smallest $s$-$t$-cut in a flow network? Or, more general, about this problem: 

A $k$th smallest $s$-$t$ cut $(S,T)$ is any $s$-$t$ cut, such that there are exactly $k-1$ $s$-$t$ cuts whose capacities 

Multicuts or multiway cuts are (edge) cuts of minimum capacity that separate each pair of a set of terminals (a subset of the entire node set). For two terminals, this is the classical $s$-$t$ mincut problem. For $k \geq 3$ terminals, the problem becomes NP-hard. Such a multicut may or may not be composed of mincuts that isolate each single terminal from the others (in case of $k=3$, 2 isolating cuts suffice to separate all pairs). I am interested in a multicut variant, where the multicut needs to be composed of such mincuts (one for each terminal). I call such a multicut a mincut-multicut (do you know a better name?). 

I only consider $k=3$ from now on, so let the set of terminals be $T = \{t_1,t_2,t_3\}$. The minimum conceivable capacity possible for a multicut in a given 3-terminal network $N$ is $\frac{c_1+c_2+c_3}{2}$, where $c_i$ is the capacity of a min-cut separating $t_i$ from the other two terminals in $N$. I wonder what the complexity is to decide whether a given network allows a multicut of this minimum capacity. Formally, I would like to know the complexity of: Minimum 3-Multicut 

Certainly, the capacity of such a mincut-multicut may be greater than of a conventional multicut, but not smaller. For each terminal, there may be an exponential number of mincuts isolating it from the others. For each terminal, we need to choose one of these exponentially many in order to compose the multicut. If we can choose single terminal isolating mincuts such that the combination of them partitions the set of all nodes into $k$ sets (one for each terminal), we have found a mincut multicut of minimum capacity. It seems, however, quite challenging to determine whether such a mincut multicut exists. 

While this problem clearly is in NP, it is not clear whether this problem is in P since each single terminal can be separated from the other two terminals by up to exponentially in $|V|$ many min-cuts. On the other side, a promising candidate for a reduction to show NP-hardness is k-Multicut. However, it is not clear how such a reduction might work. 

Due to the submodularity of min-cuts, the sets $S_1,S_2,S_3$ are closed with respect to $\cap$ and $\cup$ as well. I cannot come up with a polynomial algorithm for NEC, neither can I proof NP-hardness (it clearly is in NP). Question: Do you have any ideas concerning NEC's complexity? The reason NEC appears to be harder than ECV is that the number of elements of each set system may be exponential in $|V|$. Iterating over all possible solutions is not possible anymore. At the same time, these exponentially many min-cuts are given in a data structure (a network) that may make it easier to find a solution. Also note that not for every three sets closed with respect to $\cap$ and $\cup$, there is a 4-terminal network encoding these sets in the above described way. This interdependency may be exploited (and at the same time makes it hard to find a reduction...). 

You are probably speaking about something like a process ontology. Lately, that research has focused (moved?) on semantic workflows, e.g. to model processes in science, related with reproducibility. Similarly, we can also find the application of ontologies to business processes. For more classical (lower level?) approaches, you may be interested in the homoiconicity property of programming languages like Lisp. In such a case these "networks" usually take the shape of parse trees, i.e. the representation is at a syntactic level and semantics are implicit, e.g. loops and recursive calls are still just a syntactic tree, even if the execution would not result into a tree. 

I'm sorry I don't fully understand the question, but if the point is to find the maximum, considering the local maximum is the global, then any gradient based method is going to take you directly to it. For instance artificial neural networks (gradient backpropagation). 

There are problems that are decidable, there are some that are undecidable, there is semidecidability, etc. In this case I wonder whether a problem can be meta-undecidable. This means (at least in my head) we cannot tell whether it is decidable or not. Maybe it's known decidability is undecidable (everything is meta-undecidable) and no algorithm exists to prove decidability for anything, so decidability has to be proven by hand on a case by case basis. Maybe my question doesn't make sense. Maybe I'm assuming we are carbon machines running very complex algorithms and that's why the question makes sense only in my head. Please let me know if the question needs further clarification. I may need that myself at this moment. Thank you. 

The entropy refers to a set of symbols (a text in your case, or the set of words in a language). The self-information refers to a symbol in a set (a word in your case). The information content of a text depends on how common the words in the text are wrt the global usage of those words. E.g. "Next saturday morning" should have less self-information (as a named entity) than "Large Hadron Collider", each of the words should have less self information (except for large, maybe). However the entropy may be higher in the first case, there are not many Large Hadron "things". Maybe Markov chains or some model that is better suited for probability in natural language would give you better (more significant and useful) results. 

I'm searching for an authoritative definition of resolution (logic resolution). Preferably on a reference freely available on the Internet (so I can read it right now). If this is too broad then resolution for first order logic. Thank you. 

Any normal activation function should do. Multi-layer perceptrons are universal approximators, so you should have no problems. You can find this even on wikipedia. The limitation for perceptrons is linear separability. Intuitively, each new layer allows to classify inputs according to as many parameters as neurons in the the previous layer obtaining a classification in as many dimensions as neurons in the next layer. In short, for one layer you have to keep linear separability, but according to that linear separability you are kind of "bending" (non isomorphic transformation) that hyperspace into a different one, where linear separability may be possible, or at least one layer closer.