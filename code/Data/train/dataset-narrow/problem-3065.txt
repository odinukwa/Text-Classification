If you will have a lot of instances, K-NN will quickly become very computationally expensive. I would suggest using percent match. The computer can learn the distribution of the different types of strings that you have. When a new string comes, it will compare the string with all other strings and can calculate the likelihood that the string came from a specific class. Then you can pick the greatest likelihood which is the most likely class that the new string will belong to. 

Then we will extract the second column, this contains the probability of each instance being in Class 1. 

taken from the link that @Hobbes reference. You will notice that there is no flip of the kernel $g$ like we did for the explicit computation of the convolution above. This is a matter of notation as @Media points out. This should be called cross-correlation. However, computationally this difference does not affect the performance of the algorithm because the kernel is being trained such that its weights are best suited for the operation, thus adding the flip operation would simply make the algorithm learn the weights in different cells of the kernel to accommodate the flip. So we can omit the flip. 

We can see that this line indeed describes the data pretty well for a linear regression. Deeper However, your data looks like it would be better fit using a polynomial. You should try $y = w_1 x_1^2 + w_2 x_1 + b$ To do this add a new feature in the $X$ matrix which corresponds to $x^2$. 

No, since you trained on patches of the image, you will need to feed in patches. This will make the denoising process of a large image very slow. 

You have time series data which is used to measure the acceleration. You which to identify when the machine is in its nominal state (OFF) and anomalous state (ON). This problem would be best solved using anomaly detection algorithms. But, there are so many ways that you can approach this problem. Preparing you data All of the methods will rely on the feature extraction method you select. Assuming we continue to use the 3 sample time window as you suggested. In this algorithm you will calculate a statistic for this nominal state $y = 0$. I would suggest the mean as I assume you are already doing, take the average of the three sample resultant accelerations. You will then be left with a large number of values in a training set $S$ defined as $S = \{s_0, s_1, ..., s_n \}$ where $s$ is the mean of the tree samples in a window. $s$ is defined as $s_i = \frac{1}{3} \sum_{k=i-2}^{i} x_k$ where $x$ is your sample observations and $i\geq2$. Then collect more data if it is possible with the machine active such that $y = 1$. Now you can choose if you want to train your algorithm on a one-class dataset (pure anomlay detection). A biased dataset (anomaly detection) or a well-balanced dataset. The balance of the dataset is the ratio between the two classes in your dataset. A perfect dataset for a 2-class classifier would be 1:1. 50% of the data belonging to each class. You seem to have a biased dataset, assuming you don't want to waste a lot of electricity. Do note that there is nothing stopping you from keeping the neighboring samples split as an instance in your dataset. For example: $x_i$ $x_{i-1}$ $x_{i-2}$ | $y_i$ This would make a 3-dimensional input space for a specific output which is defined for the currently taken sample. 

This problem is perfectly suited for a neural network. Your model will have 40 input nodes (this is fine), then you will have some arbitrary hidden layers, you need to tune this, and 20 outputs. After the training process you can even get a probability for each of them. This can be used to rank suggestions for potential students! 

Data pre-processing and feature extraction are the two most important parts of a machine learning technique. That's right, NOT THE MODEL. If you have good features then even a very simple model will get amazing results. Data pre-processing goes from your raw data and remolds it to be better suited to machine learning algorithms. This means pulling out important statistics from your data or converting your data into other formats in order to it being more representative. For example if you are using a technique which is sensitive to range, then you should normalize all your features. If you are using text data you should build word vectors. There are countless ways pre-processing and feature extraction can be implemented. Then, you want to use feature selection. From all the information you extracted from your data not all of it will be useful. There are machine learning algorithms such as: PCA, LDA, cross-correlation, etc. Which will select the features that are the most representative and ignore the rest. 

However, you will see that this does not work either. I suspect you are attempting to evaluate the price for a new $size = 2000$. You will need to reshape the input to your regression for this to work. 

Common misconception: A shallow method CAN and WILL perform better than a deep learning technique if you have properly selected your features. feature extraction is the most important aspect of a machine learning architecture. 

Now we will have a training set which contains $n$ instances and a testing set with a third as many. $m$ is the number of possible inputs. For cat vs. dog this would be $m=2$. You will be using your more general case where $m=10$. Each entry in the matrix $x$ has the vector with one-hot encoded vector where the index in accordance with the label is 1. We need to reshape the data for it to fit with the Keras structure. 

I think your best bet would be transfer learning. Start with a model that has already been trained with a wider dataset such as the ones presented here. From there you can train the model with your specific dataset. You can then use output nodes for the labels which you have available to you, and you can get the predictions for the other images from the pre-trained model which are usually trained for thousands of different classes. Alternatively, you can train a model with all the output classes you have in your label set and another output for "other". Then when an output node is selected you can pass that same input to the pre-trained model which was trained with your data as well. 

A good classifier will be as close to the blue line as possible. Note, that a line below the diagonal is also good, you should just invert your decision. You have a consistent classifier but you are inverting the decision. Intuitively, consider a sensor which reads a value and it is above a threshold $\theta$ we call it a detection. If we lower the threshold of detection, we should expect more detection, however if we end up with an equal amount of true positives and false negatives, then we are going along the diagonal line. This is a bad classifier. 

Why model neural networks as a graph In graph theory nodes are connected by vertices. Each neuron takes in some inputs and produces an output. The inputs to this node are not the same as the outputs of the previous layer. They are affected by some weight. Thus we can consider these weights to be the vertices of the graph that link nodes together. 

This will not work. The first error is the lack of brackets around the contents of your print statement which is required in Python 3. Change this first to 

The autoencoder which uses convolutional layers is essentially a subset of CNN architectures. The idea of an encoder is exactly as you stated you want to go from a space $\mathbb{R}^n$ to $\mathbb{R}^m$ where $m<n$. There are a lot of ways to compress information in such a way and these techniques used to be called compressed sensing. The idea was then to find automated ways to generate this mapping from n-dimensions to m-dimensions. As the name suggests it is an autoencoder. Lately the most popular method for doing this is using a CNN let's just refer to this specific type of autoencoder as an autoencoder for simplicity, but remember there are other ways of doing this. The autoencoder has a special structure where we will constrain the number of parameters at the center layer. The encoder is the part of the network which compresses the information into the m-dimensional space. We then use a decoder to reconstruct the input from the compressed data. An autoencoder is trained by feeding the same input and output. We want the network to recreate the input as closely as possible. If the output is equal to the input then we have perfect reconstruction and all the information contained in the input is contained. When we constrain the dimensions there will always be some information loss. We will want to minimize this loss. Here is an example of a vanilla autoencoder 

This should be a starting point. Let us know if you fall into any problems, and let us know what accuracy you are getting we can then look deeper into these models and better suit them to your data source. 

Choosing an algorithm More classes will result in a higher dimensional output, thus contributing to the complexity of your model. For example, if you have a model which discriminates between 2 classes with a set dataset size. Then further discrimination (increasing the number of output classes) will cause the model to have higher bias. You should thus expect to see greater test error if you do not increase the size of your dataset. If you have a set dataset X. Then you need to find the correct balance between bias and model complexity to get the optimal results. For example, a neural network based technique (highly complex) is not a good algorithm to use for a limited dataset with many output classes. However, Naive Baye's or Random Forest would be. 

All together this is just a CNN and the backpropagation will be computed in the exact same way. First we pass through a noise vector, it goes through the generator, some random image gets produced at its output, that then goes through the discriminator and gets classified as $artificial$. But, we are expecting the discriminator to be fooled in this case, so this is an error, it should have been labeled as $real$. We then use backpropagation to get the error contribution of each model parameter. We will then use gradient descent to update all the parameters associated with the generator. 

Are there better techniques for doing video classification compared to using temporal segment networks? I would suggest to train the network to detect bird species in image space without the temporal dimension. This will make the network have much less parameters to tune and you will likely get better results. Then for a novel video, you can detect the species for each frame and do a majority vote to decide what species was in the video. Are there any preprocessing steps that should be done to improve accuracy? If you train in image space then you can use a DataGenerator on the image and transform it in all kinds of ways, the position and rotation of the bird does not affect the species. This can be done through the Keras pipeline. You can check how to do this in general here. How to train the model to generalize better to be less sensitive to lighting and weather? You can try to pass your images through some pretrained segmentation network to identify background and foreground and then set that area to a static color. Perhaps black (i.e. all 0's). This will make the decision boundary for the network easier. However, I think you should focus on the other points, the network should inherently detect what features are the most salient for discriminating different species, if you have sufficient training samples the weather and background will be low information features and will thus not be used. How can the model be trained to classify unknown species? This would be called anomaly detection. This is a much harder problem. If you have birds of other species available in your dataset then you can add a class label for other. This is still standard supervised learning. If this is not available it will be very difficult. Most anomaly detection algorithms do not work well for high dimensional datasets. Then you will need a way to compress the information in your input images. This can be achieved with some transfer learning on a pretrained autoencoder network trained on natural images such as ImageNET, then you can use transfer learning using your bird images to further tune the network for your specific use case. You can then use this compressed feature space using one of the anomaly detection techniques I described here. 

Ok I was able to recreate the error. The TimeDistributed layer applies the dense layer to each of the time slices. In this model you will have 100 different time slices as defined by sum_txt_length and the RepeatVector layer. These time slices will be propagated through the network all the way to the TimeDistributed layer. Thus, your $y$ vector must have dimensions $(None, 100, 10000)$. You can use to get the expected dimensions after each layer. You will need to fix the output layer to match the label dimensions. I believe you will need to add a layer before the densely connected layer, and do not use the TimeDistributed layer at the output. That way your output will be of size $vocab_size$, thus predicting the next vocabulary word given an input. 

Let us formulate this problem in such a way that it can be understood from a machine learning perspective. You have a set of instances $X$ where each instance $x_i \in \mathbb{R}^m$ where $m$ is the dimensionality of the instance. In other words $m$ is the number of features that describe the instance. Your problem intends to go from a set of features to a class label good or bad. Thus, this is a mapping from $\mathbb{R}^m$ to $y \in \{0, 1\}$. How to achieve this mapping? This is when we will use the machine learning algorithms. We will train a model to effectively approximate the function which gives the output label from a set of inputs. It is evident that sparse features (low information entropy) will complicate the mapping function and will thus provide worse results. This is why feature engineering is of upmost importance for machine learning. It is probably the hardest part of the machine learning pipeline, however it is the lead factor in dictating your results. You can use some feature reduction techniques in order to remove features which are uninformative with respect to the output label. Some techniques that I use frequently are principle component analysis (PCA), linear discriminant analysis (LDA). Alternatively, you can use some projection methods to reduce the dimensionality of the data whilst maintaining separation between the classes. Such techniques are Isomap, MDS, Spectral Embeddings and TSNE. You can check to see which is best suited for your type of data. How to choose a model? Firstly, your problem is a supervised classification problem. This already narrows the types of models you can use. Furthermore, model selection is based on some key factors such as: the number of instances you have, the number of features per instance and the number of output nodes. You should also keep in consideration that the separability of the probability distribution between the output classes will impact the performance of the model directly. For example discriminating between cars and oranges is much easier than oranges and clementines. In your case, you have 1,000 instances and around 13 features. This means that deep learning based techniques are possible but discouraged. You do not have enough data. You can then attempt the following popular classification models 

Yes. The centroid will converge to the center of all your data and this will occur in a single iteration. This is due to all the data points belonging to a single centroid, thus it will be centered according to all these instances immediately. 

Training the discriminator The discriminator has two output nodes and is used to distinguish real instances from artificial instances. To train the discriminator we will generate $m$ instances using forward passes from the generator, these are the artificial instances, their label will be $y = 0$. To generate these we simply pass a noise vector as the input to the model. We will also use $m$ instances from the real data these will have labels $y = 1$. 

Simply put, almost every machine learning algorithm is well suited for this purpose. It just depends on how much data is available to you and it's distribution. 

It is always best to approach a problem using the simplest possible tools. In your case there is no need to use a machine learning (AI) model to detect the word sale. To find the word sale within a long datastream, I would do the following: parse through the entire String while looking at segments of size 4 at a time, convert the substring to lowercase. Determine if this substring is "sale". If it is save the index of the "s" into a list. Return the list which will include the index of each start of the word sale within your text.