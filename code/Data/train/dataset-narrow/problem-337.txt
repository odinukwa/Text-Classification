802.11 data rates are only reflective of how many "bits per second" are transmitted. Whether those bits are made up of headers or actual data is not reflected in the data rate. This is also true of wired speeds. So, generally speaking in networking, the more efficient the traffic is on the header to data payload ratio, the better off you are. 

All operations that take place in a network device are programs. Normally, when most people think of running a program, this is done in the system's processor (CPU), memory, and storage resources. This is software processing. It is flexible, easy to update, and can be shared by different programs easily. It can be used to run programs that weren't even thought of at the time the device was manufactured. However, many hardware components also contain programs embedded into the silicon itself during manufacturing or using ROM or flash memory. This is specialized and often limited to running only the type of program it is designed to run. It may or may not be upgradeable, but the process is typically more involved. However, when running the program it is designed to run, it runs that program VERY fast as there is no loading the program into/out of memory, storage or the CPU. Generally speaking, you want as much of your network traffic to be hardware processed rather than software processed. The reason being that hardware processing is fast and dedicated to the process. Software based processing is shared and may be subject to delays as other programs use the same resources. While that should provide an answer to your question about "what does S/W and H/W mean," I will expand on that to say that technology is always advancing and changing. What was software based yesterday may be hardware based today. Flash or solid state storage being used generally would have been unthinkable ten or fifteen years ago. It was simply too expensive to use on that scale. However, this has changed rapidly and continues to change. Who now doesn't have a USB flash drive? How many people are using SSD in computers? Yes, it is still expensive when priced per MB compared to traditional storage, but that gap is closing. Company specific, proprietary hardware based ASICs in switching have long been the kings of performance. In the last several years, "commodity silicon" is rapidly evolving the networking landscape. At a much lower cost, it can provide equivalent (depending on who you talk to) performance as ASIC based hardware. ASICs also always improve, adding additional functionality into the hardware itself. Ten years ago you couldn't find a switch with IPv6 functionality built into the hardware, but today you shouldn't consider an enterprise class switch that doesn't have it in hardware (RA guard, etc). 

Simply because the TCP (the transport layer) doesn't indicate where data used by the application starts/stops itself. It only keeps track of the transport layer session and the payload the segment(s) contains. Just because you know where a segment starts and how big a segment is, doesn't mean that matches the size of the data being transferred. The transport layer payload can contain part, all or multiple pieces of data needed by the application. Stated another way, the data used by the application may fit neatly into the payload of a single TCP segment, may be larger than the limits of the TCP segment and need multiple segments to deliver, or may be small enough that multiple pieces of data can be contained in one TCP segment. Further, a TCP session can be used to transfer one "piece of data", multiple pieces of data or even be used bidirectionally/interactively. Or one "piece of data" from an application can be sent over multiple different TCP sessions. If the transport layer were to indicate when data starts/ends, it would need to know how to figure out where data starts and ends for every possibly type of data it could be used to transport. This simply isn't necessary or even practical. It just needs to transport data to the other side and hand it off to the correct application. 

Specifics depend on chosen vendor/product, but the basic premise would be through the use of ACLs and/or firewall features. While there may also be means of simply not routing any traffic from one VLAN to another on the Cisco/EdgeRouter, depending on how the CPE device plays in the mix it may allow traffic to get "redirected" back into a different VLAN. 

However key to any good firewall deployment today would be looking into a firewall that is application aware. This can provide additional protection beyond the traditional "limit access to IP/port" and state rules. For instance, it may be able to detect XSS or SQL injection attacks and block those as well. 

No. There should be no noticeable difference in performance between an access port and trunk port. However, look a couple paragraphs up as there may be configuration that is applied differently to access ports and trunk/tagged ports. This configuration may have some impact on the port operation. 

If this improves your perfornamce, then it is most likely a hidden node problem. Hidden node problems are most pronounced in a wireless environement with multiple stations connected, espeially when all of them are transmitting/receiving data. By removing the other stations from being an active part of the network, this should reduce the number of collisions that occur. If this doesn't improve performance, then you will need to look at other sources of interference as has been mentioned in other answers. 

Absolutely, if the the network design called for it. Every network is different, so there may or may not be a need for it. Then again, there may be no need for OSPF at all. Just because OSPF has been deployed on a network doesn't mean the person(s) who deployed it understood OSPF and as such it is not always deployed correctly for the situation. I have seen single area networks that should have used multiple areas. I have also seen multiple area OSPF deployments that made no sense and should have been run as a single area. 

If your servers are using STP or your switches are not, then you can look at other options. If they are Cisco switches, by default they will be using a process called CDP (Cisco Discovery Protocol). This is often disabled in some environments so you may need to consider enabling it to test. Some other vendors also support CDP as well, but it may be disabled by default. You can capture these with tcpdump as follows: 

TFTP uses UDP for the transfer, which as you indicate is a connectionless protocol. FTP, SCP, HTTP or other methods of transfer typically use TCP. UDP requires less overhead and is generally faster than TCP. There is no TCP acknowledgements nor the TCP window to account for during the transfer. There are generally other methods to verify the data once it has arrived, typically by comparing a calculated hash to a known value. Since the transfer typically takes place over your own network under your control, which hopefully is reliable, they generally turn out fine. Also, many devices will allow you to set a primary and a backup image to boot from, so you can configure the device to boot from the backup if the primary image fails to load for some reason. If you are using an unreliable network, another option is typically available to use such as FTP. However you will still probably want to use any verification methods you have available to ensure data integrity. 

This is really a design decision and you have already gotten some good answers. There are many reasons to do both, but here are a couple for doing it either way. If you have multiple load balancers in some sort of active/active state, you would want to leave this disabled so that the servers behind the load balancer respond to the correct load balancer. If they were to send traffic back to the wrong load balancer, this could create problems (again based on design). However if your servers are doing logging or need to use the actual source address in any responses, then turning this on will be required. 

No, this is normal operation. You just need to understand the limitations of the technology in use. WPA/WPA2-Personal provides encryption to your wireless communications. That is it. It is not meant to provide any sort of benefit that authentication normally provides nor to validate the network to which you are connecting. Additionally, any device with the PSK that also captures the WPA/WPA2 encryption handshake (i.e. when the client device connects to the wireless network) will also be able to decrypt all the data. Does this make it insecure. No, rather it is just providing the level of security for which it is designed. If you want a more secure option, then WPA/WPA2-Enterprise is the way to go. Depending on the deployment choices, it can provide unique encryption material for each connection, validate the user and/or device connecting to the network and validate that the network to which you are connecting. This does come as a trade off as it is more complex on both sides of the connection and can require user education to maintain security. This is a normal security vs. ease of use trade off. Generally speaking, WPA/WPA2-Personal is sufficient for most small deployments. Note: some portions of this answer have been copied from my own answers to similar questions elsewhere on the SE network. As they are my own answers, I am not citing them. 

Finally, consider what happens if you lose a link in the LAG. Given two links to start, this would reduce my LAG to one link. If I start with three, I will still have two left. Sure, balanced flows appeal to the obsessive compulsive parts of my personality, but this still doesn't reflect balanced traffic. Aside from that, more links is better any way I look at it. 

"Interface doesn't go down physically" - each device running RST generates BPDUs (rather than just relaying) and they are used as keep alives. Failure to receive BPDUs will result in aging out the information. "Need to go through STP learning stages before forwarding traffic" - RST is able to actively confirm that a port can safely transition to forwarding state without waiting for timers. 

I will preface this by saying this is a terrible answer, first because I can't find any of the references and am going off of memory. Second because I haven't put much thought into WEP for so long that my memory fails me. As to why it was 152-bit, IIRC, this was a 128-bit standard WEP key with an additional 24-bit extension. The 24-bit extension was one of several proprietary extensions in an attempt to make WEP more secure (or at least appear more secure). Cisco's MIC and TKIP won out as the most widely recognized extensions to make WEP more secure (more commonly known as WPA). 152-bit WEP ultimately faded away as many proprietary extensions do when they don't get enough industry support and/or are over shadowed by better extensions. Ultimately, it really doesn't matter at this point though as WEP is a fundamentally broken technology and WEP, WPA/TKIP and WPA2/TKIP are all defunct technologies. By this I mean that the 802.11n amendment says an AP has to disable the high throughput data rates and function as an 802.11a/g device if any of the above are enabled.