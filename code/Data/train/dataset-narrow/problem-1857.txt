Situations get more complicated with scale. Starting out, you would simply deploy all content to all servers at the same time manually. You could write a script to do this. As you can larger, configuration management software can help. Also, OpenEFS would be a suitable solution. With static content and certain types of content, there are more options available. These decisions because more glaring when you attempt to modify or upload content via the load balanced application itself, as if you do not handle this well, you will result in data partition. For example, an uploaded file will only be on one of the load balanced servers. In no particular order: 

Try instead. If this file applies to the sub-directory with , try specifying . Best practices often entail specifying these settings within httpd.conf instead if you're the system administrator. Also, it's more risky from a security perspective even to enable .htaccess files in the first place. If the Web server or application user were compromised, this would be one of the first places that could be written to without complete compromise. If specified in the Apache configuration file instead, you would often specify within a section. Apache Docs 

I have been researching high availability (HA) solutions for MySQL between data centers. For servers located in the same physical environment, I have preferred dual master with heartbeat (floating VIP) using an active passive approach. The heartbeat is over both a serial connection as well as an ethernet connection. Ultimately, my goal is to maintain this same level of availability but between data centers. I want to dynamically failover between both data centers without manual intervention and still maintain data integrity. There would be BGP on top. Web clusters in both locations, which would have the potential to route to the databases between both sides. If the Internet connection went down on site 1, clients would route through site 2, to the Web cluster, and then to the database in site 1 if the link between both sites is still up. With this scenario, due to the lack of physical link (serial) there is a more likely chance of split brain. If the WAN went down between both sites, the VIP would end up on both sites, where a variety of unpleasant scenarios could introduce desync. Another potential issue I see is difficulty scaling this infrastructure to a third data center in the future. The network layer is not a focus. The architecture is flexible at this stage. Again, my focus is a solution for maintaining data integrity as well as automatic failover with the MySQL databases. I would likely design the rest around this. Can you recommend a proven solution for MySQL HA between two physically diverse sites? Thank you for taking the time to read this. I look forward to reading your recommendations. 

I typically use the iptraf utility for this purpose. Many distributions have this packaged but do not install it by default. Additionally, historical graphing such as that which is created by Cacti is useful. 

It sounds like you're specifying the authentication settings within the . Typically, these settings are specified under the directive. You could also use files, but specifying in the Apache conf is a good default, as it has less exposure. Apache Documentation 

If PHP is configured properly in Apache and the client is functioning properly, the PHP script may not be specifying the proper MIME type for the content output. The often has to be printed for the browser to understand how to handle the data. What is the content of the file that Firefox is attempting to download? 

My experience is more focused with Jboss and Tomcat. However, I suspect it applies to Glassfish as well. The Java keytool is what is used to manipulate keystores. If you generated the key with openssl, keytool neither supports importing nor exporting keys. For exporting a key, this tool can be used. For importing, this tool can be used. Depending on the type of certificate and the authority, you will likely have to import the root certificate as well. Here are some of my notes for basic commands.. 

Caching can be controlled via headers that are produced in the content. The content could have the and headers specified. More details Edit 1 These headers can be introduced as part of the content being served, which could apply to images as well. This is something I've pushed through our development team in the past, as I prefer to cache static content on our proxies. For your purpose, it sounds like mod_expires would be simpler. mod_expires is probably not enabled because it is superfluous by default. Best practices tends to disable any unnecessary modules as part of initial configuration. If you ask nicely and you're not using commodity hosting, maybe they'll enable it for you. Edit 2 Just found this. If mod_headers is loaded, you can use it to set the expiration. Using mod_headers 

You can use variations of this theme if you prefer different timestamps or filenames. Be aware that this solution is not robust enough to handle multiple filenames. 

This reply is primarily to espouse the benefits of properly maintaining access within your IT department. Your situation examples the benefit of an audit trail and proper access control. For example, all access would require an access request ticket with approval, without exception. Upon termination, you audit the ticket system. For common roles within your company, the access can be standardized and even easier to eliminate. For IT roles, we have a spreadsheet that we work out of for terminations. It lists everything to prevent oversight. We also audit our access request tickets and our work logging system, as all production changes are documented there. Administrators should also have individual user accounts, which they use to access administrative privileges. root and administrative accounts should not be authenticated to directly. While this is not technically infallible, it enables an audit trail as well as individual accountability. With this, locking all his accounts would be a first step and then you change all admin accounts. If you have not already, I would encourage you to implement some of these solutions if not all of them. I consider them integral and it reduces risk when an involuntary termination occurs. First, remove all external facing access. Any access the person could use without being on premise. Then, change all passwords. Every administrator password, every system password, every application password, every vendor account password, every support account password-- everything. If the risk for retaliation is great, you might expire all employees' passwords as well. Since you do not have the root passwords to the Linux servers, you can boot in single user mode and change it. With GRUB and LILO, you would simply append . The methods are similar. As others have recommended, audit all crontabs (located in /var/spool/cron), system users, running daemons, ssh keypairs, and the systems in general. While rebuilding is the only way to be certain, it should not be necessary in most cases. Any respectful professional would not risk their career on such a guttural reaction. It would also enable pursuit of both criminal and civil damages by your employer. Ultimately, I would suggest having a serious discussion regarding the risks with your manager after performing due diligence with removal. 

Links to official and recommended documentation exist on the Netfilter Web site. This is not a new subject, resources are limitless. Most basic commands are fairly intuitive and can easily be reference to the manpage. netfilter, which is the kernel level technology that enables the packet filtering, is quite advanced. There are additional tables that can mangle packets, translate packets, and otherwise affect routing. The utility is the userland tool for interacting with netfilter. If you wish to learn about advanced functionality, I suggest you reference the aforementioned documentation. For an introduction to the basic functionality, please read further. To list all existing rules: 

Specifically in regards to availability monitoring, I'm a huge Nagios fan. If you haven't considered it, I'd strongly recommend it. Others like monit. 

Yes. Keep in mind, the queries will replicate and execute identically on the slave as with the master. If the column is on the end of the table, it could simply use the default value. If it is in the middle, you could introduce data type conflicts or potentially even replication failure if the insert failed. 

It goes straight to the background as you detail for me. I do have a keypair setup on my test server, however. Do you already have something listening on 8888 on your local server? Do you already have an instance in the background? 

You're going to have to make compromises. You could enforce policy by having the users connect via VPN and then to the domain on the road. This is unlikely to be infallible, as you will either prevent access to the device or be unable to enforce policy. It cannot be assumed that an Internet connection will always be available or have enough bandwidth available, which will be requisite to connect to your network. This is fine for policy enforcement but if updates are downloaded from servers on your network it could delay their installation. For remote users on company equipment, we often join to the domain and have them connect to the VPN or plug in locally. These users are more often on site than not, however, which makes logistics less difficult. Based on your requirements, the best approach will likely be to setup group policies that enforce your update policies and then have them get updates from Microsoft or the antivirus vendor, as it will be more flexible. For remote access, you could use Remote Assistance. I'd strongly encourage full disk encryption as well. True Crypt is a fantastic Open Source solution. 

What you are describing sounds more akin to a shared hosting environment. With a shared server, I would typically configure each user's Web sites within their home directories within public_html. If they had multiple domains, I would follow a similar convention as above but within this directory. I would also separate the and within their home directory but outside of the Web tree. SuExec and and mod_suphp would also be advised to prevent users' data being exposed to other users on the system and to limit scope of any users' Web sites being vulnerable. If the users did not have domains, you could use subdomains within a primary domain or mod_userdir. If you would rather not take the more organized approach, I would suggest using sub-directories, groups, and the SGID bit within /var/www. Nevertheless, that would not be the ideal approach in almost all situations. 

Assuming your friend uses Windows, have them use WinSCP to access your server via SSH. Accessing user accounts via an unencrypted protocol is a security risk. If you do not know how to connect to the server, perhaps you should contact the administrator of the server or support department of the ISP providing it. Putty is a good Windows client and most UNIX based operating systems include SSH clients by default. to add an account. Read the manpage. to set his password. Read the manpage. to create a group. Read the manpage. to add your username and their username to the group. Read the manpage. to change group ownership on the web tree. You can use the commands below here instead of the next chmod commands and any other directory within that tree to make default ownership be groupname on permission, which will allow you both to work there. Then, make group writeable: 

You can use acls and other parameters to specify the situation in which you would not want to cache. For directories: 

Linux shows threads as separate processes. MySQL starts one thread per connection. Running with . For example, will display how the threads are related. How many connections are there to MySQL? Run in the MySQL client. From what you're showing, I'm not even confident that there's anything wrong. 

You're right, you should add it to the path. Typically server-wide shell environment for bash is in . For (t)csh it is . Rather than setting the system-wide, I'd recommend setting it for your individual user. You would specify the in or in your home directory. If you want recommendations for a different shell, you will need to identify what shell you use. If your situation is a special case, you will need to provide additional details, such as how you are authenticating to what user.