Cloud Vision is an API with already trained models. If you want to train your own model you can use tensorflow, theano, caffe... there are several frameworks for it. None of them is easy. 

I have used TensorFlow before for training a model to recognize images (something similar to this example: $URL$ I was trying to do something similar with inception-v4 from scratch (using the model from $URL$ But it was not working, it wasn't learning, so I decided to reduce my code and simplify everything. The model is a hidden layer with the same neurons as inputs and what the model has to learn is that input=output. Seems pretty simple and easy, but the results of my code are far from good. I have tried different things: sigmoid cross entropy instead of softmax cross entropy, other optimizers, different learning rates, models more comples with more neuros and more layers, reduce_sum instead of reduce mean for the loss ... But the only times I got "good results" (>90%) it seems because the model falls into a local minimum and I am not able to replicate them. Usually I see 70% precision with a high loss (>=0.7). My conclusion is that there is something wrong in my code that I am not able to spot and that is the reason the learning process is slow and get stuck at high losses. Otherwise the code with inception-v4 for images will at least learn something and don't get the loss stuck at the same value all the time. 

Obviously, I can start by removing stopwords (e.g. did), by naming entities (e.g. road -> High Street), by defining synonyms and by applying a text similarity measure (e.g. Levenshtein distance etc). However, I feel like reinventing the wheel if I do this. Therefore, my question is: Are there any APIs which can compare strings in terms of semantic similarity (without even requiring training)? I know that there software platforms such as Dialogflow which are suitable for these tasks but still you must explicitly state all the variations of the same question so that you will get the same answer. Therefore, I look for a API where you will explicitly state only one of these variations of the same question (e.g. Why is there traffic at High Street?) and then the API will figure out by itself which other variations are identical to it in terms of meaning or not. 

I want to create a chatbot which informs the user about traffic at the streets but not in real-time for the moment. I have created a small database with MySQL which has some data stored regarding traffic and I fetch them with a PHP script whenever this is appropriate depending on the interaction of the user with the chatbot. I wonder how to deal with the case when the user asks variations of the same question which therefore can be answered with the same answer. For example: 

...and I also had to index into the nested Model object (my first layer is a Model object as is shown above) to get the 'block2_con1' layer: 

I know that keras provides a dictionary containing the mapping from class names to class as part of from its class ($URL$ HOWEVER, is there a way to access the corresponding class labels from an existing saved model (a model saved in as an .h5 file)? This seems important when putting my model into production and serving predictions since the classes are not known upfront and therefore the images are not separated in pre-labeled directories. Similarly, are there any good examples of how a keras model should be deployed into production (especially for mobile)? Thanks! 

The key is to first do on the Model object, then do another on that specifying the specific layer, THEN do .output: 

I have a fine-tuned network that I created which uses vgg16 as it's base. I am following section 5.4.2 Visualizing CovNet Filters in Deep Learning With Python (which is very similar to the guide on the Keras blog to visualize convnet filters here). The guide simply uses the vgg16 network. My fine tuned model uses the vgg16 model as the base, for example: 

I am working with Python. I have 3000 thousands images of front-faced watches like the following ones: Watch1, Watch2, Watch3. I want to find an API which receives this collection of photos or even others taken under less ideal conditions (different background colour, darker lightning etc) and which finds/matches the most similar watches with one each other. By similarity I mean that I expect that a round in shape, brown watch with thin lace will be only matched with watches of round shape, of dark colour and with thin lace (from the same collection of photos). I am aware of APIs of this kind from , , , , , etc but I am not sure that they will perform so well in a so specialised application. For example, these APIs are useful for matching car images with car images and not with food images but matching among the same kind of objects (e.g. watches) based on a very high level of detail (shape, colour, thickness etc) is significantly more demanding. For instance, this is an application on a specific kind of objects like clothes with Indico: $URL$ However, if you notice it, the results are not that good and essentially they could be retrieved to a great extent even by simply applying PCA and KNN to these images. Therefore, my question is: Is there any API which can match similar images based on a high level of detail? 

The "class_indices" attribute in Kerasâ€™ flow_from_directory(directory) creates a dictionary of the classes and their index in the output array: 

Layer (type) Output Shape Param # ======================================================================= vgg16 (Model) (None, 4, 4, 512) 14714688 ________________________________________________________________________ flatten_1 (Flatten) (None, 8192) 0 ________________________________________________________________________ dense_7 (Dense) (None, 256) 2097408 ________________________________________________________________________ dense_8 (Dense) (None, 3) 771 ======================================================================== Total params: 16,812,867 Trainable params: 16,812,867 Non-trainable params: 0 I'm running into an issue when I run this line: where when I use my fine tuned network I get a result that's a "NoneType" Here is the code from the guide: 

In a convolutional neural network (CNN), since the RGB values get multiplied in the first convolutional layer, does this mean that color is essentially only extracted in the very first layer? Snippets from Stanford CS231n Chapter on CNN: 

I believe the key is that when the filter is convolving some part of the image (the "receptive field") each number in the filter (i.e. each weight) is first flattened into vector format. Likewise, the pixels of the image are also flattened into vector format. THEN, the dot product is calculated. Which is the exact same thing as finding the sum of the element-by-element (element-wise) multiplication. Of course, these flattened vectors can also be combined in a matrix format, as the below image shows. In this case then true matrix multiplication can be used, but it's important to note that the flattening the image pixels from each convolution and also the weights filter is the precursor.