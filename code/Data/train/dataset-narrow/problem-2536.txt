I would tend to assume you have already read through Reynold's (the guy who invented boids) page about flocking, but since you didn't directly mention it I'm going to link it here: $URL$ He gives great explanations of how several aspects of his flocking algorithms work, including Cohesion, Separation, and Alignment. 

Then when you save the game just save which entity it was (ie. the ID). When loading the saved game refer back to the entities data file: 

I would do WASD for movement and mouse for shooting, with the aim restricted to 8 directions. So in other words instead of pointing directly at the mouse cursor you point in whichever of the 8 directions is closest to the mouse cursor. I don't know off the top of my head the code to do this but I'm pretty sure it'll be something simple involving atan2() 

You don't have any transitions from run/walk back to jump. Look at the arrows; you only have a transition from jump to run, not back from run to jump. In other words, transitions are not bi-directional; they're only one way. If you want a transition that goes back to a previous state, then you need another arrow. 

You can drag-and-drop objects from the Project view just as you would in the main scene, linking them to variables in the Inspector. That is, click-and-drag the prefab asset in the Project view, and then drop it onto the variable in the Inspector. 

I don't think you want a time delay between jumps; what you want is to only jump once when a button is pressed. That is, don't jump over and over while the button is held down, require a new button press each time. 

There are two reasons I can see immediately, both to do with simplicity: it's simpler to program the AI, and it's simpler for the player to understand. Now for certain games it may be an advantage if enemy characters can switch weapons for the situation (for starters, it's more realistic) but for most games it can be a significant problem if the player gets confused about what tactics to expect and counter. Players like being able to think "oh there's one of the grenade dudes, I better watch out for grenades." As for complication in AI, there's just plain less thinking for the enemy to do if it doesn't have to consider which weapon to use. Consider your own thought process when deciding which weapon to use; now try to model that thought process as a set of instructions. You'll be considering damage per shot with how accurate the shots are, weighing effectiveness against different sorts of targets, and balancing all those factors against how much ammo you have. Pretty complicated huh? 

Put the data in an external file that gets loaded by the object factory. Both XML and JSON are very useful for this purpose: 

You've already identified the main difference: PlayOneShot can play multiple sounds without cutting each other off. On the flipside however that means* you can't stop the audio clip either; it'll just play all the way through, with no way to stop it early. *I think; I can't test this at the moment, but I recall that calling Stop() after PlayOneShot() does nothing. 

You've already started so keep learning programming. Don't only learn about game programming, but work on some game projects. Don't tell your judgmental relatives that you're learning to make games, just tell them you're learning to program computers. Then when you graduate go work in a discipline of your choosing, maybe games or maybe not. 

This is a pretty straightforward coding problem, so don't shy away from writing a custom script to handle this problem. You just need to: 

Although that will snap to the new direction instantly; if you want the player to smoothly rotate to the new direction then you should use Lerp: 

Notice in that documentation the optional last parameter: BuildTarget targetPlatform = BuildTarget.WebPlayer In other words, if you don't explicitly set the target platform then it will default to target web player. To make it target the current platform, tell it: EditorUserBuildSettings.activeBuildTarget Frankly, I would file this as a bug against that documentation. They should really make this fact more explicit, perhaps by adding it to their sample code or at least having a WARNING note that you should use .activeBuildTarget for mobile platforms. 

This seems like a perfect time for LOD swapping. As in use a low resolution model with opacity maps when seen at a distance, and then swap in a more detailed cart when the camera gets close. I mean, I'm assuming here that the shopping cart is somehow really important to the scene, because needing the metal rods to look super realistic seems pretty odd to me. 

Incidentally, the last time I was working with 3D on iPhone I was using miniB3D, the iOS port of miniB3D, which itself was an OpenGL port of Blitz3D: $URL$ Unfortunately while there used to be a good .b3d exporter for Blender, I don't think that works in the latest version of Blender. 

It uses a bloom shader. That's a post-processing effect that makes bright areas of the rendering appear to glow. 

A good architecture is keeping the GameState and ScreenManager objects separate so they don't directly know about each other and only indirectly communicate though an EventDispatcher object; ScreenManager registers with EventDispatcher as listening for messages about models being picked, and GameState sends a message to EventDispatcher about a model being picked (along with a parameter for which model was picked). Going in the other direction is as simple as having ScreenManager dispatch events about buttons being clicked, and GameState listens for messages about buttons being clicked. Now you can change either the GameState object or the ScreenManager at will and, as long as you don't change the messages each is dispatching/listening to, changing one will never affect the other. I believe this is the Observer pattern, although frankly my knowledge of design patterns is kinda ad hoc. EDIT: I misunderstood the question slightly, but I'm not changing my answer because it's still how I would do it. The relationship between the Game object and the ScreenManager in GSM is different from what I originally thought you were talking about, but using an EventDispatcher like this is still how I would pass messages around. 

I normally do all three. You make it sound like you need to choose 1 method and use it for all graphics, but that's not true. Each method has different pros and cons, so I use different methods on different images. For example, my UI generally needs to be super-sharp, but it's also not full of important tiny details; perfect candidate for #2. On the other hand, background images are naturally large (often fullscreen) but can be blurry, so on those I use #1. 

Well first off an enum defines what the values can be, not what the values are. Thus you still need to declare another variable after you've declared the enum. For example: 

As you (I think) alluded to, Unity's roadmap has plans for a tilemap editor. I'm looking forward to it, because right now it's a little confusing how to proceed. In the meantime, the best approach I'm seeing is to make your map in Tiled and then use X-UniTMX to import it into Unity. Obviously that's not procedural generation though; I'd imagine you would use the same technique as X-UniTMX is using to create the mesh. 

Here are a few links about game design and level design collected on my webpage: $URL$ Especially take note of the link "evaluating game mechanics for depth" because that is a really informative article about level designers. 

By "level", it sounds like you mean you don't want the geometry in your level file, only where objects are located. A more clear term for that is "placement editor" (ie. use Blender to place objects around the scene) I've written a Python script to do just that; it saves custom properties in addition to regular position/rotation/etc, and you can save the data as either XML or JSON. (and then just for kicks I ported the script to Maya) 

What Thomas said in his comment: the surface normals on the floor are pointed up, while the walls are pointed sideways. (I'm leaving my response about the dot product because that will work too and helps explain how to work with 3D vectors, but phillips comment points out a simpler approach I didn't think of) Just check the normal's Z component. So, if you wanted "floors" to be surfaces with normals < 45°, you could check if the Z value is > 0.707 (because sin(45°)=0.707). In fact, this is how Unreal checks if the Pawn is on a "floor" -- it compares the floor's normal Z value to WalkableFloorZ, which is 0.7 by default. 

Then you still need to structure your code so that other parts of your code can access this "settings" object. I don't know if you need help with that part too, but common patterns to address this problem include singletons (those are frowned on these days) or service locators or dependency injection. 

The answer to this problem: Get the dot product of the player's left direction with the direction from the player to the shooter. That'll be positive if the shooter is to the player's left, or negative if the shooter is to the player's right. In general, learn about useful linear algebra operations like dot and cross product. Start here $URL$