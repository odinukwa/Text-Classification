To fix existing databases that already have all their files in the mysql data root. You need to rename all the tables into a different database name. 

If you need to have parallel threads then you can re-enable them once the slave has caught up or gotten past the event causing problems. I would try a different like conservative 

I recently broke replication and when I tried to get past the one incorrect transaction. I got the following. 

This is well documented, Please google and find instructions. 2. Stop Parallel slave threads This was part of the problem as seen in the original question. 

I can reverse this process to re-enable Parallel slave threads when I am done. And I know that GTID is working. 3. Enable GTID replication I can now try restarting the slave with GTID enabled. On the master 

You want to use the option. This will allow you to create up to 3 different backups - each effectively a mirror or copy of the main backup you are doing. In your case: 

For most applications hitting SQL Server 2008 R2, you would be fine going either way, I think. support is being removed after SQL Server 2012 but until it is, you'd likely be fine either way based on your standards and your needs. That said, I'd look to future supportability and consider going with the Native Client. I would say a more important decisions would be to consider using the Native Client for your .net apps if you aren't already. 

From a SQL Server Perspective if you are doing the to allow for dynamic passing of parameters and skipping a parameter from being evaluated, I would suggest you read a couple articles from SQL Server MV Erland Sommarskog. His approach removes the need to do some other tricks inside of dynamic SQL (like the construct or using a construct). The the 1=1 shouldn't cause performance issues as @JNK mentioned (I've +1'd his answer there and that is the one that should be accepted), I think you'll find some good tips from Erland's article around Dynamic SQL and you'll also see he still uses the one for the cases where no parameters are passed but he avoids them for individual parameters that aren't passed, he simply doesn't mention them in the resulting where clause at all. 

Will run without errors, you can to close it when you are happy that your slave has caught up. This is not much different then, but it does it auto magically. 

Then rename the tables into a temp database and rename the tables back. This should take a few minutes depending on how many files need to be renamed. You should have a working database without any config changes required for your applications. 

Now when I check the slave it has some events to skip to get back into the same state as the master. 

Is there a way to get tokudb to put all the files it creates for a database into subdirectories. When I started using tokudb the most irritating thing I have found is that it puts all its files into the MySQL root database directory. This makes it difficult to see which files belong to which database and difficult to see how much space each database schema is using. 

I normalized the tables to avoid using NULLs. The problem is that some of these tables depend on each other due to business processes. Some devices must be sanitized, and some are tracked in another system. All devices will eventually be disposed in the Disposal table. The issue is that I need to perform checks, such as if the boolean field is true, then the cannot be entered until the fields are entered. Also, if the boolean value is true, then the fields must be entered before the can be entered. If I merge all of these columns into the table then I will have NULL fields, but I will be able to manage all of the business rules using CHECK constraints. The alternative is to leave the tables as they are, and manage the business logic in the stored procedure by selecting from the tables to check if records exist and then throw appropriate errors. Is this a case where NULL can be used appropriately? The boolean fields and basically give meaning to the NULL fields. If is then the device is not tracked in the other system and and are NULL, and I know that they should be NULL becuase it is not tracked in the other system. Likewise, and I know will be aswell, and a can be entered at any time. If is , then and will be required, and if and are NULL, then I know they have not been officially removed from that system yet and thus cannot have a until they are entered. So it's a question between separate tables/no NULLs/enforce rules in stored procedures vs combined table/NULLs/enforce rules in CHECK constraints. I understand that querying with NULLs in the picture can be complex and have somewhat undefined behavior, so separate tables and stored procedures seem beneficial in that sense. Alternatively, being able to use CHECK constraints and have the rules built into the table seems equally beneficial. Any thoughts? Thanks for reading. Please ask for clarification where needed. Update Example table if they were merged and I allowed NULLs. 

Unfortunately, no. You are installing a program and making service and registry changes, etc. You need to run setup as an account that has local administrative rights. No workaround to this that wouldn't involve some back door into administrative rights. Best bet is to ask for someone to temporarily give you the rights or install it for you. Source: The Microsoft SQL Server books online article for how to install SQL Server 2008 R2Express 

Old question, I understand, but an answer in addition to the above answers exists since SQL Server 2016 SP1. SQL Server 2016 introduced "Always Encrypted" as well as other security features like Dynamic Data Masking and Row Level Security. These were features originally released in Enterprise Edition Only. They now exist as features available in any SQL Server 2016 SKU - Enterprise, Standard or even Express. Worth checking out and worth looking at the upgrade process now. You can read more about this on Microsoft's post about SP1 for SQL Server 2016.. 

I want to be able to skip events and not worry about trying to figure out or increase the GTID position for everyone. 

I have found in production that Parallel_Mode is the most likely cause of my problems. I recommend using a different value from 

I found the following worked for me. This does not restore a slave into state that is an exact replica of master. There will be data differences. I will use pt-table-sync to fix those. 1. Restart Replication without GTID method 2. Stop Parallel slave threads 3. Enable GTID replication 4. Using percona-toolkit pt-slave-restart to skip past all the errors. 1. Restart Replication without GTID method Using master binglog position 

I now use to restart slaves as I don't have to think about sequence number and a whole bundle of other things that take too long when I just want to get the slave started. 

There really isn't a short answer because there are a few hidden questions in the question. A few thoughts to help here: 1.) The Browser service is not cluster aware, so it generally would be just running on each node. The browser is really used to handle incoming connections to a SQL instance. When you don't have a fixed port, are using named instances and in other situations the browser handles the "finding" of the instance a client desires to connect to. So if you have it running on the active node and it is being used to direct connections, I would make sure it is automatic and running on each node. 2.) That said the browser shouldn't prevent DLLs from being found or take any part in preventing or allowing a failover. So the issue you are having with failing over is most likely not related to the browser but something else. Instead you should be looking at things like - Have you failed over before? Is this a new install of a second node? Have there been any required restarts missed on that node? Did the install throw up any errors? Are there issues in the clustering logs? Can you post the exact error you are receiving? Have you searched for that exact error? 

Design 2 In this design I thought I would use a bridge/association/junction table to decide which network statuses are valid for a device. It looks like this: 

This is an inventory database for IT assets. The models used are trimmed in order to focus on the problem at hand. Using SQL Server 2008. Thanks for taking the time to read and for any input you can provide. My design includes a table which holds the various devices that can be entered into inventory. Each device has a boolean flag, which states whether a device has network capability, e.g., for most computers , for hard drives ; some printers will be true, and others will be false. You get the idea. The field determines if network-related information is relevant when an inventory record is created. Design 1 My first design uses an index on and to use in a foreign key constraint with the table. 

Short Answer: Seeing higher IO stalls may or may not be a problem in an of itself. You need to look at more information to suss out if it you have an issue. It does seem a bit high, yes, but are you suffering? If so, it is probably because either your IO system is not handling the load right (because it can't, because you have everything on one drive or some other reason) or you are doing too much in TempDB (changing the first problem - the IO performance - is probably an easier and more efficient fix, but first determine if you have a problem) The longer discussion/answer: There are two questions at play here - 1.) What do I do when I see high IO Stalls? First off, "high" is in the eye of the beholder. If you were to ask 10 DBAs what "too high" is for IO stalls you'd probably get 2-3 different answers with numbers in them, 5-6 "It depends" answers and one blank stare. My assumption is an average of 400ms is potentially too high here, especially when the other DBs are 2ms or lower for the average stall time. Regardless of which database is seeing the high stalls you should approach it the same way. An IO stall is what it sounds like... An IO request taking longer than expected.. Stalling. These happen. They happen all the time in a system with resources being shared and finite resources (really all of our systems). They become an issue when the stalls become performance problems or lead to them. So I trust that you are looking here as a proactive part of monitoring or because you were experiencing performance issues that you are troubleshooting. We also don't want to get lost in just IO stalls. We are looking at a piece of the puzzle and not the big picture. It can be troublesome to just look at wait stats or file stats since SQL was last restarted because you are looking at all time and some maintenance window or heavy load window could skew counters. So make sure you look at the full picture. But when I suspect I have a disk performance issue or see something off in a query like this, I normally follow a process that looks like: