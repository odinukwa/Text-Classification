See Microsoft Technet article on share permissions for the low down. The short story of it is that the most restrictive permission applies, whether you apply it via Share permissions or NTFS. So you have the option of setting Read/Write permission to the share or folder, then reduced permissions on a subfolder via NTFS. This means you should probably set the least restrictive permission on the share (Everyone get's Read/Write) then restrict further using NTFS. See also Microsoft's Best Practice for Shared Folders. 

If you specifically need SSH, then some sort of VPN client on the Mac that links it to your network would allow this. When you need to get access, they fire up the VPN, which assigns them a static address on your network, then you SSH on. 

Like Christopher Cashell says you need to log the traffic to know what is required by all your systems. Given that your business may not accept a DENY rule that blocks all traffic while you figure out what is required, you could set a PERMIT rule instead, and log what is permitted, and analyse that. You can then use that to set up more specific PERMIT rules and then switch on the DENY as the last rule for that interface. That way you are only allowing the traffic you permitted. Microsoft documentation should give you the ports their systems need - but your going to have to hunt them down for each application you are using. 

I'd stick with whatever the current memory is. Don't mix and match. ECC registered means it is buffered. I think you might find the Crucial memory is the same spec. Of course, depending on what the servers are actually doing, you might not want to compromise on the quality of the components, which might turn out to be the cost differential here, rather than features. 

Assuming you have Windows desktops, you can use GPO to set an automatic shut down after a certain time of inactivity - you don't want to automatically shut down someones computer while they are working late. Ensure corporate software has some enforced auto-save setting so people who have had to run out without shutting down and forgetting to save don't lose work. Make sure you know which users use non-standard software that can't have such a setting enabled, and either exclude their desktops from the rule, or clearly warn them to save before leaving work. That takes care of shutting down and saving power. You can then use Wake On LAN within something like SCCM to wake up computers and install updates at a scheduled time. I'm not sure how this works with things like hibernation or standby. 

Cisco ASA 5505 in remote site, connecting as Easy VPN client to ASA 5510 at HQ. VPN light is green, and shows active security association. However connectivity to some of our subnets is not working. Running shows IPSEC SAs exist for most of our subnets, but not all. Not surprisingly, for subnets where there is no SA, there is no connectivity possible. When this happens, running or power cycling the remote ASA restores the IPSEC tunnels, and access to all the subnets is possible again. When this occurs it is intermittent. No config changes have happened with the devices affected. Here are my questions: 

Normal router should work fine, as long as you are not expecting it to do the physical layer SDSL connection. Connection would be: SDSL Equipment -- Router -- LAN I only say this because it is so common to find combined ADSL modem, router, switch, firewall thought of as just an ADSL router. I have seen a router (ASA 5505) do the PPoE negotation which would normally be handled by the ADSL equipment, but the ADSL modem was still required. Aside - is "ADSL modem" the correct name for the bit that talks to the phone line? 

You could just store a password protected private key on a USB flash drive, and use that in your authentication. Not sure how you would make this easy for the user (depends on your OS), but it is an option. As pointed out, you probably don't want this to be the only authentication, as it can be stolen, lost etc. But it would be cheap and does count as part of three factor authentication, which I guess is what you are trying to achieve. Answering the "something else" part of your question - I've been using Swivel's PINsafe product for a couple of years, which gives what they call 2.5 factor authentication. It doesn't quite give what you are looking for, but after the initial investment in the product, you don't have the cost of distributing any devices, but is pretty secure against all but the most elaborate key loggers. If you were worried about that, you probably wouldn't be looking for a cheap solution :-) 

Best practice would be to continue only allowing key based logins, and just put up with being locked out when you are without your key. I am sure there is some way of reconfiguring your sshd config to allow password logins, but how are you going to secure that? Your security is only as good as its weakest link. There isn't really a "best practice" option between allowing password logins and private key authentication. You have to choose whichever one is most acceptable for you, and just take the problems that come with either on the chin... 

Your Apache config looks ok to me. The DNS record for mydomain.gov.br should point at the same IP address as the www. one, unless Apache is also listening on this IP address - but if it were then your site would probably work. You need to check your DNS service to figure out where the record for the www. free record is being defined. 

This shows that the ASA is configured to use compatible versions of the ASA and ASDM images. Check which versions have loaded: 

Where is your server located? If it is the same location (network wise that is) as your testers so has a local IP address as well as a public IP address, then you could have the server listen on a local IP address in the same range as your testers, and use local DNS to resolve the live site to this IP address. You will still need to edit your vhosts for this, but frankly I don't see any way of avoiding that given what information you have given. 

Where is your MX record stored? Presumably on a local DNS server, as domain3 isn't a public domain. In which case, check where your server is doing it's DNS lookups. If it is going straight to a public DNS server, then you will see this message. Assuming that domain3 is not just an example. 

It depends almost entirely on where the server is in relation to the developer. If both are behind the same firewall, and you trust your local network environment, then opening up MySQL locally wouldn't seem a particular risk. On the other hand, if your server is running on the Internet 24/7 with a public IP address, you want to lock it down as much as you can - cue another link to [Implementing network security on Centos/RHEL Servers] which goes through a number of things better than I could ever do1. In terms of the best way of securing a connection, it depends what you want to do. If just run SQL queries by hand, then SSH is all you need and the above link will get you there in a secure way. If you are wanting to use some GUI front end, then VPN access as mentioned by Linker3000, and succinctly detailed by adirau would be best bet. I wouldn't recommend putting phpmyadmin on as a publicly accessible service, unless you can secure it with more than a username password. If you need phpmyadmin, I'd only access through a VPN tunnel. Don't have a link like $URL$ - it will be probed within minutes, and any future exploits for phpmyadmin will be tried. I've seen my own server logs, and this is right at the top of failed http requests! 

I work on Sharepoint 2007 intranet with 500+Gb of documents. Running a full crawl takes over 48 hours. When we first set Sharepoint up, and it was much smaller, we ran Full crawl jobs on a weekly basis, and incrementals each night. Is there any benefit to running full crawls? Or should I reduce the frequency to monthly - or even do it less frequently than that? 

I have a simple executable to run as a scheduled task on some Windows Server 2008 R2. The executable downloads a file from a couple of locations, then writes the time to download the files to a SQL database. The problem I am having is that when I run the EXE using my account, it works absolutely fine. When I run it as an account set up specially for use with this scheduled task the task fails. It manages to download the files ok, but fails when trying to write to the database. The user used to write to the database is hard coded within the EXE. My account is a Domain Admin, whereas the account to run the task as is not. I am trying to figure out which of the Security Settings in Group or Local Policy might be missing, or if their is something else. The EXE throws an error message: 

It appears they are using CIDR notation for specifying the range of addresses to set to. This is quite valid. Just because this notation can also be used to specify a subnet doesn't mean it is always used for this. It is also used in routing tables, ACLs and other places where it doesn't necessarily imply a subnet (but might). It's use is more flexible than that! Read up more on CIDR notation here. 

Ok - tracked it down. It wasn't a misconfiguration of my network. It was a misconfiguration of the HA settings for the cluster. The VMware hosts are connected to two Dell 6248 core switches with default routing to a VRRP address. This address isn't pingable, and VMware uses the gateway as the isolation address. I discovered this through the error messages for the host - the specific error (can't contact isolation address: ip address) isn't displayed at the cluster level. I fixed this by adding the following values to the advanced options for HA: 

It might be possible, but it will be much easier and more supportable if you generate a CSR from IIS on the Exchange Server and then use that to create a new certificate. Most Certificate Vendors will allow you to revoke a key and replace it with a new one that has the same expiry date, without incurring extra charges. Unless you have a very good reason to do otherwise, I recommend starting over. 

You need to put your .htaccess file in the server folder abc - so you need to check the path to the root of your site, and then put the .htaccess in the abc folder in that root folder. For example, <- put .htaccess here. 

Making changes outside of the change control procedure. Even if any bad thing that happens unrelated to the change you made, your ass will still get kicked as badly by those higher up the pecking order. Of course, practially speaking, some changes have to be made faster than the change control procedure allows, but in each case the risk to career is higher. Sometimes it is better to work to rule and let the system fail, than go against the system... 

See Microsoft Technet article on share permissions for the low down. The short story of it is that the most restrictive permission applies, whether you apply it via Share permissions or NTFS. So you have the option of setting Read/Write permission to the share or folder, then reduced permissions on a subfolder via NTFS. This means you should probably set the least restrictive permission on the share (Everyone get's Read/Write) then restrict further using NTFS. See also Microsoft's Best Practice for Shared Folders. Key things to note: 

You should look at your data and figure out whether you can just add new drive(s) and copy your data across. This entirely depends on the nature of your data and function of the server in question. For example, if it is a file server used for storing user data in shares, you can just move the data and recreate Shares with the same names at the new location. The same would apply for static files served via IIS - you can copy the data to a new location, reconfigure IIS to point at the new location, then delete the files in the old location. However, if the data is being accessed frequently, such as database files, this may not be an option without some downtime for your applications. That would probably be less downtime than restoring from a Ghost image or rebuilding the server. If there isn't extra space within the chassis, you can expand drive space as above using external storage. SCSI, eSATA, iSCSI, NFS or even USB might meet your access requirements and budget allowing you to shuffle data about with minimum downtime. 

I've managed to get certificates onto an Android running FroYo by emailing the cert to an address the phone can access. Opening the attached certificate from the mail client resulted in installation. Unfortunately, Android FroYo doesn't properly support Cisco VPN, so I didn't get a sucessful test however - but the certs certainly appeared to be there. 

I noticed that a traceroute to the remote site included the same IP address, somewhere between our ISP and the ISP the remote site uses. I'm also seeing a message immediately after before saying 

I want to monitor packet loss on my ASA 5505 VPN endpoints using SNMP. This is so I can graph the rates in Cacti and/or get alerts in Nagios. However, I am not sure what SNMP values I should use to measure packet loss. In the ASA I can run to show traffic statistics for the interface connected to the Internet. This shows 1 minute and 5 minute drop rates. 

In some locations, the only option for an internet connection is to use some form of Satellite dish. Like any wireless medium these can be quite tricky to troubleshoot. What factors have you observed which have caused problems with a VSAT connection? What resolutions did you find? What measures have you taken to mitigate against these factors? 

This is just in case it is something simple - if you have a network issue with the crawl server then you won't get any further. Check for any errors under Central Admin > Application Management > Check Services Enabled on this Farm - that should highlight any issues with SSPs etc. Also check your server application logs, and Sharepoint ULS logs (Codeplex has a great Sharepoint Solution for this - LogViewer) Also - can you specify a little more about your environment: 

You may find that during testing of this, web caching may result in you seeing the wrong page. Your browser/OS probably has a local cache, which you can usually bypass by pressing CTRL+F5 to refresh the page. It isn't that likely that a cache outside you control (e.g. your place of work, or your ISP) has cached content for the URL, but not impossible. You can also fall foul of DNS cacheing. You can clear your local DNS cache in Windows by typing and then see whether re-loading the page results in the content you expect. Again, there may be DNS cache in between you and the nameservers for this domain, and they can cause problems too based on the TTL for the record. Finally, it can take some time for DNS changes to be reflected globally (up to 48 hours) so there is a chance that you won't see the results of your change for some time. From what I can see your settings are correct, so probably you just need to wait a while - generally it is a lot less than 48 hours. 

It depends on the configuration of the firewall, and how the apps running on the local machine talk to each other. I think it would be possible (but highly unlikely unless you did it deliberately) to block localhost traffic. More likely what you need to consider is the firewall blocking outbound traffic from your applications, preventing them from accessing services on other servers. That is quite commonly configured, and some firewalls will have very secure default policies that close off most outgoing traffic.