And there lies space for novel solutions. Since it's not just one or two queries (but one third of them), they must have relevant results. If you type in something much too obscure in a Google search, it shan't take longer to return a list of results, but will most probably show you something it inferred you'd like to say. Or it may simply state that there was no document with such terms -- or even cut down your search to 32 words (which just happened to me in a random test here). There are dozens of appliable heuristics, which may be either to ignore some words, or to try to break the query into smaller ones, and gather the most popular results. And all these solutions can be tailored and tweaked to respect feasible waiting times of, say, less then a second? :D 

The list pointed may give you some insights on which features would be nice to select. For example, considering the second most correlated feature, # of google +1's, it may be possible to add some probability of a user making use of such service if he/she accesses many pages with high # of google +1 (infer "user context"). Thus, you could try to "guess" some other relations that may shed light on interesting features for your tracking app. 

So, the only consensus here is the resulting hyperplane, computed from the closest documents of each class. In other words, the classes are attributed to each point by calculating the distance from the point to the hyperplane derived. If the distance is positive, it belongs to a certain class, otherwise, it belongs to the other one. 

Since you are saying that you got box coordinates in the original image, why not reduce coordinates by exactly same scale. For example a coordinate say $(100,100)$ in the image of say $(1024,1024)$ size when resized to say $(256,256)$ will be $(25,25)$ (assuming you didn't crop anything in the mean while), which is in general $$(x_{new},y_{new}) = (\frac{x_{old}*l_{new}}{l_{old}},\frac{y_{old}*b_{new}}{b_{old}})$$ where $l$ is the length of the image, $b$ is the width. In times of fractional answers after reduction make sure your bounding box covers a bigger area. For example, Y-coordinate for the left side of bounding box should be pushed towards the Y-axis while on the right should be pushed towards the ceil value like say you got $25.5$ by reducing, go to 25 when on the left side while $26$ when on right side of bounding box. Similarly up and down for $x's$. Hope this helps. 

The metrics you calculate are of two types, metrics that depict the entire prediction model you have built like which will be same in both the cases of your pseudo code. While the others like says how precise are you in explaining particular class of interest ( can also be expressed this way in multi-class classification, see the diagram). This score depends on which class you had selected as a positive one. If you put positive class as face of your model, then it is called and , if vice versa. Coming to the multi-class classification, the core definition holds the same. Now the matrix will be ( being number of classes). The sample matrix looks likes this. 

As far as practical applications are concerned : Here are a few: 1) Aircraft Engine Anomaly Detection: Input Features can be heat generated by engine, vibrational intensity, fuel consumed etc etc. Here outliers can be sent for testing again and further decisions can be made. 2) Fraud Detection : Features can be features of users activities on a website. We can model probabilities from the data. Identify unusual behavior by checking probability less than certain fixed threshold. 3) Monitoring Computers in a data center : features can be memory use, no of disk accesses, CPU load, network traffic etc. Abnormal behavior here can help predict future breakdowns. Anomaly Detection is done assuming our data has a probability distribution(gaussian). We can plot data to see if thats the case, if not we can make it gaussian using log transforms. Gaussian distribution specifies the regions and probabilities of our data lying in those regions. For example : replace original feature x -> Log(x) or feature x -> (x)^4/3 etc.. Also regarding the threshold value which decides outliers you can play with it and see that with Higher threshold you will be rejecting more entries and this might be required where doctors are trying to isolate cancer patients amongst many normal ones without taking any risk/chances. Again outliers here doesn't mean cancer patient but definitely worth a medical test. And you can set it to lower value if you are getting too many normal data flagged as outliers. We have skewed data sets since we have more examples of one kind than the other. For example when we get air craft engine data we might just have data for few bad ones and mostly for good ones.Use of cross validation data is suggested.F1-Score is a pretty good metric to evaluate the performance of the algorithm. 

The author has taken as in this case. The derivative of this function can be re-written like $$\sigma(z) = \frac{1}{1+e^{-x}}$$ whose derivative $$\sigma'(z) = \frac{e^{-x}}{(1+e^{-x})^2}$$ which can rewritten as $$\frac{1+e^{-x}-1}{(1+e^{-x})^2} \rightarrow \sigma(z) * (1 - \sigma(z))$$. This is subtly mentioned in eq.3 of chap1. 

As @Emre has pointed out in comments, you need a pandas custom aggregator. So since you need a custom by . Create a custom aggregator as 

This is because you were importing a class as name 'scalar' and modifying it by assigning value obtained after transforming. Just change 

My idea is to basically smooth till you get your head and shoulders i.e., three maxima. Warning: Smoothing though reduces the amount of noise (not in literal noise sense) on the curve, it tends to shift the curve from its original position to represent it. A sample Python implementation will be like 

Frankly, I don't think it matters, whether you use city-block or any generalization of Minkowski distance in this case, as long as the metric don't give different expected values when you are calculating string/vector distances. You can use city-block distance as it is computationally faster than Euclidean if you had got many combinations to calculate. 

I think the documentation is kind of self-explanatory here. Fit takes in array of size in which each element is the class of the datum or if the data point belongs to multiple classes, the input would be obviously of size . That is what you gave in as input in your example. Each point can belong to any of the three classes. That is why you have as number of classes. So as mentioned in the documentation if you try 

Although I don't think that using or not MapReduce is restricted information inside Google, I'm not conversant about this point. However, Google's implementation of MapReduce (which is surely not Hadoop) must have lots of optimizations, many involving the aspects discussed above. So, the architecture of MapReduce probably helps guiding how the computations are physically distributed, but there are many other points to be considered to justify such speed in querying time. 

The answers presented so far are very nice, but I was also expecting an emphasis on a particular difference between parallel and distributed processing: the code executed. Considering parallel processes, the code executed is the same, regardless of the level of parallelism (instruction, data, task). You write a single code, and it will be executed by different threads/processors, e.g., while computing matrices products, or generating permutations. On the other hand, distributed computing involves the execution of different algorithms/programs at the same time in different processors (from one or more machines). Such computations are later merged into a intermediate/final results by using the available means of data communication/synchronization (shared memory, network). Further, distributed computing is very appealing for BigData processing, as it allows for exploiting disk parallelism (usually the bottleneck for large databases). Finally, for the level of parallelism, it may be taken rather as a constraint on the synchronization. For example, in GPGPU, which is single-instruction multiple-data (SIMD), the parallelism occurs by having different inputs for a single instruction, each pair (data_i, instruction) being executed by a different thread. Such is the restraint that, in case of divergent branches, it is necessary to discard lots of unnecessary computations, until the threads reconverge. For CPU threads, though, they commonly diverge; yet, one may use synchronization structures to grant concurrent execution of specific sections of the code. 

This statement can be explained in general, not only for neural nets. Assume you know the relationship is linear for your data, but you got some error and you are not happy with this, then say you add an extra term say $ax^2$ to this equation and the model now has an extra parameter to tune and this makes our line into curve. For a neural net we basically start by random weights from a distribution and then we keep reducing them until we feel like our training error is cool. Assuming that you are using a multi-layer perceptron (fully connected), pruning is one of the techniques to remove weights from the model which might not be so contributing still making sure your model still behaves well with lesser number of parameters. SVD on this weight matrix can be one of the useful techniques to do this. But how many weights to keep depends on how many singular values you keep for reconstructing the matrix. I heard that early stopping can reduce overfitting, but I had never really done this, so can't really give my opinions about this. Hope this helps. 

The diagonal elements explains the number of 's predicted as . Now there are precision values for each class. Precision for class is how many values were truly predicted as divided by how many are predicted as (FP's included), which is the sum of the first column. Not but not the least if you adamantly want a precision like metrics for the entire model, you got methods, which are helpful in giving a combined metric. This blog post explains it pretty well. Hope this clears something. 

I suggest , go for Anomaly detection: Anomaly Detection is done assuming our data has a probability distribution(gaussian). We can plot data to see if thats the case, if not we can make it gaussian using log transforms. Gaussian distribution specifies the regions and probabilities of our data lying in those regions. For example : replace original feature x -> Log(x) or feature x -> (x)^4/3 etc.. Also regarding the threshold value which decides outliers you can play with it and see that with Higher threshold you will be rejecting more entries and this might be required where doctors are trying to isolate cancer patients amongst many normal ones without taking any risk/chances. Again outliers here doesn't mean cancer patient but definitely worth a medical test. And you can set it to lower value if you are getting too many normal data flagged as outliers. We have skewed data sets since we have more examples of one kind than the other. For example when we get air craft engine data we might just have data for few bad ones and mostly for good ones.Use of cross validation data is suggested.F1-Score is a pretty good metric to evaluate the performance of the algorithm. To get a proper hold on this topic , I also suggest go through anomaly detection course videos by Andrew NG in machine learning on Coursera. Free course and very nicely made. 

Please check this website: $URL$ its a collection of data portals from around the world. Might find here what you are looking for. 

Given a set of weights (more than one being varied), they do not linearly add to produce an output like $y=w_1x_1+...+w_nx_n$ but rather non-linearly like $y=w_1w_2..w_n*x$ (each $w_i$ is a dimension in the hyperspace) And I think it would become more clearer from the statement "Output is linear to any weight $w_i$ but non-linear to weights $w_i$". 

Overfitted model is something that shows very less error on your training set and then when you test it on a test set, it fails badly. This might be due to class imbalance and you might be giving something that it has not seen or due to using more number of features than you were supposed to, making the model to fit in every datum. Wiki page is nice overview read for this. Answering your next question, F1 score as might already be knowing has two components in it, Precision and Recall. Check whether there are reasonable number of TP,TN when compared to FN, FP and if you find those numbers to be fair enough, then F1-score is just a metric for you. There is no benchmark score for an awesome F1 score. The decrease might be just because your model has just more data to predict now. So overfitting can lead to less F1-score but reduced F1-score does not mean overfitting always. 

I think what the statement meant was when given weights $w_1,...,w_n$ are fixed, output is linear proportional to $x$, but as it mentions 

Sparse matrix is a matrix that has most of its elements as zero. Matrix on the other side of it, is Dense. csr_matrix creates an all zero element matrix and if you are updating the elements as