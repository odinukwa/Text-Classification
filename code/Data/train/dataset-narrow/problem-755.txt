There's no answer that applies in all cases. In general, however... If the lookup list is small and you can cache it(or use a cache data source), there's not much performance penalty to doing it in SSIS. If you want crossref a list of 50 location codes to names of cities, go for it. It's nice to see all the process on-screen in one place, rather than buried in sql statements. TSQL will be better-performing in most cases, since it knows the most about the data and the query optimizer is always going to be smarter than you. If all the data is in one DB, you can hide a lot of complexity in a sql query source. If the data is spread out across different systems, the middle ground is to to do an SSIS merge join from each system. Trying to do that at the RDBMS level is madness. Always do the sorting in the source query, though. SSIS Sorting is almost always a bad idea. 

This query works in the simplest case, but does not allow adding attributes to the root element, which is required in my case. 

I have a server with a working installation of the Sql Server 2012 SSIS Catalog. I need to set up an additional instance including the SSIS Package Store service as an interim step while the packages are being re-written. The Package Store is a per-server feature, not a per-instance feature. Can these two features operate side-by-side? 

Give something like the below a try... You'll obviously need to plug in your variables for your environment, check the data types (may need to add logic to keep leading zeros?), change from the final temp tables to your regular table(s), etc. Works fine for me for import from XML files to temp tables without deleting the files afterwards but adding logic to delete files from the UNC path shouldn't be too difficult with another xp_cmdshell command. 

Condsiderations for security options in SQL Server (Two things to mention for typical simple configurations) 

Once you get all the needed information from the vendor and confirm the login is defined on the primary instance of Microsoft SQL Server, you can execute a CREATE LOGIN statement and pass that information in to create the login to match how it's defined on the primary server. Example T-SQL 

I think it depends on how well you want to lock it down really, and how trustworthy they are for what you are allowing them to have access to. 

Below is a script of the T-SQL part of this process which I run after doing the above 4 steps, and it always works in my environment to accomplish what seems to be similar to what you've explained that you're trying to accomplish. I also have to ensure the AD account has a strong/complex password, and set it to never expire. 

I have a cmdexec step in a sql agent job that includes a redirection into a file at the end. It works as expected from a CMD shell running in the context of SQL Agent service account and produces a file in the desired location. If I run it as an agent job, however, the step 'succeeds' but never produces the output file. In both cases, the service account obviously has filesystem permissions and system rights sufficient to perform this action. Is the cmdexec environment more restrictive in someway than just running cmd.exe? 

This is almost exactly the example used for 'factless fact tables' in analysis services, which illustrates the point. The thing represented by the table is a 'booking', which has no natural primary key. The true key would be a composite key of location-timeslot. You need to create an artificial primary key if you don't want to do that or the DBMS does not support that. You need at least these tables- Location(room #, Capacity) Timeslot(day,starttime), module(code,name). It's not clear if the person is tied to the module or tied to the booking. It's not clear if the extension is tied to the person or the location. 

I'm starting a few new database projects and I'm attempting to create them at Data Tier Applications. There are two items I'm not able to find documentation for. I would like to set the db owner to SA and set the initial filesize and growth rate. Even if those items are outside the scope of the app, I would expect that there would some way to specify that at publish time, either in SSDT or SSMS. I can find no documentation either way. Is this the case? 

Members of this role in the database you create it on should only have access to the applicable metadata as long as they don't have other permissions to database objects such as explicit SELECT access to the a table or a member of the db_datareader fixed database role. 

Can you point the 'full path' to the UNC path i.e. instead and see if that works? Just like when you map the "X" drive to just use that in the full path of one of your packages and run to see if it'll work. If one of your jobs work like that (assuming most all are setup the same and this way, etc), you can probably script out the SQL Agent jobs through SSMS by pressing F7 (once SQL Agent jobs is highlighted), selecting them all from the right pane window, right click, then create to new query window, then do a mass CTRL+H and do a find and replace to replace with , and then run that. Just be sure the SSIS proxy account or the SQL Server Agent account has appropriate NTFS and SHARE permissions where the SSIS packages reside to read them. 

The people or security contexts running the expensive queries on the wrong instance should be identified and then notified to change their processes on the instance and DB to start running their stuff on the instance you want those run on. Since the secondary is read-only/standby, we would assume these expensive queries are SELECT statements only where the primary DB that's not in standby would have to run queries that update data or modify objects regardless of the expense. You could also look into why some of these queries are so expensive in the first place, see if adding indexes would help, rewriting queries for performance tuning, etc. I think that'd be a good root cause type solution. It seems like it'd be tricky to do this with one of more AD groups though as the changes are replicated from primary to secondary and secondary would be in standby to accept transaction log changes from primary when those are applied so I'm not sure how you would accomplish this in that sort of configuration with log shipping. I'm also not certain how often your tran logs are being restored to secondary as I thought when LSRestore jobs run, it disconnects all session on secondary until those transactions are committed, so I assume it's once a day or not too often or people would be screaming about their queries, etc. getting disconnected during normal hours. If once-a-day restores are occurring for LSRestore jobs on secondary, then this would mean the data gotten from that server is 24 hours old or however long between your LSBackup, LSCopy, and LSRestore jobs. So who can use which DB with this regard may depend on how fresh their query results need to be from the business side, etc. Lots of factors to consider here but getting to the root cause and having bad performing queries tuned, adding indexes, etc. may be the best solution as well as having the people with access take responsibility with their processes to not hose up the performance for others when they run their stuff. 

I'm trying to make a multi-statement table valued function and failing due to what I think is a limitation of functions and CTEs. The intended function defines an input parameter(@Param) and and output table(@ResultTable). It then executes a complex insert statement involving a CTE into that table variable which is (of necessity) terminated by a semicolon. I then attempt another complex update statement to that table variable involving a CTE and receive an error saying that "Must declare the scalar variable "@ResultTable". Apparently it has dropped out of scope somehow. I have used this sort of pattern in the past, so my only thought is that the CTEs seem to limit the scope in some way. Is this a known limitation? 

I'm troubleshooting a SSRS security issue where one user is being denied access, while another user with apparently identical settings is working as expected. I'm seeing "The permissions granted to user 'mydomain\myAccount' are insufficient for performing this operation" in the browser, but curiously NOT seeing the (rsAccessDenied) at the end. Is this indicative of something that can help me troubleshoot the issue? 

In my specific case, this was a result of cell security. The role did not have access to all the fields returned in the drillthrough action. I missed this because there are two very similarly-named fields and my eye did not notice. It's still odd that a security denial creates a query timeout, but at least this note is here for the next person. 

You can use pt-query-digest which is part of the Percona Toolkit to identify queries that are running long. It can analyze queries from General Query log, Binary Log and the Slow Query Log. Usage: to get the slowest queries from logs, where slow.log is the log file name. The log file is usually in the default directory. The option can be used to save historical data and is helpful for analyzing Query performance over time. If the slow-query-log option is disabled, you will need to enable it to log the query information. Refer to the Slow Query Log official documentation on how to enable it and for all the available configuration parameters. 

From this its clear that or is the same for the table and all the indexes under it and is nothing but the for an index. 

Refer to Partitioned Tables and Indexes on MSDN. is the column name that's commonly known as is the index_id. Lets do some testing now: I have used AdventureWorks 2012 from CodePlex. 

In the above output, index_id is just an id for the indexes. 1 for Clustered index and the others (2-7) for other non clustered indexes. 

In SQLServer, there is no option to backup/restore just the tables. You could create an SSIS package to import data from specific/all tables and schedule it using the SQL Agent or run manually. As both the databases Production and testing are on the same server, the data load may be faster compared to pulling data from a remote server. To minimize the load on the production database, restore the backup to a new database and then import the data to the Test database. If you drop and recreate the tables for the refresh make sure the indexes are created after the data import. Or if you choose to bulk import on to an existing table, drop the indexes and recreate them later and change the recovery model to simple or bulk logged. If you have few very large tables, you can probably consider moving to separate file groups and do a partial backup and restore. You can also try third part tools like Dell Litespeed, Idera virtual database or Apex SQL restore etc. for object level restore. 

I'm trying to perform the same scenario as in the following link; Create a SSIS Script Component as a Data Source that uses a pre-existing HTTP Connection Manager to retreive a page with GET and emit rows into the Data Flow pipeline. $URL$ My target platform is SQL Server 2008 and therefore C#. The MSDN documentation gives examples of File and SQL Connection Managers but not HTTP ones. $URL$ The specific problem is that I can NOT figure out why there's no HttpClientConnection constructor in my current context. The MSDN documentation of that class does not seem to apply in the case of Script Components and translating this to something useful is apparently beyond me. $URL$ My non-working code looks like this - 

I have a number of XML Schema Definitions describing business objects. It's unclear to me what the correct way to deploy these is if my intention is to make typed xml columns. The name Xml Schema Collection implies that more than one Xsd Document definition can exist in a collection, but any example I look at shows a Collection as the specific constraint on the column, not a document within it. Do I create a schema collection called 'BusinessObjects' and create individual Xsd Documents inside it? Do I make one Collection for each Document? 

You need to work your way up the networking stack to determine where the issue is. Can you ping the destination from the source? Can you telnet to the postgres port(5432 by default) from the source? Can you connect to the postgres service with a management tool(pgadmin or psql) from the source? 

It would seem the explanation would be that your your committed transaction sizes with the operations are HUGE. Simply make the commit\batch size parameters of your logic in SSIS or your execute TSQL in the package of a smaller size. Try testing with 100, 1000, 10000, 100000, and so on to see what gives you the best result and prevents the issue from occurring. If the transactions are smaller, once the committed transactions are committed in recovery mode, then the log space of the committed transactions can be reused by subsequent (or other) transactions. 

Note, sometimes with the SQL Server and Visual Studio and other components, software, etc. as such, you have to install them in the order based on their version or things in the Windows registry, DLL component, etc. just get out of wack. For example, if you have some component (or software) of the 2005 and 2008 version you need, it's best to install the oldest first (2005) and then once that is complete install the newest (2008). I've seen this too many times where someone installs some old component or software as such on top of newer software, etc. that's already installed then the system start having strange problems. If all else fails, you can backup your important data, and then do a fresh wipe of your HD and install a fresh copy of Windows. Microsoft has no way of emulating all software on everyone computers in the whole world, so perhaps when you upgraded and then downgraded, something just got corrupt and there was something on your system that they didn't test with, etc. I typically always do fresh system upgrades rather than in-place upgrades but I'm in a business environment mainly too. 

After investigating a production issue, I have found a number of references to SSIS packages getting stuck in 'validation' when run with DTEXEC or through as SQL Agent job steps. Specifics of my situation aside, is there a technique to make this more robust so that in a production environment, I am notified if a job starts, but does not complete? Is it possible to have a timeout period on validation so that the execution explicitly fails? Or have a way to specify number of retries? 

Do maintenance plans provide any functionality beyond their component parts? It would seem that the only value added is the convenience of grouping subplans into logical bundles, and the simplified GUI for creation. All the actual work is being done by the underlying SSIS tasks which are being scheduled as agent jobs. Is there anything more to it than that? 

Import it as a string and do a derived column to convert it to a float. Whatever rows fail conversion will be re-directed to the error output and you can at least see what the problem is. Flat File connections fail completely when they encounter problems. It's generally more productive to bring them in as loosely as possible and adding structure inside SSIS. You almost certainly don't want to be using floats anyway. Use 'numeric' once they're inside the package. 

I have a drill-through problem apparently related to security. Users in one role are seeing timeout failures when trying to invoke a drill-through action. They have permission on the action through that role and seem to have all necessary dimension and cell security rights. Profiler is not showing me any obvious reason this is failing. What else can I use to debug this? If I modify the user's role membership, the action works as expected, so I'm confident this is related to security somehow.