Consider a kind of automata similar to common DFAs or NFAs where it is possible to represent succinctly linear chains of states. In other words, an automaton like this: 

The answer was buried in a small section of the same paper that I was citing. Adding past operators to TPTL, in contrast of what happens with LTL, causes a huge increase in complexity as the satisfiability problem becomes non-elementary. The fact is proven in the paper by showing how a mixture of future and past operators, combined with the freeze quantifier, can emulate an arbitrary first-order existential quantifier. 

This question is important in functional programming since usual representation of graphs are inelegant and inefficient to use in purely functional languages. A nice approach was presented at ICFP last year: "Algebraic Graphs with Class (Functional Pearl)", by Andrey Mokhov. I don't know if it fully answers your needs, but it can represent algebraically a wide range of different types of directed and undirected graphs. 

where the thick edge represent the chain of states, where each state is connected to the next by a single edge and all the edges are labeled in the same way, in this case by $a$. So this is not really a counter or anything fancy, it is just a succinct representation of a very limited special case. By succinct, I mean that by representing the $k$ parameter in binary, the second automaton can be represented in logarithmically less space than the first. Let's call this kind of automata the "succinct automata", SA, so say DSA and NSA for short for the deterministic and nondeterministic variants. Now, my question concerns the complexity of boolean operations over this kind of automata. In details: 

My feeling on all of these after having though about it a bit is that an exponential increase in size is needed in most of the cases, or that the results must be nondeterministic. So the question is really: is anybody aware of a place where this kind of problems have been addressed? Has this variant of finite automata being studied before? 

Given two NSAs $\mathcal{A}$ and $\mathcal{B}$, is it possible to build the NSAs for $\mathcal{L}(\mathcal{A})\cup\mathcal{L}(\mathcal{B})$ and $\mathcal{L}(\mathcal{A})\cap\mathcal{L}(\mathcal{B})$, of size still polynomial in the size of $\mathcal{A}$ and $\mathcal{B}$ (i.e. without paying for the unrolling of the chains before computing the results)? Is it possible to compute those operations on DSAs (deterministic) guaranteeing that the resulting automata stay deterministic (and still polynomial size)? Is it possible to determinize an NSA with only a singly-exponential blowup (i.e. without paying for the unrolling of the chains before paying for the classic determinization)? 

It seems to me that the macro language employed by $\TeX$ can maybe be seen as some kind of term rewriting system or some kind of programming language with call-by-name scoping. Even modern implementations of the $\TeX$ engine (e.g. $\mathit{Xe}\TeX$) interpret code in a quite direct way and I'm not aware of any attempt at optimizing the execution (like modern optimizing interpreters can do). However, devising correct optimization passes for a language like $\TeX$ is going to be very difficult because of the "action at a distance" that macro redefinitions can have, and the ability of redefining macros by calling them by name. So implementing an hypothetical optimizing interpreter for $\TeX$ sounds a very difficult problem in practice but also a very useful one, since $\TeX$ is used all over math and science and slow compilation times are a known drawback of the system. Note that the majority of time is spent interpreting code, not computing the actual typesetting, especially when computationally heavy packages are used (such as ). Maybe a formal semantics for the language could be a start to address the problem. So has the semantics of the $\TeX$ programming language ever been formalized? 

They show that for linear objective functions (where the objective value is the sum of element weights), the greedy algorithm will work exactly on the structure they define as a matroid embedding; They give a similar characterization for so-called bottleneck objectives (where the objective value of a set is equal to the minimum over the individual element weights); and They give an exact characterization of which objective functions (beyond linear ones) are optimized by the greedy algorithm on matroid embeddings. 

A really simple solution: Build a suffix tree for the first string, $S$, and annotate all nodes with $s$. Then insert all suffixes of the second string, $T$. Annotate nodes you pass through or create with $t$. The path label for any node that is annotated with both $s$ and $t$ is a substring of both $S$ and $T$. (See, for example, these lecture notes a quick web search turned up.) 

You could get a Google Scholar profile, and it'll keep feeding you recommendations it thinks will be relevant to you, based on your publications. 

Shooting from the hip here, but I think this might, in fact, be a simple problem to solve in pseudopolynomial time, using dynamic programming (DP). Not sure if that's acceptable, or if you need a polynomial-time algorithm? 

Actually, the complete and general description of a problem that can be solved by a greedy algorithm is a matroid embedding, which generalizes both the concept of a matroid and that of a greedoid. The answer is no—a problem solvable by a greedy algorithm need not have a matroid structure, but it will have the structure of a matroid embedding (which is, alas, much more complicated). A mental model for some of this could be finding minimum spanning trees. The structure used by Kruskal's algorithm is a matroid, but that used by Prim's algorithm (which requires a start node) is not. (It is, however, a greedoid—and a matroid embedding.) Helman et al. (1993), in their paper An Exact Characterization of Greedy Structures define their notion of a greedy algorithm in terms of set systems, which is the same formalism that is used for matroids and greedoids. A set system $(S,\mathcal{C})$ consists of a set $S$ and a collection $\mathcal{C}$ of subsets of $S$, the so-called feasible sets. A basis for the set system is a maximal feasible set, that is, a set that is feasible but not contained in any other feasible set. An objective function $f:2^S\rightarrow\mathbb{R}$ associates each subset of $S$ with a value. An optimization problem, in this formalism, consists in finding a basis of maximum objective value for a given set system and objective function. The greedy algorithm, defined in terms of this formalism, is quite simple: You start with the empty set, and successively add a single element until you reach a basis, always ensuring that (i) your set is feasible at each step, and (ii) the element you add maximizes the objective function of the resulting result, wrt. all the alternative elements you could have added. (That is, conceptually, you try adding all feasible alternatives, and choose the one yielding the highest objective value.) You could, perhaps, argue that there might be other forms of greedy algorithm, but there are several textbooks on algorithms and combinatorial optimization that describe this set-system based algorithm as the greedy algorithm. That doesn't prevent you from describing something that doesn't fit, but could still be called greedy, I suppose. (Still, this does cover anything that could potentially have a matroid structure, for example, though it is much more general.) What Helman et al. do is that they describe when this algorithm will work. More specifically: 

(as |X|=5, three new elements were added and mapped to 0). Then for $x\in X$ the projection of $x$ to $p$, $p(x)=(d_1,d_2,\dots,d_n)$ such that $d_i=p_i$ for all $p_i\in \{0,1\}$ and the $ith$ value in $x$ if $p_i=*$. For example, $x=(0,1,1)$ and $p=(1,*,0)$ will result in $(1,1,0)$. It is assumed that most of the classes in the literature are projecton-closed. Consider the concept $f_3$ and $p=001$, the projection $f_p$ is a new concept such that $f_p(x)=f_3(p(x))$ for every $x\in X$. As $p$ projects every $x$ to $001$ and $f_3(001)=1$ we have $f_p=\{1,1,1,1,1,1,1,1\}$ . If my understanding is correct then any concept class where initially $|X|\neq 2^n$ will not be projection-closed. Is this true? I'm 99% sure I'm wrong but don't know where. 

Given a $k$-regular graph $G$, the number of acyclic orientations $Acy(G)$ is $\chi(-1)$ where $\chi$ is the chromatic polynomial of $G$. How many bipolar orientations does $G$ have? Is there an upper bound for it? I assume it should be exponentially lower than $Acy(G)$ but didn't succeed in finding a known result connecting these two numbers. 

Given an undirected graph $G(V,E)$ and a bipolar orientation $s$ over $G$, consider the problem of identifying $s$ by finding the minimum number of edges such that when orienting them in a particular way they give arise only to $s$ as bipolar orientation. An orientation $s$ for a graph $G$ is bipolar if $s$ is acyclic and has a single source and single sink. That is, find the minimum subset of edges $\hat{E}\subseteq E$ such that if we orient them $\vec{E}$, the only bipolar orientation for the mixed graph $(V,E-\hat{E},\vec{E})$ is $s$. Is this problem known? has been tackled before? Any pointer is appreciated. 

A boolean function $f(x_1,x_2,\dots,x_n)$ is $k$-Junta if it depends on at most $k$ variables. Consider the class $\mathcal{J}_{\leq k}$ of all $k$-Juntas over $n$ variables, what is the VC dimension of this class? Or at least is there any known method to construct the largest shattered set for small values of $k$ say when $k=1$. 

I am wondering why some methods transform the underlying DAG-based graphical model (Bayesian Networks for example) to a joint tree$^1$? What are the advantages? I believe it's for computational purposes. If that's the case under what circumstances it is not recommended to transform the underlying DAG to jointtrees? Speaking about graphical models in general (whether they are probabilistic or not), is there some guidelines when to transform them i.e. decompose them? 

Let $H$ be a hypothesis class with VC dimension $d$. In supervised learning, we need almost $O(\frac{d}{\epsilon})$ random labelled examples to return a hypothesis within $\epsilon$ from the target (separable case). Active learning potential lies in its exponential saving in terms of the number of queries required compared to the supervised learning. This big saving in known for some structures ( for example learning threshold functions requires $O(\frac{1}{\epsilon})$ examples while we can achieve the same goal with $log \frac{1}{\epsilon}$ in active learning ). What characterizes this exponential reduction in general? Are there certain properties such that if $H$ satisfies them then active learning would help? 

The definition is different from restricting boolean functions to a specific range $A\subseteq X$ and can be stated as follows : 

Although others have pointed out the answer. I thought I may make it self-contained and show why teaching dimension is the answer. Consider a concept class $C$ over input space $X$. A set of elements $S\subseteq X$ is called a teaching set for a concept $f$ if $f$ is the only concept in $C$ consistent with $S$. Let $\mathcal{T}(f)$ be the set of all teaching sets for $f$ and define TD$(f,C)=min\{\ |S|\ | \ S\in \mathcal{T}(f) \}$ to be the teaching dimension of $f$. i.e., the cardinality of the smallest teaching set TS$_{min}(f)$ in $\mathcal{T}(f)$. Similarly, consider TD$(C)=$max$_{f\in C}$TD$(f,C)$ to be the teaching dimension of $C$. The minimum number of queries needed to identify $f$ is TD$(f,C)$. This happens when the query strategy uses the sequence TS$_{min}(f)$. As for any fewer queries we have at least two concepts consistent with it. And TD$(C)$ is the minimum for any $f$. 

As far as I can see, the value of each item depends on which bin it is added to. Its full quality if it is added to its primary preference, and a reduced quality (reduced by -0.5 and -1, respectively) for its secondary and tertiary preferences. Do I understand you correctly? In this case, this problem can be formulated as a min-cost flow problem, which resembles min-cost bipartite matching (but with the added twist of bin capacity). I.e., it is not an NP-hard bin packing problem at all (unless P=NP). Construct a flow network with a source, a sink, and two "layers" of nodes, corresponding to the items (first layer) and bins (second layer). Add edges from the source to the items (zero cost, capacity 1) and from the bins to the sink (zero cost, capacity equal to the bin capacity, i.e., from 1 to 3). From each item, you add an edge to each of its primary, secondary and tertiary bins, with capacity 1 and a cost of its adjusted quality multiplied by -1 (to go from a positive value to a negative cost). Now just run a standard min-cost-flow (or min-cost max-flow) algorithm to get your answer. Of course, not all items will be matched if the total capacity is less than the number of items, but the match will produce the matching that gives the greatest total (adjusted) quality. If you don't want to muck about with min-cost flow, you could split each bin node into multiple nodes (the number of nodes corresponding to the bin capacity), and duplicate the edges from the items. So if an item has an edge with a given cost to a bin node with a capacity of 3, you'd now have 3 bin nodes, and that item would have an edge to each of them with the given cost. You could then just use an algorithm for min-cost bipartite matching, such as the Hungarian algorithm. (You will now no longer have a source or a sink, of course.) This latter version is probably more practical to implement, and libraries for the Kuhn-Munkres algorithm are available in multiple languages. 

For every linear objective function, $(E,\mathcal{F})$ has an optimal basis. $(E,\mathcal{F})$ is a matroid embedding. For every linear objective function, the greedy bases of $(E,\mathcal{F})$ are exactly its optimal bases. 

Assuming that you’re using BFS — or, more likely, a bidirectional BFS — and that you have no guiding heuristic (for an A* search or the like), what you’d probably like to optimize is the time it takes to consider the neighbors of each newly discovered node. After all, if each node has 100 000 neighbors, but there are only about 1 000 000 nodes in total, most nodes will be irrelevant (i.e., already visited) really fast. The “normal” approach would be to have, say, adjacency lists for every node, and some global set data structure (e.g., a hash) for determining whether you’ve already seen a given node. This would keep the “already visited” checks fast, but you’d have to wade through lots of irrelevant neighbors. One alternative would be to do it the other way around. Keep a global structure (like a linked list) that lets you iterate over the remaining relevant nodes (and remove the ones you visit), and then have a lookup-table for every node instead. If the node you’re looking at in the global list is found in the local lookup-table, you add it to your queue and remove it from the global list. That way, the number of potential nodes you’d look at would (probably) decrease quite a bit. This approach would only help after a while, though; at least in the first iteration, it would be better to iterate over the neighbors of a given node, and to look them up in a global look-up table. You can do both, however… For the local tables, you could use some compact (possibly perfect) hash tables that would let you efficiently check for membership, as well as iterate over the neighbors in linear time. Or, if you’d like to keep things simple (probably a good idea in this case), just keep the neighbor IDs in a sorted array, and use bisection for lookup checks. For the global list/table you need something more, however. You’d like the structure to do several things: