I'd consolidate the log files first, before worrying about the right size to set the log. To do that, you would need to use the clause in a operation, so that the second file can be emptied out and then deleted. Once you are back to a single log file, you can then shrink right back down to zero (to remove any fragmentation), and then resize it appropriately, to ensure good distribution of Virtual Log Files (VLFs) inside the transaction log file. There's no way to know what the "right" size is for your log file until you run a normal workload against the database, and see how big it gets. Once you know that, you shrink it down again, and then resize back to that value. Make sure you set a fixed growth setting appropriate for the performance of your underlying I/O subsystem, and not a percentage file growth. You might want to set it to 100 MB, or 1 GB. It's entirely up to you. Regarding , all it does is free any inactive portion, at the end of the transaction log file, back to the operating system. No data inside the log is moved. If there's no inactive portion of the log at the end of the file, no space will be released. The Simple Recovery Model does not allow point-in-time recovery, but I think you know that. If you want to do point in time recovery, you'll need the Full Recovery Model, and perform regular transaction log backups. 

Being that this is Access, you'll have to check the documentation for the statement, for replacing values with a . However, once you create this query, you should then be able to edit it in the designer. 

There's a lot to unpack here. You said in the comments that it's a vendor-built SQL Server database, around 1 TB in size, and performs poorly. You're hoping that moving to the cloud can address performance and allow remote access. That's two separate things, so I'll answer them separately below. (My answers are tilted to Microsoft, because that's what I know. Prices are roughly equivalent for Amazon Web Services.) Performance With vendor systems it can be tricky to make code changes. For that reason, I'd focus on making sure SQL Server (and the operating system under it) is configured correctly: 

You'll need to use Notepad with administrative rights, to be able to make any changes to that file. (However, I'm voting to close this question as a duplicate of the one you posted.) 

Yes I see that fragmentation all the time with GUID columns, due to the random inserts and page splits. In your case, this could be a result of a non-standard fill factor to account for GUID fields. A clustering key should be on an ever-increasing, narrow data type, like or , and the fill factor set as close to 100 (or 0) as possible. That all said, rebuilding a clustered index due to fragmentation is a time-consuming process, and it generates a lot of transaction log, so it's generally more accepted to simply update the statistics fairly frequently. You can worry about fragmentation less frequently, say once a month. 

SQL Server native backups to a network share, or "local" drive if you're on an Azure VM (I tend to avoid writing directly to Blob Storage for backups, because it's slow). A process that copies these backups to "offsite" storage. In most cases these days, this is Azure Cool Blob Storage, with some form of Geo-Redundancy. My process might be AzCopy, or PowerShell, a batch file, or my own Database Backup Sync tool. It doesn't matter how you do it, as long as you're doing it. The point is to get a copy of the backup files as far away from the source as soon after the backup as possible. 

A lock will be taken by whichever process accesses the row (or table) first, and the second process will have to wait until the lock is released. This happens automatically. Occasionally, there will be a deadlock where two processes attempt to update the same record, and one of these will be killed by SQL Server. The resolution there is to retry the process again. In your example, if two users try to update a row at the same time, the first user there will change it, and then the second user will change it again once the lock is released. That's literally how it was designed to work. If you want to prevent the second user from changing the record because the first user already changed it, that's application logic, well outside of the scope of SQL Server. 

Do this. Add the extra column. The cost of the additional memory is nothing compared to anything else you try. 

You will have to lock the table if you want to add a Primary Key. However (and this will require some storage space) ... 

This is the cumulative total of all non-clustered indexes (keeping in mind that each of them contains a copy of the clustered index). 

No. I would create my test database with the same schema as the production database, and then write a custom script to sync the data in the primary filegroup (this is fairly easy as your production system is the source, so the sync is only in one direction). Then with the filestream data, I would stub the files, which could be done any number of ways. Off the top of my head, I'd somehow generate zero-byte files on the test side. 

Why not have always populated? When anyone who creates a shopping bag, they get an auto assigned, but there's another field on the account table with a boolean flag set true by default. You could keep things like customer details in a separate table with a 1:1 map to the account table, for customers who do create accounts. Your challenge then is that you might have repeat customers with different s, but might be worth it depending on how likely customers will create accounts. 

There is no way to recover a transactionally-consistent database from only half of the backup media set. The problem is that, while a full backup contains the data file, there is also a portion of the transaction log, at the end of the backup file, to ensure the restored database is transactionally consistent. Even if you did manage to recreate portions of the MDF file from this backup, you would still be missing the transaction log that rolls back in-flight transactions and rolls forward completed transactions that took place after the backup started. Making matters worse, if the striped backup is written out as interleaved files (which I believe it is), you'd have one out of every two 8KB data pages, and a corrupted log at the end. There's no functional way of establishing any consistency with this kind of recovery. 

Maybe. SQL Server does not care if you create duplicate indexes, but just having the same number of rows does not make it a duplicate. Read more here about determining if an index is a duplicate. The reason there are 830 rows in each index, is because the table has 830 rows, so each index contains the same number of rows, each of which maps back to the row in the clustered index (depending on which column(s) are indexed). 

Don't think of it as a backup, think of it as the fastest (RTO) way to recover the database to a consistent state, at a point in time (RPO) that satisfies the business requirements. Stated more plainly: 

What is (upper case)? That's for the code page. You should have according to MSDN. I think you mean (lower case), which is for the character data type. 

You've got some code, somewhere, converting your ID to an field. I don't know how or why this is happening, but that's causing it to ignore your index, resulting in a full scan instead of a seek. 

The reason your process isn't working is that it's waiting to release the inactive portion of the log using a transaction log backup. Switching to Simple Recovery Model bypasses this issue. If you really need to shrink down the transaction log, so that you can regrow it (to reduce the number of VLFs, for instance), this is the process: 1) Switch to Simple Recovery Model 2) Run a CHECKPOINT and DBCC DROPCLEANBUFFERS (just in case) 3) Shrink the log file 4) Switch back to Full Recovery Model 5) Grow the log in reasonable increments (I use 4000MB increments) 6) Set autogrowth to a value your I/O subsystem can handle 7) Run a full or differential backup to restart the log backup chain. 

The DBCC FREEPROCCACHE command is not beneficial (for index maintenance), and should not be executed in this context. It flushes the plan cache, which causes all query plans to be recompiled when they are executed again. If there's no way to turn that setting off manually, it is better to use recommended third-party index maintenance scripts, such as those by Ola Hallengren or Minionware. EDIT: For an index re-organize, you should add a step to update statistics as well, because they are not updated automatically. This step alone has the same effect as invalidating stale plans, without the negative of flushing the entire cache. The above third-party maintenance solutions include a step to only update statistics that need updating. 

To summarise, if you just want to return a value for a client application, you probably don't need an output parameter. However, if you want to pass values around in T-SQL between stored procedures, they can be very useful. For what it's worth, I hardly use output parameters. 

Using the GRANT Shema permissions on my made-up domain group, I can assign SELECT permissions as follows: 

As a data professional, I'm going to tell you straight up to leave the constraints in the database. However, there's no right or wrong answer, because it depends on what risk you're prepared to deal with. If you don't want relational integrity, use XML or JSON or some other thing that isn't an RDBMS. But if you want to use SQL Server or Oracle or PostgreSQL, go right ahead and manage it in the app. Somewhere along the line, though, there will be data corruption and orphaned records. That risk is real, but I can't tell you how big of a risk it is. If the effort of fixing corruption like that is less than the effort of maintaining constraints in the database, that's a manageable risk. On the other hand, if you can't bear the thought of fixing corruption or orphaned data, then leave the constraints in the database and spend some money on the right tools to help you with database changes and migration. P.S. "Agile" is not an excuse for bad design or lack of documentation. 

In the same article, there are several options to remedy this problem. I would try creating an SPN manually. To manually set up an SPN for SQL Server, refer to KB article 319723, where it is recommended to use the tool on the domain controller. Scroll down to Step 3: Configure the SQL Server service to create SPNs dynamically on that article. You have to modify permissions to allow and for the SQL Server service account, and it's quite convoluted. For what it's worth, sometimes deleting a server name from there gets SPN to work, too. Good luck! 

Under the covers, SQL Server will return the entire rowset from the , and then assigns every value (really quickly) to the variable. Of course, the last value in the will be what is finally assigned. Here's where it gets tricky. The order of what is returned, with no or clause, can depend on any number of conditions. Usually it's returned in the order of the clustered index, but that's not guaranteed. 

Anything left over from that, can be allocated to SQL Server, assuming it's a dedicated instance. If your VM only has 6 GB available, the recommended value is 3 GB. To change it, you would write something like this (or change it in the Management Studio UI): 

The Page Life Expectancy counter tells you for how many seconds an 8KB data page will remain in the buffer pool, before being flushed out. A low number can be a sign of memory pressure, but it may also be a sign of nothing. If your data access patterns involve a lot of report-style queries, reading a lot of data from large tables, this could be normal. Monitor the value over several days, at different times in the day, and you may find that this is an outlier. I have a customer whose PLE goes to zero every time a certain large index is rebuilt, but it steadily increases after that. My first sense is to review indexes, and see if a well-placed index can help with large reads. Another best practice is to avoid using if you can help it. You may need to increase the physical RAM in the server, but that shouldn't be your first reaction. (Read more here about PLE and NUMA nodes: $URL$ 

If you want to programmatically add attachments, you would do something like this (notice how the second and third files begin with a semi-colon): 

Summary Research pricing carefully, being wary of licensing, storage, and compute costs. Investigate putting down a proper firewall that can do VPN access. Even if you decide to go to the cloud (I would consider SQL Server on a VM), be sure to fix the performance on your current SQL Server instance first, so that you don't run into the same problems when you're paying by the minute. 

You're running out of RAM on the server. One of the comments refers to lowering your Max Server Memory setting. For an instance with only 4GB RAM (which is not really enough to run SQL Server), your Max Server Memory should be set to 2048MB at most, possibly even lower. 

will tell the query where the is, but then you have to use the and operators respectively, to select that portion of the string. In the case of , it's straightforward, and you were almost there in your logic, if not in the implementation. For the , you need to subtract the position of the from full length of the string (that's what is doing). 

Change the to . There's a chance there are missing foreign key values that are causing it to not find any matches across the whole set. Where you see NULL in the result set, will be an indicator where there is no matching foreign key. 

All indexes consume space. A clustered index is the data in the table, and each non-clustered index is a copy of whatever data that it is indexing. 

Looking at your command, it appears you're missing credentials, so I'd give this a try (I've shortened the other parameters as well, and removed the TCP port): 

An odd number of votes will break a tie. It's always better to have a witness when you have two replicas for this reason. You can learn more about quorum here: 

In other words, focus less on where the files are going, and more on the reliability of the backup files, and your restoration process, if everything catches on fire. That means having checksums enabled on backups, a regular test of restoring the databases (on another machine somewhere), followed by a to ensure consistency in the database. The elegance of Blob Storage as a secondary store for backups is that you can pull the files down from any Internet-connected machine and test the restore process. 

Michael pointed you to the source (Paul Randal's "Anatomy of a Page" and "Anatomy of a Record") in an above comment, but to summarise: