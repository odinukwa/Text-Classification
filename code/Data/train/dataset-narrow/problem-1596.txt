There isn't any way to match upon text, but you can forward all messages to an address to a program. You need to add an alias to your system aliases file, usually /etc/postfix/aliases or to the user's .forward file. The first option has the better flexibility, cause you can have an aliases for an address which doesn't actually map to an account. The alias should be something like 

It says that any request with only one / in it, will be redirected, anything with two / will not match the rule, so will be passed over. 

Ping is really only good for testing 'is the system up'. The response time is a bit of a red herring. In some systems ICMP ping responses are given a low priority, and therefore it's not a problem if the response time varies. I've also seen someone worry about ping response times and it turned out to be an problem on his own PC, not the systems he was worried about. If this is the only thing you're seeing, I'd not worry about it. If you're seeing other problems, then I'd look at those problems. 

You should approach it in the other way, instead of limiting these log files, work out a system where there is always enough log space so that they can never fill the disk. My primary way of doing this is simply calculating how much space is likely to be needed, and giving it lots of space, but I've also got a script which will check for diskspace in the logs directory, and if it's getting tight will automatically rotate the log files, compress old versions, and erase those which are old enough that they'll be on archived media. 

ls -ld /DriveXT will tell you the permissions on /DriveXT. You can't get the permissions on the lower ones, they are probably '--x' for you for MMT, and '---' for you for LP2010. If you want to find out the permissions on the lower directories, you'll have to be root, or user with 'r' permission on those directories. 

You can also use the screen program, and pressing ctrl-a h will write the screen to a file 'hardcopy.n'. 

It's confusing to try to understand what your problem is, you don't make it clear if you're checking a file on web1 on web1, or db1 on web1, or whatever. Please give a good description of what's you're actually seeing, without confusing the issue with copying and chmoding. Something simple like I create a file on db1, with permissions x:y and on web1 I see permissions a:b and on web2 I see permissions c:d. First thing, using NFS, any file which is owned by root will usually be shared so that it's owned by nobody. This means that if you have root on the client machine, you effectively don't have root on the server. I think that explains some of what you're seeing. Secondly, if you are running NFS, it's vital that the userid->username mappings are identical on all the servers. Unix filesystems only store a numeric id for userid & groupid, which are then mapped to usernames by programs like ls. Are you sure that they are all in sync? It could be that you've got a mismatch. Finally, tar p is an option for extracting, not creating tars. It's ignored when creating tars, and even when it's used, it's not going to set the ownership to what they originally were. -p basically means, ignore the umask. Tar will create files owned by you only, unless you're root. 

auto_increment: This value will automatically increase. primary key: This is the column which holds the first, and probably most logical, sort order. Almost always this means that it's unique. index: This column is one which might be chosen as a sort order. Any column or combination of columns can be indexed, not just the primary key. unique: This is a special index, where each value is going to appear only once. 

According to the tutorials I have read, the Active Directory replication subsystem does not handle synchronization of SYSVOL and NETLOGON folder contents between domain controllers. Instead, it handles topology discovery (KCC, ISTG) and synchronization of changes committed against the LDAP databases. Replication of SYSVOL and NETLOGON shares are handled by either FRS (File Replication Service) or DFS-R (Distributed File System Replication) subsystem. 

In my system (Arch Linux), the parameter doesn't seem to change the address. Perhaps the crontab line needs to be changed to this instead: 

The custom script may restart the machine via the out-of-band management interface (iDRAC on Dell servers): 

EDIT: Actually, I forgot to remove the closing brace in a second test. In fact, keepalived startup process ignores the missing brace, but the failover doesn't work during the runtime. 

In this scenario, if Postfix queries , the first entry matches and (the captured string) is returned. Or else, the second entry matches and is always returned. 

I would answer that you could install and run the script via the Shell Execute operator (). However, it will probably not work, because the system will certainly be unresponsible after a kernel oops and will not run the custom script. Because of that, I suggest you to run a script in another server when a kernel oops occurs. For example: 

If the mail server uses an invalid or self-signed certificate, you may need to set , and context options. I am not a PHPMailer user. I guess you may need to set attribute. Please, test this piece of code (reference): 

The RFC's that specify how a MTA should handle MX records are RFC974, RFC1123 section 5.3.4, RFC2821 section 5 and RFC5321 section 5. RFC974 status is now HISTORIC. According to it, MTA's are expected to query the list of MX records associated to a domain and are "encouraged" to try all (or a fixed number of) SMTP servers, in ascending order of preference. If there are multiple MX records with an equal preference value, MTA's must try to deliver the message to all SMTP servers until one succeeds. The order of attempts is a MTA's choice, that is, the RFC doesn't rule whether SMTP servers must be contacted at random or in the order given by the DNS server. In addition, the RFC doesn't rule how to handle a MX register that references multiple A records. 

Add the argument to the command, which means "do not send any messages if the body is empty". Therefore, change the crontab line to: 

According to the client log, the OpenVPN client did not add a static route to the OpenVPN server through the original default gateway (the one used before the connection establishes). This prevents OpenVPN client packets from reaching the server, because of the absence of a route to it. I suggest you to change the server config, replacing the line: 

I suggest you to verify the DFS Replication health between the servers, by following this article: $URL$ : 

In [Case1], the socket that connects to the client during the file transfer doesn't belong to the process. Instead, it belongs to a process that is owned by root. So, the rule that sets the mark prevails. 

I figured that CentOS's creates a chain in table for each active zone, which gets evaluated when packets that matches zone definitions arrive. So / rules can be added there. 

In the server that eventually produces kernel oops, forward messages to a "monitor" server using the netconsole module. 

Note: I admit I am proposing a weird and possibly unsupported solution, which may get broken on future versions. Nonetheless, it will certainly work on a CentOS 7 server. I suggest you to directly add such port forwarding rule into the / stack. You can achieve that by using 's direct rules. For example: 

Usually web servers are configured to only allow access to user files in a specific directory, traditionally the public_html directory. The url format would be $URL$ which would translate to /home/user/public_html/dir/file.html This is for security reasons. Imagine if anyone could access your .ssh_keys directory from the web, anyone could break into that account. For that reason, there isn't any way that you do what you want directly. On the other hand, there is nothing wrong with accessing a server by hostname. A virtual host is just another name for that same server, which usually has different content served out. There is nothing you can do with a virtual host which you can't also do with the correct hostname. 

You have a bad installation somehow. It is conventional on Unix that a program exists in /usr/sbin/sendmail$, which acts as an interface to the local MTA. This MTA is rarely sendmail nowadays, but other MTA's have a compatible program which is installed here. sendmail.h would be part of the source code for sendmail, and sendmail.0 and sendmail.8 would be the man pages, so it looks like you've somehow got the source code for the right sendmail in /usr/sbin You need to clean this up, and get a proper sendmail program into /usr/sbin/sendmail to fix php. Since you've not told us the distribution, nor which MTA you're actually using, no-one else can help you. $ = Actually one or more of /usr/lib/sendmail and /usr/bin/sendmail as well as /usr/sbin/sendmail, programs will either search these paths, or have one or more hardcoded into them, or ask at installation time. It looks like your program is using /usr/sbin/sendmail. Regardless of which, the installation for the MTA will do it. 

It's the amount of physical memory being used for cache memory. Even though top lists it at the end of swap, it's really got nothing to do swap space. Red Hat has a nice explanation of all the different types of memory usage. 

Here is a good paper on what ICE is, and what it does. Basically ICE is a inter process communication protocol, with authentication, protocol negotiation and potentially multiplexing built in. It allows two X clients to talk directly to each other, for example, a video player program could potentially talk to a jukebox program to update each other. As Richard Holloway says, the .ICEAuthority file is for authentication. It contains a number of random cookies. If two programs have the same cookie, then they're allowed to talk to each other. In practice this either means that they're reading the same .ICEAuthority file, or the cookies have been added. In a lot of ways it's similar to the xauth program & the .Xauthority file, except that .ICEAuthority is used for client to client, while .Xauthority is for client to server. 

You should give a full path, because you don't know the context that it will be executed in. If your program exits with 67, then this will be bounce the message as unknown user, 0 will drop the message. Anything else will be retried until the message times out and bounces. Be careful of security - you're basically allowing anyone on the Internet to run a program on your system, so don't trust user input, and sanitize it before you use it. 

It depends totally on what you're trying to do. A static website? Probably no problem at all? A website where each page requires a large amount of CPU & memory before it can output the results? You're going to have problems. Is it mainly plain text? Does it output a 1Mb graphic for each user? The best way to know is to use benchmarking software to find out what the site's requirements are, and by extension, if it can handle the load you're expecting. Here is a list of different programs. Loadrunner will without doubt do it for you, but I'm sure there are others which will work just as well. 

A Squid server configured as a forward proxy is able to receive plain HTTP requests from clients and forward HTTPS requests to upstream servers transparently. However, an external URL rewrite program is needed. Write the following URL rewriting program into : 

RFC1123 status is INTERNET STANDARD. Section 5.3.4 aims to "refine" the RFC974 procedures about how to handle MX records. It now requires MTA's to try all SMTP servers in ascending order of preference until one succeeds. However it still allows a configurable limit on the number of tries. If there are multiple MX records with an equal preference value, the RFC recommends (and doesn't require) MTA's to select one record at random. However, if a MX record references multiple A records (IPv4 addresses), the RFC requires the MTA's to contact all these addresses until one succeeds, in the order given by the DNS server. 

In Ubuntu, FFmpeg headers are distributed among several packages, which are built from the source package. I guess you should install all of them (list here): 

The method you use for remote script execution generates a conflict between code and data flows. I mean... During the execution of , you allocates the standard input (STDIN) for the code that will execute remotely. But if the executed code needs a data input, it will be fed by the next code line (because the STDIN is transmitting lines of code to the remote process). If you want to use STDIN to supply input data to a script that runs remotely, you may have to use another stream for the code. OpenSSH seems not to have a native file descriptor forwarding feature yet, that is, STDIN (file descriptor number 0) seems to be the only file descriptor which could be used for data forwarding from the local terminal to the remote session. As a workaround, you could copy the script file in advance, for example: 

You can add a or a restriction at the end of if you intend to receive messages from authenticated users only. However, if you are going to receive messages from domains that you don't own (i.e. a public mail server), you have to follow other alternatives, as an explicit or will reject those messages. Apply the resolution pointed by @Jenny D, that is, either set restriction or configure Sender Policy Framework. Also, you can apply additional anti-spam techniques: 

shows the process that reads the file is a process whose PID is and owner is . shows the socket that sends the file data to the remote client is a process whose PID is and owner is . 

EDIT: Actually, my executable comes from s-nail package. The argument is not supported by GNU mailutils, so my solution doesn't work. You can use heirloom-mailx instead of GNU Mailutils. Install the package and modify the crontab line to: 

Postfix seems to use in one of its access restriction lists. I couldn't figure an exact cause of the issue. Please, could you add in the question: 

Your firewall rules seem correct. However, redirection of packets that enters the AP should be configured in the chain. Rules added into the chain affect locally-generated packets only. So, the following rule should be added: 

The line above tells to authorize user to run any commands as any user that is member of the group. The related sections from are: 

=> User didn't authenticate, so this restriction is ignored. => User doesn't belong to the internal network, so this restriction is ignored. => Postfix seems to have deprecated this restriction, so it may also be ignored. => Postfix detects that it is the final destination of the message, so the restriction is ignored. An implicit takes place, so Postfix finally accepts and delivers the message.