With this setup, devices are archived by setting field to true and entering an . I could query any device easily whether it is active or archived. (Please ignore the field, as this is used for an unrelated concept). Take notice of the Phone subtype table, where I had to propagate an index of DeviceID and IsArchived flag because phone numbers must be unique for active devices. I have to do this with other subtype tables as well. I don't know if this is a good or bad design. This part really confuses me... What are best practices for handling soft deletes where foreign key values can be marked as deleted. The only thing I can think of is create a routine that searches for all records that are related to deleted data, and create a report for users to resolve the discrepancies. For example, if a table of locations is related to the devices table, and some locations are soft deleted, then the devices refer to locations that no longer exist and must be moved. By the way, I'm using MS SQL Server 2008 R2, and I plan on using Entity Framework 4 in my application. I value database maintainability over performance. Thank you for reading. 

Problem I'm trying to figure out how I would query the database if I don't know if a device is active or archived? For example, if a user has a serial number and wants to find out information about the device, and they are unaware of whether it has been archived. Option 1: Create a view based on a union all? Option 2: Query the active database and then query the archive if the first query returns nothing? The saga continues... An associate suggested that I eliminate the archive database and use a soft delete scheme. I built a model using this idea, and then started running into a host of other problems. Here are the same tables using the soft delete scheme. Soft Delete Example 

Design 2 In this design I thought I would use a bridge/association/junction table to decide which network statuses are valid for a device. It looks like this: 

Due to this design's granularity, I could theoretically allow any mix of statuses for devices with this design, but I wanted to control it so I wrote some triggers to only insert the correct mix of statuses depending on whether the device is network capable. Triggers as follows: 

I put check constraints on the table as follows to ensure a network status and network information can only be provided if the device is capable of connecting to a network. 

I'm designing an asset management database that tracks IT hardware. I decided to use a supertype/subtype design. I'm at a point where I want to track history of changes for devices. I wanted to use a separate history table, but I can't decide how to track history for changes made to subtype tables. If I use separate history tables for each subtype table I can reconstruct records by joining them with the supertype history table, except in the case where subtype history tables change independently of the supertype history table. By independently, I mean there are x updates to data in the supertype table, creating x supertype history records, and y updates to a subtype table creating y subtype history records. If the changes are made on the same day, how would I reconstruct records? Is this a good use of supertype/subtype, or should I denormalize the tables? Otherwise, can anyone suggest any way to approach the history issue for this type of design? Using MS SQL Server 2008. Here is a very simplified ERD: 

This design eliminates the need to propagate across the tables. The issue I see with this design is that every device that has network capability will have records in paired with ids 1, 2 and 3, while devices that can't connect to a network will be paired only with id 4. It seems like a lot of extra records that all mean the same thing: devices that can be networked can only use statuses 1, 2 and 3, and devices that can't network only use 4. This design seems to be more "relationally correct", but also smells a bit. Update The following update proposes variations on Design 1. I come across situations like this often, where there are many ways to achieve the same end result. I never know how to tell if there are hidden problems with the designs, and I can't judge when to normalize or denormalize. Is one of these designs preferred over the other and why? Design 1.1 

You may need to install at least Service Pack 2 or higher of SQL Server 2008 R2 on Windows Server 2012 R2. I would suggest you to install the latest Service Pack 3. You can get the installer here. Slipstream your installer to SP3. This guide will help you on how to slipstream. You can also read this KB Article for OS and SQL Server Version Compatibility. Edit (to complete my answer with the following solution based on the KB): 

You might be doing a large batch of transaction processing that could cause log reader agent to slowly read the t-log. Or NOT properly maintaining the t-log that could cause increasingly huge t-log. Ask around what's being process at that time. (watch out for index maintenance - Log Reader Agent will appear hung as it scans more log records.) Then check for VLF (Virtual Log File) by issuing on the publisher database. See if you have a lot of VLFs. Keep VLFs in check and t-log size at minimum. The key is to properly manage your transaction log when dealing with replication. Read on how to Optimize the Transaction Log. On your Log Reader Agent Profile, you can change the and to higher value. Be careful on these 2 parameters as it can cause Log Reader Agent to scan more transactions and could spend more time scanning and slowly delivering to distribution database. It's rarely you need to change parameters (even the ) on Log Reader Agent so use it with caution. 

For 7GB to 9GB of database, I would advise you to just use backup and restore strategy. This is the most simplest and effective way of migrating small database. I highly recommend you look at dbatools to help you automate your migration (it also migrate your objects like logins). The most important part of your migration is you practice your rollback and making sure application/users can still connect to your database after migration. Always test before you go live. 

It could be someone or a process/apps deleted and inserted the data in subscriber db (while cmds are being applied). Review your subscriber security settings and check what process/apps are accessing the subscriber db. Multiple publications connected to subscriber db. pub1 (deleted the data first) then pub2 tried to UPDATE/DELETE the data which could cause an error 20598. Triggers on subscriber tables that could delete/insert/update the data. 

You are right! I think this is a bug or by-design. I was able to repo the scenario. So basically when you run Ola's script with this: Or native: When you execute a new full backup or full backup or even differential backup, All your previous log backups will be deleted after you run another log backup (Ola's log backup script with param). Tested using Ola's script version: October 7, 2016. Based from Ola's website: 

This will give us an idea on how long it will take to apply the commands in subscriber db. The before and after picture of commands stats is important as we can determine how slow is slow or what have changed since last baseline. 

No. This is normal. You are experiencing too many applications installed on your database server. Start migrating your database (SQL Server database engine) to its own server or start uninstalling all other applications in the database server. Currently your server have the following: , , , , installed. Not to mentioned if you have monitoring tools or anti-virus installed. When you login to the server, you also use memory. filter drivers also use memory. Check out this blog post by Jonathan. Let's say, you've a barebone database server (without the other application installed fighting for memory), you can at least have 27GB of memory for your SQL Server. 

We need to step back a bit and apply basic SQL Server troubleshooting on subscriber db as well. Check for blocking, IO contention, network issue, check for wait stats, triggers, cursors, long running job/transactions, service broker, AGs redo queue, etc. From my experience, the silent killer of replication performance is... triggers. You wouldn't be able to catch it unless you're using profiler or you understand how the data flow works. These could quickly stock up the pending commands to be delivered. 

For the record, this design seems a bit absurd even to me, but this is my thought process. In this one, the presence of a in the table is equivalent to saying = true in Design 1. The has a foreign key constraint and is used to ensure only networkable devices are entered. Can do this using a CHECK constraint (see below), and by making a computed column that is equal to . 

This is an inventory database for IT assets. The models used are trimmed in order to focus on the problem at hand. Using SQL Server 2008. Thanks for taking the time to read and for any input you can provide. My design includes a table which holds the various devices that can be entered into inventory. Each device has a boolean flag, which states whether a device has network capability, e.g., for most computers , for hard drives ; some printers will be true, and others will be false. You get the idea. The field determines if network-related information is relevant when an inventory record is created. Design 1 My first design uses an index on and to use in a foreign key constraint with the table. 

The table looks like this in this setup (Notice the addition of record with id #4, and the field which specifies that this status is for use with devices that can can connect to a network): 

I'm building an inventory database that tracks computer equipment and other hardware devices. At some point in any device's life it is retired and gets archived. After it becomes archived it needs to be tracked as it is removed from service and properly disposed. I originally designed the archiving mechanism using an exact replica of the active database that would receive its data using a trigger on delete from the active database. The archive database includes replicas of all the related tables because as certain foreign related records are no longer pertinent, they should not be accessible to users to use with new devices, but are required for referential integrity and querying with the archive tables. Keep in mind that the concept of archive here is not just to keep a history or a log. The archive is a part of the business process, and users will need to query and update devices that are both active and archived. The ERD below uses the table as an example where all entries and updates are copied to the table. When users should no longer be able to enter inventory records of a certain device type, it is deleted from the table, but remains in the archive table. This pattern is used on all tables to ensure the archives refer to valid data, hence the replica of all tables. Active Table Example (Other related tables omitted) 

The issue I have with this design is I'm not sure if the relationship with and / is a good or bad design decision. Is propagating a non-key field like to other tables a bad design? I don't have enough experience with database design to make an informed decision. 

This question regards the proper use of NULL and utilizing CHECK constraints for business logic vs stored procedures. I have the following tables setup. 

I normalized the tables to avoid using NULLs. The problem is that some of these tables depend on each other due to business processes. Some devices must be sanitized, and some are tracked in another system. All devices will eventually be disposed in the Disposal table. The issue is that I need to perform checks, such as if the boolean field is true, then the cannot be entered until the fields are entered. Also, if the boolean value is true, then the fields must be entered before the can be entered. If I merge all of these columns into the table then I will have NULL fields, but I will be able to manage all of the business rules using CHECK constraints. The alternative is to leave the tables as they are, and manage the business logic in the stored procedure by selecting from the tables to check if records exist and then throw appropriate errors. Is this a case where NULL can be used appropriately? The boolean fields and basically give meaning to the NULL fields. If is then the device is not tracked in the other system and and are NULL, and I know that they should be NULL becuase it is not tracked in the other system. Likewise, and I know will be aswell, and a can be entered at any time. If is , then and will be required, and if and are NULL, then I know they have not been officially removed from that system yet and thus cannot have a until they are entered. So it's a question between separate tables/no NULLs/enforce rules in stored procedures vs combined table/NULLs/enforce rules in CHECK constraints. I understand that querying with NULLs in the picture can be complex and have somewhat undefined behavior, so separate tables and stored procedures seem beneficial in that sense. Alternatively, being able to use CHECK constraints and have the rules built into the table seems equally beneficial. Any thoughts? Thanks for reading. Please ask for clarification where needed. Update Example table if they were merged and I allowed NULLs.