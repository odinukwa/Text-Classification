Looking by the way you make these levels, as a sublevel has the same prefix as its parent, if you sort the data by , you will have data sorted in a way that if you see a level node, the last level node you processed will be its parent. For example when you iterate after sorting and is 13010190, you know that 13010100 was already processed (because the number is lower) and that any nodes processed between 13010100 and 13010190 will be of level 4, because the next possible level 3 node (13010200) would appear after 13010190. With this in mind, we can first sort all data by and then just add each node to its lower level last seen node, which will be the right parent for it. To avoid border cases with level 1 nodes, I chose to make a fake level 0 node called root which will have all level 1 nodes. Its children would be what you need called in your first example. Also, I didn´t create the properties and as that can be easily checked by . If its in your requirement to have them, should be easy to add them. This code would also work for more than 4 levels as long as it follows same format. Here is the code: 

In your case, you start with 10-50 and 0-15. It did not give enough overlap. Is it worth comparing 0-15 with the other slotA ranges? No, because the others will start at more than 50, and they won´t overlap. Is it worth comparing 10-50 with other slotB ranges? Yes, because there could be a 20-30 range afterwards. Basically, 50 is greater than 15, so thats our hint that we can keep growing that 15 going to next range from slotB and as long as its less than 50, we might keep having an overlap. If you keep doing this, you will arrive at desired answer. There might be data structures suited for this, but its still simple enough to not need one. 

Your code is pretty good, except for using , as that can be quite expensive: . Here is how I would do it to avoid using that. Assuming hashmap access is and n being the max length between both strings, my implementation runs in 

As for runtime duration, there is not really much you can do, as the algorithm is already optimum. However, code can be simplified a bit. Here are some tips: 

Check my comment to fix your current version. Since you also asked for a version that only used your pointsRemaining, here it is. Note that this code does not compile, but should get you the idea of doing flood fill with this approach. The worst complexity of this is . This might sound worse than your current version that is , but the worst case is considering that all 2d matrix points are in . A more accurate complexity of this version is With this approach we avoid having to store the 2d matrix, intead just keeping the points for each row in a set for fast lookup. This approach will work very well if the amount of points from is small compared to all possible points. 

The most expensive part of the algorithm is the sorting, which I´m not sure if you needed it. If you do not care about order, no need to sort them. The important part is that we only add each number to the result array once, and we process each number once, so except for the sorting, the complexity should be linear on the amount of numbers in input. 

I went full performance here, so no forEach and such. The idea is to use associative arrays and assume they cost O(1) for speed improvement. The steps of this algorithm: 

Okay, found problem online and solved it. The trick to the problem (as with most problems) lies in the constraints: 

Performance review: Your biggest bottleneck is the , which is turning your algorithm from being O(n log n) to O(n^2). A simple way to fix this is using a new vector and input the unique keys intead of removing duplicated ones from your old vector. 

Here is an alternative way of calculating the churn rates which would be optimal efficient wise without altering the data you have. Note that maybe pandas queries may outperform python loops used here so your solution could run faster even though its less efficient. Also, I am assuming no user can have multiple subscriptions on a same day (intervals for user subscription don´t overlap) and that an user startDate - endDate range is at least 1 month 

You can then combine this with @scnerd solution to add multiprocessing If you know your data is all the same type, you can further optimize it: 

I will assume all your words only have a-z characters. With that, an efficient check can be made by preprocessing your dictionary: Pseudocode: 1) Preprocessing: 

This problem is much easier looking it backwards: Instead of processing the input in the order given, process it in reverse order. You know the last number in your answer will be 1, because no numbers follow it. Now the previous one could be a 2 if the sign is different with the last one or a 1 if its the same sign. So basically, each step back can either add 1 to the list of consecutive numbers with alternating signs or reset the count to 1. The code: 

Be careful that the random can be between 0 and 4 when it should be between 1 and 4. If you want to make it more object oriented, I would separate the interface (alert and prompt) from the game logic. As this is just a game with 1 ship, I would only do a Game and Ship objects to represent it, but for more complex versions other objects may apply. Here is how I would implement it having custom columns: 

Update: checked how these ratios are calculated, here is a more efficient answer that avoids a lot of checks between pairs: 

Now, reading some special properties about this numbers, we can make a much better algorithm: we generate only the possible numbers that satisfy necessary conditions to be highly composite and then we process only those to filter the ones that were really highly composite: 

The main point about breadth-first search is that when you visit a node, you are visting it with the shortest distance possible. That´s why visiting a node twice does not make sense, as on your second visit you are visiting it with a higher or equal distance. In your approach, although you are setting your current node as visited using , The correct approach is to mark the ones you add to the queue as visited, and not the node that you removed from the queue. Think that until a node is popped from the queue, it is not marked as visited so while it´s true that from the moment it gets popped it won´t get visited again, it´s entirely possible (and highly likely) that your queue has multiple of this same node already stored. I would remove the line , change your : 

For every row in the csv, generate 2 events: subscribed with and unsubscribed with Sort all these events by date in ascending order Set , and Preprocess data: keep amount of unsubscriptions total up to each day and amount of actually subscribed users on each day 

As for performance: in the worse case(element is not there), will have to check all values of the array to check if the value is in it. And doing this for all values of the second array, means O(n * m) where n is the size and m is the size. If n = m, we can consider that it takes O(n*n), which is quite expensive. You could improve this by sorting both arrays and doing a smarter check, but even that will have the cost of the sorting, which is O(n * log n) with a good implementation Having said all that, what you really want is a structure to check if you have seen or not an element that can check it really quick. In javascript, we can use objets to do this. As its native code we don´t really know the complexity of access, but with testing a bit it is clear that it´s much better (in theory access should be O(1) or O(log n)) The pseudo code is like this: 

First of all, checking in a range if you can write a letter or not can be improved to a O(1) check, in my case I used for that. Another thing that can improve yours is adding a cache, because the longer the string (and the more * it has), the more repeats you are making in the recursive calls. In my case I used for that. With just those 2 changes I think your code would be greatly improved. Another observation is that it is not necessary to try for each index writing the 3 letters. When we have a wildcard, we only need to try using the letter from previous index and the letter that is more ahead () because the third letter has no inference in that range. When we don´t have a wildcard, there is only 1 choice of letter to make. My final observation is that if there is "nothing for a long way", it is not necessary to try writing at several indexes thinking it is necessary to "leave space" in case we have a different letter "soon". This completely cuts the branching in these cases, which further helps on performance. I defined "long way" as block_size * 2 as I am sure that is enough, but maybe it can be less than that. Took me a while but can´t find anything else to improve. Although my code works for your test cases, more testing should be done as there might be some bugs. Your code took me 12s to solve, haven´t meassured mine but its definitely under 1s . Here is my own solution with all these ideas: 

After searching the original problem on codechef, it becomes clear that B can be 1 and N be 1,000,000,000. In that case, your code will iterate all values and it will take long, as its O(n). This can be reduced to O(1). You need to see this problem as a function, and once you find the correct function, you want to maximize it. In the first example, N is 10 and B is 2, so we can do at most 5 clicks on second button. If we decide to click first button, we will always need to click it 2 times, because if we leave 1 energy remaining we can´t use it for second button. Another important observation is that you always need to click the first button X times, and then spend the rest on second button. I leave that for you to think why. So in this case, you can do the following: 0 * 5 (click 5 times second button and 0 the first) 2 * 4 (click 4 times second button and 2 the first) 4 * 3 (click 3 times second button and 4 the first) 6 * 2 (click 2 times second button and 6 the first) 8 * 1 (click 1 times second button and 8 the first) It is clear that our function in this case is f(n) = n * (10 - 2n), with n being the amount of clicks on first button. Expanding, we get -2n^2 + 10n. Deriving that, we get -4n + 10. If we consider the maximum to be at 0 (which it is), n = 2.5 would archieve that maximum. If we replace on the ecuation, 2.5 * (10 - 2*2.5) = 2.5 * 5 = 12.5 However we can´t actually make 2.5 clicks on first button, so we need to consider either 2 clicks or 3, and take the best one. In this case, both cases will give us the answer : 12. In cases where N is not a multiple of B, your first steps will always be clicking first button until remaining energy is a multiple of B and then you have the same problem as before. More generally, f(n) = N % B + n * ((N - N % B) - B*n) and the n that maximizes the function is the solution of 2Bn = (N - N % B), so n = (N - N % B) / 2B. Be careful that n can be decimal, so you should consider using floor. This way you can compute the answer in O(1) and remove your loop. 

There are a lot of queries, but a small amount of cities. It would seem more important to optimize answering a query time, which is usually done with some precalculation or caching. In this case, we can think that as there are at most 3000 cities, there is at most 3000 distinct city types. So all queries that don´t ask for an invalid city type (no city has that city type) will have one of those 3000 types. We can precalculate for all cities, the query of going to right or left for each of these types, ending with a O(cities * distinct types * 2) complexity, which would be 3000 * 3000 * 2 at most, much better than the original 500000 * 3000. Then for each query, if the type of city is a valid type (at least one of the cities has it), we have already preprocessed the answer and can answer in O(1). If it isn´t, we just answer with -1. Another hint is the topic of the problem, which is for practicing multi dimentional arrays. We use this to store the calculation of all valid queries to then answer in O(1). I advice you to try coding the problem again with these hints. If you get stuck: 

What you are doing here is first calculating primes under sqrt and then crossing out the multiples like sieve says. However the standard approach is to calculate primes as you go and cross out multiples of each prime : 

The main thing you are missing is that when total is 0 (a multiple of 101), you can stop the recursion and just make all other operators be , as you already reached a multiple of 101 and product will keep it like that. As for the order of operators in the recursive function, the right choice is to put at the end, which seems to be what you have right now. The main reason is that 101 is prime, so you won´t get a multiple of 101 by the products of numbers. So it´s better to leave that choice for last, as or have higher chances of generating a multiple of 101 quicker. Update: I have tried your resursion solver with the stopping when reaching to 0 remainder but it still gives time out in a few cases, so ended up trying a different approach: dynamic programming. The trick is that 101 is small, so the possible remainders are only between 0 and 100. The idea is that when looking at the number, you don´t really care if the accumulated result from previous operations is 2 or 103 or 204, you only care that its remainder with 101 is 2. With that in mind, we can make a boolean array where we will store the remainders that can be generated by a combination of operations using the previous numbers. So for each of these remainders we can get with the previous numbers (0 to ), the remainders we could possibly get in the step are the same but adding, substracting and multiplying the number by that remainder. I decided to keep a 2 dimensional array, where the first dimension is the step and the second one the remainder: So for each number in the array, if we are on the step we iterate and look for the true values. For those, we set the new , , to true. Note that we need to add modulo to that to keep it between 0 and 100. The complexity of this is N * 100. Considering worst case, 10^4 * 100 = 10^6 which should fit nicely in the given time. With this done, we know for each step what remainders we can get. So according to the problem, remainders[array.length - 1][0] should always be true as it can always be obtained. There is still the problem that we don´t know what operation we used for each step so we can´t reconstruct the operations just by storing these values in remainders. I suggest you to think over how to do this. If you still can´t manage, here is my full code that passed the tests. I don´t usually use java so my code surely has some improvements to make.