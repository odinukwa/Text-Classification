which redirects me to the paper Log space recognition and translation of parenthesis languages by Lynch who proved that $\mathsf{Dyck}(1)$ (that is, normal balanced parentheses) is in $\mathsf{TC_0}$. Any related papers are welcomed as well. Thanks! 

In low-level complexity classes, there is an interesting problem about the characterization of $\mathsf{NL}$. OPEN: 

Section 3.5.1 seems relevant, a space-analog notion similar to the average-time complexity is defined. Hopefully with a deeper reading we'll find some problems that has been analysed :) Also in the paper 

Distance preserver with various parameters are known to exist. I'm particularly interested in the one with the following properties: 

Link-cut tree is a data structure invented by Sleator and Tarjan, which supports various operations and queries on a $n$-node forest in time $O(\log n)$. (For example, operation link combines two trees in the forest into one, while operation cut divide a tree in the forest into two trees.) Several applications are known by using link-cut trees, and here I'm particularly interested in Goodrich's separator decomposition, which given a $n$-node plane graph $G$ one can obtain a corresponding binary tree where nodes are subgraphs of $G$, and the children of a node $H$ are the subgraphs of $H$ divided by the separator on $H$. Such a decomposition can be easily constructed in $O(n \log n)$ time (since a separator can be found in $O(n)$ time, and since the separator divides the graph so balanced, after $O(\log n)$ level of separations the leaves of the tree are of size $O(1)$). The main contribution of Goodrich is that he can construct such a decomposition in time $O(n)$, by maintaining and reusing the data structures used for finding separators in each level. One of the data structures being used in the construction, is indeed the link-cut tree. In page 7 of the paper by Goodrich, he claimed that the initialization of the link-cut tree can be done in time $O(n)$. While I go through all the papers cited there, it seems to me that if we construct a link-cut tree via operation link, it takes time $O(n \log n)$ in total. 

Here is one problem described in the book "A second course in formal languages and automata theory" by Shallit. 

See A survey on crossing numbers by Richter and Salazar for reference. So if there is a variant of the Fáry theorem on graphs with bounded crossing numbers, it should be constrained with $\mathsf{cr}(G) \leq 3$. For a small example with $\overline{\mathsf{cr}}(G) \neq \mathsf{cr}(G)$, consider complete graph on 8 vertices. It has $\mathsf{cr}(K_8) = 18$ and $\overline{\mathsf{cr}}(K_8) = 19$. 

Another reason would be that we have much more knowledge in binary relations than any other n-ary relations for n greater than 2. Naturally we consider binary relations between objects, like adjacency, nonempty intersection, equivalence, etc. So we can define graphs in terms of binary relations, and even define graph based on some binary relation on another graph. (For example, line graphs, clique trees, tree decompositions...) But as for other n-ary relations, we do not have much understanding. For example, it takes some time to come up with an interesting ternary relation; (Okay, partially due to my ignorance) properties are weaker and tools are much lesser in the study of ternary relations. (How do we define symmetric or transitive ternary relations? Both of them are among the most important relations one can study.) But still I don't know why this happens between binary and ternary relations. Maybe as turkistany said this question is hard and may be related to the understanding of P/NP problem. 

This is known as the "time-dependent shortest path" problem. Indeed research has been done for this problem; see for example the classical paper by Orda and Rom, and this recent SODA paper which proves that when the weight function of each edge is polynomial-size piecewise-linear, then the shortest path between two fixed points changes $n^{\Theta(\log n)}$ times. Dijkstra's algorithm can indeed be used for this problem, when the waiting policy is imposed, that is, wait at a node if that reduces the final arriving time. Without the waiting policy the situation is much wilder: the shortest path may not be simple, subpath of a shortest path might not be the shortest between the two endpoints of the subpath, paths traversing through infinite number of edges may have finite arriving time, etc. See again the paper by Orda and Rom for more discussion. 

There is a really nice talk on the dynamic lower bounds on graphs by Mihai Pătraşcu. In summary (on p.20 of the slides), we have the lower bounds in terms of query time $t_q$ and update time $t_u$ (insert an edge): 

To clarify my interest, any algorithm which deals with GENERAL graphs are nice. Although algorithms for subclasses of graphs are OK too, that may be not on my attentions. Thank you all! 

Furthermore, under plausible complexity assumption: $\mathsf{SPACE[n]}$ requires exponential circuits, Matching in general graphs is in $\mathsf{SPL}$ [ARZ98]. 

They provide an upper bound on the spectral radius for a genus $g$ graph, as stated in the following theorem. 

Proofs can be founded in standard algebra textbooks. Also you can prove it yourself by induction on the iterations of gcd process. In general this is true in every Euclidean domain $R$ with a multiplicative Euclidean function $f$. In the case here when $R = \mathbf{Z}$, we have $f(x) = |x|$ which is multiplicative. 

The entry in the complexity zoo is surprisingly detailed; it claims that NLOGLOG = co-NLOGLOG in the paper 

The observation can be easily seem by representing as hypergraphs. Let A be nodes of the graph G, an n-subset A' of A is the node set of a complete n-subgraph in G. B' is the set of k-hyperedges in the complete subgraph (a 2-hyperedge is a normal edge), and K-subsets of B' are the every combinations (there are $({|B'| \atop K})$ in total, where |B'| = ${n \choose k}$) of K k-hyperedges. The observation states: any K-tuple of hyperedges in G belongs to at most one complete n-subgraph, which is obvious for ${n \choose k}$ > K > $({n-1 \atop k})$, since any two complete n-subgraphs intersect at most n-1 nodes, with at most $({n-1 \atop k})$ hyperedges. Then we can assign different colors within K-subsets C' of a particular B' constructed by a n-subset A', since any element in C' will not occur as another K-subset of B'' constructed by a n-subset A''. For any K-subset of B not constructed by any n-subset of A, we assign random color on it. Now we have a coloring function f, with the property that no B' constructed by n-subset of A is monochromatic, that is, some of the K-subsets of B' have different colors. 

G is planar and unweighted (that is, all edges of G has weight one), T has size $O(n^{0.5})$, and H has size (the number of nodes and edges) $o(n)$. (It would be nice if we have $O(\frac{n}{\log\log n})$.) 

We have a deterministic polynomial-time algorithm for the problem, simply by running primality test on each number larger than $n$ start from $n+1$. (Of course, $n$ should be large enough; for small $n$ we treated separately.) But I'm not sure this can be proved unconditionally. 

I'm not really familiar with the topic, but with some searching there are some results. For example, Efficient parallel algorithms on interval graphs by Das and Chen, and A simple optimal parallel algorithm for the minimum coloring problem on interval graphs by Yu and Yang both gave an algorithm with $O(\log n)$ time and $O(n)$ processors on the EREW PRAM model. Even for the chordal graphs (which is a superclass of the interval graphs) there is an algorithm with $O(\log^2 n)$ time and $O(n)$ processors on the CRCW PRAM model, in the paper Efficient parallel algorithms for chordal graphs by Klein. Maybe you can take a look at the papers and references, to make sure if there are any additional assumptions, like whether the interval representations are given or not. 

Polynomial methods, say Combinatorial Nullstellensatz and Chevalley–Warning theorem are powerful tools in additive combinatorics. By representing a problem with proper polynomials, they can guarantee the existence of a solution, or the number of solutions to the polynomials. They have been used to solve problems like restricted sumsets or zero-sum problems, and some of the theorems in this area can be proved only by such methods. To me the non-constructive manner of these methods are truly amazing, and I'm curious about that how we can apply these methods to prove any interesting inclusions and separations of complexity classes (even if the result can be solved by other methods). 

generalize the notion of Lebesgue measure into complexity classes, and many following works can be found on the internet. Intuitively, consider the $\mathsf{P}$ vs $\mathsf{NP}$ problem. If we can define (yes we can) a measure on complexity classes with respect to a large class, say $\mathsf{ESPACE} = \mathsf{DSPACE}[2^{O(n)}]$, and prove that the measure of $\mathsf{P}$ is smaller than the measure of $\mathsf{NP}$, then $\mathsf{P} \neq \mathsf{NP}$. Moreover, we can prove statement like "almost all functions in $\mathsf{ESPACE}$ need $\Omega(2^n/n)$ gates", which extends Shannon bound to a restricted class $\mathsf{ESPACE}$. 

But after a brief reading, I do not see any claim about the fact that NLOGLOG is closed under complement; maybe a deeper look is needed. And the main result they have is that there are no nondeterministic fully space-constructible unbounded monotone increasing $s(n)$ functions for $s(n) = o(\log n)$. It is known that if such functions exists, then $\mathsf{SPACE}[s(n)] \neq \mathsf{NSPACE}[s(n)]$. And in the conclusion the author claimed that " ...this main separation problem remains open. " As @chazisop said, the relations of these low-level complexity classes are depended on the models, and it is stated in the entry of the zoo that 

For the only if direction, it can be proved by considering different types of edges in $G'$. Every edge other than $e$ (including those newly added edges) will be in at least one triangle (the one that contains the auxiliary node); and $e$ will be in a chordless odd cycle in $G′$ since there is a chordless odd cycle passing through $e$ in $G$, and the cycle is not removed during the node-deleting process. For the if direction, since every edges other than $e$ must be in at least one triangle, we only have to worry about the edge $e$. There is a chordless odd cycle passing through $e$ in $G'$ ($G'$ is a chordless odd cycle completion). The cycle cannot have length 3 by construction of $G'$, and since the cycle cannot contain any auxiliary nodes (by chordless property), it will be in graph $G$ as well. Therefore the proof is complete. 

Be careful of the definitions here. Normally we view $k$-clique problem as parameterized in $k$, so there is no sub-exponential time algorithm for it assuming Exponential time hypothesis. But here we let the problem be parameterized by the input size $O(m+n)$, thus the problem can be solved in $2^{O(\sqrt{m}\log m)}$, which is a sub-exponential time algorithm. And the theorem tells us that the $k$-clique problem is fixed parameter tractable under the some twist of the parameter $k$, which is reasonable. In general, problems in SUBEPT under SERF-reductions (sub-exponential reduction families) can be transformed into problems in FPT under FPT-reductions. (Theorem 20 in the paper) Furthermore, the connections are even stronger since they provided an isomorphism theorem between a whole hierarchy of problems in exponential time complexity theory and parameterized complexity theory. (Theorem 25 and 47) Though the isomorphism is not complete (there are some missing links between them), it is still nice to have a clear picture about these problems, and we can study sub-exponential time algorithms via parameterized complexity. See the survey by Jörg Flum and Martin Grohe, together with Jacobo Torán, the editor of the complexity column, for more information. 

The main problem is that Statement I is false, as @Marek just said. There are different graphs transformed into the same graph. For a concrete example, both (a) tetrahedron (double edges) and (b) octahedron transformed into (c) truncated octahedron. (a) (b) (c) 

Fun with Algorithms! Although it is a conference not a journal that you might looking for, among other things several NP-hardness results about (combinatorial) games are published here, including this one. However this is a triennial conference (except for 2014, where the last conference was in 2012), so if you want to publish it soon then there might be a better choice. 

We can use this to estimate a lower bound for genus of a graph, if the spectral radius of the graph is large enough. For more precise bound for the big-O constant please see the paper. The property as being a bipartite graph seems to help little here. They are able to provide a bipartite instance where the inequality on planar graphs is best possible. 

For detecting induced theta and pyramid, it is known to be in polynomial time. (In fact, the best known algorithm for theta takes $O(n^{11})$ time, and for pyramid it takes $O(n^9)$.) But for detecting an induced prism, the problem becomes NP-hard. One can see "Detecting induced subgraphs" by Leveque, Lin, Maffray and Trotignon for reference. The reason that theta and pyramid are relatively easy is related to the "three-in-a-tree" problem, described in "The three-in-a-tree problem" by Chudnovsky and Seymour. 

Let G be an n-node undirected graph, and let T be a node subset of V(G) called terminals. A distance preserver of (G,T) is a graph H satisfying the property $$d_H(u,v) = d_G(u,v)$$ for all nodes u, v in T. (Note that H is NOT necessarily a subgraph of G.) For example, let G be the following graph (a) and T be the nodes on the external face. Then graph (b) is a distance preserver of (G,T). 

(First of all, sorry for the long article which makes you want to skip through, but since the background and motivations are important to this question or it would be nonsense to the main problem, forgive me if this makes you sleep. I owe you a cup of coffee.) Background Isolation lemma, one of the best tool in complexity theory invented by K. Mulmuley, U. Vazirani and V. Vazirani in "Matching is as easy as matrix inversion", have been used to prove many amazing results like an RNC-algorithm for matching in the above paper; Valiant–Vazirani theorem which is a randomized reduction from NP problems to USAT; NL/poly=UL/poly; and many others. Surveys and introduction in blogs are also all over the web. In the center of the isolation lemma, we use randomization to assign weights on the base set $U$, such that for a family $\mathcal{F}$ of subsets of $U$, there is a unique minimum weighted subset in $F$ with high probability. Formally,