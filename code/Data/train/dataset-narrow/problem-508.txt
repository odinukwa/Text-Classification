Was setting the locking options on the indexes the direct cause? Why exactly would those settings cause deadlocks? 

We have a set of SQL Server instances (2008 R2) that run on the same virtual server. Each instance represents a stage in our development cycle (Dev/Test/Stage/etc.). The instances all need to be online at any given time, but load tends to be isolated to one instance as a time, depending on where we are in the release lifecycle. SQL Server seems to grab and hold whatever amount of memory we set as the max. What I'd like to know if there is any way of configuring the instances to be "smart" about memory consumption so they are not blocking each other from getting memory when needed. 

I added the ClearDB addon to Heroku, but upon using the created username to create a new database, the error appeared: 

I am trying to build the multi-level model in rub on rails to allow the following to happen: When a user creates a campaign they will select some categories and some metros from tables Category and Metro. Those selections will then be placed within two tables as campaign_category and campaign_metro. But upon the creation of those entries I wish for then another table to be populated with the permutation of those combinations within that table or a series of tables to allow for linking the campaign, the category and metros selected for a user to set a bid price on. For example say a user creates a campaign that has id 10 and selects category with id 1 and 2, and metros with id 3 and 4. In the tables campaign_category appears: 

Hopefully a simple question. What BI tools (commercial or OSS) support self-service reporting or self-service BI via an HTML web interface? I specified HTML because I know of at least one that can do it via the web, but only in Silverlight. 

A reindexing script was recently run in our dev environment that set and on all the indexes in the database. This was done while testing new reindexing scripts--it was not done intentionally. After this script was run, we immediately started seeing a large number of deadlocks where we normally never saw them. 

If speed is your one and only concern, and the data is for reading only (not updating/creating): all other things being equal, storing all the data in one table will be faster. Storing it in separate tables is storing it "normalized" (Normalization), and storing it in one table is storing it "denormalized" (Denormalization), 

Parsing the SQL and making decisions (mark ambiguous items for user) Finding the SQL inside the java code 

Whelp, Sorry I couldn't get back here earlier, things ended up getting very busy for a while. I appreciate all the help from everyone. It seems part of the problem was that some options do not accept as their values. Other than that, just setting my value higher seems to have done the trick. I also have to have a large enough setting for the otherwise Mysql does not have enough space for the bulk inserts, since approx. 25% of the buffer size is allowed I think. With these settings it holds flushing until an entire bulk insert is done, and then flushes to the hard drive. I get about 20-25MB write speed with pauses as it waits for the next insert, instead of the constant 1MB write speed I used to have. The only improvement left is to add enough ram to the box for it to support the workload. Here's my settings if anyone is interested: 

Pretty simple question: is it standard/best practice to backup dev and QA instances of the database? Assume that the schema information is kept in some form of source control. 

We have been experiencing some relatively large I/O (IOPS) spikes on the SAN that supports our production SQL Server. The spikes seem to fall at an exact time after the hour, each hour. We have investigated every known scheduled task source (SQL Agent, backups, scheduled SSRS Reports, etc.) and can't find any rhyme or reason to it so far. We have use Activity Monitor as well, but it has not yielded any answers to date. How would one go about definitively finding the source of I/O spikes? Are there monitoring tools (commercial or otherwise) that would help pinpoint the problem? 

How can I allow for the third level table to be populated with the total permutations of selected category and metro upon the creation of a campaign? 

and in table campaign_metro appears: In campagin_category_metro I want to have those permuted rows from the tables above as: 

I am trying to build the multi-level model as demonstrated. When a user creates a campaign they will select some categories and some metros from tables Category and Metro. Those selections will then be placed within two tables as campaign_category and campaign_metro. But upon the creation of those entries I wish for then another table to be populated with the permutation of those combinations within table campaign_category_metro to allow for linking the campaign, the category and metros selected for a user to set a bid price on. For example say a user creates a campaign that has id 10 and selects category with id 1 and 2, and metros with id 3 and 4. In the tables campaign_category appears: 

I am a big fan of the simple diagramming tool that comes with SSMS, and use it frequently. When I save changes to the model, I have it configured to automatically generate the change scripts that go along with the save. I then save (and source control) the resulting change script. This works great and an important piece of the process my team(s) uses. What occasionally happens is that a save fails, and I still get the option to save my change script. I then fix the problem and save again (which results in another change script). I'm never clear what I need to do at this point to maintain a consistent set of change scripts. There seems to be overlap between the two scripts (the failed and the successful), but they are not identical. If I want to continue to use this feature, what should I be doing with the resulting script as soon as I get a failed save of the model? 

Hopefully the SQL should be fairly recognizable. It exists in a string, and sql follows a fairly ridged format. Also, your non-aliased items will follow a simple format too. They will be in . Skip those, and only add aliases for not in that format. If you load in the db schema, you even get all valid to look for. Finally, if there is an ambiguous item, then write some context to stdout, and ask the user for input. I would suggest using python and the PLY library. You get a full blown lex/yacc combo, with an easier language than C. Open a new question on SO if you want more help/or to scream at me. ;) 

Given the readout notes on database heroku_09ace026abf6348, I have Attempting to access the Users and Privileges tab to alter the user, the below returns: 

and in table campaign_metro appears: How can I allow for the third level table (or series of tables) to be populated with the total permutations of selected category and metro upon the creation of a campaign and also work with ruby on rails active record constructs? 

Then a user can set a bid on those campaign/metro combinations. As of now, in creating a campaign, and selecting categories and metros I can get the tables campaign_category and campaign_metro populated but the final table, campaign_category_metro, remains blank. Below is my model as is: First Level: 

I've got a windows 7 64 bit machine that I'm using for some load testing of a mysql db. My program uses sqlalchemy to connect and run several statements on said database. These bulk loads all happen within a single transaction, all keys are disabled beforehand, and each csv file is only a few megabytes large. The problem I've run into is that the test machine gets IO bound. It has enough ram available (12G) to hold the entire transaction in memory and do a single flush out the other end. As far as I understand the manual, the innodb tables shouldn't touch the hard drive until it flushes the dirty pages at transaction completion. The total data to be loaded is about 1G, spread across the different tables. It ends up taking 37 minutes to load it all. Here's my current test settings for perusal. I'd also be happy to report the results from or similar queries if necessary. To recap, I need to know if 37 minutes is a fast insert speed for this data size, and what I can do to increase the insert speed. Edit: Whoops! I forgot some important info.