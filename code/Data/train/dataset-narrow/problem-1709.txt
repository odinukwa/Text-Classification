I know I can use IAM roles, but I found it's just too many moving parts, and complicates any scripts that have to use the access key/secret key (e.g. rewriting /etc/apt/* lines when it changes). Not to mention there is no way to attach roles to existing instances, which makes it even more pain. It's also not possible to simply restrict access by using VPC subnet, because S3 bucket access goes via public EC2 interface. 

Is there a way to let anonymous access to a certain S3 bucket only from my EC2 instances (all of them) within a single AWS account? 

It's single availability zone no backups not in a security group that's reachable from the outside world 

For some reason the CPU usage is at 20% CPU while I'm doing absolutely nothing, exactly every 10 minutes spiking to 28-30%. I thought there was something wrong with the instance, so I've re-created it, same thing. What does this? Is this an RDS phenomenon in general or is this specific to the burst capable instance classes? 

I have googled high and low but there doesn't seem to be any example doing raid10 with megaraid (only the syntax). Can anyone explain what is wrong? 

I need to configure Postfix to send for a specific domain. I have tried to google for returning custom error codes for a domain with no luck so far. 

When I create a new Outlook profile and re-add POP account, Outlook starts to redownload old messages from POP server if Outlook had previously configured with "Leave messages on server" setting. This occurs even when the very same PST file is used as message store. I would like to know, where does Outlook keep the information associated with "Leave messages on the server" setting. And if there is a way to "cheat" Outlook into thinking that it has already downloaded these mails (when I use the same PST from it's older install instance). 

What would be the best way to pass sensitive data to EC2 instance (on boot or otherwise) that only root can access? 

I cannot use UserData, because anyone can read it. I cannot use private S3 buckets for the same reason (metadata and hence credentials can be accessed by anyone on the box). I'd strongly prefer not to bake my own AMI, as it's quite a hassle. 

If a directory "foo" is owned by user A and contains a directory "bar", which is owned by root, user A can simply remove it with , which is logical, because "foo" is writable by user A. But if the directory "bar" contains another root-owned file, the directory can't be removed, because files in it must be removed first, so it becomes empty. But "bar" itself is not writable, so it's not possible to remove files in it. Is there a way around it? Or, convince me otherwise why it's necessary. 

But the last step never completes (Executing is displayed forever). When Management Studio is forcibly closed and restarted the new linked server is there but only contains Catalogues subitem. If we try to expand it, Management Studio goes into loop yet again. 

Both machines run Windows Server 2008 R2 now and have 10Gbit Supermicro AOC-STGN-i2S (actually they are Intel 82599 bearing Supermicro logo) in PCIe x4 slots- with a SFP+ direct attached twin axial cable between them. The second server is for testing only. First I installed ESXi on the 2nd and used the 1st as a datastore. I noticed that according to CrystalDiskMark, a VM on ESX only got 325 MB/s seq transfer rate (tried with both NFS and ISCSI). I ran the same test on the first server locally and got ~1000 MB/s. I wondered if the network link really kills 2/3 of speed, so I replaced 2nd's hard drive and installed Windows Server 2008 R2 and tried Jperf and NTTtcp. Jperf showed 400 MB/s and NTttcp showed 4300-4600Mbit/s. Windows Task Manager showed some 600 000 000 bytes per interval which translates to 4.47 Gigabits. I verified that both ends had full duplex and tried toggling jumbo frames on and off both ends but the difference was only 580 000 000 vs 600 000 000 bytes per interval. Why the throughput I'm seeing is only about half the theoretical maximum of 10 gigabits? ADDENDUM NTTtcp command lines: 

I don't specify any access/secret keys in the command line. My instance role was manually created (by me) and definitely does NOT grant any permissions on resources. 

Within plain EC2 environment, managing access to other AWS resources is fairly straightforward with IAM roles and credentials (automatically fetched from instance metadata). Even easier with CloudFormation, where you can create roles on the fly when you assign a particular application role to an instance. If I wanted to migrate to Docker and have a kind of M-to-N deployment, where I have M machines, and N applications running on it, how should I go about restricting access to per-application AWS resources? Instance metadata is accessible by anyone on the host, so I'd have every application being able to see/modify data of every other application in the same deployment environment. What are the best practices for supplying security credentials to application containers running in such environment? 

I have a server with 3 hard drives: one System Drive and two drives in RAID1 for data. Raid1 is done via Intel Storage Matrix software. Today Intel Storage Matrix said that array is degraded and must be rebuilt. I clicked rebuild array. After that I was no longer near this server. When I got back I was told that the process had finished but since they noticed that other computers could longer access the shared folders on the server, they restarted the server. After I logged back in, the data drive was gone for Windows (D: was now CDROM). And the Intel Storage Matrix Console showed a different picture: the little cross on red circle had gone away from "Port 0: INTEL SSDSA2M040G2GC" and this Port0 itself had moved from under "RAID Hard Drives" to "Non-RAID Hard Drives". When Intel Matrix Storage Console is opened, it asks if I want to reset hard drives to non-raid drives: 

When creating an autoscaling group I can choose an ordered list of termination policies for its instances. Amazon's documentation states that 

But it's not ideal, as it doesn't handle restarts and remote endpoint downtime very well, because it doesn't have anything like 's pooling, so I'll get duplicate logs and/or drop logs. Given that CoreOS has no package management, is there a conventional way to solve this painlessly? 

I am trying to use AWS autoscaling lifecycle hooks in a template that encapsulates the following things: 

I want to get an automated process for AMI creation going, and one piece remaining is automatically cleaning up the instance after image creation. The instance is booted with a user-data script that does the necessary setup, then kicks off image creation from self using AWS CLI. Then it shuts down. I could go with option and wait there until the image is ready, then terminate, but the docs state that "file system integrity on the created image can't be guaranteed", so I want to avoid using it. What's the best way to kill the instance from itself after image creation is completed? 

We recently migrated from Exchange 2003 to 2010. Half of our Outlook users are using Outlook from terminal server which is in the same domain as Exchange. The other half of Outlooks are installed on laptops which are not on domain. They usually use OpenVPN to connect to Exchange (and also to other services) but occasionally, when they are located somewhere where there are most outgoing ports blocked (hotels mostly), they use OutlookAnywhere. We have 2 certificates: one for 'ourexchangeserver', self-signed, and other for '*.ourexternaldomain.tld', signed by StartCom. By opening EMC > Server Configuration > Exchange Certificates, we can assign IMAP, POP, SMTP and IIS services to a given certificate. The problem is that RPC also seems to use this same certificate. So when we assign the wildcard certificate to IIS, we can access OWA externally without any security alerts but Outlooks display a security alert that host name is invalid (does not match the issued to field on the presented certificate). When we assign the self-signed certificate to IIS, it's the other way around: Outlooks don't complain but browser displays the same security alert when visiting OWA. My certificate provider (StartCom) does not allow me to generate a certificate issued to a host with missing or nonexistent domain part. Would it be possible to configure Exchange 2010 with these 2 certificates so that OWA would present the public certificate and RPC traffic would be covered with the self signed certificate? 

Basically what it ways on the tin, how can I create individual per-instance alarms inside an auto-scaling group created with a CloudFormation template? I can reference the ASG itself in an alarm and create ASG-level alarms, but cannot seem to specify dimensions to be "any EC2 instance belonging to this ASG". Is it possible or is my only option user-data script? 

I want to aggregate CoreOS logs to Papertrail service, which basically provides a syslog endpoint for aggregate logging. Common advice for this setup seems to be starting a service that does something like this: 

I want to forward TCP connections on a certain port of the machine A to another port on the machine B (which is actually the same that originated the connection to machine A) and simulate random or deterministic packet drops. How can I do it with iptables? 

Two way Intel E5504 @ 2GHz, 24GB RAM, 12x32GB Intel X25-E SSDs in RAID10. Intel Core2 6400 @ 2.12GHz, 3GB RAM, simple 80GB SATA drive. 

When you close VMware Player, you have two options: 1. Shutting down. 2. Suspending. Shutting down the virtual machine is like powering off your computer. Suspending actually creates a snapshot of your VMâ€™s state at that point, which you can restart from it later on. VMware Player only supports one snapshot per VM. However, if you need multiple snapshots, you can consider buying VMware workstation version. Source: $URL$ But you could make a copy of the virtual machine folder while it's suspended and name the folder MyVM_Snapshot1 etc. 

The users of a Windows Server 2003 terminal server should be able to use their national identity card (a smart card) inside the terminal session. This requires the Remote Desktop Connection's [x] Smart Cards checkbox to be checked. Unfortunately (for us), when they have their card in the reader and connect using Remote Desktop and that checkbox is checked (so that they can use the card inside the terminal session), they are "greeted" with PIN prompt instead of username and password prompt. They can click Cancel or press Esc every time and log in with username and password but is it possible to get rid of that PIN prompt without losing smart card support inside the terminal session (so that they can still authenticate themselves in a browser inside the terminal session)?