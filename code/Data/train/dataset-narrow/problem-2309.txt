If there is an optimal pps (pps = propositional proof system, an optimal pps is a pps that can p-simulate any other proof system) then the pps EF (Extended Frege) strengthened with propositional axioms stating the soundness of the optimal propositional proof system will be optimal. More generally EF + Soundness of pps P can p-simulate P for any P. So EF has a kind of generality that you don't need to change the logic or the underlying pps structure, but just add propositional axioms to it to get any arbitrary strong pps. In particular, if there is a super pps (a pps which has polynomial size proofs for all tautologies) then EF + Soundness of that pps will be a super pps. Note that $NP = coNP$ is equivalent to the existence of a super pps. Soundness of a pps P is a (sequence of) propositional formulas stating that if $\pi$ is a proof in P for $\varphi$, then $\varphi$ is true. In summery, there is no need to go outside propositional logic. ps: Note that all pps are effective by definition, a pps has a polynomial time verifier, and therefore its theorems are computably enumerable. $NP=coNP$ means that there is a super pps for the propositional formulas. Being propositional is important here. We know that there is no such thing for stronger logics but their nonexistence does not seem to have any effect on $NP$ vs $coNP$. 

So the power of $\mathsf{TC^0}$ is essentially sorting a binary string (eg. 100011 to 000111). This is true more generally when the numbers in the array are bounded. My question is what if the numbers are not bounded? Is the problem of sorting an array of unbounded numbers still in $\mathsf{TC^0}$? Is it complete for a larger class like $\mathsf{NC^1}$? 

I am interested in the complexity of this problem, in particular simple proofs showing the problem is in $\mathsf{AC^0}$ (or even $\mathsf{TC^0}$ or $\mathsf{NC^1}$). The common balancing arguments like those based on Spira's lemma apply repeated structural modifications to the formula tree which seem to only give $BFB \in \mathsf{NC^2}$. I have a proof for $BFB \in \mathsf{AC^0}$, however the proof is not simple and depends on the proof of $BFE \in \mathsf{NC^1}$. 

Many believe that $\mathsf{BPP} = \mathsf{P} \subseteq \mathsf{NP}$. However we only know that $\mathsf{BPP}$ is in the second level of polynomial hierarchy, i.e. $\mathsf{BPP}\subseteq \Sigma^ \mathsf{P}_2 \cap \Pi^ \mathsf{P}_2$. A step towards showing $\mathsf{BPP} = \mathsf{P}$ is to first bring it down to the first level of the polynomial hierarchy, i.e. $\mathsf{BPP} \subseteq \mathsf{NP}$. The containment would mean that nondeterminism is at least as powerful as randomness for polynomial time. It also means that if for a problem we can find the answers using efficient (polynomial time) randomized algorithms then we can verify the answers efficiently (in polynomial time) . 

Please note that I don't want pathological functions like uncomputable one mentioned above, I want natural functions that people are really interested in computing and it is plausible that can be or could have been computed uniformly. Edit: I know that $BPP \subseteq P/poly$. So an answer which is not a derandomizations result is more interesting for me. Edit 2: As András Salamon and Tsuyoshi Ito have said in their answers, $Sparse \subset P/poly$, and there are interesting problems in $Sparse$ that are not known to be in $P$, so formally they have answered what I have asked, but that does not help with what I am really interested in since the reason that they are in $P/poly$ is the possibility of hard coding a sparse language into the circuit. A language which is not sparse would be more interesting. 

I am looking for resources (preferably a handbook) on advanced topics in algorithms (topics beyond what is covered in algorithms textbooks like CLRS and DPV). The type of material that can be used for teaching a topics in algorithms course like Erik Demaine and David Karger's Advanced Algorithms course. Resources that would give an overview of the field (like a handbook) are preferable, but more focused resources like Vijay Vazirani's "Approximation Algorithms" book are also fine. 

QBFs where the quantifier free part is monotone in all variables. Then this is in $\mathsf{P}$ as Ryan noted. Because the quantifier-free part is monotone in quantified variables we can remove the quantifiers and replace the existentially quantified variables with 1 and universally quantified variables with 0, the original quantified formula is true iff this modified quantifier-free formula is true, and this reduces the problem to monotone formula evaluation which is in (and complete for) $\mathsf{NC^1} \subseteq \mathsf{P}$. (If we are taking supremum of a function over a variable and the function is monotone in that variable we only need to compute the value of the function over the maximal values for that variable.) QBFs which are monotone in the input variables. This is $\mathsf{PSpace}$ under $\mathsf{AC^0}$ reductions, the reduction of the QBF to this version is simple and is similar to the proof for monotone boolean formula evaluation being complete for $\mathsf{NC^1}$ or monotone circuit value being complete for $\mathsf{P}$. 

$n$ and $k$ are both parameters that can increase to arbitrarily large values, not constants. Your function is $g(n,k) = n^k$ and it is not in $O(f(k)*n^c)$ for any function $f$ and constant $c$. The whole point here is that $f$ does not depend on $n$ but only on $k$ and the exponent $c$ in $n^c$ is an absolute constant that does not depend on $k$ or $n$. 

Relativization and Algebraization are bit more tricky and dependent on the way we define the relaztivization for these classes. But as a general rule simple diagonalization (a diagonalization which uses the same counter-example for all machines computing the same function, i.e. the counter-example only depends on what machines in the smaller compute and does not depend on their code and how they compute) cannot separate these classes. It is possible to extract non-simple diagonalization functions from indirect diagonaliztion results like time-space lower-bounds for SAT. 

For functions, when we fix an order the definition of lowerbound/upperbound follows from it. If the order relation is asymptotic majorization (ignoring constant factors) $$f \preceq g : \exists c,n \forall m>n. \ f(x) \leq c g(x) $$ then the definition is the usual definition of $O$ and $\Omega$. Both of them are used extensively in other areas like combinatorics. But when we talk about a lowerbound for a problem (an algorithm) what we really want to say is that the problem requires certain amount of resources (for running the algorithm solving the problem). Often the complexity classes are parametrized by functions like $\mathbf{Time}(t(n))$, and we simple say that the problem is lower bounded by a function, but this only works for nice functions (e.g. the running time of the algorithm is monotone, etc.). What we want to say in these cases is that we need $n^2$ running time to solve the problem, i.e. less than $n^2$ running time is not sufficient, which formally becomes Lance's definition that the running time of the algorithm is not in $o(t(n))$. 

The first studies the formalization of existing lower bound proofs in weak fragments of arithmetic (upper bounds on hardness of proving complexity theory lower bounds). The second paper is about lower bounds on the hardness of proving stronger complexity theory lower bounds: how much math do we need to prove $\mathsf{P} \neq \mathsf{NP}$? Do we need $ZFC$? $ZF$? $PA$? Maybe at least $PV$? ($PV$ is considered the theory corresponding to reasoning using concepts in $\mathsf{P}$). An ideal result for the second paper would have been: 

Another interesting survey about UGC by Khot cited in [1] which is more math oriented: S. Khot, Inapproximability of NP-complete Problems, Discrete Fourier Analysis, and Geometry, ICM 2010. 

According to the Complexity Zoo, $\mathsf{Reg} \subseteq \mathsf{NC^1}$ and we know that $\mathsf{Reg}$ cannot count so $\mathsf{TC^0} \not\subseteq \mathsf{Reg}$. However it doesn't say if $\mathsf{Reg} \subseteq \mathsf{TC^0}$ or not. Since we don't know $\mathsf{NC^1}\not\subseteq\mathsf{TC^0}$ we also don't know $\mathsf{Reg} \not\subseteq \mathsf{TC^0}$. 

For general circuits, we know that there are problems in $\Sigma^p_2 \cap \Pi^p_2$ that require circuits of size $\Omega(n^k)$, this is due to Ravi Kannan (1981) and is based on his result that $PH$ contains such problems. I think the best lowerbounds for $NP$ are still around $5n$. See Arora and Barak's book, page 297. Richard J. Lipton had a post on his blog about these results, also see this one. 

Cook-Reckhow propositional proof systems are nonunifrom. E.g. the computational complexity counterpart to the class of polynomial-size $\mathsf{Extended Frege}$ proofs is the nonuniform complexity class $\mathsf{P/poly}$. We have to look at their uniform counterparts: E.g. the proof complexity counterpart for $\mathsf{P}$ are bounded arithmetic theories like Cook's theory $\mathsf{PV}$ (standing for polynomial-time verifiable), Buss's theory $\mathsf{S}^1_2$, ... Cook and Urquhart used Cook's theory $\mathsf{PV}$ to define a theory of higher-types polynomial-time computable functions $\mathsf{PV}^\omega$ in the following paper: 

The answer is yes, pick your favorite way of coding terms in F, simply because they are countable number of expressions. Just being able to assign numbers to expressions in a system is not useful by itself, the question is can it be done in a manner that supports some operations, e.g. substitution, inside the system. The answer is again yes, the standard one works, operations are computable in Girard's F. 

The proof for $\mathbf{P}=\mathbf{AltTime}(O(1))$ ($=\mathbf{PH}$) is an induction using $\mathbf{P}=\mathbf{NP}$. The induction shows that for any natural number $k$, $\mathbf{P}=\mathbf{AltTime}(k)$ (and $\mathbf{AltTime}(O(1))$ is just their union). The induction does not work when the number of alternation can change with the input size (i.e. when the number of possible alternations of the machine is not a number but a function of the input size, i.e. we are not showing that an execution of the machine on a single input can be reduced to no alternation, we are showing that the executions of the machine on all inputs can be "uniformly" reduced to no alternation). Let's look at a similar but simpler statement. We want to show that the identity function $id(n)=n$ eventually dominates all constant functions ($f \ll g$ iff for all but finitely many $n$ $f(n) \leq g(n)$). It can be proven say by induction. For all $k$, $k \ll n$ (i.e. $f_k \ll id$ where $f_k(n)=k$), but we don't have this for non-constant functions like $n^2$, $n^2 \not \ll n$. 

It would be better if you specified what you mean exactly by hyper-computation and gave evidence for why you think it has "died down". In any case, assuming that you are talking about computation of functions over natural numbers (and finite strings) (since I think it is clear that models for higher type computation is a very active area, e.g. CCA) and models of computation not equivalent to computability defined by Turing machines, I don't think the claim is correct, for example see CiE'05 and CiE'11. Also see the criticisms made against the claim that hyper-computation is something new: 

See the Wikipedia article for propositional proof complexity. p-simulation is similar to reductions between complexity classes. Existence of a p-optimal proof system is a well-known open problem in proof complexity. It is statement 7 in Krajicek and Pudlak 1989. See page 1066 and 1067 for a list of statements implied/implying it. There are more statements that you can find by Googling. There are experts who conjecture that extended Frege is an p-optimal proof system. Existence of a p-optimal proof system is a weaker statement than existence of a super proof system (which is equivalent to $\mathsf{NP}=\mathsf{coNP}$). Jan Krajicek and P. Pudlak, "Propositional Proof Systems, the Consistency of First Order Theories and the Complexity of Computations", J. Symbolic Logic, 54(3), (1989), pp. 1063-1079. Yijia Chen and Jörg Flum, "On optimal proof systems and logics for PTIME", 2010. Zenon Sadowski, "On an optimal propositional proof system and the structure of easy subsets of TAUT", 2002. 

Was it known that Boolean formulas can be balanced in $\mathsf{AC^0}$ ($BFB\in \mathsf{AC^1}$)? Is there a simpler argument (e.g. not relying on $BFE\in \mathsf{NC^1}$) for $BFB\in\mathsf{AC^0}$? 

Was it known that Boolean formulas can be balanced in $\mathsf{AC^0}$? Is there a simple proof of Boolean formula balancing being in $\mathsf{AC^0}$? 

Can nondeterminism speed-up deterministic computation? If yes, how much? By speeding-up deterministic computation by nondeterminism I mean results of the form: 

circuit is encoded as the adjacency matrix of the connection graphs where gates are numbered from $1$ to $n$, there is no wire from $g_i$ to $g_j$ then $i>j$, and the list of gate types. You can also assume that the number of gates and the size of the input are also provided (in unary). the direct connection language of the circuit (which is used in the definition of $\mathsf{DLogTime}$-uniformity of circuits), the extended connection language of the circuit (which is used in the definition of $\mathsf{DLogTime}$-uniformity for $\mathsf{NC^1}$). 

Yes, the topic has been studied in proof complexity. It is called Bounded Reverse Mathematics. You can find a table containing some reverse mathematics results on page 8 of Cook and Nguyen's book, "Logical Foundations of Proof Complexity", 2010. Some of Steve Cook's previous students have worked on similar topics, e.g. Nguyen's thesis, "Bounded Reverse Mathematics", University of Toronto, 2008. Alexander Razborov (also other proof complexity theorists) has some results on the weak theories needed to formalize the circuit complexity techniques and prove circuit complexity lowerbounds. He obtains some unprovability results for weak theories, but the theories are considered too weak. All of these results are provable in $RCA_0$ (Simpson's base theory for Reverse Mathematics), so AFAIK we don't have independence results from strong theories (and in fact such independence results would have strong consequences as Neel has mentioned, see Ben-David's work (and related results) on independence of $\mathbf{P} vs. \mathbf{NP}$ from $PA_1$ where $PA_1$ is an extension of $PA$). 

note 1: on page 49 Kushilevitz and Nisan mention a framework which involves a referee but seems very different from what I am asking. note 2: I am not sure if calling R a referee is the right thing, please comment if you have a better suggestion. 

We know that circuits of depth $k$ can be evaluated with circuits of depth $k+c$ where $c$ is a universal constant. This means circuits of depth $k\lg n + o(\lg n)$ can be evaluated by a circuit of depth $O(\lg n)$. However $O(\lg n)$ doesn't contain a function that eventually dominates all functions in $O(\lg n)$. We know that formula evaluation problem is in $\mathsf{ALogTime}$. Every $\mathsf{NC^1}$ circuit is equivalent to a Boolean formula. Can't we compute the extended connection representation of an equivalent Boolean formula from that of a given $\mathsf{NC^1}$ circuit in $\mathsf{ALogTime}$? The extended connection representation of a circuit includes 

The statement is false for any polynomial time recognizable family of tautologies: the proof system will simply check if the formula is one of them and accept if it is. Proof length of them will be O(1). So I don't think any explicit example is known. Recall that we don't even know if SAT $\notin$ DTime(O(n)) so we also don't know if SAT$\notin$ coNTime(O(n)) which is equivalent to your question TAUT $\notin$ NTime(O(n)) and follows from a positive answer to your question. On the other hand, if NP is not equal to coNP, then the family consisting of all tautologies has super polynomial length in any efficient proof system. Part of what people try to do in prof complexity is to rule out interesting classes of algorithms. A lower bound in a proof system implies a lower bound for all (co-nondeterministic) algorithms whose correctness can be efficiently proven in the proof system (if we can formalize and prove the correctness of an algorithm for SAT in a proof system we can consider its failing execution to find a satisfying assignment as proofs of the tautologihood). 

The parameter for the family is the length of the proofs. Soundness is also known as the Reflection in the proof complexity literature. We say two ppses are equivalent when the size of shortest proofs of tautologies in them are polynomially related, i.e. each one can simulate the other one with at most a polynomial proof size increase. The choice of $EF$ is not essential for the equivalence in the question and one can take a weaker pps like Resolution (with some modifications). AFAIK this stronger version is first stated by Jan Krajicek. The statement in the question is essentially saying that the soundness tautology for $Q$ is universal/complete for $Q$. 

It is easy to see that such a function can be turned into a one-way function as if it was easy to find $k$ from $\varphi_k$ then we can find the answer by computing $A(k)_2$. On the other hand, let $f$ be a one-way function. We can express $f(x)=y$ as a polynomial-size circuit since $f$ is computable in polynomial-time (and we can turn it into a formula by introducing new variables for all gates and locally enforcing the condition for the correctness of the computation as in Tsien's translation). Let's consider $y$ as parameter and denote the resulting formula as $\varphi_{f,y}(x)$. We can ask if there is any $x$ which satisfies $\varphi_{f,y}(x)$. Any polynomial-time algorithm solving these $SAT$ instances with non-negligible probability will break the one-way function $f$. However this uses the fact that the adversary needs to find a witness not just the fact that the formula is satisfiable or not (but I think we can get around this issue by using the hard-bit of $f$). See also chapters 29 and 30 of Jan Krajicek's book "Forcing with Random Variables", 2011 on proof complexity generators.