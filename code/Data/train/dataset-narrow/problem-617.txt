As already stated in the responses below your request it is most certainly due to the unbalanced nature of F and T causing SQL Server to prefer scan over seek. Though this is quite some simplification as you are querying a view containing quite a lot of tables and WHERE as well as JOIN conditions. In case you query "[...] and MOVIsResolved = 'F'" the SQL Optimizer most likely prefers a SCAN over a SEEK, as the given result is that large. with "[...] ='T'" it prefers a SEEK as the result set is small enough. And finally with "[...] <>'F'" the optimizer has no other choice but to SCAN and check each entry, whether the column contains not 'F'. In the end for you it is quite clear "<> F" equals "= T" as you know there can only be these 2. SQL Server does not know this. Therefore the resulting execution plans for "<> 'F'" and "= 'F'" are the same. As an alternative you might wanna look into OPTION( OPTIMIZE FOR UNKNOWN ), thought I do not know, whether this actually works with hard coded queries as yours. 

This answer will require some dynamic SQL and potentially could make use of a while/do loop. Basically the idea will be to figure our how to define your list of tables and column names. If you really are just looking for a list of tables where a column name exists, you will probably want to make use of the sys.syscolumns table to query the column names. 

Once you have the anchor statement, you can UNION below it for all iterations down the chain. In fact, I'm not going to bother rewriting everything Pinal Dave said since it's so right on point in this case. 

If you want to have a computer be assigned to more than one user, you're going to need a junction table between the two to store the relationships. This was not stated in the requirements but it seems like a logical approach given the project scope. 1. Account 2. Computers 3. Account_Computer_Assignments. The third one is your junction table that stores the relationship between the other two. It's primary key would consist of IDAccounts and IDComputers using your naming convention. A foreign key relating back to their parent tables would also be advisable. This structure will also allow for multiple users to link to the same computer. I don't know if that is the intended behavior but it will allow for that with this design. If that is not intended, and a single computer can only be monitored by one and only one user, then a single field on the Computer table for the AccountID is all that is needed. No junction table. 

Beware the OS authentication is done by client machine, not by the database server. I think that within reasonable pessimism to Oracle tools you could expect that it boils down to this: database server receives a TCP connection from whichever IP address that might pass the network path, and that connection just claims "I promise I've done OS authentication and, trust me, this connection is made on behalf of OPS$JohnSmithCEO." Neither the database client or database server is given OS password for additional OS authentication! And you can't even trust that this connection came from a valid Oracle software. It can be man-in-the-middle in reality. It's worse than telnet. Whatever "Windows-specific" checks are done by Oracle, they seem contrived and you can't really trust they are complete and secure. 

I guess this is one of the "It depends"-moments. In a regular execution there is no or at least nearly no difference. Thought it will become more interesting once the queries are more complex. More complex scenarios are e.g.: 

I would have added LEFT and CROSS Joins as well, thought they are already part of your example. About the "experience in SQL Server normalization" - I'd say, besides looking in the code, there will not be much you can hold your self on. It is often just a "It depends!" 

I also have stumbled about the article $URL$ and cannot find the correlation to the scripts above. Also I'm wondering, why there exists a differentiation for "@logicalCPUs >= 8 and @HTEnabled = 1 and @NoofNUMA = 1" and "@logicalCPUs >= 8 and @HTEnabled = 1 and @NoofNUMA > 1" as the result becomes the same. After all I ended up writing my own piece of code matching the article from above, though even there I would have loved a more precise definition and/or differentiation about "processors" "CPU" and "physical processors". Feel free to have your spin with it. 

The 'UPDATED' column from this query can be used to do a quick find and replace on whatever database or table name might be found in the view definition. You will want to make sure you do the same for every possible version of the text you are searching. For instance a database called 'db' might be written as [db] or db. Once you have what you want from the query just copy and paste to a query window and execute the whole thing. Please make sure to backup the database prior to running it. Also carefully proof read the output text so you can verify that the replace worked correctly and did not accidentally change more than you intended. BTW, the answer WEI_DBA gave is perfectly acceptable also. It's just a different approach to the same thing. In his example, the GUI will aide in getting all of the object definitions to the query window. Then you would just use Find/Replace(ctrl+h) to replace the database name. To each his own in this case. I don't know that there's an advantage in either approach except maybe that my approach will allow you to limit the query only to certain objects (views in this case) with additional filters added if needed in the where clause. Generate scripts will allow you to filter to only views, stored procs, etc, and then individually select objects but additional filters will be limited to whatever the UI can do. 

Verify here you have had only a single control file and not two or three of them. If you have had more than one control file, then this instruction is not for you. 

In Oracle starts a PL/SQL block. In other words, after you ought to provide a text of a program that is written in a procedural language that is somewhat different than SQL (although it bears some similarities). Your sqlplus had read everything but it was not yet parsing or executing anything, it was waiting for an and a line containing only a slash like this: 

You don't want your test database to retain the same internal id number as the production database. Hence it would be best to use RMAN's command DUPLICATE, because it sets a different id (and also a new database name). This command is specifically designed to do what you require. 

Well, actually having a PK with 5+ columns is not necessarily bad in itself. It becomes bad once the PK is also the clustered index as that one would count as the row identifier and thus would be added to each row in a NC index. This would drastically increase the required space. It would also be bad once you actually use the PK by another FK, as you have to have the data of all 5+ columns in both the current table as well as the one referencing from. Once again it will increase the storage by a lot! Performance-wise it will be bad once the PK is been used as an index - let it be solely within the table or in conjunction with a FK - as a bigger PK-Key containing 5+ columns will take more space, thus less entries will fit within a page and henceforth more pages need to be read to analyze the index. That said - there might always be a good reason for actually doing so anyway, like e.g. a fact table. Therefore the best answer would actually be as in most cases: It depends! Regards Dennis 

The green bug you're seeing (look closely) means the package have been compiled with DEBUG option. It is also seen on bare procedures and functions. It is a coincidence they don't expand - maybe a bug (duh) in SQLdeveloper :) On mine, these expand without problem. 

Don't use bare because without SET UNTIL it doesn't work as you'd expect in some scenarios. I wouldn't say any of these two solutions are inherently safe, both have as many pitfalls as any other Oracle's advanced features, so you need to do some research upfront. 

The "device" in RMAN is a misnomer, it should be really called "storage". The "sbt" (synonym of "sbt_tape") is a misnomer again, as it has NOTHING to do with any tapes, it should be simply called "non-rman". This is just an empty placeholder, to be filled with any "plugin"; the plugin is called by Oracle either the "Media Manager library" or SBT_LIBRARY. This plugin allows rman to store and retrieve files through it, so rman only tells that it needs a file handle "xyz" (file is identified by a string handle) and doesn't need to know how the file is delivered, from tape or anything. The plugin is normally a part of an independent backup software, such as IBM TSM or Symantec NetBackup or many others. Oracle provides a simple emulator SBT_LIBRARY=oracle.disksbt for testing. Since you didn't fill that placeholder with any "Media Manager library", you receive an error message as expected. 

As long as it is run as a single transaction, the value will be re-used on each line, not recalculated. I'm not 100% sure about Postgres or SQL Lite but I'm fairly confident (98%) they will work the same. In the past when I was more confused about this question, I would pre-fetch the date value to a variable and then reuse the variable in the where clause. Now I realize that was pointless unless I have more than one place in a longer set of statements where I would like to use the value. As a test, move the calculated date value to the select clause and execute against a large table that takes more than a few seconds to return. You will notice that the first line will have the same date/time value as the last record. 

The next thing in terms of cost is an OS (machine) and its IP address. You cannot afford a separate system for every TNS name. So crmdb.mydomain.local is not the only name for the IP address; the same IP address would have more names, like financedb.mydomain.local. Your OS admin would decide how to do this best, and how to determine the main hostname of the OS. They have the same problem with many other systems - multiple names referring to one OS - so they should have a solution at hand. The only people who are confused now are DBAs and OS admins, they see multiple hostnames leading to the same IP address. But users don't care about that and are not confused by that. (By the way this approach is coherent with SCAN. ) The next thing in terms of cost is either one of the two: an Oracle instance or "administrative cost of separating schemas out of instance". The tradeoff is for you to decide. 

If going down the view approach which would limit disk IO, be more normalized and MAY perform better(depending on the use of the value), it would look something like this; 

SSIS is going to be your best bet in this case I believe. If you already license SQL Server, it should be included as well. An SSIS package has the ability to access and utilize multiple ODBC or OLEDB sources and destinations including MySQL. It can be run using a SQL Agent job or just using DTEXEC from a Windows scheduled job. Also you can easily build logging, error checking and validation. 

If you want to see all values from Table2 and see all records from table1 where there is a matching email, do this;