Even though the first query supposedly has the higher cost by a 4:1 margin it runs faster than the second one. Why is it that a simple distinct added to the query will add the (what I assume to always be bad, corrections are welcome) hash match operator? And why does it have the higher query cost relative to the second query if it runs faster. 

If I have two tables that have identical constraints, datatypes and primary keys is there a way to 'import' the index from one table to another if they're on the same server, but in separate databases? 

I'm beginning to learn some about looking at execution plans and making queries more efficient Consider these two basic queries 

I'm new to the database world and have recently started working with a large database with several tables with mainly varchar text and integers. The two largest tables are of ~50 million and 25~million rows. The database contains about 350,000 ID numbers for people and I often need to retrieve information about all individuals that involves joins to both of my very large tables (they're one to many relationships). These queries are also temporal in nature and involve the operator to determine events that happen without a certain time frame. It will often take 10-15 minutes for some of these queries to run (I'm still learning and try new indexes to see if I can improve performance. After running out of ram running a particular query I had to my computer froze and I had to reboot. Even after restarting I was unable to detach, drop connections and delete my log files to delete my database (which was in recovery mode). I booted into safe mode to delete the mdf and ldf files and saw that my log file was twice the size of the data file. If I routinely need to run queries that will return aggregate, temporal information on tables of the above-specified size, is there anything I can do to prevent log bloat? Also, I know SQL Server eats resources for lunch, but what type of specs would a computer need to have to run a query like the following with the table sizes listed above? (it takes an hour on my local machine) EDIT: this database is static in nature and will not have anything added to it. it is also only unavailable to one user, me. I'm not sure what type of recovery it had, I don't have that PC in front of me at the moment. query: 

I don't think Oracle keeps track of past closed queries. However, you can find out what cursors a session has opened with . Since many applications cache the cursors for later reuse (this is automatic in PL/SQL: a cursor won't be completely discarded unless you reach the maximum number of open cursors), in many cases all past queries will be in this view: 

You can create the database link by connecting directly to the remote database. As suggested in the askTom discussion, you can also use or to create a distinct remote transaction that can initiate the DDL statement. 

Range partitioning involves a bit more maintenance because you have to create the partitions yourself. However, once the partitions are created, range and interval work similarly. 

first define a window with . This window could have a 24 hour duration and repeat every day so that it is always open. modify your job submission so that they run under this window (parameter of the procedure). when you need to enter maintenance and/or release resource, use . When your maintenance is done, use . 

You have the basics right. There is only one type of commit (no normal, fast...). from the concepts doc: 

(additional restrictions on updating views apply) In your example you update table only. Oracle has to make sure that for a single row of this table, only one row of the other can be found. This seems to be the case since you have a PK on . Therefore you should be able to update the join. See for example this SQLFiddle with a similar setup. In your case you should either: 

In Oracle, DDL on remote database is not permitted. One likely reason is that a distributed transaction commit can not be initiated at the remote site (you can't ) and since DDL statements include a commit they are not permitted. You would get an with other DDL statements: 

I have found only one way to get to use the invariant culture: pass a string starting with the null character for the parameter. 

Although the error message reported in that KB article is different than the one you are reporting, the contributing factors sound very similar. SQL Server 2016 SP1 CU3 first included the fix, as seen in its hotfix list. However, there have been reports that it did not resolve the issue in all situations. SQL Server 2016 SP1 CU4 also includes a (presumably updated) fix for this, and KB 4019893 has since been updated to show SP1 CU4 as the version the issue was fixed in. Unfortunately, I can confirm from my own experience that even the fix in SP1 CU4 does not fully resolve that issue. I currently have one TDE-enabled database that still produces consistently corrupt backups even on SP1 CU4 when using (via > 64 KB) and . I also have several dozen other TDE-enabled databases in this environment that consistently don't produce corrupt backups under those settings, including one that is a variation of the one that does, with a nearly identical schema but smaller dataset. This would seem to indicate that Microsoft is indeed chipping away at the scenarios that can cause this, but has not resolved all of them yet. I've not yet tried using to work around this issue, as referenced in another answer and the SQLCAT blog post, but I'll provide an update here if I'm able to try that and it resolves the issue. The one database I have that reproduces this is unfortunately rather large (~1 TB), and resides in a Development / QA cluster that doesn't have much extra storage space available (at least at that scale), so testing variations of this has proven to be logistically challenging and time-consuming. 

Despite that note, the blog post has not been updated with any further information since then. However, KB 4019893 may also address this: