Assuming you really need the account to be member of the administrator group, then the proper way to implement this type of delegation of duty is to place the machine behind a separate firewall system and use a different set of credentials to manage that device. Another solution would be to use a regular user account and grant it the rights needed for doing whatever needs to be done. Unfortunately, it isn't always possible to implement either of these. In this case, I suggest that you describe the actual problem you want to solve (instead of the issue you're having with a solution you already chose) and see if someone can offer some advise. 

WoW64 means "Windows on Windows 64". Basically, it's the 32-bits environment inside a windows 64 OS. Every win32 executable will be running "under" WoW64: there is no other way for the to run because they cannot use the 64bit version of the OS functions: they need to go through the translation layer provided by WoW64. 

You need to combine @MDarra and @LarsWA answers to make it work: setup both sites to use host headers in your binding as well as port 80. basically, you'll have the following bindings: 

It looks like this is part of a device driver for a Matrox graphic card. Assuming that the serer is using that brand of graphic card, you could try uninstalling the driver completely (reverting to VGA) and installing it again. 

I have seen this behavior in other server software where nod32 is installed (as well as random crashes). This included all version of the software up and including 5.0.x. Disabling the real-time scanning component solved the issue for us. ESET support wasn't able to help us in any other way. We have not however, updated or tested version 6 so maybe it does fix it. In any case, I encourage you to get into contact with ESET support. 

As I understand it, an instance needs to be granted access to resources in order to do anything with CloudFormation. But when I run this on a Beanstalk web server instance: 

I don't specify any access/secret keys in the command line. My instance role was manually created (by me) and definitely does NOT grant any permissions on resources. 

I want to forward TCP connections on a certain port of the machine A to another port on the machine B (which is actually the same that originated the connection to machine A) and simulate random or deterministic packet drops. How can I do it with iptables? 

Basically what it ways on the tin, how can I create individual per-instance alarms inside an auto-scaling group created with a CloudFormation template? I can reference the ASG itself in an alarm and create ASG-level alarms, but cannot seem to specify dimensions to be "any EC2 instance belonging to this ASG". Is it possible or is my only option user-data script? 

How come I can still read any CF metadata? I noticed that in the client code, the script goes to use instance credentials ( is True) 

But it's not ideal, as it doesn't handle restarts and remote endpoint downtime very well, because it doesn't have anything like 's pooling, so I'll get duplicate logs and/or drop logs. Given that CoreOS has no package management, is there a conventional way to solve this painlessly? 

When creating an autoscaling group I can choose an ordered list of termination policies for its instances. Amazon's documentation states that 

Yes, you can perform this, but it isn't exactly trivial. First, you need to establish a certificate authority that trusted by the client. Once oyu have done that, you can use the sslbump feature of Squid to perform the decryption (see $URL$ While you do not explain the context of what you want to do, it is probably worthwhile mentioning that if what you want to do is debug outgoing HTTPS connections coming from a windows machine, you can perform this in a much simpler way by using the Fiddler web debugger ($URL$ 

(or sometimes straight ) Which tells the mail client that there is multiple parts in the message. At this stage, each part must indicate what type it is supposed to be in (in addition to several other parameters). All that to say that, if you mail application sends the message as plain text and you want to convert that as HTML, you will have to, at the very least, change the message's content-type header in addition to change the message body itself. 

Check your machine's BIOS. There is usually an option that controls how the system will behave when a power failure occurred. Typically, you want to change that setting from "do nothing" to "automatically turn on when power is applied". Unfortunately, details changes from BIOS version to BIOS version so I can't give you more details about what that option is called, but it should be in the power management options. 

Actually, you can setup the new DCs with new names, transfer the roles, decommission the old DCs and then rename the new ones. However, as mentioned by other comment, it's neither necessary nor desirable to do so if these machines are only DCs. Is there any other requirements you forgot to mention ? 

I know I can use IAM roles, but I found it's just too many moving parts, and complicates any scripts that have to use the access key/secret key (e.g. rewriting /etc/apt/* lines when it changes). Not to mention there is no way to attach roles to existing instances, which makes it even more pain. It's also not possible to simply restrict access by using VPC subnet, because S3 bucket access goes via public EC2 interface. 

But it glosses over the specifics of how these policies are combined and when the "fall through" happens to the next policy in the list, i.e. under what conditions each policy fails and moves on to the next policy in the list. For example, I have a policy list in my group and yet after scaling up and then down, the scaling group proceeded to terminate by newest (and healthy) instance (newer by a large margin), and I can't figure out why. Additionally, according to the same doc, default policy is actually itself a combination of policies, and includes and as two of its steps. If I have a list that includes , does it evaluate and twice? Lastly, does the termination consider load balancer? For example, if my new instance failed to initialise properly and is not in-service with the load balancer, and is in effect, will scale-down action kill the unhealthy instance first even though it's newer? 

For some reason the CPU usage is at 20% CPU while I'm doing absolutely nothing, exactly every 10 minutes spiking to 28-30%. I thought there was something wrong with the instance, so I've re-created it, same thing. What does this? Is this an RDS phenomenon in general or is this specific to the burst capable instance classes? 

Correct me if I'm wrong, but JFS is not a full journaling file system: it only handles the metadata in the journal. This means that the fsck command will take a looong time to complete if you have lots of data. I suggest you investigate the possibility to switch to a fully journaled file system (etx3/4): that should remove the need for the command to be run in case of abrupt failure. 

If that explanation is not sufficient, could you please clarify exactly what you want to protect by SSL encapsulation ? 

The only thing that worked was take the machine to a completely different network (an ethernet port on the WRT54GL wifi router that we use to connect customer to a dedicated ADSL line that has nothing to do with our internal network). I'm out of ideas here. There is something that changed in our internal configuration and not the whole network is having a fit when it is asked to handle that mac address. When I start a PING on the faulty machine and uses Wireshark on the destination, I see the same thing happening: when the ping on the machine fails, i see nothing in my trace. When the ping succeeds, I see the echo request coming in and the replies going out. The MAC address I see in the packets matches. I'm really out of ideas here about what is happening. Anyone cares to make a suggestion ? 

Once all of this has been done, enable the WAF and it should work. If it doesn't, then start by creating a new firewall profile, select the "monitor" mode and leave everything unchecked: that's the most permissive firewall rule possible and test again. If that doesn't fix it, the review the logs on your web server: did it receive the requests ? Did it reply to them with a valid response code ? Etc. If all else fail, please describe your setup in detail, including what application you're running on the web server (because some simply do not work with Sophos WAF, like OWA or Citrix web interface) 

I cannot use UserData, because anyone can read it. I cannot use private S3 buckets for the same reason (metadata and hence credentials can be accessed by anyone on the box). I'd strongly prefer not to bake my own AMI, as it's quite a hassle. 

Is it possible to buy an intermediate certificate to use it to sign subdomain certificates? It has to be recognised by browsers and I can't use a wildcard certificate. The search turned up nothing so far. Is anyone issuing such certificates? 

What would be the best way to pass sensitive data to EC2 instance (on boot or otherwise) that only root can access? 

I want to aggregate CoreOS logs to Papertrail service, which basically provides a syslog endpoint for aggregate logging. Common advice for this setup seems to be starting a service that does something like this: 

How can I tell (in ) if I'm running in interactive mode, or, say, executing a command over ssh. I want to avoid printing of ANSI escape sequences in if it's the latter. 

It's single availability zone no backups not in a security group that's reachable from the outside world 

I want to get an automated process for AMI creation going, and one piece remaining is automatically cleaning up the instance after image creation. The instance is booted with a user-data script that does the necessary setup, then kicks off image creation from self using AWS CLI. Then it shuts down. I could go with option and wait there until the image is ready, then terminate, but the docs state that "file system integrity on the created image can't be guaranteed", so I want to avoid using it. What's the best way to kill the instance from itself after image creation is completed? 

I'm not sure exactly what your question is so feel free to update it if that is not the answer you're looking for. If you're asking whether logging into a compromised server with a public key will the private key on the client to be compromised then the answer is no: the private key always stay on your machine and is only used to sign the authentication token you sent to the server. That doesn't means it has no consequences, though: once you're logged on into a compromised machine, everything you do during this session is potentially compromised as well. for instance, if you use a private key stored on that server to access another server, that key will potentially be compromised . In fact, if you perform any other kind of login from within the SSH session, then the credentials used are potentially compromised. 

RFC 2821's "VRFY" SMTP command is supposed to do that but, due to spammers abusing this command for validating mail addresses lists it's often disabled on the servers and SMTP gateways. 

I assume you want to deny the execute right not only to these folders but to the whole tree starting from them (otherwise, there is no point in doing what you want to do). The - obvious - consequence will be that any executable located in these will fail to run. Unfortunately, this will include a rather large number of legitimate applications. %localappdata% and %appdata% are the most problematic ones: Dropbox, Chrome, SkyDrive, for instance, will fail to work. Most automatic uploaders and many installers will also fail to work. %UserProfile% is even worse since it includes both %localappdata% and %appdata% as well as a number of other folders. In short: if you prevent applications from running from these folders, your system might get pretty much unusable. %temp% is different. While you might occasionally have legitimate programs running from there, it's quite infrequent and usually easy to work around. Unfortunately, %temp% expands to different folders depending on the user context you're expanding it from: it might end up in %localappdata%\temp (in the context of a user) or %SystemRoot%\temp (in the context of the system) so you will have to secure each location individually. %temp% is also a good candidate because that's where most mail programs will save attachments before opening them: that will help in many cases of mail-based maleware. One good trick is to change the premissions on the C:\Users\Default\AppData\Local\temp and C:\Users\DefaultAppPool\AppData\Local\Temp folders when you setup the system (and, of course, %SystemRoot%\temp). Windows will copy these folders when it creates new profiles and so new users will have a secured environment. You might want to add %UserProfile%\Downloads to your list: this is where most browsers will same the user's downloaded files and denying execution from there will also increase security. 

I am trying to use AWS autoscaling lifecycle hooks in a template that encapsulates the following things: 

When an ASG is launched, the queue ends up with two test notifications from the creation of lifecycle hooks, but no notifications for instance launch. And here is the race condition. object references (and hence depends on it). This dictates the order in which CloudFormation creates resources (the group is created first). The problem is that the group starts launching instances before the hook creation is complete (instance launch is not a part of the template, so it starts executing in parallel). By the time the hook is created, there are no more events to post as the instances were already created. Is there any way to work around it and catch launch events at stack launch time? 

with associated scale up/down policies, launch configuration, IAM role, etc. 2 of for EC2 launching/terminating events. (in a simplified example) where lifecycle notifications get posted. role for the autoscaling group to post notifications to the SQS queue. 

Is there a way to let anonymous access to a certain S3 bucket only from my EC2 instances (all of them) within a single AWS account? 

Within plain EC2 environment, managing access to other AWS resources is fairly straightforward with IAM roles and credentials (automatically fetched from instance metadata). Even easier with CloudFormation, where you can create roles on the fly when you assign a particular application role to an instance. If I wanted to migrate to Docker and have a kind of M-to-N deployment, where I have M machines, and N applications running on it, how should I go about restricting access to per-application AWS resources? Instance metadata is accessible by anyone on the host, so I'd have every application being able to see/modify data of every other application in the same deployment environment. What are the best practices for supplying security credentials to application containers running in such environment? 

The guys responsible for the DB performance tuning asked me to change the queue depth on the two HP p410(i) controllers of my DL380 G6 server. They want me to switch it from automatic to 64. Unfortunately, I can't find a way to do it. I've tried to bring up the array config CLI and type: 

No. You can't really disallow access to these files. What you can do, however, is hide and restrict the drive so they won't be able to navigate is through explorer. It's not bulletproof (because it relies on application supporting the limitation) though. 

Have you considered using VMs instead of trying to stuff everything on the hardware ? That would make your system easier to backup and manage. You might need a couple of additional windows server licenses but not only will you get around the issue of having multiple services running on the same OS but you'll also gain the ability spread the load more efficiently between physical machines. It will also make DR much easier assuming you have a good backup policy. 

The problem can be solved by synchronizing the zoom factors of both screens but, of course, that is a major inconvenience when the user has several screens with very different DPIs (typically, a laptop or tablet with a QHD or UHD screen and a main display with a 1080p one). In such a situation, the user has more or less to stop using one of the screens. 

Assuming you are using a Windows 2008R2 system and a 2008R2 AD, you can use a managed service account for this. This technet blog entry has a pretty good summary of how to use managed service accounts but, here are the basic principles: A managed service account is an AD account that is strongly tied to a computer and that has an automatically managed password. You don't create the password and nobody needs to know about it but since it's an AD account, you can use it for network ACLs which makes it perfect for your scenario. Using a MSA for a scheduled task will, however, require you to use the command-line to create your tasks (see this and this thread for more details).