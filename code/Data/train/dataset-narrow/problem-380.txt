Also one small note for the multiple schemas approach - put different applications data into separate tablespaces. This will add just a few minutes while creating users but may save a lot of maintenance time later. Trust me. :) 

Prepare transportable tablespaces metadata set. Copy expdp file to location accessible by impdp from destination database. datafiles to new location. Import TT metadata set. 

Then you would not get wasted space in tablespace and also even in case your data grows fast and catches up with autoextend operation - there will be only small delay in writes because 10GB will not take a long time to format. And Oracle will autoextend tablespace proactively. It will not wait for the last blocks to fill up. 

This is one of the reasons why good Oracle DBA can save quite a lot of money with licenses. First instead of using many core CPUs look into Intel "Oracle EE" line of server CPUs. Next - put ton of RAM which is way cheaper nowadays than Oracle licenses. Then you can use Database Smart Flash Cache. MLC SSDs for servers are not that expensive. But of course there is huge price gap between Standard Edition One and Enterprise Edition if we are talking about single dual CPU server. 

First insert/select goes from dba_objects then it is faster to insert/select from filler table itself. Now you just have to check datafile using RMAN: 

I agree, this appears to be a deficiency. As a workaround, you could perform the XML schema update on the Publisher and then post the XML schema update to Subscribers using sp_addscriptexec. 

You may want to look into Publishing Stored Procedure Execution in Transactional Replication which can provide significantly better performance for large batch operations since only the procedure execution is replicated, bypassing the operation being sent to Subscribers as a large, multi-step transaction. If you have anymore questions please let me know. I hope this helps. 

Yes, your publication database is your source database. There are some considerations when backing up the publication database, as well as other replicated databases, such as the distribution and subscription databases. This is covered in Strategies for Backing Up and Restoring Snapshot and Transactional Replication. 

The first link you provided is official documentation for SQL Server 2012. $URL$ The maximum number of articles supported in Merge Replication is 256. This means that Microsoft will ensure up to 256 articles will work as expected. I'm able to exceed 256 articles as well but I wouldn't do it in production. You may want to consider splitting this up into multiple publications for production purposes. 

I believe your DBA is referring to Publishing Stored Procedure Execution in Transactional Replication. With Transactional Replication you have the option to published the definition of the stored procedure or the execution of the stored procedure. Publishing stored procedure execution can provide significantly better performance for large batch operations since only the procedure execution is replicated, bypassing the operation being sent to Subscribers as a large, multi-step transaction. If you have anymore questions please let me know. I hope this helps. 

INSERT ... SELECT syntax reference. Of course id field has to have unique key and your csv files has to fit in memory. MEMORY engine tables are very fast so you should have just tiny overhead to load data in two steps. As one of drawbacks they have just table level locks so their use is limited. But that does not impact this data load scenario. 

It calculates dates of next Sunday and 1st of next month and then returns the one which will be sooner. Just 'Sunday' in next_day is NLS dependent. That should give your required interval in call to DBMS_REFRESH.MAKE procedure: 

Not exactly an answer to your question but amount of changes generated by the session can be found using such query. 

Also there are a bit different instructions in $URL$ but I believe things a bit changed now when flash MOS page is retired. 

Usual rules apply. Code changes should not be required but something will break if code is more complex. Also there will be some SQL plans regressions. Official documentation about deprecated features is there: Deprecated and Desupported Features for Oracle Database 12c Also you can check Metalink note 567171.1 about parameter. This parameter is used to enable or disable certain bugfixes which affect query behaviour. In 11.2.0.4 the view contains 854 rows which means that there are so many potential quirks while upgrading to the newer version of Oracle Database. I am sure that 12c contains much more rows. 

Add the Merge Agent parameters -OutputVerboseLevel 4 -Output C:\TEMP\mergeagent.log to the Run Agent Merge Agent job step to find out where it is timing out. 

Replication across non-trusted domains or workgroups can be done using Windows Authentication by configuring pass-through authentication. Create a local Windows account on both the Publisher and Subscriber that has the same username and password. Use this account for the replication agent process account and have the connections to the publisher, distributor, and/or subscriber impersonate this account. Ensure the account has the permissions required in Replication Agent Security Model. This approach is covered in the section Use Windows Authentication to Set Up Replication Between Two Computers Running SQL Server in Non-Trusted Domains in HOW TO: Replicate Between Computers Running SQL Server in Non-Trusted Domains or Across the Internet. 

You can use sp_showlineage and sp_showcolv to inspect the lineage and columns that have been changed. The sp_showcolv results contain a column, colidx, which is the index of the column(s) that were changed for a given version of the row. If you need to take a more proactive approach, you can setup an auditing scheme. I've provided an example of how to setup an auditing scheme on my blog: $URL$ 

The Query Timeout message you are seeing is your Distribution Agent timing out, not your Log Reader Agent. You need to increase your Distribution Agent profile Query Timeout parameter value to alleviate the error. Please see the section To view and edit the parameters associated with a profile in Work with Replication Agent Profiles to edit the QueryTimeout parameter value for your Distribution Agent. Note that if you run your Distribution Agent continuously that you will need to restart your Distribution Agent for the profile change to take affect. 

If that looks too simple - keep in mind that you will have to drop and later recreate foreign keys if you have any. :) 

Now you can unconfigure and remove old machines from old cluster. Of course Oracle version on new machines has to be the same as on old machines. Or it is possible to do upgrade right away on new machines. You should test procedure of course. There are quite a few possible problems on the way. The idea is that Oracle DB does not store anything in "cluster". All the data is in datafiles, controlfiles, redo logs and spfile. Which is stored on ASM can can be mounted on another server. 

With any more complex data it is almost impossible to restore data in MySQL Cluster in one step. Usually one needs two steps: 

Maybe db_cache_size, shared_pool_size, sga_target or other memory related parameters are set to non zero? Remember that when using AMM those parameters specify minimum memory allocated for particular pool. So if sga_target is 6GB you will not be allowed to set memory_target to 4GB. Also sum of internal variables __sga_target, __db_cache_size, etc. may be more than your specified value of 4GB. If you see those symptoms you can cleanup pfile bounce Oracle with pfile and recreate spfile. In the same step you can also set to zero. 

Everybody here are missing one point - SQLs in question are retrieving 1000 rows. And one cannot retrieve 1000 rows fast unless most of the data is cached. If data is not cached one has to do 1000 random reads sequentially to retrieve the data. Good disk based storage with fast disks will give you up to ~200 reads per second. Common disks even in RAID I doubt that manage even 100. That means 10+ seconds to get results even with the best indexes. So in the longer run such data model and queries won't work. Now the queries run fast when you hit cached data. 

It looks like you have some orphaned replication bits and/or orphaned replication agent jobs. Note that Distribution.dbo.sp_MSremove_published_jobs will not remove your Distribution Agent jobs. You will have to manually delete the orphaned job(s). Locate the job(s) under SQL Server Agent -> Jobs. The Distribution Agent job names will have the format Publisher-PublicationDatabase-Publication-Subscriber-integer. Right-click the job -> Delete. I hope this helps. 

Just keep in mind that the indexes that you place on the subscriber(s) will need to scripted out and included in a post-snapshot script in case you ever reinitialize the subscriber(s). 

Please check for orphaned rows for deleted subscriptions in sysmergesubscriptions and delete them. For example, the subscriber that this is failing on may have duplicate entries in sysmergesubscriptions, one being orphaned from a previously deleted subscription. This has been the cause of this error and solution for clients of mine in the past, ymmv. I hope this helps. 

You will need to add both the principal server and the mirror server to Replication Monitor. This is covered in Database Mirroring and Replication (SQL Server): 

You will need to run a validation to determine how out of sync your publisher and subscriber are. Then, use the tablediff utility or Red Gate SQL Data Compare to generate the script(s) necessary to get the subscriber back in sync. Then apply the script(s) at subscriber to bring the data back into convergence. 

Replication across non-trusted domains or workgroups can be done using Push Subscriptions in conjunction with SQL Authentication for the replication agent process accounts. Alternatively, you can also use Windows Authentication by configuring pass-through authentication. To use pass-through authentication, create a local Windows account on both the Publisher and Subscriber that has the same username and password. Use this account for the replication agent process account and have the connections to the publisher, distributor, and/or subscriber impersonate this account. Ensure the account has the permissions required in Replication Agent Security Model. This approach is covered in the section Use Windows Authentication to Set Up Replication Between Two Computers Running SQL Server in Non-Trusted Domains in HOW TO: Replicate Between Computers Running SQL Server in Non-Trusted Domains or Across the Internet. If you have anymore questions, please let me know. I hope this helps. 

I was using the following steps to perform backup and restore. In the first step I generate dump script to make schemes structures backup. 

30min for 1TB is quite normal. That all writing stops is also normal if your tablespace completely run out of space. When writes has nowhere to go they have to wait for the RESIZE operation to complete. If one extends datafile while there is still space in it database I/O does not stop. Just why you extended for such huge amount? Now those +1TB will add to your RMAN backup. Of course they will compress well but still not 100% and RMAN will need time to read all those blocks. I would setup autoextend on those tablespaces: 

First step has to be run on all nodes of the cluster. Second step though - just on one node which will rebuild all the indexes. For foreign keys you can try to split first step into two: 

By "default" you can only select data from tables and views in your schema. To create different objects you need appropriate privileges granted. 

If it really shows here you have two options. You can simply ignore it. Once the block will be assigned to some object it will be reformated and corruption will go away. If you want to fix corrupted blocks then you will have to create the object which would occupy your corrupted blocks. Let's say your corrupted block exists in tablespace and datafile . First you have to make datafiles of tablespace not autoextendable. That is not to inflate datafile size. And then you'll create filler table. 

It's hard to say exactly but it looks like something went wrong during the reinitialization process. Transactions are flowing from Publisher to Distributor but the Subscriber is requiring a new snapshot. I'd recommend generating a new snapshot and then it will be reapplied by the Distribution Agent. At that point you should be back in sync. 

The max text repl size options exists to limit how much LOB data can be added to a replicated column. If you turn off the LOB size limit and a user inserts a large amount of LOB data, latency will increase and replication will come to a crawl. The max text repl size option can be used to prevent this. 

It really depends on how old the snapshot is. After the snapshot is applied, the agent will need to also send all of the changes that have occurred since the snapshot was taken. Depending on how many changes have occurred, this may or may not affect other existing subscribers. I tend to take a new snapshot if possible before reinitializing a subscriber although sometimes that is not an option. I wouldn't worry too much about it impacting other subscribers. 

SQL Server asks for a root snapshot folder path when you Configured Publishing and Distribution. After configuring publishing and distribution, all subsequent publications created will use that snapshot folder but will have their own sub-folder within that folder. If you wish to have your snapshot files for a particular publication be put into a different root snapshot folder, specify this by passing in the path to the @alt_snapshot_folder parameter of sp_addpublication or sp_addmergepublication when you create the publication. Alternatively, this can be done from the Publication Properties dialog after you have created your publication and before generating a snapshot on the Snapshot page, Location of snapshot files section, Put files in the following folder. 

Ultimate source for such answers is Oracle Database Licensing Information. Sadly to downgrade from Enterprise to Standard Edition you have to export all the data install Oracle Standard Edition and import data (Doc ID 139642.1). From your list "Automatic SQL Tuning Advisor" is Enterprise Edition only feature. As a rule of thumb - features which needs AWR are Enterprise Edition only plus you need to buy . 

Even such query counts as scan if id field has btree index. In your case we almost all DELETE queries are running in loop with 'LIMIT 1000' or similar value. 

Certifications are just cherry on the top. But you have to bake the cake first. They mean nothing if you do not have any real life experience. But when you have - they give you a bit of an advantage in your job interview. Also do not concentrate just on Oracle. Most DBAs are qualified in several databases. Start with MySQL, PostgreSQL. They are simplier to start but still the core principles are the same. Also it should be much easier to find some real life practice with those databases. Then continue climbing the ladder. Good luck! 

It seems that for solving your problem you chose the wrong tool. MySQL Cluster is good when you do mostly key based lookups from multiple threads from memory based tables. Disk based tables may be useful when you have rarely read data. Or your work dataset is small portion of the table which should fit into memory cache whose size is defined by DiskPageBufferMemory config variable. If your queries need many range or full scans - MySQL Cluster is slow even on physical machines. That is because such queries need a lot of data exchanges between data nodes. Try pinging between your data nodes. And for range scan data nodes may need to exchange hundreds and thousands of such messages. Also MySQL previously stated that for data nodes your should use physical machines and have good interconnect for data node traffic. I doubt that this recommendation is no more valid nowadays. And I think you should try cleaning up your config. For testing most of those things hardly changes anything and some setting may be slowing down things. Try such simplified section: 

I think backup and restore would be the fastest way to get a new copy of the database from PRODUCTION to TEST. This can be automated easily. I don't think replication is the best feature to use here in my opinion. Unless you only plan on replicating a subset of the data. In that case, Snapshot Replication may be a good fit. 

The MSmerge_history table does have a column named and it's of the type , it just isn't documented. To answer your question, based on my tests it appears to be local server time. The column has a default value of . 

With Merge there is an option to upload pending changes before reinitialization but since your subscriptions have been removed you won't have this option. The way to handle this is to manually sync the databases using tablediff utility or Red Gate's SQL Data Compare. Then recreate the publication from the manually merged database - then subscriptions. 

Yes, this still holds true. It is covered in Upgrade Replicated Databases in the SQL Server 2014 documentation. 

If you change a user's permissions to a particular table BEFORE a Subscriber has been initialized or reinitialized, if the article property Copy permissions is set to true, the permissions will be copied to the Subscriber when the snapshot is applied. If you change a user's permissions to a particular table AFTER a Subscriber has been initialized, the permissions will not be replicated on the fly, unless you generate a new snapshot and mark the Subscriber for reinitialization. Note that the article property Copy permissions must be set to true. What is typically done is if you need to change a user's permissions to a particular table AFTER a Subscriber has been initialized, and you need the permissions to be present at the Subscriber without reinitializing the Subscriber, the script to grant permissions are posted to the Subscriber using sp_addscriptexec. I have a post detailing sp_addscriptexec located here at Executing scripts with sp_addscriptexec.