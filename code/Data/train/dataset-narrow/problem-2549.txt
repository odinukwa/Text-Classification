The third matrix (C) is the one that transforms from world space, into camera space. This matrix is a translation matrix with a translation of (0, 0, 10), because I want the camera to be located behind the object, so the object must be positioned 10 units into the z axis. 

One way I deal with such stuff is I put a member in the base class that describes the type of the derived object. For instance, GameObject can have a member called type which is an enum or w/e. Then, you could do 

Thanks to everyone for their responses, I have finally figured out the problem. Turns out I had to interpolate the positions, as they are transformed in view space, instead of the light direction. Apparently light direction does not linearly interpolate properly. Here are my modified shaders: 

Why does adding these two threads slow down my application? Why doesn't it work when compiling for Release or with optimizations? Can I speed up the application with threads? If so, how? 

And finally, the fourth matrix is the projection matrix (P). Bearing in mind that the eye is at the origin of the world space and the projection plane is defined by z = 1, the projection matrix is: 

IANAL, this is not legal advice, gamedev.SE is not a good site for legal Q&A. You may have noticed that most promotional material either avoids mentioning competitors, or avoids mentioning them by name. Example (Colgate): 

As for simplification via merging individuals into groups, the nearest thing I can think of is Stochastic Simplification, used for foliage and hair/fur mainly - it was used by Pixar in Cars for the bushes and Ratatouille for the fur. Check out these slides which get the general ideas across: 

Spritesheets aren't limited to animations, that's just one way to use them. An animation is just a way to display different frames at different times. By manually setting the frame of a sprite, you can display a specific part of the spritesheet. 

Of course, you can have SDKs for things that aren't engines, which adds a bit of confusion. The DirectX SDK for example doesn't include a game engine, as DirectX is a graphics/multimedia framework. 

In practice this tends to end up in the shooter's favour, as most FPS games have fast projectile weapons that require instant feedback for shooters, but keep in mind that anything you decide will be a compromise. Also, contrary to many gamers' complaints, it is the sum of latencies between shooter and target that determines the severity of the compromise, and in one-on-one encounters there is no inherent advantage to either player - both players experience the same degraded gameplay, irrespective of their individual latencies to the server. Here are some general strategies and examples (the Gaffer on games article expands on some of these concepts): Server authority (listed in the article as Client/Server): this is where the server is the ultimate authoritative frame of reference for everything, including whether one player has hit another. This was common in the early days of internet FPS games, when titles like Quake were most popular. This disadvantaged all players for the sake of the game, and disadvantaged high-latency players more, as no action begins until the server hears about it. For twitch-shooters such as Quake this was acceptable, as one needed a low latency to get the best experience anyway. The downside was that even modest amounts of latency - 100-150ms - was noticeable for human players, and they had to compensate by aiming ahead and so on. Client-side Prediction: often goes hand-in-hand with latency compensation, this is where the client is the authoritative frame of reference (either the shooter or the target, as I mentioned), and the server maintains a buffer of previous state in order to validate the client's prediction. Games using this architecture became common in the early 2000's, especially for "realistic" shooters using fast projectile weapons such as Counter Strike. Latency caps: to mitigate the harmful effects introduced with client-side prediction, many games and server admins chose to disallow high-latency players from playing. This ensures that the experience is consistent for the low-latency players. Although this seems harsh, with the spread of broadband internet and matchmaking services, the pain can be reduced. Latency-capped client-side prediction: this is the same as client-side prediction, but where the compensation is limited up to a maximum latency. Any players above this threshold will only have their latency compensated up to that threshold, no more. This threshold should suit the nature of the game, but 200ms sounds like a good compromise. This means that for players with latencies below the 200ms, it's identical to normal client-side prediction, except it feels like no one has a latency any greater than 200ms. For players with latencies above the threshold however, it feels like a server authority game where everyone else has a latency of 0 but they have a latency of X - 200ms. I'm not aware of any games that do this but I'd be surprised if this has never been attempted before. 

I worked around the problem by adding the triangles which vertex/vertices go over the split plane position to the other child too so they exist in both children. Of course, this affects the subsequent split positions slightly but it's not a serious problem. One solution would be to clip the triangles which go over the split position. 

I'm applying phong shading onto a single giant triangle, and I'd like the light's coordinates to coincide with the camera's coordinates in 3D space. In order to do this, whenever I update the camera's coordinates, I also update the light's coordinates. However, diffuse and specular lighting don't "focus" exactly at the camera position when I bring the camera close to the triangle, instead they do it a few units too far. The triangle is .5 units below the XZ plane and parallel to it. Here is a picture demonstrating this effect. 

After I get the result, I divide x and y coordinates by w to get the actual screen coordinates. Apparenly, I'm doing something wrong or missing something completely here, because it's not rendering properly. Here's a picture of what is supposed to be the bottom side of the Stanford Dragon: 

When doing complex projects like games, you often can't make all the features you want, because you're running out of time/money or because they didn't turn out to be as good as you expected. This is known as feature creep. But there's a flip side to this; you will also find features that you didn't think you needed, but as the project takes shape their need becomes apparent. This is why people build prototypes - so they can learn what works and what doesn't, so they can cut features that don't work, but also so they can find new features that would be awesome. If you're not doing either of those things you're not learning, you're doing it wrong. This is basically the sentiment expressed in a talk called Advanced Prototyping, by Chris Hecker and Chaim Gingold, which includes many examples from their work on Spore, where they were often surprised by what worked and what didn't, going as far as redesigning whole systems based on prototype feedback. Another example is the design process for Left 4 Dead; at first the game only had basic zombies, but from playtesting with experienced players, the designers found that the players stuck together effectively and the game was too easy, so they added special zombies to break the team apart and keep the game exciting. Of course, this doesn't mean you should build your game without any prior design at all. You should make sure that the core of your game is fun before proceeding, because that's often the hardest part of making a game. 

I'm implementing a software renderer with this rasterization method, however, I was wondering if there is a possibility to improve it, or if there exists an alternative technique that is much faster. I'm specifically interested in rendering small triangles, like the ones from this 100k poly dragon: 

EDIT In.LightDir is computed for each pixel. It is first computed for each vertex in the vertex shader Out.LightDir = normalize(mul(lightPos - In.Pos, Matrix3));, then it is interpolated across the whole triangle. lightPos is just the camera's position (which coincides with the light): 

I'm trying to get an object from object space, into projected space using these intermediate matrices: The first matrix (I) is the one that transforms from object space into inertial space, but since my object is not rotated or translated in any way inside the object space, this matrix is the 4x4 identity matrix. The second matrix (W) is the one that transforms from inertial space into world space, which is just a scale transform matrix of factor a = 14.1 on all coordinates, since the inertial space origin coincides with the world space origin. 

Using these two properties, work your way up from the leaves to the root, labelling each node with the number of children, in lexicographical order. For example, your Root in Example 1 will be labelled (0, 0, (0, 1)) - it has three children, the first/second have 0 children, and the third has 2 children which have 0 and 1 children respectively. Finally you just compare the root labels to see if the trees are the same. I haven't done this kind of subject and I've only read this paper a few minutes ago so I can't vouch for its correctness; hope it helps anyway. 

It's not too difficult, provided that you have a sudoku solver. Making sudoku solvers is a hard / interesting problem, so it's best to save it for a different question. Or you can just read this and see how you go. 

That is, as long as there is at least one loop, it's not a perfect maze. It doesn't have to have more than one solution to be non-perfect, but since this is GD.SE I assume multiple solutions is a lot more relevant. So how does one make a maze with multiple solutions? You'd need to remove walls that are along the solution path. This can be done deliberately, or just randomly remove walls but check how many solutions there are using a maze solver. This still might not create interesting mazes though; a maze might have multiple solutions but if you're only adding a tiny loop in the solution, gameplay-wise it won't feel like multiple solutions - maybe a shortcut at best. On the maze classification page you've linked, there is this interesting section: 

As you can see, the method I'm using is not perfect either, as it leaves small gaps from time to time (at least I think that's what's happening). I don't mind using assembly optimizations. Pseudocode or actual code (C/C++ or similar) is appreciated. Thanks in advance. 

I'm making a software renderer which does per-polygon rasterization using a floating point digital differential analyzer algorithm. My idea was to create two threads for rasterization and have them work like so: one thread draws each even scanline in a polygon and the other thread draws each odd scanline, and they both start working at the same time, but the main application waits for both of them to finish and then pauses them before continuing with other computations. As this is the first time I'm making a threaded application, I'm not sure if the following method for thread synchronization is correct: First of all, I use two global variables to control the two threads, if a global variable is set to 1, that means the thread can start working, otherwise it must not work. This is checked by the thread running an infinite loop and if it detects that the global variable has changed its value, it does its job and then sets the variable back to 0 again. The main program also uses an empty while to check when both variables become 0 after setting them to 1. Second, each thread is assigned a global structure which contains information about the triangle that is about to be rasterized. The structures are filled in by the main program before setting the global variables to 1. My dilemma is that, while this process works under some conditions, it slows down the program considerably, and also it fails to run properly when compiled for Release in Visual Studio, or when compiled with any sort of -O optimization with gcc (i.e. nothing on screen, even SEGFAULTs). The program isn't much faster by default without threads, which you can see for yourself by commenting out the #define THREADS directive, but if I apply optimizations, it becomes much faster (especially with gcc -Ofast -march=native). N.B. It might not compile with gcc because of fscanf_s calls, but you can replace those with the usual fscanf, if you wish to use gcc. Because there is a lot of code, too much for here or pastebin, I created a git repository where you can view it. My questions are: 

This is because when the projection clashes with the camera, it disrupts the visual coherence of the scene, making the scene and the subjects seem out of place. I won't say you definitely can't do it, it may even be a distinctive style, but according to common practice it would be awkward. 

Well according to the rules of the game, the AI is doing the right thing - moving towards the player every time! It's only due to a quirky situation that the player can "trap" the AI in a loop: 

I'm not aware of this ever being used, and I suspect it's because it has very limited benefit for greatly increased complexity and bandwidth use. It might make an interesting experiment, but it seems impractical. Consider that to support such a scheme, and assuming that most traffic is generated from player actions, you would need to double the downstream traffic and multiply the upstream traffic for all clients, not to mention opening a large amount of new connections. Double downstream because now you are receiving player commands from the peers as well as the server. Multiple upstream because clients need to send their commands to all peers as well as the server. But perhaps the biggest cost is that you run into tricky situations when you now have three points of reference. With a traditional client-server architecture, there are two: the client and the server (technically the other clients have their own points of reference but the server multiplexes them into a consistent, single one using techniques under the latency compensation umbrella). Now you have three (or more): client, server, and peer. Consider this scenario: 

Texture coordinates are expressed as floating points values between the limits 0 and 1. What you are doing is sending 5 and 32 which get clamped to 1, resulting in the image becoming transparent between 0 and 1, which encompasses the whole thing. What you need to do is divide, either in the shader or outside (preferably outside, on the cpu) by the actual width and height of the texture. For example, say you have a 32x64 texture, and you want it to be transparent in the rect x1=0, y1=0, x2=5, y2=10. You have to divide x1 and x2 by 32 and y1 and y2 by 64, and then do the comparisons. Something like this: 

I'm making a collage of lots 16x16 renders on a 512x512 texture, of the same scene, from various viewing positions and angles, preferably lots of times per second. I've profiled my program (which contained a glDrawElements call per mesh), and the multiple glDrawElements calls seemed to slow it down a lot. In order to optimize, I've resorted to instanced rendering. However, the main problem I'm having is changing the viewport between, say, every 3 instance renderings, or so. I was thinking of adding a fourth matrix, which would scale the perspectively-projected vertices of a would-be 16x16 picture, translate them so that the little images don't overlap, and based on the little picture's position and size (16x16 pixels) on screen, use the `discard' command in the pixel shader. How do I scale the projected vertices from the current large viewport into a 16x16 smaller version of it and translate them inside the former at a certain position? Or, more clearly, how do I change the viewport during a glDrawElementsInstanced call, every N instances? EDIT Here's a visualization of what I'm trying to achieve: Keep in mind, I can't change the viewport as I want to do this during a call to glDrawEleemntsInstanced, every 3 instances, or so. How do I compute a matrix or what do I have to do to get the post-projection vertices scaled and translated so that I'll have the full image scaled in a 16x16 portion of the screen? 

But since you deal with first, you "listen to" it (go left), and by the time you get to and attempt to move left, you get stopped dead. Notice that if you had done this in the reverse order ( then ) it would have worked, which explains why this bug appears only if you're moving up and not down. Solution I think there may be many ways to solve this, but one simple method is this: 

Now compare this with a typical Perlin noise generator, and you can see how it's much better at resembling mountain ranges. This just looks like blobs to me: 

Your easiest option is to just use an existing library or game engine that supports the Tiled map format. Then you don't have to worry about GIDs or which sprites to use; you simply load the Tiled map and the associated spritesheets, and the Tiled API will let you render the map on a per-layer basis - no need to worry about individual tiles or sprites. This is because the GID is an internal ID used by the Tiled map format; it's an ID that uniquely identifies a type of tile in a map, given that the map could contain multiple layers and spritesheets. The map itself is expressed as a bunch of layers, and each layer contains a big list of GIDs. Elsewhere in the map file, there will be a section describing the tilesets used, and each of them will occupy a range of GIDs. That is, GIDs 1-128 (for example) might use the first tileset, GIDs 129-512 might use the second and so on. You can see how GIDs can be fragile: if you change the order of the tilesets, the ranges of the GIDs will change. How to tell which tiles are which This is all fine if all you care about is rendering. For games this is not nearly enough. We might want to know whether tiles are: