Initialize $N$ in time within $O(n^3)$ (or better via faster matrix multiplications algorithms). The values of $N$ form a spanning tree of at most $n$ edges and no shortest path contains twice the same edge. Consider an update decreasing the weight of the edge $\{i,k\}$. For each entry $j$ of the row $i$ in $N$, 

The best known deterministic algorithm in linear space runs in time within $O(n\lg\lg n)$ and was presented by Han in 2004. The best known randomized algorithm in linear space runs in time within $O(n\sqrt{\lg\lg n})$ and was presented by Han and Thorup in 2012. 

In the algebraic decision tree, the result is clear: Elmasry and Belal (Verification Of Minimum Redundancy Prefix Codes)'s lower bound shows that the worst case complexity of computing an optimal prefix free code over all possible sequences of $\sigma$ frequencies is $\Theta(\sigma\lg\sigma)$. In the $\Omega(\lg \sigma)$-word RAM model, combining Van Leeuwen's reduction to sorting (On The Construction Of Huffman Trees) with any $O(\sigma\lg\lg\sigma)$ sorting algorithm in the RAM model yields an upper bound of $O(\sigma(1+\lg\lg\sigma))$, which many people seem to assume to be optimal, probably by analogy with the situation in the algebraic decision tree model. But the only lower bound available is $\Omega(\sigma)$, the cost of reading the frequencies, leaving a gap between the two bounds. What is the best upper bound known on the asymptotic complexity of computing an optimal prefix free code in the word RAM model? Is it $\Theta(\sigma\lg\lg\sigma)$ or $\Theta(\sigma)$? Or somewhere in between? 

In any case, the complexity is clearly within $O(n)$ where $n$ is the number of nodes being colored (for both problems). 

In some of my publications (e.g. $URL$ $URL$ I used this trick to sort faster and to generate a compressed data structure for permutation. It seems that this trick was introduced before, just in the context of sorting faster, but neither me nor my student have been able to find back the reference? 

I join the bibtex references below. @TechReport{1997-TR-MinimalMergesort-Takaoka, author = {Tadao Takaoka}, title = {Minimal Mergesort}, institution = {University of Canterbury}, year = 1997, note = {$URL$ last accessed [2016-08-23 Tue]}, abstract = {We present a new adaptive sorting algorithm, called minimal merge sort, which merges the ascending runs in the input list from shorter to longer, that is, merging the shortest two lists each time. We show that this algorithm is optimal with respect to the new measure of presortedness, called entropy.} } @InProceedings{2009-Chapter-PartialSolutionAndEntropy-Takaoka, author="Takaoka, Tadao", editor="Kr{\'a}lovi{\v{c}}, Rastislav and Niwi{\'{n}}ski, Damian", title="Partial Solution and Entropy", bookTitle="Mathematical Foundations of Computer Science 2009: 34th International Symposium, MFCS 2009, Novy Smokovec, High Tatras, Slovakia, August 24-28, 2009. Proceedings", year="2009", publisher="Springer Berlin Heidelberg", address="Berlin, Heidelberg", pages="700--711", isbn="978-3-642-03816-7", doi="10.1007/978-3-642-03816-7_59", url="$URL$ abstract = {If the given problem instance is partially solved, we want to minimize our effort to solve the problem using that information. In this paper we introduce the measure of entropy H(S) for uncertainty in partially solved input data S(X) = (X 1, ..., X k ), where X is the entire data set, and each X i is already solved. We use the entropy measure to analyze three example problems, sorting, shortest paths and minimum spanning trees. For sorting X i is an ascending run, and for shortest paths, X i is an acyclic part in the given graph. For minimum spanning trees, X i is interpreted as a partially obtained minimum spanning tree for a subgraph. The entropy measure, H(S), is defined by regarding p i  = |X i |/|X| as a probability measure, that is, H(S)=−nΣki=1pilogpi, where n=Σki=1|Xi|. Then we show that we can sort the input data S(X) in O(H(S)) time, and solve the shortest path problem in O(m + H(S)) time where m is the number of edges of the graph. Finally we show that the minimum spanning tree is computed in O(m + H(S)) time.} } @article{2010-JIP-EntropyAsComputationalComplexity-TakaokaNakagawa, author = {Tadao Takaoka and Yuji Nakagawa}, title = {Entropy as Computational Complexity}, journal = jip, volume = {18}, pages = {227--241}, year = {2010}, url = {$URL$ doi = {10.2197/ipsjjip.18.227}, timestamp = {Wed, 14 Sep 2011 13:30:52 +0200}, biburl = {$URL$ bibsource = {dblp computer science bibliography, $URL$ abstract = {If the given problem instance is partially solved, we want to minimize our effort to solve the problem using that information. In this paper we introduce the measure of entropy, H (S), for uncertainty in partially solved input data S (X) = (X1, . . . , Xk), where X is the entire data set, and each Xi is already solved. We propose a generic algorithm that merges Xi's repeatedly, and finishes when k becomes 1. We use the entropy measure to analyze three example problems, sorting, shortest paths and minimum spanning trees. For sorting Xi is an ascending run, and for minimum spanning trees, Xi is interpreted as a partially obtained minimum spanning tree for a subgraph. For shortest paths, Xi is an acyclic part in the given graph. When k is small, the graph can be regarded as nearly acyclic. The entropy measure, H (S), is defined by regarding pi = ¦Xi¦/¦X¦ as a probability measure, that is, H (S) = -n (p1 log p1 + . . . + pk log pk), where n = ¦X1¦ + . . . + ¦Xk¦. We show that we can sort the input data S (X) in O (H (S)) time, and that we can complete the minimum cost spanning tree in O (m + H (S)) time, where m in the number of edges. Then we solve the shortest path problem in O (m + H (S)) time. Finally we define dual entropy on the partitioning process, whereby we give the time bounds on a generic quicksort and the shortest path problem for another kind of nearly acyclic graphs.} } 

Knuth mentions runs in his description of "natural merge sort" (page 160 of the tome 3 of the 3rd edition), but he analyzes its complexity only on average (which yields n/2 runs), not in function of the number ρ of runs in 1985, Mannila proved that Natural Mergesort takes time within O(n(1+lg(r+1))) to sort n elements forming r runs. in 1996, Tadao Takaoka ($URL$ introduced the notion of the entropy of the distribution of the sizes (n1,…,nρ) of the runs in an tech report 4 pages article titled "Minimal MergeSort", and proved a complexity of $n(1+2H(n1,…,nρ))$ comparisons in 2009, the technique was presented at the symposium Mathematical Fundation of Computer Science (along with results on sorting, shortest paths and minimum spanning trees). in 2010, it was published in the journal of Information Processing under the title "Entropy as Computational Complexity", with an added co-author, Yuji Nakagawa ($URL$ 

Compression is just an opportunistic way of encoding things, and when asking for "the best compression ratio that can be achievable by lossless data compression", you need to be more specific about the context of the compression: the compression ratio is the ratio between the size of the compression and the size of a "raw" encoding, but the size of the "raw" encoding depends of the hypothesis on your object (i.e. the size of its domain, or the "size of the bag from which it comes"). As a simplistic example, consider the task of encoding a positive integer $n>0$: 

Some useful reference and facts 1) Pareto Principle in general The Pareto Principle is named after Vilfredo Pareto, who introduced it to describe the distribution of wealth (dixit management.about.com, because "twenty percent of the people owned eighty percent of the wealth") in the early 1900s. It has been applied in popular media to describe the state of many other unrelated systems, as for example by Tim Ferriss stating that 20% of your clients generate 80% of your revenue (so that you can afford losing 80% of your clients if it frees time to increase your revenue by 20%). The formal definition from $URL$ is "A rule of thumb that states that 80% of outcomes can be attributed to 20% of the causes for a given event." 2) Pareto Principle in Computer Science The Pareto principle is no more than a particular property of power laws and exponential distributions. Since it has entered the popular culture it has been misused in various ways, I think that researchers prefer to refer define models using power laws and using their properties rather than referring to the Pareto principle and risk being misunderstood. The fact that the only example about computer science in the wikipedia page on the Pareto Principle is 2 lines long and about Microsoft supports this intuition. 3) Pareto Principle on Execution time Concerning the execution time, I found only a few references to the application you mentioned, i.e. "only a small portion (10%-20%) of the overall code is actually performance critical" on $URL$ and even there they do insist that it is a rule of thumb, not a real property (as one would expect). Dixit $URL$ "Although the Pareto principle is frequently mentioned in software optimization discussions, the way this principle affects the optimization process is usually left obscure." I hope it helps! 

Here is how I explained it to my mother, hopefully it will serve you :) There are problems for which it is easy to find a solution (P, but less call them "easily solvable"), problems for which it is easy to check if a given solution is correct (NP, but let's call them "easily checkable"), and problems which are neither easily solvable nor easily checkable. For simplicity assume that "Easy" is formally defined, and that each problem has a unique solution. Now, people have been able to prove (using mathematics) interesting relations between those two notions of "easily solvable" and "easily checkable", such that some problems are not easily solvable, and that some others are not easily checkable. A basic example of such result is that a problem which is easily solvable is also easily checkable: just find its solution and compare it to the solution given. Tantalizingly enough, for a lot of practical problems (such as deciding if there is a possible assignment of students to professors and classrooms, when there is very little margin) it is not known if there is an "easy" way to solve it, but it is known how to check easily if a solution is correct or not. People tried a lot and failed, then tried to prove that it was not possible and failed as well: they just don't know. Some think that all problems which are easily checkable are easily solvable (we just should think more about it), some think the contrary, that we should not waste our time trying to find easy solutions to these problems. What we found out is how to show links between problems (e.g. if you know how to go to school, you know how to go to the bakery which is just in front) and easily checkable problems which are linked to all other easily checkable problems (NP-complete, but let's call them "key problems") such that if someone, one day, shows that one of the key problems is easily solved, then all problems which are easily checkable are also easily solvable (i.e. P=NP). On the other hand, if someone show that one of the key problem cannot be easily solvable, then none of the others can be easily solvable either (i.e. P<>NP). So the question is tantalizing, and relatively important in practice (although some argue that we should rather focus on alternate definitions of "easy"), and people are investing quite a lot of money and time in the debate. 

identifying sequences of consecutive positions in increasing orders (aka runs) in linear time; then repeatedly merging the two shortest such sequences and adding the result of this merging to the list of sorted fragments. 

The Flood Fill algorithm is a particular case of the Depth First Seach algorithm, on regular mesh graphs: