Lamport's Part Time Parliament made a breakthrough in distributed computing, but the paper was so (purposely!) obfuscated that people couldn't understand it -- as far as I know, it took him around 10 years to get it published (past editors) in its obfuscated form. Eventually Lamport followed up with Paxos Made Simple, which had the following abstract: "The Paxos algorithm, when presented in plain English, is very simple." 

The LPN problem is indeed believed to be hard, but like most problems we believe are hard, the main reason for it is that many smart people have tried to find an efficient algorithm and failed. The best "evidence" for LPN's hardness comes from the high statistical query dimension of the parity problem. Statistical queries capture most known learning algorithms, except for gaussian elimination (which fails whenever noise is introduced), hashing, and techniques similar to these two. It is hard to design non statistical-query algorithms, and this is the main bottleneck. Other evidence of LPN's hardness is its relationship to other hard problems (like LWE, SVP as you've pointed out). For the SQ-hardness, here is the link to Kearns's ('98) paper. For progress on the upper bounds on this problem, there are several results: 

There have been efforts to parallelize most of the well-known classifiers, including boosting [a paper], SVM [a paper], and even decision trees [a paper]. Of course, by admitting parallelism, you sometimes lose out on other aspects, whether it be algorithm implementability, sample complexity, or other usual suspects. From the theory end, the question is harder because when you talk about learning you have to think about the target function. For example, we don't even know decision trees to be PAC-learnable, so if the target (as well as the method) is a decision tree, then we can't even learn it (yet) without introducing extra facets to the problem. Boosting gets around that by assuming a weak learning condition, SVM a margin, etc. I think those assumptions transfer to the parallel case to give you PAC learning. But as always, there's a big gap between the frontiers (and thereby concerns) of theory and of practice. For example, in practice, it matters whether the parallelism is over cores or clusters. One algorithm developed especially for practical use in large-data settings is VW, and it's starting to support parallelism. You might be interested in the papers in the NIPS 2010 workshop on practical parallel learning. 

I imagine there are lots of possible approaches to this problem (many of which I'm sure you considered) -- here are a few ideas/references. 

Are there any known languages that require $o(\log n)$ space but are also not regular? I'm looking for a problem "just below" undirected connectivity. 

Various encryption schemes would be considered broken if an adversary could have a non-negligible edge in predicting the first (or any) bit of an encrypted message. I am looking for a slightly stronger guarantee. In particular, does any cryptographic system or crypto assumption (that relies on keys, not a one time pad) guarantee that any linear combination of the encrypted bits cannot be predicted after an adversary sees only a polynomial number of messages (possibly of his choosing)? Note -- I am not at all an expert in cryptography, and this may be considered trivial. 

from comments, by request: In terms of running time, performance, something else? In terms of error rates, these two methods are hard to compare theoretically. AdaBoost's performance depends on the choice of a base learner and whether the weak learning assumption is satisfied in the particular instance. ANN will work for some problems/distributions but not others. I think this question would be better answered in a different forum. 

In a related not-quite answer to @jagadish's, after being defined, Costas arrays were quickly found for very small numbers, and were later found for sizes $p-1$, where $p$ is prime. However, it is open whether they exist for all $n$ and computer searches are making people believe that they do not exist for $n=32$. 

You can cover boosting. It's very clever, easy to implement, is widely used in practice, and doesn't require much prerequisite knowledge to understand. 

I'm guessing nobody really knows this statistic, especially in computer science where it's not clear what counts as a publication. Are all conference papers actually "publications," even in non-selective venues? Do all workshops count or only the selective ones? Do journal papers count in addition to their conference counterparts? What about letters papers? And where does arXiv stand? Even worse, there's no good tool for you to get an estimate. For example, in machine learning, two of the top conferences (NIPS and AISTATS) aren't always represented in DBLP. The best is probably to visit the websites of other postdocs in fields similar to your own and see what they've done. I'm also a postdoc, so I might not have have the correct perspective, but I also think that the number of papers doesn't really matter; what matters is the quality, consistency, and impact of your work. Finally, "where you stand" relative to your competition depends on what job you'd like. My feeling is that, in general, people who get research positions at good labs or faculty positions at research universities probably have at least a couple publications in the top venues of their field per postdoc (and/or end-of-Ph.D.) year. 

Angluin and Laird ('88) formalized learning with randomly corrupted data in the model "PAC with random classification noise" (or noisy PAC). This model is similar to PAC learning, except for the labels of the examples given to the learner are corrupted (flipped), independently at random, with probability $\eta < 1/2$. To help characterize what is learnable in the noisy PAC model, Kearns ('93) introduced the Statistical Query model (SQ) for learning. In this model, a learner can query a statistical oracle for properties of the target distribution, and he showed that any class that is SQ learnable is learnable in noisy PAC. Kearns also proved that parities on $n$ variables cannot be learned in time faster than $2^{n/c}$ for some constant $c$. Then Blum et al. ('00) separated noisy PAC from SQ by showing that parities on the first $(\log(n) \log\log(n))$ are polynomial-time learnable in the noisy PAC model but not in the SQ model. My question is this: 

Computer vision is not well defined as a theory problem, but machine learning does have a nice theoretical framework called PAC learning in which one can try to address your question. It is open whether learning is hard, i.e. whether there exists an class that is impossible to efficiently (PAC) learn. Some results that indicate learning is probably hard: 

This is called the explore/exploit trade-off in machine learning. This scenario is precisely captured by the multi-armed bandit problem with i.i.d. payoffs. Algorithms such as UCB and Exp3 will get you an expected payoff within $\tilde{O}(\sqrt{Kn})$ of the optimal, and you asymptotically cannot do better. 

I think the reference you are looking for is the paper "Broadcasting algorithms in radio networks with unknown topology" by Czumaj and Rytter. It seems this paper makes some improvements, but I think it depends on the specifics of the model. 

PAC comes in two flavors -- "information theoretic PAC" and "efficient PAC." The latter asks for computational efficiency whereas the former cares only about sample size. One usually understands which is referred to from context. Indeed, it is not known whether (efficient) PAC learning is NP-hard in general, but results on the cryptographic hardness of learning as well as on hardness of proper learning make it universally believed that learning is hard. 

SAT solvers don't "use" CNF -- they are (often) given CNF as inputs and do their best to solve the CNF they are given. As your question points out, representation is everything -- it is much easier to tell whether a DNF is satisfiable than a CNF of the same size. This leads to the question of why SAT solvers can't just turn their given CNF into a DNF and solve the resulting DNF, and trying this is a good exercise to go through in understanding issues of representation. 

From the theory end, you should understand how SVMs work (and why they do). For example, I like the scribe notes from this class for a quick explanation and something like this tutorial for a more in-depth one. You might also want to consider other machine learning techniques that are suited for binary classification on large data ie. boosting, random forests, ... As for dealing with open source code issues and with parsing HTML, stackoverflow might be a better place to seek answers. EDIT: Given your modified question, I can address your concrete questions about SVMs directly: 

Without looking at specifics, growing a tree using information gain is a greedy procedure and won't always get you to the smallest tree consistent with the data, even if one exists. If there are ties among attributes, you break them arbitrarily. 

Here's another one I found on Scott Aaronson's blog (and the Q+A is taken from there): In his Ph.D. thesis, Turing studied the question ($F_α$ is a theory): 

The first positive example will remove enough literals so that the conjunction will become satisfiable. For example if the first positive example is $x_1\overline{x_2}x_3$ this removes $\overline{x_1}$, $x_2$ and $\overline{x_3}$. So the only time you will end up with an unsatisfiable clause is if you get only negative examples, which is okay, as the correct thing to do is to classify everything as negative anyway. Also remember that in PAC learning, the learner gets to choose how many examples to see -- so you generally don't have to worry about "not getting enough examples" (as long as you need a number of examples polynomial in the number of relevant parameters). 

Guha et al. '03 give an approximation algorithm for k-median clustering in the streaming model. Their algorithm divides the data into disjoint pieces, finds O(k) centers for each disjoint piece, and then combines the results to get the k centers. This seems to be the type of algorithm you're looking for. 

Reductions are defined with respect to the decision version of the problems. Approximation ratios for their optimization versions are a separate question, which seems related but doesn't necessarily have to be. So to answer your question with a question, from a philosophical perspective, why should you expect the class NPC to preserve approximation ratios when it isn't defined with respect to them in the first place? 

EDIT below: It seems to me that the budget constraint (not going below $0$) makes the problem intractable. Imagine you have a budget of $1$. The adversary can make one of the arms always pay off and the rest never pay off. So w.p. $(n-1)/n$ you go bust in the first round while the optimal strategy gets $T$ dollars after $T$ rounds. So your expected regret is at least $(n-1)T/n$ and you can't hope for a high probability bound at all. It also seems that this can work for any initial budget. Say you start with $B$ dollars. Then the adversary can set all but one arms to pay $0$ and one arm to pay something like $2B$ w.p. $1/B$. I guess if you have a limit on the possible payout amount and a high enough initial budget, then this might leave room for an interesting problem. 

One way I have often found theory problems to work on is by reading about an area and trying to figure out exactly what the state of the art is on a problem. Invariably, some basic questions end up being left unanswered, and that’s where I will start my research. Such questions are sometimes left unanswered not because they are too difficult, but because nobody formalized things properly, and this is something that can come naturally to a theorist who is just trying to understand what’s known and what’s not. And the ability to formalize problems is a key aspect of doing successful theory. Another natural way to do successful theory is by trying to solve known open problems, for example those found in the discussion sections of recent papers. There, the key is having a good enough understanding of your own strengths and interests that you choose problems where you might make progress. This takes some experience and also some self-awareness, which you can actively think about and cultivate as you go through your PhD. And as you do your PhD, you’ll develop these skills and more, including the ability to grasp proofs in papers quicker, and if things go well, you’ll be coming up with your own directions naturally by the end. 

Originally, Logitboost was derived by Friedman, Hastie, and Tibshirani (paper) -- their algorithm internally used a numerical procedure, via Newtons method to solve a regression problem. Later, Collins, Schapire, and Singer (paper) found an equivalent formulation, with a single-line modification from AdaBoost, setting $$D(i) \propto \frac{1}{1+e^{y_i f_{t-1}(x_i)}}.$$ Schapire has a nice summary of boosting here, which also discusses Logitboost.