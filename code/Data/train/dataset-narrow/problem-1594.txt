I've bought a domain, say . I'd like to set up google apps to work with it. As such, I'd like to create subdomains like , , etc. To integrate it with google, I must add to all of them CNAME records to . However my domain registrar says that they can create only one CNAME in my domain, for which they chose . The rest of them, they say, I must create as A type records. Are they being lazy, or is there some limitation here that I'm not aware of? 

It's all about marketing and getting more money for Microsoft. Therefore the desktop versions of Windows have a severe limit on concurrent requests in IIS. Don't know the numbers by heart, but it would choke immediately if used under production load. You can however run 3rd party webservers just fine. I just don't know if any of them support ASP.NET. 

For small businesses with less than 10 PCs it's often an overkill to even have a central server, much less buy a Windows Domain Server which costs heaps of cash. However the need remains for people to share files and printers amongst their computers. The standard solution would be to share the folders publicly. But sometimes that can be undesireable, for example if some unauthorized laptops appear in the network regularly. Another solution is to create the same users on all computers (including setting the same passwords). This allows for authorized access, but adding new users or chaning passwords is a pain. It would be great if it was possible to set up one of the machines as a central "user database". Other computers could then authenticate against that computer, and even set permissions on shares. Is this possible somehow? 

Is there any software that would test the reliability of a CPU? Like, check how all the instructions are working, verify that cache is not damaged, check for known CPU bugs, etc. I've got a machine in which I suspect that the CPU might be failing, but the symptoms are weird enough that I'm not certain... 

I've got a small jack-of-all-trades server running CentOS 7 Linux. Now I need to add email support to it (SMTP/POP3/IMAP/webmail). Most of it is pretty clear, except for one thing - I'd like to separate linux users from email users. That is, for every email account I don't want to make a server user as well. Also, if there are emails jenny@domain1.com and jenny@domain2.com, I'd like them to be separate accounts. And, of course, users should be able to use their email addresses as usernames for the mail-related services. How can I do that? I don't even know what keywords to search for (yes, I've tried googling, but came up empty handed). 

1. Handling arguments Despite what Tero Kilkanen says, nginx is perfectly capable of handling arguments: this is done by using the and variables from the core module. 2. context As Glueon pointed out, is usable in , and contexts, as the documentation states. is to be avoided at all costs, so I won't be going inot details about it here. Now should you use in or ? In Blocks of s will be considered sequentially, and the first matching will be used. Your configuration thus depends on the order of directives, which is bad (it is one of the things which are wrong with Apache). See for yourself: 

You are mistaken about the way nginx operates. Requests are processed neither one after another nor simultaneously (which is impossible). nginx reacts on events corresponding to differents stages during requests processing, which kind of multiplexes them. The upstream module (and its directive of the same name) you point to is the way to go, along with the proxy_pass directive. The center of your focus might be the variable. As its name indicates, it stores response time of the upstream servers and might be dealt with to change the selection of the backend server nginx shall choose. To do so, you can tweak your upstream servers group to add a weight to each and different flags to impact the default round-robin mechanism. You can then change weights according to the response times you collected by generating files you include in your upstream servers group configuration. Note that changing the configuration will need nginx to be reloaded. 

If I understood your question right, you just need to redirect each location to a different backend, each listening on a different port, like the following: 

Having the content enciphered before transmission to the TCP 443 port is thus a requirement per RFC. You cannot beat it. The reason for the requirement is an enforcement of the separation of OSI layers, TLS (SSL) being in the 5th one and HTTP in the 7th. There is no way they can understand each other. That is reflected in softwares, where processing of those parts are usually done by different code pieces (nginx handles HTTP, but defers TLS to an external library it has been linked with). 

Using prefix locations, you ensure that whatever ordering you use, the result will always be the same. 3. Minimal configuration Here is a minimal configuration aiming in the direction of what you wish. There is no obvious translation rule from 2xu-glid to Glide%20SX%202012%20-%20%20Super%20Partno. If that depends on non-trivial conditions and complex checks, it is probably better to use scripting to do it. Using inside nginx is dangerous without knowing what you do and could wreak havoc. You can also use a bunch of nested locations to split your translation rules into modular units. Note that I use a single rewrite here, so it does not need a flag nor does it need to be enclosed in a location. That is because there is only a signle way that configuration might be resolved. Also note the use of at the end to ensure no file is ever tried to be served (the default behavior, when no suitable location is found, is to try to serve either a file or , depending on whether the URI ends with ). Since no is defined, the default internally set would have been used. 

I don't know what the best practice for kerberos is with regards to security. I was wondering is it a good idea to allow a kerberos server to be public so public servers can use single-sign on or is it something that is only reserved for internal lan. Also the kerberos server is an Windows Server that has other services on it as well. What would you guys do? 

From my perspective I would just pop it on EC2, why do the extra step. I wouldn't use the HSQL memory database but set up a different one such as mysql as I have had a bad experience performance wise with the default memory db. Also, the default amazon AMI has been a pain imo to set up graphical programs as X Window System is not listed under yum grouplist; so you will end up having to do some manual config me thinks. As far as the amazon reservations go. On Demand is the most expensive hourly. if you run it all year your bill will probably be around 30% more. Then there are the 3 RI levels: light,medium, heavy. All require some intial money down. light means the least money down but also the least savings from on demand. heavy requires the most down but has the most savings hourly compared to on demand. If you going to run the instance all year round for sure then go heavy. As as far as performance of a micro goes. Well, it is the cheapest but if I remember its not guaranteed even 1 cpu; its more for burstable hits. If its going to be used lightly you might be able to get away with it, but if you got a lot of developers and they are going to be generating those reports you might wanna aim for something a little more beefier. It is a shame m1.smalls are still 32-bit. 

They probably mean set the block size to 4k. $URL$ I think the default used to be 512Bytes for a lot of file systems but people moved to using 4k. There should be a way of setting it when you create the filesystem. 

I am not familiar with pip or easy install but my guess is that /opt/python2.6/include is not hardcoded in those two app installers. It probably just searches /usr/include and /usr/local/include. Try creating an alias to the python2.6 directory in /usr/include or something like that or check the flags for ./configure on those apps. You might be able to specify a directory. 

I use a utility called Eraser. I think it was recommended in some Microsoft TechNet posting or MSDN. And it's totally free. 

I've got a Linux server I can only connect to remotely. I want to backup it, but it'll have to be over the Internet, and I've chosen Google Drive to hold the backups. The only piece of puzzle I still don't have is how to package and compress all the files. I want compression, because space on the Google Drive is limited, and it would also reduce upload times. I could of course use the standard tar+gzip/bzip, or zip, or maybe even something fancy like 7z for best compression. But what I'm wondering about is this - many of the files that will need to be backed up will be things like JPEG images, which don't compress well at all, no matter which compressor I use. It would be faster if those files were copied to the target archive as-is, rather than compressed. Other files are text files, which compress better with a specialized algorithm (can you tell yet that I'm backing up websites?). Is there some kind of archiver which recognizes such files (by file extension would be fine) and applies a different algorithm for them? I think I've seen one somewhere, but I don't remember which one it was and if it has a Linux version. Or perhaps I'm overthinking this? 

In a course about Windows Server administration that I took at the university they told us that a normal user could add 10 computers to a domain. Over the years however I've added and removed way more than 10 computers (virtual machines included) to the domain without any problems, while some colleagues of mine swear that they can't add any computers. My account is not priviledged in any aspect, I'm just a regular user like everyone else. So... what are the rules governing this? Can they be changed by the admin? Is there any way I can find out with my user-level-priviledges? Just curious. :) 

My guess is that it should be at least as good as tapes. Perhaps even better, because the magnetic disk itself is in a contained environment and magnetically shielded. However, I'm pretty sure that nobody really knows what happens to a HDD after, say, 20 years of unuse. It's the same thing as with CD's and DVD's (the archival-grade ones) - they simply haven't been around that long. Well, OK, hard drives were there 20 years ago as well, but comparing them to hard drives of today is pointless. 

Your question is unclear. Do you wish to rewrite an URI (a location) to another ( to )? Or do you wish to serve content for with the files contained at ? nginx' configuration works differently from Apache's, that is a fact. Saying it is 'more confusion' is a personal feeling. I would object that, on the contrary of Apache, nginx allows to create lean, clean and order-independent configurations, guaranteeing readability, maintenance and scalability. It is normal to be lost because there is a learning curve for every new technology you need to handle. I suppose Apache's configuration is not always straightforward and when you started with it, it was not that obvious how to deal with it... Regarding , it is mixing content with configuration, polluting repositories with files you then need to protect to avoid serving them... nginx allows you as much granularity and modularity you wish through the use of the directive, which provides the nice feature of separating the configuration in multiple files, adding them manually where you wish inside the upper-level configuration and even include whole directories at once, allowing a per-server, per-whatever configuration file. If you need specific rules for a location, just create block for it. And if you wish to isolate a whole branch from the locations tree in a separate configuration file, you can. This is a cleaner/more readable/more maintainable way of doing what was doing wrong. 

Mind the ending , replacing the original URI. If you wish to forward the original URI to backends, remove anything after the pair. 

As pointed out, your configuration snippet looks good and accessing in port should be served in the 3rd block you provided. If it does not work as you describe, it might be because it is not loaded. 

seems to be pointing to the same IP address as No virtual server more suitable than the one being found for your request to , it is selected to serve the request, the same as if it was having the flag on its directive. 

It runs flawlessly, so the most probable cause of your trouble is that your configuration is not applied correctly. I would guess that changes you made on the side of adding the directive broke it, making the variable inexistent. Try to: 

Note: is to be preferred to wherever possible, as the latter might result in an unwanted behavior, conflicting with other directives such as . 

What you could do is uploading configuration files including directives to a specific directory. That directory would be included (ie through ) in your configuration. You will need to issue a signal to nginx master to reload the configuration though. @MichaelHampton is right about the fact you do no need to restart the server. Reloading the server configuration without downtime can be done by issuing the command or . There is no 'persistent directory watch + reload on filesystem change' feature in nginx. You will need to script it (ie with Lua), but that is not recommended anyway. You could create a cron task for regularly reloading nginx configuration... looks also dirty if you ask me.