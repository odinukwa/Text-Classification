The primitive recursive functions are defined over the natural numbers. However, it seems as if the concept should generalise to other data types, allowing one to talk about primitive recursive functions that map lists to binary trees, for example. By analogy, partial recursive functions over the natural numbers generalise nicely to computable functions on any data type, and I'd like to understand how to make the same kind of generalisation for primitive recursive functions. Intuitively, if I were to define a simple imperative language that allowed basic operations on, say lists (such as concatenation, taking the head and tail, comparison of elements) and a form of iteration that requires knowing in advance how many iterations will occur (such as iterating over the elements in an immutable list), then such a language should at most be able to compute the primitive recursive functions over lists. But how can I understand this formally, and more specifically, how would I go about proving that my language computes all primitive recursive functions over lists and not just a subset of them? To be clear, I'm interested in understanding primitive recursive functions as a well-defined class of functions (if indeed they are), rather than just in the operation of primitive recursion itself, which seems straightforward. I'd be interested in pointers to anything that's been written on primitive recursion over general data structures, or indeed in any context other than the natural numbers. update: I may have found an answer, in a 1996 paper called Walther Recursion, by McAllester and Arkoudas. (It's not clear where it was published.) This seems to contain a generalised version of primitive recursion as well as the more powerful Walther recursion. I intend to write a self-answer once I've digested this, but in the meantime this note might be helpful to others with the same question. 

I have a slightly crazy idea that would require me to play around with an actual implementation of such a language - I want to be able to generate arbitrary (self-terminating) binary strings, have them interpreted as programs, and get binary strings as output. Of course, in principle I could just invent some kind of trivial prefix-free encoding of ASCII, run it through an interpreter for my favourite language, and convert STDOUT to binary. However, if I do this, the chances of it doing anything other than terminate immediately (with a syntax error) are vanishingly small. Thus, I also have the following requirement: 

For the purpose of this question, you can assume that $q$'s Voronoi cell is always bounded (for example $q$ always lies in the convex hull of $P$). Is there anything known about this problem ? Some constraints: 

I am aware of the "famous" DIMACS graph format (which frankly looks a little clunky to me - "c" for a comment line ?) and the METIS file format. While it's not particularly hard to invent my own graph format, it's nice to follow something accepted so that reuse of code for generating examples and testing algorithms is easier. 

This is the "reverse" direction, but the well known Aanderaa-Rosenberg-Karp conjecture applies to graph properties that are monotone upwards (i.e if G satisfies the property, then so does any graph on the same nodes whose edge set contains E(G)). 

One important reason why problems that look equally hard to compute exactly might be very different to approximate relates to the fragility of NP-completeness reductions. The simplest example I can think of the relation between VERTEX COVER and MAX INDEPENDENT SET. The two problems are complementary: given a graph $G = (V, E)$ with a vertex cover $S$, the set $V\setminus S$ is an independent set. This is why it's easy to show that one problem is NP-hard by reduction from the other. But suppose I can get "close" to a good vertex cover, in that instead of finding the smallest cover $S$, I find a worse cover $S'$ where $|S'| = 2 |S|$. This means that instead of finding the maximum independent set of size $n - |S|$, I've found one of size $n - 2|S|$. Suppose in this graph the max independent set size was $n/2$. Then essentially my approximation to the vertex cover found nothing at all. In other words, the NP-hardness reduction only preserved the exact solution relationship, not the approximate solution relationship. To preserve approximations, you need a different kind of reduction called an $L$-reduction, and once you limit yourself to such reductions, you realize that the two problems are indeed quite different. 

This is purely US-centric: other countries have different funding models. This is also from the perspective of an academic with a Ph.D, rather than a graduate student As Jamie and Peter point out, the primary purpose of funding is to support graduate students. A secondary purpose is to support yourself during the summer. It's not widely known, but most US-based academics aren't paid for the 3 months of summer, and use grant money as salary for those months (I'll not discuss the limitations of NSF vs DARPA etc etc). So you say, "I don't need students, I'll just work with colleagues". Great ! but then you need money to visit them. Without grant money, you have to wait your turn for whatever meager departmental funds might be available for travel (usually minimal). So you then say "Fine ! I'll use skype and email to collaborate". Great ! but then you need to travel to a conference to give a talk. How do you fund that ? So you say "Fine ! I'll just publish in journals and on the arxiv, and the brilliance of my research will shine through". Um, yeah.... If you're a junior academic, not getting funding can also affect your ability to retain your job itself. Funding is a major income source for most American universities. None of this is ideal. But that's how the system is currently structured, dating back to Vannevar Bush, the founding of the NSF, and the mutation of the university into a research-generating enterprise. 

My question is whether such a language exists - either as a formal definition in the literature that wouldn't be too time-consuming to implement, or as a command line tool or (better) a library that can be called from C++ or Python. Given the amount that has been written about this construct, I would find it mildly surprising if no-one had constructed a concrete example, but I haven't been able to track one down. 

Consider a function $f:\mathbb{N} \to \{0,1\}$ whose is defined in terms of some universal Turing machine $U$. If $U$ halts when given $x$ as input then $f(x)=1$, otherwise $f(x)=0$. Clearly the function $f$ is undecidable. Now consider a new function $g:\mathbb{N}\to \{0,1\}$, defined by $$ g(x) = \begin{cases} f(x/10^{6}) & \text{if $x \mathop{\%} 10^{6}=0$} \\ 0 & \text{otherwise.} \end{cases} $$ It's equally clear that $g$ is undecidable. However, informally it seems that there should be some sense in which $g$ is less undecidable than $f$, since, informally speaking, it looks like the value of $g$ can be determined for $99.999\%$ of its inputs just by checking if they're divisible by a million. My first question is whether there's any sense in which the intuitive claim that $f$ is more undecidable than $g$ can be made into a rigorous one. If there is, I'm interested in whether there are any functions that are particularly "densely" undecidable. I have in mind something like a universal Turing machine that not only halts for about 50% of possible inputs, but also when it fails to halt it does so for a wide variety of different non-trivial reasons. (If this were not the case one could generate a second Turing machine to spot the majority of non-halting cases, so it wouldn't be densely undecidable.) To ask more or less the same question in a different way: the vast majority of randomly-generated strings will halt immediately (with a compile error) if fed to a C compiler. The probability that randomly generated C code will fail to halt is very low, and the probability that it will fail to halt for some non-trivial reason is much lower still. I'm interested in whether one can define a language (or model of computation) that, when given random input, is quite likely to do something strange and non-trivial that makes its output difficult to predict. 

A theoretical construct that comes up a lot in algorithmic computability theory is a universal prefix-free language. For my purposes, this is a language with the following properties: 

I'll take "greatly influenced" as a soft constraint rather than as a reduction. In that sense, MANY problems in computational geometry have running times that are bounded by some combinatorial structure underlying them. for example, the complexity of computing an arrangement of shapes is directly linked to the intrinsic complexity of such arrangements. Another, topical example of this is that various problems in point pattern matching have running times that boil down to estimating quantities like the number of repeated distances in a point set, and so on. 

It sounds from your question (the use of the word 'willing') that one deterrent is the unwillingness of students to present. Since it's in your research group, I'm assuming that being scooped is not something they need to worry about :). So the real reason is that they feel afraid: of seeming clueless, of not knowing the answer and having someone else find it in 5 minutes after they start presenting, or of people deeming their ideas to be uninteresting. In other words, the imposter syndrome. One of the most important transitions a grad student has to go through is the point when they realize that they can actually play in the research world, and make contributions of their own. This often comes when their first paper gets published, but can also happen the first time they talk to a "senior" researcher and realize they know more than that person about their topic of interest, or even the first time they prove a brand new result. One advice for you might be to give such talks of your own, where you show "how the sausage is made" and show how ill-formed your own initial ideas might be. That gives them a model to work with. At the very least, you can give them examples of the winding path a research project takes. A second advice you can give them is to remember that most likely no one knows as much about their problem as they do. They may not internalize this just yet, but repeated reminders might help :) When they actually give the talk, Sylvain's advice is very good: however I don't think the 1/3 rule applies here since in your research group there are probably no outside experts and probably no generic audience. But I'll mention a line I heard yesterday in the context of journalism, but which applies very well for talks by inexperienced people: