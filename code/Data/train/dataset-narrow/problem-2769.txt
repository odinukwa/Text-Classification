FuzzYspo0N's answer in the comments is a very good one. Basically this problem boils down to making game IDs unique enough to avoid collisions, without making them arbitrarily complex. Standard 128-bit GUIDs get their uniqueness by using local information (like your MAC and other factors) to make the likelihood of a collision between machines very small, then add a random and time based factor to make the likelihood of a collision within that machine very small. You can follow the same strategy to generate your own IDs. Start with something memorable to the user but relatively unique (if the user has a global username that's a good start, or the machine name/IP), and add other factors until you've generated something unique. By picking something local like machine name, you restrict the set of potential collisions enough that you can start to query if there are any. For example, once you've restricted it to just that user, or just that machine, you know exactly where you need to go to check whether or not that UID is already in use. For example if I know my username is unique system wide, then MrCranky:1 is a valid UID. If I can check whether MrCranky:1 is already in use (by some other method), then I can just keep trying numbers until I find a unique one. By using some other factor (like randomness or time), I can increase the likelihood that I pick an unused ID first time. E.g. If you know you can't feasibly create more than one session a second, then using MrCranky:122730 (the current time, to the second) will get me a unique ID that's relatively memorable to the user. As long as you have some piece of relatively unique information (doesn't have to be perfect, just mostly unique), you can use that as a starting block, and that means the truly unique portion (an integer code of some kind) can be much much shorter, and thus the UID is more memorable. 

Worth checking the recent post from Richard Bartle (co-creator of MUD), on the evolution of the DPS/tank/heal team structure, and alternatives. 

I have a 'full width UI' script that deduces the aspect ratio of the screen relative to some reference screen dimensions, and adds a scale to the transform to compensate for a change in aspect ratio. Then I build my UI to the reference screen dimensions (in my case 1024 x 768). At runtime, the actual screen width is asked for, and the adjustment ratio calculated. E.g. if the screen width is 1280, then a ratio of is needed. That ratio is set as a scale along the x axis on a transform which is the parent of all of my UI, effectively stretching it horizontally until it occupies the whole screen. NB: This is separate from the code which scales the entire UI to fit the resolution (e.g. if the screen was set to 800x600 instead of 1024x768, then my UI needs to be scaled down by 0.78125 in both axes to occupy the same space. In practice, I handle that part by altering the camera projection, but I call it out because changing the aspect ratio is a separate thing than changing the resolution (because the latter needs to preserve the aspect ratio, not alter it). 

Now, the dispatcher logic is probably the thing you want to code as events. I.e. in HandleKeyPressed, if you detect a press of the W key, then you raise the 'go forward' event. That way multiple listeners can react to a single event. This dispatching through events forms your control mapping. The rest of your game doesn't have to know that it's the W key that maps to going forward, because it's bound to a logical control event instead of a physical one. That allows you to rebind W to some arbitrary key, and not have to change a bunch of code elsewhere that assumes that forward is always W. 

As a general design pattern, manifests are useful when you want to collect all the information about a disparate set of objects into a single place. It doesn't have to be about archive/packed files, or about indirection to allow things to be moved without recompiling/updating original references. Indeed the latter can introduce more problems than it solves, so you would only do that if it solved a particular need you had. The big advantage of manifests is that they act as an index to a large amount of data in a single compact place. As such they improve performance (because you can simply load the entire manifest from disc and keep it in memory), especially in the case where you need to iterate over multiple objects, but you don't know in advance what those objects will be. If the objects are on disc, especially if they are in multiple places, you have to touch the filesystem every time you want to iterate over files. For disc-based file systems, the time needed to touch the filesystem is prohibitive, so iterating over files in a directory is a massive cost. By pre-building a manifest of files at build time (NB: not compile time), you trade that cost for memory usage. Archive files require the use of manifests, simply because the table-of-contents for the archive is essentially a manifest itself, so you get the behaviour for free. And if you are required to use manifests for assets in one location, it can be cleaner to insist that all assets be referenced through manifests; allowing you to abstract the actual storage location/mechanism of the assets from the references to those assets in code. That way you can have a single asset reference type in your code, and not have to make the distinction between file paths, archive file path + offset, or sub-assets. 

This is a business decision rather than a technical one. You have to look at the set of potential customers you are excluding by insisting on DX10/11 (as mentioned elsewhere, maybe 57% of Unity users), and compare it against the extra sales you expect by using tech only available in DX10/11. If your game is not graphically pushing boundaries, you're probably better off making it look as good as it can on with DX9 tech, where you can maximise your potential sales. If it's supposed to be cutting edge, then limiting it to DX9 tech might make it look dated. The number of potential sales lost by excluding XP customers will pretty much only go down from here. How fast it will drop is not clear, but as has been mentioned, the W7 platform is much more attractive than Vista, so you can probably expect a much more rapid uptake now than while Vista was the current OS. But certainly this will become less and less relevant as a specific issue. 

As has been said, decoupling the AI logic from the visual logic would be a good thing, because while they are related, they definitely don't want to be closely tied. There are certainly situations where you don't want to move to a new AI state until some visual things have happened, but that is a specific case of the more general problem: that you don't always want to be doing something every frame. Most times you start out coding AI in an immediate way: every tick you ask the entity what it would like to do this tick, and it considers all its options and makes its decision (keep moving, accelerate, turn around, fire, etc.). You'll quickly run into the limitations of that. In general, you want your AI state machine to be able to simply (and cheaply) do nothing, until 'something else' happens. That might be a timer elapsing, an animation completing, or some other trigger (perhaps from other entities in the world). Moving to a state machine is the first step towards a more flexible system. But in general I think you want to get away from the notion of 'per-tick' updates for your higher level logic. Higher logic doesn't tend to operate in terms of a regular update cycle like that. It works in terms of events and actions. Actions will very rarely take only one frame to complete. Some may be instantaneous, some may take many ticks. Your systems shouldn't care, and should support all types. It should be possible to tie actions to events occurring, so that your actor can make new decisions based on the fact that an event has occurred. A queue of some sort is a good way for the actor to maintain memory of 'what am I doing/thinking' between ticks. That's not to say it must blindly work its way through the queue though. It should regularly be looking at its immediate circumstances and re-evaluating. Some circumstances might cause it to junk everything in its queue and replace it with a new set of actions ("Run away!"). Sometimes it might just want to slot in a high priority action at the head of the queue ("bend over and pick up this thing that just landed at your feet, then continue"). Other times it might put in the new action at the end of the queue. So don't think of a queue like a list of orders to be followed, think of it like a prioritised to-do list that the actor wants to tackle. There are lower level subsystems that put each order into practice (e.g. the queue holds 'go to A'), and a lower level path-finding system actually translates that into a 'how do I move this tick'. 

Essentially a series of flood fills - one for each cell in the grid, but skipping over any you've already visited, and dropping out early if you find a cell not connected to a group you've already flood-filled/gathered but of the same type as that group. Written blind, so please excuse any code typos or holes; highlight them in a comment and I will amend. Also assuming a 2D array storing tiles, and variables detailing dimensions. With this algorithm, you don't need a Tile class at all - just a 2D array of ints would do just fine.