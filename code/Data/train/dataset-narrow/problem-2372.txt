When an event occurs you iterate through the z-order list, doing a simple hit test for the bounding box of each widget. The first time the hit test succeeds, you know where the event occurred, and can do event dispatch. 

Weak normalization means: there exists a reduction strategy that will lead to a normal form. People often use confluence to mean what rewriting people call "local confluence": this is the property that if $a \leadsto b$ and $a \leadsto c$, then $b$ and $c$ can be joined -- i.e., there exists a $d$ such that $b \leadsto^\ast d$ and $c \leadsto^\ast d$. 

The intuition behind coherence spaces is that the elements of a coherence space represent observations of some underlying data, and the coherence relation tells you whether two observations could have come from the same piece of data. Concretely, suppose we have a set of animals 

You give the usual typing rules to judge the well-formedness of a term. The first line of terms are the usual simply-typed lambda calculus, the second two lines are the propositions of higher-order logic (typed as elements of $\mathrm{prop}$), and the third line are whatever constants you use to form individuals (elements of $\iota$). Then the idea is that you want to extend the Kripke semantics for first-order intuitionistic logic to higher-order logic, by extending the hyperdoctrine semantics with additional structure. A first-order hyperdoctrine is a functor $P : C^\mathrm{op} \to \mathrm{Poset}$ between a category $C$ with products (used to interpret the terms in context), and a category of posets (the truth value lattices), satisfying some conditions to make substitution work right. To get to IHOL, you additionally assert that 

What you are looking for are called "self types", and have been studied theoretically for ~20 years or so. For example, see Safe Type Checking in a Statically-Typed Object-Oriented Programming Language by Kim B. Bruce, in the 1993 POPL (Principles of Programming Languages), pp. 285-298. Off the top of my head, John Mitchell and Kathleen Fisher have also worked on this, and they may also be described in Abadi and Cardelli's book A Theory of Objects. Scala supports them, but I don't know if other mainstream OO languages do. 

For fun, I've been looking at the interpretation of linear logic in terms of finite-dimensional vector spaces, and ran into an interesting question about the interpretation of double-negation-elimination in this category. As is well-known, vector spaces have a natural notion of dual space -- given a vector space $V$,(over a field $R$) the dual space $V^\ast$ is just the set of linear functions into $V \multimap R$. Furthemore, it is well-known (indeed, so well-known that I don't know who proved it) that every finite-dimensional vector space is isomorphic to its double-dual. One direction of the isomorphism $V \to V^{\ast\ast}$ is easy (it's just the map $f : V \to V^{\ast\ast} \triangleq v \mapsto \lambda k.\;k(v)$). The other direction is trickier. The usual proof goes something like this: 

This says that if we can figure out any relation such that (a) the initial states are related, and (b) from related initial states, any I/O action the first machine can take can be mimicked by the second machine in a way that keeps you in the relation, and (c) similarly, the first machine can mimic anything the second can do. Lamport's notation with primes is a way of concisely describing input-output relations. In this case, the state sets are the program counter of the first program, and the two program counters of the second program. The bisimulation relation is $R(pc, (p, c)) \triangleq pc = p \oplus c$, and then the bisimulation conditions follow trivially (since the relation expressions are equal under the subsitution). The general theory at work is the theory of coinduction. State machines are representions of corecursively defined sets, and bisimulations tell you when two state machines are representations of the same potentially-infinite object. Incidentally, this paper is written in a rather polemical style. Equally polemically, I'll point out that the style he advocates in this paper simply fails should you ever need to verify part of a program in isolation -- for example, if you want to prove a library implementation correct, or verifying a program using higher-order functions (or even just dynamic linking). But both of us will use coinduction, nonetheless. 

It's well-known that in System F, you can encode binary products with the type $$ A \times B \triangleq \forall\alpha.\; (A \to B \to \alpha) \to \alpha $$ You can then define projection functions $\pi_1 : A \times B \to A$ and $\pi_2 : A \times B \to B$. This isn't so surprising, even though the natural reading of the F type is of a pair with a let-style elimination $\mathsf{let}\;(x,y) = p \;\mathsf{in}\; e$, because the two kinds of pair are interderivable in intuitionistic logic. Now, in a dependent type theory with impredicative quantification, you can follow the same pattern to encode a dependent record type $\Sigma x:A.\; B[x]$ as $$ \Sigma x:A.\;B[x] \triangleq \forall\alpha.\; (\Pi x:A.\; B[x] \to \alpha) \to \alpha $$ But in this case, there isn't a simple way of defining the projective eliminators $\pi_1 : \Sigma x:A.\;B[x] \to A$ and $\pi_2 : \Pi p:(\Sigma x:A.\;B[x]).\; B[\pi_1\,p]$. However, if the type theory is parametric, you can use parametricity to show that $\pi_2$ is definable. This appears to be known --- see, for example, this Agda development by Dan Doel in which he derives it without comment --- but I can't seem to find a reference for this fact. Does anyone know a reference for the fact that parametricity allows defining projective eliminations for dependent types? EDIT: The closest thing I've found so far is this 2001 paper by Herman Geuvers, Induction is not derivable in second order dependent type theory, in which he proves that you can't do it without parametricity. 

(As an aside, how do you put accents into the text? I've dropped an accent from both his first and last names. EDIT: Name fixed. I'm leaving this parenthetical in so that the comments to the post continue to make sense.) 

You're essentially asking for resources that will let you turn your existing knowledge of logic, recursion theory, and category theory into knowledge about theoretical computer science.I would suggest looking at realizability theory, especially via its connections to topos theory and categorical proof theory. Here are a handful of suggestions; my advice is to pick one and go into depth. With the exception of Taylor's book (which explains this), my suggestions assume you have been exposed to enough lambda calculus and category theory to have seen categorical interpretations of the simply-typed lambda calculus. 

I'll first restate your question, and then try to answer it. As I understand it, the question you are trying to answer is, "if I have a scene graph/widget tree, and I get an event, how can I figure out how to dispatch that event to the appropriate subnodes of the tree?" The answer is that the data structures you have outlined are not sufficient by themselves to do the job; you need a bit more information. The primitive events the OS produces tell you the positions of the mouse, and the scene graph gives you a logical decomposition of the structure of the GUI. Therefore, you need some additional data structures to translate between the widget tree and what it looks like geometrically. The simplest case is when every widget is an axis-aligned rectangle. What you do then is: 

This is a tricky question! I'll tell you my personal opinion, and I emphasize that this is my opinion. I do not think pi-calculus is directly suitable as a notation for concurrent programming. However, I think you should definitely study it before designing a concurrent programming language. The reason is that pi-calculus gives a low-level --- but importantly, compositional! --- account of concurrency. As a result, it can express everything you want, but not always conveniently. Explaining this comment requires thinking a bit about types. First, useful programming languages generally need some kind of type discipline in order to build abstractions. In particular, you need some kind of function type to make use of procedural abstractions when building software. Now, the natural type discipline of pi-calculus is some variant of classical linear logic. See, for instance, Abramsky's paper Process Realizability, which shows how you interpret simple concurrent programs as proofs of propositions from linear logic. (The literature contains a lot of work on session types for typing pi-calculus programs, but session types and linear types are very closely related.) However, I said that the natural type discipline of pi-calculus is classical linear logic, and this is the source of the difficulty in using it directly as a programming language. In most presentations of classical linear logic, the (linear) function type $A \multimap B$ is not a primitive. Instead, you encode function types using de Morgan duality $A^\bot â…‹ B$. This is just fine from the POV type theory, but it's awkward when programming. The reason is that programmers end up managing not just their function calls, but also the call stack. (Indeed, encodings of lambda calculus into pi calculus typically end up looking like CPS transforms.) Now, typing ensures that they will never screw this up, but nevertheless it's a lot of bookkeeping foisted onto the programmer. This is not a problem unique to concurrency theory --- the mu-calculus gives a good proof-theoretic account of sequential control operators like call/cc, but at the price of making the stack explicit, which makes it an awkward programming language. So when designing a concurrent programming language, my opinion is that you should design your language with higher-level abstractions than the raw pi-calculus, but you should make sure that it cleanly translated into a sensible typed process calculus. (A nice recent example of this is Tonhino, Caires and Pfenning's Higher-Order Processes, Functions and Sessions: A Monadic Integration.) 

I don't think that the real problem is the question of what unbounded means; this is no worse than any other situation in parsing. The trouble lies with characterizing backreferences, which are both very powerful and very limited: they allow description of some non-context-free languages, without allowing some context-free languages. For example, the regex matches strings of the form $a^n \cdot b \cdot a^n \cdot b \cdot a^n$, and you can use the pumping lemma to show this is not a context-free language. However, on the other hand, regexes with backreferences don't seem to be sufficient to match the balanced parenthesis language, which is the prototypical context-free language. It's easy enough to give a denotational semantics saying what strings are in a language to regexes, but giving a good automata-theoretic characterization seems much more challenging. It's something like a register machine, into whose registers you can copy substrings of your input, and which you can use to test your current string against, but for which you lack the ability to modify these registers. People doing finite model theory have a bunch of funky machine models, and it would be interesting to know if this corresponds to any of their models.