Update: even for $k=2$ it would be really useful to have a way to store only $n/2$ vertices. Or is there actually a stronger lower bound $cn$ for some constant $c$, regardless of $k$? 

Using Papadimitriou's bound on the solutions of an instance of INTEGER LINEAR PROGRAMMING, one can also show that the version where the numbers must all be non-negative is also in NP. 

Some graph classes allow polynomial-time algorithms for problems that are NP-hard for the class of all graphs. For instance, for perfect graphs, one can find a largest independent set in polynomial time (thanks to vzn in a comment for jogging my memory). Via a product construction, this also allows a unified explanation for several apparently quite different CSPs being tractable (such as those with tree structure which are usually solved by hierarchical decomposition, and the All-Different constraint that is usually solved by perfect matching). It could be argued that perfect graphs are "easy" because they allow nice semidefinite programming formulations of the problems in question (and therefore fall under linear algebra and/or convexity). However, I'm not sure that completely captures what is going on. 

Footnote: The term prop is from my thesis; it is meant to convey the idea that such partial assignments are proper, and also that they support the set of solutions. This is in contrast to a nogood, which is the accepted term to describe a partial assignment that cannot be extended to a solution. The word "nogood" was introduced by Richard Stallman and Gerald Sussman in 1976, when RMS was still an AI researcher instead of software freedom activist. 

It sounds like you are interested in all-different constraints (and your last sentence is on the right track). These are non-trivial instances of the pigeonhole principle, where the number of pigeons is not necessarily greater than the number of holes, and in addition some pigeons may be barred from some of the holes. All-different constraints can be decided by matching in low-order polynomial time. 

I am not as convinced as @Andrew Morgan is that this is "fair standard fare", and would also welcome pointers to a citable reduction. In particular, I do not see how to maintain a linear blowup if $k$ depends on $n$, because encoding that the independent set is large enough with linear blowup seems to require a simulation of any threshold gate by a linear-size circuit over $\{\land,\lor,\lnot\}$. (Edit: The most intricate part of the reduction is how to enforce the size-$k$ constraint; as pointed out in another answer, a threshold gate can be simulated by a linear-size circuit that adds up the number of set bits and compares the resulting binary number to $k$.) Here is a reduction along the lines hinted at in the comments. For convenience let $n=|V|$, $m=|H|$, and assume that $k \le (n+1)/2$. First create the direct encoding of $G$ as an intermediate SAT instance $I$, with variables $V$ and clauses $H'$. Variable $u\in V$ in $I$ is set in an assignment iff $u\in S$. Each edge $h\in H$ is mapped to a clause $\{\lnot u\mid u\in h\}\in H'$, encoding the constraint "at least one of the vertices in $h$ should not be in $S$". To ensure that a solution of SAT instance $I$ sets at least $k$ variables to true, some additional clauses need to be added to $H'$. The straightforward way to do this is by adding $f(n,k)=\binom{n}{n-k+1}$ clauses, each containing $n-k+1$ positive literals. Alas, this requires between $(n/(k-1))^{k-1}$ and $(en/(k-1))^{k-1}$ new clauses. This blows up the final graph by a factor that is a polynomial with degree depending on $k$, and when $k$ grows with $n$, even very slowly, the blowup is superpolynomial. An improvement is to use a monotone threshold gate simulated by $f(n,k)=O(n\log k)$ clauses (each of size 3) to encode the constraint that at least $k$ of the variables must be set. This works for sufficiently large $k$. However, if $k$ is not sufficiently large to dominate the constants (which can be galactic), then we use Dunne's earlier construction instead (Theorem 3.14 in his book), with $f(n,k)=kn+O(n^{1-1/k})$ clauses. Either way, $m'=|H'|= m+f(n,k)$ depends only quite weakly on $k$. Edit: In fact, the dependence on $k$ can be eliminated altogether by using a non-monotone circuit that counts the number of set input bits, and compares the resulting $\log n$ bit binary number to the binary representation of $k$; such a circuit can be simulated by $O(n)$ clauses of size 3. Hence $H'$ will contain the $m$ original clauses together with $O(n)$ new clauses of size 3. Now create the graph $G'$ with vertices $\{(u,g)\mid u\in g,g\in H'\}$ and an edge between $(u,g)$ and $(v,h)$ whenever $g=h$ or $u=\lnot v$. Edit: $G'$ has as many vertices as the number of vertices appearing in the edges of $G$ (i.e. counting a vertex every time it appears in an edge), together with $O(n)$ additional vertices. I then claim that $G'$ has an independent set of size at least $m'$ iff $I$ has a solution iff $G$ contains an independent set of size at least $k$. 

My favourite example is a classic 1977 result of Ashok Chandra and Philip Merlin. They showed that the query containment problem was decidable for conjunctive queries. The conjunctive query containment problem turns out to be equivalent to deciding whether there is a homomorphism between the two input queries. This rephrases a semantics problem, involving quantification over an infinite set, into a syntactic one, requiring only checking a finite number of possible homomorphisms. The homomorphism certificate is only of linear size and so the problem is in NP. 

To add to Daniel's answer, the precise density of hereditary classes has been extensively investigated in combinatorics. For a class $C$ of structures, the unlabelled slice $C_n$ is the set of isomorphism classes of structures in $C$ that have $n$ vertices. The (unlabelled) speed of a class $C$ of structures is $|C_n|$. Denote the class of graphs by $G$. The question is asking whether $\lim_{n\to \infty} |Q_n|/|G_n| = 1$ for any hereditary class of graphs $Q$. Since the limit is always 0 for hereditary $Q$, a fundamental question is then how the function $|Q_n|$ itself behaves. Let $p(n)$ denote the number of integer partitions, where $p(n) = 2^{\Theta(\sqrt{n})}$. It turns out that the unlabelled speed "jumps": either $|Q_n|$ is polynomially bounded, or otherwise $|Q_n| = \Omega(p(n))$.