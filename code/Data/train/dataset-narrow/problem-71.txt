Assuming you are not going to filter the buffer using HW texture filtering: since you only need 4 values for the material index, pack it in the sign bits of uv x and y. 

A group from UC Santa Barbara published the Siggraph 2015 paper A Machine Learning Approach for Filtering Monte Carlo Noise describing a technique which trains a neural network to select filtering parameters for path tracing. The paper details the structure of the neural network, the filter parameters used and how several secondary features are pre-computed to feed the neural network. 

A standard technique is to use a ray/sphere intersection test inside a pixel shader. One draws a billboard large enough cover the area the sphere could appear on the screen (or drawing a bounding cube) to reduce the number of wasted intersection tests (other geometric shapes could be used). The fragment is killed if the intersection test fails; if an intersection occurs, the appropriate depth value is calculated in the pixel shader and output for further visibility tests. 

GPUs can efficiently scale an image by an arbitrary amount (within limits - display options fall within those limits by design) either using a 3D rendering operation or as the signal is sent from GPU to the display. Both of these paths have fully dedicated hardware for the arbitrary resizing and are not likely to be optimized for doubling or halving. Both support a technique called "bilinear filtering" though advances in display hardware may provide higher quality. If a game is run at non native resolution on a laptop (TVs and desktop displays have their own scalers, but laptop displays generally rely on the GPU), one of these two methods will be used. Display scaling is essentially "free", so it is preferred and the lower the resolution the game is originally rendered the faster the frame rate (or the lower power consumption) - with no special performance benefit for, say, doubling. If a 3D operation is required (say, due to a limitation or issue in the display scaler), the scale operation is not free, but is going to go up with the source and destination sizes smoothly, with no special casing at doubling/halving. The game may look far better at certain scaling factors, but that's up to the user to decide. Edit: The intuition behind the question is correct: if GPUs didn't have dedicated scaling hardware, doubling/halving could implemented more efficiently than arbitrary scaling, either in shader code or via a slightly simpler hardware design - though the 3D rendering operation is a requirement of graphics APIs and display scaling is a requirement from systems vendors. 

Your original image is very low in quality and talking with experance image with such dimension should have 3MB size but your file is already compressed using lossy data DCT algorithm, deflated upto 0.1 rate and had lost a lot of data before you download it! In order to reduce the dimensions and preserving the filesize ratio, image editor program needs to extract the image and omit the lines vertically and horiznotally to reach to the desired dimensions and compress it again using a lossy data algorithm upto 10 percent but unfortunately this time the image will loose too much data and you will unable to recognize the faces in it. So you must select a higher quality cause the file just can not tolerate to loose any data again! This means you can not compress the extracted file as well as it was at first place. Comparing JPEG encoding (after resizing the image ) with PNG which uses ZLIb algorithm(non lossy data) I believe you may recieve a better result if you save your altered dimension file in to PNG, the image has a lot of same color pixel that will let ZLIB to compress it very well. This is only true in case you have a file which is once compressed using JPEG. 

Added after edit The following picture shows a real world example for an electricity network which should be considered as raw data, in order to create a graph from this network, some preliminary tasks must be done. Input Data: 

Down the road, I'm planning to implement one or another GPU based ambient occlusion technique. My goal will be to approximate a physically based offline renderer, so I was pretty happy when Wikipedia led me to Approximating Dynamic Global Illumination in Image Space, which describes SSDO. Based on the references, it looks to be from the 2008 era. Are there more modern approaches with similar accuracy goals? 

I've got bilinear filtering on in the texture (without it, I get serious shadow acne). Sadly, my attempt at PCF failed too - it looks just as ragged: 

To make the models more readable onscreen, I've implemented basic shadow mapping for a single directional light. Since my scene isn't changing, I just render the shadow map once using an axis aligned box of the scene to figure out what the bounds of the shadow map should be. The shadows and bounds look like I've got all the matrices right, but coming in a bit closer looks terrible: 

Shader compilers are extremely aggressive about unrolling since early HW often didn't have flow control and the cost on more recent HW can vary. If you have a benchmark you are actively testing against and a range of relevant hardware, then try things to see what happens. Your dynamic loop is more amenable to developer intervention than a static loop - but leaving it to the compiler is still good advice unless you have a benchmark available. With a benchmark, exploration is worthwhile (and fun). BTW, the biggest loss with a dynamic loop on a GPU is that individual "threads" in a wavefront/warp will finish at different times. The threads that stop later force all the ones that finish early to execute NOPs. Nested loops should be carefully thought through: I implemented a block based entropy decoder that encoded runs of zeros (for JPEG like compression). The natural implementation was to decode the runs in a tight inner loop - which meant often only one thread was making progress; by flattening the loop and explicitly testing in each thread if it was currently decoding a run or not, I kept all threads active through the fixed length loop (the decoded blocks were all the same size). If the threads were like CPU threads, the change would have been terrible, but on the GPU I was running on, I got a 6 fold increase in performance (which was still terrible - there weren't enough blocks to keep the GPU busy - but it was a proof of concept). 

When applying multiple textures to a mesh, like for bump-mapping, I usually bind the textures to the first few fixed texture units, e.g.: diffuse = unit 0, bump = unit 1, specular = unit 2, then keep reusing those to each different mesh with different textures. But I've always wondered why supports so many texture units (in the previous link, it says at least 80). So it occurred to me that one possible way of managing textures is to bind distinct textures to each available unit and leave them enabled, just updating the uniform sampler index. That should improve rendering perf by reducing the number of textures switches. If you have less textures than the max texture units, you never have to unbind a texture. Is this standard practice on real-time OpenGL applications (I believe this also applies to D3D)? And are there any non obvious performance implications of taking this approach? Memory overhead perhaps? 

But we are still missing a key piece here, and that's how to determine which pages must be loaded from storage into the cache and consequently into the . That's where the Feedback Pass and the enter. The Feedback Pass is a pre-render of the view, with a custom shader and at a much lower resolution, that will write the ids of the required pages to the color framebuffer. That colorful patchwork of the cube and sphere above are actual page indexes encoded as an RGBA color. This pre-pass rendering is then read into main memory and processed by the to decode the page indexes and fire the new requests with the . After the Feedback pre-pass, the scene can be rendered normally with the VT lookup shaders. But note that we don't wait for new page request to finish, that would be terrible, because we'd simply block on synchronous file IO. The requests are asynchronous and might or might not be ready by the time the final view is rendered. If they are ready, sweet, but if not, we always keep a locked page of a low-res mipmap in the cache as a fallback, so we have some texture data in there to use, but it is going to be blurry. Others resources worth checking out 

I'm fiddling with simple procedural 3D modeling to make teeny buildings for 3D printing. An Example: 

Does this look like "typical" projective aliasing? Could dynamic rendering of the shadow map, using the view frustum clipped to the scene, possibly with cascading, address the issue? Edit: Adding a closeup of bilinear filtering, post shadow comparison, just to show what I get. Bad shadow acne shows up because of interior edges; I'm modeling with stacked virtual blocks and not doing a proper union operation. From my reading, implementing boolean operations on polyhedra is not trivial, but will also allow me to implement static shadow volumes as well as clean up for some 3D printing software. 

Instead of a screen plane in front of the eye, it describes a film plane, where the image is projected, to explicitly model camera optics. You don't need to compute the focal point in doing ray tracing - it's just a way to find the plane of focus for depth of field effects. For depth of field effects I use a standard perspective projection but jitter the position of the eye on a circle parallel to the focal plane - and I make sure all my rays for a given pixel go through the same spot on the focal plane making it sharp while stuff in front of or behind that plane ends up blurry. It's a simple model of something like an aperture, and gives pretty good results. 

I am writing a software and I need to represent a graph in a orthogonal manner from topological data (vector of edges,vertices and their connectivity data) Graphs consist of a set of vertices and a set of edges, each connecting two vertices.A vertex may have any number of connected edges so it makes the problem a lot more complicated. I have read some articles and its looks like that the Kandinsky model is the post popular one. However I just don't know the algorithm, any other solution (algorithm) that solve the problem is also very welcome. 

The result I am looking for is something like below, there are some characteristics if you take a look much closer : The Red Polygon in the middle of the above picture(input data) represents an electricity substation which is a node itself and can be connected to more than 4 edges. There are more red polygons but only one can be fit into the above picture however as you may see, the following picture could cover much more than a red polygon that means it can map a bigger area, so the following picture is much more denser. In Schematics diagrams, red polygons (Substations) usually maintain their position against each other relatively so if we manage to see beyond the extents of the above map by zooming out for example, we should almost see a triangle that is obviously can be seen at the below while having the left one at the left, the down one at the down ..... (this is not a rule, but I thought it could be a head start for desired algorithm) Orthogonal Diagram: 

GPUs compress multisample data (for bandwidth) and dithering would defeat this - so this is not supported. GPUs also have techniques to sample at higher frequencies than they store values (see QUINCUNX anti aliasing for an example that's been around a while), saving overall memory. 

History: ATI Introduced PN triangles (a basic approach to tessellation) in its first generation to include programmable HW - so it's been around about as long as programmable HW shaders. It was deprecated in ATI's next generation, but tessellation was revived in the HW that became the basis of the Xbox 360 (a few ATI demos showed it off on PCs). Microsoft then incorporated the feature into DX11 (although not compatible with ATI/AMD's existing HW), making tessellation support a de-facto requirement for all GPU makers. 

From the reading I've done, I understand the peter panning and what I might do about it, but the ragged edges, which I believe is a form of projective aliasing, look so bad that I think something is wrong in my basic implementation. I hacked the pixel shader to show the shadow texel boundaries: 

Your transform looks correct. To transform from world to eye coordinates, I I always use a "lookat" transform, defined by 3 vectors: $\bf{e}$, $\bf{a}$ and $\bf{u}$; in english, the eye position, the point it's looking at, and an up vector, which must not be in the same direction as $\bf{a} - \bf{e}$ (more specifically, not a multiple of it). The space is defined using $\bf{z} = {{(\bf{e} - \bf{a})}\over{|\bf{e} - \bf{a}}|}$, which means the negative z axis points in the direction of what I'm looking at (this works well for OpenGL); $\bf{x} = {{\bf{u} \times \bf{z}}\over{|\bf{u} \times \bf{z}|}}$ and $\bf{y} = \bf{z} \times \bf{x}$, which, again, for OpenGL, makes $\bf{x}$ rightward on the screen and $\bf{y}$ upward. To transform into eye coordinates is a matter of subtracting they eye's position, then projecting into the space defined by the vectors above: $$\begin{bmatrix} \bf{x}_x & \bf{x}_y & \bf{x}_z & 0 \\ \bf{y}_x & \bf{y}_y & \bf{y}_z & 0 \\ \bf{z}_x & \bf{z}_y & \bf{z}_z & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix}\begin{bmatrix} 1 & 0 & 0 & -\bf{e}_x \\ 0 & 1 & 0 & -\bf{e}_y \\ 0 & 0 & 1 & -\bf{e}_z \\ 0 & 0 & 0 & 1 \end{bmatrix} $$ Which is exactly what you have to begin with. 

VT is still a somewhat hot topic on Computer Graphics, so there's tons of good material available, you should be able to find a lot more. If there's anything else I can add to this answer, please feel free to ask. I'm a bit rusty on the topic, haven't read much about it for the past year, but it is alway good for the memory to revisit stuff :) 

It is possible to do what you describe, but I'm afraid it is not a trivial process. Actually, this will be very tied to the Operating Systems you are targeting and possibly also requiring specific tweaks for the given game/application. I would start looking into DLL injection techniques, which should allow, for instance, intercepting calls to the Direct3D (Windows) or OpenGL APIs, which you can then use to copy de framebuffers of the application. A quick Internet search turns up this and this with detailed explanation. I once wrote a small-scale OpenGL interceptor on MacOS by injecting a custom shared module, but it was under very controlled circumstances and making a lot of assumptions, such as having root privileges in the system, so for a production tool, it would be a lot more complicated. You can find another very interesting project here about intercepting the D3D API to install an automated AI bot on Starcraft, which should touch on very similar concepts to what you are looking for. Once you manage to intercept the underlying rendering API, you could then perhaps hook your code into each or equivalent call to just copy the previous framebuffer before the swap, then save it (this is where something like would come into play). Copying and saving the framebuffer in an efficient manner is also challenging. The easy way is to just dump each frame as an uncompressed RGB image, but then you'll wind up with hundreds of gigabytes of data for just a couple of minutes of gameplay, so unless you have a nice HDD array sitting at the corner of your table , you'll need to look into compressing the frames somehow. The downside of compressing the frames now is that it takes a lot of processing, so a naive approach might turn the once interactive application you are trying to record into an interactive slide-show . So at this stage you'd have to start thinking about optimizations. Don't know of any projects out there providing a library for efficient frame capture for games, but this would certainly be something nice to have! I think one thing that might be holding such project back is, like I mentioned at the beginning, that this is a very system dependent thing, so cases of use will most likely be limited to a single platform or OS. 

Open GL and other graphics APIs support floating point formats smaller than 32 bits (e.g. see $URL$ While GPUs seem to handle these formats natively, CPUs have limited support (x86 architecture chips can convert to/from 16 bit floats using the AVX instructions). It is sometimes convenient to pack/unpack these formats on the CPU , but it is not obvious how to do so efficiently. What are the fastest techniques to convert between 32 bit floating point and the small float formats on the CPU? Are libraries or sample implementations available? 

My experience working with shader compiler stacks a few years back is that they are extremely aggressive, and I doubt you will see any perf difference, but I would suggest testing as much as you can. I would generally recommend providing the compiler (and human readers) with more information where the language allows it, marking parameters according to their usage. Treating an in-only parameter as in/out is more error prone for humans than compilers. Some detail: shaders run almost entirely using registers for variables (I worked with architectures that supported up to 256 32 bit registers) - spilling is hugely expensive. Physical registers are shared between shader invocations - think of this as threads in a HyperThread sense sharing a register pool - and if the shader can be complied to use fewer registers there's greater parallelism. The result is shader compilers work very hard to minimize the number of registers used - without spilling, of course. Thus inlining is common since it's desirable to optimize register allocation across the whole shader anyway.