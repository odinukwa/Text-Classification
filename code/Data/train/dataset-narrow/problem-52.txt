so that these are centered around the Z-axis. In order to generate an oblique projection you can use the above mentioned method which takes input the coordinates for left, right, top, bottom, near and far planes. The other way around is to build your own custom projection matrix, and load it directly. The matrix should be stored as an array 

I Recently posted this question on SO but didn't got any response so i thought to post it here since it's somewhat related to Raytracing. I am making a real time ray tracer in OpenGL using Compute Shaders for my project and was following this link as a reference. The link tells to first draw a full screen quad, then store all the individual pixel colors gotten through intersections in a texture and render the texture to the quad. However i was thinking can't we use Frame Buffer Objects to display the texture image instead of rendering the quad and save the over head? Like I save all the colors using ImageStore and GlBindImageTexture in a texture, then attach it to a FBO to display it. And since I won't be using any rendering commands I won't be causing a Feedback loop as in writing and reading the same texture? Here is the snippet 

Next is differential radiance. We can think of it as an infinitesimal quantity of radiance emitted or recieved in a very small solid angle $d\omega$. Next is Irradiance. Irradiance isn't normally associated with a direction. According to Wikipedia it's 

So thanks to joojaa I finally got a hint, and I searched on the net further and found this link which cleared all the doubts and has my answer. Though I am still posting it here as a summary. So anyone reading this and who has similar problem to mine here is what I understood. Suppose we are considering the tait-bryan angle order X-Y-Z that is rotate first along X then Y and finally Z. Also to make it clear we are rotating around the "fixed" axes since a rotation matrix always rotates around the fixed axes. The matrix doesn't know anything about the axes moving or anything. 

As you can see in the Y-Z-X order, the Y axis remains there as in 3rd picture causing the axes to coincide... 

So i searched a lot after this and I think it was my confusion on FrameBuffer Objects. I thought you could use FBO's just like a default FrameBuffer and display the texture image attached to it but you can't. It's only used for offscreen rendering. So while you can use rendering commands to draw something to a "texture image" attached to it, you can't "display the image" by making it default framebuffer or something like that. 

OpenGL (scanline rendering) is always going to be way faster than ray tracing. Ray tracing computes pixels one by one using physics (at least a good 90% of the time it's going to be real physics). Although some of it can now be done with a GPU, it is not really helping as much as one would expect. Calculating speculative lightning is not something you can easily do in parallel (ray 1 will bounce from object A to object B, ray 2 will bounce from object A to object C... you can't parallelize the computations on B and C.) The GPU helps with the matrix math, though. In comparison, OpenGL is a total fake in comparison. It renders triangles really fast with shades coming from textures and fast gradient computations from your various light sources. 

However, there is a way to retrieve the information through VESA. Any modern monitor should be able to handle that. Only I have no clue how that's done. Under Linux you have a command called which can be used to find out the info: 

I'm wondering whether some of you would have tested and seen quite a difference in using a compressed texture even when the OpenGL environment does not need saving any memory (i.e. the card has more memory than necessary to support the textures uncompressed.) I'm thinking that a compressed texture would use less RAM and thus reading it and decompressing in the GPU would be faster because the number of memory accesses is lower. Whereas, a non-compressed texture would rely more on memory access (GPU I/O) than processing and that is likely slower. The main things I've read about texture compression has been about how the compression saves space. Not so much about how it can save execution time with the GPU. (Note that I specifically will be using an nVidia, Pascal for now. But I would hope that it works similarly whatever the GPU.) 

I would suggest you look at how that tool does it and you'd get your info right at hand. From what I've read over the years, the VESA info can be wrong once in a while, but it's quite reliable now a day, I would think. 

I want to create a scene where a room includes a projector and a white screen. How is a Ray Tracing environment functioning in such a setup? Is the light from the projector going through a slide and then the color reflecting on the white screen? If so, wouldn't any light ray bounce on that screen creating a mess? 

First of all we need to understand why do we need 4x4 matrices in the first place. With 3x3, we couldn't represent translation as it wasn't a linear transformation (it displaces the origin). So in order to avoid extra work, homogeneous coordinates and affine transformation was introduced. Now instead of doing $v' = Lv + t$ where is a linear transform and is the translation, we can do $v' = Av$ Where is the affine matrix. This makes it cleaner. So 4x4 matrices are a real necessity, we just can't work without them. In order to distinguish between vectors and points we use for points and for vectors. So you are suggesting to make this 4th dimension implicit and don't store it as it'll actually use space/memory. 

In the book Computer Graphics Principles and Practice, they use the term specular reflection when they want to imagine things resembling a mirror and glossy reflection when things like a polished door knob or an orange skin. The charts shows you exactly that. When a material has more specular color, it should have less diffuse color due to the conservation of energy. That is, the sum of the light reflected specularly and light absorbed and emitted in random directions must be less than equal to 100% (the amount of light incident on the surface). Hence when you increase the specular color the material tends to go white or have a slight tint of the color like in metals. Where as glossy surfaces can have more diffuse color but show a specular highlight like the surface of an orange skin. So assuming the CGPP's point of view, we can say in pure specular reflection, the diffuse part is much less than the glossy part. Where as in glossy reflection the diffuse part is usually greater. 

I think you are confusing all the binding targets thingy. From what I see your vertex data is coming from compute shader after some processing and now you want to pass it to the vertex shader. You can create a buffer object once, use it as an SSBO for use in a compute shader then use it as a VBO for use in rendering. i.e 

If we rotate around the X-axis we can see that the plane will either roll left or right. Suppose we rotate a little around the X-axis and then rotate around so that it is pointing straight up like this 

It was getting a little big to fit in the comments so posting it as an answer instead. Might not be a solution to your problem but the concept is related. People usually forget that whenever you define a transformation matrix by placing respective basis vectors in respective columns, you are specifying that with respect to another basis usually the right-handed or left handed world coordinate space. For example your above matrix \begin{bmatrix} r_x & u_x & -l_x & 0 \\ r_y & u_y & -l_y & 0 \\ r_z & u_z & -l_z & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix} Reason why you put sign in the third column is you are working with a right handed coordinate system and the camera is looking in the negative direction (the -Z axis). So all these basis vectors are also with respect to another basis vector. Let's make this matrix simple so we can understand what happens when we multiply a matrix with a vector. Let the matrix $M$ be \begin{bmatrix} 1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & -1 & 0 \\ 0 & 0 & 0 & 1 \end{bmatrix} This is your camera initially without any rotation/translation etc. This means your camera space $+Z$ axis maps to world space $-Z$ axis. Now consider a camera space coordinate $[2,3,6,0]^T$. Multiplying this vector by the matrix gives you $[2,3,-6,0]^T$. This is the world space representation of $[2,3,6,0]^T$. So as Nathan pointed your matrix transforms from camera space to world space. To do the inverse we just take the inverse of the matrix which is the transpose if the basis is ortho-normal. One of the reasons the transformation matrix is built this way is because it's much easier. It's easier to think about the transformed basis vectors of camera or any other space with respect to the world, write the matrix then invert it to go from the world space to that target space. 

I'm working with OpenGL and facing some difficulties because I'm not familiar with OpenGL. I tried to search related example in Google, but I could not find some useful code. There are five arrays. I bound and changed the data in compute shahder as follows: 

What's worng with me? (If I changed the dvec4 in the above shader to vec4, the triangle was drawn, but maybe it's not double anymore. I want to use double in shader.) 

Hnece, we can get the $\alpha, \beta, \gamma$. If $0 \le\alpha, \beta, \gamma \le 1$ and $\alpha+ \beta + \gamma = 1$, then there is intersecton. else No intersection!. (The page 19, 20 is explaining how to calculate the 'Area'. (Actually, we can project the 3D space points to 2D space for performance, but avoid the perpendicular projection!) ) 

I'm doing a project with assimp. I got confused with the weird situation for me. I think the both code are exactly same, but the result is different. Why the codes act differently? (vertex shader) 

Why sending data from gpu to cpu is slower than cpu to gpu? I heard the relation is similar to network situation, in detail, the upload is slower than download. However, I could not understand the meaning. In my opinion, the sending operation should be same because the two sending buses (cpu to gpu and gpu to cpu) can be made with the same performance (I mean bendwith). 

Let's suppose the start point of the ray $R$ is $(x_0, y_0, z_0)$ and direction is $(x_d, y_d, z_d)$, then $R = (x_0, y_0, z_0) + (x_d, y_d, z_d)t$ , and the plane is $ax + by + cz + d = 0$. To check intersection between the ray and plane, calculate the below equation. $t = \frac{-(ax_0 + by_0 + cz_0 + d)}{ax_d + by_d + cz+d}$ If $t \ge 0$, then there is a possibility of intersection, so go second step. else there is not intersection. 

Supposing a behavior function $C(x_1, \ldots, x_n)$, then we have a scalar potential energy function $E = \frac{k_s}{2}C\cdot C$ where $k_S$ is stiffness constant. Hence, the force is as follows $f_i = -\frac{\partial E}{\partial x_i} = -k_sC\frac{\partial C}{\partial x_i}$ I caught up so far, but couldn't understand when adding damping. The paper read $f_i = (-k_sC - k_d\dot{C})\frac{\partial C}{\partial x_i}$ , where $k_d$ is damping constant and $\dot{C}$ is derivative of $C$. How to derive the damping part? I know damping equation is $f_d = -k_dv$, In detail, supposing there are two end of points in a spirng, called $p_a, p_b$. then the damping force $f_d = -k_d\frac{\dot{l}\cdot l}{|l|}\frac{l}{|l|}$ in 3D sapce ($\dot{l}$ is $v_a - v_b$, $l = p_a - p_b$) However, I could not correspond this equation to the above damping part.