The obvious answer would be Lance Fortnow's book The Golden Ticket but I can't say anything more about it, as I've not read it myself. (If somebody has read it and wants to say more, please leave a separate answer and I'll delete this one.) 

1 More formally, for every formula $\phi$, there is a deterministic algorithm and a constant $k$ such that the algorithm decides whether $\cal{A}\vDash\phi$ in time at most $|A|^k$. 

I hope this doesn't come across as picking holes in the question. The first point, as raised by dkuper, is that ordinary circuits necessarily implement total functions (every input gives either true or false), whereas Turing machines implement partial functions (inputs lead to acceptance, rejection or "no answer", i.e., non-termination). If you want to simulate a Turing machine, you're either going to need to add something to your model of circuits that allows them to compute partial functions, or implement separate circuits for, say, "The machine accepts this input", "The machine rejects this input" and "The machine doesn't halt on this input." The second point is, what does it mean to have a circuit with infinitely many gates? Any circuit with an infinite number of gates and a single output gate must have an infinite path and/or a gate with infinite in-degree. What does it mean to evaluate an infinite path? Can that be done in finite time? Is it OK to have a gate with infinite in-degree, given that such a thing couldn't possibly be realised? (OK, the infinite tape of a Turing machine can't be physically realised but it suffices to have a finite amount of tape and promise to add more whenever the head reaches the end.) Now let's look at the more specific questions. The version where the Turing machine $M$ and its input $x$ are fixed, is trivial, as are all problems with a finite number of instances (one, in this case). The circuit is the constant "true" if $M$ accepts $x$, the constant "false" if it rejects and, some circuit with no value if it doesn't halt (or whatever you decided to do to represent non-terminating computations in your circuit model). We know that this circuit exists but, of course, the function that maps $M$ and $x$ to the required circuit is uncomputable. Finally, the case where the circuit's input is (a coding of) the Turing machine and its input. You have to decide how to code that as the input to an infinite circuit. One way would be to code machine and input as a finite binary string and have the infinite sequence of inputs $x_i$ and $y_i$ ($i>0$) to the circuit such that, the circuit evaluates to false if every $y_i=0$ and, otherwise, putting $\ell=\min\{i\mid y_i=1\}$, the sequence $x_1\dots x_{\ell}$ is the binary string coding the input. Now, take the infinite set $S$ of pairs $(M_j,x_j)$ ($j>0$) such that $M_j$ is a machine that accepts input $x_j$. The required circuit is just an infinite disjunction, where the $j$th disjunct says "My input codes $(M_j,x_j)$"; again, you need to deal with non-terminating computations in some way. Alternatively, you go the more conventional route of having a family of circuits, one for each input length, as described by dkuper. 

Something has gone wrong, here. Let us start with the basic definitions. A Hamiltonian cycle is a cycle is an enumeration $v_1, \dots, v_n$ of the vertices of a graph such that there is an edge from $v_i$ to $v_{i+1}$ for $1\leq i<n$ and an edge from $v_n$ to $v_1$. In the Traveling Salesman Problem (TSP), we are given $n$ cities and, for every pair of cities, the non-negative cost of traveling between them. We want to know the minimum cost of starting at the city of our choice, visiting every other city exactly once, in the order of our choice, and returning directly to the start point. There are no restrictions on the costs: it could cost \$1 to get from London to Paris, \$1 to get from Paris to New York but \$1,000,000 to get directly from London to New York. Thus, TSP is the problem of finding a minimum-weight Hamiltonian cycle in a complete graph with arbitrary non-negative weights. The Euclidean Traveling Salesman Problem is TSP with the restriction that the cities are distinct points in Euclidean space and the cost of getting from A to B is the distance between those two points. Euclidean TSP is the problem of finding a minimum-weight Hamiltonian cycle in an arbitrary Euclidean complete graph. A Euclidean graph is an embedding of a graph in the Euclidean plane. Each vertex is a point and each edge is a straight line between its endpoints. If there is an edge between $x$ and $y$, the weight of that edge is its length. Yet a third problem is to find the minimum weight Hamiltonian cycle in a Euclidean graph, which I'll call Euclidean MinHam. You appear to be confusing Euclidean TSP and Euclidean MinHam. In Euclidean TSP, there is an edge between every pair of vertices; in Euclidean MinHam, there may or may not be an edge between every pair of vertices. This is crucially important. Your claimed algorithm for Euclidean TSP is to generate a Hamiltonian cycle and then adjust it until you get one of minimum weight. I do not believe that will give an efficient algorithm but it is possible that it will. A key point is that generating a Hamiltonian cycle is trivial because the graph you start with is a complete graph: every enumeration of the vertices is a Hamiltonian cycle. Your second claim is that you can solve Euclidean MinHam by the same method. However, to even get this started, you need to find some Hamiltonian cycle in your Euclidean graph, to begin the optimization process. This problem is already NP-hard. Your third claim is that you can find a Hamiltonian cycle in a general graph $G$ by giving every edge weight 1 and every non-edge weight 2, and then feeding the resulting graph to your Euclidean TSP algorithm. This cannot possibly work. Your Euclidean TSP algorithm is supposed to work by exploiting the geometric properties of the problem (for example, that the minimum-weight solution cannot cross itself). This property does not hold for general weighted graphs. Your transformed version of $G$ is not a Euclidean graph so, if your algorithm requires Euclidean graphs, it cannot work on $G$. 

People informally use the phrase "a logic for P" for the notion of a logic that captures P, which corresponds to Chen and Flum's definition of a "P-bounded logic for P". Essentially, this means that the logic defines exactly the polynomial-time properties of relational structures and formulae can be evaluated in polynomial time.1 Specifically, it's no use if your logic defines exactly the polytime properties but it takes exponential time to figure out whether a formula's true. I think the confusion has arisen because Chen and Flum use the phrase "a logic for P" to mean just any logic that defines exactly the polytime properties of structures, without the requirement that formulae can be evaluated in polytime. The logic $L_\leq$ is a logic for P in this specific sense. Every polytime property is definable by a formula $\phi\in L_\leq$ but the semantics of the logic requires testing whether $\phi$ is $|A|$-invariant and there is no known way to do that in polynomial time. $L_\leq$ defines all polytime properties because, by the result of Immerman and Vardi cited by Chen and Flum, LFP captures P on the class of ordered structures. More specifically, any ordering of the structure's universe lets you use tuples in $A^k$ to represent $k$-digit numbers in base $|A|$, which you can use as both for timestamps and to say "the $i$th character on the tape is $c$. You can then use a fixed-point formula that says, "If the configuration at time $t$ is $C_t$, then the configuration at time $t+1$ is $C_{t+1}$. The resulting formula doesn't depend on which ordering is chosen, so it's order-invariant, as required. Choosing a different ordering is just permuting the meaning of the digits in your base $|A|$ numbers. Note that order-invariance in general is not decidable, which is why Chen and Flum restrict to order-invariance only on structures up to the size of the one we're evaluating the formula on. That's decidable but not known to be in P. (But the formulas constructed above are, in fact, order-invariant across all structures.) 

If P=NP, there must be polynomial-time algorithms for NP-complete problems. However, there might not be any algorithm that provably solves an NP-complete problem and provably runs in polynomial time. 

A trivial answer which is close to some of what appears above but, I think, distinct. Fix any polynomial-time computable coding $f\colon\mathbb{N}^3\to\mathbb{N}$ of triples $\langle k,m,w\rangle$ as natural numbers. The set of values $f(k,m,w)$ such that the $m$th nondeterministic Turing machine accepts its $w$th input in at most $n^{\log k}$ steps (where $n$ is the length of that input) is NP-complete. ($\log k$ so that we're effectively coding $k$ in unary.) That set of values can be represented as a set of paths. 

I assume you've looked online, though I'd be amazed if a 1980s master's thesis was online. Ask one of the authors: they should have read the thesis before citing it, which suggests that at least one of them has access to a copy or, at least, had access at the time they wrote the paper. If that fails, try asking your university library. Also, although the citation doesn't mention it, be prepared for the thesis to be in Russian. 

If we didn't have the guarantee that there is at least one Hamiltonian path, the following would show NP-hardness. 

A graphical user interface that contains more than a few tens of components would be very confusing for the user. This means that, whatever data structure is used, it doesn't have to scale to large instances. The user is unlikely to generate more than a couple of mouse-clicks per second, so whatever data structure is used, it doesn't even have to be processed very fast. So it sounds like this is a problem that has already been solved as well as it needs to be solved and, in fact, that almost anything you could try would work on a modern computer. 

This is undecidable for Turing-powerful languages. Proof is by reduction from the all-inputs halting problem ("Does Turing Machine $M$ halt for all inputs?"), which is undecidable. Suppose the optimizer $O$ described in the question exists. Given a machine $M$, we can easily produce a machine $Z(M)$ that behaves identically to $M$ except that, whenever it halts, it outputs zero instead of whatever output $M$ was going to give. But, now, $M$ halts on all inputs if, and only if, $Z(M)$ outputs zero for all inputs if, and only if, $O(Z(M))$ is the program that immediately returns zero and halts. 

A calculus is just a system of reasoning. One particular calculus (well, actually two closely related calculi: the differential calculus and the integral calculus) has become so widespread that it is just known as "calculus", as if it were the only one. But, as you have observed, there are other calculi, such as the lambda calculus, mu calculus, pi calculus, propositional calculus, predicate calculus, sequent calculus and Professor Calculus. 

B. Roberts and D. P. Kroese, "Estimating the number of $s$--$t$ paths in a graph". Journal of Graph Algorithms and Applications, 11(1):195-214, 2007. L. G. Valiant, "The complexity of enumeration and reliability problems". SIAM Journal on Computing 8(3):410-421, 1979. 

You're assuming that numbers can only be represented as fractions (either literally, by having a datatype storing integer numerator and denominator, or implicitly by using some kind of floating point representation) but this isn't true. For example, you can easily represent rational complex numbers by storing the rational real and imaginary parts. Similarly, you can represent all numbers of the form $a+b\sqrt{2}$ for rational $a$, $b$, and compute exactly with them (note that this class of numbers is closed under addition, subtraction, multiplication and division). Going a little bit farther, it's not hard to represent all algebraic numbers (i.e., all numbers that are roots of polynomials with rational coefficients). And, hey, you might was well throw in your favourite transcendental (non-algebraic) constants, such as $\pi$ and $\mathrm{e}$. As to why it's important to be able to deal with irrational numbers: much of mathematics uses them and we want to be able to both do mathematics with computers, and use mathematics to analyze computation. 

— Garey and Johnson, Computers and Intractability, Freeman, 1979. Citation is to private communication with E. R. Berlekamp. 

"Select a largest/leftmost" is the usual way of phrasing this. If, in the particular situation, you feel it would help to make it explicit, you could say something like "Select a largest element from the set $X$ (which is not necessarily unique)". 

To say that Turing machines and the lambda calculus have equivalent power means that they compute/define the same class of functions, which is a pretty coarse-grained equivalence. If you want to consider efficiency, then you've moved from computability theory to complexity theory and you're looking at a finer-grained equivalence. 

I think the best way to proceed is to have a chat with your algorithms professor. Together, you can figure out exactly what you have achieved and then decide if it's something that's worth taking farther. That might mean dropping it, it might mean publishing what you have already, or it might mean extending the results and publishing something more significant. It's very hard for us to give specific advice here as it's not clear to us exactly what you've achieved. In fact it's not clear to me that you're sure exactly what you've achieved. It's possible that your algorithm can be extended to do more than you realise but, because of the questions above about how it manages to, for example, output a spanning tree in constant time, it's also possible that it does less than you think. I think you need to share your work with somebody who can help you figure out exactly what it does. 

The individual statements of the form $A\Rightarrow B$ are sequents and the things that look like fractions are inference rules. 

There is no P-vs-NP dichotomy for problems on cubic graphs because there are problems on cubic graphs of arbitrarily high complexity. We can code arbitrary binary strings as cubic graphs, thus translating any question about binary strings into a question about cubic graphs. For $k\geq 5$, let $W_k$ be the graph consisting of two disjoint $k$-cycles $x_1\dots x_k x_1$ and $y_1\dots y_k y_1$, along with the matching $x_2y_2, \dots, x_ky_k$. (Thus, every vertex except $x_1$ and $x_2$ has degree 3.) Given an arbitrary binary string $s=s_1\dots s_\ell$, let $H_1 \dots H_{\ell+4}$ be the following sequence of graphs. For $1\leq i\leq \ell$, let $H_i = W_5$ if $s_i=0$ and $H_i = W_7$ if $s_i=1$. Let $H_{\ell+1}=H_{\ell+4}=W_9$, $H_{\ell+2} = W_5$ and $H_{\ell+3} = W_7$. Now, let $G_s$ be the graph made from the sequence of $H$'s by adding an edge between a degree-2 vertex in $H_i$ and one in $H_{i+1}$ ($i<\ell+4$) and likewise between $H_{\ell+4}$ and $H_1$, thus making a connected, cubic graph. (The purpose of $H_{\ell+1}, \dots, H_{\ell+4}$ is just to establish the direction of the cycle.) Now, take any language $\mathcal{L}\subseteq \{0,1\}^*$ – it doesn't even have to be decidable. $\mathcal{G} = \{G_s\mid s\in\mathcal{L}\}$ is a subset of the cubic graphs and determining membership of $\mathcal{G}$ has the same complexity as determining membership of $\mathcal{L}$. 

No, there is no such theorem. There cannot be such a theorem because a theorem is a formal, proven statement of mathematics and "natural interesting property" is not a formal property and, therefore, nothing can possibly be proven about it. 

This statement is incorrect. Consider the programming language in which the semantics of every string is "Ignore your input and halt immediately". This programming language is not Turing complete but every program is an interpreter for the language. 

"Ear decomposition" is not a term I've heard before but it's incorrectly defined, as you've observed, since it doesn't require that the graph be connected, let-alone 2-connected. An "ear" needs to be defined as having each endpoint adjacent to at least one further vertex of $G$. (Actually, each endpoint would have to be adjacent to at least two further vertices, or the path wouldn't be maximal.) With the correct definition, the theorem is true. 

Write $h(G)$ for the set of edges of the digraph $G$ that appear in every Hamiltonian path. The key fact is the following. Lemma. For a digraph $G$, $h(G)=E(G)$ if, and only if, $G$ is a directed path or $G$ has no Hamiltonian path. Proof. If $G$ is a directed path, then clearly $h(G)=E(G)$; if $G$ has no Hamiltonian path then, for any edge $e\in G$ there is no Hamiltonian path that does not contain $e$ so $e\in h(G)$. Conversely, suppose that $h(G)=E(G)$. If $G$ is a directed path, then we are done. If not, then any Hamiltonian path in $G$ would have to be the whole of $G$, which is impossible, since $G$ is not a directed path. Therefore, $G$ has no Hamiltonian path.    $\Box$ As a result, it is NP-hard to compute $h(G)$ since, given $h(G)$, we can determine whether an input graph $G$ has a Hamiltonian path by checking whether $G$ is a path and whether $h(G)=E(G)$. Thus, there is (probably) no polynomial-time algorithm to compute $h(G)$.