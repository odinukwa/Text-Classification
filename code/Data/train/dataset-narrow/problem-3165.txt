For binary classification you can use as the output layer but you should consider that if you do so, your last layer have to have two neurons, each corresponds to one specific class. Moreover, you have to use as the loss function. Change the following code: 

At coursera, the homework of third week of convolutional networks by professor Andrew Ng, is about this. I recommend you to see that homework. It also implements the paper. I can't add the code here, but the architecture of the network is as follows: 

You have to use score. A simple solution for that is to use confusion matrix. The way you can find score for each class is simple. your true labels for each class can be considered as true predictions and the rest which are classified wrongly as the other classes should be added to specify the number of false predictions. For each class, you can find the score. For more details take a look at F1-score per class for multi-class classification. You can take a look at this implementation. 

val_loss is the value of cost function for your cross validation data and loss is the value of cost function for your training data. On validation data, neurons using drop out do not drop random neurons. The reason is that during training we use drop out in order to add some noise for avoiding over-fitting. During calculating cross validation, we are in recall phase and not in training phase. We use all the capabilities of the network. Thanks to one of our dear friends, I quote and explain the contents from here which are definitely useful. validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling. validation_data: tuple (x_val, y_val) or tuple (x_val, y_val, val_sample_weights) on which to evaluate the loss and any model metrics at the end of each epoch. The model will not be trained on this data. This will override validation_split. As you can see 

Personally, I don't use vagrant with a local provisioning. I have installed a Spark cluster locally without HDFS which allows me to experiment and develop easily without the overwhelm of a virtual machine. HDFS is not a requirement for local clusters, and it's also a kind of a system administration nightmare if you just need it for local testing. Spark works very fine with local file systems which you'll also have to port when deploy on your Cloud of course. Nevertheless, You can use vagrant with AWS provisioning to create a cluster for heavier testing. Note: AWS works with S3 and not HDFS. AWS's HDFS version is ephemeral and if you put down the cluster, you'll loose all your computations. For persistence, you'll need to write back to S3. 

Now, concerning the usual flow of building and validating such models, your code doesn't deal with that. It's usually done through quality measures variations. It can also be done through Feature extraction techniques. With such techniques, you should consider the following. For each association rule, you'll need to measure the improvements in accuracy that a commonly used predictor can obtain from an additional feature, constructed according to the exceptions to the rule. In other terms, you'll have to select a reference set of rules that should help your model perform better. I strongly advice you to read this paper about the topic. So now what does that mean ? This means that you'll need to implement that pipeline yourself because it's not implemented in Spark yet. I hope that this answers your question. 

When you have 32 feature maps with height and width equal to 100 and the depth of each equal to one it means that you have 32 planes, a common jargon among vision people, with 100 by 100 entries. You can set the height and width of the next layer and they can be arbitrary. You can also set the number of feature maps but the depth of each feature map would be equal to the number of feature maps of the previous layer. So it should be 16 * 9 * 9 * 32 if you set height and width equal to 9 and the number of feature maps to 16. As you can see in 16 * 9 * 9 * 32, 32 is located at the end, this is called channels last. You can not set the depth, because it should be equal to the number of channels, features maps, of the previous layer. 16 * 9 * 9 * 32 means that you have 16 feature maps of dimension 9 * 9 * 32, so the output of each feature map would be the member-wise product of all the outputs of the feature maps of the previous layer and each of 9 * 9 * 32 kernels. Consequently the result would be 16 planes. I highly recommend you taking a look at here. 

Suppose that I have an input image with 224 * 224 * 3 dimensions. I pass it through a convolution layer with 64 filters and same convolution operation. Suppose that I want to reconstruct the original image using the output of the convolution layer which has the size 224 * 224 * 64. I have the following questions: 

OPTICS gets rid of $\varepsilon$, you might want to have a look at it. Especially the reachability plot is a way to visualize what good choices of $\varepsilon$ in DBSCAN might be. Wikipedia (article) illustrates it pretty well. The image on the top left shows the data points, the image on the bottom left is the reachability plot: 

"online" as in "in the internet". "online" as in "online algorithms" which get input piece-by-piece and have to make a decision before knowing all of the input (e.g. when you have severe time constraints or when you can't store all the data like in CERN where you have to throw most of the data away right after you get it) "online" in handwriting recognition in contrast to the offline OCR methods (see On-line Recognition of Handwritten Mathematical Symbols if you're interested). Online methods use the way how symbols were written, offline methods only use the image. 

Hence the cross entropy loss dropped (as expected), the probability for the correct class increased (as expecte) but it makes a mistake (if you simply take the argmax). 

both of them might have the advantage, that the filter does not detect a border, where no border is. 

However, please keep in mind that the problem might not be your implementation, but rather the network architecture / hyperparameters such as the number of epochs you're training or the training data. Also, very important: I doubt that you will get good results for $y = x^2$, except if $x$ is restricted to $[-1,1]$ or something similar simple. Please keep the domain of your output layer in mind. 

If you want to understand how spark internals work, I suggest that you watch the presentation made by Databricks about the topics 

EDIT: To avoid confusion for some concerning PCA and Dimension Reduction, I add the following details : PCA will allow you compute the principal components of your vector model, so the information are not lost but "synthesized". Unfortunately there is no other imaginable way to display 39 dimensions on a 2/3 dimension screen. If you wish to analyze correlations between your 39 features, maybe you should consider another visualization technique. I would recommend a scatter plot matrix in this case. 

You can drop the i3 recommendation since you don't consider it good enough. Nevertheless, the parameter of your recommendation engine must be determined of course with the help of your evaluation metrics. Considering software solutions, I use Apache Spark MLlib with Scala as a base for my recommendation engine algorithms where you can compute item cosine similarity easily per example where you are using in-house implementation or an approximation with the DIMSUM algorithm. I hope this helps! 

Now all we have to do is a simple group by and perform a collect_list aggregation on the first dataframe : 

You can also write it using a SQL dialect by registering the DataFrame as a temp table and then query on it use the SQLContext or HiveContext : 

You can also set a threshold where you can drop a recommendation at a certain limit of your prediction level. E.g. Let's say an user alpha has the following recommendations with a threshold of 0.7 

Reading the Zeiler&Fergus paper (my summary), I wonder how exactly they trained the deconv net. What was their data? I think for one CNN which they want to analyze, they train exactly one deconv net (in contrast to training one deconv net per layer). The featuers (inputs) of the deconv net are the activations of the layer they want to analyze. The output they train them on are the activations that actually was the input of the layer they want to analyze. So although they have one deconv-net in total, they train it layer-wise. So for each training run, the weights of only one deconv layer are adjusted. However, I wonder why the images look that unrealistic: 

I know that neural networks can deal pretty well with labeling errors in classification problems. Meaning if you have a large dataset and a couple of examples have the wrong label, they get basically ignored. But for this kind of problem I'm not too sure. A first experiment indicates that they do smooth values. Are there choices in architecture / training which help the smoothing / averaging / removal of noise? What I tried I created a network which can solve this kind of regression problem without noise. It gets a MSE of about . When I add a bit of noise to the training set only, I get an MSE of : 

I've tried to find it in the only resource they referenced in this context (D.L. Reilly, L.N. Cooper, C. Elbaum: "A Neural Model for Category Learning"), but sadly I don't have access to that one. I found an explanation on $URL$ 

We will use implicits now to convert our data into a DataFrame after converting it to Transactions : 

Scan based operations are basically all the operations that require evaluating the predicate on an RDD. In other terms, each time your create an RDD or a DataFrame in which you need to compute a predicate like performing a filter, map on a case class, per example, or even explain method will be considered as a scan based operation. To be more clear, let's review the definition of a predicate. A predicate or a functional predicate is a logical symbol that may be applied to an object term to produce another object term. Functional predicates are also sometimes called mappings, but that term can have other meanings as well. Example : 

Like @SeanOwen pointed out its called . spark.mllib’s FP-growth implementation takes it as a hyper-parameter under . It is the minimum support for an itemset to be identified as frequent, e.g : if an item appears 4 out of 5 transactions, it has a support of 4/5=0.8. Usage: 

You are probably thinking in terms of regular SQL but spark sql is a bit different. You'll need to group by field before performing your aggregation. Thus the following, you can write your query as followed : 

There might be different ways to do that, like considering implicit ratings like views or clicks. But basically, you can consider a rating of 1.0 for each user-item pair you have. This way, your prediction will be between 0 and 1 which you can consider similar to a click prediction probability. 

Yes, that is correct. For example, think of a polynomial $a_n x^n + a_{n-1} x^{n-1} + \dots + a_2 x^2 + a_1 x^1 + a_0 x^0$ which should fit 100 data points $(x_, y_i)$ where all $y_i$ were generated by one polynomial with some noise. Of course, you could always perfectly fit the model to the data making the MSE error $$MSE = \sum_{(x_i, y_i)} (y_i - \text{predict}_{\text{Model}}(x_i))^2$$ be 0. But considering that there is noise, you might actually prefer a "simpler" model. One way to think about simplicity is having weights $a_i$ which are smaller. This is done by adding a regularization term to the error. One common regularization is $L_1$ regularization ($+ \sum_{1}^{100} | a_i |$), another one is $L_2$ regulariazion ($+ \sum_{1}^{100} a_i^2$). 

For my answers, I assume you are talking about batch (not mini-batch or stochastic) gradient descent. 

After downloading the imagenet urls (link), I see that it is a single 1.1 GB text file which starts like this: 

I've understood that SVMs are binary, linear classifiers (without the kernel trick). They have training data $(x_i, y_i)$ where $x_i$ is a vector and $y_i \in \{-1, 1\}$ is the class. As they are binary, linear classifiers the task is to find a hyperplane which separates the data points with the label $-1$ from the data points with the label $+1$. Assume for now, that the data points are linearly separable and we don't need slack variables. Now I've read that the training problem is now the following optimization problem: 

It's not important to adjust to the opponent, as Go is only about winning or losing. There is no bigger reward the faster / the more obvious it wins. Or to put it different: only the current board situation is important in a min-Max setting (although the value approximation of a state admittedly depends a bit on the opponent) 

You are actually creating and Frequent Pattern model without generating candidates. So actually you'll need to generate afterwards if you need to use them with : 

FP-growth is a frequent pattern association rule learning algorithm. Thus it's a rule based machine learning algorithm. When you call the following : 

I hope that this answers your question. Note: If you wish to know what's the difference between RDD and DataFrames, I advice you to read Databrick's blog entry about it here. 

Since you are using Spark 1.6, I'd rather do these kind of transformations with DataFrame as it's much easier to manipulate. You'll need to use SQLContext implicits for this : 

Like I said in the comment, you'll need to perform dimension reduction, otherwise you'll not be able to visualize the $\mathbb{R}^n$ vector space and this is why : Visualization of high-dimensional data sets is one of the traditional applications of dimensionality reduction methods such as PCA (Principal components analysis). In high-dimensional data, such as experimental data where each dimension corresponds to a different measured variable, dependencies between different dimensions often restrict the data points to a manifold whose dimensionality is much lower than the dimensionality of the data space. Many methods are designed for manifold learning, that is, to find and unfold the lower-dimensional manifold. There has been a research boom in manifold learning since 2000, and there now exist many methods that are known to unfold at least certain kinds of manifolds successfully. One of the most used methods for dimension reduction is called PCA or Principal component analysis. PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. You can read more on this topics here. So once you reduce your high dimensional space into a ${\mathbb{R}^3}$ or ${\mathbb{R}^2}$ space you will able to project it using your adequate visualization method. References :