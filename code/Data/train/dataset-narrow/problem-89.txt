Is there any difference between registers types in hlsl? For example when using register for will it be somehow precache like textures? According to microsoft's presentation about resource in DX12 and hlsl 5.1 this registers mean nothing, but maybe they only mean that there isn't any real registers like on CPU? 

What is the best way to draw unlit objects with deferred? Like skybox, particles or just glowing objects? I thought about mask in alpha channel, check it in light pass and just set diffuse color but it won't work with light blending. My second idea was stencil buffer with pass after light pass that only fill unlit space. But switching pso (dx12) only for color copy doesn't sound like a good idea. And of course pass only for unlit objects after light pass, but I don't want to create another list only for this (and again, i have to switch pso). 

What are ways to render high resolution screenshot from application? Problem is I have to capture screenshot in higher resolution that it is present on screen. Resolution can be higher than 4K so making one big render target isn't option also rendering parts of scene and merging them will have artifacts in effects like or . Are there any other options to do this? Rendering is done with classic triangle rasterisation and it have to stay this way. //EDIT Screenshots are from game and saved in . Can't render to higher resolution because no everyone have enough (it can be higer than 7680x4320) I use standard with . 

With an high threshold you will find coarser edges and you might miss some, conversely, with a low threshold you might detect false edges. You have to experiment to find the threshold that better suits your needs. The reason why these functions work is worth mentioning but I don't have time for it now, I am likely to update this answer later on :) Screen space post-process You could go fancier than this, now the field of Edge detection in image processing is immense. I could cite you tens of good ways to detect edge detection according to your needs, but let's keep it simple for now, if you are interested I can cite you more options! So the idea would be similar to the one above, with the difference that you could look at a wider neighbourhood and use a set of weights on sorrounding samples if you want. Typically, you run a convolution over your image with a kernel that gives you as a result a good gradient info. A very common choice is the Sobel kernel                                    Which respectively give you gradients in x and y directions:                                    You can get the single value out of the gradient as $ GradientMagnitude = \sqrt{ (Gradient_x) ^ 2 + (Gradient_y) ^ 2 } $ Then you can threshold as the same way I mentioned above. This kernel as you can see give more weight to the central pixel, so effectively is computing the gradient + a bit of smoothing which traditionally helps (often the image is gaussian blurred to eliminate small edges). The above works quite well, but if you don't like the smoothing you can use the Prewitt kernels:                                                     (Note I am in a rush, will write proper formatted text instead of images soon! ) Really there are plenty more kernels and techniques to find edge detection in an image process-y way rather than real time graphics, so I have excluded more convoluted (pun not intended) methods as probably you'd be just fine with dFdx/y functions. 

Feel free to point out any mistakes I made, even if not related to the issue. I've tested the materials in Unreal, and they looked correct. I doubt this is solely due to the lack of Image-Based Lighting. I also doubt this is an issue with using point lighting. Additionally, if I turn down the roughness by a lot, it becomes too dark a high roughness to be the issue. 

I'm having problems with Ambient Occlusion. I've tried to follow both John Chapman's improvements over the Crytek AO and LearnOpenGL's tutorial of it when I failed to properly obtain the correct result. $URL$ As the camera turns or moves, the screen flashes consistently though it seems arbitrary. Can anyone tell me what I'm doing wrong? 

Gloss and Specularity are features of the surface. In modern PBR terms we usually refer to the smoothness and metalness of a surface instead (unless you use a specular workflow, then metalness is still linked to specular). Basically, dielectric surfaces reflect around 4% of light in a specular way, and the rest is diffuse. Metallic surfaces reflect no light diffused, and all specular. Specular light bounces directly back to the eye and shows clear reflections of the environment more. Diffuse bounces around a lot more and therefore gives a more matte result. Gloss is another term for smoothness of a surface. When a surface is rougher its specular light is bounced all over, and so the reflection appears more blurry. TL;DR: Specular is how shiny it is and usually is higher with metals. Gloss is how clear the reflections are. Keep in mind that there are some surfaces that don't fall into the metalness workflow, especially when they have a coat on top. There are also materials in between metal and dielectric (semiconductors / metalloids). 

After this is the time of applying the actual lighting. During the lighting pass for each light you want to draw a light volume that depends on the type of light: 

In the pixel shader of this pass you pass your GBuffers and perform your lighting and shading using the information in them. This way you process only the pixels affected by each of the lights having a sensible speed-up if compared to the classical forward rendering. It has also various disadvantages, most notably the handling of transparent objects and higher consumption of bandwidth and video memory. But also it is trickier to handle various models for materials. You have other side-advantages (as having lots of info ready for post-processing) and is also pretty easy to implement. But this is not the coolest thing around for lots of lights anymore. Newer techniques are for example Tiled rendering ones. The main idea of those is to subdivide the scene in screen space "tiles" and assign to each tile the lights affecting it. This exists both in a deferred and forward fashion. These techniques lead to some problems when you have various depth discontinuities in a tile, but is generally faster than the classical deferred and it solves various problems of it. For example, among the advantages, with tiled deferred you read the GBuffers once per lit fragment and pixels in the same tile coherently process the same lights. Further evolution on this side is the Clustered shading which is conceptually similar to the tiled based approaches, having instead of screen space tiles, clusters with a 3D extent. This method handles better the depth discontinuities problem and generally performs better than the tiled methods. IMPORTANT NOTE: I have described the basics of the deferred shading. There are multiple variations, optimizations and improvements around so I urge you to experiment with a simple version and then do some researches on other techniques such the one I mentioned above. 

I'm still working on my BRDF equations. Theoretically, all equations are correct, but the hilights are super sharp. I'm not sure if this is a result of me not using Image-Based Lighting (I currently use just an env map with a LOD function). I'm using materials from here: $URL$ - I use Specular/Gloss but the materials use the fragment shader to convert it. 

I'm trying to condense my Deferred Rendering G-Buffer. So I have some questions about getting 2-component Screenspace Normals. I know Frostbite and Killzone (the only two AAA company's G-Buffers I could find) use them. How are screenspace normals created, and is this step before or after using normal maps or bump maps? If it's done before using normal maps, how are normal maps going to be affected by the screenspace-ness of the normals, and if after, how can you justify using a Model-View matrix on all fragments rather on vertices? Isn't that a lot more calculations? Finally, how are they unpacked? I realize you can get the blue component using pythagoras, but how are they returned to world space? 

Ambient Occlusion is a physical approximation to occlude ambient light. If ambient light is not considered post-processing, why is AO? Ambient occlusion has the same inputs as regular deferred lighting, plus kernels and a noise map, but essentially, it can be handled the same way. Is there a reason why it's considered post-processing and deferred lighting/shading isn't? 

There are several techniques used. A simple, but limited, post-process approach that is not really used any more consists in reconstructing the world space position of a pixel using both the view projection matrix from current and previous frame. Using these two values you can compute the velocity at a pixel and blur accordingly, sampling along the velocity vector with how many samples you want ( the more sample the blurrier, but the more expensive as well). This method unfortunately takes into account just the camera movement and therefore is not accurate enough if you have a dynamic scene with fast moving objects. As you mentioned, other more modern techniques use a velocity buffer. This velocity buffer can be created during the base pass in the deferred pipeline transforming each vertex using the world-view-projection matrix of the previous frame and then computing in the pixel shader the velocity vector using the screen space position of both frames. With this velocity buffer you can, in a post-process step, sample across the per-pixel velocity direction like in the approach I described above. The motion blur that you will get is obviously not camera-only; however you get an overhead in terms of memory (you need to keep an extra buffer) and time (you need to render additional info per pixel). One technique based on velocity buffer that I particularly like is McGuire et al. that IIRC, is used with some variations in Unreal Engine 4 and many other games.