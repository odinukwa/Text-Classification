This made me wonder about non-relativizing results in computability. All results I know from computability theory do relativize to computation with oracles. Are there results in computability theory that do not relativize? I.e. results which hold for computability but do not hold for computability relative to some oracle? By result I mean a known theorem in computability theory, not some cooked up statement. If the notion of relativization doesn't make sense for the result then it is not what I am looking for. It is also interesting to know if the result can be stated in the language of Synthetic Computability Theory or not. 

I was reading Andrej Bauer's paper First Steps in Synthetic Computability Theory. In the conclusion he notes that 

ETH states that SAT cannot be solved in the worst case in subexponential time. What about average case? Are there natural problems in NP that are conjectured to be exponentially hard in the average case? Take average case to mean average running time with uniform distribution on the inputs. 

How hard is this problem? If we choose $a,b,p,m$ randomly but "wisely" will $f$ be a pseudorandom number generator secure against TC0 or AC0 circuits? What is the smallest value of $k$ where $Q$ can be solved? What is the fastest known algorithm for $Q$? 

Recently I have seen several articles on arxiv that refer to a proof system called sum-of-squares. Can someone explain what is a sum-of-squares proof and why such proofs are important/interesting? How are they related to other algebraic proof systems? Are they some kind of dual to Lassere? 

Stephen Smale claims in Mathematical Problems for the Next Century that $$NP \not\subseteq BPP \implies NP_{\mathbb{C}} \not\subseteq P_{\mathbb{C}}.$$ Can someone sketch the argument or provide a reference? Is there any similar result in the reverse direction? $NP_{\mathbb{C}}$ (definition) and $P_{\mathbb{C}}$ (definition) are NP and P over complex numbers $\mathbb{C}$ using the Blum–Shub–Smale machine model. 

Manuel Blum is a well-known theoretical computer scientist and a Turing award winner. But more interestingly, he has the highest number of students who have gone on to win a Turing award (Leonard Adleman, Shafi Goldwasser, Silvio Micali) in the whole computer science. The list of his students is amazing and even more so if we include the students of his students. Can anyone comment on Manuel's supervisory style? What makes him so successful in training exceptional researchers? Anything that can help other supervisors be more successful in training exceptional researchers? 

Is there a down to earth explanation of what Lenstra is doing? What is the space requirement? What is a good reference for this type of mixed integer linear program? 

Can we find $X$ in $O(n_1^{cn_1}((n_2+1)m)^cL)$ arithmetic operations on $O(L^c)$ bit words where non-negative $c$ is fixed and $L$ is number of bits needed in any entry of $A$ or $B$? 

Given $f(x_1,\dots,x_n)\in\Bbb Q[x_1,\dots,x_n]$ of form $\prod_{i=1}^df_i(x_1,\dots,x_n)$ where each of $f,f_i$ are homogeneous and each $f_i$ is irreducible what is the best technique to factor such polynomials? Assume $GCD$ of coefficients is $1$ after removing denominator. 

We know that for every counting problem $\#A$ in $\#P$, there is a probabilistic algorithm $\mathcal C$ that on input $x$, computes with high probability a value $v$ such that $$(1 − ε)\#A(x) ≤ v ≤ (1 + ε)\#A(x)$$ in time polynomial in $|x|$ and within $\frac1ε$ multiplicative factor, using an oracle for $NP$. That is $\#P$ can be approximated in $BPP^{NP}$. 

I studied Lenstra's paper $URL$ I have no clue what complexity he provides on Mixed Integer Programming (it is too terse and it is not a stand alone paper as he assumes arguments of Khaichayan and Von zur Gathen and Sieveking). However I understand from the line '..we indicate an algorithm for the solution of this problem that is polynomial for any fixed value of $n$, the number of integer variables' that the complexity is polynomial if number of integer variables is fixed. I am interested in complexity with fixed number of integer variables and polynomial number of real variables. Suppose we have $A\in\Bbb Z^{m\times(n_1+n_2)}$ and $B\in\Bbb Z^m$ and asked to find $X\in\Bbb Z^{n_1}\times \Bbb R^{n_2}$ in $AX\leq B$ then what is the complexity with with which we can find $X$? 

The above scaling is consistent with case $n_1=0$ (Real Linear Programming) or $n_2=0$ (Integer Linear Programming). 

What does norm $1$ mean in $btt(1)$? Is there illustrative examples that help understand from hierarchy between $PP$ and $PSPACE$ the different reductions? Which is the standard reduction existence or non-existence that we care when we prove class inclusions or should prove to separate classes? 

We know $\#P\subseteq {PPAD}\implies PH\subseteq P^{{PPAD}}\subseteq P^{{NP}}$ and the polynomial hierarchy collapses ($FP^{PPAD}=PPAD$ following Emil Jerabek's comment). 

Razborov proved that the monotone function matching is not in mP. But can we compute matching using a polynomial size circuit with a few negations? Is there a P/poly circuit with $O(n^\epsilon)$ negations that computes matching? What is the trade-off between the number of negations and the size for matching? 

I am trying to understand the bitcoin protocol in the context of computational cryptographic security. The question is a reference request to foundations of cryptography articles on bitcoin. My first question is what abstract cryptographic protocol bitcoin is trying to implement? Do we have a definition of electronic money/digital currency in cryptography that captures bitcoin? What are the security requirements for a secure electronic money? If the answer is yes, companies like eBay provide a centralized mean of electronic money transfer. Does considering a decentralized electronic money changes the definition of abstract cryptographic protocol for electronic money? Or is it just the same concept but in a model where there is no trusted third party? Can the adversary break the protocol if it has more computational power than the combined computational power of other (honest) parties? Assume that we have $n$ parties $P_i$ for $1 \leq i \leq n$ plus an adversary $A$ networked and the adversary wants to break the bitcoin protocol. For simplicity let's assume that the network graph is $K_{n+1}$ and adversary does not control the network and simply is a party like others. What would be the exact mathematical claim about the security of the protocol in this simple case? 

Is it true that for all $n$ there are $n$ pairwise nonhomomorphic graphs with $poly(n)$ vertices? Is there a polynomial time algorithm for constructing such families of graphs? 

Lynch's Distributed Algorithms book is a classic but it is from 1996 and rather out of date. Are there any recent distributed computing books that can be used as textbooks for a graduate distributed computing and algorithms course? 

I am looking for examples of results which go against people's intuition for a general audience talk. Results which if asked from non-experts "what does your intuition tell you?", almost all would get it wrong. Results' statement should be easily explainable to undergraduates in cs/math. I am mainly looking for results in computer science. What are the most counterintuitive/unexpected results (of general interest) in your area? 

Suppose there is a very long string $S\in \Sigma^N$ with length $N$, where $\Sigma$ is a relatively small alphabet (for example, $\Sigma=\{'a', 'b', \ldots, 'z'\}$). Now, given a budget $B$, the goal is to find the solution of following problem: $$ \begin{align*} \min_{l\in\mathbb{N}} &\quad l \\ \text{s.t. }& \max_{s\subset S, |s|=l} \ \text{freq}(s) < B \end{align*} $$ i.e., I want to find a length $l_c$, such that for "any" substring $s\subset S$ with length $|s|>l$, it occurs no more than $B$ times. Assume every character in $S$ obeys the same known distribution $D$: $S[i]\sim D, \forall i\in[N]$. Here we can allow some violations in the constraint, only requiring $$ \Pr[\max_{s\subset S, |s|=l} \ \text{freq}(s) < B] > 1-\delta. $$ Currently I'm using a brute force approach, testing each $l$ from small to large, where for each $l$ I scan through the whole string $S$ to determine if there is one substring occur too often. This is very inefficient since $|S|$ is huge. So I want to know if there exists some sub-linear (w.r.t. $N$) algorithm that can estimate the maximum frequency of substrings with fixed length. 

The result relies on the assumption that the payment rule is given in the previous form. Why does it must be in that form? The proof feels like some circular reasoning. Is this because that we are restricted on searching only for DSIC mechanism? Furthermore, sometimes there exists no monotone allocation rule to optimize the deduced virtual welfare, but does this also mean that there exists no monotone rule to optimize the original profit? 

I am learning algorithmic game theory with the lecture notes posted by Tim Roughgarden. In lecture 5 it is proved that the problem of revenue (or profit) maximization in single-parameter environment is equivalent to maximizing something called "virtual welfare". I find it hard to understand the logic of the proof. The very first step of the proof assumes that the payment rule is exactly in the form given by Myerson's lemma: $$p_i(b_i,\mathbf{b}_{-i})=\int_0^{b_i}z\cdot x'_i(z,\mathbf{b}_i)\mathrm{dz}$$ where $x'_i(z,\mathbf{b}_i)$ is the derivative of the allocation rule $x'_i(z,\mathbf{b}_i)$. Then substitute the formula of payment into the profit object $$\mathbb{E}\left[\sum_{i=1}^n\mathbf{p}(\mathbf{v})\right]$$ with some calculus the "virtual-welfare" magically shows up. Then the author claims that maximizing the virtual welfare is equivalent to maximizing revenue, and if the corresponding allocation rule is monotone then we get a optimal DSIC mechanism. I have two questions about this: