This is not possible in general. The property you give for the relation between and asserts that there is an injection from into . Hence if is a larger type than (e.g., ) then no such embedding can exist. 

How are fixed point theorems stated in constructive analysis? Also, are there any concise references to constructive metric spaces? 

It's actually surprisingly hard to relate SMT to type-theoretic/categorical approaches to logic -- the results in this area are very recent! This is because categorical logic is primarily conceived of as a semantics of proofs, and SMT arises from the model-theoretic view of logic, which is a semantics of provability. A formula in SMT is a boolean combination of primitive assertions over some theory. For example, for linear arithmetic you might have the following terms and primitive formulas. $$ \newcommand{\bnfalt}{\;\;|\;\;} \array{ \mbox{Terms} & t & ::= & x \bnfalt n \bnfalt t + t \bnfalt -t \\ \mbox{Primitive Formulas} & p & ::= & t \leq t \bnfalt t = t \bnfalt t < t \\ \mbox{SMT Formulas} & \phi & ::= & p \bnfalt \top \bnfalt \phi \land \phi' \bnfalt \lnot \phi \\ }$$ Given a formula $\phi$, an SMT solver will either give you a satisfying assignment for the variables -- i.e., it will give you a number for each variable, such that substituting that number for each variable gives you a true boolean formula -- or it will tell you that the formula $\phi$ is unsatisfiable. The fact that this is all happening in classical logic is quite important; the architecture of SMT solvers relies upon this fact. Basically, SMT solvers are built as a combination of an DPLL-based SAT solver, which communicates with a theory solver, which deduces equalities from pure conjunctions of theory formulas. There are also various conditions under which you can combine solvers for different theories, which is important because it gives researchers a good "API" for figuring out if particular theories will fit together nicely. A good reference to the standard view of SMT is Clark Barrett and Cesare Tinelli's handbook chapter on Satisfiability Modulo Theories. Work on how to interpret SMT in terms of proof theory is much newer. The basic idea is to relate DPLL(T) with "focused" or "polarized" calculi for proof search. Focused calculi are variants of the sequent calculus originally designed to optimize proof search, but which turned out to work because they expose fundamental ideas in proof theory. Two good references to this are Mahfuza Farooque's PhD thesis, Automated reasoning techniques as proof-search in sequent calculus, in which she relates the actions of a DPLL(T) algorithm to search in a particular focused calculus, and StÃ©phane Graham-Lengrand's habilitation thesis Polarities & Focussing: a journey from Realisability to Automated Reasoning, in which he gives a broader perspective on this work (eg, by connecting it to work on biorthogonality). 

Showing that from two cut-free derivations $\Gamma \vdash A$ and $\Gamma, A \vdash C$ you can produce a cut-free derivation of $\Gamma \vdash C$ is called cut admissibility. Admissibility of cut is actually equivalent to showing that cut can be eliminated. If you have cut-elimination, you just cut the two derivations together and then eliminate the cut. On the other hand, if you have cut-admissibility, you can use it to recursively eliminate all the cuts, starting at the leaves and working down to the root. A good description of this proof, for a variety of logics, can be found in Frank Pfenning's 1995 LICS paper, Structural Cut Elimination. 

Defunctionalization is a program transformation that converts higher-order programs into first-order programs. The idea is that given a program, there are only finitely many lambda-abstractions, so you can replace each lambda with an id, and each function application with a call to an apply procedure which branches on that id. This is sometimes used in compilers for functional languages, but its applicability is limited by the fact that defunctionalization is a whole-program transformation (you must statically know all of the functions in the program), and so only whole-program compilers make use of it. However, Pottier and Gauthier have a given a polymorphic typed defunctionalization algorithm using a more sophisticated typing involving GADTs. Now, given their encoding, it's possible to add a catch-all case to their lambda datatype that isn't a tag, but which contains a higher-order function. This means that it should be possible to use their encoding to defunctionalize on a module-by-module basis. Has anyone done this, and point me to a compiler using this idea? (Toy compilers are okay, and in fact preferred.) 

As it happens, I'm writing a paper about this now. IMO, a good way to think about futures or promises is in terms of the Curry-Howard correspondence for temporal logic. Basically, the idea behind futures is that it is a data structure representing a computation that is in progress, and upon which you can synchronize. In terms of temporal logic, this is the eventually operator $\Diamond A$. This has a monadic structure: $$ \begin{array}{lcl} \mathrm{return} & : & A \to \Diamond A \\ \mathrm{bind} & : & (A \to \Diamond{B}) \to \Diamond{A} \to \Diamond{B} \end{array} $$ in which the $\mathrm{return}$ operation spawns a process that immediately returns its argument, and $\mathrm{bind}$ creates a new process which waits for $a$'s value, applies $f$ to that value, and then waits for the $B$-value before returning. The Promises/A proposal for CommonJS calls the monadic bind operation , and Scala 2.10 just gives it the standard monadic interface. The dual to the eventually operator $\Diamond{A}$ is the always operator $\Box{A}$ of temporal logic, which says that at every instant, you get an $A$. When you pass from a Kripke semantics of temporal logic (where you just model provability) to a categorial semantics of a $\lambda$-calculus (where you model lambda-terms/proofs also), it turns out there are actually multiple ways to do this. The simplest thing you can do is to take $\Box{A} \triangleq A$, on the grounds that once you have an $A$, you always have it. This works, but is kind of boring, IMO. :) The most natural (IMO) thing to do is to take $\Box{A} \triangleq \mathsf{Stream}\;A$, which permits you to get a (potentially different) $A$ at each instant. Then, you can see the comonadic style of functional reactive programming (FRP) (first proposed by Tarmo Uustalu and Varmo Vene) as the dual to monadic style of programming with futures. However, the comonadic $\lambda$-calculus as they suggest, despite its elegance, causes a serious loss of expressiveness relative to programming explicitly with streams, since the category of free coalgebras they use turns out to have too few global elements to denote many interesting programs, especially fixed points. Nick Benton and I have argued for programming explicitly with streams in our paper Ultrametric Semantics of Reactive Programs. Subsequently, Alan Jeffrey suggested using LTL as a type system in his paper LTL types FRP, an observation that Wolfgang Jeltsch also made in his paper Towards a Common Categorical Semantics for Linear-Time Temporal Logic and Functional Reactive Programming. The difference between the view Nick and I take, and the one that Alan and Wolfgang take is best understood (IMO) by comparing the construction given in Birkedal et al.'s First steps in synthetic guarded domain theory: step-indexing in the topos of trees with Alan's paper. The topos of trees (presheaves over the natural numbers ordered by size) is very similar to category of ultrametric spaces Nick and I used, but much easier to compare with Alan's category (presheaves over a discrete category of time), since these are both presheaf categories. If you're interested in futures specifically for concurrency, then it might be a better idea to look at CTL rather than LTL, though. AFAIK, that's presently unexplored territory! EDIT: here's a link to the draft. The paper is mostly about implementing typed FRP, so the language is synchronous. But most of the discussion of futures/events in section 3.3 should basically apply to truly concurrent languages as well. 

Since eta-conversion is part of the equational theory of the lambda-calculus, this has no effect on the semantics of the program. Furthermore, in the lambda calculus, all of the recursion schemes you list above are instances of the iteration scheme at higher type. (The appropriate calculus is Goedel's System T.) I'll illustrate with some Ocaml code. First, we can start with a datatype for natural numbers, and define primitive iteration for it: 

These two relations are equivalent, in the sense that $e \Downarrow v$ if and only if $e \mapsto^{*} v$. However, some things are easier to define one way or another. For example, we can say that a set of terms $R \subseteq \mathrm{Exp}$ is closed under expansion if it satisfies the property that: $$\forall e \in R, e' \in \mathrm{Exp}.\;e' \mapsto e \;\mathrm{implies}\; e' \in R$$ Likewise, a set $R$ is closed under reduction if if satisfies: $$\forall e \in R, e' \in \mathrm{Exp}.\;e \mapsto e' \;\mathrm{implies}\; e' \in R$$ It's not immediately obvious to me how to formulate expansion or contraction in terms of a big-step semantics. Does anyone know how? 

One extra thing subtyping gives you is that subsumption implies that lots of coherence properties hold. A dependent type theory also needs a notion of proof irrelevance to model everything you can do with subtypes. For example, in dependent type theory you can approximate forming a subset with a dependent record: $$ \{x \in S \;|; P(x)\} \mbox{ vs. } \Sigma x:S.\;P(x) $$ However, note that the cardinality of the subset will be smaller than $S$, whereas the dependent record can have a bigger cardinality (since there can be many possible proofs of $P(x)$ for each $x:$). To faithfully represent subtyping (which says that if $X <: Y$, and $x:X$ then $x:Y$), you need $P(x)$ to be proof-irrelevant -- that is, for there to be at most one inhabitant of the type $P(x)$. Once you have that, you can systematically elaborate subtyping into dependent type theory. See William Lovas's thesis for an example of adding subtyping to a dependent type theory (in this case, Twelf). 

I think my comment was a little cryptic, so let me unpack. The key intuition behind hylomorphisms is that they let you reify the call graph as a data structure. You unfold a datastructure to build a representation of the call graph, and then you fold over the intermediate structure to consume and finish the computation. Lists are a little misleading, because in a recursive algorithm over a list, the call graph will also form a list. However, in general this will not be the case: the call graph will have a different shape than your input data, and so you'll need to introduce a new datatype to represent it. As an example. here's your binary search example. I'll use Ocaml, since I'm more familiar with it, but it should be fairly easy to transcribe it to Haskell. First, let's introduce a datatype of binary search trees. We'll just assume they maintain the invariant in a node that the left subnode contains the elements smaller than and the right subtree contains the elements bigger than .