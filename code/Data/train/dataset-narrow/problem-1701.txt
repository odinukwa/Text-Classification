But if you can get this working with local users, you can probably work to get auth set up differently if you need. I will say that I started with an already-working AnyConnect config and then just added these lines: 

Run a system call tracer (such as , if this is a Linux server) and see if that reveals what lighttpd is doing. 

I have an FAS270C. For months, I've been running it in a split-head manner (that is, with each head serving data totally independently, and without any clustering even being enabled) in order to facilitate moving some data around. I finally got everything situated, moved all the data to one of the heads, and was trying to get clustering set back up. Now when I try to install OnTap onto the "new" head, it cannot see any of the disks in the head shelf. (That is, the shelf into which the heads are inserted.) I've booted into maintenance mode, and it shows me that the 0b adapter, which should be the adapter that that shelf and its disks should be presented on, is in "OFFLINE (physical)" state. If I try to enable it with either "storage enable adapter 0b" or "fcadmin online 0b", it waits for about 30 seconds and then says: 

A data center is a physical facility where (usually) multiple companies' computers are located. It is often so that servers can have a larger-bandwidth Internet connection than the company can get in their own facility, as well as having people dedicated to facilities management, including cooling, power, fire prevention, security, etc. A data warehouse is a type of database, or a manner of using a database, to collect large amounts of data. They are not even remotely interchangeable terms. 

Programs do more than wait on the CPU. They wait on disk an network I/O; they wait on user input. Not every program that runs is going to use 100% of the CPU for top's refresh quantum. For example, when nothing's running, do you see consuming 100% CPU? No. 

However, the memory metrics appear to just report the amount of memory used by the processes running inside the Cloud Foundry container. If you've dealt with monitoring Java apps, you're aware that JVMs will eventually, potentially, consume as much memory as you've told them they can have, even if the Java application is not using it. (In other words, it does its own memory management.) This means that the actual process might be consuming its full 1GB of memory, but that doesn't imply that it's about to run out of memory. In my experience, useful monitoring of Java app memory utilization depends on getting data from the JVM itself, via something like JMX (such as via ). However, Cloud Foundry does not seem to provide information about where the app is actually running. (In fact, I've found some people claiming that Cloud Foundry explicitly will not tell you that information.) And that means I have nowhere to connect a JMX client to. If I want to monitor the memory utilization of my Java apps inside Cloud Foundry, how should I do so? Feel free to rebut any of my claims if you know better. 

The device that is performing your NAT simply isn't NATing properly when the request is coming from the internal network. This is common; Cisco ASAs have this "problem" by default. Depending on your device, you may be able to configure it to allow this type of connection (commonly called a "hairpin"), or it may not be possible and you'll have to have DNS resolve differently for internal clients. 

It does not seem CPU-bound. shows that a from the affected filesystem to consumes about 18% of one core (out of 48) and the rest of the cores are showing over 95% idle. 

I don't see that there is any way to do that by filename. You could set an attribute on the file you don't want copied that you "know" isn't on any other files in the source tree and then use /xa to exclude files with that attribute. Failing that, though, you're going to need to use another tool. This will work: 

It took forever to find where it was happening, but it turns out that there were filters within the VPN blocking LDAP (and other) traffic. I cleared those filters and now it's working. 

One oddity is how the raw devices are set up. The storage controller is an MFI controller, and each raw disk is actually configured in the MFI controller as a 1-disk RAID0 volume: 

where the backslash ("") tells the shell to accept the following character literally and not do any expansion on it. But there are some instances where you end up having to do it the multi-string-concatenation way, not that I can actually come up with an example right now. 

I am running a JVM to support ElasticSearch. I am still working on sizing and tuning, so I left the JVM's max heap size at ElasticSearch's default of 1GB. After putting data in the database, I find that the JVM's process is showing 50GB in SIZE in output. It appears that this is actually causing performance problems on the system; other processes are having trouble allocating memory. In asking the ElasticSearch community, they suggested that it's "just" filesystem caching. In my experience, filesystem caching doesn't show up as memory used by a particular process. Of course, they may have been talking about something other than the OS's filesystem cache, maybe something that the JVM or ElasticSearch itself is doing on top of the OS. But they also said that it would be released if needed, and that didn't seem to be happening. So can anyone help me figure out how to tune the JVM, or maybe ElasticSearch itself, to not use so much RAM. System is Solaris 10 x86 with 72GB RAM. JVM is "Java(TM) SE Runtime Environment (build 1.7.0_45-b18)". 

Turns out that I had accidentally set some tapes to expire (not the backups on them, the tapes themselves). This is the "expiration date" as listed in the output of or "Volume Expiration" in the GUI's "Change Volumes" dialog. I cleared this expiration and that solved the problem. 

It also looks like if there are no scheduled meetings "nearby" that Outlook also shows "No Information". I guess Outlook and/or Exchange don't really publish "Free/Busy" information, but just "Busy" information, so that there's no difference between "Free for a long time" and "No Information". Anyway, I scheduled a weekly meeting at 4AM on Sundays and that seems to have resolved that problem as well. 

Yes; give automount the "--ghost" flag. Don't know offhand where that's configured in Debian-esque init files. Aha. Edit /etc/default/autofs to contain the line: 

The client is in an office connected remotely via MPLS to the data center where our domain controllers exist. I don't seem to have anything blocking connectivity to the DCs, but I don't have total control over the MPLS circuit, so it's possible that there's something blocking connectivity. I have tried multiple clients (Win7 Ultimate and WinXP SP3) in the one office and get the same symptoms on all of them. I have no trouble connecting to either of the domain controllers, though I have, admittedly, not tried every possible port. ICMP, LDAP, DNS, and SMB connections all work fine. Client DNS is pointing to the DCs, and "example.local" resolves to the two IP addresses of the DCs. I get this output from the NetLogon Test command line utility: 

I'm making assumptions about the network assigned by pptpd. Also, that route won't survive a reboot, so you probably want to add the route to a configuration file. Which file and what syntax depends on your distribution. 

Update Now that we know it's an MTU issue, and one that involves only your Ubuntu router, that would imply that your Ubuntu router is breaking packet fragmentation somehow. Is it running a firewall? Is it blocking ICMP? If so, try disabling that. (Obviously, move back to the standard MTU of 1500 first; otherwise you'll be debugging a problem that doesn't exist any more.) 

In my experience, files that have the file descriptor of in output are the executable file itself and shared objects. The man page says that it means "program text (code and data)". While debugging a problem, I found a large number of data files (specifically, ElasticSearch database index files) that reported as . These are definitely not executable files. The process was ElasticSearch itself, which is a java process, if that helps point someone in the right direction. I want to understand how this process is opening and using these files that gets it to be reported in this way. I'm trying to understand some memory utilization, and I suspect that these open files are related to some metrics I'm seeing in some way. The system is Solaris 10 x86.