The following query matches your description from UPDATE 1 (with inline comments showing how it matches) and also yields the correct result for your SQL Fiddle (there is no longer an unwanted row for for the sale). 

Here is a full test script that creates your dummy data, runs the query, and confirms the results. You can optionally add the following index in order to optimize the lookup of the most recent row, but it would depend on your workload whether the index is worth the extra overhead it incurs when inserting and deleting data. 

Maintenance cadence We already have a nightly maintenance window, and the fragmentation calculation is very cheap to compute. So we will be running this check each night and then only performing the more expensive operation of actually rebuilding a full-text index when necessary based on the 10% fragmentation threshold. 

Query #1: Inserting 5MM rows using INSERT...WITH (TABLOCK) Consider the following query, which inserts 5MM rows into a heap. This query executes in and generates of transaction log data as reported by . 

To store rows in a b-tree and perform a seek, all that is needed is an ordering in which the rows should be sorted. Just like you can sort on , you can also sort on the tuple . In the latter case, rows are first sorted by and then any ties are broken by sorting by . The resulting index will use the same b-tree structure, but the value for each entry in the b-tree will be a tuple. 

Based on guidance from Aaron's answer as well as additional research, here is a quick write-up of the approach I took. From what I can tell, the options for inspecting fragmentation of system base tables are limited. I went ahead and filed a Connect issue to provide better visibility, but in the meantime it seems that the options include things like examining the buffer pool or checking the average # of bytes per row. I then created a procedure to perform `ALTER INDEX...REORGANIZE on all system base tables. Executing this procedure on a few of our most (ab)used dev servers showed that the cumulative size of the system base tables was trimmed by up to 50GB (with ~5MM user tables on the system, so clearly an extreme case). One of our nightly maintenance tasks, which helps to clean up many of the user tables created by various unit tests and development, was previously taking ~50 minutes to complete. A combination of , , and showed that the waits were dominated by I/O on the system base tables. After the reorganization of all system base tables, the maintenance task dropped to ~15 minutes. There were still some I/O waits, but they were significantly diminished, perhaps due to a greater amount of the data remaining in cache and/or more readaheads due to lower fragmentation. Therefore, my conclusion is that adding for system base tables into a maintenance plan may be a useful thing to consider, but likely only if you have a scenario where an unusual number of objects are being created on a database. 

When finding distinct rows across two tables where we can't necessarily ensure are pre-sorted, is it a good idea to use a rather than a ? Are there any downsides to this approach? If it is consistently faster, why does the query optimizer not choose the same plan for UNION that the would use? We were able to bring a specific production query from ~10 minutes to ~3 minutes by re-writing a as a . A seems like the more intuitive way to write the logic, but upon exploring both options, I have observed that the is more efficient in terms of both memory and CPU usage. See the following scripts if you'd like to run a simplified and anonymized version of our production query: Setup script 

But it sounds like you might actually want to grab all transactions that occur yesterday or earlier. In that case, you could simply check for all transactions that are less than today. 

Thanks for adding the query plan; it is very informative. I have a number of recommendations based on the query plan, but first a caveat: don't just take what I say and assume it's correct, try it out (ideally in your testing environment) first and make sure you understand why the changes do or don't improve your query! The query plan: an overview From this query plan (as well as the corresponding XML), we can immediately see a few useful pieces of information: 

One caveat to point out is that these indexed views aren't really doing any aggregation. We still have the same number of data elements as before, but they are simply pivoted into a smaller number of rows (and larger number of columns). Even so, I have often observed significant performance gains from operating on a pivoted row set that contains fewer rows to represent the same data. I'll defer to deeper experts on why this might be, but it is fairly intuitive given the row-based processing model where SQL Server needs to pull each row through each query plan operator, presumably incurring some overhead for each row while doing so (at least for row-mode execution, which covers everything except parallel columnstore plans). One last note is that you will often get better and faster responses if you publish a snippet of SQL code that generates some basic test data in your schema (similar to the "set up test data" section of the script above) in addition to just showing the schema. This makes it much easier for someone to help you out and lowers the possibility of mis-interpreting the question. 

This also yields a plan that is able to utilize batch mode and provides even better performance than the original answer. (Although in both cases the performance is fast enough that any selecting or writing the data to a table quickly becomes the bottlneck.) The approach also avoids playing games like multiplying by 0. Sometimes it's best to think simple! 

I wasn't able to find any good resources online, so I did some more hands-on research and thought it would be useful to post the resulting full-text maintenance plan we are implementing based on that research. 

Use for one-time insert operations if minimally logging is required. As Paul points out, this will ensure minimal logging regardless of the row estimate Wherever possible, write queries in a simple manner that the query optimizer can reason about effectively. It may be possible to break up a query into multiple pieces, for example, in order to allow statistics to be built on an intermediate table. If you have access to SQL Server 2014, try it out on your query; in my actual production case, I just tried it out and the new Cardinality Estimator yielded a much higher (and better) estimate; the query then was minimally logged. But this may not be helpful if you need to support SQL 2012 and earlier. If you're desperate, hacky solutions like this one may apply! 

I think you may want something like the following. Note that there was an error in the SQL Fiddle, so the first query corrects the data so that the desired results are returned! 

Note that the parameter has the following comment and Books Online specifies that it is for internal use only. 

I think that the latter ( is able to get a proper estimate based on parameter sniffing for the value of . However, the former () is likely not able to do so, perhaps because is complicated expression that processes partition elimiation: . In this case, there are 100 partitions and the row estimate is exactly 100 times too low because SQL Server isn't able to process the partition elimination in the same smart way that it processes the actual seek on the column and uses an estimate for the runtime parameter value of 3. This theory is supported by the way that the estimated rows varies if you remove partitions; if you use 90 partitions instead, the estimate will be 3333.33 (300000 / 90). In our own queries, we typically use a literal (e.g., in this case) or use OPTION RECOMPILE whenever we are writing a query that is going to take advantage of partition elimination. This practice has worked fairly well for us given that the number of queries on the system is modest and query compilation overhead for queries against large partitioned tables is not a concern for us. Not necessarily a satisfying answer, but it might work for you. 

You can capture the using the query in the section below or you could make a small edit to to expose it in the results. (It's already captured in the intermediate table that is used within ). In each of the cases where I observed a query plan, the query plan was available shortly afterwards (< 1 second in all my cases) via this method. 

However, there are a few things that remain confusing and leave me wondering if I am interpreting the problem correctly: 

What are the side-effects of CHANGE_TRACKING_MIN_VALID_VERSION? As far as I can tell from the documentation, there aren't any "side-effects" that will meaningfully impact you. As Aaron mentions, you'll get the same error if you try to use in a function. In both cases, I would suspect (but am not sure) that and are accessing and/or modifying an internal data structure (e.g., a random generator in the case of ) and SQL Server is biasing towards caution in rejecting their usage in a function. 

This actually yields a fairly simple overall query plan, even when looking at the both of the two relevant query plans together: 

In this case, the non-clustered index is essentially a duplicate of the primary key*. It will take up extra space and make inserts/updates/deletes slower, but it will not add any benefit. The reason is that any query that can be answered by seeking on column can perform that seek on the primary key and has no need for the non-clustered index. However, if your actual table has many additional columns beyond , , and (e.g., let's say there are 100 other columns as well), it's possible that the non-clustered index could improve queries that use only , , and . In that case, it would provide a smaller data structure that can be used to access only these three columns. It would depend on your workload whether this is worth the cost of maintaining the index. 

The first thing that I noticed is that the query plan compilation time was over 3 seconds for each query. Wow, this is a really complex query! Because the solution space of potential execution plans is so large (it grows exponentially with the number of the number of objects involved in the query), SQL Server is only going to be able to explore a tiny fraction of the potential query plans when coming up with a plan for these queries. Remember that SQL Server's job isn't to create the best query plan possible, but instead to create a query plan that is good enough and to do so as quickly as possible. I have often found that small changes in the way a query is formulated, even if they don't impact the logic of the query, can have a significant impact on the query plan. Anecdotally, this grows more and more likely as the query grows more and more complex. One possible reason that this could happen is that a tweak to the query might cause SQL Server to begin cost-based optimization with a different initial plan. As cost-based optimization proceeds, this different starting point could yield a different exploration of the space of potential query plans--kind of like a different random seed impacts random number generation. Note that the query plans you provided are significantly different (compare images of the plan shape below!) and SQL Server actually does estimate that the @table variable plan is slightly cheaper. In terms of why the table variable vs. temp table would have such an impact on cost-based optimization, I'll hazard an only-partially-educated guess: inserting into a table variable forces a serial plan (see the of that appears in the table variable plan, but not the temp table plan), and this may impact the code path that the query optimizer takes either generating an initial plan or in some phase of plan optimization. If possible, the next step that I would try is to simplify the query so that fewer tables are used and/or the query is split into multiple queries (with intermediate #temp tables) so that each query is simpler and has better statistics available. If that's not possible, you could also try more hacky options such as using query hints (e.g., force MAXDOP 1 on the temp table query, and see if the plan comes out more like the table variable query). Query plan with #temp table: 

Optimization: columnstore or not This is a tough question, but on balance I would not recommend columnstore for you in this case. The primary reason is that you are on SQL 2012, so if you are able to upgrade to SQL 2014 I think it might be worth trying out columnstore. In general, your query is the type that columnstore was designed for and could benefit greatly from the reduced I/O of columnstore and the greater CPU efficiency of batch mode. However, the limitations of columnstore in SQL 2012 are just too great, and the tempdb spill behavior, where any spill will cause SQL Server to abandon batch mode entirely, can be a devastating penalty that might come into play with the large volumes of rows you are working with. If you do go with columnstore on SQL 2012, be prepared to baby-sit all of your queries very closely and ensure that batch mode can always be used. Optimization: more partitions? I don't think that more partitions will help this particular query. You are welcome to try it, of course, but keep in mind that partitioning is primarily a data management feature (the ability to swap in new data in your ETL processes via and not a performance feature. It can obviously help performance in some cases, but similarly it can hurt performance in others (e.g., lots of singleton seeks that now have to be performed once per partition). If you do go with columnstore, I think that loading your data for better segment elimination will be more important than partitioning; ideally you will probably want as many rows in each partition as possible in order to have full columnstore segments and great compression rates. Optimization: improving cardinality estimates Because you have a huge fact table and a handful of very small (hundreds or thousands of rows) set of rows from each dimension table, I would recommend an approach where you explicitly create a temporary table containing only the dimension rows that you plan to use. For example, rather than join to with a complicated logic like , you should write a pre-proccessing query to extract only the rows from that you care about and add the appropriate PK to those rows. This will allow SQL Server to create statistics on just the rows you are actually using, which may yield better cardinality estimates throughout the plan. Because this pre-processing would be such a trivial amount of work compared to the overall query complexity, I would highly recommend this option. Optimization: reducing thread skew It's likely that extracting the data from into it's own table and adding a primary key to that table would also help to reduce thread skew (an imbalance of work across threads). Here's a picture that helps show why: 

And finally, if you are interested in going a little bit deeper into how the query optimizer works, I have found Paul White's blog to be a great resource! 

* The primary key is the clustered index here and, as Max Vernon points out, the clustered index is the table. So another way of putting it is that your non-clustered index is duplicating the whole table. 

The sys.partition_range_values documentation states that , which appears to be used to define the partition number, is an 

I'm not sure if it's possible to do this so that none of the references to the table need to change. It seems like the fact that a is needed is a fundamental change that all calling code will need to be aware of. However, it is possible to design this in a way that the resource can be queried in either of the following simple ways. One of these options might have been an easier change to make and maintain across so many different places. 

Let's say that you have a billion row table () with rows divided equally into 1,000 partitions (). A single seek is roughly in a non-partitioned table. However, this same seek operation becomes roughly in this hypothetical partitioned table. So the seek now performs over 500x more work if data from all partitions is needed (or sometimes even if it isn't needed, but SQL can't prove that based on your query). Note that this can come up both when you explicitly query the table for one row (or a small range of rows) and in more complex queries when the partitioned table appears in the innder side of a loop join. The good news is that SQL Server is reasonably good about taking this into account in cost-based optimization, but that still typically means that you get a hash join when a loop-seek into a non-partitioned table would have been far more optimal. Thread skew in parallel query execution In parallel query plans, threads are allocated to partitions. If there is one partition that is much larger than the others, queries against the table may be particularly susceptible to thread skew. It's possible that one thread gets too high a proportion of rows and is processing long after the other threads have done their work. This situation can happen with non-partitioned tables as well, but any partition functions that do not equally distribute rows are particularly vulnerable. See Parallel Query Execution Strategy for Partitioned Objects for a more detailed description of the allocation of threads to partitions. For example: