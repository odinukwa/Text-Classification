Try the following solution. Note that in your question you group by the field in the table, but that won't be possible because a single channel might be included in multiple purchases and if you group by then you won't find the maximum across all purchases, you'll just get the maximum for each single purchase (which is really not a maximum at all). 

I'm going to assume that has a data type of therefore it would be more correct to convert to a string before concatenating it to other strings. It might not give an error in Access, but you're taking your chances that you'll like the date format that Access chooses. 

Every character has one and only one , right? So it seems like that field should be on the table: (?) Each character could have multiple classes so that should probably be in a one-to-many relationship in the table which has the primary key of both and tables. As for the (), each character can have zero to many proficiencies so they must be stored in their own table, but if you want to produce a 'report' which shows all the proficiencies as a delimited list, then you could do it like this: 

Add a column to the table which flags whether the row has been summarized into the summary table Add a partial index to the table for only the unprocessed rows, this could include all columns Now when you go to load yesterday's data you are only looking through rows which you know have not been processed Identify yesterday's rows as part of the load process and insert them into a temp table, then insert the summary of the rows into the summary table Join the temp table to the source table on and mark the rows as processed (should be quick as you have the ids and aren't doing a scan) 

column would need to be part of the underlying tables needs to be part of the primary key of each underlying table needs to have constraints on it in each table such that a given value could not possibly refer to more than one table e.g. 

The following will work on sql-server 2012+. Note that there were a couple of inconsistencies between your supplied SQL and the sample schema that you provided and I think I have followed the names in your SQL. 

This may be ugly, but it doesn't require multiple steps, or knowing the number of products up front. Edit Should be easier to get the output that you now describe. Just remove the final and : 

The with some modification the view should be updateable, but not in its current form. $URL$ gives the rules for Partitioned Views Some things you would need to change: 

If you didn't explicitly set it, then objects created by you will be created under the default schema for your user. In SSMS, look in the database -> Security -> Users -> -> Properties -> General -> Default schema 

If I understand correctly, you are doing two separate statements. First check to see if insert is okay, then second do the insert. I wonder if a trigger might be a solution to your woes. I haven't any experience with creating triggers specifically in Azure, but try something like this: 

So consider your situation in regards to CPU, RAM and storage capacities. Don't forget to consider columnstore indexes which offer both good compression and query performance. 

Maybe I'm overthinking things but I don't really like this because it's currently doing 12 table scans to get the result. The optimiser is also currently choosing to sort the data and do merge joins to achieve the but I'm not worried at this stage about it's choice of doing that. Is there another way that I could do this which doesn't result in additional table scans every time the hierarchy gets one level deeper? 

Add two fields to the table: , Add a generated field to the table which is stored and indexed with a descending sort on the index key. Add a trigger to the table so that when row is inserted the respective is incremented by one and the value of the vote is added to the . (Likewise for or .) 

If you have 6 entity types with different attributes then they should be stored in 6 different tables. If they have attributes that are largely overlapping then you might consider storing multiple different types of entities in the same table. 

Basically, you're scanning the table once for each of the values, and then doing an on the results to only get rows back where all three exist. 

Note the use of , partitioned by and sorted descending by to get the final value, and the use of to get the running total of the discount. You can uncomment the commented select statement to view all rows, not just the final row for each . 

There are some other conditions too, which you can read about in that link, but those are the obvious changes that would need to occur. 

Having multiple fields that reference another table doesn't make it a many-to-many relationship. You simply have multiple one-to-many relationships between the tables. Your diagram is correct. 

This would give you the two rows with the lowest and you will still get the same two rows even when you add other fields to the list. You might be thinking that means that the database will give you the first two, but it actually means that it should stop returning rows after it has returned two rows. And since you didn't have an it could just return whichever two rows are most convenient. 

This table will have more rows than your original table, but each time you query the table should be faster because the query can go straight to the correct rows rather than reading the whole table and evaluating the filter each time. 

Sometimes, perhaps due to some maintenance task which doesn't respect compression settings, indexes and tables have compression turned off so carefully consider any routine tasks you have which rebuild indexes. Space saving can be significant When a query against a compressed table/index reads data from disk it was quicker (in my experience) than a query reading an uncompressed table due to the I/O limitations of disk. When the pages were already in the buffer pool, though, the query was slower than when compared to uncompressed data because of the additional CPU involved in uncompressing the data. 

There is a potential, AFAIK, if you're using transactional replication that you will get a big queue of transactions that can't be applied to your subscriber (assuming that your publisher is quite busy otherwise you wouldn't be using replication). This may need to be managed, possibly by doing a instead of a or some such, but you can gauge that best for your situation. 

If the purpose of the unique constraint (with its associated unique index) is simply to enforce uniqueness then you should change the column order for your unique constraint to and then your index will be redundant and can be dropped. Otherwise there may be some benefit from having another index on your table with different leading columns, you should consider your workload, if the table is often accessed with a predicate on or on both and and maybe that is a good case to have those columns first in the key. Also @srutzky comment about the clustered index has merit and is worth considering. 

Note, that you should put whichever column is likely to narrow the search best first. I assume that your three columns are equally selective so just picked one. The other columns are included so that it can be an index-only read. 

I don't think it's possible. The notion seems to fly in the face of everything that we know about partitioning and clustered indexes, but I might be wrong. One possibility is a partitioned view over two tables, one with a clustered index, one as a heap. I have included a demo script for this, but it does come with a number of limitations which might be critical: 

Here is the final script that I have, after integrating the feedback from various respondents (just wanted to collect it all together for future readers): Disable Constraints and Indexes 

This second approach would be easier to extend to accept an arbitrary number of tags, though the first approach would not be impossible. 

The standard approach is to use a table for all customers. Storing data in row in format would mean that the data is only readable by your application, not by other client software that accesses the database. It may also invalidate built in protections that databases have relating to the ACID properties of transactions. A properly designed table should have performance no worse than storing transactions in each row. 

I have some tables in the staging area of a data warehouse that I'm filling with data from some flat, comma-delimited text extracts from another system. When the data comes in the hierarchy of parents for each element is presented in columns labelled ... where the immediate parent of the current node is in and the top level parent could be in any column ( is mostly ). 

In the flat-file connection manager, in the General pane, there is an option to set a . If you set this to double quote (") for your file then SSIS should ignore any row delimiters that are contained between the double quotes. There is also an property that could help. See this msdn page for more. 

A second option which may have more merit in your case is a filtered index which you could add over the old part of your table and include a large number of columns so that it covers the queries that you do on the old part of the table (this would serve to speed up reads to the old part of the table but wouldn't slow writes in the recent part). 

I don't know that the query could be refactored much and still be logically correct. And since I don't have your dataset it is difficult to test possible solutions for performance. This query should still be logically the same, but may be executed in a more efficient way. 

As Joe suggested, you should use the function to compare values to the previous row. It could be implemented like this: 

Please note that depending upon your data type, you may need to cast some of your strings to Unicode/non-Unicode types to get everything to work smoothly. The syntax uses the ternary operator $URL$ 

Here are a couple of alternate approaches which don't involve using . Use the operator against the the sets of returned for each tag: 

Now if we want to see free slots for a particular doctor, all we need to do is specify which doctor, and how long the slot is that we're looking for: