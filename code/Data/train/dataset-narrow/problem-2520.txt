I would expect that the empty relation corresponds to the always-zero probability distribution. Disjoint union corresponds to addition of probability distributions. However, standard union would be more complicated. I don't really know if there is some way to encode the standard union for probability distributions. (I have heard people say that probability distributions form a boolean algebra. So there must be some operation that serves as the union.) 

This doesn't say explicitly that the term was coined by Gordon. But, given that the memo is titled "Lambda-definability and logical relations" as if "logical relation" is an already known idea, and the second para says "construct certain, so-called, logical relations," I think it very likely that Gordon coined the term and Plotkin used it hence. (Plotkin confirmed to me that whatever he wrote in the memo is correct.) Gordon is credited again at the top of p. 12, 

Unfortunately, the remark of Wadler is too cryptic for me to tell what use he wanted to make of "lax natural transformations". Here is a guess. Relation-preservation squares can often be recast as lax commutative squares. This is how they used to be written in old automata theory papers/books. See paragraph 1.2 in my Notes on Semigroups. To do this kind of thing, you have to mix up relations and morphisms and pretend that they are the same. I am also not sure that it buys you anything new. It is just uglier notation for saying the same thing as relation-preservation. Please feel free to explore the connection, but I am not confident that you will find anything new by doing it. 

It is obvious that these four rules are complete for deducing clauses, i.e., Proposition 1 For any clause $C$ and set of clauses ${\cal S}$, we have ${\cal S} \models C$ if and only if ${\cal S} \vdash C$. Refutation proof converts the problem of ${\cal S} \vdash C$ to ${\cal S} \cup N(C) \vdash \bot$, where $N(C) = \{\{\bar{A}\} \mid A \in C\}$ is the collection of clauses representing the negation of $C$. It is clear that ${\cal S} \vdash C$ if and only if ${\cal S} \cup N(C) \vdash \bot$. Our four-rule system is still adequate for proving the converted problem, but we notice that we don't need identity and weakening any more. The remaining two rules are called the "resolution proof procedure". Proposition 2 For any clause $C$ and set of clauses ${\cal S}$, we have ${\cal S} \models C$ if and only if ${\cal S} \cup N(C) \vdash \bot$ using only cut and contraction. The point of converting the problem to refutation proofs is two-fold: 

This does not answer your (interesting) question, but it may be worth mentioning that this variant (which does not use geodesic distance) has been studied: Thomas Shermer, "Hiding people in polygons," Computing, Volume 42, Numbers 2-3 (1989), 109-131 (Springer link): 

Here is a figure from the first:     Note that the shapes mentioned in Yoshio Okamoto's post appear in this hierarchy. And if you are wondering what a (Barbados-induced?) "palm" polygon looks like...              

After hearing Emo Welzl speak on the subject this summer, I know the number of of triangulations of a set of $n$ points in the plane is somewhere between about $\Omega(8.48^n)$ and $O(30^n)$. Apologies if I am out-of-date; updates welcomed. I mentioned this in class, and wanted to follow up with brief, sage remarks to give students a sense for (a) why it has proved so difficult to nail down this quantity, and (b) why so many care to nail it down. I found I did not have adequate answers to illuminate either issue; so much for my sageness! I'd appreciate your take on these admittedly vague questions. Thanks! 

If one restricts Turing Machines to a finite tape (i.e., to use bounded space $S$), then the halting problem is decidable, essentially because after a number of steps (which can be calculated from the number of states $Q$, and $S$, and the alphabet size), a configuration must be repeated. 

The question has been answered in the comments by Tsuyoshi & Chandra! I am adding this CW answer so I can accept it to indicate the question is closed. Thanks, everyone! 

Godfried reports that his hierarchy paper (cited by Aaron) never got written, but some of its ideas appeared in these two papers. 

Computing the Cheeger constant of a graph, also known as the isoperimetric constant (because it is essentially a minimum area/volume ratio), is known to be NP-complete. Generally it is approximated. I am interested to learn if exact polynomial algorithms are known for special classes of graphs. For example, is it still NP-complete for regular graphs? For distance-regular graphs? (I have not studied the existing NP-completeness proofs to examine their assumptions.) Literature pointers appreciated—thanks! 

"Two-Convex Polygons," O. Aichholzer, F. Aurenhammer, F. Hurtado, P.A. Ramos, J. Urrutia, 2009.            

This is not a serious answer, just an opportunity to mention that Godfried Toussaint introduced sail polygons in his 1985 paper "A simple linear algorithm for intersecting convex polygons":        I don't think this particular class found many subsequent uses. :-) 

I feel I should know this... But I am not finding a definitive reference. Is it $\Omega(n^d)$? How about the $d{=}2$ specialization: The largest area bounded cell in an arrangement of lines? 

I think Simon PJ's quote is a bit of an off-hand remark actually. Similarity between languages is determined by what consensus has been generated in the researcher and language designer community. There is no question that there is a higher degree of consensus in the functional programming community than in the imperative programming community. But it is also the case that the functional programming languages are mostly designed by researchers rather than practitioners. So, it is natural for such consensus to emerge. Almost all functional languages use garbage collected memory management and recursive data structures (originated by Lisp), most of them use "algebraic" data types and pattern matching (originated by Hope), a lot of them use higher-order functions and polymorphic functions (originated by ML). Beyond that, the consensus disappears. They differ in module systems used, how state change operations and other computational effects should be handled, and the evaluation order (call-by-name vs call-by-value) etc. Imperative programming languages generally use nested control structures (originated by Algol 60), and type systems (originated by Algol 60, but consolidated by Algol 68). They generally have cumbersome surface syntax (again going back to Algol 60), make half-hearted attempts to handle higher-order functions and polymorphic types, and differ in their support for block structure and module systems. There is probably more uniformity in the evaluation order because, after the 60's, call-by-name has essentially disappeared from imperative languages. So, it is not clear to me that the difference between the two classes of languages in their uniformity is all that great. It would be really worthwhile to bring the cleaner and uniform notations of functional programming into imperative programming languages. I see that Scala has made a beginning in that direction. It remains to be seen whether the trend will continue. 

The standard presentation of program transformation ideas is unsound, quite unfortunately. They usually think of program transformation as forward deduction. Using equational reasoning, you can deduce new facts from the old program, and lo and behold, it gives a better program! While the new facts are quite clearly facts, nothing guarantees that they constitute a program. The problem you have spotted is this naive presentation of the program transformation ideas. However, it is possible to think of very much the same process as a backward deduction. We think of the old program as a specification, and derive a new program that satisfies the specification. Since equational reasoning is often bidirectional, you would see very similar steps in deriving the new program. However, the meaning of these steps is opposite to what the naive methods imply. When you use this "correct" method, you reason about termination while introducing recursion, and always derive a correct program. For a paper describing the technique, see Deductive and inductive synthesis of equational programs (PDF). This was done for first-order "equational" programs, but very similar ideas should be applicable to terminating functional programs. If you are interested in non-terminating programs, I think you could use coinduction techniques (an off-the-cuff intuition). However, my efforts to apply domain theory techniques to do these kinds of transformations did not bear fruit. It might still be possible, but I haven't seen anything written on it. 

Doesn't this give you what you want? Because you only care about the combinatorics, your problem is to embed a graph in a surface, I gather a surface of genus zero. 

Suitably generalized to $n \times n$ boards, Checkers, Chess, and Go are each EXPTIME-Complete. (See the Wikipedia Game Complexity table.) Each of these two-person games of perfect information has a simple, finite set of move / capture / promotion / win rules. Presumably there are an infinite number of sets of rules that lead to games of complexity (at least) EXPTIME-Complete. 

Not an answer, just a few references. First, I wrote a paper (long ago!) on the case where every point in the given set $V$ must be a polygon corner. In that case, it is not surprising that there is (at most) one polygon, and it is easy to find: "Uniqueness of Orthogonal Connect-the-Dots," in Computational Morphology, Ed. G. T. Toussaint, Elsevier, North-Holland, 1988, 97-104. Second, there is a beautiful update to this work by Maarten Löffler and Elena Mumford, in a paper, "Connected Rectilinear Graphs on Point Sets," Journal of Computational Geometry, 2(1), 1–15, 2011. From their Abstract: 

I would be interested to learn of anyone's experience using mastery-based (or "mastery-level") grading in a Theory of Computation course. Usually this requires—at a minimum— a detailed taxonomy of competencies, and what it means to demonstrate mastery of each competency. 

I am interested in learning connections between "chaos," or more broadly, dynamical systems, and the $P{=}NP$ question. Here is an example of the type of literature I am seeking: 

(2) I also found a 2007 German Ph.D. thesis, "Facility Location and Related Problems," by Martin Romauch (PDF link), that includes a chapter on the "Vertex Guard Double Cover problem," showing that it is NP-hard for polygons with holes. He also shows that the right combinatorial bound is $\lfloor 2n/3 \rfloor$ (disappointingly obvious!). I have only skimmed through this, but it is certainly worth a look. 

Does anyone know of work on computing the Voronoi diagram of a set of points on a polyhedron, where distance is measured by shortest paths on the surface? I am particularly interested in convex polyhedra. I have a vague memory that this has been explored, but my memory is too vague to locate any papers. Thanks for pointers! 

You can use Google Scholar to track those later papers that cite these to find improvements and related work. 

I know that it is undecidable to determine if a set of tiles can tile the plane, a result of Berger using Wang tiles. My question is whether it is also known to be undecidable to determine if a single given tile can tile the plane, a monohedral tiling. If this remains unsettled, I would be interested to know what is the minimum cardinality of a set of tiles for which there is an undecidability proof. (I have not yet accessed Berger's proof.) 

The fact that each step is expressed compositionally doesn't mean that the entire behaviour is expressed compositionally. There is a nice article by Carl Gunter called Forms of Semantic Specification, where the different methods of specifying semantics are compared and contrasted. Much of this material is also reproduced in the first chapter of his "Semantics of Programming Languages" text. This should hopefully clarify the picture. Another word about "operational semantics". In the early days, the term "operational" was used to refer to any semantic definition that referred to detailed evaluation steps. Both denotational semanticists and axiomatic proponents looked down upon "operational" semantics, regarding it as being low-level and machine-oriented. I think this was really based on their belief that higher level descriptions were possible. These beliefs were shattered as soon as they considered concurrency. de Bakker and Zucker's Processes and denotational semantics of concurrency has these interesting passages: 

I would add that, typically, one uses the term "calculus" when the evaluation rules are expressed at the level of the source language. One use the term "abstract machine" when addition "machine-level" concepts are used in describing the evaluation (such as stores, pointers, stacks, etc.). 

There are unfortunate confusions in the way you have posed the question. Programs have semantics. Programming Languages are given semantic definitions. In more detail: every program has a meaning, either as a computation or as a mathematical function (relation, trace set, strategy,...). A semantic definition is given for an entire programming language, so that the meaning of every program in the language is defined. Programs can also be given specifications. While the semantics of the program says what the program means, its specification states what we care about its behaviour. So, the specification can be partial. It need not state what the program does under all possible situations. Neither does it need to state everything about the outputs of the program, only the properties we care about. The sample specification you have shown for the minimum function is incomplete. It says that the output has to be smaller than (or equal to) both its inputs, but it does not say that the output must be one of the two inputs. To make it complete (which is apparently the "intent"), you need to add this condition 

I don't have a specific recommendation for your reading list, but I want to alert you to the excellent survey on "Rewrite Systems" by Dershowitz and Jouannaud in Handbook of Theoretical Computer Science, volume B. 

A lot of the senior Computer Scientists in Britain have had industrial experience before they came to work in academics. Christopher Strachey, the founder of denotational semantics, was a consultant programmer before entering academics. Tony Hoare, the founder of axiomatic semantics, worked in industry (Eliott Computers) for several years. Samson Abramsky, who holds the Christopher Strachey Chair at Oxford, in fact developed his interest in Computer Science during his work in industry (GEC). Cliff Jones, a Fellow of RAEng, worked in IBM, Vienna for several years before coming to do his PhD, and did another stint at a start-up company called Harlequin even afterwards. I have to say that all of them probably did innovative R&D type of work while in industry, which might be necessary to keep your mind active in thinking about research directions.