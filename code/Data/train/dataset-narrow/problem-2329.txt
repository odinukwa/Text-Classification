I'm looking to build notations for large countable ordinals in a "natural way". By "natural way" I mean that given an inductive data type X, that equality should be the usual recursive equality (the same that in Haskell would produce) and the order should be the usual recursive lexicographical order (the same that in Haskell would produce), and there is a decidable predicate that determines if a member of X is a valid ordinal notation or not. For example, ordinals less than ε0 can be represented by hereditarily finite sorted lists and satisfies these requirements. Define X to be μα. μβ. 1 + α×β, a.k.a. hereditarily finite lists. Define to check that X is sorted and all members of X are . The valid members of X are all ordinals less than ε0 under the usual lexicographical order. I conjecture that μα0.… μαn. 1 + α0×…×αn can be used to define ordinals less than φn + 1(0), where φ is the Veblen function, in a similar way. As you can see I run out of μ quantifiers at φω(0). Can I build larger ordinal notations satisfying my requirements? I was hoping to get as far as Γ0. Can I get larger ordinals if I drop my decidability requirement on my validity predicate? 

The StreamMemo library for Coq illustrates how to memoize a function over the natural numbers. In particular when , the shares the computation of recursive calls. Suppose instead of natural numbers, we want to memoize recursive functions over binary trees: 

I have created a "solution" that recursively memoizes structurally recursive functions of binary trees in Coq. My gist is at $URL$ It operates similarly to Saizan's solution, by stratifying binary trees based on a size metric, in my case counting the number of internal nodes of binary trees, and producing a lazy stream of containers of all solutions for a particular size. Sharing happens because of a let statement in the stream generator that holds the initial part of the stream to be used in later parts of the stream. Examples show that for , evaluating a perfect binary tree with 8 levels after having evaluated a perfect binary tree with 9 levels is much faster than only evaluating a perfect binary tree with 8 levels. However, I'm hesitant to accept this answer because the overhead of this particular solution is bad that it performs much worse than my memoization without structural recursion for my examples of practical inputs. Naturally, I want a solution that performs better under reasonable inputs. I have some further comments at "[Coq-Club] How can you build a coinductive memoization table for recursive functions over binary trees?". 

To maximize the expression, given $x_i$, should set $y_i = m$ for the value of $i$ maximizing $x_i - \alpha$. This implies you should set $x_2, \dots, x_n = \alpha$ and $x_1 = 1 - (n-1) \alpha$. (I assume here that $0^0 = 1$; otherwise you must set $x_i = \alpha + \epsilon$, and the resulting function does not realize its supremum) 

The posting on MathOverflow tells how to go from a small number of independent Uniform[0,1] random variables to a larger number of pairwise-independent Uniform[0,1] random variables. You can of course go back and forth between Uniform[0,1] and Gaussian by inverting the CDF. But that requires numerical analysis as the CDF is not closed-form. However, there is a simpler way to from Gaussian to uniform. Given two independent Gaussians $X_1, X_2$, the angle $\arctan(X_1/X_2)$ is uniform in the range $[0,2 \pi]$. Similarly, the Box-Muller method transforms two independent Uniform[0,1] variables into two independent Gaussian random variables. Using these two transformations, you consume two Gaussians to produce a uniform or two uniforms to produce a Gaussian. So there is only a factor of $O(1)$ in the sampling efficiency. Furthermore, no inversion of the Normal cdf is required. 

The set of transcendentals is not open in $\mathbf R$ (in particular, it is dense and codense in $\mathbf R$. Hence it is undecidable. 

The following fact seems to be used implicitly in cs theory, particularly algorithms. Given a RAM machine $M$ running in time $O(f(n))$, another RAM machine $M'$ can simulate $M$ in time $O(f(n))$. This differs from the case for Turing machines, where $M'$ may require $O(f(n) log(f(n))$ time. I say this is often used implicitly because many papers will simply say something like "run $M$, but keep track of certain auxiliarily information as you do so". This is really simulating $M$, but for RAM machines the distinction is not so important because running times are not (asymptotically) affected. Is there a reference for this theorem? I am summarizing the situation correctly? 

A graph $G=(V,E)$ is a chordal graph, if it does not contain an induced cycle of length at least four. We say a graph $H$ is a chordalization of graph $G$, if $H$ contains $G$ as a subgraph, and $H$ is chordal. $Q_1$: Find minimum number of edges whose addition to a given graph makes the graph a chordal graph. According to this, $Q_1$ is NP-hard. $Q_2$: Find a chordalization that does not introduce new $K_4$? What is the complexity of $Q_2$? Is $Q_2$ harder than $Q_1$? { Remark: After Florent comment, I changed $Q_1$ from the following: $Q_1$ in first version of my post: What is the complexity of giving an arbitrary chordalization of input graph? } 

The algebraic connectivity of a graph G is the second-smallest eigenvalue of the Laplacian matrix of G. This eigenvalue is greater than 0 if and only if G is a connected graph. The magnitude of this value reflects how well connected the overall graph is. for an example, "adding self-loops" does not change laplacian eigenvalues (specially algebraic connectivity) of graph. Because, laplacian(G)= D-A is invariant with respect to adding self-loops. My question is: Does anyone has studied effect of different operations (such as edge contraction) on spectrum of laplacian? do you know good references? Remark: the exact definition of the algebraic connectivity depends on the type of Laplacian used. For this question I prefer to use Fan Chung definition in SPECTRAL GRAPH THEORY. In this book Fan Chung has uesed a rescaled version of the Laplacian, eliminating the dependence on the number of vertices. 

Another typical case of $NPI$ problem is when there is a witness of length $\omega(\log n)$ but smaller than $n^{O(1)}$. The problem of the existence of a clique of size $\log n$ in a graph is a typical example -- in this case, the witness (the specific clique) requires $O(\log^2 n)$ bits. Assuming the Exponential Time Hypothesis, such a problem is easier than an $NP$-complete problem (which requires time $\exp(n^{O(1)})$) but harder than a polynomial time problem. 

Suppose you are given input $w = \langle M, x, t \rangle$ and are asked to decide if RAM machine $M$ terminates on input $x$ after $t$ steps. By the time hierarchy theorem, the optimal algorithm to decide this is to simulate the execution of $M(x)$ for $t$ steps, which can be done in time $O(t)$. (Note: for Turing machines, simulating the execution of $M$ takes $O(t \log t)$ steps; we only know a lower bound of $\Omega(t)$. So, this is not quite optimal for Turing machines specifically). There are some other problems which contain the version of the halting problem as a sub-case. For example, deciding whether a sentence $\theta$ is a consequence of the WS1S takes time $2 \uparrow \uparrow O(|\theta|)$ and this is optimal. 

Let $T$ be any recursively axiomatized but undecidable theory (such as arithmetic, set theory, etc.) Then the logical consequences of $T$ are recursively enumerable but not recursive. 

A very common technique in the analysis of algorithms involving random permutations is to have each element $x$ select a rank $\rho(x)$ uniformly at random from the real interval $[0,1]$. The permutation $\pi$ is then formed by sorting on ranks. Is it possible to invert this transformation? That is, given a permutation $\pi$ generated randomly in $S_n$, choose ranks $\rho(x)$ (with some additional randomness) such that 

As far as I know, following operations convert a $PCP_{1,s}[O(\log n),O(1)]$ , to a $PCP_{1,s’}[O(\log n),O(1)]$, with following $s’$ : 

1- The PCP Theorem by Gap Amplification by Irit Dinur 2- On Dinur’s Proof of the PCP Theorem by jaikumar Radhakrishnan and Madhu Sudan 3- Chapter 22 from Arora and barak book: COMPUTATIONAL COMPLEXITY A Modern Approach 4- Robust PCPs of Proximity and Shorter PCPs by Prahladh Harsha (that covers first proof of the PCP therorem) 

Why is "long code test" also called "dictatorship test"? I got really confused when I read about it in Arora's survey. 

Reduction from PCPs allow us to prove hardness of approximation results for a number of constraint satisfaction problems. I've seen such a reductions only for Max-CSPs. Is this possible only for Max-CSPs? In other words, can someone get hardness of approximation result for a Min-CSP (for example, Vertex-Cover or sparsest-cut) by reduction from PCPs? If it is always possible to convert a Min-CSP to an equivalent Max-CSP, then maybe this answers my question. But, I don’t know whether it is always possible or not. 

1- Probabilistically Checkable Proofs and Codes by Irit Dinur 2- Probabilistically checkable proofs by Madhu Sudan 3- Chapter 9 from Goldreich book: Computational complexity, A conceptual perspective 

Could you please correct me if I have made any mistakes? What is special with ½ in serial repetition or Dinur’s transformation? why not another constant, like 1/3 or else? Are such a results true for PCPs with imperfect completeness? 

A lot of approximation algorithms are based on relaxation. The way it usually works is this. You take the original problem and relax it some large class of efficiently solvable problem (e.g. relax an IP to an LP). Any solution to the original problem of cost $X$ gives a corresponding solution to the relaxation of cost $X$. You then convert a solution to the relaxed problem of cost $Y$ to a solution to the original problem of cost $\alpha Y$ for some $\alpha > 1$. This implies that you have an $\alpha$ approximation. But what if you could argue that the original feasible solution of cost $X$ guarantees the existence of a solution to the relaxation of cost $X/\beta$? Then you would get an $\alpha/\beta$ approximation algorithm. Has this strategy ever been employed? Is there ever a case when you can show that a feasible solution to the original problem corresponds to a feasible solution to the relaxation of strictly lower cost ? 

The definitions of efficient reducibility are motivated in part by an analogy with recursion theory. In recursion theory, the m-reductions are closely connected to the arithmetical hierarchy. (m-reductions preserve arithmetical degree). Arithmetical classifications are important beyond mere computability. For example, one can say that true $\Sigma_1$ statements are provable in Robinson's $Q$. In complexity theory, there is also a notion of "polynomial hierarchy", though unlike the arithmetical hierarchy it is only conjectured to exist. This leads to classifications that are more subtle than "Is this problem as hard to solve as NP?"