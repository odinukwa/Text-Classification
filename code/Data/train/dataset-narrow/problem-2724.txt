I think that you are approaching this on the wrong side. Question: what you need to license, the source code or a compiled software? In the first scenario you probably want to stick with something like the GPL, BSD or MIT licenses, in the second case you probably just need an EULA. You also can mix this 2 requirements but i don't think that you need to give a license for your source code in your case, the user will never see your source code; you also appear to not being interested in patents, and patents are the only way to prove that you own a particular asset of your software or you own the rights for an UI, a file format, or some other pieces of your code including the design and its own implementations. 

you will be fine with a commercial solution, otherwise if you want the maximum flexibility and you have the know-how, you probably want to code your own stuff and avoid spending money and legal issues. Also all the software that you mentioned offers legal problems when it comes to using them on the job, some of them offers more complex issues, because for example the UDK, it's not really free for every use, if you are going to use it in the place where you work, you have to pay, no matter what you are producing with it. There are also nasty things like the standard Autodesk EULA allows Autodesk to basically fetch your computer for data without explicit warnings ans in "silent mode". If i was you, i would switch to Blender and Gimp, this 2 are really powerful software, with a rich set of APIs and 0 legal issues, and they are free. 

Open Dynamics Engine is another semi-popular open source middleware solution for physics and collision. $URL$ PhysX is another popular collision/physics middleware from NVIDIA. Binary available. $URL$ Last but not least is Havok which is the gold standard of collision/physics. Binary available. $URL$ 

While doing a BS in CS at a California State University there was only one game development course which was group based where each group was to deliver a complete game from scratch in 10 weeks. Each group consisted of 4 programmers. This single game was worth 100% of the grade. It was straight C++ and OpenGL with weekly deliveries from all groups. One of the hardest classes I've ever had but at the same time we learned everything about how game engines really work. Rarely do students learn this anymore since most are spoiled with engines or frameworks that abstract all the "hard" stuff away. My professor published a paper about the class in 37th ASEE/IEEE Frontiers in Education Conference 2007 Student Teamwork: A Capstone Course in Game Programming The game my group created Images from my Portfolio Video of the game from another teammate 

Ai Game Dev Excellent source of Game Ai information from videos, to interviews, to articles, etc etc. The best content has to be paid for but is worth it since there are a lot of interviews with Game AI developers about techniques that aren't published anywhere else. On top of that it allows you access to their AI Sandbox application for testing out your own AI implementations. 

Well there are certainly copyright trolls around this topic. If you ever try to use the word "Edge" in your game title, prepare to be sued by Tim Langdell Bytes: Tim Langdell & the IGDA 

Well, Game engine is a generic term, Physics engine is more specific, the "problem" is that the functionalities that a game engine provides are up to the developers that have coded that particular game engine. There are very basic game engine that have no physic support or they expect you to add to it manually, and game engine that support physics and fractures in real time. Your view shouldn't be about how they work together in the first place, just look at what a game engine offers and if you need a physics engine add it to your code. There are also some engines that mimic the physic with pre-baked collision and explosions, there are several approach to this, depending on what you have, what you want to achieve and what is your target machine, you better look to the features and how they are implemented, only the name "physics engine" can't tell you what you are dealing with. 

There is this Windows Advanced Rasterization Platform on the most recent Windows platforms which i think it's what you are looking for or there are commercial solutions like Pixomatic. 

There are frameworks like FreeGLUT that give a basic input/output abilities with keyboard and mouse, but if you want more there are the Visual C++ APIs for Windows. 

Having an application that uses shaders that have been wrote in GLSL, what is the best strategy for the distribution in the real world and for the desktop and mobile? I'm aiming to distribute this in a binary form or as plain serialized text, i would like a good suggestion on this. 

Unity is a very popular engine/WYSIWYG editor which allows programmers to expose easy to modify "components" which artists/designers can attach to entities to give functionality within the game. A bunch of components come standard which provide a lot of the basic functionality for games. From the Unity site: 

Sergio you might want to aim more toward a Game Development math book like Essential Mathematics for Games and Interactive Applications, Second Edition: A Programmer's Guide Instead of the classical Linear Algebra you would learn in college. Also like Ron Warholic said, stating what your math comfort level is would better help us taylor a specific book. 

You can kill two birds in one stone. USC Interactive Media Division or USC Game Development The Princeton Review rated USC as the number 1 Game Design school, Digipen rated 2nd. $URL$ 

The Mario AI Allows you to implement an AI Agent to control Mario. Different levels of map details are available to allow a simple implementation or implementations with near engine level map details. The API is a server/server type implementation using Java. Additionally a Level Generation API is provided for creating user generated levels. 

PEP-0342 details coroutine implementation in Python. You can create your own scheduler and tasks which can simulate multithreading on a single thread. This is the same way that Javascript frameworks allow for multithreaded like processing even though Javascript is run on a single process. 

If you use triangle picking, you only need to solve for a line/plane intersection test to find the point on the triangle that you clicked on. Use the raycast as your line and treat the triangle as a plane. 

I'm starting with the programmable pipeline and the shaders in C++ for OpenGL 3.0+, i would love to be able to change some settings on the fly, for example replacing a function with another function, supposing that i have a shader with an operation like 

There are different technologies for this, there is no standard, at least no one that i'm aware of. The multi-monitor technology from ATI is named eyefinity and it's probably the most mature technology among the ones available on the market. The eyefinity capabilities are accessible through the AMD display library SDK . Nvidia calls its multi-monitor technology nVidia Surround and there are little to none informations for the developers, there is this page that mix the surround technology with the 3D technology and i don't think that is useful at all. If you are interested in this you can try to browser and ask in the Developer Zone. 

my goal is replacing with any other operation on the fly with my C++ program, maybe with a GUI, but in general terms with C++, the problem is that until now i'm able to run a shader only after compiling it, so i have something like a static approach. The same goal applies to the values of the vars that i would also like to change through C++. It's possible with a programmable pipeline? I'm not interested in performance, just if is possible for the C++ to dynamically communicate and exchange data with the OpenGL pipeline. 

This isn't clear for me, if i use the drivers from the GPU manufacturer and they support OpenGL 3.0 and/or above, i can always make an OpenGL ES 2.0 application work? 

I've been learning Android in my spare time, and one of the first things every tutorial/guide will introduce to you is the concept of an "Activity." Everyone loosely defines an Activity as a screen for doing a certain activity. My question is, how relevant/important is it to have multiple Activities for a game? Are there any guidelines for breaking a game down into multiple Activities? To explain what I mean, let's take a game like Final Fantasy. In Final Fantasy, you have several different screens that get used over and over and over again: 

Using libgdx here. I've just finished learning some of the basics of creating a 2D environment and using an OrthographicCamera to view it. The tutorials I went through, however, hardcoded their tiled map in, and none made mention of how to do it any other way. By tiled map, I mean like Final Fantasy 1, where the world map is a grid of squares, each with a different texture. So for example, I've got a 6 tile x 6 tile map, using the following code: 

Given the random nature of the environment, for loops don't really help as I have to start and finish a loop before I was able to do enough to make it worth setting up the loop. I can see how a loop might be helpful for like tiling an ocean or something, but not in the above case. The above code DOES get me my final desired output, however, if I were to decide I wanted to move a piece or swap two pieces out, oh boy, what a nightmare, even with just a 6x6 test piece, much less a 1000x1000 world map. There must be a better way of doing this. Someone on some post somewhere (can't find it now, of course) said to check out MapEditor. Looks legit. The question is, if that is the answer, how can I make something in MapEditor and have the output map plug in to a variable in my code? I need the tiles as objects in my code, because for example, I determine whether or not a tile is can be passed through or collided with based on my TileTyle enum variable. Are there alternative/language "native" (i.e. not using an outside tool) methods to doing this? 

Yes, also this is a common scenario for both desktop and mobile users, with OpenGL the fixed pipeline approach is just deprecated and when using old code on modern devices you are just guessing, because any GPU maker that wants to stick with a modern OpenGL approach is not forced to support old functions and the old approach for the pipeline. These days you get OpenGL ES 2 capable devices from the low end market up to the high end, and the OpenGL ES 3 will come soon, adopting OpenGL ES 1.x is just an old and deprecated approach for the market of today. OpenGL ES 2 introduces the programmable pipeline on mobile devices discarding the old fixed-pipeline approach; probably for this reason you find it more difficult to use, but just a change of mindset and some hours passed on coding will make you change your idea about this. 

The only real thing that is different is the amount of devices, Apple just sell 1-2-3 new product each year, Android offers 1 new product every day/week. The emulator it's not buggy, it's just not intended for profiling, if you want to profile an Android application you have to do the same thing that you have done for iOS: consider the lowest profile device that match your requirements and buying it. You are supposed to have at least a basic know-how about the ARM architecture, otherwise you can make a difference between all the devices on the market, begin to outline the hardware features that are important for your application and buy that device for real testing. 

A good response should be fact based, and any subjective subjects like difficulty should be based on an explained experiences with the product. 

The equivalent to Navigation Meshes for 3D spaces is Navigation Volumes. Havok AI implements both navigation volumes and a volume pathfinder as shown in their GDC 2011 demonstration. The principle of A* in a volume is the same as A* on a navigation mesh. Since A* will find a path over any graph it doesn't matter if the graph is represented by a point to multiple points, a polygon to multiple polygons, or a volume to multiple volumes. The algorithm will still find a solution if one exists. Some slight nuances that are different with paths found on navigation meshes is how you determine path points at the edge of line segments, at the ends, or maybe at the middle? The same can be true of of navigation volumes, to determine the cost to traverse to the next volume you'll typically have to pick a point within the volume, midpoint/edge/etc. This all essentially boils down to the heuristic part of the A* algorithm you must supply yourself, or use a basic Euclidean distance algorithm. Path Following is not Pathfinding How your AI determines to follow this path is something completely different and is referred to as Path Following. The typical strategy for Path Following is to allow your AI to look ahead of where it's traveling to see if it can short cut the path to make more natural curved movements. Havok AI Demo at GDC 2011