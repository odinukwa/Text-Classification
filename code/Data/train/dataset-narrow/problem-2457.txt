Lov Grover published an article in 1997 in which he shows that if you can query the database on multiple items, then a single query suffices to find the marked element. However, it requires a number of preprocessing and postprocessing steps in $\Omega(N\log N)$. If you let $S_1, \dots, S_N$ denote the elements of the database, you query the oracle with the string $S_{i_1}, \dots, S_{i_\eta}$ for some number $\eta$ and the oracle returns $1$ if the marked state appears an odd number of times in the string and $0$ if it appears an even number of times. You query this oracle on a superposition $(|S_1\rangle+ \dots+ |S_N\rangle)^\eta$ and then apply the inversion about the mean operator from Grover's algorithm. Now in each the the $\eta$ subsystems, the marked element has a greater amplitude than the unmarked ones. Measuring all subsystems yield the marked state with greater probability and to have sufficient certitude about the resulting state, $\eta$ must be in $\Omega(N\log N)$. 

Note that when the poset $I$ has no arcs, then we recover the usual constructions (disjoint union and function dominance). In particular I would like to know whether (i) there could exist an axiomatic / category-theoretic definition of these "directed" operations, (ii) there are other examples of structures displaying this property. 

I update my reply in light of Aravind's comment. The claim in my previous answer was clearly incorrect but, in my defence, I never had a good intuition about graphs. By adapting Aravind's argument you can show an upper bound of $O(\sqrt{n})$, as every $n$-vertex perfect graph has an independent set or a clique of size $\sqrt{n}$. It should be possible to show the tightness of the bound by reasoning on the tree-representation of the cograph. Consider for instance a complete binary tree $T$ with $N = 2^n$ vertices, and label its nodes as series/parallel in an alternating fashion. It represents a cograph $G$, and any $P_3$-free subgraph $H$ of $G$ must span a subtree of $T$ that does not contain three alternations along a root-leaf path; it should imply that this graph may have at most $O(\sqrt{N})$ vertices, but I leave the details to you. A related question would be to look for a partition in bicluster graphs, as they are exactly the bipartite $P_4$-free graphs. Observe that this problem can be solved in polynomial time by computing the chromatic number of the graph: an arbitrary pairing between color classes then gives the optimal partition. 

I am currently searching for a Ph.D. project and this is the kind of stuff that interests me. I would certainly like to contribute to solving this problem, if it is still open. That is why I would like to know some of the recent developments on this. Is this still an open problem? What contributions were made in the last decade? 

Regarding question A, it is possible to adapt the PRNG to yield uniformly distributed numbers using this. However, the range of this new uniform generator has size smaller than $K$, unless the PRNG is itself uniform. The number of possible outcomes of this new generator will depend on the min-entropy of the original generator. To get some intuition on why this is so, try to view the problem from an adversarial point of view where the goal is to guess the outcome of the PRNG. If $P_{guess}>1/K$ is the probability of the most probable outcome of the PRNG, then there is no deterministic way to lower the adversary's guessing probability without adding randomness. Therefore if you are to transform the outcome to a uniform distribution then all the equaly likely outcomes must happen with probability at least $P_{guess}$, so there can be at most $1/P_{guess}$ outcomes. Of couse, I'm assuming that the adversary knows the deterministic procedure that is applied, as is usually the case in cryptography. So the answer to question B is that this nesting trick does not increase randomness, it can only decrease it. There is no deterministic procedure that can increase randomness. Hope this helps 

It can be seen that this property is preserved by some natural operations: parallel composition, adjonction of a minimal or maximal element. This immediately implies the Knuth hook formula for forests, although it has a number of different proofs (including some of algorithmic nature). I'd like to know if there are examples of other operations preserving/breaking the existence of a hook length formula, in particular is it possible to define a gluing operation extending the above adjonction operation? 

Some structures have a property of closure by a "sum" or "product" operation. Given a family of structures $(S_i)_{i \in I}$, we can then define a new structure denoted by $\sum_{i \in I} S_i$, resp. $\prod_{i \in I} S_i$; structures enjoying this property are vector spaces for instance. I consider these operations as "undirected" as the result does not depend on the order of the summands, up to isomorphism. I am interested in structures for which we can define a "directed sum" / "directed product" operation, meaning that the definition of the sum / product could now depend on the structure of the index set $I$ (e.g. it could be a totally ordered set and we would compute non-commutative sums / products according to this order). To illustrate this, consider the following definitions for posets. Let $I = (G,\leq)$ be a poset and for each $i \in G$ let $S_i = (U_i,\leq_i)$ be a poset. 

I am working on an algorithm for which the running time is a random variable $X$ that has finite expected value, but infinite variance. Are there examples of other algorithms for which this is the case? Are any such algorithm used in practice, or does the infinite variance make the running time too unpredictable? 

The decisional version of your problem can be written in the folowing form: $$EQ(s_1,t_1)\lor EQ(s_2,t_2)\lor \dots \lor EQ(s_n,t_n).$$ This is very similar to the inner product $$x_1y_1\lor x_2y_2\lor\dots\lor x_ny_n.$$ The communication complexity of those two problems is equivalent, since equality can be done at constant randomized communication cost. Also, finding an $i$ such that $s_i=t_i$ is at least as hard as the decisional version of the problem. Therefore, your problem is as hard as solving $IP$ when only one clause is satisfied. I have no direct proof, but my intuition lets me believe that $IP$ with only 1 satisfied clause is as hard as general $IP$. So the randomized complexity of your problem would be in $\Theta(n)$ if I am not mistaken. I hope this helps. 

Let $w_r$ be the random word assigned to the root at the end of this process. Can we devise a polynomial-time strategy to maximize $Var(w_r) = \sum_{i \in [n]} Var(w_r[i])$? This could be done either 'globally' (by constructing the tree at once) or 'online' (by selecting a 'locally optimal' matching for each generation). The intuition is that a node $u$ represents a 'genetic group' which profiles is described by the distribution of $w_u$, and that we would try to mix groups at each generation in order to maximize the genetic diversity. This is probably NOT desirable in practice due to the possible effects of dominant/recessive mutations. 

By a graded PSG, we mean a pair $(S,r)$ consisting of a PSG $S = (X,*)$ and of a rank function $r : X \rightarrow \mathbb{N}$ such that whenever $x*y$ is defined, $r(x*y) = r(x)+r(y)$. This holds for instance for the interval algebra of an Eulerian poset (see On the Foundations of Combinatorial Theory I. Theory of Moebius Functions, by G.C. Rota). By analogy with the situation in posets, we may define a 'triangular matrix' $\lambda \in \mathbb{R}^X$ by $\lambda(x) = 1$ for every $x \in X$, and a mapping $\zeta : \mathbb{R}^X \rightarrow \mathbb{R}^X$ by $\zeta(x) = \lambda * x$. It is known that for a partial semigroup obtained from the interval algebra of an Eulerian poset, the function $\lambda$ has an inverse $\mu \in \mathbb{R}^X$ given by $\mu([x,y]) = (-1)^{r(y)-r(x)}$. This suggests a possible extension to certain graded PSGs, which leads me to ask the following. Question: for a graded PSG, under what conditions does $\lambda$ have an inverse $\mu$? When this holds, is it always true that $\mu(x) = (-1)^{r(x)}$? (NOTE: I renamed the rank function $r$ instead of $l$ for greater consistency.) 

I wouldn't think so. I'm assuming your $a_n$'s are normalized, i.e. that $||\sum_n a_n |n\rangle|| =1$. But then your wish output is not normalized since $$||\sum_n e^{ia_n} |n\rangle|| = \sqrt{\sum_n |e^{ia_n}|^2} = \sqrt N$$ where I assume there are $N$ terms in the sum. Quantum (unitary) operations have to preserve the norm so there can't be one that maps the first state to the second one. 

Brassard, Hoyer, Mosca and Tapp showed that the generalized Grover search, called amplitude amplification, can be used to obtain a quadratic speed-up on a large class of classical heuristics. The intuition behind their idea is that classical heuristics use randomness to search for a solution to a given problem, so we can use amplitude amplification to search the set of random strings for one that will make the heuristic find a good solution. This yields a quadratic speed-up in the running time of the algorithm. See section 3 of the paper linked above for more details. 

I'm interested in the possibility of fast algorithms for the following two problems on permutations. 1) Given a permutation $\pi$ and an integer $k$, compute a pair $(\mathcal{C},\mathcal{A})$ where $\mathcal{C}$ is a maximum $k$-chain and $\mathcal{A}$ is a corresponding $l$-antichain certifying the optimality of $\mathcal{C}$. If you only care about $\mathcal{C}$, this can be done in $O(k n \log n)$ by an involved algorithm ('Maximum k-Chains in Planar Point Sets: Combinatorial Structure and Algorithms' by S. Felsner and L. Wernisch). It seems possible to obtain a simpler algorithm that would also provide a certificate. 2) Given a permutation $\pi$, compute the pair of Young tableaux $(P,Q)$ associated to $\pi$ by the Robinson-Schensted bijection. Is it possible to do it in subquadratic time? Note: if we only care about the first $k$ rows of each tableau this can be done in $O(k n \log n)$ time in the RAM model; by Greene theorem, the cumulated size of the first $k$ rows gives the cost of an optimum solution for 1), without providing an explicit solution. Thus question 1) seems to require a different approach. 

Back in 2005, Scott Aaronson posted a list of 10 "semi-grand" challenges for quantum computing theory which contained the following challenge: 

For computational complexity, there is no proof that quantum computers are better than classical computers because of how hard it is to obtain lower-bounds on the hardness of problems. However, there are settings in which a quantum computer provably does better than a classical one. The most famous of these examples is in the blackbox model in which you have access via blackbox to a function $f:\{0,1\}^n\mapsto \{0,1\}$ and you want to find the unique $x$ for which $f$ evaluates to 1. The complexity measure in this case is the number of calls to $f$. Classicaly, you cannot do better than guessing $x$ at random which takes on average $\Omega(2^n)$ queries to $f$. However, using Grover's algorithm you can achieve the same task in $O(\sqrt{2^n})$. For further provable separations, you can look into communication complexity where we know how to prove lower bounds. There are tasks that two quantum computers communicating through a quantum channel can accomplish with less communication than two classical computers. For example computing the inner product of two strings, one of the hardest problems in communication complexity, has a speedup when using quantum computers. 

Are there linear-time recognition algorithms for these classes? By the previous remark, an algorithm for class 2 would immediately yield an algorithm for class 1, although I suspect a more direct algorithm to exist in that case. What is the complexity of the subpattern problem for two skew-merged or two vexillary permutations? The subpattern problem is known to be polynomial for separable and 2-increasing permutations, and these classes seem the next to study. 

A partial semigroup (or PSG) consists of a set $X$ and of a partial composition law $*$ defined over $X$, that is to say: (1) $x*y$ is not always defined, (2) if $(x*y)*z$ is defined, so is $x*(y*z)$, and the two values are equal, (3) if $x*(y*z)$ is defined, so is $(x*y)*z$, and the two values are equal. Let $\mathbb{R}$ be a ring. We may then define a convolution product over $\mathbb{R}^X$ : given two functions $f,g$, we define a function $f*g$ such that 

In stochastic simulation, we are often interested in estimating the expected value of a random variable. The expected value of a continuous random variable is an integral over the real numbers. To estimate this quantity, we use the Monte Carlo method which consists of generating instances of this random variable from pseudorandom uniform variables. From these uniform variables, we can generate random variables from a given distribution by inverting the cumulative distribution function which is defined itself as an integral. 

Not sure if this is directly linked to your question, but reading it made me think about an article by Peter Høyer I read some years ago. In it, he shows how the most popular quantum algorithms like Grover's or Shor's follow the same pattern of applying what he calls "conjugated operators" and he builds new algorithms also based on that same pattern. As I said, it's been a few years since I've read it so my description is a bit sloppy, but here's the link in case you want to check it out. $URL$