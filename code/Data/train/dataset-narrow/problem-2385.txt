$\text{tailNext}(\vec{e})$: The dart leaving $\text{tail}(\vec{e})$ next in counterclockwise order after $\vec{e}$. $\text{flip}(\vec{e})$: The “same” dart as $\vec{e}$, but with $\text{left}(\vec{e})$ and $\text{right}(\vec{e})$ swapped. $\text{rotate}(\vec{e})$: The dual dart obtained by giving $\vec{e}$ a quarter turn counterclockwise around its midpoint. 

Write a paper that you're proud of and that your target audience will be eager to read. (This requires knowing what your target audience cares about.) There are roughly three classes of journals: 

Consider an electrical network modeled as a planar graph G, where each edge represents a 1Ω resistor. How quickly can we compute the exact effective resistance between two vertices in G? Equivalently, how quickly can we compute the exact current flowing along each edge if we attach a 1V battery to two vertices in G? Kirchhoff's well-known voltage and current laws reduce this problem to solving a system of linear equations with one variable per edge. More recent results — described explicitly by Klein and Randić (1993) but implicit in earlier work of Doyle and Snell (1984) — reduce the problem to solving a linear system with one variable per vertex, representing that node's potential; the matrix for this linear system is the Laplacian matrix of the graph. Either linear system can be solved exactly in $O(n^{3/2})$ time using nested dissection and planar separators [Lipton Rose Tarjan 1979]. Is this the fastest algorithm known? Recent seminal results of Spielman, Teng, and others imply that the Laplacian system in arbitrary graphs can be solved approximately in near-linear time. See [Koutis Miller Peng 2010] for the current best running time, and this amazing article by Erica Klarreich at the Simons Foundation for a high-level overview. But I'm specifically interested in exact algorithms for planar graphs. Assume a model of computation that supports exact real arithmetic in constant time. 

For the past three years, SOCG has included a satellite event called the Young Researchers Forum, which is a venue for early students and other newcomers to computational geometry to present preliminary or ongoing research. It's not meant specifically for undergraduates, but undergrads are certainly welcome. Most major theory conferences have been able to attract funding from NSF to support student travel. Again, undergraduates are eligible for this funding, although students speaking at the conference (including the Young Researchers Forum at SOCG) generally have higher priority. 

Have any more exact values of $V_t(n)$ been computed in the last 36 years? I'm particularly interested in exact values of $M(n) = V_{\lceil n/2 \rceil}(n)$, the minimum number of comparisons required to compute the median. 

Hopcroft's problem is conjectured to require $\Theta(n^{4/3})$ time: Given a set of $n$ points and a set of $n$ lines in the plane, does any point lie on any line? 

First of all, computational geometers don't think of it as the BSS model. The real RAM model was defined by Michael Shamos in his 1978 PhD thesis (Computational Geometry), which arguably launched the field. Franco Preparata revised and extended Shamos' thesis into the first computational geometry textbook, published in 1985. The real RAM is also equivalent (except for uniformity; see Pascal's answer!) to the algebraic computation tree model defined by Ben-Or in 1983. Blum, Shub, and Smale's efforts were published in 1989, well after the real-RAM had been established, and were almost completely ignored by the computational geometry community. Most (classical) results in computational geometry are heavily tied to issues in combinatorial geometry, for which assumptions about coordinates being integral or algebraic are (at best) irrelevant distractions. Speaking as a native, it seems completely natural to consider arbitrary points, lines, circles, and the like as first class objects when proving things about them, and therefore equally natural when designing and analyzing algorithms to compute with them. For most (classical) geometric algorithms, this attitude is reasonable even in practice. Most algorithms for planar geometric problems are built on top of a very small number of geometric primitives: Is point $p$ to the left or right of point $q$? Above, below, or on the line through points $q$ and $r$? Inside, outside, or on the circle determined by points $q,r,s$? Left or right of the intersection of segments $qr$ and $st$? Each of these primitives is implemented by evaluating the sign of a low-degree polynomial in the input coordinates. (So these algorithms can be described in the weaker algebraic decision tree model.) If the input coordinates happen to be integers, these primitives can be evaluated exactly with only constant-factor increase in precision, and so running times on the real RAM and the integer RAM are the same. For similar reasons, when most people think about sorting algorithms, they don't care what they're sorting, as long as the data comes from a totally ordered universe and any two values can be compared in constant time. So the community developed a separation of concerns between the design of “real” geometric algorithms and their practical implementation; hence the development of packages like LEDA and CGAL. Even for people working on exact computation, there is a distinction between the real algorithm, which uses exact real arithmetic as part of the underlying model, and the implementation, which is forced by the otherwise irrelevant limitations of physical computing devices to use discrete computation. Within this worldview, for example, the most important open problem in computational geometry is the existence of a polynomial-time algorithm for linear programming. No, the ellipsoid and interior-point methods don't count. Unlike the simplex algorithm, those algorithms aren't guaranteed to terminate unless the constraint matrix happens to be rational. (There are combinatorial types of convex polytopes that can only be represented by irrational constraint matrices, so this is a nontrivial restriction.) And even when the constraint matrix is rational, the running times of those algorithms aren't bounded by any function of the input size (dimension$\times$#constraints). There are a few geometric algorithms that really do rely heavily on the algebraic computation tree model, and therefore cannot be implemented exactly and efficiently on physical computers. One good example is minimum-link paths in simple polygons, which can be computed in linear time on a real RAM, but require a quadratic number of bits in the worst-case to represent exactly. Another good example is Chazelle's hierarchical cuttings, which are used in the most efficient algorithms known for simplex range searching. These cuttings use a hierarchy of sets of triangles, where the vertices of triangles at each level are intersection points of lines through edges of triangles at previous levels. Thus, even if the input coordinates are integers, the vertex coordinates for these triangles are algebraic numbers of unbounded degree; nevertheless, the algorithms for constructing and using cuttings assume that coordinates can be manipulated exactly in constant time. So, my short, personally biased answer is this: TTE, domain theory, Ko-Friedman, and other models of “realistic” real-number computation all address issues that the computational geometry community, on the whole, just doesn't care about. 

As Tsuyoshi points out in the comments, the following answer is about upper bounds for $I$ in terms of $F$ and $V$, while the original question is about the lower bound $I \ge 2(F+V)$. (Sorry!) See David Eppstein's response for the correct answer to the original question. 

Here is a diagram of the algorithm for $n=18$ and $k=4$. Data travels from left to right; each box is a $k$-sorter. 

One of my friends spent a sabbatical year watching a baseball game in every major league stadium in North America. Without flying. (He didn't quite succeed; three stadiums were under construction that year.) 

Maybe. Becker, Coron, and Joux [EUROCRYPT 2011 describe an algorithm to solve random hard instances of Subset Sum (in fact, the more general Knapsack problem) in $\tilde{O}(2^{0.291n})$ time, beating the "meet in the middle" time bound $\tilde{O}(2^{n/2})$ by Shamir and Schroeppel [SICOMP 1981]. (Here, $\tilde{O}(f(n))$ is standard shorthand for $O(f(n)\operatorname{polylog} f(n))$.) More recently, Dinur, Dunkelman, Keller, and Shamir [Crypto 2012], which establish better time-space tradeoffs (but do not improve worst-case running times) for a general class of cryptography problems that includes Knapsack. Becker's techniques rely crucially on the observation that addition is both commutative and associative, but Dinur's techniques do not; for example, they also derive faster algorithms for solving random hard instances of permutation puzzles like the Rubik's cube. It seems likely that Dinur's techniques would imply faster algorithms for (at least random hard instances of) your problem. 

The sums of square roots problem: Given two sequences $a_1, a_2, \dots, a_n$ and $b_1, b_2, \dots, b_n$ of positive integers, is $A := \sum_i \sqrt{a_i}$ less than, equal to, or greater than $B := \sum_i \sqrt{b_i}$? 

I need some suggestions on what skills or knowledge I should accumulate before I graduate and apply for graduate school, or get a job in industry (hopefully more than just programming). Basically, I'm looking for a good direction. What are you really passionate about? Among the things that you're really passionate about, what are you really good at? Do that. Talk to everyone you can who does that. Figure out who does that well and who just spouts shiny nonsense about doing that. Ask the good ones how they got good at doing that. Do what they do. Most of what they do won't work for you, but until you try and fail yourself, you won't know what works and what doesn't. And you will fail; brush it off, get back up, and keep doing that. Don't avoid the uncomfortable, frustrating, boring, but necessary stuff you have to do to learn to do that well and to keep doing that. Oh, and as long as you're in college, you might as well take some classes. 

As @MarkusBläser points out, Knuth's table seems to already incorporate more recent results from Bill Gasarch, Wayne Kelly, and Bill Pugh (Finding the ith largest of n for small i,n. SIGACT News 27(2):88-96, 1996.) 

You can guess which method I recommend. My recent journal paper with Sergio Cabello and Erin Chambers describes both of these perturbation schemes in more detail. There are also more specialized methods for specific algorithms and specific classes of graphs. 

Matoušek's stronger time bound is correct. The proof of Theorem 6.1 (in the journal version) uses an indirection trick that reduces the space bound required for logarithmic query time from $O(n^d)$ to $O(n^d/\operatorname{polylog} n)$. Intuitively, the trick is to cluster the points into subsets of polylogarithmic size, build a linear-space data structure for each subset, and then build a standard logarithmic-query-time structure over the subsets. Plugging the improved space bound into Matoušek's multi-level/tradeoff machinery—described in gory generality in the longer version of Agarwal's survey—gives Matoušek's form of the time-space tradeoff. (In fact, the indirection trick is just a very careful application of the standard tradeoff machinery.) 

— James Mill (writing pseudonymously as “P.Q.”), “Theory and Practice”, London and Westminster Review, April 1836 

This is a well-known variant of Klee's measure problem. The best algorithm known, due to Timothy Chan, runs in roughly $O(n^{d/2})$ time, ignoring polylogarithmic factors. Faster algorithms are known for the special cases of fat boxes and boxes with one or more fixed coordinates; see recent work by Karl Bringmann and Hakan Yıldız, respectively. 

I don't know. In particular, the randomized perturbation scheme I described does not have this property. 

The Zone Theorem is a key step in the proof that the standard recursive incremental algorithm to build an arrangement of $n$ hyperplanes in $\mathbb{R}^d$ runs in $O(n^d)$ time. In 1990, Raimund Seidel discovered that the published proof was incorrect, after being challenged on a subtle technical point by a student in his computational geometry class. Meanwhile, a huge literature on hyperplane/halfspace/simplex/semialgebraic range searching had been developed, all of which relied on the $O(n^d)$ construction time for arrangements, which in turn relied on the Zone Theorem. (None of those authors noticed the bug. Raimund had taught the published "proof" in detail for several years before he was challenged.) Fortunately, Edelsbrunner, Seidel, and Sharir almost immediately found a correct (and much simpler!) proof of the Zone Theorem [New Results and New Trends in CS 1991, SICOMP 1993]. 

Give away USB sticks as proceedings Yes, the papers are online, but conferences rarely have working wireless in the conference rooms or nearby public areas. CD-ROMs are the current electronic medium of hoice, but a large fraction of people have tablets and netbooks without CD drives. USB drives cost less than dirt, and they work with almost any computer whose name doesn't match the regexp "iP*". If you don't want to pre-fill each USB stick with the electronic proceedings, let people fill their own sticks with the electronic proceedings, either from a PC (or three) at the registration desk, or from a colleague who already dumped their stick onto some other device. 

The main motive behind this question is to understand where am I standing with my own performance and productivity. Then you're asking the wrong question. The right metric to examine isn't the number of publication, but rather your visibility and reputation (and ultimately impact) within the research community. If the intellectual leaders of your target subcommunity are eager to write you good recommendation letters, it doesn't matter whether you publish one paper a year or ten. Similarly, if the intellectual leaders in your target subcommunity are not eager to write you good recommendation letters, it doesn't matter whether you publish one paper a year or ten.