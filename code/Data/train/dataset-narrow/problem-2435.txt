$O$ contains exactly $n$ points. Such $n$ points are linearly independent. Such $n$ points are those at minimum distance from the hyperplane represented by $c$. More precisely, let $d( x, c )$ be the distance of a point $x \in \{0,1\}^n$ from the hyperplane $c$. Then, $\forall B \subseteq S_c$ such that $B$ satisfies 1 and 2 it is the case that $\sum_{x \in B} d(x, c) \geq \sum_{x \in O} d(x, c)$. In other words $O$ is, among all the subsets of $S_c$ satisfying both conditions 1 and 2, the one that minimizes the sum of the distances of its points from the hyperplane $c$. 

As a TCS amateur, I'm reading some popular, very introductory material on quantum computing. Here are the few elementary bits of information I've learned so far: 

Since you now want to share your algorithm, my personal suggestion is the following: build a very simple web site. The site should make available these 2 things: 

Basically, a DNF formula is a disjunction of clauses $c_1 \lor ... \lor c_m$, where each clause $c_i = l_{i,1} \land ... \land l_{i,k}$ is a conjuction of literals. Let's call a clause $c_i$ conflicting if and only if it contains both a literal $l$ and its negation $\lnot l$. It's easy to see that each non-conflicting clause just encodes $2^{n-k}$ solutions of the formula. So the whole DNF is just an enumeration of solutions. A formula may have exponentially many solutions, so the corresponding DNF formula may have exponentially many clauses. Try to convert this CNF formula: $l_1 \lor l_2 \lor l_3 \lor l_4$ $l_5 \lor l_6 \lor l_7 \lor l_8$ $l_9 \lor l_{10} \lor l_{11} \lor l_{12}$ $l_{13} \lor l_{14} \lor l_{15} \lor l_{16}$ $l_{17} \lor l_{18} \lor l_{19} \lor l_{20}$ to its corresponding DNF formula: you'll get too many clauses. In one word: CNF is compact, while DNF is not; CNF is implicit, while DNF is explicit. The following problem is NP-complete: given a DNF instance, is there an assignment of variables that falsifies all the clauses? 

Brendan McKay's nauty (No AUTomorphisms, Yes?) program solves the canonical labeling problem of graphs (simultaneously solving the Graph Isomorphism and Graph Automorphism problems) and has exponential worst-case performance (Miyazaki, 1996). However, it works very quickly for most graphs, especially those with a few automorphisms. Specifically, the algorithm begins by partitioning the vertices by degree, then by the degree between each part. When this process stabilizes, a choice must be made to distinguish a vertex in a non-trivial part, and this leads to the exponential behavior. In most graphs, the depth of this branching procedure is small. 

Write everything, all the time. In TeX, preferably. Whether you are considering a question or proving a lemma, put it in a digital format as soon as possible. Write the necessary background. Try to keep the thoughts organized in a narrative. Having all of these things in a digital form makes paper-writing much easier, but still a lot of work. In my experience, it helps a lot to start again from scratch. This allows for a clean start to find an improved organization. Also, proofs are always easier the second or third time around. 

My question is, does this latter claim actually hold and if it does, is there a write-up of the proof somewhere? As a background, I've been trying to understand the area around the Exponential Time Hypothesis. IPZ define subexponential problems as ones that have $O(2^{\varepsilon n})$ algorithm for each $\varepsilon > 0$, but this apparently is not sufficient in the light of the current knowledge to imply the existence of a subexponential algorithm for the problem. The same gap seems to be present in the SERF reducibility, but I am partially expecting that I am missing something here... 

For the multiple pattern case, it seems that simply scanning for each of the might be the best possible solution, at least unless the strong exponential-time hypothesis fails. Recall that given sets $S_1, S_2, \dotsc, S_n$ and $T_1, T_2, \dotsc, T_n$ over universe $[m]$, if we could decide if there are $S_i$ and $T_j$ such that $S_i \cup T_j = [m]$ in time $O(n^{2-\varepsilon}\operatorname{poly}(m))$, then SETH fails, i.e. we have a CNF-SAT algorithm with running time $O^*\bigl(2^{(1-\varepsilon/2)n}\bigr)$. Given sets $S_1, S_2, \dotsc, S_n$ and $T_1, T_2, \dotsc, T_n$, we encode the above problem as multi-pattern matching with don't cares over binary alphabet as follows: 

As a decision problem, Monotone-2SAT is trivial. The answer is always YES: just set every variable to true. But consider its counting version, called #Monotone-2SAT: 

The number $C$ of vertex covers of a graph $G = (V, E)$ can be either polynomial in $|V|$ or superpolynomial in $|V|$. $C$ being superpolynomial in $|V|$ doesn't necessarily mean that $C$ is hard to determine. For instance, consider the graph $G = (V, E)$ where $|V| = 2n$ and $E$ is defined as $E = \{ \{ i,j \} | i \mod 2 = 1 \land j = i + 1 \}$: such graph has a number of vertex covers $C=3^{|E|} = 3^{\frac{|V|}{2}}$ which is both superpolynomial in $|V|$ and very easy to determine. On the other hand, knowing in advance that $C$ is polynomial in $|V|$ allows us to use brute force keeping the running time polynomial in $|V|$. So it seems that determing $C$ is hard only if it is superpolynomial in $|V|$, but not always (as the above example demonstrates). 

The ODD EVEN DELTA problem is #P-hard, even on 3-regular bipartite planar graphs. Let $\mathcal{C}$ be the set of vertex covers of a general graph $G$. Then, assuming $G$ has no isolated vertices, the following equation holds (refer to the above article for the proof): $$|\mathcal{C}| = 2^{|V|} - \sum_{k = 2}^{|V|} \Delta_k \cdot 2^{|V|-k}$$ Counting vertex covers is #P-complete even on 3-regular bipartite planar graphs, and it can be done with a linear number of calls to an ODD EVEN DELTA oracle. 

Now it's clear that a pattern $1\langle S_i \rangle 1$ can match the text at an occurrence of $1[T_j]1$, and only when $S_i \cup T_j = [m]$. The total length of patterns and the length of the text are both $O(nm)$, for instance so a near-linear single-pass algorithm for multiple patterns would give substantial improvements over best known CNF-SAT algorithms... (Note that this does not say anything about algorithms that use lots of time preprocessing the patterns, say, quadratic in the total length of the patterns.) 

The maximum independent set problem is a packing problem (you can think it as packing disjoint stars), and it has a well-known algorithm with running time $2^k \operatorname{poly(n)}$ in graphs with treewidth at most $k$. 

I have a question concerning the SERF-reducibility of Impagliazzo, Paturi and Zane and subexponential algorithms. The definition of SERF-reducibility gives the following: 

This manuscript seems to prove exactly that. (It doesn't; the complexity parameter is $|T|$, not the treewidth.) In general, most NP-hard optimisation problems have polynomial-time algorithms when the input is restricted to bounded-treewidth graphs. These algorithms use the rather well-known tree-decomposition machinery, which is also used in the linked paper. EDIT: The undirected Steiner tree is, on the other hand, known to be fixed-parameter tractable with regards to parameter $w$ the treewidth of the underlying graph. My suggestion would be to try and adapt this algorithm to the directed case, which would in particular give a polynomial time algorithm for series-parallel graphs. 

De Loera, Lee, Malkin, and Margulies developed an algorithm for determining infeasibility of certain combinatorial problems using Hilbert's Nullstellensatz. For example, they take the constraints of 3-coloring a graph and describe that problem as a system of polynomial equations. They compute a set of certificate polynomials which verify that these polynomials have no solutions (using the Nullstellensatz). To make this computation finite, they bound the degree of the certificates. For the 3-coloring case, they were unable to find a non-3-colorable graph which required degree higher than four for the certificate. If P $\neq$ NP, then P $\neq$ coNP and the degree of a Nullstellensatz Certificate for this problem is unbounded, as a finite bound would yield a polynomial-time algorithm. 

To get things rolling, I have a potential game and would like feedback. Let $k \geq 2$ be an integer and $m$ be an integer at least $3k+1$ with $m \not\equiv 0 {\pmod {k+1}}$. The cycle-power game is the 2P1R game where the provers attempt to convince the verifier that the graph $C_{m}^k$ is $k+1$ colorable. Here, $C_m^k$ is the graph with vertices given by integers modulo $m$ with edges if the mod-$m$ distance is at most $k$. If there is a $k+1$-coloring of $C_m^{k}$, it must be given by choosing an ordering of $\{1,\dots,k\}$ and coloring the numbers $\{0,\dots,m-1\}$ in this order, since each set of sequential $k+1$ integers in $\{0,\dots,m-1\}$ form a clique. Since $m$ is not a multiple of $k+1$, there will be some point where this coloring fails. The verifier either asks for a single vertex from both players, to verify that the colors match, or asks for an edge to verify that the colors are different. I believe this is a good example for two reasons: 

In exact exponential algorithmics, the subset convolution is a particularly useful algebraic technique for solving covering, packing and partitioning problems. It generally works very well when the objects to be packed are 'hard', like dominating or independent sets. For example, the best known $k$-colouring (i.e. partition into independent sets) algorithm uses subset convolution. See e.g. Exact Exponential Algorithms by Fomin and Kratsch or this paper by Björklund et al. The subset convolution is most often used in the context of exponential algorithms, but there are some useful applications in polynomial side of things. In particular, see this paper for the so-called 'counting in halves' approach to packing problems. My intuition is that since the subset convolution -type methods count the number of solutions instead of finding just one, they usually cannot be used to obtain fixed parameter algorithms. Also, they are rather space-intensive; their space complexity often equals their time complexity. 

Let $[n] = \{ 1, 2, \dotsc, n \}$, and let $\mathcal{F} = \{S_1, S_2, \dotsc, S_m \} \subseteq 2^{[n]}$ be the input set family. Unless I misunderstood your problem formulation, we want to find a minimum-size set $T \subseteq [n]$ such that $T \not\subseteq S_i$ for all $i = 1, 2, \dotsc, m$. To answer your question, note that $T \not\subseteq S_i$ if and only if $T \cap ([n] \setminus S_i) \not= \emptyset$. That is, $T$ has to intersect the complement of each $S_i$. But this means that your problem is, essentially, equivalent to the hitting set problem (consider hitting set with input $\mathcal{G} = \{ [n] \setminus S_i \ \colon \ i = 1, 2, \dotsc, m \}$): 

Counting the number of perfect matchings in a bipartite graph is immediately reducible to computing the permanent. Since finding a perfect matching in a non-bipartite graph is in NP, there exists some reduction from non-bipartite graphs to the permanent, but it may involve a nasty polynomial blowup by using Cook's reduction to SAT and then Valiant's theorem to reduce to the permanent. An efficient and natural reduction $f$ from a non-bipartite graph $G$ to a matrix $A = f(G)$ where $\operatorname{perm}(A) = \Phi(G)$ would be useful for an actual implementation to count perfect matchings by using existing, heavily-optimized libraries that compute the permanent. Updated: I added a bounty for an answer including an efficiently-computable function to take an arbitrary graph $G$ to a bipartite graph $H$ with the same number of perfect matchings and no more than $O(n^2)$ vertices. 

This answer is more of a toy problem than a real research problem. My typical example of a log-space algorithm to give to programmer friends is the following puzzle: 

This process will eventually terminate. If there is no loop, it will take $n$ steps. If there is a loop, the two-step pointer cannot pass the one-step pointer without a collision, and this occurs before the one-step pointer finishes the loop (which is under $n$ steps). 

Hitting set is known to be NP-complete and cannot be, loosely speaking, solved faster than in time $O(2^n)$ unless the Strong Exponential-time Hypothesis fails. 

Networks, Crowds, and Markets: Reasoning About a Highly Connected World, by Easley and Kleinberg probably should be mentioned here. It is rather elementary, but gives a wide selection of social sciences topics that have been considered from a CS point of view and provides a lot of references. Someone with more experience in the field can perhaps tell us how close the book is to the current state of the art in the field? As a more particular answer, with the proliferation of various social networking sites, computer science has become quite relevant in analysing the huge social network data sets from such sites. 

If you're interested in set families with $n = \omega(2^{d/2})$, then an another solution conceptually very similar to the one outlined in Yuval's answer is to compute zeta transform $$f\zeta(T) = \sum_{S \subseteq T} f(S)\,,$$ where $f \colon 2^{[d]} \to \mathbb{R}$ is the indicator function of the input family $\mathcal{F} = \{ S_1, S_2, \dotsc, S_n \}$. That is, $f(S) = 1$ if $S \in \mathcal{F}$ and $f(S) = 0$ otherwise. Clearly there are sets $S_i \not= S_j$ such that $S_i \subseteq S_j$ if and only if $f\zeta(S) > 1$ for some $S \in \mathcal{F}$. The zeta transform can be computed in time $O(d2^d)$ using Yates's algorithm, see for example Knuth's TAOCP, vol. 2, §4.6.4. The algorithm itself is a fairly straightforward dynamic programming, and it is easy to modify it to give an example of an included set if one exists.