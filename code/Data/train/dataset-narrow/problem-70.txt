You can adjust the quality by altering the option in the command line (lower numbers are better quality). Don't forget to change the resolution () and framerate () to match what you want, as well. Depending on the quality, how fast your PC is, etc, it might not quite work in real-time. It's a good idea to set up your app to use a fixed per-frame timestep when you're in video capture mode, so the resulting animation doesn't depend on how much slowdown there was in the capture process. 

The endpoint of the line doesn't extend out to the edge of the box because you're using the circle equation with a fixed radius: 

A good way to improve sprite rendering efficiency is geometry instancing. This lets you define a mesh using one vertex/index buffer pair, then render many instances of that mesh in a single draw call using a second vertex buffer whose elements define the instances. This is more widely supported than geometry shaders; in particular, it's is supported in WebGL via the ANGLE_instanced_arrays extension. To apply this to sprites, one vertex buffer could define the shape of a sprite (i.e. a quad made of two triangles), then another vertex buffer could contain per-sprite information such as position, rotation, size, and so on. If there are 4 vertices per sprite and N sprites, the vertex shader will be called a total of 4N times, onces for each vertex in each sprite. You'd use the vertex shader to put the information from the two buffers together, transforming each vertex appropriately to the sprite's properties. This allows the GPU to do the work of generating all the vertices for the sprites, and saves on data uploaded from the CPU. Coincidentally, I published a small OpenGL instancing demo recently. Using instancing this way does require that all the sprites are drawn with the same state, i.e. the same shader, textures, blending mode and so on. If different states are needed for some sprites, a separate batch of instances will be needed for each state combination. 

Physically, the origin of diffuse light is subsurface scattering, which happens continuously as light travels through a material. So, the proportion of transmitted light depends on the thickness of the object. There's no precise equivalent to the Fresnel law, but maybe the closest thing is the Beer–Lambert law. It states that the transmitted light falls off exponentially with distance, proportional to $e^{-\sigma x}$, where $x$ is the distance the light has traveled through the material and $\sigma$ is the extinction coefficient (with units of inverse length). The non-transmitted portion of the light may be partly absorbed, and partly scattered through the material. You get the appearance of "diffuse" when typical length scale of scattering is smaller than you can see ($1/\sigma \ll 1\text{ pixel}$). Then the light gets scattered so many times in such a short distance that by the time it comes back out, its distribution looks isotropic (Lambertian). The apparent diffuse color is controlled by the wavelength-dependence of the scattering and attenuation coefficients. When the scattering length scale is larger, you get a soft appearance to the material (as in wax, marble, human skin, etc), or partial translucency. 

This time we ended up with the game running at 36 fps, the maximum speed possible given the CPU load in this case. But this isn't a divisor of the refresh rate, so we get uneven frame delivery to the display, with some frames shown for two vsync periods and some for only one. This will cause judder. Here the GPU goes idle because it's waiting for the CPU to give it more work—not because it's waiting for a backbuffer to be available to render into. The swapchain thus doesn't put back-pressure on the GPU (or at least doesn't do so consistently), and the GPU in turn doesn't put back-pressure on the CPU. So the CPU just runs as fast as it can, oblivious to the irregular frame times appearing on the display. It's possible to "fix" this so that the game still discretizes to vsync-friendly rates even in the CPU-limited case. The game code could try to recognize this situation and compensate by sleeping its CPU thread to artificially lock itself to 30 fps, or by increasing its swap interval to 2. Alternatively, the driver could do the same thing by inserting extra sleeps in / (the NVIDIA drivers do have this as a control panel option; I don't know about others). But in any case, someone has to detect the problem and do something about it; the game won't naturally settle on a vsync-friendly rate the way it does when it's GPU-limited. This was all a simplified case in which both CPU and GPU workloads are perfectly steady. In real life, of course, there's variability in the workloads and frame times, which can cause even more interesting things to happen. The CPU render-ahead queue and triple buffering can come into play then. 

If you want to be extra fancy, then instead of picking independent random angles for every pixel, you might try stratified sampling or a low-discrepancy sequence. That may give better final results for whatever effect you're using the jitter texture for, due to better local sample distribution. 

The vertex shader only runs per vertex, not for every point in the square. So the four vertices are mapped to (±2, ±2), and then the GPU draws a polygon between those vertices, which does cross the viewport. The points interior to the polygon do not go through the transform, so the resulting point set is not $(-\infty,-2] \cup [2,\infty)$ after all. For this reason, one can't generally do non-linear geometry transformations in the vertex shader alone—except for perspective projection, which has special hardware support. (You run into this problem as well if you try to do any non-linear camera projection, such as a fisheye projection.) There are two ways to solve it: 

Sample locations with a uniform pattern will create aliasing in the output, whenever there are geometric features of size comparable to or smaller than the sampling grid. That's the reason why "jaggies" exist: because images are made of a uniform square pixel grid, and when you render (for example) an angled line without antialiasing, it crosses rows/columns of pixels at regular intervals, creating a regular pattern of stairstep artifacts in the resulting image. Supersampling at a finer uniform grid will improve things, but the image will still have similar artifacts—just less badly. You can see this with MSAA, as in this comparison image from an NVIDIA presentation on temporal antialiasing: 

The answer in the screenshot is wrong on two counts. First, you're correct that in the middle matrix the −1 should be on the third row, not the second. The other error is that the $u, v$ basis vectors are normalized, but not orthogonalized, so $M$ is not in general a rotation matrix; therefore its inverse is not simply its transpose. (The reflection doesn't actually need an orthonormal basis, so you could fix the answer either way: by using the full inverse for $M$, or by fully orthonormalizing $u$ and $v$.) The rule of thumb for setting up a local coordinate system (I assume orthonormal) for a given object is to set some of the axes to be lined up with the object, then choose arbitrary perpendicular vectors for any remaining axes. 

I don't think there are any specific limits. Each GPU can probably drive three or four monitors, and with multiple GPUs in a system, I don't think there's any barrier to hooking up the maximum number of monitors to each GPU. Here's a video, for instance, detailing a 16-monitor machine with 4 GPUs, on Windows. Googling around, I did find some people stating that Windows has a 16-monitor or 64-monitor overall limit, but I couldn't verify this from any official documentation. I also saw some claims that Windows has a limit on the overall desktop size that the monitors are virtually placed within, but again could not verify that. In Linux any such limits (if they exist) would naturally be different, and presumably would depend on which window system you're using. The other potential limiting factor would be CPU performance and PCIe bus bandwidth for keeping all those GPUs fed with data. 

You can technically choose the distance between events however you want, as long as you correctly weight the path for the probability that a photon can make it between two adjacent events without interacting with the medium. In other words, each path segment within the medium contributes a weight factor of $e^{-\sigma x}$, where $x$ is the length of the segment. (This is assuming a homogeneous medium, but see section 4.2 in the Marschner notes linked above for what to do if it's inhomogeneous.) Given this, a usually good choice for the distance is to importance-sample it from the exponential distribution. In other words, you set $x = -(\ln \xi)/\sigma$ and then leave out the $e^{-\sigma x}$ factor from the path weight. Then, to account for absorption, you can use Russian roulette to kill off a fraction $\sigma_a/\sigma$ of the paths at each event. This is particularly necessary for very large or infinite media (think atmospheric scattering) where the path could bounce around for an arbitrarily long time if it's not killed. If you're only dealing with small and not-too-dense media, then it might be better to just factor in a weight of $1 - \sigma_a/\sigma$ per event, rather than using Russian roulette. No, if you follow the importance-sampling procedure just described, Beer-Lambert is already incorporated implicitly in the sampling, so you don't want to apply it to the path weights. The volumetric equivalent to a BSDF is the combination of the scattering and absorption coefficients $\sigma_s, \sigma_a$ and the phase function. By convention, the coefficients control the overall balance of transmission, scattering, and absorption, while the phase function is always normalized. You could do something like this for BSDFs too; you could factor out the overall albedo, and have the directional dependence always be normalized. It's mostly a matter of convention AFAICT. Try "participating media" (that is, a volumetric "medium"—plural "media"—which "participates" in light transport), and "volumetric path tracing".