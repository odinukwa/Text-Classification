I've done a lot of experimenting and here are my findings. GIN and sorting GIN index currently (as of version 9.4) can not assist ordering. 

But Postgres does not want to use that for sorting. Even when I remove specifier in the query. Any alternative approaches to optimize the task are very welcome. 

The following query plan was obtained from smaller test system (4680 products in selected category, 200k products total in the table): 

This brings us back to and ~80 ms for 4680 products. Still can be slow when number of products grows to 100k. 

Playing with various offsets and product counts I could not make PostgreSQL use additional B-tree index. So I went classical way and created junction table: 

Still not using B-tree index, resultset did not fit , hence poor results. But under some circumstances, having large number of products and small offset PostgreSQL now decides to use B-tree index: 

I have a messages table in a database, which include a sender id and a message type (and of course many more columns not relevant for this question). I try to create a query which counts how many messages of each type a user have send. e.g. if I have the following table: 

So basically I'll have to do a migrate a database live over the course of a few days, while simultaneously updating the application live such that it can handle the new application, without being able to search for the usage of each table. With all these handicaps taken into consideration I came up with the following plan: 

So in fact I want to group by message_type and user_id, but instead of generating multiple rows per user, I want to create multiple columns, one for each message_type Can I achieve this without hardcoding the message types in my query? 

I have 3 servers, one of which has a linked server configured that points to the other - I'll call this Server A. Server A has over 100 user databases for various purposes. Server B is running SQL 2005 which we're trying to eliminate. Server C has copies of some of the databases from Server B, and we're migrating applications from Server B's database copies to Server C's. When I'm on Server B, I can see connections from Server A to a certain database but I don't know how to tell what procedure, task, or job from Server A is using that linked server connection to Server B. In order to retire the database on Server B, I need to re-point Server A's connections to a database on Server C; but in order to do that, I need to know what procedures, tasks, or jobs on Server A are using that connection so they can be updated. Is there a way to see the dependencies on a linked server without disabling the linked server to see what starts failing? 

I try to create a good database design to represent this data, however there are quite a few difficulties. A design I came up with is as followed: 

Using this strategy I will have 2 synced databases after step 3. Initially all queries will go to the old database, but while I'm updating the queries slowly the old database will be used less, and the new one more. Taking this "sub-optimal" situation in mind, is this a good strategy to fix some of the problems? What things should I take into consideration while doing this? Note that I fully understand that this is a very risky suicide mission. However, I'll have to do something in a short amount of time, otherwise the website becomes entirely unusable. 

However, I already defined using my keys that table1.a references to table2.b, so it seems to me that it shouldn't be to hard to make a DBMS system automatically use table1.a and table2.b as the join columns, such that one can simply use: 

As we can see was not enough so we had (the number here is approximate, it needs slightly more than 32MB for in-memory quicksort). Reduce memory footprint Don't select full records for sorting, use ids, apply sort, offset and limit, then load just 10 records we need: 

Note #1: 82 ms might not look that scary but that is because sort buffer fits into memory. Once I select all columns from products table ( and in real life there are about 60 columns) it goes to doubling execution time. And that is only for 4680 products. Action point #1 (comes from Note #1): In order to reduce memory footprint of sort operation and therefore speed it up a little it would be wise to fetch, sort and limit product ids first, then fetch full records: 

work_mem Thanks Chris for pointing out to this configuration parameter. It defaults to 4MB, and in case your recordset is larger, increasing to proper value (can be found from ) can significantly speed up sort operations. 

The easy solutions fall into "push" or "pull" strategies, which each have their benefits and drawbacks. 1) Push data from the Oracle database into SQL server, via a scheduled job or table triggers. Doing this, you'll have to remember to update the links to SQL if that database ever moves. With a table trigger, you'll also have some performance overhead on writes, updates or deletes to the table. 2) Pull data from the Oracle database using SQL server, via scheduled Merge job or SSIS ETL package. Using either kind of scheduled job you'll be slightly behind the live table updates, which is usually not a problem. The job can run as often as necessary and possible to keep the gap small. A DBA shouldn't have a problem setting this up with you, but if you're unfamiliar with database internals it can be difficult to choose and implement an option. Third party tools might be able to help if a DBA is not available. You should also be able to find guides on each of these options if you're going to implement one yourself. 

However, there is a problem with this design: a row should only be allowed to reference a connection when the education type matches. E.g. a row can only reference a row in the table that references a row in the table with type == master. Would it be possible to add a constraint which can check exactly that? If not, what other options are available? 

I "inherited" a web application which is designed and implemented horribly (both the application and the database). For example, the main data is stored using a sort of emulated key-value storage in a Postgres 8.2 database, making it virtually impossible to extract useful data from it in a reasonable amount of time. Currently I'm working hard on replacing the entire application + database, however it will take a few months before the new application is finished. This is a problem since the website is really slow due to the extremely bad database design and even worse queries. Therefore I'm planning to fix the database design and the queries on the live site until the website has an acceptable load time as a temporary solution. I do however have a few limitations to work around, the most problematic ones are: 

Original query I've populated my database with 650k products with some categories holding up to 40k products. I've simplified query a bit by removing clause: 

And this is the worst scenario with large number of products in chosen category and large offset. When offset=300 execution time is just 0.5 ms. Unfortunately maintaining such a junction table requires extra effort. It could be accomplished via indexed materialized views, but that is only useful when your data updates rarely, cause refreshing such materialized view is quite a heavy operation. So I am staying with GIN index so far, with increased and reduced memory footprint query. 

This one helps a lot unless there are too many products in one category. It quickly filters out products that belong to that category but then there is a sort operation that has to be done the hard way (without index). A have installed extension allowing me to build multi-column GIN index like this: 

Short answer: test your backups, and validate the backup frequency. It's difficult to write a long enough answer to properly address this topic; I'll explain a little on my short answer and give you some links to more information. Testing backups is extremely important; merely taking backups will not ensure that they're useful. Most DBAs will periodically restore the backups to another database or another server - for practice and to test the validity of the backup. The frequency of backups is important to address "minimal loss" - it depends on how frequently the database is used, and more importantly, how much data loss is acceptable. If the users/analysts can lose an entire day of data and be okay with re-entering it, you only need daily backups. If they can only lose 5 minutes, you need to be taking log backups every 5 minutes to satisfy that requirement. Communication with the database stakeholders is key to identifying a proper backup strategy. The other thing they can tell you is how long ago they will need to restore. If there are occasionally mistakes they need "rolled back" up to 3 months ago, your backup plan needs to keep backups around for 90 days so you're able to pull from those. Check out Brent Ozar's site for a more complete discussion of backup practices and why and when to employ them.