Your problem is not related to deferred shading whatsoever, you need to implement the basic core elements of a renderer before you try to speed up some specific part. When you have finished with what concept3d has explained, if you actually find that you need to optimize the deferred shader itself (as opposed to the whole rasterization pass) you can implement Tile-Based Deferred Shading. If you are not limited by the number of dynamic lights you should consider why you are using deferred shading at all, but if you are then you will want to try the optimization that made Battlefield 3 possible. (They hint to it in slide 10 of their public PDF: $URL$ 

EDIT: Looking over your code again I see another issue: glBufferData needs to know the total size in bytes, not the size per vertex, since it needs to copy all the data when you call it. Should look something like this: 

The easy way: Use glReadPixels on the framebuffer every time you are about to swap, then feed these frames into libavcodec / libavformat (ffmpeg). 

No: Then you want time scaling. This is usually boiling down the sound to a procedural / time domain representation, it is an active area of research and doesn't sound perfect. The common techniques are: 

EDIT: Thanks for the comment, I did misspell Pannini. And to make this edit worthwhile here are a few more: 

As long as color space is handled properly in each engine and you do not use any of the engines lighting effects, they will be the same. Dynamic effects such as SSAO will cause there to be a noticeable difference if you use the engines to their fullest. 

You're using VBONormal to supply data to all 3 gl*Pointer calls, you need to call the APIs in the correct order. You are giving a pointer into CPU memory, but when a VBO is enabled, the pointer needs to be relative to the VBO. What concept3d said about your indices is correct also, I'll leave it to you to fix that (but it should work the way you have it, just very slowly) 

Given a set of points on a line, plugging the coordinates into the line equation (y=m*x+b) yields a system of equations which can be solved to find the slope-intercept form of the given line. Starting with the player's position we assert that: 

Here is one way to do it, the idea behind this method is rotating the whole image to make the line straight while working on it. I am only using half of the matrix because the line is flattened horizontally and so the Y coordinate doesn't matter to us then. 0. Convert your startPoint and endPoint into vectors: 

And now by substituting the time of the peak into the amplitude function, we can express the magnitude of the peak after a target number of equilibrium crossings: 

Once you have that working you can take it a step further by introducing asynchronous DMA mappings using a functions like glMapBuffer. In this case, you will still pass an "event" to your render thread to do the OpenGL call, but instead of waiting for the upload, you will receive a pointer where OpenGL wants the data, and it doesn't care what thread puts it there. It might go like this: You get your network event, send a "buffer request event" to your main render thread. Still working from the network thread, continue loading / preparing your data and then when it's ready for upload, wait for the pointer back from the OpenGL thread, copy the data into it and then send ... another ... event back to your renderer to tell it "The data is there, do and set up other GL stuff now" 

Pack the textures together into one big texture, use this for the entire VBO. In your vertices, scale and offset the texture coordinates to "select" which texture to render with, (you'll have to keep track of what texture landed where in the big texture when you make your texture atlas) 

"Clearing the scene" will not produce this motion blur, you want to draw your frames into a RenderTexture, then draw this texture onto the main screen with your Alpha Blending enabled. Never clear the screen except on window resize. 

The "Existing Speed" is also forced non-negative here; When the player is falling, a negative existing jump speed will compensate for their falling, allowing them to bounce on thin air if they trigger the jump while falling. Now that we know how much to reduce the delta velocity precisely, we can compute the effective "Jump Vector" by scaling the Jump Normal to the target delta velocity. 

Fitting is the tool for the job. You can research least squares fitting for a more in-depth explanation. is only a simplified example. 

Which is just a damped harmonic oscillator, and since we already know that only the under-damped case need analysis, we can obtain a nice solution: 

Well if you absolutely need NASA space mission perfection, the only way to really be sure it rendered correctly is to do it yourself in software and compare it to the OpenGL output... That aside, if you can rely on an estimate of the number of pixels then you could investigate using Occlusion Queries to make a guess as to whether or not it rendered properly. 

While I personally love bunny hopping... As a starting point we should know the intended "Jump Speed" as a delta velocity. This figure represents the velocity increase (in the line with the "Jump Normal") during the instant of jumping once. Any velocity the player already has in line with the Jump Normal can be seen as a pre-existing "Jump Energy". This leads to a straightforward solution: The instantaneous delta velocity can be limited such that it never results in the player being accelerated beyond the target velocity. In order to measure your pre-existing Jump Speed, we can take the dot product of your normalized your Jump Vector, and your player's velocity: 

Which I have then plugged into the most basic Euler integrator, with a step size of 1/10, and it produced this graph: 

Another possible method, maybe applicable to old or very low end hardware could be copying the depth buffer, which you can find an example of using PBOs (Pixel Buffer Objects) here. 

You can build a skeleton system that can technically make all moves (even if all it can do is lose stupidly), and use an artificial neural network (like FANN...) as the decision making backend. Letting the bot in its weakest form battle against a clone of itself, you can trivially synthesize a training dataset of unlimited size by logging each game and computing statistics relevant to each move. It can be somewhat artistic choosing the right size network but with a good dataset, it should be easy to tell when you've got it right. Be careful not to overshoot the size, or else it will tend to "memorize" the dataset instead of finding the appropriate algorithm. I hope AI by neural networks is not too cliche ;) 

I'm using Hooke's Law here as the definition of a spring. () Given the derivatives of position and velocity, are velocity and force respectively, we can construct a differential equation for the stretching of the spring. 

You can generate each the normal map, pixel by pixel, by computing the respective the X and Y gradients from the height map using a Sobel operator. 

No, Pincushion distortion is not actually the inverse of Barrel distortion. (Proof below) This paper seems to be shooting for exactly what you want: $URL$ (Formulas inside) 

The GL_REPEAT mode has textures repeat when you go past (0,0) to (1,1) range The GL_CLAMP_TO_EDGE mode has textures stop at the last pixel when you fall off the edge. The GL_CLAMP and GL_CLAMP_TO_BORDER are depreciated because all texture borders must be 0 pixels, so the modes don't make sense anymore. (somewhere around GL3 I believe) There are more modes so make sure to read the docs. (Examples: GL_MIRRORED_REPEAT, GL_MIRROR_CLAMP_TO_EDGE) 

Use the "less than or equal to" depth compare function instead of a hacky bias, and render transparent polygons last using painters algorithm (as it looks like you are already doing). 

Yes: Then you want time stretching. This is simply interpolating / combining samples to generate the same overall waveform in a different number of samples. The obvious nearest neighbor approach will cause severe aliasing but technically works. To make it sound better one can use a higher order function like sinc interpolation. 

You can use Nem's tool called Crafty to export to .OBJ, then just throw it into your pipeline like any other model. 

If you would like a video larger than the screen, there is the hard way: Use an FBO to render your frames into a texture, then retrieve your data through glGetTexImage and feed it to your video encoder. (If performance is absolutely critical for some reason, instead of glGetTexImage, you would ask the graphics driver to do async DMA uploads using a PBO, and use multiple buffers to allow one image to copy & encode while the next renders) 

They are the same concept. EDIT: Displacement can be composed of heights in more than one dimension! Don't start any semantic wars over this, but realistically, height mapping usually refers to large scale / tessellation based techniques, where displacement mapping usually refers to small scale / raytracing techniques. 

represent the relationship of t (time), h (height), impulse, and gravity constrain the height to its global maxima 

Your screen capture appears to contain high frequency aliasing which supports this theory. Here I have zoomed in on the characteristic artifacts which often arise from severe undersampling: 

I have not analysed the convergence properties of this formula with respect to the initial guess. With that said, seeding with appears to place the attractor into the correct basin. As a quick test, I've plugged in these values: 

After testing a few ideas, I'm focusing on this answer because it is the cleanest code. The algorithm is very similar to your approximation, except I've modified it to iterate to a given accuracy: 

Warning: Not a unity user, so I wouldn't know if unity offers these techniques. The concepts are simple and applicable regardless. (And this just got bumped) 

A simple answer could be a simplified axis sweep broadphase. This involves sorting your list of "enemyships" (the larger list that changes less) by some coordinate, and then taking an early out when you know it's impossible to collide. The cost of the sort should be almost free since it only has to do work when new enemy ships are added. It might look like this (UNTESTED): 

Or you can use the CPU if you aren't satisfied with how mipmaps look, but going further than that in image quality would be a much longer answer, and probably more suited to another stack network site as it's more of general programming algorithm question. With that said, SDL_rotozoom can give you the basic idea of how it can work at the lowest level. 

In that use case you should be (and are, I guess) using additive blending, not alpha transparency. This mode performs "source = source + destination" 

That idea is prone to failure as you have found out, luckily there is a closed form solution available, I will do in-depth on that here. 

Your code is only sampling the corner texels, so is only valid for a 2x2 filter. Sample all the texels within your filter shape to achieve correct results. 

EDIT: Your question is WAY out of scope for SDL. If you just want a library (instead of understanding what you are doing) then try SoundTouch or Rubber Band. 

Here's the algorithm I see happening in your question: (I use "inside" meaning specifically not ON the cut line but only inside it) 

You must use a valid buffer bind target, in this case GL_ARRAY_BUFFER. As it stands now, you're using your model coordinates as the texture coordinates also, because they are still the active array buffer when you call glTexCoordPointer. 

Make your player taller. Currently you are not tall enough to give the illusion of size you want. The reason you can get so close is obvious if you imagine real life, because even while standing against a wall, your relative height is what determines your perception of size, not the distance. 

Setting this equal to our "Stopped" threshold, we can then solve for the desired Friction coefficient: 

invert the object's transform apply that transformation to your world space rotation the result is your object space rotation 

Proof as promised: By contradiction for simplicity. Extracting the relevant fact about Barrel (and/or Pincushion) distortion: (2): We construct an equation we can solve for our correction constant "j" in terms of the distortion constant "k": (3): (4): Which we can then re-arrange to isolate "j": (5): Which depends on the independent variable, and thus is not a constant. Therefore a Pincushion or Barrel distortion does not compensate its complement distortion. 

Load your big texture and enable/create mipmaps. Create your real texture at the perfect size. Use an FBO to draw the big texture as a quad into the small texture. Delete the big texture from memory. 

You embed the scripting engine inside the game engine. I've never used this but you probably want boost::python 

Use an accumulation buffer, this way if you wait too long you don't waste the left-over. (But get your timing logic straight first) 

The singular value decomposition produces the Eigenvalues with largest magnitude, and their corresponding Eigenvectors. If one were to decompose every Eigenvector of a matrix, they have performed a Principle Component Analysis. The principal components represent the 'n'-dimensional axes on which the most variance is encoded. Therefore they can be used as lossy compression; they optimize which normal vector in the 'n'-space to direct a scalar value, such that the line defined by this normal best represents the information in the original matrix. 

One possible way to decouple this example would be to use a stack of event handlers. Your program can iterate the stack in order, trying each listener until one handles the event: 

Resample the image as soon as you know the target size. You can use the GPU to resample with hardware acceleration: 

First-part Answer: id Tech 1 (aka DOOM engine) Second-part Answer: A simple 2D game is a fun way to learn. Go ahead :) 

Quick Answer: Depends heavily on your usage pattern. You are, in fact, describing an entity in your question. Tiles are generally following a geometrically regular tessellation (Quads, Hexagons, Pentagons+Hexagons, Octagons+Quads, etc), and thus can gain the benefits of deterministic procedural layout. If you have true tiles creating the majority of the world, definitely take advantage of the spatial structure and create a specialized system apart from the entities. On the other hand if your "tiles" are irregularly positioned or shaped, then it doesn't make sense to call them tiles at all. In either situation, you probably should not differentiate between static and dynamic entities at all without a good reason! Your systems should decide how to organize and process the workload, not your data. Example: Even if an entity is not "static" you might gain a benefit from letting it be considered static while it's inactive. (This time limit can be estimated from how long it takes to update your acceleration structure). To reiterate, data represents what to do, the system knows how to do it. PS: It's only bad "practice" if your skills don't improve by it.