I'm officially stumped now. I also checked Microsoft Connect and did not see any thing for this issue. SP1 for SQL Server 2008 R2 has not been applied but I don't see anything in that documentation that would have any effect on this issue. 

Depending on the number of databases you can use the above function against a list of each database name populated in a variable and output it all at the same time, if dealing with one server: 

This will return a number between 1 and 7, with 1 = Sunday and 7 = Saturday. So from that task you would have two precedence constraints going to each path of your maintenance plan. If the value is 1 (one) then it goes to the rebuild index task, if it is anything else it goes to the reorganize task. Although if I was going into this much detail I would stop using the index task with maintenance plans. I would suggest going to using T-SQL code that actually helps you maintenance the index fragmentation much more efficiently. I will say though I don't know if SQL Server has improved in 2012 with the index task. The most well known solution for index fragmentation management can be found here, Ola Hallengren. 

SSMS will not prompt for a login if you are already connected to an instance (e.g. Object Explorer), this is by design. If you want to change connections simply right-click in the query window and go to change connection. Your request to include the username and password in the file somewhere is bad security. Especially if this particular account has elevated privileges to the environment. If you are looking to automate the process, allow minimal involvement, allow anyone to execute the queries, and run them against any number of servers then I would suggest building the queries in PowerShell. A simple function that accepts say a server name and the script file can be written around using or .NET code if you are so inclined. Or write a function for each query and then just have them pass in the server name. 

Your problems will be based on how you wrote your process. So just a few things I can think of right off to consider. Although any answer is going to an opinion of the writer. 

Then your CSV file should contain the column names in the first row. The other option would be to do this in your PowerShell command but it is much cleaner to do this in your SQL query. 

Since you are using SQL Server 2008 R2 you can simply create your SQL Agent Job with the step configured as a "PowerShell" type, instead of trying to use the CmdExec. You only need to include the "meat" of your script. Since you are using the PowerShell type of a SQL Agent step it has already called the powershell.exe and imported the SQLPS module for you. So an example: 

It is much easier to modify maintenance plans directly than try to add additional flags within the SQL Agent job step. As well the flag you are referring to is regarding the space within the database itself, not just the physical log file. It is intended to shrink or release the unused space to the specified percentage. Not something you should be doing if you don't have to. Log truncation occurs upon completion of a full or log backup, or whenever SQL Server issues a checkpoint. You can read more on that process specific to SQL Server 2000 here. If you are trying to keep the log file under size due to disk space issues you need to increase the frequency of log backups that are occurring. If that is not something that can be done you either need to look at adding additional drive space, or find out specifically what is causing the log file to grow out of control. Shrinking the log file does not cause any breaks in the backup chain. It is however not something that is advised to automate. If the log file is growing out of control so much that it requires you to constantly shrink it down then all you are doing is covering up the bigger problem. You need to determine what is causing the log growth and take corrective action from there. 

You can review this MSSQLTip to get more information on what and mean in regards to SQL Server Clusters: SQL Server Clustering Active vs Passive In your situation, and based on your comments, the only thing you need to do is configure the dependencies for your resources. So if I have a cluster resource for the the SQL Server Cluster Name and it fails over, then all of the dependent resources will follow suite. I have not messed with or ever configured a SQL Server 2000 cluster so I do not know what how this was done during installation. I am aware that starting with SQL Server 2005 the installation did more of this for you: Failover cluster resource dependencies in SQL Server 

Natively to SQL Server, no. You would have to restore the backup in order to access it. There are some third party tools that can do this, Idera has one I have heard is pretty good but does cost money. 

I wrote this function sometime back for an SSIS package to provide to a client as an example, but it should work in your case of just passing in the query to execute in order to get an Excel file. It works the same as if you went into Excel to the Data ribbon and selected "From text" to get external data: 

Testing It To test it through SQL Server you can just try to backup a database that does not exist like this: 

I believe your issue is going to be the provider. Since PowerShell steps in SQL Server Agent automatically put you into the context of that provider some commands that work in your normal console will not function the same way. A write up was done here with . You basically have to tell the provider you want to use. Your code would look something like below: 

Second Step: Whichever backup plan you choose ensure it meets the recovery requirements for the application AND/OR the business. So while either schedule will work the will likely lessen the amount of IO activity that is caused when the backups overlap. It would based on the size of the database and how much activity SQL Server has to backup during the differential and log backups. Depending on how you setup you could end up with all three backups trying to occur at the same time frame. 

The PSChildName and other references to the replica names show that the backslash is being presented as the ASCII hex equivalent value of "5C". So to work with named instances you have to use this path format: 

The DBCC commands are only going to clear the memory buffers they are not going to release the memory back to the OS. Do you know that SQL Server is actually consuming the memory? I would suggest looking at setting up Perfmon session or start collecting DMV information after a restart to find out what SQL Server is doing and working on. Also take note if users are doing more work than normal during your collection time (such as End-of-Month processing, etc). Are you running SSRS, SSIS, or SSAS on the same server? You have 1200 databases on the system, what is the largest size DB you have? 

Audit events are actually not available through Extended Events. You would need to use SQL Server Audit, which more or less works on top of the Extended Events engine (at least from what I understand). Steps to go with: 

If you really need it you can utilize and PowerShell, however you will need to be connected to a SQL Server instance with PowerShell installed. As well you would need to enable , which the procedures I use to collect this enables it and then disables it once it is finished. Just my preference. I built the below based on this article by Laerte Junior here. 

To try and resolve the issue various paths to take are offered online and in books. I generally start by looking at the execution plan of the query/procedure and focus on the areas that are showing parallel execution. Then from there go through trying to tune the query first and then as last resort may start using query hints. The most common query hint you will see mentioned to resolve these deadlocks is implementing . However, before doing that you might check to see what the server level MAXDOP and Cost Threshold are set to. Cost Threshold is generally set to 5 by default and I like to raise that to 35 or 40 to start out with, if the query in question has a low cost for that section of code it may not need to run in parallel at all. I'm not all that fond of using MAXDOP query hints but that does not mean they do not have their place and purpose. just my opinion. 

In regard to the 3 servers and the same cluster, it will depend on what type of HA is required. In most cases you could have a 3 replica AG where SSRS is reporting on the secondary replica (read-only routing) instead of the primary replica. This is common with OLTP ssytems that have reporting requirements. For that specific requirement will be purely based on the vendor and your business owners requirements for HA. There is documentation on this type of architecture but not in a single document. Either way, if you are not even remotely familiar with these technologies that documentation is not going to help understanding the concepts. Which you need to understand in order to support it and ensure you are deploying each component properly. You need to tell your management they need to bring in another resource (hire DBA or reach out to 3rd party service that offers DBA support). 

100% agreement with @Aaron on this. Athough your comment strikes a chord with most folks I am sure... You should not be planning to stay on SQL Server 2005, that follows the normal lifecycle support Microsoft has for everyone of their products. This goes along with what Aaron spoke of regarding bugs that are marked fixed in the next release, be it SQL 2008, 2008 R2 or even 2012. The same fate is meet with the operating system as well. If you move your SQL 2005 instance to each new release of the OS you are going to hit a release that will not support SQL 2005 being run on it. That is just the normal business planning that has to be considered when working in a Windows environment, and if you work in other environments as well I am sure. 

I supported PCI and HIPPA compliant data as a DBA contractor for the Air Force and as a consultant for hospitals. It is part of a DBA's job to have access to data, whether it is encrypted or not. The PCI compliance data was encrypted by the application, but I could still see it decrypted because I had access to the DLL that handled it...quick PowerShell script let me see all of it. Which I ended up using that PowerShell script to switch the data over to let SQL Server maintain the encryption. If you do not trust your DBAs to perform their job and hold up the integrity of the system, and security of that system, then you need to bring that up with management. Your security officer (if you are at a bank you do have one of these) should be made aware of this situation. DBAs are the gate keepers of all data for an organization, it is part of the job. Plain and simply, if you do not want them to have access remove them from the server and database instance. By remove I mean delete their account from it all. 

Create an Event Handler for for the package level (Excutable is set to the package name). Configured the SMTP connection and added in an Send Mail Task Within that task I configured an expression for the and . Those two expressions are noted below.