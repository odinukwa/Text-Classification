This will only cycle through clusterList one time and will instantly merge and destroy the clusters that are smaller than T. Assuming you have experience with object oriented programming, this would be pretty straightforward to implement. Even if you do this within an array, you can just clusters after you merge them with other clusters. Hope this helps! 

Notice how the is very high and all of the other metrics are very low. Now look at the class balance of your problem: $$TP+FN=Actual Positive=31,245$$ $$TN+FP=Actual Negative=508$$ So your data is heavily skewed toward positives. To have gotten a model that is producing this poorly, I think you may have provided the model with the as the cross validation metric. The is a very bad cross validation metric in this case since it will result in poor and poor . is also not a good metric as your model could classify everything as positive and get an accuracy of: $$AC=\frac{31,245}{31,763}=.984$$ For cases like this where the classes are grossly weighted toward one value, I suggest using the as your cross validation metric. The is the of and and hence balances these two factors nicely. Wikipedia actually has a very nice explanation of classification metrics here and this paper is top notch if you even need to understand multi-class metrics and confusion matrices. Hope this helps! 

I suggest using supervised learning and employing a linear model: linear regression. This is a perfectly linear system (y=x+1), so linear regression will work just fine i.e. perfectly. Further, you have an infinite amount of data you can use to train the system, so it should be easy to train ;-) I jest... I think that two data points will be sufficient, again, since it is perfectly linear! Pedagogically, the triviality of this extremely simple linear system gets a little more interesting when you try analogue based methods like support vector machine (SVM) - which should also be able to provide a perfect result, decision trees or random forests, and even naive Bayes regressors. Though its useful to start learning with very simple systems, I suggest quickly moving on to something more complex, i.e. don't get too stuck in your own head for the trivial linear model. Don't forget that the power of data science and machine learning lies in its statistical nature, so try to find a test case that includes some statistical variation in the input. Hope this helps! 

No, you are misinterpreting his comments. If you have data that has some outliers in it then the outliers will extend beyond 3 standard deviations. Then if you standardize the data some will extend beyond the [-3,3] region. He is simply saying that you need to remove your outliers so the outliers don't reap havoc on your stochastic gradient descent algorithm. He is NOT saying that you need to use some weird scaling algorithm. You should standardize your data by subtracting the mean and dividing by the standard deviation, and then remove any points that extend beyond [-3,3], which are the outliers. In stochastic gradient descent, the presence of outliers could increase the instability of the minimization and make it thrash around excessively, so its best to remove them. If the sparseness of the data prevents removal then... Do you need to use stochastic gradient descent, or can you just use gradient descent? Gradient descent (GD) might help to alleviate some of the problems relating to convergence. Finally, if GD is having trouble converging, you could always do an direct solve (e.g. direct matrix inversion) rather than an iterative solve. Hope this helps! 

I think it could be quite pedagogically useful to also solve your original problem using this algorithm and then also construct a solution using or and you will gain a deep understanding of the juxtaposition of those three algorithms. Hope this helps! 

This all comes with the understanding that in time-series analysis the cross-correlation is a measure of similarity of two series as a function of the lag of one relative to the other. 

This depends a lot on you, your ability to apply your knowledge of statistics, computer science and mathematics to abstract problems and also your problem solving intuition and prowess. A great many jobs in the US require Masters or Ph.D.s, but there are some that do not. But add to this, your lack of a degree in a truly quantitative discipline and it might make things tough. Though you have cited some background in statistics, being a data science requires that you have the mathematical maturity to digest research papers and text books to apply abstract concepts to the real world. This typically requires the experience that is common to a mathematics BS or a Masters or Ph.D. in a slightly less rigorous field like engineering. I would certainly consider taking 2 years of calculus and enough linear algebra to get past matrix manipulation and really understand Fourier analysis, orthogonal vector spaces and Hilbert space. I would advise you to seriously consider a Masters or Ph.D. program. If you don't want to do this, then try to take full advantage of your CSE minor to be a really strong programmer. You should be able to find a job in a smaller startup which will allow you to prove yourself over time. Be prepared that a lot of first time jobs may be Data Engineer positions, which require a lot of ETL and data munging. Good luck in whatever you decide to pursue! 

The way that you have phrased this question makes it tough for people to answer without first offering you some background on linear regression (LR). Its great that you are interested in learning some ML and LR is a great place to start. Linear regression is really just finding a line (or plain or hyperplain) that maps a relationship between two or more variables or features. The important requirement is that the target variable be a continuous numerical value. So its less about finding a problem for which linear regression "works" and more about finding some data that interests you and playing with it using linear regression. I suggest you download some open data in the realms that you have mentioned. There is plenty of open data in every topic that you mentioned that contain continuous numerical values that you can predict using other features of the data. 

If an exhaustive nonlinear scan is too expensive and a linear scan doesn't yield the best results then I suggest you try a stochastic nonlinear search i.e. a random search for hyperparameter optimization. Scikit learn has a user friendly description in its user guide. Here is a paper on random search for hyperparameter optimization. 

, which is just the dot product normalized by the length of the two vectors being dotted because it does not take magnitude into account e.g. $(1,1)\cdot(1,1)=1=(1,1)\cdot(2,2)$ Perhaps you could therefor use the sorensen dice coefficient or possibly 1-soresen: 

First, the big picture answer is that many online clustering algorithm are surprisingly stable when they have been well trained with a large cohort of initial data. However, its still a problem if you want to really nail down the cluster identities of points while allowing the algorithm to react to new data. The trickiness of you point is briefly addressed in Introduction to Machine Learning By Ethem Alpaydin. On page 319 he derives the online k-means algorithm through the application of stochastic gradient descent, but mentions that the arises when choosing a value for the learning rate. A small learning rate results in stability, but the system looses adaptability where as a larger learning rate gains adaptability, but looses cluster stability. I believe the best path forward is to choose an implementation of online clustering which allows you to control the stochastic gradient descent algorithm and then choose the learning rate so that you maximize stability and adaptability as best as you can using a sound cross-validation procedure. Another method that I've seen employed is some sort of forgetting algorithm e.g. forgetting older points as the data stream matures. This allows for a fairly stable system on fast time scales and allows for evolution on slower time scales. was created to try to solve the . You might find this article interesting. I'm not well-versed enough in R to suggest an algorithm, but I suggest you look for a algorithm that allows you to control the learning rate in its stochastic gradient descent algorithm. I hope this helps! 

They are not specifically referring to a plot based approach. They are referring to a class of methods that must be employed when the data is not normal enough or not well-powered enough to use regular statistics. Parametric and nonparametric are two broad classifications of statistical procedures with loose definitions separating them: 

You are 95% there, you just have one hangup... The vectorization that you are doing is alternatively known as binarization or one-hot encoding. The only thing you need to do now is break apart all of those vectors and think of them as individual features. So instead of thinking of the question one vector as $(0,0,1,0)$ and the question two vector as $(0,1,0,0)$, you can now think of them as individual features. So this: 

This was the idea behind paranoid linux which began as fiction and became a real project that did not reach fruition: "Paranoid Linux is an operating system that assumes that its operator is under assault from the government (it was intended for use by Chinese and Syrian dissidents), and it does everything it can to keep your communications and documents a secret. It even throws up a bunch of "chaff" communications that are supposed to disguise the fact that you're doing anything covert. So while you're receiving a political message one character at a time, ParanoidLinux is pretending to surf the Web and fill in questionnaires and flirt in chat-rooms. Meanwhile, one in every five hundred characters you receive is your real message, a needle buried in a huge haystack. ~Cory Doctorow (Little Brother, 2008) When those words were written, ParanoidLinux was just a fiction. It is our goal to make this a reality. The project officially started on May 14th, and has been growing ever since. We welcome your ideas, contributions, designs, or code. You can find us on freenode's irc server in the #paranoidlinux channel. Hope to see you there!" I think the most modern equivalent is Pirate Linux along with the TOR Project. BTW, Cory Doctorow's book "Little Brother" is kind of a fun read if you are a geek. If you are reading this then you probably ARE a geek :-) The reality is that it is very difficult to hide one's true intent when high quality clustering, classification and anomaly/novelty detection is used. I work with guys who have done this type of behavior detection for some very high profile three letter agencies and it is very very easy to detect nefarious behavior if you have enough data. So much so that undercover good guys sometimes clearly show up in the data sets. 

The nonlinear basis function has higher variance. See how it is noisier than the linear kernel! The linear kernel has lower variance. See how it is less noisy! Now lets assess bias - Which kernel is more accurate? We can add the errors that you provided. The nonlinear kernel has a total error of ~550+325=~875. The linear kernel has an error of ~690+~50=~740. So the linear kernel seems to do better overall, but they are pretty close overall. This is were things get tricky! Putting it all together See how the linear kernel did a poor job on 1's and a really great job on 0's. This is pretty unbalanced. Where as the nonlinear kernel is more balanced. It kind of seems like the sweet spot might be to create a balanced model that doesn't have such high variance. How do we control for high variance? Bingo - regularization. We can add regularization to the nonlinear model and we will probably see much better results. This is the C parameter in scikit learn SVMs, which you will want to increase from the default. We could also play with the gamma parameter. Gamma controlls the width of the Gaussian. Maybe try increasing that one slightly to get less noisy results e.g. a larger no-man's land between classes. Hope this helps!