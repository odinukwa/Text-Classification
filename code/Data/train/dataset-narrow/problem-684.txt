This question is too broad, I'll try to give you some guidelines and tips. Your databases are quite small, MySQL can handle databases much larger than that. I don't see the need to switch to another database. You should also use a separate partition for the mysqldata directory so if the system partition fills up the database is still safe (and vice versa). Use tools such as Mysqltuner to check if your MySQL config is correct with respect to your hardware. You can also do periodic benchmarks with sysbench to ensure your server is working smoothly. By the way, I'd recommend that you switch to MySQL 5.6 which is the latest stable version. 

It is clear that the table is badly damaged. The application using the database still works (although with reduced performances), and so do statements. The mysqlcheck tool cannot be used to fix it, so the solution I'm aware of is to do a , drop the table, and do a during the next maintenance window. Are there drawbacks with this way of proceeding, and are there other options to fix the table? EDIT: I've restarted the MySQL server with increasing values of from 1 to 4, and the result is always the same: 

There's no need to have one table Users and another table Persons. I've never seen a social network that allows a single user to create more than one profile. True, there are people that keep multiple profiles, but these are done by opening multiple accounts (sometimes against the TOS). You could use this model: 

Personally I like to design databases that allow for flexibility, but the choice here it's up to you. Choice of the DB engine A MySQL database with InnoDB (which is the standard engine in MySQL) will work fine, so it's a good choice. 

The int values mapped to each field set the visibility of the field with respect to the different groups, e.g.: 

Your question isn't very detailed, anyway here's how you could implement what you want: The simplest way to model a privacy setting schema is probably to add a visibility field to each relevant property of the user profile: 

I've seen a sudden increase of binlog files, very large in number and size, in all three nodes of a Percona XtraDB cluster: 

On MySQL command line on the remote machine, do a , then copy&paste the relevant part of the output on MySQL command line on the local machine. 

It depends on what you want to log. One solution (taken from here) that does not use the general query log is the following: 

You can say that a database is normalized in 1NF (or 2NF, 3NF, etc.) You can also say that a database is in 1NF (or 2NF, 3NF, etc.). That's equivalent, even if I would prefer the first sentence. If you just say that a database is normalized, it means that it's in one of the Normal Forms. I would avoid saying that, because without specifying the NF you don't convey much information to the reader. Since normalization works through a series of stages, a database in 2NF is also in 1NF (plus no partial dependencies), a database in 3NF is also in 2NF (plus no transitive dependencies), and so on. The process of transforming the database structure in an higher form (e.g. from 1NF to 2NF) is called normalizing a database. The opposite process is called denormalizing a database. 

and the same for the email address. This schema only modelizes the contacts. You'll have to implement a different table to store the data for the users of your application and bind the contacts (the first table above) included in a user's addressbook to the correct user. 

A product may fail multiple times but each repair concerns only one product, so it's a one-to-many relationship. You can hence have simply one table (that stores a repair event) referring to . 

This solution uses the MySQL variable which stores SQL queries executed each time a user logs in. Now each time a user logs in, it will be logged into the table . Be sure to check the permissions of all of your users to ensure that they can insert a record into the table. Another solution, available only for MySQL Enterprise and that I include here for completeness' sake, is to install the MySQL Enterprise Audit plugin. 

However, the Time is always 0 and the process Ids change all the time. My understanding is that the query waits for a table lock but the lock is released after less than one second. Is this a normal/acceptable behaviour or could it be worth to do some fine tuning on the query cache, perhaps removing it completely? 

You should remember that a MySQL dump is basically a big SQL script textfile that creates the database(s) schema and inserts all data into it. So once mysqldump exits correctly, you can be pretty sure that everything went well (assuming of course that you're backing up the correct databases!) I'm not aware of any way a mysqldump file could end up corrupted (unless there were filesystem issues, in which case any file could be mangled). MySQL is (rightfully) pretty squeamish about it; if there were some corrupted tables in the database, mysqld would crash and would not allow mysqldump to terminate, in order to protect database integrity. 

How do you get the size (in bytes) of the result of a SQL query, in MySQL 5.6? will return the number of fetched rows, but not their size. My aim is to evaluate common queries to know a lower bound value for so that they can be served by the Query Cache. 

It appears that an user lanced a script that does quite a lot of SELECTs and UPDATEs over many connections. However, it surprises me how he managed to DoS the server. Is this a reasonable cause, or might be something else, perhaps some misconfiguration? 

I'm managing a e-commerce site which uses a popular online shopping cart software running on MySQL 5.6. Yesterday I noticed that reports that 990 of 1000 queries are waiting for a query cache lock: 

What is the best practice to run on a cluster? I've done some experiments in a test environment without users, and it appears that an on a node does not automatically propagate its effect to the other nodes. This is consistent with the fact that this command modifies the indexes and the table's storage space, not its contents or its definition. 

You need to know the size in bytes of each field according to the data type (MySQL reference here), then sum up these values together. 

It's your duty to take care that the same data is not entered again i.e. verify that the student exists and, if yes, edit his record instead of creating a new one. 

It appears that you want to consider a contact (identified by a phone number) as a single object, and share this object amongst users which happen to have the same person in their addressbook. This is a terrible idea as if one user modifies the contact in his addressbook, the changes propagate to other users' addressbooks. The proper way to do this is to consider every entry in a user's addressbook as an object: