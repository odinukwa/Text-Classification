In current version (Pg 9.0-9.3) there is no fast "fail-back" mechanism. You will have to follow official guidelines which state: 

In my opinion it is not related to or Windows issues. Per pg_basebackup docs, the main data directory will be placed in the target directory, but all other tablespaces will be placed in the same absolute path as they have on the server. 

The answer depends on what is really going on the wire (that is, in client session). First, what is a deadlock? It is a cycle in lock dependency graph. Whatever it means, to make a deadlock occur, you need to have some locks first. Locks normally do not live outside of transactions/queries. A connection itself cannot cause any deadlocks if it does not put locks on database objects. (Shared object access is main purpose of locks) If the connection opens a transaction, does some queries (effectively locking some database objects), and then still executes or sleeps in middle of the transaction (without commit/rollback), it COULD cause a deadlock - when sibling transactions access same resources. If it is just an idle connection - there's no risk of deadlock. 

Yes. If you cannot get rid of MandatoryData object, you should have one such object per book. Otherwise I would simplify the model and move any mandatory attributes to the Book object, which will reduce ne04j database size. Modeling relationships which are always 1:1 is a bit silly. 

No it's not. Unless your field set is very dynamic (no single authority, people can invent fields on the fly). 

If you really want two queries, you can use special FOUND variable to test if previous query gave any result: 

So it looks like pgbouncer cannot spawn enough parallel server sessions? I'm not sure how that can happen while it looks that no basic resources are saturated. (CPU, mem, # of connections) I also tried with 5000 connections (using 10 parallel pgbenches) - same effect. Only about 50 cl_active. When I run pgbench connecting directly to postgres (port 5432) - it spawns hundreds of sessions without problem. I'd be grateful for any hints. Can this be pgbouncer, postgres or kernel parameters issue? 

Synonyms: In this table we can attach a tag as a synonym to another tag (e.g photo can be a synonym for photography) 

We create 1 notification AND link the users to that notification in another table () We create a seperate table for message notifications. So that we can facilitate for multiple users per notif?! 

So we are basically reversing the ids around on the second row. But couldn't this be done in 1 row?! The person who built this is no longer here and I'm wondering why it could possibly be setup like this. Why not just store one row? Like explained here (this is how I thought it should be done as well): $URL$ 

Users of our app will be able to attach tags to there articles. If the user adds photopgraphy as the article title, our search will also show articles that have the photopgraphy synonyms attached as a tag. Note that we aren't using SQL for search, we're using elaticsearch. The sql will just be used for storing the data so it can later be imported into elasticsearch. 

Would doing this be a lot more efficient? I might be missing something about the current way we're doing it (is it good OR bad?) 

I came across the structure of some code in a project that I'm working on & I'm not sure why it's structured this way. Basically what we want todo is store who is friends with who in our networks table. 

So I can add many synonyms to one tag. Is this solution viable? Could it be improved somehow? I will be writing this schema in postgres. Appreciate any tips :) Use cases: 

At the moment, for each contact request, we would need to store rows, for every single message, we would need to UPDATE existing rows (each user has there own notif)...this is the main reason I want to find an optimal solution! Because when we add group chats, storing and updating 2..* rows will get ugly, and I can see data getting out of sync quickly. I have thought of 3 solutions: 

RULEs or triggers are a performance overhead, and can be avoided. Consider something along these lines: 

Notes on performance With small tables (less than 1000000 rows), any solution will work. is slightly faster: 

See docs. Use only session parameters, your application must know which statements will be silenced. Technically, you could make the setting permanent (in postgresql.conf) but it's not a good idea. In some cases this will be a footgun. 

Only thing I do not understand in your question is "database agnostic" - what does it mean, precisely? 

Decide your vertexes (nodes, objects) and edges (relationships). Convert relational data to cypher, declaring all items and all relationships explicit. 

(...and so on - every client fails) Comments and ideas What's interesting - even if the clients work in "new connection for each transaction" mode (add "-C" option to pgbench), same error occurs. Not always, and not for every client, but it does. I asked this question some time ago on mailing list: $URL$ - this SO post is just a copy. For more audience: Same thing applies to rotating a partition. So it might be interesting for all people who use partitioning. This is all on PostgreSQL 9.0, Linux. Solution (thanks to Chris Travers) 

To calculate TPS (transactions per second), run the query several times and calculate difference over time interval. There are ready made tools for that, one of them is $URL$ More info: $URL$ 

To test if the index can help at all, you could repeat the query with sequential scan preference set to zero: 

All operating systems and all applications use a concept called "caching". It means - when the data is first read from a slow memory device (like, a hard disk), it is saved in a fast memory device (like, RAM) for some time, to facilitate faster lookups. The same applies to RDBMS. First time the data blocks that build up your query results are read from disk, second time they are read from memory. Details can be explored using OS and database tools. If you specify what RDBMS and what OS you are on, we can help you get the details. For PostgreSQL it's about EXPLAIN command. 

My system receives data feeds. Each data feed will end up creating inserts and/or updates to most tables in the (relational) database. I need to capture the snapshot of what the entire database looked like after each data feed is received. Basically I need a way to version the database each time a data feed is run through the system. Note, by capturing a snapshot, I dont mean literally taking a snapshot of the database, but rather writing history records or some such mechanism so that I can query the database across "versions" to see what changed between versions (among other use cases) Do known data model designs exist that can capture a snapshot of a database version like this? 

Create a version field on the table you want to use optimistic locking for e.g. column name = "version" On selects, make sure to include the version column and make note of the version On a subsequent update to the record, the update statement should issue "where version = X" where X is the version we received in #2 and set the version field during that update statement to X + 1 Perform a on the record we are going to update so that we serialize who can make changes to the record we are trying to update. 

How does one correctly implement optimistic locking in MySQL? Our team has deduced that we must do #4 below or else there is a risk that another thread can update the same version of the record, but we'd like to validate that this is the best way to do it. 

To clarify, we are trying to prevent two threads which select the same record in the same time window where they grab the same version of the record from overwriting each others if they were to try and update the record at the same time. We believe that unless we do #4, there is a chance, that if both threads enter their respective transactions at the same time (but have not issued their updates yet), when they go to update, the second thread that will use the UPDATE ... where version = X will be operating on old data. Are we correct in thinking we must do this pessimistic locking when updating even though we are using version fields/optimistic locking? 

In any modern RDBMS (including Oracle, Microsoft SQL Server and PostgreSQL - I'm sure about these) this will have no effect on performance. As someone noted, this will impact only query planning phase. Hence difference will be visible only when you run thousands of iterations of a simplistic query which does not return any data, like this one: 

For better effect please post results of and . Also let us know the dataset size and resource configuration parameters (, ) as well as database host parameters (OS, memory, disks). 

Make sure not only partitions are indexed, but also the master table is indexed in same way and ANALYZEd. This could make the planner include index-based estimates on a single partition, but ignore them on master table level. If expression index or statistics for master table is missing, the planner is not able to infer join cardinality from this condition - even if it has perfect statistics for partitions. It's just a guess because you did not provide full schema. Please let me know if this helps. 

To redirect traffic from primary to standby you need some external tool which will do the failover procedure - using either dns-based or IP-based or other failover method. PostgreSQL itself does not know how to redirect traffic or do anything outside the database scope. Popular tools are pgpool (in layer 7) or Linux HA or corosync and friends (in lower layers). 

The boolean is not needed here. Existing row means true, missing row means false. Possibly, if this is a very large scale app, and if you can simplify the model, then you can save some resources: 

notification ID will be a monotonic bigint never going down and store only one the last ID read (which means all previous notifications for given user are also read) - this will be just one field in table.