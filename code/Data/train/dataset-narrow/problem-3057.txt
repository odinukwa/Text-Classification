Data science, being a new term, covers a broad spectrum of jobs. At one end you are expected to write production code. At the other end you do statistics in packaged software. They also call such people statisticians or analysts. So decide what you enjoy doing before you leap. If you just want to analyze data, you could definitely get by with R or python as long as you're mathematically proficient. I find that in these kind of jobs, your communication and social skills matter too, since you have to explain the data to executives and the like. 

I think we've covered this question before but I can't find my response... A lot of work has been done in this area. I don't know of any packages, but the basic idea is to discretize the search space. One way is to just to use the cellular grid, which makes sense if you have that information. Another is to first extract the "locations" from the location trace; i.e., places the users revisits and spends significant time in. Then you can apply well-known techniques such as HMMs to estimate the next discrete location from the recent past. Most of the papers I read on this subject are about a decade old, so they came out before the deep learning flurry. If they were revisiting the subject I'm sure I'd see papers using recurrent neural networks. Here are some papers for inspiration: 

Standard factorization machines have fields too. The "novelty" here seems to be the use of GBDT features and the application of the hashing tricks. Not to great effect, it seems: check out the minute range in performance on the last slide. 

It makes unit testing easier; invariant to the size of the sample. Reference: the github discussion that led to the change. 

Flat means parallel to the x axis; having a small slope. The smaller w is, the closer f(x) is to b; recall that $f(x) \equiv \left< w, x \right> + b$. One way to think about this is as a form of regularization; the flatter the function, the simpler or more parsimonious it is. This applies to both classification and regression. 

Welcome to DataScience.SE! I'd never heard of this problem so I looked it up. It is explained on the third slide of this presentation by Geoff Hinton: 

This problem is called real-time video stitching. It's pretty hard to do yourself if you're not familiar with computer vision, so here are some off-the-shelf tools. edit: If you are looking for a paper: Stitching videos streamed by mobile phones in real-time, or Real-time photogrammetric stitching of high resolution video on COTS hardware. 

I would recommend familiarizing yourself with AWS spot instances. It's the most practical solution I can think of for your problem, and it works your computer too. So, no you don't have to buy an Nvidia card, but as of today you will want to use one since almost all the solutions rely on them. 

The purpose of the encoder (green part) is to determine this document vector, so if you want to provide it yourself use the decoder and feed the embedding as the initial state. I suppose you could use your embedding as a prior for the encoder output. One alternative to your suggestion, which might work, is to use the embeddings as an MSE regularizer to the encoder output. 

It means that the next vector should be perpendicular to all the previous ones with respect to a matrix. It's like how the natural basis vectors are perpendicular to each other, with the added twist of a matrix: $\mathrm {x^T A y} = 0$ instead of $\mathrm{x^T y} = 0$ 

Your question is really about the method of Lagrange multipliers in constrained optimization, not logistic regression per se. The gist of it is that a constrained optimization problem can be recast as an unconstrained optimization problem by adding a term, called the regularizer, and vice versa. The sphere comes from recasting the unconstrained problem into a constrained one; recall that a constant $L_2$ norm defines a hypersphere. 

Note how the sample functions (and thus error bars) expand and contract as you depart and approach the measurements, as you would intuitively expect. 

You can make plenty of money as a software engineer. It depends on how good you are, and how scarce your skills are. If you transition to data science, you'll be starting from close to scratch, and will be paid accordingly. I don't know where you live, but you can currently make $150K in Silicon Valley as a data scientist or a software engineer. Search for salary surveys in your area. A certificate won't get you hired -- passing the interview will -- but it will help you get past HR. Maybe you should take the classes just to see if you enjoy it. 

I have no personal experience with it, but I came across Accord, which has limited support for neural networks. Honestly, .net does not have a machine learning community, so you might be better off using something else if you do not want to DIY. 

You almost solved the problem in the last paragraph. Expressed more formally, your cost function could be $$\frac{1}{N} \sum_{i,j} c_{i,j} y_{i,j} \log x_{i,j}$$ where $i$ runs over items/documents, and $j$ runs over classes, $x$ is your prediction, $y$ is the binary label (1 if item $i$ has class $j$), and $0 < c < 1$ is your confidence. This is a simple modification of the cross entropy. When the confidence $c$ is low, the value of the prediction matters less. 

Discrete Orthogonal Moments Based Framework for Assessing Blurriness of Camera Captured Document Images 

Let the credit card statement be the ground truth, and the receipts be the noisy inputs. For a given line item, find the receipt with the smallest distance. If the distance is small enough, declare a match. This is a threshold you will have to experimentally determine. You can let the distance be the sum of the distances for the amount and date. A heuristic for these individual distances is simply the edit distance. A more sophisticated approach would be to model the OCR error using labeled data to determine the most likely input; print something with known text in a similar typeface to the receipts so you can learn which characters are commonly confused with one another, and thereby estimate the most likely input sequence. Going another step further, you could jointly model the density between price, store, and item, so you can recognize for example that a vegetable bought at your local grocery store probably does not cost \$100 but \$1.00 If your OCR software gives you confidence estimates over the characters and possible guesses, you can use that too. 

You don't necessarily have to train any differently if you are happy with the result. If you aren't, undersample the majority class and/or adjust the class weights in your loss function. The thing you really have to be more careful about is the evaluation of the result because a naive metric like the accuracy would give you false hope since the imbalance allows you to achieve a high accuracy by simply always predicting the majority class or randomly predicting each class according to its probability of occurrence. It is this baseline against which you have to judge your success; like the coefficient of determination in regression, where the baseline is the sample mean. You could use a graphical approach (ROC or precision/recall curves), or report a size-weighted average of the class positive rate instead of the accuracy. Here are some other ideas: Comparison of Evaluation Metrics in Classification Applications with Imbalanced Datasets. 

They've created a graph from the news articles, topics, and named entities (locations, persons, companies, organizations). There are a lot of things going on here, but k-means is not one of them. If I had to do this I would use a named entity recognition (NER) and document embeddings (doc2vec, etc.). Once you have the embeddings, and the edges (thanks to NER), use a graph layout algorithm like force direction. If the graph is too dense, thin out the weaker edges. If all that sounds Chinese to you, start by reading about "named entity recognition", and "word embeddings". The idea is to attach a number (or rather, a vector) to everything from a word to a document. Twitter is a different beast. The textual content, such as it is, won't play nice with these embedding algorithms, but you have hashtags and strong social signals; mentions, retweets, and follows. This is pretty complicated, too, so I'll leave you with a relevant paper: Twitter-Network Topic Model: A Full Bayesian Treatment for Social Network and Text Modeling. 

You can. Nobody is going to demand a degree, but they will expect you to pass the interviews. I would recommend creating a blog and do some Kaggle competitions to show off your skills. I would also recommend taking more serious maths courses from the statistics department. 

This seems like a matter of opinion to me, but here are some applications of machine learning that you seen very often in Silicon Valley: 

There are online models and there are offline models. Online models make sense when you need to have your model constantly be updated. Otherwise you can save money by updating your model as frequently as necessary; nightly, weekly, etc. The machines used to train models are resources that cost money, after all, and you do not want to reserve them longer than necessary. One trick to save time is to use the last run's model parameters as initial values for the next iteration. 

Dryad is an academic project, whereas Spark is widely deployed in production, and now has a company behind it for support. Just focus on Spark. 

Yes, but no-one can tell if they will work well for your problem, so just try it and see. Don't give up if it does not work at first, because training neural networks requires some practice; there are lots of parameters, and not every configuration will work well. Even the optimization algorithm is a hyperparameter. 

The quality of your features might actually be better than you think. If they provide linear separability, nonlinear kernels will overfit more readily than a linear kernel, leading to your result. 

Google does not currently have any public GPU or TPU offerings, but they might be able to help if you work for a large company. Contact them directly. Source: Google reps, just today. 

I tried running a RANSAC model on your data, but got worse results than a straight linear regressor. The ten-fold cross-validated mean absolute error over all three response variables (r,g,b) for the linear model was about 0.37. I also ran a random forest model for comparison, and got about the same score. This suggests that the linear model is not shabby, but it is up to you to decide if either is good enough. 

Why should it work? is an argument of cross_val_score, not score. The documentation says "returns the mean accuracy on the given test data and labels." This behavior is not specific to , but common to classifiers in sklearn. Perhaps you want sklearn.metrics? 

You can rank the paths by frequency using the database (SQL), constraining the start-, end- points as necessary once you fix the length of the window. If you let the path length be variable then you will not be able to do it all in SQL. In that case you can learn the transition probabilities between states then solve a weighted shortest path problem, where the distances are log likelihoods. Or you can use a heuristic like A* search. I do not know of any library that will do all this since it is very ad hoc, but you can do the visualization in networkx. 

What difference does it make? Nothing stops you from applying to all of them. The real question is, can you and do you want to write software or do you just want to analyze data? Pay attention to the job descriptions in this regard. Data analysts are typically expected to be strong in classical statistics, while machine engineers are expected to know ... machine learning. Then every field has certain problems that you need to be familiar with before taking the technical interview, such as recommender systems for media companies, and time series forecasting for business/finance. You can find out what they are by reading around and actually applying to companies. Expect several rounds of interviews, including mathematical and programming challenges (real-time or off-line), and questions about your research. Once again, every field needs data scientists, so this is not a criterion. 

A standard trick is to estimate the logarithm of the desired quantity, then take its exponential, which is always positive. The drawback is that the error is optimized for the log, which treats differences in order of magnitude as equal. Another option is to do your regression as usual then project onto the feasible set (use the positive part of the output; $max(0, \cdot)$) 

As you probably know, the Gaussian distribution is fully specified by its mean and covariance. theta0, thetaL, and thetaU are, respectively, initial values, lower bounds, and upper bounds on the covariance. You can set them based on your knowledge of the particulars of the problem. To learn more about Gaussian processes, I suggest visiting this great Web site: $URL$ It even offers a free ebook. 

This is a fascinating combinatorial problem. I would featuring each pixel using its full temporal trajectory, then embed them in a grid using the k nearest neighbors. The real goal is to maximize the likelihood of the video being a sequence of natural (real life) images, which you can test with a classifier, but you might be able to get away with just a smoothness cost; say, the sum of differences between adjacent pixels. Once you have started filling in the grid, smoothness constraints will reduce the search space (since a pixel will have to be close to multiple other pixels), thus speeding things up, assuming you are using an efficient data structure for querying the nearest neighbors; see for example $URL$ 

Some discriminative features like presence of URLs, frequency of proper punctuation and spelling mistakes translate easily. For semantic features you can use multilingual word embeddings, so your content can be treated by the same classifier, regardless of language. My educated guess is that you should be able to detect most spam without going this far. 

SFrame is not used much in industry, so I'd stick to pandas or Spark DataFrames. But they're all rather similar, and you should not spend much time thinking about it: it is easy to pick these things up, and employers understand this. Concentrate on the algorithms; that's the real "foundation", not the tools. 

It depends on whether you're going to write production code or even pseudo-code that will be implemented by others. If so, yes you need computer science skills. If you're going to merely analyze data, you can technically get away without it, but some companies (e.g., Facebook) expect all technical employees to know computer science. Computer science is useful even to pure analysts because it will help you implement algorithms not covered by your libraries. I make use of recursion and dynamic programming, for example, but I wrote production code. That's my deliverable, after analysis and prototyping. 

I would train a gamma-distributed GLM; one for each language. Now, I assume the set of parameters is fixed within each language. You can incorporate ordinal variables (basic -> intermediate -> expert) using dummy coding (i.e., treat them as categorical variables). If there is no intermediate level one bit will do (0 = basic, 1 = expert). And so on for the other properties. You can find free software to implement GLMs in various language, including python and R. 

You need to ensure the package spark-csv is loaded; e.g., by invoking the with the flag . After that you can use as you did, or . You might need to use instead of just ; I don't know, I haven't tried. 

These are not necessarily simple, but they're representative. If you want to give a demo, I suggest the last one. You could show them how to find similar images. 

You can do this in pandas since your data set is small. For "big" data that does not fit in memory you would want to use a database; PostgreSQL with the PostGIS extension would be ideal, since it handles the nearest neighbor part, which is the most challenging aspect. Here are some sample queries, in python. Show me the Globs where average of each Glob in file Efficiency is greater than 50% 

Welcome to DataScience.SE! You need to ensure that your training distribution is similar to your test distribution in order to get the best results. This can be done through stratified sampling. It might be that there is no problem, and that the classifier is simply allocating its resources to best classify the majority case, though I'm a bit confused by your description because you say that B and C dominate and the "other two classes" (A and D) are "mapped to A and B quite often". One option is to use more discriminating features, if you can think of any. Another is to partition the large classes into (B becomes the union of $B_i$ and likewise for C) if applicable. This can help if the shape of the class is complex by breaking it up into learnable pieces. It helps to plot your inputs, colored by class, to understand which classes overlap or have complex shapes. Finally, you can modify your loss function if getting certain classes wrong is more important than others.