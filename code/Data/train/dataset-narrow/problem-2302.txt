Yes, there is a known relativization barrier. It's given by $A:=TQBF$, because Emil Jeřábek (see comments) is right: the statement that $TQBF$ is $PSPACE$-complete under logspace many-one reductions is well known. 

Motivation to use this question as an opportunity to get familiar with nauty/Traces and the practical aspects of graph isomorphism was provided by vzn. The benefit of learning how to use state of the art programs for graph isomorphisms made it worthwhile to sink some time for finding a counter-example (which I strongly believed to exist). 

because even if the rows have been perfectly matched, it doesn't follow that the vertex labels match the labels given by any isomorphism $φ$. Because a hole in the correctness proof was identified, the above counter-example should be sufficient for refuting claimed correctness of the proposed algorithm. 

You probably mean that they behave very different from bounded fan-in gates. I guess $\mathbf{NC}^0$ is too weak and $\mathbf{NC}^1$ is too strong for describing real hardware (circuits). Real hardware needs at least addition and storage-access (multiplexer), which are both in $\mathbf{AC}^0$. Other nice to have functionality like multiplication and division is in $\mathbf{TC}^0$. It would be interesting whether current hardware contains any functionality (i.e. "fast" processor instruction) not in $\mathbf{TC}^0$. At least I am not aware of any. 

Minimum Dominating Set is not an isomorphism problem, hence there is no reason why it should be expected to be reducible to GI. 

For a stack operation $(s,t) \in \Gamma^{\{0,1\}}\times\Gamma^{\{0,1\}}$, there are the three cases $(s,t)=(a,\epsilon)$, $(s,t)=(\epsilon,a)$, and $(s,t)=(a,b)$. The stack operation $(a,\epsilon)$ gets reversed to $(\epsilon,a)$ as follows 

I omitted the definitions for "spelling" and "rational labels", because I only want to know who first introduced nondeterministic M-automata, and proved the equivalence of (1) and (2). 

Before this question, my opinion was that Graph Isomorphism might be in P, i.e. that there is no evidence to believe that GI is not in P. So I asked myself what would count as evidence for me: If there were mature algorithms for $p$-group isomorphism that fully exploited the available structure of $p$-groups and still would have no hope to achieve polynomial runtime, then I would agree that GI is probably not in P. There are known algorithms that exploit the available structure like Isomorphism testing for $p$-groups. by O'Brien (1994), but I haven't read it in sufficient detail to judge whether it fully exploits the available structure, or whether there is any hope to improve this algorithm (without exploiting additional non-obvious structure of $p$-groups) to achieve polynomial runtime. But I knew that Dick Lipton called for action near the end of 2011 to clarify the computational complexity of the group isomorphism problem in general, and of the $p$-group isomorphism problem specifically. So I googled for 

I think this a misanalysis of the "co" prefix in this case. "Coroutine" is "co" in the sense of "co-worker"; something that works together with another. The term precedes by a long way the gross overuse for programming concepts of the prefix "co" in the Category Theoretic sense of a dual of another concept. (Yes, there is editorial content there. ;-) ) 

Just because you can always embed a lower-order type into the higher-order universe doesn't imply that the reverse is true. The higher-order universe is always strictly larger than the lower-order one, so there is no problem with respect to predicativity. 

Though it many not be obviously directly related, one thing that comes to mind is the concept of "blame" by Wadler et al.. This give you a theoretical basis to think about mixing together different typing regimes into a coherent whole. In essence, blame allows you to mix together languages with weaker type guarantees with languages that have stronger type guarantees without losing all the benefits of the strong guarantees. The idea is that the parts of the system with weaker guarantees will get the "blame" if certain things go wrong, localizing runtime type errors. Hopefully you can see how that might be useful for FFI and bindings that apply to languages with varying type systems. Edit: See Sam TH's answer for a fuller intellectual history of the concept of "blame". 

This started as a comment under Andrej Bauer's answer, but it got too big. I think an obvious definition of ambiguity from a Finite Model Theory point of view would be: $ambiguous(\phi) \implies \exists M_1,M_2 | M_1 \vDash \phi \wedge M_2 \vDash \phi \wedge M_1 \vDash \psi \wedge M_2 \nvDash \psi$ In words, there exist distinct models of your grammar encoded as a formula $\phi$ that can be distinguished by some formula $\psi$, perhaps a sub-formula of $\phi$. You can connect this to Andrej's response about proofs through Descriptive Complexity. The combination of the existence of an encoding of a particular model plus its acceptance by an appropriate TM as a model of a given formula IS a proof that the axioms and inferences (and hence an equivalent grammar) encoded in that formula are consistent. To make this fully compatible with Andrej's answer, you would have to say that the model is "generated" by the formula acting as a filter on the space of all possible finite models (or something like that), with the encoding and action of filtering on the input model as the "proof". The distinct proofs then witness the ambiguity. This may not be a popular sentiment, but I tend to think of finite model theory and proof theory as the same thing seen from different angles. ;-) 

We can add to these caveats the observation that practical implementations of quantum computers will have noise anyway. This model of computation is interesting primarily for theoretical reasons, as one concerned with composing unitary transformations rather than feasible computation, and also as an exact version of $\mathsf{BQP}$. In particular, despite the caveats above, we have $\mathsf{P} \subseteq \mathsf{EQP} \subseteq \mathsf{BQP}$. The reason for defining $\mathsf{EQP}$ in the way I do is so that DISCRETE-LOG can be put into $\mathsf{EQP}$. By [ Mosca+Zalka 2003 ], there is a polynomial-time algorithm to construct a unitary circuit which exactly solves instances of DISCRETE-LOG by producing exact versions of the QFT depending on the input modulus. I believe that we can then put DISCRETE-LOG into $\mathsf{EQP}$, as defined above, by embedding the elements of circuit-construction into the way that the gate coefficients are computed. (So the result DISCRETE-LOG $\in \mathsf{EQP}$ essentially holds by fiat, but relying on the construction of Mosca+Zalka.) Suspending unitarity Let $\mathsf{EQP_{\mathrm{GL}}}$ be the computational class that we get if we suspend the restriction that gates be unitary, and allow them to range over invertible transformations. Can we place this class (or even characterize it) in terms of other traditional non-deterministic classes $\mathbf C$? One of my reasons for asking: if $\mathsf{BQP}_{\mathrm{GL}}$ is the class of problems efficiently solvable with bounded error, by uniform "non-unitary quantum" circuit families — where YES instances give an output of $|1\rangle$ with probability at least 2/3, and NO instances with probability at most 1/3 (after normalizing the state-vector) — then [Aaronson 2005] shows that $\mathsf{BQP}_{\mathrm{GL}} = \mathsf{PP}$. That is: suspending unitarity is in this case equivalent to allowing unbounded error. Does a similar result, or any clear result, obtain for $\mathsf{EQP_{\mathrm{GL}}}$? 

The circuits are just compositions of these in the sensible way. We might also allow fan-out, in the form of circuits which unitarily embed $|0\rangle \mapsto |00\cdots 0\rangle$ and $|1\rangle \mapsto |11\cdots 1\rangle$; we should at the very least permit these maps at the input, to allow each (nominally classical) input bit to be copied. It seems reasonable either to consider the entire continuum of such gates, or to restrict to some finite collection of such gates. Any choice gives rise to a different "quantum monotone gate basis" for the circuits; one can consider what properties different monotone bases have. The states $\rho_{00}, \rho_\mu, \rho_\nu, \rho_{11}$ can be chosen completely independently, subject to the monotonicity constraint; it would undoubtedly be interesting (and probably practical to bound error) to set $\rho_{00} = |0\rangle\!\langle 0|$ and $\rho_{11} = |1\rangle\!\langle 1|$, though I see no reason to require this in the theory. Obviously AND and OR are gates of this type, where $\rho_\mu = \rho_\nu = |0\rangle\!\langle 0|$ and $\rho_\mu = \rho_\nu = |1\rangle\!\langle 1|$ respectively, whatever one chooses $|\mu\rangle$ or $|\nu\rangle$ to be. For any constant k, one might also consider gate bases including k-input-one-output gates. The simplest approach in this case would probably be to allow gates $\mathsf G: \mathbf H^{\otimes k} \to \mathbf H$ which may be implemented as above, allowing any decomposition of the subspaces $\mathcal V_w \leqslant \mathcal H_2^{\otimes k}$ of each Hamming weight $0 \leqslant w \leqslant k$, and to require that $$ \max_{|\psi\rangle \in \mathcal V_w}\;\langle 1 | \,\mathsf G\Bigl( |\psi\rangle\!\langle\psi| \Bigr)\, |1\rangle \;\;\leqslant\;\; \min_{|\psi\rangle \in \mathcal V_{w+1}}\;\langle 1 | \,\mathsf G\Bigl( |\psi\rangle\!\langle\psi| \Bigr)\, |1\rangle $$ for each $0 \leqslant w < k$. It is not clear how much additional computational power this would give you (nor even in the classical case). I don't know whether there's anything interesting to say about such circuits beyond the classical case, but this seems to me to be the most promising candidate definition of a "quantum monotone circuit". A quantum variant of Razborov's result Consider the exposition by Tim Gowers of the results of Alon & Boppana (1987), Combinatorica 7 pp. 1–22 which strengthen Razborov's results (and makes explicit some of his techniques) for the monotone complexity of CLIQUE. Gowers presents this in terms of a recursive construction of a family of sets, staring from the "half-spaces" $$E_j = \Bigl\{ \mathbf x \in \{0,1\}^n \;:\; x_j = 1 \Bigr\}$$ of the boolean cube for each $1 \leqslant j \leqslant n$. If we remove the priviledged position of the standard basis in the base sets, in analogy to the Quantum Lovász Local Lemma, we may consider a subspace of $\mathcal H_2^{\otimes n}$ to correspond to a binary proposition (whether a state belongs to the subspace, or is instead orthogonal to it) which might arise from measurement. For instance, we may consider $n$ subspaces $\mathcal A_j \leqslant \mathcal H_2^{\otimes n}$ given by $$\begin{align*} \mathcal A_j = U_j \mathcal E_j \;, & \text{ for each $1 \leqslant j \leqslant n$} \\ \text{where } &\mathcal E_j := \Bigl\{ | \mathbf x \rangle \;:\; \mathbf x \in E_j \Bigr\} ; \\ &U_j : \mathcal H_2^{\otimes n} \to \mathcal H_2^{\otimes n} \text{ a unitary of bounded complexity}. \end{align*}$$ We allow the quantum-logical analogues of conjunction and disjunction of subspaces: $$\begin{gather*} \mathcal A \wedge \mathcal B = \mathcal A \cap \mathcal B ; \\ \mathcal A \vee \mathcal B = \mathcal A + \mathcal B = \Bigl\{ \mathbf a + \mathbf b \,:\, \mathbf a \in \mathcal A\;,\; \mathbf b \in \mathcal B \Bigr\} . \end{gather*}$$ We then ask how long a recursive construction of conjunctions and disjunctions of spaces are required to obtain a space $C$, such that the projector $\Pi_C$ onto $C$ differs only slightly from the projector $\Pi_{K(r)}$ onto the space spanned by the indicator functions of graphs having cliques of size $r$; for instance, so that $\| \Pi_C - \Pi_{K(r)} \|_\infty <\; 1/\mathrm{poly}(n) $. The monotonic part is involved in the quantum logical operations, and the primitive propositions about the input are quantum as well. In the general case, there is a problem with treating this as a computational problem: the disjunction doesn't correspond to any knowledge which could be obtained with certainty by measurements on a finite number of copies using black-box measurements for $\mathcal A$ and $\mathcal B$ alone, unless they are the images of commuting projectors. This general problem can still be treated as an interesting result about geometrico-combinatorical complexity, and might give rise to results related to frustrated local Hamiltionians. However, it might be more natural to just require that the subspaces $\mathcal A_j$ arise from commuting projectors, in which case the disjunction is just the classical OR of the measurement outcomes of those projectors. Then we may require that the unitaries $U_j$ all be the same, and this becomes a problem about a unitary circuit (which gives rise to the "primitive events") with monotone classical post-processing (which performs the logical operations on those events). Note also that if we do not impose any further restrictions on the spaces $\mathcal A_j$, it may being a subspace with very high overlap with some space $\mathcal E_k^\bot$ spanned by standard basis states $\mathbf x \in \bar E_k$, which are those binary strings in which $x_k = 0$. 

It is is worth thinking about WHY intuistionistic logic is the natural logic for computation, since all too often people get lost in the technical details and fail to grasp the essence of the issue. Very simply, classical logic is a logic of perfect information: all statements within the system are assumed to be known or knowable as unambiguously true or false. Intuistionistic logic, on the other hand, has room for statements with unknown and unknowable truth values. This is essential for computation, since, thanks to the undecidability of termination in the general case, it will not always be certain what the truth value of some statements will be, or even whether or not a truth value can ever be assigned to certain statements. Beyond this, it turns out that even in strongly normalizing environments, where termination is always guaranteed, classical logic is still problematic, since double negation elimination $\neg\neg P \implies P$ ultimately boils down to being able to pull a value "out of thin air" rather than directly computing it. In my opinion, these "semantic" reasons are a much more important motivation for the use of intuistionistic logic for computation than any other technical reasons one could marshal. 

A fairly comprehensive discussion of this stuff can be found in this book: Lectures on the Curry-Howard Isomorphism. This is based on the freely available older version: Lectures on the Curry-Howard Isomorphism. 

If you are having trouble with the concept of least fixed point, I would recommend spending some time getting a background in more general order theory. Davey and Priestley, Introduction to Lattices and Order is a good intro. To see why the transitive closure is the least fixed point, imagine building up the closure from an empty set, applying the logical formula one step at a time. The least fixed point arrives when you can't add any new edges using the formula. The requirement that the formula be positive ensures that the process is monotonic, i.e. that it grows at each step. If you had a negative subformula, you could have the case where on some steps the set of edges would decrease, and this could lead to a non-terminating oscillation up and down, rather than a convergence to the LFP. 

Here $P(Q)$ is the power set of $Q$ and $ssM(Q)$ is the space of substochatic matrices on $Q$. A right substochastic matrix is a nonnegative real matrix, with each row summing to at most 1. There are many different reasonable acceptance conditions The transitions are only one part of a machine, initial and final states, possible output and acceptance conditions are also important. However, there are only very few non-eqivalent acceptance conditions for deterministic machines, a number of reasonable acceptance conditions for non-deterministic machines (NP, coNP, #P, ...), and many possible acceptance conditions for probabilistic machines. Hence this answer focuses primarily on the transitions. Reversibility is non-trivial for probabilistic machines A partial function is reversible iff it is injective. A relation is always reversible in a certain sense, by taking the opposite relation (i.e. reversing the direction of the arrows). For a substochastic matrix, taking the transposed matrix is analogous to taking the opposite relation. In general, the transposed matrix is not a substochastic matrix. If it is, then the matrix is said to be doubly substochastic. In general $P P^T P\neq P$, even for a doubly substochastic matrix $P$, so one can wonder whether this is a reasonable notion of reversibility at all. It is reasonable, because the probability to reach state $B$ from state $A$ in $k$ forward steps is identical to the probability to reach state $A$ from state $B$ in $k$ backward steps. Each path from A to B has the same probability forward and backward. If suitable acceptance conditions (and other boundary conditions) are selected, then doubly substochastic matrices are an appropriate notion of reversibility for probabilistic machines. Reversibility is tricky even for non-deterministic machines Just like in general $P P^T P\neq P$, in general $R\circ R^{op}\circ R \neq R$ for a binary relation $R$. If $R$ describes a partial function, then $R\circ R^{op}\circ R = R$ and $R^{op}\circ R\circ R^{op} = R^{op}$. Even if relations $P$ and $Q$ should be strictly reversible in this sense, this doesn't imply that $P\circ Q$ will be strictly reversible too. So let's ignore strict reversibility now (even so it feels interesting), and focus on reversal by taking the opposite relation. A similar explanation like for the probabilistic case shows that this reversal works fine if suitable acceptance conditions are used. These considerations also make sense for pushdown automata This post suggests that one motivation for non-determinism is to remove that asymmetry between forward steps and backward steps. Is this symmetry of non-determinism limited to finite automata? Here are corresponding symmetric definitions for pushdown automata 

I will go out on a limb here and say that, if you are willing to squint a bit, proofs and terminating programs can be identified. Any terminating program is a proof that you can take its input and produce its output. This is a very basic kind of proof of implication. Of course, to make this implication carry information more meaningful than stating the obvious, you need to be able to show that the program works for any and all instances of input drawn form some class with logical meaning. (And so too for the output.) From the other direction, any proof with finite inference steps is a symbolic program manipulating objects in some logical system. (If we don't worry too much about what the logical symbols and rules mean computationally.) Types and propositions can be made to work similarly. Any type T can be assigned the proposition $\exists x : x \vdash T$ with obvious truth conditions. Any proposition can be turned into the type of it's proofs. This is pretty simplistic, but I think it does suggest the robustness of the idea. (Even if some people are bound not to like it. ;-) ) 

I'm wondering if anyone knows of a formalization (even limited) of any part of finite model theory in any of the major proof assistants. (I'm most familiar with Coq, but Isabelle, Agda, etc. would acceptable.) Especially of interest would be any of the results in descriptive complexity. 

CoC is most likely the way to go. Just dive into Coq and work through a nice tutorial like Software Foundations (which Pierce of TaPL and ATTaPL is involved in). Once you get a feel for the practical aspects of the dependent typing, go back to the theoretical sources: they'll make a lot more sense then. Your list of features sounds basically correct, but seeing how they play out in practice is worth a thousand feature points. (Another, slightly more advanced tutorial is Adam Chlipala's Certified Programming with Dependent Types) 

and tacitly defines $\mathsf{co\text-}$ on page 12 in the usual way. Kozen's treatment of these operators is enough to indicate how they are connected with the "usual" complexity classes, and to describe Toda's theorem, but does not much discuss their relationships and only mentions them for a total of 6 pages (in what is after all a book covering a much wider topic). Hopefully someone can provide a better reference than this. 

This is a [technical but relatively well-known] upper bound for the class $\mathsf{BQP}$. Its relationship to other "gap-defined classes" could be summarised as $$ \mathsf{SPP \subseteq LWPP \subseteq WPP \subseteq AWPP \subseteq PP}, $$ where $\mathsf{LWPP}$ is notable as a [once more technical but relatively-well known] upper bound on $\mathsf{EQP}$ (with algebraic coefficients over any constant set), and $\mathsf{SPP}$ is the gap-definable version of $\mathsf{UP}$. Thus all of these contain $\mathrm{UNIQUE\text-SAT}$. By Valiant-Vazirani and the closure of $\mathsf{BQP}$ under subroutines, if $\mathrm{UNIQUE\text-SAT} \in \mathsf{BQP}$, we then have $\mathsf{NP \subseteq PH \subseteq BQP}$. We may reasonably consider this to be unlikely, so $\mathrm{UNIQUE\text-SAT}$ is an easy example of a problem in $\mathsf{AWPP}$ which one may consider unlikely to be in $\mathsf{BQP}$ (albeit a problem which belongs to classes which "seem much smaller" than $\mathsf{AWPP}$ as well). The relationship of $\mathsf{MQ^2}$ to $\mathsf{BQP}$ The class $\mathsf{AWPP}$ also upper bounds the following (not generally well-known) class defined by Tušarová [arXiv:cs/0507057]: 

In a pure mathematical sense, you could in principle create models of computation using any sort of recursively composable structure, so long as you can describe how it represents a transformation of suitably represented input data to output data. But in an applied mathematical sense — or more accurately, in an actual scientific sense — there is a question of whether such models of computation correspond to (i.e. models well) anything which is observed in practise (e.g. perhaps because we observe it in machines constructed to do the computations). We're confident that permutation matrices and stochastic matrices, composed by products on local systems, represents a feasible model of computation for transforming probability distributions. It is also accepted in principle that unitary transformations on unit-2-norm wave functions (composed in a similar way) is not unreasonable as a model of computation; showing that it is actually feasible is widely accepted as a (very challenging!) engineering problem. Both of these models of computation can be subsumed into the formalism of CPTP super-operators (which map linear operators to other linear operators, in a way which preserves the trace, and robustly maps positive-semidefinite operators to other such operators), which in certain respects is a better way of describing quantum computation than by unitary transformations or projectors alone. Whether there are strictly more general (in the sense of more powerful, and using the same sort of representation of the input and output data) models of computation than unitary transformations or CPTP superoperators is in essence a question of theoretical physics. So the answer is "maybe — but we don't know yet, and don't have convincing reasons to believe in any particular one". 

That definition implies that there is no required unique bottom in a predomain, i.e. each down set (reverse chain) could have a distinct least element, but that there is no necessary least element for the whole structure. You could form a domain by appending a bottom under all other least elements, or by identifying all least elements (i.e. say they are all the same element) if that is semantically viable in your structure. 

I don't know of any work that pursues this line, but a few moments thought about it led me to this hypothesis: wouldn't the "root" of the exponential type just be the codomain, and the "logarithm" of the exponential just the domain? 

I would recommend investigating the field of Finite Model Theory and more particularly its sub-field Descriptive Complexity. It can be used to model such sorts of problems. 

Let me offer the simple, intuitive way that I think about this. If you restrict yourself to closed lambda expressions, you have an equivalent of the combinatory logic. In fact with just a few simple closed lambda expressions you can generate all the others. Closed lambda expressions give you the equivalent of implications where any conclusion/output you reach is either something you put in as an input, or something that you built by combining your inputs (in the general case, possibly recursively). This means that you can't pull a result "out of thin air" the way you can with non-constructive logics / mathematics. The only tricky bit left is how you handle negation / non-termination, which is a whole area by itself, but hopefully I've already given you the simple, but deep, correspondence between the three that you are asking for. 

I find that the most "natural" way to get an intuition of complexity classes is to revert to Turing's starting point and try to solve the problem "manually". Start with a sorting task. From a jumble of, say, five words have the class order them alphabetically. This should be easy. Then double the number of words, and repeat the exercise. It will be obvious that, though the second problem is harder, it isn't that much harder. Next try a traveling salesman task. Start with a grid of say three cities with distances between them. The class will probably be able to solve this in short order. Now double the number of cities to six, and continue with the exercise until everyone's head is spinning. An experience like this is very likely to leave a lasting visceral impression that a purely technical introduction may not.