I've setup a port knocking scheme to protect port 22 using iptable rules. On top of that I've setup some "honey pot" rules to disable port knocking for the client for several minutes if they hit common ports that are not used (port 21 for example) or ports around the knocking sequence. This seems to work well (I believe it's a better option than changing the port to something obscure). For SSHD password logins are disabled anyway - an encrypted private/public key combo is required. On this page there is a "drip pan" scheme that if unused ports are hit the client cannot communicate with anything (even open ports) for 60 seconds. My biggest concern is "--hitcount 3" seems to be triggered by any simple request to a single port. So my question is, what do you think about this kind of setup? Is the "drip pan" too much on a web server, or just right? Other than keeping a drop in-out policy and only opening what is needed, do you guys have any other firewall security advice? Edit: I'm using CentOS 5.7. We're assuming as "secure as possible within reason". A server would have to pass PCI/SAS compliances (and other similar standards). Focus would be the firewall, not necessarily vulnerabilities with individual services that could come be discovered (that's another topic). Anything that I'm missing related to the firewall or anything on the outside. Want to make it as difficult as possible for an attacker to gain access. 

I was reading an article on Linux Journal about Memcached and it mentions Netbooting multiple servers off a single root image: 

You should avoid trying to do this with PHP. By the time PHP gets involved, it's already too late - the memory has already been allocated. You can ban IP addresses at any layer, but the lowest level that uses the least amount of resources is the route you want to take. This is usually the firewall. At the very least, iptables (linux firewall) is what you want to use. There are tools that others have mentioned, such as Fail2Ban, that can automate this for you. External firewall would be better. Besides trying to ban offending IP addresses, you should try to make better use of your resources. If a request takes less resources it will take longer for an attack to be effective. Apache also uses a lot of memory. If you're using mod_php, it's even worse because PHP is loaded inside of every Apache child process. This means even requests to static content (css/js/images) are loading PHP even when PHP isn't being used. You can solve this problem by using FastCGI instead. mod_fcgid is a good option. There are also other web servers that are more resource efficient. Nginx is my favorite. There's also Lighttpd. A lot of people like Litespeed (drop in replacement for Apache). If you want to stick with Apache, consider tuning it as best you can. Consider disabling .htaccess. Here's a good explanation why. 

I have a write-heavy application. The application is best compared to surveys - the customer creates custom questionares and this is saved to the database. Most of the requests are from their users submitting these forms. Later on our customers do complex reports and graphs on these submissions. Making sure our application server (PHP) and the web server (Nginx) scales is quite easy, the trouble is scaling the database server onto multiple servers. A lot of applications are more read heavy, so typically you'll have a master-slave replication setup where all writes go to a single master, but reads are distributed to the slaves. For us this doesn't work because we're doing writes most of the time. I've seen mention of a master-master setup, but this typically hits a snag with auto incremented primary keys. The solution is typically to have one server do odd numbers, and the other do evens. I want to avoid that. On some similar questions I've seen mention of the Tungsten Replicator and how it gives you a lot more flexibility with replication. Would this help me at all? What kind of benefits would this give me that MySQL's built in replication can not provide? There is also MySQL Cluster, but this typically hits a snag with very large databases and complex queries (joins). I need to be able to run complex reports, so this probably won't work for me. I'm looking for redundancy, automatic fail over, distributing requests, and data integrity. Are there other RDMS that provide better solutions that are suitable for the web? 

What you're describing is based on the header. If you could do this with Nginx, you'd have to do it with an if block, which would be inefficient. I'm not sure you could do it any other way. But you shouldn't be doing this anyway as, as you said, it isn't really secure, just hidden. You should look at these: $URL$ $URL$ Digest would be more secure. Basic transmits user/password in the clear. But either would be far more secure than what you're describing as the pages are actually protected. 

I need to deploy vSphere Server Appliance 5.1. I have vSphere Client running locally and my internet upload is capped at 3 Mbps. It says it's going to take about 200 minutes to upload. When selecting a URL as opposed to a local file, does vSphere Client download it locally and then upload, or does it download the OVA directly to the server? My goal is to avoid waiting 3 1/2 hours for this to upload. If specifying a URL isn't any faster, are there any other methods that would allow me to deploy from the datacenter instead of my office? We don't have any Windows VM's installed on our cluster. So unfortunately I don't have a Windows machine with faster upload speed. 

The company I use ranges from $136-$495 per month, per server, ranging from low-end (AMD Athlon 64 X2 4600+) to enterprise (Intel Dual Xeon X5650, A/B Power). The biggest pro for renting a server is if hardware fails they replace it for you. But if you have enterprise servers, $400+ a month can get expensive. At what point would colocation be more cost effective? As in, how many servers would have have to be renting for colocation to be better? Any other pros and cons? 

Assuming the software is the same (usually isn't), virtual firewalls can be better than a physical firewall because you have better redundancy. A firewall is just a server with CPU, RAM, and uplink adapters. It's the same argument as a physical web server verses a virtual one. If the hardware fails a virtual server can be migrated to another host automatically. The only downtime is the amount of time it takes for the virtual firewall be migrated to another host, and perhaps the time it takes for the OS to boot. A physical firewall is bound to the resources it has. A virtual firewall is limited to the resources inside a host. Typically x86 hardware is far cheaper than that of an physical enterprise firewall. What you have to consider is the cost the hardware, plus cost of the software (if not using open source), plus the cost of your time (which will depend on the software vendor you go with). After you compare the cost, what features are you getting on either side? When comparing firewalls, virtual or physical, it really depends on the feature set. Cisco firewalls have a feature called HSRP which allows you to run two firewalls as one (master and slave) for failover. Non-Cisco firewalls have a similar technology called VRRP. There's also CARP. When comparing a physical firewall to a virtual one make sure you're doing an apples to apples comparison. What features are important to you? What is the configuration like? Is this software used by other enterprises? If you need powerful routing, Vyatta is a good bet. It has firewall capabilities. It has a very Ciso-like configuration console. They have a free community edition at vyatta.org and a supported version (with some extra featutes) at vyatta.com. The documentation is very clean and straightforward. If you need a powerful firewall, take a look at pfSense. It can also do routing. We decided to run two Vyatta instances with VRRP on our ESXi hosts. To get the redundancy we needed with Cisco (two power supplies per firewall, two firewalls) it would have cost $15-30k. For us Vyatta community edition was a good option. It has a command line only interface, but with the documentation it was easy to configure.