There's a very good reason why the use of Microsoft's multicast NLB is not very popular in a whole lot of networks - it's a hack that breaks several basic operating practices and RFC's. So - under normal circumstances multicast means that the destination IP of a packet is somewhere in the 224.0.0.0/4 range, with this range broken up into various well-known chunks. In turn, there is a mapping of these IP multicast group addresses to certain MAC addresses. Here is a Microsoft article describing how that mapping is achieved. Anyhow - what we normally have is that traffic is sent to a multicast group IP that is mapped to a multicast MAC address. The switch is smart enough to snoop on the Internet Group Management Protocol (IGMP) which is how individual end hosts signify which multicast groups they want to receive. When the switch sees an IGMP join request, it programs the port such that any packets to the multicast MAC are copied down. This means that on a network with 100 hosts if only 10 signify interest in this multicast group then traffic bound to that group only shows up on those 10 ports (...vs all 100 in the case of a broadcast). Where Microsoft broke this is that they require a mapping of a unicast IP to a multicast MAC. They also require the underlying switch to either turn off IGMP snooping (...thus causing the cluster packets to be sent to every host in the subnet, whether they need them or not) or to statically map this address to the physical ports occupied by their servers (hint: this is NOT supported in lots of hardware and breaks vswitches and virtualization pretty horribly as well). So - in essence Microsoft wrote a really, really hacky and awful way of doing this that requires that the underlying network layer break all kinds of accepted best practices for how networks otherwise work. To their credit they also have some newer modes (IGMP based) that aren't quite as bad but, ultimately, it's now a lot more common in new installations to see folks dropping back to a unicast-based mode that uses some kind of external load balancer to achieve the same effect in a more sane manner. 

Route table poisoning used to happen accidentally (and with some regularity) when RIP (v1) was commonly used as a mechanism for end hosts to find redundant gateways in the days before FHRP's (i.e HSRP) came into use. Misconfigured hosts (usually UNIX sysadmins forgetting to add the -q / quiet option to ripd) would advertise bad routing into a shared subnet. This would not only cause the other hosts in the network to start sending traffic to the wrong place but could (would) propagate through the routers to the rest of the environment. Several elements of best-practice came out of this: 

So, yes, RIPv1 was (is) certainly vulnerable to receiving bad information (intentionally or otherwise). It's not necessarily the use of UDP per se that's the origin of this but rather a fundamentally obsolete protocol implementation (regardless of protocol choice) as well as bad/sloppy practice on the part of network engineers. 

Which models? Sampling enabled? Flow aggregation? Expected traffic mix? All of these need to be asked- and answered- to determine the use of Netflow. In general the following observations tend to be true- 1.) The amount of actual traffic generated by NDE is very rarely an issue for enterprise networks. Thousands of flows per minute can translate to a few kilobits of actual exported data. If a given device is large enough to export a substantial volume of data, chances are excellent that this device has ample bandwidth available. 2.) CPU overhead can be a factor on some platforms, but typically only if CPU utilization is already on the high side, and moreso on older software-based platforms. If CPU is low and the box handles it in hardware, I wouldn't generally worry. 3.) On newer boxes you have the ability to define what sorts of traffic you want to observe, methods to aggregate updates and - most importantly - facilities for taking a sampling of traffic - so analyzing one in one thousand vs all packets on the box. This makes a huge difference - and, indeed, makes it even -possible- in higher volume environments. 4.) As I alluded to above, the type of traffic you're monitoring can make a tremendous difference. Hundreds of gigabits in a few long-lived flows consumes few resources while a few megabits of DoS attack from random addresses/ports can be a lot more stressful. Ultimately, however, if you've turned on Netflow for diagnostic purposes and the CPU isn't getting hit too hard then it's more than likely fine to leave on for the duration. I'll also mention that NF was intended to be used on on ongoing basis - and that most shops who use it run it that way. NB: Your colleague may have been using RE-based flow generation on Juniper boxes. This requires explicitly redirecting traffic to the processor on the box and is generally recommended against unless fairly heavily sampled. On newer geart (both Juniper and Cisco) most of the heavy lifting is dealt with in hardware and thus isn't as potentially service effecting. 

In general the use of multiple production DHCP servers within an organization tends to be a function of scale and availability. Scale is going to be driven by the number of clients, the churn of said clients and, of course, the duration between reservation requests. Availability is just a question of how much impact the loss of a server would create vs the cost to maintain (and synchronize) a redundant pair. It's hard to make a definitive recommendation without knowing a bit more about the environment but at ~50 concurrent users it's incredibly unlikely that you'd be taxing just about any platform in common use today. As to availability - losing DHCP can render the network virtually unusable. My usual tendency is to run redundant pairs if at all possible, but without knowing design/platform/budget constraints I can't really say if it would make sense in your environment. 

Usually this is discussed in the context of network capture appliances. So - for example - if you have taps on four links and a given packet happens to traverse all four then you'll naturally see four copies of the same packet. De-duplication is the capability to deliver only one copy of this packet to a protocol analyzer. ETA: Some actual use-cases might help to illustrate both the need and the function better. Imagine a classic enterprise network that consists of dozens of switches organized into a series of core, aggregation and end-device access switches. A few of these switches might connect to the firewalls and routers that go to the Internet while others connect to a WAN and others still are for servers and users. If there's a desire to be able to monitor traffic on a wide-scale basis it tends to make sense to deploy some combination of in-line taps and on-switch port mirroring (i.e. SPAN ports). The result of this (common) monitoring mechanism is that the output of these various taps is brought back to a common point for analysis by sniffers, performance analyzers, security devices, etc. One problem that arises with this is that I'll see the same packet as it traverses each switch. In the case of communications between two servers on two access switches I might see that packet three times - once on its source, once on an intermediate aggregation switch and then again on its destination. At the same time if I want to see traffic from one of those servers to the Internet I could see the same packet on its source, the aggregation, a core switch and then whatever connects to the Internet (four copies of the same packet). De-duplication (and, yes, it's a real feature that several vendors implement in actual shipping products) makes sure that an otherwise identical packet is only presented once. This is really crucial because the condition of seeing the same packet multiple times can also be indicative of forwarding loops or other network problems. Similarly any kind of performance management solution is going to be gathering inflated data, security devices may see a DoS/traffic flood, etc. Incidentally - the same issue occurs when gathering Netflow/sFlow data from multiple switches, but it's arguably somewhat trickier to implement de-dupe in that situation as there's a need to interpolate information that's not exactly in sync (vs looking at packets that should hash equivalently). Either way the root cause is the same - taking a bunch of isolate point-in-time data and aggregating it on a device that has no way of knowing that it's receiving the same information but processed serially. I suppose one could set up three or four parallel packet captures on an as-needed basis (..and varying based on which switches in the path had the needed information) but it ends up being a lot more convenient to be able to look at the traffic from a bunch of different places at once and have it pre-processed in such a way that you can immediately look at what's interesting rather than spending hours (or likely days) determining where to look and how to pull apart the information. 

First off I'd want to identify exactly how the latency in question is being identified. Is it a particular application running slowly or a wide variety of traffic that can be specifically observed with either long transit times or loss? If it is something specific to one type of application traffic and it isn't the actual application (a very common theme, unfortunately) then I'd want to understand the nature of the traffic that was slow and try to see if I could isolate it. Is it high-bandwidth / large packet that's causing the issue? Lots of small packets? Finally, some basic isolation might shed some light - can you send the same type of traffic experiencing high latency between two hosts on the same vSwitch (i.e. exclude the physical switch)? Between hosts on two different hypervisors on the same blade switch (exclude the Nexus and its connections)? The other common causes should also be ruled out - taking errors on any physical interfaces (in either direction), weird STP/L2 configuration problems (incredibly common on blade switch configurations, unfortunately), tell-tale error messages, counters for packet drops on anything in line, etc. 

As mentioned by others in the comments section, you are losing enough BGP keep-alive packets to cause the session to fail. When the session drops then all traffic will likely cease until BGP re-establishes its session (which is a minimum of 60 seconds and usually a fair amount longer). So - you've got two options: either find a mechanism to prioritize BGP traffic or make BGP less sensitive to packet loss. The former (prioritization) likely isn't going to be doable given that, as you point out, you're relying on a public network and the other side of the session is outside your control. You can certainly prioritize and protect the packets you actually transmit, of course, but that has no guarantee of being honored a hop beyond the router you manage. The latter approach (BGP less sensitive) may be a more pragmatic option. In this case I would suggest making your dead timer longer. At the moment you have packets being sent every 5 seconds, with the session timing out if a response isn't received in 30. Try increasing the latter value to 60 or 90 (also consider increasing the third number, as it is the minimum you will allow negotiated and we don't know the peer's info). This gives you a better shot of getting a keep-alive through during a bursty period. Finding an optimum value for keep-alives is always a compromise between stability and detection speed. I would add, though, that if this is the only path to AWS for you - or if the alternate path is substantially less desirable - that erring on the side of slow failure detection actually makes a lot of sense. 

8 routers form adjacencies to both the DR and BDR (8*2 = 16) plus an adjacency formed between the DR and BDR equals 17. If a full mesh were required it would be 10 routers each with 9 connections. Counted as adjacencies this is (9*10)/2 = 45. 

Slightly different tools that work best in different places. The offset-list allows adjustment to routes as they propagate into a given part of the network without altering the metrics of the originating network. In the case of two networks interconnecting at multiple points via EIGRP this allows a given path to be biased upward or downward without playing with a metric that might cause different (likely undesirable) behavior in the existing topology. Delay is generally the preferred method for tweaking path selection on a universal basis. Bandwidth can work, of course, but always remember that EIGRP considers the lowest bandwidth link in a given path - which means dropping your 1G link to 900M doesn't mean much if it ultimately goes through a 100M connection. Delay, in contrast, is directly additive on a per-hop basis and, as such, has a much greater effect on the route ultimately calculated. The other K values (reliability, utilization, MTU, etc) should basically -never- be touched. These function are either better accomplished through other mechanisms, will create rolling oscillation issued in your network or just shouldn't realistically be a point of consideration in the first place. The largest EIGRP deployments in the world do fine without touching the other K values. If it is suggested that these values be set I would take a harder look at the requirements and tools available. 

So - that's how you define a mac access list. As per my comment, however, it may not behave the way you think it does. It's actually intended for use with bridging - so, for example, blocking a specific MAC address from passing through a bridge group. It's not really analogous to the MAC filtering you might find on a consumer router in that there isn't a way to simply say "don't accept traffic from mac address x" on a standard routed interface. If you want to use IRB (see CCO Transparent Bridging Configuration Guide) you could put the IP on a BVI and then join it and one or more Ethernet interfaces into a bridge-group and then apply MAC filters via access-expressions on the physical interfaces (...thus allowing/preventing certain MAC addresses, or ranges of addresses, from making it to the BVI). The best way to put it might actually be that the sort of MAC filtering you see on a consumer network device is actually part of the switch (..to include any integrated AP) while IP-based rules are likely part of the router. To this end - if you were using a Cisco switch then you could apply MAC filters in a pretty straightforward way via VACL's or PACL's. On a device acting as a pure router, however, it's going to be a different story. 

Yes. This can be done and does work on a number of Cisco platforms. It is specifically applicable in access control situations (traffic filters, route distribution control, etc). It is not supported for interface addressing. That said, it's not very readable and - in my experience, is usually a way to save a few lines of ACL and is a classic example of how preoptimization is the root of all evil. 

SW3 receives the same computed value from SW2 via the two links. When performing its own calculation it will add the locally determined value for its respective links. It doesn't need to know what any of SW2's link bandwidths may be - including the links it has to SW3. 

Failover time is going to be dictated by two chief factors: 1.) Time to detection - How quickly the connected systems can determine a link has failed. 2.) Time to convergence - Once the failure is detected how quickly traffic can be redirected to a working path. Honestly the first item (failure detection) is harder to get right and ends up adding the most time to the process. Loss of physical link is clearly an easy indicator but it's important to remember that this may only be seen in one direction (ex: one strand of fiber fails, but not the other) and that any kind of intervening repeater may actually mask the behavior. To address this the best mechanisms tend to use some kind of lightweight echo mechanism - constant OAM traffic of some sort moving back and forth between connected peers to validate that data is actually passing. Synchronous communication links tend to incorporate this into the basic protocol pretty effectively (SONET being a great example) but we've had to graft technologies on to accommodate higher-level protocols - either in the form of protocol hellos/keep-alives (as found in routing protocols) or, more recently, with lightweight protocols like bidirectional forwarding detection (BFD), which was mentioned in a recent question. The idea with bfd is the use of a common keepalive mechanism (again - very lightweight) operating at a high frequency (usually in the low hundreds of milliseconds), often with hardware assist. The second part (reconvergence) has a lot of other issues associated but its difficulty tends to be directly proportionate to the width of the network. For example - reconverging connectivity between two switches or routers with a pair of redundant links is trivial. Finding an alternate path on a complex international network with thousands of network devices? A whole other ballgame. This, incidentally, is where SONET's gold-standard of 50ms comes from - as APS calls for each node to have an alternate path already hot and ready to receive failed traffic. So - to answer the question... The best possible case is one where a link fails quickly and completely (i.e. someone snips a cable). This delivers immediate results to both connected devices and, in practice, you're not going to see a whole lot of difference between removing one of a pair of equal-cost routes from an L3 forwarding table versus updating the hash tables in an L2 port channel. That said - if you're running an L2 port channel without a protocol to detect a link failure and one link happens to go unidirectional then you might well hit a situation where some portion of traffic is silently dropped on an indefinite basis (i.e. no recovery). If you're relying on LACP or UDLD to pick up this condition then it may take ones- or tens- of seconds to detect (depending on how the protocols are configured). A stock configuration of OSPF is going to take 40 seconds (4 consecutive losses of a 10 second keepalive) to mark a link as failed. A vanilla BGP connection on some implementations could easily be 3+ minutes. If you add BFD to any of these protocols (LACP / OSPF / BGP) then detection time could be as quick as ~150-200 milliseconds but in actual practice is probably more like 300ms in the real world. So is 1ms consistently possible under all conditions on common hardware? Probably not, unless you've got hardware capable of reliably sending, receiving and processing OA&M traffic at double-digit nanosecond precision (and there is a whole rat's nest of issues keeping such mechanisms stable). The real question tends to be figuring out the convergence speed that makes the most sense given the protocols running over the link. For standard Ethernet and IP getting in the < 250-300 ms range (from actual failure to full recovery) for any circumstances (with low double digits under common circumstances) has proven more than sufficient.