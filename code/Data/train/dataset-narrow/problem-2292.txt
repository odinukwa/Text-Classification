(from comments, per request) Here is a sketch of a polynomial-time solution. First, interpret matrix $A$ as a bipartite graph $G=(V \cup U,E)$, with $m$ nodes in part $V$ and $n$ nodes in part $U$, $n \le m$. Then column-paths are $U$-saturating matchings, i.e., matchings of size $n$ (each node of $U$ is incident to precisely one edge in the matching). The union of $k$ such matchings is a simple $k$-matching of size $kn$ (each node of $U$ is "covered" precisely $k$ times). Conversely, if there is a simple $k$-matching of size $kn$, by Hall's theorem there are also $k$ disjoint $U$-saturating matchings. Hence to solve the problem it is enough to find a maximum-size simple $k$-matching. This is possible in polynomial time. 

As was already mentioned, if you are interested in theoretical running time guarantees, this question is a duplicate. But I'd like to point out that if you really want to solve a concrete problem (like the colouring problem that you mentioned), I think that it makes absolutely no sense at all to study theoretical upper bounds. Even though you wanted to avoid "engineering" aspects, I'd suggest that you just take some popular SAT solvers, try them out, and see what happens (most of them can read the same DIMACS file format, so it is easy to try different solvers). You may have both positive and negative surprises. Recently I had a family of SAT instances; a bunch of instances with tens of thousands of variables and more than one million clauses turned out to be easy to solve, while seemingly much simpler instances with just hundreds of variables and thousands of clauses were far too difficult for any solver that I tried. 

Initialise $x \gets v$ and $y \gets -v$. Then increment both $x$ and $y$ in parallel until one of them is equal to 0. 

(Not an answer, but a bit too long comment.) One aspect that has not been covered in the other answers is the fact that "higher-level" languages typically provide additional safety nets, which may involve some run-time overhead. As a concrete example, I have occasionally compared the performance of C code and equivalent Java code (truly equivalent – forget about classes and objects, your high-level data structure is ), and I have also had a look at the machine code that is generated by modern compilers in each case. While the quality of compilers varies a bit, and just-in-time things have their usual advantages and disadvantages, it seemed that a major difference was related to the safety nets. In Java, array references such as involve two sanity checks: pointer is not null, and array index is valid. Of course some of these checks can be optimised away, and some of them can be moved outside the inner loops. Modern CPUs also help tremendously, as these are branches that can be predicted well. Nevertheless, a number of these checks are there and they take a non-trivial amount of time. I think now we are at a point where we can approach the question from a TCS perspective, especially from the perspective of the theory of programming languages: Would it be possible to, e.g., re-design Java so that none of these run-time checks are necessary, without losing any of the safety nets, and without putting any significant additional burden on the programmer? 

Does this problem have a name? (It sounds like a very standard question in the context of sequence alignment.) Is it hard? If it is hard, is it fixed-parameter tractable with $K$ as the parameter? Are there efficient approximation algorithms? (E.g., find a solution with at most $2k$ cheap and $2K$ expensive operations if a solution with $k$ cheap and $K$ expensive operations exist.) 

Doesn't the usual (pseudo-polynomial time) dynamic programming algorithm solve this, with a minor tweak? The usual approach is to have a table with flags "there is a subset of $x_1,x_2,...,x_i$ with sum $s$" for all $i = 1, 2, ..., n$ and $s = 0, 1, ..., K$. The minor modification is to have a table with flags "there is a subset of $x_1,x_2,...,x_i$ with $c$ elements and sum $s$" for all $i = 1, 2, ..., n$, $c = 1, 2, ..., a$, and $s = 0, 1, ..., K$. 

Please note the keywords "deterministic" and "worst-case"; the analysis of simple randomised algorithms fairly easily leads to complicated expressions. Of course what is "complicated" is a matter of taste. Anyway, I would prefer to see an expression that is far too ugly to put in the title of your paper. And I would prefer a complicated function of one natural parameter (input size, number of nodes, etc.). 

Hence in any solution, for every maximal class $[i]$, we have to choose at least one node $j \in [i]$ such that we do not get the benefit of $c(j)$. We now construct a solution as follows: For each maximal class $[i]$, choose a node $j \in [i]$ that minimises $c(j)$; we say that $j$ is a sink node. Now assume that $k \in N$ is not a sink node. There are too cases: 

These questions were originally motivated by edge dominating sets and minimum maximal matchings. It is well known (and fairly easy to see) that a minimum maximal matching is a minimum edge dominating set; conversely, given a minimum edge dominating set, it is easy to construct a minimum maximal matching. So they are, in essence, the same problem. Both problems are NP-hard and APX-hard. There is a trivial 2-approximation algorithm: any maximal matching. However, their "natural" LP relaxations look very different. If you take the edge dominating set problem and form a natural LP relaxation, you get a covering LP. However, if you take the problem of finding a minimum maximal matching and try to come up with an LP relaxation, then what do you get? Well, of course fractional matchings are feasible solutions of a packing LP; then maximal fractional matchings are maximal solutions of such LPs, and minimum maximal fractional matchings are therefore minimum maximal solutions of such LPs. :) 

Now all this is fine as long as you study problems that are "truly distributed" in the sense that the running time of your algorithm is smaller than the diameter of the graph, i.e., no node needs to have full information on the structure of the graph. However, there are also many problems that are inherently global: the fastest algorithm in this model has running time that is linear in the diameter of the graph. In the study of those problems, the above model no longer makes any sense, and then we need to resort to something else. Typically, one starts to pay attention to the total number of messages or bits communicated in the network. That's one reason why we get several different models. 

Minimum cut (specifically, vertex cut). (In this case $f$ would be something like this: 0 if removing the nodes in the set $S$ does not partition $G$ in at least two components, and $|V| - |S|$ otherwise. Then maximising $f$ is equivalent to finding a minimum cut, which can be done in polynomial time.) You can also define a similar function that corresponds to minimum edge cuts. (For example, $f(S)$ is 0 if $S = \emptyset$ or $S = V$; otherwise it is $|E| - |X|$, where $X$ is the set of edges that have one endpoint in $S$ and the other endpoint in $V \setminus S$.) 

Kalai's 2-page SODA paper gives a simple and efficient algorithm for pattern matching with don't cares (wildcards that match one character). In essence, it is as easy as convolution. But what happens if we are searching for multiple patterns with don't cares? Can we still somehow solve it with e.g. FFT-based techniques? 

All integers of type Y are also of type X. However, there are also 768 integers of type Z that are neither of type X nor of type Y. These begin with '1000011111...', '0111100000...', or '1111100000...' 

The linked lists are just $k$-element arrays of indexes, so they are lightweight (except that the locality of memory access is poor). 

That is, we can get the benefit of $c(k)$ for all nodes $k \in N$ that are not sink nodes; the solution is optimal. 

PS. I thought I would not make this a "big-list question", and not CW. I'd like to find a single excellent example (if it exists at all). Hence please post another answer only if you think that it is "better" than any of the answers so far. 

I don't know anything about the topic, so let me try to give a very simple example and let's see what others say. Program: 

No. Fix a $k$, and let $x_1, x_2, ..., x_k$ be the variables in your Horn formula. Then for all $i$ the clause $(x_i)$ is a Horn clause – there is at most one positive literal. Using such clauses, you can construct $2^k - 1$ different Horn formulas: $(x_1)$; $(x_2)$; $(x_1) \wedge (x_2)$; $(x_3)$; etc. Each of these is trivially satisfiable – just set $x_i = 1$ for all $i$. The length of each formula is $O(k \log k)$, assuming any reasonable encoding. (You can add extra requirements like "all clauses must contain at least one negative literal", "all variables must occur in at least one clause", etc., but this doesn't really change anything; just tweak the above construction slightly.) 

Let us consider your "base problem" and the special case of $p = 1/2$. Assume that we are given a graph $G = (V,E)$ and a positive integer $k \le |V|$. Construct an instance of your problem as follows: 

Usually, you should be able to figure out who are the leading researchers or research groups in your research area. Everything else is then usually fairly straightforward: just find out in which conferences the leading researchers publish their work, and in which conferences they serve in programme committees, etc. Most likely those would be the most relevant conferences for your work as well. The same applies to journals as well. (Note that the most relevant conference is not necessarily the same as the most prestigious conference, but it might be a good idea to start with relevant conferences... Your work might have much more impact that way, even if it does not look that impressive in your CV.)