$P=NP$ is not enough, we need the algorithm to be feasible in practice, an algorithm in $DTime(10000n^{10000})$ would not help. it will find a proof only if there is one (i.e. the sentence is not an undecidable sentence in ZFC), moreover the proof should be feasibly short. it can be the case that $P\neq NP$, but we can still find these feasible proofs algorithmically, for example if $NP=DTime(n^{\log^* n})$. 

We know that DPLL based SAT-solvers fail to answer correctly on unsatisfiable instances of $\mathrm{PHP}$ (pigeon hole principle), e.g. on "there is a injective mapping from $n+1$ to $n$": $$\mathrm{PHP^{n+1}_{n}} := \left(\bigwedge_{i\in[n+1]} \ \bigvee_{j\in[n]} \ p_{i,j}\right) \wedge \left(\bigwedge_{i\neq i'\in[n+1]} \ \bigwedge_{j\in[n]} \ (\lnot p_{i,j} \vee \lnot p_{i',j})\right)$$ I am looking for results about how they perform on satisfiable instances of $\mathrm{PHP}$, e.g. on "there is a injective mapping from $n$ to $n$". Do they find a satisfying assignment quickly on such instances? 

To go over constant times, we need to do the quantifier removal effectively. We can do this because the Cook-Levin theorem is a constructive theorem, in fact it gives a polynomial time algorithm $Cook$ s.t. 

Counting the number of 1s in a given binary string is $\mathsf{TC^0}$-complete and cannot be decided using any polynomial-size CNF. In fact you need exponentially large CNFs to decide them. So if you want $y$ to be part of the input then the size is going to be exponential in $n$. If $y$ is fixed, then there are CNF's with $O({n \choose y})$ gates: check if any $y$bits of $x$ are 1: $$\exists I \in {n \choose y} \ \forall j \in I. \ x_{j} \land \forall j\notin I. \ \lnot x_j $$ For $y = \frac{n}{2}$ this gives an exponential-size CNF. I haven't checked it but I think the proof for parity not in $\mathsf{AC^0}$ can be adopted to prove an exponential size lower-bound for this function. 

A finite prefix of $G$ is $G \cap \{0,1,\ldots n\}$ for some $n$. It is easier if you look at the characteristic function of $G$ which can we viewed as an infinite word in $2^\omega$. Then a finite prefix is a finite initial part of $G$. For forcing, there are many resources, depends on what you want to learn. A point to start is the Wikipedia pages and the references listed there: Forcing (set theory), Forcing (recursion theory). Timothy Chow's article A Beginner's Guide to Forcing is a good introduction. 

Concrete Mathematics: A Foundation for Computer Science, by Ronald Graham, Donald Knuth, and Oren Patashnik. An amazing book with lots of funny side notes. :) (See also DEK's GKP page.) 

I (also) agree with Anrej. I think descriptive complexity is a computation-less characterization (which makes it interesting in its own way) and therefore the computational ambiguity examples from formal languages theory (automata/grammars/...) that you gave look to be in a quite different domain. In descriptive complexity languages correspond to complexity classes and queries (in a language) correspond to computational problems (not algorithms). There is no intended way of checking/computing a query AFAIK, so if you are not looking for computational ambiguity IMHO those examples are misleading. 

It is a common opinion among experts that Turing's work on foundation of mathematics is part of the foundation of computer science and computers. There are many ideas essential to working of modern computers that came from Turing's work. Part of the motivation for Turing's research were the question about the foundations of mathematics like formalizing and clarifying the meaning of mechanical/algorithmic computation going back to at lease Hilbert's problems. However this doesn't mean that Turing's work was aimed at creating a computing device. AFAIK, the code-breaking work of Turing are part of a much later period of his life. In any case, if you want to learn more about Turing there are considerable amount of resources that you can refer to. In particular check Alan Turing's page by Andrew Hodges. Turing work is very important but there are also many other ideas that were essential to the creation of computers. If you want to learn more about history of computing check the Wikipedia articles on history of computing, history of computing hardware and timeline of computing. 

A bound on fan-out of input bits and gates will make the size of the circuit linear. Let $k$ be a bound on the fan-out of the gates and inputs. It is a DAG with max out degree bounded by $k$ and max path of length $d$. The number of available wires in each level can increase $k$ times, and the number of available wires at top is $kn$, so the total number of wires in the circuit is at most $kn \sum_{i=0}^d k^i \leq k^{d+1} n$ which is $O(n)$. Any $\mathsf{AC^0}$ function which requires super-linear size will separate the class of functions with bounded fan-out (applied also to input bits) from $\mathsf{AC^0}$. Here are some examples: 

There has been some follow up work which you can find by looking at articles citing this paper. Implicit complexity theory that Martin mentions is also influenced by this work. Check out Simone Martini's survey slides and Ugo Dal Lago's survey article to get a picture of what is implicit complexity theory and its history. 

There are hierarchies in propositional proof complexity similar to those in circuit complexity. E.g. $G_i$ propositional roof systems are similar to $\mathsf{PH}$, C-Frege proof systems for $C \subset \mathsf{P}$ are similar to circuit complexity classes $C$, and so on. There are also hierarchies in bounded arithmetics, e.g. $\mathsf{S^i_j}$ theories, etc. 

As Joe said the answer highly depends on what you mean by information. I guess you don't mean the definition in computer science but are using it as the best word for expressing some intuitive concept. But if your question is about how a simple rule can generate such a complex structure then the answer is because it is defined recursively, i.e. Mandelbrot Set is a fix-point as mentioned by Turkistany. Recursive definitions can increase the complexity of a set (membership of a point in the set, distance from the set, ...) considerably. The original equation over complex numbers is not very complicated, but recursive use of it (or almost any other equation) can create high complexity sets. So I would say that the complexity of the set is coming from the recursiveness/fixed-point. I suggest having a look at the dynamical systems, fractals, and (mathematical) chaos theory books. I would also suggest Mark Braverman's papers on computability/non-computability of Julia sets and his book with Michael Yampolsky: Computability of Julia Sets, Springer 2008. 

If by "in terms of circuits" you mean nonuniform circuits then the answer is negative. If the description of circuits is not uniform, it will allow non-computable functions to be used to define circuits which in turn will be able to compute non-computable functions. We can always build a circuit of size $1$ computing $f(|x|)$ where $f$ is a function computable by whatever means we are using to describe the circuits. On the other hand, if we are allowed to restrict to uniform circuits to define the circuits, then the answer is obviously positive. And we can use $FO$ (which is equal to $DLogTime$ and uniform $AC^0$) to define uniformity. $FO$ is conceptually very close to circuits. It seems to me that the main point here is that we need some model of uniform computation to define uniformity for circuits, if the description of circuits is given by means which are not uniform, the circuits can be nonuniform. 

Knapsack problem. It can be solved in polytime by dynamic programming if you give the target weight in unary. Similarly other problems which are weekly polynomial time (pseudo-polynomial time) but not strongly polynomial time would need some part of input to be in unary. (You may also want to search for weakly NP-complete.) 

He doesn't cite any references for it and Google doesn't return any results so I don't think he is really quoting from anywhere. The idea that a proof is a "construction" (a term in intuitionistic/constructive mathematics with a very close meaning to what we call algorithm nowadays) goes back to at least Luitzen Egbertus Jan Brouwer. Note that Brouwer was talking about informal proofs as typically stated by mathematicians, not formalized proofs which are a very recent phenomenon. The idea that a formal proof is a like a program in a programming language can be traced back to at least to the BHK interpretation of intuitionistic logic by Arend Heyting. The formal correspondence is Curry-Howard isomorphism. 

SAT-solvers are another common class of heuristics. There are many and of course they take exponential time in the worst case. My suggestion is to explain to the reviewers that the problem is NP-complete and cannot be solved or approximated in less than exponential time if there are such results. That should suffice if your algorithm outperforms the best known algorithm on standard benchmarks for the problem. Also you might want to submit your paper to a venue which had published heuristics for the problem before. 

For more see the section in Reckhow's thesis discussing polynomial simulation between proof systems with different class of formulas. 

Note: This is not really answering the question, this is just some comments posted as an answer. :) Note that in VO, one is defining sets over the set of natural numbers (similar to sets defined in recursion theory) where as in descriptive complexity setting (SO, $\exists$-SO, SO-Horn) we are talking about finite structures. An SO formula in the former setting will give not $PH$ but the whole analytical hierarchy as Arthur Milchior has written in his answer. IMHO, a better comparison would be with bounded arithmetic theories. I don't think you can get below c.e. sets unless you bound all quantifiers to finite domains, to get $P$ or $NP$ the size of domains should be very small. 

Take any non-trivial algorithm with bounded runtime, e.g. AKS primality testing algorithm (I don't think anyone would refer to AKS as "trivial"). It is not Turing-complete, in fact, no algorithm with computably bounded runtime can be Turing-complete. (This means that no algorithm which always terminates can be Turing-complete since the run-time of any such algorithm would be computable.) 

Let $F$ be the set of functions over real numbers. Consider the structure $M = \langle F, <, \leq, =, \geq, > \rangle$ where the $<, \leq, =, \geq, >$ are defined as asymptotic notions $o$, $O$, $\Theta$, $\Omega$, $\omega$. For example, for $f,g\in F$, $f\leq g$ iff $f \in O(g)$. Consider the first-order theory of this structure $T = Th(M)$. Questions: 

Many countries organize summer schools for their IOI teams (consisting of high school students aged roughly 16 IIRC). The one we have in Iran used to have the following courses: 

In my experience, it is not difficult to teach basic topics in combinatorics, graph theory, programming, algorithms and similar topics. You may want to look up topics covered in IOI competitions and national competitions. There are summer schools and workshops related to IOI competitions starting at quite early age. My personal favorite topic for such workshops is combinatorial game theory since it is easy to motive by playing games with the audience. Also check ACM's K-12 CS Curriculum Resources, particularly page 11 and 12 of A Model Curriculum for K–12. 

Martin Fürer, "The Tight Deterministic Time Hierarchy", 1982 Piergiorgio Odifreddi, "Classical Recursion Theory", vol. II, 1999 (page 84) Juris Hartmanis, "Computational Complexity of One-Tape Turing Machine Computations", 1968 F.C. Hennie and R.E. Stearns, "Two-Tape Simulation of Multitape Turing Machines", 1966 Lance Fortnow and Rahul Santhanam's paper "Time Hierarchies: A Survey", 2007 

Consider circuit value problem and Boolean formula evaluation for various small complexity classes. Deterministic sequential time complexity of them are the similar as far as we know, yet they are very different from circuit complexity perspective. Similarity in one particular type of resource on one model doesn't imply similarity for other resources in other models. One problem can be such that we can exploit parallel computation for one while we can't do that for another one, yet their sequential time complexity can be the same. When can we expect a more robust relation between the complexity of two problems across models and different resources? When they're are robust reduction between them in both directions that respects resources in those models. Edit: multiplication has subexponential size depth 3 circuits. Proving a lower-bound of that kind for determinant would show it is not in $\mathsf{NL}$ separating it from $\mathsf{NC^2}$ which is not known. 

1) The only non-structural rule is resolution (on atoms). $$ \varphi\lor C, \psi\lor \overline{C} \over \varphi\lor \psi$$ However a rule by itself doesn't give a proof system. See part 3. 2) Think about it this way: is Gentzen's sequent calculus PK complete if we are using some other set of connectives in place of $\{\land, \lor, \lnot\}$? The logical connectives one uses is important for logic results like completeness. It is with respect to the formulas in that language that a proof system can be complete. PK cannot talk about formulas in some other language. Your issue with resolution is similar. Yes, if we are talking about completeness regarding general formulas with $\{\land, \lor, \lnot\}$ connectives then resolution is not complete, but neither are sequent calculus and natural deduction with respect to formulas that are not in their language. As long as there is a "nice" translation from one language to another one we can talk about completeness. What matters essentially is that we can translate formulas from one to the other one and vice versa efficiently. You can check Robert Reckhow's thesis where he deals with the issue of connective and shows that for Frege systems the length of proofs doesn't change more than a polynomial so it is fine in a sense to pick any set of adequate connectives that you like. The situation for resolution is similar. By reduction from SAT to 3SAT we can restrict our attention to CNFs and the transformation can be done very efficiently. Note that resolution is not alone here, the issue applies to other proof systems also. Take for example Bounded-Depth Frege where the depth of formulas must to be bounded by a constant so by definition it cannot prove any unbounded-depth families of formulas. 3) Let us define what it means for a proposition proof system to be complete. By Cook-Reckhow, a propositional proof system $P$ is a binary relation $\vdash_P$ satisfying the following conditions: 

I am looking for a good relatively complete and up-to-date book or survey about heuristic algorithm design techniques. 

References: [CR96] S. Chaudhuri and J. Radhakrishnan, "Deterministic Restrictions in Circuit Complexity", 1996 [Ros08] Benjamin Rossman, "On the Constant-Depth Complexity of k-Clique", 2008 [Juk] Stasys Jukna, "Boolean Function Complexity: Advances and Frontiers", draft 

this does not apply to complexity classes which does not satisfy the requirements stated in Kozen's paper (i.e. are not Kozen "complexiy classes"). it is a very general kind of diagonalization. Kozen shows in the same paper that there are not "diagonalizations" which would satisfy some expected conditions for separating the classes like $P$ and $PSpace$. There are results by Lance Fornow and others (e.g. time-space trade off results) (including some of Ryan William's work) where diagonalization is used in an indirect way. This can be turned into direct "diagonalization" but it will not satisfy the nice properties that one might expect (like independence of counterexample for a set in the smaller class from the codes of the machines for that class, and it seems that is the reason they don't relativize). the important thing is that the more general a method is, the more limited its applications are (if it is used by itself) because the method needs to work for more cases and this is a restriction on the method, we can not use the specific information we have about the problem if it is not shared or cannot be replaced by something similar for other problems that we want to apply the method to them. We can turn the separation arguments into "diagonalization" arguments (considering the restriction I mentioned above), but the fact that "the diagonalizing function really separates the classes" itself needs a proof. Kozen's paper show that there exists a diagonalizing function if the classes are different, but how can we know that a given function is really diagonalizing? We need a proof! And the paper (AFAIU) doesn't give us any idea on how to come up with those proofs. If we have a separation argument we can turn it into a diagonalization proof, but that is only after having a proof. The original proof will serve as a part of the new diagonalization proof, it will show that the function is really diagonalizing. (And in a sense, the diagonalization proof constructed from Kozen's paper will not be "canonical" since it will be completely dependent on the original argument.)