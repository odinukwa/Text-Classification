Looks to me like you'll need to make a script (shell or Perl or whatever, but personally I'd use Perl) to parse the output of an ls -laF command and then invoke a touch command. In pseudo-code, that would look loosey like this: 

I'm having good luck with a Barracuda Backup appliance. Its not cheap (a few thousand at the beginning and a monthly cost for off-site storage), but it gives you a simple to use tool with lots of technical support. It will also send the data off-site. The least troublesome product that I've used so far. By comparison, I've directly been involved with Retrospect (versions 4 through 6), Time Navigator, NetVault, and Barracuda Backup Service. I've been indirectly involved with the built-in backup in Windows, Veritas, and Backup Exec. On Unix and Mac, I've used rsync (command line) and a few Mac-only products. Not sure if you have the budget or space for it, but thought I'd suggest it. 

I'm trying to get FreeBSD installed on a Dell R210 server with the H200 hard drive controller. I can boot with the 8.2-RELEASE ISO and the 8-STABLE-20110522 ISO. My problem is getting it to boot from the internal HDs. It seems that I need a driver in the 8-STABLE branch (not in 8.2-RELEASE) in order to use the disks as da0 and da1. It doesn't work with them in a RAID-1 configuration at all, unfortunately. So I booted from the 8-STABLE-20110522 disk and can install with it. However, I think that I can only install official releases, not from the stable branch. So... 1) Am I wrong? Is there a way to install 8-STABLE while booted from that DVD? 2) Is there a way to cvsup/makeworld the internal drive while booted from a DVD? Thanks in advance! 

Keep in mind that RAID-1 is a way to make 2 drives act like one. So when you write to "storage," you will be writing to both drives. When you read a file, it will read from whichever disk is available first. Theoretically, this makes a penalty on writes and a boost on reads. In reality, you might not notice a speed difference. The real reason that this is important is that it means that there is no such thing as a drive "taking over" when another drive fails. They're both in constant use. Case in point: I had a Mac Pro with two 500GB drives in a RAID-1 array at work. One day I randomly checked that server and discovered that one of the drives was actually dead for the last few months. No one noticed a difference. This computer was the file server for an entire elementary school. I'll answer question #3 by telling you what happened after finding the dead HD in the Mac Pro. I replaced the drive with only a few minutes of down-time. I replaced it AND added a third drive as a hot spare. So I now have 3 HDs acting like one storage device, i.e. a single "drive," of 500GB. Again, no one can tell the difference. As a side note: Please note that RAID is NOT a backup solution. I have had situations [plural] where I've lost data in a RAID array. My backups were well worth their expense those days. 

After migrating to a new VPS I had some users complaining about slow loading images on their sites. After creating some test files with dd I realized that I can download all files via sshfs with full speed while downloads via web are painfully slow. The larger the file is and the longer the transfer takes, the slower the transfer speed gets. I thought I had some problems with Apache and just spend the whole evening with replacing Apache2 against nginx for static file serving - with no effect at all. No I/O wait states in top. Tons of RAM free, no high CPU utilization and hdparm shows a decent I/O performance at all times. I just have no idea anymore, what's happening on this server. This is a link to a demo file: $URL$ Anybody an idea what I can check out? 

I'm trying to figure out how TTLs are handled in DNS (in the end I'm trying to figure out the fastest way to switch over a domain), but somehow I'm stuck in interpreting TTL values. For example I moved a domain today which had the following DNS records before moving it to me: host -a example.com ns.old-provider.net 

I'm afraid I must answer my own question and the answer seems to be No. Using the command it turns out the original cert delivered by Comodo already lacks the needed flags in the field. It doesn't have the feature. The Comodo cert looks like this: 

Coming from a Linux background and now having to manage some Windows servers (2008R2 & 2012R2) I'm wondering which services are safe to run directly over the internet. The servers are all internet facing without additional hardware firewall or a internal VPN management network. The services in doubt are: 

The certificate chain of your certificate is incomplete. Most likely your provider failed to install some intermediate certificate when installing the new certificate. Most times such intermediate certificates are provided by the SSL authority, to provide support for some older browsers and operating systems. That's the reason, that while it works for you, it doesn't work for some of your clients. An really great utility to check for SSL issues with your website is the SSL Server test by SSLlabs. As you can see in the link above, not only are you having a chain issue here, but also the signature algorithm used to create your cert is a weak one, your webserver is still vulnarable to the POODLE attack and still supports RC4, which is also considered unsecure ... I don't want to say anything against your webserver provider, but in your position I would mail them, that they fix all this issues ASAP, or change to another provider ... 

I recently created a ZFS volume to test its compression capabilities. I'm comparing it side by side to an ext4 volume. After creating the new volume and turning compression on with I copied a ~3GB file from the ext4 volume to the ZFS file but the file is the same exact size on the ZFS drive (I used to see this). I gzipped the file manually to see what the compression should be (I understand there are different levels but just to get a ballpark) and just using the file size was cut in half. My ZFS settings also show compression is turned on: 

I'm on my server and I can't ping anything outside. I tried for example google.com. How can I diagnose this issue? I can ping my localhost (ping works) This is my traceroute to google.com: 

I was recently given a bunch of keys and a bunch of servers and had to do some detective work to figure out which key matched which server. After a few tries (maybe 3?) SSH locked me out. My guess is due to the setting. We have tracked down the correct key but now I can't use it because of the error message: 

I thought restarting the server would work but no luck. Even trying to SSH in with root gives me the same error. It seems a bit weird that I would get completely locked out of a server due to this and it would never reset. Is there something I'm missing about how to reset this? It's actually hard to google for information about this because everyone experiencing this problem seems to have a bunch of keys in that a dumb client in cycling through but I am setting my key very specifically: 

I'm running an EC2 instance. According to EC2 specs, it should come with . When I SSH into my new machine, I don't see any of these drives. All I see are my EBS volumes: 

I don't think this config actually matters but posting anyway. I'm using the stock supervisor global config. I installed supervisor via standard ...and just to be sure: 

There is nothing in the logs (My global config has ) and there is nothing in the "program logs" either (). I have a similar provisioning process for other servers that run ruby daemons so to dig in a bit further, I even creating a very simple ruby script and made the thing so there are no permission issues: 

in supervisor init script. It looks like for some reason supervisord was installed in on this machine. Super annoying that nothing is printed... 

This seems like it should work because the group deploy can write and mima is in that group, but I do get a permissions error. 

The main difference between your two choices is 3 year warranty vs 5 years and SOHO usage area vs. certification for datacenters. In terms of raw I/O performance, there is not much of a difference between 5400/5900 and 7200 rpm S-ATA disks nowadays (that was different a few years ago). In the end therefore the array with more disks will very likely show a superior performance, as RAID10 scales with the number of disks. What I would guess is more important in this setup, then the number of drives is, get a proper hardware RAID controller with BBU+Cache if you can afford it - or even better go with SSD only from the start on. Also please be aware, that ESXi natively only supports hardware RAID! While there are workarounds to use software RAID with ESXi, I personally wouldn't advice using these, if you are looking for a decent performance. Protip: If you are on a really tight budget to self-build this storage server and we are talking about a non mission-critical system, then go fetch yourself an OEM version of a proper RAID controller (IBM M5015, DELL PERC H710, etc.) on ebay. 

I guess you are already aware of it, but the setup you are looking at, is badly broken to begin with. How can a server with 5 physical disks be running two RAID5 arrays? You need atleast 6 disks for that. You are missing one disk to begin with. 

The task I want to accomplish now: I want to build a offsite server that is a perfect mirror of this backup server and which is syncing daily with the above described system. The Problem: I haven't found any solution yet, which lets me easily sync the whole server / zfs filesystem including all snapshots. I only know that you can send a single snapshot to a remote zfs filesystem via zfs send. Obviously it would be great to not have to recreate the complex pool & snapshot structure on the offsite server because I feel this would be a total mess maintenance wise. Is there any solution which allows to mirror a complete zfs filesystem including snapshots? 

As you can see I'm using the Google DNS server to check, so I guess it can be save that the TTL is respected. I must be missing something. But I can't figure it out - can anyone give me a hint here, please? Is the problem the TTL of the SOA record, so this must be lowered before making the switch? 

Basically this error message only tells you that the battery of your RAID controller is bad/gone/not present and that your virtual disk caching mode was therefor set from write-back to write-through. The reasoning behind this is that your controllers cache is not backed by the BBU (anymore) so it's unsafe to use it for write-caching in the event of a power-loss. In the moment the only problem which should occur is slightly degraded write performance. Normally the PERC5 also beeps on boot and I'm pretty sure to remember they also have a special error message for battery low-power, which makes me think that in your case: