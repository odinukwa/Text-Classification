(There would be a way to use arrays, only - "FORTRAN", don't currently feel like coding that.) Using above and an open coded stack: 

The implied any aspect of the code posted is fair game for feedback and criticism justifies asking on CR even without an(y) explicit question. I'm taken against guessing what any piece of code is there for: Why another python quicksort? doesn't get answered in the code. While the docstring looks charming, the parameter to is over-specified: everything needed is "subscripting" to get&set items and comparable items. (Numeric items could be deemed handy for picking as a pivot the mean value of several.) The code mentions neither nor : should be specified to return its (ordered) parameter. , like beauty, lies in the eye of the beholder - to mine, both are close regarding code. I like python for its ability to express things with minimum ado. (I don't quite like & - sometimes use &.) In demonstration code, it seems to have grown conventional to factor out partitioning the current sequence from . True? The code doesn't use as a stack. As far as means o(n) space (in addition to output is where input used to be), the implementation presented is not in-place. More efficient? Sure - 

using that, I get 1, 0, 2, 0, 1, 2, 1, 0, 2, 2, 1, 1, 1, 1, 4, 0, 1, 2, 1, 1, 4… ways to "consecutively sum to" n=1..21 (throwing this at OEIS yields - nothing. Going from to 1: oeis.org/search?q=2,1,3,1,2,3,2,1,3,3,2,2,2,2,5,1,2,3,2,2,5 - in the paragraph, you may find the generating function - and a description:) number of odd divisors (including n), plus one if n is a triangular number (sum from 1..k for some k). 

Short of implementing the Strassen algorithm, you can try to approach sweet oblivion in (even a 2×2) matrix multiplication by reversing the iteration order of columns/rows (so that elements used last in the previous iteration get used first in the current one) and picking an evaluation order where each dot product reuses one column or row from the previous dot product. The ease of getting all those indexes wrong is another reason to factor out : 

(Not touching direct use of the Dirichlet series or stopping summation on the first difference between successive partial sums that is "too small in relation".) The difference any given partial sum and the preceding one is the term just added. Given that the range is from about 1.6 to 1.0, I suggest to just define a constant lower bound for terms to consider: (syntax?) 

(Given a choice, I wouldn't read a single shortish input using file (or stream or channel) IO - just use an argument to main.) is neither documented nor named suggestively. It doesn't even live up to it's name, not returning a value, but using a class data member for result communication. This prevents multiple&concurrent use - instead, return a value. RE60K (and jschnei) showed "analysis from the sum formula". Alternatively, start from the sum target: to be a sum of two consecutive natural numbers, it needs to be odd. Tabulating this for several sum lengths: 

The idea to split the input list is uncalled for - when merging begins with runs of length 1, begin by taking items from the input one by one. Posting this mostly to illustrate my comments about how to split (defaulting on the counting variant - would have a different interface and look tedious as well as boring (and pointless: see introductory remark): keep account of list lengths in mergeSort(Impl). Pass a count into split(), return node after that count and terminate head at the node.) If there was value in putting items in one of two lists alternately, have a look at . Without giving it due diligence: 

(If this worked (don't have a C++-environment I dare to try boost in), this would "trace" partial sums to the precision explicitly set, presumably.) 

(Using , MultiMap doesn't hurt as much as it used to, even less with .) Using probably doesn't reduce max. memory usage, trimming the elements of might help a bit. You could use and to improve upon the static and . Then, you can roll your own (drop 's size, have the element in a or an Object refer to the array, …) - if s don't kill you, returning a view might. Or use 3rd party collections - Goldman Sachs s (&/ look good, s not bad. 

For the hell of micro-benchmarking without the likes of Java Microbenchmarking Harness or jmicrobench(no idea whether this is official) (or that most visible one for those who don't have issues with empires), I tinkered around picking up ideas from rolfl and chillworld 

Comment your code. Specify whether negative numbers are to be considered, and how those are represented. Describe the algorithm you use. To elaborate on my comment to Timothy Truckle's answer, an adoption of source code found in Warren, Henry S.: Hacker's Delight, Addison-Wesley, 9780321842688 (with ancestry in HAKMEM) - invented time and again: 

(This is an extended (and formatted…) comment (don't like the repetition of the non-pair comparisons at the beginning and end of the sequence). Not having used C++ in a dozen years, I'm hopeless with move semantics, for one. (And not quite destined to review C++ postings…)) Let me try something about the non-pair comparisons: 

Follow up to Selection sort with reduced comparison count - semi-final Iteration? My goal (and excuse not to tag reinventing…) is to have presentable code to argue the viability of reducing the number of comparisons used by selection sort (for readers not necessarily versed in the implementation language chosen). In this iteration, that is prominently and from the methods , , and , including intrusive comparison accounting for emphasis (and the possible addition of for computing the number of operations of interest). Note to reviewers of previous iterations: finding suggestions not followed, be assured they were not ignored - feel free to chat (or flame) me, regardless. 

(in , I'd stop short of : the comment doesn't apply to the NULL-checks.) : I'd assign the malloc()-result to a local variable returning false if NULL, freeing and assigning to otherwise. The else-branch could be merged with the last part of the then - I'd prefer an early-out, instead. (Naming: I'm spoiled by java.util.*.ensureCapacity) An alternate name and rendition of : 

Triggered by Finding overlaps between two lists of axis-aligned rectangles, I tried to code "rectilinear" intersection using line sweep - in python. I'm not keen on discussing (2D) line sweep in general here (which is why this isn't tagged algorithm). The code looks small enough to stay a single module. I didn't take the time to digest PEP 8 & Co. 

What exactly shall Generating the Cartesian product of N-arrays mean? If it is providing a list of comma-separated representations of all combinations of choices given as a two-dimensional assortment of values, it is not building all such representations and adding them to a . 

(For decent handling of most significant bit set (see , too), use - just slightly harder to read and explain.) 

You can do as a decent compiler would: precompute some expressions, and reduce the "strength" of others. For worse rather than better, most opportunities for this are in the constructor. Part of the performance of on object depends on its interface - I've included an alternative function that finds the (position of the) next match, if any. Benchmarking is difficult, micro or not: beware "printing" and use help. 

With determination, the bit-hack variants of population count and parity can be coded without undue regard to type sizes: 

I do not consider transforming data to facilitate processing , unless it explicitly violates part of the task description. Things I don't like (this far, you saw it coming) about the execution (of said approach): 

For readability, you can and should separate the concerns of picking a pivot index, partition, and sort. Of the mentioned in the wikipedia article, two reduce the likelihood and severity of worst case behaviour: 

Zhu, Wu, & Wang claim in a 2014 JofCIS article that their "Efficient Algorithm for Chain Multiplication of Dense Matrices" is both simpler and faster than Hu/Shins 1981 O(nlogn) algorithm (is that real Pascal code?) - any takers for a free implementation? 

sums it up nicely - you might have been explicit about the dimensions of improvement, though. The first thing to consider with non-throw-away code is readability/maintainability. Enough has been said about premature optimisation - I find instances of premature analysis with your code. The very concept of (y-coordinate)collisions is flaky - just rotate so slightly that formerly identical y-coordinate values get separated while none separate before collide. 

I guess I'd add an alias for "the other side" everywhere a method name is decorated left or right, the way the library grew a . For , you could swap accessors (including, depending on implementation details, iterators) instead of references - better yet, create a class with swapped accessors and mutate the object on . What about ? 

Finger exercises (WiP), with much less than due commenting, QA (do as I say, don't do as I do…), many more casts than pretty (extensive type hierarchies without extensive method overriding), …: 

In each iteration of 's outer loop, stays the same for all the iterations of the inner loop, as does its contribution to the set. If you didn't literally concatenate strings, it was apparent that the same sets were created for the 2nd ticket's digits time and again - and the sets "checked for completeness" were the unions of the digit/char sets of tickets i and j: It looks advantageous to create each ticket's set once and for all and think hard about what can be done to reduce the number of set unions to evaluate - if ticket i consists of 2 distinct digits, only, there's no need to pair with any ticket consisting of no more than 7 distinct digits. If one ticket contains every digit, it is a winner paired with every other ticket … If and when coding that proves not fast enough, note that the digit sets are quite small and reconsider their representation. 

I don't intend to (directly) address/solve your counting problem, but comment on the code presented: 

While documentation of the interface has priority, I'm not happy with a text book reference as the only comment. In both implementations, I don't see the need to clone the matrices. I think this is a documentation issue: 1) doesn't specify anything, in particular, neither whether it modifies or nor whether it might return either one; 2) "extends ". Aware of the presentation of the algorithm in CLRS, I'm still against single letter variable names (with qualified exceptions for loop control variables of innermost loops (and for the count nobody could mistake another for)). Is that for splits and for minimums (as far as evaluated)? I don't like guessing. (Speaking of which: the choice of for the minimums deserves a comment. I'd use 

(Usually, Eulerian circuits are considered for undirected finite graphs.) The question is tagged java - while the syntax of the code presented looks the type, the variable handling looks Fortran. and are instance members - needlessly. While it may be useful to have an abstraction input&print, having it print the result of a non-trivial algorithm looks odd. Specify and implement an method (and have that handle input using try with resources). shifts vertex numbers to differ from the problem statement - don't. Why have an array to hold input values that do have different meaning? Just use and . Similarly, consider using and instead of a two-dimensional array. (Given a constant time for the collections of outgoing edges, you could do without .) Much as I like code commented: is redundant (I'd drop the comment before the conditional statement and let comment the condition (slightly overstating the issue - a connected graph without edges still might contain one vertex; input spec: nVertices ≤ nEdges).) You don't need - assign to directly. I think depends on specifying a (strongly) connected graph: its doc comment should reflect that. An alternative design would introduce a , each instance holding an assortment of other vertices that can be reached from this one. Up to now, I didn't try and convince myself does indeed put together an Eulerian circuit for each connected "even" graph - in part due to lack of a test scaffold in the question. 

This is going to be a performance at the cost of readability post, see explicit praise for the comments, only for a regular code review. There always are "microefficiencies" one could worry about - and shouldn't, at least not until all bigger points are taken care of and a problem persists. Still, it is advisable to start with emphasis on readability and improve the algorithm there. One thing that might catch a would-be performance coder unawares is Java io performance - in particular, gets bashed. (I haven't tried to do useful measurements, I suspect it is comparatively bad only with big input files and no buffering.) For the hell of it: 

Not much of a code review, too long for a comment - suggestion: Do a line sweep over your (iso-oriented ("axis-parallel")?) rectangles computing "vertical" and "horizontal" scales sufficient to them as needed. (With identical y-coordinates or enclosure, adjust x-scaling and vice-versa. With overlap or enclosure in both directions, main concerns are current scales and relative amount of overlap. With identical mid-point, you're screwed.) Scale everything as computed, eyeball the result and refine your requirement description. 

This is quicksort, using Hoare's partition scheme with a twist (use two reads and two writes to resolve one inversion wrt. pivot instead of a swap (/"exchange" - making it the counterexample to labelling quicksort "partition-exchange sort": a direct exchange is not essential (partition is)), conventionally taken to be equivalent to three reads&writes, each. With today's memory hierarchies, don't expect it to be any faster because of this). This is a respected algorithm in wide use, if with three-way-partition, even dual pivot values. (There is a bug in your implementation: stuck if both and index an element equal to the pivot value ().) 

- I can't make approach or code any clearer. LeetCode's follow on: can a single solution achieve O(n) time and O(1) space?: I know how to do it modifying the input, I doubt it can be done without. There are many ways to skin cats, thinking how easy it would be using python, I gave turning s into s a try, with loop jamming and use of a sentinel thrown in: 

comparative-review: In both variants, both "traversing indices" get compared to their (respective upper) limit in each traversal of the "proper merge loop". While "the copy2res-loops" should never execute in , the "-index" gets compared to its limit right from the start. In , the "proper merge loop" isn't traversed as often. Exactly one of the copy2res-loops should be traversed for the same total. remarks: 

What do you need a new word for? Just remove the one found, up its count, and re-insert. (If this was the second time to manipulate , this would be an opportunity to re-think 's interface: perhaps or would be more useful than .) and next remove it… Don't write naked code - use a foreach loop, or streams. Here, should do. (Proper handling of obviates the ) Source organisation: I'd make a parameter to - which doesn't lend itself to dynamic/incremental use. , invoked/invocable more than once, would seem better. Strategy: Try to make do with run-time supplied classes. You stated that you want to be able to query which words are most frequent "dynamically". Imagine keeping s of words, one for each count. For each word, remove from current (if any) and insert into set of words occurring once more frequently. With standard s - say, s, keep a "global" set of your s to determine the count. Set up a test scaffold to get your approach working. Benchmark some to get an idea of resource usage. If and only if not acceptable, set and document an improvement goal and use your "first" approach as a baseline. Stressing fast What is most frequent? queries (and using a Map instead of ):