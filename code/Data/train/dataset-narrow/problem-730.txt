I will answer what you asked, but first let me tell you that I don't understand why you want to do that. An autoincremental id is very good for this task. But it is correct to also use a timestamp column, because it is a bad practice to rely on an id for sorting. Why? Because there are cases when its order might not be chronological - for example, if you use Galera cluster and you have failovers. To do what you asked, first create this index: 

First of all, I discourage you from using the slow log with output, in production. The reason is that writes are locking, which limits the concurrency of your workload. Moreover, CSV stored engine does not support indexes. Any non-trivial query will be very slow, so I don't see any advantages in doing this. Of course you could use MyISAM and add indexes, but then writes will become more expensive. That said, the correct way to "rotate" the table is copying it and than truncate it. You will probably lose some queries every time: the ones ran after the copy but before the statement. 

A quick look to the syntax will answer your question: accepts a list of tables, so it won't lock tables created after the lock has been acquired. acquires a global lock, which affects of course . I understand that you don't want to use it precisely because the lock is global; but is meant for copying all tables, is not. There is also another risk: the connection holding the lock could die, realising the lock. This risk, of course, also exists in the case of . However, you also stated that you can stop writes to the databases you want to copy. If you can do this reliably, you don't really need to lock tables. You can simply: 

Concern:App knows lock expiry time but still checks Answer: A 3rd party CRM application can also add lock in the table. So application is the not the only way to add lock. So application does not always knows what lock expiration times are. Concern:30 seconds locking period Performance issue Answer: This was just an example, the actual locking period is configuration and default is 5 seconds. Concern:Caller needs to go to sleep if entity is locked. Answer: Caller also has a mechanism to request a notification when locks are released. All these requests go to a queue. So it is critical that the caller be notified (in the sequence they requested for a lock) when the lock is expired. So caller going to random sleep may not be an option. 

One solution could be to use DBMS_SCHEDULER package and create a scheduled job. But I could not find anywhere in the documentation, some way for the job to notify application server. It can send an email but that wont help me much. Second option could be to use "Database Change Notification feature" but this is triggered on a DML or DDL change on the DB object which is not happening in my case. 

I wanted to know if DBMA_REDEFINITON package allows a WHERE clause to filter contents before migration. I have a partitioned table and wants to copy data and constraint to another table using DBMA_REDEFINITON but while copying contents, I do not want to copy a particular partition from the original table. Is it possible to drop this partition using WHERE clause. The question came from following information given on Oracle Tips site 

Avoid a single database. My recommendaion is: start with separate databases on the same server - to reduce the costs, including maintenance costs. If workload increases, you can setup a new machine and move some databases. If only one application's workload increases, you can move only that one. To do this, it is important to monitor the workloads of different applications. So I recommend to install User Statistics plugin, from Percona. And yes, the good way to distribute the workload and face crashes is to use replication or a cluster (replication is much simpler). Nowadays, we need to have no single points of failure. With MySQL you will also have another way to reduce costs: if you have applications on 3 servers, you can replicate all those databases to 1 slave - this is called multisource replication. Other benefits of using multiple databases include: 

is only needed because created_at is not unique. If is not the first column, MySQL will have to read all rows and copy them to a temporary table (which could be in-memory or on-disk) to sort them. If you decide to use the id, just keep the above snippets, but replace with . 

As another answer highlighted, Query Cache is not the only cache. Its use is not even advisable in most cases - in fact, it was removed in MySQL 8.0. The reason is that it has scalability problems (it's governed by a global lock) and it invalidates data far too frequently to be useful in a normal workload. But InnoDB buffer pool contains indexes and data accessed frequently. After your first query, some indexes and data are cached, so next time they will be read from memory. Probably some data/index pages are accessed only once per query, in which case you should be able to see a slighter difference between second and third execution (first time these pages are not cached). How to avoid the difference between query execution times? Well, there is no way to make the query faster the very first time it runs, as it needs to read from disk. But then, if your buffer pool is big enough, your query will always be fast. Keep in mind that a big buffer pool is very important for MySQL performance. The general recommendation is to keep it 75-80% of total memory. But in reality, things are more complex: 

My requirement is from the example below, can a new table have only partitions Part_2 and Part3. (Dropping partition Part_1 using WHERE clause) 

Now this SQL is internally generated by Oracle to perform row migration for child PLAN table. To resolve the issue I tried following changes: 

I am using Oracle 11g. I have a requirement to drop a partition and rebuild global indexes. The query below does job well but BLOCKS all DML operations on the table until the indexes are rebuilt. 

But If I break the query into 2 parts and rebuilt indexes with ONLINE option separately, DML queries DOES NOT get blocked while indexes are being rebuilt 

Failing Edge Case Since partition for inserting data is decided based on submitteddate which is a current date, there will be a situation where an order comes at 2016-11-30 at 11:59PM and data in ORDER table is inserted in NOV2016 partition but data in ORDER_LINE and PLAN table is inserted on DEC2016 partition as by the time inserts are done, date may change in the system. When I try to drop Nov2016 partition from all tables (child first due to FK constraint), ORDER_LINE and PLAN table drop partition might go through but ORDER table partition drop will fail as orderid from Nov2016 partition would be pointing to the DEC2016 partition data in other 2 tables. How do I make sure that the orders inserted on date change still goes to same partition across all tables? Added info (based on @dezso reply) Dezso's transaction suggestions makes sense. But to make question concise, I left some details. With those details, the suggested solution might differ a bit. Application supports 2 databases, Oracle and Postgres. For Oracle, partitioning has been implemented using Reference partitioning with partitioned ORDER table and child tables are referenced partitioned based on foreign keys. For postgres, since there is no reference partitioning option like Oracle, each table was supposed to be individually partitioned using submitteddate. The plan was not to add submitteddate to each table but to use inheritance like below 

Of course you will be able to obtain the same information with multiple queries, or with . But it seems to me less clean and, should you ever need to add or remove a table, or change the usage of (from 20,25,30 to 20,30,40), you'll need to make much more changes. There are even more obvious case when the "one table" solution is better: maybe one day you'll need a query which returns people whose age is less than 23. Flexible designs are usually better. For specific use cases, you can always create views, if really needed. 

...note that you should do them in the opposite order. It makes no sense to get the coordinates from a running master, those coordinates have no use for you. Note down the coordinates before restarting the master: those coordinates describe the state of initial slave state (the files you scp to the slave). 

Replication cannot work in your case, because developers databases will often change, and conflicts with the master break replication (by design). Also, developers cannot rely on data that are constantly changing. You need to take some form of backup and send it to developers machines. Here are the best solutions in my opinion: 

There are of course alternatives more intrinsically secure, like mysqldump and Xtrabackup. But I'll assume that you know how they work and you decided that a simple file copy is better for your use case (for example, because data are too big for mysqldump and most of your tables are not InnoDB). 

If you eventually decide to use master-master, I suggest to regularly test failover during normal work hours with a non-crazy traffic. So you will avoid bad surprises when the situation is bad. Hope this helps. 

My ordering application uses Oracle 11g Database. This DB has a primary table ORDERS and multiple child tables like ORDER_DETAILS, PLAN etc. ORDERS table is LIST partitioned on STATUS column and all other tables are referenced partitioned with ORDERID as a foreign key. At peak load, when order status is changed and ORDERS table row is moved from one partition to another, Oracle performs row migration for all the child tables referenced partitioned by ORDERS table. Due to many tables that depend on ORDERS table, large number of row movements happen causing a deadlock in one of the child table. My question is, how to resolve a deadlock caused in the ORACLE's internal row migration step? Here is an example setup: ORDERS table: 

So for some reason, ORACLE is not taking Index into consideration while running update query on PLAN table. Am I missing something? 

After 30 seconds a notification needs to be sent out to all clients that a lock has expired and this employee is available again for updates. Now to identify if LOCK_UNTIL duration has reached, application makes a SQL call to database every 2 seconds to see if 10:00:30AM has reached. Performance Issue: This call every 2 seconds is causing lot of overhead on the database and on the application server. I am looking for a better ways where Oracle itself initiates a notification to the server when lock expiration time has reached. Is there any way I can achieve this? Possible solutions: 

There are many more child table where ORDERS is there parent table. Under heavy load, when ORDER status is changed which causes row movement between partition, following deadlock error is printed in the log ORA-00060: deadlock detected while waiting for resource In the Oracle trace log, I see following SQL causing deadlock 

This query doesn't take advantage of the primary key in any way. The number of rows examined (about 160K rows) shows this. So your assumption that your does nothing is incorrect: it examines several rows. The reason why your SELECT is faster is pretty clear. This expression is computed once, at the beginning of query execution: 

I don't think that your approach is wrong, but I don't have much information. Your question is clear, but this space is limited compared to the complexity of your system. Definitely you shouldn't consider using Clickhouse for OLTP. Not only because and are not (yet) supported, but also because this database is designed to provide good performance for analytics. It lacks more or less everything is needed to optimize an OLTP workload. Kafka is a good idea? Maybe. But you won't have transactions, for example. I suggest to try to optimize your MySQL environment first. Some very generic points - sorry if they sound obvious to you, but of course I can't know your skills lever: 

I don't think that someone can give you magic numbers like the acceptable number of columns, or information too strictly related to your workload, like if it is a good idea to split the table. There are too many variables: number/types of existing columns and indexes, number of queries, how many columns you read per query, and so on. Proper tests will give you a good answer. All we can say is that, yes, common sense says that such a table should be split if possible. But then, every query will need to read from multiple partitions? Every new row will have matches in all partitions? This could slow down your application. Ideally, most of the queries should be able to read from only one partition. So the first suggestion would be to check with developers if some queries can be rewritten so that they will read less columns - possibly by rewriting some . You also ask about the best possible match condition. Here the answer is easy: join by primary key. All matching rows should have the same . It should be an columns on only one table. You first insert new rows into the table, then to the others, in this way: 

But havnt got success yet. How do I handle deadlock for this scenario? ------------------------- UPDATE ------------------------- As per suggestions suggested by Wernfried and Gandolf989, I verified if all my foreign keys have indexes on them by running query given in the Gandolf989 answer. Result was "No Rows Found". So it means, all the indexes seems to be in place. But while analyzing I realized, if I check an explain plan for a simple query like below, I see FULL table scan on the PLAN table even after having an index on ORDERID column. 

I have confirmed that OrderID column (Foreign Key column) in the PLAN table has index on it. Tried increasing PCTFREE parameter on the table. 

Current Setup: My application uses Java (Spring) and Oracle 11g and has functionality where logical locks are placed on an object before updates are made in the table. For example there are 2 tables EMPLOYEE and EMPLOYEE_LOCK. When any update is made to employee, an entry is inserted into EMPLOYEE_LOCK table to indicate that for next 30 seconds a particular employee is locked. So EMPLOYEE_LOCK table looks like below (as of 10AM) 

The down side of this approach is the time between the execution of drop partition and rebuilding index, those indexes will be unusable that might create performance problems. So my question is, is there any option where I can drop partition and rebuild index "online" in one query? Currently I dont think we have following option. 

My primary goal (with this question) was to see if Oracle can give me some way to identify this expiration time trigger and initiate an activity rather than Application server initiating one.