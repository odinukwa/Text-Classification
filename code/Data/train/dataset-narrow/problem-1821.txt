Turns out this is a bug in the version of OpenSwan I am using. I am using Amazon Linux AMI and the problem RPM is: 

The Docker image used will be the image created by the Dockerfile, and not the image specified in Dockerfile.aws.json In my case, I don't want Elasticbeanstalk to build my Docker image on the fly. Instead, I want to use an image from my AWS ECR. In that case, can I just specify a Dockerfile that only includes: 

I am running an OpenSwan server to facilitate client-server connections into a secure data centre. I have a problem with the standard L2TP over IPSEC client in MacOS, specifically when using WIFI. When I connect for the first time, it works fine. When I disconnect and try to connect again, it fails at the authentication step (shared secret). From what I can see, when the MAC is using WIFI, it doesn't get time to send a DELETE signal to OpenSwan, so as far as OpenSwan is concerned the peer still exists. I can see this in the OpenSwan logs: 

However, this retains my $HOME variable with the same value as my initial user, so mercurial could not find the required trusted information in /root/.hgrc When I established a root shell with 

Create the user in the Default Directory (Active Directory) with whatever permission you want Follow the steps in the link above to create a new subscription user, which you can then choose from the list of users in the Active Directory Log out of everything and then try to login at $URL$ as the new user 

Turns out this is pretty straightforward (it not very obvious in the documentation). Rather than use source_profile, you use credential_source: 

The only substantial difference between Amazon Linux and CentOS is that Amazon Linux has its own RPM repos (hosted by Amazon) so you may find that versions of software that are available in CentOS are not available in Amazon Linux. This isn't that frequent that it would cause a problem for standard usage situations. On the other hand, Amazon Linux is designed to be used with AWS, so it includes things like the AWS client utility, which is one less thing to worry about if you need your servers to interact with the AWS API (eg if you were copying data to S3). Another thing to consider in terms of using Amazon Linux is that every time you start a server with Amazon Linux the server will automatically install any outstanding security RPMs for that distributions. There is very handy if you have dynamic infrastructure in which you are starting/stopping instances every day, or use autoscaling extensively. For me, the best thing about Amazon Linux is that it comes with very little installed on the OS other than the basic utilities required to connect to it. From that point of view, you don't need to worry about uninstalling things like Web Servers, SMB daemons, printing services etc and instead can just install the precise set of packages you require. If you are really worried about support, you can pay $49 per month for developer level support, from which you will get answers to your questions with 24 hours. 

I've made some progress on this. The issue appears to relate to the choice of using ram nodes rather than disc nodes in the rabbitmq.config file. From the documentation: 

To /root/.hgrc When mercurial is executed, its looks in $HOME/.hgrc for trust relationships. On my existing server, the puppet agent was being executed with cron, so cron would have seen $HOME as /root/.hgrc On the cloned server, I was running the puppet update interactively, having opened a root shell using 

If you have configured a VirtualHost, and Apache is giving you a warning that there are no VirtualHosts configured, the problem is with your VirtualHosts config, not your SSL certs. I'd check you VirtualHosts config for syntax errors that may have occurred while you were updating it to take account of your new certificate. 

Use a zip archive as your application version Include a Dockerfile.aws.json file in the archive Include a Dockerfile file in the archive Include a .ebextensions folder in your archive 

The config/config.yml file exists, and had the necessary config to match the RAILS_ENV variable that is being passed to it. My suspicion is that the Docker container is having difficultly reading a file into memory, perhaps due to the way bundle install is being run as root, and is not creating the necessary CONFIG object for Rails to read. I am not a Ruby developer (I'm Operations), so please forgive any absence of basic Ruby knowledge. 

Another proposed solution here: $URL$ Use a generic certificate for all agents connecting to the puppet master 

I am using the Logstash S3 Input plugin to process S3 access logs. The access logs are all stored in a single bucket, and there are thousands of them. I have set up the plugin to only include S3 objects with a certain prefix (based on date eg 2016-06). However, I can see that Logstash is re-polling every object in the Bucket, and not taking account of objects it has previously analysed. 

If your server doesn't support SNI, or you're concerned about browsers that don't, you'll need a Cert per domain and you'll need an ip address per domain. Prior to SNI, when a client made a https connection, the server had to accept it and decrypt it before it knew that http host the client wanted to connect to. In that instance, if the server was serving multiple http hosts on the same ip address, the server would always assume that the client wanted to connect to the first ordered host in the configuration. If the certificate served in response didn't match the host requested by the client, the browser would generate an error. Hence, you had to run different domains on different ip addresses. Where SNI is enabled, the server presents all available certs to the client request, and the client can match its host request to the correct cert if its exists, meaning a browser error is only generated if none of the certs match. This allows you have host multiple https hosts on the same ip address. You will still however need an https cert per domain (free or commercial). 

Why not write your own command/script that checks each server certificate, aggregates the data, and then alerts if a percentage of checks fail? You can check Cert expiry with openssl from a command prompt: 

(172.31.100.102 = Nat Instance 2) This suggests that while traceroute may be aware of a specific route to a specific network, it will only report an attempt to follow that route if routing is allowed on the default gateway for that route. If not, it will attempt to follow the default route and report success or failure for the default route only. I'm sure this is consistent with the design of traceroute, but probably signals that traceroute may not be the best tool to debug routing issues (its more a tool to debug network issues). 

The RabbitMQ clustering document $URL$ describes a process whereby you can deploy a configuration file for clustering so that a cluster is automatically created when your rabbitmq nodes boot.eg 

It doesn't work. I get permission denied when I attempt to deploy instances as a user who is a member of a group where this policy applies. Is there some other policy I need to include with this to allow instance deployment in this way? 

Check that the NRPE service in running on the destination server Check that you have configured allowed_hosts correctly on the destination server so that the Nagios server is allowed connect (nrpe.cfg) Check that no firewalls are blocking the connections 

I use the following directive in sshd_config to detect if the user trying to login to a server is called developer, and issue a bash script to the user if that is the case: 

You apply this file on each of the 3 nodes at /etc/rabbitmq/rabbitmq.config, and then start rabbitmq on each and a cluster will apparently form. This doesn't appear to work too well eg If you start rabbit2, and rabbit3 hasn't already come up, the service will not start on rabbit2, as it is trying to create a cluster with rabbit3. Equally, if you only apply the config on rabbit1, and have pre-started rabbit2 and rabbit3, rabbit1 will form a cluster with rabbit2 only, as, according to the documentation (I don't understand why): 

However, when I close the connection on the MAC, DPD doesn't seem to kick in. OpenSwan just keeps logging errors about the connection. Just looking for suggestions re. a fix. 

There are a variety of solutions for this. OpenVPN will work, but my preferred solution is to set up an L2TP over IPSEC VPN, using an OpenSwan VPN server inside your AWS VPC. IMHO, IPSEC VPNs are a better option that SSL/TLS based VPNs (eg OpenVPN), given all the issues with SSL over the last few years. This allows you to create a local VPN connection on your desktop computer that you can fire up when ever you need access to your VPC. You can choose to direct all your traffic through that or just particular routes. The only qualification I would add is that getting that connection to work in Windows 7 requires a Registry tweak. It works out of the box with no additional software in Mac/Ubuntu. $URL$ Remember to disable Source/Destination checking on the EC2 instance you use for terminating the VPN inside EC2. 

There are various ways to trigger a Jenkins job from an SCM like Bitbucket, but what I want to do specifically is trigger a build using the branch that is the source of the Pull Request. Up to now, we have used the Bitbucket Pull Request Builder, but it is very flaky and unreliable, and not supported well. $URL$ Bitbucket do supply quite good features in terms of Webhooks, which when used with the Jenkins Git Plugin, do allow for triggering of builds based on various Bitbucket events (eg a Pull Request update). There is also the Bitbucket Webhook plugin, but again that doesn't offer much in terms of dynamically choosing the branch you want to build. $URL$ However, what this seems to do is trigger a poll of the repo, where is then tries to build any branch that is different from the main branch. Our use case is that we allow developers create their own branches, for which they then create Pull Requests to the development branch. There doesn't seem to be any way to trigger a build that uses the developer created branch as the build branch (other than the aforementioned Bitbucket Pull Request Builder). Am I right or wrong in this? 

That figure is a total. It arises from your uniq -c command. Your command produces a large number of blank lines, due to the way you are using cut, and uniq is counting the number of blank lines it finds. It isn't the number of connections from a particular (unknown) ip address. 

I am deploying a docker-compose stack of 5 applications on a single AWS EC2 host with 32GB RAM. This includes: 2 x Java containers, 2 x Rails containers, 1 x Elasticsearch/Logstash/Kibana (ELK) container (from $URL$ When I bring the stack up for the first time, all containers start. The ELK container takes about 3 minutes to start. The others come up straight away. But the ELK containers exits after about 5 minutes. I can see from the logs that the elasticsearch service will not start. The log messages indicates a memory limitation error. However, when I then tear everything down, and bring it up again, all the containers start straight away, including the ELK container, and everything remains stable. The issue only occurs the first time I start the stack on a new EC2 instance. I can see from the docker stats that the ELK container is only using 2-3GB of the 32GB RAM available on the instance. The ELK container is configured as follows: 

This seems to be the default behaviour for this plugin, so it has to be managed using the plugin features. Basically, you have to set up the plugin to backup-then-delete the objects with a prefix to the same bucket. In that way, Logstash will skip object when it polls the bucket after the next interval. Sample config: 

Its possible that this is occurring because I am using a larger Ec2 instance (32GB RAM). It may not be an issue on smaller instances. 

The IAM service is Amazon AWS permits the creation of policies based on date, as described here: $URL$ This seems to refer to allowing/denying actions based on calendar date. eg after 12:00 on March 1st 2015 Is it possible to create a condition that permits/denies action on the basis of time of day eg between 14:00 and 18:00 on any given day? 

Which does install and set Ruby 2.3.1 as the default, the "bundle install" step just times out when the Docker image is building. When I open a shell to the container, and reset Ruby to 2.2.5, bundle install works. Is it possible to use this image with Ruby 2.3.1 instead of Ruby 2.2.5? 

Thanks to CtrlDot for the lead. The link in that answer might as well be in arabic if you are not familiar with Active Directory and Microsoft authentication regime, which must new Azure users will not be. I was able to piece this together from the following link: $URL$ Steps: