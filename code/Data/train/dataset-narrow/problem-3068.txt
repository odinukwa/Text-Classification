Rating bias and scale can easily be accounted for by standardization. The point of using Euclidean similarity metrics in vector space co-embeddings is that it reduces the recommendation problem to one of finding the nearest neighbors, which can be done efficiently both exactly and approximately. What you don't want to do in real-life settings is to have to compare every item/user pair and sort them according to some expensive metric. That just doesn't scale. One trick is to use an approximation to cull the herd to a managable size of tentative recommendations, then to run your expensive ranking on top of that. edit: Microsoft Research is presenting a paper that covers this very topic at RecSys right now: Speeding Up the Xbox Recommender System Using a Euclidean Transformation for Inner-Product Spaces 

The following general answer is my uneducated guess, so take it with grain of salt. Hopefully, it makes sense. I think that the best way to describe or analyze experiments (as any other systems, in general) is to build their statistical (multivariate) models and evaluate them. Depending on whether environments for your set of experiments are represented by the same model or different, I see the following approaches: 1) Single model approach. Define experiments' statistical model for all environments (dependent and independent variables, data types, assumptions, constraints). Analyze it (most likely, using regression analysis). Compare results across variables, which determine (influence) different environments. 2) Multiple models approach. The same steps as previous case, but compare results across models, corresponding to different environments. 

The following great article by Sebastian Raschka on Bayesian approach to text classification should be very helpful for your task. I also highly recommend his excellent blog on data science topics, as an additional general reference: $URL$ You may also check this educational report on text classification: $URL$ It might provide you with some additional ideas. 

This is not a problem about neural networks per se, but about representing textual data in machine learning. You can represent the movies, cast, and theme as categorical variables. The plot is more complicated; you'd probably want a topic model for that, but I'd leave that out until you get the hang of things. It does precisely that textual "downsampling" you mentioned. Take a look at this tutorial to learn how to encode categorical variables for neural networks. And good luck! 

Clustering I think a tree is a perfectly appropriate data structure in this case. You don't need an embedding to do clustering; there are similarity-based approaches too, and defining a similarity function in your case is straightforward: I'd say it's a function of the common depth between the two items in consideration. You will probably want to define a root to "normalize" the scores. For example, given the root "a/b/" and the paths: 

Also check built-in data sets in the open source software Parallel Sets, which is focused on the categorical data visualization: $URL$ 

I think that it is impossible to answer this question comprehensively, at least for the following reasons: 

I see at least five ways to approach this problem of finding a data scientist position/work specifically at non-profit, non-governmental or similar organizations, as I describe below. I hope that this is helpful. 

Very interesting question (+1). While I am not aware of any software tools that currently offer comprehensive functionality for feature engineering, there is definitely a wide range of options in that regard. Currently, as far as I know, feature engineering is still largely a laborious and manual process (i.e., see this blog post). Speaking about the feature engineering subject domain, this excellent article by Jason Brownlee provides a rather comprehensive overview of the topic. Ben Lorica, Chief Data Scientist and Director of Content Strategy for Data at O'Reilly Media Inc., has written a very nice article, describing the state-of-art (as of June 2014) approaches, methods, tools and startups in the area of automating (or, as he put it, streamlining) feature engineering. I took a brief look at some startups that Ben has referenced and a product by Skytree indeed looks quite impressive, especially in regard to the subject of this question. Having said that, some of their claims sound really suspicious to me (i.e., "Skytree speeds up machine learning methods by up to 150x compared to open source options"). Continuing talking about commercial data science and machine learning offerings, I have to mention solutions by Microsoft, in particular their Azure Machine Learning Studio. This Web-based product is quite powerful and elegant and offers some feature engineering functionality (FEF). For an example of some simple FEF, see this nice video. Returning to the question, I think that the simplest approach one can apply for automating feature engineering is to use corresponding IDEs. Since you (me, too) are interested in R language as a data science backend, I would suggest to check, in addition to RStudio, another similar open source IDE, called RKWard. One of the advantages of RKWard vs RStudio is that it supports writing plugins for the IDE, thus, enabling data scientists to automate feature engineering and streamline their R-based data analysis. Finally, on the other side of the spectrum of feature engineering solutions we can find some research projects. The two most notable seem to be Stanford University's Columbus project, described in detail in the corresponding research paper, and Brainwash, described in this paper. 

I went to the trouble of looking at your file now that you linked to it, and the problem is that you have removed to much information, so your index is not unique. Indices always have to be unique, so either you add that information back, or you preprocess the data before pivoting such that the duplication is resolved. I chose to retain the extra columns; I don't know if this is what you want, but it should help you understand what it takes to make pivoting work: 

I would use a visual analysis. Since you know there is a repetition every 256 bytes, create an image 256 pixels wide by however many deep, and encode the data using brightness. In (i)python it would look like this: 

Wrong question. Big data is not a question of this or that language, but cluster computing. For me it's implicit in the definition; if you can find a way to process your data on your laptop it just isn't big data. Spark is the de facto standard today for cluster computing. It comes with many of its own munging primitives, borrowed from numerous languages (think dataframes and functional programming), and bindings to them. Scala is the best language in terms of Spark API coverage, followed by python, then R. If you want to experiment with Spark you can rent managed instances from Google on DataProc or spin up your own. 

I agree with @ssdecontrol that a minimal reproducible example would be the most helpful. However, looking at your code (pay attention to the sequence ), I believe that the issue you are experiencing is due to an inappropriate setting of R's global option. It appears that your current setting is likely , which refers to converting warnings to errors, whereas, you, most likely want the setting , which is to treat warnings as such, without converting them to errors. If that is the case, you just need to set the option appropriately: 

Check the Stanford NLP Group's open source software ($URL$ in particular, Stanford Classifier ($URL$ The software is written in , which will likely delight you, but also has bindings for some other languages. Note, the licensing - if you plan to use their code in commercial products, you have to acquire commercial license. Another interesting set of open source libraries, IMHO suitable for this task and much more, is parallel framework for machine learning GraphLab ($URL$ which includes clustering library, implementing various clustering algorithms ($URL$ It is especially suitable for very large volume of data (like you have), as it implements model and, thus, supports multicore and multiprocessor parallel processing. You most likely are aware of the following, but I will mention it just in case. Natural Language Toolkit (NLTK) for ($URL$ contains modules for clustering/classifying/categorizing text. Check the relevant chapter in the : $URL$ UPDATE: Speaking of algorithms, it seems that you've tried most of the ones from , such as illustrated in this topic extraction example: $URL$ However, you may find useful other libraries, which implement a wide variety of clustering algorithms, including Non-Negative Matrix Factorization (NMF). One of such libraries is Python Matrix Factorization (PyMF) with home page at $URL$ and source code at $URL$ Another, even more interesting, library, also Python-based, is NIMFA, which implements various NMF algorithms: $URL$ Here's a research paper, describing : $URL$ Here's an example from its documentation, which presents the solution for very similar text processing problem of topic clustering: $URL$ 

Your first reference states that sigmoid kernels behave like RBFs for certain parameters. This makes them suited to nonlinear classification. You probably know that the sigmoid kernel is only conditionally PSD, and thus sometimes does not correspond to the kernel function of any implicit feature map per Mercer's theorem. 

Try running it in pypy or numba Find a faster implementation. Unfortunately I can not recommend one. Parallelize the loop over the documents. Not so hard since your vocabulary is precomputed. (Even if it weren't you could get away with it using the hashing trick.) Combine this with the first bullet. Rewrite the inner loop in Cython. Rewrite the whole thing in a faster language like C++ or Scala. 

Define a new column, then use these ids to select the relevant rows and set that column to the appropriate id: 

I would recommend you to start from reading the draft of the introductory book "Sentiment analysis and opinion mining" by Bing Liu. The draft in a PDF document format is available for free here. More details about the new upcoming book of this author, as well as comprehensive information on the topic of aspect-based sentiment analysis, with references and links to data sets, are available at this page: $URL$ Another interesting resource is a survey book "Opinion mining and sentiment analysis by Bo Pang and Lillian Lee. The book is available in print and as a downloadable PDF e-book in a published version or an author-formatted version, which are almost identical in terms of contents. 

Knowledge is a general term and I don't think that there exist definitions of knowledge for specific disciplines, domains and areas of study. Therefore, in my opinion, knowledge, for a particular subject domain, can be defined just as a domain-specific (or context-specific, as mentioned by @JGreenwell +1) perspective (projection) of a general concept of knowledge. 

Let $x, y \in \mathbb R^N, \mathbf \alpha \equiv (\alpha_0, \dots, \alpha_N) \equiv (\alpha_0, \mathbf \alpha'_0), \; \mathbf z^\alpha \equiv z_0^{\alpha_0} \dots z_N^{\alpha_N}$ and ${d \choose \alpha}$ be the multinomial coefficient. Then $$\left(\left<\mathbf x, \mathbf y \right> + c\right)^d = \sum_{|\mathbf \alpha| = d} {d \choose \alpha} c^{\alpha_0} (\mathbf x \mathbf y)^{\mathbf \alpha'_0}$$ If you redistribute the coefficient ${d \choose \alpha} c^{\alpha_0}$ between $\Phi(\mathbf x)$ and $\Phi(\mathbf y)$, it follows that $$\Phi(\mathbf z) = \left(\sqrt{ {d \choose \alpha} c^{\alpha_0}} \mathbf z^{\alpha'_0} \right)_{\forall |\mathbf \alpha| = d}$$ References 

It means the average predictions of the user across all items, and the average of each item across all users accurately predicts the ratings -- you have an easy data set. 

The 1/2 coefficient is merely for convenience; it makes the derivative, which is the function actually being optimized, look nicer. The 1/m is more fundamental; it suggests that we are interested in the mean squared error. This allows you to make fair comparisons when changing the sample size, and prevents overflow. So called "stochastic" optimizers use a subset of the data set (m' < m). When you introduce a regularizer (an additive term to the objective function), using the 1/m factor allows you to use the same coefficient for the regularizer regardless of the sample size. As for the question of why the square and not simply the difference: don't you want underestimates to be penalized similarly to overestimates? Squaring eliminates the effect of the sign of the error. Taking the absolute value (L1 norm) does too, but its derivative is undefined at the origin, so it requires more sophistication to use. The L1 norm has its uses, so keep it in mind, and perhaps ask the teacher if (s)he's going to cover it.