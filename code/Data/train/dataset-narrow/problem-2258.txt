Let $P$ be a prime. Given a number $a$, what is the computational complexity in establishing if $a$ is a cubic or higher order residue modulo $P$? Are there any good algorithms? 

Is P=BPP strong enough to derandomize Vazirani-Valiant reduction? If not what other ingredients are necessary to derandomize Vazirani-Valiant reduction? 

Problem Instances at given $\alpha>0$. $(1)$ Given $a_1,\dots,a_{n^\alpha}\in\Bbb Z$ with $|a_i|\in(2^{n-1},2^n-1)$ is there a subset of that sums to $0$? $(2)$ Given $a_1,\dots,a_{n}\in\Bbb Z$ with $|a_i|\in(2^{n-1},2^n-1)$ is there a subset of size at most $n^\alpha$ that sums to $0$? At what range of $\alpha$ are these problems $NP$-hard? Is the problem studied anywhere? 

So it seems highly plausible it should be the case that $\#$ of perfect matchings on planar graph should be computable in linear time with $O(n)$ bit complexity. However if we use $Det(M)$ directly we cannot avoid $O(n^2)$ at best. But there may be an indirect way to compute $Det(M)$. 

Have any fixed parameter integer programming algorithms described in Integer programming with a fixed number of variables been implemented? Is there a reference code that researchers can use? 

The subgraph isomorphism problem problem is to determine given $G$ and $H$ whether $G$ is a subgraph of $H$. Let $G$ and $H$ be regular graphs with degree of $H$ greater than degree of $G$. Does the subgraph isomorphism problem remain NP-complete for the following case: $1.$ Girth of $G$ and $H$ are fixed, say $girth=3$ or a fixed $h$? How large should a fixed $k$-diameter $d_H$-regular graph $H$ be for it to have a fixed $k$-diameter $d_G$-regular subgraph $G$ of vertex count $n_G$ when both have the same girth? 

BTW: The list of known CC-complete problems has grown since both versions of the book. See this paper by Greenlaw and Kanlabutra. 

The natural model related to decision trees that can simulates RAMs is the branching program. Basically, it is a decision tree with common subtrees coalesced yielding a DAG. Time T and space S on a RAM can be simulated in height T and size 2^S on a branching program. (You might need to use multi-way branching.) For decision problems, it is clear that any decision tree only needs height = # inputs and space=total # of bits in the input. Note that with multi-way branching one might have the # bits in the input larger than the usual measure of the # of inputs (e.g. n pointers each taking log n bits.) For such problems with nlog n total input bits one can prove that certain problems cannot be solved in time O(n) and space=O(n) bits. Is that the form of you problem? You seem to suggest that you are using the # of outputs to try to get a larger lower bound. It is usual for multi-output problems to allow many outputs along a single edge rather than at leaf nodes (see, for example, Borodin-Cook's 1982 paper on sorting lower bounds). However, even without this assumption, one also can compute any function with height = # inputs and space = # input bits. (Read and remember the input, and output all the values at each leaf node.) 

Lattice-basis reduction (LLL algorithm). This the basis for efficient integer polynomial factorization and some efficient cryptanalytic algorithms like breaking of linear-congruential generators and low-degree RSA. In some sense you can view the Euclidean algorithm as a special case. 

Does a single tape Turing machine with access to $\mathsf{PSPACE}$ oracle needs more than $\mathsf O(1)$ working tape memory and $\mathsf O(1)$ working time to solve $\mathsf{NP}$-complete problem? What is largest complexity class oracle that a single tape Turing machine could need so that it will need $\omega(1)$ Space time resource (as against separate space and time) to solve $\mathsf{NP}$-complete problem? 

Can we have randomized $2^{n^{o(1)}}poly(m)$ time in above statement? Is there a similar result that would give $E\not\subseteq SIZE(2^{\delta n})$ (this would give $P=BPP$)? How large can $o(1)$ be in the statement above and in 1. if applicable (can it be as large as $1/\log\log n$)? 

$URL$ says deciding diameter $2$ radius $1$ graph isomorphism is $GI$ complete. Is it possible only the diameter $2$ radius $1$ bipartite graph isomorphism (there is only one structure for this I can think of - vertices of color $1$ in first and third column with vertices of color $2$ in second column and first and third column having no direct edges) is $GI$ complete? 

There are studies about approximation algorithms for NP complete problems in Polynomial time and exact algorithms in exponential time. Are there studies about approximation algorithms for NP complete problems in subexponential time of form $2^{n^{\delta_2}}$ where $\delta_2\in(0,1)$? I am particularly interested in what is known about hard to polynomial time approximable problems such as Independence number and Clique number in subexponential time? Note that ETH only prohibits exact computation in such a time frame. Say Independence number is $\alpha(G)=2^{r(n)n}$ on a graph with vertex count $|V|=2^{s(n)n}$ for some $0<r(n)<s(n)$. Is an $2^{(r(n)n)^{\delta_1}}$-factor approximation scheme possible for Independence number in time $2^{|V|^{\delta_2}}=2^{2^{\delta_2s(n) n}}$ where $0<\delta_1<1$ and $0<\delta_2<1$ are some fixed positive reals? That is for every $\delta_1\in(0,1)$ is there a $\delta_2\in (0,1)$ such that $\alpha(G)$ can be approximated within $2^{\log_2^{\delta_1}(\alpha(G))}=2^{(r(n)n)^{\delta_1}}$ factor in time $2^{|V|^{\delta_2}}=2^{2^{\delta_2s(n) n}}$? 

The only other thing I would add to what has been said is, if you haven't already, do a course in linear algebra. It will help you understand things like what $R^n$ means and concepts like that. Sometimes courses on a topic will give you a brief background in the linear algebra concepts and ideas specific to the material at hand, but they are never very deep; and I always find that it is a lot easier to comprehend and absorb an idea if you have a good understanding of the underlying mechanics. I would suggest the coursera course Coding the Matrix: Linear Algebra Through Computer Science Applications from Brown University. 

I have just been thinking about the simulated binary crossover (SBX) operator used in the NSGA-II algorithm and other real-valued genetic algorithms; and i am wondering if there is any reason that there always needs to be a crossover from both parents to both children? do both children need to be the "opposite" of each other in order to preserve diversity? or is it something that is just assumed? As far as i can tell, the SBX operator seems to basically work by flipping a coin for each indexed element along a pair of strings to decide if the operator is called, and if the coin flip is successful, it generates a pair of values that are close to the original parent values in accordance with some probability distribution, and either swaps them between the children (p1_i->c2_i | p2_i->c1_i), or inserts them into their respective children (p1_i->c1_i | p2_i->c2_i).. but the key being, that the two offspring are always the "inverse" of eachother.. eg: p1 = [4 6 2 0 3] p2 = [7 1 9 8 5] c1 = [4 1 9 0 3] c2 = [7 6 2 8 5] (to make my point easier to explain, i have omitted the fact that the child values are usually perturbed slightly from their original parent values when using SBX - ie. I am taking a child value of 2.2 to mean the same as a parent value of 2) You can see, in this way, c1 is the "inverse" of c2. Is there any reason why both children can't take on the characteristics of the same parent (p1_i->c1_i | p1_i->c2_i)? eg: p1 = [4 6 2 0 3] p2 = [7 1 9 8 5] c1' = [4 6 9 0 5] c2' = [4 1 2 0 5] I think I understand why this isn't the case with standard binary crossover, as you are dealing with blocks of values, but with real valued crossover you are taking each individual element in the string separately, so why can't both children take on the same value with a certain probability? The only reason that I can think of is that it might be in order to maintain a higher diversity among the child solutions; and if a solution such as c2' was required, then it must come from a different crossover event in the future. Is my thinking on the right track? or is there another reason? EDIT: I suppose the solution to this would be to generate two potential child values per crossover but only one child, which tosses a coin to see which of the potential values it gets; however this would require twice the number of crossover operations. 

Integer programming is NP-hard. What is the status of integer programming problem that decides between existence of $\leq1$ solution and $>1$ solutions (note $0$ solutions falls in $\leq1$ category)? Integer programming in fixed parameters is P. What is the status of integer programming problem in fixed parameters that decides between existence of $\leq1$ solution and $>1$ solutions (note $0$ solutions falls in $\leq1$ category)? 

A paper was posted in arxiv $URL$ titled 'Rectangular Kronecker coefficients and plethysms in geometric complexity theory' by Christian Ikenmeyer and Greta Panova with first line in abstract stating 'We prove that in the geometric complexity theory program the vanishing of rectangular Kronecker coefficients cannot be used to prove superpolynomial determinantal complexity lower bounds for the permanent polynomial'. If correct what is the implication of this result to the geometric complexity theory program for separating permanent from determinant? 

Given an $n\times n$ Vandermonde integer matrix with structured integers (such as arithmetic or geometric progression). Is complexity of approximately computing Vandermonde determinant upto multiplicative error $\epsilon_m$ and additive error $\epsilon_a$ studied in literature? 

What is the best lower bound for algebraic formulas for Permanent of a matrix given that the formulas have no negative sign? Is there an exponential lower bound known for such formulas and what would be a good reference on the topic? 

$\underline{\mathsf{EQUAL\mbox{ }k-COMPLEMENTARY\mbox{ }SUBSET\mbox{ }SUM(EkCSS)} }$Problem: Input: $a_1,\dots,a_n,b\in \mathbb Z$, with distinct $a_i$ and $k\in\Bbb Z^+$. Output: $k$ $\mbox{ }\underline{not\mbox{ }necessarily\mbox{ }disjoint}$ subsets of $a_i$s of sizes $n_1,\dots,n_k$ satisfying $\sum_{i=1}^kn_i=n$ such that each subset sums to $b$. (1) Is $\mathsf{EkCSS}$ $NP$-complete? (2) If we replace requiring $k$ subsets by requiring $\lceil\log^cn\rceil$ subsets for some fixed $c\in\Bbb R^+$ does the problem remain NP complete? (3) Is $\mathsf{EkCSS-Diff}\mbox{ }$ $NP$-complete where $\mathsf{EkCSS-Diff}$ is same as $\mathsf{EkCSS}$ but with added condition $n_i\neq n_j\forall i\neq j$ (different subset sizes)?