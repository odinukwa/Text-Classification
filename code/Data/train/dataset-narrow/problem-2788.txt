"Dedicated" in the configuration name refers to the game's dedicated multiplayer server. It's for sever operators exclusively, and runs an empty multiplayer server that exists solely to receive connections from other clients. You don't want that. Run a configuration that starts with "Debug" or "Release" instead of "Dedicated." 

I realized I should probably update this, in case anyone reading this neglected to click through to the thread on the UDK forums. I was told by a moderator that everything I want to do is impossible without engine modifications. 

Scaleform GFx is a commercial middleware library for integrating Adobe Flash movies (optimized rendering, ActionScript, and more) into your engine. In a past life, Scaleform GFx's codebase was that of GameSWF. GameSWF is open source. I planned to add more to this answer, but by the time I got back to it, most of main points I wanted to hit on were mentioned in other answers. The only other thing I can think to mention that's relevant to the Flash-based approach is the fact that Adobe has released the source code to their ActionScript virtual machine, which now lives at the Tamarin Project at Mozilla. 

The character moves 5 pixels over 60 frames. The longer your cycle takes the bigger delta time will get. For every framerate (1/30, 1/25, etc) the result will be the same. 

There is no right answer to this. It all depends on the implementation of your algorithms. Just go with one you think is best. Don't try to optimize at this early stage. If you are often deleting objects and recreate them, I suggest you look at how object-pools are implemented. Edit: Why complicate things with slots and what not. Why not just use a stack and pop the last item off and reuse it? So when you add one, you will do ++, when you pop one you do -- to keep track of your end index. 

I would say neither. The board and player are 2 objects managed by a different object that runs logic depending on the information given by the board and applied onto the player. So you move the player by the "game logic". Which depends on the information returned by the board. Which in this scenario could be the cell(A space on your board) relatively from the players current position to the next calculated cell determined by the die. A Cell holds various information, like which player owns it ( if we are talking about monopoly) trough id. The condition the player must meet before progressing any further etc. Your player could simply be a sprite, and by using an additional object you can keep track of stats like id and position. ( although you could just program the sprite and the stats into one object) This information is passed onto the board itself and processed internally to handle its own information. It then gives information back to the "game logic" and the player is moved by the conditions that are given. So basically : 

There are so many special case considerations with the simulation of wheeled vehicle physics that you are unlikely to get a satisfying result by simply torquing cylindrical collision primitives. You should consider instead using the special Wheel Collider component. 

Have you read Johannes Spohr's thesis on "Pace" and its renderer? It describes a so-called "submission engine"* parallel renderer, and may give you some ideas. Here is the summary page (in German), and here is a direct link to the PDF which is in English. (*: this link also goes to the article where I originally heard about the thesis.) EDIT: I'd only skimmed this previously, and I just looked at it again... and realized it really glosses over scene graph details. I guess I didn't realize how orthogonal his design was. Sorry if it's not particularly helpful. 

Is it possible to override all the materials on the client (en masse), such as those being used for the terrain or BSP geometry in the current level? (For implementing alternate vision modes and things of that nature.) 

The tools that come with the UDK are of course very polished, and it's hard to beat free Scaleform and SpeedTree, but I'm starting to think the platform is a terrible fit for anyone who wants to go above and beyond drag-and-drop material editing in terms of graphics. I feel like I have much more control over the rendering in a Source engine mod, for example. 

dt (delta time) is the time between each cycle/render frame (or any time stamp you desire) of your loop. With this delta time we can stretch certain values over time. Just like in the real world we measure certain physics properties over time. Let's say we run our game 60 frames a second. If we want our our player to move 5 pixels per second we do 

Now you could write your keybindings to a simple text file. And just set those variables when you read them. ( havn't tried reading text/xml/json files in Unity but im sure it's possible ) The idea here is that the keys are stored in simple variables. Which you could easily change. It should be trivial then to read/write them away. So making an interface in game where you can set your keys shouldn't be a problem either. 

Simply put, I iterate over the points on my t line. Check if t is between any of the points and calculate the inner interpolation of two points. Using the simple linear interpolation equation: v0+(v1-v0)*t; So a table of: 

If you were to employ post-process antialiasing such as SMAA, it would not differentiate between geometric edges and texel edges. This might be sufficient in conjunction with the nearest neighbor texture filtering. 

The other answer doesn't give you the whole story, even with the addition of Tetrad's comment. It's of vital importance to take into account the non-linear distribution of precision for values stored in the Z-buffer. zNear has a much greater impact on artifacts such as Z-fighting than zFar does. For a better explanation of why, see this article: $URL$ 

The Cg toolkit supports its own version of effect files, CgFX. These support techniques, states, annotations... I don't have any experience with them personally, but they are supported by FX Composer, at least. 

No, nor did the MD3 format used in Quake 3. As far as I know, skeletal animation was introduced as recently as id Tech 4 and the MD5 model format. 

There are a few relevant resources on the UDN about this subject (keywords "level streaming"), and even a convenient "hub" article that links to all of them: $URL$ "The level streaming functionality in Unreal Engine 3 makes it possible to load and unload map files into memory as well as toggle their visibility all during play. This makes it possible to have worlds broken up into smaller chunks so that only the relevant parts of the world are taking up resources and being rendered at any point. If done properly, this allows for the creation of very large, seamless levels that can make the player feel as if they are playing within a world that dwarfs them in size." 

A library simply refers to a collection of classes/functions. There is really not much to it. A Game engine can be released as a library, it's not going to change anything. Afterall software is build from a collection of classes and/or functions. Where a game engine refers to the basic software of your game. When you speak of a game engine there is at least an architecture involved that handles the bare minimum of the game structure( Entities/Gameobject, rendering, etc ). A lot of the technical stuff is automated for you. Game engines dictate how certain things are done ( adding scenes, entities/gameobject, loading assets, etc ). All you have to do is add gamelogic and give it an artistic flair ( assets-sound/models/shaders/whatever ). Game engines exists so that they can boost production. Why or how they do certain things in certain ways is arbitrary ( programming styles as well as work environments can play a big role. To each their own). 

Think for a moment what happens if bSpawnSandbag is true. It will always draw the sandbag. However the only input you are using is the playerPos.X/Y for where to draw it. Of course it's going to draw it at the players position. It's continues and not a one time thing. Code isn't called once unless your structure is specified/setup as such. As long the bSpawnSandbag is true, sandbag will be drawn. I don't know what properties your sprites has. Simply keep track of the position when space is hit 

However, this will move the entity's left edge to the colliding tile's left edge. That's bad - it places your entity squarely inside the collidee. Your top and left checks need to take into account the width and height of your entity. 

I would say that the most time-consuming task that needs to happen in most games is recreating video memory resources (textures, render targets, meshes) in response to a "device lost" event (note that this is specific to Direct3D 9 and earlier). Taken from MSDN: $URL$ 

You should definitely check out the Halo Wars GDC presentation, "The Terrain of Next-Gen." It discusses using full vector field displacement instead of simple height field displacement. For something a little less revolutionary, maybe check into geometry clipmaps. There's a good article in GPU Gems 2 here. 

I swear I added this an answer to this question back closer to when it was originally asked... Check the "max frames to render ahead" setting in the NVIDIA Control Panel, or whatever the comparable setting in the AMD drivers is if using an AMD card. I believe it defaults to 3, which would correspond exactly to what PIX is showing you (the GPU is rendering 3 frames ahead of what it's currently displaying). 

(Mind you the tiles near the bottom are elevated) which places the tiles correctly, but their and values are not meaningful at all. In my iteration I've assigned the and values of the array indices to the and properties. Their values don't represent the point in "3d space". But it does however correctly helps convert to screen space with the multipliers and ( tile halfwidht/height). The purple sprite in the image should overlap between tiles depending on their , and . From which depth is calculated and then used to sort my sprite. But since the and values are assigned to such a small number which does not represent the tiles actual point in "3d space" the depth of the tiles are incorrect. Q:How do I calculate the and positions for the tiles which are then used to correctly calculate screenspace? 

Seriously, Google that stuff: $URL$ There is a complete manual on unity. Took me just one google search. 

But for some reason the bigger the angle, the closer you have to be to the center in order for it to detect you. For smaller angles it's the opposite: Here is what I have now. (click the " Play flash full screen " , for some reason the hosting settings won't show it in it's original size) You can see that the bigger views actually respond only when you are near the center. While the smaller views start rotating even though you're not even in their view. for now I'm using the mouse coordinates. I'm pretty sure all vectors operate in the same space coordinates. Properties and constructor: 

If you are interested in the nitty gritty reality of high resolution timing on the PC, you should read the article Timing Pitfalls and Solutions by Jan Wassenberg. It is 5 years old now, but unfortunately the landscape has not improved significantly as far as I'm aware. You may also be interested in the related GameDev.net discussion thread and article. Unfortunately, the formatting is badly broken on the latter, presumably from years of site redesigns, but the content by and large appears to simply be an earlier iteration of what's in the PDF I linked above. The forum thread in particular has links that are supposed to go to source code, but they all 404 at this point. The article, meanwhile, has a small snippet of code at the bottom that you can get to a semi-readable view of by looking at the article revision history. 

Cg is not technically HLSL, but syntactically they are virtually identical. Having said that, it's an older book, but NVIDIA has made "The Cg Tutorial" free to read online. It covers the basics of the programmable pipeline in a very comprehensive fashion, and it should all translate to HLSL with minimal effort. 

I'm wondering how objects follow each other where they move over the previous position of the object in front of them. When the leading object stops, all that are following should stop at their position. So how is this achieved? Like so: 

2: We first create a vector ( 2D in this case with component x and y ) by taking the difference from both positions ( mouse - player ). 3: We then Normalize it to create a so called " unit vector ". Which means to bring the length of our vector to 1. This is done by dividing both x and y component of the vector by the length/magnitude. We need this because this is our direction vector. It simply tells in which direction we are heading for the x and y axis. 4: Now we have the direction we just need to multiply it by a scalar ( or simply put by your speed). Since the length is 1. Any number we multiply with will result in a new vector with the length equal to your given speed. Just remember that a unit vector represents the direction of your vector. Basically the red dotted lines is how much it moves in the x direction and how much it moves in the y direction per frame. So recap: Unit vector = direction Magnitude/Length/speed = steps to move per frame on x and y axis. (The lengths in the drawing are just for visual aid, they are not meant to be accurate ) Hope this helps.