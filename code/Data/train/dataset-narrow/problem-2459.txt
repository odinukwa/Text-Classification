Almost every algorithm that works in the PAC model (with the exception of parity learning algorithms) can be made to work in the SQ model. See e.g. this paper of Blum et al. in which several popular algorithms are translated into their SQ equivalents (Practical Privacy: the SuLQ framework). The paper is in principle concerned with "privacy", but you can ignore that -- it is really just implementing algorithms with SQ queries. Agnostic learning, on the other hand, is much harder in the SQ model: computational issues aside (though these are important), the sample complexity required for agnostic learning is roughly the same as that required for exact learning, if you actually have access to the data points. On the other hand, agnostic learning becomes much harder in the SQ model -- you will usually need to make superpolynomially many queries, even for classes as simple as monotone disjunctions. See this paper by Feldman (A complete characterization of statistical query learning with applications to evolvability) or this recent paper by Gupta et al. (Privately Releasing Conjunctions and the Statistical Query Barrier) 

There is nothing about Chernoff Hoeffding bounds that is specific to boolean variables. If $X_1,\ldots,X_n$ are i.i.d. real valued random variables with $0 \leq X_i \leq 1$ you can apply a Chernoff bound. A good reference is "Concentration of Measure for the Analysis of Randomized Algorithms" ($URL$ 

In the context of Turing Machines, "non-deterministic" really means "parallel". A randomized algorithm can randomly explore the branches of the computation tree of a non-deterministic Turing machine, but a non-deterministic Turing machine can explore them -all- at the same time, which is what gives it its power. In other contexts (I can't tell from your quote if you are talking about Turing Machines), a randomized algorithm might intentionally be using randomness, whereas an algorithm that you wanted to be deterministic might end up exhibiting non-determinism because of a bug... In response to your edit, when people say "choose an element from a set non-deterministically", its possible they might just mean "randomly". However, it is also possible that they mean "magically choose the -right- element from the set". A common way to view non-deterministic turing machines is that they first magically "guess" a solution, and then check its correctness. Of course, you can view this magic guess as just the result of checking all possibilities in parallel. 

One that I like is sometimes called a "Coarse Correlated Equilibrium". This is actually the limiting set of efficient "No-Regret" dynamics. These have several nice properties, not least of which is that they can be reached by efficient, de-coupled dynamics, and include Nash equilibria as a special case (so are ``strictly more plausible'' as a prediction of behavior). What might make them somewhat similar to what you are asking about, is that these learning dynamics need not ever converge to a fixed point -- indeed, they may cycle forever. Nevertheless, it is often possible to bound the fast convergence of social welfare under these dynamics (i.e. price of anarchy over coarse correlated equilibria), and whats more, often the social welfare is no worse over coarse correlated equilibria than it is over Nash equilibria. Some relevant papers: $URL$ $URL$ $URL$ 

Mechanism design is basically just algorithm design, where you don't control the inputs: instead, you assume that the inputs are controlled by different agents, who each have their own set of feasible actions, and their own utility function over outcomes, and are acting to maximize their own utility (and not yours, as the mechanism designer). So you are constrained in how you can design your algorithm, because you have to design it in such a way so as to manage the incentives of the agents so that they will want to share their data with you. The classic example is designing an auction for a good in such a way so that the bidders will reveal to you how much they really value the good. So the fundamental characteristic in mechanism design is that you have agents who will act independently of the mechanism to further their own goals. Its hard to tell from your description, but it sounds like you simply have an algorithms problem. Possibly your problem is to design an algorithm to compute an equilibrium of some sort, but this would not typically be thought of as mechanism design. 

For example, the graph cut function $$f(S) = \sum_{(u,v) \in E : u \in S, v \not\in S}w((u,v))$$ is an interesting property of subsets of vertices, but cannot be efficiently maximized. The edge density function is another example of an interesting property that alas, cannot be efficiently maximized. I'm looking for functions that are equally interesting, but can be efficiently maximized. I'll let the definition of "interesting" be somewhat vague, but I want the maximization problem to be non-trivial. For example it should not be that you can determine the answer without examining the edges of the graph (so constant functions, and the cardinality function are not interesting). It should also not be the case that $f$ is really just encoding some other function with a polynomially sized domain by padding it into the domain $2^V$ (i.e. I don't want there to be some small domain $X$, and some function $m:2^S\rightarrow X$ known before looking at the graph, such that the function of interest is really $g:X\rightarrow \mathbb{R}$, and $f(S) = g(m(S))$ If this is the case, then the "maximization" problem is really just a question of evaluating the function on all inputs.) Edit: Its true that sometimes minimization problems are easy if you ignore the edge weights (although not minimizing the cut function, since I allow negative edge weights). But I'm explicitly interested in maximization problems. It does not become an issue in natural weighted problems in this setting though. 

Estimating the volume of a convex polytope and the closely related task of sampling from it have applications in private data release. Roughly, the problem you want to solve is: given a collection of numeric valued queries on a database, come up with answers to those questions that are as close as possible to the real answers, while satisfying differential privacy. In some range of parameters, the optimal algorithm for solving this problem has a geometric description, and implementing it involves sampling from a convex polytope. See here: $URL$ 

Here is an example that may not be exactly what you want, but is still emblematic of a general class of problems. In this instance, the hardness is only for agnostic learning, and only for "proper" learning in the sense that the representation class is restricted, and the easyness over the uniform distribution is for trivial reasons. Width-k conjunctions are NP hard to learn using halfspaces (and in particular using conjunctions) over arbitrary distributions, even for $k > \log(n)$: $URL$ On the other hand, for $k \gg \log n$, width $k$ conjunctions are trivial to learn over the uniform distribution over examples using conjunctions, since every width-k conjunction will label a $1-1/2^k$ fraction of examples "0", so every width-k conjunction is a good hypothesis. I would guess that there are similar examples that hold more broadly. Notice that easyness of learning over the uniform distribution holds for any "unbalanced" function, for the same trivial reason. Take a class of functions that is hard to learn even non-agnostically (poly-sized circuits, say). I would think that you could modify the lower-bound proof to hold even if you modify the circuits to be unbalanced: to evaluate to $1$ only on an $\epsilon$ fraction of random inputs. Meanwhile the hard distribution could still be one in which $\Pr[A(x) = 1] = 1/2$. 

Say that I have a set of $n$ points $N$, and am interested in metrics $d:N\times N \rightarrow \mathbb{R}$ over $N$. Let $M$ denote the set of all metrics over $N$. Now let me define the distance between two metrics $d_1$ and $d_2$ in $M$ to be: $$\partial(d_1, d_2) = \left|\sum_{i,j \in N}d_1(i,j)-d_2(i,j)\right|$$ It isn't hard to see that $(M, \partial)$ itself forms a metric space. I am interested in the size of the smallest $\epsilon$-net of $(M, \partial)$. (i.e. the smallest subset $S \subset M$ such that for all $d \in M$ there is some $d' \in S$ such that $\partial(d, d') \leq \epsilon$.) Are bounds on this quantity known, and/or are there standard techniques for estimating quantities like this? EDIT: As Suresh points out, there is no finite $\epsilon$-net if we are talking about unbounded metrics. Let us consider normalized metrics $M$ such that for all $d \in M$, and for all $i,j \in N$, $d(i,j) \leq 1$. Of course now for all $d_1,d_2 \in M$, $\partial(d_1,d_2) < n^2$. 

If the learning algorithm is proper (i.e. it always produces a hypothesis from the class $F_n$), then it also gives a testing algorithm -- simply run the learning algorithm, and see whether the hypothesis it produced has error rate $<\epsilon$, which can be done with only $\approx 1/\epsilon^2$ samples. If it does, since the hypothesis is in $F_n$, this is a constructive proof that the function you are testing has distance at most $\epsilon$ from $F_n$. If the algorithm was a PAC learning algorithm for $F_n$, then when $f \in F_n$, it must generate such a hypothesis. So any proper learning algorithm can be converted to a testing algorithm with only an additional $\approx 1/\epsilon^2$ samples at most. Moreover, if you are only worried about sample complexity and not computational efficiency, then without loss of generality you can always use a proper PAC learning algorithm. Since the sample complexity of learning is $\mathrm{VCDIM}(F_n)/\epsilon^2$, this means you can always test with at most this many samples. However, generally testing is easier than learning. For example, linear functions in $d$ dimensions require $d$ samples to learn, but only a constant number of samples to test. 

Say that I have a weighted graph $G = (V,E,w)$ such that $w:E\rightarrow [-1,1]$ is the weighting function -- note that negative weights are allowed. Say that $f:2^V\rightarrow \mathbb{R}$ defines a property of any subset of the vertices $S \subset V$. 

No class is too simple! I know that even monotone conjunctions are not known to be agnostically learnable over arbitrary distributions, so I'm just looking for nontrivial classes of functions. 

No, no-regret dynamics do not converge to Nash equilibrium in general games, and its not hard to think of examples. On the other hand, no regret dynamics do always converge to coarse correlated equilibrium in any game, and no-internal regret dynamics always converge to the set of correlated equilibria. They are also known to converge to Nash equilibrium in several special cases of n-player games: for example, separable zero sum games, congestion games, and a few others. In general, there are no efficient learning dynamics that converge to Nash equilibrium though. Not only is computing a Nash equilibrium PPAD hard, but even ignoring computation, for any "decoupled learning dynamics" (among which no regret dynamics are a special case) must exchange exponential communication before finding a Nash equilibrium in general games. 

This is obviously a very open ended question, but so as to stay on topic, here is one CS theory approach to the idea of "fairness" and how to enforce it. "Fairness Through Awareness" Dwork, Hardt, Pitassi, Reingold, Zemel $URL$ 

Check out the figure that accompanies Adam Kalai's 1 page SODA paper, "Generating Random Factored Numbers, Easily": link 

To continue along the lines of Deigo's answer, standard sample complexity bounds from learning theory tell you that if you are satisfied with finding a program which is "approximately correct", you don't need to try very many points at all. Lets say we are encoding programs in binary, so that there are only $2^d$ programs of length d. Lets suppose also that there is some distribution over input examples $D$. Perhaps your goal is to find a program which you are pretty sure is almost right ("Probably Approximately Correct" i.e. as in Valiants PAC learning model). That is, you want to run an algorithm that will take in a small number of samples $x \sim D$ together with $f(x)$, and will with probability at least $(1-\delta)$ output some program $P$ which agrees with $f$ on at least a $(1-\epsilon)$ fraction of inputs drawn from $D$. We will simply draw $m$ examples $x \sim D$, and output any program $P$ of length $\leq d$ that agrees with $f$ on all of the examples. (One is guaranteed to exist since we assume $f$ has Kolmogorov complexity at most $d$)... What is the probability that a particular program $P$ that disagrees with $f$ on more than a n $\epsilon$ fraction of examples is consistent with the $m$ examples we selected? It is at most $(1-\epsilon)^m$. We would like to take this probability to be at most $\delta/2^d$ so that we can take a union bound over all $2^d$ programs and say that with probability at least $1-\delta$, no "bad" program is consistent with our drawn examples. Solving, we see that it is sufficient to take only $$m \geq \frac{1}{\epsilon}\left(d+\log 1/\delta\right)$$ examples. (i.e. only linearly many in the Kolmogorov complexity of $f$...) BTW, arguments like this can be used to justify "Occam's Razor": given a fixed number of observations, among all of the theories that explain them, you should choose the one with lowest Kolmogorov complexity, because there is the least chance of overfitting. Of course, if you only want to check a single fixed program in this way, you only need $O(\log(1/\delta)/\epsilon)$ examples...