In general, your approach will get stuck in local minima. This is why it is not scientifically accepted. (Notice that this may be different in very special cases, in particular if the performance of the algorithm is a strictly convex function of all input parameters). To see how the approach fails, suppose your machine learning algorithm has two parameters, $x$ and $y$, which can be either $0$ or $1$. The default values are $x=1$ and $y=1.$ The performance of your machine learning algorithm is $f$ and should be as high as possible. Assume the following performance levels $f(x,y)$: 

What does it mean that squared Euclidean distance gives the same ranking as Euclidean distance? Suppose we have $x, y, z$ such that $d(x,y) < d(x,z)$. Then $d(x,y)^2 < d(x,z)^2$ as well: It ranks points in the same way as Euclidean distance. This is good to know. For instance, this tells us that the $k$-nearest neighbors classifiers gives the exact same results for squared Euclidean distance and Euclidean distance. 

3. Predicting via k-nearest neighbors and averaging We now have a notion of distance between point sets. This makes it possible to use k-nearest neighbors classification: Given a test point set, we find the point sets in our training sample that have the smallest Hausdorff distance relative to the test point set, and obtain their labels. Now comes the second problem: How do we turn these labels into a prediction for the test point set? I took the simplest approach: average the labels and predict the point in the test point set that is closest to the average. 

This is not terribly surprising: LSTMs should be fed with normalized data! So I normalized the data by rescaling it to the interval $[-1, 1]$. Phew, things are fine again: 

2. Comparing point sets via Hausdorff distance Let us tackle the first problem: How should we compare different point sets? The number of points in the point sets is different. Also remember that the order in which we write down the points should not matter: Comparing to the point set should yield the same result as comparing to the point set . My approach is to compare point sets via their Hausdorff distance: 

We know that Euclidean distance is a metric. Let us check whether squared Euclidean distance is also a metric. I will use the definition from Wikipedia (Ankit Seth's definition is equivalent). 

Datum is latin for 'a given', and data is the plural, meaning 'the givens.' If you want to be precise, you should refer to a single observation as a datum and multiple observations as data. Notice that the latter is a plural word: 'The data are [not is] hard to interpret.' But usage has changed. People, especially in Data Science, usually seem to talk about 'a single data point' and say that 'the data is hard to interpret.' See this discussion for example. My recommendation: As long as you are talking to Data Scientists and IT guys, ban datum from your vocabulary and use data with the singular. Reconsider once you are a professor in Oxford. 

Using this function via brings down prediction accuracy to 45%. This shows how important it is to think about the decision rules that generate your labels. If you have an idea why people choose certain points, this will help you find the best algorithm. Some ways to improve this algorithm: (1) Use a different distance function instead of Hausdorff distance, (2) use something more sophisticated than k-nearest neighbors, (3) improve how the selected training labels are turned into a prediction. 

You could treat your results as an undirected graph with weighted edges. Your nodes are A, B, C, etc. and your vertices are the connections between items, weighted by similarity value. Here is a Python function that could be a great starting point. It draws thicker connections for vertices with a larger weight. I copied it and replaced the example data with your data points above: 

Yes, this is overfitting. Financial time series exhibit many peculiarities, including heteroscedasticity, high tail risk, all kinds of seasonality and all kinds of momentum. Machine learning algorithms tend to pick up these patterns and overfit on them. To my knowledge, there is no machine learning algorithm that can compete with classical time series analysis when it comes to understanding stock prices. Please also keep in mind that if someone were able to make predictions like 'tomorrow the stock price is going to rise by 5%', they could become a billionairy within a few weeks. Edit: You ask if there is any chance of training a network that is better than random guessing. Yes, for two reasons. First, if you try hundreds of parameter combinations and evaluate them all on the same test set, you become the victim of what might be called 'overfitting of the second degree.' Performance on the test set becomes part of your decision process (because you throw away models that perform poorly on the test set and keep models that perform well). Thus you are essentially training on the test set. If you keep doing this long enough, you will achieve 100% accuracy on the test set. But use this seemingly perfect model on new, previously unseen test data, and you will be disappointed. Second, heteroscedasticity, seasonality and momentum all introduce a little bit of 'real' predictability into your time series. If you get very lucky, your algorithm might pick up on these patterns and at the same time ignore all the weird noise that stock prices tend to exhibit. This is incredibly unlikely, and it is probably irrelevant. You should not compare your prediction accuracy to random guessing, but to a reasonable baseline model. For monthly prices of individual stocks, this could be the Fama-French three-factor model (models for daily or intraday stock prices will send you a bit deeper into the rabbit hole). I bet you five dollars that you will not be able to beat that model's performance on previously unseen data. My advice to you: Don't clean the lion's cage on your first day as a zookeeper. Start with something nice and easy. 

Multiplication is not a linear operation. Your linear SVM constructs a (hyper-)plane $$ w_0 = w_1 x_1 + w_2 x_2 $$ for some weights $w_0, w_1, w_2.$ By introducing the AND-feature, you add another dimension: $$ w_0 = w_1 x_1 + w_2 x_2 + w_3 x_1 x_2. $$ It might well be that your two-dimensional data set is not linearly separable, but the three-dimensional data set is. A small addition: Would adding the OR-feature increase performance even further? No, because it is a linear combination of the other three features: $x \vee y = x + y - (x \wedge y)$ where $\vee$ is OR and $\wedge$ is AND. 

This does not sound like a machine learning problem. It seems to me that you could come up with a list of rules that determine whether two files belong together. As S van Balen said, you should take a look at the and packages in Python. (If you are on Linux, a Bash script would probably be much more elegant than a solution in Python). The following links might be helpful: 

I am not entirely sure if I understand your question correctly. What result do you want for the following dataframe: because there are 2 different NaN-combinations in 5 rows, or because there are 4 changes in the index composition in 5 rows?