As the game continues to scroll, both models are onscreen, giving the illusion of a seamlessly tiling world: 

Since the unit circle has a radius of one, the length of the vector will also have a length of one, so it's already normalized for you. 

It's not true that "every game ever made" uses the reloading mechanic you've described. I can think of least one (Red Orchestra) that didn't, and others which I believe didn't as well (Day of Defeat, Rainbow Six). But that's not the point. This is simply a game mechanics decision, often made because the designers felt that the complexity introduced by a more complex or realistic reloading mechanic would not add any additional fun to the game (and in fact might actively annoy players). It's not all that dissimilar from the choice not to require characters to feed themselves, or stop for bathroom breaks, in most games. Thus, the decision was made to opt for a simply mechanic that eschews realism in favor of a system that treats every gun as having a giant continuous "hopper" of bullets you just dump your spares into when you reload. If you think you can build an interesting, fun mechanic by forcing players to be more careful with their reloads (and I think it's totally doable), then by all means go for it. 

Generally, in a stateful graphics API, a material's settings will apply to anything rendered after the material state is set to the API. Whether or not the effect of that material will be per-vertex or per-pixel depends on the actual vertex and fragment shading programs that execute for the rendering of each individual primitive. If you're using the materials built in to fixed function D3D9 pipeline, then the resulting object color is vertex-oriented, but the material's impact will be apparent for every object you render until you change the material. Writing your own material system can allow you to embed material data into the vertex attributes, if you so choose (either directly into the vertex stream or indirectly via texture lookups) and thus have finer control over the material properties of an object without having to render that object in multiple draw batches just to swap out material data. Materials rarely apply to bones in any significant fashion (although that might make for an interested system to write: assigning materials to bones, blending material effects using the vertex weights). 

This depends on the kind of animation you're talking about. There's really no advantage here if you're just moving the text around, rotating it, scaling it, making it go in sine wave patterns, et cetera. If you want to deform the actual glyphs in some interesting and unusual way, then sure. 

This was done via a special deal with Apple, and cannot be done in general. One way you might handle the situation is to make the game free on the store, but have most of the gameplay behind an in-app purchase paywall (such that your app essentially serves as a free demo that allows you to buy the upgrade to the full game). Then, you'd correlate all the FB likes with the Apple IDs of the corresponding FB users and issue the tokens for the upgrade IAP to those Apple IDs. Unfortunately, there is no mechanism to do that Facebook-to-Apple-ID join, so you'd need to do it manually or build your own webservice that users used which would collect their information, do the "like" on their behalf, and then store the correlated IDs in a database you could process. But even then, there isn't a way for you to issue tokens for IAPs (or apps themselves) arbitrarily. You can request promotional codes from Apple which could you then distribute to FB likers, but the number of promotional codes you can acquire is pretty limited, and unless you expect your total userbase to be well under that limit (less what you'd want to use for actual promotional purposes), doing this would probably be a terrible idea. 

A more general way to handle this would be to insert an abstraction layer between the animation and the frames of the animation on your sprite sheet. Right now, you are just assuming that an animation consists of the sequential playback of frames 1 through frame N of some sprite sheet, and now you'd like to change that to the meaning is "frames 1 through N, and then frames N - 1 through 1 in reverse." floAr's answer (introduce a variable indicating whether the animation is traversing frames forward or backwards) is perfectly acceptable and will do exactly what you want. What I propose is a more generalized approach where an animation is a list of arbitrary frame numbers from the sprite sheet (instead of the implicit sequential list you are currently working with). This requires additional data and as such as a slight overhead in terms of memory and content creation time. To accomplish this, you'd read the list of frames (which I will call an "animation sequence") from a data file (which can be as simple as a text file), which might look like this: 

Have you considered an arcball implementation? They are relatively straightforward and produce reasonably intuitive means of manipulating objects in 3D space. Essentially, you project the mouse coordinates from the current and previous frame onto an imaginary sphere that surrounds your object (in this case, it is a sphere, so there's that) and then compute the shortest arc quaternion between the two points. You can then convert that quaternion to a matrix to apply it. Do note that APIs choose slightly different conventions for quaternion representations and your math may need to be adjusted to account for that (I don't know what convention jMonkeyEngine employs). 

Don't make the listener inside the object static. A static class member means the object is shared across all instances of the type (all instances of in this case). That's not want you want here: you want each to have it's own listener, so client code can listen to the individual world or worlds it cares about. 

Per-pixel rendering operations are not particularly efficient on modern hardware. In the past, on classic game consoles and older PCs, they could be made usefully efficient because you had direct access to video RAM. On modern machines this is no longer the case. Consequently, if you want to do per-pixel manipulation efficiently, you are better off constructing your own in-memory buffer of the pixel data (something as simple as a pointing at a buffer of bytes, assuming you want four-component RGBA pixels) and directly manipulating that. At the conclusion of every frame, you then use a modern, efficient method to render this buffer to the screen. Typically, these days, that would involve updating a GPU texture with your pixel buffer and rendering a fullscreen quad with that texture mapped accordingly. Further, when actually manipulating the pixels, it's better not to actually set each pixel individually, if possible. Most of the time you'll be rendering objects that consist of more than one pixel, and in those cases you should use specialized routines to blit the data. Focus on accessing the memory of the destination buffer (and source buffer, if you are copying a sprite) in a cache-coherent fashion. Copy entire rows of data when possible (for opaque sprites, for example, you can the rows of the sprite). Another technique commonly used when doing this kind of thing is to use "dirty rectangles," that is, tracking which regions have changed and only re-rendering those (the previous frame bounds of a sprite, the current frame bounds of a sprite, for example -- or their union if they overlap, as they commonly will). This will generally also involve keeping a separate buffer containing all the static background imagery in a scene, so you can then (for each dirty rectangle), blit the appropriate portion of the background into the rectangle, then do the appropriate foreground drawing. 

The server should maintain all the positions to "make sure" anyway -- this is how you prevent trivial cheating. Most browsers have a "debug mode" they can be put into. It's trivial for me to hit F12 on my copy of IE, for example, and have access to a JS debugger I can use to halt execution, inspect variables, and change them. So you should be aware that a client could potentially do that. Even without that sort of effort on the end-user's part, you shouldn't rely on your simulation being in exactly the same state at exactly the same time on all machines; that level of synchronization is basically improbable to achieve for real-time games, since you are dealing with the internet and all the latency involved, floating point inaccuracies, clock drifts, et cetera. Most of the time things will be "good enough" that you can do what you are asking and send only the updates to the client, allow the clients to simulate locally (this is good, in fact, because it allows you to hide some of the latency and make your game feel better) but you should still simulate everything on the server and periodically ensure the client and server are in sync. You should not trust client state, ever (not just for browser-based games either). 

There are almost zero formal jobs in the industry for that kind of educational background, I'm afraid. The most likely place you'd find a job that utilizes those skills would be in the economics analysis areas occasionally employed by studios that ship large MMOs. Those "departments," if they exist at all, are typically very small (often only a single person) in size and you won't likely be getting that kind of job right out of college. Because the economics of online computer games differ somewhat from the economics of reality, studios that have those positions will prefer senior people who also have other experience in the games industry. I hate to break it to you, but in that position you often won't have that much creative input either. Of course this varies from studio to studio, but I wouldn't hold my breath. Additionally, you seem to be somewhat out of touch with the reality of the games industry, which is one fraught with crunch and unpaid overtime. If you're looking for a leisurely life in this field, you probably won't find it easily. It exists, but it may take you a long time to get there. This isn't to say that you won't be able to get a job in the industry. Actual game designers tend to have a wide variety of backgrounds, because that helps bring a sense of well-roundedness to the entire team. You should know that in a position as a game designer, you may end up calling on the skills you learned in those college programs occasionally, but they won't be a part of the majority of your day-to-day tasks in all likelihood. 

I prefer the "exit" versions since they more strongly imply the destructive nature of the operation the user would perform. You could also just use "Exit to Title" if you prefer brevity, or just overload "exit game" and use it in-game to return the title screen. If you are worried that overloading the term might confuse users, you could provide some kind of tooltip or help prompt -- either when the option is selected or hovered over, or in a confirmation dialog, which you can bake together with the typical notice about unsaved data being lost. For example, if the user selects "Exit Game" during gameplay, a prompt can appear reading "You are about to exit the game and return to the title screen. All unsaved data will be lost. Do you want to exit?" The actual verb "quit" in UI is far more common on the Mac (and apparently PS3) than it is on Windows or Xbox. The latter platforms tend to prefer "exit" or some variant thereof. 

There probably is market research data available that one could use to draw correlations to a renaissance of the adventure game genre. Such data would hardly constitute proof, though. The best proof would be in the sales and profit/loss numbers directly from the publisher or developer (which aren't typically released, unless they are buried amidst the earnings reports of large public publishers). The market research data in question is generally not freely available (sometimes a subset of it is), but you can contact various market research firms (these guys, these guys, or these guys for example) to see about purchasing some. 

You probably don't want to physically remove the device to stimulate the error, as most consumer PCs are not set up to safely hot-swap that way. Similarly it's not that easy to make the device crash and have to reset. Toggling GPUs is easy enough, usually, if you have them and have the ability to force the machine to use one or the other, but that's pretty hardware-dependent. So you should update your drivers to trigger the error. Downgrading drivers should work as well, so you can just toggle between two driver versions while your code is executing to make sure you react correctly. 

Generally, one would try to create the device with hardware support and if the creation of that device failed, fall back to software. As long as you handle the failure appropriately, there's no particular danger in "checking" for vertex processing capabilities this way and it is less error prone in weird edge cases than using the method, for the reasons you outlined. 

There are a few ways to manipulate individual pixels in 3D graphics APIs. The simplest is to render a 1x1 primitive. Contrary to what you are assuming, this is not that inefficient. Rendering primitives (triangles) is what the 3D graphics API and hardware are designed to do. If you're already rendering a bunch of geometry anyway, a few hundred extra triangles isn't going to cause a significant impact. You can use simplified vertex formats and shaders to further ameliorate what little impact there is. If you have a relatively few number of pixels to manipulate and each pixel could be manipulated more-or-less independently, rendering small quads or triangles is a reasonable approach. If you have many pixels to manipulate, or the manipulation requires extensive access to neighboring pixel data or other CPU-side data, the next most-typical approach is usually to render the scene normally first, into a render target texture, and then map that texture on the CPU and manipulate the appropriate pixels. Finally, you render a full-screen quad containing the manipulated texture to the screen. There are several variations to this technique. For example, if you don't need to read the existing contents of the framebuffer you can eschew that step and just manipulate CPU-side bits of a fullscreen texture, rendering that over top your normal scene using alpha blending. This approach or some slight variation thereof is what you may want to consider if you have complicated needs. There are APIs, particularly in OpenGL, that let you "directly" manipulate the pixels of the frame buffer (). These aren't generally a good idea, because unlike rendering 1x1 triangles, this is not an operation the GPU is optimized for, and reading back the framebuffer incurs not only the usual performance hit for reading data from the GPU, but a huge pipeline stall because the GPU can't concurrently manipulate the framebuffer while you are reading it and writing it back. I would recommend you start with the 1x1 triangle approach, since unless you're already doing framebuffer readbacks or fullscreen quad passes, it will likely integrate into your rendering flow very easily. You can then verify if the performance is acceptable before possibly moving on to one of the more complicated options.