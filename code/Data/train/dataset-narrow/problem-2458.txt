Motivation Assuming $\mathsf{P}\ne\mathsf{NP}$, it is impossible to efficiently decide membership in an NP-complete language. I would like to assign probability to such membership, in some sense. Definitions The following definitions use the approach to average-case complexity given in e.g. O. Goldreich "Computational Complexity" chapter 10. Consider $(L, X_n)$ an average-case decision problem where $L$ is a language and $X_n$ is a family of random variables which can be sampled by an algorithm with polynomial running time. Consider $A$ an algorithm computing for each input string $x$ an infinite binary fraction in $[0, 1]$ (informally regarded as a probability). We call such algorithms "estimators". $A$ is called efficient when the time of computing $k$ digits of $A(x)$ is bounded by $p(|x|, 2^k)$ with $p$ polynomial. Define the error function to be $$\epsilon_n(A; L):=-E_{X_n}[\chi_L(x)\ln A(x) + (1-\chi_L(x))\ln(1-A(x))]$$ where $\chi_L(x)$ is the characteristic function of $L$. It is easy to see that assuming $\mathsf{sampP}\ne\mathsf{sampNP}$, for $(L, X_n)$ which is $\mathsf{sampNP}$-complete there is a polynomial $q$ s.t. $$\forall n \exists m > n:\epsilon_m(A; L) > \frac{1}{|q(m)|}$$ Define $A$ to be optimal for $(L, X_n)$ when it is efficient and for any efficient $B$ and polynomial $r$: $$\forall n >> 0:\epsilon_n(A; L) - \epsilon_n(B; L) < \frac{1}{|r(n)|}$$ Question 

Also, consider the same question for the extensive-form game with perfect recall that results when we allow each player to remember history, i.e. each player's moves can depend both on the current observable part of the position and the history of observable positions and moves for the same player. Note that in this case the game is known to be solvable in time polynomial in $2^{(m+n)N}$ due to the result of Koller and Megiddo. 

Note that, as opposed to the tree case, I don't even know whether there is a Nash equilibrium in behavioral strategies. As another variant, instead of labeling the terminal nodes by payoffs we can label every edge by a payoff so that the total payoff is a sum over traversed edges (this is similar to the games defined by McMahan and Gordon). For trees this doesn't matter but for DAGs separating vertices in order to remember accumulated payoff can again cause an exponential expansion (although if we're willing to settle on approximating with $\frac{1}{poly(n)}$, this expansion can probably be contained). If the problems above are likely to require superpolynomial time, I am also interested in the special cases in which one of the players has perfect information (the case where both players have perfect information is solvable by backward induction). 

$K$ is at most $O(n)$ since we can construct a program which encodes the longest-running program of length $n$ and checks whether the input program halts before that one. $C(n)$ is at most $O(2^n)$ because we can construct a "look-up table" circuit and $T(n)$ is at most $O(2^n)$ as noted above What other bounds on $K$, $C$, $T$ can be found? In particular, are $C$ and/or $T$ less than exponential? EDIT: Actually, $K(n) = n + O(1)$. To see this, consider $H_n$ an algorithm solving the halting problem for all inputs of length at most $n$ and $P$ the following program. $P$ runs $H_n$ on $P$ itself. If result is "halts", it goes into an infinite loop. If result is "doesn't halt", it terminates. $H_n$ fails to evaluate $P$'s halting correctly therefore the length of $P$ is greater than $n$. On the other hand $P$ is only longer than $H_n$ by a constant so $H_n$ can't be much shorter than $n$. EDIT: If the halting problem is in $P/poly$ i.e. $C$ is polynomial, then $NP \subset P/poly$ (which implies $PH = \Sigma_2$). To see this consider $S \subset \{0,1\}^*$ a decision problem in $NP$ and $V$ a verifier program for $S$. Deciding whether $x \in S$ is equivalent to solving the halting problem for the following program $Q_x$: "Loop over all $p \in \{0,1\}^*$, halt if $V(x,p) = 1$". The size of $Q_x$ is the same as the size of $x$, up to a constant. Therefore if we can solve the halting problem for $Q_x$ in polynomial time with polynomial advice, we can decide $x \in S$ in polynomial time with polynomial advice Note that $C$ is polynomial iff $T$ is polynomial. Consider $R_n$ a family of circuits solving the halting problem. Then we can construct an infinite-advice program $H$ for solving the halting problem by encoding $R_n$ as advice. This yields $$T(n) = O(n \, C(n) \ln C(n))$$ On the other hand if we have $H$ an infinite-advice program solving the halting problem, we can construct a circuit $R_n$ representing the computation process of $H$ on an input of size $n$. The size of this circuit is the product of the spatial complexity by the temporal complexity so $$C(n) = O(T(n)^2)$$ EDIT: If the halting problem is in $coNP/poly$ then $NP \subset coNP/poly$. This is due to reasoning similar to above i.e. an existential quantifier can be replaced by a universal quantifier at the cost of requiring polynomial advice. I think this also implies some kind of collapse of the polynomial hierarchy EDIT: It is possible to construct a specific infinite-advice algorithm of optimal complexity, analogous to Levin search for $NP$ problems. As opposed to the case of $NP$, there is no way to verify correctness of solutions, on the other hand it is possible to restrict the dovetailing only to valid programs. This is done by encoding all programs which solve the halting problem together with their respective infinite advice sequences in the infinite advice of our algorithm. The penalty incurred by using this encoding is at most polynomial, hence the resulting algorithm has complexity which is optimal up to a polynomial 

Define a function $u:\lbrace 0,1 \rbrace^* \rightarrow \mathbb{R}$ to be an asymptotic optimization problem when the following conditions hold: 

Fix $X \subset \lbrace 0,1 \rbrace^* \times \lbrace 0,1 \rbrace^*$ an NP-complete search problem e.g. the search form of SAT. Levin search provides an algorithm $L$ for solving $X$ which is optimal in some sense. Specifically, the algorithm is "Execute all possible programs $P$ in dovetailing on the input $x$, once some $P$ returns answer $y$ tests whether it's correct". It is optimal in the sense that given a program $P$ that solves $X$ with time complexity $t_P(n)$, the time complexity $t_L(n)$ of $L$ satisfies $$t_L(n) < 2^{|P|}p(t_P(n))$$ where $p$ is a fixed polynomial which depends on the precise computation model $L$'s optimality can be formulated in a somewhat stronger way. Namely, for every $M \subset \lbrace 0,1 \rbrace^*$ and $Q$ a program solving $X$ with promise $M$ in time $t^M_Q(n)$, the time complexity $t_L^M(n)$ of $L$ restricted to inputs in $M$ satisfies $$t_L^M(n) < 2^{|Q|}q(n, t^M_Q(n))$$ where $q$ is a fixed polynomial. The crucial difference is that $t^M_Q(n)$ can be e.g. polynomial even if $P \neq NP$ The obvious "weakness" of $L$ is the large factor $2^{|Q|}$ in this bound. It is easy to see that if there is an algorithm satisfying a bound of the same form with $2^{|Q|}$ replaced by a polynomial in $|Q|$ then $P = NP$. This is because we can take $Q$ to be a program solving some given instance of $X$ by hard-coding the answer. Similarly, if $2^{|Q|}$ can be replaced by a sub-exponential function of $|Q|$ then the exponential time hypothesis is violated. However, the answer to following question is less obvious (to me): 

It is well known that $\mathsf{NL} \subseteq \mathsf{NC} \subseteq \mathsf{P}$, both inclusions conjectured to be proper. On the other hand $\mathsf{NP} \supseteq \mathsf{P}$, also probably a proper inclusion. There are classes naturally interpolating between $\mathsf{NL}$ and $\mathsf{NP}$, namely $\mathsf{NSC}^k$, languages decidable by non-deterministic Turing machines in simultaneous polynomial time complexity and $O(\log^k n)$ space complexity. How big are these classes? We have $\mathsf{NSC}^k \subseteq \mathsf{polyL}$ since $NSPACE(O(\log^k n)) \subseteq DSPACE(O(\log^{2k} n))$ 

$K(n)$ is the Kolomogorov complexity of the string $h_{<2^n}$ of length $2^n$ whose $k$-th bit is 1 iff the $k$-th program halts $C(n)$ is the minimal size of a Boolean circuit solving the halting problem for programs of size at most $n$ $T(n)$ is the time complexity of the halting problem made solvable by introducing an extra-tape into our Turing machine on which an infinite bit-string is written in the initial state. For example the bit-string can be an infinite look-up table: the $k$-th bit is 1 iff the $k$-th program halts. This allows a simple look-up algorithm to solve the halting problem but the complexity would be $O(2^n)$ 

A universal computer is a program that can execute any other program. It is interesting to ask whether there are "booster" computers that execute programs faster than they execute "on their own". In the simplest sense of the question, the answer is negative, as can be seen by the following argument. Suppose $V$ is a universal computer. Consider the following program $P$: "Run $V$ on my own source code. If it halts in less than $n$ steps, produce output which is different than the output of $V(P)$. Otherwise produce the output '0' (say)". It is clear that $V$ will take at least $n$ steps to execute $P$ and $P$ will execute in approximately $n$ steps on its own. So $P$ and $V(P)$ take approximately the same time. However, what happens if we allow a pre-computed cache of arbitrary size? Formally, we consider a universal Turing machine with the addition of a special read-only tape on which an infinite computable bit-string $s$ is written in the initial state. Fix $U$ an "ordinary" universal computer program, i.e. $U$ doesn't use the special tape. The question is then 

Consider $l,m,n,N \in \mathbb{N}$ and circuits $C: \{0,1\}^{l+m} \rightarrow \{0,1\}^l$, $D: \{0,1\}^{l+n} \rightarrow \{0,1\}^l$. Consider the following zero-sum two-layer extensive-form game with perfect information: 

The following seems to me like a natural definition and I wonder whether it's been studied somewhere Consider $\mathsf{X} \subset 2^{\lbrace 0, 1 \rbrace^*}$ a set of languages. Then $K \subset \lbrace 0, 1 \rbrace^\omega$ is called "$\mathsf{X}$-verifiable information" when there is $L \in \mathsf{X}$ s.t. (i) Given $x \in L$, every prefix of $x$ is in $L$ (ii) Given $f \in K$, every prefix of $f$ is in $L$ (iii) Given $f \notin K$, the length $n$ prefix of $f$ is outside $L$ for $n >> 0$ For example $\lbrace f \rbrace$ is $\mathsf{R}$-verifiable information iff $f$ is computable. This can be seen by constructing an algorithm which runs the verification on all strings of length $n$ and collects the prefixes of length $m$ of those strings which passed the verification. For $n >> m$, the only prefix which remains is the correct one However if $K$ is $\mathsf{R}$-verifiable information it doesn't mean every $f \in K$ is computable: for example consider $K = \lbrace 0, 1 \rbrace^\omega$ A non-trivial example of $\lbrace f \rbrace$ which is $\mathsf{P}$-verifiable is as follows. Consider $L \in \mathsf{NP} \cap \mathsf{coNP}$ and let $f$ be an encoding of $L$ together with the corresponding $\mathsf{NP}$ and $\mathsf{coNP}$ witnesses (i.e. for each $x \in \lbrace 0, 1 \rbrace^*$, $f$ encodes either an $\mathsf{NP}$-witness proving $x \in L$ or a $\mathsf{coNP}$-witness proving $x \notin L$) 

The following is a way to represent two-player zero-sum games in extensive form. Consider a directed acyclic graph $G$ where each non-terminal vertex is one of 3 types: player 1 vertex, player 2 vertex or chance vertex. There is a designated initial vertex $v_0$. The vertices of each player are divided into information sets. The edges outcoming from player nodes are labeled by actions, s.t. vertices in the same information set have the same set of actions and vertices in different information sets have disjoint sets of actions. The edges outcoming from chance nodes are labeled by transition probabilities. The terminal vertices are labeled by payoffs. We say that such a game has "perfect recall" when knowing the sequence of actions that player $i \in \{1,2\}$ took before reaching a given information set $U$ of player $i$ tells us nothing about which particular vertex $v \in U$ we reached, i.e. all vertices in $U$ have the same possible histories from the perspective of the given player. Koller and Megiddo have established that zero-sum two-player games in extensive form with perfect recall can be solved in polynomial time (i.e. we can find a Nash equilibrium in behavioral strategies). This assumes the game is given as an explicit tree. Now, unpacking the DAG into a tree can obviously increase its size exponentially. The question is thus: 

One of the most celebrated results in computer science is that the halting problem is undecidable. However there are still notions of complexity that are applicable. Here are 3 that I have in mind: 

A computable predictor is an algorithm $A$ computing a function $f_A : \{0,1\}^* \rightarrow \{0,1\}$. We regarding the function as providing a predicted continuation of a finite binary sequence. We define an infinite binary sequence $\alpha \in \{0,1\}^\omega$ to be comprehensible for $A$ when $$\exists n > 0 \, \forall m > n \, f_A(\alpha_{<m})=\alpha_m$$ i.e. sufficiently late in the sequence, $A$ always predicts $\alpha$ correctly. Obviously $\alpha$ can only be comprehensible when it's computable. It's also easy to see that for any computable predictor $A$, $\exists \alpha$ computable s.t. $\alpha$ is not comprehensible for $A$. For example, we can define $\alpha$ recursively by $$\alpha_{n} := \lnot f_A(\alpha_{<n})$$ On the other hand, it is possible to define $p : \{0,1\}^* \rightarrow \{0,1\}$ uncomputable s.t. all computable sequences are comprehensible for $p$. For example, $p(x)$ can be defined to be $\alpha_{|x|}$ where $\alpha$ is the minimal Kolmogorov complexity infinite sequence satisfying $\alpha_{<|x|} = x$. So, it is impossible to construct a universal computable predictor, but we can try to make it "approximately universal". Formally, an infinite sequence of computable predictors $\{A_n\}_{n \in \mathbb{N}}$ is called asymptotically universal when $\forall \alpha \in \{0,1\}^\omega$ computable $\exists n > 0 \, \forall m > n: \alpha$ is comprehensible for $A_m$. It is easy to construct an example of such a sequence. Namely, define $A_n(x)$ to be the following program: "Run the first $n$ programs producing infinite sequences by dovetailing. The first time one of those programs produces output $y$ s.t. $|y| = |x| + 1$ and $y_{<|x|} = x$, terminate and produce the output $y_{|x|}$. If all of the programs produced outputs of length > $|x|$ and neither satisfied the condition, terminate and produce the output $0$". Suppose $A_n$ from the above example is a run on $\alpha \in \{0,1\}^\omega$ produced by an algorithm $B$. For $n >> 0$ the time of the computation of $A_n(\alpha_{<k})$ is bounded by $p(t(k), 2^{|B|})$ where $t$ is the time complexity of $B$ and $p$ is polynomial. That is: $$\forall B \, \exists n \, \forall N > n \, \forall k: T(A_N(\alpha(B)_{<k})) < p(t(k), 2^{|B|})$$ It seems natural to ask whether this can be improved. Specifically, we define an asymptotically universal sequence of computable predictors $\{E_n\}_{n \in \mathbb{N}}$ to be efficient when the time of the computation $E_n(\alpha_{<k})$ as above can be bounded by $q(t(k),|B|)$ with $q$ polynomial, given $n >> 0$. The question is thus