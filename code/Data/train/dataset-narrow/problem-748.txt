I made it work. The problem is how the bootstrap script tries to start the server. I tried on different machines and all had the same symptoms when trying to bootstrap the cluster using the script galera_new_cluster. It has something to do with systemd, I guess. bootstrap the new cluster "by hand" using 

I'm on 2 debian8 box's with mysql-5.7.19. I have a master slave replication running, replicating one db (db1.1) from master to slave (db2.1). Replicated db's on master and slave have same names. Master has another db (db1.2) that pulls some statistical data out of db1.1. That happens by means of some views (getting data from db1.1) in db1.2 and an event in db1.2 that calls a procedure. Now I like to take that statistical workload of from master and put it on the slave. I created another db, db2.2 on slave with identical table and views as in db1.2. Names of db1.2 and db2.2 are different. Created same procedure on slave as on master and an event with another name on slave.db2.2 that calls that procedure. If I call the procedure by hand on slave it does what it supposed to do. Copies all data as expected. But calling that procedure by an event on slave doesn't run. Names of the events on master and slave are different. What I'm doing wrong? 

By default, Teradata SQL Assistant will attempt to query the views , , to populate the Database Explorer. It is possible in your environment that those objects are not accessible to developers or end users via the user. Instead, you may need to modify your connection settings ODBC or .Net Provider to use the X-Views in DBC. These are a collection of views which restrict the rows returned based on the privileges your user account has been granted to access or which you have created. The ODBC DSN, .Net Provider, and JDBC drivers for Teradata have a means to use the X-Views by default to enable database tools such as SQL Assistant or Teradata Studio/Studio Express to populate the database explorer controls "transparently". Try this and see if it works. 

The endian is different between Windows and Linux. You can use transportable table spaces to move your database. At some point you need to change the endian of your transportable table space export. Here is the documentation. Transporting Data Across Platforms 

If you have more than one backup of your control file, try using the oldest backup. Essentially the message is telling you that the control file is newer than the rest of the backup. for example if you backup the control file, then backup the data base, then backup the control file. Doing a restore using the second backup of the control file will result in the error that you are getting. Doing a restore using the older or first backup of the control file should work. 

It is the alias used to reference the derived table. In the outer SELECT if you fully qualified the column references they would read 

Most data models are lacking in a good DATE Dimension and thus force developers and report developers to rely on date arithmetic to find date boundaries that are relevant to the business model. (Fiscal Year, Fiscal Quarter, Fiscal Period, Calendar Quarter, etc.) A good CALENDAR table would go a long way to making your life easier. A simple EventDate BETWEEN SYSDATE - 458 and SYSDATE risks truncating dates out of your oldest quarter. Take TODAY as an example: SYSDATE - 458 yields 2010-09-28. If my math is correct the 3rd Quarter of 2010 started on July 1, 2010. You need roughly 548 days to make sure you are covering the entire range of current quarter plus the previous four full quarters. Trouble is that when you this will cause some overlap as your current quarter is partially complete. So you are faced with some additional logic to truncate out the fifth oldest quarter that you don't wish to include. My PL/SQL isn't the sharpest right now to write that logic but I hope the explanation helps shed some light on the approach you will need to take. 

did the trick on all machines I tried. And while starting the other nodes, it is also a good idea not use the "systemd way" systemd mechanism has a timeout, and if your sync with the other nodes takes a bit longer, systemd spits out an error, but mysql keeps running and does what it is supposed to do. It's not a bad thing, but it's kind of confusing. So for now I recommend, starting the other nodes also "by hand" 

I have a server landscape, running mysql 5.7 on debian 8 in some GTID based replication configuration. All is high load servers and 200+GB databases. Now I got some new servers. I like to run that new servers on debian 9 and the recommended mariadb 10.2.X expanding it later to a MariaDB Galera Cluster. I've read some, how to migrate from mysql to mariadb, but anyhow none of them seems to fit for me. Different GTID handling and innodb format between mysql and mariadb does scare me a bit. After all I have to run some tests. So, my idea is: on one of the masters (mysql 5.7), -flush tables with read lock -make a snapshot (LVM) of the database partition -show master status to get the GTID executed -unlock tables -copy that snapshot to a new server to /var/lib/mysql -install mariadb on the new server would mariadb upgrade that data in /var/lib/mysql Any recommendation or another ideas on that? Ju 

I took some code snippets from a backup script that I use. You can redirect all output with the exec command, then redirect it back. You can also surround everything with braces and redirect that to a log file. Personally I think bash is a better shell to program in, if you don't have bash ksh works too. 

You can define your own exceptions. If you have a package for defining business logic, you can define some exceptions in the package header then raise the proper exception when needed. Which means that you need to keep the exception numbers unique so that you can handle the exceptions from the calling procedures. ssociating a PL/SQL Exception with a Number (EXCEPTION_INIT Pragma) 

There are a multitude of factors that go into determining which Teradata platform and the configuration of the platform that will suite your needs. Teradata has spent untold amounts of money on intellectual property and decades of experience working with potential customers to help them properly size a configuration that not only meets the immediate needs of a customer but provides them capacity for which the environment can adequately grow and evolve. I would strongly suggest you reach out to Teradata directly and engage them in a pre-sales capacity if your company is considering their technology to meet the needs of your data warehouse environment. For a sandbox environment, you could may be able to get away with using the one terabyte version of Teradata Express on an adequately sized server or consider using Amazon EC2 to stand up a instance of Teradata to complete a proof of concept. It should be noted that either of these options should not be used to gauge the performance of a production environment for service level agreements but whether or not the technology will accomplish what you are trying to do. 

So, for the after world. I did it by following my idea and it worked out well. A few minor things has to be taken into account. After copying data over from one server to the other. The *.pem and the auto.cnf files in /var/lib/mysql/ have to be deleted on the new machine. Deleting auto.cnf is really important because it holds the servers ID and you don't wanna end up running several servers with same server ID, what will cause you problems afterwards. On a fresh installed machine, before installing mariadb, there isn't a user mysql nor group mysql on the system, so change the /var/lib/mysql and it's sub folders to be owned by root:root. I copied the entire /etc/mysql folder from source machine to destination machine, to make the mariadb install script believe there was a running mysql installation before on that machine. In the config file /etc/mysql/mysql.conf.d/mysql.conf I changed the value of the server_id variable to be unique in the entire server landscape. After that you are ready to fire the mariadb installation on the new machine using apt. While installing, mariadb install script detects the mysql stuff and tries to run the mysql-upgrade on it. That will probably fail, as in my case, but that's not a big deal. It is because the install script tries to run mysql-upgrade as root user on your freshly installed mariadb server with old data in place. But there are differences between mysql and mariadb in system tables and in what root on a local machine can do and what not, and system tables needs to be upgraded by the upgrade script. I never run things as root user on the database, so I have my own user with rights do do everything on each of my databases, and that user credentials come with the data from the old machine. So I run the update script just from the command line using my user credentials instead of the root credentials. And it turned out to run well, complaining about some missing stuff, but that is what the update script is written for. To detect missing or changed stuff and correct it. Restart mysql and there we are. All my users and 230+GB of data right into access managed now by mariadb on a debian 9 machine. Hope it helps someone. Ju 

You probably need to drop the tablespace, not just the datafile. alter tablespace my_ts offline; drop tablespace my_ts INCLUDING CONTENTS CASCADE CONSTRAINTS; 

When in doubt ask Tom... backup up and recovery generating extra redo Which is good reason to use RMAN for backups rather than scripted hot backups using begin backup/end backup. 

You can use PL/SQL and schedule jobs to populate tables or schedule jobs to update materialized views. Then once the data has been updated you can query as much as you want, since the heavy lifting will be done. You should also try tuning the queries. If you have an index with the filter columns and any other columns that you need, then you can avoid reading the index then the table. If you start indexes with the low cardinality columns going towards high cardinality columns, you can reduce the amount of rows looked at. Look at materialized views and including rowid so that you can try to do a fast refresh. 

Starting point at the sandbox sb1, sb2, sb3: All boxes identical. A Fresh debian 9 install mariadb 10.2 Galera3 build a test galera cluster with empty databases works fine. All three nodes come up and work as expected. then after stopping the test cluster copy a LVM snapshot from a production mariadb server over to sb1 start mariadb server with standalone config to see if all databases and tables are correct. (config I took from the server the data come from) shutdown and restart the standalone to see if there is a problem. All is fine, server starts and stops as expected. Then change configs to fit galera try to bootstrap with > galera_new_cluster and that $URL$ is what happens. my.cnf 

Teradata's optimizer quite possibly will re-write Method 3 to the same query plan as Method 2. Method 1 will result in an INNER JOIN because the qualification on the table shouldn't be in the WHERE clause but the ON clause of the condition. If you were to place an aggregate step such as a DISTINCT or GROUP BY in the derived table of Method 3 you will find the optimizer will likely satisfy the derived table as an individual step without re-writing the plan. I would suggest that you run the EXPLAIN for each query and compare the output. 

Sounds like you are describing dimensional data modeling where you have a fact table supported by multiple dimension tables. Your Line Item table is your "fact" table and then you have a product and price "dimension" tables. Your price table could very well be a fact table or a slowly changing dimension table as well because there is a time element associated with a product's price changing over time.