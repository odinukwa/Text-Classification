However the group_concat seems very confusing to use with a group by statement - or simulating group_concat with SQL Server. So instead, how bout this. select max(employee), min(employee) 

I'm running an ETL process that is writing about 2 million rows to a SQL Server database. I'm trying to optimize the time it takes for pure inserts (I guess updates is a different story). I'm wondering what the biggest bottleneck is, or best way to reduce time, for basic inserts into a SQL database. I mean, the first thing, probably, is the size of the data, right? Both the number of rows, the number of columns, and the data size in each column. Some of these may not be able to be minimized, the KB/ footprint of each row is one thing that can be potentially optimized, right? What else can be optimized or is the largest factor? Is it the transmission medium? I mean, how much magnitude of difference is there between writing to a database that's on the same computer, vs writing across a web connection (that is robust, fast, and has a ping of 1 ms?). Finally --- why is it that multiple parallel connections to the database seem to speed up the process to a point? I mean, when I have 20 connections making inserts round-robin style, it's about 6-7x faster than one connection writing ALL the data. I'm curious why this is. Right now I have 2.2 million rows totaling 2.7 GB. That's 1.23 kb per row. Right now inserting 1000 rows at at time (1.23 MB) using 14 connections takes 6.7 seconds. That's a snail-paced 10.66 rows per second. Even assuming 1 connection would be just as fast (it isn't) that's 150 rows/ second at best, which is not exactly "fast" either. I'm writing over a super-fast, robust web connection b/c we can't have the ETL process on the same space as the data warehouse. So .. how can I optimize for speed here? The reason for 1000 rows at a time is because the data comes from pages of 1000 - but optimizing the parsing is a separate issue for now. I do have one primary index I believe, but nothing too write-expensive. Right now I've simply been doing Monte Carlo like testing (try it and see what works) but I need something more focused. 

Currently I have ETL software installed/ running on a machine separate from the destination data warehouse database (for security purposes apparently). So it must send data/ write records across a network/ web connection, unfortunately. Due to this final step (write the data to a database over the web/ network) being a bottleneck, I have used parallelization to speed things up. I have the ETL software "round-robin" the data to 15 copies of a SQL insertion step. It has worked just fine in this regard. 15 connections are opened, the data is written in 20 seconds, and this process is repeated about 1,500 times for the initial data load. Recently, we have switched our date warehouse from Machine 1 to Machine 2. Now I'm getting connection errors: 

I'm not a general IT expert. Sometimes with SQL queries ... they will take 10 seconds about 95% of the time. Every so often, the exact same query will take 5-10 minutes. As will others. The constant is the host computer and current time --- not the query itself. I'm just wondering if there's any way for me to test or verify what the problem is that is slowing down the query (and that it is, indeed, slowing the query). Would I remote into the host machine and check the memory usage/ processes? The host machine is also in a foreign country --- USA to Germany. Again, 95% of the time, this doesn't matter. I'm wondering if there are temporary 'network spike' issues. I want to know so I can actually complain to IT there ... but I want to pinpoint exactly what's causing the query issues. Maybe this is out of scope of this board ... I know a showplan wouldn't really indicate anything ... or would it? Would network or memory issues slow all steps the same duration? Is there an easier way to troubleshoot this? 

Just wondering - I made a trivial change to a view. No additional tables were added, although I guess the logic was changed a bit and a subquery was added --- but again, no additional base tables. Suddenly, it seems the permissions to other users on the view have changed/ become restricted. What's going on here? 

I'm very green when it comes to the world of database permissions management in SQL Sever. Let's keep an example simple. Say account 'admin' is the owner of schemas A, B, and C. There another account 'minion' that you want to have full rights (update/delete/insert/select/alter) on any object (table/view) created under schemas A, B, and C. Is this possible? Or do you have to execute a grant statement each and every time you add a table/ view under these schemas? (seems a bit silly to me). 

Does it make a difference whether that table has 1 field, 2 fields, or 100 fields, in actual query time? I mean all fields other than "order_value." Right now I'm pushing data to a data warehouse. Sometimes I dump fields into the table that "may be used in the future, someday" - but they aren't being queried right now, by anything. Would these 'extraneous' fields affect select statements that do no include them, directly or indirectly (no * I mean)? 

I'm running an ETL process and streaming data into a MySQL table. Now it is being written over a web connection (fairly fast one) -- so that can be a bottleneck. Anyway, it's a basic insert/ update function. It's a list of IDs as the primary key/ index .... and then a few attributes. If a new ID is found, insert, otherwise, update ... you get the idea. Currently doing an "update, else insert" function based on the ID (indexed) is taking 13 rows/ second (which seems pretty abysmal, right?). This is comparing 1000 rows to a database of 250k records, for context. When doing a "pure" insert everything approach, for comparison, already speeds up the process to 26 rows/ second. The thing with the pure "insert" approach is that I can have 20 parallel connections "inserting" at once ... (20 is max allowed by web host) ... whereas any "update" function cannot have any parallels running. Thus 26 x 20 = 520 r/s. Quite greater than 13 r/s, especially if I can rig something up that allows even more data pushed through in parallel. My question is ... given the massive benefit of inserting vs. updating, is there a way to duplicate the 'update' functionality (I only want the most recent insert of a given ID to survive) .... by doing a massive insert, then running a delete function after the fact, that deletes duplicate IDs that aren't the 'newest' ? Is this something easy to implement, or something that comes up often? What else I can do to ensure this update process is faster? I know getting rid of the 'web connection' between the ETL tool and DB is a start, but what else? This seems like it would be a fairly common problem. Ultimately there are 20 columns, max of probably varchar(50) ... should I be getting a lot more than 13 rows processed/ second? 

I'm wondering how I can join the 5 answer columns above into meaningful text (or at least the integer) without doing 5 joins. I mean say I have the table 'responses' and the table 'answer_key'. My initial thought would be go through the motion: 

My rookie thoughts: Aren't these problems irrelevant to splitting the database? They are simply problems that need to be solved on their own either way. 

I need a way to get subjective call QA scoring into our business intelligence system. It currently lives in Excel. The problem with a direct upload from Excel to the DB is that there is no validation or response from the db necessarily (although I guess I can program these in). Is there a solution that already exists that allows a non-technical user to import data, and already have it validated against the database? There are myriad data entry software solutions out there, but as far as I know, actually receiving input from sql server (These are the valid values) is something that's not very common. MS Access is a possible solution - however, for some reason, their data entry forms do not allow for a bulk copy/ paste, which would slow down end users. You have to enter one item at a time. 

One database is harder to document and draw schema diagrams for than 7 databases (we will have views that will span multiple database). 

I've been doing a lot of reading on Data Warehouse architecture as we are thinking about reforming our current setup feeding our BI. The 'maturity' of the Data Warehouse is quite low, despite being rapidly deployed. I'm still confused over a "Data Warehouse" vs a "Data Mart." What we currently do: Go to source systems. Use an ETL process to insert "cleaned + transformed" data into data tables. We named these tables thinks like surveymonkey.FactSurvey. IE, the schemas are named after the data sources to aid in comprehensibility or 'what is this, where did it come from'. Tables are labeled as fact or dimension tables. Currently we have 13-14 of these 'schema-sources' each with say 8-9 tables in one giant database. How do we integrate data? Conformed dimensions, of course. Either dimensions are "cleaned/ conformed" via logic in the ETL scripts, or they are "cleaned" post-load in views atop the tables. In my view, this giant database is a "data warehouse" - it feeds our business intelligence OLAP cubes. What exactly is a data mart, and what is the purpose? I thought we were already following the Kimball approach -- fact and dimension tables. ETL from source to database. A series of 'views' denormalize the data to feed into the cubes. But isn't it the Kimball approach that says "well a data warehouse is simply all the data marts combined together" -- that sounds kind of like what we're doing. Dump all the transformed source data into one database. Views join table within source schema and between them (integrating business data). I guess I'm fairly lost at what defines a "data mart" in the logic here. Maybe I should post a separate question here, but then does one divide "data marts" via schema, or different databases? I'm not sure the logic of dividing a Data Warehouse into separate Data Marts -- they all feed ONE BI application we are using. This BI application imports ALL data once a day into itself. If Finance uses their Finance data 10,000x more often than Marketing queries the Marketing table, that is completely irrelevant to our database structure as they aren't directly querying/ hitting our Data Warehouse whatsoever. And what is the logic of separating Data Marts via Department or Function, generally? Is it a performance issue or a UX issue? If performance, then Department shouldn't be the divisor, IO load or usage should be ... Does anyone know what the difference is here in terms of architecture and use cases? I've read a lot online, but honestly, it seems like most authors are waxing philosophical and not talking about specific tables or brass tacks. Maybe it would be easier if the literal architecture/ db/ schema structure were explained. 

The process works just fine on Machine 1. It's suddenly failing on Machine 2. I'm inclined to believe that the server settings are the issue. After talking to our DBA, I was informed that the suspected difference is that Machine 1 has service pack 1 installed for windows and Machine 2 has service pack 2 installed for windows. Apparently the latter may be blocking connections after a certain point to prevent a DDOS attack --- or enough open connections may cause the firewall to jump in. Here are my thoughts. The connections are closing just fine. I know this because (for testing) - I used a SQL DB on HostGator which limits me to 24 simultaneous connections (and would let me know if I exceeded it). But maybe merely opening connections repeatedly, or sessions (?) -- is triggering some kind of security/ rate-limiting on this server. My question is --- how do I proceed from here? The way I see it, I have two options: 

I'm doing some data analysis and want to find an easy way to examine all the members of each "group" in a group by function. Like, 3 agents may be involved in an order. I want to quickly examine the three agents that were 'grouped' in this order for various reasons. Usually, I would use group_concat for this (easy way to see all grouped strings). However replicating that using a 'group by' appears difficult and unwieldy for now in SQL sever. Right now, rough-and-dirty, I would max(username) and min(username) to quickly find 2 (and 90% of orders probably have 2 or less people. Is there a way to do mid(username) or 2nd-highest(username), or percentile(50th, username)? That would be a great, quick way to find this relevant data. For some reason, the previous answers I've seen describing group_concat on SQL Server do not sound straightforward to me. Sample data for instance: