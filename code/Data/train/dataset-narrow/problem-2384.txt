Using the relation between total variation and $L_1$/$\ell_1$ distance of the probability/distribution/mass functions, we have $$\begin{align} d_{\rm TV}(D_1, D_2) &= \frac{1}{2}\lVert D_1-D_2\rVert_1 = \frac{1}{2}\lVert \beta D_2 +(1-\beta)D_3 - D_2\rVert_1\\ &= \frac{1-\beta}{2}\lVert D_3 - D_2\rVert_1 = (1-\beta)d_{\rm TV}(D_2, D_3). \end{align}$$ 

Edit: As pointed out in the comments by Denis Pankratov, the first variant admits a trivial $O(\log n)$ upper bound (sending $\lvert x\rvert$ to Bob is enough). 

[1] Tight Bounds on the Fourier Spectrum of $\mathsf{AC}_0$, A. Tal. CCC'17. [2] On polynomial approximations to $\mathsf{AC}_0$, P. Harsha and S. Srinivasan. RANDOM 2016, 

The $\textsf{GapHammingDistance}$ problem over $\{0,1\}^n$ is defined as follows: Alice (resp. Bob) is given an input $x\in\{0,1\}^n$ (resp, $y\in\{0,1\}^n$), under the promise that their Hamming distance satisfies $$ \operatorname{d}_H(x,y) \in \left\{0,\dots,\frac{n}{2}-\sqrt{n}\right\}\cup\left\{\frac{n}{2}+\sqrt{n},\dots,n\right\}. $$ Their goal is to decide, wth probability at least $2/3$, whether $\operatorname{d}_H(x,y) \leq \frac{n}{2}-\sqrt{n}$; or $\operatorname{d}_H(x,y) \geq \frac{n}{2}+\sqrt{n}$. A lower bound of $\Omega(n)$ is know for the one-way communication (randomized) version of this problem, as well as for the unbounded two-way communication. Now, I am interested in the following two variants: is a linear lower bound (or a lower bound) known for the one-way randomized communication complexity of this problem, under the additional constraints: 

Here's what I can find after spending some time on Google Scholar: My measure of sharpness of a vertex is related to an already studied concept called the Angular Resolution. From Wikipedia: 

Chapter 1 of the book The Probabilistic Method, by Alon and Spencer mentions the following problem: Given a graph $G$, decide if its edge connectivity is at least $n/2$ or not. The author mentions the existence of a $O(n^3)$ algorithm by Matula and improves it to $O(n^{8/3}\log n)$. My question is, what's the best known running time for this problem? Let me describe the improved algorithm. First, decide if $G$ has its minimum degree at least $n/2$ or not. If not, then the edge connectivity is clearly less than $n/2$. Next, if that is not the case, then compute a dominating set $U$ of $G$ of size $O(\log n)$. This can be done in time $O(n^2)$, by an algorithm described in the previous section of the book. Next, it uses the following not very difficult to prove fact: If the minimum degree is $\delta$, then for any edge cut of size at most $\delta$ that divides $V$ into $V_1$ and $V_2$, any dominating set of $G$ must have its vertices in both $V_1$ and $V_2$. Now consider the dominating set $U = \{u_1, \ldots , u_k\}$. Since $G$ has minimum degree $n/2$, any edge cut of size less than $n/2$ must also separate $U$. Thus for each $i\in \{2, k\}$, we find the size of the smallest edge cut that separates $u_1$ and $u_i$. Each of these things can be done in time $O(n^{8/3})$ using a max-flow algorithm. Thus total time taken is $O(n^{8/3}\log n)$. 

I realize the problem itself would need a more thorough and precise formulation to be tackled, as in the above we deal with full real numbers (so "hiding" information in a single real number, say the weight of a node in the middle layer, would give a very easy way out). But with this dealt with appropriately, hopefully there are non-trivial statements to be made with regard to what compression can be achieved, with respect to some distribution over the inputs? Has this type of question been looked at from a theoretical viewpoint, in our community or another? 

Tutorials on the Foundations of Cryptography (edited by Yehuda Lindell) Dedicated to Oded Goldreich From the Springer page: 

For randomized algorithms $\mathcal{A}$ taking real values, the "median trick" is a simple way to reduce the probability of failure to any threshold $\delta > 0$, at the cost of only a multiplicative $t=O(\log\frac{1}{\delta})$ overhead. Namely, if the $\mathcal{A}$'s output falls into a "good range" $I=[a,b]$ with probability (at least) $2/3$, then running independent copies $\mathcal{A}_1,\dots,\mathcal{A}_t$ and taking the median of their outputs $a_1,\dots,a_t$ will result in a value falling in $I$ with probability at least $1-\delta$ by Chernoff/Hoeffding bounds. Is there any generalization of this "trick" to higher dimensions, say $\mathbb{R}^d$, where the good range is now a convex set (or a ball, or any sufficiently nice and structured set)? That is, given a randomized algorithm $\mathcal{A}$ outputting values in $\mathbb{R}^d$, and a "good set" $S\subseteq \mathbb{R}^d$ such that $\mathbb{P}_r\{ \mathcal{A}(x,r) \in S \} \geq 2/3$ for all $x$, how can one boost the probability of success to $1-\delta$ with only a logarithmic cost in $1/\delta$? (Phrased differently: given fixed, arbirary $a_1,\dots, a_t\in \mathbb{R}^d$ with the guarantee that at least $\frac{2t}{3}$ of the $a_i$'s belong to $S$, is there a procedure outputting a value from $S$? If so, is there an efficient one?) And what is the minimum set of assumptions one needs on $S$ for the above to be achievable? Sorry if this turns out to be trivial -- I couldn't find a reference on this question... 

What are the standard problems we can reduce from to prove $\Omega(n\log n)$ lower bounds? Of course, state problems other than sorting and element distinctness. 

Here's a (loose?) upper bound on the number of meals you can serve. Let $|S| = n$ and assume that $n$ is divisible by $k$. Also, assume that you have exactly $n/k$ tables and you want each table to be full during each meal. For each meal, construct a graph with a node for each person in $S$ and an edge when two people share a table. This graph is a collection of $n/k$ cliques each of size $k$. Thus the number of edges in the graph is $\Theta(nk)$. Since you don't want any edge to occur in two different meals, and since the total number of edges possible on a vertex set of size $n$ is $\Theta(n^2)$, this shows you can only serve $O(n/k)$ meals. Actually, it's not difficult to find the constants here and when you do the math, you get an upper bound of exactly $\frac{n-1}{k-1}$, which, for your typical values, is 11. 

[BCOST15] Eric Blais, Clément L. Canonne, Igor Carboni Oliveira, Rocco A. Servedio, Li-Yang Tan. Learning Circuits with few Negations. APPROX-RANDOM 2015: 512-527 

Here is a detailed outline, not entirely made rigorous. Setting $b_n \stackrel{\rm def}{=} \frac{1}{6}-Y_n$, with $b_0 = 1/6$, we have $$ b_{n+1} = \frac{4}{3}\left( \sqrt{1+\frac{3}{2} b_n - \frac{9}{2} b_n^2} - 1\right)\tag{1} $$ (I like to set things near zero.) By induction, $b_n \geq 0$ for every $n$, and a simple computation shows that $b_{n+1} - b_n \leq 0$ for all $n$. By monotone convergence, $(b_n)_n$ converges, and the fixed point being zero we have $$\lim_{n\to \infty}b_n = 0\,.\tag{2}$$ We can do a Taylor expansion of (1) to get $$ b_{n+1} = b_n - \frac{27}{8}b_n^2 + o(b_n^2)\tag{3} $$ i.e., summing, $$b_{n} = b_0 - \frac{27}{8}\sum_{k=0}^{n-1} \left(b_k^2 + o(b_k^2)\right)\,\tag{4}$$ Solving the recurrence $$ \tilde{b}_{n} = \frac{1}{6} - \frac{27}{8}\sum_{k=0}^{n-1} \tilde{b}_{k}^2$$ yields $\tilde{b}_{n} = \frac{1+o(1)}{6+\frac{27}{8}n} \displaystyle\operatorname*{\sim}_{n\to\infty} \frac{8}{27n}$. (For instance, solving the continuous version $h' = -\frac{27}{8}h^2$ with $h(0)=1/6$.) "Therefore", $b_n \displaystyle\operatorname*{\sim}_{n\to\infty} \tilde{b}_{n}$ and $$ Y_n = \frac{1}{6} - \frac{8}{27n} + o\left(\frac{1}{n}\right)\,.\tag{5} $$ 

Sorry for creating confusion. Editing the question to accomodate issues raised in the comments... We know that finding the convex hull of $n$ points on a plane has a lower bound of $\Omega(n\log n)$ on its running time. However, if the points are given in the order in which they occur along some simple polygon that has those points as its vertices, then their convex hull can be found in linear time. I find this intriguing because there are probably too many simple polygons that have the given points as their vertices and therefore, intuitively, the order along one of them sounds like a very useless piece of information. And yet, it helps. So my question is, are there other places where the same information helps in bringing down the running time of an algorithm? As a side, I also want to know bounds on the number of permutations of a given set of points on a plane for which there is a simple polygon with those points as its vertices so that the order in which the points occur along the polygon is the same as the order in the permutation. What's known about this? 

According to [MOS04] (Section 5.2), the class of $k$-juntas over $\{-1,1\}^n$ can be learnt with membership queries in time $\operatorname{poly}(2^k, n)$ — cf. also footnote 8 of [BL97], p. 17. Edit: as pointed out in the comment, this is probably not what you were looking for (in terms of attribute-efficiency). 

A FOCS'15 paper by Lee, Sidford, and Wong [LSW15] can be leveraged to obtain such minimization guarantees -- cf. Section 5 (specifically, Corollary 5.4) in our recent paper ([BCELR16]). 

Let $f\colon 2^{[n]} \to \mathbb{R}$ be a submodular function (one can assume $f$ is bounded, if this helps). We are given noisy oracle access to $f$: on any $S$ and for any $\tau > 0$, one can obtain an additive $\tau$-approximation of $f(S)$ (at cost $\operatorname{poly}(1/\tau)$). I am interested in what is known in minimizing (up to some arbitrary (additive) accuracy $\alpha>0$) $f$, given this sort of access: If we had exact oracle access, then this could be done in polynomial time, exactly; what about robustness to approximate queries? Is there any algorithm (or, on the other side of the spectrum, lower bounds) for it? 

One of the nice things about having evolved in a universe with three spatial dimensions is that we have developed problem solving skills pertaining to objects in space. Thus, for example, we can think of a triplet of numbers as a point in 3-d and hence computation about triplets of numbers as computation about points in 3-d, which can then be solved using our intuition about space. This seems to suggest that it should be possible at times to solve a completely non-geometric problem using techniques from geometry. Does anyone know of such examples? Of course, the terms 'geometric' and 'non-geometric' are slightly vague here. One can argue that any geometric problem is actually non-geometric if you replace all points with their co-ordinates. But intuitively, the definition is clear. Let's just say that we call something geometric if we would consider sending a paper about it to SoCG. 

Yuval Peres gave the answer in terms of the Kullback-Leibler divergence. Another way is to recall that the sample complexity will be captured by the inverse of the squared Hellinger distance between the two coins. Now, letting $D_p$ and $D_{p+\varepsilon}$ be the distributions of a Bernoulli random variable with parameter $p$ and $p+\varepsilon$ respectively, $$\begin{align} d_H(D_p,D_{p+\varepsilon})^2 &= \frac{1}{2}\lVert D_p-D_{p+\varepsilon}\rVert^2_2 = \frac{1}{{2}}\left((\sqrt{p}-\sqrt{p+\varepsilon})^2+(\sqrt{1-p}-\sqrt{1-p-\varepsilon})^2\right) \\ &= \frac{1}{2}\left({p(1-\sqrt{1+\varepsilon/p})^2+(1-p)(\sqrt{1}-\sqrt{1-\varepsilon/(1-p)})^2}\right) \end{align}$$ Assuming wlog $p\leq 1/2$, we can see easily by a Taylor expansion that this is $$\begin{align} d_H(D_p,D_{p+\varepsilon})^2 &= \Theta\left(\frac{\varepsilon^2}{p}\right) \end{align}$$ leading to the same answer as Yuval Peres' (from a different method). Interestingly, this also shows the usual observation, that the quadratic relation between TV and Hellinger distance can matter a lot: for $p=1/2$, the bound $1/TV$ (i.e., $\Omega(1/\varepsilon^2)$ here) is tight; but for $p=O(\varepsilon)$, then it is quadratically worse than the optimal, which is $1/d_H^2$ (that is, $\Omega(1/\varepsilon)$).