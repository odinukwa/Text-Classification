I understand this is an older question with older correct answers. But fine grained auditing is now available in SQL Server 2016 with the introduction of SP1. Many features found before in Enterprise Edition only (Compression, Partitioning, Row Level Security, Fine grained auditing, Always Encrypted, In-Memory OLTP, Columnstore, Changed Data Capture, etc.) are now available in Standard, and even Express edition in most cases. Worth looking at the post from Microsoft. 

The only times I have ever seen the State 8 error for 18456 (Password Mismatch) was in fact a password mismatch. Try a new password. Type it in notepad, copy and paste it to SQL and then do the same into your app config. Make sure nothing is happening to that password when it is being handed off to SQL - no trimming, extra spaces, etc. But then the second error you got seems to indicate you may have gotten past that bad password error and are now unable to access your database. That can be one of a few things: 1.) Verify the database specified really does exist. 2.) Verify that the login you are using has been granted appropriate permissions into that database (If the permissions are at the SQL Server system administrator level then this shouldn't be a necessary step) That's where I'd start anyway. 

This is an older question. But Microsoft just gave you a great new answer if you are using SQL Server 2016 and still in Standard. Partitioning (among many other features like Compression, Columnstore, In-Memory OLTP, Row Level Security, etc.) is available in SQL Server standard. Probably too late for this question. But not too late for someone finding this question today. SQL Server 2016 Service Pack 1 is where this change was introduced. You can read more on Microsoft's post about it. 

To the question you asked: I wouldn't rely on disk queue alone. In fact I rarely even ever look at disk queue lengths unless I'm getting in deep with a problem. It is best to look at your disk's latency. Those are the Avg. Disk Sec/Read (or /Write and /Transfer) counters. That tells you what your disk latency is from Windows' perspective. So the time that the request was taking after sent to the disk and brought back.. Disk Queuing nowadays doesn't tell you a lot because most IO subsystems are able to handle a disk queue depth and have multiple spindles doing work in your RAID group often. Finally - In this case - your disk queue length doesn't even look that bad. From here it looks like the max it was in the time of this screenshot (for the average length) was 1.377. That's nothing on most SQL Server systems. Look at your actual latency. Also I don't look at % Disk Time.. I look at the idle time instead. That is a more reliable counter and you just have to do a little math to read it.. The more idle, the less activity. To The General Question Behind Your Question I'll ask this one by starting with a question - Why did you go right to your IO? There could be any number of things causing your slowdown. And to answer that exhaustively here is tough but a high level of some things to look at/consider: 

The article isn't saying "if you want to use a Domain account, we suggest you use a local account" - but I totally see why anyone would read it that way. The article is saying, instead, "Use a domain account where you can. IF your computers are not in the same domain, here's a little trick you can use." That trick, sometimes referred to as pass through authentication has you create an account on each computer. Same name. Same password. By doing that - windows will pass the credentials through and, for all intents and purposes of this conversation, the apps (SQL and Replication objects) won't realize it isn't really a "domain account" - and it will just work. If, however, the computers are on the same domain? Just use the domain account. If you go back to that first link you commented with - you'll notice the "if your replication topology includes computers that are not in the same domain..." line isn't bulleted or indented. Really the bullet about the local to each should be indented off of that. 

More information would help here and I've asked those questions in the comments. That said - a few thoughts: Basically to access a SQL Server database you need a few things: 

The answer here is "it depends" - While actually blocked? A session should not use any resources. It is in a state basically saying "I sure would like to do my work, hey SQL Server may I have a lock on such and such a resource" so while that resource is locked - the session is basically not able to do anything except wait for resource availability. Once that has been cleared it can then start running or join the runnable queue waiting for its scheduler time. Now you could still see IO and CPU increasing on a session with many queries or steps that is blocked in different spots. So you could look and see it is currently blocked, but see the CPU increased or IO - that would be because it got past a block did some work for a statement but was then blocked again. I'll often diagnose blocking with a tool that shows query duration and CPU/IO metrics for a session but maybe missed tracking blocks if an execution of a query is higher than normal duration wise but remains within its normal limits for CPU and IO. It isn't only blocking that will present that way but it often is - so it becomes a tool to say "hey let's look and see if we see any blocking". I might also suggest you have a look at sp_whoisactive. A great script which will be much more informative than SP_Who2. 

Shorter answer to the title of the question: No. Why would you want to lose the ability to treat a date like a date. So important for sorting, date functions, etc. A few thoughts to get you started at least, not sure which DBMS while answering, answering based on my experience with SQL Server: 1.) GUID as a Primary Key is generally not a great idea. Not in SQL Server especially. What's wrong with an Integer primary key? Yes whatever your Primary Key is becomes your Foreign Key in the child table(s) so that is large and probably not necessary. 2.) I would use a regular datetime column. Performance wise, if you are well indexed, you shouldn't notice a difference here. And a datetime column is more versatile as, well, a date. You can ask questions of it that you can't ask as easily of an INT column with built in datetime functions. If you don't need time and you are on a DBMS or version that has just the DATE you can use that data type. 3.) Yes. Especially if CustomerID is a foreign key to a Customers table. Good to index. Whether or not you need to make an index on CustomerID AND the Date column depends on how the queries will typically look. If you are often querying joining to customer and specifying a date range you may find it beneficial to have the date. You may find it beneficial to include some other columns to cover other queries as parts of the key or included columns. It really depends on your queries and data, though. As far as clustering on the date column. That's a hard one. If this were a fact table in a warehouse and every single query was always on a date range, then there are some benefits there. If this is an operational invoice table, I imagine your app also joining into invoices in other approaches also. I also imagine invoices being queried by invoice IDs stored in other tables, etc. So I don't believe there is enough to determine clustered key. I'm of the school that prefers a simple surrogate key for OLTP tables. An InvoiceID INT (or BIGINT if you really would blow out an INT) setup as an identity column so it is always increasing and avoiding page splits. But I don't know if there is a definite wrong answer here (well there are many, but you haven't proposed any of those) 

Well so I see there is a vote to close afoot here, and it makes sense - this is an opinion based question. There is no right or wrong answer here. But before the close votes come in - a few things for you to consider: 

"How bad is it?" depends on the degree to which you are suffering now or could suffer with increased workload in the future. One major point of suffering with plan cache pollution could be too many single use plans bloating your plan cache leading to inefficient cache usage. Another point of suffering could be high compilations/second - so in an environment with a heavy workload and a lot of activity, there is a cost associated with compiling over and over. You can see the impact of compilations/sec in perfmon (SQL Server Statistics:Compilations/sec). This can look like CPU pressure. To your performance/applications, this can look like increased query duration waiting for needless compiles each time it runs. You can see the impact to the plan cache from the memory bloat by this query borrowed from Glenn Berry's Diagnostic scripts. How big is your SQLCP plan cache? 

There are a few things going on in your question. To start I presume you are referring to an Always On Availability Group, just to be clear. You shared a link about multi-subnet failover. Multi-subnet failovers are a feature, not a requirement. So the two nodes in your data center don't need to be on different subnets. Even when you go to San Jose later, it depends on if the subnet is "stretched" there whether you even need to worry about multi-subnet failover or not. There are also various network connections involved in an Always On Availability Group. At the very least there needs to be a public Network that all the nodes can see each other on and receive traffic from the outside on. You could setup with just this one network. You can be, as your link indicates , on separate subnets, but if you are on one? That makes life easier. You can also add (and I say should) other networks if you don't then this one public network handles all of the functions: Cluster Communications/Heartbeat - this is a network where the windows cluster nodes can send heartbeat traffic. These don't need to communicate on the public network and just need to be routable to one another. SQL Server Availability Group Replication Network - The Availability Group replicas talk to each other over a network. You don't have to configure a separate network, but many environments prefer to segregate their traffic to a network that only has this replication traffic. These NICs would not need to speak to the public or rest of the netowrk, and just need to be routable to each other. You didn't ask about those extra networks, and it seemed like you were just talking about the public network, but I wanted to throw a few thoughts out. 

This question is asked in various forms here but the question boils down to: I know shrinking a database is risky. In this case, I've removed so much data and I'll never use it again. 

In SQL Server, you need to keep table names and column names static in code that isn't dynamic. To do what you are trying to do, you'd have to use Dynamic SQL - declare variables for your SQL Statement, populate the SQL statement dynamically - using your @TableName variable to supply the table name - then execute the dynamic SQL. There are a lot of gotchas to think about with Dynamic SQL. A good place to start is a rather lengthy piece on the good and bad side of Dynamic SQL from Erland Sommarskog in this article, "The Curse and Blessings of Dynamic SQL" 

There are three answers and some great advice is included in each answer in part. That said I wanted to add a bit more from another perspective. Talking about database files here... Right Sizing Is Best As most have said or hinted at - it is far better to "right-size" your database for current and future needs. TomTom is right to point out that there is a performance hit there, but KookieMonster is also right to point out that Instant File Initialization (IFI because I'm lazy) helps that.. Even with IFI - I prefer to find out about the sizing needs of my databases at their deployment and in their possible futures as best as I can. And then I try to size for that plus an "overage" based on what I know about the project/about the potential to grow.... Note: This is not an exact science, and there will be times (many times) that you get this wrong, that's okay.. Autogrowth is a good thing.. I just prefer to try and stay on top of it. Why? Because I don't want to have that awkward conversation with a SAN admin when I start running out of space. And I don't want a SAN admin to have to make that choice to do some ugly things behind the scenes to give me space. I like to preallocate, watch my free space used in the database over time and use Auto-Growth As an Emergency Helper Autogrowth for data files is not evil, especially with IFI. But I like to monitor space used inside of a DB file and use the ability to automatically grow as a band-aid. This way I stay on top of the growth of the databases I'm responsible for as a DBA. Your monitoring regiment should include checking for used space and looking at that. What Setting Is Best? So with this in mind, it almost shouldn't matter what you use because you are helping manage the growth yourself. If you see your space used curve steeper than you like, you can look at your calculations and preallocate more space in one big growth. Even still, I am not a big fan of percentages. To me it is non-deterministic and a sign that someone isn't managing the growth in a lot of situations. I just prefer to have that level of control, and I pick a space that I feel is appropriate based on the needs of the database. Big Caveat "It Depends" - if your database is small and likely not going to be a big boon on disk ever then I wouldn't cry about keeping it at a percentage or even paying a bit less attention to it's setting. If I go to a client and see a 750GB database still at default growth percentages and no log file management, I cry a little inside. If I go to a client with a 1.25GB database that's been around for 3 years and still set to the defaults? I mention it in my report, but I have the whole "There are best practices.. And then there are situations where you are fine either way" conversation with them.. Now if IFI was disabled, and they had a valid reason to keep it disabled? I'd still probably say something with more seriousness to the smaller database, and I'd really say something like "Let's go crazy and grow this 4 times to preallocate some space" and risk "wasting" 2 to 3 GB of their disk space. Transaction Log Files Are a different matter. Log files do not/cannot take advantage of IFI. And they don't like autogrowth because of a little thing called VLF fragmentation (that link takes you to a lot more links all about the topic). I prefer to right size my transaction logs, watch them, and reevaluate what that right size is. I keep autogrowth on (a production transaction log file filling up also makes me cry inside.. actually outside too). And I right size them in "chunks" as per the guidance on the link provided for VLF. 

Well you can see which indexes are not built online by looking at the requirements for online index rebuilds. It isn't that the maintenance plan or database engine will arbitrarily decide which are online and which are offline. To see which index is rebuild at what point, you'd have to profile the activity with something like sp_whoisactive, extended events or SQL Trace - but you should make sure you don't trace too much or cause too much load from your checking. Also I notice that you are sorting your results in TempDB - is there a reason you've selected that? Have you noticed any issues in TempDB size with that set? I know I've said this before in other answers to you so I apologize if this sounds like I'm hammering you here - but I would strongly suggest you look at a script like the Ola Hallengren maintenance solution. There are others - SQL Server MVP Michelle Ufford has also written one. These scripts do a few things that maintenance plans just don't do and the benefits seem to be exactly what you want: