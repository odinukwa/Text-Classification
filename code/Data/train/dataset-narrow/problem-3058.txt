You do feature scaling for accelerating learning process. Features may have different scales. One maybe from 1 to 10 and one may be from -100 to 1000. Using normalization, you make the scale of them the same as each other, helps accelerate the learning process. You should find the mean and variance for each feature separately on your training data. then during training and testing each feature should be reduced by the corresponding mean and be divided by the corresponding standard deviation. So yes, for each feature during testing and training you have to provide same values for mean and std which are obtained using training data. I suggest you taking a look at here. 

Actually, if you change the input the input size, nothing goes wrong with the convolutional layers but the outputs of these layers increases and that will cause to the increased number of inputs to the dense layers 1. Consequently, you will have to have extra weights and you have to train them. That is why it is better to have a fixed size input. But there are other solutions that can be expanded to your task. Take a look at the contents of the third week of this course. As you will see, you have to input patches of the image to the network but what is done there is for detection tasks, you have to expand it to your task, but maybe resizing all inputs to a predefined size is simpler. 

Actually this is called . What I'm going to tell you is inspired from the lectures of this scientist. For finding digits you don't need to design a network. You should extract them and then feed them to a network in turn. First of all you have to read your image. 

It seems that after some epochs your training oscillates. I guess the reason is that the learning rate is high. Try to set the decay parameter of optimizer to a number more than one and resume training. 

You can read the popular paper Understanding Neural Networks Through Deep Visualization which discusses visualization of convolutional nets. Its implementation not only displays each layer but also depicts the activations, weights, deconvolutions and many other things that are deeply discussed in the paper. It's code is in . The interesting part is that you can replace the pre-trained model with your own. 

Now suppose that you have the following problem and you are asked to separate the classes. In this case the justification is exactly like the above's. 

Whenever you have features that they have different scale and it is significant for some features, you should standardize your feature. Take a look at here. 

I quote the answers from What is a bilinear tensor layer (in contrast to a standard linear neural network layer) or how can I imagine it?. A bilinear function is a function of two inputs $x$ and $y$ that is linear in each input separately. Simple bilinear functions on vectors are the dot product or the element-wise product. Let $M$ be a matrix. The function $f(x,y)=x^⊤My=∑_iM_ijx_iy_j$ is bilinear in $x$ and $y$. In fact, any scalar bilinear function on two vectors takes this form. Note that a bilinear function is a linear combination of $x_iy_j$ whereas a linear function such as $g(x,y)=Ax+By$ can only have $x_i$ or $y_i$. For neural nets, that means a bilinear function allows for richer interactions between inputs. Now what if you want a bilinear function that outputs a vector? Well, you simply define a matrix $M_k$ for each coordinate of the output and you end up with a stack of matrices. That stack of matrices is called a tensor (3-mode tensor to be exact). You can imagine the bilinear tensor product with two vectors as $x^⊤M_ky$ computed on each “slice” of the tensor. Bilinear Models consists of two feature extractors whose outputs are multiplied using an outer product at each location of the image and pooled to obtain an image descriptor. 1 

I have a number of participants from a study and am trying to classify them into five "perceived effort" zones. However, I am finding that the accuracy is sitting at around 70% - 75% using various machine learning algorithms, which is OK but I think this could be more accurate by making the features relative for each participant. Each zone corresponds to the participant's perceived effort: 

I would like to run the same algorithms on data that has feature values relative for each participant, because one participant's zone 3 maybe be the equivalent to another's zone 4. In other words, the zones are not uniform. One idea I had was to make the feature's relative to to each participants max of that feature. As an example: 

There are a few steps you can take to choose features for linear regression: 1 - Exclude variables that are highly correlated with each other. If variables are highly correlated you are essentially inputting the same information multiple times which can cause over-fitting and does not satisfy the properties no multi-collinearity for linear regression. You can create a Pearson correlation matrix and decided which variables are too highly correlated using some chosen threshold i.e only keep variables with a correlation coefficient of < 0.3 2 - If you have many variables you could perform principal component analysis (PCA) to reduce the dimensions of the data and use those as your linear regression features. The idea of PCA is reduce dimensions while holding all of the information. Each component from PCA are uncorrelated, satisfying the no multi-collinearity property. 3 - There is also a method known as stepwise linear regression. You allow all variables to enter the model and it will iteratively remove and add variables until the model with the highest R-squared (or whatever your chosen model metric is) is produced. You do have to be cautious using the stepwise method as it can lead to overfitting, but it can give an indication on what features to use. Here's some info on stepwise: $URL$ 4 - If you are using R, there is a brilliant package called "caret" that can help with feature selection. Here is a fantastic link to use as a guide: $URL$ Hope this helps out as a starting point 

I would now like to assess the distances between the 6 known groups. Is this done as simply as generating a distance matrix using each group's coordinates for each of the principal components? If so, I am leaning towards using Manhattan distance to get the absolute distance. Here are the coordinates of each group: 

The goal is to assess similarity and dissimilarity between 6 known groups. The original data began with the 6 known groups and 2,700+ variables all on a scale of 0 to 100. I have performed PCA to reduce the 2700+ variables into 5 principal components using the dudi.pca function from the ade4 package in R. Here are the Eigenvalues for the components: 

We have been tasked with building an in-house sentiment tool and we are going to use it on a multitude of data sources; survey responses, reviews, social listening etc.. This may be obvious to some but I would like to hear some thoughts. So my question is, would you train separate models for each data source since the documents from each source would be quite different? We would then apply the appropriate model depending on the data source. Or is it best practice to train one model using a sample of all the data sources and apply that model to everything? 

However, a few of my features have values that range from -3.6 to + 2.5, and I am stuck on how to make such features relative to the participant. Question: How to make sample feature values relative to the groups/participants for supervised classification machine learning?