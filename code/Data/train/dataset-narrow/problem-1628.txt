Is it possible the VM has been hacked? Are the tools you're using to monitor the list of processes and memory from a known-good, read-only medium? You could have a rootkit installed which hides the leaky processes. 

Apache has a module that can forward requests to tomcat. Is this module enabled? Try disabling it if you don't need it. 

In addition, you often get server-class hardware such as network cards, which may mean better performance (not guaranteed). If your needs are small or your budget really constrained, a non-server computer can act as a server. But you will likely pay for it down the road in maintenance costs. Also take note of the warranty and service contract differences. Does your workstation have the same on-site repair, with the same guarantees for response time from a technician? Etc. 

I'd like to add a me-too to Mike Arthur's answer. I've used a Linux desktop since 1997 and have migrated my home directory along the way. Most apps just work. The only problems I've had was when I switched distributions. Upgrading from RedHat to newer RedHat or RedHat to Fedora or Fedora to newer Fedora is usually really easy. But when I switched from RedHat to Mandrake (now Mandriva) and Mandrake to Fedora it was not a pretty sight. Mandrake and Fedora both used a customized KDE directory structure that was incompatible with each other. As a consequence at that time I deleted most of my .kde directory to recreate the settings. But since then I've not really had problems. In any case, you can always just erase whatever's in your home directory that you don't want after a system update. I find it extremely convenient that my data is separate from my programs. Even if you have to blow away your whole home directory every two years, as a Fedora user that would save me about 2-3 backup-restore cycles (Fedora updates every 6 months). You can also set up a directory for use with programs. I typically have /usr/local as a separate partition. I install anything I compile from scratch there. This doesn't work as flawlessly as in the data case because many times the distros are not binary compatible with each other. But at least your programs are all there and you can assess them as needed. 

We have a Cisco CSM-S content switch with SSL. Currently our website is behind this switch, which performs load balancing for the HTTP and SSL sites. The http and https sites use different hostnames 

The problem might be the format of the data that is sent to the printer. Do you use native printer drivers on the windows workstation? If so, you might need to enable "raw" printing, that is, sending raw binary print codes from the workstation through to the printer. You might need to edit the cups configuration file and uncomment to enable raw printing. I'm not sure what the security ramifications of this are. The other thing that may or may not work for you is setting your printer up as a postscript printer, using a generic postscript driver on the Windows client. However, I've rarely gotten this to work properly. 

It looks like with ext4, drbd is using about half the bandwidth it uses with the raw device so there's a bottleneck that is not the network. 

According to Throughput overhead expectations, the bottleneck would be whichever is slower, the network or the disk and DRBD should have an overhead of 3%. In my case network and I/O seem to be pretty evenly matched. It sounds like I should be able to get around 100 MB/s. So, with the raw drbd device, I get 

This doesn't seem right. There must be some other factor playing into this that I'm not aware of. global_common.conf 

The network is 1Gb connected via a switch. I know that a direct connection is recommended, but could it make this much of a difference? Edited I just tried monitoring the bandwidth used to try to see what's happening. I used ibmonitor and measured average bandwidth while I ran the dd test 10 times. I got: 

I wrote some scripts to automatically start everything up, configured grub on the failover so that it has the right options to allow you to boot from either a small mirroring partition or the failover partition. I tested it and it works. The problem I ran into is that, about once a week, the primary machine seems to completely freeze up. You can't ssh into it, the console won't respond and, after rebooting the machine, the log entries just stop at a certain time and nothing in the log that indicates an error. I disconnected the NBD partition and ran everything with just the local disk in the RAID array and it's run for a month without any problems. Is NBD unstable? Could RAID decide to disconnect the local partition and run off the nbd partition at the same moment that the network fails in some way? Is this just the wrong way to go about it? Thanks. 

What I wanted to do is to create a mirror of one machine's disk on a failover machine so that, in the event the primary machine failed, I would just reboot the failover, select a different root partition and be ready to go. I set it up like this: 

A primary machine and a failover machine. Both machines have a RAID partition defined. The failover machine serves its RAID partion via nbd-server. The primary machine mounts the failover's RAID partition via nbd-client. On the primary machine the two RAID partitions are combined via mdadm into a single RAID device with the flag set for the remote parition. 

I see a much larger performance hit with DRBD than their user manual says I should get. I'm using DRBD 8.3.7 (Fedora 13 RPMs). I've setup a DRBD test and measured throughput of disk and network without DRBD: 

I have 2 webapps deployed in the same JBoss/Jetty server. In Jetty 5.1.14 I had the following jetty-web.xml which configured one of the apps to run as a virtual host (on the same port): 

Each distro has different strengths and different philosophies. Ubuntu aims to be easy to use. They are based on Debian but adopt a slightly more pragmatic approach, as opposed to Debian, which is more pure in their quest for Freedom. Ubuntu has LTS releases which are supported for 3 years. I'd say that's a minimum req for anyone intending to use lots of installs. You don't want to upgrade your production machines' OSes every 6 months. Fedora likes to be close to the cutting edge. Each Fedora release is similar to a Beta of RedHat's workstation/server product. Fedora releases every 6 months and each release is only supported until the next two releases are out. So you should plan to upgrade once a year to keep up with security releases. This is fine for a small number of non-critical machines, but I would avoid it on production servers unless you intend to take over security patch maintenance. Given a choice between Fedora or Ubuntu, for production work, I'd be inclinded to choose a Ubuntu LTS release, for the 3 year support window. Given the choice of any Linux distro, most of the major vendors are fairly reliable; I'd be inclined to choose CentOS or RedHat because I'm familiar with Fedora/RedHat's configuration, having used it for 12 years now. 

Generally the main difference is in the quality and features of the hardware, as well as the support offered by the vendor. Server hardware often has features to ensure uptime or easy maintenance in a data centre: 

I finally solved my problem. I suppose it was a Fedora problem all along but Fedora was not letting me edit system connections in NetworkManager. I changed NetworkManager's configuration so that it used its native connection information backend and edited the policy to allow my user account to edit the connection information; once I did that the "available to all users" checkbox finally started working. Thanks to all those who answered. 

It's basically a tradeoff between the cost and the benefit. Java, like all apps, has security flaws. Sun updates their JVM every now and then and the Linux vendors that ship Java also do the same. How do you plan to push updates to this machine? How important is it? If the machine has proper firewalls and limited network daemons running, Java as an interpreter on the disk is probably not much of a hazard. Java typically doesn't run as root. Almost any arguments that can be made about Java can be made about Perl, Mono, GCC, or any software that can run arbitrary code. Furthermore most Linux software is typically shipped by a distro, so you can often rely on the distro's updates to keep things secure. Sun Java, in this case, would be the same as any third party software. Does it matter if this third-party utility requires a library that happens to be Java? Maybe not. The scenario is slightly different if you are talking about Java running a service application, such as Tomcat or JBoss, where Java is then listening to the network. In that case the security risk is higher. But you have the same security risks with any network-facing application such as apache or ssh.