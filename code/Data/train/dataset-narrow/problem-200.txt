They are different concepts at different layers. A broadcast domain may cover multiple collision domains, or (if all the links are full duplex) there may be no collision domains at all but there is still a broadcast domain. 

Theres various possibilities here depending on the configuration of different devices. Most likely your ISP will simply block the packet based on either reverse path filtering or an explicit block on private IP addresses. If they don't block it then likely someone else along the route will. If the packet does reach the destination server(because noone is performing filtering best practices) then a reply will be generated but that reply will not find it's way back to your network. It will either be dropped due to lack of a route or routed to somewhere in the destination network that happens to use the same private addresses you used. 

per-packet load balancing is rarely used through, most load balancing is done on a basis of "flows" because re-ordering packets within a flow often leads to poor performance. The flow is characterised by some combination of header fields, often source/destination IPs and source/destination ports. In a per flow load balanced environment it is possible to get useful traceroutes but you have to be careful what traceroute implementation you use. To get a self-consistent trace the traceroute implementation must ensure that the relavent header fields are kept consistent such that all packets are seen as part of the same flow. I recall a presentation (think it was a uknof video) which mentioned a traceroute monitoring tool that had the ability to trace with consistent source ports across multiple traces and report which paths had problems. It looks like the video was $URL$ and the tool was "fbtracert" 

On clients, each socket is bound either explicitly or implicitly to a local port. This binding lasts as long as the socket does. Whether the same socket is used for multiple requests or a new socket is created for each request is up to the application developer. On network address translators a timeout based policy is normally used. If no packets are seen for a given mapping during the timeout period then the mapping is discarded. 

* a Link being a group of machines that are connected by a protocol at a lower level than IP. For example a single Ethernet VLAN (or a single ethernet network without VLANs) ** There were concerns about the blurring line between private addresses and provider independent global addresses. 

Many people in the IPv6 community are/were ivory tower types who deseperately wanted to belive that the only reason for using NAT was address conservation and/or to convince people that there were better soloutions to their problems than NAT. They did propose alternative soloutions but many of those soloutions carried high costs of their own (for example for "hiding the network structure" they recommended giving out IP addresses at random across the organisation which would massively bloat internal routing tables). Not everyone agrees with them though and as IPv6 has become more widespread NAT soloutions have become available. It is not really possible to do NAT with link local addresses as they are forbidden from passing over routers (and a NAT can be viewed as a special router) but it is possible to do it with "Unique local" addresses since semantically they are equivilent to global addresses. It is also possible to translate from one global address to another which is useful sometimes (for example when switching to a backup ISP). 

Libraries loaded by the application are generally used to implement stuff that runs on top of TCP/UDP including: 

It is certainly possible to mix brands. There are pros and cons to doing so, on the one hand mixing vendors reduces the risk of a single bug taking down everything and puts you in a stronger negotatings position when it comes to buying stuff. On the other hand mixing brands can expose interoperability bugs, may limit the use of some features and requires you to maintain familiarity with both. 

I can see a few reasons why the setup they went for may be preferable to the one you propose. Firstly I don't think you should discount physical constraints. The core and border routers may well be in different physical locations and adding an extra cable between those locations may be expensive. Secondly I expect doing it on the border routers is actually simpler. If a packet reaches a border router it's headed out of the network. Therefore the border router can have very simple rules along the lines of "send anything with a source IP matching 10.0.0.0/9 to nat box a" and "send anything with a source IP matching 10.128.0.0/9 to nat box b". On the other hand the core routers would need more complex rules along the lines of "send anything with a source IP matching 10.0.0.0/9 to nat box and a destination IP that is not in 10.0.0.0/8 or 1.2.0.0/16 to NAT box a" and "send anything with a source IP matching 10.128.0.0/9 to nat box and a destination IP that is not in 10.0.0.0/8 or 1.2.0.0/16 to NAT box b". Finally it may simply have been a case of the border routers having the needed features and the core routers not having them. Routing based on source address is a fairly advanced feature. 

There are many possibilities and substantial further testing is needed to narrow down which ones apply in your case. One is that the speed test you are using only actually tests for a very short period, so although you think you are testing multiple machines at the same time you are not. Another is that the speed test is downloading the test file from a cache in your network, not the internet. Another is that the 155Mbps is not in fact a hard limit but a soft limit which is only enforced after it is exceeded for a sustained period. Another is that your ISP only enforces the speed cap on traffic it considers "out of network" and there is a speed test server for the site you are using within your ISPs network. Another is that your ISP simply forgot to set up the limit on your line. ISPs are run by humans who screw up from time to time. Another is that your ISP is playing the "cheat at speedtests" game and this has resulted in speed test traffic bypassing the limiter. 

An application level gateway that understands the application protocol in question. Assuming the TCP connections are carrying some kind of request/response protocol the gateway will need to recognise the beginning and end of each request and response, merge the stream of requests from different clients to send them to the device and somehow keep track of which request belongs to which client so it can send the responses back to the correct client. 

As IPv4 addresses get in more and more limited supply the market price of an IPv4 address is likely to rise (it's currently about $10 per IP which is pretty cheap). As the price rises people will re-evaluate what applications can really justify a public IPv4 address and what applications will have to make-do without one. There are mechanisms that can be used to provide some degree of connectivity to the IPv4 internet while reducing the consumption of public IPv4. On the client side a provider can either use conventional IPv4 NAT ("CGN") or they can use IPv6 based transition mechanisms like NAT64 or DS-Lite. Mobile providers have been doing this for years, fixed-line providers are just starting to trial it. I expect the lowest tier customers to be pushed behind ISP level NAT first with more expensive packages retaining a public IPV4 address. NAT doesn't really work on the server side. You can forward ports but that doesn't really help when everyone wants ports 80 and 443. Instead the solution there is reverse proxies. These accept the TCP connections from clients and then look at what the client sends to decide where to forward the connection to. For plain http they can use the http host header (as used in name-based virtual hosting). For https if the client supports SNI then they can use the SNI information to forward the connection at a TCP level without decrypting the ssl/tls session. If SNI is not supported then the proxy must terminate the ssl/tls connection and look at the host header inside. If the certificate the server presents doesn't cover the domain the client is trying to access this will cause a certificate warning. Hosting with provider operated reverse proxies is still a fairly new thing (I'm only aware of one provider doing it) but I expect it to get more common as the IPv4 crunch deepens. 

Ping generates a packet destined for 192.168.91.5 The kernel looks up the destination in the routing table. The packet matches the default route (aka the default gateway). The next hop IP address is 192.168.1.1 The kernel looks up 192.168.1.1 in the arp table and most likely finds a match (you tend to talk to your default gateway quite a lot). The ping gets sent. 

Some vendors further divide their smart switches into multiple teirs, for example TP-link have their "websmart/easysmart" line which seem like total crap and their "smart" line which seems fairly decent. Before buying any peice of networking gear I would suggest downloading and reading the manual. This will tell you whether it suffers from brain damage like no VLAN setting for the management port and whether it supports the features you need. 

Your router may need replacement or a firmware upgrade to support the latter four options. The advantage of the latter two options is that the ISPs equipment is "mostly stateless", meaning that asymmetric routing and re-routes won't break things. It is unlikely at IPv4-only clients on your network will be able to access IPv6 only resources on the internet. 

That is more of a policy thing than a technical thing. Your ISP could offer a "dynamic public IP" option but they have chosen not to. So your choices are a dynamic private IP behind a NAT or a static public IP. 

Looking at the manual for your router I belive options 1 and 2 are not possible with your current router but option 3 looks doable. 

there are at least two uses of the "unspecified address" that I am aware of. Firstly before assigning it's first (link local) ipv6 address a node is expected to perform duplicate address detection. To do that it must send a neighbour solicitation packet but it doesn't have any address yet. Secondly the address is used as a wildcard in the "sockets" API for an application to tell the OS that it wants to listen on all addresses. 

In general the way to isolate a device is to put it on it's own VLAN. If you need to keep the IP addresses the same then you will need some clever tricks on the router. Basically on the interface facing the isolated device you need to do proxy arp for all IPs while on the interface facing the rest of the network you need to do proxy arp for the IP of the isolated device. 

Unfortunately there is a lot of reference material out there that is horribly outdated and/or doesn't make a clear distinction between "original Ethernet" and "modern Ethernet". 

Rewriting the MAC address would add considerable complexity (the switch would have to know about higher level protocols like arp so it could rewrite address resoloution), would make troubleshooting harder, would prevent protocols like STP from working and would generally be a PITA. It's also not normally needed. Which is not to say it's not possible. ebtables (the layer 2 counterpart to iptables) does have some options for MAC address translation. This can be useful if you have switches that don't use per-vlan MAC tables and you want to do some layer 2 filtering. $URL$ 

A VLAN is an Ethernet level concept, a subnet is an IP level concept. A VLAN splits an Ethernet network into multiple logically seperate Ethernet networks. A subnet defines which hosts a host will try to communicate with directly verses which hosts will need to go via a router. It also defines "network" and "broadcast" addresses. It is common practice to have a 1:1 mapping between subnets and VLANs but it is perfectly possible to have multiple subnets on the same VLAN. Equally it is possible to use proxy arp to split a subnet between multiple VLANs or even have two VLANs using the same IP subnet for different purposes. 

There are a few reasons why network standards limit the maximum size of a packet. Firstly the bigger a packet is the longer it ties up the line for. That means more latency and jitter for other users sharing the line. Secondly most network devices work with complete packets. The whole packet is received into a buffer before being passed on to the next processing step. These buffers have finite size. A large packet buffered at every step will also take much longer to deliver than a sequence of small packets. Thirdly with most standards if a packet is corrupted in transit the whole packet must be discarded and re-sent. So larger packets mean more data is re-sent when corruption occours. 

Even if all ports are the same speed switches need to be able to use store and forward as a fallback. Otherwise when the network started to get busy a large number of packets would end up being dropped because their outgoing port was blocked with another transmission. Regarding speeds if the incoming and outgoing ports are different speeds then the switch must take steps to accomodate that. If the outgoing port is slower than the incoming port then it can start outgoing trasnmission as soon as it has received the frame headers and determined the destination port but it will end up having to buffer the bulk of the frame. If the outgoing port is faster than the incoming port then it can't start transmission until it is sure it has received a sufficient proportion of the frame. Since "Ethernet II" frames don't have a length field that practically means wating for the end of packet marker making the switch effectively "store and forward". As to shared memory verses per port buffers it would be possible to build a pure shared memory based switch but the demands on the shared memory system would be extremely high. The shared memory would have to be able to support multiple simultanious streams of data running at different speeds and writing to different parts of the memory. The total bandwidth of the shared memory would have to be extremely high.