We're running 4 webservers through NLB, however every once in a while a single IIS dies on us (not the Windows, it still works, it's just the IIS returns 503 Service unavailable) Is it somehow possible to setup NLB to detect this, and stop sending traffic to that particular server ? Furthermore it'd be preferable if it could notify someone when this happens. 

After replacing these webservers with 2012 equivalents, this query always returns 0, even though my sites run ASP.Net 4. Does anyone know why this happens, and how to fix it ? 

After changing this to false the IIS stopped doing these full flushes. It still flushes some files regurlarly, but it's no longer a complete flush (around 50k files stay in cache) Our file cache hit % has increased from 5% to 33%, so this definately helps a lot. Not sure why kernel caching causes this behaviour, but I can live with the slight overhead of caching in usermode. Thought I'd add this as an answer to help others with similar issues. 

We're having a few issues with our image server. It's a Win 2008 running IIS 7.5 and it only serves static content: images. It has run without issues for quite a while, until recently when we disabled Output Caching, as we noticed having it enabled meant it sent no-cache host-headers to the clients (forcing them to fetch the images from the server every time) We've read quite a bit about it, and it seems IIS just works that way - either you use Output Caching or you get to use cache host-headers. Anyway having disabled the Output Cache, we now experience random 5 minutes intervals, where all requests just get a 503 Service Unavailable. During this period the "Files cached" performance counter staggers (neither increased nor decreased) and after the period all caches are flushed. You might find it weird I talk about caching, since we disabled Output Caching. The thing is we changed the ObjectTTL parameter in registry, so we cache files for 3 minutes (which has worked very well, our Disk I/O dropped significantly) So even with Output Caching disabled, we're still caching plenty of files - if we could just get rid of the random 503 it'd be perfect :-D We don't get any messages in the Windows event log during these 503 intervals, so we're pretty stumped as to what to do. Any ideas are very welcome :-) 

You might want to increase your shared memory settings when starting tomcat. this can be a temp folder issue (think of no space, no permissions, as Scott did write) Additionally it is possible that this is just too much ram you use: Increase your Xmx and Xms values and you might want to update your heap settings aswell. Ceck the SF search for "OOM tomcat" and you will find the rest of the things you need to learn. 

if its not a copy past error that your "proxy access" is via https then i suggest you check your vhost configuration for SSL. for all i know you need your mod_proxy setup in both your normal *:80 and *:443 Virtualhost Directive. so either your Virtualhost Directive does not have the entry or you should show us the *:443 Entry to try to help you more specifically. 

Yes they can. Basically this is a "reverse DNS query" (see the comments for another link). There are search engines which output you the amount of domains on your IP See for example sameid.net as there is most of the time a reason to use SSL for viewing a site or entering data and, unless you do use SNI, you would need at least one IP for every domain anyway. 

Out-Of-Office replies do usually get sent via return-path, yes. It's correct to the RFC specs, too. More info here: $URL$ 

Doesn't exist without custom hardware or software. The reason it doesn't exist is that your 'fast' 1GbE links are slow as hell when you compare them to the interconnect speeds inside any given PC. Here's a few examples: 

It looks like you've defined the group policy objects settings in the 'User' tree, but are trying to apply it on an OU that contains computer objects. GPOs won't work this way. See the part of the GPO report that says this: 

I think the key points with Netware will be: - Is your virtual environment going to support the file systems Netware uses? - Does the system depend on IPX/SPX? I know they implemented TCP/IP support from version 4.11 but it wasn't 'complete' at that point. The next version I'm familiar with is 6.5 where it was TCP/IP native. If you're stuck with some non-TCP/IP networking in the mix somewhere, I think you'll encounter problems running as a VM. As for the file system... a cold clone then power on just for testing purposes. 

Do some discovery to work out where each app is storing the settings that the user inputs on those 'first run' dialogs. I'm guessing most of them will be in the registry or INI files. Then pre-populate those reg keys and config files. If you can't do it by pre-baking the profile, try via a login script. 

We're running a Win 2008 R2 server with IIS 7.5 for serving image files. It's only used for static content, and file caching has been set up to cache files for 10 minutes. However the IIS frequently completely flushes the cache (seen by using Perfmon) It's not application pool recycling, it's not because the TTL has expired, so now I'm at a loss :-( I've included a screenshot of the perfmon graph where you can clearly see the issue. Is there anywhere I can see WHY it's doing these flushes ? (Note: I'm aware I could maybe detect it by attaching a debugger to the process, but that's not an option because it's a production server, and it cannot handle the slowdown a debugger would cause) 

I've been using the following WMI query to check how many IIS sites are currently running on my Windows 2008 webservers: 

We finally solved the issue. It was apparently caused by IIS checking for web.config changes in subdirectories. By setting allowSubDirConfig="false" in our applicationhost.config file, the problem stopped. This is somewhat weird though, as we haven't got any more than 1 web.config file, and it's in the root of the project - still if it works, why complain :-D 

Depending on your exact setup you might want to change the location slightly. Dont forgot to logrotate them if you have a high traffic site. Normally this should be (at least in debian) the default setting for how i remember it. If you have empty logs you might want to check the permissions of the folder/files so that the user the apache2 process runs under can access and write to them. Remember a reload of the process is (for what i know) needed for rereading the configuration files. 

Basically what happens is that dhclient adds a route to the routing table. It tries this while the route is already in the table. Check 

ftp uses 2 ports, 20/TCP DATA Port, 21/TCP Control Port check your firewall,routing settings for that port aswell. Depending on your connection to the server you might need to port-forward the passive port rage. see Proftpd online documents set in the config file: 

Do they have to change the lease time after getting the ip or is it ok to get it first? You can change the lease time per dhcp range. see the manpage. $URL$ Search for Lease To give you a hint how one could solve this: Personally i would go for a short lease on first connect, detect which user/device you give the ip too, and the renewal lease is the lease with the correct lease time. Though this depending on the amount of devices might have issues when you reload the configuration often (to have the new lease time read and send out) 

Finally found the answer to the problem. We had this in our web.config for the site (despite not running any managed code on the site) 

We're running a somewhat large site and have recently started have performance issues. If I open FireBug in FireFox and loads up a page, it'll sometimes wait for 6-7 seconds, before receiving the initial page. Then it'll start requesting javascripts, and they'll wait 6-7 seconds as well. After maybe 6-8 requests, it'll start serving them with a minimum of wait time (10-20 ms) We're hosting the site on Windows 2008 R2 running IIS 7.5. The site is in asp.net 3.5. We're also logging the time from PreInit to PreRender, and it's seldomly above 100ms, so it's not the code or database, which causes the initial wait of 6-7 seconds. And the code/database obviously have no effect on the javascripts (or images), which wait for 6-7 seconds as well. We're at a loss here, so any help is highly appreciated. EDIT: We've just discovered that it is in fact the DB, which is causing the trouble. Every 3 minutes a single core peaks in kernel time, and this corresponds with when the site is slow. Next step is to figure out whether it's a driver or whatnot. (It's the system process) 

You should be able to strap the disks together using this method, too, and present it all as large aggregate LUNs. 

Is the partition marked System Reserved? If so, this is required by the OS and shouldn't be deleted. However, it also shouldn't have a drive letter assigned, and should only be 100Mb. Has the server been built fully with apps etc, or is this still just a base OS install with no configuration? Outside of that, then it does sound like Dell cruft. In which case you may be able to remove the partition using a liveCD such as gParted. However before you do this, I would ensure you're in a position to effectively lose the whole OS install. gParted works most of the time, but not 100%, and I can't give you a guarantee that this mystery partition is completely safe to delete. 

Not right now. Currently RemoteFX serving is only supported on nVidia Quadro Pro chipsets and AMD FirePros. Best info I found is direct from nVidia: $URL$ "Quadro 4000, 5000 and 6000 professional graphics cards, as well as the M2070Q server graphics module" And ATI: $URL$ "AMD’s ATI FirePro™ v5800, v7800 and v8800 Series professional graphics cards." To date, MS has really struggled to package this whole thing into anything coherent, and the poor communication around it reflects this. 

Manufacturer: Product Name: Version: SATA controller: 2x SSD, 2x hdd each drive can do Sata Rev3 (6.0Gb/s) 

please check if the mapping of the user table did work correctly. as i have a similiar setup (virtual user/domain with mysql) i had issues with using the correct syntax/query for the user request. for now i would check if the user has actually a folder on the server. you use uid 5000 which could mean you used some setup where the user under which all virtual users are created is named vmail. if this is the case your setup (domains/username) should contain something similiar to this: 

Dont forgot to reload or restart the proftpd deamon. Then you would connect to your (home) router and set "port forwarding" "game application sharing" or similiar named (depending on the brand of your router) How you exactly port forward the ports to your PC is written in the manual of your router usually. 

the chrooted root directory is writeable by the user, this is not allowed anymore by the update Marek mentioned. So fixing it would require you to: Change the write permissions of the chrooted home root f.e. 

You could use a service script yourself (good) or include it in the mysql script (temp. workarround, ugly, wont work after updates.) something like /etc/init.d/mysql should exist. then you would search for and before mysql is actually started you would add your part of the script. (might differ a bit with newer versions though) on one of my hosts this would look like it: 

The primary benefit of cloud services to business* is found where a business is small enough that it's more economical to outsource to a third party than attempt to run an in-house version at a small scale. The problem with evaluating the benefit, however, is that a great deal of the IT cost that can be saved are intangible. Dollar-for-dollar, even small players can purchase 1Tb of SAN storage space cheaper than it can be had over a 3-year period with a cloud services provider. But once you strip out the time spend administering your chosen storage technology, the risks of losing disks or a staff member screwing up a configuration, power, cooling and ongoing support, the cloud offering may work out more attractive. Also, it tends to be the case that the the parties responsible for a companies financials favour predictable interval charges to one-off unpredictable charges. This means 'renting' is more attractive than 'buying' even if buying is cheaper overall. A vendor offering you a monthly rate for X amount of disk, bandwidth, RAM, CPU or whatever is more attractive from a financial standpoint than outright purchase of expensive IT equipment (plus all the aforementioned intangible costs associated with this). The benefit here in moving 'into the cloud' should start to become apparent to small and medium business, if their current situation is the Datacenter-in-a-broom-cupboard approach that we're probably all familiar with. Cloud providers can provide the same or similar service and do so reliably and cheaply due to the economies of scale that small businesses simply can't tap. However as you alluded in your question, companies who's business 'is IT' are often of a scale where these benefits start to be realized internally and in those cases a 'cloud provider' often can't offer as much. However, this needs to be taken on a system-by-system basis, because even a company that specializes in (for example) online data storage will probably benefit from moving their internal IT needs (email, test environments, document management) out 'to the cloud'. Their core business would of course remain in their own racks via dedicated links that they control, but they can easily remove the hassle of maintaining unrelated IT stuff (that every other company under the sun also requires) by farming it out to someone who specializes in delivering that service. This means the IT bods at that company can then spend their days focusing on their core business (in this case, their file hosting service) rather than running around wasting hours troubleshooting their company exchange server. *Bear in mind that I work for a cloud services partner and spend my days helping clients 'move into the cloud', so my perspective on this is both informed and biased in equal measure.