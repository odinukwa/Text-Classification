I'm stuck at setting transaction isolation level. Here's my scenario that happens in the application: 

Begin transaction (serializable) Get unprocessed messages (using the flag) Set their to true (in RAM) and update their status Commit transaction Do the business Set their to false (in RAM) Begin transaction (serializable) Update their status Commit transaction 

Of course the overall design and query is much more complex and more details are in action in selecting next new word for a given learner. Now, imagine that words list contains 100K words, and a learner has already learnt more than 5K words. Using the given query, this gets slower and slower and slower by more learners learning more words. Is there a better design for these types of business requirements? How to design for scale in this case? 

Learner can declare that he/she knows a given word Learner can memorize the given word (to be managed by the application later) Learner can choose to ignore the given word, so that it won't be given to him/her later Admin can ignore a word in general, so that it won't be given to any learner A list of words exist in database, as the reference Next word to be given to the student, should be new to him/her 

Without knowing the query set that's going to hit this structure, I'll give a generalized answer: I try to use the TIMESTAMP type for this, which works well with time/date functions, and is timezone agnostic (under the hood it is storing a unix timestamp). By default, this will only be a 4 byte wide key (instead of an 8 byte bigint), so if you index other columns (which always tacks on the primary key at the end...albeit hidden), those structures can stay skinny and quick as well...and also not eat your RAM ;-). TIMESTAMP by default has a resolution of 1 second, which is overkill for your purposes, but the easy use of date functions on it makes that worth it. Main downside: TIMESTAMP is based on a 32-bit unix timestamp integer (it stores that way, but displays as a date/time), so it does eventually run out. At that time (decades from now, when RAM is cheap) you can adjust it, and the date/time display/handling nature of the type will allow you to make the change transparently to your software. If you want to use it as an integer, you can use FROM_UNIXTIME(your_integer_here) to insert, and UNIX_TIMESTAMP() in queries that need that. I'm going to guess though that the date/time format is easier to work with for you though. Also, you do not require a second UNIQUE key, as any PRIMARY key implies this already, and is included for free in the storage/RAM price of your data table (which is sorted by PK as a B-Tree...your table is an index, and can/will be used as such). 

This of course results in tremendous problems, because I'm working on a text-processing application and data comes almost from everywhere and I need to normalize text before processing it. If I know the reason of difference, I might find a solution to handle it. Thank you. 

I have table that has fields, and another table called , which has a one-to-one relationship to the table. stores information about the message, including its text, and subject, and the recipient, etc. 

Instance A gets some unprocessed messages While instance A is setting the to true in RAM, instance B gets some messages, and chances are that it fetches one or more of the messages which are already fetched by instance A 

And it has non-clustered indexes on and fields. I want to find what a has sent us in a specified period. Thus I run this query: 

If you want to store it as a hierarchical structure, the pattern that would best serve you is the "Closure Table" pattern, which allows for subtree operations (select/update/delete all descendants of x). A generic example (closure table and data table ) of what flagging an entry and all children at all levels below it would look like this: 

This is an alternative solution that is similar to Rick James' answer, but with a way to avoid app walking, and enjoy a fixed number (1 or 3) of statements to UPDATE the entries in all 3 applicable levels: Create a table for each of your well-defined levels of event (I will call them , , and to keep this generic), so that has field ., and middle has field .. If your case remains simple, you may be able to enjoy MySQL's ability to update more than one table simultaneously, and require only ONE statement: 

You haven't provided any way of telling what the problem really is. One thing to check though are whether you have any long running transactions. They might be idle but still able to see old versions of the view rows. That would mean that each refresh would effectively add another copy of you view's materialized rows. See what is in the system view and pay close attention to the timestamps. 

Step one - go into task manager, have a look and see if anything is actually happening. If you can't see a lot of cpu and/or disk bandwidth then it's not doing anything. Step two - check what PostgreSQL says is happening. There are two system views useful here: pg_stat_activity and pg_locks. See what's in both. The database/relation numbers in pg_locks are internal OIDs - try SELECT OID,* FROM pg_class to see which tables/indexes are being locked. My guess is that the system is idle and it's just waiting for an exclusive lock on the table in question and something else is blocking it. The two system views should show you what is happening though. 

But this database is used by many concurrent threads, therefore it's possible for a message to be taken by more than one thread, hence be sent twice or even more. If they were a single table, I could use statement to select unsent messages in a single transaction. What options do I have though to make selection transactional, so that a message won't be sent twice? 

This works fine when runs sequentially. But when I run multiple instances of my application (concurrency), I see that some messages are processed twice or thrice. Here's what happens: 

Of course it can be solved in a minute of Googling around, via some tricks and scripts. However, the more we search the less we find out about the reason behind this. Why it has happened in the first place at all? What possible reasons caused it, and might cause it again? Was it because of an attack? We have no clue, and any help is appreciated. Update: in this post it's been argued that a policy check can make this happen. We haven't set a policy so far, and we do not even know where are the policies. Update2: We realized that some of the threads in some applications kept working, even though other threads and other applications were encountering message. Could it be that doesn't require re-authentication? How is that possible? 

If I were you, I would use a view to transform the date with the DATE_FORMAT function: $URL$ $URL$ To avoid refactoring your SELECTs, rename the table out of the way, and use the original name for the view. As long as you keep the view simple like that, it will perform about as well as the physical table (negligible overhead) on anything except for anything that might enjoy an index on that date field. Because the DATE_FORMAT function make the view non-writeable, you still have to refactor your write statements to use DATE_FORMAT to go the other way. 

Otherwise, you will always only need 3 statements to select/update/delete all rows relevant to an entry in : 

The table it is referring to used to exist there, but was renamed into a different schema and then dropped, or possibly just dropped (foggy memory) in order to get all references to it to follow into the other schema. Ultimately, the table was drop-swapped for one intentionally established as a copy via replication. It doesn't appear in information schema, doesn't appear in the SHOW TABLES list, can't be queried with SELECT statements, and DROP/ALTER TABLE for that name in that schema (after getting a view out of the way) just gets us: 

And this query works lightening-speed fast for past 2 years. But when I change the part to a closer date, it freezes out and takes more than 2 minutes to complete. In other words, based on different inputs, it behaves differently, sometimes even hanging out and not returning for more than 10 minutes. I expected a consistent behavior. What do I miss about indexing? What can cause this inconsistent performance? Update: I changed names of columns and table, so I can't attach execution plan as a picture. But here's the issue. Thanks for guiding me. when I change value of date parameter, SQL changes index seek from to . I never thought that SQL creates execution plan based on the value of parameters. How that could be? 

As soon as I start this query, it finishes. Thus, I get no chance to see to find out what type of lock this query has applied on database objects. I can inflate this query with transaction statements: 

I can think of a perfect case for it, and we have tested thoroughly and run it in production...I call it the "fast lane" clustering strategy: If you do read-write splitting with a proxy like MaxScale, or your application is capable, you can send some of the reads for those seldom invalidated tables only to slaves that have the query cache turned on, and the rest to other slaves with it turned off. We do this and handle 4M calls per minute to the cluster during our load tests (not benchmark...the real deal) as a result. The app does wait on master_pos_wait() for some things, so it is throttled by the replication thread, and although we have seen it with a status of waiting on Qcache invalidation at very high throughput, those throughput levels are higher than the cluster is even capable of without Qcache. This works because there's rarely anything relevant in the tiny query cache on those machines to invalidate (those queries are only relevant to infrequently updated tables). These boxes are our "fast lane". For the rest of the queries that the application does, they don't have to contend with Qcache since they go to boxes without it turned on. 

OK - if you can't dump + restore, and don't want to understand your system well enough to set up replication then you've got only one option left. You will need to run identical setups at each end (version of PostgreSQL, 32/64-bit, compilation options, file paths etc). Then, scp the entire data directory - including all the WAL files (pg_xlog) and all other sub-directories. During a quiet period, halt your primary server and use rsync to bring your copy up-to-date with respect to the original. Then, start the copied server and off you go. Whether you can do this in less than 4 hours, I don't know. It will depend on how efficient rsync really is, the latency between both systems, how many updates you have had etc. It might be that running rsync before halting the primary server will make the real run go faster. Difficult to say. 

It's been too long to remember how we got here accurately, but we get the following error on MySQL start (though it does still start the service with InnoDB): 

...exactly as we expect. There is, however, now a view by the same name in its place (which refers to the new table in the other schema), and it has a corresponding .frm file, and of course, no .ibd file, as we would expect. Further, the table in the new schema works just as expected, as does the view in the old schema that aliases it. In fact, as far as we can tell, this is causing no problems...we just want to stop worrying whoever sees the log. We can even create and drop a table in its place (after moving the view and before putting it back), and still get that error. So why does MySQL expect an ibd file by that name in the old schema? How do we convince it to forget?