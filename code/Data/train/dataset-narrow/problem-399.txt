One way to achieve that would be using to stuff all the products for each order into one line, then match up all of the orders that have the same products and use again to stuff all of them into the same line. See example: 

You could deal with the issue by specifying the style and doing an explicit from string to datetime rather than an implicit conversion. $URL$ 

Then you could also add some constraint to the table if you want to enforce a rule that two courses can't be timetabled at the same time. If a course has multiple sessions during the week each will be a row in this table. 

That looks a little awkward, so if anyone can improve that date logic happy to take suggestions. If a doctor wants a break, then enter the break as an appointment and it won't be available for booking. Note that the table constraints don't enforce non-overlapping appointments. This is possible but it's more complicated. If this were my system I'd think about some system (e.g. trigger) to finally verify that the appointment doesn't overlap with an existing one at the time of insert, but that's up to you. 

As mentioned, the leading % makes things tricky, but if the event_details field contains a list of delimited items and you are searching for one of those items then you could create a child table event_history_details that contains one row for each item in the event_details field for each event. Such a setup would scale well and be SARGable. Unfortunately it involves changing more than just the query. 

I want to count correct col upon ct_id(course) col. I queried a lot but gained no success. How to query such a thing? for example in SELECT I should have two columns: -correctNo -incorrectNo for each course(ct_id). Note: The last column(correct) is a derived column: IF(selected_answer=answer,1,0) as correct 

is just 4,688! It is not much compared to the total data of the collection which is 30M documents. When Mongo gets slow when it has domino effect, CPU usage and Memory is not high. Mongo just uses 40% of the memory. Disk partition is if that helps. Another example of a very slow query in full details: 

An easy php based web application to produce very realistic data to populate your table. Its online site has limitation, but if you download the source code and install it on your PC then you can generate every kind of data that you want. It supports csv,sql,html,etc file formats. 

Now users in the name of who has been added by uid=60 should not have be shown! The result of this query is empty. I can't figure that out, what I'm doing wrong? 

I changed hostname from to . Moreover SSL keys are generated for . Mongo is now up and running with . 

My current storage engine is and its compression level is as default, snappy. I've come across MongoDB documentation and it's been mentioned that using zlib compress better but needs more CPU. I want to know will store more data in memory compared to as it compress the data? I have a server with 16 CPU cores. As RAM is more expensive I'd rather to save on memory in case it keeps more data. Is this correct? Can I blindly switch to zlib to cache more data and improve read performance? NOTE: Our server is read intensive. 

Of course, you may not have permission to do 2 or 3, in which case it's a question for the person that does have permission. 

When I load this data into my data warehouse I load it into a table that has a parent-child relationship like this: 

The drawbacks to this approach are that now your writes take longer so that you can do faster reads, and this might not be suitable if the filter that you use in your query on the column isn't predictable. 

Storing in mysql table should be fine. You need to consider how the data is going to be accessed and retrieved when you're designing the table and indexes on it. I'd say that you're likely to be doing queries which return all of the past searches for a particular user (or perhaps even just the most recent by user) so it would probably make sense to cluster the table by and . This design should also help reduce contention if lots of inserts are happening at the same time since each user will be inserting into a different part of the table. 

Any queries that read the table or with isolation level will proceed despite your . More info about lock modes can be found here: $URL$ Personally I'd say that if you have queries reading from the table then they should fully anticipate that they will get bad results so blocking those reads wouldn't be a priority. If your those queries need to return data that is actually correct then they should not use . $URL$ 

We have about 15GB of free memory that does not use! On peak we reach 400 OPS, and 500 connections per seconds. Is there anything I could do to improve the performance? 

EDIT 2: The output of in order to remove extra information based on read and write: $URL$ Another result which reached around 1000: $URL$ 

I have enabled SSL on with optional SSL connection: . I have used in order to obtain SSL certificates. SSLs works as expected and in file I can see that: 

But in the query I get the result I want, the part is not . Is there someone who could explain this and shed some light on the subject? 

It's good to note that I have done the same exact procedure on another server and it went all ok and MongoDB works as expected. Now my config is as below: 

I have used to generate SSL certification. All the private key, chain and certificate is generated by `let's encrypt. Now to use it in I first merged and into a file called : 

In my recent project which is about social networking for an Asian country, I'm in doubt whether I use the below SQL statement for counting records: 

I have a unique compound key like fr(fromid,toid) in the table, when I run the query with explain I get the following result: 

It is not necessary to distinguish between measures and dimensions when defining your columnstore. All columns are stored in the same manner. All columns included in any query should be in the columnstore, in most cases it is best to put every column in the table into the columnstore. 

This is an alternative scheme to the ones already proposed, it's a bit like the summary table, except that it isn't aggregated, just 'partitioned' by player. 

Test this and see if progress is being made. Further steps may be necessary but hopefully this is in the right direction. 

I think this is probably a good design that you proposed. Just make sure that the table is indexed properly. Consider that every query to the table will probably filter by content_type so that should probably be the leading column in a multi-field index. 

Edit As per discussion about the proper way to handle revisions with this is an alternate version which does not consider a revision with when looking for the latest revision. The has been moved into the inner subquery. 

We can insert some data to this table to see what it looks like. Note that the third insert will fail because it is prevented by our constraint. The doctor can't have two appointments starting at the same time. 

Problem solved: It should be an unknown option as I've put it below [mysql] not [mysqld]. I put all the parameters below and I went through the process again and now it works just fine. files are in their respective database folder. 

How filtering fields affect performance? Is the performance related to the size of data that is transmitted over network? or the size of data that will be hold in memory? How exactly this performance is improved? What is this performance that is been mentioned in documentation? I have slow MongoDB queries. Is returning a subset affect my slow query (I have compound index on the field)? 

NOTE: My mongoDB is a single primary node in which replication is enabled (Use to capture changes on DB) Now when I restart it gives error below: 

I want to get one post(e.g p_id=8) and all of its comments. how to do that? How to get count(*) from comment table while your getting your post record? 

: Number of times the operation acquired the lock in the specified mode is so high compared to the fast query (on another collection) that has the below status: 

When there is a unique key in a table (not ) and procedure is running, on duplicate key error the whole process will be halted. I want to resume on error and call the procedure again. The procedure: 

If a user can have no more than a single role, and this will ever change, then put directly on the table, then the relationship between table and table is a single role to many users. 

In SQL server you can only add a single user per statement, so your script is about as good as it can get (aside from adding Billy and Bobby to the role). $URL$ 

Let's assume that you have a numbers table. If you don't many other people have described how to create one. If all else fails, this could create one for you but it's probably not the best way. 

You need to specify which columns you want to insert into in the insert statement. In the statement you wrote, the engine doesn't know which 7 columns out of the 8 to put the data in. $URL$ 

You mention that you added single-column indexes for each foreign key in your fact table. Often at least some of the foreign keys have low cardinality so they are likely not useful in an index on their own. $URL$ They may be more useful as part of a multi-column index which you can design based on the way that you expect users to query the table. If your workload suits it, then non-clustered columnstore index should be considered on large dimension tables and fact tables. They are ideally suited for data warehouse workloads. $URL$ Since you are using 2014 then non-clustered is the only option if you want to keep constraints and other indexes. 

There is about 100,000 records in ips table. Is there any query to the following scenario: Check if is equal to , if it's equal then add country.coutryid to that record. I couldn't think of any way to do it. Do you have any idea how to do that? 

I've read in an article about social networking database schema and saw that they've used varchar for those columns that I mentioned in the question title. It's not normalized! Isn't it better to have a table for sports and then put the foreign keys in user's profile table? In this case we'll have much less redundancy? Please correct me if I'm wrong. Which approach is better have another tables for fav_sports,fav_videos,fav_music, etc or put them all in user's table? 

I've seen so many times that we use numbers like 5,6 or 10 at most. What is this concurrency? Doesn't it refer to number of simultaneous users in bench? EDIT I'm not talking about any specific benchmarking tools, I mean in general. In benchmarks for databases OR for apache servers we use low concurrent numbers. why is that the case? 

I want to fetch those users that is not in a specific user's circle (Like G+). It means that user has not yet added those people to his/her circle. I tried the below query but it was empty: 

It could go something like this. Grab the old data into a temp table. Insert it into the history table then delete from the live table. I'll leave the error handling as an exercise for the reader. 

After the data is inserted, it counts how many rows in the table overlap the time range of the inserted row. If there are more than 1 (remember, the row has already been inserted so it will overlap) then the transaction is rolled back. 

The query you wrote is basically asking the database to give you any two rows of data (any two orders). If you wrote the query with an clause then your results should be the same each time. 

When you created the clustered index with the rows in the table were shuffled to be in that order. When you dropped the index the rows were not reshuffled. There is no guarantee that the rows will be returned by a query in the order that they are stored, but often this is what happens. If the order is important then you would add an (and would be well advised to cluster the table on that field too to avoid sorting every time). But don't choose a clustering key just because of that. For a more thorough discussion of clustering and heaps see $URL$ 

Why is this happening? P.S.: I should note that Horde_groupware database have innoDB tables, when everything is messed up and I I get the error says bad information in .frm file. 

I want to insert about 14000 records in a junction table, but the problem arise when there is a duplicate key for unique(iq_id,q_id)? what do do? 

I've read in a forum that if your database capacity is more than 1G you should buy its license. Is this true? How much will it cost? 

As option separates table files instead of putting all data and indexes of DBs into one ibdata file, is using this option improve speed of alter table? I have a table of 40M rows and when I alter a specific table it takes about 5 to 6 hours. Does this solution help? Is there other ways around to improve alter table speed on heavy tables? 

I want to fill this table with lots of data. Let's say millions of records. How to do this? I need to produce unique emails. Different data for emails. I know I can use but don't know how to produce random data 

Why it reports that ? Is there something in between that I have missed? Could someone shed some light on this? 

Or update a field like comment_no in shares(posts) table? I have fields like shares_no,comments_no,likes_no. Everytime a user click the likes_no field will be incremented and other fields as I explained. This way I have to update every record each time a user post a comment or shares a post or likes a post. Should I use and so on for getting the record count or just updating a record. There will about 10 to 20 posts each second. Site's traffic will rose. Which approach should I use? Which one is more expensive? EDIT: I've used as below to speed things up: 

Consider having a row in the table for each attempt that the user makes at the test. This means, then, that you would have to make the composite primary key of the table to be and (which can be an column). I would maybe think that the table should be a many-to-many relationship between and rather than between and , assuming that each time a user attempts the test they may get different questions. Either way the primary key on this table needs to include more than the single foreign key, it should include both foreign keys. 

Or you could use a to find people with both tags. (Note that the was added to handle the case that a person may have the same tag twice. If that is impossible, it's safe to remove.) 

I would suggest an table which stores the current appointments for each doctor. We can add some constraints on this table which limit the appointment start times to even ten-minute times (e.g. 9.00, 9.10, 9.20) plus add some other common sense checks like after and doctor can't have two appointments starting at the same time. Assume that you'd also like doctors to only work between 9am and 5pm, because everyone needs some work-life balance. 

Your table is extremely wide, and every time that you run a query it needs to read the entire table to find the rows needed. There are two ways that you could reduce the amount that must be read, firstly by reducing the number of rows that the DB must read (e.g. by clustering the table by ) but you suggest that most of the time, most of the rows will match the date criteria, so that probably won't get you very far. The other possibility to reduce the size of the data is to reduce the width of the rows. I don't know a whole lot about baseball so I don't understand what those fields are storing, but I can guess that almost all of the fields in your table don't need to store numbers as high as 2 billion. I think I counted 88 fields for about 352 bytes per row. Changing them to or could save 180-260 bytes per row. Along similar lines I think there are 28 fields devoted to ids of umpires, managers and pitchers. Perhaps if this data is infrequently queried it could go into another table with the same primary key; only join the tables when necessary. Maybe some other fields too. If you could do both of these then scanning the table would probably be far quicker which can make up for be fact that your queries seem very difficult to index for.