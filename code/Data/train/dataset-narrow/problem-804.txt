So I need to be able to extract the extensions up to 10 characters. I tried TOKEN() but with folder names possibly having periods, that didn't work well - adding as a token delimiter helped some, but I still got odd extensions from folder names. I can't find a combination of SUBSTRING(), RIGHT(), FINDSTRING(), and/or TOKEN() that will meet the rules and the derived column definition doesn't allow logic like IF or IIF. Some false positives are expected and I plan to sort them out after importing. If it helps here, I have a second column in the CSVs which is extracted by SSIS, it's the size of the file (for folders it populates a 0). I haven't gotten this to matter either, because of the lack of IF or IIF in derived column definition. 

23068672 bytes = 22 MB, not 20 GB. If it were really 20 GB the only thing I could think that would make it that big would be reusing the same file for multiple backups and appending each backup to it. 

I'm asked to help a developer figure out why the "random" data we should be getting from Oracle is exactly the same results each time you run it, rather than being actually random. SQL 2012, Oracle 11g, linked server is using OLE DB connector type. When they run the following query directly in the Oracle DB, they get proper "random" results. Through the linked server in SQL, they'll always get the same rows (and I've verified this by running the linked server query myself). 

Edit: After reading initial comments and looking into this closer with the BI guy, the query Tableau generated is very poor and could use significant cleanup. The database has a total of ~3900 rows in all tables, so even the 94k means a significant amount of crossing and duplication is already occurring from one-to-many relationships and Left Join. I simplified the query a bit, removing about half the columns in the SELECT portion. 

Note: I'm not an Oracle DBA but I work with one; he didn't see an issue with the Oracle part of the above query. Also, I edited a lot of stuff out and replaced with XX for privacy. Each time we run this query in SSMS, we get the same 200 rows. I even connected from another SQL server that has the same linked server and ran this query, and got the same 200 rows (the same Oracle user is used for both connections). This leads me to believe Oracle may be caching the results; but somehow it's not doing this when the query's run in Oracle directly. Is there a way to force it to return new results through openquery? (so that the 200 rows it returns are a truly random sampling each time) 

First, I'll say that is frowned upon for production code; if you're just testing or running a user query periodically I can overlook this, but if this will be part of a stored procedure or something lasting, you should replace with the list of columns you're looking for. Second, you have a couple of options. You can exclude '%text%*4' like Michelle pointed out using WHERE NOT LIKE. If you're going to try to include it, you'll need to cast or convert the normal contents of the column to a string type like . That should allow the '%text%*4' to come through in the results. The other way which would retrieve the linked comment you're looking for could look something like this: 

You're getting the from the clause of your Case structure because NULL doesn't evaluate. NULL bypasses the test. Null values can be difficult to navigate. Most recently I've seen this article from Brent Ozar's team discussing NULL values and their effect on join logic and performance. For most evaluations you can use ISNULL but keep in mind that may affect query performance. 

Populate the ContactInformation table with the PersonID, then an email address/phone number/mailing address/twitter handle/etc and mark it with a ContactType. This could be 'MA' for mailing address, 'EM' for email, "TW' for Twitter, 'TT' for Term Time as in your question. Triggers could validate any inserts to make sure contacts don't have a TT (term time) address unless they're a student. Hope that helps you think outside the box. It may be too hard to make these changes at this point, if you're already deep in the application. Edit: I didn't feel I properly addressed your direct question. If I were you and making deeper changes isn't possible or feasible at this point, I would add TermTime address to the Students table. It belongs there because 1) it only applies to Students, so having it in the Person or Contact tables (as they exist now) is less than ideal, and 2) Each student should have one so it's already part of their student record and 3) this can be done with the least disruption to the existing schema. 

I'm going to disagree that SSDs are the de facto standard, though they certainly are far more common than a short time ago. I know my company doesn't set any standards, but nearly all of our virtual database servers still use an auto-tiering partial-flash SAN, against my better judgement And unfortunately, I'm going to start my answer with an "it depends." In general, index maintenance is far less important than it was in prior years and older versions - specifically, the long and deep index rebuilds that used to be offline-only. The days of "reorganize everything and rebuild if > 30% fragmented" are pretty much over. I think most would agree that rebuilds are no longer worth doing, and SSDs do play a part in that. On the other hand, statistics have less to do with fragmentation and physical distribution of data on disks and more to do with logical distribution of data across/among the table(s), so this can still be important. Whether it's important for a given workload depends on a lot of factors. If you can test stopping these maintenance routines in a non-Prod system, you can look for poorly-performing queries, then analyze the DB engine's steps in the Execution plan. All of the statistics issues I've seen (and I've been dealing with some recently) have been easily visible by looking at the operators in an execution plan and seeing huge differences between "estimated number of rows" vs "actual number of rows". If the queries perform poorly enough, execute a quick statistics update and try again. It's a bit of work going through all of these steps, but once you have a handle on the system you can tell if statistics maintenance jobs are worthless overhead or important and necessary. Erin Stellato has several great blog posts about statistics that can help with background information on why fresh statistics can be important, and also when they're most useful and how to detect if they're not updated frequently enough. 

No, the applies to the query and not to the table created by . Using is unnecessary in this statement, because it doesn't even guarantee the rows are inserted into the new table in that order. See INTO Clause (Transact-SQL) 

Your function, is not able to open the file you're passing to it with the variable. I don't know if you redacted the filename, or if it actually has an empty string in single quotes, which would mean the variable was empty. You probably should debug and check the results from . Also note, these functions are deprecated and it's recommended to use Extended Events instead. sys.fn_trace_gettable (Transact-SQL) 

I wouldn't have the ContactInformation linked 1-to-1 to Person anyway; storing several addresses like that in ContactInformation violates normal form. Instead I'd have ContactInformation with its own PK, an FK for PersonID back to the Person table, and store as many Contacts as necessary for each Person. 

I'm importing a large set (126M rows) of file data from CSVs. Import works fine, except I'm trying to derive a column as I import rather than trying to script it out afterward (due to the number of rows). SSIS is not terribly helpful with string manipulation functions, and the possibilities I have are pretty complicated as well. The derived column needs to reflect the file extensions. File extensions range from 1-10 characters (anything longer can be truncated at 10), and are (of course) separated from the file name and path by a period. They don't correspond to a concise list of file types (something like "docx, xlsx, accdb, msg" are Office types). The file path may also have periods in it. And in some cases it will not have files at the end of the path, just a folder name itself. Some examples: 

I have a BI admin that created a tool-generated query from Tableau, connecting to a custom database application that tracks issue reports for follow-up action. I can make almost any changes to the database, though I'm not the owner. The requirement we're having trouble with is that certain "admin users" need to be able to see ALL database items, while "normal users" should only see items that pertain to them (that they reported or they're responsible for resolving). The BI admin handles this with a Cross Join to the "Groups" table, filtering for the Admins key - and this takes the result set from 94K rows to 2.4M rows, and the query response time from 12s to 5m12s. Obviously Cross Join is to be avoided wherever possible, but I'm not familiar enough with Tableau or other reporting suites to suggest an alternative way to handle admin access vs normal user access. What's the proper way to do something like this?