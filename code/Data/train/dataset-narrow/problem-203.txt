From the info provided in the update this really seems to be an issue ISP side. However, I would contact the ISP support with factual information, explaining what you told us here (connection drop this often, come back when they reset their equipment), provide them the tcpdump, and ask them to assist you in troubleshooting the issue. Then let them draw their own conclusion. This approach is often more successful than saying straight away "fix you damn stuff". And some times, it saves your face when it appears that the issue was actually your side. 

I don't understand what kind of answer you expect. You have to figure out how the cabling is done. There must be some kind of logic and you should be able to find it without testing all 400 cables. You have to use a tone generator and a probe to identify in which panel a few jacks are connected and how each patch panel is linked to the main patch in the networking room. Looking to the numbering at each side you should be able to guess the logic in the numbering... if there's any. 

3 - To ensure that VLAN can not communicate together, you have to configure ACL (Cisco) or Firewall Rules (Ubiquity). The SMB RV325 support 50 ACL rules, while the Ubiquity should not have limitation (other than memory) and you can either use interface based firewall or zone based firewall. 

You just need to wait for the port to goes in forwarding mode. By default on Cisco switches (and it seems in packet tracer), when a port goes UP, the STP process block the port until it determines that the network is loop-free, by listening for BPDU on this port. To avoid this you can configure ports that are connected to hosts (I.E. not other switches) with the command. This will cause the port to goes in forwarding mode immediately upon connection, but still run STP discovery and block the port if a loop is detected. 

I have a Brocade FastIron FESX648 and I'm attempting to increase the ARP cache timeout on a port that is connected to an IXP. According to the Brocade docs, I can increase the timeout up to 4 hours with the setting . I've tried applying this setting globally, specifically to the IXP-facing port and variety of other combinations (lower and higher timeout values), but nothing seems to work. Whenever I run I can see that the ARP entries for IXP hosts are continually refreshed and never age higher than 4 minutes. Has anyone faced a similar problem with Brocade FastIron gear and have any config suggestions or possible workarounds? 

I use an iSavi IsatHub for Internet connectivity when I'm in the backcountry out of cellular service. This device is a BGAN terminal and it works by establishing a data link with an Inmarsat satellite and then provides a WiFi network to connect your devices. Data usage is very expensive, so it offers a number of safeguards to prevent accidental usage. One of those safeguards is a firewall. I only need SSH connectivity, so I blocked all outgoing ports with the exception of TCP port 22, UDP 53 and TCP 53. All inbound traffic is allowed. This past week I was in the backcountry far from cell service. I fired up the IsatHub and connected my phone to the WiFi network. Much to my surprise, I started receiving text messages to my Verizon phone. More to my surprise, I was able to reply to those text messages and have a back and forth conversation. The satellite terminal has its own SIM card and SMS capabilities, but these messages were received over my Verizon SIM. I've searched the net for any documentation on network ports used for WiFi texting and calling. I found this and it does include TCP53 and UDP53 (which I assume are for DNS lookups), but it also includes UDP500 and UDP4500 which are standard IPSEC VPN ports. Does anyone have an idea of how this was possible? How the heck was I able to send text messages over WiFi with my Verizon phone when I've blocked all outgoing ports except those needed for SSH and DNS? Is it somehow related to accepting all inbound traffic? Could Verizon be re-appropriating port 53 for WiFi texting? Edit: When I returned home, I connected my phone (airplane mode, wifi turned on) to my home WiFi network, sent a test text message and took a capture of the network ports in use. The only ports I saw in use between my phone and Verizon-owned IP addresses were ports UDP500, UDP4500 and TCP443 (500 and 4500 being used for the IPSEC VPN ports I mentioned earlier). 

ETA: To clarify, by clientless VPN I'm referring to the Cisco VPN usable through a web browser - almost a sort of portal page. The one that looks basically like this, although the appearance has been updated slightly since - Cisco example screenshot 

Having some links that use the clientless VPN and some that don't is in fact possible through content-rewrite rules ($URL$ 

There is no recycle bin equivalent when deleting CIFS files from a Windows server via the clientless VPN. There is no way to enable any kind of confirmation dialogue for file deletions at this point in time. So people can be trying to click the "favourite" button, be off by a millimetre, and accidentally delete with no confirmation and no trash to recover from. :-( They've put in the ability to turn on confirmations as a feature request, but there's no timeline on when it might be implemented. So guess we won't be using this feature, which is sad. I added our nbns server to the Cisco config, and now clicking the Browse entire network button shows me the domain. However, when I click on the domain, it says "Failed to retrieve servers". A had a support tech look at this, and he said the config all looks fine, and he found a few other instances of this for other users. He said he'd investigate and get back to me. 

While waiting, I disabled file browsing. A few days later, when he wanted me to show him the issue, I turned it back on and it was working! (There's still a second domain showing in there that isn't on my nbns server...still no idea what the deal is with that. But the real domain is working.) So I don't know if the browsing just basically needed a kick in the pants after I added the nbns server config? No idea. But it's okay now. 

Unless the wifi is intended for administrative access, this would not be a great idea. You will encounter all kinds of interference and increased latency. Your server cabinet is most likely bonded to the ground and essentially amounts to a Faraday cage. Not to mention all the other large metal objects in a datacenter that could cause interference. The datacenter is also very unlikely to allow you to stick antennas outside of your cabinet to improve the signal. Point-to-point wireless bridges such as these work very well and are widely used. However, they are typically deployed on rooftops where unobstructed line of sight can be established and are used for pulling connectivity into a building where fiber is lacking. 

Anyone have any ideas where I should look next? The results of #2, #3, and #11 in particular are really throwing me for a loop... 

No other servers in the cabinet are experiencing packet loss. The gateway switch and bad server can ping each other without issue. If I log into another server in the cabinet and attempt to ping the bad server, then I do get the packet loss. The routing table on the bad server is fine -- the default route points to the proper gateway, no other entries exist (except for local IPv4 assignments). Firewalls have been disabled. No VPN setup is in effect (i.e., routing table on the bad server just has the default route). CPU load and network traffic are both very low. Server has been power cycled. Speed and duplex settings are set to auto-neg and are the same on both the switch and server. Forced 100mbit full on both ends, still had the packet loss. There are no port errors (no drops, collisions, FCS etc) recorded on the switch. CPU utilization on the switch is low ($URL$ 

We are using an ASA-5505, and the current version of the Cisco AnyConnect client on Windows 10 machines. Sometimes, some of our staff members log into the VPN and cannot access any of the resources (even though it seems like it connects fine). Often, the next time they try to connect it's fine, but not always - sometimes it can take hours or days to resolve. It's not limited to one or two users, but when it does happen to someone, other people are on and connected just fine. So it doesn't seem to be user-specific or time-specific. All users are on the same group policy, the same Windows OU, everything. Something I noticed today is that the one client having this issue right now - when I look at the connection details, it's missing a section that the other connections have. The working ones have "AnyConnect-Parent SSL-Tunnel DTLS-Tunnel". The non-working one has just "AnyConnect-Parent SSL-Tunnel". (We have split tunneling enabled, which generally works just fine, and the affected users always have internet access.) The affected user's ipconfig matches exactly that of a working user - DNS servers all getting set, etc... They can ping the main DC/DNS server. But they cannot resolve any DNS, or even browse to any files using the IP. I've got them using clientless VPN as a workaround, but I'd really like to figure out the issue here. I hate intermittent problems... 

"By default, the security appliance rewrites, or transforms, all clientless traffic. You might not want some applications and web resources (for example, public websites) to go through the ASA. The ASA therefore lets you create rewrite rules that let users browse certain sites and applications without going through the ASA. This is similar to split-tunneling in an IPSec VPN connection." However - and I have confirmation of this from a Cisco support rep to back up my test results - this does not work in combination with SSO. If you're using the rewrite rules you cannot send parameters of any kind. 

The -l option is for the buffer and doesn't influence the amount of data transferred. You have to specify the desired amount of data with the client-only option -n in KByte or MByte. So for 10GB, use Example: With the defaut buffer size of 8KB: 

When you connect from the LAN , the last criteria is not met and so the port forwarding doesn't apply Some routers, but not all, allow to configure a rule that will work on connection coming through the LAN interface. This is know as "NAT hairpin" or "NAT loopback". Edit If your router support NAT hairpin, this is the easiest way to go. Usually the need for such feature is that you want to connect from both inside and outside with the same DNS name. In this case, is sometime used. Split DNS is when you have, for the same domain name an internal DNS server that resolve the names with the internal (private) IP while the public DNS server resolve the same name with the public IP. So when accessible from the LAN, the internal DNS server will respond with and you can connect to the server without going through the router. When accessing from the Internet, the public DNS server will respond with Some DNS servers allow to have a single DNS to answer with a different response based on (for example) the asker IP address, in which case you can configure such a split DNS scheme on a single server. 

just a guess, but a possible cause of this is the presence of active NAT translations. Since there's existing translations in the NAT table the connections from the internal devices will match those translations which are valid for WAN1, and try to go out WAN1. Since the route doesn't exist anymore the connection fail. Each tentative will refresh the timeout of the NAT entry and so the issue persist until the NAT table is cleared. 

This ended up being a failing switch. A couple days later we started having issues on ports 37-48. The FESX648-PREM is powered by port ASICs which control port regions. Those regions are: 1-12, 13-24, 25-36 and 37-48. One of the failure modes on this box is that a port ASIC can die and cause forwarding problems. The "bad server" above, was the only server we had in use on the 37-48 region. So when we switched the port and re-tested, we had the same result because the failing ASIC affected multiple ports. We replaced the entire switch and that resolved the issue. 

I'm working on testing several FESX448-PREM switches. One of the switches in my test group is known to be bad. It was previously installed as a top of cabinet switch, 42 servers were connected to it, all port lights came on, full duplex, no errors, low CPU, etc but ports 13-24 would not forward traffic. As I understand it, this is due to a bad ASIC that covers port region 13-24. However, I now have this bad switch at my work bench and I cannot replicate the same forwarding issue with port region 13-24. At my work bench, I have port 1 as the uplink and I've been connecting my laptop to ports 2-48 sequentially using a CAT6 cable while running a continuous ping to a public IP. Interestingly, all the ports now work fine -- port region 13-24 no longer has forwarding issues. Does anyone know how this is possible? If there's a bad ASIC for port region 13-24, then I'd expect this problem to occur 100% of the time. I tried a couple other things afterwards. I had the theory that I needed more ports active at once in order to trigger the forwarding issue. So I first took a layer 2 switch and connected it on a bunch of ports with the FESX448. CPU usage immediately went to 100% on the FESX448. I figured something recursive routing was happening with the layer 2 switch. Next, I put the layer 2 switch into boot monitor mode so it wouldn't do any routing. That resolved the 100% CPU issue, but again I'm still unable to replicate the traffic forwarding issues with ports 13-24. Any suggestions on how I can replicate the forwarding issue and effectively test the remaining switches would be much appreciated! 

This is not the hosts that decide which route a packet will follow, each router in the path make it's own decision. (Actually, the originating host could use the IP strict source option to force the packets to go through a specific route, but it's rarely, if ever, used, and it's totally ignored by routers on the Internet) So each router can change the router of packets depending on the network condition (link drop, congestion on a link, load balancing...) What a host can decide is to alter it's TCP window (flow control), to modify the rate at which it sends information, but this doesn't impact routing. Except for Policy Based Routing, routing is a layer 3 decision that doesn't take into account layer 4 (TCP / UDP) information, so it's performed in the same way for TCP / UDP / ICMP etc... 

Yes serial communication are reliable, as long as the cables meet the specifications, are not damaged and don't suffer too much interference from external sources. The same is true for other cable types however, like UTP. If a cable gives loses, well it doesn't meet anymore the specs, so it has to be changed. 

So while those switches have only 40/10G interfaces (except for the management interface) those interfaces can operate at gigabit speed. 

The easiest way to do this would be to use a linux machine with and OpenVSwitch give you a fine control over traffic forwarding and provide, among many other features: 

You can use a single DHCP server to serve multiple VLAN, but to do so you need to setup DHCP relay. It must be setup on each device that act as a gateway for a VLAN. You must also set a different DHCP pool for each VLAN. The exact steps depends on the equipment used so its difficult to give you a more precise answer.