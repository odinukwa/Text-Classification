Horses for courses. Works pretty much the same as your method. I'd write it your way in this instance if I were me. But it IS a good conceptual exercise to try to think of and compare different approaches like you're doing. Keep it up! :) One thing to note: you should really reference the source table in the SELECT clause. To avoid ambiguity or future complications: 

Like MguerraTorres suggests, SSRS has probably got confused somewhere, and the value suggests it's getting cast to some kind of floating point datatype somewhere along the line. Try checking the datatype SSRS is assigning by looking at the xml in the .rdl file raw text. for a simple bigint column I'm seeing an SSRS field definition something like this: 

You're cross joining to your "inp" query, but then using the results from that in the where clause for the subquery of your main where clause. Effectively then the subquery will run once for every row of the zip table. Which will slow things a tad. I'm slightly rusty as to what you could achieve with any PL/SQL around this, so in pure SQL you could write it as: 

Here's the trick you're after - use the CTE to build a list of each region, cross referenced with every region which counts towards it: 

and then when you look at the code your mind kind of fills in the parenthesis where you think it ought to be. Whereas order or precedence rules actually perform it thus: 

at that point, and the file will be deleted. But once I've read the results in to a data frame it's like it's now keeping a file handle open and calls to file.remove() result in an error message like this: 

I've used SSRS for a subset of this kind of activity. It's not great for using as a full-on data input program for obvious reason, but there's no particular problem with running insert/update/delete statements inside datasets. What I've tended to use it for is more "clicky" kinds of updates. Where clicking on a hyperlink within a report causes something to happen - be it insert something in to the database, update a value somewhere, etc. And the activity that happens is controlled by the hyperlink passing in the various parameters to control that. I've tended to use a report where the hyperlink runs a new instance of the same report, but controls what gets updated by passing in new parameters. Mainly so that all the code is in a single report. I've made a quick demo report to demonstrate the technique in action, if that'll help? Available at: $URL$ You can do quite sophisticated stuff within the confines of this technique, but it's definitely not a data entry portal for entering entire rows of data. I'd be interested to hear if anyone's really used it for that in earnest. 

You don't need to display any fields in this special group, it'll simply use the page break setting as per your requirements.. 

Knowing, and understanding, different options to obtain the same results is key to getting familiar with SQL, IMHO. No matter how obscure and esoteric the problem might be. Anyway, I know I'm late to the party, but here's a different take on a subquery method.. 

OK, just to follow up my correct (yay!) comment answer - in my experience almost everything along the lines of "weird issue" involving data selection, involves a rogue, unparenthesised "OR" somewhere in the where clause. It's always the first place I'd look. Typically you write some code along the lines of: 

Yeah, SSRS can be a little cumbersome with its handling of empty/null values. Try adding an explicit isnothing() before the yellow option: 

I think I've been able to fix what I think is broadly the same issue. First thing in the morning, if I go to our Report Server URL, it loads quick enough and I'm able to browse the menu straight away no problem. But as soon as I click on an actual report, there's that 2 minute wait before the report actually executes... [Quick test... it was only 75 seconds for me this time, but it felt like longer!] Running a report from a command line using the "RS" command seems to start up (wake up!) whetever part of the architecture isn't otherwise being started and the first report takes a normal amount of time to execute. I shall shedule that as a task and forget about it. RS is a bit fiddly to use, since you need to script a bit of VBA to execute the report, but I already needed to work that part out for another job I had.. Edit: Added script below I can't really take all the credit for this, nor cite references I'm afraid. It's cobbled together from 3 or 4 different resources already out there, and adapted to suit my needs (specifically, I wanted to generate a set of pdf exports for a range of inputs). And then simplified a little bit more for here: Script file "RunReport.rss" 

If I rm() the dataset containing the imagine data frame at this point, it still won't let me delete the working file. Has anybody solved this? Or anyone more familar with R offer any assistance? 

Might be a little quicker. Essentially it ought to behave much like your original working query did. 

You might then be able to edit the datatype directly in that text stanza and trick SSRS into sorting itself out 

since, you're future-proofing against someone adding to the code later and accidentally writing something like: 

(Note: I'm not sure that adding supplementary increments to questions is quite how stackexchange is supposed to work, and you haven't supplied an updated fiddle for this - so this is untested. But here goes anyway...) You just need to change a couple of lines in the body of the select, thus: 

A good trick to use when coding, which I tend to do automatically now, is as soon as you use an "OR" anywhere in your code, then add some parenthesis to make it totally clear. Even if your where clause is simply: 

And works fine. Except that the working file, the one with the name generated by tempfile() remains in the working R folder on the server. All the other working files get cleaned up at the end of the process. Im my case, I don't want the image file to remain on the server, as it potentially contains sensitive information. -- I'm pretty new to R, so I've been experimenting with the code in an interactive line-by-line fashion in R console: I find that before running that line with the call to readBin() to assign the contents to the output data set, I can simply use the R command 

You need to make certain that the quorum is set up according to your needs. In most cases when you have an odd number of nodes node majority is the best way to go so make certain that your quorum is set up accordingly. In a 3 node cluster you can setup node majority as your quorum mode and then you can survive the failure of a single node within the cluster . That is in most cases sufficient. If your cluster spans multiple sites you might want to fiddle with the quorum-vote weight depending on your failover plans. 

This will mark all the MSDTC transactions as failed and allow you to start the database. It's a better idea to use backup/restore to refresh the database though 

Why not use a master server for the SQL Server agent? You could even use a third server to create and schedule the jobs on both the servers and you get a central repository and log server for the jobs. A small check in the agent job will make certain that it's running on the active server. 

The above statement create the table in the target database and bulk insert the data. If you want to copy the data to a different instance you are best off using SSIS. 

The rollback time is highly dependent on how busy the server is but will take at least the same time as the elapsed time, safe bet is elapsed time x 1.5 

The user account that is running the SQL Server agent might not have permissions to use the database mail profile you can check that with EXECUTE msdb.dbo.sysmail_help_profileaccount_sp; But you are having some issues with the agent account security, please try the following steps. 

There are two possible ways to ensure that you encrypt connections from clients to the SQL Server. The first option is to setup IPsec either manually on both clients and server or by using a IPSec policy or you can setup SSL encryption. The simplest way to do so is to use the self signed certificates from the server but as those are exchanged during the connection handshake their security is limited so it's recommended to use certificates from a trusted root. You can create a test certificate with MAKECERT.exe by following TheSQLDude instructions and copy the self created root certificate to all the clients. So to set up SSL encryption over the wire in SQL Server you either need a certificate signed by a trusted provider, which means that your SQL Server needs to have a valid FQDN as no certificate providers create certificates for invalid Fully-Qualified Domain Names (eg .local). Or you can also install your own trusted PKI infrastructure for instance Active Directory Certificate Services (ADCS). This is needed as all the clients need to trust the root certificate that signs the certificate you install on the SQL Server. When creating a certificate for SQL server the certificate needs to be created with certain extended properties most importantly it needs to be setup as Server Authentication (1.3.6.1.5.5.7.3.1) which means you might need to create a template in ADCS or you can create the certificate CSR using certreq For this you need to create an csr.inf file 

Stop the applications, make certain that all the users are disconnected and make a tail_of_the_log backup of the database Change the recovery model to simple Upgrade Change recovery model to full and do a new full backup 

To be able to check all jobs you would need go add the user to the SQLAgentReaderRole in the msdb database which also gives that user permission to create jobs. To minimize the access granted you can also grant the user select permission on the tables used by the sql server agent 

For most parts I'm referencing Paul Randall's Inside the storage engine blog series. The only way to reclaim unused space from database files in SQLServer is using the DBCC SHRINK command which reallocates data within the database files freeing pages and after removing them from the Global Allcation map removes them from the database file. This operation is slow, creates fragmentation within the database and is even slower when dealing with LOB pages as those are stored as linked lists within the database files. Since you are dropping the NTEXT column you will have to wait for the ghost cleanup process to drop the data before shrinking. Now having lots of free space in the database files will actually do you no harm, if you have the disk space, backup compression will take care of the free space within the files. If you absolutely want to make the files smaller you can create a new filegroup with the database and make it the default and then move the tables into the new filegroup but this can take time and cause downtime. I have used the technique explained here by Bob Pusateri with good results. 

Yes it is possible to set one of the instances to run on port 1433. It is also safe as the dynamic ports will be set in the range of 49152 and 65535 as stated above. You use the configuration manager to set one of the instances to start at port 1433 (clearing the configuration for the dynamic port) and it will run on that port afterwards. 

Short of restarting the SQL Server service or the box this is enough. If you want to be absolutely fresh you might want to drop user created statistics from your last run. 

Sort of, you will double the resources but every instance will only be active on one node at the same time. Any SQL Server instance will have exclusive access to it's storage on the node where it's currently active - If all the storage for multiple instances comes from the same SAN one instance can affect the others No you cant, sorry 

For this to work for you, you will need to setup the trace to start with the server. For that you need to make sure that the filename is unique each time the trace is started, create a procedure from the trace definition and make it a startup procedure. 

You are running SQL Server build 9.00.5294.00 that is more current than SP4 which is 9.00.5000. Your server has already been updated. ($URL$ (Build 7601: Service Pack 1) is the version of the operating system.