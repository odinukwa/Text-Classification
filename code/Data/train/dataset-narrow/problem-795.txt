Sorry if this is redundant, but due to the crazy naming of the tools, it's hard to find the answer to the question. Question 1 Will SSIS packages, reports and so forth built with Microsoft SQL Server Data Tools - Business Intelligence (SSDT-BI) for Visual Studio 2013 work on SQL Server 2008 R2? Question 2 I'm currently using SQL Server Business Intelligence Development Studio (BIDS) for Microsoft Visual Studio 2008. I want to potentially upgrade to Data Tools - Business Intelligence for Visual Studio 2013. I assume I would need to A) purchase a new copy of Visual Studio 2013 and then B) download the free SSDT-BI software? That's assuming SSDT-BI for VS2013 works for 2008 R2. 

For question 2. I think I will break the large table up into many different tables and then use a view to join them together. This will allow me to have something like table partitioning without the feature of table partitioning as outlined in this article. $URL$ View with schemabinding that unions all the tables together. Then I can insert data and select data from this view as if it was the primary fact table. I would only need to ensure all my SELECT queries on this view include the column I chose to partition the tables with to get the full benefit. 

So how does an estimate of 404986 going into the sort operator comes out as an estimate of 100? Is it just an arbitrary number since it cant sniff the variable in the TOP operator? 

and got around 1400MB/s I was also able to get comparable throughput using a T-SQL query as below and calculating the throughput from the number of reads and time. I got this from Glenn Berry SQL Course on PluralSight "Improving Storage Subsystem Performance ". 

All of the scenarios work if I . So does XACT_ABORT has any kind of restriction when it comes to linked server? Edit - Did some more testing and looks like it has to do something to do with the number of rows also Possible Repo On ServerA 

I am testing -SubscriptionStreams parameter in replication and wanted to make sure it is using multiple streams when applying transactions on subscription. When I check the subscriber there is only one SPID applying the transaction. May be my understanding is wrong, but is it supposed to have multiple SPID's when SubscriptionStreams is enabled or it does it internally. Is there any way to verify that it has picked up the parameter change? Publisher and distributor are SQL 2014, subscriber is SQL 1008R2 

SSIS is not really my forte. I noticed you have a table lock option on OLE DB destinations where the entire table becomes locked in what I assume is during the transaction of inserting data. What happens if you have a flat file data source with a conditional split that is parsing out data into 5 or more OLE DB destinations that are targeting the same table with table lock on? Would each OLE DB destination get blocked by one another in this scenario if data is being fast loaded (inserted) into the destinations themselves? I removed the table lock in my instance and everything seemed fine. It was splitting the data and inserting records at about 1 million records per minute. 

USQL Just the base USQL job that defines the schema then selects all fields to a new directory. No transformation happening outside of leaving out the headers. The file is CSV, comma delimited with double quotes on strings. Schema is all strings regardless of data type. Extractors tried is TEXT and CSV with both set to be encoded:UTF8 even though both are default to UTF8 according to Azure documentation on the system. Other Notes This same document was uploaded in the past to BLOB storage and imported in the same fashion into Azure Data Warehouse without errors via Polybase. 

Optimizing a Stored procedure is too broad of a topic. Firstly if you are getting a consistent run time from your stored procedure we may be able to rule our parameter sniffing issue I would suggest you start with querying the procedure cache to find out which query inside the stored procedure is taking up time and then start optimizing it Sample query 

What I am trying to understand are the key locks on the non-clustered index(indid 2). Why are there two key lock on non-clustered index? If I check dbcc page on page id 248, I could locate the obvious one((1bfceb831cd9)) which is the lock for the entry for the record 6 which got changed to 7. Output of DBCC PAGE below 

I mean SQL know its partitioned by date so I would assume it should just do a , Scan on each partition which would be much more efficient. I remember vaguely reading somewhere that ordered scan is not supported in partitioned tables, is that correct or is it something else that is causing this behavior ? 

What is a statistical database? At a high level, it's just a type of database that only stores statistical data. An example is a census database. Typically, access control for a pure SDB is straightforward: Certain users are authorized to access the entire database. It's about the data not the engine Keep in mind, this is about the data and not the type of relational database management system you chose such as MySQL, SQL Server, Oracle, etc. Any database can be a statistical database if you store statistical data in said database such as census information and so forth. Typical structures for statistical databases That being said, most statistical databases are Online Analytical Processing (OLAP) which is characterized by relatively low volume of transactions. Queries are often very complex and involve aggregations (e.g.: derived aggregated information and not derived individual information). OLAP vs OLTP Can statistical databases be created using SQL Yep. Any database engine can be used to create a statistical database if the data is one that provides data of a statistical nature, such as counts and averages and the users accessing the database are querying aggregate, or statistical, data from said database as opposed to individual records on the users the statistics are ultimately based on. Being most of the popular database engines use a form of SQL to get data to and from the underlying system, then the answer to the question is, "yes". Why is individual information restricted? This greatly depends on the business or organization using the statistical database for their needs. In general, individual records, what the statistics are based on are restricted for privacy reasons. For example, medical statistics based on medical records from the local hospital. Therefore, security is a big concern for statistical databases in order to help prohibit users from unearthing individual information from the use of aggregated statistical information. Examples of how individual records can be compromised with statistics 

On the other server though, I can get the high throughput numbers from the diskio tool but using SQL server method I am not able to get the throughput numbers. the SQL numbers are close to what I get if I run diskspd in single thread, even though the plan is running in parallel. So I was wondering what are the things I can check to see why SQL Server's IO is slow or why SQL Server is not able to push more IO's through. 

And now the , Scan is gone and it has to scan the entire table to get that value. Why do you do this SQL Server? 

So as you can see from the estimated number of rows, the first one is 3.2511 which came from the density vector and for the second one the estimated number of rows of 7 comes from the histogram. So is it true that SQL Server can sniff the variable when we recompile adhoc query or is it something that I do not understand? 

I have a bulk loading process that loads millions of records into a few fact tables within a warehouse. Those tables are primarily clustered on time. The non-clustered indexes are in place for how the data is used to speed up performance. I typically drop some of the non-clustered indexes to both speed up inserts and to reduce index fragmentation after large data loads. However, this process of dropping and rebuilding is causing a huge amount of time as the data grows. Example: One table took 2 hours to apply a new non-clustered index on 100m+ rows. Likewise, if I leave the non-clustered indexes in place, they will increase the inserts 3x to 10x in some cases that force your hand to drop and rebuild. While it's awesome to drop and rebuild indexes, they don't really pan out that well as data grows in those tables. What options are available to me? Should I bulk up the server with more memory (currently 32GB) and cpu (4 vCPU)? Should I rethink my indexes? Should I find a balance of keeping some indexes on for reorganization versus dropping and rebuilding? (Note: I don't have enterprise edition.) I'm thinking my only option is enterprise edition with table partitioning where I can rebuild indexes per partition as opposed to the whole table. 

This morning i was woken up by a transaction log full alert on one of our database. This server is an alwayson cluster and also a transactional replication subscriber. I checked log_reuse_wait_desc and it showed logbackup. Someone had accidentally disabled the logbackup jobs 4 days earlier, I re-enabled the log backup job and the log got cleared. Since it was 4am I thought I will go to office later that morning and shirnk the log as it has grown to 400GB. 10AM- Im in office and I check the log usage before shrinking and it was around 16%. I was surprised and check the log_reuse_wait_desc, which showed replication. I was confused cause this was a replication subscriber. We then saw that the db was enabled for CDC and thought that might be the cause, so disabled CDC and now the log_reuse_wait_desc shows AVAILABILITY_REPLICA. The log usage meanwhile still steadily growing and its at 17% now. I check the alwayson dashboard and check the sent and redo queue and both are virtually zero. I am not sure why the log reuse is showing as AVAILABILITY_REPLICA and unable to clear the log. Any idea why this is happening? 

I have a two part question really for anyone working with data warehousing where they have large fact tables. Question 1 Lets say you have a table that has 500 million or more records in it that is clustered on time. You are only posting incremental records for the last 24 hours on a daily basis to this table. How do you handle inserting delayed records that are a month or older to that table? Would you do nothing unless the insert caused a lot of fragmentation or would you attempt to drop the indexes, insert and rebuild? I do not have enterprise edition available to me for table partitioning. Question 2 If you have a fact table that is growing large like the above example, would it be wise to split the fact table up into multiple tables or would it be better to look towards adding additional files to the filegroup of said table if table partitioning is not an option? Thanks in advance. I'm dealing with some large growth and trying to approach it the right way.