It looks like there is special structure in your images that might allow you to do better than using generic image inpainting methods. It's hard to know from just one example image, but it is possible that you might be able to build a satisfactory method using only very simple features of the input image. In particular, you could train a classifier that, given a black pixel, tries to predict the correct color for that pixel. In your specific example, some features that look useful are: "the most common color in the same column", "the most common color in the same row", "the color of the closest non-black pixel above your pixel (in the same column)", "the color of the closest non-black pixel below your pixel (in the same column)", "the color of the closest non-black pixel to the right of your pixel (in the same row)", "the color of the closest non-black pixel to the left of your pixel (in the same row)". I suggest trying to train a simple classifer on a few simple features like this, and see how well it works. If you want to get fancy, you could try modelling this as a Markov random field over a grid graph. In other words, each pixel is a vertex in the graph, and two pixels are connected by an edge if they are adjacent. There are then standard methods to learn the underlying density functions given some training examples, and once you know the density functions, you could then use maximum likelihood methods to infer the values of the black pixels. 

The author is right. You are wrong. There's no such thing as "only metadata leak" or "can't leak really". It's like saying "not pregnant really" -- either you're pregnant, or you're not. Same here -- either data can leak, or it can't. In this case, data can leak. Maybe only partial data, but it's hard to know just how partial and just how bad the impact of that might be. It might be very bad, or it might not; but since you can't know, testing with a methodology where you know the results might be meaningless isn't a good idea. 

You have to decide what you want to maximize. Classifying by comparing the probability to 0.5 is appropriate if you want to maximize accuracy. It's not appropriate if you want to maximize the f1 metric. If you want to maximize accuracy, always predicting zero is the optimal classifier. Alternatively, given a probability score $p$, another option is to randomly flip a biased coin; with probability $p$, output classification 1, otherwise output classification 0. This doesn't always predict zero. However it's probably not actually any better in any useful way. If you want to maximize f1 metric, one approach is to train your classifier to predict a probability, then choose a threshold that maximizes the f1 score. The threshold probably won't be 0.5. Another option is to understand the cost of type I errors vs type II errors, and then assign class weights accordingly. 

No, you probably don't want to try all possible cut points in a serious implementation. That's how we describe it in simple introductions to ID3, because it's easier to understand, but it's typically not how it is actually implemented, because it is slow. In particular, if there are $n$ data points, then you'll need to test $n-1$ candidate thresholds; using the naive algorithm to calculate the information gain of each of those candidate thresholds takes $O(n)$ time per candidate, for a total of $O(n^2)$ time. In practice, there are optimizations that speed this up significantly: 

I suspect the reason is that the class balance in your test set is different from the class balance in your training set. That will throw everything off. The fundamental assumption made by statistical machine learning methods (including logistic regression) is that the distribution of data in the test set matches the distribution of data in the training set. SMOTE can throw that off. It sounds like you have used SMOTE to augment the training set by adding additional synthetic positive instances (i.e., oversampling the minority class) -- but you haven't added any negative instances. So, the class balance in the training set might have shifted from 0.5%/99.5% to something like (say) 10%/90%, while the class balance in the test set remains 0.5%/99.5%. That's bad; it will cause the classifier to over-predict positive instances. For some classifiers, it's not a major problem, but I expect that logistic regression might be more sensitive to this mismatch between training distribution and test distribution. Here are two candidate solutions for the problem that you can try: 

Instead of trying to re-order the columns and rows, I would suggest trying to find some other way to visualize the data. Here's one possible alternative suggestion. You could cluster the classes, say into ~ 20 clusters, where each cluster has ~ 20 classes in it, using some kind of clustering algorithm that puts similar classes together into the same cluster (e.g., if two classes are frequently confused with each other, they should be more likely to be in the same cluster). Then you can show a coarse-grained confusion matrix, with one row/column per cluster; the cell at $(i,j)$ shows how often an instance of some class in cluster $i$ is predicted to have some class in cluster $j$. Also, you can have ~ 20 fine-grained confusion matrices: for each cluster, you can show the confusion matrix of classes, for the ~ 20 classes in each cluster. Of course, you could also extend this by using hierarchical clustering and have confusion matrices at multiple granularities. There may be other possible visualization strategies as well. As a general philosophical point: it might also help to clarify your goals (what you want to get out of the visualization). You can distinguish two kinds of uses of visualization: 

I have a binary classification task where all of my features are boolean (0 or 1). I have been considering two possible supervised learning algorithms: 

This falls into the category of "distance metric learning" or "similarity learning"; specifically, it is what Wikipedia calls a "classification similarity learning problem". In your specific case, I suggest you use logistic regression to classify pairs of addresses as either "Similar" or "Not Similar". In particular, map a pair of addresses (Address1,Address2) to a $d$-dimensional feature vector $x \in \mathbb{R}^d$. Then, train a logistic regression model to predict "Similar" vs "Not Similar" from the feature vector $x$. This turns your problem into a boolean classification task, and solves it by applying logistic regression. How do you form a feature vector? That's where you'll use your domain knowledge to identify features, where each feature represents a factor that might be helpful in determining whether the two addresses match. You could parse the address into elements (house number, street name, city, state, zip code), and have one feature (one dimension) per element of the address: 

If you want to compute (approximate) nearest neighbors in 10-dimensional data, I would recommend using a k-d tree. A k-d tree can handle skewed/non-uniform data sets and adjusts naturally to that. More generally, you could look at data structures for nearest neighbor search, including binary space partitioning trees and metric trees. But k-d trees are a good starting point, and they support both exact nearest neighbor queries as well as approximate nearest neighbor queries. I would expect this to work better than locality-sensitive hashing (LSH). 

Contrary to what you wrote in the first sentence of the question, your problem is not an instance of integer linear programming (ILP) and cannot be formulated as an ILP problem. If you used the $L_1$ norm (instead of the $L_2$ norm), it could be formulated as an ILP problem. You'd introduce additional variables $t_1,\dots,t_n$, add the linear inequalities $$-t_j \le \left(v - \sum_{i=1}^K b_ix_i \right)_j \le t_j$$ where $(\cdots)_j$ denotes the $j$th coefficient of the vector, and then minimize the objective function $t_1+\dots+t_n$. With the $L_2$ norm, this is no longer an ILP problem. It is an instance of integer quadratic programming. You could try applying an off-the-shelf solver for mixed-integer quadratic programming (MIQP). However, MIQP is pretty tough in general, so I don't know whether this will be effective. As another alternative, you could relax your MIQP instance to an instance of quadratic programming or semidefinitive programming, solve for a solution in the reals, and then round each real number to the nearest integer (or use randomized rounding), and hope that the resulting solution is "pretty good". This might be more computationally feasible (as quadratic programming / semidefinitive programming over the reals is easier than MIQP) but there are no guarantees on the quality of the resulting solution; it might be arbitrarily bad. Your problem seems related to the Closest Vector Problem (CVP) in lattices, which is believed to be hard. Here you have the additional constraint that the coefficients be 0 or 1 (instead of them being arbitrary integers). If $n$ is not too large, you might be able to use existing algorithms, like LLL basis reduction. I don't know whether this will work or not. 

Then, train a classifier using logistic regression to predict "Similar" vs "Not Similar" from these features, using your labelled training set. One nice thing about this approach is that it is easy to add other features, if you discover that the classifier makes mistakes. For instance, it is easy to add the edit distance between both addresses as another feature, or to replace the edit distance with a thresholded version of the edit distance, or feed each address into a geolocation service to get back (latitude,longitude) and then compute the distance between those two locations, or any other scheme of your choice. And, of course, you can replace logistic regression with any other boolean classification method, such as random forests. However, logistic regression is probably a nice simple starting point for this problem. 

That's a wonderful question, and beautifully posed. I love how you capture the essence of the issue in such a clean way. Unfortunately, the answer is that your data set does not have enough information to help you decide which cats you should feed. Without a properly controlled experiment, you don't have a way to infer causality; you can't rule out the possibility of confounding factors. Suppose that 70% of the cats he feeds make it to the top, and 20% of the cats he doesn't feed make it to the top. Let me make up four alternative explanations for why that might happen: 

In particular, here's why I'm thinking they are equivalent. A single gradient boosting decision stump is very simple: it is equivalent to adding a constant $a_i$ if feature $i$ is 1, or adding the constant $b_i$ if feature $i$ is 0. This can be equivalently expressed as $(a_i-b_i)x_i + b_i$, where $x_i$ is the value of feature $i$. Each stump branches on a single feature, so contributes a term of the form $(a_i-b_i)x_i + b_i$ to the total sum. Thus the total sum of the gradient boosted stumps can be expressed in the form $$S = \sum_{i=1}^n (a_i-b_i) x_i + b_i,$$ or equivalently, in the form $$S = c_0 + \sum_{i=1}^n c_i x_i.$$ That's exactly the form of a final logit for a logistic regression model. That would suggest to me that fitting a gradient boosting model using the cross-entropy loss (which is equivalent to the logistic loss for binary classification) should be equivalent to fitting a logistic regression model, at least in the case where the number of stumps in gradient boosting is sufficient large. 

Visualizing the data is also typically helpful. For instance, you can look for outliers and examine them more closely with manual inspection. 

The problem is that you only have positive instances (businesses who have become a customer) but no negative instances (businesses who haven't become a customer). This prevents use of supervised learning. You could try unsupervised learning -- e.g., a one-class classifier -- but I don't expect it to be particularly effective. If you had data on businesses who you tried approaching, but ultimately did not become a customer of yours, then you could try applying supervised learning.