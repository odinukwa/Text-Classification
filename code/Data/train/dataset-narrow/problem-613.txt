You can extract the SQL from a specific schema with the Use mysqlbinlog utility against all binary logs that have the data and time ranges. You will have to name the database you are extracting: 

You will have to put it back if you want to use LOAD DATA INFILE, SELECT ... INTO OUTFILE, or LOAD_FILE(). OPTION #2 Try setting this: secure_file_priv OPTION #3 Give a different password. When Developers login as 

These will recommend the right sizes for the MyISAM Key Cache for existing MyISAM and the InnoDB Buffer Pool for InnoDB data and indexes. While running these SQL queries recommend the buffer sizes, the human factor must still kick in. We must plan data usage based on hardware and frequently accessed data. We must also ask: Of the amount of buffer recommended, how much of my working set will actually reside in memory? If you configure MySQL to use 75% of RAM for InnoDB, even on a dedicated MySQL server, the OS will get a busy paging to disk. Just because MySQL can use huge numbers for buffer sizes does not mean it's OK to push the limits. Two have two basic storage engines to consider. Use of commodity hardware merely demonstrates MySQL configurability with noticeable results. Unless you are using 32-bit RHEL (if you are, please stop using it right now, get plenty of coffee, go upgrade), raising buffer sizes in storage beasts can be well trusted. For example, my employer has a client with 3 DB servers dedicated to MySQL only. Each has 192GB of RAM of which 162GB is the InnoDB Buffer Pool. The working set is actually 170GB. While there is a little swapping and paging going on, the DB performance is astounding. OS has about 30GB of RAM for itself (16GB of RAM disk for temp tables). I am sure that these servers are just little lizards compared to the komodo dragon-sized DB servers out there. MySQL can be up to the task if configured properly. Keep in mind that InnoDB puts its own checks and balances in place because the current source code limits InnoDB log files to 4GB. This is the case because InnoDB was originally designed with commodity hardware in mind. In light of this, ACID transaction throughput could possibly bottleneck there regardless of the storage beast's hardware setup and the version of MySQL you choose to run. CONCLUSION It's OK to push limits on Big Beefy DB Servers. You should always do so with consideration to the OS in terms of Memory, Swap Space, Working Dataset, and number of transactions expected. 

Generally, I prefer to add some CLR functions when possible to implement some of the ideas from the book Cryptography in the Database, such as hashing and salting. This way the clients don't need to worry about future maintenance developers incorrectly implementing password (or other data requiring protection) security. I've worked at companies where encrypting stored procedures was done to preserve business trade secrets (frequently from the employees of the firm). Some of these databases would be hosted on the customers' servers, so security of the source code was paramount. If CLR in the database was available back then, they would have proceeded in that direction. Finally, there are times when reporting functions are hard to implement in pure SQL, so a function can be implemented in CLR could allow for more complicated functions to be kept away from the report itself. 

My understanding (I haven't worked with this in over a year) is that "merge replication" requires a sql server instance to act as a publisher. If you want to only have an SQL CE database at each end, you need to write your own code with sync framework (if you see articles on "sync services" they are using the sync framework). 

The results should be identical on Master and Slave. CAVEAT I have never used pt-table-sync. I have been using mk-table-sync (MAATKIT tool from Percona) for years and I know it has always worked for me. I fully trust Percona's pt-table-sync is at the least the same, if not superior, quality. 

To detect a Master and Slave being out of sync without downloading any tools, pick any table and run CHECKSUM TABLE against a table on the Master and the Slave's copy of the same table. EXAMPLE If you have a table , run the command against it: 

It may not be a good idea to perform an on my_table when you are right in the middle of an after trigger on the same table. Also, notice you are also causing a DELETE under on the same table. You are probably better off writing this trigger as a Stored Procedure and manually using the Call to it instead of nesting UPDATE of a table inside DELETE on the same table. 

The trigger code you just displayed in your question needs to be manually defined on each table you intend to copy from. For tables x, y and z the answer to your question is: 

Now, I did some digging around. First thing I did was to check if the server is up and running. So I entered the following: 

I don't know who this ELF guy is, but at this point, I assumed I'm screwed. After more research on the internet, I found out something about Oracle variables and paths not setting themselves correctly, but none of them were of any help. sqlplus: command not found Go To Oracle Home Any ideas? Thanks. Additional Info: 

Take note that being a Linux noob, I actually have no idea if this command holds true to every Linux installation. All I know is that it works on my friend's PC. My PC, on the other hand, produces some interesting results: 

I've recently installed Fedora 19 on an older PC, to serve as a test server of sorts. After a lot of hiccups, I was able to install Oracle 10g XE. Because I'm using Linux - which I, admittedly, have little experience of - I used the command line (rpm) to install the file. Everything proceeded without a hitch, though I didn't understand much of it. When I try to log in using sqlplus, though, the command line returns: 

Even reads in InnoDB tend to shroud rows with MVCC protection to allow repeatable reads and permit transactions to hit the same rows being read. Thus, reads as well as writes produce disk I/O in ibdata1. Using innodb_file_per_table may relieve some of the disk I/O by separating Table Data and Index pages from ibdata1 into files. Yet, I would expect a somwehat noticeable performance improvement only for a limited time in a RAID5 environment. The table interaction is still somewhat the same. Every access to a file is always preceded by reference checks against ibdata1. While the separation can bring significant performance changes, RAID5 would be what they call in the chemistry world, a limiting reagent. Any benefits expected from InnoDB layout changes would be neutralized by outside factors, such as RAID5. The presence of extra tablespace files due to innodb_file_per_table buys you nothing over time but just the presence of extra tablespace files. MyISAM When it comes to MyISAM, RAID5 is OK in a read-heavy, low-write environment provided you map all temp tables (using tmpdir) to another disk, separate from the RAID5. Please remember that table data pages live in files and its corresponding index pages live in files. A write-heavy environment (INSERTs, UPDATEs, DELETEs) will obligate RAID5 to slow things down. Given MyISAM's locking behavior (full table lock with each INSERT, UPDATE, and DELETE) in a write-heavy environment, a steady stream of DML will keep RAID5 rather busy and have DB users enter a brief-but-annoying time warp waiting for DML to complete. Conclusion Under the hood, RAID5 has the following characteristics for writing with parity 

Give them logon and view data rights; however to perform DBAly duties, use a separate login with elevated privileges. I know one financial customer that does this - the regular windows authentication based logins were limited in the damage they could inadvertantly do. Restores and running DML required running with the separate SQL authentication login. One government agency I worked with used 2 separate logins for each server/db admin. So if was my domain login (this login would have regular privileges), then would be my separate login. You get into trouble if you use your admin account all the time, but then it lacks permissions to other things (like no email. Oh, you say that like it is a bad thing...). The current government agency I'm working with has each server/db admins having privileges elevated above the standard user, but not quite admin (think of it as the group). Domain admin functions are performed with a shared domain admin account. A common error is restoring the wrong database (like QA restored over the production server), and this isn't going to be solved via restricted rights or multiple logins. Doing potentially destructive things in pairs is one way to minimize the risks. 

Did you ever wonder why mysqld did not crash when doing ? Doing simply passes through the data folder looking for files. Running this query 

In a separate script, load every table_name in mydb.TablesLeftToEnableKeys to bulk Step04 : For each table_name in Step03, do the 

The third method is not possible because explicit DDL and DDL via Dynamic SQL are not allowed in MySQL Triggers. You may have to create a regular table using either the MyISAM or MEMORY storage engine. Then, you can have the trigger compile your data to a table that actually exists. MyISAM is better because should a server go down, the compiled data thus far is on disk. MEMORY tables are faster to write to, but are gone on system restart. DO NOT USE CREATE TEMPORARY TABLE AT ALL because such tables only last as long as the DB connection lives, and would be private unto the call of the trigger also. Even worse off, if you are using MySQL Replication and you run on the slave, any tables created via CREATE TEMPORARY TABLE disappears from the SQL Thread and replication breaks immediately when you run and those temp tables no longer exist. 

I'm currently using MySQL version 5.0.16, on my local computer (which runs a 32-bit OS), for easier access while developing my program. Now that I'm ready to deploy my program, I'd like to test the connection over a network, and use a 64-bit server to hold the database. Up til now, I've only used the 32-bit version, and my backup 'process' would be to copy the database folder from C:\Program Files\MySQL\MySQL Server 5.0\data and paste it onto the same directory of another computer. I'm not sure if that would work. I tried using both MySQL Query Browser and Command Line Client to do the following: mysqldump -u root -proot Isys-TMS(EEI1) > isystms.sql; But both simply return this error: "You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'mysqldump -u root -proot Isys-TMS(EEI1) > isystms.sql' at line 1" Which is not at all helpful. (Though I'm not sure if I did it right on the command line client, as opposed to Windows' own CMD) I'm also assuming I can simply use this .sql output on the new 64-bit MySQL, so if that's not the case, what else can I do to solve this problem? Thanks. Notes: