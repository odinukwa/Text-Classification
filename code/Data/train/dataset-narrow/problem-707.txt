You can't directly filter SSMS's commands, but here's an easy workaround: Create another table with the subset of data you want to export: 

Are you doing your backups to a neutral independent server like a file share? If you're doing them locally (like to a drive on SQL01A or SQL01B) then log shipping won't work since the file path would change whenever there's a failover. Instead, on SQL01A and SQL01B, write your backups to a UNC path (like \\myfileserver\myfileshare) and then SQL02 will always be able to find them there. 

It's not pretty - that's a lot of work, with an estimated subtree cost of around 30k. (You can see it by hovering your mouse over the select operator over at PasteThePlan.) So what happens if we only want rows 100-200? We can use this syntax in SQL Server 2012+: 

There's no such thing as the 100th-200th rows, because you don't specify an ORDER BY. Order isn't guaranteed unless you include the ORDER BY for a whole lot of interesting reasons, but that's not really the point here. So to illustrate your point, let's use a table - I'm going to use the Users table from the Stack Overflow data dump, and run this query: 

Most of the index creation will happen without blocking, although not all - read this DBA.se question & answer for details about when even online index creation blocks other queries. Your server may still be much slower during the index creation, though - creating indexes can be hard work, taxing your SQL Server's storage. 

For Microsoft SQL Server, the answer is also no, and the mechanics of it are pretty interesting. The Books Online page about scanning explains the concept of merry-go-round scans in the Advanced Scanning section: 

Things in the select are returned only if there are rows returned in the FROM statement. First, let's think of it conceptually. Query 1 is like: 

Update with sp_WhoIsActive - in the sp_WhoIsActive screenshot you posted, you've got a couple of queries that are waiting on ASYNC_NETWORK_IO. For those, refer to the above instructions. In the remainder of the queries, look at the "status" column of sp_WhoIsActive - the majority of them are "sleeping." That means they're not working at all - they're waiting for the apps on the other end of the pipe to send their next command. They have transactions open (see the "open_tran_count" column) but there's nothing SQL Server can do to speed up a sleeping transaction. These queries have been open for over forty minutes (the first column in sp_WhoIsActive. They're just not doing anything anymore. You've gotta get those folks to commit their transactions and close their connections. This isn't a performance tuning issue. Everything we're seeing here points to a scenario where we're waiting on the app. 

Then plan guides are a good fix. Just know that if you violate any of these rules (like if the queries start changing) then the plan guides won't work. 

And count to 5. Within 5 seconds, SQL Server wakes up the deadlock monitor, who looks around, finds that these two queries are blocking each other, and...the select wins! The select returns data, and the update's window says: 

Just because you create a role in the master database doesn't make it a server level role. It's still a database role - it just lives in a really important database. SQL Server 2008R2 simply didn't have user defined server level roles. For that feature, you'll need to upgrade to SQL Server 2012. You asked for something authoritative, and the best answer there is good ol' Books Online, where they list what's new in SQ Server 2012 security features: $URL$ 

Because they think there's a memory problem - SQL Server uses all of the memory available to it, up to its max memory setting (and even beyond.) Unknowing folks go into Task Manager, see SQL Server using a lot of memory, and think, "There must be a memory leak - I'll stop and restart SQL Server, and see what happens." Sure enough, that frees up a lot of memory (because SQL Server doesn't allocate it all right away by default), so they think they fixed the bug. Next thing you know, they're restarting SQL Server weekly. Because they think there's a CPU problem - queries will use a ton of CPU resources, especially in the case of parameter sniffing issues. Unknowing folks try to connect into the SQL Server without knowing about the Dedicated Admin Connection (DAC), be unable to connect, and just run out of options. They restart because the executives are standing behind them, wanting a solution fast. Because they've heard it fixes corruption - when folks run into a corruption issue, they're often willing to try anything to fix it. Because they want a rollback to finish - they kill a query, and it sticks in rollback for a while because they didn't know rolling back a query is single-threaded. After minutes (or hours) of waiting, they restart the SQL Server, thinking the rollback won't be necessary when it starts back up again. Sadly, they're wrong, and SQL Server just keeps right on going with the rollback upon startup. 

Here's a few reasons why queries won't show up in sys.dm_exec_query_plan, the DMV used by sp_BlitzCache: 

The script you linked to uses SQL Server's built-in Blocked Process Report. It doesn't have any configuration options about the level of blocking - it simply wakes up on an interval (say, every 5 seconds), checks for blocking, and if it finds any, runs the script. There is a Blocked Process Threshold setting, but that just determines the interval at which the Blocked Process Report runs - like every 5 seconds. (Don't go lower than that.) If you want to build your own custom Blocked Process Report and put logic in there about how much blocking is occurring, and in which databases, that would be an exercise left for the reader. You can start with Darko's Building a Custom Blocked Process Report. From my own experience over the years, I've seen people who wanted to build these kinds of reports, and then the very next thing they do is build an Outlook rule to move all of these alerts into a folder, and they read 'em later. If you're going to do that, don't bother sending emails. Just set up an Extended Events session to look for blocked processes and deadlocks - note that you'll need to change the file locations & names for your server: 

Erin Stellato's Stairway to Extended Events is a good starting point here (it's behind a registration wall, but it's worth it.) She explains how you can use the SSMS GUI to create an Extended Events session that will trigger a dump, for example. Understand that taking an action like creating a dump will be very performance-intensive. 

You've got two different questions in here: Q: Sometimes a rogue developer runs a crazy huge query on a production server during work hours, causing the TempDB data files to blow up huge. A: When that happens, all of the TempDB data files will grow roughly equally, so you won't have to worry about shrinking specific ones. Frankly, you don't want to shrink them - if you've got drive space set aside for TempDB, just leave these files in place. Why keep re-fighting the same battle? You'll have a rogue developer run another query in a few weeks. Just leave this in place. You don't get bonused based off empty drive space. Now, having said that, if you've got TempDB's data and log files on the same volume as your user databases (or heaven forbid, the boot drive), then that's the real root cause, and you need to fix that. Even if you don't put them on separate spindles, TempDB should be on a separate logical volume to mitigate this exact problem. When it fills up, it fills up - but it doesn't take user databases (or the entire server, in the event of a full C drive) offline. Q: (DBCC SHRINKFILE) seems to not work very often in 2008 It's more a function of how SQL Server relies more and more on TempDB in each release. When something's active in TempDB, you can't move its data around, and each new version of SQL Server works more in TempDB. For example, when you enable Read Committed Snapshot Isolation, the version store it uses lives in TempDB. When you use AlwaysOn Availability Groups, it tracks user database statistics in TempDB too. This is just another reason why you set aside a logical volume for TempDB, size the data files to fill it up, and then walk away - your work here is done, and don't try to shrink those files. 

No, but here's a few options: Run a server-side trace. You don't get the Profiler GUI, but the good news is that this kind of tracing is faster. (Heck, I recommend this to folks even when they have Windows on the desktop.) SQL Server Central has a good Stairway to Server-side Tracing. Server-side traces can output to file or to table - I'd caution against writing the trace data into a table on the same server that you're monitoring because that'll have a performance impact. Run Profiler in a VM. After all, if you're managing SQL Server, you probably need SQL Server Management Studio anyway, and that's still Windows-only. (That's how I manage SQL Server personally - I'm a Microsoft Certified Master of SQL Server, and I've been using Macs since the mid-2000s.) Run sp_BlitzCache. The open source sp_BlitzCache analyzes the most resource-intensive queries in your plan cache - without starting a heavy-overhead trace or XE session. SQL Server is already gathering this data for you on every supported version/edition. Yes, right now, even as you're reading advice on a web site. You can run it from anything app runs T-SQL, like Microsoft's new cross-platform SQL Operations Studio. (Disclaimer: I'm one of the sp_BlitzCache authors.) Use Extended Events instead. (This isn't really a solution, but I know someone's going to suggest it, so might as well get it out of the way. The Extended Events people are like vegans, crossfitters, and atheists: they can't wait to tell you about their religion.) Extended Events is the replacement for Profiler, but the thing is, if you're just getting started, you're going to want to use SSMS's excellent wizard for setting up a new XE session - meaning, you still need SSMS. Get a third party monitoring tool. If you need to know what's happening on your SQL Server on a regular basis, this is usually more lightweight than running Profiler. Plus, they have way more smarts built in to tell you what's been happening. 

As discussed the last time you asked this question, your top wait is ASYNC_NETWORK_IO. SQL Server is sitting around waiting for the machine on the other end of the pipe to digest the next row of query results. I got this info from the waits stats results of sp_Blitz (thanks for pasting that in): 

Good news: 150,000 forwarded records isn't actually that bad, depending on what kind of time span we're talking about. Forwarded records are tracked as long as the server is up (with some gotchas around a bug in specific 2012/2014 builds.) Even when it does work online, your users can notice it depending on your IO throughput, size of the table, number of nonclustered indexes, and your workloads. Here's how I tackle it: 

Run that query and bask in the warm knowledge that comes with reading - not just clicking on - the documentation. ;-) 

If you monitor the Perfmon counter for Available Memory over time, and you consistently notice that most of the memory is going unused, then you can inch up max server memory to a higher number. 

Don't tune with percentages. Your server will always have 100% of its waits on something - but there just may not be much load. Your server is the classic example. Say you only run 1 query per hour. All of the query's data is already cached in memory, and there's no one else competing for locks. That query has several parallel tasks in it, and they all need to get CPU time. That query's just going to work for a while, then finish. If you looked at signal waits, it might seem like 90-100% of the time, WHEN IT WAS WAITING, it was waiting to get on CPU. But it just wasn't waiting that much. Instead, look at wait time per core, per second (or per hour) as described here: $URL$ (Disclaimer: I wrote that.) That way, you can see if the server's really doing any work at all. I bet in your server, you're only waiting a few minutes per hour - meaning, you just don't need to bother tuning that server overall. It doesn't mean every QUERY is fast - it just means you can't do server-level tuning, and you have to switch to query-level tuning. (You have to ask why the query is slow rather than asking why the server is slow.) 

Generally in our setup guide, I recommend that you leave 4GB or 10% free for the operating system, whichever is larger. In this case, 4GB is larger, so you'd set max server memory to 28000. The free memory is for: 

Theoretically yes, practically no. If SQL Server can guarantee that a field is unique, then it can build execution plans differently. Rob Farley blogged an example with AdventureWorks where the unique index isn't even shown in the execution plan, and yet its existence let SQL Server build a better plan. If your data really does qualify for a unique index, then specifying it as unique will ensure that SQL Server always estimates 1 row to come back when you search for one value. That'll mean you're more likely to get more granular locks instead of table locks. However, if you're worried about lock escalation, it's probably because you're not searching for equality. You're joining to other tables, or using range searches. In those cases, the uniqueness of an index isn't going to save you. 

Usage statistics come from sys.dm_db_index_usage_stats, which tracks the number of execution plans that include an operator touching that index. It's reset on SQL Server service restart, or when the index is modified. Operational statistics come from sys.dm_db_index_operational_stats, which track the number of times the index has actually been touched. It's reset on a different schedule - when that object's metadata disappears from cache. 

At first, it basically looks like sp_WhoIsActive, but here's some of the additional columns it shows: 

Welcome to Stack. For reference, when you ask a question, keep it to one question per question. You've got a few different questions in here: 1. How many threads should I use? There's no "right" number. I happen to use 8 in my short SQLIO instructions, and in my longer (overnight) tests, I try lots of combinations (2, 4, 8, 16, 32, etc). The more threads you use, the more load you're throwing at the storage. Most of the time, though, I can hit the storage's limits with just 8 threads as described in that post. 2. Do Perfmon metrics map up to SQLIO numbers? No, for lots of reasons. You may have different things happening on the storage at the same time, your drive format may not match the IO that SQLIO is doing, you may be testing multiple logical drives on the same physical drives, etc. 3. What SQLIO parameters test the disk to extremes? You have to try lots of combinations because different types of storage responds to different loads differently. There is no single "worst case scenario" for all types of storage. 

I think what you're looking for is, "What are all the things I need to keep in mind when designing a data warehouse?" There's a lot of good resources out there on it: 

In SQL Server 2017, after applying Cumulative Update 7, there's a new system stored procedure in master called sp_restore_filelistonly. What's it for? 

If the job is still running, check what command it's currently executing with Adam Machanic's excellent free stored procedure, sp_WhoIsActive. After installing it (typically in the master database), you can run sp_WhoIsActive to list the running queries and see what command they're executing right now. There's even a Percent Complete column that gets populated for backups and restores. You might have a particularly large database (or log file) that's just in the midst of getting backed up. For further followups & clarification, try taking a picture or copy/pasting the sp_WhoIsActive results in, showing what the maintenance plan's currently doing. 

Your original sample code wasn't re-runnable (the table creation wasn't in there, or the population of rows) so I rewrote it to flesh it out and make it re-runnable. The problem is that your query is trivially simple: it doesn't matter whether stats are updated or not, it's going to produce the same plan. This tweak will make your SELECT query bypass trivial optimization: 

SQL Server Management Studio sends a request to FORBISPRD08\BIS01, who sends a request to FORNSQPRD02\NAV01, who has the data, and sends it back to FORBISPRD08\BIS01, who forwards it on to SQL Server Management Studio 

With database mirroring, you have to create a snapshot of the database in order to run queries against it. A few problems there: 

I’m going to reword your questions a little: Q: Should I log Agent jobs directly to a remote file server? No, because if you lose network connectivity or the file share goes down, your Agent job could fail. Your Agent job might not require network connectivity, like an update stats job, so that failure would suck. Q: If I want to centralize Agent job history, how should I do it? Consider logging to a table (and Ola’s scripts support this too.) That way you can centralize the data in whatever method you like, like replication or log shipping. I like logging to a DBA utility database, and then restoring that from all my servers to one central server daily. Then I use union all views to combine the data from all my servers. It’s not up to the second, though.