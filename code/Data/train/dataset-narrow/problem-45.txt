Case #3 proving that texture drawing works by drawing to front buffer (I put a texture into a fbo and never drew to the fbo) 

Most texture data is stored in a format where the rgb brightness is incorrect because it has not been square rooted (right?) therefore if we want to display it correctly When using textures is the gaussian formula technically incorrect? If so what would be the mathematically correct method. What part of this problem is the monitor responsible for? Does it square or squareroot the values it gets? What part of the problem is the GPU responsible for when you ask it to sample a color at a point? Does it square or squareroot the values it gets? Do people know about this property and just not act on it because square rooting a number is an expensive computation? If so do we have a good way to quickly approximate the squareroot of numbers from 0-1? How do you apply the knowledge from that video in your graphics? 

I am trying to recreate a filter that Facebook released back in 2015 when gay marriage was legalized in the US. Unfortunately, that filter no longer works/exists so people are left with only knockoff ones that dont do quite as good as a job. None of them manage to do quite what the actual one did espeically in the red, yellow, and purple stripes. I am trying to figure out what kind of filter they used to achieve this. I have tried a bunch of colors with each of the blend modes available in my photo editing software with no luck. Perhaps it is more complicated than just one math equation? Perhaps they processed the background image before putting on the overlay? I did find in the blog that the algorithm to achieve this was a O(n^2) algorithm but that is about all I know. Any ideas of what could have caused this? Below are some reference photos I used to attempt to find a pattern. Example 1: Mark Zuckerburg (note this isn't exact. I couldn't perfectly recreate the crop they used) 

Basics of Spherical Harmonics Spherical Harmonics is a way to represent a 2D function on a surface of a sphere. Instead of spatial domain (like cubemap), SH is defined in frequency domain with some interesting properties and operations relevant to lighting that can be performed efficiently. With increasing "order" of SH you can represent higher frequencies (details) of functions as illustrated in the image below ($l$ is the SH order). By scaling and summing the below "basis functions" you can represent any kind of 2D function on the sphere up to the frequency defined by the functions. The basis functions are defined with "associated Legendre polynomials", but usually you don't need to derive these yourself but can use existing derivations for real spherical harmonics. 

Normally $sin(\theta)$ isn't written as part of the pdf, because it's an artifact from using double integral for spherical integration. The double integral does the integration over a rectangular domain (in you case of size $2\pi$ x $\pi/2$), while you want integration over a sphere/hemisphere, so $sin(\theta)$ is added there as a weight function to transform rectangular to spherical integration. A common notation is to use integration over solid angle with single integral and differential solid angle $d\omega$ denoting 2D integration over sphere, and you won't see the $sin(\theta)$ term in there. To clarify, the notation with single integral has constant infinitesimally small solid angle $d\omega$ over the sphere. However, for the double integral over $\theta$ and $\phi$ the solid angle isn't constant (think of sphere tessellation) but a function of $sin(\theta)$. In case of Monte Carlo integration and even distribution of samples over the hemisphere, each sample represents solid angle of $2\pi/N$ steradians and there's no $sin(\theta)$ solid angle weighting required. 

At least on iOS every frame you are likely to get a completely new texture that you need to draw to due to the OS switching between a couple textures each frame. My question is essentially if the OS feels the need to use multiple textures should we also be using multiple textures for some reason when doing full screen post processing? Is there ever a time where switching between off screen textures can improve performance? 

Although I am not sure it is physically accurate to have those spikes on the edges of the diamond I sure do like the effect that creates. 

What it does is draw capsules on the screen. Basically these capsules connect where the particle was to where it is. On the cpu I compute the vertices of this with some linear algebra, but in screen space terms the rectangle we will be shading has a maximum width of and a minimum height of . If the capsule is moving faster then then the capsules geometry will start to get longer to accommodate. Here are some diagrams of what is going on: This is an image of a single capsule. Because the distance between the current and last position is not large enough so the screen-space geometry is a square. You can see that to compensate that lighter area is shaded in as well to connect the two points. 

Yes, you should set the lightmap texture as the render target and use UV-coordinates as vertex position coordinates in your shader to bake lighting into lightmaps. This way you'll write the baked lighting data of triangles into the correct lightmap texels that are used for rendering the triangles on the screen. The UV-map should be unique and no triangles should overlap in the UV-space. The lightmap data needs to be also expanded so that when you later use bilinear filtering to fetch the data from the texture that you don't end up having dark borders following triangle edges at UV seams. Instead of baking indirect illumination into surface lightmaps, you could also use volume textures. Nice thing is that the with volume textures you'll have the indirect illumination data available everywhere in the scene and you don't need unique UV-mapping, but you'll also have less resolution due to the added memory usage. If you need to bake indirect illumination into textures depends on the complexity of the scene, quality of the lighting and performance/mem usage you want to achieve, so there's no easy answer. 

I have a game with similar unprocessed graphics to his. I have been trying to get a glow effect like this working but it has been near impossible. I am currently using separable convolution gaussian blur methods in addition to downscaling my blur mask. Still, it barely runs in real-time. It is nowhere near as big as a radius as this app pulls off and it certainly isn't as good of quality. Is there some method I am not thinking of? I am open to the possibility (but doubtful) that they are using really large textures with a falloff distance or that lighting is computed from the distance from a fragment to light sources. If you play with it and notice when the FPS drops it it just doesn't seem to have the right FPS to particle amount and HDR on/off quality to it to be that. 

My goal is to take a point that is inside of a circle with a given radius and put it on the circumference. Recently I have been normalizing the vector between the point and the center of the circle then multiplying that by the radius. However I need (if possible) a less computationally expensive method because the distance formula is expensive. Another thought I had was to use "atan2" to get the angle between the two points and then use sine and cosine multiplied by the radius to get the point on the circumference. Which method do you think would be faster for the computer to process? Can you think of a faster method. Details about the simulation This is an ios application written in swift. Basically there are a bunch of particles moving around randomly. And the user is putting down fingers. Each finger is a circle with a radius that grows as time goes on. The part that is inneficent is that if the dot is ever inside of any of the circles (attached to touchscreen touches) that it goes on the circumference of the circle. 

If there is no divergence (i.e. all threads in a wave take the same branch) newer GPU's can skip all the work within the if-branch. If there's divergence, then code in both branches is executed, but thread execution mask basically defines which threads execute code in which branch (code in non-executed branches for threads are effectively NOPed out). This is basically the same as predicated branching but happens dynamically based on divergence. On GCN architecture at least the branching itself is basically free or at least very cheap (handled by a separate unit running parallel to ALU for example), but something to keep in mind is that branching also tends to increase GPR pressure, which in turn lowers the occupancy. Low occupancy means there can be less threads in flight at once which influences GPU's ability to hide memory latencies. This may or may not be an issue in your shaders depending on how they access memory, i.e. you should see performance increase along with increased occupancy in shaders with heavy memory access. So it's still good to optimize out branching where reasonable, but I think it's one of the things good left to the optimization stage where you need to squeeze max performance out of your shaders. 

When you have to code a software to support multiple graphics libraries how do you generally do it? Do you loose any efficiency with the technique you use? 

I have been working on how the GPU does parellel processing, and branching. However I am not yet to the point where I know how to make this shader more efficent. Essentially I dont know enough about how it works. Anyway I have the following shader: 

The thing is these capsules are all individual, sometimes every capsule will be elongated, sometimes it will only be a small percent. This shader needs to be as steady fps as possible. Now here are some tips that are useful and might help make things more efficent: 

I realized that the problem was that I was sending the instance id to the fragment shader via a varying, somehow that was causing issues even though each vertex should have had the same id. 

I am starting to think about how I can make my app (currently written in Metal) available on older device that dont have Metal as well as Android. This is a predicament because while I could write two separate rendering classes that use the different frameworks it doesn’t teach me anything about how to code for multiple graphics libraries. I can only get away with this because my rendering code is fairly short and has few states and draw calls. I want to learn for the future. Writing a renderer that supports multiple frameworks especially becomes difficult when thinking of the two libraries I hope to support — OpenGL and Metal — which are quite different in how they expose the GPU. I am curious how exactly one usually does design their code for apps that can target multiple graphics languages. Here are my theories of what could be done: 

You can determine by calculating line-plane intersection. Your line starts at and has direction , and plane . This can be done as follows: $$x=\frac{(V-P)\cdot N}{N\cdot D}$$ is the distance from along to the intersection point with (assuming both and are unit vectors) 

Lowering register pressure doesn't necessarily give you any performance boost though. I recently went through this exercise myself on GCN architectures (for a simple ray tracer) and reduced register pressure so that it increased occupancy from 2 to 4, which had no impact on performance. It's generally a good idea to reduce the pressure if you need to hide memory latencies, but it really depends on what the real bottlenecks in your shaders are. Since you are working on a GPU ray tracer, you might get better improvement in performance by paying attention to the divergence of threads and try to improve this instead. For example by trying to group rays based on location and direction to reduce divergence in KD-tree traversal per wave. I'm not familiar with the paper you are referring to, and you may already have considered this though. 

I was messing with glowing lighting when I noticed an odd artifact in my first try. For some reason I can see three defined edges inside of the glowing pattern. I downloaded the image and marked where I see the edges. 

Write two different rendering classes keep the data and simulation stuff in its own area and just vary the renderer. Take high level functions like createTexture or copyTextureOnto and have an if statement checking which language the device supports and have a custom implementation for it. Coming up with your own graphics objects that are akin to the objects from the framework (such as MTLCommandEncoder or MTLTexture) and use those to build you app and just use the implantation that matches your graphics frameworks. Essentially recode the functions one of your graphics libraries has that you use and just add an if statement to your implementation is called on a OpenGL supporting device. 

I am running this fragment shader on every pixel on screen. I am trying to make it as efficient as possible. BTW I am running open gl es 2.0. The code segment below is only a sample from the code, there are about 56 different calls to Gaussian() spread across different functions. I am wondering wether it would be valuable to replace the calls to Gaussian() with there appropriate resulting float value. I know a lot of times stuff like this is pre-calculated on compilation of the shader, or calculated only once because the gpu realizes it is the same calculation for all fragments. So would it be worthwhile for me to manually calculate each of these and replace them with their values? 

I think you need to split your question into real-time and offline PBR research. For real-time PBR it's mostly about finding fast approximations for path traced equivalent. For example we know how to implement area lighting with contact hardening shadows with a simple MC integrator, but finding fast approximation for this is a complex task requiring strong background in mathematics and rendering algorithms/data structures. For offline PBR research I think it's partly about understanding optics and limitations of current PBR algorithms and where the two diverges in a significant & meaningful way in the worlds we try to simulate. For example we know that the microfacet model is an approximations of the more complex way light interacts with surfaces, but are there important situations where this approximation fails in a significant way that we would need a better model? Also even in offline rendering the performance is an issue, and all kinds of clever mathematical techniques needs to be developed and employed to speed up computations. For example calculating multiple scattering in participating media with brute force path tracing is extremely time consuming and clever techniques are needed to make this computation feasible for movies for example. Or techniques such as importance sampling is developed to reduce the computation in an unbiased way. For both real-time and offline PBR research you need sufficient background information in the domain of research and good mathematical/algorithmic toolbox to be able to make any meaningful contribution to the research. 

Right now I am just taking the oscillating float time and passing it in directly, but eventually I will put it on a function so it fades in, temporarily is extra bright, then goes to the source texture. Edit: I solved my problem, to my surprise the GLSL log function is in base e rather then 10. 

So I have a batch draw call I am doing with various squares. I have a 4x4 tile map with the numbers 1-16 going in right/down order. When it gets to the fragment shader their is a varying float "id" that holds the number. In a perfect world if the id was 0, it would sample the top left and display "0", if the id was 4 it would sample the top right and display "4". Thankfully this is mostly working! However the numbers 5, 9, and 13 (which happen to be on the left of the tile map flicker! The values on these squares just change frequently. I have traced it down to being the fault of the sample location. And probably this function here: The goal is to take the id and return the proper row and column to texture map.