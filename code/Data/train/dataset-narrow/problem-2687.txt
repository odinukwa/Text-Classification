This is a very short question. I want to implement ambient light in the color of daylight from the sun. What is the color value of sun light? 

After struggling a lot with this issue, I finally came up with a solution. To use both, a texture atlas and mipmapping, I need to perform the downsampling myself, because OpenGL would otherwise interpolate over the boundaries of tiles in the atlas. Moreover I needed to set the right texture parameters, because interpolating between mipmaps would also cause texture bleeding. 

It is very likely that you can fix the issue by your self then. Otherwise please update your question with screenshots of those debugging steps so that we can provide further help. 

It completely depends on whether you can make assumptions about what's common for all renderable entities or not. In my engine, all objects are rendered the same way, so the just need to provide vbos, textures and transformations. Then the renderer fetches all of them, thus no OpenGL functions calls are needed in the different objects at all. 

The solution was to create a character or actor class representing a rigid body in the physics simulation. An instance of that class can be attached to the camera. Both actor and camera have distinctive transforms thus position and rotation. The camera then updates its position and yaw from the attached actor's rigid body. Pitch and roll aren't synchronized. The position can be shifted say 1.8 meters up so that the viewpoint is located where in reality eyes are located. Moreover the rigid body contained in an actor is restricted to not change pitch and roll, so that the collision shape cannot fall on its side. That way the camera can freely rotate in pitch and yaw, the actor turns around with the camera but doesn't rotate upwards and downwards. So the collision shape, which might be a cylinder or an ellipsoid, always stays upright. To move the camera, we can simply apply forces to the attached actor instance. 

To more your camera, you can first calculate the vector for the directions forward and sidewards (in this implementation to the right side). Then update the position based on that vector, the speed of movement, and the elapsed time . Relying on the time is important to make the movement independent from the FPS of your game. Otherwise the camera rotation would be faster on faster machines and vise versa. Calculating the forward vector is just basic trigonometry. If you have no basic understanding of sine and cosine you can look it up on Wikipedia or somewhere else. After computing the forward vector, we can derive the sidewards vector like shown in the code. 

I hope this helps as an addition to the other answer already posted. Personally, I see the last point promising for the feature and would like to know if someone tries to use this form of monetizing in a game engine or game. 

I think ray trace rendering had to be done on the CPU for a long time. But since we have compute shaders in OpenGL 4.3 now, it might be possible to move the computations on the GPU and perform passable real time rendering. What approaches for GPU based ray tracing are there already? Can it compete with rasterization rendering nowadays? 

I implemented deferred rendering in my little engine using framebuffers but there is no dynamic lighting for now. The g-buffer contains positions and normals in view space and albedo. To implement real time lighting, I guess that I need to perform one pass for each light. Given the position and shape of a light, how can find all pixels affected by the light? I want to support point lights, spot lights and directional lights. 

This may be a winding issue. Are you sure that the texture coordinates are parsed in the right sense of rotation? However, this how you should debug you program. 

I know that it may not be your aim to code instructions for generating every texture you want to use. But it may also be a challenge and comes not only with the orientation independent advantage but also with heavily reduced storage space and unlimited zoom-in detail. I would be very interested if you would try to implement solid noise textures for your terrain. 

Given your description, your camera representation might look like this. I also included the headers of your maths library GLM needed for the following implementation. 

Force makes rigid bodies accelerate, so their speed grows over time. Most physics engines reset the applied force every frame. Therefore, you need to apply force for several frames to give the bodies time to accelerate. Another solution would be to set speed directly, instead of applying force. That would result in instant motion. 

There might be other useful features to raise the ability and easiness to mod a game, but programming style, documentation and scripting might be the most important factors. 

Since you have split up the mesh now, you can perform LOD and culling techniques on it to skip rendering of hidden chunks. 

Maybe there is a cool way to all the functionality of GLM over to Javascript. Are there things like C++ to Javascript source code converters? Or automatically binding a whole C++ namespace to scripting? I don't think that is possible. But Javascript is such a dynamic language, there must be a way to make the GLM types available for it. What ways are there for making the vector types, and preferably also the useful geometry helper functions, available in scripting? 

I want water to be a gameplay object in my game. The player should be able to take in in a bucket and tip it out somewhere in the world. Water should flow in some way so that it can be used to build rivers and transport game objects or to create particular apparatuses. It's comparable to the game Minecraft where you can build cool things involving water or just create just some small waterfalls. However, there my game isn't made out of blocks so there is smooth terrain. In most games water is simply placed at a given height. That works well for lakes and Oceans, but for flowing rivers and usage in apparatuses this concept is to limited. In Minecraft water is implemented as a block which spreads inside the blocky grid, too. That approach is a bit more what I need to realize my idea but I don't use these blocks and therefore such water simulation would look very misplaced. What concepts of representing water which the player can interact with in the mentioned manners are there? How can I realize oceans, rivers, buckets and so on with one general approach? There shouldn't be an infinite amount of water. If you fill your bucket, the volume of the lake the water was taken from should decrease unnoticeable. Moreover if the player would place water somewhere that volume of water should never go missing. 

There are many factors that improve modding abilities in a game. Nevertheless, most games don't allow you to replace core programmings. But if your "hope is that anything can be replaced with custom code", there are two key features you should provide to the community. 

Your question is very general and I haven't faced that problem yet. But I have a basic idea how this can work. In your example, the input thread need to communicate with the state thread and the state thread need to communicate with the rendering thread. There are two concepts coming to my mind here. First, you could make use of double buffers. For example, the state thread provides its results in the front buffer. While the renderer reads them, it continues working and stores the results in the back buffer. Each time new results are available, the buffers are switched so that the newest state is read always. Another concept that might be favorable is a queue. For example, the input thread captures the input events and simple adds the actions to a thread safe queue. The state thread can fetch them anytime and they are removed from the queue after that. In contrast to this way of using a double buffer, the queue ensure that all input events arrive. My personal guess is that, as you already mentioned, the lack of synchronization wouldn't be too important. Since input, state and renderer have a higher frequency then the human eye can notice (hopefully) there might be no need for synchronization. But as I already said I haven't faced this problem yet so I cannot assure that totally. 

Another way would be to procedurally generate your textures. This approach comes with its advantages and disadvantages. Most important tradeoff is that you can't draw your textures in an editor but have to code them using noise functions. The huge gain in your situation is that you can texture your terrain completely orientation independent. This is done by using so called solid noise. It is a three dimensional noise function. Using that you can code a function to generate the color value of any given position in 3d space. Then you sample just the points lying on the mesh faces. In other words, you carve out from the three dimensional texture. This is from the libnoise glossary. 

Store the existence of terrain in your data structure. That would be a boolean value where true means solid and false means air. You could also extend this and store a material as integer, where 0 means air, 1 means dirt, 2 means stone, 3 means wood, and so on. To generate a mesh from that data, use marching cubes or a similar algorithm. It calculates a polygon mesh out of voxel data. By the way, I suggest octree if you want a tree data structure. It is the 3d equivalent of quadtree and therefore it fits your world better. Although I would go with a simple 3d array for now. Later on, for larger worlds, you can decide whether to use octrees or divide the world in chunks. To save space, you can use run-length encoding. There is a very good article on the blog 0 FPS discussing different data structure for such terrains, including array, array chunks, octree and a combination of interval tree and hash table which shows up to be the fastest structure.