When the exact definition is in question, the glossary helps. How this data is handled determines what PCI Self Assessment Questionnaire (SAQ) is applicable to your business. Unfortunately, you do not provide enough information for me to confidently identify what SAQ is applicable to your business. An excerpt from the SAQ guide should help: SAQ A -- Card-not-present (e-commerce or mail/telephone-order) merchants, all cardholder data functions outsourced. This would never apply to face-to-face merchants. SAQ B -- Imprint-only merchants with no electronic cardholder data storage, or standalone, dialout terminal merchants with no electronic cardholder data storage SAQ C-VT -- Merchants using only web-based virtual terminals, no electronic cardholder data storage SAQ C -- Merchants with payment application systems connected to the Internet, no electronic cardholder data storage SAQ D -- All other merchants not included in descriptions for SAQ types A through C above, and all service providers defined by a payment brand as eligible to complete an SAQ. Additionally, the volume of transactions you process determines what PCI level is applicable to your business. While this varies between card companies slightly, they are usually very similar. Additionally, the requirements for levels vary between Service Providers and Merchants. All levels require quarterly scans. Most require annual self assessments. Finally, at level 1, you must have a Qualified Security Assessor (QSA / auditor) complete your Report on Compliance. (ROC) While if you fall under the qualifications identified above, you will officially have to be PCI compliant on some level. Nevertheless, your bank or acquirer is going to ultimately determine your PCI reporting requirements. Do your homework and then contact your bank, they are your best bet for determining the final expectations. 

If it is convenient for you as a temporary solution, it should be perfectly acceptable. I cannot think of many scenarios where having multiple PTR records with the same hostname will introduce any technical issues. One potential scenario would be mail delivery on the new server. At least, if the forward lookup resolves to the old server. Fickle mail servers will bounce mail without hostnames/IPs being able to resolve both ways and match. Outside of that, and I'm really trying, I can't think of any. If there's more, it's likely to be of limited scope like above. 

While you cannot do this with native MySQL support, as many others have pointed out, in most cases it can be done fairly easily with a script. However, easiness will depend on how complex your schema is. Here's an example of a query that deletes from a table that has a timestamp field using date and time functions... 

If you want the RHEL5 package, you can download the kernel source RPM here. If you want the kernel source from the primary distribution, you can get it from kernel.org. Depending on the particular driver or part of the kernel you want to look at, it may be in the directory. 

When I want to log a root session I use rootsh. If admins are required to use sudo for all commands, it will also log all commands. You might take a look at screen too, as it might accomplish what you're attempting to do. Ultimately, logging may not be the best situation for your workflow. Requiring a motd, wiki, or simple Web site to be updated for all changes may be better. 

You can specify a specific IP in place with the asterisk, as long as the IP is specified with NameVirtualHost. The name is specified in and . The asterisk is matching all IP addresses that Apache binds to in the . 

is typically the latest version and is usually a symlink but it depends on a mirror. We use this for our local mirror: 

Yes, schema changes replicate as any other event. MySQL slaves replicate master's binary logs into a relay log and then executes the events. Of course, this is if or another variable identifies the specific scope in question to execute the statements. There was a bug in MySQL, where if you executed alterations by specifying the table as , it would not replicate. It required a statement to proceed the queries that matched a variable to actually execute the queries. I believe this is addressed in the current version but is still something to be aware of. This functionality is well documented on the MySQL Web site. 

What options are there for session handling with a modern .NET IIS Web architecture? I understand that IIS' native session support loses sessions upon restart. Furthermore, Microsoft recommends sessions be stored in the database, which is my preferred solution. I am attempting to architect a highly available .NET IIS platform, which scales horizontally for a third party application. The application is developed externally and while I can push for storing in a database but that will not be possible for an immediate deadline. The third party recommends using persistent sessions, which is something I avoid. Sticky sessions will lessen availability, as when the Web server becomes unavailable it will cause accessibility issues to the clients that were previously using it. Moreover, it lessens the load balancing functionality, as a single server may receive potentially more requests than the others. IIS will still lose sessions upon restart, which is something I would prefer to eliminate. I also do not want to store the session in a cookie on the client side. I want to enable complete high availability, while still allowing application deployment to idle servers without impact. Additionally, full load balancing. Can I enable this functionality on this technology platform without rewriting the application? What options do I have? 

There is no verification process for specifying the "From" header if you have relay access to a SMTP server. However, if you do not have relay access, you will be limited to sending e-Mail that the server receives mail for. If you run an e-Mail server, you can specify most e-Mail headers freely, which includes the From header. Put simply, the SMTP protocol does not prevent this from occurring. 

No. specifies the databases to be replicated. However, I'd say the performance you're describing is better aligned with "clustering." You might take a look at MySQL cluster (the NDB engine), as it may meet part of your expectations. 

Your gateway, for whatever reason, has 192.168.135.101 bound to it. I bet if you brought down eth0:1 and the ARP table didn't have an entry for the 192.168.135.101, it will still respond to an ICMP request. Focus on the machine with the 00:1A:A2:2D:2A:04 address, it's the culprit. 

I typically do not on a filesystem that is used for data storage. For example, if only MySQL data is stored in the filesystem, and MySQL runs as the user, I do not want the data to be wasted. For the filesystem and other filesystems that are integral to the system's operation, that reservation is well advised. 

Why would it be overkill? If you want to dynamically manage the volumes, LVM is the way to do it. RAID1 and DRDB are good high availability solutions. I like to separate volumes that I need to manage with different mount options, partitions, and settings. Another primary reason would be a volume that I expect to grow, as I would dedicate a separate disk set to it for an easier upgrade path. 

I'm addressing this question solely on a technical level. Perhaps you should considering building a device specific for this purpose instead of using the SonicWall feature. Default policy of drop is generally recommended for traffic going outgoing. At the very least, 80, 443, 8080, and 8443. Then require a proxy to access the Internet. I'd recommend SQUID and SquidGuard, which can filter access to Web sites. Configure it to require authentication, which can even be integrated with AD. A privileged group would be able to bypass the filtering. For reporting, something like MySar or SARG works. The solution can be simple and quick or more involved depending on your requirements but this technology will solve all the problems. 

It will be difficult to locate secure infrastructure without being more hands on. You will likely be unable to fully secure the data stored on a hosting environment such as Amazon EC2. Specifically in regards to credit card storage and PCI, it's generally recommended not to store data in those environments. Even having a dedicated server will not meet PCI requirements for storing data, as you will need to physically secure the space and have controlled physical access. To meet most security requirements, you would need your own space with access control and physical security. For example, a fully enclosed cage within a shared data center could qualify. Most lower level products such as virtual private servers and dedicated servers would be unlikely to qualify. The regulations applicable are going to be unique to the data, which will help clarify the exact requirements. If you want to get an idea for what you are getting into, you can look at the PCI DSS. Good luck.