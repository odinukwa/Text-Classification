In chapter 8 (page 288) of the "Handbook of Applied Cryptography," the authors describe an attack against RSA with small exponent. Let there be 3 parties with independent RSA public keys $(e_1,n_1)$, $(e_2,n_2)$, and $(e_3,n_3)$, with $e_1=e_2=e_3=3$. With very high probability, the moduli are coprime; otherwise, it would be possible to factor them. Consider a forth party, who wishes to secretly send a single message $m$ to the 3 parties above. He computes $c_i = m^3 \pmod{n_i}$, and sends $c_i$'s on the channel. The adversary, who eavesdrops the ciphertexts, can simply apply the Chinese Remainder Theorem to recover $m$. (See the above chapter for exact computations.) 

Some open problems (perhaps solved, but not that I know of) regarding complexity-theoretic aspects of PoKs: 

The construct by Pat Morin and many others are reviewed in Building Secure Block Ciphers on Generic Attacks Assumptions. 

The FNP version of an NP-complete language $L$ asks to solve the following problem on every input $x$: 1) If $x \not\in L$, output "NO". 2) If $x \in L$, output some witness "w" such that a polynomial-time entity, given $(x,w)$, can verify that $x \in L$. Consider the transformation $T$ of the above scheme: If the FNP machine answered "NO", $T$ answers "NO". Otherwise, $T$ answers "YES". Obviously $T$ is a Karp-reduction of any NP-complete problem to the corresponding FNP-complete instance. 

5 recent papers on the topic can be found in the proceedings of TCC 2011 (see chapters Black-Box Constructions and Separations and Black-Box Separations, pages 541--629). They have surveys of previous results, which are certainly useful. 

A basic operation can change the label of the node. For instance, $x$ can be changed to 3, or + can be changed to $\times$. A basic operation can build an expression tree on top of N (see the example below). 

61 thought P≠NP. 9 thought P=NP. 4 thought that it is independent. While no particular axiom system was mentioned, I assume they think it is independent of ZFC. 3 just stated that it is NOT independent of Primitive Recursive Arithmetic. 1 said it would depend on the model. 22 offered no opinion. 

People who are working in different areas of computer science may benefit from various subfields of analysis. To give you a concrete example, I'll describe my own case. I'm conducting research in foundations of cryptography. In this field (as well as in the computational complexity), there's a construct called the random oracle (see also this page). Its various properties are sometimes studied by exploiting tools from measure theory, which is a subfield of analysis. Such treatment can be found in this paper, as well as in several papers which cite it. You can also take a look at Basics of Algebra and Analysis for Computer Science by Jean Gallier. It's a book in progress, and tells you what's new in the field. 

Heller, H. 1984. On relativized polynomial and exponential computations. SIAM J. Comput. 13, 4 (Nov. 1984), 717-725. DOI= $URL$ Heller, H. 1986. On relativized exponential and probabilistic complexity classes. Inf. Control 71, 3 (Dec. 1986), 231-243. DOI= $URL$ 

You can take a look at the following paper: Dwork, C. and Stockmeyer, L. 1992. Finite state verifiers I: the power of interaction. J. ACM 39, 4 (Oct. 1992), 800-828. DOI= $URL$ or the related paper: Dwork, C. and Stockmeyer, L. 1992. Finite state verifiers II: zero knowledge. J. ACM 39, 4 (Oct. 1992), 829-858. DOI= $URL$ 

Some functions are conjectured to have that property, aptly called moderately hard. They were first proposed in the context of spam fighting, and then found their ways into more complicated applications, such as concurrent zero knowledge and timed commitment. They usually use a function of the form $f(x) = g^{2^{2^x}} \bmod N$, first suggested by Rivest, Shamir and Wagner: 

In the following, I try to minimize the background knowledge needed to understand the answer. If you need more details, I suggest taking a look at section 12.2 of Cryptography and Network Security Principles and Practices, Fourth Edition. (Though it requires a fair knowledge of crypto.) 

Just found the following two websites. They were recommended by some folks, but I have not evaluated them yet: 

Shoup shows how these games are connected, and thus proves the following: Any oracle machine that makes at most $q$ queries to its oracle, its RF/RP-advantage is at most $\frac{q^2}{2} \cdot 2^{-\ell}$. 

PS: You asked for a book, but I believe this article: The Computational Complexity of Some Problems of Linear Algebra is also useful (yet it dates back to 1999). 

I found the book Pairwise Independence and Derandomization on the subject, but it's more research-oriented than tutorial oriented. I'm new to the subject of "Derandomization," and as such, I wanted to know which reference to start from? I prefer one that discusses literature and history, as well as the technical details. 

More formally, let $G$ be an RSA public-key generator, and $A$ be a PPT adversary. Is it possible to prove, under the RSA assumption, that the success probability of $A$ is negligible in the experiment below? 

This is somehow different from what you asked, but since it is related, I thought I could mention it. Carter & Wegman (1977) introduced the notion of universal hashing. The notion was used in numerous papers (Sipser (1983), Stockmeyer (1983), Babai (1985), and Goldwasser & Sipser (1986)) to prove approximate lower bounds. This was until 1987, in which Fortnow made use of the universal hashing to prove approximate upper bounds. (In fact, to provide a protocol for proving approximate upper bounds.) 

First take a look at Merkle–Damgård construction. Virtually all hash functions follow such construction. Informally, it applies a compression function iteratively to reduce the input size to get some fixed-length output. For instance, you can hash a whole DVD (~ 4.3 GB) and get a 128-bit code. Let $M$ be the input to an MD-based hash function. The MD construction appends a pad and the length of $M$ to it, so as to prevent several attacks. Whirlpool uses a compression function named $W$. $W$ is similar to a block cipher named Rijndael, which is now standardized under the name AES (Advanced Encryption Standard). Rijndael has 3 variants: 128-bit, 192-bit, and 256-bit. The inventors of Whirlpool decided that no Rijndael variant is secure enough to be used as the compression function for a hash. Thus, $W$ is designed so as it accepts 512-bit inputs, and produces 512-bit outputs. The key size of $W$ is 512 bits as well. Whirlpool works as follows. Let $M$ be the input. $M$ is divided into 512-bit segments: $M=(M_0,M_1,\ldots,M_t)$. Let $h_0$ be some initial value (fixed by Whirlpool standard). For $i=1,2,\ldots,t$, apply $W$ iteratively as follows: $h_i = W(h_{i-1},m_i) \oplus h_{i-1} \oplus m_i$ where the first input to $W$ is a block-to-be-encrypted, and its second input is the encryption key. $\oplus$ denotes the XOR operation. $h_t$ is considered as the output of the Whirlpool hash function. The whole complexity lies in designing $W$. As pointed out before, it is similar to Rijndael, so you can understand it if you get familiar with Rijndael, on which I have designed a set of slides. The slides are self-contained and do not assume any math background beyond high school. 

The classical reference is "Introduction To Automata Theory, Languages and Computation" (by Hopcroft, Motwani, and Ullman). Some people also recommend the much older "Formal Languages and Their Relation to Automata" (by Hopcroft and Ullman). I, however, like "Introduction to the Theory of Computation" (by Sipser). It is very well written, and is a relatively new book.