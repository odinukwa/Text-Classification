I can't really give any guarantees for how efficient it'd be, but it should be correct, and it would probably be a lot better than a brute-force solution. If nothing else, I hope this gives you some ideas for further investigations. 

If you need the tree to be dynamic, there are lots of ways of doing that as well. For example, you could recursively traverse the tree, finding the location that fits best (by going through nodes with the highest overlap/smallest Hamming distance), updating the unions (bounding predicates) as you go. Deletions are a bit more tricky, perhaps; you could just mark objects as missing, and then rebuild subtrees (or the entire structure) when the number of marked objects hits a certain threshold. Whether this works well for your application could be hard to say a priori, but should be easy to check experimentally. Also, there is a lot of room for tweaking. 

Yes, you certainly can (based on the fact that any subpath of a minimal path must also be minimal). That is, any shortest path entering $H_i$ at $u$ and leaving at $v$ must follow the shortest path from $u$ to $v$ in $H_i$. Basically, you can compute the shortest-path distance matrix $D$ for any $H_i$ (it would be the same for all of them, of course), and replace every subgraph $H_i$ by one consisting only of the in- and out-nodes (that is, the nodes connected to other subgraphs; presumably fewer than $|V_i|$), and use only direct edges from the in-nodes to the out-nodes, with weights given by $D$. You don't need to explicitly construct this new graph, of course. If you have the macrostructure of $G$ available in implicit form, you can compute $D$, and use that together with the macrostructure of $G$ in a (slightly customized) DP algorithm for finding the shortest path. 

For a really thorough listing of various distances, metric or not, that are in use, I recommend Encyclopedia of Distances by Deza and Deza. (It is also available electronically from Springer.) For a really simple example of a family of non-metric distances, you can simply let $p$ be less than 1 for an $L_p$ distance, for example. For a family that is in heavy, actual use, you could look into various forms of weighted edit distance. There's also the cosine measure, Jeffrey divergence, partial Hausdorff distance, dynamic time warping distance, and domain-dependent distances such as the COSIMIR model, all discussed in Skopal's paper on indexing non-metric distances. 

My problem is related to the standard bin packing problem (where you have bins of capacity $1$, items of capacity $(0,1]$, and want to pack the items into as few bins as possible), but there are a couple of twists. First, you have a fixed number of bins, and want to fit as many items as possible into them. As far as I can see, this is not obviously equivalent (except for the decision versions of the problems). Or perhaps I’m missing something obvious? Second, some items are not permitted in some bins. That is, for each item, there is a list of forbidden bins. Does this problem have a name? Even a name for the version without the forbidden bins would be useful. (I’m sure the “as many items as possible” part must be a totally standard packing problem?) What I’m ultimately looking for is a way of attacking the problem (an exponential, exact DP algorithm; an approximation algorithm; a good heuristic/B&B solution…), or some starting-point that could be adapted. 

I'm planning on running an “experiment” when teaching my algorithms class this fall, with one very old, limited computer (main limiting factor is probably memory—possibly as low as 16KB) and one modern/standard one. The idea being to solve a problem with a polynomial one, running on the slow computer, and an exponential one on the fast one (and, of course, have the slow one win). The problem is finding a suitable problem—one where the running times will be really different for instances of very limited size (and, preferably, where the data structures are quite simple; the primitive computer is … primitive). I originally thought about sorting algorithms (e.g., quadratic vs. linear), but that would require far too large instances (unless I went with bogosort, for example). At the moment, the only (rather boring) example I've thought of is computing Fibonacci numbers the smart and the stupid way. It would be nice to have something a little less tired/overused, and preferably something (semi-)obviously useful. Any ideas/suggestions? 

Just an initial association—one kind of preprocessing based only on distances is metric indexing. There are even some specific, simple structures for integer metrics, like in your case. That'll only let you retrieve all nodes within a given distance, but maybe you could use the basic ideas somehow? For example, one common idea in metric indexing is is to use sample objects as pivot points in the distance space, and to use the triangle inequality to find cheap heuristics and bounds for the (expensive) actual distance. Applied to your example of a graph $G=(V,E)$ with $n$ nodes under the unit-weight graph geodesic $d(\cdot,\cdot)$, you could select a subset $P\subset V$ of $m$ pivot nodes, and then pre-compute an $n\times m$ matrix $T$ of distances. Assuming one-to-one shortest path (given your mention of $A^*$), if you're going from $s$ to $t$, you would then fetch two rows $x=T[s]$ and $y=T[t]$, and compute $h(s,t)=L_\infty(x,y)$, which is simply $\max_{i=1\ldots m} |x_i-y_i|$. Because of the triangle inequality, this will be a lower bound to the the actual distance $d(s, t)$, i.e., $h(s,t)\leq d(s,t)$. (See my tutorial for a more thorough explanation.) The heuristic is (as mentioned) guaranteed to be a lower bound, and the more pivots you add, the tighter it will (probably) be. Randomly selected pivots work well in practice, so the method is really simple. In metric indexing, this lower-bounding heuristic is used for filtering out candidate objects during search (we only want close objects, so those guaranteed to be far away are eliminated). In your case, you could use the bound as a heuristic for $A^*$. Because it's a lower bound, $A^*$ will work correctly, and the number of pivots is a parameter for you to set. The more memory you have, the better your heuristic will be. It works for any starting node and any ending node, and it's really simple. And, as requested, for a constant number of pivots, it takes $\mathcal{O}(n)$ memory. As for the number of pivots needed—there is some contention in general. Some people claim the optimal number grows with data size (e.g., logarithmically), and some claim it's a function of the inherent hardness of the distance function, regardless of problem size. I guess you could experiment. And while there are specialized algorithms for picking out good pivots, you could just try random ones first; they usually work quite well. (Come to think of it, I once read some papers on using waypoint or somesuch with $A^*$. I don't remember the details, but that, too, was based on selecting some representative nodes and using them—probably in a manner at least similar to what I've described. I have a look and see if I can find that material again.) 

The simplest thing to do would be to set up the greedy algorithm for the problem, and then look for a counter-example. If you find one, you've got your answer. Otherwise there are many ways of proving that greed works. There are some issues with this, of course (such as how specifically to formulate the greedy algorithm). As for characterizing which problems can and which problems can't be solved greedily, there is a general answer to that, too. In fact, in their paper “An Exact Characterization of Greedy Structures” (SIAM J. Discrete Math. 6, pp. 274-283), Helman, Moret and Shapiro gave a formal description of just this (called a matroid embedding, which generalizes both matroids and greedoids). From the abstract: “The authors present exact characterizations of structures on which the greedy algorithm produces optimal solutions.” In general, the greedy algorithm can be formulated in terms of weighted set systems $(E,\mathcal{F},w)$. You have a set $E$ (for example, the edges, in the case of minimum spanning trees), and you have a set $\mathcal{F}\subseteq 2^E$ of subsets of $E$ (think partial spanning forests, for the problem of minimum spanning trees). $\mathcal{F}$ represents the valid partial solutions constructed by combining elements from $E$. There is also the weight function, $w$, which gives you the weight or cost of any element in $\mathcal{F}$. We usually assume this to be linear—i.e., each element in $E$ has a weight, and the weights of the (partial) solutions are just the sum of the element weights. (For example, the weight of a spanning tree is the sum of its edge weights.) In this context, Helman et al. showed that the following are equivalent: 

I recently read a proof that intended to show that a problem was strongly NP-hard, simply by reducing to it (in polynomial time) from a strongly NP-hard problem. This didn’t make any sense to me. I would have thought that you’d have to show that any numbers used in the reduction and the instances of the problem you’re reducing to were polynomially bounded in the problem size. I then saw that Wikipedia gave the same general instructions for this sort of proof, but I wasn’t really convinced until I saw Garey & Johnson say basically the same thing. To be specific, they say, “If $\Pi$ is NP-hard in the strong sense and there exists a pseudo-polynomial transformation from $\Pi$ to $\Pi'$, then $\Pi'$ is NP-hard in the strong sense,” and “Note that, by definition, a polynomial time algorithm is also a pseudo-polynomial time algorithm." Of course, I take the word of Garey & Johnson on this—I just don’t understand how it can be correct, which is what I’d like some help with. Here’s my (presumably flawed) reasoning… There are strongly NP-complete problems, and all these are (by definition) strongly NP-hard as well as NP-complete. Every NP-complete problem can (by definition) be reduced to any other in polynomial (and therefore pseudopolynomial) time. Given the statements of Garey & Johnson, it would therefore seem to me that every NP-complete problem is strongly NP-complete, and, therefore, that every NP-hard problem is strongly NP-hard. This, of course, makes the concept of strong NP-hardness meaningless … so what am I missing? Edit/update (based on Tsuyoshi Ito’s answer): The requirement (d) from Garey & Johnson’s definition of a (pseudo)polynomial transformation (the kind of reduction needed to confer NP-hardness in the strong sense) is that the largest numerical magnitude in the resulting instance be polynomially bounded, as a function of the problem size and maximal numerical magnitude of the original. This, of course, means that if the original problem is NP-hard in the strong sense (that is, even when its numerical magnitudes are polynomially bounded in the problem size), this will also be true of the problem you reduce to. This would not necessarily be the case for an ordinary polytime reduction (that is, one without this extra requirement). 

What you're describing, where you minimize the sum of $f(u,v)w(u,v)$ with a given flow value $|f|$ isn't, in fact, the max-flow problem—it's the min-cost flow problem. If you constrain the flow value to be 1, and all capacities are set to 1, it's pretty clear that this problem is equivalent to finding the shortest path. Basically, unit-capacity flow problems find edge-disjoint paths. If you set the flow value to be 1, you'll find only a single path. If you're working with min-cost flow, that path will necessarily be the shortest path (and, therefore, the cheapest flow). If this is not what the book is asking, and it's really asking you about max-flow, I suspect there must be some details missing from your description. (For example, you could easily have a single node $t'$ before $t$, with $c(t',t)=1$, so max-flow would have a flow value of 1, yielding a single path; however, that would still ignore the edge costs, and would just find any path, unless you're specifically using the Edmonds-Karp algorithm, which uses BFS to find its augmenting paths; in that case, you would, at least, find the shortest unweighted path.) 

You could use the Metropolis-Hastings algorithm (or perhaps Gibbs sampling) to sample your sequence, as long as you can calculate the desired probability for the entire sequence (or a function that is proportional to the probability). As I understand you, you wish to let the probability of $a$ coming before $b$ be set to the odds $a\mathop{:}b$. If all such decisions were independent, you could simply multiply the probabilities (derived from the odds) over all the pairs—but of course they're not. This might be a good enough approximation of what you're after, though, depending on what the application is. Anyway, once you have your probability function $f(s)$ (defined over the entire sequence $s$), an application of the Metropolis algorithm (a special case of Metropolis-Hastings) would be something like this: 

First, for the “getting closer” part, I agree with Neil Young: If you mean “the distance remaining along the solution path,” that follows from the fact that you have positive edge weights. If you mean “the shortest distance from the node to $e$ along any path will decrease” then you can just first compute all distances (normal shortest paths) to $e$ and delete all edges that increase this, as they cannot be part of the solution. As for the remainder of the solution, you can indeed solve that by dynamic programming, but not just the one with the maxmimum number of nodes—there may be many such paths, and not all of them will necessarily have a weighted length below your given threshold. You can solve it, though, and even with an out-of-the-box shortest path algorithm, by simply modifying the weights (assuming you've deleted any offending edges, as described above, if that's what you meant in your problem description; otherwise, you don't need to do any of that). The idea is that in order to find a path with a length below a given threshold, you can simply find the shortest one. If it's not below the threshold, no such path could be found anyway. The only remaining wrinkle is how to find the path among those with the maximum number of nodes (i.e., edges, as you have one more node than edges in a path) that has the minimum edge sum (i.e., weighted length). To do this, you can use the standard trick of having to “digits” in your cost. The idea here is that you have two different costs or objective functions, but one takes precedence, that is, it forms the “most significant digit” in the cost value. By simply multiplying this cost with a high enough value, you get what you want. So basically, you just find the shortest path from $s$ to $e$ where each $(u, v)$ has a cost of $-k + w(u,v)$, where $k$ is some large (enough) constant. (If you have $m$ edges, and the maximum edge cost is $W$, you could for example let $k=m\cdot W$, as this would be larger than the length of any path.) This basically makes each step in your path add a cost of $-k$, which means that it will always pay to find a path with more edges (and thereby, nodes) regardless of the weighted length (sum of $w(·\,,·)$) of the path. In this way, you're guaranteed to find the one with the most nodes, as you require. However, among the paths with the maximum number of nodes, you will get the same number of $-k$ values, but the sum of the edge weights $w(u,v)$ will vary, and it will always pay to minimize this (and therefore, any shortest-path algorithm will minimize it). In this way, among the paths with the maximum number of nodes, you will find the shortest one (i.e., with the lowest edge sum). If this weighted length is below the threshold $d$, you've solved your problem. If it is not, there is no solution. This will work not only for DAGs, but (if you use Bellman-Ford, for example, rather than the plain DAG-Shortest-Path) for general directed graphs where no cycles can be reached from $s$. (Any cycle in the graph will yield a negative cycle with the given weighte function, because $k$ is so large.) 

I would say the answer is clearly no (that is, no one knows), because no one knows whether the NP-complete problems can be solved in polynomial time, let alone pseudo-polynomial time. (Every polynomial algorithm is, of course, pseudopolynomial.) If you can find a problem in NPC that cannot be solved in pseudopolynomial time, you have just proven that P≠NP, so I think it’s safe to say that no such examples will be produced any time soon. 

And then you let this run for “a while”—exactly how long would depend on the length of your sequence (though the number of iterations should be on the order of the autocorrelation of the Markov process; you can probably find more details on this in a good book on Monte Carlo methods). 

Start with an arbitrary ordering $s$. Generate a different ordering, $s'$, for example by randomly swapping two elements. Compute the acceptance ratio $\alpha = f(s')/f(s)$. If $\alpha \geq 1$, accept $s'$, that is, use it instead of $s$ for the next iteration. Go to 2. Otherwise, accept $s'$ with a probability $\alpha$. Go to 2.