First, indexing. Many people dont realize that foreign keys do not automatically get indexes. Since they are used in joins they almost always should have an index. Closely examine all cursors to see if they can be replaced by set-based code instead. I have changed code that ran for hours to seconds by doing this. Avoid subqueries. If you have them in code replace them with joins or joins to derived tables. Make sure your where clause is sargeable. Learn to read execution plans. Make sure the office has a couple of good books on performance tuning. Table variables are better than temp tables in some instances and temp tables perform better in others, If you need to use them, try both and see which works better in that particular case. 

While I'm not wild about the idea of separate databases per customer (They are horrible to maintain as the database struture changes), if you must and you want a unique id you can either use GUIDs or design each table to include a client_id field and make that in combination with the regular autogenerated id be the PK. I would suggest you do make the ids unique as sooner or later you will be combining two customers because Company A bought out Company B. This is much harder if you can't guarantee uniqueness of ids. You might also consider if you can make schemas work for you to separate clients instead of databases. At least that way, you can have foreign key constraints to one common set of lookup tables. It would be a little simpler to maintain. 

Like all IT projects it is both. You cannot succeed without a strong business need and support nor can you succeed without people with the techincial skills to put the project together. 

We do this, the environment varaiable is at the user level, so the user for each environment that runs the SQL agent jobs is different. So on the the dev environment our agent user is something like SQLDev and on the QA environment it is something like SQLQA. If you are not running from jobs (which I highly suggest doing except on dev while doing actual development), yes you have to change the enviroment variable to the correct one. We have created cmd line scripts to do that easily and placed them on our desktops, so that we can easily switch the environment variables. 

Not sure why it needs to be recomplied but until you get to the bottom of this consider making the proc recompile every time it is run with the following code, at least that way it won't fail every morning: 

This is a really tough problem in the US. Names are not unique and often change during a person's lifetime or are presented differntly (Rob versus Robert for instance), so they can never be used to identify the patient except in conjunction with some more realiable information. Health insurance number and provider changes much more frequently and may be the same for multiple members of the family. SSN is supposedly unique, but there is fraud around it. Same with Driver's liscense number which of course not everyone will have. Personally, I would start with insurance policy number and date of birth and name combination, then ssn and date of birth and name combination. I would check address and phone to give me additonal assurance when they match but not much weight if they don't. Additonally I would use blood type as a rule out factor if it is known (and we all know the hospital vampires will be taking blood samples) as that doesn't change. Name matching would have to be fuzzy match due to the name varaition problem. Other things should generally look for an exact match first themna fuzzy match if the name confidence is really high (could have been a typo entering the SSN). 

When you are buying new the choice is far different than when you are considering upgrading. Buying new it is my belief that you should always buy the newest version you can get. 2008 version will be no longer supported far earlier than the 2012 version. Better to start new with the lastest as you will be using this backend for a long time. As to the need to the first service pack, it will be out before you know it and since you are doing new development, the problems it fixes will likely not affect you as much a legacy database with millions of records would face. Now if you are just getting a new server but putting an old database on it, then the question becomes what are you upgrading from? If the database is already a 2008 database, it will be significantly less risky to use the same version. If you are upgrading, check to see if you can upgrade directly to 2012 from your version. 

I don't know about mysql but in SQL server, you would use this type of thing when you can't afford to have gaps in the sequence from rollbacks. Often this is a requirement for invoice numbering for accounting auditing. 

You don't need a diagram for it to work, you need one to train the future developers! However, if what you really have is a database with no actual formal PK/FK relationships set up (Which you can do when creating diagrams as well as through better ways such as SQL scripts), then that is a different story. That is something that will eventually cause your data to be inconsistent and is a real problem indeed to other users of the database. 

I know SQL Server will take up all memory on a server by design, but this is my desktop not a network server. What should I be looking at in terms of what they installed that needs to be changed so that I can work again? I am running both SQL Server 2014 and SQl server 2008 SSMS as well as SSIS packages for both. I have no SSMS open and no are there programs besides this and task Manager Lync and my system is up to 94% of my memory being used just after a reboot. I never had this issue before they loaded 2014. But it has been continual since they did. I am sure there is some setting I need to change, but can't find anything on it as all the things I find seem to refer to how memory is handled on a server. It currently is running SQLServer.exe even though I have not opened SSMS since the reboot. I have 6 GB of memory on an Intel Xeon CPU and Windows 7. It is an HP Z600 workstation. 

You cannot just swap right and left joins. They mean two different things. You also need to understand the effect of having a where condition on the table on the left side of a right join or the right side of a left join. See this link to understand why this changes teh join to an inner join. $URL$ See if this code is what you want: 

I think you need the OUTPUT clause to return any new recordids. Further depending on what you want to do, you may be interested in using a MERGE statment instead of just an insert. For instnce someone whose email has remained teh same but whose address has changed coudl be updated rather than having a new record added as seems to be the case in what you are doing. 

Data belongs in the child table. We keep it correct with a trigger on the table to ensure one and only record is marked as favorite (or in our case as the preferred address). However, @ypercube's idea of a separate table is a good one as well. 

To some extent it depends on how you intend to use that data. If this is an audit table used strictly to be able to occasionally research who changed what and when and occasionally restore bad changes, then option 2 is fine. If you intend to display user history to the application or through reporting, use option 3. I can think of no circumstance where I would use option one as it beciomes a places where blocking happens. 

I'm going to add one more scenario to @ooutwire's answer. You may also want to create an audit trail if you need to be able to selectively fix bad data changes. For instance, suppose I have imports of the sales roster for the client. One time they accidentally remove a name and our process assumes she doesn't work there anymore and removes her access. But she does still work there and in order to immediately correct the problem (I can't just run the previous roster as that would remove the new employees), I can find the data that was changed through the audit log and restore her. Or if someone maliciously changes data or runs a bad table update query (I saw someone update the entire table once because he forgot a where condition, the audit tables saved his job), it is easy to get it back. I have never worked on an Enterprise system that didn't use and need auditing. This data is critical to the success of the business and the ability to fix it quickly when things go wrong is priceless. 

You say inserts/updates/deletes are taking alot of time? Do you have any triggers on the tables in question? 10 million records is a tiny, tiny table and SQL server should be able to handle it easily. I don't think archiving is going to help with inserts/updates/deletes. You may also be having trouble with updates and deletes if you are using cascading updates or deletes. You could be changing/delete thousands of records for every change or delete in the parent table. Additionally if you have unnecessary indexes that will cause extra work. Addtionally are you doing row-by-row work when you need to be doing set-based work? This could include using cursors or while loops, using correlated sub-queries and using scalar functions. 

Consider also that there is no reason why you can't use a relational database for some things and the nosql database for other things. 

It of course would depend on what type of temps you are recording and what temperature type you are using and what level of detail you need, but Decimal (4, 1) would be what you would want for most outside temperatures if you are using a Farenheit scale. Celsius outside temps are probaly OK with Decimal (3, 1). If you are doing scientific work, you might want more places possibly both in front of and behind the decimal. 

So by separating out the different chunks of information you want, you can check each part individually (using the commented out selects, by uncommenting each one individually and only running as far as that select) and if you needed to make a change to the expense calculation (in this example), it is easier to find than when they are all mixed together into one massive query. Of course the actual reporting queries I use this for are generally much more complicated than the example. 

Personally I would use SSIS to do this task. First I would bulk insert into staging tables or use a data flow, then do whatever clean up and transform tasks you need and load the transformed data into a final set of staging tables. Then use the data flow task to send the data to the production tables. The reason why I would have two separate sets of staging tables, one with the raw data and one with the transformed data is that it makes it easier to research data problems that come up with running imports over time and if the transform is done before the final load, then the final load affecting prod will be faster. Note SSIS in a dataflow does process one record at a time, but it does it much faster than the average cursor especially if you are not doing an data transforms at that point. Further in SSIS, you can send bad data (things like invalid values, missing required data etc.) to an exception table so you can inform the data provider of the problem but still process the rest of the data. 

Since you provide no common field to join on, I'm going to assume you want a cross join, where every customer will be linked to every product. 

We require all database structure changes to be done with scripts (even on dev) and saved in subversion. Then on a set schedule we refresh dev from prod and they have to rerun their scripts to get back to where they were in the development cycle. This helps ensure that everything is done through scripts and that they have scripts ready when it's time for deployment. I know in 2008 you can set up DDL Triggers to track database structural changes, can you do this in 2005? This way at least you can find out when someone changes a setting who did it and find out why. 

It's a good idea to havae database in any event. We use ours to store exception records, configurations, logging information on packages that have been run, staging table, etc. 

These queries differ significantly and it is no wonder they have a time difference. The first has a join (please stop using the SQL antipattern of an implicit join) which is certainly going to be more time consuming than just querying one table. And then you havea calcluastion that you do not have inteh second query. Further if the indexing is bad on the join fields (FKS do not automatically get indexed) then the query will be slower still. 

It also depends to a very large extent on the purpose of the database. A database that collects data from sensors for instance that loads thousands of records per second likely has more inserts than anything else. An online store database likely has more selects becasue people look at far more stuff than they buy. A mature Enterprise business system may have more updates than anything, particularly if most of the new records are inserted through some type of bulk insert process. It would be a rare database that would have more deletes than anything else. Generally that would happen only on rare occasions such as losing a major client and removing theire data. If you have more DDL than DML statments and the database isn't in the process of setting up, then you probably are doing something drastically wrong becasue structure should not change as frequently as data. 

First thing I would look at is updating statistics (not the same thing as indexing, look in BOL for more information, look up UPDATE STATISTICS) and possibly rebuilding indexes. Statistics should be updated regularly in amaintenance job and if you had a dba you would be doing that. Likely you are now getting a less useful execution plan for many things. Next thing I would look at is hiring a dba, there is no excuse for not having a dba when you havea system with millions of records. 

We do something similar with client id. Yes it can improve performance if you don't need to go through all those intervening tables in every query. However, and it's an important however, this is best done only if you are using a surrogate key that never changes. Otherwise a change of the project id could require a cascade of updates that affect every table and lockup the system. I suspect project_id is as unlikely to change as client Id (client name, now that's another story) and so you might be fine. But please do consider if you will have updates to the field you denormalize, Also it is critical to set up a way to make sure the tables with the denormalized fields cannot get out of synch with the main table (PK/FK relationships are good for this and you might need cascading updates set (although I personally prefer not to use them if I can help it).) 

Of course creating views that call other views is a performance killer too. Do not go down that route. Write the queries you need and don't use either TVFs or views if you want performance. It is the layering that is creating the problem, this is almost always a bad thing to do when querying a database and you can quickly end up hitting the limit of the number of tables you can reference too, especially since you often end up referencing the same tables in different layers. Further, while this seems as if it woudl be easier to maintain, it is not. Try debugging or adding a column due to a new requirement when the layer you need to fix is at the bottom. 

One place besides recursion where I find CTEs incredibly useful is when creating complex reporting queries. I use a series of CTEs to get chunks of the data I need and then combine in the final select. I find they are easier to maintain than doing the same thing with a lot of derived tables or 20 joins and I find that I can be more assured that it returns the correct data with no effect of multiple records due to the one-many relationships in all the different joins. Let me give a quick example: 

We have a good sized Enterprise system that has many of SSIS packages taking data in and out of the system daily. Some of the strategies we have used are: Only process deltas especially in large files. To figure which records are deltas we use change tracking and send that data to tables in a separate database, so the process of figuring out the deltas when we receive a full file is pushed off onto a staging server. Only records that are new or changed go to the real production server. On our busiest server, we have moved all teh processing except the final load off to a completely separate server. If the file goes directly to table that does not also have transactional data changes or multiple data sources, we might have two tables (and A and B version) and a view that selects from the active table. So we do our processing on the inactive version (possibly including dropping and recreating indexes), switch the table in the view (meaning there is about 1 second of down time fromteh user perspective), then update the new inactive table. SSIS packages can be designed to run very quickly or not so quickly. So we may spend a lot of time performance tuning one that takes too long and affects production. Run the packages during the time period when there is the least usage on the server.