You can set up each group to contain states that work with states in such a way that they determine both what needs to be as well as what needs to be . It does however seem like a strange approach. Typically if you want to replace a node, you remove the old virtual machine altogether, and instantiate a new virtual machine for the new role. That way you make sure things are clean and don't risk arbitrary changes to have been made that might interfere with the new role. 

The current device attachment limits are described at $URL$ and for the currently most typical HVM instance types they include: 

On your local DNS server, the one that your clients are directly asking, create a new DNS zone named and put whatever you need in it. Just make sure this DNS view is not exposed to the Internet. And that none of your internal users will ever actually want to see the real yourexample.com. :) If your clients are not using your local DNS servers, then either change them to use one, or do something fugly such as intercept all DNS traffic on your border gateway and transparently redirect it to your local DNS server. (Ugh.) 

However, it does not work. I believe this is due to the and it's parent folder using the following permissions: 

How would one go about objectively benchmarking different x86 server for their potential as x86 routers? One idea I had is to setup two subnets, 10.0.0.0/24 and 10.0.1.0/24, on opposite sides of a router. Then setup an Iperf client and server on the two subnets. However, this would only test raw throughput, and not how services like NAT and firewall rules act under load. What is an objective way to test the throughput a router could handle in reality, eg. test NAT, firewall, and thoughput all at the same time. EDIT: Is there a way to test a routers throughput directly without dealing with the bandwidth limits of the CAT6 cable in between. 

By default, nginx will honor the received from the backend for its own proxy cache, see e.g. $URL$ So it seems that the best solution would be to change your backend to stop emitting these headers in case of errors. If you really want to override those 500s in nginx, maybe use proxy_cache_valid with 500 and 0 as the parameters? It definitely means second-guessing the backend, so it could have unintended consequences either way. 

There is an extended test mode, invoked with the command line option , which does this. For example: 

You can set up a state which checks whether the home directory of the user is in the right place, and if not, executes some commands that move it there. Something like: 

We just moved into a new office. The previous tenants went out of business, and left behind anything they couldn't sell off in a hurry. One of the objects left behind was this thing: 

The reason that doesn't work is that the bit is used as the ID for the state, and no two IDs can ever be the same in a SLS file since IDs are global. However, there's an alternative way to write states: 

I have noticed that in most web browsers if they can't make a connection using domain.com, then they will automatically (and near instantaneously) jump to www.domain.com. Make sure your server is actually answering the request for domain.com. Do the following to test, and please leave a comment with results (this assumes the use of a Linux workstation, maybe someone else can post a Windows/Mac version): Make sure the domain resolves when used by itself: 

I'm used to asking strictly technical questions, so I hope this "Dear Abby" style is appropriate. Potentially-Excessive Background I'm a web designer and developer, good at a lot of things and excellent at very few. I've used various VPS products as development servers, I've tried all kinds of shared hosting, and my high-traffic experience ends with CloudFlare and bumping up resources—no distributed computing, failover setups, load balancing, or big-kid networking. I've set up a mail server and web servers from base distros and know that I'd put everyone at risk offering to do those things for clients. (So I don't.) Let's say I have a client, a medium-sized company well-established on one of the thriving US coasts, and they run an off-the-shelf PHP/MySQL CMS along with a custom-built PHP/MySQL app. I can keep that software updated, healthy, and neatly deployed—no problem. Now let's say this client also has an internal IT department that doesn't seem up to the task of maintaining the site and server: inconsistent version control, questionable production "fixes", and with a reluctance to share access, ask for help, or take thoughtful advice. An otherwise stable VPS has been crippled, and the client knows but isn't sure what to do. I want to help responsibly and not just blame the current situation on iffy internal decisions, but I don't know what to recommend. The Question This is a client that benefits from the resources and availability of a good VPS product, with an IT department that may not be well-suited for managing such a server. Does this mean that managed hosting is ultimately an ideal fit? Management software updates (Plesk, Cpanel, etc.), security patches, and server maintenance should all be handled by somebody other than the client's IT, and ideally there'd be guaranteed, scalable resources with solid uptime and an SLA to match. I see that Media Temple is offering managed hosting now, but I'm interested in reputable suggestions if this is what I'm looking for in the first place. 

One of my clients is a small school district in Texas. Like any school, they often have problems with network'd peripherals such as printers, et al. It would be nice to be able to simply "listen" to what the printer and PC are saying to each other (or not saying more importantly)... The problem is that I can't find old-style "hubs" anymore, and even if I could, it's not a long-term solution. All of the devices that I have found to replicate the purpose of a simple hub are either $100+ or are difficult to throw into a networking tool kit (aka my backpack)... Now that hubs are dead, what's the new low-cost standard for simple packet capture in the networking world? 

You're probably entering these URLs in this shorthand form in your web browser, and it's not guessing what you want correctly; it probably remembered from history that when you type "api.example.com" that it should use HTTPS, but for new URLs "api.example.com/whatever" it doesn't have history so it tries HTTP first. If you're using the same IP address for all of this, regardless of the fact that $URL$ is going to the process listening on the HTTPS port (443), $URL$ will still go to the process listening on the HTTP port (80). If you want to avoid this while keeping the same IP address, set up an api.example.com virtual host in the HTTP-only server that redirects every request it receives to HTTPS. To actually split them up the hard way, add a new IP address for api.example.com to the server and make the two HTTP(S) servers listen each on their own IP, and update DNS so api.example.com points to the new IP address. 

Does anyone know where I can find the specs for what each wire inside CAT 6 does? Specifically, which are for sending, and which are for receiving? Both 100mbit and 1Gbit specs would be useful, thank you in advance... 

It doesn't run, no errors are present in /var/log/salt-apply.log, and cron does appear to be running the hourly run-parts: 

I'm new to VLANs, so take that into consideration... Lets say that I had a server set up to create a virtual interface that is set to be tagged as VLAN 3. For example, the following Debian config: 

We have a cron script meant to run Salt in our environment along with several other steps, but for some reason it isn't executing it at all. 

Here's a fairly simple Fail2ban configuration for PostgreSQL based on the HOWTO linked above but fine-tuned to actually work with Ubuntu packages, catch another error condition and skip over various debug messages to make it quicker: : 

The curl client isn't caching files, but the remote server network might well be. Try adding an arbitrary query string variable to the URL to see if you can reproduce it. 

When packets come in via interface X, and are responded to immediately by the same machine, then your source-based routing rule can catch the response because its source IP address will necessarily match the rule. When these packets are subsequently forwarded via interface Y to another IP, that first part of the communication will work. But as soon as the first response packet arrives from this other IP, it will enter the routing rule parser with its own source IP address, not this router's. It will then miss the source-based routing rules, and instead match the default lookup, which will be destination-based as usual. What you will need to do is to mark packets as they arrive on interface X, with this mark persisting across forwarding, and then match that mark in routing rules so a different outgoing routing table is used. 

Fixed... Turns out that I downloaded the wrong file from GoDaddy, when I downloaded the "SSL bundle," I originally downloaded the one meant for "other." When I downloaded the one meant for "Apache" instead, everything now works... Thanks for you help though Falcon Momot... 

Does anyone know of a way to do the following on Linux. I can't seem to find a way to run a script after the user is authenticated, but before the session starts... 

However, each server has a different /etc/sudoers file, and sourcing them all from one location would be impractical. Is there a way in Salt to ensure a single line (or group of lines) exists in a file, rather than managing that whole file? 

Requests to several websites on Media Temple's Grid Server (shared hosting; more than one account) are timing out for me today. Everything was fine earlier this morning, and for no apparent reason these URLs are timing out in the browser, via FTP client, and SSH. Clues/notes: 

Have I missed something obvious? I'm not sure what to do when the traceroute looks normal but I'm hopelessly unable to access these servers that are clearly up and running. 

A client of mine is a workshop-based school that gets ~80K page views per month, caters to creative professionals, and offers course registration (and payment) online. The payment portions of the site are hosted by a third party and are already secured with SSL. My question: are there any disadvantages to serving the entire site via an https connection, redirecting any http requests? I'm unsure of the SEO impact, but bandwidth and speed aren't issues since we use a CDN service. Basic membership details are collected over http (name, email, employer), and after a few years we finally got a complaint about the fact that this isn't more secure. Before buttoning up the entire site, I want to understand the implications of that choice.