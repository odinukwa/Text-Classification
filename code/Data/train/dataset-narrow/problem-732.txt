The best two books here are both from Itzik Ben Gan - Inside SQL Server T-SQL Querying and Inside SQL Server T-SQL Programming. Read the Querying one first. Covers all the important information on how queries are processed, thinking in terms of sets, etc. Covers all aspects of querying. The second book goes into programming constructs when working with T-SQL. Amazing books. 

So How Do you Start? Entry level jobs. Perhaps getting a job as a developer or general IT admin with some DB skills on the job. Going to community events and user groups and learning and applying yourself. Trying some volunteer or helping gigs with non profits or start ups. Basically get near databases. Work out a career path where you can do something with data. I started out working as a support rep. Dealing with all sorts of questions, but SQL Server questions were one of the types. I studied hard, became a go to person for tough SQL problems and moved up from there. Then I worked as a Jr. DBA someplace and the rest is history. Patience I was just discussing this question with a friend on twitter and they mentioned "you don't get there overnight" - That is great advice. I see a lot of people starting out who want to be the senior DBA right now. So you need a little patience and humility thrown in there. The first years of a DBA career are learning, figuring out which way you want to go and a lot of support time doing tasks that are basic. While you learn in these tasks and disciplines and prove yourself, more and more tasks will be added and more responsibility granted. If you stick with it, work hard, keep things online, build the right character and grow in your skills continually, you will get to senior DBA and do more fun stuff (well combined with more meetings and time spent with project managers) - but it is a marathon and not a sprint, in the DBA world. These answers are opinions, and that is why this whole thread not survive, but if it helps you out - copy and paste it and give some of the advice a whirl. Best of luck! 

Try using Ola Halengren's SQL Server Maintenance Solution. It has a lot of options, most important thing - it skips indexes that doesn't need maintenance (depending how You configure it). There are few thing about index maintenance to remember. From my experience 3-4 days for full rebuilds on bigger databases isn't that much, depending on Your storage. In some cases isn't needed, is often enough for fragmentation between 5% and 30% (as stated in Microsoft recommendations in Books Online, and any kind of index maintenance is recommended only for indexes bigger than 1000 pages, while difference in performance might be noticable on indexes bigger than 50000 pages (as stated here). In most cases index defragmentation isn't source of performance problems either, it just masks other issues hidden beneath. Defragmenting them too often won't improve Your performance and will stretch Your maintenance windows. Another thing is, that maintenance creates noticeable load on IO and CPU, rebuild/reorganize operations are written in transaction log (for databases in mode). So Your log files will get bigger, same with log backups. More read about index maintenance on Brent Ozar's blog posts: $URL$ $URL$ $URL$ 

Yes, OS (Windows Server) and database (SQL Server) are separate entities. So the only question is compatibility between those two products. I'm guessing it was tested and should be ok, but at this moment SQL Server 2012 doesn't have Windows Server 2016 listed (and won't have, at least not till September): $URL$ Best thing You can do is download latest Technical Preview and test it with SQL Server 2012 in Your environment: $URL$ 

Disclaimer - I'm not a huge sys admin buff, so my information may not be correct.. I also realize that the error says "host" - but I don't think that means it has to be a host entry in pg_hba.conf When you connect to localhost, it uses the loopback address 127.0.0.1 (unless that configuration on your computer has been tampered with) to connect to your system, which counts as a "HOST" connection.. When you connect to "myComputer" (which happens to be your computer name), I'm pretty sure your OS is smart enough to say "oh hey! that's me!" and doesn't even bother resolving an ip address .. it just does a direct connection. This would be a "LOCAL" connection for postgres.. Try adding to your pg_hba.conf file.. Then either restart the server, or connect as a superuser and issue (preferred method). 

It will return relatively quickly because, while it still does a sequential scan, it can STOP after it gets past 1000 rows. If I then adjust it to: 

But that's only one possible solution. Naturally, there would be several steps required to achieve this - this is simply what the end version of the tables would look like.. The advantage is that the insert/select from WEB_GK_STATE would be pretty much unaltered (assuming you have a trigger to give the WEB_GK_STATE_ID a valid value). Your stored procedure would have to be altered (minimally), and any SQL in the code that looked at the two tables would need to be reviewed and have it's JOIN condition (if there is one) altered.. I don't think that would be much work, though. EDIT Given further information, instead of changing your existing two tables, I would add a third.. 

No, shrink won't break Your log shipping configuration. But You must be aware that both shrink (and rebuild/reorganize You will have to do afterwards) will make Your transaction log files grow a lot. All of those operations cause a lot of I/O load that is logged to transaction logs. This, in practice, might mean that while Your log shipping won't break, it will make restore last a lot longer, depending on Your backup/copy/restore jobs frequency of course. That might lead to secondary falling behind a bit till shrink's (and defrag's) end. Remember that shrink has serious issues, read Paul Randal's post about it, if You haven't already: $URL$ $URL$ $URL$ 

It will affect overall server performance (so second database will be affected). Shrinking database affects I/O mostly, as it moves pages around data files. But whole operation does put load on CPUs too and it's fully logged, so with database in mode it will put a strain on Your transaction log file too - it will grow up a lot. Shrinking file has more downsides You should be aware of. It might make Your datafiles bigger (because it needs space to move pages around). It will cause fragmentation, and rebuilding indexes needs space in data files too. Only time shrink is relatively safe is using option. Try avoiding data file shrink, as there are better options. For example moving tables and indexes into new filegroup and dropping old one - this one also requires provisioning enough space for operation, but is more elegant than shrink and doesn't cause fragmentation. Of course there are situations when shrink is ok (or is the only possible option), but You have to be aware of it's downsides and remember to defragment database afterwards. When space is an issue, defragmenting with or will prevent database from growing up again after shrink Paul Randal wrote few important articles about shrinking: $URL$ $URL$ 

It sounds like streaming replication with a hot standby is the solution you're looking for. Streaming Replication details the steps of how you go about setting it up. The hot standby option that they go over (they have it put to "on" which is what you will want) will allow you to run SELECTS against the 2nd database. Under this setup, however, your production application would only write everything to the master database ... postgres itself would then stream those changes to the 2nd database. 

If you consider that hard coding, then (as far as I'm aware) your only other option would be dynamic SQL within a plpgsql block. 

As always, the documentation is an excellent reference. In here, it shows and condition is defined (further down the page) as 

One approach to improve readability would be to use a CTE to add a couple of calculated columns.. However, this approach could also be slow, depending on the size of the table.. But it would look something like this 

Also, there are about 1k distinct ID values in the hist table. With regard to data distribution.. Of the ~1k IDs, 50 have less than 100 entries in the table, 70 have between 100 and 1000 entries, 146 have between 1000 and 10000 entries, and the rest range from 10k to 60k entries. Over half of the entries have at least 30k records. 

It appears You had some MySQL/MariaDB/Percona instance previously installed, and some of data and config files were left in Your OS. The way that worked for me on that case ( with installed), with forgotten Maria's root password, was: 

You can find more on database storage in Oracle documentation: $URL$ $URL$ $URL$ Some basic information about and : $URL$ $URL$ $URL$ 

I assume You are asking about database, right? Neither of those two operations is really responsible for improving performance (and shrink doesn't improve anything about database performance/speed). 

It's in syntax, so You might want to check some tutorial or course, like one from CodeAcademy. Another great source helpful with learning about joins is Jeff Atwood's visual explanation of sql joins. Just like @a_horse_with_no_name and @Vladimir_Oselsky commented, You should switch from old ANSI-89 SQL syntax to current ANSI-92, which allows for easier to write (and read) queries with more flexible syntax. 

Mostly because performance issues are hidden somewhere else and defragmenting indexes works as quick band-aid (that doesn't last long, tho). It could be issue with statistics not up to date, or other issues. Just like Brent Ozar and Kendra Little wrote in their articles. 

Oracle doesn't work like (or engines like or ). There is no concept of storing tables as stand alone files in filesystem. Your data is stored physically in (big) , which contain many logical objects, like or . are also grouped in another logical objects, called . While this doesn't cover whole concept of Oracle's storage system, it should show You, in short, how it is designed. It's bit more complicated with database on , which is Oracle's volume manager and file system (and a variation/implementation of ), or, in older versions, mechanism called . If You have administrative rights in database You can check this query. It will show You data files and their location on filesystem: 

One major problem with using "*" in your queries is that if the table structure ever changes, it will affect every single query that is using that table, which will often times break code. Where as if you are specifying the column names, the chances of that happening are significantly reduced. The resulting error messages that the database will give you are also a lot easier to understand when it comes to "what broke?" 

Again - If you try to apply it in a , you'll get a casting error. It MAY be possible to do that and have some special type of "if error ignore" statement, but I'm not familiar with what that might be. If the above query is slow, you could improve the speed of it by creating a partial index for it to use. 

Essentially adding a 3rd parameter to your query .. one use case would use the 3rd param and set the 2nd param to FALSE, while another would set the 2nd param to TRUE.. 

pg_restore has a --clean flag (or possibly --create) which will auto delete data before running operations.. The Excellent Documentation should help you greatly... Just to clarify, in case it's confusing: 

Just like @KookieMonster noticed, You have Auto Shrink turned on. And one of disadvantages of shrink commands is fragmenting Your indexes once again: $URL$ 

CPU limits are described in various Microsoft documents, for example in Compute capacity limits by edition of SQL Server. On 3rd party resources, TF 8017 is documented on Steinar Anderson's Trace Flag list which, by the way, links to this question. 

To add more to what Raadee and Paul White (also confirm what eckes's comment already stated), TF 8017 is enabled by default in all SQL Server Express Edition versions since 2005. It's probably a way of throttling number of CPUs (sockets and/or cores) unsupported by SQL Server edition. Tested on: 

Another popular idea used in cases like this would be moving relation between and out of their own tables. Create third table, that will store just the relations between both existing tables and will allow for n:n relationship. Columns could look like this: with both of them being foreign keys to and . All You need would be adding unique constraint on both columns. Be aware that this (normalised) approach creates some overhead, as it will require additional in queries, so it might not be solution for Your data volume. 

Look at the wait statistics on the server. @swasheck shared a great link as a comment in a below answer. This takes you to Paul Randal's post on looking at and analyzing wait statistics in SQL Server. Go there. What kind of waits are you seeing? Do you see waits related to IO performance (, , , etc. ?). If you do this is another indication that you have some IO related performance issues, just like the IO stalls. But it gives you another form of agreement here. Look at the IO performance. In particular, look inside of perfmon at the and counters. These measure your latency. Watch these counters over a period of time saved to a performance log file. What did you see for averages? If you are seeing numbers over 0.020 seconds (20ms) this could be an issue. If you see numbers over 40-50ms avg or higher is a more firm indication of a problem. Also look at your spikes? How high do they go and how long do they last? If you see spikes into the hundreds of ms and they last for tens or scores of seconds or more and/or happen frequently you are more likely to have an issue with your IO performance for your workload. Look at your IO setup. What is it? Local disks? SAN? Storage Array? What kind of throughout and IOPs should you see out of this? Is it sufficient for what you are trying to do? You may have undersized your IO for your workload. Don't just look at your physical spindles, RAID settings, etc. Look at your paths to your disks. Are you pushing everything through a single 1GB link that you are sharing with a lot of other traffic? Can you look at disk performance metrics from the storage's perspective. 

Based off what you have pasted ... It appears that, because you are using a LIMIT on each .. the second query will run faster because it only has to apply one filter (project_id = '') while the second has to apply two filters (project_id = '' and kind = ''). As a result of using limit, it takes less TIME to just spit out the first 100 results where the kind doesn't matter .... it takes more TIME to spit out 100 results where the kind DOES matter.. If you remove the LIMIT, I'm sure you'll see that, ignoring time for the query to run, the query with two filters will most likely return less rows (which, I'm guessing, will cause it to take less time overall ... but that really depends on what indexes exist. Based off of your pasted explains, though, it looks like you have appropriate indexes). If you remove the LIMIT, I -believe- you'll still see two index scans for the two-filter query .. but one index scan and one sequential scan on the query with only one filter. 

You could potentially add some WHERE clause in the CTE to reduce the result set initially, but that might add unnecessary complexity.