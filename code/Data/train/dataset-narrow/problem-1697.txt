I've got a hosted exchange solution through Apptix, which isn't the problem, I think, but it may be relevant. I have my main account, msimmons@mydomain.com, and to that, I have an alias, matt.simmons@mydomain.com. Whenever I send an email to matt.simmons@mydomain.com, I examine the headers, and I see the "To:" field being correct, "To: matt.simmons@mydomain.com". All is well. I recently set up another user, services@mydomain.com to function as a multipurpose mailbox. I aliased "nagios@mydomain.com" to the services account in the same method that I did "matt.simmons@mydomain.com", however nothing I have sent to "nagios@mydomain.com" actually goes TO "nagios@mydomain.com". All of the headers say "To: services@mydomain.com". This makes it extremely difficult to filter based on headers alone. Does anyone have any feedback on what settings I would need to look at in order to fix that? 

I want to start scheduling remote mysqldump crons, and I'd prefer to use a special account for that purposes. I want to grant that user the minimum permissions for getting a full dump, but I'm not sure the best way to go about that. Is it as simple as 

If www-data is an account (probably the one that apache runs as?), then you need to either add meder into the appropriate group in /etc/group or you can create a new group with both meder and www-data (by using groupadd). Once you do that, when you login, you can run "newgrp " to make that your "current" group, and then all the files you create will have a group ownership of the shared group. 

Right now, we've got a business critical application that installs an Outlook add-in. It is a bit slow to load, which makes Outlook 2013 kill it after a few runs, but I've mitigated the issue of Outlook 2013 killing the add-in by using the Office 2013 Group Policy Administrative Templates "List of managed add-ins" policy. I don't see a similar policy in the 2007/2010 templates, and have to remove the resiliency key from the user's profile whenever the key gets added for whatever reason. Is there a way that I can replicate the functionality of the 2013 policy for 2007 and 2010, or even disable resiliency entirely if granularity is not an option? If so, how? I've found several ways to selectively kill the plugin-specific resiliency subkey automatically, but this still requires the user to restart outlook if the plugin doesn't load because it was disabled automatically. 

Compile the above and run from a command line to get a list of valid cipher strings for your system. 

At least sometimes, when our Exchange server gets rebooted for whatever reason, the ValidPorts data at HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Rpc\RpcProxy resets to using only the bare machine name and the FQDN for the local domain, but loses the additional outside domain information added so that outlook-anywhere will work by accessing the external domain name. I'm not sure what's causing this issue, and my google-fu is failing to find anything useful so far. I could, as a workaround, use a script to make sure that the data for the key/value is correct whenever the server starts up, but that doesn't get to the root of the issue and could have unintended consequences. The computer is running Windows Server 2008 R2 and Exchange 2007 SP3 EDIT: I just edited this key a few hours ago and came back to the server to see that it had changed WITHOUT a reboot or any software/patches being installed. I'm looking through scheduled tasks and RSOP now for clues. EDIT: I don't see anything in RSOP or Tasks that points to why this key keeps reverting. Logoff/logon doesn't seem to be the cause either. EDIT: Triple checking my settings: Outlook Anywhere was initially configured using the Exchange Management Console. In the console, if I go to server configuration -> Client Access, go to the server properties -> Outlook Anywhere tab, the External host name field is populated correctly. 

I find it hard to believe that no one has mentioned Redhat Spacewalk. It's the free open source equivalent to the Redhat Network's Satellite system. It allows you to manage your entire infrastructure of CentOS, Fedora, or Scientific Linux installations. It is essentially meant for what you're wanting to do. Of course, you're using Ubuntu, as opposed to Redhat derived distros. Fortunately, the Ubuntu world has what you're looking for in Landscape. It comes free with a support contract, or it's $150/node. Expensive, but it's a trade off. If you don't go with Landscape (or migrate to RH for spacewalk), then Puppet/CFengine might be your best bet. 

You cannot make scripts SUID. Fortunately. You may be interested in the SUID-wrapper program here, though: $URL$ I should also add, please please please make sure that you really need to do this before you do it. SUID binaries can be a great big gaping hole in your system. 

curl has several retry options (along with being able to specify infinite retries). rsync doesn't have any built in retry schemes, you could wrap it in a script to do infinite retries until it completed successfully. 

I think it depends on what your uptime requirements are, and what level of "grey market" you're dealing with. If your uptime requirements are high, then you want to rely on infrastructure redundancy, so that the loss of a single machine doesn't mean the loss of services to your customers. Buy double, build in redundancy, and monitor your hosts and network so that you know when you need to replace something. If your uptime requirements aren't that high, but you just need working hardware, then evaluate the people you buy from. Don't buy stuff that isn't guaranteed non-DOA. If you can, buy from refurb shops with at least 90 day warranties, though a year would be great if you could afford it, and make sure that you can get spare parts for whatever you're buying from another source if the original closes up. 

You've got this set up to listen to , not . won't match , since there aren't enough s in the host name. 

The PowerEdge 2850 is a silly beast for OS installs. The easiest route is to use Dell's System Build and Update Utility. Boot to it, that'll help you set up your system so that you can install Windows on it. This one here is compatible with a 2850 and Windows 2008 R2. Someone on the Dell forums was able to get 2012 up and running on a 2800, which someone in the Spiceworks community was able to replicate with a 2850 and 2012 R2. These methods are not supported by Dell, so your mileage may vary. 

So, the essential effect of a scheduled outage is to cease polling during that interval, so that the actual outage (caused by planned maintenance) is not recorded. So, to mimic this, you could retroactively delete the entries logged by the poller during a scheduled maintenance that was not included in OpenNMS. This seems to require creating SQL queries to modify the events database. See event maintenance for more information on how to specifically accomplish this goal, as the specific answer will be dependent on specifically the items you wish to delete. 

Yes stsadm is fine, it will backup the database contents and site. You should also do a SQL backup of the backend db just in case. We have a scheduled task that runs stsadm daily and dumps the backup to a folder. From there our backup software slurps it up and puts it on tape. EDIT - it appears my advice above is only good for small (<15GB) sites according to Technet. For bigger sites they recommend not using stsadm. This is news to me too, so I'd better read the link! 

There is no open-source alternative that can do all that. Samba can do a useful subset. Why are you asking? 

Have you installed the VMware Tools into the guest? I seem to recall there being a shrink option in there. Hang on, I'll just check... ...hmmm apparently the Tools shrink option doesn't work for pre-allocated disk (or one with snapshots). Did you pre-allocate the disk? Or did you just let it grow to the current size? 

What speed is the link between the 2 sites? What is the rate of change? 100MB/hr? 1GB/day? That will determine the tool to use. Tools like robocopy (and FRS) copy the entire file even if a single byte changed. I suspect rsync does too. DFS since Windows 2003 R2 is the successor to FRS and does byte-level replication. That is, only the changed bytes are transmitted, not the entire file. This can lead to substantial transfer time savings. 

Caveats: this will require either using SCCM/SCUP, or some third party tool like LUP or WSUSPackagePublisher or learning the WSUS API and developing your own method of publishing updates. This also means you'll have to research proper installation commands and detection methods for the updates you wish to overrule in this manner. Added benefits: This would be having more control over the software in your environment, as you can manage how updates get installed that might have weird side-effects. Also, you can actually manage more than just Microsoft products using this method; I've used this to provide updates to just about every user application in one mid-sized business. There are also companies that provide updates for third party applications for use with WSUS. Adobe, for example, provides updates for at least Acrobat, Reader, and Flash Player through its catalogs. 

The IIS SMTP server does this already by design. What you're looking for is the drop folder. You'll need to make the SMTP server think that it is the mailroot for the secure domain, and after that, it'll start putting emails it receives for that domain in the drop folder. See How the SMTP Service works for more information. 

In IIS, I created a new Application Pool named I changed the Application Pool Defaults to set Load User Profile to I started the Application Pool, Testing I checked to see if the user profile was created at \Users\Testing: no I added a website to the Application Pool and restarted the website I checked again to see if the user profile was created: still no I navigated to the website in my browser, so the application pool would load I checked a third time to see if the user profile was created: yes I checked to see if the user registry was loaded in , (by checking to see whether the file located at was loaded, and it was, with SID ) I checked for the presence of the key, it's present, and has values and pointing to . 

I am using the latest VMware Converter Standalone to p2v a physical Windows 2000 Professional SP4 PC. The PC is a standard Pentium with IDE disk from circa 2001. The disk is 20GB partitioned logically into C: and D. It converts with no errors (I did both disks into one VMDK). When I power on the VM in VMware Workstation 6.5 (or Vmware Player 2.5) it gets to the Win 2000 boot graphic then I get a BSOD with the classic 0x7B Stop error: inaccessible_boot_device. Is there anything I can do to get the vm to boot? I am lost for ideas, normally p2v of a basic IDE pc works flawlessly. I'm willing to put a bounty on this as I am trying to sort this out for a client urgently. 

I always thought the fact that you could change NT Workstation 3.51 to NT Server with a registry change was pretty cool. And says everything about Microsoft's market segementation strategies. 

Where to begin? This is a disaster waiting to happen. A Sysadmins primary job function is to ensure data is backed up and recoverable. Everything else is secondary. No if's no but's. Here are a few things you can do: 

The registry is also littered with references to syswow64\msxml4.dll and syswow64\msxml4r.dll in the following locations (“\…” indicates several subkeys contain references): 

The trick here is to not have Windows Update do the install via the Automatic Updates mechanism. You can set it to automatically download, but for automatic installs, there's no way to stop the reboot timer from triggering unless there's a user logged into the system, such as with the No auto-restart with logged on users for scheduled automatic updates installations policy. Since this is for servers, I'm going to assume that this is not the default case, and that nobody being logged in doesn't mean the machine's resources aren't necessary at the moment. Set up a scheduled task that will trigger the install of the updates and report when the updates are finished, or some other action, so that you know the computer is due for a restart. I very quickly modified the script found here to suit your needs: 

(These were the only two versions I found on my network at the time of removal) Now, in WSUS, I’m still seeing these as being installed, and when I look at the machines themselves, I found the following files on the computers (there's a tab between columns): 

If you log into the server then you aren't accessing it through a share. You're just hitting the NTFS security directly, share perms have no effect. 

I hate using SSMS because it is slow and cumbersome. The older Enterprise Manager in SQL 2000 was quick to load and much snappier in reponse to actions. That's progress for you. 

I had an existing 2-way DFS-R replication topology set up, consisting of two servers, one was Windows 2003 R2 and the other Windows 2008 R2. This was working fine. Last week I upgraded the Win 2003 server to Windows 2008 R2. It was a VM, so the upgrade process just involved creating a new Win 2008 R2 OS C: drive and attaching the data disks (vmdk files) from the old VM. I then renamed the old Win 2003 VM to server-old and renamed the new Win 2008 VM to the original old name, like so: 

You could still run a local login script on each pc, that connects to a "master" pc with the software on it and installs it from there. Checkout gpedit.msc and go to User Configuration -> Windows Settings -> Scripts (Logon/Logoff). Edit the Logon item and point to your script (batch file would be ok). The user would need admin rights locally and rights on the master pc. 

There's a subtle difference in behavior that the 2012 version "optionally removes" the feature, and that must be included as a parameter to remove the management console as part of the uninstallation of the feature, and must be included to remove the feature files from the computer that would allow the feature to be reinstalled. All other differences are clear by their presence/omission in the documentation. Remove-WindowsFeature (2008 R2 documentation): 

Like the title says, I've got a Dell M6600, running Windows 7, that connects to the network just fine, I can use it to browse the web, including $URL$ I can connect to it via RDP, it was joined to the domain (more below), no noticeable slowness in the network connection. The issue is that the active networks are reporting Access Type: No internet access, and I can't connect to some things, like I can't activate Office 2013 because "We are unable to connect right now. Please check your network and try again later". No software firewall is enabled. Here's what I've tried: 

As an aside: you can further reduce the number of alert emails from a given system by using one script for multiple triggers by passing arguments/flags into the script to indicate the source of the issue. You'll have to keep track of the last alert time on a per issue basis (one way to do this are to keep the source/timestamp pairs in a dictionary/hash table and save the object to a file using Export-CliXML and load the object with Import-CliXML), then you can have the body of the email be a summary of all current issues reported in this manner.