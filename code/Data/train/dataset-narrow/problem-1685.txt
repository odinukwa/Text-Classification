The closest thing I'm aware of to what you are asking is the Securich project. It creates a set of stored procedures and helpers that you can use to create pseudo-roles and manage a lot of privileges in MySQL. Note that I haven't really used it myself; I've just watched the blog posts with interest. You can read more at $URL$ It was also presented last year at the MySQL conference: $URL$ although I can't find the slide deck, and Sheeri K. Cabral has mentioned it in her talk at IOUG Collaborate: $URL$ 

You might take a look at XtraBackup Manager, by Lachlan Mulcahy. (It's not a Percona product; it's a community-oriented manager for Percona XtraBackup). If it doesn't meet your needs now, it might meet some or all of them in the future. 

At a 5GB data size, you almost surely cannot and should not use a dump (myslqdump, phpmyadmin, etc) as a backup. The reason isn't the backup. The reason is because the restore will take a long time, potentially many days depending on the table structure and your server hardware. You need some type of file backup. Whether you use LVM snapshot or Percona XtraBackup or rsync or something else will depend on the storage engine you use, the hardware, filesystem, and a number of other factors. This is much too complex of a question to answer without a lot more detail, but I can simplify as follows: 

I created the pt-stalk tool in Percona Toolkit to help diagnose problems that happen when you are not looking. It will not only help you catch the problematic query, it will gather enough information for you to determine whether the query is the problem -- and if not, what else might be. 

I assume this is straightforward, but I'm lost in the abstractions in Chef. I want to create a new Unix group on Red Hat Linux, called deployers, and then add some of my users to that group. What are the steps to achieve this? 

The answer in this case seems to have been that the support system sends mails with duplicate IDs, one for the message, and one for the change of ticket status, and Zimbra's default action is not to deliver duplicate mails. The answer was therefore to run 

Are the wildcard certificates from the same issuer? On the evidence it would look like you don't have a valid CA for Comodo. In your apache configuration you would usually have settings for SSLCertificateFile and SSLCertificateChainFile so check that. 

but it's worth reading up on that or testing your sites with Google Sitespeed or similar and following the recommendations there. You can also enabled compression if it isn't - the settings are in but may need enabling for file types. Nginx doesn't actually cache: static files are served from memory and the mechanism is quite flexible but you may have to look at your application delivery. There is a plugin for nginx in front of Wordpress that expires the cache and this can be useful. Buffering is about the response from the backend and while the default settings are good and flexible you may have to tweak a little if you are seeing timeouts at the front end, especially if your backend is outside of your local network, but don't do anything unless you have to. If you are using pretty URLs with Wordpress using on your backend, you can stay with that. That's about as much as SEO means with regard to nginx in your setup. 

Delivery seems a lot more successful now and it did make sense as an issue in this case. The function isn't documented officially as far as I can see. 

It's a wholly personal/commercial decision. If you run your nameservers on the ISPConfig and cPanel servers (which I personally don't like but it's the default setup) then you will presumably be serving domains from each system separately, but if you are offering a unified product then having nameservers in the same domain is a useful, if cosmetic, detail. 

We have a new office that was punched down with Ethernet in every office and every cubicle. The electrician didn't mark the ports on the patch panel or the wall ports, so I don't know where say port 1 on the patch panel goes in the office. I've heard of people using a tone generator to figure out where the cables are but I'm not sure what that entails. I was also thinking of looking up ARP entries on our switches to find out what port is getting what MAC address and then cross referencing it with a list of MAC addresses on all of the PCs in the office. This sounds unreliable though. Is there a quick and easy way to label these ports? 

Is there a way to estimate how much space I would save by compressing a folder with NTFS compression without actually compressing the folder? This is on a Windows 2008 R2 server in case that makes a difference. 

Add the 'Back Up Database Task' to the maintenance plan. Edit the task. Select backup type: 'Transaction Log'. Select any database running with Full Recovery Model. Close the task. Re-open the task. Notice that the database I selected is not checked anymore. 

Are you connecting the X25-M SSD to the backplane? There's a known issue with Nexenta and accessing the L2ARC over a backplane. Your best bet is to connect the SSD directly into a SATA port on the motherboard. Make sure it's configured to use AHCI as well. If you're running anything mission critical on this server I would switch to a SLC SSD (like the X25-E or a STEC SSD). That being said, you'll probably be ok with the X25-M if it's not. 

First things first: SPF only specifies what servers will send outgoing emails from your domain. I came up with this record: v=spf1 a mx ip4:1.1.1.1 include:_spf.google.com include:emailpublishers.com ~all Definition: a and mx: allow your domain's @ A record and MX servers to send email. This covers your domain's @ A record and all of your MX records (may be a little redundant in this case but doesn't hurt). ip4:1.1.1.1: the ip address should be auxiliary.com's outgoing email server ip addresses. you may have to add several of these but this allows auxiliary.com to send emails on your behalf include:_spf.google.com: this allows google apps to send email for main.com (this is why the mx tag above may be redundant since main.com's mx records are google apps) include:emailpublishers.com: you'll want to ask emailpublishers.com what SPF to use for this one but I imagine it's similar. If they have one, replace this one with theirs. ~all: softfail any emails from main.com that come from servers not listed in this record. This tells spam filters to use SPF as one of the criteria to flag an email as spam. using a -all is a hardfail, which means anything the SPF record doesn't catch is spam. This can lead to false positives though. (edit) Once you're done, test it out by sending emails to a Gmail account. Gmail logs in the headers whether the email passed a SPF check. It's incredibly useful for testing. (edit 2) The 'a' in the SPF record only allows your domain's @ A record to send mail, not all of your A records. E.g. example.org with the SPF record v=spf1 a ~all would allow example.org to send mail, but not beta.example.org or testing.example.org. Fixed it above. 

It's almost certainly going to be restrictions on the hotel network or upstream. If you haven't got a usable phone signal you may not be able to do much. If you could get to somewhere with a less restrictive policy you may be able to make changes but again it could be chance. Your best option in that case would probably be to open SSH on port 443 if you're not using HTTPS as that is unlikely to be blocked. 

For me, the main concern would be the security and usage policies of the survey service. In an ideal world they would be using a known mail relay service and would have no other access to your mail apart from authorisation to send it, which would be, in an ideal world, a case of adding their servers to your SPF record and signing mail with a DKIM key which can be then revoked when the campaign is finished. However, phishing or spoofing would then be an abuse of their trust, but as the mail is being returned to a mailbox in your domain you shouldn't lose your internal security procedures. From your point of view you need to be able to trust the survey provider, who should be able to provide some kind of guarantee. 

For that matter you could do that and handle the routing in code as you can pass the variable as part of the request. 

It's a fine solution and, assuming that the front end server isn't too far away from the backend servers, doesn't affect performance significantly. You can terminate your SSL certificates in nginx but you could have also SSL certificates on the backend to encrypt the connection between end and backend depending on where your systems are hosted. The general rule is to terminate SSL as far up the chain as you can. 

The mutex locking isn't needed for a single server as it's basically to prevent write conflicts, but if you are going to scale out and will potentially be writing any files into your shared filesystem from multiple servers consider using it. Use the apr default or for most purposes. 

The problems you're seeing could be caused by dozens of different root causes. You definitely should not guess; you certainly need to measure and diagnose carefully. If you gather enough information, the true cause will be evident, and the solution will be obvious (provided that you also understand enough about the server's internals to know what you're looking at). If you guess and try to do things such as reconfiguring the server, my experience shows that you can make the problem much worse, or cause other problems, and you'll never really know whether any specific change helped or not. I would suggest using the pt-stalk tool from Percona Toolkit to capture a set of diagnostic data from the server when one of the spikes of slowness happens, and ditto for when it's running more quickly. There should almost certainly be enough information there to understand what's happening. If you are not comfortable making a diagnosis from it, any competent MySQL support provider should have no problem if you tarball the samples from pt-stalk and send them over. I don't mean to be too repetitive or insistent, but again please don't use trial-and-error on this one. 

You can use the nc_cmd configuration option in the template, as stated in the comment just above your last comment to the bug report on the Google Code Cacti template project, where you linked to this question. In addition, you should know that you are using outdated templates. As stated on the Better Cacti Templates project, that project has been discontinued and is now part of the Percona Monitoring Plugins. See $URL$ and note that this software is fully covered by Percona support or consulting contracts, which is a great way to get help with installation. Or, in the unlikely event that there's a real incompatibility that won't let you get the templates to install as-is, you'll be able to get bug fixes to the software. 

There isn't a good read-made tool to do this. pt-table-sync doesn't work exactly as you've been told (I wrote it ;) but it's not the right solution. I have seen its bidirectional syncing functionality used to reconcile servers to a central source after being intentionally disconnected and updated, but this isn't the right solution for what you need. I don't normally give sales pitches on topics like this, but this is honestly a case where I'd engage Percona to develop a new tool for you. Some people have written little scripts that suit their personal scenarios, but a high-quality solution for general use doesn't exist yet, and isn't really that hard. The main thing is that you need formal testing, and Percona Toolkit's components are already 90% of the way to what you need -- it just needs a little bit of glue between them. You could do this yourself, of course, but why make a square wheel and end up maintaining it yourself and discovering all of its bugs when you least desire it? That said (end sales pitch, sorry) -- the solution I'd suggest should avoid blackhole tables and go with simpler, less troublesome techniques. (Yeah -- I wrote High Performance MySQL, too. I know. Back then I hadn't seen as many problems with blackhole tables as I have today.) It should do roughly what Rolando suggests, but there are some subtleties. For example, it shouldn't let the I/O thread stream a bunch of data from the master, getting far ahead of the SQL thread, then throw it all away when it round-robins to the next server. That would be really wasteful and cause a lot of impact on the master. There are a lot of little details like this that need to be taken care of -- another that comes to mind is not switching masters when a replication-caused temporary table is in use. 

Are you trying to input audio or output audio with this sound card? If it's input, you can connect almost any USB microphone and Windows Server 2008 will recognize it as a USB recording device (no additional drivers needed). Even the Xbox 360 headset will be recognized.If it's output, you can purchase a USB DAC and convert digital files to analog. Here are some high end USB microphone vendors: Rode Podcaster $URL$ Samson C01U $URL$ USB DAC Vendor: $URL$ If you really need an internal sound card, your best bet is going to be one of those Creative or ASUS sound cards w/ Vista drivers. Very few vendors, if any, are going to pay the expense for official WHQL driver certification for Windows Server when the number of potential users is a few thousand at most. 

This isn't a direct solution, but would it be possible to use a 3rd party email marketing company to send your emails? This way you don't have to worry about a lot of these issues. ConstantContact.com is one. Eloqua seems to be a popular one for large companies. 

I've seen this happen with ESXi 4.1 hosts after a patch accidentally wacked the /tmp/scratch folder. You might want to check if that directory still exists on the hosts that exited maintenance mode automatically. If they're missing, you'll want to mkdir to create it. Also, you'll want to check if persistent scratch is setup correctly on each host by following this VMware KB article: VMware KB: Creating a persistent scratch location for ESXi 4.x and 5.x 

We are thinking about upgrading to SBS 2011. We would also like a way to collaborate with our external clients using Sharepoint Foundation, which is built into SBS 2011. Is this possible? I know in full Sharepoint you can publish client facing sites where you can share documents back and forth, but I wasn't sure if Sharepoint Foundation could do this. 

We have a Windows 2008 server that we want to move offsite to the cloud (we're using Rackspace). It has to talk to our Active Directory domain constantly, so we need a stable site-to-site VPN between it and our datacenter. Rackspace doesn't offer a virtual VPN device, and a quick search shows that Cisco-Windows VPN tunnels don't work well, if it all. We do have a Windows 2008 server in our datacenter that the offsite server could connect to. What is the best way to get these two Windows 2008 servers to connect to each other? 

However, this will reject or or blacklist a message with a response, and you may not want the sender to know that it is being rejected. You may also only want the mail to be rejected for a particular user, in which case I would suggest using or a policy server. Procmail is powerful but can be hard to get into. Postfix's architecture means that an external server, even plain may do what you require. 

I would tend to recommend running as the system user, so in your case, and as a non-privileged user, which can be the same as the shell user. You don't have to have your non-privileged user in the group as passes requests to . for execution. The setup suggested by the Magento docs assumes and , which would require the permissions setup as described above. 

if the first command doesn't show a downgrade path. As always, test this before doing it on a production machine. 

SSL certificates verify the domain or hostname, not the IP address. However, most web servers and browsers now support Server Name Indication (SNI) and Subject Alternative Name (SAN) which means you don't need a second IP address. For that matter you don't need to buy certificates any more. LetsEncrypt Certbot provides Domain Verification certificates for multiple domains for free. 

You would need a HTTPS to handle HTTPS requests and redirect them to HTTP. However, this VirtualHost would still need valid certificates to work correctly as the redirect happens after the SSL negotiation, so it's better consider creating new certificates using LetsEncrypt as it doesn't only provide increased security but improves search ranking. 

It's unlikely that the load balancer is going to send out mail so the origin address is probably still going to be 1.1.1.1 - from the point of view of postfix mail will usually be coming from the primary IP of the web server unless it has been configured otherwise. 

You have built httpd correctly but the default directory for the version built from source is and if you have done as root it will be installed there. The usual approach is to symlink to for consistency. Looking at the initial install, moving and creating a symlink to should work with your init file, but you could also check that that matches your installation. If you need to do this again you could also consider building an RPM, the apache source comes with a build file that will do it, and it can be run on a VM running CentOS 5.11.