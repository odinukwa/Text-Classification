Why is the database on System drive (C:\ ) ? Dont you have any other storage / SAN attached to the server ? Where is the log location ? How is the database being used - no of concurrent users, load on the server, RAM on the server, no of cpu's on the server ? Why is a 1 GB database split up into 7 files ? Do you really need the database having multiple files ? Also, note that you can have 1 primary filegroup(.mdf) and then rest are secondary filegroups (.*n*df). When you say "performance issues" what is actually happening - what is slow or not working ? Did you ran a short trace during the period when performance is degraded ? 

Using Native tools : tablediff : the tablediff utility compares the data in the source table to the table in the destination table. powershell : compare-object allows you to achieve that. here is a good example third party: redgate schema and data compare. You can even use powershell and schema/data compare to automate things. 

You should check if the on secondary server for the disk that you are trying to restore is NOT 512 bytes. You can install the Hotfix for SQL Server 2008 R2 SP2 or correct the disk bytes per physical sector. 

There are many free tools - native and opensource available for comparing schema between 2 databases: 

This is my interpretation .. SQL Server 2016 allows you to configure POLYBASE - accesses and combines both non-relational and relational data - from SQL Server. It allows you to run queries on external data in Hadoop or Azure blob storage. SQL server has a column to identify that the table is external. You can find more on external tables using catalogue view. 

The rowcount in a table does not dictate the use of Index. Its your queries (pattern) that should dictate the need for creating indexes, so that the data retrieval is as fast as possible. The query optimizer relies on statistics to produce optimal query plan. From SQL Server Index Design Guide : 

Below statement will make sure that UserA is able to see schemaA and NOT schemaB. The good thing is that you can just add users to role and they will inherit all the permissions granted to that role. 

Since your database is 100GB, you can use backup and restore method with compression (make sure Instant file Initialization is enabled). Another approach will be to use logshipping with delaying the restoring of log files on the standby server and having the database in standby read mode ( WITH STANDBY). The way you do is - 

You are missing creating the base result set - which is the highest level in the hierarchy. This is identified by . So if you might have a CEO that does not report to anyone, he will have it set to NULL. 

The most important thing when doing a large update or delete is to avoid the transaction log growing out of control. To allow log reuse & avoid bloating to Tlog (Transaction log truncation) 

Note: You have asked a lot in just one question. Though, if you have performance issues, then I would recommend you to start with the Glenn's DMV queries or Brent's sp_AskBrent. If you want to find if your server and databases are configured properly and if they might have performance concerns or might be misconfigured (away from standard best practices), Brent's sp_BLITZ and SQL Power Doc will do a really nice job. 

This is expected behaviour. When you sql server agent, the congiguration option changes the and to . When you start sql agent, then the and for changes back to . It is important to run when you enable the configuration option for the first time using 

Microsoft does not support making any changes to the system tables (replication, logshipping) except you are told by CSS. This does not mean that you cannot create those indexes, it just means that if you open a case with Microsoft they wont support unless you remove the indexes that you have created by yourself. Check this connect as won't fix - Add index on [distribution].[dbo].[MSrepl_commands] So my recommendation would be, baseline your server instance. Get data for a longer duration and if you are facing problems then address them by creating appropriate indexes. Test, test and test all your changes first and always be on the supported side when you work on a vendor software. You can file a connect item and if Microsoft addresses it, then it will be included as hotfix or in future versions. 

Remember to remove parameter if you want to migrate all linked servers from one instance to another. 

Auditing has been more improved in SQL Server 2008 and up. There are many ways to trace a SELECT .... (make sure you are not doing it against an HR database :-)) Option 1: Through SQL trace, where in depending on what trace events have been selected, you can get logged/audited. Option 2: Under database --> Security --> Database Audit Specifications . Here and here are 2 articles that have explained Option 2 in detail. 

This is bigger than a comment but something to test first and then implement : You are logshipping 800+ databases. Thats a large amount of databases that you logship every 15 mins. You should offload some databases to another server. IMHO 800 databases on a single server is a lot! We had similar problem with logshipping when we logshipped 200+ databases from NY to LD region. What we did is as below : 

Above all are few areas to look into, but it will be difficult for you to corelate and find a pattern. Best is to scan your web-server IIS error logs using tools like LogParser. Additional References : 

Instead of writing your own solution, I would highly recommend to use Ola Hallengren's SQL Server Backup Solution. Refer to : Bad habits to kick : avoiding the schema prefix by Aaron Bertrand. Below should work for you (I have not tested it): 

Michael J. Swart has a very interesting post on Itâ€™s Hard To Destroy Data - esp the summary table with different options. 

No, restoring a transaction log is sequential. Transaction log relies on LSN (Log Sequence Number) Also, you cannot restore your database with just transaction log. It requires the main database file (MDF) and NDF (secondary datafiles if there are any). 

This is a myth. A backup will generate additional disk IO but wont cause any blocking or performance degradation. Its always advisable to perform full backups during less server activity. Additionally, depending on your SQL Server version & edition - if you are using SQL Server 2014 Enterprise edition then AlwaysON Availability Group is another area worth exploring. 

Note: I agree with @AaronBertrand that you should create a physical table - that will be a driver table for the update statement and just update the table when you want to modify or insert more values if you want other tables to be updated. 

is your friend since it exposes the Windows version of the SID. Refer to Aaron's solution : Map between SQL Server SIDs and Windows SIDs 

What you are telling is not entirely possible out of box (keeping in mind your 3 bullet points). If the above suggestion - does not work (provided you do proper testing), then you can use the . Do not use - see kb/308886, SQL Server Read-Consistency Problems by Itzik Ben-Gan, Putting NOLOCK everywhere - By Aaron Bertrand and SQL Server NOLOCK Hint & other poor ideas. hint will help in your scenario. The gist of hint is - if there is a row level lock then SQL server wont read it. 

A system with heavy usage of tempdb, you will see many dirty tempdb pages in the buffer pool. Remember that the Buffer pool is used for caching index and table pages as they are modified or read from the disk. Also, there is a concept called buffer pool disfavoring which prevent pages from a different database being flushed from the buffer pool when a large operation occurs (e.g. bulk activities). 

This means, you have to patch Publisher first and then Subscriber to be 100% sure that you wont be encountering any issues. 

Log the output of stored procedure to a physical table in the sql server database. e.g. Use or to export data to text file. (Optional.. but important to manage table growth) Have a purging process to purge out the old records or before next execution of the SP, truncate the table. 

I normally put the original values in temp table and then once done with the script/task, I revert it back. 

SSIS will commit the task with the sequence container. IMHO, best is to use TSQL Transactions instead of SSIS Transactions. Also refer to : Design Pattern: Avoiding Transactions in SSIS 

For Complete Documenting sql server, I highly recommend : SQL Server & Windows Documentation Using Windows PowerShell written by Kendal Van Dyke Brief description from the link : 

A differential backup is useful to speed up the RTO (Recovery Time Objective = Time it takes to recover your database) as opposed to more frequent full backups which will be a problem for large databases or restoring volume of transaction log backups as they could grow large over time frame. Note: A COPY_ONLY full backup does not reset the differential base, so a COPY_ONLY backup cannot serve as a differential base. References : 

Gentle reminder to read - Bad Habits : Using MDF/LDF Files as "Backups" If you are fully aware and have a situation that you cannot avoid, you can use dbatool's - Mount-DbaDatabase 

If you go to server properties, in the database settings, you will find the default database data, log and backup locations : 

For 64 bit OS, you dont need AWE and even enabling AWE using has no meaning. Refer to Great SQL Server Debates: Lock Pages in Memory by Jonathan Kehayias : 

The database backup includes all the data in the database when the FULL backup was taken including the database users. When you restore the database on Test server, make sure to SYNC Orphan users. 

Best is to use Ola's script (as David mentioned) as the Backup solution is customizable as per your need. 

Seperate spindles is the best recommendation. Read on When should you put data and logs on the same drive? 

Yes you can do that programatically using t-SQL and a sql agent job. Basically, you have to query distribution..MSSubscriptions and check the status column value to be 0 (Inactive). Status of the subscription: 0 = Inactive; 1 = Subscribed; 2 = Active Below T-SQL Will help you : 

SQLCMD is a viable option, but for such a huge script, I have never tested to confirm if it actually is capable of handling it or not. Also, I recommend to have as if something goes wrong, you will atleast have a log file to see what went wrong. Below is what I would recommend : 

However, if you configure an instance of SQL Server to use dynamic port allocation, and you restart the instance of SQL Server, the registry values are set as follows: 

No. Service accounts do not affect performance in any way. Its all about security ! From the SQL Server 2012 security best practice whitepaper (warning: word doc) : 

A differential or FULL backup does not break LSN chain. ONLY an adhoc log backup will break the LSN chain. 

Above will generate scripts for bulk insert using SSMS set in TEXT MODE as output. Copy the generated script and run it in another window using SSMS. 

Minimal condition for a version row to be garbage collected is when SQL Server determines, based on the transactional states, that this version is no longer needed. Also, referring to my SQL Server internals notes, for management of version store, SQL Server will perform a regular cleanup every minute with a background process to reclaim all the reusable space from the version store. You can monitor in perfmon counter to find out rate at which space is acquired and released from version store in KB/Sec. Check : Managing TempDB in SQL Server: TempDB Basics (Version Store: Simple Example) 

To be 100% confident, a restore of database is required. You need a seperate server to do the restores. Since you are using Ols's backup solution, the entire restore and verify process can be automated using dbatools - esp specifying parameter e.g. Below powershell script Scans all the backup files in stored in an Ola Hallengreen style folder structure, filters them and restores the database to the folder on 

This might imply that you are not authorized to do such operation. You can ask your DBA to help you out once you have put in a proper support ticket. Also, its quiet important to wipe out PII information when you do a restore on a NON PROD system - as @MichaelGreen pointed out. 

Lastly some good references that will help you : Deep Dive on Initialize from Backup for Transactional Replication Replicating Non-Clustered Indexes Improves Subscriber Query Performance Follow the Data in Transactional Replication - Whitepaper Troubleshooting Transactional Replication Scaling Out the Distribution Database 

Below script will iterate the SQL Server services and will add you to the sysadmin role wherever you're missing : 

AlwaysON will provide failover in terms of seconds, but it depends on your n/w. EDIT : If you implement AlwaysON (which fits in your situation), below is my recommendation: Assuming (from your post) DB1 is the main database and DB2 is for reporting. Configure DB1 as primary in AlwaysON. You get a warm standby database on the secondary server (say DB1_Replica). Configure DB1_Replica as readable and have asynchronous-Commit mode configured. 

For small tables above approache using CTE does not matter for performance. For big table, use WINDOWING functions. A very good article is written by Aaron Bertrand : Best approaches for running totals â€“ updated for SQL Server 2012 

Yes and No .. depends on what is your downtime (maintenance window), database size or amount of data you are replicating (whole or subset of articles). Script out current replication setup (both create and drop). First make sure that you don't have any pending commands to replicate and no user connections while you are doing a cut-over to new server. Below are some of the ways you can choose to ease your migration : 

You cannot restore a database that is involved in Mirroring without breaking/removing Mirroring. To remove mirroring 

This is totally dependent on opinions. There is no right or wrong answer to this question. if you are using SQL Server, then the naming convention that I would follow is databasename.schema_nmae.tableName. So for storing changes made to table, it will be . Make sure you are in the right database when you create the table. For sql server 2005 and up, dbo is the default schema.