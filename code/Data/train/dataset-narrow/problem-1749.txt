The easiest way would be to download an rpm packet with the newest version of RPM and then upgrade (. You may need to download several more packages if the new version will require some packets that you do not have installed. 

Run run then ask each of the name servers above you what is their opinion about name servers for your.domain.com. To give an example with att.com: 

What do you mean by reliability of a file system? If you want to minimize chances of data loss in case of a sudden system abort (power failure/kernel panic/whatever), you may try data=journal and sync options. This will kill performance, but reduce risks of data loss. I think you should better define what you mean by reliability of file system. What you expect from it? Against what failure modes do you want to protect yourself? It may very well be, that using UPSes, reliable redundant hardware and solid software would be better option than tinkering with file system mount options. 

Here is a very quick-and-dirty description of how DNS system works, and here is a bit longer explanation. Before you start tinkering with DNS try to grasp how the system works. Depending on how your systems are set up you could just add Server 1's IP address (possibly as an alias) to Server 2's network card and be done. If it won't work for whatever reason (e.g. routing), you have at least 3 possible roads: 

For btrfs you need option to enable TRIM support. A very simple but working test for functional TRIM is here: $URL$ 

I'm not sure what you try to accomplish and it is possible that your goal can be achieved in a simpler way than what you've envisioned. Anyhow, the simplest idea that I can think of is to add to your shutdown scripts one that does . Then add to your . Then modify your network startup scripts to check for presence of this lock file. If it does not exists, it means, that the network is being brought up between a shutdown and execution of , which is the last of scripts being run at system boot. 

First things first: a very well-done, systematic and thorough debugging, good job. On my RHEL 5.6 box I always get a return code of 1 if I try to kill a non-existing pid. I tried as both root and a non-privileged user, both with full path and a with just the command name. I also get only terse , with no elaborate error messages. It may be a good idea to run and see if somebody didn't replace with a new and improved version. Even if rpm verification says the file is pristine, I'd try renaming and copying over a binary from a working machine. If the file replacement helps and you don't uncover a legitimate the source of the change, then regardless of output of rpm verification I'd assume the machine was compromised. 

An insane, albeit possibly working, approach would be to do at boot time and force a reboot, when incorrect ID is detected. Note the insane part (and make sure to have some precautions against infinite boot loop). I think that womble's suggestion to replace the device with something that works every time is the right way. 

In order to prevent loss of data in case of power loss you have to use synchronous writes ( option in fstab). This is going to kill your performance and persuade you to think of better alternatives. ext4 is ready for production use. It is default in server oriented distributions for some time now. In case of a power outage use a UPS. If you do not have the budget for a UPS, then your data isn't worth protecting against power outage, right? Write caching is used in practically all contemporary filesystems: ext3, jfs2, ext4, btrfs, zfs, you name it. Database writers are very conscious about data handling and pay attention to syncing proper data at proper times. 

Linux auditd ($URL$ will give you most power in watching who did what. You have more about it in DerfK's answer. Nothing, however, is going to tell you, who logged in as webadmin, if there are n people who have access to the webadmin account. I would suggest to use named accounts for each user and then either use su - or sudo to run commands from the "function" account. 

Yes, reliable synchronisation to a single clock source is possible. It's not reliable, because you have no redundancy, but that's the only problem. In your place I would just remove the local time source. On my machines I usually just use external ntpd servers. You need the local clock only if you need to provide service to clients (think: ntpd server for your internal network) when you have no connection to internet and cannot sync to other servers. 

I think it may be OS-dependent. On AIX 6.1 you have ($URL$ "(cs is) Number of context switches per second observed in the interval. The physical CPU resource is subdivided into logical time slices of 10 milliseconds each. Assuming a thread is scheduled for execution, it will run until its time slice expires, until it is preempted, or until it voluntarily gives up control of the CPU. When another thread is given control of the CPU, the context or working environment of the previous thread must be saved and the context of the current thread must be loaded. The operating system has a very efficient context switching procedure, so each switch is inexpensive in terms of resources. Any significant increase in context switches, such as when cs is a lot higher than the disk I/O and network packet rate, should be cause for further investigation." If you have sources of the vmstat on your system you may look inside and try to find out what it does. 

A program named eats your memory. Run as root. This will give you path to the binary. This can give you some information (if it is in ) or it may not (if it's in ). If your distribution is rpm-based you can run to determine package to which the file belongs. Then will give you package description. It is also possible that somebody compiled some code, named the resulting binary analog and run it. Binary name doesn't tell much. 

into cron to be run every 5 minutes or so and after a crash have a look what was eating your CPUs just before server crashed. 

You need to give execute permissions to the user under which your web server is running to all directories in the path /home/Dropbox/www and /opt/lampp/htodcs/. You also need to give that user read permission in the last directory of both paths. 

I think the quickest solution may be to run: This should show you all the files that have in them string 'mail('. mail() is the function which PHP uses to send mail, so you may be able to identify vhost and script in one go. 

4 disk RAID 10 gives you performance of 2 disks for writes and 4 disks for reads (absolutely best-case scenario). A 7200 rpm HDDs should give 75-100 IOps. What kind of performance do you see? Do you read close to 100 in ? If the primary load is generated by a database, what makes you think it is going to be mainly sequential? Databases are the stereotypical random access case. You can use to see average request size. will additionally give you information on I/O merges done in the kernel. Does it agree with your expectation of mainly sequential reads? What fsync() kernel bug do you mean? What filesystem do you use? What mount options? option can buy you noticeable speed up on ext[34], because modification of access time can mean extra write for every read of a file (worst case, high-res timestamps). Answer section ;) Firmware update may help, but do not expect miracles. You may gain couple percent, not RAID 10 is the best level for performance (if you want to keep redundancy), so it shouldn't cause problems in and of itself. However, you may have partitions and / or LVs not aligned with stripe size. This could potentially double IOs needed for small random reads (worst case scenario), and will impose overhead on any type of I/O. Power Saving mode shouldn't cost you much. From what you tell us the disks are too busy to be spun down, and CPU is waiting for I/O anyhow. 

files. If file's contents is 0, then the device isn't removable, so it cannot be an external USB drive. Some SATA drives report themselves as removable, so it may not be definitive. 

Here j.gtld-servers.net says, that name servers for att.com are ns[123].attdns.com, and ns3.attdns.com says, there actually is a 4th one. If you ask all name servers for com. about NS records for att.com. ( then etc.) you'll have complete picture, what a properly configured client can receive when he asks for a name server for your domain. Then ask each of the actual name servers for your domain (ns[1234].attdns.com. in the example) what do they think about NS records for their domain (att.com in the example) and you have completely complete picture. It may be a bit tedious, but if all the answers you get are sane, then your configuration is demonstrably correct. You did your part to shorten the transition phase, and finally all will be well, but you cannot do anything about ISPs purposefully ignoring TTL for DNS entries to lower load on their servers. 

Too broad question. I won't speculate on minimal specs, but if you are going to serve large files to two dozen customers, then SATA+RAID5/6 seems like the way to go performance-wise. NAS vs custom built Linux box. Two factors here: 1) Availability of admins experienced with Linux whom you trust enough to believe if they say "Yes, we can do this". 2) HW support of the parts you order. Still, if you decide to build your own system, then you may be able to have 3 boxes for the price of 1 NAS system (according to your own calculations), so you could have 2 in production and one idle, ready to be cannibalized in case of any failure. NAS boxes give you: External support -- somebody else to blame if things go pear-shaped ;). Usually more user-friendly interface (click here to setup your box) and, usually, ability to replace failed disk by a trained monkey. They also have a limited feature set, which you cannot realistically expect to be changed on your request. Custom-made Linux system will be much more flexible, give more bang per buck, no artificial limits of feature lists and ease of expansion (throw more HDDs into it). OTOH they will require somebody, who knows what he is doing, to handle them. Also, if your main board/disk controller dies terribly, you can just move the HDDs into any Linux box and have your data available. This may not be the case with a NAS box if it uses a HW RAID solution. As far as OS-X support goes I think you should be happy with NFS, but last time I was configuring a file server for Macs it was XXth century outside and the boxes talked AppleTalk.