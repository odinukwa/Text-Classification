Why sending data from gpu to cpu is slower than cpu to gpu? I heard the relation is similar to network situation, in detail, the upload is slower than download. However, I could not understand the meaning. In my opinion, the sending operation should be same because the two sending buses (cpu to gpu and gpu to cpu) can be made with the same performance (I mean bendwith). 

I'am curious about compute shader in OpenGL. Let's assume the number of points (vec4) is 900 and the work group size(= the number of work items) is 256 Then, We would have four work groups because 900/256 = 3.xx, so need to plus 1. The code is as follows: 

Let's suppose the start point of the ray $R$ is $(x_0, y_0, z_0)$ and direction is $(x_d, y_d, z_d)$, then $R = (x_0, y_0, z_0) + (x_d, y_d, z_d)t$ , and the plane is $ax + by + cz + d = 0$. To check intersection between the ray and plane, calculate the below equation. $t = \frac{-(ax_0 + by_0 + cz_0 + d)}{ax_d + by_d + cz+d}$ If $t \ge 0$, then there is a possibility of intersection, so go second step. else there is not intersection. 

I'm using GLUI for user interface. I tried to add Listbox with several items. I could add items like below image, but if I clicked the one of them (None, AAA, BBB). The program was stopped. It looks like accessing NULL data. 

What's worng with me? (If I changed the dvec4 in the above shader to vec4, the triangle was drawn, but maybe it's not double anymore. I want to use double in shader.) 

I'm working with OpenGL and facing some difficulties because I'm not familiar with OpenGL. I tried to search related example in Google, but I could not find some useful code. There are five arrays. I bound and changed the data in compute shahder as follows: 

In detail, supposing I want to change the second vertex element to (1.0f, 1.0f, 1.0f). I know it with compute shader, but I just wnat to know how to do it without compute shader or OpenCl. Is it possible to change the buffer data uploaed directly without uploading the whole data again? 

I'm studying spring model. There is a suggested equation (Hooke's Law vector form) But, I couldn't understand how to derive that equation. I'm reading 'Computer Animation Algorithms and Techniques Third Edition. In the page 241, (7.88) The equation is as follows: ($|v_{1}^* - v_{2}^*|$: the rest length) ($|v_{1} - v_{2}|$: the current length) $F_s = (\frac{k_s|v_1 -v_2| - |v_{1}^* - v_{2}^*|}{{|v_{1}^* - v_{2}^*|}})\frac{(v_1 -v_2)}{|v_1 -v_2|}$ // I think the $k_s$ is typo. (I referred to $URL$ So, maybe $F_s = k_s(\frac{|v_1 -v_2| - |v_{1}^* - v_{2}^*|}{\color{red}{|v_{1}^* - v_{2}^*|}})\frac{(v_1 -v_2)}{|v_1 -v_2|}\cdot\cdot$(1) is correct. However, I wonder why the red part divides the equation? If I know correctly about Hooke's Law, $F=-k_s(current_{lenght} - rest_{length})\cdot\cdot$(2). It means that the red part should be removed like that $F_s = k_s(|v_1 -v_2| - |v_{1}^* - v_{2}^*|)\frac{(v_1 -v_2)}{|v_1 -v_2|}\cdot\cdot$(3). For example, in 3D space, there is a spring that has the spring term as $k_s$. The rest length is '3', so let's assume the left rest point, $v_2^*$, is fixed at (0,0,0) and the right rest point, $v_1^*$, is (3,0,0). And then we moved $v_1^*$ to $v_1 $=(5, 0, 0) with some force. We can calculate the force with (2), and the force is $-2k_s = -k_s(|(5,0,0) - (0,0,0)| - |(3,0,0) - (0,0,0)|)$ But if we use equation (1), the result is different. $F_s = -k_s(\frac{|v_1 -v_2| - |v_{1}^* - v_{2}^*|}{{|v_{1}^* - v_{2}^*|}})\frac{(v_1 -v_2)}{|v_1 -v_2|} \\\quad= -k_s(\frac{|(5,0,0) -(0,0,0)| - |(3,0,0) - (0,0,0)|}{{|(3,0,0) - (0,0,0)|}})\frac{((5,0,0) -(0,0,0))}{|(5,0,0) -(0,0,0)|} \\\quad= -\frac{2}{3}k_s(1,0,0)$ The force amount of $-\frac{2}{3}k_s(1,0,0)$ is totally different from $-2k_s$ Could you explain what I'm wrong? 

I'm doing a project with assimp. I got confused with the weird situation for me. I think the both code are exactly same, but the result is different. Why the codes act differently? (vertex shader) 

Hnece, we can get the $\alpha, \beta, \gamma$. If $0 \le\alpha, \beta, \gamma \le 1$ and $\alpha+ \beta + \gamma = 1$, then there is intersecton. else No intersection!. (The page 19, 20 is explaining how to calculate the 'Area'. (Actually, we can project the 3D space points to 2D space for performance, but avoid the perpendicular projection!) ) 

I took a few days and wrote something to do this. It was not really as straightforward as I was expecting so I'll share what I learned. What was hard Here are the major headaches I encountered. Dealing with JPEG Compression Artifacts When sampling the values for the legend, I encountered many JPEG compression artifacts. This required me to apply a median filter. The media filter width needed to be a parameter of the function (needs different values depending on how big the legend is.) Dealing with "bands" in the legend The legend in your first example is divided into discrete sections (bands) and the numeric quantity desired is probably the middle of the range that the colour spans. For example, the left-most box covers values from -6500 to -6000 and should probably return "-6250" when trying to sample a single colour of that value. Also, consequently, no pixel will return the value "0". There is no colour representation for "0" (in that first map anyway.) Dealing with differences in tone / value. Sometimes the exact colour in the map doesn't exist in the legend, especially due to the text labels, so you have to do a "closest" matching algorithm and the hue seems to be more important than the value (black&white-ness) so it was better to evaluate "distance" between colours in a colourspace other than RGB. Text overwrites data There are plenty of black pixels in the map due to the labels, but these need to be ignored somehow. It makes sense to try to infer what the value behind the text is, and only colours that are not in the legend qualify for this kind of inference. Manually selecting points on the legend is not precise Each time I eye-balled it and manually found a point that looked like approximately the end-point, I'd choose a slightly different pixel, resulting in slightly different values between attempts. Perhaps a snapping technique would be better where I could "hint" where the legend is, but it would find the high-contrast edge markings and snap the hint point to the exact point. Most maps don't have continuous legends. Most other images that I went searching for with test data had discontinuities in their legend (discrete boxes of colour) as opposed to a nice continuous linear gradient like you provided. This makes it extra difficult to automatically come up with the mapping from colour to value just by analyzing the positions of the colours in the legend. I was disappointed when trying to apply it to most maps. Visualization tips It was very useful (especially while debugging) to visually show the point on the legend that it thought best corresponded to the colour being picked in the map. Persistence It's useful to persist the location of the endpoints of the linear gradient portion of the legend and the values that are associated with them between runs of the application. Don't underestimate the value of this. It's annoying to have to specify all that stuff every time. What worked well Represent the legend as all the pixels along a line segment Specify two (x,y) locations of the start and end of a linear gradient (the legend) and manually define the values that are associated with them. build up a discrete sample set for the gradient Sample all the pixels in between the start and end points to build a linear gradient dataset. Median filter Apply a median filter to the linear gradient dataset to filter out JPEG noise. Quantize the dataset to locate centroids Optionally filter out bands (long runs of samples in the linear gradient dataset that have approximately the same colour) and replace them with a single sample at the centroid (average position). Use bilinear sampling when picking a pixel Pick a pixel in the map and perform a bilinear sample to smooth out pixellation. This seems to help improve the smoothness and resolution of the map. Linear search is fine Linearly search for the data point in the linear gradient for the closest colour. For an interactive picker working from mouse clicks, you have lots of time to perform a linear search. No fancy hashing, indexing or optimized searching was required. Hue is important in the colour distance function Compare distance between colours using a distance function in a colourspace that tolerates brightness difference more than hue difference. (like YUV, for example) 

Short answer, set the precision of the image to a higher value. Long answer, When looking at a gamma correction curve, you can see that the lower values get changed much more, this means that the difference between lower values will get greater and that causes this effect. You have a limited amount of values for a color channel and this means that when it rounds the number down to the 8 bit value, it makes some neighboring pixels the same color. You don't notice this in a normal image, but after gamma correcting it this effect comes out, because you lose that bit of change in the color that later on can become a much bigger difference in color. To fix this, you just give the gamma correction more precision to also get that small color change it would've else rounded away. In the actual shader, it has floating point color information and that means that it has more precision and thus not the weird artifact. So when making the frame buffer, you can use for example GL_RGBA16F or GL_RGBA32F. This would increase the precision and remove the artifact. I hope that this can help you fix the problem, good luck and have a great day! 

We want to get $pMin$ and $pMax$. $pMin$ should be the lowest value for all the components. $pMax$ should be the highest value for all the components. That would make calculations a lot easier of course. When looking at the graph we can easily find $pMin$ and $pMax$. 

So you could say that a vector is a direction with scale, and a point is a location. So, if you transform a vector you just rotate and scale it. With a point you also translate it (the rotation and scaling of a point is around the origin, since it iss just a location the point itself cannot be rotated). Most of the times a vector and a point are put into the same container, a vector with 4 components. The only difference is the w component. If the w component is 0, then it is a direction. If it is 1 then the vector is a point. The reason for this can be found in the matrix itself. It makes use of the way you multiply a vector with 4 components with a 4x4 matrix. If you do not know how that works, I would suggest a quick google. Most of the times you use a 4x4 matrix. A normal transformation matrix could look like this: \begin{bmatrix}rot+scale&rot+scale&rot+scale&translation\\rot+scale&rot+scale&rot+scale&translation\\rot+scale&rot+scale&rot+scale&translation\\0&0&0&1\end{bmatrix} (The rotation and scale are put in the 3x3 area you could say, so for just rotation and scale a 3x3 matrix could also be used, but when translation comes in, we need that 4th column.) As you can see, if the last component is 0, then you have a multiplication with 0 and therefore the result is 0 and there is no translation. This makes it easy in computer graphics with polygonal objects. You have the same transformation matrix to transform the positions but also the normals. Because the normals have their w component set to 0 and the positions' w component is 1, the normals are just rotated (and also scaled which can lead to some weird stuff, so most of the times the normal is normalized after. It isn't actually recommended to use the same matrix for positions and rotations because of the weird stuff! Look at @JarkkoL 's comment.) and the positions are translated (and rotated and scaled around the origin). Hope I did not make a mistake :P, and this helped you! 

We see three vertices, separated by a white space. Each vertex has two numbers. The first number is the index for the position. The second number is the index for the normal. They are both different. Wavefront does not need to care about caching. It just needs to store the data by using the least amount of memory (if we load everything in to memory. Saving it in plain text is of course not efficient). When looking at your .obj file, we can see that we have 8 positions, but only 6 normals. We have been able to remove 6 floating point numbers in total (2 normals consisting of 3 floats). Wavefront specifies different indices for each vertex attribute. For a vertex, the position could be number 5, but the normal is number 4. Conclusion Wavefront and OpenGL use a different way of storing the data. This means that when parsing your wavefront file, you need to re-index the data to what OpenGL wants. What happened with you, is that OpenGL thought that the indices for the normals were actually indices for positions and that you had twice as many faces. What I would do, is to save all that wavefront data to temporary lists. You create your main vertex buffers. You go through every vertex of every face. You look at the data of that vertex. Then you see if you already have that data combined in your buffers. If so, you use the index of that data, else you add the new data to your buffers and add that index.