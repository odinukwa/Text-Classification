I'm wondering that is there any feature that help in differentiating of the following two images. I mean differentiating in related numbers. Thank you 

I have a classification problem. I want to reduce number of features to 4 (I have 30). I'm wondering why I get better result in classification when I use correlation based feature selection(cfs) first and then employ pca in comparison with just employing pca (the latter one is worse than the first one). It also should be mentioned that data loss in the second approach (just pca) 0.2-variance cover:0.8- and in the first one is 0.4 -variance coverd: 0.6! Thank you in advance 

I want to combine the following vectors in a way that just the red point (number 7) becomes inconsistent with other points( become an outlier and become distant from other points) and other points become consistent with each other. Please note that I have tested mahalanobis distance and Kullback-Leibler divergence between two vectors but they were not so good and detects. a=[1.3269 1.3354 1.3318 1.3282 1.34666 1.3460 1.36084 1.3526 1.3539 1.3510 1.3480 1.3479 1.34893] b=[0.0352,0.0992,0.1570,0.1431,0.1634,0.1629,0.1046,0.1655,0.1635,0.1642,0.1658,0.1666,0.15735] Thanks in advance. 

I have a dataset which belongs to a hospital. It contains data about patients and healthy people. The problem is separating healthy ones from patients. I add some new features to dataset to solve this problem. When I reduce the dimensions of data including the new features and visualize the data, the patient and healthy individuals are distinguishable(visually separable). Now if one asks what is the relation between the used approach (feature extraction, visualization, using the human ability, unsupervised methods) and expert system, what can I say? How can I use this method to build an expert system? Thank you in advance. 

I wonder how can I give weight to my feature before employing PCA. I mean somehow weighted PCA. Because I know that one of the features is better than others and want to give importance to it in creating components (It is not possible to select only that feature. I should have others impact too) 

I have a dataset which has two class. It has 13 features. They are values which are sent from 13 sensors. Label is True or False. When I use mad outlier detection, when the label is false(really there are some outliers), it detects the outliers correctly. I mean for example when sensors 1 and 3 are modified which leads to having label "False", it detects 1 and 3 exactly and don't raise wrong additional sensor's number or fewer than actual one. The problem is when the the label is True and there is no outlier. It still outputs some sensors as outlier. Actually it raise false positive. This makes hard to know that this is really outlier or not. How can I solve this problem and finally be sure that this result is outlier or not? I have tested combination of this method with some other outlier detection results. I also used clustering in combination with MAD but I have not reached to a systematic solution. Btw, Maybe you ask when you have label why you use outlier detection. No, I should develop an unsupervised method and checking label is just to be sure about the method accuracy. Thanks in advance. 

C#, Javascript, R, SQL Java, C#, Javascript, Objective C, Swift, SQL C#, SQL Python Solutions are also welcome 

What are good films for teaching data-driven decision making? In one of my classes, I asked my students to watch Moneyball at home and we discussed it in the class. It was successful and they learned much more than they could by reading a chapter of a textbook. Are there films with similar theme suitable for my purpose? 

I recommend calculating 1-gram, 2-grams and ... frequencies, sorting based on frequencies and observing what pops up. Sometimes you see patterns that need extra data cleansing. 

I have a dataframe in R containing a variable for programming languages. This is extracted from a multiple choice questionnaire in a survey. As a result, any programming language may fall into the beginning, middle or end of the whole string. I added a binary variable for any programming language. Here is a sample of my data: 

I recommend starting from Coursera specializations in data science. The data science specialization by Johns Hopkins is the oldest running specialization. I do not recommend books and kaggle. They only confuse you in the beginning. Keep in mind that coding is the easiest part of data science and you have to learn a lot. For getting an idea about the field, this Venn Diagram is a good start. 

Take a look at O'Reilly free data reports . You can find reports on Banking and Fintech, Sports, Fashion, Music, Health, Oil and Gas and so on. Keep in mind that McKinsey report mentioned earlier is a classic report and a must-read. 

I have problem with one-word languages such as C, R, D and Java . I made the variables for C#, C++ , Javascript, Ruby and others using the for loop but I have problems with these four. What regular expression can I use so that it covers Java but not Javascript (in the beginning, middle and end of the string) and covers R but not Ruby and so on? Any other solution for making the binary variables are also welcome. 

I found a PhD in Energy Informatics at Karlsruhe Institute of Technology (KIT) and PhD in Learning Analytics at University of Technology Sydney 

I have two lists: The first one includes sentences and the second one includes parts of speech (POS) tags. I want to use POS tags as a filter and find phrases according to a pattern such as 'JJ JJ NNS' (adjective+adjective+noun) How can I do that? 

The problem is the string. It includes quotations while E(network)[str] does not work with quotations. 

Try regression diagnostics. There are a couple of methods to help you make sense of the data and the model. Try this link, go to resources tab and download Chapter 6 - Diagnosing Problems in Linear and Generalized Linear Models. The code is in R. However you can find the python equivalent of the code. 

As far as I know, there is not a rule to say which algorithm works for which dataset. Just make sure your dataset and variables of interest fulfill the pre-assumptions of running each algorithm and give it a try. For example, linear regression has some pre-assumptions such as normality of resuduals, homoscedasticity (the variability in the response variable is the same at all levels of the explanatory variable) and so on. Just check these for your variables and give the algorithm a try. You can use a point and click software to see the results without getting involved in the code and parameter setting. If you are an R user, rattle package will be a very useful tool at this stage. You do your job in point and click mode and you have access to the code behind it. 

Where is the difference between one-class, binary-class and multinominal-class classification? If I like to classify text in lets say four classes and also want the system to be able to tell me that none of these classes matches the unknown/untrained test-data. Couldn't I just use all the methods that I mentioned above to reach my goal? e.g. I could describe C1, C2, C3 and C4 as four different trainings-sets for binary-classification and use the trained models to label an unknow data-set ... Just by saying, Training-Set for C1 contains class 1 (all good samples for C1) and class 0 (mix of all C2, C3 and C4 as bad samples for C1). Is unlabeled-data C1 -> 1 or 0 Is unlabeled-data C2 -> 1 or 0 ... and so on ... For multinominal classification I could just define a training-set containing all good sample data for C1, C2, C3 and C4 in one training-set and then use the one resulting model for classification ... But where is the difference between this two methods? (except of that I have to use different algorithms) And how would I define a training-set for the described problem of categorizing data in those four classes using one-class classfication (is that even possible)? Excuse me if I'm completely wrong in my thinking. Would appreciate an answer that makes the methodology a little bit clearer to me =) 

Where is the difference between one-class, binary-class and multinominal-class classification? If I like to classify text in lets say four classes and also want the system to be able to tell me that none of these classes matches the unknown/untrained test-data. Couldn't I just use all the methods that I mentioned above to reach my goal? e.g. I could describe C1, C2, C3 and C4 as four different trainings-sets for binary-classification and use the trained models to label an unknow data-set ... Just by saying, Training-Set for C1 contains class 1 (all good samples for C1) and class 0 (mix of all C2, C3 and C4 as bad samples for C1). Is unlabeled-data C1 -> 1 or 0 Is unlabeled-data C2 -> 1 or 0 ... and so on ... For multinominal classification I could just define a training-set containing all good sample data for C1, C2, C3 and C4 in one training-set and then use the one resulting model for classification ... But where is the difference between this two methods? (except of that I have to use different algorithms) And how would I define a training-set for the described problem of categorizing data in those four classes using one-class classfication (is that even possible)? Excuse me if I'm completely wrong in my thinking. Would appreciate an answer that makes the methodology a little bit clearer to me =) 

The most online tutorials like to use a simple example to introduce to machine learning by classify unknown text in spam or not spam. They say that this is a binary-class problem. But why is this a binary-class problem? I think it is a one-class problem! I do only need positive samples of my inbox to learn what is not spam. If I do take a bunch of not spam textes as positiv samples and a bunch of spam-mails as negativ samples, then of course it's possible to train a binary-classifier and make predictions from unlabeled data, but where is the difference to the onc-class-approach? There I would just define a training-set of all non spam examples and train some one-class classifier. What do you think?