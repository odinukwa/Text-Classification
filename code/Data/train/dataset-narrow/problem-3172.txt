b) The example below shows an extreme example of a feature that may not be very informative by itself, but that is very informative together with another one (feature_2). 

Choosing the best set of features is the objective of feature engineering. Feature engineering combines domain expertise and algorithms in order to extract the best set of predictors for a given problem. The process follows these steps: 

Graphs are a very flexible form of data representation, and therefore have been applied to machine learning in many different ways in the past. You can take a look to the papers that are submitted to specialized conferences like S+SSPR (The joint IAPR International Workshops on Structural and Syntactic Pattern Recognition and Statistical Techniques in Pattern Recognition) and GBR (Workshop on Graph-based Representations in Pattern Recognition) to start getting a good idea of potential applications. Some examples: 

There is not such a thing that a perfect mapping between type of problems and algorithms. The model you may end up using will strongly depend on the nature of your data. You would need to explore the data and ask the domain experts in order to make an informed decision on what method to use. Experience will make the process easier with time, but it is likely that you will have to apply different types of models to your problem in order to look for the best one. 

My suggestion is just a greedy brute force approach. I am sure that more efficient solutions may exist, but it may be a first step towards your final solution. Store in a table, for each cluster which size is lesser than T, the distance to its closest neighbouring cluster and the ID of that closest cluster. Now, sort the table entries in ascending distance order. Merge the cluster in the first entry with its closest neighbouring one. If the closest cluster also had an entry in the table, and its size got larger than T, remove its corresponding entry. Finally, update all the entries of the table, and sort them in ascending distance order again. Repeat the steps above until there are not more entries in the table. 

As I said, there are many other options, but these ones are the simplest ones I can think of, and the ones I would start from. 

MDS only requires a distance matrix which stores the distance between each pair of data examples. How to compute that distance depends on the kind of data you are dealing with. If you only have numerical (real valued) features, you can use the Euclidean distance, but that is not always the case. For instance, if you have both numerical and categorical variables, you may apply any metric for mixed data, like for instance this one. Regarding your second question, I need further clarification. What do you mean by "converging? Are you asking if both algorithms converge to the same solution? Is that is your question, the answer is no, because the aim of both algorithms is different. MDS projects your data onto a 2D plane by trying to keep relative distances, whether PCA projects data by focusing on those "directions" in which there is more variability. 

It is very hard to evaluate the correctness of the results produced by an unsupervised algorithm. In many cases, such evaluation will be totally subjective, and it will require some knowledge about the domain of the problem. If we focus on clustering algorithms (as stated in a previous answer, PCA is not a clustering algorithm), many cluster validation measures may be applied, like the ones enumerated in the "Evaluation and assessment" section in the "Cluster analysis" Wikipedia page. These measures return a number that you can use to compare different clustering solutions, in terms of cluster compactness (how close to each other are the elements within each cluster) and separation (how far to each other are the elements from different clusters). You can, of course, use these measures in order to perform hyperparameter selection (lattice size, structure, learning rate, etc.) by means of cross validation. However, it must be noted that different cluster validation measures may produce different validation results, and as a consequence, your best clustering solution may vary depending on the chosen measure. Therefore, even the choice of a validation measure is subjective. Once again, knowledge about your data is very important to take this decision. 

Your training and test errors are affected by the size of the training. Take a look to this plot, usually known as a learning curve: 

There are many ways in which you can compute a distance between time series, and the method to use will depend on your data. As stated by other answers, Dynamic Time Warping may be the way to go ($URL$ However, this method assumes that there may be a non-linear warp between different parts of the time series. If you are not expecting warping or delays in the signal, something as simple as Euclidean distance may be a better way to go. Of course, you should apply Euclidean distance only after you applied some preprocessing (for instance, amplitude scaling). Take a look to the following presentation, that introduces the pros and cons of these methods and discuss preprocessing in more detail: $URL$ 

According to this interesting paper, Manhattan distance (L1 norm) may be preferable to Euclidean distance (L2 norm) for the case of high dimensional data: $URL$ The authors of the paper even go a step further and suggest to use Lk norm distances, with a fractional value of k, for very high dimensional data in order to improve the results of distance-based algorithms, like clustering. 

Naive Bayes is just one of the several approaches that you may apply in order to solve the Titanic's problem. The aim of the Kaggle's Titanic problem is to build a classification system that is able to predict one outcome (whether one person survived or not) given some input data. The survival table is a training dataset, that is, a table containing a set of examples to train your system with. As I mentioned before, you could apply Naive Bayes to build your classification system to solve the Titanic problem. Naive Bayes is one of the simplest classification algorithms out there. It assumes that the data in your dataset has a very specific structure. Sometimes Naive Bayes can provide you with results that are good enough. Even if that is not the case, Naive Bayes may be useful as a first step; the information you obtain by analyzing Naive Bayes' results, and by further data analysis, will help you to choose which classification algorithm you could try next. Other examples of classification methods are k-nearest neighbours, neural networks, and logistic regression, but this is just a short list. If you are new to Machine Learning, I recommend you to take a look to this course from Stanford: $URL$