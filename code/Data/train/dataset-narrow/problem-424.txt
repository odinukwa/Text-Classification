ORA-24247 is something you would get after you already have a connection and are trying to use a network resource (such as UTL_HTTP). It sounds like you need to create an ACL (Access Control List). See $URL$ 

I have a case open with Oracle and they say they can't reproduce this. Running the following in 11.2.0.1 or lower produces two rows from both of these queries, but in 11.2.0.2 the second query returns three rows unless the case statement is removed. Can anyone confirm this behavior? 

This worked great! Unfortunately it did not work when we tried to script the creating of a materialized view owned by AU that uses its private database link. It appears that it connects to the remote database using the logged in user (sys) not the user that owns the link (AU) and therefore fails: 

There are problems that will occur if a session with a different date format runs the code. Statement Failure 

The concatenation is occurring before the addition causing it to fail trying to add a string to a number. Concatenation and addition both have the same level of operator precedence, but are processed from left to right, so if you had removed the leading concatenation, the trailing one would succeed. As Balazs explained, adding parenthesis fixes the order of execution problem, but it is still doing implicit date to string conversion in a format that depends on the session settings. As miracle173 pointed out, the best solution is to make your code independent of session settings by doing explicit date conversion as follows: 

It may seem that an application called ASH Viewer wouldn't work with Standard Edition because it does not have the Diagnostic pack and therefore does not have v$Active_Session_History, but it does. When a new connection is created it asks whether the target is Standard Edition or Enterprise Edition. If Standard Edition is selected it does not use v$Active_Session_History. This can be demonstrated by querying the view with control_management_pack_access set to NONE, which among other things causes the view to return no records. Pros: 

As Niall said this data is not available. It would be crude, but you could roll your own solution for this. By periodically querying sys.file$ for size changes, you could determine that one or more auto-extend events have occurred and then based on the size change and the auto-increment value determine how many extensions have taken place since the last check. The more often you check the more closely you could identify the times. Inserting the results of every second (processed separately) would give you about as clear a picture as possible. While this does technically answer your question, the advantage of having this information would not be worth the overhead. I highly recommend NOT doing this. 

It would probably be overkill, but you could install Oracle Express Edition and use database links to have it retrieve the data to be written from the remote database. You would probably want the logic to remain on the existing server and just have the XE do the file writing. This method would allow you to re-use almost all your code, but it has the downside of requiring an additional installation of Oracle and the associated maintenance. 

(a in .NET classically includes the message you mentioned - "Object reference not set to an instance of an object") You should try updating to 17.6 (build 14.0.17230.0) to see if the problem is resolved there. 

A few things could be going on here. The files are in the wrong place, or have an unexpected extension This one seems unlikely, since you've probably been using Ola's scripts for all your backups, but just in case you've changed a setting (like the directory location, or the log file extension). The "cleanup" part of Ola's scripts are looking for backups in this location (starting with the server / folder you specified in the @Directory parameter). In your question you used so I'll continue with that: 

I would highly recommend that you test your new T:\ drive using Crystal Disk Mark. Check out the guide from Brent Ozar here: How to Test Your Storage with CrystalDiskMark Compare the results from the T:\ drive with 

And then: Check to see if there is an explicit maximum preventing your log file from growing If this is the case, set to a larger number to accommodate your actual transactional load. The number for this will depend on your usual number of transactions, and what recovery model you're using. Check to see if autogrowth has been disabled (growth = 0) If it is disabled, you could enable it. If you can't do that, and you're in FULL recovery model, you could schedule more frequent log backups. If you can't do that, and you're in SIMPLE recovery model, you'll need to increase the size of the log file manually until it's large enough to handle the transactions you have between automatic or indirect checkpoints. It's possible, maybe, that the disk of your Azure storage is full and that's what is preventing the file from growing (although I'd expect different error messages). 

Note: to get the best help on this question, please include your actual RESTORE statement, and the specific error message that you're getting. When using In-Memory OLTP, SQL Server has to create a new folder named "xtp" in the root of the default file location for the SQL Server instance. This folder contains the DLLs for compiled stored procedures and other in-memory objects. You can find more details about that here: In-Memory OLTP files â€“what are they and how can I relocate them? If you've changed the location for your data files, you may need to update SQL Server's access to the file system there: Configure File System Permissions for Database Engine Access As a test / workaround, you could manually create the "xtp" folder, and then try the restore again. 

If the SSD is slower than those other two devices, and nothing else has changed* in your setup, it's likely there is a problem with the disk itself, or with the driver being used, or the controller for the array this disk sits in, etc. *things that might have changed since you moved tempdb: 

Your transaction log is full. Looking at the screenshot you shared, it's only 59 MB. Which is pretty small. One would think it would just keep growing to accommodate more transactions. But it can't for some reason. SQL Server wants to start re-using the log file. But you've got that pesky thing happening. This means that there are modified date pages in memory that haven't been persisted to the data file on disk, and thus SQL Server can't start re-using the part of the transaction log that documents those potentially-unpersisted transactions. A temporary fix would be to run the command manually on the server, to try and force those buffered pages to be flushed to disk. You may have to run the command more than once, but this will allow that transaction log file to start being reused. The bigger problem is with your transaction log. You should run this query: 

So far everything seems to work faster than the normal update statement. The problem now is I would want to get the remaining data from the original table which I do not need to update but I do need to be included on my updated table/list. What I tried to do at first was use DELETE on the same original table using the same where clause so that in theory, everything that should be left on that table should be all the data that i do not need to update, leaving me now with the two tables: 

Given these considerations, I have come up with this sizing estimate and as to why I chose these: (Please correct me if I'm wrong since I am not an experienced DBA) USERS tablespace (40 gigabyte) ~ I might use a bigfile tablespace data file instead of a regular one. I am assuming that the size of the input text file would be of the same size when I transfer it to a table. The reason why I doubled it is because I am going to use CTAS which from what I understand would be similar to copying the table itself. TEMP/UNDO tablespaces (20 gigabyte for each) ~ From what I understand, temp is used for sorting data and UNDO would be used for rollback and updates (please correct me if I am wrong about this), and given that most of my queries and update statements would load most if not all of the 100 million records on my table, I am thinking that the exact size for sorting and undo should be the same as the data file size itself. (Again this might be a less educated guess and please correct me if I am mistaken) Summing that up I would have around 80 gigabytes of tablespaces on my database machine, exclusing the redo logs and the indexes which I think I'll put on another tablespace sizing around 10-20 gigabytes giving me a total of 100 gigabytes So right now I am looking at 100 gigabytes table space needed for an input of 20 gigabytes. Is this okay or am I doing something wrong here and blowing the tablespace sizing out of proportion? 

What other alternatives can I use in order to get the data that's the opposite of my WHERE clause (take note that the where clause in this example has been simplified so I'm not looking for an answer of NOT EXISTS/NOT IN/NOT EQUALS plus those clauses are slower too compared to positive clauses) I have ruled out deletion by partition since the data I need to update and not update can exist in different partitions, as well as TRUNCATE since I'm not updating all of the data, just part of it. Is there some kind of JOIN statement I use with my TABLE1 and TABLE1_TEMPORARY in order to filter out the data that does not need to be updated? I would also like to achieve this using as less REDO/UNDO/LOGGING as possible. Thanks in advance. 

First of all what kind of database are you using? And are you referring to table partitioning? Anyways for Oracle, please see this link.. $URL$ I'm not too sure about how this would be done in PostgreSQL, but I think partitioning is much less relevant for PostgreSQL rather than for Oracle