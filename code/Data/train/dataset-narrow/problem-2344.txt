These conditions are very easy to check, being simple inequalities among the input parameters, so the existence question can be answered effortlessly. Furthermore, the proof of the theorem is constructive, resolving the construction issue, as well. On the other hand, this result does not appear standard enough, so that you can expect everybody to know about it. Can you provide further examples in this spirit, where knowing a (not so standard) theorem greatly simplifies a task? 

Several answers pointed out that the premise of my question (the relative scarcity of natural $\mathsf{NPI}$-candidates) might be questionable. After some thinking, I must accept that they indeed have a point. In fact, one can even go as far as to make the case that there are actually more natural $\mathsf{NPI}$ candidates than natural $\mathsf{NP}$-complete problems. The argument could go as follows. Consider the LOGCLIQUE problem, which aims at deciding whether an $n$-vertex input graph has a clique of size $\geq \log n$. This is a natural $\mathsf{NPI}$ candidate. Now, the same type of "scaling down" can be carried out on any $\mathsf{NP}$-complete problem. Simply replace the question "does the input string $x$ have a property $Q$?" by the scaled down question "does $x$ have a logarithmically sized substring that has property $Q$?" (We may restrict ourselves only to those substrings that represent the appropriate type of structure, such as subgraphs etc.) Arguably, if the original problem was natural, the scaling down does not change this, since we only alter the size of what is sought for. The resulting problem will be an $\mathsf{NPI}$ candidate, since it is solvable in quasi-polynomial time, but still unlikely to fall into $\mathsf{P}$, as the mere size restriction probably does not introduce new structure. This way, we can construct a natural $\mathsf{NPI}$ candidate for every natural $\mathsf{NP}$-complete problem. Additionally, there are also generic candidates that do not arise via scaling down, such as Graph Isomorphism, Factoring etc. Thus, one can indeed make the case that "natural-$\mathsf{NPI}$" is actually more populous than "natural $\mathsf{NPC}$." Of course, this scaling down process, using Scott's nice metaphor, gives an obvious reason for resisting the "gravitational pull" of SAT. While there are papers published about LOGCLIQUE and similar problems, they did not draw too much attention, as these problems are less exciting than the the generic $\mathsf{NPI}$ candidates, where there is no clear understanding of how the gravitational pull is resisted, without falling into $\mathsf{P}$. 

Background It's well known that, in a bicartesian closed category (BCCC), if the initial and final object coincide (that is, if the category has a zero object) the category collapses (with all types being isomorphic) by $A \cong A \times 1 \cong A \times 0 \cong 0$ for all $A$. This means, for instance, that the category of pointed sets, with its zero object, can't be bicartesian closed. But the category SCpo also has a zero object for the same reason: all objects are structured sets (CPOs) with a bottom element $\bot$, and arrows are strict (and ($\omega$-)continuous) functions, so they preserve $\bot$. Indeed, this is attributed to Smyth and Plotkin (1982), who describe this category as $\mathbf{CPO}_{\bot}$ and state it lacks categorical products; other categories they consider lack other features of a BiCCC (e.g., their $\mathbf{CPO}$ lacks sums). What is not clear to me is whether every way of handling $\bot$ falls into this trap. However, knowledgeable people on Reddit seem to repeat this claim without good sources (Filinski's master thesis was the best reference I got, and it doesn't lay out a generic categorical argument). 

Remark: Finding the max number of vertex- or edge-disjoint paths is well known to be solvable by network flow techniques in polynomial time. Does the geometric requirement make the problem harder? Edit: Let us assume that the vertices have polynomially bounded integer coordinates, in terms of the number of vertices. Furthermore, assume an oracle is available that can determine for any two edge-curves whether they intersect or not. When the edges are represented by straight lines, this is straightforward, but in the general case it may be hard to decide whether two curves intersect. 

Is there a graph class for which the chromatic number can be computed in polynomial time, but finding an actual $k$-coloring with $k=\chi(G)$ is NP-hard? Without any further restriction the answer would be yes. For example, it is known that in the class of 3-chromatic graphs it is still NP-hard to find a 3-coloring, while the chromatic number is trivial: it is 3, by definition. The above example, however, could be called "cheating" in a sense, because it makes the chromatic number easy by shifting the hardness to the definition of the graph class. Therefore, I think, the right question is this: Is there a graph class that can be recognized in polynomial time, and the chromatic number of any graph $G$ in this class can also be computed in polynomial time, yet finding an actual $k=\chi(G)$-coloring for $G$ is NP-hard? 

I was reading about Call-by-Push-Value in the introducing paper from 1999, but I have some confusion, partially because of my unfamiliarity with domain theory. I might have figured it out, but I'd hope to get it confirmed. (If this is not the appropriate venue, advise is welcome on which one is). I was confused by the statement: 

*Apparently, some (non-formalists) claim that set theory is not "just syntax", but something ontologically different. I'll ignore this subtle philosophical issue; the only reference I know on it is Raymond Turner's Understanding Programming Languages. 

A nice example is Tate et al.'s Generating Compiler Optimizations from Proofs. He uses pullbacks and pushouts as generalized unions and intersections, in categories where arrows are (IIRC) substitutions. Ross Tate claims (on the paper webpage) that details were overwhelming without the abstraction afforded by category theory. Personally, I'd like to submit as "suggestive evidence" (if there can be any evidence of such a claim) diagrams (6) and (7) in their paper — they look complex enough in diagrammatic form. Let me quote their comments inline. 

For a correctness proof, I'm looking for a usable notion of program equivalence $\cong$ for Barendregt's pure type systems (PTSs); missing that, for enough specific type systems. My goal is simply to use the notion, not to investigate it for its own sake. This notion should be "extensional" — in particular, to prove that $t_1 \cong t_2$, it should be enough to prove that $t_1\; v \cong t_2\; v$ for all values $v$ of the appropriate type. Denotational equivalence Denotational equivalence easily satisfies all the right lemmas, but a denotational semantics for arbitrary PTS seems rather challenging — it'd appear hard already for System F. Contextual/observational equivalence The obvious alternative are then various forms of contextual equivalence (two terms are equivalent if no ground context can distinguish them), but its definition is not immediately usable; the various lemmas aren't trivial to prove. Have they been proved for PTS? Alternatively, would the theory be an "obvious extension", or is there reason to believe the theory would be significantly different? EDIT: I didn't say what's hard above. Easy part: the definition Defining the equivalence is not too hard, and the definition appears in many papers (starting at least from Plotkin 1975's study of PCF, if not earlier — the source might be Morris's PhD thesis from 1968). We $t_1 \cong t_2$ if for all ground contexts $C$, $C[t_1] \simeq C[t_2]$ — that is, $C[t_1]$ and $C[t_2]$ give the same result. You have a few choices here with lots of alternatives: For instance, in a strongly normalizing language, if you have a ground type of naturals, you can say that ground contexts are the ones that return naturals, and then $a \simeq b$ means that $a$ and $b$ evaluate to the same number. With nontermination, for reasonable languages it is enough to use "X terminates" as observation, because if two programs are equivalent when observing termination, they're also equivalent when observing the result. Hard part: the proofs However, those papers often don't explain how hard it is to actually use this definition. All the references below show how to deal with this problem that, but the needed theory is harder than one thinks. How do we prove that $t_1 \cong t_2$? Do we actually do case analysis and induction on contexts? You don't want to do that. As Martin Berger points out, you want to use, instead, either bisimulation (as done by Pitts) or a logical equivalence relation (that Harper simply calls "logical equivalence"). Finally, how do you prove extensionality as defined above? Harper solves these questions in 10 pages for System T, through considerable cleverness and logical relations. Pitts takes more. Some languages are yet more complex. How to deal with this I'm actually tempted to make my proofs conditionally on a conjectured theory of equivalence for PTS, but the actual theories require nontrivial arguments, so I'm not sure how likely such a conjecture would be to hold. I'm aware (though not in detail) of the following works: 

I am looking for nice examples, where the following phenomenon occurs: (1) An algorithmic problem looks hard, if you want to solve it working from the definitions and using standard results only. (2) On the other hand, it becomes easy, if you know some (not so standard) theorems. The goal of this is to illustrate for students that learning more theorems can be useful, even for those who are outside of the theory field (such as software engineers, computer engineers etc). Here is an example: 

Instance: An undirected graph $G$ with two distinguished vertices $s\neq t$, and an integer $k\geq 2$. Question: Does there exist an $s-t$ path in $G$, such that the path touches at most $k$ vertices? (A vertex is touched by the path if the vertex is either on the path, or has a neighbor on the path.) 

Let $Q$ be a hereditary class of graphs. (Hereditary = closed with respect to taking induced subgraphs.) Let $Q_n$ denote the set of $n$-vertex graphs in $Q$. Let us say that $Q$ contains almost all graphs, if the fraction of all $n$-vertex graphs falling in $Q_n$ approaches 1, as $n\rightarrow\infty$. Question: Is it possible that a hereditary graph class $Q$ contains almost all graphs, but for every $n$ there is at least one graph that is not in $Q_n$? 

Then, to show that logical equivalence implies observational equivalence, one only need show that logical equivalence is a consistent congruence. However, the other direction requires some more work: in particular, to show that logical equivalence is a congruence, one does proceed by induction on contexts. EDIT: there's a big problem with this approach: you wouldn't get . Let be the type of vectors of naturals of length (assuming that can be defined or encoded), and consider using as a context (well, you'd need in fact a context ending in , but bear with me). Since $n + 1 = 1 + n$ is not a definitional equivalence, types and are incompatible, hence and are not observationally equivalent. 

You can "link" those types and handlers with a specialized linker. Unlike using open datatypes, you concatenate the list of constructors and the case functions. But the linker must add cases for partial application: Without whole-program analysis, you cannot predict which partial applications can be used for which function, so you add all cases. An $n$-ary function can be partially applied to $i$ arguments (with $0< i < n$) and produce a function of arity $n-i$, which can be partially applied again. 

As the answers point out, $PH=PSPACE$ would still have significant consequences, even though not as numerous and dramatic ones as $NP=PSPACE$. Turning the issue on its head, it could be viewed as "empirical evidence" to support $NP\neq PH$. After all, if $NP=PH$, then the two statements ($PH=PSPACE$ and $NP=PSPACE$) must have the same consequences. As the second hypothesis has noticeably more and stronger known consequences, that can be viewed as empirical evidence to support that the left-hand sides in the equations must be different, that is $NP\neq PH$ (which, in turn, is equivalent to $NP\neq coNP$). 

While I don't know a specific (conjectured) example in $QP-\beta P$, there is still rather compelling evidence that $\beta P$ is a proper subset of $QP$. Namely, these classes behave very differently in their relationship to $NP$: $\bullet$ It is obvious from the definition that $\beta P\subseteq NP$. $\bullet$ On the other hand, $QP\subseteq NP$ is not known, and it would be very hard to prove, since it implies $P\neq NP$. (In fact, it is an even stronger statement than $P\neq NP$.) Such a very different behavior relative to $NP$ seems to provide a fairly strong reason to believe that $\beta P\neq QP$. 

Scientists learn new knowledge for humankind, and then share what they learn through research papers. Moreover, we live in information overload, so readers only have time for the most important knowledge. 

EDIT: again, (part of) the basic idea is that pushouts act as a union with some glue. This allows defining "rewrite rules" for graphs — you match the left-hand side to the graph, and then glue the right-hand side to the (rest of) the graph in a corresponding manner. I'm afraid I can't add details because I've never gotten more than the intuition. 

Thanks for the question; I had similar questions few years ago, before starting in research (I'm not necessarily assuming that's your case). I've looked at a couple of the links, and they don't really look like research papers in form; I mostly can't really tell if their technical content could be made into a paper because I'm not an expert in the field, but I'm guessing "no". If you compare your links to a paper, you'll notice several differences. But highlighting them and their rationale might help. The key concepts are: 

Access to a $SAT$ oracle would provide a major, super-polynomial speed-up for everything in ${\bf NP}-{\bf P}$ (assuming the set is not empty). It is less clear, however, how much would $\bf P$ benefit from this oracle access. Of course, the speed-up in $\bf P$ cannot be super-polynomial, but it can still be polynomial. For example, could we find a shortest path faster with a $SAT$ oracle, than without it? How about some more sophisticated tasks, such as submodular function minimization or linear programming? Would they (or other natural problems in $\bf P$) benefit from a $SAT$ oracle? More generally, if we can pick any problem in ${\bf NP}-{\bf P}$, and use an oracle for it, then which of the problems in $\bf P$ could see a speed-up? 

Since it is easy to check whether a graph is one of the graphs allowed by the Theorem, this provides us with a polynomial-time algorithm for the decision problem. Notes: (1) The proof of the theorem is not at all easy. (2) Once we decided that two disjoint circuits exist, it seems less clear how to solve the associated search problem, that is, how to actually find such circuits. The theorem does not give direct advice to that. 

I expect the answer to be "obviously yes", but to my inexperienced eye, that's not directly obvious, because the definition of infinite Böhm-reduction does not include a transitivity rule (it wouldn't work), and because I couldn't find a relevant lemma in the papers themselves. I'm referring in particular to the definition by Czajka [1] of the relation $\rightarrow^\infty_{\beta\bot}$, called infinitary Böhm-reduction. I've looked at [2], which however does not include Böhm-reduction (such that the defined relation isn't confluent IIUC, which is a problem for me). Rationale: Defining reduction for infinitary $\lambda$-calculus is tricky. In particular, you cannot create an "infinite transitive closure" which allows an infinite number of transitivity steps, but you need to be more careful. In particular, if you define multi-step reduction coinductively, you cannot include a transitive rule, lest your relation becomes total and thus degenerate. So one ends up doing transitivity elimination, which is not always trivial; and given how unintuitive coinduction is, I'm afraid I'd fool myself when attempting a proof. [1] Łukasz Czajka, 2014. A Coinductive Confluence Proof for Infinitary Lambda-Calculus. Proc. of Rewriting and Typed Lambda Calculi, Springer. $URL$ [2] Jorg Endrullis and Andrew Polonsky, 2011. Infinitary Rewriting Coinductively. In Proc. of TYPES, volume 19 of LIPIcs, pages 16–27. Schloss Dagstuhl. $URL$ [3] Richard Kennaway, Jan Willem Klop, M. Ronan Sleep, and Fer-Jan de Vries, 1997. Infinitary lambda calculus. Theoretical Computer Science, 175(1):93–125. $URL$ 

Consider the (simple, undirected) complete graph $K_n$. Is it possible to partition its edges into less than $n$ cliques, each having less than $n$ vertices? Note: It is easy to see that $n$ cliques of size $<n$ always suffice. Take a $K_{n-1}$ as one clique, and take a $K_2$ for each of the $n-1$ edges that are not contained in the $K_{n-1}$. This gives an edge partition into $n$ cliques, each with size $<n$. The question is whether there is such a partition into less than $n$ cliques of size $<n$. 

Note that we require that the parameters are exactly equal to the given numbers, they are not just bounds. If you want to solve this from scratch, it might appear rather hard. On the other hand, if you are familiar with the following theorem (see Extremal Graph Theory by B. Bollobas), the situation becomes quite different. 

Here is an example, which appears a natural problem. Instance: Positive integers, $d_1,\ldots,d_n$ and $k$, all bounded from above by $n$. Question: Does there exist a $k$-colorable graph with degree sequence $d_1,\ldots,d_n$ ? Here the input can be described with $O(n\log n)$ bits, but the witness may require $\Omega(n^2)$ bits. Remark: I do not have a reference that this particular problem is indeed NP-complete. But the requirement of $k$-colorability could be replaced by any other NP-complete condition; the problem will likely become NP-complete for some condition, if not for this one.