The answer here depends on a lot of things, since you are using direct disks (Local RAID) and not SAN disks which are wide striped (I assume), sequential operations are faster, so the logs (tempdb included) are benefited in E: . However there can be a situation where you don't want tempdb activity (logging activity) to interfere with the logging on your other databases, this can be the case if your tempdb activity is erratic and not related to application performance, while the logging performance of your databases is directly important to your use case. I would generally test both setups and see which does better under the load you intend to use (the benchmarks could be transaction times for SQL Server, or disk queue lengths or writes\sec on the log drive). 

However, this requires a bit of code change since the table name is hardcoded in the data access layer of the application. What are the drawbacks, or potential issues if I create a new schema when a user duplicate the table, instead of dynamically generate a new table name? 

Does SQL need to rebuild the clustered index? What if this table is a heap and not a clustered index? 

Would this help speed up the delete because it doesn't have to do cache as much data, or it doesn't matter that much since I have an index on CreationDate? Thanks 

To add to ConcernedOfTunbridgeWe's comment, there is a querying performance advantage if you have a DateDimension and a TimeDimension table. Consider the fact table has both a dateDimensionKey and TimeDimensionKey. If you want to find the number of something between 8 am to 5 pm, you can simply do 

The most common cause of this is virus scanning, as virus scanners tend to lock files. Try excluding the directory from scanning. 

I imagine the unescaped character in your password is causing problems in the shell. Escape it and you should be ok. 

The figures in the view are aggregated from over a period of time (either 15 or 60 seconds). figures should closely mirror what you'd see from a tool such as , , , or , but they will never be identical as it is impossible to synchronise the performance monitoring tool data collection with the data collection performed by the Oracle process. As a result, the aggregated data in will also never perfectly match monitoring tool output, but should be pretty close. is indeed + . 

Basically, Oracle is assuming that all of the data in the source table may take up 3 bytes per character, due to characterset conversion. 

If you have index added to the TimeDimensionKey, you will only need an Index Seek for the result on your fact table to search data Without TimeDimension table, you will have to do something like the following, 

This will probably requires a table scan because the database engine need to calculate the result for every rows before it can figure out what data can be return. 

I notice the query has wait_type of PAGEIOLATCH_SH. In fact, I notice some other Select statements are having the same problem. What is PAGEIOLATCH_SH? What other information should I look for to fix to issue? Ultimately how do I solve this performance issue? Thanks 

I have a very large table (200M+ row), and it is clustered indexed. I would like to change one of the foreign key column from a bigint to smallint. My questions are: 

There's no fool-proof way, but here's a list of ideas for you: /etc/oratab: Little script to give you a list of SIDs in the : 

The clause in the DDL statement causes the actual population to be deferred until the first refresh. While Oracle parses the actual SQL used to populate the view, it does not execute it & will therefore not pick up "runtime" problems. This is easily demonstrated. Parsing error, due to not being a datatype: 

Several things. 1) Your trigger doesn't look right - I think you need the trigger to be on the table, rather than the table, and have a newly inserted/updated train have its maximum weight checked against the maximum weight for the class. 2) There is no column in the . Assuming the above is true, this is the trigger code you require: 

When a cube is "materialized", all combinations of aggregates over all dimensions are computed and stored.. the WITH CUBE operator does exactly this.. $URL$ 

Nope no native method in 2005 (there are DMVs in 2012, dm_os_volume_stats), however this "hacky" xp_cmdshell might work.. $URL$ 

You can run you query in a transaction and run sp_lock @@spid after running the query, you will know all the locks held. 

You can do this using SQL Server alerts, which can trigger a job after the WMI event occurs. Your WMI Query in this case will be 

Looks like that is a default setting in postgresql 8.5 onwards to guarantee recovery of higher precisions floats.. $URL$ 

Yes it does take a shared lock on the rows that it reads by default (it also takes an Intent Shared lock on all the pages of the clustered index that it will read), this is done to prevent dirty reads. However there are ways to bypass this (SQL Server has the nolock hint). If the statement is not in a BEGIN TRAN the lock is released after the SELECT statement has run. More info can be found here: $URL$ $URL$ 

This is happening because it's trying to create a duplicate refresh DBMS_JOB with the ID 438. Just re-run the failing statements manually and specify an unused job ID. 

.. creates a functional index on the table, that uses the function on the column . In simple terms, Oracle creates an index that pre-calculates the value of , thus making any lookups faster. The query you have posted: 

... orders the data by customer (OM01U1.OM01015) and last maintenance date (EC01_LASMTCDAT) and assigns a number (r) to each last maintenance date by customer, starting with 1. Using an outer query, we then pick just the r=1 rows to get the data you need. Note: Completely untested as I don't have your data. 

Obviously you'd be better off putting the table in an appropriate schema and granting on a per-user basis. If you need help with another approach (session tracing), let me know. 

I have a program that insert row to a table though multiple connections. For each insert, the connection would open and then close. When I run the program, the insert were fast. However, once it reach about 3000 rows or so, SQL server start slowing down. Eventually it become 30x slower than it should be. The insert statement is NOT a stored procedure, by simply dynamic sql every time when insert into the database. This is a SQL Express, so memory utilization is limited. CPU utilization is low when running the insert statements. If I kill the application and run the process again, the insert will be fast again, for the first few minutes until it become a snail again. When I ran the following SQL 

Database schema helps grouping tables, or help identify multiple tables that have the same table name. Most of the schema usage that I have found so far are for grouping tables, isolating tables among the users (or talents), allow different permission based on the schema. In my case, I have a predefined table definition, and I like my user to be able to duplicate this table multiple times in my application. Obviously, I can't have the user create the new tables with the same name. My original idea is to allow the user specify a table name suffix: 

You were nearly there... Just put a space between them (as that's how you want them to be formatted): 

Oracle 11g doesn't support the clause, though the impending 12c release is rumored to support it. Anyway, you can do this using an analytic windowing function: 

... the local time is 11:00:00, which is 16:00:00 UTC (11:00:00 + 5 hours, because it is 5 hours behind UTC). The two times and dates in your example are identical when converted to UTC - that is what the sentence in your question means. 

The latest draft SQL standard that I could find on the internet (dated 21/12/2011) has the following available for use in a query expression: 

It's utter madness and there's no justification for it. was created to represent the absence of a value & to use an actual value like -5000 is bonkers. Ordinarily I wouldn't write an answer this short, but the question deserves to be one of the most visible on dba.se & the more answers the better. 

I get an arithmetic overflow on a datetime column (error converting to smalldatetime), however the destinations schema that got created has a datetime column and not a smalldatetime column.. 

Since one car may not have more than one parking contract, it is fine to have CAR as a part of the PARKING CONTRACT TABLE CREATE TABLE PARKINGCONT_TBL(contract_id,,car_regnumber,,owner_id [FOR_KEY TO OWNER_TBL] CREATE TABLE OWNER_TBL(owner_id,) The question of adding a customer without a contract should not arise because whatever is inserting (the application) should create a contract (not a customer), so a owner is added only when a contract is created (there is no way to enforce this, since there is no dependency between owner and contract, and creating one will result in a cyclical dependency) 

Running DBCC SHRINKFILE without the TRUNCATEONLY option on the primary will propagate the change to the secondary once the log is applied, so it will shrink to the same size.. 

I wouldn't set any storage clauses, unless you have a good reason to do so. Documentation link here. 

That's the missing row. The view joins with on the signature column - there's no matching row in , which I suspect is causing the problem. 

In case you were wondering, "" is a reference to the newly inserted row, and each column can be referenced individually. Test case: 

No, you have to upgrade in stages, which wouldn't actually be possible now because Oracle 8.x and 9.x are now unsupported and the software is unavailable (unless you have an Oracle support contract and ask them for copies of the software). Read the OracleÂ® Database Upgrade Guide 11g Release 2 (11.2) for more information. Your best bet, assuming you have the 7.x database up and running, is to use to export the schemas you require, then import them into a fresh 11.2.x database.