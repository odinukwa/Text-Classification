That comment refers to Paul Randal's blog post Adventures in query tuning: unexpected key lookups. This points out that a cursor type is dynamic optimistic by default. From the Microsoft Docs, you can see that the "optimistic" part is what's causing all your problematic key lookups: 

If the SSD is slower than those other two devices, and nothing else has changed* in your setup, it's likely there is a problem with the disk itself, or with the driver being used, or the controller for the array this disk sits in, etc. *things that might have changed since you moved tempdb: 

And then: Check to see if there is an explicit maximum preventing your log file from growing If this is the case, set to a larger number to accommodate your actual transactional load. The number for this will depend on your usual number of transactions, and what recovery model you're using. Check to see if autogrowth has been disabled (growth = 0) If it is disabled, you could enable it. If you can't do that, and you're in FULL recovery model, you could schedule more frequent log backups. If you can't do that, and you're in SIMPLE recovery model, you'll need to increase the size of the log file manually until it's large enough to handle the transactions you have between automatic or indirect checkpoints. It's possible, maybe, that the disk of your Azure storage is full and that's what is preventing the file from growing (although I'd expect different error messages). 

This is far and away your best, and EASIEST, option. This is the home run. Do this. Option 2: Rebuild online Index rebuilds require a SCH-M lock. If you add to your command, that lock will be deferred until the very end of the rebuild operation, which might increase the potential that whatever is preventing your maintenance task from completing has released its locks. Option 3: Identify the blocking query This is probably the most work of the options. You can run sp_WhoIsActive while the command is running, and it should show you what else is running, and specifically it will show you what other session is blocking the command from acquiring its lock. At that point, you have a bunch of options to deal with the problem: 

There are many factors that can affect replication. Some common factors that I've come across are related to slow networking, blocking and storage issues. But the one that may be a factor is the performance problem with virtual log files. If you are initializing a database with a large amount of data and the default growth factors are in place, then sql server will grow the data 1mb at a time and the log 10 percent at a time. To check for the VLF issue, run dbcc loginfo. If that command returns over 100 records, I would be concerned. If it returns thousands of records, then it will have a real impact on performance. There are plenty of articles written on this subject. The basic fix is to adjust the autogrowth settings on all data and log files to reasonable sizes, then shrink the log file and initialize it back to the original size. I would check all of the databases, including the distribution database. There could also be other reasons why the distribution database is slow to distribute the transactions. I've experienced this on several transactional replication systems in the past and was suprised at the impact of correcting this every time. I would also suspect locking at the publisher. Identify the spids involved in the replication and use the dynamic management view sys.dm_os_waiting_tasks to determine which waits are involved in these sessions. This will help identify what these subscriptions are waiting on. 

What you are looking for is called Transactional Replication, which is very commonly used and a good solution for avoiding contention on a transactional database. There are three databases in a transactional replication system, which are the publisher, the distributor and the subscriber. The publisher is the source database, the distributor is where all changes to the publisher are sent for distribution to one or more subscribers. The distribution database can be on the same server as the publisher or a different server. When you create a publication, you choose the articles to be included, such as tables, views, procedures, etc, and whenever a change is made to any of the objects and gets recorded in the transaction log, an agent, which is an executable, reads the transaction log, packages the change and sends it to the distribution database. Then a separate agent, or executable, takes the packages from the distribution database and applies them to the subscriber. You will be setting up a publication and subscriber on servers 1,2 and 3 and have all subscriptions point to a database on server 4. There are several articles on MSDN that have good descriptions of transactional replication. 

Outside of that method, I noticed another small difference that causes the nolock version to run slower: Releasing Locks The nolock branch appears to more aggressively run the method, which you can see in this screenshot: 

NOTE: this might not be the type of answer you're looking for. But perhaps it will be helpful to other potential answerers as far as providing clues as where to start looking When I run these queries under ETW tracing (using PerfView), I get the following results: 

You absolutely do need to specify intent when making read-only queries from your application in order for the listener to know to route queries to the readable secondary. From the docs page you linked to: 

All that to say: seeing peaks and valleys in the number of pages being flushed to disk by checkpoint operations is completely normal. If the purpose of the performance testing you mentioned is to normalize (as much as possible) the number of pages flushed to disk per second, consider reading the documentation page I linked to above and adjusting those settings. 

A few things could be going on here. The files are in the wrong place, or have an unexpected extension This one seems unlikely, since you've probably been using Ola's scripts for all your backups, but just in case you've changed a setting (like the directory location, or the log file extension). The "cleanup" part of Ola's scripts are looking for backups in this location (starting with the server / folder you specified in the @Directory parameter). In your question you used so I'll continue with that: 

Checkpoints, like most things in SQL Server, are a big topic. A good place to start is this Microsoft Docs page: Database Checkpoints (SQL Server) To respond to your main question though, check out this specific quote from that page: 

If you confirm it's running, and you're still not seeing results in your file, please script out the event session in SSMS and update your question with the results. Perhaps it's configured differently by mistake, or it is writing the file to a place you don't expect. 

I have been working on a SSIS package. Package is extracting data from Access Databases , excel sheets and SQL server databases and loading into SQL Server database. Its been working fine but all of a sudden it has started throwing an error, Its not a run-time error, but whenever I open the solution in Visual Studio, the takes a couple of minutes to validate all the package components (which is expected because the package is huge) at the end of the validation it throws the following error: 

Hi I am trying to create a SQL Server Failover cluster. Windows Cluster is configured and working as expected but now when I try to install SQL Server it throws the following error: 

I have sql server 2008 R2 64bit Developer's Edition Installed on my machine. And Microsoft Office 2010 Professional 32bit. I have been trying to import some Excel data from an Excel sheet into a sql server database. I have used the following query to do this: Query 

I have a pretty complicated situation at hand, let me try to explain. I have a SQL Server and a Web Server in a separate domain (lets call it domain A). SQL Server has the databases for Reporting services, The web server has reporting services installed. The SSRS also have SSL certificates installed and we are using https protocol to connect to the reports manager. Now I need to give access to users from domain B to connect to the Reports Manager (report server on Domain A). The users from Domain B cannot have any logins in the Domain A where the reporting services are installed, I know to access the reports manager I need to add domain logins/groups to the reports manager and assign them appropriate roles to access the reports. What options do I have (if any) to give access to users from Domain B to connect to a Reports Manager on Domain A? Important Note: The access to reports server is via NLB with external facing IP, the reports server (web server) or the SQL server does not have any external facing IP 

(a in .NET classically includes the message you mentioned - "Object reference not set to an instance of an object") You should try updating to 17.6 (build 14.0.17230.0) to see if the problem is resolved there. 

The plain select is on top, at 12ms, while the nolock version is on the bottom at 26ms (14ms longer). You can also see in the "When" column that the code was executed more frequently during the sample. This may be an implementation detail of nolock, but it seems to introduce quite a bit of overhead for small samples. 

The solution you linked to overcomes the problem with connecting to multi-subnet failover groups by using the connection string setting. The way that setting deals with the problem is attempting many connections in parallel, rather than one after the other (the default behavior). This increases the chances that a connection will be made successfully before reaching the connection timeout limit. If you don't want to use that option (you mentioned not wanting the overhead of configuring ODBC connections on all your cluster servers), another solution would be to increase the (linked) server-level connection timeout setting to a value that's high enough to deal with the serial connection attempts. The default connection timeout on linked servers is 10 seconds (represented by "0" on the settings screen). You can increase the timeout to 20 seconds by doing the following: 

We can then take the difference of the two snapshots and determine how many bytes were read / written during the job. You can also use those numbers to calculate overall latency during that period. Note: a more granular approach would be to log the results of that query to a table every 5 minutes (or less if you want) 

I would highly recommend that you test your new T:\ drive using Crystal Disk Mark. Check out the guide from Brent Ozar here: How to Test Your Storage with CrystalDiskMark Compare the results from the T:\ drive with 

Note: to get the best help on this question, please include your actual RESTORE statement, and the specific error message that you're getting. When using In-Memory OLTP, SQL Server has to create a new folder named "xtp" in the root of the default file location for the SQL Server instance. This folder contains the DLLs for compiled stored procedures and other in-memory objects. You can find more details about that here: In-Memory OLTP files â€“what are they and how can I relocate them? If you've changed the location for your data files, you may need to update SQL Server's access to the file system there: Configure File System Permissions for Database Engine Access As a test / workaround, you could manually create the "xtp" folder, and then try the restore again. 

Also, when you create a symmetric key, you can specify the argument key_source, which forms the basis of creating the actual key, but if you don't the database engine will create a random key for you. The symmetric key is protected by the certificate, not a derivative of it. It would be very dangerous if the symmetric key were able to be derived from the certificate or it's private key. The Open Master Key command is redundant since it is already been opened so that the private key from the certificate can be used. I would also highly advise against using the master database for column level encryption for your user data. I hope that the above description was clear because I wanted you to understand why you are having a problem before providing the resolution. The problem is that the Service Master Key on your local SQL server instance can't decrypt the Database Master Key. You can fix this in one of three ways. Back up the SMK from production and restore it on your local SQL Server or backup the DMK for the production database and restore it on the database on your local SQL Server or move the command to open the database master key by password before the open symmetric key command. Backing up the DMK would be the better and less impactful choice because restoring an SMK could be resource intensive. I would advise one of the first two resolutions since you don't want to put passwords in your code for security reasons. 

It seems like you've been reading about Snapshot Publications and replication. There are three items in SQL Server with the name snapshot. One is the database snapshot. When a database snapshot is created, one new file per existing data file is created and the database engine will save the original page for any changed page in the database into those files. Database snapshot files are just a collection of pages since the snapshot creation and won't be useful on any other server. Another type of snapshot is a Snapshot Publication, which is part of the replication system. When you create a Snapshot Publication, you specify which articles, which can be tables, views procedures,ect, to include in the publication, specify a subscriber and schedule it for delivery. You can configure it to drop the existing articles at the subscriber and recreate them so they will be replaced every time the snapshot is pushed. It will synchronize all articles between the publisher and subscriber at the time it is run. Which brings be to the third instance of an object named snapshot. Within the replication system there is an agent or executable named the snapshot agent, whose job is to take a snapshot of the articles in the publication at the time it is executed. If you really need to transfer the entire database from one server to another on a regular basis, you can design a backup system that takes full backups once per week, for example, with differential backups daily and transaction log backups hourly for production. Then transfer the full backup once per week to the test environment and leave it there. After that you would only need to transfer the differential backup daily to restore using the full followed by the differential. I would also advise turning on backup compression to minimize the sizes of the files involved.