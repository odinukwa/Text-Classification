As stated, your approach is problematic, because if 2 bitmaps have evenly spaced differences then in any rotation, there will be differences on some high order bits. You can generalized your approach by permuting the bit position in a more complex fashion. Indeed, if you select a random permutation of bits, then all differences between 2 bitmaps with distance $5$ will appear in the 16 low-order bits with probability better than $1/50$. So repeating a few hundred times you should find a very large proportion of your bitmap pairs. For each trial, the number of pairs to test (with the same 16 high bits) is close to $64\cdot N$ (for $N\approx 2^{22}$). However, I would also try the following approach. Built a list of your bitmaps modified in at most 2 bit positions and sort this list. If there are collisions within this list, you have two bitmaps within distance $4$. Then enumerate all values of your initial bitmaps modified three positions and search them in the list to find pairs of bitmaps at distance $5$. The memory cost of this approach requires storing $529\cdot N$ elements and the number of elements to search in the second phase is $4960\cdot N$. 

The probability that $5$ differences are located in the $16$ low order bit after a random permutation of the $32$ bit-positions is just a quotient of two binomials: $$ \frac{\binom{16}{5}}{\binom{32}{5}}\approx 0.0217 $$ Construction of the lists, for each element in the original list, put in the augmented list: the element itself, all elements differing in one position and all elements differing in two positions (keeping the information about the original element). The number of copies for each element is $1+32+\binom{32}{2}=529.$ Any collision within this list (detected after sort) corresponds to two original element at distance at most $4$. Note that each pair can be detected several times so you will need to remove duplicates (but this was already the case with your initial algorithm). For the final pass, it is preferable to prune the augmented list of elements to keep only those at exact distance $2$ from their original element. Then, for each original element, create the $\binom{32}{3}=4960$ elements at distance $3$ and search them within the augmented list. Once again, you need to remove duplicates since each pair is going to be detected $\binom{5}{3}=10$ times. [With extra care, you can probably anticipate/avoid most duplicates but I am not sure whether it is worth the effort.] 

Answer to questions 1 and 2: No. This is also the answer to 3 if $M$ is required to always halt (isn't this implied by the definition of a learner, or do you mean the program produced by $M$ must always halt?). Here's a proof for 1 (2 and 3 work pretty much the same way) Suppose there's an algorithm $A$ which takes the description of a learner $N$ and outputs a learner $M$ which behaves differently. Let $N$ be a learner which runs $A$ on its own description, then runs the output $A(N) = M$ on the input text to produce a program that enumerates some set. By definition of $A$, $N$ and $M$ behave differently, but by construction they behave identically, contradicting the existence of $A$. This same argument can be pretty easily modified to show that 2 is impossible. It can also be used to show that 3 is impossible if $M$ must always halt. Not a real answer for question 4, but you might find this helpful: A prudent learner cannot converge on both every text for a finite set and on every text for the set of all natural numbers. Let $N$ be a learner that converges on every text for a finite set. Now, consider the following text for the set of all natural numbers. It contains all numbers in order, separated by differing numbers of 1s. After $i$, it contains enough 1s that $N$ will return a program which enumerates the set $1,\ldots,i$. $N$ will do this because it converges on every finite set. Because it will return a program enumerating a finite set after every prefix of this text, it does not converge on this text. So if you let $M_1$ be the learner which converges on all finite sets and $M_2$ be the learner which converges on all texts for the set of all natural numbers, at least one of $M_1,M_2$ satisfies the criteria in question 4. You can extend this to a countably infinite set of learners in which at least all but one learner satisfies the criteria in question 4 by producing a machine $M_i$ for each natural number $i$ which converges on all finite sets which don't contain powers of the $i$th prime and also converges on the set of all powers of the $i$th prime. By similar logic as above, $N$ can't converge on all sets that any two of these converge on. You can use this set of learners to find an $M$ satisfying 4 with arbitrarily high probability. 

On a course, when shift systems were being introduced, the lector said that "if the shift of symbols sequence reminds you Turing machine, then it is a very correct association": $\sigma(\ldots, x_{-1}, x_0, x_1, \ldots) = (\ldots, x_0, x_1, x_2, \ldots)$ I asked him about concretes but he gave me vague answers that seem to only refer to the general notion of universality that can be proved by reducing a system to Turing machine. So he did not confirm his suggestion, that shifting sequence of symbols is analogous to moving the tape in Turing machine. But I am confused. Can this be clarified? 

There are many universal computation systems. Turing machines, tag systems, rewrite systems, cellular automata to name just a few. The universality of a system is proved via reduction from a known universal (Turing-complete) model. I am wondering if it is possible to define abstract conditions, that make each of the systems universal. For example, in definitions of algebraic structures, there are conditions (like: Closure, Associativity, etc.) that can be examined in a structure to determine if it is e.g. a group. There is no reduction from other structure, that is already known to be group. Is it possible to define in similar way the universality? Are there any related works on the topic? 

Are there any interesting proofs on unconventional computing? When reading on the topic I most often see soft analysis â€“ like following paragraph from Wikipedia (Natural computing): 

I attached a picture, where the energy dissipation (entropy increase) on information erasure is explained. Is the explanation correct? 

"RESTORE TO ONE" - is it correct to identify the operation as "information erasure"? It looks like a negation, not like "information erasure". I would say that the erasurement would be performed, when the "ball" (the black dot near "0" in the picture) would be returned to the meta-stable point between 0 and 1. Maybe the intention of the author was to show, that "negation" has "erasurement" embedded in it? Then, he states that "computing depends on information erasure" - this statement is too strong? - we have reversible computation that do not erase information, correct? The picture and point 2. (above) seem to be a very good metaphore of Landauer's principle. Does it match Landauer's principle closely, or maybe even perfectly - or is it just a picture to help only conceptually understand the principle? 

Let $k$ be the finite number of agents you're allowed to choose. If there are no critical services, finding the set of $k$ agents that maximize the number of non-critical services provided is equivalent to Max-$k$-Cover, which can't be approximated better than $1-1/e$ unless P=NP. So for an exact solution, you'll have to use something capable of solving NP-hard problems. If you'd be happy with an approximation, you're in luck. The constraint that you have to choose $k$ agents such that there are sufficiently many of them providing each service is a matroid constraint. This paper shows how to achieve a $1-1/e$ approximation on set cover problems under a matroid constraint. If you want something simpler to implement, the matroid greedy algorithm gives a $1/2$ approximation on submodular set functions. So you just need to repeatedly choose the agent that provides the most new non-critical services, after at each step removing from consideration any agent whose choice would prevent you from providing enough of each critical service before reaching $k$ agents. If each agent provides unique non-critical services, you're just maximizing a weight function over a matroid, so the greedy algorithm gives an exact solution. 

Here's an answer to your second question. Consider a learner $M$ which always produces a machine that enumerates every natural number. This converges on any text which contains every natural number. In the next paragraph, we will show that there is a one-to-one mapping from the set of all texts to the texts on which $M$ converges. Therefore, $M$ converges on as many texts as there are. It is more powerful than any text which converges on a set of smaller cardinality, and no learner can converge on more sets. So either $M$ is more powerful than $N$, or no learner is. Let $T$ be a text. Consider the representation $T_b$ of $T$ in binary. We match this with a text $f(T)$ which we construct as follows. If bit $i$ of $T_b$ is equal to 0, then positions $2i-1$ and $2i$ of $f(T)$ are $2i-1$ and $2i$, in order. If bit $i$ is 0, then positions $2i-1$ and $2i$ of $f(T)$ are $2i$ and $2i-1$, reversing the order. For example, if $T_b$ begins $01100$, then $f(T)$ begins $1,2,4,3,6,5,7,8,9,10$. If two texts $T,T'$ are such that $f(T) = f(T')$, then they have identical binary representations, so $T=T'$. Thus, this mapping is one-to-one, so the cardinality of the set of texts on which $M$ converges is the same as that of the set of all texts.