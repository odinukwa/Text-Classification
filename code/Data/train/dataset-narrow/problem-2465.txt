If we are asking for a simple cycle the problem is NP-complete by a reduction from the Hamiltonian cycle problem. We want to find a Hamiltonian cycle in a graph $G$. We assign weight 1 to all edges of $G$, and add to this graph all the other edges with weight $\infty$. We have thus created a clique, which is obviously Hamiltonian, and we ask whether there exist a cycle with weight $n$ in it. 

General TSP is not approximable within any factor, so your algorithm doesn't really have a chance of working well. A worst-case example: We select one Hamiltonian cycle in the clique and give all its edges weight 1 (assume this cycle is $(v_1, v_2, ..., v_n, v_1)$. All the other edges have some big weight $\alpha(n)$. Now let's have your algorithm start with odd vertices. Between any odd vertices there are only edges with weight $\alpha(n)$ and we don't have any tie breaking rule, so the cycle on odd vertices your algorithm constructs can be totally arbitrary (we can for egample forbid it to contain any edges of the form $(v_i, v_{i+2})$. When the algorithm starts working on the even vertices, it will have just two possible places to put $v_i$: after $v_{i-1}$ or before $v_{i+1}$. Again we have no tie-breaking rule, so we can assume that it always choses to put $v_i$ after $v_{i-1}$ (of course we do all this modulo $n$). When we look at the constructed cycle, we see that it is built of alternating weight 1 and weight $\alpha(n)$ edges. So we have $OPT = n$, and our solution has weight ${n \over 2} + {\alpha(n) \over 2}$. We can chose $\alpha$ arbitrarily to disprove any claimed approximation factor. There are some technicalities involved with encoding weight $\alpha$; when we increase $\alpha$ the input size also grows (but more slowly), but still I think we can get any factor. 

Usually I give the factoring problem as example; I first ask for the number that divide 15; usually people can answer 3, 5, and have fun wondering if 1 and 15 are correct answer. Then I give a huge number (more than 10 digits) and ask if they can tell me what are the dividers; and I explain that, even for computer scientist, this is a really hard question. Then if I have time, I try to explain that the question is either to figure out how to solve this problem, or to prove that it will always take a lot of time( a notion that we precisely know how to define). And then a little word of cryptography, to explain why it is usedd, and a word about how many time it take team of scientist to break the key of number with hundreds of digit (I avoid to speak of bits because people seems to better know what a digit is) 

Finally, I will introduce the real structure I'm interested in, which is more complicated and probably less usual. A partial preorder $P$ can be seen as partial function from $[1,r]^2$ to $\{<,>,=\}$, where $i=_P j$ if $i<j$ and $j<i$ in $P$ and $f(i,j)$ is undefined if neither $i<j$ nor $i>j$ in $P$. We can associate to $P$ a subset of $\mathbb N^r$ called $\mathbb N_P$ defined by $(x_1,\dots,x_r)\in \mathbb N_P$ if for all $i<_Pj$ $x_i<x_j$ and for all $i=_Pj$, $x_i=x_j$. The real data structure I must study is the structure of partial function from $[1,r]^2$ to $\{<,\le,>,\ge,=,\not=\}$. I will call this structure "extended preorder". Let $P$ be an extended preorder, where $(x_1,\dots,x_r)\in \mathbb N_P$ if for all $i<_Pj$ $x_i<x_j$ and for all $i=_Pj$, $x_i=x_j$, for all $i\le j$, $x_i\le x_j$, and for all $i\not_P=j$, $x_i\not=x_j$. I say that a total preorder $T$ is included in an extended preorder $P$ if $i\le_Pj$ implies $i<_Tj$ or $i=_Tj$, and $i\not=_Pj$ imply $i<_Tj$ or $j<_Ti$. The operation I must have are still the same, having a set $S$ of extended preorder, verify if an extended preorder is incompatible with every extended preorder of the set, and verify that every total preorder is included in an extended preorder of the set. Or to say it another way, $(\mathbb N_P)_{P\in S}$ is a partition of $\mathbb N^r$. 

Conditioned on the event that the graph obtained from the sample is regular, the probability of the property being satisfied is very high (something like $1 - 1/n^c$) Given any graph G and any other graph G' that can be obtained by removing edges from G. If G satisfies the property then so does G'. 

I am trying to understand exactly what the lower bounds for the query complexity of statistical algorithms imply for convex relaxations for the planted clique problem ? A recent paper by Feldman, Perkins and Vempala builds the connection between Statistical Algorithms and Convex Relaxations by showing that for detecting planted k-CSPs, lower bounds on the complexity of Statistical Algorithms can be translated to lower bounds on the dimension of canonical convex relaxations. I was wondering if a similar thing can be said about Planted Clique lower bounds for Statistical Algorithms (which were proven here). In particular what kind (if any) of lower bounds on Convex Relaxations do these imply? 

Let $f:\{-1,1\}^n \rightarrow \{-1,1\}$ be any boolean function. Let $Maj_n$ represent the majority function. Let $\langle f,g \rangle = E[f(x)g(x)]$ and $\mathcal{I}(f) = E_x[\# i, s.t. f(x)\neq f(x \oplus e_i)]$ be the total influence of a function under the standard definitions. Then does the following identity always hold? $$ \langle f,Maj_n \rangle \leq c*\frac{\mathcal{I}(f)}{\mathcal{I}(Maj_n)} $$ where c is some universal constant. I believe it might be true but have not been able to prove it. I have tried some standard functions and they seem to work. I may be mistaken as I am very new to this area. Could you please suggest a way to prove it ? or some references regarding this Thanks in advance 

I think it can be compressed to linear space. I assume the machine $M$ has one tape. Initially the first $n$ cells contain the input, which is followed by $2^O(n)$ zeroes. $M$ is oblivious and always makes $k$ passes through the tape (from left end to right and back), and then accepts or rejects. Let $p_i$ denote the state the machine enters after reading the $n$-th cell on $i$-th pass during the left-right phase, and let $q_i$ denote the state the machine enters after reading the $n+1$-th cell on $i$-th pass during the right-left phase. Note that $q_1$ is a function of $p_1$ alone, and generally $q_i$ depends only on the sequence $p_1, p_2, \dots, p_i$ (this sequence has length at most $k$, which is fixed). Therefore we can always compute the state $q_i$ without actually continuing the pass through the extra scratch space - the transitions can be hardcoded into the description of the machine. 

Basically, $M$ will for all $i \in 1, 2, ...$ simulate $R$ on input $x$ and on every string from ${{0,1}}^i$ as a prefix of the string on $R$'s random tape. Now: 

If I understand correctly, you just want a definition of the logic; then you can find a definition of MSO$_j$ (where 2 is a special case) by example in ''Elements of Finite Model Theory'' of Leonid Libkin. Even if you are interested in infinite model, the definition of the logic is the same. Quickly is just the set of formulae with existantial quantification over sets of the elements of the universe, then existantial quantifications over those sets; then a first-order formula. 

NSPACE(0)P=RE wich I guess is tad bit absurd. Indeed, let L be a language recursively enumerable, M a TM who recognise L and M′ a TM that read an input and a number n of "1" and then simulates M for this input on n steps. Then without using any space I could copy the input on the oracle tape, guess the number of 1 needed and query M′. Then, M' will accept iff M accept and have an input big enough to be polynomial. 

I have a question that seems to me really natural and have probably already been studied. But keyword search on this site or google does not seems to help me to find any relevent paper. I have got a finite non deterministic automaton $A$ over an alphabet $\alpha$ without epsilon-transition. What can I tell about the number of different path the automaton could take for accepting a word ? In particular, I want to know if this number is bounded, or if for every $c$ I can find a word $w_c$ that is accepted in at least $c$ different way by the automaton. Right now, I can find some necessary, and some sufficient condition, but not any necessary and sufficient condition, for the number to be unbounded By clarity, I'll define the way I cound the number of accepting path. Let $w\in\alpha^*$ and $q$ a state, I can define the number of path to $q$ by inuction on $|w|$ by $N(\epsilon,q)=1$ if $q\in I$ else $0$, where $I$ is the set of initial state and $F$ of final state. $N(ws,q)=\sum_{q'\in Q\atop \delta(q',s)=q}N(w,q')$. Then the number of path is $\sum_{q \in F}N(w,q)$. 

OK, so your goal is to show that $CLASS_1[g(f(n))] = CLASS_2[h(f(n))]$ basing on $CLASS_1[g(n)] = CLASS_2[h(n)]$ (we don't specify what exactly are this classes, we just know that they're somehow parametrized with the input size). We have a language $L \in CLASS_1[g(f(n))]$, decided by some algorithm $A$. Now we make a language $L'$ by padding each word in $x \in L$, so that it's length is now $f(n)$, and we see that it is contained in $CLASS_1[g(n)]$ (our new algorithm $A'$ basically just ignores the added zeroes and runs $A$ on the real, short input). What we do is: we take a language from the bigger class and we pad it, so that it can be solved by a weaker algorithm giving us containment in the smaller class - the weaker algorithm can do it, because it has the same amount of 'real work' to do as before, but it has its restrictions (being a function of the input length) lifted by extending the input. Now we know that $L' \in CLASS_1[g(n)]$ and hence $L' \in CLASS_2[h(n)]$ (decided by some algorithm $B'$). We would like to get from here to $L \in CLASS_2[h(f(n))]$. But that is straightforward - algorithm $B$ deciding $L$ just pads the input accordingly and runs $B'$ on the padded input. This step may be summarized as follows: we want to decide $L$ in the bigger, more resourceful class. Using our extra resources we pad the input and run the algorithm deciding the padded language. Of course there are some technical detailes involved here (f.e. we have to make sure that the padding can be implemented in the classes we consider) but I just ignore them to give the general intuition. 

${\Sigma_2^P}^{NP}$ is the set of language decided by an alternating turing machine in existential, and then universal state, with an oracle in NP. Both the universal and the existantial part can querye NP. Hence, in this case you decided to write this as $(NP^{NP})^{A}$ then the way you should think of it is as $(NP^{NP^A\cup A})$ (by $\cup$ I mean an oracle either to $A$ or to an $NP^A$ language). Hence ${\Sigma_2^P}^{NP}$ is equal to $(NP^{(NP^{NP})})^{NP}$ which is certainly equal to $(NP^{NP^{NP}})$ since every query you could make to the $NP$ oracle, you could make it to the $NP^{NP}$ oracle. 

To answer to your comment, I guess I should make another answer, speaking only on Krom and Horn (May be I should ask a question about those to CSTheory) I suggest that you read section 5.3 page 34 of my paper about the problem I met on Horn and Krom in High Order logic. You will meet the same problem in Variable Order (which is clearly a superset of High Order). I don't know if you did pay attention to it, but SO(krom) is equal to P when the first order is universal; indeed you can express NP-complete problem if you add existantial first order variable. (I don't remember the example I had before, I can try to search it if you want it) I don't know what this syntactical resctriction would become for high order or variable order logic... my point is just that you should also think of a good way to restrain quantifiers, because restraining the quantifier-free part alone is not usefull (at least for Krom formulae) 

I am asking the question on a slightly abstract level and it may depend on the specifics but it would be great to have related references or ideas. Consider the random graph model $G_{n,p}$ where its a random graph on n vertices and each edge is selected with probability $p$. I want to prove that a certain property $P$ is satisfied by graphs in this model with high probability ($\rightarrow 1$ as $n \rightarrow \infty$). I know/can prove the following two facts 

Can the above two statements be enough to make a general statement about the probability of the satisfaction of the property in $G_{n,p}$ Any references would be very helpful? 

I am interested in the following problem which seems like an extension of the Kruskal-Katona Theorem. Let $A_k \subseteq \{0,1\}^n$ be a subset of the hypercube such that every element in $A$ has exactly $k$ ones. For any element $x \in \{0,1\}^n$ let $N_l(x)$ be the set of elements obtained by flipping one of the 1's in x to 0. (Generally referred to as the lower shadow of X) Let the majority upper shadow of $A_k$ referred to as $M_u(A_k)$ be the set such that for each $a \in M_u(A_k)$ number of ones in $a = k+1$ and $|N_l(a) \cap A_k| \geq (\frac{k+1}{2})$. That is more than half of a's neighbours are present in $A_k$. Given the size of $A_k$ can we put an upper bound on the size of $M_u(A_k)$. Has this problem been studied and are there results are relevant to the above. Note that in case $|A_k| = \binom{n}{k}$ we of course have that $|M_u(A_k)|=\binom{n}{k+1}$. In general I am looking at the size of $A_k$ to be $\epsilon\cdot \binom{n}{k}$ where $\epsilon$ is a small constant. Could you also refer to me a good survey of the Kruskal-Katona Theorem in general , one that surveys recent results in this setting ? Thanks in advance 

edit: I just realized some of the things I wrote were total nonsense, sorry for that. Now I changed the proof and made the definition of probabilistic machine I am using more precise. I don't know whether I get right your definition of probabilistic Turing machine: it is a machine with an additional tape on which an infinite incompressible string is written, and beside that it acts just like a deterministic machine? If we fix the incompressible string, the class we get doesn't seem to be interesting. I think we can define a probabilistic Turing machine in several ways. I will use a definition that seems quite natural (and for which my proof works ;) Let's define a probabilistic machine like that: it gets an additional tape on which some infinite string is written, we say that this machine decides a language $L$ if for every $x \in L$ it halts and accepts with probability $>\frac{1}{2}$, when the probability is taken over those additional random strings, and for every $x \not \in L$ it halts and rejects with probability $>\frac{1}{2}$. We will now show that if there exists such a probabilistic machine $P$ that solves the halting problem for the deterministic machines, we could use it to build a deterministic machine $H$ that solves the halting problem for the deterministic machines - and we know that such a machine cannot exist. Assume such $P$ exists. We can construct a deterministic machine $M$ that takes as an input a probabilistic machine $R$ with some input $x$, which