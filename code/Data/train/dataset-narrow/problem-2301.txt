Are there efficient graph partitioning algorithms with guaranteed upper bounds on the bipartition width in terms of the total number of vertices of the graph, or another non-spectral quantity (diameter, girth, ...), for regular graphs? Most of the bounds I find in the literature are spectral (like Cheeger inequalities and other bounds on the algebraic connectivity). One could combine these bounds with bounds on the spectrum itself, such as the one in this article, to bound something like the sparsest cut, but this approach provides no guarantees with respect to the actual width of the cut (or maybe it does and it is just not clear to me?). A (loosely) related upper bound is that the bisection width of cubic graphs with $n$ vertices is at most $(\frac16+\epsilon)n, \epsilon>0$ (from this paper). Are there similar worst-case bounds for heuristic bipartition algorithms? 

The classic reduction from 3SAT to PLANAR-3SAT requires a removal of $O(m^2)$ crossings from a rectilinear representation of 3SAT with $m$ clauses. However, the crossing number inequality suggests that it may not be necessary to have that many crossings. Is there a way to reduce 3SAT to PLANAR-3SAT in time $O(m^{2-\varepsilon})$ with $\varepsilon>0$ (or less) for subclasses of 3SAT (e.g., read-(at most)-$r$ 3SAT with bounded $r$)? 

We can see this as an instance of communication complexity having some influence. The idea that two observers must communicate implicitly assumes some constraint which a physicist would call no signaling. Turning this idea around, what types of correlations are possible between two measurement devices constrained by no signaling? This is what Popescu & Rohrlich study. They showed that this set of allowable correlations is strictly larger than those allowed by quantum mechanics, which are in turn strictly larger than those allowed by classical physics. The question then presents itself, what makes the set of quantum correlations the "right" set of correlations, and not those allowed by no signaling? To address this question, let's make the bare-bones assumption that there exist functions for which the communication complexity is non-trivial. Here non-trivial just means that to jointly compute a boolean function f(x,y), it takes more than just a single bit (2). Well surprisingly, even this very weak complexity-theoretic assumption is sufficient to restrict the space of allowable correlations. 

Consider the NP-complete problem, for $k>2$, of $k$-coloring a graph: Either the graph is not $k$ colorable, or it has at least $k!$ distinct $k$-colorings, simply due to permutation symmetry of the $k$ colors. For $k=2$, the problem matches your specifications: 0 or 1 accepting paths (the latter of which will never occur) will not convince the verifier, while 2 solutions (or multiples thereof) will. 

To turn the question around, one could equivalently ask: "How large must my prime be in order to be secure until year X against the resource plausibly available to a potent (e.g. government-funded) attacker?" This question is being regularly reassessed in various studies yielding recommended key lengths for crypto systems. A regularly updated web page summarizing these reports can be found here. 

We all know the PCP Theorem. Is there any software package availalbe taking a CNF in e.g. DIMACS format as input, and producing a PCP encoding in the same format as output? It might be interesting to run existing SAT solvers on such encoded instances and compare the performance relative to plain instances. 

If you'll allow me to generalize a tiny bit... Let's extend the question and ask for other complexity-theoretic hardness assumptions and their consequences for scientific experiments. (I'll focus on physics.) Recently there was a rather successful program to try to understand the set of allowable correlations between two measurement devices which, while spatially separated, perform a measurement on a (possibly non-locally correlated) physical system (1). Under this and similar setups, one can use the assumptions about the hardness of communication complexity to derive tight bounds which reproduce the allowable correlations for quantum mechanics. To give you a flavor, let me describe an earlier result in this regard. A Popescu-Rohrlich box (or PR box) is an imaginary device which reproduces correlations between the measurement devices which are consistent with the principle that no information can travel faster than light (called the principle of no signaling). 

or a forward literature search from the two other papers that I cited. This also raises the interesting question about why the communication setting seems much more amenable to analysis than the computation setting. Perhaps that could be the subject of another posted question on cstheory. 

The classical theory of fault tolerance was pioneered by John von Neumann. I think this is the original reference: von Neumann, J. (1956). "Probabilistic Logics and Synthesis of Reliable Organisms from Unreliable Components", in Automata Studies, eds. C. Shannon and J. McCarthy, Princeton University Press, pp. 43–98 

Circuits made out of general linear operators are $PP$-complete. See the PostBQP paper by Scott Aaronson or Schuch's paper on the computational power of PEPS and tensor network contraction. 

There is a published paper by Laszlo Babai (1990) in the form of a fable about Arthur and Merlin describing the dramatic sequence of events leading the community up to the IP=PSPACE result in 1989, which was very much unbelievable just a year earlier. 

The paper "BQP and the Polynomial Hierarchy" by Scott Aaronson directly addresses your question. If P=NP, then PH would collapse. If furthermore BQP were in PH, then no quantum speed-up would be possible in that case. On the other hand, Aaronson gives evidence for a problem with quantum speedup outside PH, thus such a speed-up would survive a collapse of PH. 

Which natural (well studied) classes of graphs have treewidth that scales as $\Theta(n^\alpha)$ in the number $n$ of vertices, with $1/2 < \alpha < 1$? 

3-regular bipartite planar graphs appear in a variety of NP- / #P-complete problems. Suppose one wants to test the complexity of these problems via numerical experiments. Is there an efficient way to generate random planar cubic bipartite graphs? I only know of algorithms that can efficiently generate random graphs with only subsets of the set of properties {cubic,planar,bipartite}, and it seems that generating those and then naively testing for the remainder of the properties would be terribly inefficient if one wants graphs with >100 vertices. 

Are there any libraries of #2-SAT instances that are hard to solve with state-of-the-art exact solvers (sharpSAT, cachet, ...)? Alternatively: are there practical ways to generate hard #2-SAT instances? I have only been able to find benchmark instances for #k-SAT for $k\ge3$. 

This paper defines the problem $\mathrm{B{\scriptsize OUNDED} \ D{\scriptsize EGREE}\ C{\scriptsize ONTRACTION}}$ as follows: Instance: A graph $G$ and two integers $d$ and $k$. Question: Is there a graph $H\in \mathcal{H}_{\le d}$, where $\mathcal{H}_{\le d}$ denotes the class of graphs that have maximum degree at most $d$, such that $G$ is $k$-contractible to $H$? Theorem 3 of that work states that $\mathrm{B{\scriptsize OUNDED} \ D{\scriptsize EGREE}\ C{\scriptsize ONTRACTION}}$ is NP-complete for any fixed $d\ge2$. I am interested in the similar problem, which one might call $\mathrm{B{\scriptsize OUNDED}\ D{\scriptsize EGREE}\ F{\scriptsize ULL}\ C{\scriptsize ONTRACTION}}$: Instance: A graph $G$ and an integer $d$. Question: Can $G$ be fully contracted (reduced to a single vertex) ensuring that $H\in \mathcal{H}_{\le d}$, for every intermediate graph $H$ generated in the contraction sequence? Has this problem ever been studied? Is it NP-complete in general for connected graphs? Is it easier for any well-studied classes of graphs (for which the decision is not immediately obvious, e.g., cycles)? 

You might be interested in Scott Aaronson's recent paper Why Philosophers Should Care About Computational Complexity, to appear in Computability: Gödel, Turing, Church, and Beyond, edited by B. J. Copeland, C. Posy, and O. Shagrir, MIT Press, 2012. ECCC TR11-108, arXiv:1108.1791. 

In arXiv:quant-ph/0409035v2 Buhrman and Spalek present a quantum algorithm beating the Coppersmith-Winograd algorithm in cases where the output matrix has few nonzero entries. Update: There is also a slightly improved quantum algorithm by Dörn and Thierauf. Update: There is an improved quantum algorithm by Le Gall beating Burhman and Spalek in general. 

On the second question: $URL$ gives a classical QMA-complete eigenvalue problem related Markov chains. 

You might be interested in Scott Aaronson's talk "Has There Been Progress on the P vs. NP Question?" (earlier version) 

Physics models reality with theories that define a concept of time-dependent state associated to a system and a time-evolution operator that describes how this state evolves. As soon as you find a physical system that (after some state-space discretization) implements the state space of your Turing machine, and that features interaction terms that implement (maybe after some time discretization) the time evolution according to the state transition table of your Turing machine on its state space, you have found a Turing-complete physical model of your system. Thus you can arguable say, your system "is" Turing-complete. When looking at quantum computing, you will find discussions of implications physical theories have on the Turing model of computation. For example, physical theories have to be reversible. A property, which is not shared by ordinary Turing machines. Yet there is no loss in generality, as any Turing machine can be simulated by a reversible one, with some overhead that can trade-off time vs. space, etc. 

(1) Take for example the experiments measuring something known as the CHSH inequality (a type of Bell inequality), where the physical system consists of two entangled photons, and the measurements are polarization measurements on the individual photons at two spatially distant locations. (2) This single bit is necessary whenever f(x,y) actually depends on both x and y, since sending zero bits would violate no signaling. 

Note that a weaker result was already proven in the Ph.D. thesis of Wim van Dam. What Brassard et al. prove is that having access to PR boxes, even ones which are faulty and only produce the correct correlation some of the time, enables one to completely trivialize communication complexity. In this world, every two-variable Boolean function can be jointly computed by transmitting only a single bit. This seems pretty absurd, so let's look at it conversely. We can take the non-triviality of communication complexity as an axiom, and this allows us to derive the fact that we don't observe certain stronger-than-quantum correlations in our experiments. This program using communication complexity has been surprisingly successful, perhaps much more so than the corresponding one for computational complexity. The papers above are really just the tip of the iceberg. A good place to begin further reading is this review: