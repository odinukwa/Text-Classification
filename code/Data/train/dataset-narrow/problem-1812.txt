to Log into the remote system and type xclock &. This starts a X clock program that can be used for testing the forwarding connection. If the X clock window is displayed properly, you have X11 forwarding working. 

I ssh on remote host but terminal performance is poor. Symbols I am typing are not shown immediately, but with some delay. Sometimes two symbols are shown at one time after delay. 

Maybe some program is locking this file? It can be another copy of your program. Does show anything? 

Yes, it have. You should look at flag in your . Here is the output from my system that uses this processor (two lines are for two cores): 

This command is available in most modern Linux distributions. It fills your disk with zeros 10 times. Information on the disk can be partially recovered only using very expensive techniques, that buyer of the old netbook, I think, doesn't posses. 

I've 2 remote HP Proliant DL180 G6 servers with HP Integrated Lights Out enabled on both and I have to install RHEL5 on these servers. I try to do it with ILO - mounted ISO file from local hard drive as Virtual CD-ROM, and performed installation from ILO console. OS installation on the first server was successful. But I have to make 3 tries to achive this. The channel between me and the servers is too slow - they are in datacenter. The installation takes a long time - 5-6 hours. So sometimes Virtual CD accidently unmounts and installation hangs. How can I perform the installation on the other server in more convinient way? 

I think, you have Samba problems. Be sure that you export your share in and it has proper permissions (for debug purposes I recommend you to make it public). Also you should check and running: 

I have HAProxy 1.5 running on Ubuntu 14.04 (modified). It accepts connections on http and https ports. Two backend applications process requests using persistent connection. When I create around 2200 client connections haproxy stops accepting additional connections. But I want this system to accept at least 10K simultaneous connections. Here is connection statistics: 

You need user account on the server (login/password may be different, than for your ftp-account). This user must be or have permissions to run . 

I think you have some kind of MTU problem. It happens when you send packet larger then minimum MTU on the network path (PMTU) with bit set, and ICMP error message is somewhere blocked. You should check local and remote firewalls to allow ICMP first. Then trace path to see what the PMTU is and where package drop can occure. Turn on bit! You should do this on the same port you use for client-server communications. Use hping2, for example. If nothing helped, turn off PMTU discovery on both machines. 

This asks foo.bar 100 times for URI /test with fixed rate 10 connection per second. If you want to ask different URI's, run many httperf instances in parallel mode with different --uri parameters. 

You should place it somewhere outside webserver root, for example. It has security considerations: webserver code must not be exposed to uploaded code, and it can happen, if you mix these two things and set wrong permissions. And you keep your content between upgrades. If users upload small files, the best way is to store them in database. 

While installing package I get some additional packages on dependency. But when I remove installed package with yum, the dependency packages are not removed, though I don't need them anymore. How can I remove them automatically? 

I think, this potentional issue is called load-balancing router. It routes your request sometimes to the 1st interface and sometimes to the 2nd. They have different latency. For example, we have optics on wan1 and ADSL on wan2. For one particular server ping through wan1 was 17 ms and through ADSL - 112 ms. 

I want to figure out some problems I faced performing the task. First, I had to mount .iso from Settings menu and put the tic "Connect on power on" - it was unchecked by default, and VM tried to load from net. Then, ESXi 4.0 has no clone fascility. You need to manually copy all the files from folder VM1 to VM2, cd VM2, right mouse click on .vmx file - 'Add to invertory'. Change the hostname and IP of the second VM - its all! 

I've loaded this files to document root and opened in Firefox 3.5. I see the first frame, see controls, but can't play video. This video plays good even from my server. How should I encode video to view it in browser? UPD: If I start playing video from the middle, everything works fine. 

We've been moving servers to EC2 lately and I ran into an issue recently involving locales. We use a script to build an AMI from scratch that is largely based on a simplified RightScale script. However, we recently worked on an international project and I discovered that the locale was not set during the scripted install (issuing locale at the command line results in posix). It appear there is no i18n file by default. However, checking a development server that I installed locally (via GUI) the i18n file exists. What package(s) do I need to install and which program can I run (command line) to configure this during the scripted install? We're running the current version of CentOS. (5.4) 

Are there any ntpd messages in /var/log/messages? As I was dealing with my EC2 NTP issues I noticed that NTP didn't like being too far out of sync and wanted a manual update. Perhaps you're too far out for it to decide it will update for you. 

I'm working on getting some servers running in the EC2 environment and I'm noticing some errors with ntpd trying to sync (using CentOS). I was reading on this site and the impression I get is that I don't need to run ntpd since EC2 is Xen and the host takes care of the time for the virtual servers. $URL$ Is this accurate or do I need to figure out how to get around the error I'm having? cap_set_proc() failed to drop root privileges It looks like it involves building a new kernel and other stuff I'd rather not do if I don't have to. 

I'm wanting SNI on my CentOS 5.5 dev server and since it looks like I'm out of luck with the current repo versions of openssl and apache, before I go compiling custom RPMs I thought I'd see if I could get it working with gnutls. Anyone know how to do this? According to yum, gnutls is already installed but I don't see it in my apache modules directory. 

You can also add ports to the ProxyPass directive if you don't want your friend to change firewall rules, service setting etc. Or you can even be a reverse proxy for another external IP address too. 

For the two sites on the same box, it depends on how they're setup. If they're on different ports, define the ports, if they're paths, then you'll need to add the path on the end of the ProxyPass and ProxyPassReverse entries: 

I'm trying to write an install script for my Amazon servers and I'm getting stuck on some environment variable issues. I have a set of scripts to configure things and some of them depend on environment variables that I create in profile.d scripts. I create the profile.d script (or copy it over) and need to use the variables it sets in scripts that run later (without logging out and back in). Is there a way to load these (in a script) so future scripts take advantage of them? In the script after I create the file I tried: source /etc/profile.d/scriptname.sh and . /etc/profile.d/scriptname.sh but it only sets the environment variable for the duration of the currently running script, so any other script that gets run later can't use the values being set. How do I get them to get set for the session instead of the script? I have one master script that calls a series of small scripts to do all the configurations. 

For an internal network, if you don't want to pay for the wildcard cert (or dedicated cert for the server), simply create a self signed certificate. Since it's your company, it shouldn't be too much of an issue for the users to accept the certificate. And depending on how your network works (and your admin friendliness), you might even be able to push it to the users so they won't be bothered. If you're dealing with any sort of sensitive information, you should be doing it over HTTPS. 

For what you're doing, you're probably fine running on regular hardware. But it's always worth learning about things. As mentioned, server hardware is really designed for reliability. You get things like ECC RAM, dual or quad CPU sockets, redundant power supplies and fans, RAID for your hard drives among other things. You can also generally build "bigger" boxes than you can with "standard" hardware which is beneficial for maintenance (fewer boxes) and solutions like Virtualization (pretends to be more boxes). Google doesn't do that sort of thing and takes the approach I think you'd be comfortable with, use cheaper hardware and replace it when it fails. Google's redundancy and scaling is horizontally (more boxes). The advantage here is it's cheaper to buy at the cost of complexity of overall architecture. Applications need to be designed differently and how the overall system functions can get more complicated. You need to add load balancers and such which may or may not be possible. Assuming downtime isn't a huge concern of yours, I'd say stick with the low cost hardware, backup things properly and replace stuff if/when you need to. 

I think you should start of monitoring your memory usage/cpu usage and then monitor your server network activity. Try this tutorial. It describes the main linux monitoring tools that will help you. 

I need to monitor a single process (e.g. be warned when there are more than 3000 connections established) and collect statistics on it (e.g. to determine how many connections were established today 01:20 AM, when the server worked too slow, as client said). What tools should I use? 

You need to use non-standard ssh-ports for your VMs. I use openVZ on my server and have three virtual machines with VIDs 101,102,201. I thought, that it would be convinient to ssh into 101 VM through 101 port, into 102 through 102 and so on. To connect to the hardware node you can use 22 port. So I added this routing rules to iptables on hardware node: 

Account will expire at the end of 2009-07-03. Logged in user won't be kicked out. He won't be able to log in after he logout. 

I have a 500 GB hard drive with one NTFS-partition on it. I can mount it with Ubuntu and view the contents. But when I try to copy something, I get an I/O error. Ok, I tried to make its image with . I/O error as soon as it starts. I have installed , but its manual page says not to use it with drives, failing on I/O. Can I manage to get some information from this drive and how to do this? 

First, you must be sure, that there is your ISP fault. Try utility. It is combination of and which can provide you more information about latency bottlenecks. From its output you will find out, where the problem occurs. Simple usage example: 

You can login to EC2 instance with the same access key you use for default user. Suppose, you are running Ubuntu and default username is . Then your public key is located in Create new user as described in AWS documentation and add public key to its . Now you should be able to login to you instance with original key pair. 

In this case you will be asked for password which travel the network through encrypted ssh-channel and it will be not displayed in console. 

I have VMWare ESXi 4.0 preinstalled on the server with IP A. I also can use IPs B and C. I have KVM access to this server and I can connect to it using vSphere Client. CentOS iso-image isn't on the server. My task is to create two virtual machines on this host and install CentOS 5.4 on both. Then I have to configure network, so guest1 is accessible on IP B and guest2 is accessible on IP C. Can anyone provide me a brief howto on this topic? I tried to find it, but found nothing. 

I created , create and other directories in it, copied libraries, bash and perl binaries in proper places. Also I placed my script into . Now I can run the script such a way: 

I have hosts A,B and C. From host A I can access through ssh only B. From B I can access C. I want to be able to run X11 programs on C and forward display to A. I tried this: 

I want to connect to VPN every time goes up. I have already a script to do connect to VPN. How can I run it on interface up event on Fedora Core 12? 

I want maximum information in minimum letters. First, to be able to get information from SMS. Second, to read only the headers of e-mail messages in 90% cases. For example, the previous message can be just "" where H stands for . If this server is down, I would like to get such a message: . Instead of message, that "Swap usage on server is CRITICAL" I would like to get "[!] S: server/swap usage is >50%" where 50 was taken from the nagios config for check_swap, not hardcoded into the message. And if "Router/wan2 is CRITICAL", I want to see "ADSL channel is off". So, to summarize, I want to customize message for every service and its state separately, with ability to use plugin parameters in text. How can I achieve this? 

I want all the files in directory be owned by nginx.devel. I have performed once and update these files using . But if I create a new file and then it, it will be owned by user.user and I need to run with privileges on it.