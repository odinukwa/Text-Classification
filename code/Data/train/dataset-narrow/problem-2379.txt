Apparently there cannot be such a canonical term, for neither $\text{refl } x$ nor $\text{refl } y$ type check. Does this mean that the identity type of $A$ does not have a canonical form in general? That is, that $\text{Id}_A(x,y)$ only has a canonical form in the particular case where $x$ is judgmentally equal to $y$? But in this case how propositional equality differs in practice from judgmental equality then – except for the fact that the former occurs as a type and the other as a judgment? 

On one hand, Gödel's Second Incompleteness Theorem states that any consistent formal theory that is strong enough to express any basic arithmetical statements can't prove its own consistency. On the other hand, the Church-Rosser's property of a formal (rewriting) system tells us that it is consistent, in the sense that not all equations are derivable, for example, K$\neq$I, since they don't have the same normal form. Then the Calculus of Inductive Constructions (CIC) clearly statisfies both conditions. It is strong enough to represent arithmetical propositions (indeed, the $\lambda\beta\eta$-calculus alone is already able to encode the Church numerals and represent all primitive recursive functions). Moreover, CIC also has the confluence or Church-Rosser property. But: 

It is well-known that the Church-Rosser property holds for $\beta \eta$-reduction in simply-typed lambda calculus. This implies that the calculus is consistent, in the sense that not all equations involving $\lambda$-terms are derivable: for example, K$\neq$I, since they don't share the same normal form. It is also known that one can extend the result to pairs which correspond to product types. But I wonder if one can further extend the result for dependently typed lambda calculus (perhaps) with polymorphic types, e.g. the Calculus of Constructions? Any references would also be great! Thanks 

I'm learning to use the LEAN theorem prover and I got stuck in a proof of a simple fact in first-order logic: $$ p(x) \rightarrow \forall x p(x) $$ My code is the following: 

Or it just states that the CIC can't prove its own consistency inside the system, and somehow the confluence property is a meta-theorem? Or maybe the confluence property of CIC does not guarantee its consistency? I would highly appreciate if someone could shed some light on those issues! Thanks! 

(I'm aware that the problem is undecidable in general, but in theory this should affect us only in the sense that we are unable to rule out all invalid judgments). For simplicity, we can suppose that we already have a type checking algorithm for categorical judgments. IMHO$-$and in the spirit of the universal introduction rule of predicate logic$-$the obvious type checking algorithm for (1) to say that it is valid whenever $$\vdash a[x_1,...,x_n/t_1,...,t_n] : [x_1,...,x_n/t_1,...,t_n]A$$ for a list of arbitraries closed terms $t_1 : A_1 ,..., t_n : A_n $ (since we can't possibly consider each closed term of such types). However, this algorithm is clearly unable to validate sequents such as (3), let alone (4). There is a quick highly-inelegant fix, which is to add the condition that if you find $x : \mathsf{empty}$ in the context list, halt and automatically recognize the whole sequent as true. But what about not-so-evident vacuously evident judgments such as (4)? Anyway, how would a correct, elegant and general approach look like? PS: I'm sorry if the question is not precise enough, but I can't think of a better way to state it. [1] Martin-Löf, Constructive Mathematics and Computer Programming, 1982. 

The existence of such sets follows, for example, from ZFC plus the axiom that every cardinal is bounded by an inaccessible cardinal; we can take each set $U_i$ to be a Grothendieck universe. We define an "interpretation" to be a mapping $v$ from the set of variable names to elements of $U_2$. Given an interpretation $v$, we can define an interpretation $I_v$ of terms of the system in the evident way: 

Is there a typed lambda calculus where the corresponding logic under the Curry-Howard correspondence is consistent, and where there are typeable lambda expressions for every computable function? This is admittedly an imprecise question, lacking a precise definition of "typed lambda calculus." I'm basically wondering whether there are either (a) known examples of this, or (b) known impossibility proofs for something in this area. Edit: @cody gives a precise version of this question in his answer below: is there a logical pure type system (LPTS) which is consistent and Turing complete (in a sense defined below)? 

We have that for all terms $A$, $I_v(A) \in U_3$. Now we say that an interpretation $v$ satisfies $A : B$, written $v \models A : B$, if $I_v(A) \in I_v(B)$. We say that $\Gamma \models A : B$ if for all interpretations $v$, if $v \models x : C$ for all $(x : C) \in \Gamma$, then $v \models A : B$. It is straightforward to check that if $\Gamma \vdash A : B$, then $\Gamma \models A : B$, so this is a model of the system. But, for any variables $x,y$, it is not the case that $y : \ast \models x : y$, because we can interpret $y$ by $\emptyset$, so the system is consistent. Now, this is an answer to my original question, in the sense that this is something that it's reasonable to call a typed lambda calulus, which is consistent and Turing complete. However, it's not an answer to @cody's question, because this is not an LPTS, because of the addition of extra axioms and $\beta$-reduction rules. I imagine that the answer to @cody's question is much harder. 

Here is an answer to a variant of @cody's precisification of my question. There is a consistent LPTS which is Turing complete in roughly @cody's sense, if we allow the introduction of additional axioms and $\beta$-reduction rules. Thus strictly speaking the system is not an LPTS; it is merely something much like one. Consider the calculus of constructions (or your favorite member of the $\lambda$-cube). This is an LPTS, but we're going to add extra stuff which makes it not an LPTS. Choose constant symbols $\text{nat}, 0, S$, and add the axioms: $$ \vdash \text{nat} : \ast $$ $$ \vdash 0 : \text{nat} $$ $$ \vdash S : \text{nat} \to \text{nat} $$ Index the Turing machine programs by natural numbers, and for each natural number $e$, choose a constant symbol $f_e$, add the axiom $f_e : \text{nat} \to \text{nat}$, and for all $e,x \in \mathbb{N}$, add the $\beta$-reduction rule $$ f_e(x) \to_\beta \Phi_e(x), $$ where as usual $\Phi_e(x)$ is the output of the $e$th Turing machine program on $x$. If $\Phi_e(x)$ diverges then this rule doesn't do anything. Note that by adding these axioms and rules the system's theorems remain recursively enumerable, though its set of $\beta$-reduction rules is no longer decidable, but merely recursively enumerable. I believe we could easily keep the set of $\beta$-reduction rules decidable by spelling out explicitly the details of a model of computation in the syntax and rules of the system. Now, this theory is clearly Turing complete in roughly @cody's sense, just by brute force; but the claim is that it's also consistent. Let's construct a model of it. Let $U_1 \in U_2 \in U_3$ be three sets, such that: 

Our practical computational experience has been that estimating rank over $\mathbb{C}$ is generically tractable by steepest-descent methods ... as we understand it, this robustness arises for a geometric reason, namely, the holomorphic bisectional curvature theorem of Goldberg and Kobayashi. This is far from a rigorous proof, needless to say. 

These issues are clearly and accessibly covered in Sanjeev Arora and Boaz Barak's Computational Complexity: A Modern Approach, specifically in the chapters "The computational model and why it doesn't matter", "NP and NP completeness", and "Boolean circuits" (respectively chs. 1,2,6). Very generously, an on-line draft of this book has been provided by the authors ... this has been a big help to me in my own researches! From the viewpoint of complexity theory, the various theorems and proof technologies in Arora and Barak's exposition provide little grounds for "divorcing the process of computation from the agent of computation" (in the phrasing of the question). From a regulatory perspective, however, it is well to keep in mind the legal maxim: 

Note: The text below was intended as a comment … it definitely is not an answer, but rather a pragmatic observation that arose out of a restating of Charlie Slichter's Principles of Magnetic Resonance in the language of symplectic geometry and quantum information theory (which pulls back naturally onto polynomial-rank tensor-product state-spaces). At present we have a partial geometric understanding of these tensor-rank methods, a marginal quantum informatic understanding, essentially no complexity-theoretic or combinatoric understanding, and a working (but largely empirical) computational understanding. We are very interested to broaden, deepen, and unify this understanding, and so we hope other folks will post further answers/comments on this subject. 

The references given in answer to Quantum mechanics as a Markov process — in particular Carlton Caves' on-line notes "Completely positive maps, positive maps, and the Lindblad form" — survey physical ideas and mathematical tools that are helpful in answering the question. A key point is associated to the specific question asked "How can I get Kraus operators for the operator-sum equivalent of $M$ that are in a useful form?" For large quantum systems, a generic superoperator $M$ will not have an algorithmically compressible form. Moreover, Kraus representations are non-unique, and to the best of my (non-expert) knowledge there is no procedure that is both general and efficient for finding Kraus representations of a given $M$ that have a "useful form" (by whatever criteria are given for a form being "useful"). That deciding quantum separability is NP-hard suggests that no efficient, general representation-finding algorithm exists, even when $M$ is numerically given in its entirety. To make progress, it may be helpful to ask heuristic questions: "What is special about my particular superoperator? Can I exhibit a set of Lindbladian generators for it that have useful symmetry properties and/or generate compatible compressive flows on the Hilbert state-space? Are these Lindbladian properties associated to a natural Hilbert basis in which $M$ has a sparse, factored, or otherwise algorithmically compressible representation?" If questions like these could be efficiently answered by "turning an algorithmic crank", then quantum physics would be a far less interesting subject! :) 

In real-world medical practice, and in many other regulatory contexts such as aircraft safety, commonly it is legally and practically necessary to distinguish between hardware and software, even though all parties recognize that the mathematical foundations for this distinction are highly imperfect.