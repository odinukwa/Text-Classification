And a similar error for the start service command. My SQL agent is running with local admin privileges. So I assume is has sufficient privs to do this. What am I missing? Is there some technical limitation in SQL Agent stopping this from working? 

There are lots of tools and methods available for loading data. But the easier you want this to be for end users the more complex and sophisticated your dataloading routine will become. Data validation and business rules need to be defined and implemented. Somebody has to do it. No software is smart enough to do that "auto-magically". You could build data checking into the spreadsheet. Hopefully this would stop errors before you load. It is possible to import data directly from a db query and there is a variety of lookup and validation tools in excel. You could build an import routine in ssis and apply validations, filters, lookups and substitutions as part of the import process. It can even handle errors and redirect invalid records back to the user. But this takes time and effort. My preferred approach would be to use a db to begin with rather than excel. That way datatypes, dependencies, rules and constraints can be enforced from the start. You could develop an interface in MS-access. Unfortunately all of these things take time. The more sophisticated you want the app to be more time it will take to develop. 

You can use the latest version of SSMS with SQL 2008 R2. Im not aware if any backward compatibility problems. It will also work with SQL 2017 and Azure. SSDT is a tool used to develop SSAS, SSIS and SSRS Reports. It wont cause any problems installing alongside SSMS. SSIS changes a far bit in 2012 namely in the use of SSIS catalog. IMO its a great improvement. Depending upon your layout it may be feasible to upgrade SSIS first or independantly. Note that once you open an old solution in a new version of SSDT or visual studio it will want to upgrade the solution. Once that happens you wont be able to open it again with an old version. Ive been through a similar scenario as you describe. In the end i found it was just easier to take the plunge and upgrade to the current client tools. 

With tons of values in the Cross Apply, it just gets annoying. How can I easily export the definition of an object to have double/quad quotes? 

I apparently cannot figure out the key words to google search for what I want, so I need help finding out how to do this. I feel like this may be a duplicate, but I just cannot find this anywhere else. I would like to be able to easily save a bunch of create table/view/proc scripts to a file. The file has an easy-to-execute format that will drop and then re-create all of the objects, so it can be run on multiple databases. This format works well for me as I have to make changes to some objects during development, and then make the same changes on multiple other databases. I can just change the database and click execute. However, some of the scripts have A LOT of quotes in them, and it gets QUITE annoying to have to double/quad quote them every time I make a change or add a new object to the file. For instance... 

I have a view in which I need to Group By or SUM OVER Partition By year and quarter, but the year and quarter will be computed within the view. I would like to keep it all within a view instead of needing tables and SPs. The goal is to get SUMs by of all records within the first quarter of a year, spanning over multiple years. What I have is: 

If it matters, here is a screenshot of what I have in the detail view of the parent report. I am showing this to give context. Basically the parent report is a single cell of a table, which will feed a parameter to the sub reports, so the sub reports will only display records for a single payroll at a time. For instance, page one will be everything from payroll 1, etc... 

Running SQL 2012 SP2 (11.0.5343) on Win 2008 R2 Standard SP1 on VMWare. Edit: Updated to SQL 2012 SP3 (11.0.6020). Edit: have added more context and clarified. Following suggestion from @TheGameisWar, I queried ssisdb SELECT * FROM [SSISDB].[catalog].[event_messages] WHERE event_name = 'OnError' Returns no error messages for that date. 

Yes you can use MS Access as a front end to SQL Server. Its quite common. I have done this myself in the past. No, Access itself won't corrupt data. However your Access application needs to be designed so that it respects SQL Server constraints and rules. Also how will your Application stop users from making simple mistakes, like deleting records if they're not permitted to. The most common problem I had was dealing with concurrency. When 2 clients are doing things at the same time such as inserting a record. Do you let Access generate the primary key? or do you let SQL Server manage it? In either case after you've inserted rows how do you ensure that each client is still dealing with the same row? This is a consideration for anyone developing a multi user system. It just takes some time and thought. Some advice: Make sure that you define the primary key in SQL Server. If you don't do this Access won't allow you to update records as it can't guarantee a lock on a single record. Primary keys and indexes also have a huge impact on performance. Edit: I've just been thinking about your Question title a bit more. You mention "security breach". If security is a concern, You need to consider how users will connect to the database. Domain authentication? or SQL authentication, and how you log actions taken by users. 

This gives a good idea of what I want to do, but SSMS doesn't like it, saying , and I'll start working on sample data to help, but I wanted to get this out now in case someone knew what to do off the top of their heads without data. 

I need to get some totals output in SSRS that I am having difficulty putting together. I am not sure if I should do the totals for the below query within the query, or in SSRS. The output is going to be presented as one long table in SSRS. I have some example DDL and fake desired output below, but to describe what I am doing, here is a quick snapshot of what I am trying to mimick in SSRS. 

I have a report that is "working" fine except for an issue. I have four dropdown list parameters where I am getting the values from queries. When I select a value from a dropdown list, it selects the next value below what I wanted to select, and "removes" the first value as an option. For instance, one of the dropdowns holds a list of payrolls. When getting the list from a query, it returns payrolls 1 through 121(or so), which is perfect. However, if I try to select any of the payrolls, say payroll 10, it will show me that I selected payroll 11, and the option to select payroll 1 will no longer be available. ALSO, possibly tied to the above, the report sometimes refreshes when I select "view report" and I have to try to choose the payroll again. These are not cascading parameters. The values are all derived from two queries where I am selecting a distinct list of employees for two of the parameters and a distinct list of payrolls for the other two. I am at a loss as to why this type of behavior is even possible. 

If that value is unique and is used to identify a single row in a table ( in your example to uniquely identify a single client record) it would typically be considered a unique or primary key. If it has some meaning and you already have a separate primary key it could could also be described as a business key or natural key. Without a constraint to enforce uniqueness you cant guarantee the data is unique. You should be careful to avoid developing with that as an assumption. 

You mention that you already have a piece of sql that does what you want. So i would suggest you modify that to become an update statement and implement it as a sql task. Another option is to save the update within a stored proc and call/execute it from ssis. With a bit of thought you could probably write an update statement that only sets the values for new records or for records that have changed. Or perhaps do it as part of your ETL insert/merge. However going back to my other comments: i don't think it makes sense to have a single key or dimension for such diverse attributes. I suggest you have a look at some sample schemas to see if it could be done differently. 

No i disagree. I dont believe it should be faster on your laptop given the spec you have just described. I also dont believe a difference in version or flags should explain it either. A 9000% difference suggests something is going very very wrong. You should expect to get at least the same or similar result on the server, or even better. At this point there are too many variables in your post. I think you need to work through a process of elimination to pin down where issue exists. Is it the server hardware? Is it the sql instance? Is it db specific? Is it the table/indexes/statistics? Is it the query itself? Is it anthing to do with load or external influences? I would start by performing a simple test under the same conditions on both the server and laptop. For example a simple script which inserts a million rows of test data into a table. Run the same script on a few dbs to get a benchmark. If you get comparable performance, then start looking at your tables. Create a maintenance script to rebuild your indexes and update stats. Run the same script on both dbs before you run your query. When you run your query, check the execution plans in both environments. Are they the same?