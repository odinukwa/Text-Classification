Read about the features of PGBadger, and with those features, Query Store is definitely the way to go, and i have only a few things to add to the answer from Dan. The data in the DMV's are reset when the server is restarted for whatever reason. This is not the case for the QueryStore and is one of the big features of this. Also for the DMV sys.dm_exec_query_stats, some of the data can be flushed if the server gets under memory pressure. In SqlServer 2017, latest CU, the query store also contains wait statistics, which you would need to help you understand why a query is regressing. This is not the case in SqlServer 2016 (hoping for a future SP to include it) You will therefore have to setup some logging of wait statistics on your own. i have build a version of the code from this article to do that: $URL$ (be sure to read the articles he is linking to as well) If you are on an earlier version of SQL Server, you need to collect the query statistics yourself (the data from sys.dm_exec_query_stats) on a busy server, this needs to be done rather frequently, since the data can be flushed in pressure situations as i mentioned. As Dan, i am not sure what you are wanting to use the 'duration' for but if it is for finding ressource/query issues, you need to monitor more numbers and changes in numbers, like I/O statistics, Wait Statistics, Memory Usage and more before going into to much detail, it would be nice to hear what exactly you are trying to achieve For a complete out of the box tool that can deliver all of the stuff that PGBadger delivers, you will have to get a commercial product like the ones from Idera, RedGate and others 

used on all queries ? then the DBA will have to monitor if optimal plans are used (if data distribution have changed much since the plan was created) An extra comment. Having both KEEP PLAN and KEEPFIXED PLAN in the hints is not needed. KEEPFIXED PLAN is overruling KEEP PLAN. KEEP PLAN, disables the “6 rows” rule, as far as statistics invalidation is concerned. KEEPFIXED PLAN disables recompilations due to statistics changes completely. 

I'll Start with the second question The reason for the 'missing index' advice for Plan 2 is the subtree cost calculated The parameters passed results in a subtree cost (right click/hoover on the nested loop join) that is 10 times higher than that for Plan 1, and it is because the estimated number of rows is 149, instead of 16. What is suggested as missing is a covering index, so that it doesn't have to make 149 keylookups to get the missing values from the index. For Plan 1 it was considered ok to make 16 keylookups What you are experiencing is what Erland Sommaskog is calling a “morally equivalent plan” Kendra Litttle explains it in this article $URL$ Using the feature of Query store to force a plan, but i believe it is the same when using KEEPFIXED PLAN As to why several plans for the same query exists. Many reasons including restart, cache getting cleared due to memory pressure, changing the structure or schema of a table referenced by the query. This will cause the plan to be recompiled Normally also updating the statistics, but since you have KEEPFIXED PLAN that is not he case. Having KEEPFIXED PLAN can cause performance degregation since as your data changes, the statistics change and the optimal plan may be different, but KEEPFIXED PLAN is forcing the same plan all the time. i am guessing, but is the setting 

Record count is the same but there is a big gap between the two in terms of space used. sp_spaceused shows 7.691.344 KB as reserved while the report shows 4.340.216 KB. Which one is correct? 

I want to log the duration and time of user defined stored procedures each time they are invoked. I believe using extended events would be the way to go to achieve this. Can you please help me in defining the session and in querying the results? I am not quite sure about which event to add (sqlserver.rpc_completed?) and which target to choose (synchronous_event_counter,asynchronous_file_target or ring_buffer?). I also need help in querying the results (For example: grouping the result set so that each sp will show from top execution duration and execution time.) We use SQL Server 2008 R2. 

One of the SQL Server 2008 R2 instances I administer is running on a two node Windows Cluster (Windows Server 2008 R2 Enterprise) When I run the DMV sys.dm_server_services, SQL Server Service Account shows as Y under the 'is_clustered' column. However, Agent Service Account is an 'N' for the same field 'is_clustered'. Is it something that I should be worried about and what might be causing this? Does this mean that Agent will not fail over? Thank you 

I'd like to know if it is possible to have a single passive node for two SQL Servers in a failover clustering deployment. For example, both active servers A and B will use server C as their contingent server. If it is apt to do so, will server C run two instances of SQL Server; one for A and one for B? As a side question; what is the benefit of "dynamic quorum" and "dynamic witness"? 

Please can anyone help me in resolving the deadlock of which details and screenshots are given below? Occasionally, two stored procedures are resulting in a deadlock. I was able to pin down the statements that caused the conflict. Only one table and one clustered and one non-clustered index is involved. For demonstration purposes, I renamed the table and indexes in a test environment. How can I avoid this deadlock? Thank you 

It was all down to collation of the column. It was different from the database's (and the table's) collation. Now changed the column's collation to database's and no more implicit conversion shows up. Have no idea about the internals and why it caused the problem. 

I set up a virtual lab environment simulating a clustered network where I unticked one of the nodes as the preferred owner. I can confirm that whenever the ticked node goes offline, the unticked one comes into play and the system failbacks onto the ticked node once it comes online again. 

We are on SQL Server 2008 R2 Standard Edition. Some tables are highly fragmented. I want to see if defragmentation will improve performance and is worth the effort and tables/indexes being locked during the process. Therefore I want to restore the full backup on a test environment and simulate a live environment. What is the best way to follow? How can I capture the events going on in a live environment for some period? What tools are available for this? Thank you 

It was tempdb contention. Adding more tempdb data files and enabling trace flag 1118 solved the issue. 

I'm experiencing high Signal Wait percent while CPU utilisation is very low at one of our servers. I have gone through numerous articles online about Signal Waits and understand that it is the time spent on the runnable queue however I'm still having difficult time to understand why CPU usage is staying so low (25%) while Signal Waits' percent is high (band of 75-90%.) If a session is waiting for an available CPU to run it, why is CPU utilised so low? 

Result: 2017-03-28 14:00:59.410649 Expected: 2017-03-28 14:00:59.410648 DB2 provides the expected result by throwing away the 7th fraction digit. How can we make MSSQL not round the datetime2 value ? EDIT The application writes a java.sql.Timestamp object with 12 fraction digits to DB2 and MSSQL. In DB2 the column is a TIMESTAMP(6) and in MSSQL a DATETIME2(6). DB2 truncates from 12 fraction digits down to 6. MSSQL rounds down to 6. 

We are running sql server 2016 enterprise edition. We have a linked server configured to db2. We do a distributed transaction between mssql 2016 and db2 using MS DTC. SQL Server controls the distributed transaction. Application is running in Java using JDBC. We are using MS JDBC driver. Java does nothing in terms of distributed transaction. From Java's perspective, it is only a local transaction with MSSQL. Everything works fine and has been running 2+ years. Transactions are started and committed, or rolled back on error on both mssql and db2 However we now experience a weird problem. If we run into a duplicate key (or any other error from an SQL statement), an error is thrown in the form of a SQLException (java exception). When this occurs, it seems the transaction in progress is paused and a new transaction is started. The new transaction commits, however everything that happened before in the paused transaction does not commit (nor rollback). It just hangs. When this happens enough times, the tables lock, and we can neither read nor write from them. There is nothing done in the application that will start a new transaction at exception. The reason for why we have not had this issue in 2+ years is because this is the first time we expect an error to happen (duplicate key) and we therefore do some logic and continue. Previously we would always rollback immediately on exception. I'm not sure how to troubleshoot this. Is this a documented behavior ? Where can I see the "transaction id" so I can confirm the above ? How do I proceed ? --- Update --- xact_abort is on and therefore the duplicate key error rollsback the transaction (but the rollback is not complete, somehow the transaction is left hanging). This explains why the commit works for the work performed after the error. Also this is relevant: $URL$ The question now changes to, how to deal with statements that may throw an error but should not roll back an exception 

We are working on a multi-database application (read and write to multiple databases). The datamodel is identical on the databases. We are inserting a timestamp (12 fraction digits) value in a datetime2(6) column in MSSQL however MSSQL rounds the value making it different to other databases where the extra fraction digits are ignored. Example: 

I have 2 replicas running SQL Server 2016 Enterprise Edition in an AlwaysOn Availability Group. One replica is primary and other replica is secondary and readable. I have a business requirement that should allow one database replica to be taken offline and back online after a period of a few minutes by a custom developed APM monitor. The database replica to be taken offline can be primary or secondary. The database replica has to be take offline and online via transact SQL When the database replica is offline it should not be possible for it to execute requests. All existing connections must be moved to the other database replica or fail, so that the client can reconnect using the IP listener to the other replica. I can do a suspend or a planned manual failover but neither seems to support what I need. The suspend & manual failover only works on secondary database replicas. The suspend allows existing connections to execute requests. The manual failover just switches the primary to secondary role and vice versa. How do I do this ?