we are going to upgrade from SQL server 2012 Standard edition to SQL Server 2016 Enterprise edition. we can't have a downtime more than 30 mins .The new SQL server will use always on availably group 1 primary and 2 read-only secondaries . I am looking for the fastest way to do this upgrade. I am thinking of creating mirroring between the old server and new primary server , then shut down the application update the connection string , fail over the DB to the new server, stop mirroring, start the application. After the application is up, start in configuring the Availability group. IS this a correct way to use 

we have a SQL Server DB running on Amazon RDS, i need to refresh it. But in RDS there is no option to restore a backup. What other ways i can use to refresh this DB? It is 60 GB, and has 200 tables. Thanks 

If I have database db1 with schema sch1. I want to create another schema sch2 which will be used by different application and the tables in sch2 will be populated from tables in sch1. So my approach is to create triggers on insert, update, delete perform this task So my questions are 1- Is this a good way to do this or there is a better way? 2- I donâ€™t want to affect the performance on db1 so I am want to replicate it to different server and create the triggers on the subscriber DB, so replicated server will have Sch1 and Sch2 on it, Can I do that? This is on SQL Server 2012 Standard Edition Note: tables in sch2 have different names and different columns name. also more than one table in sch2 may get populated from data from sch1 

If I want another report application to connect to the secondary DB. The connection string should be point to secondary or the listener Also, i when i try to connect to the databases secondary server with a SQL authentication user i failed. 

On a QA server I need to give access to specific login from just one IP to log to this server. I wrote This login trigger 

SQL Server 2016 Enterprise Edition SP1 Database Mail was working fine with no problem until a server patching was performed. I tried to do all the troubleshooting available but still all mails got queued and no records in the log. Finally when I checked the database mail program I didn't find the executable file in the Binn folder or any other folders! My questions are how this happened and how to solve it ? 

Update I find this Delete in the Query Store , there were 3 plans and I forced SQL to use different plan. I am wondering Why SQL choose to use the most expensive execution plan? 

I see that i don't need all these indexes as they overlapping. What is your recommendation for best performance ? 

However the trigger is preventing the user from log although the IP exists in the IPAddress table. Any idea where in the problem in the trigger 

If I am going to set an Alwayson Availability group with 3 nodes, with read-only routing. Do I need to configure a Quorum ? 

This table is very active table and i need to avoid blocking. If i need to create indexes on this table should i create 4 indexes. 

i would like your help in the best way to tune and re-write a stored procedure. it is consuming a lot of time and don't know what should i do to make it faster 

I need some clarification regarding Always on avilabilty group in SQL SERVE 2016 connection to the secondary DB. Now i have availability group with 2 nodes. 

when the application is busy and there are many deletes coming, the performance become so bad, My questions are. How to make this delete perform better? why the granted memory is so high, it steals the memory from the buffer pool ? Note: there is on delete cascade on the 2 tables Highschoolcourse and highschoolgrade. 

I don't know much about the feature, my question is as now we can send read-only workload to secondary replica . My question is what will perform better single instance with 32 cpu 1 primary and 1 secondary with 16 cpu each or 1 primary and 2 secondaries 12 cpu each Or in other way is the number of worker threads affected by the number of cpu in the primary only or the number of cpu of all members in Availability Groups 

According to this link, turning the Parameter Sniffing option off in 2016 is equivalent to setting trace flag 4136. You should read up on that trace flag. Especially important information from that trace flag link: 

Short answer: start SQL Server with the -m flag from a local admin account. Step-by-step instructions here: 

Of course, like Erik alluded to, your actual problems probably have nothing to do with fragmentation, but hey, this was fun. 

Note how the first open transaction (Transaction ID 0000:000002fa for me) isn't committed until the end of the REBUILD ALL, but for the index-by-index rebuilds, they are successively committed. 

According to Paul Randal in his Pluralsight course "SQL Server: Logging, Recovery, and the Transaction Log" the LSN is composed of three parts: 

So by adding a recompile, you're telling the optimizer to build a plan based on whatever parameter is passed, but not to reuse the plan. With Parameter Sniffing off (and no recompile), a plan is built based off of statistics on the parameters and will be reused, even for values that would otherwise generate a different (perhaps better) plan. 

This is an application issue more than a database issue. Navision treats datetime entries as UTC, and is adding the hour in its display (assuming your local timezone is UTC+1). I have run into this myself since I work with Navision. If you have a client application, you can look at displayed datetime for a record and compare it to what you see by querying the table. 

I see no difference when using TRUNCATEONLY when compared to regular SHRINKFILE on the log file. Here's the script I built for testing (2016 dev edition, note the DROP IF EXISTS syntax): 

To answer the question in the title, whether the B-tree rebalanced during a delete, the answer appears to be no, at least in the following minimal test case. The following demo runs commands that are best left for a test environment. 

Understanding 1 is correct, and spaghettidba and Joe have good explanations. If you're interested in testing for yourself (on a test instance please), you can use the below script: 

Bad news: the plus approach in your script completely flattens out the XML document, and will need to be reworked if you have any multiple-X-per-Y structures in your document. If the dynamic SQL is a requirement, then please provide an example to test that aspect more easily. Another approach might be to build queries you need manually. If your fundamental problem is including parent info, then you might modify my demo SELECT query below. (N.B. Mikael Eriksson's answer has an improved query. Please refer to that.) Key ideas: is the context node, and gives you the parent is a different kind of XML function that belongs in the FROM section of the query and is usually seen with CROSS APPLY. It returns a pointer per match, and is what allows working with multiple rows. Read more here. is one of several methods for letting SQL Server know the value method only has one piece of data to work with (fixing the "requires a singleton" error) 

This demo shows that a delete can produce a very unbalanced b-tree, with practically all data on one side. 

Here are my own results: average time in ms for DROP/CREATE: 2547, for REBULD: 1314 It looks like in my contrived example, the winner is REBUILD. Constraints What if the index was created to support a constraint? In this case, a simple DROP and CREATE will fail because you have to drop the constraint first. Let's look at the clustered index from the previous example. 

Now let's build some procs that will DROP/CREATE or REBUILD, plus log (approximately) the time taken: 

OPTION (RECOMPILE) allows constant folding of @Step and @FindNo in your second query. Since @Step is known to equal 1, the line will be optimized away. Notice that this affects estimated rows. If you want to see a more drastic example of this, try the below code. Notice how constant folding (enabled by option recompile) allows the second query plan to completely avoid the union, since the optimizer knows no rows could be returned. 

I'm fighting against NOLOCK in my current environment. One argument I've heard is that the overhead of locking slows down a query. So, I devised a test to see just how much this overhead might be. I discovered that NOLOCK actually slows down my scan. At first I was delighted, but now I 'm just confused. Is my test invalid somehow? Shouldn't NOLOCK actually allow a slightly faster scan? What's happening here? Here's my script: 

Is there a way to separate memory allocation for databases in mysql on the same server? Say for database_1, we would like to allocate 20GB RAM, and for database_2, 10GB RAM. Is there any way this is possible. 

MySQL 5.5 introduces "columns partitioning". $URL$ I'm trying to better understand how it works when two columns are important individually. Let's say for a table which holds the messaging between two system users. We would potentially have "sender_id" and "receiver_id" columns, and we might want to query against these columns individually. If we have separate indexes on both of these columns, we can query them individually when necessary. Results are fast. But what if our table is 100M rows large, and we consider partitioning. My understanding is multiple column partitioning focuses on the first column in the columns definition, then the second one. Here is a sample table structure: 

With mysql 5.7 you can consider moving your undo logs to separate files: $URL$ This will also give you the option to truncate those logs when you like: $URL$ In my.cnf this setting should do the trick: 

Even though we are using "innodb_file_per_table = 1" from day one. The ibdata file still continues to grow. Total size of our database is around 800GB and the ibdata file is currently around 50GB. All our tables are innodb. How can it grow when we use innodb_file_per_table? Could it be blobs, mediumtext or these kinds of columns? Is there a way to shrink this considering we already use innodb_file_per_table? 

We're using Percona's pt-table-checksum for MySQL replication integrity check. Is it possible to pass some or all of the options using some sort of configuration file? For instance, can we write the list of tables we want checked in a file? I understand the manual does not talk about such a file: $URL$ 

You can change the parallel process number as you like. Since you mentioned you have myisam tables, the "rsync" parameter will help speed the backup of those. 

When starting mysql on Centos servers with a custom data directory you might get an error similar to the following: 

We recently tried setting for our mysql 5.7 server. But during the restart the server returned with: It turns out this is already documented : "The number of innodb_undo_tablespaces must be set prior to initializing InnoDB. Attempting to restart InnoDB with a greater number of undo tablespaces than you specified when you first created the database will result in a failed start and an error stating that InnoDB did not find the expected number of undo tablespaces." What does "first created the database" mean? Does the documentation mean a fresh installation of mysql where there are no databases within the server? Is there a way to bypass this requirement? 

Just updated to MySQL 5.5 and this issue is resolved. Tried it with files as large as 10GB and MySQL is not affected at all. 

$URL$ Let's say our database "concurrent insert" parameter is set to "Auto" (1). And we have a MyISAM table with a gap. When we insert new rows and fill those gaps, does the table "immediately" get ready to accept "concurrent inserts" for future insert queries? Or do we need to run "OPTIMIZE" before the table knows there are no gaps? 

Unfortunately, this setting appears to be available only on fresh mysql installs. You cannot just restart your server with this setting, you will get an error if you do. But, since you will also probably want to shrink your ibdata1 file, you will need a full dump anyways. 

Before setting up a replication within MySQL, the data first needs to be transferred to the slave. Since MyISAM tables can be copied as files. Would it be okay to copy MyISAM files, .myd .myi .frm, to the slave? Would this be enough to start the replication? 

If your chmod and chown settings are already correct you might need to look into selinux. This page explains how to configure selinux for mysql: $URL$ 

If we query for "WHERE receiver_id=5", partition pruning will not kick in, right? It will need to search all partitions. But if we were to search for "WHERE sender_id=5", then we would immediately know the result is in p0. So for a table where two columns could potentially be individually important, partitioning might not be the best solution since now we lost the benefit of a full-table index for the secondary column(receiver_id, for this case) in the columns parameter. Is that right?