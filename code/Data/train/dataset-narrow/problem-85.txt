Path tracing is the standard technique in non-realtime photorealistic rendering, and you should look specifically into bidirectional path tracing to get effects like caustics, which you can't really get with basic path tracing. Bidirectional path tracing also converges faster to the ground truth as shown in the below image: 

You can use single sided triangles for the ceiling so that they are pointing towards the room. This way the ceiling influences the GI in the room but you can see through it when observing from outside 

"decent" is quite subjective and if you are restricting the capture to certain types of surfaces and controlled lighting conditions. For example normals and other SVBRDF parameters for shiny metallic surfaces are very difficult to capture compared to non-metallic, matte and bright surfaces without texture. There are tools proposed in comments (CrazyBump, AwesomeBump) that try to do what you ask for and may generate normal maps sufficient to your requirements, but you could argue how "decent" the results are and how robust these tools are in capturing different types of surfaces. I don't know about the algorithms these tools use, but I believe they use more of an "artistic" than robust/accurate methods in generating the results. There is some recent work to estimate normal map and other SVBRDF parameters using two images or from a single image (using neural networks), which is probably your best bet. However these algorithms assume a level of repeating pattern in the input images, but this might be ok for you since you mention rock wall as an example. There are likely other constraints as well such as requiring the captured surface to be dielectric hard surface material. For more robust SVBRDF capturing you can check paper on frequency domain capture, but this is more complex capturing setup and far from a single image capture. 

No, you don't need to know this stuff to implement basic path tracer. Basic unidirectional path tracer is quite simple to implement. Just trace bunch of paths of length X for each pixel with uniform distribution over the normal oriented hemisphere at the path's intersection points. Then weight the remaining path with the BRDF at each intersection point and multiply with luminance of light once the path hits a light. You'll get quite a noisy (but unbiased!) image even for large number of paths and then you can start to look into methods to reduce noise, e.g. importance sampling & bidirectional path tracing. Just validate the more optimized path tracers towards earlier validated path tracers to avoid introducing accidental bias. 

If you want to use only one sample to approximate analytical area lighting (e.g. for real-time applications), you can use Most Representative Point (MRP) approximation as described in "Lighting in Killzone Shadow Fall", "Real Shading in Unreal Engine 4" and "Moving Frostbite to PBR". This works reasonably well for specular approximation, though the energy conservation is a challenge. What I have found working best is to find a point on a light source with smallest angle from the view reflection vector, and the solution depends on the shape of the light (for example here's my solution for rectangular lights). For diffuse evaluation there are numerical and analytical approximations depending on light shape presented in the papers, which also properly handle the horizon case. You can also try to use MRP for diffuse lighting, but I haven't found good approximations for it. Depending your application though diffuse MRP might result in feasible quality. There's also some more recent work for evaluating area lighting integrals not using MRP. "Accurate Analytic Approximations For Real-Time Specular Area Lighting" deals with the specular evaluation via edge integrals, though I recall this method is patented. "Real-Time Polygonal-Light Shading with Linearly Transformed Cosines" looks quite promising, but I haven't checked the paper in more details. For non-real time applications and ray tracing area lighting is fundamentally and mathematically much simpler problem. For your integrator you just check the incident radiance at each sample (ray intersection with the scene) and accumulate results for total radiance towards the output direction. The challenge is how to reduce variance of your integrator, but getting basic area lighting to work in a ray tracer is relatively simple. 

You can map [x, y] pixel coordinates in an image of size [width, height] to your given range as follows: 

PNG format is lossless format where for compression the image is first "filtered" and this filtered image is then passed to DEFLATE lossless compression algorithm. The purpose of filtering stage is to make the image more compressible by DEFLATE and current method uses delta-compression from previously decoded pixels. So if your plan is to pre-process the image in some lossy way, then if you can reduce the deltas so that the following entropy encoding can benefit from it, then you get better compression ratio. For example "blockifying" the image as you suggested would only require more storage for the top-left pixel in the block while the deltas for the remaining pixels would be zero, which DEFLATE can compress very efficiently. This same would apply to any continuous constant color regions. 

You could try to optimize the calculation by first casting a smaller number of "feeler rays" (say 32) to check for penumbra, and cast the remaining rays if the result is in penumbra according to the rays. Of course this can give you false negatives, but you could then try to improve the heuristics by checking the results of nearby feeler rays. 

It's not that the "light is somehow more intense", but rather that the light has to be more intense to have same flux in grazing angles as when it's parallel to the surface. If you think of flux as particles hitting to this differential surface, the number of particles hitting the surface is smaller more perpendicular the light gets to the surface. So you have to throw more particles at the surface (i.e. increase the radiance) for the same flux until when the surface is completely perpendicular to the light, none of the particles hit the surface, thus the radiance goes to infinity. 

Usually you scale first, then rotate and finally translate. The reason is because usually you want the scaling to happen along the axis of the object and rotation about the center of the object. In your case you don't really need to worry about this generic solution though, but you only need to map range [0, 800] $\rightarrow$ [-2, 2] for x-coordinate and [0, 600] $\rightarrow$ [-1.5, 1.5] for y-coordinate, in order to map screen coordinates to real/imaginary components for Mandelbrot calculation. So this is simply done by: $$real=4*(x+0.5)/800-2$$ $$imag=3*(y+0.5)/600-1.5$$ Note that you need to calculate Mandelbrot coordinates from screen coordinates and not the other way around. This is to ensure that you evaluate the Manderbrot equation for each pixel exactly once and are not left with holes in the image or do double evaluation per pixel. 

It would be too expensive (both performance and memory wise) to evaluate BRDF first for each material layer and then blend the layers. Thus the material layer parameters & textures are rather first blended together and then written to the G-buffer for a single BRDF evaluation. This is feasible as long as all the layers use the same BRDF. This doesn't generate identical results to evaluating BRDF before blending, but the results are quite often acceptable. The BRDF parameter blending can be done as an offline process that generates unique textures based on blending maps. This is efficient at run-time but of course consumes more memory since for every blend map you need to generate all the source textures. You can also blend the materials at run-time with the expense of performance, but with smaller memory budget in general. 

If you have object$\rightarrow$world space transformation matrix: $$M=T*R$$ then inverse (i.e. world$\rightarrow$object) of this transformation matrix is: $$M^{-1}=(T*R)^{-1}=R^{-1}*T^{-1}$$ So the order of multiplication of matrix inverses is reversed per matrix inversion rules. 

To my knowledge there's no known generic algorithm to accurately extract SVBRDF parameters from a single image of a non-repeating surface because of the fundamental issue that a single image can't unambiguously represent SVBRDF parameters. E.g. two different normal & albedo combinations of a Lambertian surface may result in same pixel color in a single image. 

Depends if you are talking about CPU or GPU ray tracer. For CPU you generally simply allocate an array of width * height float4's (i.e. for RGB & alpha) and for GPU you allocate a texture (e.g. R16G16B16A16F or R32G32B32A32F format). It depends on your case if you really need the alpha though. The target on CPU is then simply accessed by image[(x+y*width)*4] (image=float array), and on GPU image[uint2(x, y)] (image=UAV). You generally want this target to be a float format (16-bit or 32-bit) to be able to handle high dynamic range (HDR) of luminance in the scene (think of a scene illuminated by Sun = ~100,000 Lux vs Moon = ~1 Lux), that gets then exposed using camera settings, tone mapped and converted to sRGB space to be viewable on regular 8bpc devices. If you are with a very strict with memory budget, you may allocate LDR target (e.g. R8G8B8A8_UNORM) and perform camera exposure, tone mapping & sRGB conversion before writing the result to the render target. This has bunch of issues though, e.g. you would need the camera exposure value prior to rendering, which can be a challenge for camera auto-exposure, so it's generally adviced to use HDR target instead. 

The power spectrum emitted by light is defined with SPD (in W/m^2/nm) and you can calculate the “HVS neutral” physical light intensity, i.e. Radiant Exitance $M_e$ (in W/m^2) by integrating the spectrum over the visible light range: $$M_e=\int_{390}^{730} S(\lambda)d\lambda$$ However, RGB color space is defined in photometric units by projecting SPD using HVS weighted color matching functions. This projection is done because storing SPD would consume immense amount of memory for little gain for image visualization purposes. The projection of SPD to RGB is defined with integrals over the visible light range as follows: $$R=\int_{390}^{730} S(\lambda)\bar{r}(\lambda)d\lambda$$ $$G=\int_{390}^{730} S(\lambda)\bar{g}(\lambda)d\lambda$$ $$B=\int_{390}^{730} S(\lambda)\bar{b}(\lambda)d\lambda$$ CIE 1931 defines standard observer RGB color matching functions as shown below, based on measurement of 10 human observers