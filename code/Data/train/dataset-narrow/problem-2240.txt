I keep a list of talks/posters on my website. I do this to include links to slides, photos and video recordings. When I was applying for grad school I had them in my CV, too. I have the impression that it might have helped. However, once you have a lot of talks it becomes too much info. On some senior researchers' CVs I've seen very compact talk lists: they include their invited or conference talks as acronyms/university names and lists of years. For example, if they regularly present at STOC they might write "STOC 2003,2004,2007". I think this is mostly to show that they are involved with the community and go places regularly. As Dave mentioned, it is usually standard to cite the paper as a conference. I think the exception is when conferences publish their proceedings, or selected papers, as a special issue of a journal. The only time this happened to me, I had to go through an extra layer of review (first conference, then the journal) for one paper, so I think is appropriate to cite those as journal papers. A lot of time papers are first published in conferences, and then a year (or more!) later a longer version appears in a journal. The convention is to cite this as a journal paper with a note after: "preliminary version appeared in Conference X 2008". 

run your poly-time algorithm for returning a pair of edge-disjoint paths $(P,Q)$ with $P < Q$ from $s$ to $t$. If $(P,Q)$ is not in $D$. 2.1. Insert $(P,Q)$ into $D$ (and output if you are suppose to output as the algorithm runs). 2.2. For each edge $uv \in E(P\cup Q)$ run $F(s,t,G - \{uv\}, ^*D)$ 

Both books look at evolution through the algorithmic lens, with the first concentrating on how evolution, learning, and intelligence can be all expressed in the PAC-framework of Machine Learning. The second book, looks at how to build a toy model of evolutionary innovation using algorithmic information theory. Although the books are only loosely connected to biology, they do present computer science in a standard pop-sci way and show how it related to more common topics in pop-sci, like evolution. 

Let's say you are dealing with numbers in base $b$ with up to $n$ digits whose digit values sum to $M$. To encode a number $N$ of length $l = \lfloor \log_b N \rfloor + 1 \leq n$, just create a bit vector of length $\log n + (M - 1)\log l$. The first $\log n$ bits of the vector tell you how many digits $N$ has. The second $l$ bits tell you the position $p$ (from the right) of the first not-zero bit. The subsequent $M - 2$ blocks of $log l$ bits tell you offsets $d_i$ for $1 \leq i \leq M - 2$. You use these offsets to reconstruct the number quickly, as: $$ N = b^{l - 1} + b^{p} + \sum_{1 \leq i \leq M - 2} b^{p + \sum_{1 \leq j \leq i} d_i} $$ 

$\forall A \quad T(A) \in \Omega(|A|)$: no self-delimiting program returns an answer faster than its description. $Pr_{halt}\{ T(A) \in \omega(|A|)\} = p > 0$: the probability $p$ that a halting program runs for strictly longer than its description is some non-zero. 

Although exponential separations between bounded-error quantum query complexity ($Q(f)$) and deterministic query complexity ($D(f)$) or bounded-error randomized query complexity ($R(f)$) are known, they only apply to certain partial functions. If the partial functions have some special structures then they are also polynomially related with $D(f) = O(Q(f)^9))$. However, I am mostly concerned about total functions. In a classic paper it was shown that $D(f)$ is bounded by $O(Q(f)^6)$ for total functions, $O(Q(f)^4)$ for monotone total functions, and $O(Q(f)^2)$ for symmetric total functions. However, no greater than quadratic separations are known for these sort of functions (this separation is achieved by $OR$ for example). As far as I understand, most people conjecture that for total functions we have $D(f) = O(Q(f)^2)$. Under what conditions has this conjecture been proven (apart from symmetric functions)? What is the best current bounds on decision-tree complexity in terms of quantum query complexity for total functions? 

The tape starts with input (which is all $0$s and $1$s) followed by $\epsilon$ (blanks) The machine can use any fixed number of lines Left-jumps are allowed on any line (I will use one left jump per line) The machine can write $\epsilon$, $0$, and $1$. 

In the original $ADV^\pm$ paper the authors construct an example function for which their method overcomes both barriers. However, I have not see examples of any natural problems where this has yielded new lower bounds. 

I studied computer science and physics (with minors in math and cognitive science) in undergrad, and now do graduate studies in quantum computing. I found that taking a wide variety of courses in both CS and physics prepared me pretty well, better than concentrating in one field or the other would have. Even the courses I thought useless, like programming and software engineering (from the CS side) or experimental methods (from the Physics side) allowed me to gain a wider appreciation of both fields. The main point I'd like to stress (that some of the other answers mentioned) is not to focus too much before you even started courses. Undergrad is a great opportunity to explore and find what you are passionate about. Fields like quantum computing are trendy right now, but there is no reason trying to go into them and do research if it is not what you are truly passionate about. Before undergrad I came from a similar background as you: I did will in math and physics and what little CS I was exposed to. All my friends were going into CS, so when I started college I was going to concentrate on Physics and Political Science! After exposure to some great courses and professors, my interests drifted, and even now I am not sure if they are fully set. However, I am very happy to have gone to a school that wasn't overly focused on one field, it allowed me to develop my interests and learn from many great professors. At least early on, try to get some breadth in your curriculum. 

It is impossible to write a programming language that allows all machines that halt on all inputs and no others. However, it seems to be easy to define such a programming language for any standard complexity class. In particular, we can define a language in which we can express all efficient computations and only efficient computations. For instance, for something like $P$: take your favorite programming language, and after you write your program (corresponding to Turing Machine $M'$), add three values to the header: an integer $c$, and integer $k$, and a default output $d$. When the program is compiled, output a Turing machine $M$ that given input $x$ of size $n$ runs $M'$ on $x$ for $c n^k$ steps. If $M'$ does not halt before the $c n^k$ steps are up, output the default output $d$. Unless I am mistaken, this programming languages will allow us to express all computations in $P$ and nothing more. However, this proposed language is inherently non-interesting. My question: are there programming languages that capture subsets of computable functions (such as all efficiently computable function) in a non-trivial way? If there are not, is there a reason for this? 

In communication complexity, the log-rank conjecture states that $$cc(M) = (\log rk(M))^{O(1)}$$ Where $cc(M)$ is the communication complexity of $M(x,y)$ and $rk(M)$ is the rank of $M$ (as a matrix) over the reals. However, when you are just using the rank-method to lower bound $cc(M)$ you can use $rk$ over any field that is convenient. Why does the log-rank conjecture restrict to rk over the reals? Is the conjecture resolved for $rk$ over fields of non-zero characteristic? If not, is it of interest or is there something special about $rk$ over $\mathbb{R}$? 

Working directly with time complexity or circuit lower bounds is scary. Hence, we develop tools like query complexity (or decision-tree complexity) to get a handle on lower bounds. Since each query takes at least one unit step, and computations between queries are counted as free, time complexity is as at least as high as query complexity. However, can we say anything about the separations? I am curious about work in the classical, or quantum literature, but provide examples from QC since I am more familiar. Some famous algorithms such as Grover’s search and Shor’s period finding, the time complexity is within poly-logarithmic factors of the query complexity. For others, such as the Hidden Subgroup Problem, we have polynomial query complexity, yet polynomial time algorithms are not known. Since a gap potentially exists between time and query complexity, it is not clear that an optimal time complexity algorithm has to have the same query complexity as the optimal query complexity algorithm. 

If SAT had a subexpoential-time algorithm, the you would disprove the exponential time hypothesis. For fun cosequences: if you showed that circuit SAT over AND,OR,NOT with $n$ variables and $poly(n)$ circuit gates can be solved faster than the trivial $2^n poly(n)$ approach, then by Ryan Williams' paper you show that $NEXP \not\subseteq P/poly$. 

Long version of question Are there models of decoherence for a random walk on the line, such that as we vary the amount of decoherence we can achieve a standard deviation in position that scales as $\Theta(t^k)$ for any $1/2 \leq k \leq 1$? Alternatively for other graphs with a gap in mixing or hitting time, is there forms of decoherence so that we can have the mixing/hitting/standard deviation that goes as $f(t)$ for any $f \in \Sigma(g(t))$ and $f \in O(h(t))$ where $g(t)$ is the classical mixing/hitting/STD and $h(t)$ is the pure quantum. If this is not possible then is there a deeper reason why we see this sort of one-or-the-other behavior?