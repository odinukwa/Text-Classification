Contrarily to the $\lambda$-calculus, the interaction combinators have no underlying logical system (i.e., there is no Curry-Howard correspondence for them), it is therefore hard to say that a numeral system is more "natural" than another. (In the $\lambda$-calculus, Church numerals may be claimed to be "natural" because their type is the erasure of the second order definition of natural number, i.e. $\mathsf{Nat}(x) := \forall X.(X(0)\Rightarrow\forall y.(X(y)\Rightarrow X(Sy))\Rightarrow X(x)).$ If you erase all quantifiers and first-order terms, you get $X\to(X\to X)\to X$, which is exactly the simple type of Church integers, perhaps modulo a permutation, i.e. $(X\to X)\to X\to X$ also works. But even in the $\lambda$-calculus there are lots of other numeral systems which may be preferred to the Church integers in some cases). Anyhow, there are at least a couple of reasons why no-one bothered to explicitly define a numeral system for the interaction combinators: 

The fact that $\mathsf{Add}\ u\ v = \mathsf{Add}\ v\ u$ is not provable for arbitrary $u$ and $v$ does indeed follow from the Church-Rosser property. Write $u =_\beta v$ if $u \mathrel{({}_\beta\!\!\leftarrow\cup\rightarrow_\beta)^\ast} v$, where $\rightarrow_\beta$ is $\beta$-reduction in System F and $(\cdot)^\ast$ is the reflexive-transitive closure operator (this is a possible definition of $\beta$-equality, which Breazu-Tannen and Coquand denote simply by $=$. Here I will use the $\beta$ subscript and keep the $=$ symbol for strict equality, i.e., $\alpha$-equivalence if you like). It follows immediately from the definition that if two terms have a common reduct, i.e., $u\rightarrow_\beta^\ast w$ and $v\rightarrow_\beta^\ast w$, then $u=_\beta v$. The Church-Rosser theorem states that the converse also holds: $$u=_\beta v \quad \textrm{implies}\quad \exists w.\ u\rightarrow^\ast w\textrm{ and }v\rightarrow^\ast w.$$ It follows that $\beta$-equivalent normal forms are necessarily identical: $$u,v\textrm{ normal and }u=_\beta v\textrm{ implies }u=v.$$ Read contrapositively, the above says that different normal forms can never be $\beta$-equivalent. Now, suppose $\mathsf{Add}\ u\ v=_\beta\mathsf{Add}\ v\ u$ for arbitrary $u$, $v$. In particular, this holds when $u=x$ and $v=y$ are distinct variables of type $polyint$. But it is easy to show (just apply a couple of steps of $\rightarrow_\beta$) that $\mathsf{Add}\ x\ y=_\beta u'$ and $\mathsf{Add}\ y\ x=_\beta v'$ with $u'$ and $v'$ distinct normal forms. Contradiction. 

Augmenting Noam's answer: Removing the implicit currying, $f : A \to B \to C$ is the same thing as $uncurry( f) : A \times B \to C$. Strong monads $T$ give a map (two, actually!): $dblstr : T A \times T B \to T (A\times B)$. We therefore have a map: $ T A \times T B \xrightarrow {dblstr} T(A\times B) \xrightarrow{uncurry(f)} TC $ If we instantiate this to the continuation monad, we obtain your construction. Generalizing to $n$-variables, the following should work (I didn't check all the details through). Once we choose a permutation $\pi$ over $n$, we have a $\pi$-strength morphism $str_{\pi} : T A_1 \times \cdots \times T A_n \to T(A_1 \times \cdots \times A_n)$. (The monad laws should guarantee that it doesn't matter how we associate this permutation.) Therefore, for every $n$-ary morphism $f : A_1 \times \cdots \times A_n \to C$, we can construct: $\gamma f : TA_1 \times \cdots \times TA_n \xrightarrow{str_{\pi}} T(A_1 \times \cdots \times A_n) \xrightarrow{Tf} TC$. But I still don't think this really gives you the answer you're looking for... 

First, the property "having first-class functions" is a property of a programming language, not of particular functions in it. Either your language allows for functions to be passed around and created at will, or it doesn't. Functions that accept or return other functions as arguments are called higher-order functions. (The terminology comes from logic.) Functions that don't are called first-order functions. Perhaps this latter notion is what you wanted. 

Edit: as suggested by Guido, I add a further note on the additional interest of confluence with respect to uniqueness of normal forms (UN), which I had not thought of when writing my answer. Confluence implies the following property: $N\simeq_\beta N'$ with $N$ and $N'$ normal implies $N=N'$ (syntactic equality). In other words, distinct normal forms cannot be $\beta$-equivalent. (This immediately follows from the characterization of $\simeq_\beta$ as $\sim$ given above). This property is interesting because it implies the consistency of $\simeq_\beta$ seen as an equational theory, as soon as there is more than one normal form (as is the case, for instance, in the $\lambda$-calculus). On the other hand, UN is not enough to obtain the above property, as shown by the following rewriting system: $$N\leftarrow M\to\Omega\leftarrow M'\to N',$$ in which all five terms are distinct and $\Omega$ is a term that reduces to itself. We have $N\simeq_\beta N'$ and yet $N$ and $N'$ are distinct normal forms. 

At the request of Andrej and PhD, I am turning my comment into an answer, with apologies for self-advertising. I recently wrote a paper in which I look at how to prove the Cook-Levin theorem ($\mathsf{NP}$-completeness of SAT) using a functional language (a variant of the Î»-calculus) instead of Turing machines. A summary: 

Augmenting Andrej's answer: There is still no widespread agreement on the appropriate interface monad transformers should support in the functional programming context. Haskell's MTL is the de-facto interface, but Jaskelioff's Monatron is an alternative. One of the earlier technical reports by Moggi, an abstract view of programming languages, discusses what should be the right notion of transformer to some extent (section 4.1). In particular, he discusses the notion of an operation for a monad, which he (20 years later) revisits with Jaskelioff in monad transformers as monoid transformers. (This notion of operation is different from Plotkin and Power's notion of an algebraic operation for a monad, which amounts to a Kleisli arrow.) 

I want to strengthen Alexey's answer, and claim that the reason is that the first definition suffers from technical difficulties, and not just that the second (standard) way is more natural. Alexy's point is that the first approach, i.e.: $M \models \forall x . \phi \iff$ for all $d \in M$: $M \models \phi[x\mapsto d]$ mixes syntax and semantics. For example, let's take Alexey's example: ${(0,\infty)} \models x > 2$ Then in order to show that, one of the things we have to show is: $(0,\infty) \models \pi > 2$ The entity $\pi > 2$ is not a formula, unless our language includes the symbol $\pi$, that is interpreted in the model $M$ as the mathematical constant $\pi \approx 3.141\ldots$. A more extreme case would be to show that $M\models\sqrt[15]{15,000,000} > 2$, and again, the right hand side is a valid formula only if our language contains a binary radical symbol $\sqrt{}$, that is interpreted as the radical, and number constants $15$ and $15,000,000$. To ram the point home, consider what happens when the model we present has a more complicated structure. For example, instead of taking real numbers, take Dedekind cuts (a particular implementation of the real numbers). Then the elements of your model are not just "numbers". They are pairs of sets of rational numbers $(A,B)$ that form a Dedkind cut. Now, look at the object $({q \in \mathbb Q | q < 0 \vee q^2 < 5}, {q \in \mathbb Q | 0 \leq q \wedge q^2 > 5}) > 2$" (which is what we get when we "substitute" the Dedekind cut describing $\sqrt{5}$ in the formula $x > 2$. What is this object? It's not a formula --- it has sets, and pairs and who knows what in it. It's potentially infinite. So in order for this approach to work well, you need to extend your notion of "formula" to include such mixed entities of semantic and syntactic objects. Then you need to define operations such as substitutions on them. But now substitutions would no longer be syntactic functions: $[ x \mapsto t]: Terms \to Terms$. They would be operations on very very large collections of these generalised, semantically mixed terms. It's possible you will be able to overcome these technicalities, but I guess you will have to work very hard. The standard approach keeps the distinction between syntax and semantics. What we change is the valuation, a semantic entity, and keep formulae syntactic. 

I don't understand exactly what you are looking for, I'll try to explain the Curry-Howard correspondence in a nutshell, you'll let me know if it helps. The Curry-Howard correspondence (or isomorphism, if you wish) definitely links the three objects you mention: it actually tells that two of them, IL and $\lambda$c, are the same thing. The term is used today in a broad sense and often without referring to a specific technical statement, but it can be formulated precisely in at least two frameworks: 

As I said in my comment, the answer in general is no. The important point to understand (I say this for Viclib, who seems to be learning about these things) is that having a programming language/set of machines in which all programs/computations terminate by no means implies that function equality (i.e., whether two programs/machines compute the same function) is decidable. An easy example: take the set of polynomially-clocked Turing machines. By definition, all such machines terminate on all inputs. Now, given any Turing machine whatsoever $M$, there is a Turing machine $M_0$ that, given in input the string $x$, simulates $|x|$ steps of the computation of $M$ on a fixed input (say, the empty string) and accepts if $M$ terminates in at most $|x|$ steps, or rejects otherwise. If $N$ is a Turing machine that always immediately rejects, $M_0$ and $N$ are both (obviously) polynomially-clocked, and yet if we could decide whether $M_0$ and $N$ compute the same function (or, in this case, decide the same language), we would be able to decide whether $M$ (which, remember, is an arbitrary Turing machine) terminates on the empty string. In the case of the simply typed $\lambda$-calculus (STLC), a similar argument works, except that gauging the expressive power of the STLC is not as trivial as in the above case. When I wrote my comment, I had in mind a couple of papers by Hillebrand, Kanellakis and Mairson from the early 90s, which show that, by using more complex types than the usual type of Church integers, one may encode in the STLC sufficiently complex computations for the above argument to work. Actually, I see now that the needed material is already in Mairson's simplified proof of Statman's theorem: Harry G. Mairson, A simple proof of a theorem of Statman. Theoretical Computer Science, 103(2):387-394, 1992. (Available online here). In that paper, Mairson shows that, given any Turing machine $M$, there is a simple type $\sigma$ and a $\lambda$-term $\delta_M:\sigma\rightarrow\sigma$ encoding the transition function of $M$. (This is not obvious a priori, if one has in mind the extremely poor expressive power of the STLC on Church integers. Indeed, Mairson's encoding is not immediate). From this, it is not hard to construct a term $t_M:\mathtt{nat}[\sigma]\rightarrow\mathtt{bool}$ (where $\mathtt{nat}[\sigma]$ is the instantiation on $\sigma$ of the type of Church integers) such that $t_M\,\underline{n}$ reduces to $\underline{1}$ if $M$ terminates in at most $n$ steps when fed the empty string, or reduces to $\underline{0}$ otherwise. As above, if we were able to decide that the function represented by $t_M$ is the constant $\underline{0}$ function, we would have decided the termination of $M$ on the empty string. 

Also, sometimes properties don't commute on the nose, and the structure under consideration is of higher-dimensional (i.e., 2-categorical). As to your question 3: you can define a category without mentioning objects at all (though it's conceptually clearer if we do mention the objects). 

Disclaimer: I can only vouch for my research fields, namely formal methods, semantics and programming language theory. The situation is possibly different in other parts of the discipline. It seems that TCS has become rather conference-oriented. Researchers aim at publishing in the next conference. Sometimes a journal version appears. Sometimes it doesn't. In other disciplines (biology, mathematics, and most others I guess) this is unheard of. The effort put into writing the conference papers is a lot lesser, but in turn, the conference papers count a lot less. The "real deal" is the journal publication. Arguing whether this situation is good or bad could result in a flame war, and doesn't have a precise answer. Instead, let's try a more factual question: How did we become so conference-oriented? How did conference papers gain so much weight? 

Note that most of the categories you considered are 'abstract', i.e., they require structure or properties of an abstract category. As computer scientists we should also be familiar several concrete categories which turn out to be useful: Concrete categories 

(This is an extended comment). I may be misreading your definitions, but it seems to me that the relation you introduce, let us call it $\simeq$, is not an equivalence relation because it is not transitive. If $V_1,V_2$ are two distinct closed normal forms (values), then obviously $V_1\not\simeq V_2$. On the other hand, the genericity lemma (Proposition 14.3.24 in Barendregt's $\lambda$-calculus book) says that given an unsolvable term, say $\Omega$, for any context $C$, either $C[\Omega]\!\Uparrow$ or $C[M]$ has the same normal form (a value, if we assume to work with closed terms) for every term $M$ (including $\Omega$). This shows in particular that $\Omega\simeq V_1$ and $\Omega\simeq V_2$, hence the failure of transitivity. 

where $\mathbf M_3$ and $\mathbf M_3^\ast$ are the multiplexor and demultiplexor of rank 3 as defined in Lafont's Interaction Combinators paper (I&C, 1995). They both consist of two $\gamma$ cells and they annihilate with each other generating 3 parallel wires. 

The practical reason is that it is very convenient to include also the case "zero steps" in the definition of "many steps" (millennia of mathematical experience have taught us that it is usually a good thing to have a 0 around in our set of natural numbers). One possible technical exemplification of this (but there are probably dozens more, perhaps more interesting than this one) is that $\to^+$, the transitive closure of reduction, does not satisfy the diamond property (a.k.a. confluence), whereas $\to^\ast$ satisfies it. For example, if $I:=\lambda x.x$, there is no way to close the following critical pair by means of $\to^+$: $$I \leftarrow (\lambda x.I)(II) \to (\lambda x.I)I$$ To close the span, you need to consider the empty reduction $I\to^\ast I$. 

Lexical closures are an implementation technique in languages with first-class functions. I'm interested in a simple operational description of function closures. Does anyone know of such a description? 

There is a paper in this year's ICFP, refinement types for Haskell. The paper deals with termination checking rather than full Hoare logic, but hopefully that's a start in this direction. The related work section in that paper contains some pointers, such as Xu, Peyton-Jones, and Claessen's static contract checking for Haskell, and Sonnex, Drossopoulou, and Eisenbach's Zeno and Vytiniotis, Peyton-Jones, Claessen, and Rosen's Halo. 

My knowledge is a bit stale, as I haven't actively researched this field in the last couple of years. None of these is probably the state of the art, but a good place to start looking backwards (i.e., chase references) and forwards (i.e., see who cites it). If you're looking into information flow (making sure classified information doesn't leak to untrusted roles), a reasonable place to start is Martin Abadi et al's Dependency Core Calculus. I think it's reasonable enough that anyone who does formal methods in the area would refer to it (directly, or once removed). If you're looking into access control/authorisation (role A says role B controls the data, role B says you can access the data, etc.), Abadi recently published a tutorial book chapter on the subject, so might be a good place to start. If you're looking into authentication (whether the agent saying he is A is indeed A), I defer to someone else. I'll try to have a look later. 

Something very similar, but using light affine logic (LAL) instead of EAL, was attempted a few years ago by Baillot, Gaboardi and Mogbil (you may find the paper here). I think their work may be easily generalized to EAL, which is a more liberal system. As for the features of such language, you have polymorphism natively (EAL is a restriction of second order linear logic). As far as I know, no-one has looked at dependent types, but I don't see why they shouldn't work. In fact, untyped EAL works just as well as typed EAL, because its normalization properties do not depend on types. One consequence is that in EAL you may use arbitrary fixpoint of types (see for instance this other paper by Baillot) and define data types in the natural recursive style (like $\mathtt{list\ A := nil\mathrel | A\ *\ list\ A}$), along with the less natural (from a programming perspective) system F definition. However, by the above remark on untyped normalization, a programming language based on EAL will always be total, which means that you won't have a fixpoint combinator and the use of recursive types is not as natural as you would expect. For instance, take Scott numerals: without recursive definitions (given by the fixpoint combinator) it is hard to express anything beyond constant-time operations with this representation of integers. Therefore, you will still need to use Church numerals for iteration (i.e., $\mathtt{for}$ loops), by using which you will incur in the fundamental stratification restriction of light logics (which gives them their complexity properties): you cannot iterate a function $\mathsf{Nat}\rightarrow\mathsf{Nat}$ which has itself been defined by iteration ($\mathsf{Nat}$ here is the type of Church integers). An example: with some "Church integer hacking", it is possible to define in EAL $\mathsf{dbl}:\mathsf{Nat}\rightarrow\mathsf{Nat}$ such that $\mathsf{dbl}\ \underline{n} = \underline{2n}$ without using iteration. Then, you may iterate $\mathsf{dbl}$ to define the exponential function $\mathsf{exp}$ which, however, cannot itself be iterated. So whatever programming language based on EAL will need to have some kind of mechanism forbidding certain definitons by iteration; it s hard to imagine how such restriction would not result in a language which feels awkward to the programmer. Anyway, no-one forbids you to try and see what you can get! In any case, if you are interested in the relationship between optimal evaluation, EAL and light logics in general, I suggest you take a look at Coppola's papers from the early to mid-2000s.