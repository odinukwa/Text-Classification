Now I want to create an index on LongValue, so that I can easily look up on serialized values that represent numbers. 

I have an intermittent problem with a software installation package that installs our product (written using InstallShield/InstallScript). During the process of the installation, we restart the SQL Browser Service. Most of the time this works fine. But occasionally - and I have not worked out how to reproduce this predictably - the service fails to restart, and I find in my "Services" manager that the service status is set to "Disabled". Any ideas what would be causing the service to be disabled, and how to prevent it happening? 

Both tables have an index on their date field. Now, I want to create a view that has a full outer join between the two tables on the Date field, to show me the value for that date (if there is one) along with the for that date. 

I have been given a DMP data pump export file to import into my local Oracle instance. I've tried running this command line: 

I'm investigating the benefits of upgrading from MS SQL 2012 to 2014. One of the big selling points of SQL 2014 is the memory optimized tables, which apparently make queries super-fast. I've found that there are a few limitations on memory optimized tables, such as: 

What I would do is to create a table with the stock_id (that can be the alphanumeric code or a integer), the timestamp of the measurement and the current value. That is your entry data, 3 columns. From that point you can add columns for calculations (the difference absolute or percent) with the previous value. Having all in the same table will simplify the model and ease your queries. Try to create a date (not timestamp) column and create a partition by it. It may lighten a bit the access to the table as long as you set it in your queries. 

As you cannot use the vendor name as identifier, since it may be duplicated (as it happens with the name of a person), you need a true identifier to be set upon creation. When someone identifies themselves in your system, they cannot identify by their name, since it is not enough to uniquely identify them. Thus you don't have any other option than assigning them any proper id when you create the record on your vendors table and identify them by that id. For this purpose you can create an ad-hoc column (filled in its most simple approach with an auto incremental series), or use a contact email, Tax identifier or any other key that you know is unique, instead of the name. 

I have a business requirement to have a FK field on referencing . If there is a value in , it should be unique, but it's not mandatory, so there could be multiple records with null values. Is it possible somehow to enforce this type of uniqueness on the DB level, using either an index or a constraint? 

In other words, for all records where is 1, I want to see the and , alongside the values of and that are returned from for an input of . What's the syntax to do this? 

Checking our production logs from yesterday, we discovered a period of about 5 minutes when a whole bunch of really simple queries were timing out. Further investigation on the server logs showed a huge spike in disk activity, which led me to the conclusion that an automatic CHECKPOINT was being run on the DB at that time. That's something I really don't want to happen during peak hours. So I was thinking of scheduling a daily CHECKPOINT every day during off-peak hours. Is that a good idea? Bad idea? Waste of time? If not that, then what? 

I have often found that when I am having performance problems on my MSSQL database, I can resolve them by running . I learned this because that's always the first line of defense from tech support, right after, "Is your computer turned on?" My understanding is that SQL Server is supposed to keep stats automatically; I shouldn't need to nanny it. So I'd like to understand: what circumstances might lead to the stats falling out of date so badly that I need to update them manually? 

I'll make an assumption first. A real world person could have more than one user in your system? If a real world person (persons table) can have (and should have) only one user, why do you allow a one to many relationship between those two tables? Or I've misunderstood your model or I would create in the Logical Data Model only one table for users AND persons with all the information related to that person, including the email address. If you want to store two different email addresses you can use two different columns, as far as you don't intent to store an unlimited number of email addresses (like it happens with the delivery addresses you can have associated to your profile in online shops). I would use a user_login NOT NULL column (what you call John user, which has to be an email address used for validation upon signing up) and another column email_contact (what you call John person), with an additional flag for allowance to be contacted or not. Anyway you may want to validate this second email as well. Beside this, you can maintain a person_to_person relationship table for the dates and use it as an indicator to control if a person in publicly available or not and display its profile or not in the application layer. 

I'm trying to create an index on a pretty big table (over 8 million rows). The index creation was taking over 10 minutes, so I selected from from my process spid: 

Now all these different cases for different values of @calcType seem to be wasting a lot of space and causing me to copy and paste all over, which always sends shivers down my spine. Is there some way of declaring a function for CalcField, similar to lambda notation in C#, to make my code more compact and maintainable? I would want to do something like this: 

I tried to include the execution plan, but that sent me over the 30,000 character limit. The crazy thing is that I'm supplying AssessmentID = 1538, which is the most useful limiting information in the query, but the execution plan is all but ignoring this fact, and scanning almost every other joined table, only filtering down by AssessmentID kind of as an afterthought... 

First critique: I don't think a should consist of so many fields. A primary key is just meant to be a simple identifier of uniqueness. Arguably you could have two fields in your PK for many-to-many intersection tables, though I personally always have a single-field primary key, even for these. So I'd say your PK really should be redefined as a unique index. Then I would suggest adding to the fields of that index: 

Normalization are a set of techniques that, in a theoretical scenario, are intended to pursue consistency and ease of management and save costs. However, since you cannot work with a theoretical scenario, but with a real DBMS, you may find that a database with a certain high degree of normalization may be hard to use, and hard maintain in terms of coding and daily maintenance in a real-world scenario. Said that you may find that, under certain circumstances, you can achieve the same goal of normalization by means of other techniques while not compromising computation cost or maintenance cost, and with an acceptable increase in storage cost. This is why, along with Normalization, there is another technique called Denormalization. Of course you should do specific benchmarking tests and case studies to know which queries are the most common ones, so you can choose which columns to denormalilze. For example, if you have a country with several different country specific columns, and you have the country id (say, an INT) as a FK in the rest of your tables, as probably that id is meaningless (as intended, since it is an ID), you may foresee that every single query will need to join with the country table to retrieve the country name. So you may want to denormalize the common country name, which is used for daily reporting, while you may find that you don't need to denormalize the formal country name, which is used much less common. So analyzing the real use of your database you may try to improve their computational costs. After 17 years designing and managing databases, particularly in the DWH world, my rule of thumb is start at 3FN which to me is a good trade off between usability, consistency and computational, maintenance and storage costs. 

These all qualify as nuisances, but if I really want to work around them in order to gain the performance benefits, I can make a plan. The real kicker is the fact that you can't run an statement, and you have to go through this rigmarole every time you so much as add a field to the list of an index. Moreover, it appears that you have to shut users out of the system in order to make any schema changes to MO tables on the live DB. I find this totally outrageous, to the extent that I actually cannot believe that Microsoft could have invested so much development capital into this feature, and left it so impractical to maintain. This leads me to the conclusion that I must have gotten the wrong end of the stick; I must have misunderstood something about memory-optimized tables that has led me to believe that it is far more difficult to maintain them than it actually is. So, what have I misunderstood? Have you used MO tables? Is there some kind of secret switch or process that makes them practical to use and maintain? 

Thanks to all who answered - really appreciate the points you've given me to think about. The general feeling I got was that a single database is preferable, but I would like to add some countervailing points in favor of the sharded architecture, and addressing some of the concerns that other people have mentioned. Motivation for sharding As mentioned in the (updated) question, we're aiming for massive sales worldwide, with literally millions of users. With the best hardware and indexing in the world, a single DB server won't take the load, so we have to be able to distribute across multiple servers. And once you have to look up which server any given customer's data is on, it's not much more work to give them a dedicated database, which makes things simpler in terms of keeping people's data neatly segregated. Response to Concerns 

ETL is not a tool, but the process or group of processes intended for data integration from a source system to the destination system, generally a Data Warehouse. So if you are doing an extraction, transformation and further loading of that transformed data, you are performing de facto ETL. Someone could argue that it is harder to maintain, and more prone to errors. And they'd be right. But a poor code does not convert it into something different than code. Same applies. The tricky part is the final load process in the sense of being moving Excel files in a file system. We use at work a MapR DB based on MapR FS made out of independent files and yes, we do data integration and we finally "load" those files into their final destination, despite they are actually just files. In the end they are accessed by a Query Engine. So in the present day I would consider it an heterogeneous database and your system an ETL. Don't see why not. $URL$ 

Why do you have two separated tables? Your model is faulty by design. You don't need in this case two different tables. Just create one table for user accounts and a different one for user account type. 

And no data is imported. From what I've Googled, one possible cause of this is that I need to specify . But I have no idea what the name is of the schema in the dmp file. Any easy way to find out? EDIT: I didn't find a solution to this question, but I did find a workaround... I tracked down the guy who made the DMP, and beat got the schema name out of him. Specified according to his definition, and Hey Presto! 

I believe the mystery is cleared up. The documentation only says that the SQL Server account needs full access to the filestream folder, but when I checked who had access to the main DATA folder, I saw that the SYSTEM account also had access. I gave full rights to the FS folder to the SYSTEM user, and now I don't get this error anymore. 

Indeed, that was the answer. I set the db recovery model to "Simple", waited a minute for the filestream data to clear up, and then I could remove the filestream file and filegroup. 

What am I missing? UPDATE: here's a clue. If I delete as a user from under the database's security context, then I go to the user under the server's security context and under "User Mapping" I grant access to the database, then it starts working. It could be that the database was originally restored from a different server, which also has a user. I guess then that the user identification is not based on the username?