I'm auditing switch interface descriptions at work. I work nights and sometimes people during the day shut down their computers when they leave, so I don't see which computers are on interfaces when I'm not there. I have a mix of Cisco 3500 and 3600 series switches where it's possible to use curl (then some grep statements) 

Why when we had 20 Mbps service did we not have this problem where any one user could monopolize all the bandwidth to the internet? Am I correct in thinking that Quality of Service configurations would prevent this or does QoS simply identify which traffic has priority over another, and does nothing for competing traffic of the same type? If it's the latter, and my vocabulary is wrong, what statements do I need to configure and where (we have a Cisco 4507 core switch, a Cisco Pix on the primary ISP, a Cisco ASA on the backup ISP, and the inaccessible ISP router is a Cisco 2911) so that no one user can take all the available bandwidth to our internet connection? 

Linux will choose a new local port number for each connection you open. To see what range of port numbers your kernel is configured to use you can type: 

As previous answers also points out the solutions are going to involve either having a private connection between the two data centers or having enough IP addresses to advertise a block from each data center. Those two options are however not mutually exclusive and there are a few more aspects to keep in mind when configuring this. How to advertise if you have enough addresses You'll likely end up deciding to get an IPv6 prefix which is short enough to advertise one half from each data center, which means a /47 or shorter. You then have a choice to make in how to announce this. 

to get the running config, MAC address table, interface descriptions, etc. I'm using this to generate reports of MAC address tables and int descriptions when I'm not at work so I can audit switch interfaces to see what phones and PCs are online when I'm not in. I have MAC addresses documented from machines when they are unboxed, so I can compare the MACs I have with the ones reported in the automated curl statements. This will also be used to see if anyone's violating the "Don't BYOD" policy. This has worked flawlessly with the 3500s, 3550s, 3560s, and 3650s I have. I also have some sg300s that don't appear to be able to do that. Is that an accurate assumption? 

For historical reasons that sysctl has in its name, but the setting does in fact apply to both IPv4 and IPv6. The default range is 32768 - 61000. If your question is not what it will use for new connections but rather what it is using for currently open connections, you can use the command. For example I usually use: 

Those numbers are assuming the provider only hand out prefixes on nibble boundaries and that you want a /64 per VM. The overlap between the ranges is because I allowed for an HD-ratio anywhere between 80% and 95%. The reasons I would recommend getting a prefix routed to the physical machine and subdivide that into a link prefix per VM are twofold. 

We upgraded from 20 Mbps to 50 Mbps DIA about two months ago. This required a router upgrade as well. Since then, anyone who starts a large HTTP download - e.g. an ISO, large spreadsheet or log, movie file, etc. - can take all of the available bandwidth and block others from accessing the internet. The old router didn't have any kind of QoS configured on it. Further, it was a very vanilla config. It simply had some named ints, the security levels of each, and the IP addresses associated with each int. The ISP supplied the new router and we don't have access to view the config, but they said they don't have any kind of QoS configured on it either. My questions are: 

If you announce the two different /48 the traffic will be routed across the internet to the right data center, which keeps things simpler for you. If on the other hand you announce just the /47 in both locations you have to get the traffic to the right data center. This may be desirable if you have a private connection between the data centers that you find to be more reliable than the public internet. Doing both of the above will serve as a sort of failover. Usually the traffic will go straight to the correct data center. But your private connection will be there as backup. However if other networks think you are sending them too many announcements they may decide to ignore your /48s and use just the /47, and your private connection will see some more traffic. If you don't have a private connection between the data centers the best choice will most likely be to advertise the two /48 and not advertise an aggregated /47. All of the above applies to IPv4 as well, just with different prefix lengths. What to do if you can't get more IPv4 addresses If you go ahead and advertise a /25 from each data center there is a significant risk the advertisements will just be ignored. Even if it works today there is a risk it will stop working in the future, so you will need a different plan. If you don't have a private connection between the two data centers there is the possibility to use an IPv4 over IPv6 tunnel between the two data centers as a private connection. The obvious drawback of the tunnel approach is that the tunnel is not going to be more reliable than the internet connection between the two data centers. And avoiding using the tunnel by only advertising the specific prefixes isn't an option because those specific prefixes would be too long. An option worth pursuing if you are using the same transit provider at both locations is to advertise both the aggregated /24 and the more specific /25s. What you would need from the transit provider to advertise to the world is the /24. The two /25s you'd only need the transit provider to accept and use within their own network in order for the traffic to be routed to the correct of your two data centers. Obviously before you do anything like that you'd have to discuss it with your transit provider to ensure that it is a configuration they are willing to support. Other caveats with a tunnel Another caveat in case of any tunnel is MTU issues. You need to ensure that you aren't doing something silly on your tunnel which would cause large packets to be silently dropped. Moreover you'd better configure your servers with a low enough MSS that it will work even if the people you are communicating with are silently dropping too big errors. For a setup like the one I describe setting the MSS to 1200 should be safe. If your setup is going to involve any sort of DSR load balancing it is worth keeping in mind that the load balancing may need a tunnel as well. In that case make sure your DSR load balancer is configured such that the tunneling it is doing will be instead of the tunneling to connect your data centers - not another layer of tunnel on top of it. Conclusion The simplest solution is to just get enough IP addresses. But alternatives exist if you absolutely need them. 

on my Cisco Catalyst 4507R. Thing is, there's nothing in that slot. Back in December I did insert a module into that slot, but said the module wasn't supported, so I pulled it out a few minutes later. It appears these messages have posted regularly to my syslog server ever since. There is no clear interval between successive messages; I've seen a difference of four minutes to ten hours between messages. How do I tell the chassis there's nothing in that slot? Also, if I do populate that slot with a module, do I have to do some sort of initialization? The module I tried installing was, I believe, a 48 port RJ45 module. If not that, it could've been a nine or 18 port GBIC module. 

Though it is possible to allocate addresses to virtual machines from the link prefix connecting the physical machine to a router, that is not the approach I would recommend. Rather I would recommend that you get the hosting provider to route a shorter prefix to your physical machine. The prefix length you need depend on the number of VMs: 

Here means no reverse DNS is performed on the IP addresses. means I want to know about TCP connections. means I want to know which PID each connection is associated with (this information will be incomplete if I don't have privileges to see the information or if the socket is open by more than one PID). And means to use wide format (without that argument the IP addresses in the output may be truncated). 

We had a broadcast storm that went undiagnosed for several hours. After the problem switch was unplugged and traffic returned to normal, we had a handful of machines and switches that were broken. On one switch, the sole uplink interface had to be moved to another int. On another, one of the two etherchannel physical members is down. One machine has a SSD and it didn't boot up after it was shut down. BIOS says it's a 32 KB disk. At least one other server's fans would ramp up to high RPMs every couple of minutes. I read that storm traffic not intended for a machine is dropped by the NIC, but broadcast traffic is sent up the network stack and can cause high CPU utilization. I imagine if the OS was writing log files because of the increased network activity it could eventually fill up disk space and/or burn up an SSD because of the increased read/writes. 

One scenario that fits your description of "translate an address from a public IP address to another public IP address" happens every time I access StackExchange. Even though every computer on my network has public IP addresses I cannot access StackExchange directly because StackExchange only supports IPv4. So this answer is being sent through a NAT64 which translates public IPv6 addresses into public IPv4 addresses. Cases of translating between different public IP addresses within the same protocol also happens. I have come across a company which had a of IPv4 space and assigned addresses from that range to internal hosts. But they still used a NAT to translate the addresses such that externally traffic would be seen originate from an IPv4 address outside of that . I don't know why that company chose to use a NAT when they in fact had enough IPv4 addresses to not need it. I think every such deployment could work better without the NAT, but I can't say that with absolute certainty since I obviously don't have intimate knowledge of what happens behind every NAT in the world.