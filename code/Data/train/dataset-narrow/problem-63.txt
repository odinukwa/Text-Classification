A spherical shell of photons Imagine a point light source. Picture an instant where it emits a million photons spread evenly in all directions. At that instant, they are all in the same position, at the central point. A moment later, they have all moved the same distance and are now arranged in a small sphere with the point at its centre. A short time later they are still arranged in a sphere, but now a much larger sphere. As the sphere expands it always has the same number of photons, but they are spread out over the increasing area. Each photon has the same amount of energy it had when it first left the point source, but the photons are more spread out so a given area of the sphere now has less energy due to having fewer photons. When a photon hits a surface, it adds the same amount of energy whether it has traveled 1 metre or 100 metres. The reason the surface looks dimmer when it is further from the light source is that the photons are more spread out across that surface. Source to eye ray tracing If you wrote a ray tracer that started with rays being emitted from a point light source, and then followed them to see what they hit, you wouldn't need the $1/r^2$ term. Objects further from the light would naturally be hit by fewer rays due to the rays spreading out. Eye to source ray tracing Most ray tracers don't start the rays from the light source, as this results in calculating the paths of all the rays that never reach the eye, which is very inefficient. Instead the rays start at the eye and are traced backwards, to see what surface they came from. If the ray was then bounced from that surface in a random direction to see if it hits the point light source, the fact that the light source is a point would make the probability of hitting it zero. So instead $1/r^2$ is used to give a measure of how many rays hit the surface. Geometry of a point source This isn't a property of light, it is a property of a point source. Light traveling in all directions from a point forms spherical shells of photons, and the surface area of a sphere increases in proportion to the radius squared. If you had light being emitted that was not in all directions then the rule would be different. For example, imagine a line light source instead of a point, with all the light being emitted radially (only in directions perpendicular to the line). Now the light forms cylindrical shells of photons, and the surface area of a cylinder increases in proportion to the radius, not the radius squared. Now you would use a $1/r$ term instead of a $1/r^2$ term, and an object would need to be moved significantly further from the light source before seeing a noticeable drop in brightness. In reality, nearly every light source is equivalent to a collection of point sources - every point on an area light source emits light in all directions. Even cylindrical lights like flourescent strip lights and neon signs still emit light in all directions, so the photons form spherical shells rather than cylindrical ones. So the reduction in light level will nearly always be with $1/r^2$. 

In this instance I am trying to model ice cubes on a surface (in air, not floating in water). What techniques do I need to include in order to increase the realism? I am not looking for real time techniques, just to produce still images. I would like the ice to be photorealistic even close up, and to cast realistic caustics and shadows. I've tried using curved edges and coating my ice cubes with a thin layer of transparent material to simulate a melted layer of water, but it doesn't seem to give the impression of being wet. I've also tried embedding a transparent sphere half the size of the cube at its centre, with a fog effect, but it doesn't blend into the cube naturally - it just looks embedded. Even a series of nested spheres with gradually increasing fog still doesn't look right. 

This is modeled as spheres (rather than triangle meshes). The 4 yellowish mirror spheres are hovering just above a very large sphere to approximate a plane, and the whole scene is surrounded by a very large white emissive sphere that provides the ambient sky light. For each pixel sample rays are chosen with a Gaussian distribution around the pixel centre and more samples are chosen until the variance is sufficiently low. Rays reflect in the single specular reflection direction from the mirror spheres, and in any direction from the hemisphere of possible directions when hitting the floor. All rays eventually hit the white sky sphere and the colour is determined based on the losses along the way. Is there some vital step I'm overlooking? 

starts off negative only if > . In this case the total horizontal distance to be covered by the line is more* than twice the vertical distance, which means the first step should be purely horizontal. So if is negative then should not change. Here is the source of the problem: 

Is it realistic to render a super-high resolution image over an array of 3 by 3 or 5 by 5 (for example) stacked screens? Could I do this by combining several different GPUs, or would the only way be to use a GPU per screen and simply divide the image to be rendered? Ideally I'd like to be able to use fewer GPUs than screens. Even though one GPU is insufficient I'm hoping I won't need n GPUs for n screens. If this is possible I'd also need to know whether this requires using the same make and model of GPU or whether unrelated ones could still share processing. 

I'd like to simulate the magnification of very distant objects by the lensing effect of a less distant galaxy. Will I need to model large numbers of point masses or can I get away with just a single average point mass? I can see how to raytrace using hyperbolae for the rays influenced by a single point mass, but I wouldn't know where to begin with multiple point masses. So before I attempt to build this raytracer I'd like to know whether I'm going to be able to avoid multiple masses, and still have believable results. 

Yes, it is possible to use only integer calculations. I will describe how, but bear in mind that the difference in speed between integer arithmetic and floating point arithmetic is not as great as it was historically. If you want your code to run faster, it is best to profile and identify which parts of the code are taking up most time, before considering whether to convert a particular aspect to use integers. 

If you want 10 samples per pixel, use the function from approach 1, and call it 10 times. Currently the code uses approach 2, which causes far more than 10 samples per pixel, exploding to arbitrarily large numbers of rays until the stack overflows. Note that I've simplified the recursive calls in the pseudocode for the sake of explanation, but the same problem presents itself in the real code: calls once, then calls multiple times depending on , then each of those calls to calls again. Each time through this cycle there are times as many calls as the previous time, and Russian Roulette cannot cull enough of these calls to prevent runaway growth. 

I'm trying to learn about raytracing by implementing things in Python 3. I know this is always going to be slower than something like C++, and I know the speed could also be improved by using GPU raytracing, but for this question I'm not looking for general ways of speeding up, but specifically ways of reducing the number of samples required, which will then be useful for any language I may work with in future. 

Code review Doubly finite rays I'm used to defining a ray as a half infinite line segment - having a starting point but no ending point. I notice that this code gives a ray both a starting point and a length. The only place I can see a reason for this is when testing a shadow ray against the light source: the code checks that there are no intersections on the way to the light, so intersections behind the light (or on the light itself) must be excluded. In all other places, the ray is defined with a pseudo-infinite length (). The following suggestion won't affect the correctness of your code, but it may be more readable and avoid you having to work around the need for infinity and having to account for epsilon twice. If the ray is simply a starting point and a direction, then shadow rays can simply check that the first intersection is the light, rather than checking that there is no intersection. For example, by replacing: 

If the scene to be raytraced cannot be stored in memory, then without adding more RAM to the machine it seems unrealistic to render it in a practical time span, due to the need to load different parts of the scene from disk potentially several times per pixel. Is there any way around this? I'm trying to think of some way of performing a large number of the calculations involving a particular subset of the scene all at once, to reduce the number of times it needs to be loaded into memory. Is there any other way of improving the speed in such a case? 

should only be incremented if is not negative. This is causing vertical movement even if the first step should be purely horizontal. The same problem applies on later steps too. The code causes to switch back and for between negative and non-negative in proportion to the ratio between and . must only be incremented when is non-negative. That additional occurrence of prevents from ever staying still, causing a slope of 1 regardless of the inputs. Simply remove that line and it should work correctly. 

This is an image of three spheres on a plane (which is actually a very large sphere). One is emissive, one is reflective, and one is matt (as is the floor). Sampling is adaptive so that pixels that quickly reach a stable colour do not take up much time. I limit the total number of samples per pixel to avoid the rendering continuing for too long. Even allowed to run overnight, the resulting image is very grainy, and experimenting with smaller images suggests this size image (1366 by 768) would take weeks to converge with my current approach. My idea: concentrating samples along colour boundaries I'd like to be able to concentrate samples where they are needed, and to do this adaptively based on previous samples for the same intersection point or pixel. This will give an unknown bias in the distribution of samples, which means taking the average of the samples will give an inaccurate result. Instead I would like to consider the size of the voronoi cell on the surface of the unit hemisphere centred on the point of intersection (for sampling light incident at a point on a matt surface) or on the surface of a small circle (for sampling around a pixel). Assume that all points within that voronoi cell are receiving rays of the same colour as the centre of the voronoi cell. Now an estimate of the average colour can be obtained by weighting according to the area of each voronoi cell. Choosing new samples on the boundary between the two voronoi cells with the greatest difference in colour leads to an improvement in the estimate without needing to sample the entire hemisphere. Samples should end up more densely concentrated in areas of higher colour gradient. Areas of flat colour should end up being ignored once they have a few points near their boundary. The extra complication is that in both cases (sampling from a point on a matt surface, or sampling over a circle around a pixel centre) the simplified approach I have described is roughly equivalent to a large number of samples distributed uniformly. To make this work I would need to be able to bias the average by both the voronoi cell areas and the required distribution (cosine around the surface normal for a matt surface or gaussian around a pixel centre). So I have some more thinking to do before I could test this idea, but I wanted to check first if this has either already been done, or already ruled out as unworkable.