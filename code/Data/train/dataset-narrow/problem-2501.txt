A perfect chess player will always force a win when they can force a win and force a draw when they can force a draw. Of course, at any point if they can force a win, they can also force a draw. Also when ever one player can't force a win, the other player can force a draw. Chess without the 50 move rule or 3 fold repetition rule might not be as hard to solve as you think. It can be shown that adding in the 3 fold repetition rule makes no difference to whether a player can force a win or a draw. The number of possible ways a game can go after n moves keeps growing exponentially with n. The number of states that can occur after n moves on the other hand doesn't keep on growing exponentially because it can't exceed the total number of possible states that can occur in a legal game. According to $URL$ there are about 10^47 states that can occur in a legal game of chess. Chess can be solved as follows: take a set of states that we can prove contains all states that can occur in a legal game of chess without the 3-fold repetition rule or 50 move rule. Two different states could have the same arrangement of chess pieces and differ by whose turn it is, whether you have the right to capture by en passant, and whether a given king or rook has the right to ever castle again. Next, take all states where the minimum number of moves white can force a win in is 1 which must occur on white's turn. Next take all states where the minimum number of moves white can force a win in is 2, which means it's black's turn and no matter which move they can make, white can force a win in 1 move. Next take all states where the minimum number of moves white can force a win in is 3, which means white has a move that will give them a forced win in 2 moves but can't force a win in 1 move. Next take all states where the minimum number of moves white can force a win in is 4, which means it's black's turn and no matter which move they make, white can force a win in 3 moves but white can't currently force a win in 2 moves. Once we get to a number such that there are no states where the minimum number of moves white can force a win in is that number, we've already found all the states that white can force a win in. We can find all states that black can force a win in in a similar way. All the remaining states are ones where both players can force a draw. Since there are about 10^47 states that can occur in a legal game of chess, it would take more than our lifetime to use brute force to build a computer that will play chess perfectly no matter how it's opponent plays. I believe it hasn't been proven that there's no much shorter algorithm that can tell you how to play perfectly no matter how your opponent plays. For instance maybe only a small fraction of states that can occur in a legal game can occur in a game where you play the way that algorithm tells you to play so that algorithm works even though it only tells you how to play perfectly in all states that can occur when you have always followed that algorithm since the beginning of the game but not in all states that can occur in a legal game. Maybe in addition to that, that algorithm is a complex algorithm that for each state that can occur in a game where you have always followed it, takes way fewer steps to compute an optimal move than the number of states that can occur in a game where you have always followed it. According to $URL$ the evolutionary learning laboratories are planning to solve complex problems. Maybe some day, they'll figure out a complex strategy for playing chess perfectly. Maybe even if an algorithm that's very short and takes very few steps to compute an optimal move in any state that can occur in a game where you have always followed that algorithm doesn't exist, that still doesn't stop a human from being able to learn how to play chess perfectly. Maybe a human could continuously figure things out and retain what they figured out figure more things out from what they previously figured out and retain them by some complex method, be able to figure out from the pieces of information they previously figured out how to play perfectly with a 90 minute base time and 30 second increments in any state that can occur in a game where they play the way they play after learning way fewer bits of information than the number of states that can occur in a game where they play the way they do which they can learn in their life time especially if the technology to live 6000 years gets invented. It's probably even simpler for a player to have a strategy that ensures that if their opponent plays perfectly, they will also play perfectly. I suspect both players have a forced draw from the beginning of the game. It's probably simpler to have a strategy that forces a draw than a strategy that guarantees that if your opponent gives you a forced win, you will not lose it. A strategy that forces a draw is also a strategy that ensures that if your opponent plays perfectly, you will play perfectly. If they play perfectly, they will not give you a forced win in the first place so you will not lose a forced win after they give you one. 

The follow link gives an overview of most CMOS gates. Note that "AND OR Inverted" (AOI) and "OR AND Inverted" (OAI) in the link. These circuits are typically a fraction of the size it would take to create the same circuit using their discrete components. For example, a OAI33 circuit (taken from a commercial foundries standard cell library) takes ~$1.62^2$ area, but building the same circuit using the equivalent discrete cells takes ~$3.82^2$ area. 

I find Cooks reasoning curious. Shannons result is valid for all $f : \{0,1\}^n \to \{0,1\}$, therefore if a $\mathcal{NP}$ problem can be described in $\{0,1\}^n$ bits, there must be a {AND, OR, NOT} basis circuit that can decide if it is satisfiable in ~$2^n/n$ gates (the actual paper gives a larger upper, but finite, bound for every possible function). What this tells us is that anything that uses more than $n$ gates, where $n$ is the upper bound for the size of the $\mathcal{NP}$ problem, is using more gates than is required. Using a larger, but complete, boolean basis only reduces the number of gates required. Using a different circuit complexity model, i.e. VLSI, gives even "better" result bounds. Curious. But we know for a fact that any solution to a $\mathcal{NP}$ problem that uses more than ~$2^n/n$ "gates" (where gates is used loosely for steps / operations) is doing so in a sub-optimal fashion... and there's an infinite number of ways to find a solution in a sub-optimal fashion. On the Complexity of VLSI Implementations and Graph Representations of Boolean Functions with Application to Integer Multiplication shows that predicting circuit complexity using a OBDD model over-estimates the actual circuit complexity: 

Specifically, note the / gates which are / consisting of arity sized first function feeding the second function, where the number of first function gates is equal to the number of times arity appears. For example, represents "Two 2 input AND gates feeding a NOR gate". My point is: Taken separately, a function can be built using three 2 input OR gates and a 3 input NAND gate, for a total area of ~4.56, not including any area used for interconnect. Yet this primitive can be realized in an area of just 1.72, which means a discrete manifestation of the same boolean function consumes 2.65 times more area. Also note that the area for an $n$ input {AND, NAND, OR, NOR, XOR, XNOR} gate, where $n\ge2$, is much less than the area that it would take to build the same function using discrete 2 input gates. Also note that while the area given for {XOR, XNOR} for this process is "large" relative to the other gates, there are other ways to build the same $n$ input gates using less area. The propagation properties for the more complex primitives is also significantly better than what would be achieved using discrete gates. Why is this important? Because for me, at least, I've spent a simply enormous amount of time sifting through results from complexity theory that are built on a set of assumptions that has the effect of either rendering the result useless or wrong once the assumption is violated. The following is from Steven Cooks $\mathcal{P}$ vs $\mathcal{NP}$: 

A good place to find high speed, compact XOR / XNOR gates is in full-adders and Hamming ECC circuits (which are typically in the critical path). Also, the issue of circuit depth is typically not a concern in VLSI synchronous logic. The only depth of any consequence is the critical path, which defines the maximum clock period. The vast majority of combinatorial logic propagate their results in a fraction of the time for the critical path. Critical paths tend to occur with some combinatorial logic that needs to pass through several areas scattered over a chip. Many times it is possible to "pipeline" combinatorial logic to meet the timing constraints. This has the effect of creating a circuit that takes a new input and produces a new output every clock cycle, but has a latency of $n$ clock cycles before a given input is available on the output. This tends to make most circuits ~$O(1)$ in practice. You may find the following paper of interest, which discusses VLSI $AT^2 = \Omega(n^2)$ complexity: 

The following describes an eight transistor full adder circuit, which is typically defined in boolean algebra as $s = a \oplus b \oplus c_{in}$. For comparison, a typical 2 input {NAND, OR, XOR, etc} gate is typically composed of four to eight transistors. 

I am asking this question again. I am aware of, and have read the other similar "alternative proof TM" questions, but unfortunately, they do help me. I am looking for a TM Halting Problem proof that does not have the following properties: 

I have spent quite a bit of time looking for alternate formulations of the Halting Problem that are not just simple permutations of the original proof given by Turing. Question: Can you point me to a vetted proof of the Halting Problem that shares as absolutely as little in common with the one given by Turing? Please, instead of arguing with me as to whether or not my reasons are valid, or that I "don't get it and should just accept the proof given by Turing", it would be a great help to me and possibly someone else if you could simply help me locate an alternate proof. Yes, I am looking for proofs with certain properties, properties that inconveniently cull a number of candidates. Despite this, they are properties that I unfortunately need.