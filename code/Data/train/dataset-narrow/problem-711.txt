I recently inherited a MESS of a search. It's around 10,000 lines of code in the procs/functions alone. This search is aptly named the "Standard Search." It's a proc that calls about 6 other procs, which are entirely composed of string builders, in which each proc has between 109 and 130 parameters. These procs also call deeply nested functions which generate more strings to be assembled into a final query. Each proc can join up to 10 views, depending on the logged in user, which are abstracted from the table data by between 5 and 12 other views per primary view. So I am left with hundreds of views to comb through. On the plus side, it does have a parameter to PRINT out the final query, (unformatted of course!) It's a hot mess. The string builders also have no formatting, and they don't logically flow. Everything jumps around everywhere. Although I couid do an auto-format, to pretty print the code, that doesn't help with the formatting on the string builders, as that would involve refactoring the content of strings, which is a no-no. 

Also, feel free to vent about your experiences with tasks like these. I'm sure I'm not the only one who has been handed down tasks like these. 

Say for example, if a defect is reported in this spaghetti mess, for example a column may not be accurate in certain specific circumstances, what would be the best practices to start troubleshooting? If debugging this was a zen art, how should I prepare my mind? :) Are there any preferred SQL Profilier filter settings that have been found useful in debugging this? Any good tactics, or is it even possible to set breakpoints in nested procs when using debug mode? Tips or suggestions on providing meaningful debug logging when dealing with nested string builders? 

See installed roles here $URL$ Create a table with the installed roles. Then select from dba_roles where not exists from the table you created. 

This will show your error within the past 5-minutes. You could put this into a shell script and add a cron job that runs it every 5-minutes. 

In fact, using ORAENV is the preferred method, as this will set your ORACLE_HOME environment variable. 

Try shutting down the database and open a text editor to create a pfile. Only two parameters are required for a pfile - db_name and data_block_size. Then start up the database in no mount mode using the pfile. 

You can manually create the init.ora file. Only 2 parameters are required - db_name and db_block_size. Then run: 

We recently changed the oracle id to 502 and the group oinstall to 502. Everything under the $ORACLE_HOME/bin is owned by oracle:oinstall Since this change we can only connect locally to the database using IPC. When we try to connect remotely using TCP we the error => ORA-12537: TNS:connection closed When the oracle user tries to stop the LISTENER we get the TNS-01190 error. 

If you have more than one database on the server, you may not want to put the SID in your .bash_profile. You could use ORAENV after logging in as the Oracle user. 

A null value means auditing is turned off. You need DB_EXTENDED to capture bind variables and sql statements. 

You will need to re-start your database before auditing begins. This will enable the columns SQL_BIND and SQL_TEXT. To audit a users' session do this: 

The alert log should tell you if a data block is corrupt, no need to validate index structure. That will lock your table until completed. If no backups you might try to exporting the data using expdp. Then drop the table and re-create it and import using impdp. You will need to create the indexes, constraints and grants as well. Could also use CTAS. 

That seems like a complex plan for a view that has less than a thousand rows that may see a row or two change every few months. But it gets worse with the following other observances: 

I have a table that stores version information in multi-dotted strings, ie '4.1.345.08', '5.0.1.100', etc. I am looking for the quickest way to find out which is larger. Is there a faster, or easier way on the eyes than what I have come up with below? One caveat is that I am 90% sure that we should ONLY find dotted numeric values here, but I can't guarantee that in the future. 

Here is why s are being used as predicates. The column is formed by concatenating: During testing of these, a simple from the view returns ~309 rows, and takes 900-1400ms to execute. If I dump the strings into another table and slap an index on it, the same select returns in 20-75ms. So, long story short (and I hope you appreciated some of this sillyness) I want to be a good Samaritan and re-design and re-write this for the 99% of clients running this product who do not use any localization at all--end users are expected to use the locale even when English is a 2nd/3rd language. Since this is an unofficial hack, I am thinking of the following: 

Is there a best practice method to handle localized strings via a view? Which alternatives exist for using a as a stub? (I can write a specific for each schema owner and hard-code the language instead of relying on a variety of stubs.) Can these views be simply made deterministic by fully qualifying the nested s and then schemabinding the view stacks? 

I understand that there may be a difference in meaning or intent between the two, but are there any behavioral or performance differences between a clustered primary key and a clustered unique index? 

Currently whenever I write a query that is adding columns that can contain nulls I resort to wrapping each field in or , such as . Is there a better way to handle this, or is this the standard practice? 

I know that in SQL management studio I can right click a table/trigger/key and . Is there a way to do this programmatically, given an object's name? If so, is there a way to find all objects (primary keys, foreign keys, triggers) associated with a given table and script all of them programmatically? 

Often I find myself wanting to filter or order the results returned by ing a stored procedure - for example to order the output of by login or CPU Time, or filter it by host name. What is the best way to achieve this? 

I have a table with four columns that are all non-nullable, and the data is such that all four are needed to distinguish a unique record. This means that if I were to make a primary key, it would need to comprise all columns. Queries against the table will almost always be to pull back a single record, i.e. all columns will be filtered in the query. Since every column will need to be searched, does having a primary key benefit me at all (besides enforcing uniqueness of records)? 

I came across a view in our database today where the first statement in the where clause was . Shouldn't this return true for every record? Why would someone write this if it isn't filtering any records?