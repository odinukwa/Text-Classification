I'm sure CIFS is working because if I connect with a local machine, it works well, but not via Internet. Thanks for your enlightenment! :) 

I'm having an hard time trying to find out how to reduce this "repeated lines" for my NGinx configuration, just to change one property : 

And that, in the same server. I thought about installing only Apache on the server, that would redirect request to the dedicated Docker instance based on the server name, but then I would have to install Apache (again !) andMySQL on any Docker instances. Is this possible and moreover, is this interesting in term of performance (or not at all)? Thank you for your help. 

Every three months, my Let's Encrypt certificate expires, and my customers get an invalid https certificate. So I recently placed the following cron task : 

I'm looking to set up a webmail server that will be used by a lots of users that will receive and send emails. They will also have the possibility to forward emails they receive. I'd like to know which steps are recommanded/required to indicate to others Mail services (GMail, Outlook, etc) that my server is not used as a spam sender (disclaimer : IT's NOT ! :p) but a legitimate one. I know I have to define a SPF TXT records for example, but what others steps would you recommend me to do ? For example, is there a formula like having a proportional number of servers based on the amount of email sent (for having a different IP address) ? (something like sending a maximum of 1M emails / per IP / per day ?) Something else I'm missing ? I tried to search online, but I mostly find how to avoid emails sent with scripts (like PHP) being put in the SPAM folder. I'm looking for a server/dns configuration side. Thanks a lot for your help/tips, I appreciate ! 

From the check_http man page: -k, --header=STRING Any other tags to be sent in http header. Use multiple times for additional headers $URL$ 

cloudburst attack is more or less a fancy term for "Host code execution vulnerability from a guest operating system." How do you protect against that? The same way you protect against every products security exploits, with updates/patches. What does this means if you are using some virtual hosting provider (SaaS, PaaS, etc)? Pick one whose core virtualization technology is well tested. Xen would be a great candidate since it's open source and therefore receives external code reviews. If you are implementing virtualization in house use the most recent stable version and implement any additional host protections possible, such as chroots, selinux, jails, kernel patches like grsec, etc. 

Before you purchase a Cert from a trusted provider you must generate a CSR (certificate signing request). In your case this can be done through the CPanel interface as described here: $URL$ Once done copy-paste the CSR into the purchase wizard at the SSL provider you choose. Be careful about which email address and domain name you enter for the CSR. Also, read the SSL providers CSR directions requirements as well. Once the SSL has been generated you just go back to CPanel and copy-paste or upload it. Directions are simple and should also be in the aforementioned wiki. 

Well, I'll answer myself on that one. The problem is not from NGinx but from the backend, here, PlayFramework that returns a 404 when a HEAD is requested and the routes files does not contains HEAD. A bug has been opened for that : $URL$ 

What is wrong with this configuration ? Why a HEAD returns 404 instead of 200 ? Thanks for the help :) 

But still, the problem persist. I don't know what I missed and where to fix it. And everything I tried to search on Google doesn't really helps me (it's about /etc/aliases most of the time). Thank you for your help. (if you need more details, please ask, I'll add them) 

I want to execute a local (or remote (via http)) script when someone hits one of my website with GET request. I saw the mod_actions module for Apache which would be great if it would also work for simple GET requests : 

I'm having some security issues regarding my uWSGI configuration. Here's the current issue: I have a front server, called api.domain.tld, that have NGinx installed with that points to the uWSGI instance using a file. This uWSGI instance on the frontend server is configured that way: 

For a quick change do: "MaxRequestsPerChild 4096" to something like: 700 will help. The longer an apache process lives the more resident memory it's going to consume due to mod_php and the like. Also, enable keepalive and place aggresive timeout settings for it: 

The default configuration on most distros is going to be pretty secure. It's up to you to make it otherwise. ;) So before implementing custom htaccess rules, enabling symlink support, adding new modules, etc be sure to ask yourself how that change relates to security. Research that particular directive in the context of security via google and apache.org. The default configuration of Apache, though secure, may have modules and features you do not require. So you may wish to disable unnecessary modules, cgi support, ssi support, directory browsing, etc. There are a slew of articles available search for "hardening linux" and start with ones that don't include mod_security or re-compiling. I recommend testing each change as you make it so as to know which ones break your site. Also, often more important that securing Apache itself is securing the content Apache serves. Read about proper site permissions and the configuration files related to the languages used (php.ini). Sorry it's hard to be more specific without a more specific question. I obviously don't want to duplicate Google search results. 

(The ip is the IP to my server). I also tried this (interesting tool) : $URL$ The only problems are related to HTTPS, everything else is good. Having "everything else good" is what bother me : if it's good, why Google still reject my emails ?? What did I do wrong ? Do I have to add a SPF also to mail.mediafins.com, my MX DNS entry ? Thank you for your help, I really appreciate ! 

What does that mean ? I did the same command with a smaller file and I got a , so clearly, the first try didn't succeed completly, but I don't know what went wrong neither what to do to make it work completely. Is there a possible solution to that ? Thanks for your help ! 

My question is pretty straightforward. Using OpenOffice or Microsoft Office, is it possible to automatically lock a file for edition on the Alfresco file system via Samba? For example, if a user goes to the samba directory, click on a file for editing it, it would be locked. When he finish to edit it (close the document), this document would be reopen to everyone. Thanks for your help :) 

As TomTom said, I would contact the host and see what sort of scheduled maintenance, if any, is performed during those hours. Most likely the VPS instance is being backed up or something. Sucky no matter the reason! 

I'm coming from munin and a CPU graph contains data for system, user, nice, etc ALL on one graph. I just installed ganglia and setup the basic monitoring. It appears that each type of cpu data is a separate graph! WTF is this and can I change the defaults to combine these into a single per host? That is my question, how do I combine cpu data into a single graph. Also, can I change the layout to something closer to munin's day-week side-by-side layout? I'm trying to be impartial and give ganglia a chance. ;) 

Reboot and look for a RAID BIOS/config option (F7, etc) key will depend on RAID controller. Within the RAID BIOS you can most likely re-configure the virtal disk to be a single disk instead of two mirrored disks. This will clear the way to reformating the second drive within windows disk management interface as drive d. The specifics are dependent on the RAID controller, see their sites support section for limitations. Backup first! 

Is this possible? Would it work if instead of a socket I would allow a local (127.0.0.1) only access ? Thank you for your insighs. 

I'm not quite used to IPTables and I'm trying to run an iptables script to allow only ssh connection from all and connection to mysql server only from specified IPs. I made a bash script for this, which is lister under, but when I run this, my master-master replication stops working. For information, here's my network structure : 

My server distribute two main websites, says : www.google.com & www.facebook.com (yeah I know :p) I want them to be distributed via https. Using Apache, I defined a vhost file in sites-available/enabled containing this : 

I saw the directive, that is exactly what I want, but do I have to specify for every location {}, the proxy_* configuration ? I have a specific message to display if the user reach the quota allowed, in my app (located at /errors/413). If I go directly using my browser, it works. But using the configuration below, I've got a "504 Gateway Time-out". Why? 

Just of different way of thinking about it. I don't think you have anything to worry about, nothing is going to bite as long as you leave the directory structure untouched. Obviously other packages are going to drop their own .conf files into the conf.d dir so it would be a bad idea to rename or move it. The modules-enabled/available is eliminated but don't count on the include line being in the conf.d file for all packages. Some still add it to the httpd.conf file via the RPM post install routine. 

I know some people are using keepalived and heartbeat for active/standby but what action is taken if the haproxy process were to die? What would be nice is if the virtual IP would switch servers if the haproxy process were to die and/or a networking issue were to occur. We are currently investigating heartbeat and corosync with pacemaker. Can anyone explain their solution to this issue in depth? UPDATE: Thanks Kyle, see answer and links therein. 

Sounds like your on a tight timeline, so you should probably get some help with this one. If you do choose to go it alone you should know that most VPS accounts are a bit stingy on RAM. It's common to use a web server besides apache for this reason. Lighttpd, Nginx, etc are good choices. Stick with Ubuntu or another distro, they will have all the packages you need available in the default repo. I run CentOS but I end up adding additional repositories and setting repo priorities. Since you aren't all that familiar with such and are on a timeline you should probably stick with a debian based distro at first. Work on getting the site up under an alternative domain such as dev.domain.com so you can test it before the switch date. Only change one thing at a time and test it fully. Don't add APC, additional mysql options, ssl, all at once. I honestly believe that this could be a great learning experience in the long run but disastrous in the short. Everything I know about hosting is due to three years trial and error..not three weeks or so. Good luck. 

On my project, I will allow users to send zip files and images files (on two different form post). The project is developed with Play! Framework (not PHP). I'd like to limit the size of upload for 1Mo if it's images, and 10Mo if it's zip. Is this possible ? I saw the directive that should do what I'm looking for, but I can't find a way to apply this per mimetype. Thanks for your help :) 

I manage servers where users have their own websites on it that can be accessed by FTP (like an hosting company) and instead of working on isolating LAMP stack processes, I was wondering if it was possible to implement Docker and use an images per website. From what I understand, you can expose Docker instance via their ports, so if you run two docker instance on the same server, you'll have to expose two different ports. But is it possible to export not ports, but server name, like : 

All my users in /home/{user}/ have a specific error_log file in it that may grow overtime. So I was thinking about using logrotate to implement some kind of file reducing on it : when the file reaches 500kb, we remove the first lines to reduce it to lower than 500kb. It's not important to keep what is removed, so keeping the old lines is not necessary. I took a look at logrotate, and I came to this configuration file, but since I'm new with LogRotate, I was wondering if it would work.