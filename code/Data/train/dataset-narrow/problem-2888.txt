When using an Effect in the Toolkit (which is similar to the XNA Effect or the native D3D11 Effect API), you are not suppose to access directly the constant buffer but instead setting parameters directly on the effect: 

Yes, this behavior changed from DX9 to DX10+, don't know much about the real story behind it, but I suspect that DX9 had to remap registers between VS and PS at runtime anyway (when linking shaders) which is inefficient. In DX10+, when you compile a VertexShader and a PixelShader, you can see which register will be affected to a particular semantic (see output of fx.exe compiler), and these registers (which will almost map to an hardware register) should match between the output of a stage to the input of the next stage. Note that the input of a stage could have less semantics mapped, as long as the order is the same and there is no gap in the mapping. If you are using some legacy FX file (techniques/passes) with DX10+, the compiler will check input/output signatures for you. But if you are compiling separately VS and PS, you will have to double check any signature mismatch (or you can use D3DCompiler API to verify this yourself). There is one exception for the input of the VertexShader that is coming from the VertexBuffer and the InputLayout. The InputLayout is able to specify the byte offsets of each vertex elements and how it maps to the semantic in the VS shader, so you can have a "sparsed" mapping, the input layout can have more vertex elements/semantic bindings than the VS input, as long as all semantics from the VS are covered by the InputLayout declaration. This is handy when you have a model that has several vertex elements, and want to use the same vertex buffers for several vertex shaders that are expecting different input layouts: you can map the layout of the vertex buffer to the layout of the vertex shaders (as long as the mapping is covering all the input of the VS shader). 

There's not much to say about it, just look into basic 2D game tutorials and they should give you enough information. I've never worked with Android so I don't have any links at hand, but you need the following: 

A book which approaches C++ from an intermediate point of view and focused on techniques relevant to game programming. A lot of emphasis on performance and efficiency. Since you asked for that specifically, you might want to give it a try. It's quite good in fact. Effective C++ (and it's 2 sequels) LINK 

Concept I would solve this problem with a sprite hierarchy using a varitation of the composite design pattern. This means having each sprite store a list of the children sprites that are attached to it so that any modifications to the parent are automatically reflected in them (including translation, rotation and scaling). In my engine I have implemented it like this: 

Step 1 - Make a few adjustments to the Sprite class Assuming you already have a class in place (and you should) then you'll need to make a few modifications to it. In particular you'll need to add the list of children sprites, the local transform matrix, and a way to propagate transforms down the sprite hierarchy. I found it easiest way to do that just to pass them as a parameter when drawing. Example: 

Don't know much about internals of MonoGame, but texture are definitely not stored in a ConstantBuffer. The texture is not a constant buffer, it is a texture which has its own slot and is bounded completely separetly from the ConstantBuffer. To get the slot of the texture, this is done through the ShaderReflection object and method reflect.GetResourceBindingDescription(i);. 

Use Device.UpdateSubresource (requires texture to be declared with Usage.Default) or Map/UnMap (requires texture to be declared Usage.Dynamic).In the case of the swapchain, I guess that only UpdateSubresource will work. Keep in mind that you are refering to the low level Direct3D11 API in SharpDX, so there can't be any high level methods like "SetData". If you want to have XNA equivalent API, you have to use the Toolkit which is available from the 2.5 dev package. If you want to stick with plain Direct3D11, you will have to dig into all the details about how Direct3D11 is working. 

Instancing requires to change the InputLayout of vertex buffers and pass an instancing buffer along the mesh vertices. Unfortunately, there is nothing automatic to do this with Toolkit models, though possible but would require to dig into the internals to do this yourself. Before even trying to do some instancing with models, you should start with a basic instancing sample with your own raw vertex buffers/index buffer and effect. You will see exactly what needs to be changed and how to setup/use instancing. With this proof of concept working and with the source code of the Toolkit available, you should be able to figure out how to use existing toolkit Model data to turn it into instancing friendly. 

I tried it in Photoshop and here's what I got - It's not perfect, but at least it preserves the original colors better: 

I can't figure out your code, a lot of parts in it don't make sense, such as normalizing the position. But if I understood the problem, you want to make the model to always be looking and moving in the direction of a certain target. And you already have an helper method called that takes care of the rotation part. Then moving the model should really be as simple as taking the direction between itself and the target, and adding that direction to the model's position multiplied by a certain speed factor. Something like: 

My solution did not imply that the character can only move in one axis at a time. It can still move in any direction, but you apply forces/accelerations/velocities separately on each axis. But I agree it might not be trivial to add these changes if you already have another system in place. But keep the idea in mind in case you encounter the kinds of problems mentioned in the other post. As for the jittering problem, I remember I had it too and the solution for me was by doing on the player position after applying velocity (I did it before handling collisions, but I'm not sure if order matters). You can see that on my example, line 66, and if I remember correctly it was also present in the original sample in the method or something. From your code snippet I can't see if you're already doing it or not, but I recommend it to prevent jittering at certain speeds. Other than that suggestion, there's something wrong with your code. In particular, since is a value type, not a reference type, when you do after moving the player, you're not really pointing to the new bounds of the player. It's still the old value. You need to call again to recalculate the new bounds for the player. 

Not exactly: texture arrays are declared in HLSL as for Texture2D and not as an array of texture, so it is quite different. They are almost acting as a 3D texture, where the z is a slice of the 2D Texture (in terms of uv, it is the w dimension). The difference with 3D texture is that they are not supporting trilinear interpolation between the 2D slices but you can still select dynamically a Texture2D slice with a z/w component (unlike an array of texture), the z/w component is rounded to the nearest integer to select the z/w slice. Concerning the memory, I believe this is equivalent, for performance, not sure they give a huge boost even accessing a set of Texture2D compare to an array of texture (but they support dynamic indexing). Using is also easier as you only need to bind it to a single slot. 

As catflier mentionned, moving from a "high level" framework like XNA to a low level Direct3D11 API would require quite some work in order to achieve the same results. But there are now some options that you could also consider: 

But the method (which uses the DotNetZip library), is synchronous, takes too long, and leaves me no place to yield during the process. This resulted in the application being killed (on mobile devices) whenever I tried to extract a large zip, which I assume is due to the main thread becoming unresponsive for too long. Is this something that mobile OS are known to do (kill the app if a frame takes too long)? So, I assumed that extracting the zip on a separate thread might solve the problem, and it seems to have worked. I created this class: 

You probably only need the second line. You're creating a new list and then overwriting it immediately. Not that it doesn't work, but is wasteful. 

Old topic but I'd like to talk a bit about the subject. About grouping sprites together there are two nice ways to handle that, depending on whether you want all the sprites to be siblings (i.e. grouped but on the same level) or if you want to have an hierarchy (i.e. sprites inside other sprites, where transforming the parent also transforms the children). Here's the general idea for each of them. Scenario 1) All the sprites in the group are siblings: Let's say you have a Sprite class and a Group class with this interface (pseudocode)... 

The main reason Direct3D10 Map methods were moved to Direct3D11 DeviceContext is to support multithreading. They were previously attached to each resource (thus implicitly, a single device), but with Direct3D11, It is now possible to update the same resource from different deffered context. Concerning your issue with MapSubresource, you need to check this documentation on Resource Usage. You will see that it is not possible to use Map method with Usage.Default, as it is only working with dynamic texture. Usage.Default is suitable if you are only using UpdateSubresource. The correct way to use Map is to declare the texture with Usage.Dynamic and Map with WriteDiscard. It is not possible to keep the content of the dynamic texture (update partially) as a race condition between the GPU and CPU would arise. 

If your is pointing to the same texture, you should have the same performance, but if not, then you are just hitting a design restriction of SpriteBatch. Most implems of SpriteBatch I'm aware of (at least, XNA, SharpDX, Paradox, DirectXTk... though, don't know for sure about MonoGame...) are trying to batch draws with the same consecutive texture (see for example here, DirectXtk the C++ equivalent of SpriteBatch is assuming the same here - look at method ). If you switch between textures, the code path is much slower so this is not recommended to use SpriteBatch in this way. As suggested by @Shiro, SpriteBatch is usually more efficient if you have packed all your sprites in a sprite sheet and use this same texture in your whole batch between Begin/End. 

Here's the code I used: (Step 1) First make sure the backbuffer is being created with room for a stencil buffer: 

Since you're not setting an origin in your camera class, the origin ends up being the top left corner of the screen instead of the center of the screen. Without changing anything else, the easiest solution would be to change the assignment to: 

I'll comment on this from a game developers perspective. The author of SharpDX ran a benchmark last year in which he compared the relative performance between using a native D3D11 application written in C++ and several managed alternatives written in C# such as SharpDX, SlimDX and XNA. I'm not sure how accurate these tests really are but the results were: 

Draw the background twice, stacked vertically like on the image below. Of course the image you use must be seamlessly tileable. The red rectangle represents your screen. Start moving both background instances down. You do this by adding some value to their vertical position each frame. When they reach the position represented in the image (e.g. when ), subtract one background height from each of their vertical positions to bring them back to step 1. Repeat. 

The effect is automatically managing to update the constant buffer, upload it if it changes since last apply...etc. Though you can still update the constant buffer directly as you did. 

Minor issue: don't perform any GPU interaction in the update method but only in Draw (in fixed time step, this method can be called several times per frame). The correct way to implement the micro-synthetic test is to do it like this: 

If you are creating a Texture2D with an initial DataRectangle, the Pitch must be set to the number of bytes per row and is theoretically equal to TextureWidth * SharpDX.DXGI.FormatHelper.SizeOfInBytes(Format.XXX), unless you are laying out your data differently in memory. If your are using DeviceContext.MapSubresource, you can't determine in advance what would be the stride and you need to use the Pitch returned by the DataBox.RowPitch. The stride could be hardware dependent, depending on the layout on the GPU memory. 

Using this format when declaring a depth stencil buffer, you should be able to copy the depth buffer to another R16_FLOAT/R32_FLOAT...etc. texture. On a side note, it is often not recommended to read back data on the CPU because of the latency that will be introduced. Current techniques - on Windows Direct3D11 - tend to perform typical CPU computation on the GPU with DirectCompute. 

Seeing if a certain grid configuration matches a certain recipe is simple if you encode the 3x3 grid as a string and use a regular expression match. Speeding up the look up is a different matter, which I'll talk about in the end. Read on for more information. Step 1) Encode grid as String Simply give a char id to each type of cell and concatenate everything side by side in this order: 

I think your idea is pretty much spot on! First calculate a ray for your cursor using both the near plane and the far plane as Z values for your 2D coordinates (i.e. use 0 and 1 for your Z coordinate). Here's an helper method to handle that: 

So make sure you're keeping your vector alive. So if for instance you're just using the vector inside the load method and not storing it somewhere else, your data will be gone by the time the load method returns. Store it as a member variable of your class instead, and then get a pointer to its data with the trick above. In other words something like: