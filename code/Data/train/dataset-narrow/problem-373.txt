This will set all NEW sessions to be mixed binlog format. All existing sessions will be whatever was set previously until they end. You can also do manually to solve any problems with session specifically. 

I know what you mean. Alter table sometimes has to rebuild the whole table when you use it. Assuming that the Alter table statement is annoying you because it takes too long AND it locks your table, then you can use 2 tools to make it an "online" alter table. One if from facebook $URL$ And one is from openark $URL$ (MySQL 5.1+ only) Both these tools basically create a new table, copy the data over and keep it updated by updating both the old and the new table with new queries until the new table is completed. This means that you need to have the space available to do this operation. Hope it helps. 

You can use symbolic links with MySQL. All you would need to do is make a symbolic link for the .frm, .MYI and .MYD files of your MyISAM tables into the MySQL database schema directory that you want. After that, run and they should show up. It is also important that the MySQL user is able to access those files, so you might have a bit of an issue with user rights. Also, depending on your MySQL version, there maybe a setting in the my.cnf which enables/disables symbolic links, so you would need to keep an eye on that too. 

No i disagree. I dont believe it should be faster on your laptop given the spec you have just described. I also dont believe a difference in version or flags should explain it either. A 9000% difference suggests something is going very very wrong. You should expect to get at least the same or similar result on the server, or even better. At this point there are too many variables in your post. I think you need to work through a process of elimination to pin down where issue exists. Is it the server hardware? Is it the sql instance? Is it db specific? Is it the table/indexes/statistics? Is it the query itself? Is it anthing to do with load or external influences? I would start by performing a simple test under the same conditions on both the server and laptop. For example a simple script which inserts a million rows of test data into a table. Run the same script on a few dbs to get a benchmark. If you get comparable performance, then start looking at your tables. Create a maintenance script to rebuild your indexes and update stats. Run the same script on both dbs before you run your query. When you run your query, check the execution plans in both environments. Are they the same? 

We have a number of tables (~1M records) that have a column on them defined as: that gets auto-populated with . We use this ID for synchronizing data across multiple systems, databases, file imports/exports, etc. For this column, we have the following index created: 

As expected, this index gets fragmented pretty quickly so we have to be on the ball with keeping it rebuilt. However, this is where we have a problem. My maintenance script that goes through and rebuilds all indexes over x% fragmented with always hangs forever on these indexes. I've tried to let one run for over 3 hours without success when the database was mostly to completely idle. Just for kicks, I even put the DB into single user mode and nothing changed. I also tried with , again with no change. However, I am able to simply drop/create the index and it completes within ~15 seconds! Just to make sure there's nothing funny with my maintenance script, I've manually tried rebuilding these indexes with no success. The script I use is: and it seems like it will never succeed. Given that I can drop and recreate it in only 15 seconds, 3 hours is WAY more than plenty to see this succeed on an idle database. For now, I've altered my maintenance script to filter out indexes with "GlobalID" in the name and then have a follow-up script that drops/creates these indexes. This gets us by until we start having naming variations on these types of columns (i.e. we need a uniqueidentifier for some other purpose). Any idea why rebuilding such indexes would never finish within a reasonable amount of time? I see this happen across ~12 tables, all with essentially the same column/index on them. 

If its an ETL process high PLE isnt very important. Your focus should be on throughput rather than cacheing. PLE is an indication of how long a page is held in memory. The more volatile the data is the lower the PLE. For lookup of reference data, A high PLE indicates that is is being cached in memory and is being reused without reloading from disk. ETL processes will have low PLE as they are deleting tables and reinserting, anything that queries that data will have to fetch it from disk. That's normal. You will also see high IO (disk reads & writes) when inserting and deleting data, regardless of how much memory you have. All transactions are written to the transaction log (even if your db is in simple recovery mode) before being saved to the datafile. The more tables and queries your data moves through during your ETL the more read and writes you will see. To improve throughput and performance you should investigate if your ETL process is using bulk inserts and truncates rather than individual deletes and inserts, as significant gains can be made here. Depending upon the ETL tools you are using there are a number of features that may assist with throughput performance. I suggest you start by reading about optimizing bulk imports and minimal logging. 

RBR basically updates the slave with the entire row as it is after it has been inserted/updated and updates using the primary key of the table. The advantages to this over statement is if the master server runs a statement which requires some sort of calculation or if it has a non-deterministic function or sub query in it. With the former, you do not need to perform any calculation on the slave as you are only updating the row after the calculation has been applied to it. With the latter, it may resolve race-conditions if the non-deterministic function or sub-query ran at a time when the result would be different then the time when it is run on the slave. So you can use this to keep your data consistent with these types of queries. Also, it is important to note that you can use RBR in a session (if you are using MIXED binlog_format on your server) for part of your application that you feel has these types of statements (add to that if you create your own temp table and need the slave to do the same). You can do in your application code for that specific (and potentially problematic) part of your code. The disadvantages are if on the master, you run one statement that updates 100,000 rows. To update the slave, you just pass that one statement very quickly. In my opinion, in transactions-based applications (where everything is just 1-2 inserted rows), then RBR can be advantageous. But in any case, you should test it out for yourself. 

You mention that you already have a piece of sql that does what you want. So i would suggest you modify that to become an update statement and implement it as a sql task. Another option is to save the update within a stored proc and call/execute it from ssis. With a bit of thought you could probably write an update statement that only sets the values for new records or for records that have changed. Or perhaps do it as part of your ETL insert/merge. However going back to my other comments: i don't think it makes sense to have a single key or dimension for such diverse attributes. I suggest you have a look at some sample schemas to see if it could be done differently. 

If you are comfortable with excel you could import the data directly with an odbc connection. If you wanted olap capability you can do that with a tabular model within excel. In general most BI solutions will work with any relational database via an OLE or ODBC connection. If the product doesnt have a native adapter you could use a 3rd party ETL tool to move the raw data to a db that is supported. 

I've been given the task of producing a report which will extract data from a web service. The web service provides data from a list of sites including aggregates. Lets say total sales by month. Id like to plot these values on a map and locate them using geometry points. The developer is investigating how he can include the spatial reference in the xml reply. But before i get him change the Web service is there any limitation or restriction in ssrs that would stop this working? If so, what are my alternatives? If i can retrieve the easting and northing in the xml can i convert to a spatial datatype with an expression on the fly? 

(source) Another piece of info I've discovered is that both BlitzIndex and this script are worthless after a few events happening: 

Does this sound right to you guys? I expected it to drop most of my indexes but then to create a ton of new indexes. Also, if it takes 4 hours to analyze 9k queries, is it even feasible for me to get this to consider a normal day's worth of usage? Compared to most large databases, ours is fairly light on consumption (~50 users total). I think I'm either misunderstanding something or am simply doing something wrong. 

We have this complicated query that I'm trying to make "better" until we move it to pull from a data warehouse. I need a solution that's "good enough" for now and I think I'm about 2-3 indexes away from making that happen. I'm stuck on this part, however. I'm specifically targeting this part of my Execution Plan: 

My background: I'm a dev/architect, not a DBA. Sorry! So we have a ~400 table 75GB database. I ran Profiler for ~24 hours ("Tuning" template minus ) and have ~7GB of usage recorded in a trace file. I then ran the Database Engine Tuning Advisor on this trace file (no partitioning, keep clustered indexes, PDS Recommend: Indexes) against our production database (after hours). I gave it ~4 hours to analyze. And here are the summary results I got: 

What do you mean by archive? Do you want to delete the data? Moving it to another db or server will still take up the same space. It may improve performance but you're just moving the problem around. Do you need the data in its original form? Can you aggregate the data to reduce the number of rows? If you do move the data to another table you may find a clustered columnstore may give better compression and save disk space. Personally i would be using ssis and not a sp using linked servers. As you have a primary key id be using that as a way of identifying what has been inserted and then can be deleted from the original table. An index on the column you are using for date range would be extremely beneficial. Id also consider using smaller batches such as one days worth at a time. 

I've recently installed SSRS PowerBI on premise CTP update v14.0.1.353. Since then we've noticed a few issues and decided to uninstall or rollback to the previous version. As we noticed SSRS was still installed we decided to try and revert to the old version, and learned some painful lessons in the process. With the benefit of hindsight i should have asked: Can I have both SSRS and PowerBI on-premise installed together, and how should i configure them? 

It only tuned ~9k out of ~530k queries It recommended I drop a ton of indexes (in fact, most of them) It recommended I create 0 indexes 

However, even after adding this index, it seems the query is still doing the Key Lookup. Am I missing something obvious here or is this the right idea and I just have a problem elsewhere? Note that all statistics and indexes have been refreshed and this isn't THAT highly dynamic of a table but it is approaching ~1M records. A simplified version of this query, focusing on this table of interest, is as follows. Nothing I removed references the PrimaryTableOfInterest. 

We're working on migrating our database (~400 tables) from SQL 2008 R2 to SQL Azure. We're currently in the proof-of-concept stage, figuring out the process we'll be following. We have a process that we think is solid but I'd like to perform some validation diffs on both schema and data to confirm that we have a successful migration. I've never done this before with SQL Azure and am looking for a good way to do this. How can I perform this verification effort on both the schema and data? Ultimately, this is a one-time migration (we'll do it a few times but the real migration will only be done once). 

You can use the latest version of SSMS with SQL 2008 R2. Im not aware if any backward compatibility problems. It will also work with SQL 2017 and Azure. SSDT is a tool used to develop SSAS, SSIS and SSRS Reports. It wont cause any problems installing alongside SSMS. SSIS changes a far bit in 2012 namely in the use of SSIS catalog. IMO its a great improvement. Depending upon your layout it may be feasible to upgrade SSIS first or independantly. Note that once you open an old solution in a new version of SSDT or visual studio it will want to upgrade the solution. Once that happens you wont be able to open it again with an old version. Ive been through a similar scenario as you describe. In the end i found it was just easier to take the plunge and upgrade to the current client tools. 

No, if you are talking about partitioned tables, sql uses the where clause to filter rows. if your query doesn't have the where clause it will return all rows. if you can't test your changes you probably shouldn't be making them. 

For simplicity if this was for a limited time frame, i would go with backup and restores. Unless you are required to respond within minutes you could probably get by with daily backups. You could even set up a job to take a backup and FTP it on demand, after they have reported a defect. I wouldn't even bother restoring daily unless the issue was specific to newly created data. In my experience UAT is more about functionality. So you test with existing data and create new data. Having a db a few days behind theirs means you should be able repeat the test and reproduce the defect. This approach would get slower, more difficult and less feasible the bigger the db is. If you will be providing ongoing support, the db is huge, and you have SLA saying you have to respond within minutes, replication/synchronization of some sort might be worth the effort to set up. The best solution for you may be dictated by client security or technical policies. Work with them to ensure what you are proposing is acceptable. If not keep in mind there are lots of variations on this theme. But it all boils down to: 1. you need something to take a back up, and 2. you need something to FTP the files either on a schedule or on demand. If I was doing this personally I would begin with Ola Hallengren's backup scripts. After installing that I'd create a SQL Agent job which invokes the backup. I would then edit the agent job to add a step to FTP the files. I just had a quick search and found this example of how to use SSIS to FTP a file and I found this example of an FTP script for SQL Server. My recommendation is that you set this job up to run on a schedule, and only run it on demand if truly required. If you don't have remote access and permission to execute tasks, you can request someone on the client site execute the task for you.