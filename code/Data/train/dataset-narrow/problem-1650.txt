Creating another account with uid 0 can lead to confusing, possibly broken behavior. I would be reluctant to run any account that can't be run via or . There are any number of ways use when a system connects to an account such as: 

You should have a routing to the end point over the interface device. This will take preference to a wider routing to the servers behind the tunnel. Lower metrics are higher priority. Metric 2 won't be used if metric 1 or 0 is available. Failover routing needs to be configure with some sort of monitoring software which will change routing. Mixing routings to the same IP address is difficult to get right. 

A quick review of the documentation leads me to believe that squid will accept and cache compressed data from the servers. Compressing images will likely be counter productive. Most image formats are well compressed already and attempts to compress them usually increase the image size by the overhead of the compression algorithm. You could use apache as proxy. You should provide lots of disk space for squid to cache data. Review the caching options carefully and watch your cache statistics. I have found certain sites don't cache very well at all. 

LAN connections should be wired with T568A or T568B. The only significant difference is the green and orange the pairs are switched between the two standards. T568B is a revision of T568A, and either should work. Both ends of each cable must use the same standard. However, you can mix and match cables using both standards. USOD appears to be be a multi-line telephone cabling standard for 3 or 4 lines. It is not suitable for data use. The T568 standards can be used for two lines on the center four pins without splitting pairs. However lines 3 and 4 would be split across pairs which may cause problems. 

You can set up an outgoing server. MX is for incoming only, and does not require a PTR record. However, you may want to consider setting up as a satellite and sending all mail through the central server. This can be done with SMTP auth over TLS if you need to traverse the Enternet. If you do decide to send email directly. These are the basic things you need to do for an outgoing server (example.com). 

You will need to publish the public key in all the domains you are signing for, or the recipients won't be able to verify the signature. I've written an article on Implementing DKIM with Exim. 

If you have high numbers from a few IP addresses it will be easier to limit the connections. You can then add deny rules or rate-limit rules to to limit access from these addresses. If you are under attack you may want to get your ISP involved as they can limit connections before they reach you. 

Study the RFCs and other relevant documentation. I have more information on my blog starting with a rant on Running and Email Server. I have several other EMail articles some of which are specific. 

There are a number of tools available that are designed for scanning logs. is one of the these. You would need to setup an expression to match and configure the appropriate action. This could include temporarily blacklisting the user on the firewall. You may catch poorly configured servers that have initial retry times configured in seconds rather than minute or hours. Spambots are likely to change their sending addresses frequently so you may miss them. I've seen a number of bulk mailers retry at a fast rate using a different IP on each request. The corresponding domains, tend to be consistent on the first two or three levels. I would fix duplicates being sent by fixing the mailing list data. You will have duplicates from temporary rejects that should be retried by your mail server. Use a reasonable initial retry like 1 hour and monitor your queue for entries that have been in the queue for a while. The domain part of an email address is always case insensitive, and the left site is almost always case insensitive. Most mail servers will eliminate duplicate addresses on the same message. However, this doesn't help if messages are individualized. My server will defer delivery for over an hour for every RFC violation I detect. This includes rDNS, ELHO name matching DNS, SPF and others. There are other reasons that a message will be delayed. The reason delivery acceptance was deferred should be logged. 

Yes. As long as the router allows the Ping request to pass outbound and properly tracks the request so that it can return. Most routers I have dealt with handle this correctly. Pings in though a Firewall (NAT or not) are often blocked. There are tools which manipulate TCP packets to generate ICMP failures to provide the equivalent functionality through a Firewall. 

You will also need a corresponding (password database) specification. Using as the driver will use NSS lookups which should include NID data. As you have configured PAM you may want to try the driver. 

Also check the log messages generates when you restart bind. They should tell you what is and isn't being loaded. On Debian/Ubuntu these will be logged to . You should be able to use reload rather than restart to load your changes. Besides you can use the command to resolve names. 

To resolve the attempted deliveries to route your can use a transport or an alias. I alias root to my email address. Ubuntu uses this router to prevent deliveries running as root. 

Look at NUT. It handles this well. Define number of power supplies from each UPS and number of required power supplies. Shutdown will not be triggered as long as there are sufficient power supplies not on UPS. 

Any number of headers can disclose origin information. These include buy by no means are limited to: 

If webprod runs the tar command, the files will be owned by webprod. The files in the tar file may need to be readable by webprod. In most cases this webprod should not be the user id the web server runs as. This will limit the ability to modify your content by compromising the web server. 

Its not likely to find a 10 year old server in the racks unless it has been re-purposed, or isn't replaceable. If nothing else it becomes difficult to find replacement parts. For production use I usually schedule replacement every five years or so. If the budget doesn't allow for purchase of development hardware, they may end up being recycled as development machines. Server grade hardware is likely to run more or less forever except for moving parts. Support, power requirements, and physical size to performance become the significant factors. I believe our tax rules allow for a full write-off over three years (actually four reporting years). From a technical standpoint anything you can buy is likely already obsolete. From an accounting standpoint, if it still does the job it isn't obsolete. The performance/cost ratio is continuing to fall and may make earlier replacement a good option. 

This is the way I use these words. Others may have additional or different usages. Depending on the job at hand, I will use the terms differently. Development teams and operations teams have different needs an usage. Monitoring is monitoring. Usually it is ongoing, and preferably automated. Open source tools like , , and fall into this category. There a lot of commercial tools as well. I would also include run continuously in this category, but its results are not normally monitored. Monitoring tools can be used to trigger alerts when a monitored resource falls above or below a trigger level. Many monitoring tools work well in heterogeneous environments. Profiling is usually done on a particular program to see which code is using the most resources. Often this is CPU time, but can also include memory, I/O, and execution (wall) time. It is usually used to identify candidate code for optimization. Profiling tools tend to be language and/or platform dependent. A different kind of profiling is done using logs and/or monitoring data. This is usage profiling and can be done for a variety of reasons. I haven't found many tools to do this. I use tracing in a couple of different ways. Most frequently, I trace network routes. Depending on network and firewall settings a variety of tools may be used with more or less success. Most of these have traceroute in their name or description. Program tracing is tracing the execution of a program. This is generally done in a test situation. This can be done in a number of ways (in my order of usage and experience): 

You should be able to find the source by checking the MAC addresses embeded in the data "44 a8 12 41 1d 2b 13 8b 9c ab 34 89 10 00". 44:a8:12:41:1d:2b should be your interface's MAC address. 13:8b:9c:ab:34:89 should be the remote device's MAC address. Try checking your cache to see if you have any other addresses for those MAC addresses. should be the command to use. To check your devices MAC addresses use or . 

You may need to create the . Although it is commonly supported, it is rarely used. The above rules will work if your senders are all in the local domain(s) and should be rewritten for all local domains. If you need to rewrite depended on the domain as well as the local_part, you need to configure for virtual domains. If you have a defined a domain list the following rewrite rule should rewrite based on email-alias format files in with names like . 

Starting with should log the connections. There are additional debug bits which will provide additional information. However it may be simpler to use and filter out the connections to the LDAP (389) or LDAPS (636) with grep. This should indicate where the connections are originating. If you have root access on the originating system, you can use to determine the program originating the requests. 

I would only do this on a development server in a secured environment. Many PHP applications will generate the file to the screen so that it can be copied safely to the configuration directory. The quick (and insecure) way to do this is to execute from the directory where the file should go. Before doing this run to get the permissions you will be setting them back to. In some cases the required directory will not exist, so you will need to create it first. Immediately after the configuration file has been written, reset the directory to its original permissions. The correct command is likely or run from the directory. Verify with the ls command. Change the permissions on the configuration file so that Apache can no longer write to it (). Applications often come with example configuration files. Placing one of these in the configuration directory and editing may be a better approach. This requires that you learn and understand the configuration options. You may be able to use the online configuration script to assist your edits.