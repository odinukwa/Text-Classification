You should use function if you need to get the average Using function will return the minute part from the date. i.e. 10:05:03 and 11:05:33 will give the same result. (It is fine if the interval is less than 60 minutes, however, it is safer to use the following query:) 

Since there is a limit on the length of the index, if you want to guarantee the uniqueness of the whole field's value, you may want to add a hash field of that varchar field. Steps: 

So, here is the conclusion out of the comments: Running this: showed that the index is not being used, while in the original query the index was used. The reason is that it is enough to count the not null values of created field. It more probable that the majority of the rows do satisfy the condition on date, so the optimizer decides that it is not worth it to use the index. Instead, run a full scan. For the index to be used, smaller date range would help. i.e. . Use even smaller ranges if the index was not used. 

If the table is huge, you may want to create it without indexes and after populating the data, you create the indexes: 

In general, this design will do the job without problems, specially that you only handle 400 rows per month, which is relatively small number. However, I have few recommendations to improve the design of the table: 

To answer your first question: In the first query, some records are filtered out AFTER getting the result of the left join. These rows are the ones that meet this condition: On the other hand, in the second query, ALL result rows with "card.package_id =14" are returned regardless of the value of Example: 

Which is translated to: "The employee with id (E10) updated the order of ID (50) on 2015-05-05 at 2015-05-05 from the head office" What is good about this is that you can later add an extra field that describes the change, so you will have a tracking system :) Note please the following points: 

You may use function with to choose what to take into consideration for each row. Here is my example: 

2- Create a new table, with the same structure, and preferably add the unique key in order not to fall into the same problem in the future: 

The usage of "##" is to avoid confusion is cases where (articleId, Version) = ("32111", "2"), or ("3211", "12") 

0- means to 'replicate' the changes that happens on the 'master' table to the 'details' tables. For simplicity, assume the relation (country, city). Each city[detail] belongs to a country[master], or each city is a child of a country. With , if you delete a country, all cities belong to that country will be deleted. In your case, is the master table (call it the parent). When you delete an order, the referenced rows from and will be deleted. 1- No, not all records will be deleted. Only the referenced one will be. 2- Nothing will happen to table. (If you delete a city, the country will still be there) Regarding the design: It depends on your model, but I feel that the design is not right. i.e. I think each order should belong to a 'person', and this person's type is either natural or legal. So, I would have: 

The first query will filter out the the rows with user_id=6, although the value of package_id is 14, (and will filter out all reos with package ID <> 14), so the returned values will be: 

Make a full backup using mysqldump, or other "logical" backup tools (as opposed to physical copy of the files) restore this backup on your destination server 

You can build this query in your code, and change the limits based on your needs. Note however that when the numbers in limit get big, there might be a performance hit there. 

Your indexes involve all of 4 fields in the table. i.e. PK implicitly exists in all other secondary indexes. After each insert/update/delete, the indexes will be re-organizes, and from time to time will be optimized, which lead to slow queries some times. One way to improve that is to optimize the indexes, and that is highly dependent on the queries you run. For example, for your query, you need an index on only. Optimizing indexes helps in doing less memory paging, and higher hit ration for cached data. 

The clause after is the condition which is if met, the row will be returned with the result set. In your example, the condition is always met, so the result is the Cartesian product of the joined tables. This is similar to saying , which is always true. You will get the same result if you don't add the condition with the inner join. 

So, 21G without adding the overhead! Therefore, your first problem is the space. To answer your questions: 

This statement will add 1M entries to the binlog when it is row based, while it adds only one entry to the binlog when it is statement based. Suggested solution: Enable the general query log around the time when this this issue usually happens, and analyze all the statements to see if there are many that does update operation on huge number of rows. 

However, this might be logically wrong, as you cannot guarantee what info from other fields/tables would show from the fields that are not in the part of the query. 

The caveat here is that you have no control on which row will be returned from each set of rows that have the same values of "C" 

If the ID of this table is an auto increment one, and you don't care about its value, you will have the chance to reset it to start from 1, so you don't run out of IDs in the near future (This is totally dependent on other factors) HTH 

So, your approach is good in my opinion, with little tweaks: First, when you create the temp table, or when you are generating the hash value, use function (You mentioned is but not sure you're using it in the query): , so you don't have to deal with 'fuzziness' [Assuming that you want the preciseness to be at day level] To get the records that exist in one table, and not the other: 

If we're talking performance, first approach is better. Here is why: Whenever new data is inserted into a table, the indexes will also be updated, and physically reordered. In the case on InnoDB, not only are the index files updated, but the data file itself could be reordered, it is clustered based on the primary key. So, re-ordering/updating is an expensive operation, and you want to minimize it. When you do bulk insert, the indexes are updated at when the statement is finished. i.e. after inserting all rows. On the other hand, when you do individual inserting, physical files are updated after each insert. The overhead of the last statement in the first approach is negligible. 

Note: Increasing 's value may be helpful in your case. Edit To generate all the queries with the help of , run the following queries: 

Copying data directory would work if you are using the same version of MySQL. As per you question, you are copying from 5.6 to 5.7, so this way wouldn't work. Now: you mentioned you are "migrating", and the steps you are following are for "creating new slave"! If you want to "migrate", a better way is the following: 

In general, no. Inner and Outer joins are not affected by the order tables and columns in the query. This can be confirmed by the result of as @ypercube mentioned. However, if you join using "STRAIGHT_JOIN", then the order of the tables (not the columns) does matter. Example: 

ix_ID_SCHEME_LAST_FIRST_DOB is included in ix_ID_SCHEME_LAST_FIRST_DOB_GENDER, so it is redundant ix_ID_SCHEME_LAST_FIRST_DOB_GENDER is not useful if you provide the queries you need to run on this table, I can tell you the best indexes to have (That will also depends on data distribution, but will be better than this design :) 

I think this is a many to many relation ship, and the suggested way for that is to have a table for the relation. i.e.: 

So, you SHOULD remove this clause if it is empty. On another note, I think you should not force an index, with the exception of few rare cases. If the you notice that a query is using an index which is not optimal, you'd better analyze the situation, and know why is this happening, and fix it. (Some times, would do the job) 

From your result, it looks like you don't have the index on (name, type), so for each row in , it will do a full scan to BB. This is one enough reason to make the query slow. 

As a direct answer, what your query needs is a composite index on (shift, timestamp): However, you have a lot of indexes. and some are redundant. 

Try to load file before. If this is slow, then the data in this file has higher cardinality of key fields. Try to disable the indexes (as this is MyIsam), and load the files in whichever order, then re-enable the keys. If this is faster, then the slowness is related to the updating time of index files after certain growth: 

Storing the data in an ordered way maybe useful in some rare cases, but it doesn't guarantee that the selected rows will be ordered. You will have to use to guarantee the order of the returned rows. Is it general practice? I don't think so. Should I avoid this? Yes, at least for this specific case Alternative solution: To let this query run faster, and reduce the sorting process, add a composite index on profileid ASC and on impressions DESC: 

The master writes to its "binary log" files The slave reads its master's binary files and write them to its "relay log" files The slave executes the statements in its "relay log" files. 

[In this answer, I assume using MD5 as hashing function] The answer is YES. Adding a "hash" field and querying it would run faster. Details: When indexing a field, although the average length is 20 char, each entry in the index will be saved in its full length, i.e. 255 char. Add to this that if you are using , the entry length would be 255*3 bytes (plus the PK length). When adding a hash field, make sure it has a fixed length (32 in case of MD5), and that the CHARSET is latin, i.e. 1 byte per char. In this case, the entry in the index will be 32 bytes (plus the PK length) If you want to guarantee the uniqueness of field, it is recommended to add a unique index on the hash field (as opposed to a regular index) 

Note that you have to run query1 then query2 from the result. DO NOT run all query1 then all query2! 

One possible way of doing this is to add a text field for non-shared properties, and them is a serialized array format, or a JSON string [Possibly in a separate table]. This comes with a problem though that you cannot index the "keys", so the search will be slow if the table grows big. For example: 

If the query was , there is no worry about data inconsistency. You can run to skip the error on the slave, and re-run the optimization on each server by itself if needed. Use so the statement is not replicated: 

Stop the traffic (At least to that table if possible) Create a new table with the same fields, but no indexes: Insert into this new table all the rows that you want to keep: Create indexes on this new table. Switch the tables by renaming them: Drop the old table Restart the traffic 

This could be due to the collation that is used by the table or the field. If it is case sensitive, then "L" would not match "l". In order to know the collation, run this please: 

You cannot use clause in an statement. However, you may use if in your targer table you define nUserId or vEmail as unique keys. More info about INSERT syntax: $URL$ 

The composite index on (state, city) will not be used if you use the function in your query. You may want to update both field in both tables first: 

Why the query is slow: 's output shows the following: The query will scan ALL rows from first table, 78536406, and for each of these rows, it is will scan an average of 15 rows from the second table. In total, (78536406*15) rows will be read. This is by itself is an indicator of slowness. Taking into consideration the size of the tables, there might be some memory swap, which cause slowness as well (Reading from disk to RAM, repeatedly.) How to improve this query: One possible way would be to create a table out of the sub-query, and use INNER JOIN between the first table, and the table from the previous step.