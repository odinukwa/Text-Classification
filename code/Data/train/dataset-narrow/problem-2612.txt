What does this line mean: wiDi.(x,y) is that reffering to w * dot product of(d , (x,y)) sigma returns the sum of what exactly? 

This approach is similar to how the Jacobi iterative algorithm works with linear simultaneous systems of equations. And it's not guaranteed to converge, but in my simulator it does the job quite smoothly.. in 3D (yes, an extra dimension adds twice the difficulty!). Caveat: correct positions and velocities only after your collision detection/handling phase is over! That way you simultaneously update your colliding actors. Also, the restitution forces must be taken into account next time when you integrate for positions and velocities. EDIT: Well, I guess you're using the already abused Verlet integration method (this one's become a household name within gamedev enthusiasts). In this specter of collision handling and integration, you might want to take a look here. UPDATE: Some of the information on how to approach collision (and self collision for that matter of fact) can be found in these papers: 

If your object has a forward direction fwd=(x,y,z) and an up direction in its rest position (they all have in 3D, unless they're points or lines), then your quat orientation gives you the current fwd and up vectors. Perform collision resolution and obtain fwd' and up'. These two vectors then give you an orientation (a frame) that you can convert to a matrix and get the quat out of it describing your object's new direction of travel and up direction. 

Instead of commenting, I decided to post this as an answer. Of course, it is a subjective view, but for students trying to understand the language of game physics and maths, it's a gem. You can view the explanation here: $URL$ 

I imagine the big blue sphere to contain all links, the green half of them, the reds a quarter and so on (the picture is not accurate, but it's there to help understand the question). What I do not understand is: How can such a hierarchy speed up computations between segments collision pairs if one has to update it for a deformable linear object such as a chain/wire/etc. each frame? More clearly, what is the actual principle of collision detection broad phases in this particular case/ how can it work when the actual computation of bounding spheres is in itself a time consuming task and has to be done (since the geometry changes) in each frame update? I think I am missing a key point - if we look at the picture where the chain is in a spiral pose, we see that most spheres are already contained within half of others or do intersect them.. it's odd if this is the way it should work. 

This strategy is simple and intuitive, but the details are math/physics heavy. Perhaps some simple introduction can be found here: $URL$ UPDATE: Richard Tonge of Nvidia PhysX has explained their solver implementation for collision handling (in parallel!) for multiple objects. It does answer and explains a lot of issues: $URL$ 

In the frames of a number crunching compatible programming language (say.. C++), what would be an elegant solution for adding self collision, external collision and integration step (Euler, etc.) updates to an object? (say the already abused mass spring system). Say the object class is this one: 

A potential solution is to use a visualization technique involving the rendering of pathlines, streamlines and streaklines. You're interested in rendering pathlines. Unfortunately, I can't show you a picture of how to achieve this, but I'll explain it shortly: 

There are quite some tutorials, I'm sure, on the issue of normal mapping, but since you're here, I assume you're not satisfied with what you found. In your case, the terrain is ussualy a Monge Patch, which is not a complicated 2D manifold to perform geometric measurments using normal differential geometry. First, why a normal map technique? Because the normal computed from the geometry, by averaging the face normals at a common vertex, like you described, and then computing another interpolated, per pixel normal won't allow for micro-relief lighting. The normal computed this way is "accurate" only if there's no "skin" on your geometric surface. There are some algorithms that compute the normal map texture (the mostly blue/fuchsia RGB texture you know by now). They could rely on image processing (for example, a convolution kernel to compute the image gradients in the x and y direction and then take their cross product to come up with the 3d vector for a normal - which is quite natural for these Monge patches). You are not interested in those, since you already come with a texture. Then how to map the normal texture correctly? Its (uv) coordinates should coincide with your diffuse/ambient texture that you want to enhance with lighting computations. Like you said, the u could be in the x direction, the v in the y direction of a 2D rectangle that is viewed from above. 

Big game studios don't hire pure mathematicians. Considering the question, people with very advanced math skills are a must for developing new technologies. The very best are actually skilled academic researchers, capable to code and deliver at least decent demo applications of their algorithm. I can only pledge for one thing a game developing company (a big one!) is looking for: a talented and research oriented person that aims to improve and devise new algorithms in these fields: plausible/realistic physics at interactive frame-rates, graphics tricks that introduce a plethora of eye candy effects while making use of less memory and time, excellent knowledge about how things should be communicated on certain network topologies and so on.. Even a state of the art method/algorithm, before it makes it into the gaming industry, it is chopped down a lot, modified for speed and when you look at the actual implementation of that method and compare it with the technical documents/papers/articles it was published in, you will be astonished by how many details are not provided or by what other very complicated tricks are also used. This usually happens a lot. If you have an unique talent for APPLIED mathematics, you can study a field (Game Physics, Rendering) and select a few topics that appeal to you, then go through the state of the art reviews and see how much you understand from those. Knowing what has been done, try to understand the drawbacks and think of, perhaps, ground-breaking, never seen ideas that solve the problem at least 2x better. Then your way is paved, they'll just have to hire a guy to code the algorithms for you. Maybe that's what Havok did (I doubt it, the Havok guys started off as Trinity College Dublin PhD students and had very solid advanced Math skills). 

When you know the tangent field of moving particle, mathematically you are assured you can recover the trajectory (see the fundamental curve theorem). you do know this field since the assumed parameterization is the above one. Just substitute and for your constants and for an angular offset (what were the two airplane angles when you started your AI trajectory estimator). You ended up with a parameterization of the velocity vector in terms of time . You need to integrate this velocity field from to and get the exact location of your airplane, provided the speed is constant. (You, of course, need to multiply the velocity by that speed scalar to account for higher or lower speeds - this is unit velocity as given above!). Integrating can be done component wise and for that, if you're lazy like me or forgot much of the calculus tricks, you can use $URL$ Once you get the primitive expressions, use those to find the position (via the fundamental thm of calculus ...) 

An equivalent in 3D is quite simple, without using quaternions or anything of the sort (since it's really not the case - you need to specify a convention, not to perform computations). Hence, spherical coordinates: 

UPDATE: As pointed out in one of the other answers, you could use Euler angles. Actually, when analyzing the solution given in the first web application, we can see that this is how they do it. They're computing latitude and longitude (spherical coordinates) and from then on, they move by rotating a gimbal rig. You can picture this imagining that the z axis of a frame is pointing towards the ceiling and NEVER moves (can only rotate in place, spinning). Now, you weld an y axis to this z one and let that axis support a ring. Z and Y must be perpendicular. Denote a point X on the cross product end of YxZ. That's where your camera can point. Now try to derive the mathematical equations that allow you to cover the whole sphere by first rotating against the Z, then against the Y axis. REMEMBER: the Y axis has rotated as well, so it's not correct to name it (0,1,0) or whatever convention you have for a world axis. It's actually something in the likes of (cos(t), sin(t), 0), where t is the longitude coordinate of a spherical coordinate system. On the other hand, Z does remain fixed and you can call it (0,0,1) (or, again, whatever convention you choose). Mathematically, to rotate the camera you can first apply a rotation R_z(deltaLongitude) followed by an R_Y(currentLongitude) (endLongitude - currentLongitude). longitude is an angle in radians, in the [0, pi) interval, whereas latitude is in the (0, pi) interval. The only advantage of this rotation system is that it's easier for the human brain to accommodate to: it's pretty much how our head pan/tilts. Our entire body, perpendicular on the ground is the Z axis I was talking about, while the Y axis is free to rotate perpendicular to this axis (picture it as the your shoulder line when it is in its rest position :D). Quats are very similar to an angle-axis representation, nothing more. So if you apply a quat, you imply that the result is the input rotated against the quat's axis by the specified angle. What does that tell us? It tells me this: that the quat's axis must be perpendicular to the camera's lookat axis in order to not rotate the camera against this look at ray. If the axes are not perpendicular, there will be a roll component taking its toll, and hence the OPs problem. If required, I could provide further details (a 3D rep maybe?). This link does explain a trackball rotation concept and how to implement it.. 

where rFar is the maximum range (the red circle's radius), rNear is the minimum range (the green circle's radius). This function just maps one interval to the other, in a non-linear fashion. I'm using now the (x^2) function for below unit positive values. This function should give the user that plausible natural impression we're looking for in such games.. without a lot of mathematics/physics to support it. In the figure above, the red vector is the one that gives you the shooting direction: just normalize it (divide it through its magnitude) and negate it (add the minus sign to it). Then multiply it with the result from the rangeFunction(length) where the length is the initial length (before normalization) of this vector. Now find the angle between the blue and red vector using a 2D cartesian frame and the atan2 function. Let this angle be u. The point where the projectile will land should be (r*cos(u - pi), r*sin(u - pi)), where r = rangeFunction(length). That should be it, more or less. Check if this point is "inside a cow's aura" and then kill it, like cowboys are supposed to, right? 

So you could start with m_0 as Nathan Reed suggested and apply how many iterations you need. More details on quaternion calculus can be found on Dave Eberly's site. Special care must be given when juggling hypercomplex logarithms and exponentials as they're not entirely similar to their lower dimensional homologues. (I think the log, for example, isn't injective). More details on what applies and what doesn't are given here. (most nice properties are preserved for unit quats). 

Update I wrote an academic paper a while ago on how to benchmark and ultimately select the "best" integrator for your needs ($URL$ If you want to understand what are the differences between symplectic/energy conserving integrators, explicit and implicit ones, you need to look no further than to how the phase diagrams look like: $URL$ . For simple oscillators (e.g. the pendulum), these are 2D plots of the angular value against the angular velocity. 

Observation: the q*(0,v)*conj(q) has to be another quat of the form (0,v'). I won't go through all that apparently complicated explanation of why this happens, but if you rotate a pure imaginary quaternion (or a vector in our case!) through this method,you must get a similar kind of object: pure imaginary quat.. and you take its imaginary part as your result. There you have it, the wonderful world of rotations with quaternions in a nut(ty)shell. NOTE: to whomever jumps in with that overused phrase: quats are good because they avoid 'em gimbal lock.. should unlock their imagination first!! Quats are a mere "elegant" mathematical apparatus and can be avoided altogether by using other approaches, the one I find completely geometrically equivalent being the axis angle approach. CODE: the C++ library I fancy is rather simplistic, but has all the matrix, vector and quat operations a 3D graphics experimentalist should need without having to waste more than 15 minutes to learn it.. You can test the things I wrote here using that in 15 minutes if you're not a C++ novice. Good luck! 

One way is with overlays. It should be fast/cheap enough. UPDATE: As Lars Viklund's comment explanations go, these overlays do not refer to the older concept that predates the programmable pipeline. The older way to add overlays (or underlays for that matter of fact) had to do with creating special render context planes like described here. 

There are tons of source snippets for a method that performs a test for "point inside polygon". The principle comes from Jordan's curve theorem for polygons ($URL$ The naive way would be: having that method, call it : 

The articles in the GPU Gems are more about techniques and not really about the technical abilities of the GPU offered through a revision of one graphics library or another. Some of those gems are really gems: explaining a concept that is still of interest, both in the means of approaching the problem and in implementation details. Before this gems series, I think there was another one focused on assembly language shader code. Much of those techniques were converted to a more readable higher shader language form and are still popular if not central today. Technically, the GPU gems offer ideas for developers using HLSL up to 3.0 (DirectX 9.c) (the code is actually in Cg, a good dialect of HLSL). Equivalently, in OpenGL 3+, most core techniques are programmed the same way, using the same ideas and concepts. So, if you're not into getting the latest techie implementations running and want to get a good solid knowledge base, they're still a nice reference. UPDATE If not restricted to OpenGL, a newer series is the follow-up of ShaderX: the GPU Pro series (if you tolerate the DirectX approach and can translate the techniques to OpenGL). 

Since Render Monkey has been discontinued (perhaps due to the complexity of today's shading languages), there are few successors that can match its functionality. Is there any useful tool for material editing aimed at developers and artists alike, but concentrated (or with substantial support for) on embedded GLSL shaders? Render Monkey itself wasn't that flexible (I'm not aware if it allowed the usage of several textures, each with its own set of texture coordinates - plus, it didn't seem to be that intuitive either). Apart from complete engines, is there such a stand-alone tool that can be used together with a stand-alone engine (able to interface with a custom application)?