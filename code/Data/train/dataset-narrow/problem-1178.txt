Modern languages came from a need to keep lazy programmers happy. There is nothing worse than having to repeat the same code over and over. For you typing it, it means you are more likely to make a mistake, it means it takes longer to get a result, and it means (mostly but not always) your code will take longer to run. The worst bit about repeating code is when you have to change it. You have the variables and and for each test you do the same thing , repeating the same code. Do it once event better do the width test only once Rewrite of your code. 

Generally polling for a state change is not considered best practice so there is no idiomatic approch. The code 

JQuery Do you really need jQuery? I ask this question alot and the correct answer is not a flat "yes". You must justify the cost of all that extra code and why alternative solutions are not better. Rewrite I had to rewrite. The following does the same, in a fraction of the time and a fraction of the memory. It is a saving of cost in all areas. 

Cyclic reference danger Recursive property searches are dangerous because many objects used in javascript at some point have a reference to themselves. Eg the global object You need to check or be sure that there are no cyclic references. A common method is to use the function that will throw if an object is cyclic. This is a very inelegant method and will prevent you from finding the information you are after as you will not search if this fails. The best way (unfortunately) is to store each object you iterate and then check every other object against that array of objects, ignoring all object you have already iterated. More than one result The second problem you have is that the search will find only one path, while there could be many paths that satisfy the requirement, the first result may not be the one you are after. If you use a callback to determine the matching result you can be a lot more adaptable. The callback can accept not only the key name, but the current path and the current object the key belongs to. The function should return all paths that satisfy the condition set by the callback function. Modernise ES6 has been available in the majority of browsers for some time. It is best to use ES6 as it is a major improvement on ES5, not using it is a major No No if you plan on an IT career. If you are concerned about legacy browsers (especially that annoying thorn called IE11) you should use a transpiler (for example Babel) to ensure backward compatibility. A rewrite I present a modification that is safer and more versity and in two forms. One as a standalone function and the other as a prototype of Object. 

Which does what you intended without the risk of getting unknown properties. Dont burn cycles Good code is efficient code, with every instruction the computer executes in your code, you burn power, sucked from batteries, the grid, You chew the clients time and you extract a little more from the environment we share as a comunity. Yes your code is trivial in the scheme of things, immeasurable, but combined millions of programmes, or with fortune on your side an app you write may end up on billions of devices, and particular parts of your code may execute trillions of times a minute. Then it matters. Always (I digress, no rant... :P) code for efficiency, even the little bits combined matter. Your code could have exited early as the problem states that there is only one set of odd numbers in the array. So as soon as you find the odd set, the job is over and you should exit. On average you will do a quarter as much processing overall. On average you could do up to a quarter less processing overall. Use appropriate Objects. You are using as a map, for each unique key (number) you create an entry and use the associated value to count the number of keys. Though technically not wrong javascript provides a object that is better suited (see rewrite) The rewrite If the array of numbers was known to be sorted or numbers were grouped then a different solution could be used that is more efficient, But as there is no mention of the array being sorted or numbers grouped apart from the sample data the best is to play safe and assume unsorted data. Some may opt to sort the array to take advantage of the better solution, but the sort is expensive and would offset any benefits the more efficient method would give. UPDATE I miss read the question and thought that counts of one were to be excluded. I have changed the code via commenting out the incorrect behaviour. 

Why. it has to do with the assignment of the name. A function declaration is available in the entire scope that it is declared in. A function expression is undefined until the expression has been run. 

The degree of granularity is the number of functions to source code. The more granular code is the more functions there are. Low granularity (not many functions) is bad for many reasons (I am assuming that is self evident) so breaking down common tasks to functions is beneficial. But there is a point where increasing code granularity begins to effect source code quality and code execution efficiency. In execution the optimizer spends a lot of time working out what functions are not needed, and in-lines them, this is because function calls require CPU and memory above that of what the function does. Function call have a cost, and you can not rely on the optimizer to inline them for you. When reading code for the first time (or returning to code after some time) functions can disrupt the understanding of code because you must follow the function call to its source location. Lots of functions makes reading code harder (a spaghetti of function calls). Finding the balance comes with experience. One liners used once is bad. 

Axonometric projections With all that said the whole function is a problem as is doing a search when a direct index can be found. * note that the code below is off the top of my head and may contain typos, it is a suggestion and you should do some extra research on axonometric projections to create code more suited to your needs.* All axonometric projections (Isometric is a particular type of axonometric projection) can be defined as 4 2d vectors that define the direction and scale of each of the 3D axis X,Y,Z and the location of the origin. If we ignore the Z as we are after a cell coordinate the problem can be simplified even further. 

Review A little ambiguous, and overly complex. You need to familiarize your self with radians and forget about degrees Style Block-less blocks A block is a section of code delimited by many C like syntax languages allow you to skip the block delimiters for single line blocks after conditional statements. This can be a major source of life long frustration, and there is not a programmer alive that does not know the insidious nature of the bugs that can result when you make changes to the code and forget the 

The most important difference between the two is that function statements are hoisted and available in all part of the scope they are declared in. Function expressions are not, even if you declare them as a var (which is hoisted) it remains undefined until it is assigned a value. Function declared as statements are thus the safest form of function declaration and should be preferred over expression declarations. 

If the callback function fails to return (throws an error) on the function the property is not set and remains the init value of -1. The callback is then call in rapid succession, and if the callback continues the throw it just continues tring to catch up every time the worker sends a message. The is in effect an infinite message incomplete loop Because you call the callback function from the function any error in the callback will stop current execution context. This is not what one would expect from a timer. When you have many timers. If any callback throws the remaining callbacks (higher on the array) will not get called as the does not get to complete the timer iteration loop. 

No it is slow and GC unfriendly (plus the other stuff in the points above.) Testing Testing is how you understand your codes behaviour. You do not need to test the sub components of the logic. If you test some random arrays of integers and compare the results to a known correct result (Array.sort) and pass you also know that each sub component works. You would not expect the sort to work yet any of the sub components to fail would you? Testing is about the most brutal assault on the code you can imagine, What happens when I pass an array of millions zeros, or infinities, or NaN, or strings, 10e512, or random sparse items. It does not matter if the function can cope with the garbage in. What matters is that you know what it does do and can thus document that behaviour for users of the function. The code Radix sort is fast, your performance tests should be getting times that are very close to but you have complicated the sort making it impossible to match the native sort. Buckets You split the array into two positive and negative values (that is a partial sort and not technically part of a radix sort) The radix does not have to be just the positive digits, you can expand on that, and include 20 buckets for digits -9 to -0 and 0 to 9 that way you don't need to split the array. Max digits? You are passing over each value at least once, you can test the digit length as you go rather than as a separate pass. (finding the max value is also a partial sort, not part of a radix sort) Get digit ...is very very slow and OMDG who wrote that! To get a digit from a number positive or negative. Performance If you want speed you need to avoid array iterators like , , , etc as they come with baggage that can be avoided by using standard for, while and do while loops. Don't add default parameters if you never use them as they extra noise in the source code and overhead for the CPU. Don't copy arrays if you don't need to. You have yet you overwrite the values, well almost as you do 

You could get an earlier exit for the offset option by adding below the line but on average this increases the amount of work so best to keep it as simple as possible. 

The rules. Though I am not sure if this was the intention but you are also picking up on inherited properties that may have been protected. You need to check that you can write to the property. 

Canvas && tile maps First drop the jQuery you dont need it and it will just slow everything down. Canvas Displaying the map is much better done via a canvas. Each canvas pixel does not need an interface and can be a million (no exaggeration) times faster than a DOM element and use a tiny fraction of the memory. Though you lose the functionality of a DOM element it is very easy to implement the basic needs using the canvas. The canvas is a very powerful rendering surface. The 2D context is fully hardware accelerated and can do full screen 2D games at 60fps on average machines and on good machines it just awesome. It can also do 3D via webGL or as a hybrid 2D context and webGL working together. Avoiding the DOM by using the canvas makes many apps fly and is the best bet for game / graphic intensive applications. Tile map This type of linear map map is best as an simple array (Tile map). It saves space, access is far quicker (including neighbours), and things like coordinates can be computed from the index. Each element of the tile map holds the index into the terrain type. The world description object map holds an array of terrain type descriptions. To get the details for a tile you get the map item and use that as an index into the terrain description array. eg if the world is 50 by 50 tiles and is the tile map array. To get a terrain description for a tile 

Eloquent Eloquent is simple, efficient in both processing and memory use, easy to understand, and adaptable. Simplicity Don't store unneeded data it adds complexity. The problem is looking for averages grouped by century. That means that there is no need to store any persons age, only the sum of ages for each century and the count of deaths are needed for the solution Efficacy Doing the minimum work possible. To get a result requires, one pass to group, sum, and count the data. Then one pass over the centuries to display the results. With 39 persons and 6 centuries. the number of iterations should only be 45. Your solution passes over the data 118 times over 2 times more than needed. The solution by 200_success improves that a little with 90 iterative steps, twice as many as needed. Understandable. Code should be easy to follow. In the old days of coding there was a style of code commonly referred to as spaghetti code. To understand it you had to follow a complicated path of processing. Each time you jump from one section of source code to the next, it breaks the logic flow stored in your head. Following disjointed source code is hard. Now we have named functions. The idea is that the name of the function gives all the information needed to follow the logic without having to step into the function. This is seldom how it works, and tracing functions because the function name lacks the information to understand the process is a modern form of spaghetti code. Adaptability Time is money, and good code means that it is reusable with the minimum of effort. How long would it take to get only male or female averages. Do you need to think about it, or is the solution already in the code and you know instantly what to add, or change. Can the code handle bad data, can the code handle vary large data sets. What needs to change to make it a distributed solution. An Eloquent solution. Minimal iteration passes, efficient with no irrelevant data stored, and only 45 iterations. Understandable, not a sea of nested function calls, Logic can be followed fairly much from top to bottom without needing to step into disjointed inadequately named function calls. Could be a little simplier but I went for adaptability over simplicity. The allows a variety of groupings. The output function means it is easily be formatted to any type of visualization, storage, or additional processing, Ancestry data can be randomly split and processing distributed with output providing a collation service. No side effects and pure as Snow White. 

Faster via lookup. What I don't like about this function is its complexity grows in relationship to the value n. As the range of valid input values is small this function would best be implemented as a lookup table. For speed I would also drop the type checking. Though you add a slight overhead just after parsing the code all subsequent calls will have the same execution time and be many time faster than calculating the value. 

This is not a cover all solution but just a short cut to the standard DOM query. A rewrite I have rewritten the code using the above as a guide. I have written it to do the same as your code as I am not sure what your aim is with the zoom setting, they seem arbitrary and limited to only a small set of possible text widths. 

Why review My SVG experience is low, and I am not at all sure if this is safe or even practical for the wild. Any comments, suggestions, warnings, or improvements would be appreciated. How it works. It creates XML nodes and returns a proxy of an object holding the node. The proxy and handlers do the hard work of transforming property names and property values between JavaScript friendly and XML formats, and performing the correct action depending on property type.