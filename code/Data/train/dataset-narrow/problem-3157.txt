For the second step let's use more precise tuple-hash. It's not so simple, as previous one, but it's input is much smaller: 

You are right, cosine similarity has a lot of common with dot product of vectors. Indeed, it is a dot product, scaled by magnitude. And because of scaling it is normalized between 0 and 1. CS is preferable because it takes into account variability of data and features' relative frequencies. On the other hand, plain dot product is a little bit "cheaper" (in terms of complexity and implementation). 

Here is a kind of "classical" paper on Twitter bot detection: Who is Tweeting on Twitter: Human, Bot, or Cyborg?. It's already 6 years old, but could be a good starting point. Bots are being detected through their posting activity patterns. Also you can use texts not even trying to "understand" (or parse) them. Just try shingle-based methods, locality-sensitive hashes (LSH). I advise to use Nilsimsa, but only because that is the only LSH, I used myself. And AFAIK, initially it was built exactly for spam detection. But you can try to search for any other. And on the datasets: I don't know publicly available Facebook and Twitter spam datasets, but here is a dataset with YouTube comments, hope it will be usefull: Network Analysis of Recurring YouTube Spam Campaigns 

Want to wish you good luck. Some time ago faced with the same problem, but didn't find any satisfying solution. First of all, there is no way to get list of users, who "liked" a particular page. Even, if you are an administrator of this page (I was). One only can get list of last 3 or 5 hundred users. Friendships data for most of the users is also inaccessible. Looks like gender is the only thing from your list, that you can get. Data about pages, that exact user "likes", should be available (as it's written in docs), but in reality, through API you can collect something only for friends and FoF. Even though this data is available through web interface. So the only way is to try dirty trick with parsing and scraping (but remember, that I didn't advise it ;) ). 

Intuitively, the correlation matrix is symmetric because every variable pair has to have the same relationship (correlation) whether their correlation is in the upper right or lower left triangle. It doesn’t make sense to say the correlation between variables $X_1$ and $X_2$ is $\rho$, but the correlation between $X_2$ and $X_1$ is $\rho’\neq \rho$ if calculating a Pearson correlation (so correlation is symmetric). Mathematically, correlation between two variables, $X$ and $Y$, is commutative: $Corr(X,Y)=Corr(Y,X)$. In OP’s case, the correlation between Q1 and Q2 is the same calculation and therefore the same result as the correlation between Q2 and Q1. Therefore the correlation matrix will be symmetric. There are more mathematical reasons and proofs why a correlation matrix of real valued variables has to be symmetric and positive semi-definite, but I’ve excluded them from this answer. 

Say I have a corpus of text documents on which I have calculated each documents TfIDF vector. With this sparse matrix representation of the corpus, I can calculate similarities between documents by calculating the cosine similarity between the documents' TfIDF vectors. If I now compile a set of bigrams based on frequency, and replace instances of two words in each document with the concatenation of the bigram, what affect will this have on 1) Calculating TfIDF vectors? 2) Calculating document similarity? To be explicit, lets take an example in python: 

I always avoid for loops when operating on pandas rows - it’s slow and inefficient. If possible, try to creat some functions () and then apply these functions to the rows of the dataframe: which will vectorise the operation of applying to each row, making it much faster and more efficient. 

This function would produce a decimal hash, based on letters. This hash could be used for efficient binary tree search (BTS). But the collision is that it doesn't take into account double letters. E.g., both for 'hello' and 'ooohell' output would be the same: 2377728. Anyway, it's fast and robust to filter out most part of your dictionary. There are several options of implementing efficient search, based on this index: 

The problem with Sankey diagram in your case is that you will duplicate all the nodes (on the left, and on the right), that is messy. Networks are better, but nodes are not well structured. I suggest to try chord diagrams for such data. 

'Restoring' or 'predicting' data. You can use a bunch of different technics to complete this task, but my vote is for simplest ones (KISS, yes). E.g., in my case, for age prediction, mean of ego-network users' ages gave satisfactory results (for about 70% of users error was less than +/-3 years, in my case it was enough). It's just an idea, but you can try to use for age prediction weighted average, defining weight as similarity measure between visited sites sets of current user and others. Evaluating prediction quality. Algorithm from task-1 will produce prediction almost in all cases. And second task is to determine, if prediction is reliable. E.g., in case of ego network and age prediction: can we trust in prediction, if a user has only one 'friend' in his ego network? This task is more about machine-learning: it's a binary classification problem. You need to compose features set, form training and test samples from your data with both right and wrong predictions. Creating appropriate classifier will help you to filter out unpredictable users. But you need to determine, what are your features set. I used a number of network metrics, and summary statistics on feature of interest distribution among ego-network. 

Assuming your data is normally distributed, you could fit a Gaussian to your data and calculate the Probability Density Function (PDF). Once you have the PDF, you can set a threshold probability, below which a data point could be classified as an anomaly If you have enough data, use a Variational Autoencoder neural network. Very roughly speaking, you train this on all the data you deem to be “normal” (the neural network learns how to reconstruct the input data in the output), and then when anomalous cases are passed to the network, it can’t reconstruct it. If the network can’t reconstruct it accurately, the data is an anomaly. 

If you have enough data (>10k examples), you could even train a neural network on the data to capture the complex relationships between features which linear regression wouldn’t capture. 

One Hot encode the categorical features Use PCA to reduce the dimensionality of the data Scale the data (subtract the mean, divide by the standard deviation) Train the regression model on the reduced scaled dataset 

If you want to use an unsupervised method i.e if your data is not labelled with classes, then something like k-means clustering may be your best bet to find patterns in the data. Alternatively, if you want to do anomaly detection, 2 possible options are 

As you mentioned, each decision tree is trained on p (sometimes sqrt(p) random features). This ensures that each tree is “grown” (trained) differently so that the model 1. Does not overfit the training data (reduces variance) and 2. Generalizes better to new data (reduces bias). Therefore we don’t weigh the trees differently, as this would be similar to having all tress trained on the same features. You can change the voting threshold however from the standard 50% to anything you want (e.g 40%, 70%, 90%) which will change the precision and recall accuracies of the model. EDIT: changing the voting threshold means changing the number of trees needed to make a classification. For example, in binary classification most standard random forests require 50% or more of the trees to vote for a class for that class to “win” (be predicted to be that class). But if you change this threshold, to say 70%, then 70% or more of the trees need to vote for the same class for that class to win. 

TF-IDF denotes relative importance: importance of a term in a specific text, compared to the whole corpus. So, comparative corpus required to use TF-IDF to estimate word importance in all your 270 documents. You can use one of "common" corpora, listed here, or try to find domain-specific corpus. Then treat your 270 docs as a single text, and calculate TF-IDF scores for words, you are interested in, based on comparative corpus. 

Here for peer users you will have of predefined maximal rate. If "voter" has TR twice more than "author", "author" will recieve of predefined value, and so on. You can tune steepness with additional parameter, or try to find any other mapping transformation function. Anyway, now simply multiply maximal penalty/reward by this coefficient, and you'll get the number of point, you need to deduct or add. The only issue, I see, is a user with zero TR - such user as a voter will "give" nothing, and as an object of voting, will recieve the maximal amount of points regardless voter's rank. To avoid this, you can predefine minimal TR (like ), and don't let user's TR to fall beyond this value. 

One of the greatest examples of chord diagram was build with d3.js, and based on migration flows dataset. Here is an R wrapper for d3.js chords. Or you can try to reproduce this manual. 

First on methods and techniques: there are a lot of language-independent spam detection algorithms. The main task is feature engineering: one need to construct features, that will detect behavioral differences: 

NLTK has an excellent step by step guide on everything you need to convert human language to an SQL query using the nltk package in python. It’s rudimentary, but it answers your question. 

K-means is an iterative algorithm. So what you have in the picture may be one of the early iterations and therefore the sum of squared distances (between points and clusters) has not been minimised yet. In further iterations, the circled point would indeed be assigned to cluster centroid C1. Now, having said that, and this is purely speculation, but if the diagram in your picture is not simply 2 dimensional as we interpret it to be, but is actually a 2-d projection from a higher dimensional feature space, then indeed the circled point may be closer to centroid C2 in that higher dimensional space (although it doesn’t look like it in 2-d). 

What you could do is apply a dimensionality reduction technique such as tSNE to visualise all your features at once in 2 or 3 dimensions. Scikit learn has an excellent implementation of tSNE. However, tSNE is used more to find local structure and clusters in high dimensions, rather than relationships between features. Furthermore, distances between clusters and the reduced X-Y axes mean nothing in tSNE. If you know your data has some relationship between features, it may be best to use seaborns pairplot, as suggested by M Sef, to plot several features at once. 

I have setup a four layer CNN designed to predict two classes. The two classes are more or less in the same ratio. The negative class is 55% of the data and the positive class makes up the remaining. When I train the network on the data and look at the final results, I see two problems: 

I am trying to implement this paper $URL$ on a set of medical images. I am doing it in Keras. The network essentially consists of 4 conv and max-pool layers, followed by a fully connected layer, followed by a soft max classifier. As far as I can see, I have followed the architecture mentioned in the paper. However, the validation loss and accuracy just remains flat throughout. The accuracy seems to be fixed at ~57.5%. Any help on where I could be going wrong would be greatly appreciated. 

If I use y_predicted, I get a perfectly diagonal confusion matrix, when the console output shows an accuracy of 70% which doesn't make any sense at all. What is that I am doing wrong? 

My question is why is the network exhbiting this behaviour. I've enough training data, ~10k images in training, and the classes are more or less balanced (5400 in the negative and 4500 in the positive). The issue raised in point 2 increases with an increase in the number of epochs. I've tried decreasing the learning rate, different optimizers, increased the number of layers, decreased the number of layers, used transfer learning (VGG-16). I've also tried reducing the number of epochs, but this leads to poor classification in the negative class as well. 

PCA is a dimensionality reduction algorithm - it projects your high dimensional data onto a lower dimensional plane. This is useful for either visualisation (if you reduce to 2 or 3 dimensions to plot), or for training machine learning models. You say you want to find patterns in your data - I’m not quite sure what you mean by this. Do you want to visualise your data, train a model on it and make some prediction, or something else? To visualise high dimensional data, you could use either the tSNE (t-stochastic neighbour embedding) algorithm, or PCA. Depending on the type of data you have, you can “find patterns” in different ways. If your data is unlabelled (you don’t know the classes of each sample or there is no dependent variable) you can use unsupervised learning algorithms such as K-means clustering, K nearest neighbours, Gaussian mixture model. If your data has dependent variables, depending on whether your dependent variable is categorical or continuous, you could use classification algorithms for the former and regression algorithms for the latter. Classification algorithms include logistic regression or decision trees. Regression models include linear regression. 

It seems the Adaptive Moment Estimation (Adam) optimizer nearly always works better (faster and more reliably reaching a global minimum) when minimising the cost function in training neural nets. Why not always use Adam? Why even bother using RMSProp or momentum optimizers? 

I have a model pipeline for finding similar text documents given an input query text. The model is very simple; I have a corpus of documents on which I train a TfIDF model. When a query is input, we can infer its TfIDF vector. Finally, we compare the query's TfIDF vector to all the vectors of the documents in the corpus using cosine similarity, which finds us the "most similar" texts. My question is, is there a way to incorporate more micro structure features into the pipeline such that similar document retrieval will perform better, and if so how? By this I mean, can we use Part of Speech tagging, intent recognition or other methods, as extra features in the pipeline. I am looking for more unsupervised methods here as I have a lot of data (1TB) and it is all unstructured and un-tagged / un-labelled, but supervised suggestions will also be welcome. I appreciate that by incorporating new features, we will most likely have to move to a completely different model paradigm as TfIDF and cosine similarities will not work depending on the structure of the new features. NOTE: I have already performed a significant amount of text pre-processing on the corpus e.g. stemming, tokenizing, word replacement, stop word removal etc.