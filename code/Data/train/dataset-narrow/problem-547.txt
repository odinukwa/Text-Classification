I'm sure there's a really obvious answer but: Why is the (byte length) of a string in a field shorter than the actual string? e.g. 

Could you move the admin field to a separate table which just holds the current admin id (or list of), and then you could update that with the trigger. (of course if your admin is always id:1000, then you probably don't need the trigger to start with) You would also need to consider whether to fire this AFTER INSERT and/or UPDATE. An example trigger would be: 

I think I found the problem. Apparently the command ended at the symbol, so when the session expired the second part of the command stopped with the session. To get round this I ran: 

They all use the same table. The thing is, I am pretty certain that the queries weren't running that long. I checked to see what was going on that confirmed the were active for that amount of time. 

I'm currently investigating moving our database to an (vs buying bigger servers (again) and moving the master / slaves to these). So far my testing on a small cluster has been fruitful, and suggests that we would benefit from this (especially in data resiliency / write access). However I have one small niggling concern. We have one large table (apx 30% of the whole database size - or 60GB apx at present) which is comprised entirely of BLOB fields (80+), and a single field at the start to identify the row. Few of these contain very much data (mainly some text/ numbers/ dates etc), many are actually empty (NULL). None of these are indexed. We add new rows constantly, and read the old rows frequently too. Updates are few and far between. Is this likely to become an issue when running this in a cluster? From my reading I understand that BLOB fields are held in separate tables/pages in the background, so need to be read separately to the main data. But can't see if this is like INNODB, where the first x data is stored in the main table, the rest held separately, or if the whole thing is separate. I ran some simple tests, inserting records with similar data, (both disk based and normal), with various joins etc. which showed no differences. But I am concerned that my small test system simply isn't stressing it enough, and don't have the resources (or finances) to run a full size test yet. Can anyone with some experience of this tell me if I am going the right way with this, or am I just walking into a nightmare? 

If your shards are backed by replica sets you can make the balancer more aggressive by disabling , which reduces the write concern used for documents migrating between shards. By default, is which is equivalent to a write concern: each document move during chunk migration propagates to at least one secondary before the balancer proceeds with the next document. In MongoDB 3.0+, there is also an option to configure an explicit for the operation. For example, to disable from a shell: 

Typically you would want to install all of these in your development environment, but for production you may only want to have a subset (i.e. perhaps you don't want the tools or shell installed on every server). To list the contents of an installed package, use (you can also use as an alias for ): 

If your client applications are connecting directly to MongoDB, they will typically use a connection string which varies depending on the type of deployment: 

GridFS is for storing large binary data chunked into smaller documents (by default, 255KB each). The access pattern for GridFS is different from a sharded collection where random document distribution might be more desirable (for example, with a hashed shard key). With GridFS the documents relating to a single file are normally read sequentially: identified by unique and ordered by chunk number . The supported shard keys for GridFS enable range queries based on the order that drivers will reconstruct a GridFS byte stream. A hashed shard key does not support range queries so would be extremely unhelpful for read performance ( chunk lookups would be required and data would be randomly distributed). 

Once the shard has a primary, normal operation can resume. You should also remember to re-enable the balancer if you disabled it during your outage. 

Each shard contains a subset of data for the sharded cluster. If the replica set backing a shard becomes read-only you will get exceptions trying to write new data to that shard. The mechanics of failover and high availability for an individual shard are governed by your Replica Set configuration and deployment. 

If your database is too large for the limitations of MMAP on a 32-bit O/S, you will have to either do a partial restore or upgrade to a 64-bit environment. Note: 32-bit server builds were officially deprecated as of MongoDB 3.2 and are no longer available for MongoDB 3.4+. 

I have the below Data / Query. Is this really the most efficient way of doing this? As it seems like a lot of work to get the result. I've tried reducing it / combining the parts but this is the only way I can get it to work. (n.b. The actual table has a lot more entries over multiple days - hence the WHERE filters at the end) 

Or would an index on only suffice? It's quite a large table so I don't want to spend time adding the index if it is unlikely to help. 

Am I missing something here. I'm looking at using to allow two applications to co-ordinate which data to wok on. From reading the manual it suggests that I need to apply a timeout to the command. e.g. 

I'm tryng to set up a new Slave DB by copying another Slave DB. Normally I would simply use to dump the Slave and create a file, then import that into the new database. However, as our new server is a Linux (Debian 7) server (the slave I'm copying is on a Windows server), I figured I would try to pipe the dump straight into the new database. This seemed to go well, until my putty session expired and it all stopped. I tried it again, this time using . But still the dump stopped as soon as my session expired. Is there a way to make this work? My full dump command is: 

Do you use statement based or row based logging? If it is statement based then you most likely have set in your replication filters e.g. so that only queries where the database is are written to your binary log. e.g. 

I have also tried stopping at (the equivalent of) pos 104627, 104628, 104629 (e.g around the previous ). But each time, replication continues until the start of the next relay_log, when it stops. Am I missing something? 

1) Any , , etc type Query executed on the Master will be written to the Binary log, causing the to increase Furthermore, if the Master is receiving updates from another Master (in a Master to Master setup), AND if you have enabled these will be written to the Binary Log as well. Each time you restart the service, the Binary Log will be flushed, incrementing it by 1. 2) On the Slave, Run , and make a note of the . Now check, does this file exist on the Master still? If it doesn't then you have a problem, and will need to rebuild the slave from a copy of the master (e.g. MYSQLDUMP) Assuming it does exist you should be able to just issue a command, and it will start catching up (hopefully) Next run again and check the and . Are they both then all is good. Look for the to give you an idea of how far behind it is. If either of them is look at the Error ( or ) to get an idea of what the problem is. 

This is where the approach heads into "not advisible" territory. In theory you can get a workable configuration with a core of stable voting members in your replica set configuration plus your laptop configured as a hidden, non-voting secondary. However, there are a number of caveats to consider: 

No. A chunk in MongoDB is a logical contiguous range of one or more shard key values. A unique shard key value cannot exist in more than a single chunk range, but many documents may have the same shard key. The cardinality for your chosen shard key determines the granularity of documents that can be represented in a chunk range (with the lowest granularity being a chunk that represents a single shard key value). The relationship between shard key and chunks is illustrated in the following diagram from the MongoDB documentation explaining Choosing a Shard Key: 

The specific recommendation against ext4 is based on observed stalls during WiredTiger checkpoints as reported by a number of production users. Per SERVER-18314 (May 2015), there is a likelihood of stalls when ext4 journaling coincides with WiredTiger checkpoints. This may not be an issue for all workloads, but is a significant enough caveat that a startup warning was added in MongoDB 3.4 (released Nov 2016) for deployments using ext4 with WiredTiger. There have been no similar reports with XFS, and it has been observed to generally perform better with MongoDB. There are other available filesystems on Linux (ZFS, btrfs, ...) but these are currently not widely used in production (as compared to ext4 and XFS) so XFS is the recommended filesystem based on testing and experience. 

These storage metrics are informational counters and expected behaviour (i.e. unrelated to data integrity). There isn't any obvious actionable outcome based on these metrics, but if a collection wasn't getting much overall benefit from compression the counters might provide insight into how often pages aren't compressed (as compared to compressible pages that don't have a significant level of compression). 

Recommended approaches 1) Setup a hidden secondary in your hosted environment, instead. 2) If you really want a locally replicated copy and your replica set environment is anything other than development, I would suggest an alternative approach that sidesteps some of the potential issues with networking and visibility: you could consider using Mongo Connector. This is not an officially supported tool, but what it provides is replication-like behaviour to one or more target systems. It also doesn't remove caveats about possibly adding additional load or requiring a resync. For more background, see: Mongo Connector - Usage with MongoDB. 

Is there some trick to getting the Visual Explain feature to work in MySQL Workbench? I am running version 6.3.6 on Windows 7. I've tried simple queries with just one join. I've tried it with complex queries with 12 Joins. I've tried it with MySQL 5.5 and 5.7. But every time I just get I've looked on the Bugs for MySQL Workbench > Visual Explain, but there is no recent bugs, making me think it is something I am doing. But I can't see what. Does anyone have some suggestions? 

I have recently set up some new MySQL Databases on a remote site. They are all 5.5.47, running on Debian Servers. They are set up as one Master with multiple slaves. The Master is itself a slave from our main site. But periodically replication on one of them (never the same one, and only one at a time) gets stuck. The servers at my main site which have been running for years, are fine. The config of the servers at the new site is identical to the main site. The servers (VM's) are the same as the main site. I can see the relay logs continuing to increase in size, but the Exec_Master_Log_Pos value just sits there. The error logs are of no use as I have due to our website generating hundreds of every second. Everything else continues to work fine, until I execute a command which just hangs. After that all other related queries (, etc. also just hang.) I've tried killing the connections / queries, but it makes no difference with each of them sitting there saying 'killed', but not completing. I've checked disk space, and that is always fine. I can log in to MySQL via the terminal and run basic commands (until I run ). I have compared everything I can think of to the other servers, but there is never any difference. If I try and shutdown the database (), it hangs at Finally I resort to killing the mysqld_safe process (), which forces it to restart, and goes into crash receovery. After that it seems to be fine. Is there anything else I can check to try and find out what is happening. 

I've tried every possible combination of I can think of, but it still doesn't work. What have I missed please? 

The auditing feature (which is part of MongoDB Enterprise) is only intended to record configured operations of interest. If you want to do special processing based on queries you will have to implement this in your own application code. 

As at MongoDB 3.4 neither of these aspects have built-in server features, however both are possible to implement in application logic. The most suitable approach will depend on your requirements. Depending on the programming language you are using and the desired approach, there may even be helper libraries or implementations available. For document versioning there is a great blog series on the Ask Asya blog which includes code examples and caveats: 

The parameter controls the base port that a or server listens to for TCP connections. The HTTP interface is accessible at a port number that is 1000 greater than the base port, so given a of 28017 you can connect via port 29017 if you have enabled the HTTP interface. You can see both ports listed in the server log on startup: 

No, MongoDB does not create backups by default. Recommended backup methods are described in the MongoDB manual. I would consider the MongoDB manual the definitive reference source as it is kept current with product changes. Blog articles may be outdated or contain incomplete/incorrect advice. 

The impact of time adjustments depends on how significant the clock drift is between members of your replica set. Generally the most impactful outcome will be some confusing diagnostic information: operations during the affected period may appear to have improbably large (or small) durations. If you are running any multiple server deployment you should always use a clock sync service (eg. NTP) to avoid potential clock drift. Generally services that make small, incremental adjustments to time (eg. ) are preferable to utilities that make large adjustments (eg. ). For more information on the potential outcomes of clock skew with MongoDB, see: MongoDB and Leap Seconds. 

Low cardinality shard keys will definitely cause issues with data distribution. The most granular chunk range possible will represent a single shard key value. If a large percentage of your documents share the same shard key values, those will eventually lead to indivisible which will be ignored by the balancer. 

MongoDB Ops Manager 3.4.0 (released November, 2016) includes monitoring for hardware metrics (cpu, disk usage, and I/O utilization) via the Automation agent. For more information, see: System and Disk Alerts. Prior versions of Ops Manager did not include hardware monitoring and required the use of an external tool () for CPU & disk metrics. If you are still using an older version of Ops Manager and have installed Munin-node for monitoring CPU & I/O stats you could look into using the Munin plugin to track free disk space. Munin supports sending alerts via email as well as through Nagios and external scripts.