Here are the what your picture says and is . Now there is no specific reason I can think of why you would wanna do a shift of but the viewport allows us to draw to a specific portion of the screen. For example, in video games you would want to draw the minimap on the lower left corner of the screen, hence you could provide the values This would render a 200x200 minimap in the lower left corner. Your second picture does this but instead it chooses a value of 

So suppose I have my initial frame like in my picture (original post) leftmost. If I use with then my axis becomes and becomes . However if I now rotate with the matrix will still be rotating around the previous/original fixed axis. Now that's clear take this plane as an example. We want to rotate in the order X-Y-Z. 

This works but up-to a certain extent. Using this approach we would have 4x4 matrices and 3 dimensional row/column vectors. But now when doing multiplication we will have to make a check whether it's point or vector and based on that we multiply the elements of the 4th column of the transformation matrix by or . This alone starts creating mess, check every time whether it's a point or a vector so why not just use the space for a single float more and get rid of this stupid check. That aside most modern CPUs have SIMD registers 128 bits wide perfect to fit 4D vectors. If you are doing your calculations on the GPU, all the more reason to store 4D vectors as branching instructions are much more costly on the GPU. If you start feeling the need to use 4D, great. If not then we still have the problem of composing matrix transformations. Instead of multiplying individual transformations with our vector we would like to pre-multiply all of the transformations then do a single vector-matrix multiplication. This can't work with projective transformations where we need to divide by "something" Let's come to reason number 3. 

If you substitute it in the above matrix you'll get $ \frac{nx}{2} * x_{ndc} - \frac{1}{2} + \frac{nx}{2}$ for the $X$ value. This equals $ \frac{nx}{2} * x_{ndc} + \frac{nx - 1}{2}$ Which is the same as what is done in the picture you posted. Hope it helped somewhat. EDIT:- offset is commonly used to get to the pixel center. See this image as an example. Here the pixel has the center . This is the 0.5 offset you are seeing. I was confused at first when you were talking about an offset of negative 0.5. 

As we can easily see, by similar triangles $\triangle ABC$ and $\triangle AEF$ we have $Y_p / Y = D/-Z$ where $Y_p$ is the projected $Y$ coordinate on to the image plane. $AB = D$ is the total distance from eye to the image plane and is usually set to 1. Hence we have, $Y_p = (Y*D)/-Z$ Most APIs also set that the camera is facing towards the negative Z axis following the convention for right handed coordinate system, hence the negative sign. The projection plane is also assumed to be a rectangle instead of a square. It's 2 units long in the $Y$ direction and centered at the origin i.e in the range [-1,1] so it's 1 unit long in the positive Y axis and 1 unit long in the negative. However horizontally, its in the range $[-A, A]$ so when calculating $X_p$ we have to correct for this factor. $A = Width\ of\ rectangle/Height\ of\ rectangle$ Similarly for $X_p$ and $Z_p$ we have, $A*X_p = (X*D)/(-Z)$ $X_p = (X*D)/(-Z*A)$ $Z_p = D$ We can easily see see that $tan(Y_{fov}/2) = BC/D = 1/D$ $D = 1/tan(Y_{fov}/2)$ Here, $Y_{fov}$ is the angle of field of view in the Y axis. Since we are only talking about the upper half its $Y_{fov}/2$. Hence one way to write $X_p$ and $Y_p$ is $X_p = \frac{X}{-Z} * \frac{1}{A*tan(Y_{fov}/2)}$ $Y_p = \frac{Y}{-Z} * \frac{1}{tan(Y_{fov}/2)}$ This is what you are seeing in the matrix. The $X$ and $Y$ come from the vector at which the transformation matrix is applied and the $-Z$ term is accounted for by the $-1$ in the 4th row. This $-Z$ is stored as the $w$ coordinate which is then used in perspective division in which we divide by $w$. For more information you can check this site here Note:- Haven't discussed properly about NDC space, perspective divide and about near and far planes in $Z_p$ as that would take too much time. Just wanted to show you that it can be derived. 

First of all instead of taking you can use Gram-Schmidt orthogonalization to produce an orthogonal vector that's close to the WorldUp vector. Then you take the cross of this orthogonal vector and the look at to get the side vector. However as you mentioned this'd fail if the vectors are parallel. Hence the usual solution is to pick another vector that's not parallel to the look at one and again use Gram-Schmidt orthogonalization to produce the orthogonal vector ( camera up) and then take the cross. The maths is like this. 

so I am in the process of making my own GPU path tracer based on OpenCL as part of my bachelor thesis. I have already made a CPU and a GPU ray tracer based on OpenGL compute shaders but all I have ray-traced are simple spheres and planes. 1) Onto the main question, what do I need to path trace actual models like the teapot/dragon etc. Are there any standard, well known libraries for this purpose? Or do I have to write my own model loader? Writing my own loader seems all nice but I won't just have to read the data but maintain a proper data structure for all the triangles (K-d tree or octree perhaps) as well and then pass that structured data onto the GPU. If there is a library that can give me the data structure it'd be nice as I'll have to write the code for traversing it on the GPU only as I have already written one to traverse octrees for my GPU raytracer. 

If I understand correctly, you want to draw 2 objects mirrored with respect to an arbitrary line. Then when you move 1 object you want the other (reflection) to move with respect to that reflected line. A more sophisticated way of doing this would be using transformation matrices. That way you wouldn't have to worry about moving the mirrored triangle. You will move the original triangle, apply a transformation matrix to find the position of the mirrored triangle. Suppose we have an arbitrary line $ax+by+c = 0$ making an arbitrary angle (measured counter clockwise) with the +X axis. We can easily find slope in the form of $ y = mx + c$ and the angle $\theta = tan^-1(m)$. The mirror transformation with respect to this line can be broken in 3 affine transformation. 1) Rotate the line by $-\theta$ to coincide it with the X-axis. $\begin{bmatrix} cos(-theta) & -sin(-theta) & 0\\ sin(-theta) & cos(-theta) & 0\\ 0 & 0 & 1 \end{bmatrix}$ 2) Reflect about the X-axis. $\begin{bmatrix} 1 & 0 & 0\\ 0 & -1 & 0\\ 0 & 0 & 1 \end{bmatrix}$ 3) Rotate again by $\theta$. $\begin{bmatrix} cos(theta) & -sin(theta) & 0\\ sin(theta) & cos(theta) & 0\\ 0 & 0 & 1 \end{bmatrix}$ Now you concatenate all 3 and you get a matrix that reflects about an arbitrary line. Now we only move 1 triangle and find out the other by applying the matrix. EDIT:- As pointed out by Anthony in the comments this is only for lines passing through the origin. For arbitrary lines first translate them to the origin, then perform all these steps, then translate it back. In order to do this you can find out the x and y intercepts by putting and respectively then translate the respective coordinate by that amount. 

What it does is this. We know that world up isn't pure orthogonal to the look at vector. So we take the projection of world up onto the direction of the look at vector. Subtract the parallel part of this projection from the world up vector to obtain the remaining i.e. the orthogonal part. This vector is closer to the world up but is purely orthogonal to the look at vector. 

People always forget that there is no "camera" in OpenGL. In order to simulate a camera you have to move the whole world inversely. So if you want ur camera looking 30 degrees downward, you move the whole world 30 degrees upwards. If you want your camera moved to the left, you move the whole world right. That's why you will notice the sign in the translation vector where he used the variable . Hence the reason there is no separate "view" matrix in openGL, it's modelview matrix combined. That's the reason for taking the inverse. Since a view matrix represent's the camera's forward, up and side, you have to take the inverse since the orientation is defined for the world not the camera. 

Already asked this question on mathexchange but didn't got any promising answers so posting it here Since I'm from a CG background. I really can't understand how and why gimbal lock occurs using euler angles. First of all let me clear this "I know that gimbal lock occurs when 2 gimbals/axes coincide thus losing 1 degree of freedom" However what I'm interested in why the axes are coinciding in the first place when they are supposed to remain perpendicular when we move anyone. According to wikipedia here : 

So I want to implement a path tracer and I am confused between GPGPU computing or using OpenGL's compute shaders. I've already implemented a raytracer using GL's compute shaders. What are the main differences between CUDA/OpenCL and CS? And is it possible to see a significant speed up if I prefer one over the other? Some main differences I already know are that using CS will make it vendor independent so I could run it on PCs having Nvidia/Amd GPUs alike. CUDA supports recursion while openCL/opengl CS don't. What are other significant functionalities that would make a person prefer opencl/cuda over CS? 

There is the whole derivation of it but I'll be discussing a brief overview. This is for the perspective projection where the line joining the eye and the center of the projection/image plane is perpendicular to it. Like here 

But more commonly and what makes more sense to me, and to the answer of your question, think of irradiance as the integration of radiances over a set of directions. So we can say $E = \displaystyle\int_{\Omega} L(\omega)\; \omega.n \;d\omega\\\omega \in \Omega$ So if we integrate the radiances from every direction that leads us to the original definition of irradiance where direction isn't of concern. However usually we are concerned with only a subset of all the directions such as the Upper hemisphere or the lower hemisphere. This means for example, $\Omega = \{ \omega : \omega.n \geq 0 \}$ As we can see here, we have limited the irradiance to a set of directions, the upper hemisphere. This doesn't necessarily change it into radiance which is associated by direction. Instead what this means is, when calculating irradiance we are concerned with the light coming only from these directions, although we haven't incorporated the directional quantity into the formula like with radiance. This is the difference between irradiance from a certain direction and radiance. Think of it like this. You are holding a paper and there are 2 light bulbs in front of you. You want to measure the irradiance. Normally it would just be the radiant flux received by both bulbs per unit area. But now let's say I limit the direction so I am only concerned with the first bulb. Note that I am still calculating the "irradiance". If I move farther away the flux will decrease thus the irradiance even tho I am concerned with a specific direction. However this isn't the case with radiance where moving farther away won't change it since we divide by the solid angle too balancing the change. The last quantity is differential irradiance. I thought of it as a tiny amount of irradiance from a specific direction. (Again direction gets involved) If you think of irradiance as not assosciated with direction at all, even then when you try to think of differential irradiance you are gonna say it's a tiny amount of irradiance from a small range of direction or maybe a specific direction. That's the reason why it's small. But if you think of irradiance as the sum of all the radiances over a specific set of directions. You'll see that it makes things clearer and you'll naturally arrive at the conclusion that differential irradiance will then refer to an irradiance from a specific direction. So coming back to your question at last, I hope you might have gotten some intuition as to what "irradiance in a direction means". Mathematically proving it is no hard feat tho. The answer here explains it quite well. I'm just gonna give a brief explanation. We know the rendering equation is given as $L_{outgoing} = L_{emission} + \displaystyle\int_{\Omega} L_{incoming} \;f_{BRDF}(\omega_i, \omega_o)\; \omega_i.n \; d\omega_i$ Assuming for the moment that emission part is zero. we end up with, $L_{outgoing} = \displaystyle\int_{\Omega} L_{incoming} \;f_{BRDF}(\omega_i, \omega_o)\; \omega_i.n \; d\omega_i$ Now as I wrote before, if you forget the BRDF for the time being, we are just integrating the radiances over a given set of direction which is the same as irradiance. If we look at one instance of this summation/integration, it's gonna be $dL_{outgoing} = L_{incoming} \; \omega_i.n \; d\omega_i \; f_{BRDF}(\omega_i, \omega_o)$ $dL_{outgoing} = dE \; f_{BRDF}(\omega_i, \omega_o)$ We put $d$ with the outgoind radiance and irradiance because it's a very small part (we are looking at just one instance of that summation/integration) $f_{BRDF}(\omega_i, \omega_o) = \displaystyle\frac{dL_{outgoing}}{dE} $ Which is the ratio of the outgoing radiance to the incoming irradiance. Again this was just the way I convinced myself and might have some mistakes. Though this is the best I came up with.