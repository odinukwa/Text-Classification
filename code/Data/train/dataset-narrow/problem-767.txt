In most RDBMSs, double-quotes are what are used to specify an exact spelling of something.. (single quotes being string delimiters). 

would return one row (where type is EXAMPLE) and the count of how many rows contained that type. The filtering here takes place AS THE DB COLLECTS THE ROWS up front .. Any row that doesn't have type 'EXAMPLE' is thrown out. Only one COUNT() is calculated in this instance. One more complex example to show that the two are evaluated separately and can be combined in any query. 

I do not believe it is possible to use the wildcard in an statement. To achieve what you want to do, you need to take out all your wildcards and put them in a separate clause. 

With regard to your questions, please read the documentation completely, as it addresses all of your concerns. In terms of looking for when it doesn't exist .. the recovery process just parses all WAL files until it encounters "EOF" .. which usually happens when it tries to read from the next WAL file and it doesn't exist.. So it uses that case as a termination point for the recovery process.. The important bit here is that it reached a checkpoint in the recovery process as stated with . This means that your recovery is reliable. Continuous Archiving Documentation Section 24.3.5. - Timelines Timelines are a way for Postgres to handle "alternate universes" so to speak... For example, if you have a situation where your replication gets messed up in some way and suddenly you have slave promoted into master status (while your master never went down) ... some machines are writing to the master still while some are writing to the slave.. But it's ok, because the writes to the current master will all happen on timeline 1 and the writes to the slave will all happen on timeline 2 .. so you can, theoretically, still merge all the changes together (although I'm sure it wouldn't be easy).. I imagine it's similar to a branch in a version control system. Except there's no real "trunk". For the postgres gurus out there - if I have anything incorrect, please correct me. :) 

I would like to point out that the result of my SQL above returns 5050 because the "n+1" is never actually in the result set... (You'd need to compare for n-1 in the result set if you wanted to ignore even numbers..) 

EDIT oh - you added "list" requirement ... see @ypercube's answer, then. EDIT2 If the place_ids exist in a separate table, and they only DON'T EXIST in the table you're querying to get scores (which, imo, would be considered a 'good design decision'), then you should include that table in your query .. COALESCE() as used in @ypercube's answer should work fine in that case. 

This usually makes the process much faster, and also splits it up into two logical steps. Accept the uploaded file -> Process the uploaded file Rick James points out that a more detailed description of this process (along with ways to handle various situations) can be found here 

It's been a while since I've done anything like this, but last I recall - If you have ALL of the corresponding files, you should be able to copy them and start up the server .. it will go into recovery mode to validate that it's got everything that it needs. Of course, if it finds that it's missing something, it will likely either not start or complain in the log files that it's missing something (depending on the severity of what it's missing). The most likely files that you MAY NOT HAVE would be the WAL files (by default stored in the pg_xlog directory .. but could be different if it was configured that way). If the configuration was different, you will need to make sure that the new environment is suited to match the configuration that postgres is expecting (for example - if the WAL files were located in a different place than default). The postgres configuration files may also not be where you would expect them ... they COULD be in the base directory of the database, or they could be in the postgres config directory (which I do not recall the exact location of it - you can look in the /etc/init.d/postgres-9.3.sh file, iirc) 

The simple solution is to basically NEVER use "*" ... always SPECIFY which columns you want .. and then you can also ALIAS those columns.. 

But this seems like a REALLY bad idea ... I think you should update your post to answer Martin's question (in the comment) to get a better solution for what you want to do. (or ask a different question) 

Your query to figure out all diseases patients with heart disease have is simple at this point. Let's assume heart disease is disease_id = 1 

The difference between and is that the first gives you unique rows across all selected columns .. the second gives you one unique row per column set defined within the parenthesis, but allows for additional columns to be returned. To specify WHICH additional data is returned, you need to .. will then return the first row for each DISTINCT ON set of columns. 

Patient table - info with regard to patient Disease table - into with regard to disease Patient Disease table (join between the above two) - probably only two cols, patient id and disease id 

When you start the restored cluster, it will notice that a recovery.conf file exists and ATTEMPT to do a restore, reading the WAL archive files and applying changes. At some point, it will reach a CONSISTENT state, which is what is most important in the restore process. Based on what you have, it looks like you are not actually doing a restore to any other system, but simply starting the cluster that had been stopped. 

I don't see where you are referencing the content of your CSV anywhere in what you posted.. (at least before any future edits). You should be using the COPY command. $URL$ EDIT Since Topic and Format tables appear to be lookup tables for referential integrity, the correct course of action would be to insert the values into those tables FIRST .. and then insert into the other table using a SELECT.. Answer below: 

Edit - The below is assuming that the number of rows that would be returned by the query (if the limit wasn't present) exceeds the limit. (as in ... it regularly would return 5000 rows, but the limit forces it to return 1000) Any time you have a limit on the number of rows returned by a query, you should not expect the TIMING on that query to have any sort of relevance to performance. For example, if you take a simple query as such: 

COALESCE takes the first not-null argument to use, so if bv.affidavit happens to be NULL it will use '' instead.. I shouldn't need to test for it being NOT NULL in the follow up.. 

Based off this, I can see that the query looks normal except for the horrendous JOIN on a LIKE. To me, it looks like the query planner is either getting confused on the number of matches it EXPECTS to get from that JOIN (and thus plans a sequential scan), or reporting accurately and your query is potentially doing some sort of cartesian product. I'm curious to see what it reports with this query (I do not know if results will be the same as original query): (it just separates the LIKE join from all the others) 

There's not much information you provide, so this is only a guess. Your WHERE clause may be backward, with regard to the (+) ... I would recommend switching it to a LEFT JOIN regardless, just to make it more readable.. 

But I can't speak to how efficient it will be.... -- Additional This may work, as well. But again, efficiency will likely be poor (since it will get all results, regardless if you use it or not) 

COALESCE will let you get away with a lot of interesting things when you're dealing with the situation of "if it's null do one thing, otherwise do something else".. 

Once you have your series, you can join to it and use COALESCE to fill in 0's for where there is no data.. 

Foreign Keys in general (not just composite) MUST point to a UNIQUE KEY of some sort in another table. If they did not, there would be no relational data integrity. This is complaining because, while you have a unique key on (id) .. you do NOT have a unique key on (id, num).. Thus, as far as the DB is concerned, the pair (id, num) is not GUARANTEED to be unique. Us, as humans, can figure out it will be unique, but I'm sure there would be a lot of additional code they would have to add to make Postgres smart enough to see that "oh hey .. id is supposed to be unique, so id,num should also be unique" .. I would be highly surprised if they added that code when all you have to do is create another unique index on the two columns to fix the problem. Just to be clear, the code they would have to add wouldn't be just this simple case... it would have to handle all cases, even ones where the foreign key is on 4+ columns, etc.. I'm sure the logic would be quite complex. 

You are not referencing table b anywhere in your sql, though - please explain why you need to join to it if you're not going to be using its data? 

, as John pointed out in a comment, is for filtering on aggregates after they have been calculated, while is for filtering the rows that are gathered at the start. For example: 

Seeing as how this question is in regard to design, the answers are likely to be fairly opinionated... Here is my opinion on how it should be designed. ;) Table to store units of measure 

You may want to make sure you create this by using an appropriate user (such as the user your application uses or some dba account). Be aware that whichever user creates the extension will also own it. This will require a server restart for it to be usable (because you have to change the config in postgresql.conf a tiny bit that affects memory consumption), but it will give you statistics on all queries. Please see the documentation page for more information about what pg_stat_statements provides. See here for a quick reference on configuration of the postgresql.conf config file for pg_stat_statements. Once you have it installed and running, you can view statistics by querying the pg_stat_statements view.. 

If a unit of measure doesn't have an entry in the measure conversion table for a specific other type of unit, then there would be no direct conversion available. (For example, cm -> inches would work, but cm -> hours wouldn't..) Displaying your measurements in different units should be easy. Your measure unit can be anything you want (distance/time/etc) ... but you may want to add a measurement type .. not sure if I'd worry about that - depends on use case. 

Your query uses which will select every column from the tables (in the order that they exist within the tables). So if you have: 

Your issue is that you have defined a recursive function ... the function is calling itself. The logic that exists in it currently would result in a stack dump with OUT OF MEMORY error (or something along those lines, at least) if it didn't give you the error of "table already exists" to begin with.. 

Not entirely ... pg_dump creates a file that contains the information of your whole database that can be given to pg_restore... For PITR, you actually need to create a file-level "backup" by copying your database files and WAL files (after issuing a pg_start_backup) 

tl;dr - Currently you have a notes table, a user table, and an "allowed to see" table ... which is fine. Consider a "public flag" and adding a "not allowed to see" table. Your approach looks fine to me. Alternately, you could have an additional column on your NOTE table called "is_public" ... if that is set, then the note is a public note and available for everyone to see (thus avoiding your 1000*1000 scenario) ... also, you could have one final "exclusion" table so that a person could make a note public but then pick specific people who should NOT be able to see it. I'm not advocating one way or the other, since I think both are perfectly fine to use. You do need to make sure your join tables are properly indexed, though, otherwise queries will start to get slow. Frisbee has one important bit in his answer - your UserNotes table does not need an ID of it's own.. Join tables usually just have the PK as the composite set of IDs.. (so PK would be UserID/NoteID)