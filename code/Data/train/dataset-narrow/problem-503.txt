It's impossible to evaluate the options without knowing what the actual problem is. Your first port of call should be to identify the reason the servers run slowly - it may be memory or CPU, rather than disk throughput. Perfmon will be able to tell you this; you can also run Microsoft's PAL tool to get a better insight. If it's not disk throughput that's limiting you, option 2 and 3 are pointless. Secondly, it may well be possible to optimize the queries - this may be cheaper, quicker and less risky than changing the infrastructure. Find the query that's running slowly, work out what's constraining it, and see if you can optimize it. If you do find out that the queries are optimally tuned, and that it is disk throughput that's the bottleneck, I'd put each of the two back-end DBs on separate drives - it doesn't sound like they are competing against the front DBs (as they run at different times), but they might be competing against each other if both DBs are used in overnight batch jobs. 

I agree with TomTom on the merits of the Datamodel resource book - it will give you more insight than StackOverflow, and if memory serves, there's a fairly detailed discussion of exactly this scenario. The design you've come up with is similar to what I've seen for other systems - what you call ProductVariation is often called "variant". I'm a little confused about the role of SKU in your design - the acronym stands for "Stock Keeping Unit". As it's supposed to be unique and never change, I'd make that the primary key of your ProductVariation table, rather than ID. If you're only ever going to sell your current product range, or are happy to refactor in future, you can leave it as is; alternatively, you could consider a design with "table per subclass". This would give you a schema along the lines of: 

I'm seeing this kind of report in many JIRA bugs and I want to generate it for my mongo instance. What software was used for generating it? 

When running a journal disabled cluster, we observed frequent disk checkouts - once about every 3 seconds. This was confusing, as the docs specifically mention that checkpoints are created every 60 seconds. After digging around, we found the following comment on mongo's Jira: 

I'm running MongoDB 3.4.7 cluster (primary + 2 secondaries). Primary's cache usage is 95%, and I try to figure out what the cache contains. I ran collStats for all collections (±18,000) and summarized the "bytes currently in the cache" metric. The total is ±16GB, which is only about half of the cache size (which is 30GB). When running db.serverStatus(), there seems to be two relevant metrics: 

The first one might explain the 16GB summary of the collection cache, however I wonder what the second metric means - what else is MongoDB / WiredTiger store in the cache, and how can I break it down? Any ideas? 

I'm running mongo 3.4.7 (WiredTiger) replica set with two secondaries. According to the docs, when journaling is enabled and write concern is , the ack is sent after the data is written to "In memory". However the docs don't specify what happens when journaling is off on the server (). What happens in that case? What is the write concern waiting for? 

Whats happening is, the primary backup job is running fine ( there is a lot more history, im just showing the last 2 ). 

As Seen Here ( Microsoft's website ), this field is a field. Ok. Converting bytes to KB I have: 8,309,834 Kilobytes. but the database size inside the folder, has: 

When you drop a table, it will free the space in the database, but not back to the O.S. because your database has this space allocated. You need to shrink your database. but it's not a good Idea, because it's good to have free space in the database , so it can grow without the need to ask the o.s. to grow your file and then use it. this can cause fragmentation. 

EDIT 2: With some tests, I see that these queries shows me last backup date, even if I only have log backups. Is there a way to list "last full backup" date?? 

Just a Little help ( can't post this as a comment ) continuing @sp_BlitzErik's answer, I got some queries with Pinal and Max Vernon ( can't rememebr where ) that say how much MAXDOP you should use: 

Long story short, if you notice, there are some equal "faturamento". What I need is a of: 1) If is the same, we need the biggest . Ok with this, I can order by 3. 2) If and is the same, I need the biggest . Ok, again, I can do it with Order by 3,4. 3) If everything is the same, but is the same, I need to get the biggest . but if is different ( and faturamento, sequencia, cdcodigobarras is the same ), I need the biggest . Is this something that devs should do in the source code? I really can't think how I can do it with a Query. I've spend some time tryint to do something with count, but even thinking I'm in the right way, I can past this . 

It's really worth thinking about the types of query you are going to want to run over your data, and the structure of the rest of your database - that will almost certainly help you think through the model. For instance, you're likely to want to link your seats to tickets when you sell a seat for a performance. That suggests you have a table called "performance", and another table called "ticket". Consider the structure of "ticket" - you want to link it to the seat. In your current design, the foreign key from "ticket" to "seat" is ugly - you'd need four columns to store this, and you can't guarantee that the ticket wouldn't have links to two seats if there's a bug somewhere. That suggests that the proposals from devdigital and christiandev are probably more useful. You're also likely to want to find out how many free seats you have for a performance - again, your current design would need to query 4 tables, whereas the alternative just requires a single query. Finally, if you ever want to change the way you structure "seats" - for instance by adding a "default price" column, your current design needs you to make that change in 4 places; using a single table with an "area" foreign key allows you to make that change just once. 

Note how I've also put some of the attributes you've currently assigned to "Product" into the child tables. This keeps the schema more "self describing", but adds a lot of work - only do that if you think the product catalogue will grow in the future, and you want to avoid EAV. 

We have a cluster of 3 Mongo 3.2.13 instances running with journaling enabled, with hundreds of DBs and collections which are accessed simultaneously. We are checking the option to disable journaling, to improve the cluster's performance. According to the docs, WiredTiger performs a checkpoint every 60 seconds, so in a case of crash we should only loose up to 60 seconds of data. We are ok with that. Are there any other risks / disadvantages for disabling journaling? 

I'm running a mongo 3.4.7 cluster (8 CPU, 120GB ram, primary + 2 secondaries) with ±130 open connections, each one of them to a separate DB with ±50 collections. After a few minutes of running, cache usage percent goes to 95% and stays there. Sometimes is goes down a bit to ±93%, but quickly returns to 95%. According to the docs, when cache usage is at 95%, WiredTiger will start using application threads for eviction, which slows down the DB and can explain why cache usage doesn't get higher than 95%. What is the reason that cache eviction can't keep up with the load, while machine resources are under-utilized (CPU, Disk)? Tried increasing the number of eviction threads to the maximum of 20, no luck there. Update: Here are the result of 

This means that when journaling is disabled, and there are multiple threads writing to the DB, checkpoints become frequent and slow down the entire instance. This, of course, was a sufficient reason for us to turn on journaling again. 

I created this test index to see if I can't solve the problem but with no success ( is was using the same index before the Image below, only with 2 more fields in the include ): 

Maybe this question is a little off topic, I will close if necessary, but, is there a way to configure Management studio, to when I open it, it starts already with x connections? So I don't need to start it and connect to x instances every time. 

But as you can see, files are named after a number ( ). This number is the ID of another table, so, if the PDF file starts with , I need to insert it into the ID . I just would like a start point. I'm trying to create a cursor, looping throug all files, and using left() or substring () function to get the number, inserting it into a variable, and comparing it , like . I need something like a string like this : 

Place the id of the maintenance plan you want to delete into the below query and delete the entry from subplans table: 

Then, I want to use the inside a loop, to list all indexes of this table ( TB_INF_CLI ) of all these databases, with this query: 

I'm having problems with 2 things: One: This query will need to run with parameters ( because it's already inside a procedure). the real query needs to be this way: