Now, if you run the same query, you'll see that the column takes up to 21 bytes of storage and averages 7.69 bytes for the 200,000 rows. The is still using a fixed 4 bytes for every row. Depending on your data, you may find that one or the other approach uses more space. Of course, where there are substantial differences in space utilization, there is the potential for more substantial rounding issues when you move to floating point numbers. Note as well that the results may be version-dependent. I did my test on an 11.2 database where the documentation indicates that always takes 4 bytes of storage. In 10.2, the documentation indicates that took 5 bytes due to the inclusion of an extra length indicator. It's not obvious to me why a 32-bit value would ever need a length byte so I'm hard-pressed to understand why an earlier version of Oracle might have required the fifth byte, but the documentation is consistent that it did. 

If you neither commit nor rollback the transaction, the transaction will continue to exist indefinitely. It will continue to hold its locks, potentially blocking other sessions, until either you end the transaction via a or a or until a DBA comes along and kills the session (or until something like a network hiccup causes the connection to fail). If a DBA kills the session, they will implicitly issue a rollback for this and any other open transactions. 

The problem is that the variable that you declare in your declaration section is not the same as the variable that is declared in the context of your loop. You've declared two different variables named with different scope. If you run your block without a declaration section, you'll notice that it works even though you aren't declaring a variable in the declaration section. When you write a loop, your counter is implicitly declared and is in scope only within the loop. After the , you can no longer reference the variable. 

Broadly, they mean that the partition(s) that need to be accessed will be determined at runtime. In the first plan, where both are , some earlier step in the plan is producing one or more values for the partition key. Those partition keys are then used to determine which partitions Oracle needs to scan the index for. In the second plan, the from the global index tells Oracle with partition to probe for each row that is being retrieved. 

In response to the comment below from DylanKlomparens -- you cannot just apply the to a and expect to get the proper result so I'm not sure that I understand what you are seeing. If you just to an on a plain , the timestamp is simply treated as having the time zone that you specified. It doesn't change the time. It also doesn't depend on the session time zone. In all these cases, if represents a time, the version represents the wrong time-- there should be an 8 hour difference between the starting and ending values. 

I agree with everything gbn said. The critical bit in his comment, is "if it wasn't critical for performance". For the vast majority of OLTP operations, it really doesn't matter how many levels of nesting you have (assuming that your data model and procedural layer is designed intelligently so that you've got a normalized data model and that you're not adding layers of calls for no good reason) because it really doesn't matter if it takes an extra couple hundredths of a second to insert a new row and all the associated and rows. The ability to reuse the logic later on more than makes up for the extra overhead of calling a few more stored procedures. When you are talking about data warehousing, however, where you need to quickly process millions of rows in a relatively tight load window, procedural logic can become problematic. A hundredth of a second per row, multiplied by 10 million rows, for example, is 100,000 seconds= 1667 minutes= 27.8 hours. In that sort of environment, performance is much more critical and you're far more likely to choose a set-based approach rather than calling a lot of smaller, more modular procedures. 

What is the problem with the table becoming large? Generally, any sort of OLTP query will access the table using an appropriate index in which case the size of the table is more or less irrelevant. The cost of using an index will grow at an rate-- practically, a b*-tree index will only add one or two levels for any realistically sized table. And you can potentially limit that further by using function-based indexes to limit the size of the index by doing things like only indexing the active rows. The only queries that should care about the size of the table are queries where you want to do a full scan on the table in order to do things like produce metrics about how many tickets have been opened since the beginning of time (or, at least, over a significant fraction of history). If you are concerned about those sorts of reporting queries, you can do things like use materialized views to pre-aggregate the data. Normally, I would suggest keeping a single table and ensuring that appropriate indexes and/or materialized views exist to support the queries about whose performance you are concerned. 

There was never a rule that bitmap indexes were only useful on columns that had relatively few distinct values. That was a myth that derived from the fact that bitmap indexes aren't appropriate for columns that are unique or mostly unique and that a lot of the columns that you would want to put bitmap indexes on happen to have relatively few distinct values. Richard Foote (who probably knows more about indexes in Oracle than any other person on the planet) has a nice article on bitmap indexes with many distinct values that walks through why this is perfectly reasonable and appropriate in much more detail. A followup article comparing bitmap and b-tree indexes on columns with many distinct values is also well worth reading. 

That should work so long as the application isn't doing a into the table. Of course, it's a hack upon a hack so it's definitely not going to win any awards for clean code. 

You can install another database on the same server that is already running one instance of a RAC database, yes. 

If you still allow transactions that span multiple statements, you would still potentially have dirty reads. Imagine that I have a simple banking application where session 1 is trying to transfer $100 from account A to account B. Session 1 starts a transaction and subtracts $100 from A. Now session 2 runs a report that tries to make sure that money hasn't gone missing (which is somewhat useful if you are running a bank). If session 2 is allowed to read the updated balance from A and the un-updated balance from B because session 1 is in the middle of a transaction, it has done a dirty read. The fact that session 1 is going to eventually commit the change does not change the fact that session 2 was able to get an inconsistent view of data. Of course, a database that disallowed rollbacks but allowed transactions that spanned multiple statements would be rather problematic. If I subtract $100 from A and then try to add $100 to B but I encounter an error trying to update B (perhaps I discover that there is a hold on B or that B has been closed), I'm in a pretty unfortunate position. Assuming I don't want $100 to disappear into thin air (which will make A rather unhappy), my application has to catch the exception and manually implement a rollback by putting the $100 back. In real applications where transactions involve changes to a bunch of different tables, some of which are done by triggers that the application may not be aware of, that is a non-trivial challenge. 

In general, there are no equivalent settings. Checking for deadlocks is something that happens automatically, there is no setting that configures when that checking is done. Oracle generally has no system-level timeout for locks. You can set but that only affects how long a distributed transaction would wait for a lock to be acquired, it has no impact on non-distributed transactions. Beyond that, a session will wait for a lock indefinitely unless the session requesting the lock specifies a timeout (i.e. ). 

If you are trying to generate both a and a , it would generally make more sense to have two sequences, i.e. and , that are both referenced in your statement. You could also populate and before executing the statement by making separate calls to and use the and variables in your statement rather than referencing the sequence though that is likely a touch less efficient. 

Depending on exactly what you need (and why you're running the query), you may want to truncate the to the hour or minute so that you're always comparing against data from the top of the hour 

In general, no. A tnsnames.ora change shouldn't require a reboot but some applications will read and parse the tnsnames.ora at startup to be able to present a drop-down list of servers to the user, for example, and will cache whatever was read when the application started up rather than re-reading the file. Depending on the situation, it might be easiest to reboot a Windows client rather than figuring out how to kill and restart any applications that might have the data cached. A sqlnet.ora change might benefit from a reboot for the sake of consistency though it is not required. If you are doing something like enabling dead connection detection on a server by setting , for example, it probably makes sense to reboot the server to make sure that the setting applies to all connections rather than just new connections-- if you're trying to debug why a particular dead connection is still hanging around, knowing that the server was restarted and that you're not looking at some artifact of a connection that was opened prior to the setting being made would generally be helpful. 

You can also specify the data types of the columns that will be in your table. Since there is no way to specify the average size, though, this tends not to be nearly as accurate as doing it yourself. In our case, though, it's pretty good because our column happened to be populated with strings that were, on average, half the maximum size of the column. In our case, though, it correctly estimated the allocated size of 72 MB with 67.94 MB used. 

Why does the code need to be more efficient? It seems unlikely that you would be dropping large numbers of tables frequently or that it would take enough time to drop tables that it would be worth optimizing. What trade-offs are you willing to make to cause the code to be more efficient? And what do you mean by "efficient" in this context-- wall clock time? The resources consumed on the server? Something else? The code that you have written is very straightforward and easy to follow. You could complicate things by, for example, submitting multiple jobs that each drop a subset of the tables so that you can have multiple threads working simultaneously. But that would involve making the code much more complicated-- you'd need to assign work to different threads and then coordinate the responses from each thread to verify that all the tables were dropped successfully. It seems most unlikely that this would be a trade-off that you would really want to make though it would reduce the wall clock time needed to drop all the tables. 

Why would you want to index if you're already partitioning on ? If you are regularly querying the data looking for date ranges much smaller than your partition grain, that is, you're regularly querying for date ranges of a couple hours, you probably want to adjust your partitioning strategy to create partitions more frequently. If you query recent data in smaller intervals than old data-- for example, you regularly aggregate by hour over the past day, aggregate by day over the past week, and aggregate by month over the past year-- you may want to merge smaller partitions together as they age (potentially in addition to compressing older partitions). Additionally, are you sure that you would want to index rather than creating a materialized view that pre-aggregates the data by some smaller interval than the partition? If you're aggregating data in various queries by hour, day, and month, for example, you'd generally be better served by creating a materialized view that pre-aggregated the data at the smallest grain (hour) and an Oracle dimension object that allowed query rewrite to use the materialized view to aggregate the hourly rows into daily or monthly results rather than trying to read all the data from the base table. 

First, are you really sure that you want to be starting out with a desupported version of the database that is at least 4 major releases out of date? It would seem much more useful to download a more recent version of Oracle from the Oracle Technology Network (OTN) and to start with that. You can get the express edition of Oracle 11.2 completely free and the enterprise edition is free if you just want to learn the product. Second, when you installed Oracle, it should have prompted you to create a database as part of the installation process. If you chose not to create a database during the installation process, you should run the Database Creation Assistant (DBCA) to create a database now. Note that what lots of products refer to as a "database" is more like a schema in Oracle. In general, you would only have one Oracle database on your machine though that database may have multiple schemas. What operating system are you using? 

In Oracle (and, realistically, most any reasonable database), everything is denied other than what is granted. So simply grant the user the privileges you want him to have 

Assuming that we are talking about jobs (rather than, say, jobs scheduled using the package), you can use the package to generate the DDL. Specifically, the function. Something like 

Beyond that, I completely second @BillThor's suggestions for how to create the various lookup tables. 

Is your intention to get every row eventually? For example, are you fetching all the rows and passing them to some sort of process that needs to process every row? Or is your intention to present pages of results to a human who will likely only look at the first or second page of results. Assuming that you are presenting results to a human that will only be looking at the first couple pages of data, the query you have is likely to be reasonably efficient. Assuming that there is an index on the column, in order to fetch rows 101-200, Oracle would simply have to read the first 200 values from the index then filter out rows 1-100. That's not quite as efficient as getting the first page of results but it's still pretty efficient. Of course, as you get further and further into the data, the pagination query gets less and less efficient. That's a problem if your intention is to eventually fetch every row. It's generally not a problem, though, if you are presenting results to a human. Humans don't really care how long it takes to fetch page 50 of a result set-- they're going to give up long before then. If your intention is to send the data to a Java process to process the data (this will be substantially less efficient than processing the data in the database-- Oracle and PL/SQL are designed specifically to process large amounts of data), it would generally make sense to issue a single query without an , have a master thread on the Java application server that fetches N rows of data, spawns a thread to process that data, fetches the next N rows of data, etc. 

If, on the other hand, you are trying to say that if = 1 then must be unique but you can have duplicate values if the is something else, then you'd want a function-based unique index. 

Because of deferred segment creation. In Oracle 11.2, when you create a table with no data, you no longer allocate any space in the tablespace. Oracle doesn't actually create the segment until you try to insert data into the table. This is a difference from earlier versions in which the segment was created when the table was created rather than when data was inserted. The reason for the new feature is that there are a lot of packaged applications on the market that create potentially hundreds of tables that will never have data because the customer isn't using some specific module of the project that uses those tables. It was annoying for someone that packaged a big ERP tool, for example, to end up with potentially GB of space allocated to tables that would never have any data. You'll only see deferred segment creation if you've set to 11.2 and if is set to the default of