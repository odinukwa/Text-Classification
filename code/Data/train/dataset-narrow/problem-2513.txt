I think one issue is that we need to fix the "scale" of the problem. For example, the paper I refer to below defines NPO so that the objective function takes only positive integer values. With that definition, 1. is easy: fix say $\varepsilon = 1$, and then you get a $\max\{N_\varepsilon, 2\}$ approximation by definition. With this definition of NPO, 2. also holds. In particular, no APX-Complete problem has an asymptotic PTAS unless the polynomial hierarchy collapses. See the remark on the bottom of page 1761 of this paper, and also the proof of Theorem 4.10. On a high level, the proof shows that if a problem has an asymptotic PTAS, then it can be approximated to any degree with a constant number of queries to an NP oracle (Proposition 4.6). On the other hand, any problem $P$ which can be computed with $k$ queries to an NP oracle can be reduced to approximating an instance of an APX-Complete problem $A$ within factor $r$, where $r$ is a constant depending on $k$ (Prop. 4.8). So, if you have an APX-Complete problem which has an asymptotic PTAS, then there is some constant $h$ so that the query hierarchy collapses to level $h$, i.e. any problem solvable with $k$ queries to NP, for any integer $k > 0$, can be solved with $h$ queries to NP, where $h$ is a fixed constant which does not depend on $k$. This implies that the polynomial hierarchy collapses as well (Thm 4.2). 

You can decide if a quadratic polynomial $p: \mathbb{R}^n \rightarrow \mathbb{R}$ has real roots with some linear algebra. As you note, the general case should be hard. Observe first that $p(x) \neq 0$ for all $x \in \mathbb{R}^n$ if either $p(x) > 0$ or $p(x) < 0$ for all $x$ (this follows by continuity). So it is enough to be able to decide if $p(x) > 0$ for all $x$. In general this is related to complexity-theoretic versions of Hilbert's 17th problem: a polynomial $p(x)$ is positive over the reals if and only if you can write $p$ as the sum of squares of rational functions and a positive constant $c$ (this is a theorem by Artin). Finding this decomposition or solving the decision problem is most likely hard in general, but the quadratic case is easy, because of the magic of the spectral theorem. For more information about the general case, look at Devanur,Lipton,Vishnoi, and Monique Laurent's survey. Let us write $p(x) = p_2(x) + p_1(x) + c$ where $p_2$ is homogeneous of degree 2, $p_1$ is linear, and $c$ is a constant. Let us define $q(x_0,x) = p_2(x) + x_0p_1(x) + cx_0^2$ to be the homogenization of $p$, where $x_0$ is an additional variable. Claim. $p(x) > 0 \Leftrightarrow \forall x_0 \neq 0: q(x_0,x) > 0$ The "if" direction is easy. In the non-trivial direction, assume $p(x) > 0$ for all $x$ and assume $x_0 \neq 0$: $$ q(x_0,x) = x_0^2q\left(1,\frac{x}{x_0}\right) = x_0^2 p\left(\frac{x}{x_0}\right) > 0. $$ QED Notice also that, because $q(x_0, x)$ is continuous, if $p(x) > 0$ for all $x$ then $q(x_0,x) \geq 0$ for all $(x_0,x)$ (including $x_0 = 0$). Since $q$ is homogeneous, we can write $q(x_0,x) = y^TQy$, where $Q$ is a symmetric matrix and $y = (x_0,x)$. By the above, if $p(x) > 0$ for all $x$, then $Q$ is positive semidefinite. Moreover, $q(x_0, x) > 0$ for all $x_0 \neq 0$ if and only if the kernel of $Q$ is a subset of the hyperplane $\{y = (0,x): x \in \mathbb{R}^n\}$. Both these conditions can be decided in polynomial time by computing the SVD of $Q$. 

I also disagree with the "note", at least stated in this generality. Related to this, does anyone know if David Johnson's Kanellakis award talk is available somewhere? Also, once we realize that all NP-hard problems are equivalent with respect to the worst-case complexity of exact solutions, it's very natural to inquire about the complexity of finding approximate solutions. And Chandra makes a great point about the change of perspective that approximation algorithms bring to algorithm design. I am sure we should be able to come up with some interesting examples of approximation algorithms used to prove a theorem. For example, the $O(\log n)$ approximation to set cover can be used to show a better bound on the sample complexity of learning conjunctions. 

I think this is at least as hard as sorting a set of integers $S \subseteq [n]$ with "random advice" of size polynomial in $n$. By random advice I mean that for any $n$ there is a fixed distribution ${\cal D}_n$ (depending only on $n$) over strings of size poly($n$) and our algorithm (modeled by a RAM machine) is given random access to a single sample from ${\cal D}_n$. ${\cal D}_n$ is the (randomized) data structure after pushing $[n]$ in order, together with a hash table that maps integers to identifiers in expected $O(1)$ time. Given that setup, for an instance $S \subseteq [n]$ of the integer sorting problem, we can issue extract($S$) (actually we need the identifiers of $S$ but this mapping can be done in $O(1)$ time per item using the hash table that is part of the advice) and the input will be sorted in the time it takes to execute extract. So, the message is that, unless some "free" side information that depends only on the upper bound of the integers can make integer sorting easier, extract is as hard as integer sorting. Does this imply a relation between the two problems without the weird model? Is this notion of random advice something known? This is sort of like a MA protocol, but Merlin's message is not allowed to depend on the input and we care about Arthur's running time. 

See Theorem 3.1. for the relevant result, although stated in terms of continuity of Gaussian processes. 

Furthermore, $URL$ proves that sparsest cut is NP-hard for graphs with pathwidth 2, and has quite a few more references to approximating sparsest cut on restricted instances. I would assume that for all classes of graphs mentioned in the paper, no exact algorithms are known (as they're interested in approximations). In particular, if sparsest cut is NP-hard for graphs with pathwidth 2, it's also NP-hard for graphs of treewidth 2, and cutwidth 2. I suppose that doesn't give quite a lot of room.. maybe there is another better parameterization for sparsest cut. I am pretty sure that sparsest cut is NP-hard on regular graphs but can't find a reference. 

I doubt you can do better than $O(n^{3/2})$. Here is a lower bound, using old arguments by Spencer and Erdos, that shows that the discrepancy is $\Omega(n^{3/2})$ if you expand the set system to cuts of induced subgraphs. See the edit for a lower bound for cuts as well, suggested by Domotor. Let $V$ be the vertex set of the complete graph, and fix a function $f:{V \choose 2} \to \{-1, 1\}$. Split $V$ into $L$ and $R$, each of equal size. Let $S$ be a random subset of $L$. Then, for any $v \in R$, $$ \mathbb{P}\left(\Bigl|\sum_{u \in S}{f(u, v)}\Bigr| \geq c\sqrt{n}\right) \ge 1/4, $$ for a suitable constant $c$. (This can be proved using the Central Limit Theorem or a 4-th moment argument.) Letting $T_0$ be the subset of all $v \in R$ satisfying $\Bigl|\sum_{u \in S}{f(u, v)}\Bigr| \geq c\sqrt{n}$, we have that $\mathbb{E}|T_0| \geq n/8$. So there exists some choice of $S$ such that $T_0$ has size at least $n/8$: fix such an $S$ and pick $T = \{v \in R: \sum_{u \in S}{f(u, v)} \geq c\sqrt{n}\}$ or $T = \{v \in R: \sum_{u \in S}{f(u, v)}\leq -c\sqrt{n}\}$, whichever is larger. Clearly $$ \Bigl|\sum_{u \in S, v \in T}{f(u,v)}\Bigr| \ge \frac{cn^{3/2}}{16}. $$ EDIT: As Domotor pointed out in a comment the above suffices for a $\Omega(n^{3/2})$ lower bound on the discrepancy of cuts as well. Let $S$ and $T$ be the sets above, and $U = V \setminus (S \cup T)$. Define $f(X,Y) = \sum_{x \in X, y \in Y}{f(x,y)}$ for any two subsets $X,Y\subseteq V$. Assume without loss of generality that $f(S,T) \ge c'n^{3/2}$ for $c' = c/16$ (the case $f(S,T) \le -c'n^{3/2}$ is analogous). Then at least one of the cases below holds: