As for the size of the encoding, assume $|X|\le n/4$. The numbers $s_u$ take $O(k\log(n/k))$ bits, which will be negligible. We have $s_u\le n/(3k)$ for at least $k/4$ of the $u$'s, in which case the encoding of $X_u$ takes about $H(1/3)\tfrac nk\approx0{.}92n/k$ bits; the remaining $X_u$ take at most $n/k$ bits each. The total is at most $0{.}98n$ bits. Decoding amounts to deciding which block $i$ goes into, and then figuring out $X_u$; the latter be done easily in space $n/k+O(\log n)$, and one can reuse the space occupied by encodings of the remaining blocks (as long as the total space is at least $2n/k$, which is OK for $k$ large enough). A better analysis shows that this scheme achieves space essentially $H(1/4)n\approx0{.}812n$: let $p_u=s_u/(n/k)$. Since $|X|/n\le1/4$, the average of $p_u$ over $u<k$ is at most $1/4$. The encoding of $X_u$ takes about $H(p_u)n/k$ bits. Now, the entropy function is concave, hence the average of $H(p_u)$ over $u<k$ is at most $H(1/4)$, and the total space is $H(1/4)n+O(\log n)$. This is optimal up to the $O(\log n)$. There is of course nothing special about $1/4$. The same argument shows that for any constant $0<c<1/2$, there is an encoding scheme for $\le cn$-size subsets of $\{0,\dots,n-1\}$ that takes $H(c)n+O(\log n)$ bits, and can be decoded in-place. To some extent, it can even be used for $\le s(n)$-size subsets where $s(n)\ll n$, by taking a non-constant number of blocks ($k\ge 2/H(s(n)/n)$ or so), but then the $O(k\log(n/k))$ overhead gets more pronounced, and overtakes the main term when $s(n)$ drops below roughly $\sqrt n$. 

Oblivious classes do even better. Taking into account the objection raised by Apoorva Bhagwat, let $\mr{NLin=NTIME}(n)$. Then $\mr{NLin}\cup O_2\mr P\nsubseteq\mr{SIZE}(n^k)$ for any $k$, and the same argument yields: 

We could also accommodate strict inequalities $p_i(\vec x)>0$ by including $p_i$ among both the $f_i$’s and the $h_i$’s. We will need 

The algorithms work as follows. There are parameters $k>0$ and $n=b_0>b_1>\dots>b_k>0$ depending on $n$. We split the input size-$b_0$ array into size-$b_1$ blocks (level 1); we split each level-1 block into size $b_2$ blocks (level 2); and so on up to level $k$. (Let us pretend all $b_i$ are powers of $2$, to avoid issues with divisibility.) Then: 

${}^\dagger$ Actually, the reduction as given is even uniform $\mathrm{AC}^0$, and it can be made DLogTime if we replace $|w|$ with an upper bound that is a power of two. 

Now, the final complication. I defined the problem as approximation of $e^\alpha$, whereas the question asks for $\re e^\alpha$. This actually makes the problem harder: if $\re e^\alpha\ll\operatorname{Im}e^\alpha$, then an approximation $x+iy$ of $e^\alpha$ to $m$ bits of relative accuracy has error $y2^{-m}$, which may well be larger than $x$ itself, and in any case is not guaranteed to be bounded by $x2^{-m}$. For exponentiation of exact rationals, we can circumvent the problem: 

Blum’s speedup theorem is usually stated in the language of partially recursive functions, but up to trivial differences in notation, it works just the same in the language of $\lambda$-calculus. It says that given any reasonable complexity measure $M$ (for example, the optimal number of reductions as in the question) and a recursive function $f(x,y)$ (for example, $2^y$), we can find a recursive predicate $P(x)$ such that: 

The answer is positive if $f$ and $g$ belong to a “well-behaved” family of functions which ensures that they are asymptotically comparable; in particular, if they are (restrictions to $\mathbb N$ of) functions taken from a Hardy field. For example, this holds if both functions are first-order definable (with parameters) in the real exponential field $(\mathbb R,+,\cdot,\exp)$, or in any o-minimal expansion of the real field for that matter. 

Base conversion can be done in time $O(M(n)\log n)$, where $M(n)$ is a bound on the time complexity of multiplication of two $n$-bit integers. (We assume that $M(n)$ satisfies usual regularity conditions: it is monotone, and $2M(n)\le M(2n)$.) The standard divide-and-conquer algorithms are described e.g. in Brent&Zimmermann, Modern computer arithmetic, and go as follows: 

This is very unlikely to hold, but (as usual) impossible to rule out using current techniques, as we can’t even prove $\mathit{PSPACE}\ne P$. However, we can at least show that no relativizing argument proves such an inclusion, i.e., there are oracles $X$ such that randomized polynomial-time with a $(\Sigma^P_{(\log n)^c})^X$ oracle does not include $\#P^X$. Better yet, there are oracles where it does not even include $\oplus P^X$ (which is included in $P^{\#P^X}$): otherwise, there would exist $O((\log\log n)^c)$-depth quasipolynomial-size unbounded fan-in circuits that approximate parity with error, say, $1/4$, contradicting Håstad’s theorem. 

I’m not sure about the exact definition as given. However, the kind of search problems that has been studied the most in the literature are NP-search problems. In this context, there is no meaningful difference between “BPP-like”, “RP-like”, or “ZPP-like” randomized polynomial-time algorithms, as we can check the correctness of any purported solution in deterministic polynomial time. Thus, while the class of NP-search problems solvable in probabilistic polynomial time has been studied for a long time, it has not been generally called “Search-BPP”. In particular, the class is mentioned in Papadimitriou’s seminal papers [1,2], where it is denoted FZPP. [1] Christos Papadimitriou, On inefficient proofs of existence and complexity classes, Annals of Discrete Mathematics 51 (1992), pp. 245–250, doi: 10.1016/S0167-5060(08)70637-X. [2] Christos Papadimitriou, On the complexity of the parity argument and other inefficient proofs of existence, Journal of Computer and System Sciences 48 (1994), no. 3, pp. 498–532, doi: 10.1016/S0022-0000(05)80063-7. 

Proof sketch: On the one hand, we can first compute the fixed point approximation of $\alpha$ and then exponentiate it. Note that $m+O(1)$ bits of absolute accuracy for $\alpha$ determine $e^\alpha$ to $m$ bits of relative accuracy, and vice versa. On the other hand, if we can do algebraic number exponentiation, we can also do plain exponentiation, as we can readily convert a fixed point number (considered exact) to its representation as an algebraic number. By the comments above, this means we can also compute logarithms in the same time bound. Thus, we can approximate an algebraic number by first approximating $e^\alpha$, and then taking a logarithm. QED This splits the question to two quite unrelated problems. The best known upper bound on exponentiation is 

$\def\tc{\mathrm{TC}^0}\DeclareMathOperator\len{len}$Since there were no other takers, let me expand the comments above. The fundamental fact here is that approximations of basic arithmetic operations, and trigonometric and cyclometric functions like $\sin x$, $\cos x$, $\arctan x$, are computable in uniform $\tc$ (and therefore in logarithmic space) for fixed-point rational inputs. See e.g. Reif, Reif and Tate, Maciel and Thérien for the basic arguments, however, the most crucial result for the uniform version is the proof that integer division and iterated multiplication are in uniform $\tc$ by Hesse, Allender, and Barrington. This is optimal in that all such functions (except addition and subtraction) are $\tc$-complete. (These results are typically stated for approximation of functions on a bounded domain, but they apply to arbitrary fixed-point inputs all the same, the point being that as long as there is only a linear number of bits before the binary point, we can reduce the input modulo $2\pi$ by just computing an approximation of $2\pi$ with a sufficent accuracy, followed by a division.) This has several consequences. First, $\sin(x2^y)$ and $\cos(x2^y)$ can also be approximated in uniform $\tc$ when $y$ is negative, by converting the input to fixed-point notation up to the desired accuracy. Thus, from now on I will only consider the case with $y\ge0$. We can as well also assume $x\ge0$, as this only affects the sign of sin. Second, the problem is not really about trigonometric functions, as all the complexity is in reducing the input to a bounded domain. More precisely, we have: 

By request, I’ll turn the comment into an answer. Toda’s theorem says that $\mathrm{PH\subseteq BP\cdot\oplus P}$. Since $\mathrm{BP\cdot AM=AM}$, this shows the following implication: if $\oplus\mathrm P\subseteq\mathrm{AM}$, then the polynomial hierarchy collapses to $\mathrm{PH=AM=coAM}$. (In fact, the whole $\mathrm{Mod_2PH}$ hierarchy collapses to $\mathrm{AM=coAM}$ under the same assumption.) $\#\mathrm P$ is a class of functions, not of languages, hence it is meaningless to compare it with AM directly. However, if you consider the closely related class $\mathrm{PP}$ instead, the case is similar to $\oplus\mathrm P$: (another version of) Toda’s theorem says that $\mathrm{PH\subseteq P^{PP}=P^{\#P}}$. Thus, if we had $\mathrm{PP\subseteq AM}$, it would follow that $\mathrm{PH\subseteq P^{AM\cap coAM}=AM\cap coAM}$, so we get the same conclusion. Mutatis mutandis, the same argument suggests neither $\oplus\mathrm P$ nor $\mathrm{PP}$ is contained in $\mathrm{PH}$ as a whole. I am not aware of any nontrivial inclusions of $\oplus\mathrm P$ or $\mathrm{\#P}$ in other classes (I suppose $\mathrm{\oplus P\subseteq P^{\#P}\subseteq PSPACE}$ count as trivial; there are also levels of the counting hierarchy, but again they contain the offending classes by definition). 

The answer to “alternatively, can a directed graph have an eigenvalue with an exponentially small imaginary component” is YES (though I don’t understand what is “alternative” about this statement, as it does not in any way disprove the conjecture). As I already wrote in a comment, it is a not very difficult exercise to show that if $f\in\mathbb Z[x]$ is a monic polynomial, there exists a directed graph $G$ on $O(\deg(f)+\|f\|_1)$ vertices whose eigenvalues include all roots of $f$. This implies the statement above modulo the fact that such polynomials may have roots with exponentially small imaginary part, which I assumed to be easy to find in the literature on minimal root separation. However, I realized that such bounds are actually not as readily found as I expected, so I decided to write it as a proper answer for the record. Several examples of polynomials with exponentially small root separation are listed by Schönhage [1], in particular the family of polynomials $$x^n-2(cx-1)^2\qquad(n\ge3,c\ge2)$$ attributed to Mignotte [2] (which I cannot verify as I have no access to it at the moment). Now, these polynomials have each a pair of real roots near $1/c$ in distance $<2/c^{1+n/2}$, whereas we need a pair of complex roots. This is, however, easily accomplished by modifying the polynomial slightly: let $$f(x)=x^n\mathbin{\color{red}+}(2x-1)^2=x^n+4x^2-4x+1.$$ Clearly, this polynomial has no positive real root (and no negative real root either if $n$ is even). Moreover, it is easy to show that it still has a pair of (necessarily non-real) roots in exponentially small distance to $1/2$; if I didn’t mess up the computation, these roots are approximately $$z_\pm=\frac12\pm i2^{-1-\frac n2}+O(n2^{-n}).$$ Now, $f(x)$ can be written as the determinant of e.g. the $n\times n$ matrix $$\begin{pmatrix} x&-1\\ &x&-1\\ &&x&-1\\ &&&\ddots\\ &&&&x&-1\\ 1&-4&4&&&x \end{pmatrix}$$ and therefore as the characteristic polynomial of the adjacency matrix of the weighted directed graph $G_0$ on $n$ vertices $\{0,\dots,n-1\}$ with edges $i\to i+1$ of weight $1$ for $i=0,\dots,n-2$; $n-1\to0$ of weight $-1$; $n-1\to1$ of weight $4$; and $n-1\to2$ of weight $-4$. The eigenvalues of $G_0$ are thus exactly the roots of $f$, including $z_\pm$. Finally, the eigenvalues of $G_0$ are included among eigenvalues of the unweighted directed graph $G_1$ on $2n+6$ vertices $$0_+,0_-,\dots,(n-2)_+,(n-2)_-,(n-1)_+^0,\dots,(n-1)_+^3,(n-1)_-^0,\dots,(n-1)_-^3$$ with edges $i_+\to(i+1)_+$ and $i_-\to(i+1)_-$ for $i=0,\dots,n-3$; $(n-2)_+\to(n-1)_+^j$ and $(n-2)_-\to(n-1)_-^j$ for $j=0,\dots,3$; $(n-1)_+^0\to0_-$, $(n-1)_-^0\to0_+$; and $(n-1)_+^j\to1_+$, $(n-1)_+^j\to2_-$, $(n-1)_-^j\to1_-$, $(n-1)_-^j\to2_+$ for $j=0,\dots,3$. References: [1] A. Schönhage, Polynomial root separation examples, Journal of Symbolic Computation 41 (2006), no. 10, pp. 1080–1090, doi:10.1016/j.jsc.2006.06.003. [2] M. Mignotte, Some useful bounds, in: Buchberger, Collins, Loos (eds.), Computer Algebra: Symbolic and Algebraic Computation, 2nd ed., Springer-Verlag, 1983, pp. 259­–263, doi:10.1007/978-3-7091-7551-4_16.