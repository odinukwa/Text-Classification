Yes, this will work. The Jinja2 templating engine can access dictionary vars that originate in the role. There is no need to specify 

This is almost certainly an issue with the formatting of your public keys in the authorized_keys file. Check that there are no line breaks in the public keys (the key should all be on one line), and that there are no omitted characters. 

Does anyone know what sort of data transfer speed we should be seeing on a VPC peering connection between 2 AWS regions (in this case us-west-2 and eu-west-2)? We tested this a couple of weeks ago, and saw speeds in excess of 200MB per second. Today, we are seeing speeds at ~10MB per sec. Obviously, we are using public transport networks here, so we would expect some variance based on time, but 10MB per sec seems very slow. Given that AWS facilitates cross-region replication of DBs in RDS, presumably they anticipate much faster speeds that this. 

Set up a private DNS zone in Route53 instead. This is only available within your VPC, so you don't need to register it publicly or anything like that. Then, set up CNAME records for your DB instances, which point to the RDS domain. Set very low TTLs (< 30s). Then, configure your app to use the CNAME records. If you then need to re-direct traffic to a new RDS instance, just update the CNAME record. Also, if budget permits, you may want to consider using Multi-AZ within RDS. You'll pay twice as much, but its very handy in both scheduled and unscheduled failover situations. 

SPF and DKIM must not be seen as panacea for email deliverability problems. Period. Depending on many factors (vendor, software & others...), they may increase chances for good outcome, or decrease chances of messages being bounced at SMTP handshake level, provide sender integrity or cause email servers to bounce spammers pretending to come from your domain name, and that's it. It is also important to know that there are senders with long-established good reputation that don't use this two techniques at all (although not using SPF is really weird in 2017). I'd say your problem is well-known - Why Does hotmail still reject my emails? . With the details you have provided, I'd suspect that content of messages does have some significance to antispam software they are using - maybe try to alter content and see the results. 

After changing an IP address of email server (Postfix stack) I've found host that denies email message with: 

We know how to reject executables and other potentially dangerous file extensions via mime_header_checks directive: 

may alter your file. It does mean that anyone on the system may do it - and this is potentially a lot of third parties. Let's say you have one hacked Wordpress website, just one among the many others, which runs on apache or designated system user - this site may be used to iterate system in order to find any accessible (readable, writable, executable) file - and guess who is on board? So, stick to general recommendation to provide only minimum rights to the part of your system (whatever it may be) needed to work. permission is rarely used besides non-private non-secret type of files. 

To see what the remaining TTL on the NS record was, but now I understand that this NS record is the NS record for subdomains in the zone, and not the NS record that emanates from the root servers, which is the one that ultimately determines to which name server the query will be sent. I tested this by setting up a test record in the zone in each of the providers: 

Don't under-allocate either. If you use a load of Elastic Load Balancers, remember that they will also consume available ip addresses on your subnets. This is a particularly true if you use ElasticBeanstalk. 

If downtime is not allowed, you could clone your server instance by making an image of it, then relaunching the image with the correct security group. You could then create an ElasticLoadBalancer, place both the original and the clone behind the ELB, switch DNS (with very low TTL) from the priginal server to the ELB endpoint and the wait until the load is distributed across both. You would then drop your original server out of the ELB, using connection draining, and reverse your DNS change so that it now pointed to the ip address of the new server. You can then stop/terminate your old server, leaving you with the new one in the correct Security Group. Of course, this solution won't work with a server running a writable database, but if you've got a writable DB on a single server instance in EC2 Classic, you've got other problems. 

Basically yes. The point that tools like and are not there to provide you with 100% detection rate and malware protection. Even enterprise-class antivirus solutions is likely to fail that - but, from my experience (some of them) are much better than those two. and are here to get you rid of whole bunch of malware - and, believe me, that is a lot of them. The second part of your question is regarding missing of some very well-known malware. Unfortunately, it happens. This is subject of discussion. To me it is shocking that these two fails to detect b374k Shell. But again, it happens and proves that, even if detection rate were 99.9% one should not plan security strategy by relying on quality of antimalware scanner. 

Needless to say, you shouldn't hope for positive answer from any blacklist vendor unless you made sure that all standard conditions they may request from you are fulfilled: 

You may use Monit. This program regularly checks (on adjustable time interval - 2mins, 5mins...) number of vital system parameters, and is I think even on by default. When parameter () goes out of the adjustable threshold default is to send to you notification email. If this is favourable, you may login via ssh and do and other standard tools in order to gain quick and rough insight about what is happening. Second option would be to configure Monit's custom script execution instead of (or together with) sending of notification email. This custom script may do simple and you would have good starting point to investigate high load averages. 

httpd doesn't know what the hostname in the host header is until the decryption process takes place, so if the original request doesn't match the hostname in the certificate, you will get an error. If you want to have both domains work with https, you will need either: 

This is based on logstash finding an indent at the start of the line and combining that with the previous line. However, the logstash documentation is the only place where I can find a reference to this. The general user community seems to be using elaborate grok filters to achieve the same effect. I've tried the basic indentation pattern provided by logstash, but it doesn't work. Has anyone else managed to get this working by matching the indentation pattern? 

I set up a single Route Table that is associated with both of my public subnets. this includes route entries as follows: 

Run the following shell script in cron once per week on your servers will always have the latest Amazon Linux AMI: 

I am a long time AWS user but need to trial something on MS Azure. I have created a Free Trial subscription. I then created a Namespace in the Default Directory, and added the user to that. I want this user to be able to use Azure services as a Global Administrator. When I login as that user, I get a splash that says the user has no active subscriptions. Is this something to do with the Free Trial (ie it only allows one user)? I can see that "Rights Management" is disabled in the Default Directory, and when I try to enable it, I am told "Cannot detect Rights Management (RMS) support for Default Directory. To use RMS with this tenant, you must have a subscription that supports RMS." 

As far as I know, has on his own (I think Perl Mail::SpamAssassin). Before installing , I was using binary and daemon ( and ) plugged into MTA (Postfix via pipe). My question is: what to do - is it safe to now after installation and configuring of ? OS is Ubuntu 14.04 LTS. 

In order to diagnose why emails are not being sent, in your situation (full control over the system), it is especially important to distinguish whether this is at application or system/email level. You gave us very little details about your issue. Consider: 

Since no one answered: Clearswift Secure Email Gateway is an email security software and they are in charge of their internal blacklist and the 'world outside' does not know on basis of what criteria this blacklist is established and maintained. So, if you got new IP address which gets rejected with error message posted above you have option to contact and ask them to remove your IP address from their blacklist. 

EDIT (12.12.2017) This is a component of some email security software, not sure what kind exactly, and does not operate by (or, at least not solely by) local settings. So, this software has some kind of centralized black list and contacting administrator of other email server, although may solve specific case by manual whitelisting IP address is not good enough option. Now, the question is, how to get out of this list? 

Have managed to test this and I can confirm that it does work. Whereas ElasticBeanstalk assumes that your Dockerfile will build on and add to the base image, it is not actually necessary to do this. The Dockerfile can simple refer to the base image your want to use (eg in the AWS ECR) and the port you want to expose when it is deployed to the ElasticBeankstalk instance. This then allows you to use ebextensions in a Single Container Docker environment. 

This will revoke the agent cert on the master, remove it, remove the cert on the agent and re-generate a new one. Save this file locally and then pass it to the Amazon EC2 API when launching an new instance 

This took a lot of trial and error. Basically, when you want to limit the user based on specific resources, you need to create a Statement that first denies the ability to run instances unless conditions are met on specific arn resources, and then at the end, permit them to do anything. Update: Amazon have admitted that their docs were inaccurate: $URL$ 

From the client src, I can see that this error occurs when the path isn't found in the deployment object. Is this the correct path to use? There is only 1 volume in the deployment. 

As you see, there is a lot to check and eliminate as possible cause. By the way "System mail name" should not be but - check the facts about fully qualified domain name. 

It is not problem at all if you send mail from main server / 11.11.11.111 that is not signed with DKIM , unless you have DMARC policy defined. All that is going to happen is emails not being signed with DKIM, which is not such big deal if SPF set correctly, which is not the case in your given example. SPF is almost completely useless if "hard fail" is not designated ( at the end of SPF TXT record entry). Hard fail tells receiving MTA's around the world that only designated hosts/ip are allowed to send emails for domain and no others (thus, hard fail - reject policy). In order to establish and maintain proper SPF, try to utilize online generators (example: SPF Wizard) , and of course, monitor results - send to tools that will confirm whether record is ok or not. 

That entry corresponds with my ssh-key-based login event. SSH is listening to default port - 22. What is the role of logged port 51150 and does it mean I cannot use restrictive iptables settings that blocks all traffic incoming to ports other than ones that I have previously specified (like 20,21,25,80,443,143...) 

OK. I can explain this. Apache mod_proxy_balancer doesn't have its own independent healthcheck mechanism. The state of Balancer Members (workers) is determined based on outcome of actual forwarded users requests. Sequence is as follows: 

ie Every minute (or whatever interval you have set) Logstash starts at the beginning of the bucket and makes an AWS API call for every object it finds. It seems to do this to find out what the last modified time of the object is, so that it can include relevant files for analysis. This obviously slows everything down, and doesn't give me real time analysis of the access logs. Other than constantly updating the prefix to match only recent files, is there some way to make Logstash skip reading older S3 Objects? There is a sincedb_path parameter for the plugin, but that only seems to relate to where the data about what file has last been analysed is written. 

I run a number of standalone Logstash servers to allow review of log files from web application servers. One of these recently reported a Yellow cluster state due to unassigned shards. This is a common enough occurrence, which I usually deal with by deleting the most recent index and restarting Elasticsearch. In this case, it didn't work. When I delete the indices (either via the API or simply by deleting the files from the file system) and restart Elasticsearch, the cluster state initially is green, but as soon as the first index is created, it turns yellow with precisely 5 unassigned shards. This server was working fine for several weeks, and is not at all loaded. I've also checked that there are no other Elasticsearch servers in the CIDR (Its in a VPC in Amazon AWS anyway). I've turned on debugging in the logs, but its double dutch to me. There are no references to shards not being able to be assigned. 

emails sent with PHP, in particular case PHPMailer (non-smtp submitted, ordinary PHP ) are signed incorrectly. I have heard about phpMailDomainSigner, which is kind of hack approach, and I would like to solve this at the top - MTA / milter level if possible. Here are: /etc/opendkim.conf 

How do you set up automatic email forwarding based on conditions like: forward if from emailaccount@domain.tld forward if from domain is domain.tld forward if subject contains word 'Urgent' and so. I'm only able to forward (redirect) all emails from one mailbox to certain email address. 

This configuration should limit 4 emails per 2 minutes, thus 120 per hour - but I'm not sure if it works. 

I solved this problem not before utilizing and observing real-time traffic. Turns out that several sites - several apache's was having long lasting, self-initialized http requests to some scripts. That scripts was some Wordpress security plugin cron scripts - they were doing site scanning and they had some nasty bug that was preventing scanning termination and successful exit (this info is founded later). I cut those off and problem was immediately gone. 

Keep in mind that you do have unofficial, of-the-record and very, very, very important limits in sending emails to ESP giants like Google, Microsoft and Yahoo, and probably more. While you are limited only by hardware performance and your ISP's network infrastructure (and possibly policy) in terms of sending emails with Postfix (to extend Simon Greenwood's answer) "giant ESPs" and some spam protection software can induce you more trouble than you asked for if you do not keep your server from sending at some reasonable rate. I suggest you read awesome Postfix rate limiting â€“ Politeness goes a long way article to get some usable info. 

DNS in a VPC works as follows: When an instance starts, it registers with the DNS forwarder in the subnet. Thereafter, when any other instance in the VPC queries any hostname for that instance, it will return the private ip. When the instance is stopped, the record with the forwarder will expire. Thereafter, when any other instance in the VPC queries any hostname for that instance, it will return the private ip. This is because the local DNS forwarder doesn't have a record and forwards the request to the primary DNS servers for Amazon. 

I am trying to use puppet as follows in Amazon AWS: I deploy an instance which has hostname "server.example.com", which launches the puppet agent on boot and gets an initial software payload. I then terminate that instance. I then re-launch a new instance with the same hostname, which generates a new private key and csr for that instance with the same hostname "server.example.com" This time, the puppet agent update fails, because the private key on the new instance, doesn't match the key for that hostname on the puppet master. How do I overcome this? 

My colleague and I use Ansible to manage a group of servers. We use the .ansible.cfg file in our local home directories to setup our local environments and keep our playbooks in a git repo. When authenticating to servers, I use user1, and he uses user2. 95% of of our servers have these accounts, but historically reasons, a few servers only have a "user" account. We're using host_vars to set the remote_user variable for the minority of servers in question. However, in our playbooks, we generally user "all" to stipulate what servers we want to hit, and use the --limit parameter on the command line to specify exactly which servers should get the update. Our server farm is a legacy of mis-mash poorly engineered servers that have to be kept online until they are retired in a few years, and we've found that this approach best suits our needs. Our issue is that our remote_user parameter is set in our .ansible.cfg file, where it is exposed as environment variable rather than a script variable. That means if our task contains: