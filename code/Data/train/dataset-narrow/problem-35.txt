A quickly formulated method, read first one that popped in my brain (not best), could be. Find the closest points on a parametric spiral for each sample (read A Pixel Is Not A Little Square3). Then place the samples on a line by placing the pints in one axis by how far they are from your spiral line and the other by what the closest point is. 

Ok, monotonic interpolation depends on what you are monotonic about. For a simple 1D function interpolation monotonicity is easy to define. But for a 2D and 3D dataset its not so self evident what the situation would be. 

Image 1: A hypothetical bitmap with 15 bits per pixel (for 5 bits per color), similar arrangement is used by some bitmap formats (this actual pattern is available in BMP). For a actual format read for example about the BMP format on wikipedia. Because the lower bit depths have very little space for color values these formats are often accompanied by a lookup table instead. This allows you to store a custom set of colors very tightly packed while allowing for a sensible variance. 

Hard to say because we can not see the code. Subsurface scattering might be part of that equation. I would just point out that human brains are extremely specialized in facial recognition. It has been postulated that the brain has a inbuilt defence mechanism to detect alien impostors/anomalous people. You are right in middle of what is known as uncanny valley nothing sort of near perfect seems to get it right, it seems It is also worth noting that we do not really know what it is that is lacking that puts you in uncanny valley. So it seems to me we do not conclusively know what, if anything, is missing. So the entire thing might be because we are neglecting something important but as of yet unknown. 

Gobal illumination, radiosity, etc is not a mutually exclusive set with using a fill light to adjust the feeling. Sometimes the artist can do wonders with a few well placed lights. So yes they are used, each game and situation is different. Perhaps they are just going for a unrealistic mood or the artist wants to tweak the result a bit. 

Both steps can be easily taken care of by your cartography software. Will the data distort... yes but since you project information onto a sphere from whatever source onto a sphere and to cylindrical coordinates it will just interpolate the distortion away. If anything your just going to have slight over resolution at poles. Quite a lot of data is in fact in this format to begin with. But nearly all 3D applications can use projected textures and texture baking to do these steps for you. Also most 2D applications like Photoshop have tools for turning rectangular coordinates to polar coordinates which do the same thing The real question is what your original image data is like? 

Build a matrix with one of the vectors a=(1,0,1) now you need another vector to define the matrix vector say b=(1, 0, 0) take the cross product of a and b lets call this c. Then take the cross product of a and c for a rectified b'. Normalize a, b' and c. Your matrix T is now [a b' c] plus any affine part you deem neccesery. Then do T-1 * Rx(Angle) * T. where Rx(a) is a function that returns a rotation matrix around the x axis with a angle of a. 

Image 2: Coloring the volumes based on distance to light. Certainly the coloring algorithm may be whatever you like. for example the color could become darker the nearer you are to the extrude edge etc. And here is very quickly done extremely dirty live example of colored shadow volumes: 

Image 2: Curve network. There are many ways to do this, infinite ways in fact. Also if you want to be compatible with other mainstream CAD apps use NURBS instead of beziers than its easier for you to write a exporter/importer if your data set is rich enough. Beziers are just special cases of NURBS after all so your code itself should not need to change all that much (fitting also gets easier). What you do is simply fit a curve trough your points in the second direction this will then give you the corresponding hull in the other direction. In your case you could just make a Bezier patch for each segment and then try to deduce a natural tangent direction. One way Like I said theres a infinite ways of constructing these, you have an artistic freedom of doing whatever you see fit it's a over constrained problem. One way of constructing this would be to have a patch for each consecutive cross section. This is nice and tidy as it generalizes works well with an arbitrary number of cross sections. 

It turns out that the chromaticity chart is harder to read than I anticipated. The CMYK slice is actually triangular of sorts its just that the chromaticity chart is not really linear. The chromaticity chart is made with the assumption that light is a spectra. The curved arc is the pure spectral color of the rainbow. Everything in between is interpolated edge values. 

Image 1 Straight skeleton (magenta) and insets by finding the intersection of tubes along edges, and then the created edges on mesh (blue) From this experiment I can conclude that are many complex cases to consider. 

Image 1: Animation of arranging in tree by shooting rays You can optimize this by using bounding boxes/ bsp trees for the test areas. But for a small number of areas that might be overkill. just sorting them left to right top to bottom should suffice. You can use the same algorithms as your scan line operators. 

Trick is, to move the entire object so that the point about which you want to rotate is at the center. Then rotate and after that counter move it so that the point is were it was. In fact this is not so much of a trick, as such, nearly all graphics engines work this way. It is just abstracted away in many cases. Most often you will see it done in matrix notation like this: $$ T^{-1} R T $$ Where T is to transformation from world to pivot and R is the rotation matrix. Since it can be easily packaged this way you end up with a function called rotate about the pivot point. This is usually the reason why drawing API's allow for a transformation matrix. 

Human eye is less sensitive to chrominance than luminance, which means you have a nifty way to compress the data by not measuring chrominance for each pixel, while you do measure luminance of all pixels. Although the black and white thing is part of the reason. On the subject RGB might be more intuitive, but its not actually a very good intuition when you start going deeper into how the human senses actually work. As it turns out RGB is a gross simplification. RGB is not good for measuring how far away different colors are from each other visually for example. 

Image 2: input blue triangle as 3 points (black), return value projected segment in orange on plane. 

Image 1: You can subdivide a triangle into 3 quadrangles The reason lies in what has become called edge loops. The person doing the modeling have to anticipate how the subdivision happens as the subdivision is going to be the final shape. Unfortunately humans are only really good at deciphering the shape of the object along the edges of your primitives edges. By formulating the shape into continuous multi edge long loops helps us predict the shape after subdivision and importantly after deformation by bones etc. A triangle has a nasty way of terminating the loop so we do not understand what happens with the shape within and out of that shape. The subdivided mesh thus has a tendency to behave uncontrollably, causing undesired bumps. Note: It is possible to subdivide triangles in a way that this does not happen, they are just harder to work with and working with quads were well known by then. Now this is not actually the original reason, only it happened in roundabout way. The original reason what that the geometrical patches that they did use a as parametric primitives are square in shape. As extending a line into a surface naturally takes a square shape if you just extrude out. Having a triangle causes one edge to be degenerate and have a singularity. But this is very much related to the subdividing reason as it can be shown that a subdivision surface is just a general case of a spline patch. 

Yeah, that makes sense. Most flat panel monitors on the market have a 60Hz refresh rate! So you are not going to be able to flicker faster than 60Hz due to technical limitation. Were you to do this with a electron sweeping oscilloscope, or a LED lamp connected to a PWM source fast enough then no problem (you can find this in arduino samples). It would work. You can also emulate the effect by rendering multiple frames and then blending them. But you should take careto do correct kind of blending as the monitor output is not nesseserily linear! 

Okay let us start by pointing out that a colmun major matrix is the same as a transposed row major matrix. So: $$ {M_{cm}}^{T} = M_{rm} \tag{1} $$ Then notice that matrixes have following properties. Matrix multiplication is associative (2a) and that the distribution of transpose reverses computation order (2b). $$ A(BC) = (AB)C \tag{2a} $$ $$ (AB)^T = B^TA^T \tag{2b} $$ From these follow that the chain of multiplication events can be as long as one likes and all transpose does is inverse the computation. So given that we can conclude $$ {T_1}\ {T_2}\ \ldots\ {T_n}\ \vec{v} = \vec v\,^T\ {T_n}^T\ \ldots\ {T_2}^T\ {T_1}^T, \tag{3} $$ so calculating in column major order is the same calculation as calculating in row major orientation. The difference is that the order of computation is reversed. An interpretation of this is that the difference is the same as looking at the object from the outside or looking at the situation from the object out. Formula 3 should provide you the insight. Just one more thing a column vector is the same as a transpose of a row vector. Note: You are allowed to mix and match the computations as much as you like. All you have do is remember is that the other approach is just transposed. 

You simply dont want fully smooth results. While the commented method by Nathan Reed: "Calculate each vertex to face normal, sum them, normalize sum", generally works it sometimes fails spectacularly. But that is of no importance here, we can use that method by adding a rejection clause to it. In this case you simply want certain parts not to be smoothed against certain other parts. You want selective hard edges. So for example the flat top and bottom is separate form the triangle strip on the side, as is each flat area. 

Yes, such rendering exists. It is called a parametric surface and there are several methods you can render such entities. In this case you are talking of bezier patches. 

The relative size of the spacing of knots is irrelevant for the NURBS curve. The only thing that matters is that they keep the relation. Note this may not be wise as parametrization may have other uses behind the scenes. 

It does not relate to low level libraries. Yes, usually each frame is drawn separately, nothing can change if i dont change anything. However the draw loop is often handled by somebody else in for example game engines, etc. So in fact processing is often lower level stuff than what many people would do day to day. Pushing matrix state is pretty common, it exists even in graphics languages from 1970's. 

There is unfortunately no good answer to this question. Simply it wont work. There is no good way to define colorful, it this context. Cie is trying to capture the physical measurement. It however does not succeed very well in relating the colors to each other. Colors on the very outer arc represent spectral distributions of close to Dirac delta function. So one could construct a model that says that a color is very colorful when it is a Dirac delta. There is a unforeseen consequence of this definition though. Namely the magenta colors do not exist as Dirac Deltas. As these colors do not exist in the spectrum. So they consist of mixture of 2 wavelengths only. This would mean they are less colorful than most other colors. Other problems Unfortunately, xyY is not perceptually uniform. So a straight line on the xyY does not represent interpolations between 2 color mixtures. Therefore making a polar transformation means you will have different color bases on same coordinates. Also precieved color do not really move over to your model. To do this properly you would need to do a extremely sophisticated transformation. There are many problems with converting color to a polar coordinates in that that is exactly contrary how vision works. White is also a bit problematic in this context. The distance to full saturated signal is different for each of the 3 different cones in the eye. Hell, even what is while depends on the surrounding colors and ambient color conditions. So aim afraid your trying to force a worldview that does not exist. Finally What would this be useful for?