You can't determine configured hostname by using IP or MAC. You can do it with CDP or LLDP, if protocol is enabled on interface of the switch. 

VPNv4 is the "transport". Your VRF route-targets match on both devices. Long story short, RT is extcommunity that is sent over VPNv4 session in BGP update. If one of local import RTs matches received extcommunity, routes are being imported in VRF with this RT configured. Actually VPNv4 is quite complex and complete explanation is beyond the scope of this site. VPNv4 is not required in order to run routing protocol inside VRF. Cisco calls it VRF-lite. Your problem with state here is related to routing. 

End to end VLAN or bridged connection through router L2TPv3 VXLAN OTV Older technologies such as Frame Relay more and more. 

Both IPv4 and IPv4 vrf TEST22 address families have Lo0 as the source. As long as IPv4 adjacency is I assume Lo0 is not in VRF. That makes your BGP source inaccessible. 

It may have different purposes. One of the most popular is to announce default route to customers with . Dropping all unknown destinations (e. g. RFC1918 subnets) in order to prevent traffic loops till TTL exceeds may also be a reason. 

VPWS and Xconnect is about the same technology. VPWS is the name of technology, xconnect - keyword for configuring pseudowire. You can find different naming like VLL, L2VC, PWE3, etc. Essentially they are referring to the same technology of p2p l2 connections. You can find more info in RFC 4664 For instance section 1.3 says: 

Increased MTU in LTE network allows for more efficient exchange between eNodeB and EPC, e.g. MTU value of 1600 in S1-U is sufficient for transmitting 1500 bytes of user data, and mobile network overhead (GTP + UDP + new IP header) without fragmentation. Increased MTU in 2g/3g network brings none or almost none profit. Underlying protocols sitting between base station and packet core, and mobile device and base station still cause small packets to be transmitted. Eventually almost nothing grows up to MTU size. This is caused by scheduling mechanisms of base station. 

I think your question is way too wide. There's protocol stack for 2G, and for 3G. They're different. Frequencies are different. Even controller/core is at least logically different. For documentation you can visit $URL$ Find protocol stacks and compare them "line by line" if it meets your purpose. 

One more notice: packets with DF (don't fragment) bit can be successfully fragmented on underlying level and reassembled at packet core with DF bit preserved. E.g. pinging some host with DF bit set and ICMP payload of 1472 is possible, but packets between base station and mobile packet core are still small. 

It's kind of strange to differentiate price for the service based on technology rather than SLA. Nevertheless, the best way to find out certainly is to ask your ISP, because there're plenty of techniques to do so, such as: 

All your assumptions are correct. Subnet mask determines subnet length. So in cases you described length of network changes, but host remains inside of the network. The difference is .145 will not need routing entry (except connected 192.168.1 one) in order to reach, say, .200 with /24 and /25 mask, but it will need route in order to reach the same .200 host with /26 mask. And similarly it'll reach .1 without routing entry with /24 mask, but not with /25 and /26. 

I feel that I've done everything I can here with the USB stick. Is there specific software that I need to use to prepare a USB stick for the boot loader? I've tried diskpart and also Rufus, but it just seems to be the boot loader that doesn't like anything I've tried. Please adivse. Thanks. 

I've been given two switches to work on, which separately have been used in a stack previously. One switch for one stack, and one switch for another st.ack in a different building. So they're not related. First switch 

Some help required please. I have a stack which contains two access switches, and an interface from switch 1 has a successful link to the core. However, when I try adding a secondary link on switch 2 back to the core, the interface is in a suspended state. Access Stack 

As you can see, I've configured this as an MP frame relay all in the subnet. Now I could quite easily link all of these together by creating static maps. That's not the issue. What I'd really like to know is what configuration needs to take place in order for inverse ARP to work between the routers? My guess would be to place routes on the frame relay switch from one DLCI to an outgoing interface, but how does the frame relay switch know where the DLCI is coming from? Does it need to still be mapped somewhere else, so there's communication between the routers and frame relay switch? Otherwise it just seems like a free for all. Any help would be appreciated, as I'd really like clarity on this. Thanks, 

I have two Cisco Catalyst 2960-X Series switches which I've stacked. I want the master to be switch 1 and the member to be switch 2. After doing research on stacking the switches, it's my understanding that the election takes place with the highest priority switch becoming the master during the election process. Once I set this, the stack was restarted and the member switch that I want keeps getting elected as the master. 

I've tried to find an explanation online that just simply tells me how inverse ARP is configured, and I can't seem to find a straight answer for it. Let's use the below diagram as an example. 

Is there something wrong with my config that's causing this? EDIT: On both interfaces from the core and the access stack, there is a stationary amber light which usually indicates STP has suspended the port to avoid a loop. Port Channel 1 (Access Stack) 

I've recently been studying the fundamentals on IPv6, but there are some things that I don't quite understand about how the addressing is structured. I know that link local addresses start with FE80, unique local addresses start with FC00 and global unicast addresses start with 2001 for example. Also from what I've seen, most addresses use the /64, sometimes accompanied by EUI-64. What I don't understand is the slash notation ranges with some of the address types. For example: Global Unicast 2000::/3 Link Local FE80::/10 Unique Local FC00::/7 With the above three address types for example, could someone please explain to me what the slash notations indicate? I can't seem to find a proper explanation for this anywhere. For the Global Unicast address for example, I've mostly seen 2001:0000 etc etc addresses. Does the notation mean the most I can go up to is 2003 as it's a /3?The same with the Link Local and Unique Local. The slash notations just don't make sense to me, and the books just don't seem to bring any logic to the table. I'm more than likely way off with my assumptions here, so some clarity would be good. I'd really like to understand this, rather than just assigning 2001:0000 etc etc Global Unicast addresses, without truly understanding why I'm doing it. Thanks in advance. 

I believe this is not possible as @jwbensley said. Though it's also unlikely that cisco.com explicitly says that this functionality isn't implemented. When you're viewing attributes such as community you're looking at BGP table. Route is getting into BGP table after policy has been applied. So if you change attributes with policy, you can only see modified attributes. 

I've done some tests with different MTU values. All tests were performed for mobile data, no voice affected or tested. Results are as follows: 

PPPoE stands for Point-to-point protocol over Ethernet. It is protocol designed to transmit PPP frames over Ethernet network with additional Ethernet header. This protocol is used for obtaining PPP software properties (e. g. call to some station and thereby establish stateful connection) over Ethernet infrastructure. 

You're not actually telling BGP to run inside VRF. I would say you're activating address family ipv4 vrf inside BGP process. I. e. BGP process is always bgp 2, but it may contain different address families. Correct. 

Tier X is formal reference. Traffic flows according to IP routing table. Basically you can access some resources without even leaving your provider's network. And some others require traffic to flow through several ISPs with different "tier". 

Traffic steering is actually the task that is beyond MPLS TE itself. TE tunnel just provides the path, and it's up to other protocols and techniques to steer actual traffic. The same is true for bandwidth reservation. TE just says "there are 300 mbps reserved", but nothing stops your client from sending more than that. So, your options are policies, static routes and IGPs. If you want granular control you go with static routes and policies. If you just want to get fast restoration you go with announcing tunnels into IGP. Steering technique depends on your task. 

ARP requests are normally limited to one broadcast domain (one subnet if you like). So when host A knows (based on subnet mask) that host B is located in the same broadcast domain, it sends ARP directly to host B. With ARP host A determines B's mac and sends frame with MAC B and IP B. Otherwise it looks into IP routing table in order to determine nexthop for B's network if any. You probably have only one IPv4 route on your host - default one. So now host A should send IP packet with IP B and MAC of default gateway - that is how frame reaches IP nexthop. That is why ARP is being sent to DGW. P. S. I believe your question is off topic here. 

Generally all the devices at the AP communicate through AP. There are technologies as Wi-Di and Wi-Fi Direct, but those are beyond the scope of this question. So devices can't communicate directly in common Wi-Fi network. Regarding traffic interception, you should take into consideration ciphering here. Though there are ways to intercept traffic and read it contents, with WPA2 those attacks are very complicated. 

Flow is a chain of packets combined by some attribute. For different protocols attributes are different (ports, seq numbers, etc). Load balancing in LAG may differ, but usually per-flow load balancing is being used. When per-flow balancing is used, flow is guaranteed to be transmitted via the same link. On the other hand it is possible to balance based on packet (per-packet balancing). It's not usually the case because of issues, such as out-of-order packets. You should consider using different hash methods based on your traffic profile. If you have big diversity of MAC addresses (e. g. L2 switched domain), use MAC for hash. If you have big diversity of IP addresses (e. g. small L2 domain + routing), use IP for hash. 

I know that 90 is the administrative distance, but the metric of 30720 seems high going over fast ethernet to only one hop. Could any light be shed on why this is so high in comparison to OSPF for example? I understand that this is probably a simple case of apples and oranges being two separate routing protocols, but I'm interested to know how this is calculated. 

As you can see from the above, the calculation is 30720/28160. The second part of this calculation also seems to be the feasible distance of 28160, with the first part being the metric of 30720 from the routing table. Could someone please explain to me the purpose of this calculation? And will this figure change over time? Or is this now set in stone? Thanks in advance. 

As these switches are not related to eachother from a previous stack, how I do remove the switch provisioning for each switch? If I were to attempt to remove the setting, it comes to back to say that it can't be done whilst the switch is present. The first switch appears to be a master, with the other switch being the slave. Is there a config file that stores the settings for this? I've cleared the startup-configuration but this still remains. Would I have to connect them both with stacking cables to create a stack first before I can deactivate the setting? These switches will be used individually in the future, so this is why I want the stack removed altogether. Please advise. Thanks. 

Marking as answered as this was discovered a long time ago through numerous lab setups and research. 

As you can see, the switch with the lowest priorty is still being elected as the master, and I don't quite understand why, unless I'm misunderstanding this election concept? If I turn on the switch I want as master first, and then turn on the member switch I can make it a master this way. However, if I ever need to restart the stack then always elects itself as the master. So to clarify what I'm after, I would like the below: 

I'm looking for an understanding of how the metrics are structured within EIGRP, this includes the routing table, and also the topology table. I'm sure I'm not the only one that finds the concepts of EIGRP a tad tricky to grasp. My small network has been created and I've enabled an AS of 1 on both connections for an EIGRP neighbouring relationship. This is the output from both the routing table and the topology table: 

Recently, I've been working on two spare switches which I need to stack at some point, but before I did that I needed the clear the config on both switches. Unfortunately, I made the mistake of typing erase flash: instead of nvram: and it's obviously caused me a problem. Not a massive issue though as I could just load the IOS image of the other switch as they're both 2960x switches. I've taken a USB drive and partitioned it to 2GB and formatted it as FAT16. When loading the USB onto the working switch, it accepts it without issues and allows me to copy the IOS image to usbflash0: and when doing a dir usbflash0:, it shows me the IOS image so I know it's all ready to go. When I console into the switch that does not have an IOS image, it takes me to the boot loader which is expected. However, it's at this point that usbflash0: is not recognised, and displays the following: