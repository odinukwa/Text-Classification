The images in the link you provide, of the severed sphere and its lower-dimensional representations, go some way towards explaining the difference. The severed sphere is a set of points in a three-dimensional space, but we want a two-dimensional representation of it. The objective of manifold-learning is (shockingly) to find a manifold: a subset of that three-dimensional space which (a) closely fits all the points that make up the severed sphere, and (b) can be described with a two-dimensional coordinate system. If you look at some of the other lower-dimensional representations of the severed sphere, it's like they're taking it and flattening it out into a rectangle so it'll fit in two dimensions. It's taking the severed sphere and figuring out a new coordinate system that maps as closely as possible onto all the points that make up the severed sphere. The MDS lower-dimensional representation, though, is more like a shadow that the severed sphere casts on a wall. Rather than finding a new coordinate system that closely fits the sphere, it's just "forgetting" whichever of the dimensions it thinks it can most afford to lose while maintaining the same distance to and from all the points. A good analogy would be maps of the earth. A good map of the earth makes a new coordinate system that fits a sphere onto a 2D surface. To do this it has to distort the relative distances between places, but you end up with effective 2D coordinates that relate well to places on the globe. Instead of doing this, you could just take two photos of the earth from above the north and south pole and glue them back to back. You'd still have a 2D representation of the earth, but it doesn't work so well as a coordinate system. This isn't to say that MDS is "bad". It's just doing something different. You probably wouldn't use MDS for dimensionality reduction prior to carrying out some sort of statistical procedure, but if you're trying to produce a graphic that gives some idea of how close multidimensional points are to one another, it might be a good choice. 

Again nice question. Yes it does but that is not necessarily that bad. Longer text means that the probability of having larger number for term frequencies are higher. This is a feature itself! (Just think again that you already found a difference between classes. Well ... you want NB to do the same right?!!) The only thing is that if the difference in lengths of comments is very huge, then it might affect the value of features in a way that you need normalization (e.g. instead of counting terms as features, you can use TF-IDF which is a bounded score) Hope it helps. Good Luck! 

Concatenate all sentences of the papers of one individual and call it data. the name of individual is the label. Of course if a paper has two authors, the text will be assigned to both of them. Extract features from your text data. Unsupervised feature extraction does not care about the labels (TF-IDF is an example) and supervised methods do care. Choose a classifier and start your learning phase as always. 

Segmentation of time series i.e. you want to segment an individual time series into different time intervals according to internal similarities. Time series clustering i.e. you have several time series and you want to find different clusters according to similarities between them. 

First of all your explanation about the methods are right. The point is that Embedding algorithms are not to only visualize but basically reducing the dimentionality to cope with two main problems in Statistical Data Analysis, namely Curse of Dimentionaliy and Low-Sample Size Problem so that they are not supposed to depict physically understood features and they are not only meaningful but also necessary for data analysis! Actually the visualization is almost the last usage of embedding methods. Projecting high-dimensional data into a lower-dimension space helps to preserve the actual pair-wise distances (mainly Euclidean one) which get distorted in the high dimensions or capturing the most information embedded in the variance of different features. 

The example given in your textbook proposes a multiple linear regression with 100 predictors, all of which have a "true" regression coefficient of 0. In other words, your independent variables have no statistical association with your dependent variable. When you calculate the $p$-value of an individual coefficient, you're looking at the magnitude of the coefficient, the standard error of the coefficient, making some distributional assumptions about it, and asking the following question: "what is the probability of seeing a coefficient value this extreme if the true value is actually zero?" If our distributional assumptions are correct, any given coefficient with a true value of 0 will report a <0.05 $p$-value approximately 5% of the time. That's not so much of a problem if we only have one predictor, but by the law of large numbers, if we have lots of predictors, we'd expect 5% of them to report a $p$-value this low. This makes the $p$-values in high-dimensional regressions hard to interpret. The $F$-test is different. Instead of evaluating every single coefficient for statistical significance, it applies a single test to the entire regression. So instead of having 100 chances to throw up an erroneous $p$-value, it only has one chance. This makes the $F$-test useful for evaluating whether or not there is a regression effect for high-dimensional regressions. 

Consider how cosine similarity is calculated. Cosine similarity takes the dot product of two real vectors, and divides this value by the product of their magnitudes. By the Euclidean dot product identity, this is equal to the cosine of the angle between the two vectors. The upshot of this is a value between 1 and -1. When the value is 1, those vectors are pointing in exactly the same direction. When the value is -1, the vectors are pointing in exactly the opposite direction (one is the negation of the other). When the value is 0, the vectors are perpendicular to one another; in other words, when the value is zero, these two vectors are as unalike in the feature space as it is possible to get. The dot product is the sum of all the element-wise products of your two vectors. The bigger those numbers, the more they contribute to the cosine similarity. Now, take any feature in your vector. The fifth, say. If you set this to zero in one of your vectors, the fifth element in the element-wise product of the two vectors will also be zero, regardless of its value in the other vector. When you sum up all these element-wise products, the fifth element will not have any impact on the summation. As a result, setting a value in your feature vector to zero means it doesn't make any contribution to the cosine similarity. This is why setting a value to zero in a feature vector is equivalent to not including the feature in the calculation of cosine similarity, and does not does not distort cosine similarity. 

Good , old and unsolved question! Distributed processing of large graphs as far as I know (speaking as a graph guy) has 2 different approaches, with the knowledge of Big Data frameworks or without it. SNAP library from Jure Leskovec group at Stanford which is originally in C++ but also has a Python API (please check if you need to use C++ API or Python does the job you want to do). Using snap you can do many things on massive networks without any special knowledge of Big Data technologies. So I would say the easiest one. Using Apache Graphx is wonderful only if you have experience in Scala because there is no Python thing for that. It comes with a large stack of built in algorithms including centrality measures. So the second easiest in case you know Scala. Long time ago when I looked at GraphLab it was commercial. Now I see it goes open source so maybe you know better than me but from my out-dated knowledge I remember that it does not support a wide range of algorithms and if you need an algorithm which is not there it might get complicated to implement. On the other hand it uses Python which is cool. After all please check it again as my knowledge is for 3 years ago. If you are familiar with Big Data frameworks and working with them, Giraph and Gradoop are 2 great options. Both do fantastic jobs but you need to know some Big Data architecture e.g. working with a hadoop platform. PS 1) I have used simple NetworkX and multiprocessing to distributedly process DBLP network with 400,000 nodes and it worked well, so you need to know HOW BIG your graph is. 2) After all, I think SNAP library is a handy thing. 

Aleksander has given a very comprehensive answer but there are a few that are sued very widely: For dimensionality reduction, PCA is used.This, however, does only a linear transformation and for non-linear dimensionality reduction, Manifold learning is what you are looking for. Projecting a lower dimensional data to higher dimensions can be done using kernels. You usually do this, when your classifier is unable to find a linear plane of separation in current dimension but will be able to find a linear hyperplane that separates the classes in a higher dimension. Kernels are used widely in SVM's. 

As mentioned above, the best way is to repeatedly sample the majority class N times(sampling without replacement) and for each time, the size of negative class should be equal to the size of positive class. Now, N different classifiers can be trained and the average can be used to evaluate it. Another way is to use the technique of bootstrapping. This might introduce overfitting, but worth trying and then if neeeded can regularize the model to avoid overfitting. 

In addition to the answers posted here, if the number of positive examples are way too small when compared to the negative examples, then it comes close to being an anomaly detection problem where the positive examples are the anomalies. You have a whole range of methods for detecting anomalies ranging from using multivariate gaussian distribution to model all the points and then picking those that are 2 or 3 stds away from the mean. Another food for thought - I have seen quite a few people who randomly sample the negative examples with more examples so that both the classes are same in number. It totally depends on the problem at hand, whether we want them to be balanced or not. 

If you are using python, I would suggest using mpld3 which combines D3js javascript visualizations with matplotlib of python. The installation and usage is really simple and it has some cool plugins and interactive stuffs. $URL$ 

Super comprehensive question. You are basically asking for tones of directions! I'd suggest to start with link prediction problem. So assume you have a directed/signed/weighted graph and you want to find the most probable next interaction (link/edge). I'd suggest to have a look at Jerome Kunegis papers and you may see it as a starting point and go on from there. 

Actually what they mentioned is right. The idea of oversampling is right and is one of, in general, Resampling methods to cope with such problem. Resampling can be done through oversampling the minorities or undersampling the majorities. You may have a look at SMOTE algorithm as a well-stablished method of resampling. But about your main question: No it's not only about the consistency of distributions between test and train set. It is a bit more. As you mentioned about metrics, Just imagine accuracy score. If I have a binary classification problem with 2 classes, one 90% of the population and the other class 10%, then with no need of Machine Learning I can say my prediction is always the majority class and I have 90% accuracy! So it just does not work regardless of the consistency between train-test distributions. In such cases you may pay more attention to Precision and Recall. Usually you would like to have a classifier which minimizes the mean (usually harmonic mean) of Precision and Recall i.e. the error rate is where FP and FN are fairly small and close to each other. Harmonic mean is used instead of arithmetic mean because it supports the condition that those errors are as equal as possible. For instance if Precision is $1$ and Recall is $0$ the arithmetic mean is $0.5$ which is not illustrating the reality inside the results. But harmonic mean is $0$ which says however one of the metrics is good the other one is super bad so in general the result is not good. But there are situations in practice in which you DO NOT want to keep the errors equal. Why? See the example bellow: An Additional Point This is not exactly about your question but may help understanding.