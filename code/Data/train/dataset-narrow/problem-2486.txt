Let $P=(p_1,\ldots,p_d)$ be a distribution on $[d]$. Given $n$ iid draws from $P$, we construct some empirical estimate $\hat P_n=(\hat p_{n,1},\ldots,\hat p_{n,d})$. Let us define the $r$-risk by $$ J_n^r = \sum_{i=1}^d |p_i-\hat p_{n,i}|^r. $$ It is known (see, e.g., Lemma 2.4 here) that when $\hat P_n$ is the maximum likelihood (i.e., empirical frequency) estimator and $r\ge2$, we have $\mathbb{E}[J_n^r]\le1/n$. In particular, the expected $r$-risk decays at a dimension-free rate. It is also known that for $r=1$, the risk decays at a minimax rate of $\Theta(\sqrt{d/n})$. Question: what is known for $1<r<2$? 

A perceptron $URL$ or SVM $URL$ learning algorithm would find a separating hyperplane -- which in your case, would be something like $w=(0,1,0,0,1)$ with bias $b=-1$. 

In machine learning, the sample error of an ERM learner is a biased estimator of the true generalization error. Here the bias is not by choice but by necessity: we simply don't know of a way to construct an unbiased estimator of the generalization error, and I'm pretty sure it's impossible from general no-free-lunch principles. 

Here is a simple Turing machine segment that reads a binary word $w$ starting at a position immediately to the right of the delimeter symbol # and replaces it with $w$'s successor in the lexicographic ordering. Here, L,R,S indicate moving the read/write head Left, Right, or Stay. This simple and powerful (standard) trick avoids a brute-force enumeration by using the results of previous computations to produce future ones. If you're allowed multiple tapes, it's now quite easy to accomplish the task you defined. Otherwise, there are standard $k$-tape to $1$-tape conversions. 

In the PAC learning model, suppose the learner actually knows the sampling distribution $P$. Surely this knowledge can be exploited to yield better generalization bounds -- but how? One idea is using PAC-Bayesian techniques to convert the distribution over $X$ into a distribution over hypotheses, which is what I think this paper is doing: $URL$ Any other methods for making clever use of $P$? [Edit 25.03.2015: In light of Vitaly's answer, let me sharpen the question. Suppose that I know the sampling distribution only approximately as $Q$. Can we formulate a generalization bound in terms of $D(Q||P)$ or $||P=Q||_{TV}$?] 

Given an $n$-state DFA (over a binary or any fixed alphabet), what is the complexity of computing its Kleene closure DFA? What is the largest possible/known blow-up in the number of states? 

Here is a DFA-related question I'd here before, and it's still open as far as I know: Fix an integer $n$ and alphabet $\Sigma=\{0,1\}$. Define $DFA(n)$ to be the collection of all finite-state automata on $n$ states with starting state 1. We are considering all DFAs (not just connected, minimal, or non-degenerate ones); thus, $|DFA(n)| = n^{2n}2^n$. Now consider two strings $x,y\in\Sigma^*$ and define $K_n(x,y)$ to be the number of elements of $DFA(n)$ that accept both $x$ and $y$. Question: What is the complexity of computing $K_n(x,y)$? In particular, can $K_n(x,y)$ be computed in time $poly(n,|x|,|y|)$? This question has implications for machine learning. 

This is particularly pertinent to the CS culture, where competitive conferences are often the main publication unit, which are then (sometimes) followed by journal versions. Suppose that a paper was published at a conference as version A, and say, 5 years later, in a journal as version B (with the same title and authors). Assume also that during those 5 years, additional relevant papers on the topic were published. When doing literature review in the introduction of future papers, the author of A and B is faced with a dilemma. To accurately reflect the historical development, he wants to cite version A and then the papers that followed it. However, the paper he actually wants people to read and cite is version B -- since it's cleaner, expanded, improved, etc. He can cite both versions, but then it looks like he's trying to rack up citations. Does the CS community have a set etiquette for dealing with this situation? Is there perhaps a bibtex format where a single bib-entry covers both versions? Something that conveys the point "published first in year Y at venue V [so precedence is established] and then expanded version appeared in year Y' at venue V' [so just go read that one]"? 

For $L$-Lipschitz functions on a metric space $(X,\rho)$ with $\epsilon$-packing number $M(\epsilon)$, the $\gamma$-shattering dimension is $M(2\gamma/L)$, as proved here: $URL$ 

This is almost a duplicate (see my comment above) but I'll give a quick answer before it (likely) gets closed. The first inequality in the OP has a superfluous $\log(n/d)$, which may be removed via chaining, as discussed here: Tight VC bound for agnostic learning Once that log-factor is removed from the VC bound, it becomes optimal up to constants (i.e., essentially unimprovable). See Section 5 in the Anthony-Bartlett book: $URL$ Regarding the optimality of the Rademacher bound, see the discussion and links here concerning Sudakov's inequality:Rademacher complexity beyond the agnostic setting 

According to Ishigami Y., Tani S. (1993) The VC-dimensions of finite automata with n states, $URL$ , the VC-dimension of the concept class of $n$-state DFAs over an alphabet of size $k$ is $$ d=d(n,k) := (k-1+o(1))n\log_2 n.$$ It follows that there are at least $2^d$ distinct $n$-state automata on a $k$-letter alphabet. The upper bound on the number such automata follows from a simple counting argument (given in the paper), and is at most $2^d$. 

There's some confusion in the question which I'll try to clear up. First, let's dispense with "better sample complexity than bounds such as Chernoff bound and Hoeffding bound". These concentration results are invoked, implicitly or explicitly, in the VC and Rademacher bounds as well and are essentially unimprovable (unless you want to take into account variance information -- then use Bernstein [look up "fast rates"]; but I suspect that's beyond the scope of the OP). Second, let's clarify the term "uniform bounds". Uniform typically means over a specified function class, but you also need to specify the distribution: is it fixed, or do you seek uniform bounds over distributions as well? (In the latter case, they're called "universal".) Restricting ourselves to the binary case, for uniform universal bounds, VC provides tight upper and lower bounds, and hence essentially tells the whole story -- nothing to improve here. (I'm sweeping some subtleties under the rug; see this question: Proper PAC learning VC dimension bounds .) Now let's talk about fixed-distribution rather than universal. Now the covering numbers tell the whole story, and they also provide upper and lower bounds on Rademacher complexity. See this, quite relevant, question: Rademacher complexity beyond the agnostic setting . tl;dr it all comes down to covering numbers 

Here is what I think is a legitimate encoding of binary trees as strings in $\Sigma^*$ (which is a regular language). Consider a breadth-first enumeration of the nodes in the infinite binary tree. Each node is labeled as a letter or as NIL. The nodes with letters belong to the tree, those labeled NIL do not. The caveat is that a subword of NIL's longer than the prefix preceding it terminates the tree -- meaning that this encoding is not a bijection. For example, the string abc00de encodes the 5-node tree 

Did you want some condition to make $\mathcal{F}$ a large collection? Because the current definition allows for the following, presumably trivial, case. Consider $\mathcal{F}=\{S_1,S_2\}$, where $S_1=[n/2]$ and $S_2=[n]\setminus S_1$. Then we can take $a=n/2$, $b=n$, $c=n/2$. 

Suppose that several agents need to place points (one per agent) on the interval $[0,1]$. An agent's goal is to maximize the volume of the Voronoi cell that contains his point. When $n$ agents must place their points sequentially, is an optimal strategy known? From what I was able to glean from the literature so far, it appears I am asking for the Stackelberg solution to an $n$-player Hotelling's game on a segment. 

Let me elaborate on @domotorp's answer, since a natural first objection might be, "We're talking about secure key exchange -- who said anything about requiring randomness?" The point is that public-key cryptography requires an asymmetry in the difficulty of computing a function: it's supposed to be hard to compute except if you have a special "key". It's easy to define functions that are hard (or even impossible) to compute for everyone -- there are more such functions than algorithms. It's also easy to define functions that are easy for everyone. It's the asymmetry that requires a lot of ingenuity -- and it fundamentally depends on the computational limitations of the adversary. 

The problem you're describing can perhaps be treated as "anomaly detection". Google the term in quotes, you'll get a ton of references. So the idea would be to first train an anomaly detector to separate the "apples, oranges, and other fruits" from non-fruits (or "nonsense" in OP terminology). Then train a classifier on the fruits. Update. In light of additional answers -- and too long for a comment. The problem with declaring a "non-fruit" category highlights the distinction between classification and anomaly detection. In classification, we assume 2 or more coherent classes. Apples, oranges, bananas -- these all have well-defined common characteristics. On the other hand, "non-fruit" is not a coherent category. It can only be meaningfully defined as something like "not fulfilling enough of the fruit characteristics". Anomalous objects are not a coherent class; they are by definition anything that is not sufficiently close to a "normal" object. We do not expect examples of anomalous objects to be informative for training. After all, you can just generate random pixel-images and get tons of anomalous examples for free! This is why anomaly detection requires techniques distinct from classification. 

Here is a start: Paninski, "Variational minimax estimation of discrete distributions under KL loss", $URL$ Orlitsky, Suresh, "Competitive Distribution Estimation: Why is Good-Turing Good" $URL$ Valiant, Valiant, "The Power of Linear Estimators" $URL$ Bhattacharya, Valiant, "Testing Closeness With Unequal Sized Samples" $URL$ I'll be happy for others to add if I missed anything big... 

It's well known that the minimax sample complexity for estimating the bias $p$ of a coin to additive error $\epsilon$ with confidence $\delta$ is $\Theta(\epsilon^{-2}\log(1/\delta))$. What if we know that $p$ lies in some specified range, say $[p_0-\eta,p_0+\eta]$ -- is the minimax sample complexity known in terms of $p_0$ and $\eta$? Note that $p_0=\eta=1/2$ recovers the general setting as a special case. 

[Too long for a comment.] The question will probably get downvoted and closed, but before that happens, let me try to explain why. This is theoretical computer science, where we deal with formally statable and provable claims. A claim such as, "the problem of telling cats apart from dogs is only solvable by machine learning" is not a formal statement. Meaning: as stated, it's not even true or false, just ill-defined. Worse yet, I don't think it admits any meaningful formalization. Now just so you don't walk away empty-handed, there is a whole host of problems where machine learning currently outperforms all other approaches. Certainly object recognition is one of those problems. 

Answering my own question, with a hat tip to Misha Belkin (private correspondence). It is easy to see that the unnormalized mincut and the RatioCut algorithm (see here for precise definitions $URL$ ) are Kleinberg-consistent. This is because the intra-cluster distances/similarity does not influence the objective function in any way. The Ncut-based clustering algorithm, however, is not Kleinberg-consistent. This can be seen by making the similarity weights within some cluster sufficiently large that it is able to absorb nearby clusters with negligible extra charge. 

For $L>0$, let $F_L$ be the class of all $L$-Lipschitz functions on $[0,1]$. Let $D$ be a joint distribution on $[0,1]\times\mathbb{R}$, from which we sample $n$ iid copies $(X_i,Y_i)$. Given any $f:[0,1]\to\mathbb{R}$, $ %its empirical risk is $ $$ % R_n(f) = \frac1n\sum_{i=1}^n (f(X_i)-Y_i)^2$$ its true risk (w.r.t. $D$) is $$ R(f) = \mathbb{E}_D[(f(X)-Y)^2].$$ Define the minimax agnostic excess risk by $$ \Delta_n = \inf_{\hat f}\sup_D \mathbb{E}_D[ R(\hat f)-\inf_{f\in F_L}R(f)],$$ where the sup is over all distributions $D$ and the inf is over all estimators taking an $n$-point sample $(X_i,Y_i)$ to a function $\hat f:[0,1]\to\mathbb{R}$. What is known about the behavior of $\Delta_n$? I was only able to find mimimax rates for additive -- rather than agnostic -- noise models.