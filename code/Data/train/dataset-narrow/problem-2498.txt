It is worth noting that the problem becomes NP-hard when the restriction is relaxed slightly. With a fixed number of clauses that are also of bounded size, the average number of literals in a clause is as close to 2 as one wants, by considering an instance with enough variables. As you point out, there is then a simple upper bound which is polynomial if the clause size is bounded. In contrast, if the average number of literals per clause is at least $2 + \epsilon$ for some fixed (but arbitrarily small) $\epsilon > 0$, then the problem is NP-hard. This can be shown by reducing 3SAT to this problem, by introducing new clauses with 2 literals that are trivially satisfiable. Suppose there are $m$ clauses in the 3SAT instance; to reduce the average clause size to $(2+\epsilon)$, it is enough to add $m(1 - \epsilon)/\epsilon$ new clauses with two literals. Since $\epsilon$ is fixed and positive, the new instance is of polynomial size. This reduction also shows that even the version where the "large" clauses are restricted to 3 literals is NP-hard. The remaining case is when the few large clauses are not of bounded size; each large clause seems to make the problem harder. See the SODA 2010 paper by Pǎtraşcu and Williams for the case of two clauses: they argue that if this can be done in sub-quadratic time then we would have better algorithms for SAT. There might be an extension of their argument to more clauses, which would provide evidence that your upper bound cannot be improved (modulo some form of the exponential time hypothesis). 

In short, the learning complexity of a family of quantified formulas over a finite domain of values is determined by its clone of polymorphisms. This includes CSPs as a special case of more general quantified formulas (since a CSP instance is just an existentially quantified conjunctive formula). For the Boolean case and also for two broad classes of non-Boolean formulas, there is a dichotomy between polynomially learnable (via the Generating Set algorithm, if the clone contains a "nice" operation) and not, subject to the hypothesis that there exists a public key cryptosystem secure against chosen ciphertext attacks. 

I expect the answer is yes, and would appreciate pointers to references. I am aware of Hunter's paper from 1976, but it only deals with pairwise dependencies. Hunter considers spanning trees in the graph formed by ignoring the edges in the dependence structure of size 3 or greater. 

In a monotone CNF formula every clause contains only positive literals or only negative literals. In an intersecting monotone CNF formula every positive clause has some variable in common with every negative clause. The decision problem 

As Walter says, clauses of 2-SAT have a special form. This can be exploited to find solutions quickly. There are actually several classes of SAT instances that can be decided in polynomial time, and 2-SAT is just one of these tractable classes. There are three broad kinds of reasons for tractability: 

Edit: I'm not sure this actually answers your question, even less so for your update. The pointer to and statement of Grohe's result are correct, but I don't think the struck out text is relevant for your question. (Thanks to Stephan Kreutzer for pointing this out.) It might be worth clarifying: do you want a counting problem that is difficult in general but easy on minor-excluded classes, or a decision problem? 

INDEPENDENT SET is NP-complete for (cross,triangle)-free graphs, but can be solved in linear time for (chair,triangle)-free graphs. (The X-free graphs are those that contain no graph from X as an induced subgraph.) chair: image of chair graph from ISGCI $URL$ triangle: image of triangle graph from ISGCI $URL$ cross: image of cross graph from ISGCI $URL$ Notice that the cross is obtained from the chair by adding a single edge. 

If not, is there an efficient deterministic protocol? I would also be interested in pointers to the literature if this problem is known. The closest I have found is the Clique vs. Independent Set game of Yannakakis but this did not seem useful here. 

pitches (1996-style) database theory to finite model theorists, and in the process highlights many interesting applications of logic in databases. For more recent work (such as the theory of XML, provenance, streaming models, or graph databases) reading highly-cited research papers is a reasonable approach. 

For me, the starkest illustration of the power of non-uniformity is that a suitably padded version of the Halting Problem is already in P/1. A single bit of advice is then enough to decide this language with a trivial TM that simply returns the advice bit. Of course, padding an undecidable language by an exponential amount means it is not "morally" in P/poly. But this does show that one needs to be careful when allowing non-uniformity. 

Based on arguments from statistical physics, Zecchina and collaborators conjectured that k-SAT should become hard when $\alpha = m/n$ is near a critical value. The precise critical value depends on k, but is in the region of 3.5 to 4.5 for 3-SAT. 

and its references. The field then went fairly quiet for a long time. Recently various extensions to Datalog have been gaining a lot of attention, and the Datalog 2.0 conference in 2010 was well-attended. I'm personally quite partial to the Datalog± family of extensions, since they allow several kinds of description logics to be captured in a well-behaved and well-understood fragment of classical logic: 

A particularly striking example of a phase transition is the maximum degree bound for Exactly-$k$-SAT (X$k$SAT), in which each clause contains exactly $k$ distinct literals. The problem flips from being trivially easy (always satisfiable) to being NP-complete by adding one to the associated parameter. Let $f(k)$ denote the largest number such that any X$k$SAT instance in which any variable occurs in at most $f(k)$ clauses is guaranteed to be satisfiable. If each variable only occurs in just one clause, then the instance is trivially satisfiable (just set each variable to the value that makes the corresponding literal true). On the other hand, the collection of all $2^k$ clauses on the same $k$ variables is unsatisfiable. So it follows that $1 \le f(k) < 2^k$. An X$k$SAT instance has a natural (non-logic) meaning as asking whether there exists an $n$-bit message which avoids some specified $k$-bit submessages. One can also rescale the parameter in a natural way to $f(k)/2^k$, which then takes a real value in the interval from 0 to 1. Instances in which variables can occur at most $f(k)$ times are all trivially satisfiable. However, the class of instances in which variables can occur at most $f(k)+1$ times is already NP-complete. 

Clarification: The intended focus here is the special feature of the grammar for SAT which allows it to be recognized by an NTIME[poly($n$)] machine, rather than the NSPACE[$n$] $\subseteq$ DTIME[$2^{O(n)}$] bound. The proof of Theorem 3 in Landweber's 1963 paper constructs a CSG from a linear-bounded automaton. (Kuroda provided the converse, constructing a linear-bounded automaton for any CSG.) However, Landweber's procedure does not seem to yield a grammar for SAT that is of special form: all NSPACE[$n$] recognizers are treated in the same generic way. In other words, it is not clear why the SAT CSG should have an NP membership problem, rather than being PSPACE-complete. I was hoping for a more explicit construction that uses the NP-ness of SAT in some essential way. Perhaps a better, more precise, question is whether: 

In the two-party setting, bounds of $\Theta(n)$ bits are known for deterministic and bounded-error randomized protocols for $\text{DISJ}_n$. (Here $\text{DISJ}_n$ is the $n$-element set disjointness function; the parties are each given $n$-bit Boolean vectors $x$ and $y$, and have to decide whether there is some index $i$ so that $x_i = y_i = 1$.) In contrast, for nondeterministic protocols in the two-party setting, the best lower bound I've been able to find in the literature is $\Omega(n^{1/2})$ bits: Alexander A. Sherstov, Communication Lower Bounds Using Directional Derivatives, JACM 2014, doi:10.1145/2629334 (preprint). A large gap remains between this and the trivial upper bound of $n+1$ bits. Sherstov's bound is for multiparty protocols. 

Finally, consider really simple Datalog programs, where there are only two unary predicates $T(x)$ and $F(x)$, such that $T(x) \equiv \lnot F(x)$. In other words, each variable must take either the value "true" or "false". Each such Datalog program then corresponds to a Horn formula, a universally quantified conjunction of Horn clauses. The minimization problem for these highly restricted Datalog programs is still NP-hard, and also likely to be hard to approximate. 

The paper uses a lot of background I don't have in quantum complexity. Given all the quantum people on this site, I'd really appreciate a pointer in the right direction. How would the arguments hold up if one were to discover that the class of complex-valued matrices seen in a specific experimental setup actually corresponded to a class of distributions that was easy to sample from? Or is there something inherent in the quantum system that guarantees this cannot happen? 

The authors mention some prior work on efficient algorithms for restricted cases, such as quasi-acyclic Horn formulas and for approximation in terms of the number of different variables in the formula. You might also be interested in the literature on finding shortest implicants.