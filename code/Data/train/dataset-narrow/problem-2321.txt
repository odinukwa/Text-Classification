For typed term enumeration for program synthesis, two recent works that I know are Example-Directed Synthesis: A Type-Theoretic Interpretation by Jonathan Frankle, Peter-Michael Osera, David Walker, and Steve Zdancewic, 2016, and Program Synthesis from Polymorphic Refinement Types by Nadia Polikarpova, Ivan Kuraj, and Armando Solar-Lezama, 2016. Note that both works use refinement types as a way to encode more desired properties of the searched program -- such as a testsuite that it should pass. These refinements often constrain the search space substantially more than just types, but they are also harder to invert -- interleave with search. 

You could be interested in Jacques Garrigue's A Certified Implementation of ML with Structural Polymorphism and Recursive Types, which establishes the soundness of static and dynamic semantics and properties of type inference for a ML language with (recursion and) structural polymorphism, thus formalizing one of the more advanced corners of OCaml (polymorphic variants and object types). That said, this work is more aimed at verifying soundness of more advanced parts of the type system than at covering the feature set of existing OCaml programs. I think that in terms of trying to prove correctness of an existing OCaml program, CFML would be a better choice. 

I have used the "randomly generate terms and check that they are well-typed" approach (you mention that "untyped" terms are generated, you can also randomly generate terms in a Church-style grammar with explicit type annotations) and it worked very well in practice, it revealed all the bugs there was to find on this particular part of the project. For practical purposes I would recommend trying this first. (On the other hand, the generator was aware of scoping rules -- it maintained a set of currently bound variables for top-down generation, this is easy to implement.) The choice of order depends a lot on the application you have in mind: two distinct applications will require different orders. For example, people that work on type-directed program synthesis are not usually interested in enumerating equivalent programs, so they will make simplification rules such as assuming that "any term of type A -> B can be chosen to start with a lambda-abstraction" -- in your framework, ordering lambda-terms before all terms at this type. This order is very bad for compiler testing: if you only generate programs that are in normal form, you are unlikely to exercise much of your optimization strategies or runtime semantics. So I believe that assuming an order to abstract away from the intended usage is not a good modeling choice: its choice is tightly coupled with the intended usage, and I would rather reason about the usage than the corresponding order. 

I thought that the syntax/coinduction-based presentations of recursive subtyping ("Coinductive axiomatization of recursive type equality and subtyping", Brandt and Henglein, 1997; "Subtyping, Declaratively", Danielsson, 2010) would avoid having to think about the tree-form of mu-types, but it looks like the syntax alone does not scale that nicely to defining the meet/join, so maybe going back to the tree presentation would better reveal what should happen. 

I thought a bit about this and I am afraid that it is harder than it looks -- as you suspected! In the spirit of encouraging discussion, I'll write my chain of thoughts. The subtyping rules are often defined as judgments of the form , where is an inductive rule (you can only use them finitely many times along each path in the derivation tree) and is a coinductive rule (you can use it infinitely many times). 

Given a set $S$, its multiplicative closure is the set $$ \mathcal{M}(S) = \{s_1s_2\cdots s_k: k\in\mathbb{N},s_i\in S\} $$ of products of zero or more elements of $S$. So the multiplicative closure of $\{2,3\}$ is the set of integers of the form $2^m3^n,$ for example. I'm looking for an efficient algorithm to compute the number of elements of $\mathcal{M}(S)$ up to a given bound $x,$ given a finite set $S$ of positive integers. You may assume (it is true in my case, and simplifies the problem) that each element of $\mathcal{M}(S)$ has a unique factorization in $S$. In my case $S$ is small (150-250 elements) and $x$ is large ($10^{35}$ to $10^{50}$ or more). A naive algorithm would construct all of the numbers and count them. This is slow, and more importantly consumes a lot of memory. A better approach (dynamic programming) is to split the set roughly in half, let's say into $S_1$ and $S_2$, and construct the multiplicative closures of both. Then each element of $\mathcal{M}(S)$ is a product of an element of $\mathcal{M}(S_1)$ and an element of $\mathcal{M}(S_2)$, and by the special property no other product is equal to this value. So you can iterate through one of these, let's say $\mathcal{M}(S_1)$, and for each $m$ in that set the number of products which are at most $x$ is the number of elements in $\mathcal{M}(S_2)$ which are at most $x/m.$ This can be determined with a binary search. Is there a way to push this method further? Is there a completely different approach which is more efficient? Is there literature I can read? 

Here's an example of a simplification we can make to the problem. If one of the $n_i$ is prime, then you can 

I'm considering the problem of recognizing a language (over alphabet 0-9 and space) containing strings like "1 2 3 4 5 6" and "14 15 16 17" but not "1 3". This came up while working on a common parsing task where elements needed to be in an ordered list. It struck me that while parsing the rest of that language was regular, this part was clearly irregular -- it can recognize, for example, the language A1A2 where A is an arbitrary string 0-9. In fact it seems to be content-sensitive (and not context-free by the pumping lemma). My first question: is there a (reasonably well-known, i.e. not defined just for this problem) class of languages between context-sensitive and context-free that describes its expressive power better? I've read about Aho's indexed languages, but it's not obvious (to me!) that these are even in that class, powerful though it is. My second question is informal. It seems that this language is easy to parse, and yet it is very high on the hierarchy. Is it common to come across similar examples and is there a standard way of dealing with them? Is there an alternate grouping of classes of languages that is incompatible with inclusion on the 'usual' ones? My reason for thinking this is easy: the language can be parsed deterministically, by reading until you get to the end of the first number, checking if the next number follows, and so forth. In particular it can be parsed in O(n) time with O(n) space; the space can be reduced to $O(\sqrt n)$ without too much trouble, I think. But it's hard enough to get that kind of performance with regular languages, let alone context-free. 

All publishable research problems should have these three properties: 1) open. 2) interesting. 3) challenging. For recreational research, you can drop the third condition (or vary it based on your own abilities and energies). There are mountains of problems in combinatorics and graph theory that are wide open, but are not "core" or "fundamental" enough to have lots of people working on them. Frequently, these problems can have algorithmic interpretations. Also, some can be turned into communication complexity problems ("How many bits are required to determine if property X is true?") but these are usually either trivial or very difficult. 

Brendan McKay's nauty (No AUTomorphisms, Yes?) program solves the canonical labeling problem of graphs (simultaneously solving the Graph Isomorphism and Graph Automorphism problems) and has exponential worst-case performance (Miyazaki, 1996). However, it works very quickly for most graphs, especially those with a few automorphisms. Specifically, the algorithm begins by partitioning the vertices by degree, then by the degree between each part. When this process stabilizes, a choice must be made to distinguish a vertex in a non-trivial part, and this leads to the exponential behavior. In most graphs, the depth of this branching procedure is small. 

It is similar enough to the odd-cycle game that a strategy could be built similar to Raz's lower bound. An important part of this strategy is to randomly choose colorings across the repetitions using shared randomness. By randomizing the permutations used in the randomly generated colorings, the number of answers given at each vertex span the entire answer set in a uniform way, attacking Rao's strategy. 

This process will eventually terminate. If there is no loop, it will take $n$ steps. If there is a loop, the two-step pointer cannot pass the one-step pointer without a collision, and this occurs before the one-step pointer finishes the loop (which is under $n$ steps). 

This answer is more of a toy problem than a real research problem. My typical example of a log-space algorithm to give to programmer friends is the following puzzle: 

Write everything, all the time. In TeX, preferably. Whether you are considering a question or proving a lemma, put it in a digital format as soon as possible. Write the necessary background. Try to keep the thoughts organized in a narrative. Having all of these things in a digital form makes paper-writing much easier, but still a lot of work. In my experience, it helps a lot to start again from scratch. This allows for a clean start to find an improved organization. Also, proofs are always easier the second or third time around. 

To get things rolling, I have a potential game and would like feedback. Let $k \geq 2$ be an integer and $m$ be an integer at least $3k+1$ with $m \not\equiv 0 {\pmod {k+1}}$. The cycle-power game is the 2P1R game where the provers attempt to convince the verifier that the graph $C_{m}^k$ is $k+1$ colorable. Here, $C_m^k$ is the graph with vertices given by integers modulo $m$ with edges if the mod-$m$ distance is at most $k$. If there is a $k+1$-coloring of $C_m^{k}$, it must be given by choosing an ordering of $\{1,\dots,k\}$ and coloring the numbers $\{0,\dots,m-1\}$ in this order, since each set of sequential $k+1$ integers in $\{0,\dots,m-1\}$ form a clique. Since $m$ is not a multiple of $k+1$, there will be some point where this coloring fails. The verifier either asks for a single vertex from both players, to verify that the colors match, or asks for an edge to verify that the colors are different. I believe this is a good example for two reasons: 

For a prefix code $C:\{0,1\}^*\to\{0,1\}^*$, define $f(n)$ as the length of the longest encoding of a number with up to $n$ bits: $$ f(n)=\max_{|k|\le n}\left|C(k)\right|. $$ (Note that by taking input as $\{0,1\}^*$ rather than $\mathbb{Z}^+$ I'm distinguishing between the 1-bit number 1 and the 2-bit number 01.) The counting bound gives $f(n) \ge \left\lceil\log\left(2^n+2^{n-1}+\cdots+2^0\right)\right\rceil = \left\lceil\log\left(2^{n+1}-1\right)\right\rceil = n+1$ for $n>0$ where, throughout this post, $\log$ denotes the binary logarithm. It's easy to construct $C$ such that $f(n)=O(n)$, $f(n)=n+O(\log n)$, $f(n)=n+\log n+O(\log\log n),$ etc. by recursion (starting from, say, Elias gamma coding). Is there a prefix-free code with $f(n)\le n+(1-\varepsilon)\log n$ for some $\varepsilon>0$ and large $n$? 

The algorithms used may be adaptive. Here's (transitive reduction of) an example of a graph of the sort I'd test. As a simpler version of #1 or #2, what's the complexity for this graph? sample graph $URL$ 

I have a finite set $S$, a function $f:S\to S$, and a total order $<$ on $S$. I want to find the number of distinct cycles in $S$. For a given element $s\in S$ I can use Floyd's algorithm (or Brent's, etc.) to find the length of the cycle that repeated applications of $f$ sends $s$ to; with a bit more effort I can identify this cycle (e.g. by its $<$-minimal element). A bad method for solving the problem would be to repeat this each element, sort the resulting minimal elements discarding duplicates, and return the count. But this potentially involves many passes over the same elements and large space requirements. What methods have better time and space performance? I'm not even sure what's the best way to measure the space needed—if $f$ is the identity function then any method that stores all cycles will use $\Omega(n)$ space. 

This can be generalized to composite $n_i$ if all terms are either divisible by $n_i$ or else coprime to $n_i$. This expands the size we can approach with the naive lcm algorithm, but it doesn't fundamentally alter it. 

Valiant's theorem says that computing the permanent of an $n\times n$ matrix is #P-hard. Is the problem of determining if a permanent is 0 any easier? This arises in the context of sequence A006063 in the OEIS, where membership is determined by the sign (0 or nonzero) of a certain permanent. (A solution to the restricted problem would be enough for me.) 

Remove all $n_j$ which are multiples of $n_i$, including $n_i$ itself. Compute the new density $D_\text{new}$. The density is $D = 1 - (1-D_\text{old})(1-1/n_i).$ 

I have a recursive algorithm in which the time for each step depends on the time for smaller steps. Essentially a structure is built at steps 1, 2, ..., n which must be searched at larger heights: $$ T(n)=T(n-1)+(1+o(1))\log(T(n-1)) $$ (yes, the larger the time searching the more data is added to the structure!) It seems 'obvious' that $T(n)\approx n\log n,$ but can this be proved? In particular: $$ T(n)\stackrel{?}{=}n\log n+o(n\log n) $$ Better bounds on the error term would be great. I believe the error is $\omega(n).$ 

Given a fixed regular language R, what is the complexity of generating all members of R with length at most $n$? Suppose some reasonable model (RAM with $n$-bit words?) and a write-only output tape. The list should be in length-lexicographical order. I'm interested in the answer in terms of the output as well as the input. (If R = Σ* then you can't improve on $O(|\Sigma|^n)$ but it only takes time linear in the output.)