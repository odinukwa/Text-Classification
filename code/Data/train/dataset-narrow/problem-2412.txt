(I don't know of citable references.) There is also ongoing research on the question I asked about finding a prime between $n$ and $2n$. EDIT: The paper by Gat and Goldwasser is now published: $URL$ This paper though doesn't resolve the question of finding a prime between $n$ and $2n$. 

Suppose we have an arrangement of $n$ lines in the plane. This partitions the plane into regions. We can label each region by a string in $\{+1, -1\}^n$, where the $i$'th coordinate is +1 if the region lies above the $i$'th line and -1 if it lies below the $i$'th line. (By rotating the arrangement if necessary, we can assume that no line is perfectly vertical.) Say that a subset $S$ of $\{+1,-1\}^n$ is realizable by a linear arrangement if there exists a linear arrangement such that the labels of the regions are exactly the strings in $S$. Is there previous study of which subsets of $\{+1, -1\}^n$ are realizable by linear arrangements? One can also ask the same question for hyperplane arrangements. 

Here's a simple example of a situation where we need the notion of a field extension. Consider the problem of polynomial identity testing in the blackbox setting. That is, given a blackbox which evaluates a polynomial $p$ over a field $\mathbb{F}$, we want to know whether $p$ is the zero polynomial or not. But note that if the degree of $p$ is not less than the characteristic of the field $\mathbb{F}$, it could be that $p$ evaluates to $0$ over the entire field without actually being the zero polynomial. For instance, $p(x) = x^2 + x$ over $\mathbb{F}_2$ is not the zero polynomial, yet $p(0) = p(1) = 0$. Thus, if we feed in elements of $\mathbb{F}$ to the blackbox, we'd never be able to distinguish $p$ from the zero polynomial. The way to get around this problem is to feed into the blackbox elements from a large enough field extension of $\mathbb{F}$. 

Ali, good question. Suppose you want to show that some problem P is computationally hard. Now, you could conjecture that P is hard just based on the fact that we don't have any efficient algorithms for it yet. But this is rather flimsy evidence, no? It could be that we have missed some nice way to look at P which would make it very easy to solve. So, in order to conjecture that P is hard, we would want to accumulate more evidence. Reductions provide a tool to do exactly that! If we can reduce some other natural problem Q to P, then we have shown P is at least as hard as Q. But Q could be a problem from some completely different area of mathematics, and people may have struggled for decades to solve Q also. Thus, we can view our failure to find an efficient algorithm for Q to be evidence that P is hard. If we have lots of such Q's from many different problem domains, then we have a huge body of evidence that P is hard. This is exactly what the theory of NP-completeness provides. If you prove your problem to be NP-complete, then you have tied its hardness to the hardness of hundreds of other problems, each of significant interest to various communities. Thus, morally speaking, you can be assured that your problem is indeed hard. 

Consider the following rather fundamental problem in the world of data structures. You have a universe of size $m$. You want to store an element $u \in [m]$ as a static data structure, so that when a user wants to know if for some $x \in [m]$ whether $x=u$, only $t$ bit probes into the data structure are needed, where $t$ is some fixed constant. The goal is to minimize the space complexity of the data structure (in terms of number of bits stored). One can construct such a data structure of size $O(m^{1/t})$. The idea is simple. Divide the $\log m$ bits needed to describe $u$ into $t$ blocks. For each $i \in [t]$ and for each possible bistring of length $(\log m)/t$, store in the data structure whether the $i$'th block of $u$ equals that bitstring. Now, for the lower bound. Let $X$ be an element uniformly chosen at random from $[m]$. Clearly, $H[X] = \log m$. If $X_1, \dots, X_t$ are the $t$ bits probed in the data structure (possibly adaptively) in that sequence, then: $H[X] = H[X_1] + H[X_2 | X_1] + \cdots + H[X_t | X_1, \dots, X_{t-1}] \leq t \log s$, where $s$ is the size of the data structure. This gives: $s \geq m^{1/t}$. Tight bounds are not known if we want to store two elements and $t > 1$. See here for the best results in this direction. 

As a followup to Per Vognsen's answer, Dana Moshkovitz's proof already suggests a really easy proof for only a slightly weaker version of the Schwartz-Zippel Lemma that, I think, suffices for most applications. Let $f : \mathbb{F}^n \to \mathbb{F}$ be a nonzero polynomial of degree $d$, where $\mathbb{F}$ is a finite field of order $q$, and let $x \in \mathbb{F}^n$ be a point such that $f(x) \neq 0$. There are $(q^n-1)/(q-1)$ many distinct lines passing through $x$ such that they partition $\mathbb{F}^n-\{x\}$. The restriction of $f$ to each of these lines is a degree $d$ univariate polynomial, which is nonzero, because it is nonzero at $x$, and so, has at most $d$ zeros. Thus, the total number of zeros of $f$ is at most $d(q^n-1)/(q-1)$. Schwartz-Zippel, for comparison, gives the stronger upper bound of $d q^{n-1}$. Given this proof's easiness, I'm sure it's folklore; if not, it should be :) I'd appreciate if someone could provide a reference. 

Kirk Pruhs is a leader in the area of "green computing", the study of algorithms that treat energy as an expensive resource. Take a look at these slides and this survey by Sandy Irani and Pruhs. 

Another great example is Terry Tao's alternate proof of the Szemer√©di graph regularity lemma. He uses an information-theoretic perspective to prove a strong version of the regularity lemma, which turns out to be extremely useful in his proof of the regularity lemma for hypergraphs. Tao's proof is, by far, the most concise proof for the hypergraph regularity lemma. Let me try to explain at a very high level this information-theoretic perspective. Suppose you have a bipartite graph $G$, with the two vertex sets $V_1$ and $V_2$ and the edge set E a subset of $V_1 \times V_2$. The edge density of $G$ is $\rho = |E|/|V_1||V_2|$. We say $G$ is $\epsilon$-regular if for all $U_1 \subseteq V_1$ and $U_2 \subseteq V_2$, the edge density of the subgraph induced by $U_1$ and $U_2$ is $\rho \pm \epsilon |U_1||U_2|/|V_1||V_2|$. Now, consider selecting a vertex $x_1$ from $V_1$ and a vertex $x_2$ from $V_2$, independently and uniformly at random. If $\epsilon$ is small and $U_1, U_2$ are large, we can interpret $\epsilon$-regularity of $G$ as saying that conditioning $x_1$ to be in $U_1$ and $x_2$ to be in $U_2$ does not affect much the probability that $(x_1,x_2)$ forms an edge in $G$. In other words, even after we are given the information that $x_1$ is in $U_1$ and $x_2$ is in $U_2$, we haven't acquired much information about whether $(x_1,x_2)$ is an edge or not. The Szemeredi regularity lemma (informally) guarantees that for any graph, one can find a partition of $V_1$ and a partition of $V_2$ into subsets of constant density such that for most such pairs of subsets $U_1 \subset V_1, U_2 \subset V_2$, the induced subgraph on $U_1 \times U_2$ is $\epsilon$-regular. Making the above interpretation, given any two high-entropy variables $x_1$ and $x_2$, and given any event $E(x_1,x_2)$, it is possible to find low-entropy variables $U_1(x_1)$ and $U_2(x_2)$ -- "low-entropy" because the subsets $U_1$ and $U_2$ are of constant density -- such that $E$ is approximately independent of $x_1 | U_1$ and $x_2 | U_2$, or that the mutual information between the variables is very small. Tao actually formulates a much stronger version of the regularity lemma using this setup. For example, he doesn't require that $x_1$ and $x_2$ be independent variables (though there hasn't yet been an application of this generalization, as far as I know). 

I suppose that, at the level of generality in which I've posed the question, the paper "Compression of samplable sources" by Trevisan, Vadhan and Zuckerman (2004) also qualifies as one possible answer. They show that in many cases, if the source of input strings is of low complexity (e.g., samplable by logspace machines), then one can compress, and decompress, in polynomial time to length an additive constant away from the entropy of the source. I don't really know though if compressed sensing can be put into some larger theory of compression. 

Is there an interesting example of a randomized algorithm for a search problem that always outputs the same (correct) answer, regardless of its internal randomness, but which exploits the randomness so that its expected running time is better than the running time of the fastest known deterministic algorithm for the problem? In particular, I was wondering if there is such an algorithm for finding a prime between n and 2n. There's no known polynomial time deterministic algorithm. There's a trivial randomized algorithm that works just by sampling random integers in the interval, which works thanks to the prime number theorem. But is there an algorithm of the above kind whose expected running time is intermediate between the two? EDIT: To refine my question slightly, I wanted such an algorithm for a problem where there are many possible correct outputs, and yet the randomized algorithm settles on one independent of its randomness. I realize that the question is probably not fully specified... 

The question is about the spectral norm of the symmetric function $h: \{0,1\}^n \to [-1,+1]$ defined as $h(x) = f(1-2|x|/d)$. (I have assumed range $[-1,+1]$ instead of $[0,1]$ which is wlog by appropriate rescaling.) Ada, Fawzi and H. Hatami show that for any boolean $g: \{0,1\}^n \to \{-1, +1\}$, $$\log \|\hat{g}\|_1 = \Theta\left(r(g) \log\left(\frac{n}{r(g)}\right)\right)$$ where $r(g) = \max(r_0(g), r_1(g))$ and $r_0(g)$ and $r_1(g)$ are minimum integers such that $g$ is either constant or perfectly correlated with parity or anti-parity for $x$ with $|x| \in [r_0(g), n-r_1(g)]$. Now, look at the proof of Lemma 3.1 in the paper. Note that this straightforward argument also holds for functions $h$ mapping to $[-1,+1]$ instead of $\{-1,+1\}$. So, the same upperbound also holds for your case. This upperbound is tight at least for the boolean functions, but I don't know if there's a similar general lowerbound for non-boolean $h$.