Is it possible to find a decision tree on $n$ variables in time $n^{\log s}$, where $s$ is the smallest decision tree consistent with examples coming from a distribution (PAC model). (Blum '92) Assuming $NP \subsetneq DTIME(2^{n^\epsilon})$ for some $\epsilon < 1$, we cannot PAC learn size $s$ decision trees by size $s^k$ decision trees for any $k \ge 0$. (Alekhnovich et al. '07) 

Without going into detail about your example, this seems to be a multiclass prediction problem. As the outputs are on a range of possibilities, instead of counting mistakes (0-1 loss), you can pick a desirable loss function for your application (especially if you're in the non-realizable case) and then a suitable learning algorithm. Because you have two outputs, you can make each prediction separately to make things simpler. 

The SQ model was made to analyze noise tolerant learning -- namely an algorithm that works by making statistical queries will work under classification noise. As Aaron said, most PAC algorithms that we have turn out to have equivalents in the SQ model. The one exception is Gaussian elimination, which is used in learning parities (one can even use a clever application of it to learn log(n)loglog(n) size parities in the classification noise model). We also know that parities cannot be learned with statistical queries, and it turns out most interesting classes like decision trees can simulate parity functions. So, in our quest to get PAC learning algorithms for many interesting classes (like decision trees, DNF, etc.), we know we need fundamentally new learning algorithms that don't work in the statistical query model. 

Sure, there are lots of examples, at least in the spirit of your question. Often one gets such a result from the probabilistic method. For example, one paper that I like that runs into the problem is on reconstructing graphs in the additive model. Here, the authors show that there exists a set of $O(dn)$ queries that will (optimally) learn the target graph. Given this set, the algorithm is efficient. However, they use the probabilistic method to show the existence of this small set (for each problem size) that will work on all inputs, but do not explicitly construct it. So the best they can do is just a brute-force search through an exponential family of queries because they don't have an explicit construction. 

The discussion in the comments below indicades that I have misunderstood the question. My answer is premised on the Oracle taking no input and returning $(x, f(x))$ where $x \sim p$ or $x \sim q$, depending on $f \in F$. This is apparently not what's being asked. 

Some of you are probably aware of this, but the 17 x 17 coloring problem has been solved by Bernd Steinbach and Christian Posthoff. See Gasarch's blog post here. 

Here's an open problem relating DFA and machine learning theory: are uniformly random (random transitions and accept/reject behavior) DFA learnable in the PAC model? Note: we think arbitrary DFA are not learnable b/c of cryptographic hardness results. For random DFA, we only have SQ lower bounds, which are not as strong. 

Yes, some lower bounds are known. For example, assuming $NP \neq coNP$, you cannot even properly learn read-thrice DNF formulas in polynomial time. There is a whole paper developing such hardness results using something called the "representation problem". To answer your linked-to question: Schapire, in his dissertation, in addition to proving that "weak learning" = "strong learning," also improved on Angluin's bound and gave an algorithm that uses $O(n)$ equivalence queries and $O(n^2+ n \log m)$ membership queries for learning DFA. One easy way to get lower bounds is information-theory. You can figure out how many distinct targets there are and how many bits a query gives you, etc. These upper bounds come close but aren't there. There are also issues one needs to think about regarding how the "counterexamples" arrive to the learner. A well-chosen counterexample can give away quite a lot of information. Update to the discussion above: Angluin and Dohrn address the question learning with random counterexamples in a recent paper. 

Unfortunately, the definitions & technical details are harder to summarize, but the linked-to blog post does a good job of explaining them. 

Feel free to define "settings" and "hurts" broadly, whether in terms of problem complexity, provable guarantees, approximation ratios, or running time (I expect running time is where the more obvious answers will lie). The more interesting the example, the better! 

This is a great question, one I've put a lot of thought into. In an internet ad auction, you want a pricing policy that encourages truthful revelation. You could run a normal second price auction on the bid prices, but then the selected ads might be terrible in terms of clickthrough and profit -- what you really want to do is to look at the expected revenue of an ad, something like bid times expected clicktrough (but you can't figure out the expected clickthrough without some experimenting, which might violate "strategy-proofness"). On the other hand, you could just run an optimal contextual bandit algorithm to display the ads most clicked on, but that might not be profitable, nor easily priced. Handling both aspects simultaneously makes for a nice theory problem. One good recent paper that tackles a lot of these issues is "Truthful Mechanisms with Implicit Payment Computation." 

I don't know much crypto, but perhaps the following would work. Assuming the $p_j$'s are publicly known, all that's needed to determine the winner is to select a random number from [0,1]. Here's the process: Each agent selects a random vector in $\{0,1\}^b$, where $b$ is the number of bits of precision that are needed for the process. Then they all cryptography commit to their vectors using known protocols. Finally, once all the vectors are committed to, all their vectors are revealed (and checked) and XORed and the result is the random number to be used. Namely the resulting vector is the binary representation of digits past the decimal point. Any agent can be sure the chosen random number came uniformly at random by choosing his own vector uniformly at random. For any observer to be convinced, they have to trust that at least one agent followed the protocol, but if none did, I guess nobody really wanted a fair lottery to begin with. 

Pretty much everything else is NP-Hard to (at least properly) agnostically PAC learn. Adam Kalai's tutorial on agnostic learning may also interest you. 

I've encountered an interesting case in my own research of small differences in alphabet size making dramatic differences in the resulting theory. A rough description of the problem of learning probabilistic circuits is the following: a learner can override gates of a hidden circuit and observe the resulting output, and the goal is to produce a "functionally equivalent" circuit. For boolean circuits, whenever a gate has "influence" on the output, one can isolate an influential path from that gate to the output of the circuit. For circuits of alphabet size $\ge 3$ this becomes no longer the case -- namely, there are circuits who have gates with large influence on the output value, but no influence along any one path to the output! We found this result quite surprising. The result is somewhat technical, but if you're interested, you can contrast Lemma 8 with Section 4.1 for relevant the theorem statements. 

Without looking at any details, why do your say your algorithm beats $O(N\log N)$? In your notation, $N\log N = n\log n\log(n\log n) = n\log n\log n + n\log n\log\log n = O(n\log^2n)$. No contradiction. 

The reason many of us went into research is because we find pushing the boundaries of what is known both intellectually rewarding and enjoyable. Doing research also gives us almost unparalleled freedom to work on problems we find meaningful and interesting, and it keeps us constantly challenged (which we enjoy). TCS (as opposed to other fields) is a mathematical study of computer science. You can work on the theory aspect of lots of different fields from distributed systems to machine learning. The choice among TCS and other fields in computer science depends on where your tastes and abilities lie. If your natural interests and abilities lie more in programming or system design than in mathematical analysis, then perhaps you shouldn't go into TCS. On the other hand, if your skills and interests lie more in the mathematical aspects, then you should consider TCS. Also, you don't always have to choose one area over all others. Many people work on problems from both the theoretical and practical sides. This is common, for example, in machine learning, where we first design and analyze algorithms (often theory) and then test them in the real world (experimental design, applications, etc.). A good way to figure out what you want to do is to take classes in many different areas, and perhaps try both industry and research in your summers. By the end of your studies, you will probably have a good idea of what you want to do. 

Let me attempt to partly answer the learning part, or to at least address some connections. In learning (distributional issues aside), we usually want our algorithms to be able to not only approximate the target function (from a class) with a hypothesis, but also to be able find such a hypothesis (efficiently). So, even though a certain class can universally approximate a given target, we are not done. I am not an expert in neural networks, but training a neural network is not easy. We have algorithms for training a neural network, like backpropogation, but they won't always converge, or converge quickly enough, to meet the PAC learning criteria. Boosting, on the other hand says (roughly) how to turn a weak "approximator" into a strong one, but it in some sense assumes the difficulty away but giving the learner access to the weak approximator.