In the case of HDR, this refers to the ratio of the highest/lowest luminance values that can be perceived or represented. 

What is a good book to be used as a general reference about the architectures of GPU drivers? Example of sections I would be looking for: - Theoretical descriptions of how drivers might or should work. - Enumeration of aspects to consider when writing GPU drivers. - What higher-level graphics programmer should definitely know about the drivers they use. ... 

Actually I just had to solve the following equation to find a solution, consistent with the presence of the "2" in the numerator: $\frac{1}{1+\Lambda (m)}=\frac{2}{1+\sqrt{1+\alpha ^{2}tan_{2}\theta_{m} }}$ $1+\Lambda (m)=\frac{1+\sqrt{1+\alpha ^{2}tan_{2}\theta_{m} }}{2}$ $\Lambda (m)=\frac{-1+\sqrt{1+\alpha ^{2}tan_{2}\theta_{m} }}{2}$ 

There are many references related to the physically-based rendering of several natural features of the numan body, such as skin, hair, eyes. However, I could not find specific information about the simulation of visually realistic nails. For example, I did not find specificities of the characteristics of the reaction of human nails to light. 

I've been trying to build a simple DirectX renderer from the ground up (was an OpenGL guy before but i figure it's good to know different APIs). I've been following the guide at rastertek.com to learn how to get the basics up and running. I got as far as this lesson, following the guide verbatim except for variable names, and ran into a really weird bug. The renderer class contains 3 XMMatrix instances, world, perspective and orthographic, but trying to initialize them would crash my application - it ran fine once i commented out these lines : 

This is the main 'hard' problem remaining in real-time CG, and there is a lot of research ongoing into solving it. The biggest hurdle is that in raster graphics, each component of the scene is rendered 'in a vacuum' - each triangle is rendered without reference to any other triangles in the scene, and the same goes for pixels, as opposed to ray-tracing approaches where each ray has access to the entire scene in memory. So real-time programmers need to use hacky tricks to do stuff like reflections and shadows, and the same applies to global illumination. A cheap runtime method is to use baked light-maps, where you run something slow-but-awesome like radiosity or path-tracing offline first, then save the lighting information along with your regular vertex data. This is great for static geometry, but becomes problematic as soon as you add moving objects. Michal Iwanicki did a good presentation on how they solved this for 'The Last of Us'. Spherical Harmonics are used a lot in game engines to represent indirect light. They're basically a Fourier transform across the surface of a sphere, by discarding high-frequency components you can get visually pleasing, mostly accurate environment lighting in only 9 coefficients per-color. Unity, for example, uses S.H. to bake 'light probes' at various points in the scene, moving objects can then interpolate between nearby probes to get an approximation of the indirect light at their position. Robin Green's paper is basically the bible on this technique, but it's pretty heavy going. The hot technique at the moment seems to be Voxel Cone Tracing, which doesn't involve any pre-bake step. I'm not too familiar with it myself, but as I understand it, it involves voxelizing your scene into a low-res Minecraft-style world, placing the voxels into a quickly-traversable spatial structure like an octree, then casting a few wide rays (cones) from each point and checking which voxels they hit to gather bounce lighting. NVidia is pushing this pretty hard at the moment, and there are papers on it here and here. Hope that helps :) 

Are you trying to make your own rendering engine? Here is a solution that prioritizes visual quality ; it is slightly more complex than the object-based solution you are referring to. For better visual quality your shading should be on a per-pixel basis. (If you highly prioritize performance, or do not require detailed reflections, you should probably stick to the object-based solution though). If you are doing deferred rendering, instead of selecting only a single cubemap for each pixel, you can use a formula that combines all the cubemaps that are close enough to a pixel. A trick sometimes used to achieve this is to add a pass that computes the number of cubemaps that affect a single pixel (so you can do averages in the final shading pass). If you are doing forward rendering for transparent objects for example, I think it might be a bit trickier to compute the average of cubemaps. That's because you need to have pixel positions already available before shading ; you might think it could be possible to do it if you had a kind of depth pre-pass buffer for that purpose, however for transparent objects you usually do not want to write their depth in that buffer, because this leads to artifacts when transparent objects are overlapping. You would therefore need to have a rather complex logic to handle those cases... However even with forward rendering you can still select the closest cubemap on a per-pixel basis, although without the "averaging" formula the transitions between the boundaries of two cubemap influence zones will be very visible and reflections will not look realistic. 

As a pet project, I'm trying to build a small app that visualizes 4D polytopes. I want to use the Wythoff Construction method, where the shape is generated kaleidoscopically by the interaction of 4 mirrors using a single movable generator vertex. I know how to create a reflection matrix from a hypersurface normal, what I am looking for is an simple way to generate all possible matrices generated by the interreflections of the set of mirrors. The brute force method would be something like: 

I know that texture caching on GPU works to optimize locality in 2D (or 3D if using 3D textures). I believe this is done using space-filling curves like Morton or Hilbert curves. When passing an array of texture data to the GPU, though, for example using glTextureSubImage2D, the data is specified in scan-line order. So how and when does the conversion take place? I can think of a couple of possibilites: 

What kind of technology would be involved to simulate accurately the rendering of human nails? What kind of tricks digital artists use to simulate the appearance of human nails? 

It starts really from the basics, and provides simple implementations for the concepts that are gradually introduced. 

You can see that evaluating the performance of Multisampling Anti-Aliasing is more complicated than that of Supersampling, since it also depends on the topology of the scene (the size and shape of the triangles). 

Often a similar hardware feature is exposed via DirectX and OpenGL using different terminology. For example: Constant Buffer / Uniform Buffer Object RWBuffer / SSBO I am looking for an exhaustive chart that describes which DirectX terminology is used to refer to which OpenGL concept, and vice-versa. Where can I find such a resource? 

This structure allows you to extract all sorts of connectivity information from a mesh, such as which edges or polygons lie around a particular vertex, simply by traversing the half-edges. There's a good article explaining it here. 

The information you're looking for is defined in the 'accessors' and 'bufferViews' near the top of the source file you linked. Bufferviews simply divide the buffer up into sub ranges and define broadly what kind of data lives there using some obscure shortcodes. In this case, target 34963 means index data and 34962 means vertex data. So from the other parameters in the bufferviews you can see that the first 72 bytes are indices and the remaining 576 bytes are vertex data. The accessors are what actually define the format of the data and again use weird codes for "componentType" - 5123 is an unsigned short (2 bytes) and 5126 is single precision float (4 bytes). They also define whether they should be read singly ("SCALAR") or in vector groups (e.g. "VEC3"), and also a starting offset into the bufferview and "stride" between the start of data points like regular OpenGL vertex buffer bindings. Putting it all together, the first 72 bytes are 36 shorts representing the indices (3 * 12 triangles). The next 576 bytes are two consecutive sets of 24 vec3 (4 per square face, 12 bytes per vec3) representing vertex data. Since the first set are clamped to + or - 1/2 and the second set to + or - 1, I'm guessing first set are positions, second set are normals. The base 64 encoding just takes the raw bit stream, splits it into groups of 6 rather than 8, and maps each unique 6-bit combination onto a character. You can see this is quite wasteful as each character takes at least 8 bits to store, quite possibly 16 or more depending on the encoding used. 

Assuming your code has several locations that can allocate/release GPU memory, but you don't know which one leaks. Maybe you can try to add a GPU memory monitor to that code. When GPU memory is allocated in code, insert the buffer handle that was returned, the buffer size, the function/file name (or even better, stack trace) in a globally defined array. When GPU memory is released, remove the handle of the buffer that was released from that array. Run the application a while and exit before it crashes. Output the contents of the array before leaving. It shows information about the GPU memory that was not released. You might see some patterns leading you to understand how to fix the "leak". 

Ah, shadows. They still are bothering after so many people spent years trying to improve them. Whether your engine is deferred or forward, the shadow pass should be quite the same, and suffer the same issues. You are correct, this is a famous problem. Some methods can improve these artifacts in some cases, see the methods mentioned already, or for example "light-perspective-cascades", "variance shadow maps", "shadow atlas" (compute optimal resolution for shadows of each light given available memory, and arrange them accordingly in a big buffer), "moment shadow mapping". Some other workarounds are "baking" the shadow maps if light and object do not move. I am missing a lot, but don't recall seeing a solution that works well when resolution / number of filtering passes / samples is restricted...