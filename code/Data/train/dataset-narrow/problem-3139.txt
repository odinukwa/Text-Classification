I'm currently working on a project involving the prediction of tenancy lengths. I've so far managed to get to a point where I've processed the data and pruned my Random Forest model (via sklearn in Python) to the following accuracy levels (in days): 

Does anyone have any idea what's going on? Why would something like this be happening? I can't for the life of me figure out if I'm doing something wrong, yet I doubt it's just coincidence for some reason. 

The data table below contains cash flows going into an account. At index 0 the account starts with 50 in debt, is charged a monthly Rent of 90, and pays a certain amount every month, as well as getting additional supporting payments every 4 periods. 

While I cannot create a residual plot for this (as I have no previously known information regarding targets), I've noticed that 'Tenancy Length' strongly correlates with 'Tenancy Start', which was not the case in my trained model. 

I'm having issues with fitting a Random Forest model to a completely new dataset. I'm trying to predict tenancy lengths for current tenants. I have a dataset with tenancy information since 2008, with both tenants that have finished their contract and others which are still in their properties. Steps undertaken so far: 1) Remove the tenancies that are still on going. This is to actually get target values for "Tenancy Length". 2) Find a suitable model to train and test on the finished tenancies. I'm happy with a Random Forest, with MAE of ~35 on train and ~70 on test. 3) Create a new dataframe in python that only has the previously excluded information (tenancies that have not yet finished) in order to apply the .predict method on these. I've also combined the previous 'train' and 'test' into a single train, as there is no further need to split that data up. df = finished tenancies; currentDf = ongoing tenancies 

I've tried to find a way to create a regression equation based on historical and forecasted values (so that the true balance takes into accounts expected payments), but so far I've not managed to do so. It feels like I'm hitting a brick wall and I would appreciate information if the way I'm attempting to do so is correct or not. Note: The information provided here contains very simplified data. In reality, "Payment" can be highly irregular, while "Support" can either be a seasonal transaction that can be delayed by a number of periods, or can periodically stop and start again. I have so far been prototyping in Excel, as I'm unsure what to use to find a solution in Python. 

Pickled models probably will not work on a different computer. It is also a very insecure format (see here). Rather go to kaggle.com or github.com and look for kernels/scripts that allow you to train a model on your own. Some frameworks, such as XGBoost, have a build in saving functionality with is more reliable than pickle. 

First, you should define what you mean with similarity and a corresponding metric to measure it. The second step is to apply the metric to (A, D), (B, D), etc. Then, the set that gives the smallest value is the one with the highest similarity. If you find that your metric does not what you want, simply change it until it meets your requirements. You need to be clear about what you mean with "similarity" and how to measure it though. One possibility is to interpret the sets of parameters as a point in a N-dimensional space and calculate the average euclidean distance. 

It definitely can be associated with over-fitting. I would suggest plotting the training and validation accuracy in a single graph. So you see how both depend on the number of splits. If it is over-fitting one would expect that the training accuracy continues rising, while the validation accuracy gets to a maximum and then drops. However, you have to make sure that your training and validation sets are OK. I assume you do something like k-fold cross validation. Where every sample is k-times in a training set and one time in a validation set. And you have enough samples to train your model etc. 

For some machine learning methods it is recommended to use feature normalization to use features that are on the same scale, especially for distance based methods like k-means or when using regularization. However, in my experience, boosting tree regression works less well when I use normalized features, for some strange reason. How is your experience using feature normalization with boosted trees does it in general improve our models? 

You are trying to predict random numbers here. By design the lottery numbers are unrelated and one round has no impact on the next round. For machine learning you should use input that has some kind of "predictive power". I mean data that as some information or correlation with the target variable. You can of course train a prediction algorithm on that data like that, but it does not make sense to me. In case you find something that is a solid prediction, that might mean the lottery does not draw random numbers. ...this actually should be a comment, but StackExchange does not let me write a comment. 

Exploration: in the beginning the algorithm explores aggresively, and this reduces linearly. After say a thousand games it only explores in 10% of the moves. All other moves are based on exploitation of previous rewards. Rewards: if the game resulted in a win, then award 10 points. If the game resulted in a draw, 0 points, otherwise -5 points. Actually, these rewards can be "tuned", so that if the game was shorter and it was won, then award more points or if it was longer award less points. This way the algorithm prefers winning quickly. That means that it learns to win as soon as possible, rather than aiming to win later on. That is important so that it doesn't miss winning immediately - if it missed such a move the opponent would likely a) move there to avoid letting the AI win next time, and b) think the algorithm was stupid because it missed an "obvious" win. This algorithm does indeed learn, so I can class it as a maching learning algorithm. I think, but I am not sure, that it is a reinforced learning algorithm. However, according to $URL$ it is not temporal difference learning, because it doesn't estimate the rewards until the end, and it should be estimating the reward as it goes along. That might mean that it is not reinforced learning. Question 1: Can I successfully argue that I am estimating the reward based on history, and still claim the algorithm is reinforced learning or even Q-learning? Question 2: If I replace the reward lookup which is based on the board layout, with a neural network, where the board layout is the input and the reward is the output, could the algorithm be regarded as deep reinforcement learning? Question 3: I'd don't think that I have either a learning rate or a discount factor. Is that important? I noticed that the algorithm is pretty useless unless I train it with at least every move which an opponent tries. So in a way it feels like it's using brute force rather than really "learning". This makes me question whether or not machine learning tictactoe is really learning. I agree that using a neural network to learn image recognition can be classed as learning because when it sees an unknown image it is able to state its classification. But that is quite useless for games like tictactoe where similar looking board layouts are totally unrelated (one may lead to a win, the other may lead to a loss). So... Question 4: Can tictactoe algorithms be classed as real learning rather than simply brute force? Update: regarding rewards... when the algorithm is deciding where to go, it works out the reward for each position as follows: