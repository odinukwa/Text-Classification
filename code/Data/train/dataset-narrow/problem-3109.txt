Your padded vector should be OK as a starting model. You could take it a step further and have a specific "no character" encoding, as that will be a clearer learning signal. You say that you are using a feed-forward neural network. In that case, there is a possible change and improvement to your model, which is to use Recurrent Neural Network (RNN), probably a long short-term memory (LSTM) variation, because that has had some success in language models. In a RNN, instead of having the character encoding repeated N times, you have it represented just once, and run the network N times for each sample, feeding in each character in turn. Instead of a "no character" symbol, you would have an "end of sequence" symbol. For an open-ended model with no specific target, it is common to have the output of the network be a prediction of the next character. This allows you to explore what the model has learnt by sampling from its predictions - a lot like using n-grams to generate realistic-looking words. The advantage of a RNN for character-level language modelling is that the model is built around the assumption that each input is somehow equivalent - the same encoding is used at each time step. This is clearly true for character strings, so it can lead to better models from less training data. The main disadvantage of using a RNN is that it is more complex to understand, build and train such a model. 

That's just one of those things that happens when you implement an algorithm from scratch in order to learn it . . . I bet you've spent hours looking at the top part of your script :-) 

This is a problem, since it breaks the assumptions of the Markov Decision Process that all the algorithms are based upon. The reward at any step should only be based on current state/action. By doing this, you may invalidate the policy. It definitely would defeat dynamic programming (may prevent it converging) unless you included action history in the state (which would definitely make your state space large). However, it may still work for sampling methods with a high value of $\lambda$, because these are more robust when faced with environments which are not strictly MDPs. 

The number of examples in a batch or mini-batch, counting even a single example as a mini-batch of size 1) The number of feature maps or channels in the current layer, counting even a single grayscale image as an array of 1 channel, or RGB (or other colour space) image as 3 channels 

* In fact this will be the case in many CNN classifiers, but is more likely to be reliably linked to a person's identity if you have trained the network explicitly for distinguishing between identities in general (as opposed to just classify specific identities). The triplet loss approach is one way to train for useful representations, and makes the goal of disambiguating identity explicit. 

When you have a 3-dimensional array, what you decide to call "rows" and "columns" is also a convention. It depends partly on what that array represents, and there is no single way to visualise the contents. 

You decide what the output layer represents, so it should be OK (and probably easier to implement) to have a single fully connected layer of size $m_x + m_y + m_z$ and to interpret the output in your own code. E.g. $n_x = N_{(x)}, n_y = N_{(m_x + y)}, n_z = N_{(m_x + m_y + z)}$, where $N_{(i)}$ is the output of the $i^{th}$ neuron. In the case of regression, this should have very similar behaviour to 3 separate fully connected layers for each dimension. However, once you have either structure, it is not clear that you will be estimating reward directly any more. There is definitely no guarantee that the rewards will be the same between $n_x, n_y, n_z$, in fact they will not be in most cases. You will need to define a function $\hat{r}(x, y, z)$ that combined the outputs of the network and have your loss and error gradient calculated from that. The function could just be $\hat{r}(x, y, z) = n_x + n_y + n_z$ or maybe $\hat{r}(x, y, z) = n_x \times n_y \times n_z$ Clearly this approximation has limits built in - it will likely over-estimate and under-estimate rewards due to its structure. If the simplified structure matches your problem, then this will be fine - it may even speed convergence. But if not, it will limit performance. An alternative, if you don't want to build a network that has a large number of outputs, is to have a single reward estimate as the output, and have the action choice as an input. That has the advantage of leaving the details of the approximation to the neural network. It has the disadvantage that you would need to run the network up to $m_x * m_y * m_z$ times in order to decide the policy in each state. Because of the large number of possible actions in your case, you may want to look at a policy-based learning method, rather than DQN. There are deep NN-based versions of those algorithms. A recently-published algorithm called Asynchronous Advantage Actor-Critic (A3C) has performed well in computer gaming tasks and might be appropriate for your problem. 

Simplified network With those definitions, let's take a look at your example networks. You are running regression with cost function $C = \frac{1}{2}(y-\hat{y})^2$. You have defined $R$ as the output of the artificial neuron, but you have not defined an input value. I'll add that for completeness - call it $z$, add some indexing by layer, and I prefer lower-case for the vectors and upper case for matrices, so $r^{(1)}$ output of the first layer, $z^{(1)}$ for its input and $W^{(0)}$ for the weight connecting the neuron to its input $x$ (in a larger network, that might connect to a deeper $r$ value instead). I have also adjusted the index number for the weight matrix - why that is will become clearer for the larger network. NB I am ignoring having more than neuron in each layer for now. Looking at your simple 1 layer, 1 neuron network, the feed-forward equations are: $z^{(1)} = W^{(0)}x$ $\hat{y} = r^{(1)} = ReLU(z^{(1)})$ The derivative of the cost function w.r.t. an example estimate is: $\frac{\partial C}{\partial \hat{y}} = \frac{\partial C}{\partial r^{(1)}} = \frac{\partial}{\partial r^{(1)}}\frac{1}{2}(y-r^{(1)})^2 = \frac{1}{2}\frac{\partial}{\partial r^{(1)}}(y^2 - 2yr^{(1)} + (r^{(1)})^2) = r^{(1)} - y$ Using the chain rule for back propagation to the pre-transform ($z$) value: $\frac{\partial C}{\partial z^{(1)}} = \frac{\partial C}{\partial r^{(1)}} \frac{\partial r^{(1)}}{\partial z^{(1)}} = (r^{(1)} - y)Step(z^{(1)}) = (ReLU(z^{(1)}) - y)Step(z^{(1)})$ This $\frac{\partial C}{\partial z^{(1)}}$ is an interim stage and critical part of backprop linking steps together. Derivations often skip this part because clever combinations of cost function and output layer mean that it is simplified. Here it is not. To get the gradient with respect to the weight $W^{(0)}$, then it is another iteration of the chain rule: $\frac{\partial C}{\partial W^{(0)}} = \frac{\partial C}{\partial z^{(1)}} \frac{\partial z^{(1)}}{\partial W^{(0)}} = (ReLU(z^{(1)}) - y)Step(z^{(1)})x = (ReLU(W^{(0)}x) - y)Step(W^{(0)}x)x$ . . . because $z^{(1)} = W^{(0)}x$ therefore $\frac{\partial z^{(1)}}{\partial W^{(0)}} = x$ That is the full solution for your simplest network. However, in a layered network, you also need to carry the same logic down to the next layer. Also, you typically have more than one neuron in a layer. 

GRU and LSTM are two popular RNN variants out of many possible similar architectures motivated by similar theoretical ideas of having a "pass through" channel where gradients do not degrade as much, and a system of sigmoid-based control gates to manage signals passing between time steps. Even with LSTM, there are variations which may or may not get used, such as adding "peephole" connections between previous cell state and the gates. LSTM and GRU are the two architectures explored so far that do well across a wide range of problems, as verified by experiment. I suspect, but cannot show conclusively, that there is no strong theory that explains this rough equivalence. Instead we are left with more intuition-based theories or conjectures: 

In a scenario where consequences of prediction errors are not equivalent, you are usually still interested in training a model to predict accurately from the data set, and would not change the objective function in supervised learning. Typically when consequences of FP and FN differ, you would: 

When back propagation goes across a max pooling layer, the gradient is processed per example and assigned only to the input from the previous layer that was the maximum. Other inputs get zero gradient. When this is batched it is no different, it is just processed per example, maybe in parallel. Across a whole batch this can mean that more than one, maybe all, of the input activations to the max pool get some share of the gradient - each from a different subset of examples in the batch. 

That is a description of the training process that compresses 1M different inputs to 256-dimensional output for use as an embedding for recommendation matches. The softmax is at the output, and as far as I can see is just a normal softmax classifier output as seen in many other classifier networks (except the result is not technically being used to classify anything). I am not clear on what supervision data was used or on what the input representation was. However, I don't think it likely that 1M "classes" ever appear as e.g. 1-hot encoding, because that would not scale out usefully to the many other millions of videos - the point of the embedding is to turn disparate features of the videos into something that be used as a similarity measure, that can be run on any video stored in YouTube. 

I highly recommend this kind of dry run before putting your system to real use, otherwise you will have no confidence that your ML/approximator is working. 

The image, and variants of it that are commonly used are for illustrative purposes only. They generally do not represent data that has been extracted from real CNNs. The first "Low-level features" part of the diagram is possibly from a real network (I am not sure in this case, it looks more like a constructed filter, e.g. Sobel, to me). That is because it is feasible and relatively easy to interpret the first layer's filter weights directly as images, and the filters do indeed look like the components that they detect. The "Mid-level features" and "High-level features" in your specific diagram have probably been constructed without using a neural network. They are likely to be an artists impression of what the high level features might be. They may have been sampled from real datasets, then just cropped and arranged into the image. Caveat: I cannot find absolute evidence for the specific image being constructed for illustration only, just I suspect this to be the case. It is possible to extract visualisations of features detected by deeper layers. The two common ways to do this are: 

Your network design/logic is basically correct, but you are seeing some very common problems with neural network numerical stability. This results in your weights diverging and not training accurately. Here are the fixes, any one of them might help a little, but the first two should be used for nearly all neural network projects. 1. Inputs need to be scaled to work with neural networks. This is called input normalisation. Usually you would do this in data preprocessing, but for your simple network we can include the scaling at the input: 

You are looking at the Keras code implementing dropout for training step. In the Keras implementation, the output values are corrected during training (by dividing, in addition to randomly dropping out the values) instead of during testing (by multiplying). This is called "inverted dropout". Inverted dropout is functionally equivalent to original dropout (as per your link to Srivastava's paper), with a nice feature that the network does not use dropout layers at all during test and prediction. This is explained a little in this Keras issue. 

The reward function in the Chapter 2 test bed is simply the "true" mean value for the chosen action, plus a "noise term" which is normal distribution with mean 0, standard deviation 1. The noise has the same distribution as the initial setting of "true" values. The difference is you set the true values at the start and do not change them, then add noise on evaluation of each reward. The goal for the learner is then to find the best "true" value whilst only seeing the reward. This matches your understanding as I read it from the question. You could write it like this: Initialisation: 

Cross entropy formula given two distributions over discrete variable $x$, where $q(x)$ is the estimate for true distribution $p(x)$ is given by $$H(p,q) = -\sum_{\forall x} p(x) log(q(x))$$ For a neural network, the calculation is independent of these parts: 

There are exceptions to these rules in some architectures and learning algorithms, where perhaps an integer-based input would work, but for the most common NN types, including MLP and "deep" feed-forward networks, it is simpler and easier to use float data type. 

No, area under receiver operating characteristic (AUROC) is just one metric amongst very many possibilities, even assuming you just want to pick a standard approach. There are too many to list in a simple Stack Exchange answer. You can take a look at this list extracted from scikit learn documentantion on metrics for example, which is not by any means exhaustive: 

Each unique set of original data gets a new one-hot-encoded category assigned. It is clear that ultimately if you had $n$ original features, you would need $2^n$ such derived categories - which is an exponential relationship to $n$. Working like this, there is no generalisation possible, because any pattern you had not turned into a derived feature and learned the correct value for would not have any effect on the perceptron, it would just be encoded as all zeroes. However, it would learn to fit the training data very well, it could just associate each unique vector with a weight equal to the training output - this is effectively a table lookup. The whole point of this description is to show that hand-crafted features to "fix" perceptrons are not a good strategy. Even though they can be made to work for training data, ultimately you would be fooling yourself. 

It doesn't work quite so directly, and there as a small factor of more calculations involved (you do not calculate each derivative with a single multiplication, often there are a few, some results are re-used, and other operations may be involved). However yes you do need to calculate a derivative for each weight and bias term, and there are roughly that number of weights in your network that require the calculations done. Your suggested numbers are actually quite small compared to typical neural networks used for image problems. These typically perform millions of computations for a forward pass. 

This is just the reward scheme for maze solvers. For those the goal is to solve the maze quickly. You have a reward scheme. The best policy maximises the expected reward. That is the only measurement that a completed RL output (your policy) cares about. As all your chosen learners should learn the optimal policy, you cannot compare them on this matter. So you maybe need to look at efficiency - how quickly the algorithm converges to optimal policy. 

Here is an example of the scheme that Geoffrey Hinton describes. Say you have 4 binary features, associated with one target value and see the following data: 

It is not training. However, like with training we cannot directly measure how "off" the source that we want to change is from an ideal value, so instead we calculate how to move toward a better value by taking gradients. Back propagation is the usual method for figuring out gradients to parameters in the CNN. There are some main differences with training: 

There are many factors which influence choice of classifier algorithm. Number of target classes does not generally have an influence, compared to the nature of input features. As one example, if your input data is natural audio or image, then regardless of the number of classes, a deep convolutional neural network is very likely going to have the best performance. 

To match your description of the diagrams, let's define a "unit" as a collection of one of each type of neuron/gate used to make up the cell, that in theory could be wired together to make a working LSTM cell layer with a single scalar cell state and output value. These units are independent in that each has its own weight parameters. There are no shared parameters for the connections between input and the units, or for the recursive connections that forward state from one time step to the next. In that sense the units do not share information. However, the connections do mean that on each time step, input data and hidden state plus output from last output from all other units in the cell are combined are used in calculations. Any cell unit can base its new internal state plus its output on the values of all other outputs and internal states from other units in the cell. In this sense, the units do share information. I guess from your question that it is probably this second issue that you are concerned about, as the second diagram makes you think of a wiring diagram for a single neuron, but as explained above that is not the case.