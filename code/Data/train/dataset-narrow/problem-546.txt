To explore a bit more about the question and comment, I want to note that generally when normalizing your goal (among others) is to reduce duplication. A naive example of your table might be everything you have but adding things like the CSV list you mention, this ends with a very wide table that has multiple areas of duplication and which will require you to manage and clean your data (and is generally bad for performance.) A solution to this problem is to simply project the data into another table and provide the keys to join back to the original. Eg: 

You are grouping by the thing you are counting, not by the department name. Change your group by to: 

This table would store the ids for both the previous tables and any information that is specific to that exam's instance of that question. This would allow you to write queries such as "How many questions belong to one exam" 

Simple answer, you grouped by your sum. Solution is simply to remove that from your group by statement, eg: $URL$ 

SQL Server Setup Control /INSTALLSHAREDDIR Specifies a nondefault installation directory for 64-bit shared components. Default is %Program Files%\Microsoft SQL Server Cannot be set to %Program Files(x86)%\Microsoft SQL Server /INSTALLSHAREDWOWDIR Specifies a nondefault installation directory for 32-bit shared components. Supported only on a 64-bit system. /INSTANCEDIR Specifies a nondefault installation directory for instance-specific components. 

I have a script task that appears as running fine, but no matter what I put inside nothing ever get executed. I tried all sort of stuff: logging via the built in SSIS Logging mechanism, logging through additional variables and SQL Command task and I can never figure what's wrong. I tried to put breakpoint and it won't break during debugging. This is VS 2015 with SQL 2016. This issue happens only when deployed on one of the SQL Server. I previously has no issue at all at that server. I deployed the same package on a different SQL box (same configuration) and it ran as expected. EDIT: To better clarify the problem I started a new SSIS Package, it only 3 script tasks in it: Test Task: 

So I decided to set a lower max memory setting and monitor the server. After running CHECKDB for 50 minutes one of the svchost.exe start getting page fault and free memory quickly drop from 2GB to 0 (but the process itself was only taking 300MB). The chaos last about a minute and then the system settle down. CHECKDB wasn't interrupted this time, and the sysadmin is asked to check which underlying service is causing this issue - a new monitoring tool was installed about a month ago. 

I agree with some of the other comments though that perhaps your servers are set to auto update and this is how it could have occurred. Based on how your server is configured and combined with the changes to how CUs are installed, it could have auto updated. 

There is a lot of info out there about this behavior going back years. Here is a thread: SQL Server Management Studio slow opening new windows Common thread there seems to be SSMS is trying to reach a location in the internet. Another thing there states to change the user feedback Opt In setting. Maybe one of these options will speed it up 

You may have to shrink the data file to reclaim the space. Deleting data will not free up the space to the operating system, it will just make the currently used space less full. Truncate is similar to delete but doesn't log everything like a normal delete does. Here is a dumb analogy I use sometimes: putting a cup on a napkin can take up most of the surface space on the napkin but filling and emptying the cup has no affect on the napkin. In this analogy, the napkin is available space and the cup is your table. In your case you just emptied the cup. Now you have to remove it from the napkin, and shrink will do that for you. But what you should look into is why this table got so big in the first place. You may very well run into the same problem soon so you may have to revisit the space used and add more space to the database. 

With these and some additional databases/tables and jobs and time you can build out a basic monitoring system (but it isn't pretty) these are tools for DBAs; unless you are good at BI stuff you will struggle to find time to produce useful business friendly stuff from it, though the Ozar sp_blitz app is pretty dang cool. After spending around a year doing the free thing and resolving plenty of issues (but not getting much buy in) I was able to make it clear, after a major issue, that perf monitoring software was a priority, and we were going to buy it come hell or high water. After demoing the previously mentioned clients, I chose DPA because management could easily consume the results, though I definitely have client licenses for SQL Sentry Plan Explorer Pro (1000% worth the money) and really liked using the server version, it just didnt grab them the same way. I also tried getting SQLNexus working at one point but I ended up working a lot than I was interested in, it may suit your needs. 

Yes, SQL Server can report how long it took to do any of those actions (though you may have to run it to get some additional details such as actual row counts returned) Statistics Time 

LinkedServerName would be the real linked server name and SourceTableName would be the name of the table in the linked server. 

I would say the stored procedure itself probably does not include the comment with the current script date. SQL Server Management Studio adds that when you generate the view of the procedure. In my experience, the stored procedure itself starts with CREATE or ALTER and anything above that is added by SSMS when you look at the code. 

The USE command changes the database context to the specified database or database snapshot in SQL Server. I believe there should be a query with matching columns after your INSERT statement that would be the source for the INSERT. It may be OPENQUERY and could look like the following: 

Per Books Online, compatibility level sets certain database behaviors to be compatible with the specified version of SQL Server. As such, some queries may no longer work or may need to be slightly changed. These may be performance related especially when going to level 120 or higher as those change how the query optimizer works. If you do not change compatibility level before upgrade, the upgrade process will upgrade to lowest level compatibility on the new install, in this case to 100. The benefit of changing compatibility first before migration is to see if any queries need modification (assuming an application is attached to this database). Since this is a migration, one way to test performance is to migrate the databases to the 2016 server and then run same queries on both and gather metrics from them to compare. However, on same server performance should be similar, assuming the query is still able to run. An example of a query that will no longer run after changing compatibility mode would be as follows: 

Edit:Adding a little flavor text as requested. Basically what is happening is you are choosing to aggregate one of the values (in this case the counts of the salary) so that it "rolls up", and the group by generally indicates which value you want to do the rolling up by. It makes some sense to say "I want to group by the number of employees" but you are actually trying to express "I want to return the number of employees grouped by department." 

This table would only store information relevant to an exam, such as the title of the exam, the exam's attributes and categories. 

This means we could count all of Michael Jordan's games by sport(as he played multiple) with a query something like: 

I dont think so except for the fact that there is a 5NF, which describes a design where your joins are only on the candidate keys. Many "4NF" designs meet this criteria, but not all, and it is definitely something you can change a 4NF "into" to be be more normalized. 

It looks like based on your error that the user you are trying to create a login for already has a login in that database with another name, so you are effectively "double mapping" them. If you have access to SSMS you can see what their mapping is by using the Object Explorer and navigating the following path: Security -> Logins -> Double click on the username This should pop up a GUI, you can see the "User Mapping" in the top left hand corner, this should show you to whom the user is mapped. 

there is no min or max memory limit set, original question is missing the key word "no" there is enough room for temp DB to grow there was no disk space alert on any of the volume, so I assume disk space is not an issue database is about 50GB in size I am not aware of any system changes other than Windows Update Normally CHECKDB would take 60-90 minutes to complete at night 

We have a routine DBCC CHECKDB running at night when the database is still accessible but with very little traffic. Since last month we had several occasions of DBCC CHECKDB crashing to this error: 

Once the Test Task is completed, it is linked to a "Success Task" if the test is successful, a "Fail Task" if it fails. 

I am looking at a new SQL 2016 installation that will consist on two nodes (located in the same data center): Node 1: Production DBs Node 2: Production DBs (replica) I am thinking where to install my SSIS / SSRS / SSAS. If they don't have high availability requirement, then does it make sense to install them on node 2? The only draw back I see is the potential network congestion. So end result is Node 1: DB Engine, Production DBs Node 2: DB Engine, Production DBs (replica), SSIS, SSAS, SSRS, all supporting DB (not HA) 

SSIS can connect to many different servers based on the connection type used in the SSIS package. Since even SQL Server 2016 SSIS still uses Native Client 11 (and has since 2012), you should not have any issues connecting from SSIS 2008 R2 to SQL Server 2016. SQL Server 2008 R2 uses Native Client 10 or 10.1 but has no issues connecting to SQL Server 2016 either. In the connection manager in SSIS, you choose the way to connect to SQL Server. It could be ODBC, OLE DB, or any number of connections types. I say this to say SSIS itself does not care what version of SQL Server you are connecting to but the task you use will determine the connection type. I would assume you are using OLE DB and as such you may choose the provider in the connection string. 

I would suggest using SQL Server Data Tools (SSDT), which is essentially the latest iteration of the Visual Studio Database Projects. I use this at places that do not already have a way to source control their databases since it easily integrates with TFS, which most Visual Studio shops use for source control. A couple pros I have for using it are: 

If you find yourself saying "nullable columns" before you have written code I generally think you need to normalize more(as always, it depends.) If you want one system to maintain, the last option seems the most relevant, but "splitting them into two tables" is not really where you would want to take it. if we want to track basic things like Players, Games, and Sport type you would simply add intersection tables between the relevant things you want to store. 

As a fairly newly minted DBA under the gun, I have run the gamut of free tools and done some experimentation in the paid space (DPA, SQL Sentry, and Foglight) and it really depends on what you want the tool for. In my experience the most important thing was not just communicating performance baselines (management vastly didn't care unless there was someone to yell at), but produce something in an easy to consume format that made the priorities clear and was able to track down performance issues in production. You can absolutely build up your skills by going the free route, and the tools for SQL Server are great. 

The easiest method is (probably) to uninstall and this time run the command line installer again with a different set of flags and it will install to a different location unless you are talking about components not listed below. From: $URL$ Proper Use of Setup Parameters Use the following guidelines to develop installation commands that have correct syntax: 

I have an installation of SQL 2016 Enterprise and my boss want to deploy TDE. I googled online and all the instruction said about "you can use a self sign certificate for test/dev purpose". Most guide just said backup the master key and cert and that's it. I have two SQL Server, they sit on two separate domain. I want to make sure the database encrypted on one server can be restored on the other server. What is the proper way and step that should be applied to a production server. I think there is more than just creating a self sign cert and back it up somewhere safe. 

I am designing a web application that will have probably around ~1000 users. The website is facing the internet but it can only be used by partners (i.e. the general public cannot just "sign up an account"). Account maintenance will be handled by our own IT support. I am considering having each individual user to have their own SQL login associated. This way I do not need to store their salted/hashed password - SQL server will handle the authentication. This also allow us to use SQL Server based audit and logging tool. Is there any disadvantage doing that? 

Your server authentication mode may be set to Windows only instead of Mixed Mode, which will allow Windows logins and SQL logins. If true then the SQL login will not work. In SQL Server Management Studio, you can right-click on the server and then go to the properties. Then go to security and see how the server security is configured. You can change it there also. If it is set to Windows only then you will have to use a Windows login. If you want to change to mixed mode to use SQL login, you can make the change but that may require the instance to be reset to take affect. 

One reason for separate drives even with SAN is that a given disk or set of disks can only do so much I/O at once. Any of the things on the drive could saturate the I/O on the SAN by keeping OS, TempDB, logs, page file, and the user database on the same drive. Depending on the query and server configuration, TempDB, the page file, and the LDF could all be in contention for the I/O. By separating those, the I/O for those set of disks is exclusive to where they are allocated. Another thing to consider is any one of those things could also use all the available space if sharing disk space and instead of one component shutting down, the whole server could be shut down.