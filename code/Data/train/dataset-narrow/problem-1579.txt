Short answer: Sorta. Long Answer: Exchange allows you to export messages and mailboxes with the Export-Mail command, but, of course, it only lets you export it to Microsoft's crappy PST format. From there you have a number of utilities that can break down the pst to text. It's guaranteed to be extremely tedious. John is absolutely correct. You'd be much better off downloading the mail programmatically using POP or IMAP. 

I might start with Ubuntu, if it's your first time. Gentoo is a perfectly respectable mainstream distro though, as are SuSe, and Fedora. 

It won't automatically know which one to use, no. But you can specify this using the route command. You can try to configure the AD services to only use the IP of your "internal" NIC, but it's better if you just set your upstream firewall to block all inbound/outbound domain traffic. I don't really trust the AD not to try and make friends on the outside. 

When you want information on processes, the answer is always It is simple, and yet it has a ridiculous number of options. Try this one: 

You can use LDAP or Radius to authenticate 1 or 100 machines, and it'll make administration a hell of a lot easier. Link to ProFTPD's MOD_LDAP module Link to ProFTPD's MOD_RADIUS module 

Run a traceroute on one of the other machines and make sure it's not somehow being routed through your Mac Pro. That would be bizarre, but hey, easy to check. After that, I'd check to see if it's trying to sync something to the network before it shuts down. Start up ipTraf on one of the Linux boxes and see if it's getting any weird traffic. 

If you're blocking it through the AD, you're going to need to go to the "Active Directory Users and Computers", select the record for the computer in the domain, and go to the "Security" tab and remove everyone you don't want to have access. Might also want to go to "Local Security Settings -> User Rights Assignment" and make sure "Access this computer from the Network" is right, and "Allow logon though terminal services" as well. 

Part of the biz is becoming a "fan" of whatever server configuration works for you. For a simple app, it really shouldn't matter, but you'll find that deployment and maintenance are better for this or that server. Maybe your linux distro has better packages for apache. Maybe it's better for nginx. Maybe you can't make the configuration work for apache. Maybe you can't make it work for nginx. If you're looking for a simple "This server is superiour" answer, it's just not going to happen. They can all be configured to work very well or very poorly, and anyone who has worked in the industry has seen both and formed strong opinions. That being said, I tend to stick with Apache. Nginx is a hot up-and-comer, so jumping on that is not a bad decision, but Apache is still the gold standard. 

Ug. Well, the usual method is getting apache to coredump but that also causes a lot of insecurity, and should never be done on a production machine. You can also try xdebug. Have you changed anything recently? 

The problem with textbooks in this context, is that most schools don't teach "SQL Server 2008 in Windows 2008" for the simple reason that, when you graduate, you'll need to know "SQL Server 2012, on Windows 12" and they'll have taught you nothing. I've never read the Murach book, but I'll tell you right now, any book that says you'll learn: "How to create complex inner and outer joins, summary queries, and subqueries..." has nothing that a professional would find useful in it...that doesn't even fall in the scope of a MSSQL server book, imho...That's DBA territory. You're going to need to go out and hit the "MSSQL 2008 for Dummies" section of your local bookstore. That's the sort of place you'll find tech manuals that deal with software-specific configuration issues. I'd recommend "Microsoft SQL Server 2008 Internals"; it's not a bad book. Don't buy a book on Windows Server 2008: unless you're setting up AD on it, you won't need it. What you will find, very quickly, is that it is very easy to do very easy things, and very difficult to do everything else. The online documentation is horrible; they hired savants who know the exact example that would help you understand their cryptic instructions, and they ruthlessly expunge all those examples from their site, choosing instead to use ones that are so simple, so wholly idiotic, you'd never have needed them, or so esoteric you don't know why ANYONE would have needed them. The visual studio tools (which damn well BETTER come with the software) are very nice. If you don't know a lot of VB scripting, you're going to need to learn to love the "Business Intelligence Development Studio": it's ornery and picky, but it's better than nothing. There is no substitute for just installing it and playing with it. You can't even have good questions until you've seen it running. It's very easy to set up: just stick the disks in and go. 

Yea, you need mod_python. Or you can enable cgi support by putting in a directive in your httpd.conf...You'll need to put a cgi shebang on the first line of your script as well (i.e. ). 

Assuming your firewall doesn't start with DENY ALL as it's default state, then yes, you'll need to do DENY ALL, and then ALLOW the particular IP address and port for the connection you want to make. 

I worked with a guy once who refused (categorically) to upgrade to any browser past IE 6, and he held to that policy through hell, high water, and tears, until our own website would no longer even work on the browser installed on our internal desktops. He had the exact same view about everything else on the network that he didn't put there himself, and was just a complete misery to work with. As an admin, your job should be to help the users do their job, and to try and help out those few who have some technical competence when it's not going to be too much trouble. This is not to say that you should let every schmuck come in and plug in whatever the hell they want, and it's not to say that you should compromise your security in the name of user convenience, but your job is not to keep your network wholly pristine, it is to make it as useful as possible, and useful means getting a little dirty. 

Check the umask for user Proftpd and group nogroup. Since you're trying to set permissions higher than the system default, they may be restricted by their OWN umask values. A way to check it might be to change the umask in proftpd.conf to 777; if new files show up as 000, then you know that configuration line is working. 

That's pretty irritating for a number of reasons. You should never just automatically apply every update that comes to you, but if you ARE going to do it, you should set up a scheduled time to apply them and reboot...Leaving that to the user is absurd in a production environment. To set up a time, go to Control Panel->System->Automatic Updates(tab) and do "Automatic" and put in a time (defaults to 3:00am). To do it properly, set up WSUS, so you can deploy all updates more efficiently across the domain. 

MSSQLSERVER is the default instance name, so you don't have to specify a name in that case. It's also treated slightly differently: MSSQL Server differentiates between "Named Instances" (like SQL2005) and the "Default instance". Chances are you're having connect issues because "MSSQLSERVER" is not a named instance, but you're trying to connect as if it were. Source 

Even if you have some sort of absurd application that needs to do trillions of writes, you'd probably be better off just having a number of unconnected servers available through some sort of round-robin interface, and just combining the data later in a more controlled environment...There is virtually no way to be sure of replication when you're getting hammered like that. In the end though, most database applications don't perform many writes in comparison to the reads. 

I love virtualization, both from the perspective of an administrator, and from the perspective of a developer. On the admin side you gain a lot of flexibility, and a lot of redundancy. If the VM is too slow, you can easily migrate it to a faster machine, give it more resources. Upgrading is a snap. In the event of a disaster, you can flip the whole thing to a new machine. On the developer side, you can customize the exact os/software layout you need, and the admins don't give you as much crap since it doesn't eat up a whole machine. Performance can be an issue, but only if you're either trying to virtualize something that's too resource-intensive to easily virtualize, or if you're overloading a VM server. Mostly people don't even notice. 

IMHO, it's a little odd to change your log file by SIZE rather than by date. Most system logs (in unix or linux) rotate on a weekly or monthly basis, and not based on size...This is something I like for various reasons, and also something which, if implemented, would solve your problem. Eight years later, I don't know what the hell I was talking about here: there are tons of places where you want to rotate by size, because daily/weekly/monthly rotations can yield MASSIVE files which can cause serious issues. From a more experienced perspective, the real question is why you'd want to sit and continuously tail a file that's growing so fast that you're rotating it more than daily...It'd be like watching the Matrix stream by. These days you'd be better looking into some big data log aggregation like Splunk or Sumologic, where it can filter log events into classes and trigger based on specific log values...No need for watching live logs at all. 

Well, the 4948 is thin, so I'd be comfortable racking it with only the ears, but I don't think I'd be willing to transport it, in the rack, only secured by the ears. They can fail, and that's a lot of money down the tubes. 

It doesn't mean "try forever", it means "don't try at all." This is the server trying to politely tell the client that the server is getting ready to close his socket, and if it would please do an orderly disconnect, or send some more data, that would be wonderful. It will try X times to get the client to respond, and after X, it reclaims the socket on the system side. Setting that number to 0 would suggest to me that that server is heavily utilized, with a zero tolerance policy for orphans. It may also have been a response to a DDOS: lot of DDOS' work by opening a socket connection and then hanging on to it, doing nothing. 

Generally you're never going to have "one" external IP: when you sign up for sufficient bandwidth for a few hundred people, you're going to get at least a dozen addresses. If you start signing up for the kind of bandwidth a large school would require, you're going to get hundreds or thousands. I used to work as a junior admin at a big state school and we had a godawful huge number of addresses (4 /16s a /19, a /24, and innumerable /27s and /28's). Of course, we're talking about a huge school with a huge computer science program, and multiple campi. Generally the dorms had global ips for their routing, but for the most part the inter-dorm addresses were in the 10.0.0.0/8 block. Classrooms often had global addresses: some of the big lecture halls had their own wireless subnets (again, in the 10 block). One of the problems with a /16 of global ips is that you have to be really careful with assigning ips, and it causes some non-trivial DHCP issues in places where you allow DHCP. In some of the older dorms we had assigned global addresses, and it caused untold headaches, especially when people started setting up their own subnets with wireless routers and crap like that. Much much easier to assign 10 based local subnets (also much easier to deploy your own WAPs rather than trying to prevent students from setting up godawful conflicting ones.) These days I admin at a reasonably large company: we have a /16, and we hardly use it at all, preferring to have a big WAN connected on private 10. addresses. Using private address space makes a lot of things easier.