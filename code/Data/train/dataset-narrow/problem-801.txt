After switching to a different master you must reseed the identity value, otherwise you will get duplicates in the id column. Use the statement. 

No it is not, because identity does not guarantee a unique value. The identity property can be bypassed with (in SQL Server - you didn't specify what RDBMS you are using). A primary key constraint (and a unique constraint) uses a unique index to enforce uniqueness. 

Look to see if the data is stored somewhere else. Perhaps the system sends email confirmations. If so, retrieve the emails (from sent items or from auditing on the email system) and hire some temps to retype the information. Perhaps the system prints out reports. If so, obtain the printouts and get the temps typing. Perhaps the system exports data and sends it somewhere else. If so, get the exported data back from where it went and get those temps typing. 

It didn't mention about backups. For the meantime, you can change log backup param to as a workaround. Or consider moving to another backup tools (eg. Minion Backup or dbatools) or roll your own custom code because the last update of Ola's Maintenance Plan was on Oct 7, 2016. There's a github if you like to raise an issue/enhancement. 

Aside from what David already answered, I believe these are the most comprehensive guide to SQL Server Transaction Log Internals and Understanding How SQL Server Executes A Query that are free from the web. A must read blog. Thanks to Remus! 

You are right! I think this is a bug or by-design. I was able to repo the scenario. So basically when you run Ola's script with this: Or native: When you execute a new full backup or full backup or even differential backup, All your previous log backups will be deleted after you run another log backup (Ola's log backup script with param). Tested using Ola's script version: October 7, 2016. Based from Ola's website: 

Thus if the precision is "years" then the actual date of birth is "1985". If the precision is "months" then "March, 1985". If "days" then "25 March, 1985". And so on. Something to consider is whether it is an error to have a precision column of "months" with higher precision actually specified? That is, is ("1965.04.25 5:43:28", "months") valid or does it have to be entered as ("1965.04.01 00:00:00", "months")? If the first answer is invalid then the check constraints get a bit lengthy. As an aside, if this particular situation happened in many of the entities in the system and was used in a large number of select statements then I would consider creating a user-defined type (if using SQL server then it would be a CLR user-defined type) to avoid having to enter the same case expression over and over. 

It could be someone or a process/apps deleted and inserted the data in subscriber db (while cmds are being applied). Review your subscriber security settings and check what process/apps are accessing the subscriber db. Multiple publications connected to subscriber db. pub1 (deleted the data first) then pub2 tried to UPDATE/DELETE the data which could cause an error 20598. Triggers on subscriber tables that could delete/insert/update the data. 

We need to step back a bit and apply basic SQL Server troubleshooting on subscriber db as well. Check for blocking, IO contention, network issue, check for wait stats, triggers, cursors, long running job/transactions, service broker, AGs redo queue, etc. From my experience, the silent killer of replication performance is... triggers. You wouldn't be able to catch it unless you're using profiler or you understand how the data flow works. These could quickly stock up the pending commands to be delivered. 

When you create the management data warehouse, the current login will be added to all three roles, making it an administrator of the database. The dialog above lets you add other logins to the roles, to make those people readers, writers or administrators of the database. 

Options Unique constraints support indexing options like and , though this hasn't been the case for every versions of SQL Server. Included Columns Nonclustered indexes can include non-indexed columns (termed a covering index, this is a major performance enhancement). The indexes behind PRIMARY KEY and UNIQUE constraints cannot include columns. Hat-tip @ypercube. Filtering A Unique constraint cannot be filtered. A unique index can be filtered. 

Log Shipping essentially involves three jobs. The first job backs up the log on the primary server and stores those backups in a local folder. The second job copies those files across the network to the secondary server. The third jobs restores those backups using the WITH STANDBY option. Log Shipping is set up on an entire database. The database on the secondary server is accessible but is read-only. Requires: A shared folder on the primary server. Firewall configuration to allow the secondary server to access the file share. Security set up so the proxy on the second server has permissions to access the file share. This may be an issue for you in a hosted situation. Caveat: Log shipping is not real-time. The common interval for the first job is 15 minutes. An unplanned failover will lose data. Caveat: Log Shipping uses transaction log backups, so its design must be done in conjunction with your backup design. Database Mirroring involves the primary server sending individual transactions to the secondary server, in either a synchronous fashion or an asynchronous fashion (asynchronous is enterprise edition only). Like Log Shipping, Mirroring It is set up on an entire database. Unlike Log Shipping, the secondary database is inaccessible. Requires: An endpoint to be created on each sever for the mirroring traffic. Firewall allowing traffic from and to that one port. In a synchronous mirroring setup, an unplanned failover will not lose data. Both Mirroring and Log Shipping are creating a copy of an entire database so there is no object requirements. Heaps, Clustered Tables, tables with and without keys - all get copied. Note: Given the requirement for file sharing, I disagree with Szymon's comment about log shipping being easier to set up and maintain and requiring less resource. Installing the File Server role on a Windows Server is increasing resource requirements as well as increasing the surface area of attack. Additionally, in a log shipping unplanned failover, bringing the secondary online is a pain. Lots of steps, most of which involve running stored procedures in a query window. 

You may need to install at least Service Pack 2 or higher of SQL Server 2008 R2 on Windows Server 2012 R2. I would suggest you to install the latest Service Pack 3. You can get the installer here. Slipstream your installer to SP3. This guide will help you on how to slipstream. You can also read this KB Article for OS and SQL Server Version Compatibility. Edit (to complete my answer with the following solution based on the KB): 

I would highly recommend you to capture a baseline on replication commands stats (Distribution Agent). We can use Perfmon to monitor the following: 

Still on distribution agent job history. You can look at your stats on writer and reader thread of distribution agent. (eg.) 

It checks port tcp1433. The default instance is listening on this port by default. It checks port udp1434. The SQL Browser service is listening on this port. 

Note: I came here from another answer where I misstook SQLIO for SQLIOSim. Hopefully I'll get it right this time. :-) SQLIOSim is "NOT" an I/O Performance Tuning Tool because it uses random patterns, so is not repeatable (a primary requirement for benchmarking). It is a stress-testing tool. Additional links: 

Additionally, is the server actually called "local" or is that just anonymised? If you want to connect to the local server then the alias is a period. 

"â€¦is there any benefit to changing the GUID generation to sequential using newsequentialid()?" No. Sequential GUIDs are only appropriate when there is a clustered index on the GUID column and you want to avoid page splits caused by inserts. Edit to address the comment below: All nonclustered indexes suffer from page splits when data is inserted. For example, when you enter a record for Homer Simpson, it gets entered into the "S" leaf pages for the LastName index, possibly causing a page split. You don't however, require that customers join in strict alphabetical order. Additionally, the latch system used for index leaf and non-leaf pages means that page splits require less processing time and resource than page splits on data pages. Further to this, what would it require for the OP to change to a sequential id? They would have to replace the Default constraint on the Key column to NEWSEQUENTIALID(). This will not affect any existing rows in the table (which is good because there are foreign keys using those keys) - just the new rows. From that point on, inserted rows will have increasing keys but those increasing keys are not necessarily going to be greater than the existing data in the table (NEWSEQUENTIALID() only guarantees that the GUID is greater than any other GUID generated by NEWSEQUENTIALID() on that computer since it restarted). This means whose inserts are still going to cause page splits in the nonclustered index! 

You might be doing a large batch of transaction processing that could cause log reader agent to slowly read the t-log. Or NOT properly maintaining the t-log that could cause increasingly huge t-log. Ask around what's being process at that time. (watch out for index maintenance - Log Reader Agent will appear hung as it scans more log records.) Then check for VLF (Virtual Log File) by issuing on the publisher database. See if you have a lot of VLFs. Keep VLFs in check and t-log size at minimum. The key is to properly manage your transaction log when dealing with replication. Read on how to Optimize the Transaction Log. On your Log Reader Agent Profile, you can change the and to higher value. Be careful on these 2 parameters as it can cause Log Reader Agent to scan more transactions and could spend more time scanning and slowly delivering to distribution database. It's rarely you need to change parameters (even the ) on Log Reader Agent so use it with caution. 

Two drives, RAID 1 - Operating system, executables, pagefile. Four drives, RAID 5 - All data files (alternatively, RAID 1+0) Two drives, RAID 1 - All log files 

Fabian Pascal identifies four pragmatic criteria for choosing a key: familiarity, irreducibility, stability and simplicity. He also notes that sometimes you must make trade-offs. During the logical design phase you will identify all the candidate keys for an entity. During the physical design phase you will decide which of them will be the primary key (the one usually used in referential integrity, in other words, foreign keys). If you don't think any of the candidate keys are suitable for the primary key then you choose to use an arbitrary key, and then decide what mechanism will be used to generate the arbitrary key, perhaps the application generates it, perhaps the server does (identity, guid with newid, sequence, something else). If your systems will be multi-master in some way then application-generation might be better, as most of the server-side mechanisms don't handle this. The choice of primary key is determined by size and type - you will be ing on it, so you want it to be small. During the physical phase you will also be doing index design, which includes deciding which index will be the clustered index. You want that to be small as well, since the clustered index key is included in all non-clustered indexes. You want it to be unique, otherwise SQL Server will add a uniquifier, which has a small impact on performance. On the other hand, this is not the 70s. We now have computers that don't fill entire warehouses and take all week to run the payroll. If won't kill your performance to use a guid or a char(8) either as a clustered key or as a primary key. Unless you are UPS or AIM Healthcare (two winners of WinterCorp's 2005 TopTen survey). Edit: I just had this link arrive in my mailbox. Simple talk Primary Key Primer for SQL Server Edit: Joel's comment below is very important. Natural keys often aren't stable. Even government-issued keys (tax numbers, drivers licence numbers) that should be static can change. For example, ask Mrs. Hilda Schrader Whitcher about her two SSNs. You control the arbitrary key so you control its stability. In my (admittedly limited) experience, I can't remember a time we used a natural key. I've done work in healthcare, where there were government-issued id numbers (like NZ's Community Services Card) but not every person had them, so they were not suitable for PKs. 

This will give us an idea on how long it will take to apply the commands in subscriber db. The before and after picture of commands stats is important as we can determine how slow is slow or what have changed since last baseline. 

No. This is normal. You are experiencing too many applications installed on your database server. Start migrating your database (SQL Server database engine) to its own server or start uninstalling all other applications in the database server. Currently your server have the following: , , , , installed. Not to mentioned if you have monitoring tools or anti-virus installed. When you login to the server, you also use memory. filter drivers also use memory. Check out this blog post by Jonathan. Let's say, you've a barebone database server (without the other application installed fighting for memory), you can at least have 27GB of memory for your SQL Server.