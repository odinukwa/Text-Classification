The number of columns in a table has nothing to do with the number of columns in the projection of a statement. You can always add additional computed columns to the projection (at least practically... theoretically, you might eventually hit some sort of logical limit). The fact that you are querying here is irrelevant. You can do exactly the same thing with any table. The projection in this query 

There is no need to grant the role. In 10.2, Oracle finally reduced the set of privileges assigned to that role to just but in previous versions, that role has many more privileges than the name would imply. It would be more secure to create one users-- one that owns the tables, procedures, etc. but that does not have privileges and one that has appropriate privileges on the various objects that the application can use to connect to the database. That allows you to do things like prevent the application user from dropping tables or from deleting data from logging tables. Additionally, the owner of the tables is going to need to be granted quota on whatever tablespace or tablespaces that user's tables are going to be created in. You could grant the owner the privilege but it is more secure to grant them a smaller quota on whatever tablespaces they actually need to use. 

to get the distinct program names, but I'm interesting in how often particular groups of applications are used in a given session. So essentially, how often are these three applications grouped together. Is there a way that can get me results in the form of 

This isn't a big deal for such a small query, but most of them return many fields (often 20+) and I'm not a fan of so many case statements for obvious reasons. Because of how Reporting Services work, I can't do a general replacement either, it must be done on each and every field. Is there a more efficient method where I could replace every value in the row with 'Confidential' in a single case statement, or at least something more elegant? Edit: To clarify, that there isn't just this one field in the select. I only wrote one for the example, but in production, some reports are looking at displaying a huge amount of columns. I'm trying to avoid making two comparisons on every column for reports that could return rather large sets of data. 

I work on a team that has an SSRS setup with roughly ~1300 (and growing) canned reports. As one can imagine, this has presented problems when a breaking change is introduced to a table. Finding all the reports that may touch a table/field is error prone at best. I'm trying to programmatically build a dependency model. Getting the schema for and how views/tables relate is fairly trivial. What I'm struggling with is how to get a resolved list of fields from a given query. I can pull the query from the RDL files, but interpreting those files is far less so. Queries can contain aliases for fields and tables alike, plus there are problems such as . I'm trying to avoid a regexp hack, and I really don't want to write a SQL interpreter... My initial thought was to loop through the RDL files and parse the explain plan output. While technically possible, this doesn't give me a complete list of resolved fields. Is there any method in which I can use to get the DB analyse a given query and return a list of ? I don't mind having to do some text processing to pull out the results if necessary. 

An example of a major difference between SYS (or any other SYSDBA connection) and every other user: SYS can't do consistent read. One implication of this (there are others) is that you can't do a CONSISTENT=Y export as SYS using the old exp utility. Oracle employee and expert Tom Kyte is of the opinion that you should rarely ever use either one. Regarding SYS, he points out that it works differently as the example above indicates, but more generally he considers them to be "owned" by Oracle Corporation. If you make a change or add something in either schema and a problem occurs (e.g., a database upgrade fails), I suspect Oracle Support's answer would be, "You shouldn't have done that." 

This sounds like a job for Change Data Capture (CDC), which allows you to (among other possibilities) ship your archivelogs from the OLTP database to the reporting one, mine them for the changes, then query the changes out, ignoring any you don't want (e.g., changes of type 'D' for DELETE), and using whatever process you might devise apply those changes to your reporting tables. I have no idea how well CDC would do with a ruleset encompassing 4700 source tables from another database. I've never used it for more than about 50 tables myself. FYI, there are licensing-related limits on CDC. The full feature set is only available on Enterprise Edition. 

When using the autotrace, query A had a cost increase of 30% and Query B of nearly 40%. Obviously, I should be using query b in both cases, but I don't understand what causes them to differ. 

It sounds like your database administrator has set the value to . Change this value to , and do a restart. The restart will take a long time as normal, but the following reboots should be much faster. If this device is really a "blackbox" to you, I doubt you can change this yourself. I'd contact your DBA. See the documentation for more details. 

If you have a bunch of random fields I would set that up as an . The table would include a reference key to the original account, a field that specifies what type of attribute it is, and the value of the attribute. Alternatively, you could have them each in a series of tables. If each country has a different style zip code, for example, a zip code table might not be a bad idea. Just make sure you have a field. 

This block mixes left and inner join notation. When you mix any form of an outer join and an inner join, it becomes an inner join. If you change the block to 

No, they're not the same. Though I think they're equivalent to what your original query wanted. In an oracle database is an acceptable (if less ideal) way to write a left join. Likewise, denotes a right join. Your original join contains the set: 

This takes advantage of the fact that Oracle does not index entries where all the columns in the index are in order to reduce the size of the index. In order for your queries to use the index, however, you'd need to use the same expression that you used to create the index. Something like 

assuming that a row is smaller than . If the row is larger, then don't add 1 to the result of the . Once you know the number of rows per block, the estimated size of the table will be 

If Oracle's estimates are accurate, adding SGA will have no impact. The 1 row is your current setting. You can see that as you slowly double your SGA (up to the final row with an of 2), the estimated database time and the number of physical reads do not decrease. In Oracle's estimation, you could actually reduce the amount of RAM allocated to your SGA to 81.25% of your current setting without having any impact on performance. Of course, these estimates need to be paired with human knowledge and understanding to figure out whether they really make sense. I wouldn't go around blindly lopping 20% of a system's SGA allocation solely based on this sort of analysis unless I was pretty confident from a holistic look at the system that the SGA was over-allocated, that I had something more useful to allocate the RAM I was freeing up to, and that the workload on this system wasn't going to be increasing in the near future. 

I work on a relatively small development team that works with an Oracle (11g) database. It's recently been requested that all the developers be given the role so that we can utilize SQL Tuning Advisor when developing complex queries. Some have raised concerns that this may have significant performance and/or security implications, but I have not been able to find concrete answers to what these implications are. If this role were to be given to the various members of my team, what are the major pitfalls we should be watching out for? I've included the tag for Oracle 12c as well, since we'll be upgrading to that in the near future. If there's a significant difference between the two, I'd appreciate it if it was at least pointed out. 

My team uses Oracle 11 and SQL Developer. I've been relying heavily on explain plans lately to try and determine the most efficient way to solve various problems. Recently, a coworker pointed out that explain plan is not always accurate to what actually happens in the database, and that an autotrace is a better indication since the query is actually run against the data. Testing a query, I've gotten the following results 

The duplicated a's in the first group and b's in the second group are due to other rows in the data (I'm okay with this, but removing that would be a bonus). My main problem is I need that final a to be treated as a different group; so basically showing the first each time a cluster appears. 

I certainly wouldn't want to include the server name in the TNS alias. It would seem highly probably that this would change over time as databases move from one server to another and as organizations move to things like RAC where there would be multiple servers. Assuming that your service names are chosen meaningfully, I would expect that the service name would match the TNS alias since both are logical names for the same thing. That's not a hard and fast rule, of course. Some organizations may have reasons to have two separate logical names for a single service. For example, you may want a single TNS alias that points to the service for users in the US and the service for users in France. But for the vast majority of situations, if the TNS alias would be , the service name ought to be as well. 

The page you linked to is, quite simply, wrong. It makes no sense for a ZIP code to be a primary key in an address table. You would expect to have multiple addresses in the same ZIP code, as you said. ZIP codes cross city, county, and state lines which makes it uniquely poor as a key (postal codes in other countries may not be as problematic but data modelers always have to worry about the worst case). Plus, ZIP codes change over time for an address which is not something you want in your primary keys. In general, given that addresses tend not to have anything that works as a natural primary key, your address table will need a synthetic primary key () that has no meaning and simply acts as a primary key. Databases have different ways to generate synthetic primary keys, sequences and auto-incrementing columns are common approaches. 

Short answer: No, you can't, and not because it's a system type. You can't anchor a freestanding type to any table's column data type. %TYPE is a PL/SQL construct. CREATE [OR REPLACE] TYPE is SQL. You can't use %TYPE in SQL. It somewhat makes sense that you can't. If you use MYTABLE.MYCOLUMN%TYPE in PL/SQL, you have anchored that PL/SQL type to the table, and should the type of MYCOLUMN change PL/SQL can invalidate your code, then recompile it. It's much less clear what Oracle would have to do if your example worked. Imagine what would happen if you were storing objects of SQLID_T in a table, and the definition of SQL_ID in V_$SQL changed. Would Oracle need to change the definition of the stored objects? What if it couldn't (e.g, SQL_ID was changing from a VARCHAR2 to a NUMBER)? Should that prevent the definition of V_$SQL from changing? Or should the table that used that type become invalid in some way ... you can't SELECT from it anymore? Oracle tries to prevent this kind of thing from happening (from Oracle's Object-Relational Developer's Guide): 

However, generally speaking relying on the side effect of a function to do real work from inside a SQL query is not a good idea. The optimizer is always looking for a reason to do as little work as possible, and you might be surprised sometimes to learn that the optimizer sees the necessity of your function differently than you do.