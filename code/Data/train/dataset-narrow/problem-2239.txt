This very much depends on your model. In reality DNA computing follows (non-relativistic) physical laws, and so can be simulated on a quantum computer. Thus the best you could hope for is that it could solve BQP-complete problems. However this is actually very unlikely to be true (DNA is quite big, and so coherence isn't really an issue), and so by simulation it is almost certainly P. It is important to note, however, that this is efficiency in terms of the number of atoms used, and frankly atoms are sufficiently cheap that this number is astronomical making practical simulation of a test tube full of DNA well outside the realm of what is currently possible. As a result, many people choose to work with models that approximate what happens quite well in practice, but break when pushed to extremes. One example of this is the abstract tiling model, which it turns out is NEXP-complete (see Gottesman and Irani's paper from FOCS last year). 

It seems from the comments that Shaun has in mind something different from what is normally understood by post-selection. I now understand this to mean that the statistics for any measurements made prior to a particular postselection should not be altered by the subsequent postselection. This is akin to having a projection operator where the normalization is carried out over each branch of the wavefunction corresponding to a particular measurement resut, rather than over the wavefunction as a whole. In this case, the arguments given in other answers by myself and Neil no longer hold. Indeed it is easily seen that $P^{PP[k]}\subseteq$ MPostBQP[k], since MPostBQP$[k]$ can be viewed as a BQP machine which can make $k$ queries to a PP oracle, and hence $P^{\# P}\subseteq$ MPostBQP. So now we have a non-trivial lower bound, what about an upper bound? Well, clearly the problem is in PSPACE, but can we do better? Actually, I think we can. We can write any computation in MPostBQP as a sequence of layers of the form: quantum computation followed by a postselection, followed by a single qubit measurement. Indeed, this might be an alternate way to formulate MPostBQP[k], as a computation composed of $k$ such layers (this is slightly different from Shaun's definition which I believe is intended to count only the number of post-selections), followed by a final layer of classical post-processing. I will use this definition of MPostBQP[k] in the following, as it leads to a more aesthetically pleasing result. The below is updated from the original version to fix a hole in the proof. First we wish to calculate the outcome of the measurement of the first qubit measured (not post-selected!). To do this we first note that any quantum computation can be expressed using only Hadamard gates and Toffoli gates, and the amplitude $\alpha_w$ of a particular computational basis state $|w\rangle$ in the output can be written as the sum of at most $2^{H}$ terms $a_{j,w}$, where $H$ is the total number of Hadamard gates, each of which corresponds to a unique computational path. Clearly, $a_{j,w} = \pm 2^{-H/2}$. The probability of obtaining a final state $|w\rangle$ is then given by $\alpha_w^2 = (\sum_j a_{j,w})^2 = \sum_{i,j} a_{j,w}a_{i,w}$. We wish to calculate the total probability of measuring a 1. Let $S_0$ be the set of computational basis states which meet the post-selection criteria (i.e. the post-select qubit is 1) and result in 0 for the measured qubit, and let $S_1$ be the set of computational basis states which meet the post-selection criteria and result in 1 for the measured qubit. We can define $$\pi_0^\pm = \sum{w \in S_0} \pm \sum_{sign(a_{j,w}a_{i,w}) = \pm} a_{j,w}a_{i,w}$$ and $$\pi_1^\pm = \pm \sum_{w \in S_1} \sum_{sign(a_{j,w}a_{i,w}) = \pm} a_{j,w}a_{i,w}.$$ The in this case the probability of measuring a 1 conditioned on a 1 for the post-selected qubit is given by $\frac{\pi_1^+ - \pi_1^-}{\pi_1^+ - \pi_1^- - \pi_0^- + \pi_0^+}$. As we can determine this with 4 calls to a #P oracle. We use this to produce a random bit $b_1$ which is 1 with probability $X_1$, the same as the quantum measurement. Thus MPostBQP[1] is in $BPP^{\# P[4]}$. Next we calculate the measurement result of the second qubit. To do this, we run the same #P queries as for the first layer, but on the circuit obtained by composing the first two layers, and where we postselect on 1 for each of the post-selected qubits, but also on $b_1$ for the output of measurement 1. Note that although this is postselecting on the states of 3 qubits rather than 1, this is a trivial modification to the $\# P$ queries, by simply adding an ancilla which is set only if all 3 qubits meet the conditions required and post-selecting instead on this ancilla. This then generates the correct conditional output probabilities for the result of the second measured qubit, which we label $b_2$. Note that we have now used 8 calls to the #P oracle. We repeat this process iteratively, so that at a layer $j$ we postselect on 1 for all the $j$ preceeding post-selected qubits and on $b_{i<j}$ for all the previous measurement, and label the outcome of the corresponding $P^{\# P}$ machine $b_j$. In total this has required $4j$ oracle queries. Thus we have MPostBQP[k] $\subseteq P^{\# P[4k]}$, which combined with the previous result that $P^{PP[k]}\subseteq$ MPostBQP$[k]$, implies that $P^{PP[k]} \subseteq$ MPostBQP[k] $\subseteq BPP^{\# P[4k]}$, and hence MPostBQP $= P^{\# P}$. 

Assign each event a sequential number starting at 2. Calculate $c = (m+1)! \mbox{mod}~p$, where p is some prime number greater than m+1 but less than $2^{32}$. When registering the events, simply multiply the numbers corresponding to the events sequentially modulo p. Call this result r. To determine if an event is missing, calculate $x = (c \times r^{-1}) \mbox{mod}~p$. If $x=1$ then no events are missing, otherwise $x$ is the label for the missing event. 

As I mentioned in my comment above, and as you allude to in the question, every computation can be made reversible, and by simply retaining the extra bits, there is no inherent thermodynamic cost. Every circuit generated by using Toffoli gates and ancillas to replace irreversible gates becomes as efficient to reverse as it is to compute assuming you have access to all output bits. This is clearly not the case for the functions considered in cryptography, since many ancillae are used and discarded. It is by keeping secret this extra bits that makes the computation hard to reverse. However, by computing the function reversibly, making a copy of the subset of bits corresponding to the output, and then inverting the function the total energy cost for computing and inverting the function will be zero, while the only cost incurred will be in making the copy of the output bits, which depends only on the number of output bits and not the function being computed. This is clearly the best you can do, since it costs the same energy as simply writing the output string to an empty register. Turning to your restated question: 

There is a bit of an issue with your question. You are using the words unpredictable and secure interchangeably, which would seem to imply that you mean "unpredictable to a polynomial-time bounded adversary". This is not at all the same thing as being truly unpredictable, since clearly there is a function (in this case the PRNG) which generates them. So this is not related to any issue with proving that the sequence is 'truly' random in some sense. Rather, what you really need is that given the prior output of the PRNG it is computationally hard to predict the next number. This formulation is clearly related to P vs NP since clearly calculating a sequence of $m$ pseudo-random numbers using the generator is in FP (the functional version of P) when the key $k$ to the PRNG is known. However given a set of $m$ sequential pseudo-random numbers outputted by the device, the problem of finding $k$ is clearly in FNP (the functional version of NP). Thus proving that it is computationally easy to generate the pseudo random numbers but computationally hard to infer the key from a sample of the output of the PRNG would imply that $FP \neq FNP$ (and hence $P\neq NP$). 

Well, this can be formulated as the problem of finding the ground state of an Ising-like Hamiltonian with 3-local terms. These don't occur naturally, but you would expect them to cool similarly to other systems, so simulated annealing should work just fine for the weighted version. 

The way you formulate your question makes it sound like you care about dynamics, but since what you are looking for seems to be a steady state solution, ground states seem like a much more productive route to go down. Since you want to go beyond pairwise approximation, the most natural candidate technique seems to be matrix product states, which is a pretty hot topic at the moment for dealing with quantum ground states. The way this approach works is basically by introducing maximally entangled pairs between nodes, and at each node introducing a projector. By adding higher dimensional systems you'll capture more features of the graph. I know your problem probably isn't quantum, but I don't see why this technique still shouldn't work. You should be able to simply replace the entangled states with $\frac{1}{2}(|00\rangle \langle 00| + |11\rangle \langle 11|).$ Also, I'm not sure if this is the kind of thing you are looking for or not, but there are some recent results on realizibility of scale-free networks, showing that they exhibit two phase transitions which seems to have just been accepted to PRL. A preprint entitled "All scale-free networks are sparse" can be found as arXiv:1106:5150. 

I believe the answer is that it is possible in polynomial time as follows: Each permutation of $N$ can be expressed as a function $f(x)$ which maps an input index to the corresponding output index. Each such function can be expressed as a string $f(1) + f(2) + f(3) + ... + f(N)$, where $+$ is the concatentaion operator. Thus there is a natural lexicographic ordering on the permutations. The idea is to pick the permutations in lexicographic order. The probability $p_{i,1}$ that permutation $P_i$ will be the first permutation (in lexicographic order) from a set of $M$ permutations of $N$ elements chosen uniformly at random will be $$p_{i,1} = \frac{\binom{N-i}{M-1}}{\binom{N}{M}}.$$ By making a random choice from this probability distribution we determine the first permutation in our set. Let's assume that this permutation is indexed by $x_1$. We now need to choose the permutation which is second lowest in lexicographic order. Clearly, this is the same problem as before, but now we need to choose according to the probability distribution $$p_{i,2} = \frac{\binom{N-x_1-i}{M-2}}{\binom{N-x_1}{M-1}}.$$ For an arbitrary ranking $k$ the probability distribution will be given by $$p_{i,k} = \frac{\binom{N-i-\sum_{j=1}^{k-1} x_j}{M-k}}{\binom{N-\sum_{j=1}^{k-1}x_j}{M-k+1}}.$$ This process is repeated until we have chosen all $M$ permutations. As there is no possibility of collision, the process terminates after exactly $M$ such choices, and hence is an exact polynomial time computation assuming the random choice can be made in time $\mbox{poly}(N)$. 

Not really a full answer (nor a useful reference), but just a rather an extended comment. For any given bin, the probability of having exactly $B$ balls in the bin will be given by $p_B = \binom{m}{B} \left(\frac{1}{n}\right)^B \left(\frac{n-1}{n}\right)^{m-B}$. We can use an inequality due to Sondow, $\binom{(b+1)a}{a}<\left(\frac{(b+1)^{b+1}}{b^b}\right)^a$, to yield $p_B < \left(\frac{(r+1)^{r+1}}{r^r}\right)^B \left(\frac{1}{n}\right)^B \left(\frac{n-1}{n}\right)^{m-B}$, where $r=\frac{m}{B}-1$. Note that this bound is fairly tight, since a $\binom{(b+1)a}{a}>\frac{1}{4ab}\left(\frac{(b+1)^{b+1}}{b^b}\right)^a$. Thus we have $p_B < e^{B(r+1)\ln(r+1) - Br\ln r - m\ln n + (m-B)\ln (n-1)}$. Now, since you are interested in the probability of finding $B$ or more balls in a bin we can consider $p_{\geq B} = \sum_{b=B}^{m} p_b < \sum_{b=B}^{m} e^{b(r+1)\ln(r+1) - br\ln r - m\ln n + (m-b)\ln (n-1)}$. Rearranging the terms, we get $$p_{\geq B} < e^{-m\ln \frac{n}{n-1}} \times e^{B(r+1)\ln(r+1) - Br\ln r - B\ln (n-1)} \sum_{b=0}^{m-B} e^{b(r+1)\ln(r+1) - br\ln r - b\ln (n-1)}.$$ Note the summation above is merely a geometric series, so we can simplify this to give $$p_{\geq B} < e^{-m\ln \frac{n}{n-1}} \times e^{B(r+1)\ln(r+1) - Br\ln r - B\ln (n-1)} \times \frac{1-\left(\frac{(r+1)^{r+1}}{r^r (n-1)}\right)^{m-B+1}}{1-\left(\frac{(r+1)^{r+1}}{r^r (n-1)}\right)}.$$ If we rewrite $\frac{(r+1)^{r+1}}{r^r (n-1)}$ terms using exponentials, we get $$p_{\geq B} < e^{-m\ln \frac{n}{n-1}} \times e^{B(r+1)\ln(r+1) - Br\ln r - B\ln (n-1)} \times \frac{1-\left(e^{(r+1)\ln (r+1) - r \ln r - \ln(n-1)}\right)^{m-B+1}}{1-e^{(r+1)\ln (r+1) - r \ln r - \ln(n-1)}},$$ which then becomes $$p_{\geq B} < \frac{e^{-m\ln \frac{n}{n-1}} \times \left(e^{B((r+1)\ln(r+1) - r\ln r - \ln (n-1))} -e^{(m+1)((r+1)\ln (r+1) - r \ln r - \ln(n-1))}\right)}{1-e^{(r+1)\ln (r+1) - r \ln r - \ln(n-1)}}.$$ Now, I take it you care about finding some $B$ such that $p_{\geq B} < \frac{C}{n}$ for some constant $C$, since this gives the total probability of any bin having $B$ or more balls as bounded from above by $C$. This criteria is satisfied by taking $$\frac{e^{-m\ln \frac{n}{n-1}} \times \left(e^{B((r+1)\ln(r+1) - r\ln r - \ln (n-1))} -e^{(m+1)((r+1)\ln (r+1) - r \ln r - \ln(n-1))}\right)}{1-e^{(r+1)\ln (r+1) - r \ln r - \ln(n-1)}} = \frac{C}{n},$$ which can be rewritten as $$B = \frac{\ln\left(\frac{C}{n} e^{m\ln \frac{n}{n-1}} \left(1-e^{(r+1)\ln (r+1) - r \ln r - \ln(n-1)}\right) + e^{(m+1)((r+1)\ln (r+1) - r \ln r - \ln(n-1))}\right)}{(r+1)\ln(r+1) - r\ln r - \ln (n-1)}.$$ I'm not entirely sure how useful this comment will be to you (it's entirely possible I've made a mistake somewhere), but hopefully it can be of some use. 

This then puts MPostBQP[k] $\subseteq$ PP, for all $k$, and hence MPostBQP is no more powerful than PostBQP. 

Aside from the complexity arguments, there is another practical reason to want quantum computers. Much of the data processed on classical computers these days is derived from sensing the natural world (for example via the CCD in a digital camera). However, such measurements necessarily throw away some information about the system in order to render the measurement result as a classical bit string (for example collapsing spatial superpositions of photons), and it is not always clear which information will later be considered the most important when initially recording the data. It is, therefore, reasonable to believe that the ability to store and process quantum states directly, rather than collapsing them in some basis prior to processing will become increasingly desirable.