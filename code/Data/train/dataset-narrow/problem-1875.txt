PhysX requires CUDA support, and it looks like the Nvidia 8400 GS is too old for that. On the other hand, Nvidia seems to have released three different revisions of that model with different specifications, so it would be important to know which one of these revisions your card actually is. Nevertheless, it looks like all three revisions would be clearly outclassed by the AMD R9 270x. Given that Nvidia and AMD drivers don't always play nice together, I'd classify this set-up as "not worth the potential for trouble" and would remove the old Nvidia card and its drivers. 

As others have already commented, it does not look good. The noise indicates the disk is resetting itself over and over, trying to recalibrate. If it is visible to the computer but reports its total capacity as zero, it's how some disk models say "I'm failing my own internal self tests." From the video you posted I can see that it's a Western Digital WD3200BEVT, manufactured in January 2008. You said it was a 1 TB disk, but according to the label, its capacity is just 320 GB?? Anyway, Western Digital has a specific diagnostic program: Data Lifeguard Diagnostics. You can try it and see what it says; it might have some vendor-specific commands that generic utilities won't have. Generally, a 10-year old HDD is already past its expected end-of-life. If it lasted this long, you definitely got your money's worth off it in full. Don't waste time trying to fix it unless absolutely necessary for some reason. 

You did not mention whether your boot scheme is legacy BIOS or UEFI; with UEFI booting, there must always be one partition dedicated for the bootloader(s), which is different from the "Microsoft Reserved" partition that will be generated by default on new installations on both boot schemes. According to this MS TechNet document it is possible to not have a System Reserved partition in Windows 7. You might try disconnecting the failing HDD and then running the boot repair function from the Windows 7 installation media. Hopefully it can detect the Windows 7 installation on the SSD and reconstruct the bootloader + BCD with the existing partitions. For the future: it might be advisable to have only one disk (the intended system disk) connected to the system while installing Windows 7 or greater. By disconnecting the other disks for the time of the OS installation, you'd avoid the risk of Windows spreading itself to multiple disks. When the OS is confined to a single disk, the set-up should be much easier to clone or otherwise migrate to a new disk whenever necessary. If Windows gets to spread its component partitions to multiple disks, it will add unnecessary complications to maintenance tasks. 

To first switch into User B (using User B's password), you can use the plain old command. This will always prompt for User B's password. Then use to become root as usual. 

As far as SSH is concerned, you're logging in as the user. The daemon of Github will log the local identifier of the public key that was used in authentication, but beyond that, you don't have any personal account at the operating system level on the Github servers. The Github SSH service application itself is a different story. It sees the public key identifier reported by , and uses that to get your Github user details from the database that is also used by the Github web interface. As a result, the service will grant you appropriate access to your Github account, and no more. SSH deals with user accounts at the operating system level: it does not know about accounts set up within the context of specific applications. Your Github account is similar to e.g. a MySQL database account: it exists in the context of that application (or database engine) only. 

The "Stereo" device is technically known in Bluetooth jargon as "Advanced Audio Distribution Profile", or with an acronym A2DP. It is optimized for transferring 2-channel stereo sound with good quality in one direction only. The "Hands-free" device corresponds, respectively, to either the "Hands-Free Profile" (HFP) or "Headset Profile" (HSP). Both of them are older than A2DP, and were initially designed to pass telephone-quality sound (2-directional mono sound, limited frequency response, optimized for speech only). Both of them, and in particular the HSP profile, are designed to minimize the processing power requirements. This was important for wireless Bluetooth hands-free devices, which usually had, and still have, an extremely limited amount of battery power available because of limitations of physical size. Since then, HFP has been expanded with optional better-quality sound codecs, as newer high-efficiency processors have made it easier to implement more complicated protocols and sound codecs in low-power devices. HSP has remained the minimalist implementation, to be used when every milliwatt-second of battery capacity needs to be used efficiently, and as a fallback to be used if the more advanced protocols aren't compatible. The reason why the A2DP device gets disabled when two-way communications is required is probably again related to processing power requirements. Running both HFP/HSP and A2DP simultaneously would require running two potentially very different codec modules simultaneously, and then mixing their outputs either in digital or analog form. Digital mixing would increase processing power requirements still further, while analog mixing would make the device hardware design more complicated. Disabling A2DP while HFP or HSP is active is the easy way out: it will reduce both technical complexity and the power budget of the device. 

Adding the option to your SSH command line should get you closer to what you want: it causes the ssh client to go to the background, but only after the connection has been established and any password requests have been fulfilled, allowing to start once the SSH connection has been established. If starting the VNC server at the remote system takes a noticeable amount of time, you might want to add between the command line and the command line, to allow the VNC server to complete start-up before attempts to connect to it. Minimizing the terminal window would require sending a "minimize/iconify this window" command to the window manager of your local desktop environment. The details will depend on the type of desktop environment you're using, but in general, the commands or might be able to do it. Please see: How to hide or minimize X11 window from console? 

It is possible to make ask the password of the target user instead of the user that is initially running the command: I think SuSE even has/had this as a default setting, but in other Linux distributions and other unix-style systems that would be unusual. The setting for this in the file would be: 

You can also make this setting only affect a specific user. For example, to cause to ask the password of the target user only when userA uses the sudo command, you would set it in like this: 

starts the SSH agent process (equivalent to Pageant), and creates typically two environment variables: SSH_AUTH_SOCK and SSH_AGENT_PID (at least on Linux, don't know if Windows git-bash makes any difference here; probably not). If the SSH_AUTH_SOCK variable is set and points to a valid authentication agent socket, any process that can read the variable can use it. So you just need a way to have the value of this variable propagate from one git-bash session to another. The SSH_AGENT_PID variable is just a convenience, to allow the agent to be easily killed of if/when needed. If you can store those environment variables (or even just SSH_AUTH_SOCK) in a file, so that your subsequent git-bash windows can read it, you can script it in this way: Whenever a new git-bash is started (i.e. with the script or its git-bash equivalent) 

The devices belong to the device-mapper subsystem, and since it is the underlying mechanism that is used not only by both LVM and multipathing, but also by disk encryption and some forms of software RAID, that is not really possible. Run to see all the things device-mapper is currently involved in on your system, if you're curious. 

As jdwolf mentioned in a comment, a virtual machine that logs all file access would be a robust solution. Even if the malware can detect it's running on a VM, that's a pretty normal thing today. And if the VM seems slow, it may just be because the virtualization platform happens to be under a heavy workload. It is also possible to run a program through or similar debugging command, which records all system calls made by the program. However, a clever malware author can make their malware detect when it is run under any sort of debugging functionality (like ). Then the malware could behave innocuously that time and leave the bad stuff until later, when it's not run under debugging tools. 

Also, you might need to enter the SSH git URL in full form, that is, instead of just , you should type it like this: 

You're on the right track: the optimal solution would most likely to switch the Router B into "bridge mode" and disable its DHCP server function. Then the wireless network provided by router B would be effectively part of the same IP network segment as the wireless network and the wired connections provided by router A. There might even be a way to configure router B to "extend a wireless network" created by router A, using the wired connection between the routers as a backbone link. If both your routers have the necessary features for that, that might allow the most seamless transitions as you move from the wireless range of router A to router B or vice versa. Both routers would then use the same wireless SSID, but possibly different, non-overlapping radio channels. 

Since the SSH connection is from server A to server A, the encrypted part of the tunnel will be entirely within server A, and so pretty much useless. If the tunnel works, it means you can simply use to transfer files without making the tunnel: 

You have specified that Apache should listen to port 8443 only if is loaded (i.e. this Apache has the capability to use SSL/TLS), but in your configuration snippets, nothing seems to tell Apache to actually use SSL/TLS on port 8443. You do that by adding to the block that is supposed to use SSL/TLS: 

Since the essential file attributes like owner, size and timestamp are listed as question marks, this looks very much like some filesystem corruption. You might find more information about the input/output errors in the listing produced by the command. You might want to run the "check the disk for errors" GUI option or the command in Windows, since the command in Linux ntfs-3g is still somewhat less capable than the native Windows tools. 

Googling on "shlibtool" had this as the first hit: $URL$ It looks like might be auto-generated during the configure process if necessary and you have installed on your system. Do you have libtool installed? 

Your first two links are not about formatting the EFI partition, but about deleting the EFI NVRAM variables. These variables are not located in the EFI partition, but in the non-volatile memory of the motherboard - the same place that holds the system's BIOS settings. In Windows, if you have a system that is booting in full UEFI mode, you can open a command prompt as an administrator, and then type to see the some of the boot-related EFI variables and their values. But the presentation style hides the fact that there can be many more of these variables. If your system supports Secure Boot, there will be some standardized EFI variables related to that; if your firmware vendor so decided, all the "BIOS settings" might be accessible as UEFI variables. In , Linux offers an interface that reveals all those variables and allows the root user to modify them (as much as the firmware allows). And turns out that some early UEFI firmware versions will obey the command to delete all the UEFI variables, but aren't programmed to recover from the aftermath of such a situation. If those systems used the same type of battery-backed NVRAM as most desktops tend to do as EFI variable storage, those systems probably would have been/will be just as bricked when the system board's battery runs out. So the firmware bug that results in bricking actually works as a planned obsolescence implement too - unless an UEFI firmware update fixes it before the batteries get critically low. Formatting the EFI partition should be safe, as the system should certainly remain usable even if a failing system disk needs to be replaced. But if you have a system model which is known to have a buggy UEFI firmware, I'd check the firmware versions, and if newer versions are available, would probably upgrade first just to be safe, especially if firmware release notes say that the new version fixes important bugs. 

I agree with Claude Frantz. You could make a hotspot harder to detect by limiting its transmission power (if it has such a setting available). You could make it connectable by only you (and optionally people authorized by you) by choosing strong encryption settings. But none of this offers complete protection if the network technicians happen to have a directional Wi-Fi antenna (you can make one from a Pringles can!), a simple receiver and a signal strength meter. In a technically-oriented university, the network technicians might have talked to a professor at the local Radio Lab (or equivalent) and got to borrow some nice lab-quality equipment that can do the job. Or their secret weapon might simply be a cheap Wi-Fi signal detector from China: it might have shorter range, but in a dense hostel that might actually be an advantage. The only sure way to protect against this kind of location-finding efforts is to not be transmitting - that is, to not have an useful hot spot in the first place. If there is an "official" university Wi-Fi network that's built using enterprise-grade Wi-Fi hardware, the base stations of that network might even be configured to also detect and automatically report any unauthorized hot-spots to the network admins. If two or more base stations can hear the same hot-spot, they might even be able to automatically triangulate the position of the hot-spot and show it on the building floorplan. 

The first command reveals the PCI ID numbers of the NIC, and the second command has the NIC firmware version number in its output. If the NIC happens to be Intel 82579V (PCI ID 8086:1503), there's a known bug in it related to power saving states, that has also caused problems in Windows 8 and newer (i.e. with operating systems that are new enough to use the more advanced power saving states of modern hardware). An update from Ubuntu 14.04 to Debian 9 might have brought an equivalent update in power saving code of Linux, causing the bug to trigger. Intel has a specific firmware update tool for the 82579V chip that can even be used with NICs integrated on motherboards. Unfortunately, I think the update tool must be run in Windows.