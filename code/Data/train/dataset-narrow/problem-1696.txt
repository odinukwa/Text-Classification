I imagine you are not using user namespaces, in which case the capability CAP_SETGID isn't permitted in the docker container. You'll need to alter the containers capability sets to fix it. 

What you want is not HTB/TBF but HFSC. $URL$ You can attach netem to the leaf classes. Here is a sample script to get you started.. 

This represents the sector reported as producing an I/O error. The block size being 2k, sectors being 512 bytes. The additional one extra accounts for the starting sector offset for the partition. To correlate with SMART, we'll need to convert the value we now have into hexadecimal. 

If that looks likes its going to select all the files/directories inside of the selected paths. Note it must never select a path where "lastStable" or "lastSuccessful" is the last element of the directory path. Then you can change the command to 

Almost certainly to do with SELinux. I bet you moved your main.cf into that location. Try running to fix the labelling. 

The answer is you run , check if the setuid bit is set. If it is you'll run the program as the files owner, if not you'll run the program as you're own user. 

So to compare, in a scenario where you have 2 processes waiting to run - if you renice a process +10 it gets approximately 1/10th of the CPU time a priority 0 process has. If you renice it +19 it would get 1/100th of the CPU time a priority 0 process has. It should be noted you'll probably see your load at 1 at least during the duration of your pipeline. I imagine this would be a more elegant solution to your problem. 

The reason for this behaviour is the algorithm spends more time creating child processes than it does running the actual task (counting to 10000). Tasks not yet created cannot count towards the 'runnable' state, yet will take up %sys on CPU time as they are spawned. So, the answer could really be in your case that whatever work is being done spawns large numbers of tasks in quick succession (threads, or processes). 

This will clear the output. BE VERY AWARE: This could break the application. You are duly warned that here be dragons. 

You could try using chrt to change the scheduling policy of the tar program to SCHED_BATCH. As per the man page sched_setscheduler(2) 

You have a recursive error. You define a function called 'ssh' then call the same function in your function. This results in a recursive loop. Specify the absolute path to the binary and that should fix it. 

To cover these problems one needs a 'fencing' mechanism (lots of changing of infrastructure) to truly guarantee re-acquiring the lock in another host is a safe operation. 

Given the tables are there right now... Set the related and established rule on a per-protocol basis. 

This is due to selinux. Your script is not permitted to do a stat on as the type. This is by design. Depending on what you are trying to stat() (not just /dev/shm) can depend on what action you might want to take. If you the program to run in the label this will avoid SELinux restrictions, at the cost of no SELinux protections whatsoever for the script being ran. 

So for option 1, you might find you avoid all the semantics as the block allocation is done already (and quickly) because it was pre-allocated via and metadata is setup prior to the write. It would be useful to test if this really is the case. I have some confidence though it will improve performance. For option 2, you can use ionice a bit more. Deadline is demonstrably faster than CFQ although CFQ attempts to organize IO per-process such that you find it gives you better share of the IO through each process. I read somewhere (cant find a source now) that dirty_background_ratio will block writes against the individual committing process (effectively making the big process slower) to prevent one process starving all the others. Given how little information I can find now on that behaviour, I have less confidence this will work. Oh: I should point out that relies on extents and you'll need to use ext4. 

One way to do this would be to use to put the process into its own network namespace. This will effectively constrain the interfaces it can bind to such that it will only ever send out the interfaces exported over the namespace. This is equivalent to putting it on a VM with only one interface but 'cheaper' in the sense it doesn't require the additional VM overheads. You may also want to try putting your PREROUTING rule into the OUTPUT chain since these packets are generated locally (I assume) on the host. The routing is merely bypassed because netfilter does not analyze packets sourced locally in the PREROUTING chain. Also some iptables rules specifying the interface you permit and multicast through would be useful. 

(generally also called on x86_64) tends to map memory for zones outside of the standard 896MiB ranges directly kernel accessible on 32 bit systems. On x86_64 seems to cover all pages above 3GiB in size. contains a zone used for memory that would be accessible on 32-bit DMA devices, that is you can address them with 4 byte pointers. I believe is for 16-bit DMA devices. Generally speaking, on low memory systems wouldn't exist, given that covers all available virtual addresses already. The reason you OOM kill is because there is a memory allocation for a zone with 0 pages available. Given the out of memory handler has absolutely no way to satisfy making this zone have pages to use by swapping, killing other processes or any other trick, OOM-killer just kills it. I believe this is caused by the host VM ballooning on boot up. On KVM systems, there are two values you can set. 

I suspect your problem is in tomcat not apache, from the logs you have shown anyway. When you get 'error 110' trying to connect back into tomcat it indicates you've got a queue of connections waiting to be served that no more can fit into the listening backlog setup for the listening socket in tomcat. 

This seems to suggest to me that the numbers are counted in sector sizes (512) bytes. Given thats what value you'd end up getting if you right shifted a byte count by nine. This appears to be a similar setup between kernels 2.6.27 - 4.0.x anyways. 

It expects you to be in the correct base directory. Note it makes no assumptions as to what extension the file is or what the 'start' of the filename format is. Be careful. 

You need mod_rpaf. This module will rewrite REMOTE_ADDR in apache with another header, such as x-forwarded-for. Very useful for making PHP apps behave with load balancers. 

This is because by default, child processes inherit file descriptors of their parent. Since your web app has this port open, it trickles down for the request to the running apache web server. You need to alter your python code to do the following. 

It looks like you do not have support enabled for extended attributes on the filesystem. You may need to enable it in the kernel or mount with the 'xattr' option. On redhat systems it doesnt seem you need to explicitly set this flag, nevertheless gentoo might differ here. 

To manage the address, thats somewhat more complicated. And your liable to possibly cause the box to stop responding over the network! So be warned. For this, you'll need this policy module, it permits all domains to send/recv unlabelled packets. If you dont do this first you can lose network because by default nearly all domains have no permissions to send / recv unlabelled packets! 

If you do not want the content of your emails to be read, encrypt the contents body, not the means you use to transport it (SSL). Even if you do as @SYN says you lose control of the transport of the email as soon as it leaves your relay and hits someone elses. They may secretly relay it elsewhere and store it, for example. Email in its default form has never been secure and offers no expectation of privacy. Dont send anything through it you wouldn't be willing to shout down the street to someone.