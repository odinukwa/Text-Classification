Rule 1 of writing secure code: You must sanitize your user input. At the very least pass into you mkdir clause to prevent manipulating switches. Currently it could be used to create directories in arbitrary locations with arbitrary permissions. In of itself, it probably wont lead to a breach but you could pass stuff like: 

Also note that child pids can have independent limits. This program also sets a random limit on invocation of each child. 

If the problem is with the o/s configuration swapping out the drive does nothing. You cannot alter the state of a running O/S that simply without doing something like at least restarting services. With regards to replacing a faulting drive with a new one on USB for example that is possible to do. IF you create the USB disk as a volume group in LVM2 then you can use the pvmove command to achieve this effect. The process would be: 

This way PAM runs through and types properly which is actually very important if you want to restrict access to servies based off of IP, time or other factors of the account. Additionally if you want to make sure users inherit certain environment variables on login or disallow access to the server when SELinux is disabled. Its a common fallacy to think it runs through all the modules when PAM is enabled but password authentication is disabled. 

The problem here is not well documented but I've experienced it before. On 64 bit systems, the value you echo is not represented as a 16bit integer but a 32bit integer. Try replacing: 

Pot luck I guess if its better or worse. See the above environmental factors that might help/hinder. At the end of the day, if you are using volume management software or thin provisioning software at any point in the flow of data being written to media (on your host, at your VM level, at the SAN/Controller level) you can have no reliable expectation that the sequential read you are doing is really sequential or that the media you are writing to is consistent (if its a fast disk or data was moved to a slow disk). Virtualization is so powerful because it adds a layer of logical abstraction to a host. But it can also be potentially horrendous to perform any reliable degree of capacity management on them because of that layer of abstraction. 

When you setup a fileset in bacula, it will literally read the pathspec line-by-line and back up like this. It wont create two threads to read the different file paths in the agent. As @SpacemanSpiff said, if you wanted to to do this, the way forward would be to setup different jobs, one for each filespec you wanted to backup. 

This relies on the system call. You need at least a 3.0 kernel and glibc-2.14 for this to work. RHEL 6.5 provides support for persistent namespaces but not support for moving existing processes into new namespaces. 

You are aware that you can move into the role as right? For example, the following below would work. 

Of note is that it will tell you where the superblock locations are. Using this information, attempt to mount the drive using an alternative superblock.. 

This particular setting falls under the influence of the network namespace that docker runs in. As a general rule does alter settings that are relevent systemwide, technically speaking however you are altering settings in which returns results on a per network namespace basis. Note that is actually a symlink to as it really does reflect the settings of the namespace that you are doing the work in. 

Most ISPs filter the netbios ports. If you nmap from your windows client you'll probably see the port is listed as because your ISP is blocking it. Try configuring a tunneling protocol such as and try using this setup instead. 

The closest thing I can think of (on redhat based systems) is rpm -aV. This is the verify portion of rpm, which will attempt to check md5 sums, files modes and ownerships are correctly placed for files that are listed as part of the package being inspected, with -a you can do every package which makes up the system. 

It didn't kill rdiff-backup, it should have but its is -1000. This is caused by a bug in sshd. The bug is fixed but wont be available until the next release which is openssh 6.5. sshd fails to set the oom_score_adj of new shells it creates back to 0 if you reload it, causing all child processes you spawn via SSH (so your bash shell and any child processes that creates) to have -1000 and subsequently can hog all the memory without oom-killer killing them. The quickest way to fix this is to (assuming 7567 is the pid of sshd like in your case):- 

This sets up an ACL which will permit both user "user" and user "apache" the same level of access regardless of what standard file permissions say. In addition new files and directories created in this directory will inherit the same permissions. 

Decrypt the keyfile and use that. This offers permanent graceful restart of your services which is what you are after. But the key then is unprotected, unencrypted stored on your server. Give the service provider passwords to your certificates. This might be ideal however you need a level of guarantee that the service will be restarted with the prompt on startup. Be that they may have their monitoring software alert them for this event, however you should under these circumstances be open to situations where the key password is not entered, either at all or with a delay in the password being entered by the service provider. It will be down to you to negotiate what terms the service provider will offer you this service and what guarantees of success they will provide. If they will provide it at all. Some http servers, such as apache provide a option called SSLPassPhraseDialog which will attempt to decrypt the key using an written program that will pass the correct passphrase out when executed. This can be a shell script or program of your choosing that will do this. This has the benefit of giving you option no 1, but acts merely as a weak barrier to all the problems described in option number 1. In addition, depending how careful you are (or not) supplying a password executable this might provide a mechanism for someone to get something other than the passphrase executable to run at startup. Have your hosting provider require they supply with with a scheduled downtime (if its them doing the rebooting) so you can be ready to insert the password in yourself, or alter the updates to run at time where you can be ready to enter the pass phrase to the certificate promptly. 

You can actually do this, but you'll need to use the NF_LOG iptables target and write a program that specifically covers your needs. NF_LOG will send packets to a receiving application which (given how quickly you manage to do what you are doing) would count the packets are a per-millisecond level. You can avoid writing out every packet doing this and truncating the data out to ensure you can attempt to get the 1 million pps you need. The issue however, is no such receiving program exists -- you'd need to write one. Having done something with NF_LOG before and your statement that LOG slows down the traffic I suspect its the writing of the packets to disk that probably is sowing things down. I still suspect a million packets per second is still a high bar to achieve. Another alternative (which isn't realtime) is to use tcpdump to capture the packets and as a post step later use a program to read the pcap file and make the appropriate millisecond calculations. 

This wouldnt be smoothwall compatible but.. iptables contains a 'quota' module you can use to implement this. In order to make use of it you'd need to identify each connection in. If your doing simple natting you could potentially do that with identifying each connection by ip, but thats really trivial to spoof. If you want something more definite you could possibly use pppoe to authenticate to identify users then setup relevent firewall rules against that connections device. That can be done using scripts you can get pppd to call to configure quota for that connection. This would work regardless of ip selected by the user. 

You must open in append mode. If you want to leave it to run outside of the terminal you can just disown the job with "disown %1" If you need timestamps too, theres a couple of ways. An easy way is to use inotifywait to check if the file gets an update, then append your date to it. Itd be easy to do the above and this into a shell script and perhaps disown that instead so you get 

Authentication failed because credentials were sent but the credentials were invalid. Authentication succeeded using the credentials presented. Authentication is ignored because no credentials were supplied. 

So, now you know how, I must point out that your developer is doing it wrong. If non-blocking sockets are something they want to use, thats fine - however they should setup an on the socket and block on the poll instead. The program gains nothing from on a non blocking socket that produces -- as a matter of fact, the result is worse because nearly all system calls are a preemption point where the kernel can context switch you anyway. This developer is wasting power, CPU cycles that could be used for idling threads and is not actually gaining any benefits he/she things they are from doing it this way. If the developer wants to be 'cache-line' friendly, pin his tasks to a particular CPU and be done with it. 

The way this works is that you can hot-add memory to your server up to the available memory. Your system however is actually given the current memory. When a KVM VM boots up, it starts with the maximum allotment of memory possible to be given (the available memory). Gradually during the boot phase of the system KVM claws back this memory using its ballooning, leaving you instead with the current memory setting you have. Its my belief thats what happened here. Linode allow you to expand the memory, giving you much more at system start. This means that there is a zone at the beginning of the systems lifetime. When the hypervisor balloons it away, the normal zone rightly disappears from the memory manager. But, I suspect that the flag setting whether the said zone is available to allocate from is not cleared when it should. This leads the kernel to attempt to allocate from a zone that does not exist. In terms of resolving this you have two options.