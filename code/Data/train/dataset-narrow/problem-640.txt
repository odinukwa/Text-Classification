This is only a guess but here it goes... The only way I know that a PRIMARY KEY for a MyISAM table would be OK and all secondary indexes be out-of-date would be under this unique circumstance: Running this 

The script will stop the slave, load the data, and start the slave. What about the log file and position? Before you do STEP02, look at line 22 of 

Given this display, it is impossible to grant RELOAD, SHUTDOWN, PROCESS, FILE, SHOW DATABASES, SUPER, REPLICATION SLAVE, REPLICATION CLIENT, CREATE USER, and CREATE TABLESPACE to a database-specific user. For example, you don't want a database-specific user to 

Having a VARCHAR as a part of an index would be cumbersome to manage although doable. I would design the tables as follows 

Stop the server, if it's running. If you have the space to do so, copy the whole cluster data directory and any tablespaces to a temporary location in case you need them later. Note that this precaution will require that you have enough free space on your system to hold two copies of your existing database. If you do not have enough space, you need at the least to copy the contents of the pg_xlog subdirectory of the cluster data directory, as it might contain logs which were not archived before the system went down. Clean out all existing files and subdirectories under the cluster data directory and under the root directories of any tablespaces you are using. Restore the database files from your base backup. Be careful that they are restored with the right ownership (the database system user, not root!) and with the right permissions. If you are using tablespaces, you should verify that the symbolic links in pg_tblspc/ were correctly restored. Remove any files present in pg_xlog/; these came from the backup dump and are therefore probably obsolete rather than current. If you didn't archive pg_xlog/ at all, then recreate it, being careful to ensure that you re-establish it as a symbolic link if you had it set up that way before. If you had unarchived WAL segment files that you saved in step 2, copy them into pg_xlog/. (It is best to copy them, not move them, so that you still have the unmodified files if a problem occurs and you have to start over.) Create a recovery command file recovery.conf in the cluster data directory (see Recovery Settings). You might also want to temporarily modify pg_hba.conf to prevent ordinary users from connecting until you are sure the recovery has worked. Start the server. The server will go into recovery mode and proceed to read through the archived WAL files it needs. Should the recovery be terminated because of an external error, the server can simply be restarted and it will continue recovery. Upon completion of the recovery process, the server will rename recovery.conf to recovery.done (to prevent accidentally re-entering recovery mode in case of a crash later) and then commence normal database operations. Inspect the contents of the database to ensure you have recovered to where you want to be. If not, return to step 1. If all is well, let in your users by restoring pg_hba.conf to normal. 

That way, each pma database can be specific to the local DB instance. As for writing, you must use SUPER privilege to bypass the read_only restriction. Only other alternative is to drop the read_only option from /etc/my.cnf and restat mysql. 

This index is needed because the query has and with static values, while the has a range that that is already ordered. As for CPU usage, please add this to my.cnf (Restart required) 

to see which session settings differ from the global settings. For example, here is what I am running 

Giving more consideration to how you plan to retrieve data, how much data you need in a single query, and how you structure the query will become the determining factor as your seek good performance. EPILOGUE You are not going to get better performance because of the cost to make the JOIN happen. It's like trying to turn lead into gold (Theoretically possible, practically and financially impossible). Your best bet is to leave the table in its original state. 

Now, if you cannot take mysql down, you will have to do a bait-and-switch on s_relations. Just login to mysql and do the following: 

If the tables involved in the JOIN share identically named fields, then you can use the USING clause of the JOIN syntax. For example IF the actions table had action_id as the primary key instead of id and the types table had type_id as the primary key instead of id THEN the query could be rewritten as 

Both indexes start with , you can increase secondary index processing by at least 7.6 % getting rid one index out of 13. You need to eventually run 

Master and Slave should match in terms of who they transmit data, especially BLOB data. UPDATE 2013-07-04 07:03 EDT From your messages concerning the relay log, it looks like you have the following 

see what character set phpmyadmin operates in see what character set mysqld operates in see what character set remote database servers operates in 

If you want to copy to an already existing table , and the two tables have identical structures: APPROACH #6 

Since you started mysqld_safe to start mysqld, you just need to start mysqld_safe and explicitly choose a separate file for the error of the other MySQL Instance using the - option. 

You may find this surprising, but mysqld actually determines the correct value for you upon startup. The MySQL Documentation says the default value for is 0. Yet, when you run the command , it does not come back with 0. It returns what mysqld would comfortably run with. The MySQL Documentation says the maximum value for is 65535. I have seen some systems with the setting above 100000 upon startup (132332 to be exact). That being the case, there are some instances where you cannot set any higher. Just run the command I mentioned 

Your mission, should you decide to except it, it to carry out these steps with the hopes that Step 04 is not needed. As always, should any of your data be caught or killed, the DBA StackExchange will disavow any knowledge of your actions. This is not a tape, so it will not self-destruct in 5 seconds. UPDATE 2011-10-19 12:50 EDT I like your option 3. It seems the quickest and dirtiest. Here is how you can do this: Step 01) mysqldump everything to /root/MySQLData.sql Step 02) Drop all databases Step 03) add this to /etc/my.cnf 

If you want, you can change the format thereafter. UPDATE 2014-02-26 16:06 EST Read your last comment Just subtract the difference from 10 min (600 sec) 

Setting ft_min_word_len only affects MyISAM. You need to set innodb_ft_min_token_size to 1 since the default is 3. Once you set innodb_ft_min_token_size to 1, go back and do 

Since Master_Log_File and Relay_Master_Log_File are the same, why not use Master_Log_File? REASON #1 : SQL Error in the Slave When an SQL Error happens in M2 on the SQL Thread, Replication stops processing SQL from the Relay Logs. The IO Thread continues downloading new SQL entries from M1 and appending to M2's Relay Logs. REASON #2 : Long Running SQL Query in the Slave If a query comes along, such as that updates every row in a table and it takes 5 minutes. During those 5 minutes, the IO Thread continues downloading new SQL entries from M1 and appending to M2's Relay Logs. Given both reasons, there are some occassion where and are different. In that instance, always use . In light of this, there is another reason: REASON #3 : Log Corruption in Slave or Master There are rare times when bad network transmission will produce corrupt relay logs. When you do , an error message will explain the the binary log may be corrupt. You should start firsdt with the relay logs. To clear them out and start with fresh set, do , get the (RMLF), the (EMLP), and run this on M2 

Suggestion #2 Just change in Then Epilogue Don't restart mysql just yet. This problem is one the Master. STEP 01 : Stop all writes to the Master STEP 02 : Go to the Slave and run STEP 03 : Restart mysql on the Master STEP 04 : On the Slave, run this 

The confusion is caused by the double quotes you are using for the sudo and for the SQL GIVE IT A TRY !!! 

You are basically going to drive mysql crazy because of how the query gets evaluated. Look at the query again 

Each MYD requires a file handle. Each MYI requires a file handle. Thus, you can have 60 file handles open. Here is what you can do to see how many file handle open up after a mysql restart 

The bug may involve the character set since the last line of the SUBSTRING_INDEX Documentation says . Perhaps SUBSTRING_INDEX may be OK after all, but that's me messing around in Windows. Run the script 

When you run this, all binlogs are erased and is created. ALTERNATIVE: Shutdown mysql, delete , and startup mysql. METHOD #2 

What was net_store_data doing at that moment ? The book Understanding MySQL Internals says something about net_store_data and NULL values on Pages 74,75 under the heading Server Responses subheading Data Fields 

This will take the longest CAVEAT : you do not have to run it all. You could cut and paste a few lines at a time into mysql and convert a few tables at a time. If you would like to reverse InnoDB tables back to MyISAM, just run the process backwards: 

I agree with @RemusRusanu (+1 for his answer) in InnoDB behaves like a transactional storage engine should. Compare it to MyISAM. MyISAM If is a MyISAM table, launching is just like running . This triggers a quick lookup of the row count in the header of the MyISAM table. InnoDB If is a InnoDB table, you get hodge-podge of things going on. You have MVCC going on, governing the following: 

Either the rules have changed, or somebody missed something in compiler builds. According to the MySQL Documentation on ENUM 

The is referred to as a non-client thread in the MySQL Documentation about As far seeing localhost:#####, that number after the colon is a really a port number within mysqld assigned to localhost. UPDATE 2012-04-27 18:00 EDT Questions from your comment 

Running OPTIMIZE TABLE (reduces table fragmentation and recreate indexes) may not be necessary unless you do heavy INSERTs, UPDATEs, DELETEs. I'll say it is optional. Give it a Try !!! 

EXPLAIN was meant to do queries, not functions or procedures. When it comes to procedural code, you must think like a developer all over again. For what purposes would you optimize code, especially for a database ? 

I am sure that there are patient records have the DOB, addresses (zipcodes), and medical histories in the database. I am sure there are queries (SELECTs, JOINs) that can collect this information. This retrieval is what marketers could really use. With said information, marketing firms could search for and decide upon 

That's the Hit Rate since Uptime (Last MySQL Startup) There are two things you can do to get the Last 10 Minutes METHOD #1 Flush all Status Values, Sleep 10 min, Run Query 

If you get the same kind of message, then you have corrupt table issues in terms of the InnoDB data dictionary. If you get data, then you were simply in the wrong place at the wrong time. In your case, you were in the wrong database at the wrong time. 

The SQL thread reads binlog events of the master from the relay logs. Doing sync-binlog only flushes master logs. By flushing relay logs and the associated replication status files, this should make reading replication status and binlog events from the relay logs more accurate. Here is the MySQL Documentation for these options 

If you issue , it will happen once the rows in the table have released their locks at the end of the transaction. You could experiment (on a Dev/Staging Server, please) with using transaction isolation level to make the delete logically happen, but only on commit will it become visible and recorded permanently. In the second transaction, basically all bets are off. If you run 

As stated, if the . You may simply have lots of dirty pages in the Buffer Pool that need flushing to disk. You should be monitoring Innodb_buffer_pool_pages_dirty. There are two things you could do to improve the situation: IMPROVEMENT #1 : Upgrade to the latest MySQL I trust MySQL 5.5. I have a client going to MySQL 5.6.10 soon. I trust it as well. These versions of MySQL have the InnoDB Plugin standard. They flush dirty pages much more efficiently. You can also tune InnoDB. Under MySQL 5.1, there are 4 read IO threads and 4 write IO threads. MySQL 5.5+ allows you to increase these for better read and write InnoDB performance. InnoDB For MySQL 5.5.+ can access multiple CPUs/Core. MySQL 5.1 can do this if using MySQL 5.1.38+ and you install the InnoDB Plugin (IMHO too messy, go with MySQL 5.5/5.6). MySQL 5.1.27 cannot do this. IMPROVEMENT #2 : Get Dirty Pages to Flush More Frequently You can do this immediately with 

As long as you do not perform any INSERT/UPDATE/DELETE statements directly on the Slave, your Slave should be just fine. Otherwise, MySQL Replication could break if you INSERT an new row in mydb.mytable on the Slave and, via Replication, the Slave later detects an INSERT of a row to mydb.mytable with the same PRIMARY KEY. This produces error 1062 (Duplicate Key). The only way you could write to Slave without breaking MySQL Replication is this: 

mysqldump the data from the 20GB Instance Create a New 10G Instance load the mysqldump into the new 10GB instance verify all the data you want is present destroy the old 20GB 

Once the TokuDB storage engine was defined, it becomes responsible for updating the INFORMATION_SCHEMA database. The first thing you should do it test TokuDB's INFORMATION_SCHEMA functionality. First, run this query at a mysql client prompt: 

What's the reason -b (broadcasts) is 3 on one server and 4 on the other? In case both servers are restarted, the server with the lowest -b will bring ucarp first. As for -r, that is dead ratio. Thus, DBServer1 will come up in 15 seconds (3X5) and DBServer2 comes up in 20 seconds (4X5). Let's say DBServer1 crashes. ucarp on DBServer2 will check for 20 seconds for a handshaking fro ucarp on DBServer1. If nothing is detected, the vip-up.sh script on DBServer2 will take control of the DBVIP. OK all of this is just for failover and DBVIP management. What about the PostgreSQL database? Surprisingly, PostgreSQL is only running on one server at a time. Whoever has the DBVIP should have DRBD in Primary state. This ties back in to the vip-up script. How do you script it? Here is /usr/local/sbin/vip-down.sh script (conceptual)