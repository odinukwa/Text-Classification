I am no expert in offline renderers, but I'll give it a go until someone else comes with a better answer :) Shadow rays intersecting area light It is very common to assign an extent to your rays. How to do so? Think about the parametric formula of a ray: $$R(t) = O + t\vec{D}$$ it should immediately jumps out that how far $R(t)$ can be from the origin $O$ depends on how much we can travel in the direction $\vec{D}$, and this travelling is clearly governed by the parameter $t$. Your problem here is that you don't want to let the ray travel indefinitely, but you want to stop just before it hits the source of the light, because otherwise, as you correctly noticed, everything will result occluded. So how do you limit how much your ray can travel? You just have to limit the maximum value that $t$ can take. You have two option: 

Render your geometry and all the information needed for shading into multiple render targets. This means that typically in a basic implementation you would have a depth buffer, a buffer containing the normals of your geometry and the albedo colour. You'll soon find that you need other information of the materials (e.g. roughness, "metallic" factor etc.). 

I am well aware that there is no way to eliminate completely these problems, but is there a way to improve bilateral upsampling to partially improve on one of the two points above? And as side question, is there an equivalently good algorithm or variation that is cheaper? 

If you are interested in the subject (and to add some reference to this post) an excellent read is: Discrete Differential-Geometry Operators for Triangulated 2-Manifolds [Meyer et al. 2003]. For the images I thank my ex-professor Niloy Mitra as I found them in some notes I took for his lectures. 

Now while the option above is probably the first thing that comes to mind, it is wasteful as you'd need to both have $L_p$ around and perform some unneeded arithmetic operations if you have your light vector $\vec{D}$ as unnormalized around. In fact, this is none other than $L_p - O$ and in a case like this, the Light lies at exactly $t == 1$: 

From [McGuire et al. 2012] They divide the scene in tiles, assuming that each of these have a dominant velocity. Then, starting from the full res velocity buffer, they locate the velocity vector with higher magnitude for each tile, producing a downsampled version of the velocity buffer where 1 pixel = 1 tile. Using this buffer, in another pass, they can generate a buffer that stores for each pixel/tile the dominant velocity among the neighbourhood pixels. Note that the tile size is based on the max blur radius. Using these buffers they then proceed with the gathering step sampling in the neighbourhood dominant velocity direction as this is the most plausible direction of the blur for each sample (as I said, the tile size is the max-blur radius). The weighting used for the blur takes into account the full res velocity and depth informations. Note that the paper propose three different weighting for three different scenarios (blurry over sharp, sharp over blurry and blurry over blurry). Another important advantage of the method is that you can early-out if the dominant velocity in the neighbourhood is smaller than a certain threshold. 

A set of techniques to avoid explicit ordering go under the name of Order Independent Transparency (OIT for short). There are lots of OIT techniques. Historically one is Depth Peeling. In this approach you first render the front-most fragments/pixels, then you find the closest to the one found in the previous step and so forth, going on with as many "layer" as you need. It is called depth peeling because at each pass you "peel" one layer of depth. All your layer can be then normally recombined from back to front. To implement this algorithm you need to have a copy of the depth buffer. Another set of techniques are the blendend OIT ones. One the most recent and interesting one is the Weighted Blended OIT proposed by McGuire and Bavoil. It basically apply a weighted sum for all the surfaces that occupies a given a fragment. The weighting scheme they propose is based on camera-space Z (as an approximation to occlusion) and opacity. The idea is that if you can reduce the problem to a weighted sum, you don't really care about ordering. Other than the original paper, a great resource for implementation details and problems of Weighted Blended OIT is in Matt Pettineo's blog. As you can read from his post this technique is not a silver bullet. The main problem is that the weighting scheme is central and it needs to be tuned according to your scene/content. From his experiments, whilst the technique seems to work fine for relatively low and medium opacity, it is failing when opacity approaches 1 and so could not be used from materials where big part of the surface is opaque (he makes the example of foliage). Again, all come down to how you tune your depth-weights and finding the ones that fit perfectly your use-cases is not necessarily trivial. As for what is needed to for the Weighted Blended OIT, nothing more than two extra render targets. One that you fill with the premultiplied alpha color ( color * alpha) and alpha, both weighted accordingly. The other one for the weights only. 

and if I may add, you should expect some as well from time to time, so it's always good to account for these two special cases. "Bright pixels" With that, I assume you are talking about what is commonly referred to as fireflies. Fireflies avoidance is indeed quite a big topic and probably warrant its own question. However, just briefly. Fireflies typically occur when you lots of your paths end in areas with low light and very few paths end up in very bright light. There is lots of material on how to fight fireflies, a short google search will lead to a lot of techniques depending on your case. Just to name drop a few: 

Also, generally increasing the size of the light source tends to help as well. Remember, the goal is always to hit those bright spot with higher probability! A better solution to the problem is probably to use something like Bidirectional Path Tracing as this will have sub-paths starting from the light source as well, increasing the likelihood of sampling the bright areas. Another cause of fireflies are caustics, but I don't want to make this answer longer, so if you are interested this is well worth an additional question. 

Yes, but you need a paradigm shift. What you are accustomed is called forward rendering. You submit your geometry and then you proceed immediately with the shading pass. In the basic forward rendering you can either loop inside the shader for each light or perform one pass per light and blend the result together (with additive blending). But things have evolved quite a lot. Enters: Deferred Rendering Now that are so many variants that describe them all in details will take way more than acceptable for an answer here. So here I am just going to describe the gist of Deferred shading, there are plenty of other resources that you can easily find using google, hopefully after reading this you'll have the right keywords to find what you need. The basic idea is to defer the shading to after down the pipeline. You have two main steps: 

$$t_{light} = \frac{L_{p}[i] - O[i]}{\vec{D}[i]}$$ where $[i]$ represents the coordinate of your choice. Given this, you want to stop your shadow ray to travel just before $t$ reaches $t_{light}$. If it goes over this value, report an unoccluded result. 

Generally edge detection boils down to detect areas of the image with high gradient value. In our case we can crudely see the gradient as the derivative of the image function, therefore the magnitude of the gradient gives you an information on how much your image changes locally (in regards of neighbouring pixels/texels). Now, an edge is as you say an indication of discontinuity, so now that we defined the gradient is clear that this info is all we need. Once we find the gradient of an image, it's just a matter of applying a threshold to it to obtain a binary value edge/non-edge. How do you find this gradient is really what you are asking and I am yet to answer :) Lots of ways! Here a couple :) Built in shader functions Both hlsl and glsl offer derivative functions. In GLSL you have dFdx and dFdy that give you respectively gradient information in x and y direction. Typically these functions are evaluated in a block of 2x2 fragments. Unless you are interested in a single direction, a good way to have a compact result that indicates how strong is the gradient in the region is fwidth that gives you nothing else but the sum of the absolute value of dFdy and dFdy. You are likely to be interested in an edge on the overall image rather than on a specific channel, so you might want to transform your image function to luma. With this in mind,when it comes to edge detection your shader could include something to the like of: 

Probably the most intuitive one is to compute at what $t_{light}$ your light source lies. By using the same parametric formulation of the ray, if $L_p$ is the point you've chosen on the area light, then $t_{light}$ will be: 

Again, the amount, type and content of the buffers used varies quite a lot among different projects. You will find the set of buffers with the name of GBuffers.