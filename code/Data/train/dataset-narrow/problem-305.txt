SW-1, SW-2, RTR-1 and RTR-2 are all OSPF neighbors in Area 0. Both RTR-1 and RTR-2 are ASBR's and injecting BGP learned routes into OSPF. Each router is advertising routes into the WAN for its respective site (as well as pre-pended routes for the other site for redundancy). Routing traffic from Site 1 to Staging at Site 2 is easily accomplished by simply redistributing the static route to Staging on SW-2 into OSPF with a higher metric. Since that route gets advertised by RTR-2 into the WAN, RTR-1 will learn that route and redistribute it into OSPF with a metric of 0. The OSPF route learned on SW-1 from SW-2 would have a higher metric, thus routing would be preferred over the WAN. Return traffic from site 2 also needs to flow this way so that we avoid asymmetric routing. FBF is applied on the inbound interface (Link 4) entering SW-2. This filter will take all traffic sourced from Staging (10.100.190 /24) and make the next-hop RTR-2. This portion of the FBF is working, as I have tested in the lab. Since RTR-2's preferred route back to Site 1 is via Link 1, we need to apply FBF once again at the inbound LAN interface of RTR-2 (facing SW-2). Here's the problem... When FBF is applied on that router, OSPF adjacency with SW-2 breaks. QUESTION: Why is OSPF adjacency breaking between RTR-2 and SW-2?? Configuration for RTR-2 and SW-2 are attached: RTR-2 Configs 

Pretty much all bigger platforms have this type of low-level captures for transit packets, which are exceptionally useful when you need to verify HW is doing what configuration says, sometimes there are software defects and it does something else than expected. I know that in GSR you can see transit in memory, in Juniper Trio there is quite nice tool for it as well. Brocade can do it. It's quite baffling they are not documented in vendor pages. 

If they are used interchangeably then they are used incorrectly. Subnet refers to particular IP network, such as 192.0.2.0/28 VLAN refers to 802.1Q standard, in which you can essentially give each port unique MAC address table, effectively separating them from each other. VLAN may transport one or more subnet (but does not have to, it may be transporting something else than IP entirely). Subnet may be configured for VLAN, but does not have to be, it could be without 802.1Q or over some completely different L2 technology than ethernet. 

You can however configure two helper-addresses and filter the requests at the DHCP server, so that 'old' excludes replying to those 5 MAC addresses and 'new' only replies to them. But maybe you simply need more isolated lab environment, at least separate logical interface. You could also create interim DHCP server, which forwards requests further based on the MAC addresses, it wouldn't be very hard exercise, probably 2-3h including research for me. But would you want your production to rely on this new script, I certainly would not want it to. 

Here we have unique local addresses, as defined in RFC 4193. These are routable across subnets, and even across wide area networks, but they are not allowed on the public Internet. This makes them usable as private addresses for any network from the smallest home network to the largest global enterprises. A ULA prefix is always a /48 within fd00::/8 (technically fc00::/7 but the other half is meant to be assigned by IANA) and is meant to be constructed by an algorithm given in RFC 4193, but can be done randomly. It must not be assigned from zero or in any other pattern. If a /48 is not large enough, or two companies merge and need to connect their networks, multiple /48s can be used and the appropriate routes created. The 2^40 space of possible subnets is 256 times the number of possible IPv4 addresses, making collisions extremely unlikely if the ULA prefix is generated properly. The three addresses shown above are deprecated RFC 4941 privacy addresses. 

Let's look at a live example. This being from my Linux workstation. (And for simplicity I actually omitted a couple of addresses.) I'll explain each of the addresses in turn: 

Next we get to global IPv6 addresses. These are routable on the public Internet, and subject to firewall rules, bidirectional communication is possible. That means that anyone (with IPv6) can initiate a connection to this system. Because NAT is not in use, the connection is direct, without any need for port forwarding hackery. But a host or network firewall may still prohibit the connection. But these IPv6 addresses are also RFC 4941 privacy addresses. These addresses are created at configurable intervals, and then the previous privacy address is deprecated. Once an address is deprecated it is no longer used for new outgoing connections. After another configurable interval, the deprecated address is automatically removed from the interface, and no communication is possible at all. Privacy addresses are meant to protect the privacy of the end host, especially as it moves from one network to another. SLAAC addresses configured using modified EUI-64 always have the same last 64 bits, regardless of what network they connect to, so a host (such as a laptop) could be tracked as it moved across networks by correlating these. Privacy addresses eliminate this problem. 

OBJECTIVE: Any traffic destined to Staging from Site 1 should route via Link 2 into the WAN and NOT via Link 1. Since Link 1 will be saturated with replication traffic between the two data centers. 

As legioxi mentioned: You will need to have proper segmentation. If your switch does not do L3, consider a router on a stick approach. LAN interface of router would connect to switch port set up as a trunk. Configure sub-interfaces on the router for each VLAN and DOT1Q tag the traffic. Only trunk the necessary vlans from the switch that you need. At this point you should be able to implement ACL's to permit/block traffic from hosts in one VLAN to another. 

FWIW, I have a very similar setup, SRX240 to partner's ASA 5510. Took us many hours of working with JTAC and Cisco TAC to get it working. In the end, we ended up going with Policy-Based VPN vs Route-Based. For some reason, SRX's only seems to do RB better with SRX or Screen OS devices. If you plan to on going from SRX to any other vendor, using route-based, good luck. Save yourself the time and headache and go policy based. I know it's not preferred as you will have many more lines of code in your policies. If you're still sticking with RB, definitely check out the Juniper SRX Forum: $URL$ $URL$ In case you change your mind, here is our PB VPN config. Unfortunately I don't have ASA side since we do not manage it: 

I think best technical rationale for P2P is that you get reliable LSP delivery, in LAN it is unreliable thus it's done periodically. Also pseudonode is additional resource use and during DR failure convergence is hurt. 

Rationale here is, that we want to leak over the borders to reduce the effect of inaccurate polling intervals at your switch. You'd then plot the s1, s2, s3 and you should have much more smooth/accurate result than what you are seeing now. However I'm sure this is not novel problem and I'm sure there is formal solution how to recover optimal accuracy, unfortunately producing that solution is out of my skill set. Something math.stackexchange people would be better equipped to tackle. 

I don't think egress policing works on this platform, but you'd need to use SRR, and frankly shaping is always preferable when possible. Enabling 'mls qos' willy-nilly on 3750 can be recipe for disaster, the defaults are horrible, e.g. EF gets policed at 4%. So you should at least read: 

@mellowd is certainly right, these switches are not very usable DC switches, due to very limited buffers they will microburst and drop traffic. Consider you have 2 * 1GE ingress and 1 * 1GE egress. Worst case scenario is, that egress port starts dropping after the ingress ports have sent at the same time for 2ms. Best case scenario is, that you can handle 8ms burst. You have 2MB of egress buffer per 4 ports, so 2MB/(1Gbps/8) = 16ms maximum and 16/4 = 4ms minimum. Divide that number by amount of ingress ports wanting to send, and you get the number of how long you can handle it. That is, the more ingress ports (servers) you add, the less microbursting you can handle. If you must live with 3750/3560, you should read this document to maximize buffer use. And if you're still dropping use LACP on egress, even though your graphs show that average egress demand is very low. To prove to your managers that the buffers are insufficient monitor/tap/span your current networks switches all downlinks, then you'll have timestamps and packet-sizes going to egress and you can calculate how much over 1Gbps your instantaneous demand is and how much buffer you'll need to handle it. 

This is a fun one. It's also a global address, but it's an RFC 7217 stable privacy address. These are not simply random, as with RFC 4941 addresses, but generated using a PRNG fed a secret key combined with host-specific information and the assigned IPv6 prefix. They are still unpredictable, but once created, they remain the same for any given IPv6 prefix. Ordinary privacy addresses have the problem that a host using them has no fixed address at which it can be reached when it is on a given network. So incoming connections are impossible. This wasn't really the point, but more of a side effect (RFC 4941 section 2.4 discusses this in detail.). If you want incoming connections, but still want privacy when the host moves around, then stable privacy addresses come to the rescue. The host will get a different interface identifier when it moves to another network, but when it goes back to its original network, it will get the same interface identifier it had previously. So, on this system, outgoing connections use the RFC 4941 privacy addresses, which are rotated at intervals, while incoming connections use the stable privacy address, which is published in the global DNS. 

There are no subnet directed broadcasts in IPv6 because there are no broadcasts in IPv6 at all. One option is to send the packet directly to the IPv6 address of the host (which you of course will have to know in advance, just like the MAC address) and hope that the switch still remembers which port the host is connected to. This seems quite iffy to me and is probably not going to be reliable in all environments. Long-term the workstations should probably be waking themselves up on a schedule, or not sleeping at all. 

That would be 'sysctl -w net.inet.ip.forwarding=1' I don't have QNX to test, but I don't think you need to add any routes, as the connected interfaces already have network, i.e. route already exists. But confirm with 'netstat -r' 

You don't configure multicast address anywhere. Some box sends traffic out with multicast destination, this will automatically get L2 multicast address on the link. This L2 multicast address is normally broadcasted in L2, unless there are some specific features enabled, like IGMP snooping to stop the flooding. If IGMP snooping is enabled, then the L2 switch will learn which ports want to receive the multicast (they do IGMP join to the multicast group) and will not flood traffic to other than interested ports. If L3 is involved, then you'll learn via PIM which ports are interested in receiving the multicast. I can recommend this book for multicast basics, it's not JNPR specific. If you don't know which addresses to use, if you have 2byte ASN, you should use 233.0.0.0/8 GLOP block (with your ASN in it) if you don't have 2byte ASN your best bet is 239.0.0.0/8 which can be thought has RFC1918 (e.g. 10.0.0.0/8) for multicast. 

I don't think it's supported natively. Fortunately you don't need to create the SLAX script yourself, as such script already exists. Essentially you configure normal interface-range, but upon commit its content is expanded to the real interfaces and the interface-range config is removed, creating approximation of the IOS behavior.