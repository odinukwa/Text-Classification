Note: this is not duplicate question. I have setup AlwaysOn. I have created SQL agent jobs manually on both the nodes. right now all the active jobs are running if node is PRIMARY. Challenge My DBA will disable few SQL agent jobs manually in only one node based on business requirements. Now how does it synchronizes to node 2? Will DBA have to manually disable these related jobs in all the secondary nodes ? I am thinking of creating a table (this table will be part of availability group) where I maintain the state of the jobs. and during fail over I am planning to run the script to read the data from this table and set the state of the jobs on other nodes. Is it right way? or do we have any recommended steps? Kindly suggest 

When application send INSERT or any SQL transactions, which node first the execution happens? Is it on PRIMARY ? When node 1 fails then node 2 becomes PRIMARY, but who will make node 1 up? do I need to setup alerts? if node 1 dies then how HA and DR works? What is difference between active-passive or active-active? 

I have a scenario where SQL Server 2012 - Always-on feature configured.The primary and secondary are synchronous for failover purposes. I understand that we will have node 1 (PRIMARY) and node 2 . Also I know during fail over node 2 becomes PRIMARY. My question is, (synchronous commit setup) 

You are confused because you're mixing up a database base backup and a SQL dump (with pg_dump or pg_dumpall) which are actually two completely different things. Take a good look at the PostgreSQL documentation about Backup and Restore. These are the links to the 9.1 docs, but the basic principles of this stuff haven't changed since at least version 8.3 (except for streaming replication which was introduced with 9.0) 

I am running PostgreSQL 8.4 database server and psql terminal front-end on Ubuntu 10.04 Lucid Lynx and would like to span a single transaction over several sequential psql sessions. When I connect to my database with psql a new connection is established and a server backend process for this connection is created. When I disconnect the connection is released and the backend process terminates. A (non-XA*) transaction is bound to the scope of a connection, so obviously there is no straight forward way to span a single transaction over several psql sessions. What I would like to achieve is that the following sequence of commands can be run within a single transaction and therefore return the same transaction timestamp on each call of : 

My first shot to solve this problem was to setup the PgBouncer 1.5 connection pool and configure it as a simple proxy with exactly one connection to the target database (session pooling mode). My reasoning was that PgBouncer would establish this connection at start-up and that I can then connect/disconnect to/from the proxy with psql while the connection to the database keeps open. 

As you can see on the above picture, the amount of free space inside MyDatabase is high. I need a copy of this database so that I can test some partitions operations. Is there a way I can restore this database ignoring the free space in each file? Basically I will use my "new database" to develop and test something, I would prefer to keep it small. How could I achieve that? 

I am monitoring stored procedure executions, mainly the execution times and the CPU usage, using sys.dm_exec_query_stats in the script below. 

I know there are logins and groups that belong to roles that are have db_reader and db_writer permissions. However, they are not showing here. How can I change my script so that it would show me all the permissions for this table? 

The query plan for the original query BUT when I copy the code of the first view into the query and run it again: 

question: Can I work out exactly what trace columns has he put in this trace, without having access to the .trc file? 

I particularly have concerns when developers create new databases, even in the dev environment. why? because they start by creating the database in dev and then just ask me to copy it to live. have you done any capacity planning for this database? how big it should be and how much it would grow in a month? 

We have been saving the backups of one of our production servers (which is actually an Always On availability group) this is how I am doing my backups and saving to azure: 

I have created 2 Virtual machines in azure and configured Windows Cluster using fail over feature. I have enabled the "AlwaysOn" feature in SQL Server Configuration Manager. But I do not anything under Roles section of Failover cluster manager window as shown below. In many tutorials I see SQL Server instance names under roles. How it comes? do we really need them? 

I have given wrong name for listener while configuring the always on in SQL Server management studio. 

Where I can find detailed information on always-on architecture? I have read many article but none explains the internals. 

PORT 5022 was not added to secondary node's firewall After adding rule, it worked like charm This is how I found the solution Open command prompt 

I have put the mapped drive location path, which was FILESHARE created in azure portal. Now availability group validation fails as shown below, it says "Primary server '[MYNODENAME]' can not write to shared drive during availability group configuration" 

As per below MSDN blog, I see there are 2 ways we can create load balancer listener . $URL$ But I am NOT finding exact scenario where I need to use them? I would like external applications connect to my SQL server which is highly available (AlwaysOn). which load balancer i should configure? Article here $URL$ says that, 

One option I can see is deleting and re-creating the listener, But is it right way of doing it? will it have any side affects? $URL$ Updated - After deleting and recreating the listener I am getting following warning. 

as part of a bigger operation I have been putting together this script below that basically generates a for each user of the current database. When the user has a it comes up in the generated script. 

I need to deploy packages using visual studio 2013 but I am seen this message, the application is not installed, as you can see on the picture below. what do I need to install? 

2) I modified my create index job, adding the setting of QUOTED_IDENTIFIER as required. You can see this on the job creation code below. this is currently working. 

sometimes the database is involved in replication the permissions are overwritten - the correct way is to save the current permissions before the restore and re-apply them after the restore. Have you checked the current available disk space in all drivers, specially those used in this restore? there might be other people using or working on that specific database, and restoring over it without communication can cause someone's else work being lost in the dev environment the databases can be in simple recovery mode. Have you shrunk the log and changed the database to simple recovery mode. in this particular case, the full backup for every user database is going to run automatically every night. but anyways, have you checked for scheduled backups? have you deleted from the server the backup that you used to do the restore? the folders and drives or data and logs are different in live and test. are you sure you moved the files to the right places? Have you had a look at orphaned users and logins? 

But there is a little problem with this approach, as soon as I begin a transaction on the proxy connection and disconnect... 

you should then be able to restore the dump file with your current restore command in the cron job. Hope that helps. 

So in your case using or for backup and pg_restore or psql to restore the dump is the way to go. Hope that helps. 

Clearly this is not what I really want to do. I want to be able to execute several bash scripts that connect to the database and execute SQL statements and scripts with psql within a single transaction. * Afaik the XA protocol would allow BEGIN TRANSACTION and PREPARE TRANSACTION on different connections but PostgreSQL does not support this. 

Of course this makes perfect sense for a connection pool. Its job is to provide (a) shared connection(s) for several clients but to isolate the transactions of these clients. But for my use case a shared transaction is exactly what I would need... So my question is now, is there a way to configure PgBouncer (or another connection pool) to not release the connection upon disconnection after BEGIN/START TRANSACTION or is there another way to achieve what I would like to do? All further questions to this post, comments and of course answers appreciated! 

Make sure you are not using the option of since replication is based on triggers. Check the dump file for statements.