There is more than one way to do this in principle, but most CNNs, and most CNN libraries will do the following: 

Sklearn's module finds all classes and assigns each a numeric id starting from 0. This means that whatever your class representations are in the original data set, you now have a simple consistent way to represent each. It doesn't do one-hot encoding, although as you correctly identify, it is pretty close, and you can use those ids to quickly generate one-hot-encodings in other code. If you want one-hot encoding, you can use instead. This works very similarly: 

Probably not. Modern encryption systems are designed around cryptographic random number generators, their output is designed to be statistically indistinguishable from true randomness. Machine learning is generally based on discovering statistical patterns in the data, and with truly random data there is none. Even for flawed crypto where there is some small pattern to be found, the large amount of randomness in the input will overwhelm any direct attempt to decrypt the ciphertext. In addition, there is no heuristic you can use to tell if you are getting close to a correct decryption - a single bit out in a guess at a key for example will completely scramble the output (blame Hollywood for when it shows decryption on screen like some crossword puzzle where the correct letters drop into place). That all-or-nothingness rules out discovering algorithms via a machine learning process, even when you have the encryption key. The best you can do is brute-force all known algorithms. If you don't have the key, then you have to brute-force all possible keys too. You could explore how difficult the problem is by attempting to guess the seed value used for a random number generator. Using the Mersenne Twister RNG (the standard one used in e.g. Python), then the input could be the bit pattern for 624 32-bit unsigned integers, and the output could be the 32 bits of the seed used to generate that series. The reason I suggest those specific numbers is because it is in fact possible to crack Mersenne Twister with that much data. However, I still think that ML approaches would be entirely the wrong tool to do so. Another simple variant would be to see if you can teach a network to either produce or reverse a cryptographic hash. You could start with a known broken one such as MD5. Input and output could be 80 bits, which simplifies the architecture and pipeline enough that you could put together this test in a few hours. Even though MD5 is known to be compromised, I think there is zero chance you could teach a neural network to find any pattern. 

That is not strictly true. In general, the distinction between classifying and regression is a hard line. If you are classifying hand-written symbols into an alphabet for instance, it doesn't really matter if you are doing this for 10, 100 or 1000 classes, there is not a practical point at which the symbols turn from being a set of objects into a continuous space to perform regression over. It could be true if your target class represents a range within some continuous variable (e.g. classifying an event by some of its properties into which year it occurred). But in that case the problem is inherently a regression problem to start with. In fact you may be better off training a regression algorithm in this case even for small number of target classes, and simply binning the predictions into relevant classes. It could also be true that your target class represents a rank or sequence within an ordered set. In which case this does look more like a regression problem when you have a longer sequence. In general, if you can arrange your target classes into a meaningful sequence, then you might be able to perform some kind of ordinal regression which could be a better choice than using a classifier. However, classifying symbols/alphabets does not work this way, because the sequence in those is arbitrary. Finally, you might be facing such a large number of classes that a single classifier model is overwhelmed and you need to approach the problem differently. For an example of this last case, consider a classifier for images of pets. If it had three classes (cats, dogs, rabbits), then you'd clearly use standard classification approach. Even when classifying by breed - 100s of classes - then this approach still works well enough, as seen in ImageNet competitions. However, once you decide to try and detect the identity of each pet (still technically a class) you hit a problem using simple classifier techniques - in that case the structure of the solution needs more thought. One possible solution is a regression algorithm trained to extract biometric data from the image (nose length, distance between eyes, angle subtended between centre of jaw and ears) and move the classification stage into KNN based on a database of biometric data for observed individuals. This is how some face identifying algorithms work, by mapping images of faces into an easy-to-classify continuous space first (typically using a deep CNN), then using a simpler classifier that scales well across that space. 

Perceptrons can indeed be implemented using this kind of boolean logic. Perceptron training is about finding suitable values to go into the if/else expressions, using example data. In your first example has to be discovered, from known practice time and test results. In your second example has to be discovered by measuring some papayas. In the more general case, the perceptron finds a hyperplane (line in 2D, plane in 3D) that discriminates between two target classes based on the input feature vector $\mathbf{x}$. That hyperplane could be in any direction and is expressed mathematically in terms of the input features. It still can be expressed as an if/else statement though: If $\mathbf{\theta}^T \mathbf{x} + b > 0$: $\qquad$class A Else: $\qquad$class B The perceptron training finds parameter vector $\mathbf{\theta}$ and scalar value of bias $b$. 

The shape you have found is a common convention to represent a colour image in (x, y, colour_channel) dimensions. The key word here is convention - there is no inherently preferred way to represent a colour image in terms of fundamental maths or computing needs, and even within Python you will find multiple conventions, varying in the ordering within the dimension - e.g. OpenCV uses (x, y, channel) convention for the shape, but has channels in order BGR - so channel 0 is blue - whilst most other libraries will use RGB ordering (ignoring for now the alternative colour spaces). 

The question is asking you to make the following mapping between old representation and new representation: 

The first line defines how the weights and values will be changed. You can read this almost literally as "define a training function that uses the gradient descent optimizer to reduce the cross entropy of the supplied data". The second line invokes that function with a specific piece of data. Each time this second line is run, the weight and bias values are adjusted so that neural network outputs $y$ values a little bit closer to the correct association for each $x$ value. 

Neural style transfer is not really machine learning, but an interesting side effect/output of machine learning on image tasks. When performing neural style transfer using a pre-trained model, then a significant amount of supervised machine learning has already occurred to enable it. The style transfer algorithm is still an example of gradient-based cost function optimisation, which it shares with many supervised and unsupervised learning algorithms. The output of style transfer is partially a probe into what a network has learned about different levels of structure in the problem domain it has been trained on. However, its main use has been to generate images with altered/mixed aesthetics for art and entertainment. 

Regarding your list of current skills: None of them are directly relevant to reinforcement learning. However: 

Whether or not you should use the partial data or drop it is not possible to say in general. If you are not sure, then try both and pick the version with the best cross-validation result. 

The most common scaling operation is to take all the training data and convert it so that it has mean $0$ and standard deviation $1$ (typically per feature, as opposed to global scaling for all data as I have done here) - store the values used to achieve that and apply them to all following data for training, tests etc. 2. Adjust learning rate until you get a working value. 

It is not clear why when selecting a single class, that you would care how probability estimates were distributed amongst incorrect classes, or what the benefit would be to drive the incorrect values to be equal. For instance if $y' = [1, 0, 0, 0]$ then using the suggested formula for $J_{y'}(y)$ gives ~ 0.67 for $y = [0.7, 0.1, 0.1, 0.1]$ and ~0.72 for $y = [0.73, 0.26, 0.05, 0.05]$, yet arguably the second result is better. However, you would use this loss when dealing with non-exclusive classes (where the outputs would use sigmoid as opposed to softmax activation). 

* There is a useful definition of multi-objective optimisation, which effectively finds multiple sets of parameters that cannot be improved upon (for a very specific definition of optimality called Pareto optimality). I do not think it is commonly used in neural network frameworks such as TensorFlow. Instead I suspect that passing a vector loss function into TensorFlow optimiser will cause it to optimise a simple sum of vector components. 

The examples I have found here are about are in renderings of fluid simulations and other complex physical systems where a full simulation can be approximated and they all use neural networks to achieve a speed improvement over full simulation. 

The network doesn't store its training progress with respect to training data - this is not part of its state, because at any point you could decide to change what data set to feed it. You could maybe modify it so that it knew about the training data and progress, stored in some tensor somewhere, but that would be unusual. So, in order to do this, you will need to save and make use of additional data outside of the TensorFlow framework. Probably the simplest thing to do is add the epoch number to the filename. You are already adding the current step within the epoch, so just add in the epoch multiplied: 

More general ReLU network If we add in more generic terms, then we can work with two arbitrary layers. Call them Layer $(k)$ indexed by $i$, and Layer $(k+1)$ indexed by $j$. The weights are now a matrix. So our feed-forward equations look like this: $z^{(k+1)}_j = \sum_{\forall i} W^{(k)}_{ij}r^{(k)}_i$ $r^{(k+1)}_j = ReLU(z^{(k+1)}_j)$ In the output layer, then the initial gradient w.r.t. $r^{output}_j$ is still $r^{output}_j - y_j$. However, ignore that for now, and look at the generic way to back propagate, assuming we have already found $\frac{\partial C}{\partial r^{(k+1)}_j}$ - just note that this is ultimately where we get the output cost function gradients from. Then there are 3 equations we can write out following the chain rule: First we need to get to the neuron input before applying ReLU: 

One way of stating what you are looking for is to find a simple mathematical model to explain your data. One thing about neural networks is that (once they have more than 2 layers, and enough neurons total) they can in theory emulate any function, no matter how complex. This is useful for machine learning as often the function we want to predict is complex and cannot be expressed simply with a few operators. However, it is kind of the opposite of what you want - the neural network behaves like a "black box" and you don't get a simple function out, even if there is one driving the data. You can try to fit a model (any model) to your data using very simple forms of regression, such as linear regression. So if you are reasonably sure that your system is a cubic equation $y= ax^3 + bx^2 +cx +d$ then you could create a table like this: 

Text-to-speech suffers from problems similar to the drivers of "uncanny valley" effects when viewing faces of humanoid robots and computer renderings of faces. Our ability to distinguish depth of meaning, emotional content and other subtle cues makes humans very sensitive to small details in audio containing spoken language. Note this is not quite the same as claiming text-to-speech actually has an "uncanny valley"; there doesn't appear to be much analysis of that claim either way around. Existing text-to-speech systems have two basic generative processes: Concatenative models are essentially databases of phoneme samples that the system strings together. These suffer from lack of flexibility. Parametric models attempt to represent sound generation at a lower level to improve on this, but it is quite a challenge to create a model that includes all the fine detail that we notice. For instance, we notice many non-verbal cues including imperfections such as mouth noise, breathing etc. There has been recent progress in this area by researchers running a very detailed generative model - one that literally generates sound, sample by sample, after consuming a large amount of training data. This report on WaveNet by DeepMind team explains the different synthesis techniques used to date and shows off the capabilities of the new approach. The process creates state-of-the-art results, and it is easy to hear the improvement from examples on the site. However, it is too computationally intensive to be used in real-time systems yet. Give it a few years to be refined, and it could be the basis of far better text-to-speech systems. 

Working definitions of ReLU function and its derivative: $ReLU(x) = \begin{cases} 0, & \text{if } x < 0, \\ x, & \text{otherwise}. \end{cases}$ $\frac{d}{dx} ReLU(x) = \begin{cases} 0, & \text{if } x < 0, \\ 1, & \text{otherwise}. \end{cases}$ The derivative is the unit step function. This does ignore the discontinuity at $x=0$, where the gradient is not strictly defined, but that is not a practical concern for neural networks. With the above formula, the derivative at 0 is 1, but you could equally treat it as 0, or 0.5 with no real impact to neural network performance. 

In some RL contexts, such as control in continuous problems with function approximation, it is more convenient to work with maximising average reward, than maximising expected return. But this is not quite the same as "expected reward", due to differences in context (average reward includes averaging over the expected state distribution when following the policy)