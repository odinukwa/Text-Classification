As L.H. Kauffman, Reformulating the map color theorem, Discrete Mathematics 302 (2005) 145–172 points out, the Primality Principle due to G. Spencer-Brown as well as the Eliahou–Kryuchkov conjecture are equivalent reformulations of the FCT. 

Visibly pushdown automata (or nested word automata, if you prefer working with nested words instead of finite words) extend the expressive power of deterministic finite automata: the class of regular languages is strictly contained within the class of visibly pushdown languages. For deterministic visibly pushdown automata, the language inclusion problem can be solved in polynomial time. For more details, see the paper by Alur and Madhusudan, especially Chapter 6. By the way, the nondeterministic variant of visibly pushdown automata is exponentially more succinct than the deterministic variant, but there the language inclusion problem is EXPTIME-complete and thus intractable. Alur, R.; Madhusudan, P. (2009). "Adding nesting structure to words". Journal of the ACM 56(3): 1–43. 

Electronic Proceedings in Theoretical Computer Science is an open access series publishing workshop and conference proceedings. No charges for authors and proceedings editors. 

Regarding your second question, an explanation why the generalized star height problem is less accessible than the star height problem is the following: Already Eggan's seminal paper in 1963 contained languages of (ordinary) star height $k$, for each $k\ge 0$. Only a few years later, McNaughton, and, independently, Déjean and Schützenberger, found examples over binary alphabets. This made clear what the problem "is about". During the years that followed, there was a more or less steady flow of published results in the area of the ordinary star height problem. This gave an ever increasing body of published examples, counterexamples and phenomena surrounding this problem. In contrast, after some fifty years now, we don't know whether there is any regular language of star height at least two. So we do not even know whether there is a need for a decision procedure after all. This "complete lack of examples" indicates that it is extremely difficult to get a grip on this problem. 

Finding the answer to your question is not overly difficult, if one is used to proving PSPACE upper bounds. But I think one cannot find an answer to your question in the literature, so here it is: Given a regular expression r of alphabetic width n, i.e. with n alphabetic letters, you can enumerate all regular expressions of alphabetic width 1,2,3, one by one (see the paper by Gruber et al. 2012), and test for each such candidate expression c whether L(c) = L(r). The first expression where the test succeeds is clearly of minimum alphabetic width. Apart from the test for language equivalence, this can be implemented using "only" linear space (and using exponential time). The regular expression equivalence test can be done by converting r and c each to a nondeterministic finite automaton, and check if these automata are equivalent. The latter can be done in PSPACE as follows (this algorithm is mentioned in the paper by Bonchi and Pous 2013): For each word $w$ of length at most $2^n$, test whether $w \in L(r)$ if and only if $w \in L(s)$. If all these $2^{n+1}-1$ tests are successful, then $L(r)=L(s)$. Notes The paper by Gruber et al. (2012) also covers the computation of the number of regular languages whose minimal regular expression is of size n for small values of n. This actually uses an implementation of regular expression minimization. The test for language equivalence is not done in polynomial space as described above, but by computing the minimal DFA. This requires exponential space in the worst case. But there are more practical ways (all practical approaches seem to use exponential space) for testing the equivalence of NFAs, see $URL$ References 

In case the input graph is bipartite, all cliques in the cover must be bicliques. Thus for bipartite graphs the problem that you are describing is the biclique partition problem. The biclique partition problem (and hence the more general problem that you describe) is NP-complete, and is NP-hard to approximate within $n^{1-\epsilon}$ for every $\epsilon>0$. See here for more details: P. Chalermsook, S. Heydrich, E. Holm, A. Karrenbauer: Nearly Tight Approximability Results for Minimum Biclique Cover and Partition. In: Proceedings of 22th Annual European Symposium on Algorithms (ESA), LNCS 8737, 2014, pp. 235-246 EDIT: Some easy combinatorial bounds: Let $opt(G)$ denote the minimum number of cliques/bicliques in an edge partition. Then $opt(G)$ is at most the chromatic number of the complement graph $\overline G$. Also, if $bp(G)$ denotes the number of bicliques in a minimum biclique edge partition of $G$, then $opt(G)\le bp(G)$. If you are interested in implementing a solution... Whereas you cannot resort to approximations, then you can still use some preprocessing and a greedy heuristic. In a preprocessing step, you can put all twin vertices, i.e. pairs of vertices that have the same set of neighbors, into equivalence classes. For the heuristic, call a heuristic for maximum clique, and one for maximum biclique. If the number of edges in the biclique we found is larger than that of the found clique, add the biclique to the edge cover, remove all edges in the biclique, and continue with the remaining graph. Otherwise, we add the clique to the edge cover and proceed in a similar manner. 

I want to point out the another research problem, which concerns the interplay of very basic concepts about DFAs. It is well known that any n-state NFA can be converted into an equivalent DFA having at most $2^n$ states. This is best possible in the worst case, in the sense that there are regular languages of nondeterministic state complexity n (i.e., the number of states in a minimal NFA), but of deterministic state complexity $2^n$. There are also examples of language families, where nondeterminism can save a quadratic factor, and cases where nondeterminism does not help to save any states at all. Thus a natural question is the following: Magic number problem Is there, for each $\alpha$ between $n$ and $2^n$, a regular language $L_n$ such that the gap between nondeterministic state complexity and deterministic state complexity is exactly $\alpha$? If we completely understand the powerset construction and the Myhill-Nerode relation from a mathematical perspective, then I'll expect that one is able to construct such languages for each $\alpha$, or alternatively to specify the values of $\alpha$ for which this is impossible (if such values exist, these are referred to as "magic numbers"). It is known that there are magic numbers for input alphabet size $1$, and, since 2009, that there are no magic numbers if the alphabet size is at least $3$. But if I am not mistaken, the case of binary alphabets is still open. Galina Jirásková. Magic numbers and ternary alphabet. In: 13th International Conference on Developments in Language Theory (DLT 2009), volume 5583 of Lecture Notes in Computer Science, pages 300–311. 

The total number $k$ of alphabetic symbols in this intersection of expressions is in $O(n^2)$. Using an argument given in the proof of Theorem 13 in (1), one can prove that every acyclic CFG that generates $L_n$ must have at least $2^n/(2n) = 2^{\Omega(\sqrt{k}/\log k)}$ distinct variables, if the right-hand side of each rule has length at most $2$. The latter condition is necessary for arguing about the number of variables, since we can generate a finite language with a single variable. But from the perspective of grammar size, this condition is not really a restriction, since we can transform a CFG into this form with only a linear blowup in size, see (2). Notice that the language used by Arvind et al. is over an alphabet of size $n$, and this yields a bound of $n^n/(2n)$; but the argument carries over with obvious modifications. Still, a large gap remains between $O(4^n)$ and the abovementioned lower bound. References: 

EDIT: The answer below seems to be incorrect, as I seem to have read the results of the paper (mentioned below) too superficially. :EDIT If I understood correctly, the following paper shows (among other things) that FVS is solvable in polynomial time on graphs of maximum degree at most 3: Cao, Yixin; Chen, Jianer; Liu, Yang (2010), On Feedback Vertex Set: New Measure and New Structures in Kaplan, Haim, "SWAT 2010", LNCS 6139: 93–104. Outline They study a slightly more general problem, namely DISJOINT-FVS, where in addition to the parameters of FVS, two certain vertex sets V_1 and V_2 are given. For details, see the paper. Then, ordinary FVS instances are a special case of DISJOINT FVS instances where the given set V_2 equals the entire vertex set, and V_2 is empty. They show roughly the following: a) DISJOINT FVS can be reduced to DISJOINT FVS on graphs of minimum degree at least 3 without increasing the maximum degree. b) On 3-regular graphs, DISJOINT FVS instance can be solved in polynomial time. Together, these two results show that FVS on graphs of maximum outdegree 3 can be solved in polynomial time. Beware that I have not read the paper very thoroughly, and may have misunderstood or misstated something, or even everything. Technical details: a) This is oversimplified. Only the maximum degree in the graph induced by V_1 is not increased. b) Also oversimplified. If the graph induced by V_1 is 3-regular, then this DISJOINT FVS instance can be solved in polynomial time.