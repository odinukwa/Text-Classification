Yes, multiple papers have used this. I've heard of multiple ways to exploit this hierarchial structure. This paper Hierarchical Deep Convolutional Neural Network for Large Scale Visual Recognition uses multiple levels by predicting the more coarse distribution and I think it then passes this as features to the more low level classification. YOLO9000 actually uses the ImageNet hierarchy for the problem of object detection. Hierarchial classification is the name of the problem you are describing. 

After the convolutional part you will need to add a normal, dense layer. Concatenate it to this layer and add some more layers if necessary, to add more interactions between the temperature and the image. This wouldn't necessarily need to be too deep because it can learn to represent the image features in a way that it will combine nicely with the temperature already, hopefully. 

Directly, this is not possible. However, if you model it in a different way you can get out confidence intervals. You could instead of a normal regression approach it as estimating a continuous probability distribution. By doing this for every step you can plot your distribution. Ways to do this are Kernel Mixture Networks ($URL$ disclosure, my blog) or Density Mixture Networks ($URL$ the first uses kernels as base and estimates a mixture over these Kernels and the second one estimates a mixture of distributions, including the parameters of each of the distributions. You use the log likelihood for training the model. Another option for modeling the uncertainty is to use dropout during training and then also during inference. You do this multiple times and every time you get a sample from your posterior. You don't get distributions, only samples, but it's the easiest to implement and it works very well. In your case you have to think about the way you generate t+2 up to t+10. Depending on your current setup you might have to sample from the previous time step and feed that for the next one. That doesn't work very well with the first approach, nor with the second. If you have 10 outputs per time step (t+1 up to t+10) then all of these approaches are more clean but a bit less intuitive. 

It will not expire until Spark is out of memory, at which point it will remove RDDs from cache which are used least often. When you ask for something that has been uncached it will recalculate the pipeline and put it in cache again. If this would be too expensive, unpersist other RDDs, don't cache them in the first place or persist them on your file system. 

When a feature is not that informative of your target, the algorithm can choose not to use it. This can be for two reasons: 

Yes it can for sure, some algorithms are more robust to this than others but doing proper feature selection is adviced. This is due to the curse of dimensionality. It's not only the algorithm but also has a lot to do with the amount of data points you have compared to the number of features. If you have 10,000,000 data points 150 features is not an issue, if you have 400 data points 150 features is way too much. How can you learn something when there is so little to learn from? Overfitting is a real issue for almost any algorithm, and noisy features make it more easy to learn things that are not actually there. 

After standardizing your data you can multiply the features with weights to assign weights before the principal component analysis. Giving higher weights means the variance within the feature goes up, which makes it more important. Standardizing (mean 0 and variance 1) is important for PCA because it is looking for a new orthogonal basis where the origin stays the same, so having your data centered around the origin is good. The first principal component is the direction with the most variance, by scaling a certain feature with a weight of more than 1 will increase the variance on this axis and thus give more weight to this axis, pulling the first principal component in it's direction. 

This is not standard part of the API of DataFrames. You can either map it to a RDD, join the row entries to a string and save that or the more flexible way is to use the DataBricks spark-csv package that can be found here. If it's just one column you can map it to a RDD and just call 

I would train multiple machine learning models based on information that is available. First of all I would standardize the data per product, so that the average is 0. That way you can better compare different price categories. I would add the mean price known to your set however, standardized over your whole training data, because I can imagine policies being different for different price groups. Now you can train different models for different products. The more features you use, the less data you have so this is a trade-off that you have to test. Here is an example of what I'm talking about (non-standardized): 

As you can read in the documentation ($URL$ it's conceptually similar, but implemented in another way to add flexibility: 

Yes, if your 0.85 AUC is good enough for your use case this is a good enough model. The performance on the training set indicates how well your model knows the training set. This we don't really care about, it's just what the model tries to optimize. The performance on the test set is an indication on how well your model generalizes. This is what we care about, and your model gets to around 0.85 as an estimate for your generalization. Differences between training and testing are the norm and in this case it could be that you might get a better performance by adding stronger regularization but if 0.85 is good enough, go for it! 

This type of problem is considered to be part of 'active learning'. There is a lot of research being done on this topic at the moment, but some first approaches are relatively easy, depending on the type of model that you are using. Since you mentioned that you are using deep learning bounding box detectors, I will showcase a few examples of how to approach this problem using Convolutional neural networks. The core idea is that we want some measure of potential gain of an unlabeled sample. That way we can train our model on our labeled training set, predict the labels for our unlabeled set and measure which examples will be most useful to label. In case of classification you could use the sigmoid/softmax output and get some kind of uncertainty from there, however deep learning models are usually fairly certain about their predictions and a high probability doesn't automatically mean that it predicts it well. Another approach is to use dropout in your model during training, and then apply dropout to your predictions on your unlabeled set as well. By sampling multiple dropout masks and comparing all the different predictions, you could measure how different the outputs are. If the outputs are very similar, it's unlikely that your model will learn much more if you label this, but if the outputs vary wildly, maybe this example lives in a part of your feature space that your model doesn't know or understand very well yet. There are a lot of ways to approach this, what I have written here is just an introduction to the concept of 'active learning'. There are a lot of papers available about this topic! EDIT: I haven't actually read a lot of this research, but here are a few: $URL$ $URL$ $URL$ 

Etcetera. After this you can train a model to classify entities. Sometimes the number of bedrooms will be in text instead of numbers so then you would need to write something that translates it to numerical values but you could likely just use some dictionary to do this for you. Entity recognition models are plenty, but I think a character based LSTM approach could work well if your texts are not too long. 

Your Pipeline is now a model that expects similar input as before. You can use a CV scheme to choose your K inside your Pipeline, after it has evaluated the options and chosen the correct K it will use that to predict. For actual predictions, enter a list of tweets and it will classify them, no need to manually specify which features to use. If you want to know which features your Pipeline has chosen you can look in the properties of your 'p' object. 

Hadoop and SQL are things that you will pick up reasonably quickly and in my experience are far from necessary for a lot of jobs. I would focus on Python or on R, not on both. A lot of employers are Python and R and allow you to choose yourself as long as it's on one or both. I feel like Python is growing faster in the Data Science community than R and is in my opinion a much more sophisticated language and has a wider support for things that help with data science but are not directly related. As jab already mentioned, I think soft skills are very important, although difficult to learn without real working experience. The best thing to prepare in my opinion is just by doing some project(s) and figuring things out along the way. Join a Kaggle competition or find some personal project. I spend hours and hours working on those and I keep coming across problems that I haven't faced before, requiring me to read papers, implementing some new ideas, trying things out. This also allows you to build up a github portfolio to show off some cool projects you have been working on. 

This is not covered very well in their AlphaGo paper but I assume that their policy network has a softmax output layer with a node for all the positions on the board, including illegal ones (the one with a stone on the board already). Maybe the illegal ones are basically zero already, I'm uncertain, but I assume they manually put them to zero and then normalize the probabilities. Please correct me if I'm wrong. To dip my toes into using neural networks for game AI I decided to work on a Backgammon player. I have only implemented a 1D convolutional value network on the board and merged the convolutional layers with the bar and the bearing states. I let random players play a bunch of games, train the model on those game states and let the model play against itself a number of times, tune the model further and iterate this step a number of times. The results are promising so I was looking to add a tree search algorithm similar to AlphaGo. This is where my question comes from. I would like to use a policy network to guide the exploration through the tree, but I don't know how to represent it. If you don't take the current state in mind, you would have a huge amount of potential states in your last layer of which more than 99.99% would be illegal. If you do keep the current state in mind I don't see how I could train a network on this. Is there any way of building a policy network for Backgammon? An alternative method would be to use the value network on the potential states that you are able to go to (the network outputs a probability of winning for the active player) and transform those probabilities into a new distribution for exploration, taking in mind exploration versus exploitation. EDIT: My current representation of actions is the state it would transfer into by doing that action, which means the number of actions over all possible games is enormous. 

By using the functional API you can easily share weights between different parts of your network. In your case we have an $x$ which is our input, then we will have a layer called shared. Then we will have three different layers called sub1, sub2 and sub3 and then three output layers called out1, out2 and out3. 

After one and a half years, I come back to my answer because my previous answer was wrong. Batch size impacts learning significantly. What happens when you put a batch through your network is that you average the gradients. The concept is that if your batch size is big enough, this will provide a stable enough estimate of what the gradient of the full dataset would be. By taking samples from your dataset, you estimate the gradient while reducing computational cost significantly. The lower you go, the less accurate your esttimate will be, however in some cases these noisy gradients can actually help escape local minima. When it is too low, your network weights can just jump around if your data is noisy and it might be unable to learn or it converges very slowly, thus negatively impacting total computation time. Another advantage of batching is for GPU computation, GPUs are very good at parallelizing the calculations that happen in neural networks if part of the computation is the same (for example, repeated matrix multiplication over the same weight matrix of your network). This means that a batch size of 16 will take less than twice the amount of a batch size of 8. In the case that you do need bigger batch sizes but it will not fit on your GPU, you can feed a small batch, save the gradient estimates and feed one or more batches, and then do a weight update. This way you get a more stable gradient because you increased your virtual batch size. WRONG, OLD ANSWER: [[[No, the batch_size on average only influences the speed of your learning, not the quality of learning. The batch_sizes also don't need to be powers of 2, although I understand that certain packages only allow powers of 2. You should try to get your batch_size the highest you can that still fits the memory of your GPU to get the maximum speed possible.]]]]