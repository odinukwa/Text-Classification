Nagamochi and Ibaraki gave an algorithm to find a sparse $k$-node-connected subgraph of a given graph $G$ that contains $O(k n)$ edges where $n$ is the number of nodes in $G$. Not directly relevant perhaps but could be a useful preprocessing step. $URL$ 

Approximating the Directed Steiner tree problem to within a poly-logarithmic factor. Currently there is a quasi-polynomial time algorithm that gives an $O(\log^3 k)$-approximation. More precisely, one can obtain an $O(i^2 k^{1/i})$ approximation in $n^{O(i)}$ time. $URL$ Related to this problem is the Submodular Orienteering problem and its special cases. $URL$ 

The directed versions are much harder. A survey by Nutov available on his web page is a good starting point. $URL$ 

I can recommend my recent paper $URL$ on multicommodity flows and cuts in polymatroidal networks; we generalize several known results in standard graphs. Although we do not give a nice table summary we discuss many of the known cases. Some of the results that are not discussed in the paper but are of relevance are those for planar undirected graphs and some special cases. These include the result of Satish Rao on $O(\sqrt{\log n})$ bound on flow-cut gap and tight bound of $2$ for series-parallel graphs by Chakrabarti-Jaffe-Lee-Vincent and some others. James Lee has several papers on flows and cuts on these graphs and any relatively recent one will discuss the existing literature. 

The claim is not hard to see for specific problems though proving it for all #P-complete problems may require some more formalism. Suppose for some #P-complete problem one can obtain a $p(n)$-approximation. Given an instance $I$ make a new instance $I′$ which contains $k$ copies of $I$. The number of solutions to $I′$ is $a^k$ where $a$ is the number of solutions to $I$. Thus, choosing $k$ sufficiently large, even a polynomial-ratio approximation to $I'$ can be used to approximate $a$ pretty well. 

In the paper titled "ON MULTIDIMENSIONAL PACKING PROBLEMS" that appeared in SODA and then in SICOMP ($URL$ we considered several packing problems where items are d-dimensional vectors. We discuss PIPs which are the same as as multidimensional knapsack (we mention this) and their approximability when d is not fixed. You can find hardness results as function of d in that paper. 

If you want exact minimum cuts then one can construct a cactus representation of the minimum cuts deterministically in polynomial time and use that to enumerate all minimum cuts. See below for a reference. $URL$ More interesting and difficult case is $\alpha$-approximation mincuts for $\alpha > 1$. One can use Karger's randomized algorithm for this purpose. Simply run it sufficiently many time. However, if you want a deterministic algorithm, one can do it via tree packings. See Thorup's paper on k-cuts. $URL$ 

Take an arbitrary element $e \in \Omega$. If $f(e) = f_{\Omega-e}(e)$ then $e$ is not affected by the rest of the elements so we can choose $X_1 = \{e\}$ and $X_2 = \Omega-\{e\}$. Otherwise let $X$ be an inclusion-wise minimal subset of $\Omega-e$ such that $f(e) > f_X(e)$. Then $X \cup \{e\}$ should be in the same partition. If $X \cup \{e\} = \Omega$ we conclude that there is no desired partition, otherwise we shrink this set into a single element and recurse. 

The simplest node-weighted network design problem is the node-weighted Steiner tree problem. Klein and Ravi considered this problem and obtained an $O(\log k)$-approximation (here $k$ is the number of terminals). They also showed that the Set Cover problem reduces to it in an approximation preserving fashion and hence we should not expect any better approximation. It is not hard to show that iterated rounding is not helpful for the Set Cover problem. Consider the following example. We have $n$ elements $\{e_1,\ldots,e_n\}$ and $n$ sets $S_1,\ldots,S_n$ where $S_i$ contains all elements except $e_i$. If you look at the natural LP with a variable $x_i$ for set $S_i$ then a basic feasible optimal solution is to set $x_i = 1/(n-1)$ for each $i$. From this one can see that iterated rounding in the plain vanilla fashion won't give a good bound. Note that the integrality gap of the LP is $O(\log n)$. This example for Set Cover can be extended to the node-weighted Steiner tree problem. 

This is an edited version of a previous "answer" which incorrectly claimed a polynomial-time algorithm for the problem. What I write below is a connection to an existing problem which suggests that the problem is difficult. Let $s,t$ be two nodes in $G$ and we want to check if they are $(a,b)$-connected. That is removing any $a$ nodes and any $b$ edges should not disconnect $s$ and $t$. Another way to look at it as follows: what is the minimum number of nodes that we need to remove to reduce the edge-connectivity between $s$ and $t$ to $b$? These type of problems have been studied under the name multi-route cuts and they are dual to multi-route flows. Various approximation results have been shown though many basic problems are not yet resolved. A result of interest is the following. Suppose each edge has a cost $c(e)$ and we wish to remove the minimum-cost set of edges to reduce the edge-connectivity between $s$ and $t$ to $b$; then this problem is NP-Hard when $b$ is part of the input. This result is in the paper by Barman and Chawla: $URL$ Two papers that will appear in upcoming SODA 2012 are on multi-route cuts which have further results on the topic. The one by Chuzhoy etal has hardness results for some variants. 

An interesting and not so well-known case is the following. Suppose we have an edge-weighted graph $G$ and root node $r$. We want the minimum-cost sub-graph of $G$ such that there are $k$ edge-disjoint paths from $r$ to every node in the graph. When $k=1$ this is the min-cost arborescence problem in directed graphs and in undirected graphs it is equivalent to the MST problem. Both solvable in poly-time though the undirected case is easier. However the problem is poly-time solvable in directed graphs for any $k$ while it is NP-Hard in undirected graphs for $k=2$ (since it captures the min-cost $2$-edge-connected sub-graph problem). 

The k-cut problem is to remove a minimum number of edges to create at least k components. W[1] hard when parameterized by k but admits a 2-approximation for any k. 

Most answers have already addressed the main reason to care about the integrality gap, namely, that an approximation algorithm based solely on using the bound provided by the relaxation cannot hope to prove a ratio better than the integrality gap. Let me give two other meta reasons why the integrality gap is a useful guide. For a large class of combinatorial optimization problems the equivalence of separation and optimization shows that exact algorithms are intimately related to the convex hull of the feasible solutions for the problem. Thus the geometric and algorithmic perspective are very closely tied together. A similar formal equivalence is not known for approximation algorithms but it is a useful guide - algorithms go hand in hand with geometric relaxations. Algorithmic innovation happens when people have a concrete target to improve. Integrality gap is one such target and attempts to find better gaps has resulted in improved relaxations for various problems which in turn led to improved approximation algorithms for those problems. 

The idea of using approximation algorithms for problems in P is very old and ubiquitous. Several problems in numerical linear algebra including the problem of solving linear systems are solved via iterative methods that converge to the answer quickly. However, they don't always find the exact answer. The rate of convergence to achieve a relative or additive $\epsilon$-approximation is analyzed as a function of the problem size and $\epsilon$. There are a number of papers on solving special cases of linear programming problems such as multicommodity flows (and more generally packing and covering LPs) approximately. There is no separate theory of approximation for problems in P vs problems that are in NP (we don't know whether P is equal to NP or not). One can talk about a certain technique being applicable for a certain class of problems. For instance there are general techniques known for approximately solving packing and covering linear programs and some variants. 

Think of the hypercube as a graph $G$. Allowing arbitrary edge lengths implies that you can use edge lengths $0$ and $\infty$ to get a minor of $G$ on which you can put edge lengths. The hypercube has sufficiently large expansion that it contains, as a minor, a clique of size about $\sqrt{N}$ (ignoring polylog factors) where $N$ is the number of nodes of the hypercube. This implies you can basically have any finite metric on a set of size about $\sqrt{N}$ supported on the hyper cube. Thus the worst-case lower bounds will apply so you will get an $\Omega(\log N)$ lower bound. 

Even without the additional constraints you have the unrelated machine scheduling problem with precedence constraints is not well understood. If I am not mistaken nothing better than a polynomial factor approximation is known. It is an important open problem in approximation algorithms for scheduling problems. Some positive results are known when the precedence constraints form a tree. See the following paper. $URL$ Can you solve your problem in the simpler case when the machines are identical or related? 

The problem can be solved via the assignment problem/network flow. Create a bipartite graph with the left side consisting of $N$ vertices $a_1,\ldots,a_N$ corresponding to the sets $A_1,\ldots,A_n$. The right side has $m$ vertices $u_1,\ldots,u_m$ one for each element in $U$. Connect $a_i$ via directed arcs of capacity $1$ to each element vertex that $A_i$ contains. Now add a source vertex $s$ and connect it via an arc of capacity $k_i$ to $a_i$. Connect each $u_j$ to a sink vertex $t$ with an arc capacity of $1$. The problem has a solution if and only if there is an $s$-$f$ flow of value $\sum_i k_i$ in this network. An integral flow would give you a desired solution. 

Node-weighted multiway-cut is a problem to which vertex-cover can be reduced to in an approximation preserving way. Thus, a better than $2$-approximation for node-weighted multiway cut is not possible assuming UGC. The reduction was shown by Garg-Vazirani-Yannakakis in their paper which also established a $2$-approximation for node-weighted multiway-cut. 

The paper that @Austin Buchanan pointed to above on approximate Graph Isomorphism does not seem to correspond to the version asked. I am assuming that the adjacency matrix has $0,1$ entries in which case the objective is measuring only the matched edges. The approximate Graph Isomorphism model measures both the matched an unmatched edges which makes it a bit easier from an approximation point of view. It appears that the problem asked is at least as hard the $k$-dense-subgraph problem which currently admits only a polynomial-approximation. See $URL$ and $URL$ for more details and the current status in terms of algorithms and hardness. Now for the reduction. Suppose we want to solve the $k$-dense-subgraph problem in a graph $H$; that is we want to find a subset of $k$ nodes $S$ that maximizes the number of edges in the induced graph $G[S]$. You can reduce this to your problem by setting $G$ to be a graph consisting of a clique on $k$-vertices and $n-k$ isolated vertices, and $G'$ is set to be $H$. 

The problem is at least as hard the maximum edge-disjoint paths problem (MEDP) since we can set $d(i) = 1$ for each pair. The approximability of MEDP has been investigated extensively and in particular it is known that MEDP in directed graphs is hard to approximate to within a factor of $m^{1/2-\epsilon}$ (technically it is $n^{1/2-\epsilon}$ since the graphs are sparse). This was shown in a paper of Guruswami etal. $URL$ Roughly at the same time there was another paper by Ma and Wang established hardness of $2^{\log^{1-\epsilon} n}$ for acyclic graphs. More recent work by Chuzhoy etal showed hardness results even with congestion allowed; my understanding is that the hardness holds even for instances that are acyclic and these results are stronger than the one by Ma and Wang, however, one should look more carefully at the paper to make sure or ask the authors. 

The approximability of the separator question in the sense you want is closely related to the approximability of the uniform sparsest cut problem. An $O(\sqrt{\log n})$-factor approximation was obtained by Arora-Rao-Vazirani improving the $O(\log n)$ of Leighton and Rao; they did this for the edge case. Agrawal-Charikar-Makarychev-Makarychev used the result to obtain similar bound for directed sparsest cut (if one is interested in vertex bipartition cuts). Feige-Hajiaghayi-Lee at the same time obtained a similar bound again via ARV for vertex separators (and also pointed out that treewidth can be approximated within the same factor). One should note that there is another notion of sparsest cut in directed graphs for which Chuzhoy-Khanna showed hardness results in the non-uniform case but I am not sure about the uniform case. I think super-constant hardness results are known for (uniform) sparsest cut under UGC but I am not sure. 

APX-Hardness implies that there is a $\delta > 0$ such that the problem does not admit a $(1+\delta)$-approximation unless $P=NP$. Clearly this rules out a PTAS (assuming $P \neq NP$). As for QPTAS, it will rule it out unless you believe that NP is contained in quasi-polynomial time. As far as I know, that is the only reason why APX-Hardness rules out a QPTAS. Since a couple of people asked more details, here are some more. APX-Hardness for a minimization problem P implies that there is a fixed $\delta > 0$ and a polynomial-time reduction from 3-SAT to P such that a $(1+\delta)$-approximation for P allows one to decide whether the 3-SAT formula is satisfiable or not. If there is a QPTAS for P we can obtain for any fixed $\delta > 0$ a $(1+\delta)$-approximation in time say $n^{O(\log n)}$. But this implies that we can decide whether a 3-SAT formula is satisfiable in $n^{O(\log n)}$ time which in turn implies that NP is in QP. 

The disjoint paths problem: given $G$ and $k$ pairs of nodes, are there node disjoint paths connecting the given pairs. Parameterized by $k$, in FPT when $G$ is undirected from the seminal work of Robertson and Seymour. NP-Hard for $k=2$ when $G$ is directed - from work of Fortune, Hopcroft and Wylie (1980).