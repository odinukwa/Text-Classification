Bitmap is simply means to convert a data range to bits. How that internally works and what the conversion function is, is application dependent. Each format can work differently. Simplest possible bitmaps just store the values directly in the bits themselves. Think of the the simplest form, a pixmap but instead of ASCII values you are using full byte per color. Off course the data does not need to be byte aligned so 3 values may be crammed to 2 bytes for example. Some formats may be using indexed colors (which is what you describe in your edit), this is a form of a compression scheme. 

Image 2: End result. How to transform the magenta curves? The way I've done this now is by sampling several forms and in this case draw the new curve over the points. But id like to do this automatically and more precise. Please note: I am perfectly aware on how to do this with discrete data, I am looking for a more analytical solution if possible. 

It is bigger because it fills the same view and it ts further away. It would be smaller if it wouldn't fill the same view but then it wouldn't fill the camera view and it wouldnt work. So the inverse relation applies. If you want something to stay constant in size in the view then it needs to grow as it gets further. Your just comparing apples with oranges. 

This is called subpixel rendering. The different primary colors in your monitor are not stacked on top of each other. Instead they are arranged near each other. Different monitors have different patterns but most commonly they are aligned so that the colors are side by side. If you know the physical arrangement, then you can prepare the image so that you have calculated a different sample position for each color channel. In essence your boosting the resolution of your image so that you are treating each individual color as a separate pixel. This offcourse means that you need to know the display orientation and have some data that can be shifted. The image wouldn't be all that useful on other monitors with different pixel alignments. So this is reserved for things that are dynamically generated, most often fonts. It also means that when zoomed or presented on a medium with different or no subpixels, the effect will register as a blur. So zooming in on the pixels of a subpixel rendered image is not very good idea. For this reason it migth be a good idea to disable the effect if you make images for others to use and the subpixel alignment is unknown. 

I want to create a scene where a room includes a projector and a white screen. How is a Ray Tracing environment functioning in such a setup? Is the light from the projector going through a slide and then the color reflecting on the white screen? If so, wouldn't any light ray bounce on that screen creating a mess? 

However, there is a way to retrieve the information through VESA. Any modern monitor should be able to handle that. Only I have no clue how that's done. Under Linux you have a command called which can be used to find out the info: 

I would suggest you look at how that tool does it and you'd get your info right at hand. From what I've read over the years, the VESA info can be wrong once in a while, but it's quite reliable now a day, I would think. 

OpenGL (scanline rendering) is always going to be way faster than ray tracing. Ray tracing computes pixels one by one using physics (at least a good 90% of the time it's going to be real physics). Although some of it can now be done with a GPU, it is not really helping as much as one would expect. Calculating speculative lightning is not something you can easily do in parallel (ray 1 will bounce from object A to object B, ray 2 will bounce from object A to object C... you can't parallelize the computations on B and C.) The GPU helps with the matrix math, though. In comparison, OpenGL is a total fake in comparison. It renders triangles really fast with shades coming from textures and fast gradient computations from your various light sources. 

Ok, Xenapior and Reynolds together have the right idea. But the explanation is a bit lacking so here is a image to explain it all and some further musings. First let us start by drawing an image (yes i know that is what they say in school for you to do but nobody does it). 

You do not know what other people think, you simply do not know if they should be gamma corrected or not. that your using a application that treats images as images 

Making the bump/displacement map Ok, so now I have described that the data is up to interpretation. But why would you interpret the data as linear? Because then you can design your side profile better. See what you can do is think of 100% opacity of your brush as the target height you want to carve or layer to. You can then use the curves tool to design the shape of your stamps brush (since you think the data is linear). 

Would it be, at least theoretically, possible to render without a near clipping plane? Could raytracing ignore this perchance? Quite many budding 3d artists have a WTF moment the first time they encounter the clipping plane. If this could be avoided then it would be a big win for usability of 3D authoring apps. Its not one of those things a normal artist would think about until explained. 

Image 1: Offset side in 2D. The higher dimensional cases are more problematic, I will be examining the 3D space only. A free space curve has no inherent good second direction. Mathematicians have defined, what is essentially a hack, that the second direction is in the direction of curvature which has a mathematical definition. The only problem is that curvature is subject to change and can flip direction along the curve. This is not good. So how to counter this? Well simplest is if a curve on a surface of a 2D plane or surface. In this case you can simply use the underlying primitives directions as reference and do a 2D case. This has the added benefit of not doing a somewhat random offset direction. For the free curve case there are several approaches you can use, they all have the downside of needing to keep some extra data with you. One is to calculate the location where the curvature is 0 and "cut" the curve at that point and continue with a flipped curvature direction. In case of a straight line just continue with last good direction. A variation of this is not to use curvature but a user defined vector and use that instead. This can also be done numerically by sampling multiple points along and detect the flip by comparing nearby result and not letting it turn more than a certain degree untill you declare a flip (what your doing sound like a variation of this). One other is to propagate a vector along the curve. One way to do this is to specify a vector at each point and let the user flip them if he feels the need. Or interpolate vectors only at certain points on the curve. Most applications simply just use the will not do it or bug out option. This has the benefit of being rather sane, as the curvature based bi-normal is a bit erratic to say the least. This way the user has to come up with a way to make the directional control with for example a ruled surface. * BÃ©zier, NURBS or something other like a Catmul-Rom spline doesn't matter 

Back in the days, MSDN defined a couple of values which in most cases are useless (total crap): $URL$ 

From what I'm reading, it looks like a ray tracer views a pixel as one color and computes that color using the ray starting in the center of that pixel. Yet a pixel has a width and a height. Do we ever use the actual square (now a day pixels are considered squares) that the pixel represents to calculate it true color from all possible source within that entire square? I am thinking that this is how we come up with colors on edges of shapes in scanline rendering and was wondering whether the same could be applied to ray tracing. There is a picture which I hope better represents what I am trying to ask here. Should we only consider the pixels to be single points? Or are we to consider pixels as large areas? 

I'm wondering whether some of you would have tested and seen quite a difference in using a compressed texture even when the OpenGL environment does not need saving any memory (i.e. the card has more memory than necessary to support the textures uncompressed.) I'm thinking that a compressed texture would use less RAM and thus reading it and decompressing in the GPU would be faster because the number of memory accesses is lower. Whereas, a non-compressed texture would rely more on memory access (GPU I/O) than processing and that is likely slower. The main things I've read about texture compression has been about how the compression saves space. Not so much about how it can save execution time with the GPU. (Note that I specifically will be using an nVidia, Pascal for now. But I would hope that it works similarly whatever the GPU.)