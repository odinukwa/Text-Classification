The sample is not "defining something with itself." Rather it is defining two different kinds of things. The first line: is beginning the definition of a template for things to be made later. That definition will be handled at compile time. The third line: sets up an object that will be instantiated using some template. Later in the compile process, when the object is created, Java will look in the table for the template called "Node" that can handle some type. Since, by then, the complete template will exist, there's no problem. The key point here is that is not a template, it is object that will use the template when it is created. If you also define , then at run time, could refer to the template , or the template . Exactly which it is will be determined when the object is instantiated. If it hasn't already been covered, that would be an opportune time to cover early and late binding. 

The educational implications To begin with, the excerpt seems to be the rationale (or sales pitch) for the authors' new tool, TMOSS, rather than a warning to instructors, or an indictment of students. The authors give excessive collaboration as one factor that could identify students at-risk of plagiarizing. They neither claim that it is plagiarism, nor that it is the only factor that leads to being at-risk. The apparent objective is to identify students in need of additional help in the class, not to weed out or identify plagiarists: "it is hard to know which students need extra help," and "so we can intervene and provide additional class resources to help them succeed."[ibid.] The implications are that when students become more dependent on the assistance of others, and outside sources, for results, they develop malformed habits that reduce their learning, and perhaps affect their future work as well. Prefaced with the presumption that the classroom experience is intended to "educate" the students, finding those that might need extra help using any tool available can be helpful. If noticing that a student is engaging in excessive collaboration (though I have no clue what level is considered excessive) can help to alert the instructor to a student that needs additional help, or needs guidance in study/work habits, it may be worth considering that approach. It is possible, however, that students who engage in excessive collaboration are developing work habits which will lead to greater success in software development, especially in Open Source development, than their peers. Teaching the students how to learn, not how to work, seems more appropriate to the educational system, including Computer Science curricula. Teaching them how to work seems more appropriate in vocational training or rehabilitation. 

An interesting approach, which I have both used, and experienced as a student, is to make it truly random, and obviously so. Use a deck of playing cards split to match class size and how you want the divisions to be. For groups of 2-4, use the the face values with the correct number of suits and numbers removed to make the division equal. If it's a large class and you want groups of larger sets you can group the results some other way, say even red suits, odd red suits, even black suits, and odd black suits, to make 4 teams. Have the cards pre-selected for that class, but shuffle in class, and allow the students to select their cards one by one. Grouping the students by knowledge can have negative effects two ways. First the "advanced" students don't get to see how others might think, and secondly the less advanced students receive the message that they aren't as good as the other students. Mixing them helps the "behind" students learn from the "advanced" ones, and the advanced students learn more by "explaining" it to the others. The benefit to both is that they get to see how other people view the project, and get insights into different modes of thinking, and probably programming. 

Use the when you know how many times to execute it, such as counting, or iterating over an array. Use or if you don't. The choice between and is if you need to do the loop one time before the test use otherwise use . A full explanation can be found on Stack Overflow in this accepted answer. 

As mentioned by another user, Buffy in their answer, having a metaphor to deal with the environment of the language, or library, often helps. In widely used languages and libraries the metaphor, if chosen well, will usually be sufficient to make the students comfortable in that environment. With programs found "in the wild" there will often be no useful metaphor available, or likely even possible. Taking advantage of any disconnects that exist in the the language base used in the classroom can help the students learn to adjust to such situations later. It can also be used as an example of why good naming conventions are important. 

For evaluation of any lab assignment, College Board example, random Internet find, or custom built, there are three basic criteria to apply: 

I love the idea that you've created a lab using a classic logic puzzle. If you don't already, it might also be beneficial if you could give a real-world problem that the same logic, or concept, has been, or could be, used to solve. I've got a few ideas about changing how the lab, and its grading, breaks down. First off, it just feels wrong to have the reduction in potential points for advancing through the "hints" and is even more questionable knowing that it's on the honor system to self-report how far the student went into the hints. I'm thinking it's better to develop a rubric for grading the final project. Possibly disseminate it, or its basics, with the initial introduction of the lab project. Build into that the appropriate penalties and bonuses based on what you want them to learn, and how that fits into the total course objectives. Completing the project successfully, including all of the target objectives, should be a full-credit project. Penalties ought to be levied for failure to meet stated goals, or demonstrate targeted proficiencies. Bonuses, of course, a assessed for above and beyond work. As a compensation for the loss of good will from the prior experiment, the more transparent, and objective, the rubric is, the better off you will be. For the scaffolding of the project points, a possibility is to do a timed-release of the "hint" sections, with an advanced notice of when the next section will be released, and what it will pertain to. Those students who think they have solved the issue addressed in the pending section release can "lock-in" their solution by submitting their current work on that issue before the release. When the final project is in, you can compare what they "locked-in" with what they finally submitted, and determine if they adjusted their work to fit the released hint, or merely to correct flaws they would have found without the hint. Thereby, cheating, and self-reporting, are circumvented, and you can award bonuses in the final review for having successfully utilized their original solution (but no penalties for switching to the hint version). Each section release can have its own, unique, bonus weighted to its importance to course/project objectives and/or level of advancement from the course baseline. Overall, I think it's probably best to keep such bonuses relatively small, allowing for a total project value to be 110/100 points. The benefits of the bonus points, hopefully, will only be to compensate for points lost to other criteria in the rubric. One point I'm unclear on is the timing or the project relative to the entire term for the course, and if it's able to be adjusted. What I'm thinking here, is that the project specs, rubric, and other details can be released fairly early in the term, and the students can have the project as a continual "go to" when they are ahead of the curve on other minor assignments. As the term progresses, you can present concepts that are important to the course objectives, and relate the same concepts to the ongoing project. This will allow them to learn and practice the concepts in your normal flow. Then repeat that learning on their version of the project. The timed release of "hints" and sections could then be set for a few days, or a week, after the pertinent concept is covered in the curriculum. Time enough for them to, possibly, find their own solution, yet not so long as to delay the full project to the point of becoming unsolvable in the remaining time. As an additional point, the last section you released, with the essentially complete solution, no matter how idiosyncratic, should probably not happen. If, for whatever reason, the student cannot complete the project successfully, they simply will not get the points for the project. If the breakdown of the solution is such that you can award, per the rubric, points-per-section, then an incomplete project need not become a complete loss. They can earn points for the sections, or methods, they did successfully implement. Giving wrong answers on a test, or only completing a portion of a timed test, will not earn full marks. Neither should a project that the student is unable to complete. As a parting thought, the reluctance to proceed beyond the 100% section probably should have been anticipated. Your student body is not going to respond in a typical fashion, just because they are a select group. They don't dare risk loosing their status as a member of that select group by achieving sub optimal marks. In addition, the perception of students in that age range is that going into the field as developers is a "high stakes" realm. Shades of Facebook and Twitter here. Anything, such as low marks, that could threaten that future potential, either as an active developer or select university admissions, is to be avoided like the plague. Since they cannot conceive of failure as a possibility, they don't see the loss of 6 points as insurance against failure, but as an admission of inferiority, or unsuitability for becoming a developer. (You don't need insurance against something that cannot happen, such as a zombie apocalypse. For them, failure seems impossible, so no insurance is necessary.) 

High school students seem to have one thing in common in any country with even moderate technology: cell phones. So, use that to explain the concept in a manner that fits with what they know. The how and why of cryptography is buried in the math that's outside the bounds of what they are likely to know. If, however, the math is accepted as valid, and the computer does it all anyway, the rest is really very simple. The part of "asymmetric cryptography" that you seem to want to explain is the asymmetric part, not the cryptography part. Asymmetric cryptography, as just said, has the cryptography part, which is the cell phone itself. Exactly how it works (on the inside) is not something we think about, or need to understand. The same applies to the cryptography for most of us - the computer does the math, checks signatures, encrypts and decrypts the messages, etc. The asymmetric nature is from there being two keys that work in different ways. The public key, which can be made public and the private key which must be kept private. In connection with the cell phones the public key is the phone number and the private key is the voice mail PIN. Anyone that has the public key (phone number) can leave a voice mail. Once they save it, not even the sender can hear it, or cancel it. To hear the message, from any other phone they have to have the PIN. If they have their own phone set up so that they can get voice mail from their own phone without entering the PIN it's the same as having their computer setup so that there is no passphrase on the private key. Someone finds their phone and they can listen to all the voice mails. Someone gets access to their computer and they can read all the "private" messages. If and when the "signature" usage comes into discussion, then the phone comes back into use. When one person calls another on the phone, the receiving phone will show the phone number (public key) of the person calling. If the person receiving the call already has that public key (number) saved in their contacts, then the phone will "verify" the number and show the caller's name instead of their phone number. Eventually, when the discussion gets technical enough, the parallels between PK and cell phones will break. By then, hopefully, the asymmetric nature of it all has been grasped, and the parallels will no longer matter. 

Simple demonstration of "precision" using handheld calculators. Get a pair of calculators. One cheap model that has a limited number of digits in its display. Models can be found built in to clipboards and other "office" things. One decent model, not necessarily a good model, that has double the digits of the other calculator. If it has exactly double the display size of the first, so much the better. One point to be aware of is that many calculators, even the cheap disposable ones, will have at least one significant digit beyond the display to help hide/correct rounding errors. The demonstration will have to be tested on the specific models you use before class. Develop some short, and simple, formulas that will develop rounding errors quickly. Thirds, and harmonics thereof, usually are corrected by the hidden digit(s), but higher factors tend to do better. It can be helpful to also avoid returns that go into scientific notation value range - six or seven digits left or right of the decimal place is probably the best range to stick with. Something involving 3 or 4 digit primes as numerators and denominators should do the trick. Having the two calculators in hand, and your pre-tested formulas, have the students work out by hand, or follow you as you do, the formulas, so that they will know what the real answers are. Run the same formulas on the cheap calculator, showing where it starts to go wrong, and how fast it gets worse. Run the same formulas on the better calculator, showing how much farther into the formulas it gets before it starts to drift, and how much, less, it gets it wrong. If the demonstration has one formula that actually finishes correctly on the better calculator, but not the cheap one, so much the better. Having the other formulas not come out completely correct, even on the better calculator, leaves room for you to explain why/how rounding errors in float/double can affect programs. Adding in a formula or two that can then be run on the computer (language of choice) that shows the error delta can help drive the point home. If it's also one where they might be tempted to opt for float that is even better. 

Rather than attempt to judge the labs in question, I'll address how you can evaluate the needs of your students, and what labs they need to be given. The labs from the College Board are, as you indicated, suggestions rather than requirements. The implication is that if those labs are completed successfully the students will be prepared for related questions on the College Board's exams. The design of those labs, presumably, address the minimum competencies a student needs to pass the College Board exams. They are most likely also designed around the typical student enrolled in an AP CS A course at the high-school level. All well and good for the instructor with a class filled with these mythological typical students. Your students are not typical. They are not average, and they are not "in the norm." That's true for every instructor, so I'm not placing your class above, or below, typical; rather I'm saying that typical is a statistical contrivance, not reality. In the process of designing your course, including the labs to use, you get to take advantage of what you know about your student body that the College Board does not know. You know what their prior course work has been focused on. You know what other fields of study have been presented, and at what level, before they are enrolled in the AP CS A course in your school. Things like first exposure to Java vs. continuation of Java work, prior basic algebra vs. plane geometry and trig., other languages learned, prerequisite courses in your system for the AP CS A course, etc. You also know what you, your school, your school district, and your state education system expect to be presented, and what the stated "objectives" are for your class, and the program it is part of. What the established objectives are for your course, no matter at what level they have been "established", determines what the course content is. The labs have to mesh well with the course as a whole if they are to have an effective impact on the students reaching the objectives. If your class is the third in a series of Java classes, a lab that explores looping structures will have very little value. Conversely, if it is their first exposure to Java, than a lab that implements a 3-dimensional graphics class will probably never be completed successfully. Once you have settled how the labs will mesh with the course content, and what level of competency you can anticipate from your students, you can evaluate, or design, labs that further the objectives of your course. Obviously, since it is an AP CS A course, passing the College Board exam is included in the objectives. It can, however, be the final goal, or it can be the minimum requirement. If your objective is to prepare the students for advanced studies in CS you do not, and probably should not, limit your labs to mirror the minimum set by the sample labs. Instead you can build labs that allow the students to practice what they've been taught, even if it's beyond the College Board minimums, and expands their mental skills in a way the prepares them to learn new concepts after they've mastered what you offer. It's a difference between "teaching the test" and "teaching the subject." An unfortunate trend has been growing where states, or the College Board, have established some set of "minimum passing" evaluations, or standardized tests for graduation. In order to assure that students get high marks on those tests, meaning better ranking for their school, and maybe better funding from the state, the instructors have started to teach what's on the test, and nothing more. That is a major disservice to the students, even if it does increase funding for the schools.