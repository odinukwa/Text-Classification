You've got a few different questions in here: Does the "Lock Pages in Memory" setting preserve the plan cache? Only indirectly. LPIM means that SQL Server won't page out to disk if it comes under memory pressure, but SQL Server will still give up memory when the OS is under pressure. Jonathan Kehayias covers this in detail. The short answer is that when the box is under pressure, given your 16GB min memory setting, you would still see SQL Server giving up memory (and reducing the contents of the plan cache) even with LPIM. Does SQL 2016 SP1 CU3 clear the plan cache periodically? Not in a way that's different from other versions. Your title suggests that you're seeing different behavior since you applied the CU - if that really is the case, your best bet there would be to uninstall the CU (go back to the previous CU2), and see if the behavior changes. I bet, though, that you're seeing a behavior that just happens to have started recently. How do I see what's clearing parts of the plan cache? Since you wrote that the plan cache isn't entirely cleared, you need to see what databases/tables/indexes/stats are involved in the plans that are disappearing. For example, you may have a table that's hitting the 20% change threshold that triggers automatic stats updates. Since it's not really an emergency, I'm going to give you the casual, low-labor-intensive approach I'd take to fix it. I would run sp_BlitzCache periodically (disclaimer: I'm one of the coauthors, although my code in there sucks pretty bad), getting to know the top 10 most resource-intensive queries. (Maybe even log it to table with the OutputDatabaseName, OutputSchemaName, and OutputTableName parameters.) Then, when the plan cache clears, run it and look to see what plans disappeared. Do they have something in common, like do they all hit a major sales table? If so, what's the change rate looking like on that table? You can use the rowmodctr to get a rough idea of which tables are changing how often. Since you're using SQL Sentry, you can also set up a custom monitor based on this: 

Since you're running sp_WhoIsActive, which looks at all running queries on the server, you can run it from any database. Just choose your database context as master or tempdb in your Agent job step. 

For automatic failover, you need a tie-breaker. Otherwise, what would happen when the two servers had a network split and couldn't see each other? You wouldn't want them both automatically promoting themselves to primary, and both accepting writes. Think about what would happen on a table with an identity field, for example: both servers could quickly end up with two different records with the same identity field. This is called a split brain scenario, and you always want SQL Server to fail into a non-available state when that happens. To break the tie, database mirroring can use a witness server, but it can be an absolutely free SQL Server Express Edition. Always On Availability Groups is different than mirroring in that it's built atop Windows clustering, which means it relies on quorum voting to achieve the tiebreaker. Normally, you would want a third Windows server involved, but good news: it doesn't have to have SQL Server installed. You can use a file share to act as a quorum witness, or even a cloud witness. You could even get fancy and remove the quorum votes from one of your servers - but if that server goes down, your entire cluster will go down. 

Generally, running compression/decompression on the SQL Server itself is a bad idea. SQL Server is licensed by the CPU core, so it's the most expensive place in the shop to do CPU-intensive things like compression & decompression. Instead, consider offloading that to an application server. Now, having said that: SQL Server doesn't support the installation of programs and doesn't have a way to bundle 3rd party apps with the install. You'll want to manage the installation of other apps with a software deployment product or an installer script. 

SQL Server just uses the operating system's time. However, if your application code relies on functions like GETDATE(), then your code needs to be able to handle jumps forward (or backward) in time. For example, if your have code that does something like this: 

You can't. There's no such thing as per-database restrictions because a query can cross multiple databases (think joins and fully qualified objects.) SQL Server's Resource Governor in theory exists to support this, but it only works on things like CPU count, query workspace memory, and disk IO. It doesn't work on the buffer pool, so it still won't meet your needs. 

If you use a different database designer tool, or if you use different datatype changes, you can get different results. For example, if you change a field from a VARCHAR(MAX) to an INT, you'll lose full text indexes on the VARCHAR(MAX) field because you can't full text index a number. 

You've got a few different questions in here: Q: It takes a very long time to access the sales of a given store or a given item for any period. A: To troubleshoot that, we would need to see the execution plans of the queries involved, plus know a little about the query runtime and the hardware involved. 10mm rows in a header table and 110mm rows in a detail table isn't much at all for SQL Server, so this should be a solvable problem. Q: (Partitioning) worked great for finding the sales of an item. The problem is that many of the reports require the StoreID in addition to being over a specific date range. A: Correct, partitioning rarely makes SELECT queries faster. It's more about improving performance of bulk loads, specifically partition switching. I wouldn't think of partitioning as a solution to this problem, and indeed, it will actually make most queries worse. Q: Is there a preferred, correct, standard, etc. method for dealing with tables in the Master/Detail pattern in order to increase performance? A: Absolutely - archive older data. Figure out what you're going to let users query online at high speed, and then beyond that, move the data into a separate set of archive tables. You can use a partitioned view over the old and new tables in order to give them a single seamless view into the data for easier reporting too. There's a lot of advantages to this approach. For example, when you want to add additional fields to the current table, you can do that quickly without having to deal with a large amount of archive data. If you want to add lots of indexes to the old archive data, you can - because it's not getting tons of inserts/updates/deletes anymore. If you split the old and new data into different databases, you can even use different backup/recovery strategies with them - even while the view is in place, and users don't know the data is split. 

No, it is not possible to intercept and alter queries. If somebody dangled millions of dollars in front of me to pull it off, I can imagine building a wire-compatible app that acted as if it was taking queries, then rewrite them and pass them off to another layer. However, that's not a small exercise, and it's left for the reader. This is the kind of thing GreenSQL (now Hexatier) was doing, sitting in between the apps and SQL Server, doing caching and auditing. 

Uh oh - bad news. There's a known deadlocks by-design issue with MERGE: $URL$ And just generally, I'd avoid MERGE in general due to the problems documented by Aaron Bertrand: $URL$ 

First, here's my checklist on how to tackle corruption issues. It's a big deal - bigger than I can cover here - but based on the sounds of this, I don't think you've covered some of the basics (like trying to restore from backup, or making sure you've even got backups.) When you start getting corruption errors, every time you access storage, you may be making the problem worse. For example, I dealt with one broken storage array where the more we read data, the more it became corrupted. (I'd read one table and it was fine, and then fifteen minutes later, it was hosed too.) Just like a haunted house, you need to get off that storage device as quickly as possible, and tackle your recovery efforts somewhere else. Once you're on another storage device, if you can isolate the corruption down to a single table (which isn't likely, but you're hinting that you've done that), then: