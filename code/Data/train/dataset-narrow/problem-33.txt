Though that isn't always true either ^. My advice to OP would be to take this time and break apart the different apps and dependencies into separate docker containers. This would be the "proper" way to do it. This would give you something much more flexible in the long run. 

While yaml is more common JSON is perfectly valid for the Kubernetes API. All credit for the jq logic goes to @Jeff Mercado and his answer. 

I dealt with the same issue. Some people recommended using the Gitlab group "secrets" and using . Since I knew I was also going to deploy and would need other tools on my runner like "helm", I made my own docker container. I still keep my config for my clusters in a base64 encoded group secret but I set it like this in the dockerfile. 

You could have a branch per environment. So a "dev" branch would build and deploy the "dev" app to the "dev" environment. Though according to 12factor you should have your environment settings outside of your app repo. Though sometimes it's more work to do that than to just break that rule. 

Thanks for the great question. Nothing is really trivial the first time you do it and we all were new to something once. My first recommendation is to revisit docker. Try some different guides and tutorials. It's really simple. You have a docker file that gets "built", literally just commands you want ran on the "container" or "image". You push that image to a registry which can be public or private. You then run that image on a host machine. Docker is really important with node.js and python where you have lots of dependencies and it can really be hard to manage them sometimes. If you are using pip, and you should be, you can generate a requirments.txt file to feed to your docker container. Now you said you are using git, so I would use local git hooks. You can use these to build the docker container, run automated tests and then deploy your container. You can look up lots of different guides and tutorials on this subject. For managing your infrastructure I would you use Terraform. Terraform is great because you can spin up an environment on demand and delete it when done. My recommendation would be to start out simple and once you mastered docker and terraform you can try blue/green deployments. Now if you are using Gitlab or willing to switch, it also offers a free ci/cd service. This includes so many cool features and is really easy to use. I use it personally for all my apps. You could completely skip the local git hooks and test in the gitlab pipeline or keep them for testing each commit locally and using gitlab to build and deploy. I hope this was somewhat helpful. 

I believe some of the monitoring tools like Dynatrace and Scope show some neat stuff. This really depends on your platform (I'm assuming Kubernetes). However, if you are running something else checkout Visceral. Here is a neat video of it in action. It's more complicated to get up and going. 

I think @AnoE answer is correct as far as answering the question OP asked and providing some good tips. I also feel like the comment @Tensibai left is really important as well. 

You could also build on this over time to control Vertical and Horizontal scaling. EDIT: I actually prefer the Lambda. I always seem to run into these weird use cases where we can't shut down any of the instances, we have to do some math or investigation on which instances to shut down. In these situations a python lambda running boto3 is really helpful. 

AWS has something built in called cloudwatch alarms that can handle the basics. You could also use a Lambda Function. Give it access to the ASG, and then have cloudwatch configured so once the min amount of traffic starts coming in it sends an Event to the Lambda. 

I think you should do neither ;) Well kinda. I think you need more executors, maybe your builds are really resource intensive? I would run at least 4 but we run 6 to 8 depending on jobs. I like to match # of cores to exectors. So you might want to scale up your nodes, I think we run a M4 large for our 4-8 executors. I also think you should scale-out but you should do so smartly. Jenkins has a plugin to automatically scale-out on AWS depending on what is in the build queue. Basically you tell it how many jobs and how long the wait before it stands up a slave and sends the jobs to the new slave. You can also set the max amount of slaves, the min amount etc. 

In principle, it should be possible to, somehow examine the output of the Jinja template before it attempts to parse the output as an file. There exists a Python module for the Jinja renderer, but if I attempt to execute it on the CLI, I get an error: 

This certainly isn't the best place to put data, but it demonstrated using calling into Salt modules from Jinja to store data. I don't know of any existing Salt modules suitable for storing ephemeral data that only lives as long as the templates compile. I'll check back later and see what other solutions might work. 

When I encountered a related problem, I wound up having Jinja run execute Salt modules. In my case, it was to run (and, at the end of the template, for cleanup), with and , though in your case, you might be able to use Salt's extension to Jinja. So, something like this (rough, untested): 

and then access the value with the item key . I'd prefer not to use a UserParameter for this, though, as UserParameter require me to deploy additional configuration to each node, and also the process spawns consume system entropy. Both are things I generally prefer to avoid. 

There is currently one safe way to do this. You can use as an explicit way to grab the minion ID. is technically an implementation detail; the dictionary is a Salt internal structure that doesn't appear to be formally specified. Its candidacy for the purpose was pointed out to me in a bug comment. In the next version of Salt (so, later than 2016.11.5) the grain, will have been special-cased, making it possible to use safely. 

Why is this insecure? Well, grains are evaluated on the minion, so if a minion is sufficiently compromised, it can return whatever an attacker wishes for an arbitrary grain call. A compromised minion might claim to be an SSL (or VPN!) terminator to get at private key material, for example. Fortunately, there's a (theoretical) solution; use the minion ID. Minion IDs are unique, and they're verified on the master cryptographically. Except I can't find how to get at the minion ID without depending on grains. Remember, the grains are executed on the minion, so even if the Salt master knows at the transport layer what the minion ID is, if you query for the minion ID via grains, you're asking the minion for the ID, not the master. So how can I get at the master's concept of the minion ID from Jinja on the master? 

I have a templated in Salt I'm trying to build, but it's emitting invalid syntax, which is resulting in errors such as: 

What I want is 39620, from the line. With a combination of and , this would be as simple as ; fairly straightforward. I could do this with a UserParameter: 

From there, you connect to the host and examine the file you placed at . This makes sense; is in the namespace, while the modules you have access to from the CLI are in the namespace. It also makes sense from a data visibility standpoint; template rendering happens on the minion, where grains and such are available, and I've yet to see a module that executes minion code return arbitrary output to the master (for view on the CLI, for example); the returned data is invariably well-structured and concise. (There may be such a module, but I don't know what it is. It would be a preferable solution to dropping test files onto a minion.) edit: @gtmanfred's answer is far better and more direct, and I've accepted that one. I'm leaving this one here for informative purposes. It's not the best solution, but it does still work. 

Zabbix's builtin items for monitoring system parameters are reasonably rich, but there are occasionally things it doesn't seem to make trivial to obtain, such as how many dirty cache pages there are. For example, consider on your typical Linux system: 

I'm constructing a template to build a configuration file, and the service that consumes this file places constraints on identifier lengths. If an identifier is longer than, say, 6 characters, the service will get part-way through applying the configuration, fail, and leave the node in an inconsistent state. How can I perform an assertion to trigger a deployment transaction failure, preventing the target nodes' service from being misconfigured? My particular circumstance is Salt, but I would be curious to see how other systems solve the problem as well. 

There are a few different ways to achieve goals of this sort, each with some different tradeoffs. I'm going to describe the most common ones below. 

Along with adding the option, it also seems like this script doesn't actually do anything, instead just assigning a result to a variable. Given that you asked the AWS command to produce JSON, I think perhaps the following will do what you want: 

You can then interpolate this value in various places in your configuration using the name , like this: 

The simplest approach is to use Terraform's mechanism with autoscaling groups. An example of this pattern is included in the documentation. In this scenario, changing the AMI id causes the launch configuration to be re-created. Due to , the new configuration is created first, then a new autoscaling group is created, adding the new instances to an attached ELB. The argument to can be used to ensure that a given number of instances are present and healthy in the attached ELB before considering the autoscaling group to be created, thus delaying the destruction of the old autoscaling group and launch configuration until the new one is serving requests. The downside of this approach is the lack of control it represents. Since Terraform is thinking of the entire set of changes as a single run, it's impossible to pause after creating the new instances to allow other checks to be carried out before destroying the old ones. As a consequence, the ELB healthcheck is the only input to deciding if the new release is "good", and rolling back is impossible once the old resources have been destroyed. 

A second common approach is to adopt a sort of "blue/green deployment" pattern with explicit changes to two clusters. This is done by putting all of the per-release resources in a child module, and instantiating that module twice with different arguments. In the top-level module this would look something like the following: 

Bash is failing here because when used in this way it expects its first argument to be the filename of a script to run. The option changes this interpretation so that it will instead expect this argument to be an inline script to run, which seems to be what you intended here. This could be expressed in the Terraform configuration like this: 

Although the third option here is the most direct mapping of your CloudFormation approach, the second option is more commonly used due to it striking a reasonable compromise between control and workflow overhead. 

In the interests of readability though, I'd recommend moving this command into an external script file (e.g. ) and then referencing it from the config: 

If the goal is to allow this generated default name to be overriden optionally by a variable, then that can be achieved with some conditional logic: 

With a system of this complexity, it would likely be best to run Terraform via some sort of wrapper script or orchestration to make the release process less onerous. For example, such a script might automatically generate the new version number to avoid the risk of a human operator mistyping the date or accidentally conflicting with an existing one. There's some recommendations and caveats about running Terraform via scripts in the guide Running Terraform in Automation. 

If the goal is to define the S3 bucket name in one place and re-use it several times in the same configuration, the Local Values feature can do this if you are using Terraform v0.10.4 or newer. You can declare the named value in one of your files: 

Terraform scans the configuration directory for all files whose name end in , and loads them all as configuration. A common cause of this problem is if you accidentally duplicate one of the files under a new name, and do not update its contents. In that case, Terraform will see both files define the same resource and produce the error shown here. 

Local values are conceptually similar to module outputs but they are exposed within the same module they are defined in, rather than passing a value up to the parent module. 

Here I chose to use a "current date, release index" tuple as an identifier for a release. By running with a new value for this argument, an entirely separate state is created, independent of the last. Using tells Terraform that you don't wish to migrate the old state to the new, but rather to just switch directly to the new state path, possibly creating a new state in the process. You can then run to confirm that indeed the state is empty (and thus operations won't affect existing resources) and then run a plan/apply cycle as normal. Once you're satisfied with the new release, you can switch back to the previous version and destroy it. The version-specific configuration will need the id of the ELB from the version-agnostic configuration in order to populate the attribute of . To get access to this, we can use the data source to read the values from its state in S3: