Primarily, the concern with data-size arises because of the error that it can cause to the overlying model. With that concept in mind, you might want to measure the Bias and Variance of the model with your current data, understand the fitness of your model, and then take it from there. Here's an article that I wrote that may help you do the same: Machine Learning: How Big Should Your Data Be? This article talks about 

Are any of these data storage options designed for moderately big data i.e. data over a million observations? 

The Parallel Storage is supposed to be a high speed alternative for processing very large data sets. 

I have a dataset that has continuous independent variables and a continuous dependent variable. The data has linear relationships, so a Linear Regression Model works perfectly well. However, I want to, for experimentation, model the data using a Decision Forest Regression and compare it against the Linear Regression output. I am using Azure Machine Learning Studio for this purpose. I get 30 decision trees constructed. Since we're dealing with continuous independent variables, each tree node/leaf is defined in terms of or . It looks like this: 

Finally, do these data storage options make SAS function like a Database Management System - like SQL Server? 

You would then turn the text into features using some sklearn pre-processors. Count Vectorizer or TF-IDF Vectorizer are popular choices. You can also create your own features from keywords from the Attribute key and values you already have, such as creating a Brand indicator column if the words Samsung, Nokia or OnePlus appear in the text, but I would never use only manually specified features when modeling with text features. 

The ultimate end goal of your modeling is going to affect the way you want to format your data. It's a good practice whenever you start a machine learning project to ask yourself, what is the precise question you want to answer, because whatever model you generate, it's only going to make sense if used in the context of the question asked. If in your case you want to predict loan approvals, then first you need to check whether that information is even present in the data you have. The JSON you have just shows historical data, but do you know the outcome for each data point? Is that another feature, maybe recorded in the "someValue", "title" area of the file? Without it, you really can't do anything. If you can get that information, then it's perfectly fine to generate a variable for each year: assets_year1, assets_year2, ... assets_year8, investments_year1, ... etc. True, year8 might have a lot of Null values, but that's not necessarily bad. For example, most models working with text data consist of really sparse training matrices, yet they do very well in practice. Depending on the algorithm, it might weed out those variables anyways. With Null values, you just have to try imputing the Null values differently, and you can also create an indicator variable for whether the column has missing values, and see how different settings change performance. To take it a step further, you can also create additional features to encapsulate the nuances that are occurring over the years. Create features like the historical average, average change per year, total number of years of history, etc. Creative feature engineering is the hardest part, but can lead to big changes in performance. 

I can interpret what the for each of those models means. However, I can't understand what the means. Especially, why is it Infinity for Linear Regression and Boosted Decision Tree, and a finite value for a Decision Forest Regression? Edit: Data Description: The data that went into these three models is all continuous independent variables and a continuous dependent variable. There are a total of 542 observations and 26 variables. These 542 variables are split 70 - 30 to get training and testing datasets. Therefore, the training dataset has 379 observations and 26 variables; the testing dataset has 163 observations and 26 variables. No missing data. 

I want to understand better the differences between a and a , and the reason why these matrices are in different sizes - particularly in the number of words/terms. 

Edit 2 Possible Explanation - (click here): Apparently, Linear Regression and Boosted Trees in Azure ML don't calculate the Negative Log-Likelihood metric - and that could be the reason that NLL is infinity or undefined in both cases. 

I have a dataset that contains 20 predictor variables (both categorical and numeric) and one target variable (2 classes - Good and Bad). But, there are only 23 observations in the dataset. While I wait to receive significantly more observations, what tests / models can I perform on the available dataset to understand the variance between the good and bad cases, and to understand the variance within the cases classified as 'good'? Ideally, for the data to make sense, I would want the variance within the good cases to be low, and the variance between the good and bad cases to be high. Would multivariate analysis of variance (MANOVA) work in this case? 

If instead you want to create a second model, you can try using any machine learning model suitable for classification. I would probably stay away from larger models like Random Forests, but doing something like using the predictions as features into a Logistic Regression model can work well. But again, I would emphasize only trying to use the predictions from a holdout data set into the logistic regression model to make sure you aren't over fitting. 

However, if you see the performance on model one does better overall, you might shift the average to 

Your line just returns an array of scores for each run of the cross validation, so the error is telling you that it's just an array of numbers, not a model itself and has no defined method for fitting things. To use predict, you need to call it on a model object, which I'm assuming you're going for the VotingClassifier. Your variable has a fit method, so you want to call 

Given just one line of the data, it's a little hard to go off of, but I'm assuming you're trying to get at the number after each colon, and the number before it refers to the column name? If so, you can use read_csv with a little tweaking: 

I have a data set which has continuous independent variables and a continuous dependent variable. To predict the dependent variable using the independent variables, I've run an ensemble of regression models and tried to compare them against each other. Here are the results for reference: 

If you have the purchase data for all these customers, this could be one way to approach the problem: You could either cluster the customers into natural groups based on their most recent purchases or find association rules (the likelihood of the customer purchasing in a particular category based on their most recent purchases). I'm not sure how well this approach is going to work for you, but it worked for me in my scenario of propensity scoring. So, if it makes sense, try to relate your data to mine. I had a list of customers and their purchase behaviors. From those purchase behaviors I deduced association rules to determine which customer is likely to purchase in what product category (based on their previous purchases- associated with the previous purchases of the group). You can also include recency and frequency of the product purchases into the model to decide whether or not to recommend a particular product/offer to the customer for a specific time. Based on the confidence, support and lift metrics of those rules for each customer, I mapped them to highly likely to purchase in this category with confidence being their propensity score. Let me know if this works with your data. 

Is there any reason you have to use Naive Bayes? While Naive Bayes does handle multi-class modeling quite nicely, sometimes the "naive" assumption that each word in the text is independent of the others is too naive, especially given the size of your training set. While you may be able to increase the accuracy a little bit by doing more text processing, I wouldn't expect to see significant improvement, especially given the training size. Since Naive Bayes itself doesn't have much parameter tweaking, if you want to try more processing of the data itself to improve performance, you can try things like text count vectorization or TF-IDF vectorization to represent the text data more holistically than just keyword flagging. If you are able to implement other models, I would take a look at the scikit learn multi-class models for ideas. Tree based methods are also inherently multiclass, and have more parameters to tweak, so implementing something like Random Forests may suit you better. As for the training size itself, that does seem a little small. Especially if you are using text data, the more features you include, the more data you need to be able to stand a chance of detecting a signal in the data.