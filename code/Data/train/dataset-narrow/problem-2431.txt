Apart from specific applications, these seem to be interesting problems. Is it known what is the complexity of finding the vector of maximum $\ell_1$ norm among the unit vectors of a given subspace? 

It is known that given an $n$-point subset of $\ell_2^d$ (that is, given $n$ points in ${\mathbb R}^d$ with Euclidean distance) it is possible to embed them isometrically in $\ell^{n\choose 2}_1$. 

There are various examples in which a semidefinite programming relaxation allows an approximation that is superior to known integrality gaps for linear programming relaxations. For example, the standard linear programming relaxation of max cut has an integrality gap of 1/2, and this is true even for much more sophisticated linear programming relaxations (c.f. de la Vega-Kenyon and Schoenebeck-Trevisan-Tulsiani), but the Goemans-Williamson SDP algorithm has approximation .878... The Leighton-Rao linear programming relaxation of sparsest cut has an integrality gap $\Omega(\log n)$, but the Arora-Rao-Vazirani SDP algorithm has approximation $O(\sqrt{\log n})$. Perhaps less well known, Karloff and Zwick shows that using SDP one can approximate Max 3SAT, in the version in which clauses can have 1, 2 or 3 literals, within 7/8, while Goemans and Williamson had studied a linear programming relaxation that they used to prove a 3/4 approximation (Yannakakis had given 3/4 approximation earlier by other methods), and the Goemans-Williamson LP relaxation of Max 3SAT is easily seen to have integrality gap 3/4. 

Using, as in James King's answer, the notation $a(\Delta,k)$ for the best possible polynomial time approximation of vertex cover in $k$-uniform hypergraphs of degree at most $\Delta$, we also have (1) $a(\Delta,k) \leq \ln \Delta + O(1)$ from the greedy approximation algorithm for set cover: vertex cover in hypergraphs of degree at most $\Delta$ is the same as the set cover problem with sets of size at most $\Delta$, for which the greedy algorithm has approximation ratio at most $H_\Delta$, where $H_n = 1 + 1/2 + \ldots 1/n \leq \ln n + O(1)$ is the harmonic function. In this paper I show that (2) $\sup_k \{ a(\Delta ,k) \} \geq \ln \Delta - O(\ln\ln \Delta)$ unless $P=NP$, by changing the parameters in a reduction of Feige. 

The answer should be positive if your bounded-degree graph has both the property of having constant expansion and $\Omega( \log n)$ girth. The argument would be: start at a vertex, then for $n^\epsilon$ steps take a walk in which each step is chosen at random among those that don't take us back to where we were the step before. (So if the graph is $d$-regular we have $d-1$ random choices at each step.) Now I claim that, for every $i$ and $j$, if I look at steps $i$ and $j$ of the walk, the probability that there is an edge between the vertex at step $i$ and the vertex at step $j$ is $n^{-\Omega(1)}$. Then, if $\epsilon$ is chosen sufficiently small, a union bound will show that the walk will induce a path with probability $1-o(1)$. If $|i-j|$ is less then the girth, then the probability of an edge between $i$ and $j$ is just zero. If $j> i+ \Omega(\log n)$, then the expansion of the graph should be enough to argue that the existence of the edge $(i,j)$ happens with probability $n^{-\Omega(1)}$. This is because, for a fixed start vertex $v$, the distribution of the walk after a number of steps equal to the girth is uniform over a set of size $n^{\Omega(1)}$, and so has collision probability $n^{-\Omega(1)}$; every subsequent step should only decrease the collision probability (this is true for an actual random walk, but it should also be true for this non-backtracking walk), and so the collision probability, and hence the min-entropy, of the distribution stays $n^{-\Omega(1)}$, and the probability of hitting one of the $O(1)$ neighbors of $v$ is also $n^{-\Omega(1)}$. 

There is no known elementary proof that this a family of expanders, but the construction is very simple. 

By the way the Max Clique problem, in full generality, can be solved in time $2^{\tilde O(\sqrt N)}$ where $N$ is the size of the input. This is trivial if the graph is represented via an adjacency matrix, because then $N=|V|^2$, and a brute force search will take time $2^{O(|V|)}$. But we can get the same bound even if the graph is represented by adjacency lists, via an algorithm of running time $2^{\tilde O(\sqrt{|V| + |E|})}$. To see how, let's get a $2^{\tilde O(\sqrt{|V| + |E|})}$-time algorithm for the NP-complete decision problem in which we are given a graph $G=(V,E)$ and $k$ and we want to know if there is a clique of size $\geq k$. The algorithm simply removes all vertices of degree $< k$ and the edges incident on them, then does it again, and so on, until we are left with a vertex-induced subgraph over a subset $V'$ of vertices, each of degree $\geq k$, or with an empty graph. In the latter case, we know that no clique of size $\geq k$ can exist. In the former case, we do a brute-force search running in time roughly $|V'|^k$. Note that $|E| \geq k\cdot |V'| /2$ and $k\leq |V'|$, so that that $|E| \geq k^2/2$, and so a brute-force search running in time $|V'|^k$ is actually running in time $2^{O(\sqrt{|E|} \cdot \log |V|)}$. 

There is Behrend's construction of a large subset of $\{ 1,\ldots,N \}$ that contains no three elements in arithmetic progression (abbreviated 3AP-free). A random subset of $\{ 1,\ldots,N\}$ of size, say $N^{0.9}$ will contain lots of length-3 arithmetic progressions, but Behrend constructs a 3AP-free set of size $N^{1-o(1)}$. 

The constant does tend to 1/2 as the dimension increases. In d dimensions, you can have d+1 points at distance one from each other, so the the sum of distance-squared is ${d+1 \choose 2}$ and the maximum cut is at most $(d+1)^2/4$, which is a $\frac 12 \cdot \frac {d+1}{d}$ fraction of the total weight 

There is hardly any natural problem which is believed to be in P/poly but not in P. The artificial examples can be adapted to answer your question. Assume $E \neq NE$, then there is a unary language L in NP which is not in P -- unary means that all the strings in the language have the form $1^n$ for some n. Then define L' to be the set of all strings x such that $1^{length(x)}$ is in L. This is in NP, it is in P/poly, and it is not in average polynomial time 

? Of the standard version-control systems, which one is the easiest to use? (We are talking theoretical computer science professors here.) Are there even simpler tools that only do synch with smart merge, without version control? Conversely, do people who use version-control systems to even write a single-authored paper really feel that the unlimited-undo capability is worth the extra complexity? 

Suppose that your problem of interest is a minimization problem and that you have developed an $a$-approximate algorithm. If, on a given input, your algorithm outputs a solution of cost $c$, then the computation of the algorithm plus its analysis give a certificate that, on that input, the optimum is at least $a/c$. Clearly, $a$ is at least the optimum, so for every input we are able to certify a lower bound to the optimum which is at least a $1/c$ fraction of the optimum itself. In all the algorithms based on convex (LP and SDP) relaxations that I am aware of, the certified lower bound to the optimum is given by the optimum of the relaxation. If the relaxation has integrality gap $I$, then it is not going to be possible to achieve an approximation ratio better than $I$, unless in the analysis one introduces a lower bound technique for the optimum that is stronger than the lower bound provided by the relaxation. 

If $G: \{0,1\}^k \rightarrow \{0,1\}^{2k}$ is a polynomial time computable pseudorandom generator with security $t(k)$, that is such that every adversary running in time $\leq t(k)$ has distinguishing probability $\leq 1/t(k)$, then we can construct a family of functions $f: \{0,1\}^n \rightarrow \{0,1\}$ such that each function is computable by circuit of size $k(n)^c$, where $c$ is an absolute constant, but the truth-table of a random function from the family is indistinguishable from the truth-table of a truly random function by properties computable in time roughly $t(k(n))$, where we are free to choose $k(n)$ any way we want. Such a family of function contradicts the existence of a natural property computable in time $t(k(n))$ and useful against circuits of size $k(n)^c$. Now it is a matter to instantiate the parameters. If we believe that there are one-way functions secure against sub-exponential adversaries, then we can take $t(k) = 2^{k^{\epsilon}}$ for an absolute constant $\epsilon >0$. If there were a quasi-polytime natural property that is useful against size($n^e$) for every $e$, then we can take $k(n)= n^{e}$ for every $e$, and the property would still be able to distinguish our family of functions computed by circuits of size $n^{ce}$ from truly random functions, even though no property computable in time $\leq t(k(n)) = 2^{n^{\epsilon e}}$ can do that. This quasi-polytime computable property is computable in time $\leq 2^{n^d}$ for some absolute constant $d$, and we reach a contradiction when $e > d/\epsilon$. (Recall that $d,\epsilon$ are absolute constants and that we can take $e$ as large as we want.) The point here is the order of quantifiers. If you are looking for a specific polynomial circuit size, like $n^{10}$, you can have a quasi-polytime natural property that is useful against those circuits, because you can actually explicitly check in time roughly $2^{n^{10}}$ if a given function is computable by a circuit of size $n^{10}$. If, however, you want a quasy-polytime natural property that is simultaneously useful against all circuit sizes $n^e$ for all $e$ (that is, for each $e$, the property becomes useful after a sufficiently large $n$), then you are impyling the existence of sub-exponential distinguishers for pseduroandom generators and sub-exponential inverters for one-way functions.