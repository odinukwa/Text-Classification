A virtual CPU equates to 1 physical core, but when your VM attempts to process something, it can potentially run on any of the cores that happen to be available at that moment. The scheduler handles this, and the VM is not aware of it. You can assign multiple vCPUs to a VM which allows it to run concurrently across several cores. Cores are shared between all VMs as needed, so you could have a 4-core system, and 10 VMs running on it with 2 vCPUs assigned to each. VMs share all the cores in your system quite efficiently as determined by the scheduler. This is one of the main benefits of virtualization - making the most use of under-subscribed resources to power multiple OS instances. If your VMs are so busy that they have to contend for CPU time, the outcome is that VMs may have to wait for CPU time. Again, this is transparent to the VM and handled by the scheduler. I'm not familiar with KVM but all of the above is generic behavior for most virtualization systems. 

10 mins ago I posted a response where the problem is quite likely the same issue you're seeing - Consumer-grade hard drives have onboard error-recovery systems which can cause those drives to drop out of an active raid-array, as it stops responding for too long (and the raid controller assumes it's dead). You need enterprise-grade drives without these onboard error-recovery systems (as they're assumed to be handled by the RAID controller in enterprise deployments). As to your question of where to go next - Based on the above as you can probably gather, I'm very much in favour of utilising business-specced hardware in business scenarios, because consumer-grade hardware often causes unexpected problems with reliability and performance. Just as you're seeing right now! :) 

The two domains will not interfere with each other on the same network. There will be no trust established between them unless you manually establish one. The DHCP issue is a valid point, and your potential fix is correct - You can hand out the DNS address of one domain via DHCP, and use a forwarder to resolve the other domain's namespace. An alternative fix would be to manually configure networking for the clients on one of the domains, and point their DNS manually at the correct domain controller. You can leave the other domain's client working from DHCP. We have a few subnets that are used for internal testing and have 5+ different domains running on them, no real issues to speak of. 

The system will take a variable number of SAS cards/shelves. Each MSA70 can be daisy chained with a second one, and each card has 2 SAS ports. So 2 cards in the server = 8 shelves with a max capacity of 60Tb RAW, and you can scale all the way down to a single card with an MSA bundle deal that'll yeild the 4Tb you need. 

I don't think it's worth implementing a centralised server for your AV if you have 25 clients and are a non-profit with low budgets. You will gain max benefit with min outlay by: 

I don't really understand your terminology - Are we talking about OUs, Groups or Domains here? They're all very different things. Could you clarify this? Regarding your question about executing a script across many workstations without remoting onto them: You should be able to achieve this with PSExec. 

In your specific case, look at the number of concurrent users your system will need to support, and the kind of physical hardware specs you'd need to run it as a physical machine. If it requires a 4-processor, quad-core beast with 32Gb of RAM and a local 6-disk SAS drive stripe, it's not a good candidate for virtualisation. If it has high requirements on any one of those aspects (e.g. just needs an ultra-fast disk) it's in the 'maybe' pile and needs a round of testing before making the decision. If the database would run fine on a basic 1 or 2 processor server with a modest amount of ram (under 8Gb) and disk throughput isn't excessive, virtualise it. If the choice you're making is between purchasing brand new hardware for the system, or virtualising onto your existing VM infrastructure, then virtualise it first and migrate to a physical server only if required. The hallmark of a well planned server is that you can easily re-build it again on-demand ;) 

It wouldn't be too surprising given that the CPUs (and therefore the surrounding hardware also) are nowhere near equal. The Atom CPU is tailored towards low utilization, low-power-consumption usage, and lacks a fair number of features in order to reduce its power draw. Your specs show a number of factors straight away - The VPS has higher clock speed, larger memory bus, MUCH larger cache. 

The domain checks out OK using online DNS tools. The only noteworthy warning was: It seems you don't have MX records. This means that all the emails sent to your domain will be sent to the A record of your domain (if present), accordingly to RFC 2821 section "5. Address Resolution and Mail Handling". Checked on: $URL$ DNS does not update instantaneously and your regions DNS servers may be taking longer to update than google or OpenDNS. If you made changes within the last 24h, it's likely that it simply hasn't propogated to the regional DNS yet. In which case, you have to wait. 

Shut down all active services/applications on the server Copy all data off the second partition to a backup location Delete the second partition Use windows Disk Management (if 2008) or GParted LiveCD (older versions) to extend the C:\ partition to the desired size Re-create the second partition Copy all data back to the second partition Start your services and launch your applications 

Short answer: Neither of them. Long answer: You really need to schedule all experimentation and implementation on test servers, and during scheduled/planned outage periods. This isn't something you can add to production on-the-fly. You can approach NLB a few different ways but here's my suggestion. Allocate: 

IMO the VDI/Thin client architecture still doesn't stack up as particularly cost-effective, and won't unless the cost of thin client hardware drops significantly. The main benefit is for situations (e.g. Point of sale) where you want to keep the operation of the system as close to the centre as possible). One option I often float and implement cheaply/effectively is to setup a terminal services environment that serves up the core business apps to any and all clients - This ignores the stuff that people like to customize (development environments etc) but standardizes access for your 'cookie cutter' applications (e.g. timesheet system, payroll). This, and/or shifting some stuff out to SaaS applications (e.g. openair.com) can cut down the amount of extra hassle you have to go through for giving VPN users access (you just let them reach the TS), and less time spent configuring each PC with all the fiddly apps. 

There's 2 main components to templates, if you're running vSphere (ESX and vCenter). The template Vms themselves, and Guest Customization. If you configure guest customization (just by coping sysprep for each OS onto the vcenter), all the windows deployment steps (naming, network, license, time zone, owner info) is taken care of for you. So once that's running, all you have to do with the templates is provide a base OS configuration, patched, with a temporary network name. If you have 'standard build' apps that you must layer on your builds (e.g. monitoring agents, wallpaper) then you can do this to the template VM before marking it as a template. Beyond that, I don't think there's too much more to it, everything else you could add on is at your discretion. I find that keeping a relatively small number of templates/guest customizations, usually one per OS, is sufficient and saves a chunk of deployment time. 

For every IT-related system you maintain, throw it through this filter question: "Do we use this system in a standard manner like other companies?" Good examples of this are Email applications, Desktop workstations, and Intranets. Many companies, even specialist companies with specialist IT requirements, still use email, workstations and intranets in the same manner as most other companies out there. These systems, being used in a standard manner, are ideal candidates for outsourcing to a third party. On the other hand, systems tailored for a very specific "unique to your business" requirement, like a bespoke analog dialinig application which uses highly customized interfaces with the remote equipment. Written in Fortran. On HPUX. Yeah... don't outsource that bit. Maybe outsource the HPUX support element of it, though! Often you can extend this a bit further and find a good route. Maybe say 'OK... currently we have an email system with a bunch of custom forms and outlook add-ons that we depend on to do business. But with a few weeks of development work, and some buy-in from management, we could transition just that custom element out to a standalone system. Once you do this, you can outsource our email. I've seen companies with specialized IT requirements attempt to outsource wholesale, and encounter a whole world of pain. Likewise, I've seen companies with specialized IT requirements identify their differentiators, standardize, then outsource, and end up much more efficient as a result. 

Ms Office has TS/RDS-specific license keys which are the only ones that'll work in a TS/RDS environment. If you try and install Office on a TS/RDS with a regular key, you'll get a message telling you that it's not valid for your environment, or 'I see this is a terminal server but you're trying to use a workstation license. No can do'. You'll know you've sought out appropriate licencing once you manage to get hold of a TS/RDS key for office. As to the specifics of getting hold of one of those keys, see the dupe questions that's linked. 

Yep. The netsh command can be used with the -r -u and -p switches (remote computer, username, password respectively) to remotely configure windows networking. 

In an environment that supports it, we've found NetFlow Analyser to be a very useful tool for capacity planning, identifying bottlenecks, and monitoring the health of the network. You can even use it to check if backups are operating as expected (did x amount of data flow across Y link between hours A and B?) or to monitor disk performance on an iSCSI network (tap into the ports on your storage controllers and monitor throughput). However, it requires switch support to operate correctly, which you likely don't have given your comment about cheap switches. Based on what you've posted I'm assuming your main driver for this is that the video viewing performance is a problem? You didn't mention whether viewing/serving the video is isolated to a limited number of machines, or whether it could take place between potentially any of the machines on the network. The answer to this would completely change the approach you need to take to this problem. Do the green lines indicate video flow? Your diagram shows 12 buildings and your description hints that they're all in close proximity. What kind of environment are we looking at here? I'm guessing a school, college or hospital? My core advice here, given what you've posted, is kinda tangential to your request - Take a look at how much you're paid per hour and how busy you are. Look at the cost of purchasing a few Cisco/Juniper 24-port Gigabit switches for the core network at head office. Chances are, it's more cost effective and a better use of your time to upgrade the core network, than diagnose the existing obsolete infrastructure and attempt to tweak it. 

If you wish to change the host that the VM is running on, you need to initiate a vMotion operation to move it to another host. The 'Host' line will change correspondingly once the VM has been relocated. If you wish to change the host line from an ugly IP address to a pretty DNS name, you need to create a DNS record for the ESXi host, edit the host's DNS settings so that it uses the DNS servers, then tell it that it's now has a name under the Configuration -> DNS & Routing -> Name option. If your host is managed by a vCenter, that vCenter also needs to be able to retrieve the host's name via DNS, and you'll need to remove and re-add the host to the vCenter using its hostname instead of its IP address. 

Super basic solution: You can configure your DNS entries for the website using DNS Round Robin to point to multiple host IPs. Raise a seperate elastic IP for each front-end instance. On the DNS side, it's just a case of adding a second entry for the same hostname, with the alternative IP. Set the TTL on each entry quite low (5-60 seconds). More info here: $URL$ 

One thing that's not immediately apparent about Windows Storage Server is that it's tied to particular hardware that you comes with it pre-installed, rather than being an OS you can throw on hardware of your choice. So it's probably out as an option, unless you can purchase the additional hardware. For a VM-based solution running on your virtual environment, you could look at Windows 2008 R2 with DFS replication to a remote site. Alternatively, Openfiler with DRDB replication. Both can authenticate via AD if you need this, but I don't think either of them will do data de-duplication at this time. 

This sounds like one of those situations where you make a perfectly sensible configuration choice (disable balloon notifications to improve the user experience). Then something comes up that conflicts with that decision. At that point, you can fudge a compromise (and typically end up with a big mess, or something absurdly complicated in relation to the actual size of the problem). Alternatively, back out your change. In most cases I reckon it's best to take the learning experience, and back out of the earlier decision. tl;dr Re-enable balloon notifications. 

In a 200-person environment I would imagine you have some Windows servers floating around there somewhere. If the printer isn't already available via a print queue on one of your servers (i.e. your clients are printing directly to the printer IP), alter this so that your clients print via a shared printer queue from a server. This will have a nice side benefit of automated printer driver installations for client. Once the printer is used via a server, you can turn on printer auditing for a view of who's doing what and when. See this article for more info: $URL$ 

The snapshot feature on VMWare can be flaky if you keep a snapshot on a VM for too long. When there's a snapshot active on a VM, the system keeps a static copy of the VM hard disk & generates a kind of diff file to track changes from that point onwards. When you 'delete' a snapshot, what really happens is the diff file and base disk get merged together. The system works quite well but has limitations with: 

You cannot downgrade SQL editions without re-installing the software. You can, however, detach your databases from the Enterprise installation, uninstall, then install Standard edition and re-attach the databases. As for cruft, there won't be anything of note unless you've made lots of odd tweaks and changes. You'll want to reboot between uninstalling Enterprise, and installing Standard. 

Apologies for answering somewhat off-topic. Lately though, I cringe at home and small business folks talking about paying for AV. I really don't think it's justifiable any longer.