Some more examples are found here, including saving only improved models and loading the saved models. 

The 2015 Thumos challenge was about action recognition in video collected from youtube - totalling 13k videos (430 hrs or 45 million frames) and 101 'action classes'. The video is of activities like people brushing their teeth, playing basketball, and surfing, and some video clips included multiple activities. The video is untrimmed, so it's closer to a real-world setting than video edited down to exclusively a single action of interest. The winning and second place entries for action classification included CNNs in various guises (after a quick scan I couldn't find any entries that didn't). Another use you might be interested in is video captioning (not classification): this paper incorporates CNNs to describe in words what is taking place in each video (examples from p9 of the article, I love the descriptions at bottom right): 

Not sure if you've tried this already, but you might dig into XGBOOST feature importance to establish how your model is making predictions, then do a more in-depth comparison between the two populations at those splits. This isn't very different from what you've proposed, but it does make for a more focused analysis; you might have time to look at higher order interactions between the more important features. (I'm assuming it's not possible to cheat by training a classifier on your population B and comparing how each model makes predictions!) ELI5 and LIME also come to mind for debugging feature importance and explaining model prediction. 

Setting 'save_weights_only' to False in the Keras callback 'ModelCheckpoint' will save the full model; this example taken from the link above will save a full model every epoch, regardless of performance: 

It comes down to model interpretability and explainability. Given the output of a simpler model, it is possible to identify exactly how each input contributes to model output, but that gets more difficult as models get more complex. For example with regression you can point to the coefficients, with a decision tree you can identify the splits. And with this information, you could derive rules to explain model behaviour. However, as the number of model parameters increases, it becomes increasingly difficult to explain exactly what combinations of input lead to the final model output, or derive rules from the model's behaviour. Say in the financial industry when the COO comes over and asks 'so, why did your high frequency trading algo break the economy', he doesn't want to hear how it was built, just why it sent him bankrupt. It will be possible to state how the model was constructed, but it might not be possible to explain what combinations of factors that the model received as input led to the output, and thatâ€™s why people are talking about black boxes. 

The advantage of using pre-trained vectors is being able to inject knowledge from a larger corpus than you might have access to: word2vec has a vocabulary of 3 million words and phrases trained on the google news dataset comprising ~100 billion tokens, and there's no cost to you in training time. In addition, they are fast and easy to use, just load the embeddings and look them up. It's straightforward to substitute different sets of pre-trained vectors (fastText, GloVe etc) as one might be more suited to a particular use case. However, when your vocabulary does not have an entry in word2vec, by default you'll end up with a null entry in your embedding layer (depending on how you handle it). You'll need to consider the scale/impact and how to address it (keep/discard/consider online training). As yazhi says a decision must be made about how to handle out of vocabulary words. The advantage of learning word vectors from your own corpus is that they would be derived from your dataset, so if you have reason to believe that the composition of your data is significantly different from the corpus used for the pre-trained vectors then that may result in better downstream performance. However, that comes at a cost in time taken to train your own vector representations. 

There are a few approaches you might take to establish how and whether accuracy can be improved. Some options sketched out here in brief are to compare network results to a baseline estimator, diagnose misclassifications, apply dimensionality reduction, and network architecture troubleshooting. Without knowing much about the data, I think you could try to establish whether it is even possible to increase the accuracy of your network by comparing network performance with a baseline estimator like a decision tree or support vector machine. The bonus of using a decision tree is that an algorithm such as ID3 uses information gain to make the splits and this might give you some intuition about your data and whether it's possible to improve accuracy before sinking time into it. If you haven't already, you could do some exploratory analysis to understand how noisy the data/classes are. It may also be useful to take a closer look at the errors your network is making. A confusion matrix or classification report can help you to diagnose whether your network is struggling with (for example) one particular class or a specific area of the decision boundary. If your classes are unbalanced, you could look at re-weighting your training examples and approaches such as SMOTE etc that Benji mentions. With 70 features, the network may benefit from some dimensionality reduction. If you apply PCA to the data you can establish how many components you need to explain a reasonable amount of variance in the data, or if it is more complicated. It is possible that your network may respond better if taking the transformed data as input rather than the raw data (on that topic, have you pre-processed your data?). Finally, a broader architecture search might yield improved results - I'm assuming you may have done this already, but have you tried increasing the units in each layer or cutting out that second hidden layer? You might also try using different activation functions or a different learning rate schedule. Bengio's 'Practical recommendations for gradient-based training of deep architectures' is worth a skim (the article is aimed mainly at deeper networks, but elements of sections 3.1, 3.2 and 4 are relevant). This answer has only touched briefly on each areas but I'm hoping there are some solid leads you can chase up. 

It is at this stage that I see strange behaviour and I'm unsure how to proceed. Do I take the .best_estimator_ from the GridSearch and use this as the 'optimal' output from the grid search, and perform prediction using this estimator? If I do this I find that the stage 3 metrics are usually much lower than if I simply train on all training data and test on the test set. Or, do I simply take the output GridSearchCV object as the new estimator? If I do this I get better scores for my stage 3 metrics, but it seems odd using a GridSearchCV object instead of the intended classifier (E.g. a random Forest) ... EDIT: So my question is what is the difference between the returned GridSearchCV object and the .best_estimator_ attribute? Which one of these should I use for calculating further metrics? Can I use this output like a regular classifier (e.g. using predict), or else how should I use it? 

There are other metrics you can use to directly compare classifiers instead of accuracy, such as Precision and Recall to compare how your classifiers catch all possible true and false cases individually. This gives you a much better idea of how your classifier performs on different cases. Or F-Measure(can be used for multiclass too!) and Matthews' Correlation Coefficient (binary classifications) to get a single metric that 'combines' the performance of your classifiers in several areas such as precision and recall to get a single comparable metric. 

What you want to do is develop your code in IntelliJ, and then package your code and dependencies into an executable jar file using SBT or Maven. When you have your jar stored locally, you can use spark-submit to transfer the jar to your cluster (along with some other parameters) for execution. You might also want to take a sample of your data and store it locally so you can run spark locally and test/debug your code in IntelliJ. This can speed up development considerably, and having access to a debugger is a huge help. 

I suggest training/testing your classifier on separate splits of the original dataset, and then printing a confusion matrix: $URL$ This is a way of seeing how many of the 'true' classifications your classifier predicted correctly or incorrectly, and the same for 'false' classifications. This will give you more information than just 'accuracy', because a model trained on data where most of the classes are 1, for example, will predict 1 most of the time because it will probably report reasonably high accuracy in doing so. A confusion matrix is like a sanity check for this. 

I think it makes sense to stick with Classification here, since you already have examples of fraudulent and non-fraudulent calls that you can train on. It might also be beneficial to train several models for different regions based on IP, as well as your 'global' model, and apply the region specific and global models to incoming calls. Just a few ideas. From what I understand, real-time learning would require immediate feedback, which most fraud detection systems can't provide. For example, it may take a few days or weeks to have a case of fraud resolved (labelled fraud/not-fraud), and therefore take some time for the learning system to receive the feedback on it's prediction. 

I'm not a business analyst so I guess you'll have to take what I have to say with a pinch of salt. From my understanding, Business Analysts responsibilities are focused around improving processes within a company, for example how certain technologies could be implemented to improve a workflow, they are expected to understand how these technologies might improve the workflow, or a product etc, and manage these improvement projects. This seems to differ from Data Science on an abstract level in that it is exploring known unknowns ("can our process be improved? what technologies/methods exist that could improve it?"), whereas Data Science is great for exploring unknown unknowns. For example, why is this better for our workflow/product specifically? Data Science is great at throwing up results you don't expect, which is one reason why it is so valuable. I may be wrong in saying this, but Business Analysis seems to be relatively free-form depending on the company and the needs, whereas Data Science has a less subjective methodology. With this in mind, perhaps Data Scientists could be used to better inform your Business Analysts decisions? But the other way around, BAs could perhaps be used to better inform Data Scientists of business processes, or maybe your BA could focus on improving processes to make life easier for your Data Scientists such as data pipelines in a non-automated environment (example: how can we gather more useful data from our vehicle showrooms? What technologies do the DSs need and how can we implement them?). 

Please clarify what you are looking for in the presence of cycles. I'd assume the cycles aren't standalone- any node in a cycle could also have inputs from further upstream, and could also send output further downstream. Even in the unweighted case, it seems to me that some nodes in a cycle can be more "upstream" than others. For instance, most of the nodes in the cycle might be downstream of a set of source nodes. One node in that cycle might be the only path out of the cycle to a sink. Surely that node is more "downstream" than the others in the cycle? Sorry the above isn't just a comment - I'm too new to have high enough reputation in this group for that. So, to give an answer as well. Take the "flow network" analogy to an extreme. Imagine each source upstream of the target node puts out a water flow, with dye added at the first instant only. Treat the weights as time delays. For each node, calculate the first time the dye reaches a given node. Among all the sinks downstream of your target node, pick either the longest time or the mean time (or itself, if the node is a sink). The target node's position is then just its early arrival time divided by the sinks' mean or longest arrival time. Computationally, this earliest dye arrival time is done by some modification of breadth first traversal of the directed graph. Start at the sources with arrival times of 0, and then each node arrival time is just the minimum over all its predecessors. (As in any graph traversal, visited and completed nodes have to be remembered so cycles aren't followed more than once. This would also be called shortest distance from a set of nodes to a given set of nodes. But note that cycles aren't given any particular special treatment. If you really want cycles to matter, you could assume that output flows are split equally, and do mixing calculations, so that the dye concentration would decay gradually over time. But that would seem ridiculously complex, and take time, for probably no benefit. But in either case, the nodes in cycles won't all have the same value - some will be more "upstream" than others. If you really want all the nodes in a cycle to have the same value, you might need a separate pre-processing step to find the directed cycles, and aggregate each cycle into a single node (one for each cycle). That adds complexity, although it simplifies the shortest distance calculations since then the graph will be acyclic and a trivial breadth first traversal will work. You probably don't really want to require a global analysis of a graph to get your feature value, although that's what you had to do in your initial analysis too. If this is something you would run on a regular basis, you could probably just do local updating when changes occur, with limited propagation of earliest arrival times, assuming you could tolerate some "error" in the somewhat arbitrarily-chosen metric anyway. 

The original Savitzky-Golay paper addressed smoothing, meaning that you estimate a value for some point in the past, using values from its past and future. It sounds like what you're interested in is filtering: at time step k, you want to reduce noise and come up with a better estimate of the value at the same time step k, without knowing the future. That's important in many fields such as process control, and some fault diagnosis where delays can't be tolerated. The SG approach of calculating the constants for least squares fitting for smoothing has also been done for filtering. A curve is fit through n recent points, and instead of estimating the middle point, the end point is estimated by a least squares fit, reduced to simple multiplications by constants. There are tables of the necessary coefficients and an example at $URL$ There is no time delay in the calculations, unlike with smoothing. Also, if the input signal is a ramp, the filter will converge to the correct value once all of the points in the time window reflect that ramp. On the other hand, there are some side effects that may or may not matter to you. Even in the linear case, for a step change in the input, there is overshoot in the filter output. Higher order least-squares filtering can be quirkier, so it is rarely used. But the linear version overall is quite effective, assuming the little bit of overshoot isn't a real problem (for instance, if you don't see step inputs in the data anyway). The link above is part of a more general "Guide to Filtering", that covers some other common filters as well, such as the exponential filter and moving average filters in particular. (Filters don't have time delays, but most do lag the inputs - meaning smear out changes over time. That's a byproduct of filtering out the noise, essentially cancelling out some new noise by a certain amount of averaging against old noise). It also covers general issues for filtering, such as aliasing. There are two other filters that are not covered in the Guide that might be considered close competitors to the SG-style filtering: the alpha-beta filter and double exponential smoothing. These are covered in Wikipedia: alpha-beta filtering has its own page, and double exponential smoothing is part of the page on exponential smoothing. The goal of both is to filter/estimate both the values and the derivative, resulting in a filter that will give correct outputs to a ramp input signal. I've long suspected that these two filters are essentially the same, but never taken the time to look into it.