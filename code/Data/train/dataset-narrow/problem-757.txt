Each table will have its own little bit of disk. These are unlikely to be contiguous. Every query will have to hit every table, even if they're indexed. It is unlikely to be fast. Once again this query must change every time the list of regions changes. It can be hidden from the application somewhat by enclosing it in a view, but the maintenance task remains. As a starting point, I would maintain a single Users table with a foreign key to showroom, just as you've shown. If testing proved to be a problem in practice at the anticipated scale, on production-sized hardware, I'd consider propagating the key of Region into SubRegion and Showroom, giving them compound keys. Users table can then be indexed / clustered by Region to satisfy the given query. I may even use the natural key for this, rather than the surrogate, to remove further steps from the execution plan. The foreign key constraints will ensure only correct Region_Id appear in the child tables. This could also be achieved by demnormalization. Region_Id column is added to Users but not to SubRegion or Showroom. The value is copied in when a row is written to Users. Assuming a user never switches Showroom/ Subregion / Region you're done. The system cannot help you ensure Users.Showroom_Id and Users.Region_Id correspond to each other. 

Do the same thing for monitors and use the local table variables to populate the mapping table. Of course you want to have appropriate validation, duplicate checking and error handling in the body of the SP, too. You don't say what scripting language you use. The documentation for it will tell you how to declare and populate stored procedure parameters for SQL Server. Response to OP's EDIT #2: First, a few tips. Please post the full error message; it helps immensely with debugging. Second, if you're using SSMS you can double-click an error and it will highlight the code in error. Third, get in the habit of closing your statements with a semicolon. It is not required yet but it will be soon. If all computers have exactly one monitor then the TVP is not needed. You are correct. How many developers have only one monitor these days? I've seen finance traders' stations with eight. In these cases you do want a TVP. Please, please, please do not be tempted to write . Your code will throw the error This is because of your third INSERT statement: 

Most likely using a cluster for this query won't be beneficial. A cluster in Oracle allows data from multiple tables to be stored physically close when they share a common key (here I suppose). This allows some query to perform better but a cluster will intrinsically consume more space than standard heap tables because for each key there will be some unused space. Insert-only heap tables on the other hand are one of the most efficient way to store data space-wise, since the rows fill all blocks nicely up to the HWM. In your case since you don't have a filter so all rows will be read, producing a FULL SCAN of the data. Because the rows are stored in a more compact manner in heap tables, the cost will be less than the cost for the cluster. The cluster, however, should have an edge when you look for a specific key, but this will also depend on the distribution of the data (number of rows per key), and on the length of the rows. You could build an example where the heap tables with regular B-Tree indexes will outperform a cluster for single-key queries. In conclusion, clustering tables in Oracle will help for some queries, but will also be hurtful to others, it has restrictions and drawbacks, it is not a silver bullet for optimal performance. Heap tables are the default for a good reason: they have good performance for most queries. 

Only one base table is updated All other tables are key-preserved: each of them must have at most one row for each row of the base table. 

The most likely cause of a mutating table error is the misuse of triggers. Here is a typical example: 

In Oracle, DDL on remote database is not permitted. One likely reason is that a distributed transaction commit can not be initiated at the remote site (you can't ) and since DDL statements include a commit they are not permitted. You would get an with other DDL statements: 

There are several ways to get to this answer. I'll tackle it in two parts. The first is to find out how many groups there are. This is a simple summary query: 

Initial observations are that Index Type and Index Composition are attributes of Index, not of Acceptable_indices. There will be relationships between a security and an index; whether this is important to your use case or not can't be determined from the current information. Concentration should relate to another entity type saying what it is a concentration of. More generally you need to distinguish what is an acceptable set of values from what was actually in the transaction executed. The acceptable set is your risk and exposure policies documented in the form of a normalised data model. You have to think carefully of all the conditions, including the obscure, infrequent and exceptional ones. For each executed transaction the model must capture sufficient information to determine if it meets the risk policies or not. There must be a link between an individual transaction and the set of rules that apply to it. The easiest way is to set the rules by security type e.g. one set for bonds, one for equities, one for futures, one for options etc. Another complication is the time-dependent nature of the data. I'm guessing this application will be for analysis after the transactions are complete, rather than pre-transaction authorisation. Likely there will be a number of days between transaction and analysis. So you need to know what rules were in place when the transaction occurred. Everything must have start and end dates (or datetime values, depending if things can change intra-day). For a physical database design you could implement one table per entity type. This is the usual way in the absence of a strong incentive to do otherwise. Queries may be longer from this approach (if there are many limiting factors the query will have to bring in the table with limit's allowable values and the table including the actual executed value for each factor). Queries are very obvious, however, and easy to implement and maintain. The runtime engine should have not problem executing them. Alternatively each limit could be considered as a feature of the deal. The tables reduce to bags of feature values. To check if an executed trade breaks a limit one compare the "allowed" bag of values to the "executed" bag of values. This would be a variation on the Entity Attribute Value (EAV) design pattern. Often this pattern is considered dangerous. Queries are obscure as everything is a "bag" or a "feature"; it is never obvious what each particular feature represents. Writing queries to ensure the limit's feature is compared to the corresponding feature on the trade can be painstaking work. Maintenance even more so. Performance tends to be worse because this is adding an extra layer of abstraction on top of how the engine expects to work. It is wonderfully flexible, however. If you get things just right adding another feature can reduce to a simple cut-and-paste. A third option would be to hold each value as an attribute. This make the tables very wide. The DBMS will be able to cope with this (up to a limit!) but makes things difficult to read. The real problem comes when there can be multiple acceptable values. For your example there may be two acceptable indexes in the limits and two indexes involved in the trade. You created columns and on each of Limit and Trade table. To check validity one must compare all possible combinations i.e. 

You can just go to the table after it is created in SSMS and right click and then select the Script options in the menu. 

I cannot provide a full example right now but I would recommend looking at two modules that can help you build out the desired Excel file much easier. 

Yes you can deploy that scenario to EC2 just as you would if it was VMWare environment. However with AWS I would highly suggest you follow the guidelines on how to properly set this up from Amazon. 

The basic options for replication in SQL Server are available between AWS and on-premise...as long as you have the network access in place. I'm not sure what source you used to find it was not supported. AWS also offers the Database Migration Service which supports multiple RDBMS and various targets. You can find more information on that here. 

You will not be able to detatch system databases. If are not open to properly migrating the server level objects to another server, then I would opt for restoring system databases over trying to them. Ensure you are on the same, exact build number or master will not restore. You will also likely have to update the server name in master, if your new server does not have the same hostname. I would also insiste on a backup of every user databae before that LUN is anywhere. (Just in case.) 

Truncate Table Fetch Data (source on these is either remote database or local database where job runs) SQLBulkCopy (.NET class, into destination table of database where job runs) 

This is more for SO than DBA, but if you are using the identity/password for the app pool in IIS it will encrypt the password for you when it is stored. However, be aware that even with this process passwords can still be decrypted with not much work. The connection string itself does not allow you to enter an encrypted password, the mechanism that is saving the connection string handles encrypting sensitive data (or should). 

There seems to be something wrong here: the 0 cost on the index full scan is suspicious and if I had to guess I would say that you're missing something: probably the stats on the index. This in turn leads the optimizer to believe that it can run the FULL INDEX SCAN "for free" and goes on with a suboptimal plan. This could also be a rounding error problem, since there is very little data (1k tiny rows, probably fits in a single block!). So either there is some stats missing, or too little data to be meaningful. Interestingly, if we run your test with a large sample (say 1M rows), the optimizer is happy to go with an index scan. If we insert some data instead and do a standard stats analyze, we find a more logical plan (11.2.0.3): 

One way to force data to actually be overwritten would be to update it to a meaningless value before deleting the row. This wouldn't work with indexes since updates are translated to delete+insert in a b*tree index. 

You can create the database link by connecting directly to the remote database. As suggested in the askTom discussion, you can also use or to create a distinct remote transaction that can initiate the DDL statement. 

The jobs submitted while the window is closed should be queued and run later when the window is opened. 

I don't think Oracle keeps track of past closed queries. However, you can find out what cursors a session has opened with . Since many applications cache the cursors for later reuse (this is automatic in PL/SQL: a cursor won't be completely discarded unless you reach the maximum number of open cursors), in many cases all past queries will be in this view: 

In the first case the database will interpret PROCESSED as a variable, and its value, even if constant won't be learned until execution time. An index on will be used only if status has a strong selectivity for all values (ie there are many different values). Since you have only 3 values, Oracle makes a FULL SCAN since from its point of view any of the 3 values could be used. In the second case, an index will be used if you have statistics on this column that show that the value 0 is very selective (ie there are few rows processed). Oracle knows at compile time that this value won't change thus an index scan will always be effective. So, if you're sure that there are few rows processed, you will have to help the optimizer, for example with an hint like . Or you could use comments in your SQL: . Also remember that FULL SCAN are not evil. 

Not that I am aware of currently. I would suggest the best method of getting data out as soon as it occurs is to query the XML for the target and just shred that out. I can repeat the above example and as soon as I execute the query below I see the event. 

If you just acquired an instance of SQL Server from someone, I would first ask have you gone over the instance configuration and all to understand how the previous person had it configured? I would suggest taking a look at Brent Ozar's sp_BLITZ script. It is a stored procedure that will collect some good information for you on your instance configuration and give you a good overview of what it sees. I would read over Brent's blog on this script in detail before ever running this on your server, FULLY understand what it is doing. Regarding your database, are you running out of disk storage or database storage? How much free space does the database contain? I would first understand the architecture of the files before I start making changes. A good article to read on file management is from SQLSkills.com here. Just my $0.02... 

Additional: If you add this as a step to your job just make sure the previous step is set to only "execute next step" on successful action. On failure for your previous step should be set to stop job with as failure, based on your requirements. 

You run a server-side trace or Extended Event session to capture login activity. I also would not have just renamed it in production. If you are referring to the account itself, disable it first to verify if anything breaks; and leave that disabled for a day or up to a week. How long you leave it is based on knowledge by the application owner. If they are not confident leave it disabled for at least a month. If you are renaming just any old login to something else for a purpose, you obviously cannot go with the disable route. In that case you just have to let your trace run for a period of time to capture the activity. One other option you might try is to just put in a server trigger that just writes an audit of login activity. There are a few examples of this on the web.