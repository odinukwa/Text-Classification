The tag and the tag are (somewhat hard to use) ways to embed audio on a page... ...but what about creating sound effects in a browser-based game? E.g. if the game becomes aware you died, I plan to have some music play on the "you're dead" page. But what about some kind of fighting sound when people click an "attack" link, or the sound of flowing water when they click to "heal". Do I essentially have to look into embedding an invisible flash widget like facebook does for some of it's sounds in order to make this work, or are there other approaches? 

I'd like to use some pixel characters as a base for tweaking into characters for my browser-based-game $URL$ . (In other words, I take a base template and I modify it with colors and pixels slightly to make character variations) Now, my bbg is mostly ninja oriented, so a ninja avatar to work off of would be great, but more varied character types would be good as well. And I can probably fudge a "ninja" from some other template. But, err, free-to-use-for-commercial-purposes (e.g. public domain) would be necessary. I'm already makin' a little money off the site from Ads, and would certainly like to make more, so I can't use stuff for "non-commercial use only", unfortunately. So any source for free-for-commercial-use pixel-y 32-bit-y characters to use as a starting design base? 

Check out this simple fragment shader implementation with explanation on why's and how's $URL$ Although it is fragment/GL and not D3d9, it's simple as it gets, so you should be able to understand what you need to do. 

I was very fond of Andrew Rollings and Ernest Adams on Game Design. It goes into depths of interactive game mechanics as related to player, which is the only unique element to game development as editing is to movies. Having said that I'd consider suggesting screenplay writing material as it shows you what connects with people and what doesn't. I'm not talking about linear progress material here like stories, but what kind of characters connect with people, themes, motifs etc. In that area I suggest these: 

For example, get coordinates on a designated event in the game. For example when player dies, you get coordinates in your scenegraph where player died. So once you have values you need in your database, enumerate each coordinate how many times it appears in your list/database. You'll have a list something like: 

I come from a different stream of life (TV Production), but it's basically the same. You need a production bible. But, to put it more simply - you need what you need and nothing more. 

You are making a 3d engine. You want best of multiplatform worlds. Suddenly you realize that if you want to use Direct3D on Windows machines and OpenGL on OSX/Linux, you'll have to sacrifice supported features of both to least common denominator. Some may use OpenGL across three OS', since it appears to be least common denominator by itself. All is good. Then, you have to port your graphics API backend to Nintendo's GX, you also have to make a PS3 and Xbox360 path. What do you do? Do you design your own API which is least common denominator in itself and write backend implementations for it for each platform or do you write for each platform it's own branch? If you opt to design your own API, do you use bridge pattern or your own voodoo? Where does madness stop where you realize everything and kitchen sink approach must stop and you basically have separate engine for each platform as a branch. Or you stick to everything and kitchen sink and keep platform specifics in backend module specializations for each platform. 

It is generally assumed that the mass does not matter and they bounce up to the same height. This is because the coefficient of restitution, which lets you calculate the velocity change after the collision, does not depend on mass. The velocity right after the collision determines the height that the ball will move up to, independent of the mass (just like how mass does not affect free fall duration). So, for games, it's safe to assume that they will end up at the same height. However, in real life, it can be hard to make the two have the same contact properties and balls with different masses may end up bouncing to slightly different heights. This paper can provide further insight. 

1) Yes your observations are correct. 2) The standard global XYZ coordinate system makes sense when you think in terms of a first person shooter, when you are looking through the eyes of a character in the scene with a blank(identity) transformation matrix. Like it would when you draw a coordinate system on a piece of paper, X points to right and Y points upwards. According to the right hand rule (x->thumb, y->index finger, z->middle finger), Z points towards you. 3) It wouldn't be wrong, but it would be a diversion from standards. There are three problems that I can think of at the moment: (a) Let's say one day you want to use a physics library that uses the standard coordinate frame. If you did not follow the standard, now you have to think about the transformation that takes you from your world to the physics world. Can get annoying when you want to fix a bug. (b) When you want to share code with someone, or bring someone over to help with development, they have to get used to your convention. (c) When using standard 3D models, you always have to have a transformation above them to prevent them from looking sideways. Now to add to question 2, it is sooo useful to think of X, Y, and Z as not just three letters, but as right, up and backwards. Every character in the scene has a local coordinate system attached to them, and in their local coordinate frames X is always right, Y is up and Z is backwards. Once you have this, now you can make sense of vectors that you print out, or write your algorithms in a way that makes sense. Let's say you have two characters A and B, and you want to do something if one of them is facing the other. You can simply find B's location in A's coordinate frame (Ta^-1 * p_b), look at the vector you get and see if Z(backwards) is negative and X(right) and Y(up) are small, because that vector tells you how much backwards, right and up B is with respect to A. 

I am not sure flash allows P2P without Stratus (and v10+ only). But if you want to go socket route I suggest you give Red5 a shot $URL$ . I wrote a small how-to on installing it on ubuntu here: $URL$ It's a bit old, but should still be valid. If I were to make a networked flash game today I'd weigh my options between RED5 and SmartfoxServer $URL$ 

PixelJunk shooter from Q-Games has nice set of fluids. There is a GDC paper they have published here: $URL$ (PDF!) Jos Stam from Alias Maya fame (now Autodesk) wrote paper on real time fluids in games here: $URL$ (PDF!) And he wrote a simple FFT fluid solver here: $URL$ (PDF!) where he included source in that PDF at the end. 

I personally use Mercurial. There is an asset management oriented VCS from AVID called Alienbrain $URL$ which used to be heavily marketed towards game developers. 

Each list contains layout sheets (or character layouts for characters - front, side, 3/4), notes, mechanics notes for animation, etc. Whatever you need or might need. Story/Game production sheet. 

As Justing said, OpenCV. But if you want to get your feet wet quickly, I suggest you try dabbling with openFrameworks It has an addon that wraps OpenCV and you will learn a lot from it. 

I'd personally try with L-System. I wouldn't be surprised if there are examples you could find with google. 

I don't have a huge, high bar for 100% secure communication in this case because I just want to add fun, and give some minor/non-overwhelming benefits from the mini-games, not build a fort-knox system. And if someone is smart enough to hack up a solution to the mini-games to automate them, I won't be excessively broken up about it. Techniques? So with that in mind, are these two approaches valid? Are there other approaches to try with this? Any examples in the wild that I could learn from? 

Plan is that items will inherit all the default stats from their parent traits, and will only have over-ride traits that specify differences from what the traits set as defaults. The downside of the traits field is that while I could edit the json part by hand... ...it won't be really easy or safe to do so with data that's in the database. 

Disclaimer: I know that client-side is always to be held with suspicion, but I'm trying to conceive of a way to verify the success or failure of javascript-based mini-games. Mini-games to add fun to client-side game Here is an example scenario: I implement a javascript-dependent picture-sliding game for various pictures. You know the type: 

This isn't especially different from what Lolums is saying, but I wanted to give you a name for the process that will allow you to search for it in the future: semantic versioning 

I know exactly what's going on and it's a tricky one:) In time, since one orbits after the other (the update functions do not happen at the same time), their distance increases or decreases little by little. Right when the planet orbits a bit, you want your moon to do exactly the same motion so that their distance does not change. Otherwise you'll make an orbit around a slightly different radius. Your planet, before doing its orbit step, can remember where it was and where it went to, and tell the moon to move in the same direction with the same amount. To show you how to do that, I would need access to your planet code as well. However, it's simple subtractions and additions of transform.position values, you can also figure it out. In the meantime, below is a hacky fix that should remedy the situation if both the planet and the moon are orbiting around Vector3.up. I wasn't sure about how you use the orbit angle, so I changed that a bit. This works for me here: 

Unity animations do not animate changes to mesh details. Animations are only pos-rot-scale of nodes. Unity's skinned mesh renderer uses animations of nodes to deform meshes. How Unity deforms your mesh can be slightly different than Blender's deformation. Unity is trying to do the right thing given the armature animation and the mesh weights for the nodes of the armature. How are you animating the normals in Blender anyway? The armature in Blender isn't supposed to deform normals like the first screenshot you gave. 

or whatever your values are. You'll have a frequency paired with coordinate like a key->value pair. You can then quantify key scale to a color. For example key of 0 is black and maximum key you have in your key-value list is red, and everything between is a gradient between black, mid point yellow and max point red. Quantify each result to a color and there you have it. 

Devices like these are super cheap, and I bet you can get a hold of a second computer to run it on (it doesn't need anything special) so you can capture your game from your game running machine (be sure to have a computer with S-Video out for this cheap version). There is also something like this for higher quality capture - $URL$ but requires USB 3 and is more expensive, but you can capture HDMI and higher resolution. I really see no better option than either of these two. 

We have a mixed art/tech environment, but hiring process is always the same. Cull interesting resumes and offer candidates a task from start to finish on his own where you give him only a high concept. For programmers a small game that can be made in several days (can use programmer art or stock) where you give him a concept of what it should do and tech to do it with. A really basic game. For artists it's either a spec work or a pitch for concept. They can work on their own on that, can use whatever help they need (google, ask around, whatever) as long as they finish on scheduled deadline. Test consists of first cull if project is actually what it was supposed to be (does it work at all etc.) and where you go through process he used to make it and code review where you discuss his decisions while making it. 

The best way to increase your chances of being employed as a game developer is to get your hands dirty and actually work on the development of some small games. You say that you are "interested in programming games", which makes me believe that you haven't developed any games, yet. I advice you to stop thinking and start doing. However, the first order of business will not be to specialize in any of those fields. Let me elaborate. When I interview you for a job, if you tell me you are interested in programming games and your specialty is game AI, I will immediately ask you which games you worked on. If the answer is "none", I don't care about how many books you read or how you believe you are an expert in game AI. You won't be hired, period. To obtain game development experience in a specific field such as AI, you need to seek out others that are developing games and try to get into their teams. You want specialization, remember? A generalist can develop the whole game, but a specialist needs to work in a team. However, you will again be asked for previous work for someone to trust you with the AI of their game, let's say. Even if you will work free of charge. So, it's a chicken and egg problem right there. Basically, you can't just specialize in anything with no prior game development experience. Here's what you need to do: you need to forget about specializing and start contributing to the development of a game in any way you can. You can develop a simple game using a game engine such as Unity, or ask someone else developing a game and they will tell you what they need done. Then you'll figure it out and that will give you experience. Once you have something to show, not only you will have a better chance of joining a team, but also you'll have a better idea on fields that you can specialize in or that you don't like to work on. Trying to make a decision right now without that experience is like marrying someone without getting to know them. You cannot know if you will hate coding game AI unless you have some relevant experience. 

If you're really interested in technical aspects of screenwriting you can also take a look at Screenwriter's Bible - but that one is more oriented towards technical aspects of screenplay formats and structures. 

Here is what Mike Acton (Engine Director at Insomniac Games of Spyro the Dragon, Ratchet & Clank and Resistance fame) had to say about this when asked here. Note he was asked about both STL and Boost in general as related to usage in game dev. STL/Boost, does it belong into gamedev? If only parts of it, which ones? 

I'll correct you, since you are wrong. Let's assume you want to become a writer, a good one. Do you just need to learn to read and write and type fast on keyboard and that would be it? Of course not. If you want to become an artist SERIOUSLY consider going through real, traditional art lessons. Drawing, figure drawing especially, doodle in your notebook, give yourself tasks etc. Constantly draw and paint whatever you want to make. Parallel to that you can teach yourself some poly modeling program along with detailing program - a combo of maya+zbrush or maya+mudbox or 3dsmax+mudbox or modo+zbrush or whatever. You see, you need only a few weeks at most to learn any of these programs sufficiently to know pretty much everything you need to know. What then? You are an artist all of a sudden? Just like by learning Word you are a writer? You, hopefully, understand what I'm talking about. I'd suggest a plan: 

Try placing this code in FixedUpdate() instead of Update(). Physics motions happen in FixedUpdate() and Update() is synched with draw calls. The two don't always happen at the same frequency. Most of the time this is the cause of the jittery motion. So, if you are going to affect the motion of a physically-simulated object, you need to do it in FixedUpdate(). Now, why does it happen only when you do the normalization? Probably it's because the extra processing time required by Normalize() causes the Update calls to lag more behind FixedUpdate. The fact that it acts differently in different computers also supports this idea. How much out-of-sync Update and FixedUpdate get depends a lot on the available CPU cycles, which tends to be nondeterministic. 

You need to create variables that determine the position of your image, make your render function use those variables as the position, and change those variables when you want to move your object. Here's how you can do it: 

1) Since speed is a concern, you may want to take a look at approximate nearest neighbor algorithms. I've used ANN in the past and it performed very well for around 12 dimensions. It lets you adjust desired precision so that you can have a trade off between speed and precision and find what works best. 2) Since your visual occlusion is a black-box one (I'm assuming unpredictable moving obstacles), I'm not sure if you have much of a choice other than doing occlusion tests on the points that the NN algorithm returned. 3) I don't believe ANN supports points changing, but I'm not sure since I didn't need that. It seems Cgal and Pastel support dynamic sets, but in terms of insertion/removal of points. Perhaps the papers here would also provide some insight. I don't know if you need this advice, but I found that reusing libraries for such problems almost always is a better idea. There are so many pitfalls one can fall into while implementing the details. Good luck!