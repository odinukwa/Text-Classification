Never seen it done, certainly not for that reason. Speaking for SQL Server, it would work fine with a Bigint Identity column. But you'd have to ask whether it's worth the confusion it could cause, and the need to remember to define the clustered index descending to prevent fragmentation, etc. 

Assuming that: - You have the looping and sleep code in the CLR code - You know how to call said code from SQL agent (a stored procedure wrapper if nothing else) - Your process is ok with the occasional sub-minute downtime - Your process will not lose any data if it fails part way through (i.e. the next run will process the data the failed run was trying to process) then I would propose a SQL agent job that is sceduled for every minute, and the job step itself have say, 10 retries set (advanced page) so that it will usually restart itself immediately if it fails. Why just 10? Because if it starts consistently failing, you will want to get notification quickly (via agent job failure notification settings). That will be an email a minute. Of course, if it fails 10 times sporadically, you'll get a false alarm notification, but you can quickly see from the job history whether it's failing consistently or just sporadically. 

PWDENCRYPT isn't officially deprecated yet, so it will almost certainly be around for quite some time. My concern would be that it "uses the current version of the password hashing algorithm" which means the behaviour could change without notice. Who's to say it will continue to be sufficiently random, perhaps instead going deterministic like Hashbytes? 

When there is enough free space on the page for the record being inserted (whether in the middle or at the end), there will be no page split, as the page can simply be rewritten with the new data inserted. If there is insufficient space, then there will be a page split, with approx half the data moved to a new page. Note that if the insert is to the end of the very last page, and there's no space, then a new page will be created just for the new row and subsequent appends. That's what's known as a "good" page split, though it really isn't a split at all even though SQL server counts it as one. 

The /p:CommandTimeout=(INT32 '60') sqlpackage parameter should let you increase the timeout. If it doesn't work then it may be buggy like the timeout override method for dacfx. 

It's almost certainly a network/connectivity issue, even if the jobs don't fail at the same time. Error 14152 is logged when "Agent shuts down after unsuccessfully retrying an operation (agent encounters an error such as server not available, deadlock, connection failure, or time-out failure).", and if the last error was "existing connection was forcibly closed" that suggests there were also earlier tries with the same result even if not logged. There are many reasons this could be happening intermittently. One specific reason can be multiple NICs in the distributor having the same IP assigned. It could also be the network connection is simply overloaded at times. 

I'd second Jonathan's comments. There are many resources (to show the DBAs)showing that automated "missing indexes" analysis is very limited and often misleading. In addition to sp_blitzindex, if you get them to run the following query against each database (after a representative uptime), you can get an idea of which tables are the cause of possibly unnecessary IO due to high number of scans (considering the row counts) and lookups. And you can start to consider the indexes suggested just for those tables, and/or find the offending queries and consider indexing for them. 

Breaking up the data file as described would almost certainly have no performance benefit. The practice of splitting clustered and non-clustered indexes may benefit if you put them on separate mechanical arrays, but not on a single array, SSD or otherwise. Although, if you were maxing out throughput on your one SSD drive/array, splitting out to two SSD drives/arrays could theoretically benefit depending on controllers etc. Your server is much more likely to benefit from getting Tempdb and its log on to SSD. And if/when your workload is write-heavy, having the data logs on SSD would also likely give a substantial boost. And if you've been surviving with the current config, then I think you can be certain you won't see a downside to having all your database and log files on the same SSD array (though perhaps partitioned for space management). 

Read-only users being able to hold locks makes complete sense if you consider SERIALIZABLE and REPEATABLE READ isolation. And for cases like this where there's no logical sense in the lock, the designers probably decided it was better to have less complexity and consistent behaviour than to consider permissions in the locking code. 

Provided you're using bulk delete/update where appropriate, and have indexes to support such operations, I don't think you'll run into trouble due to the PK standard you use. It's possible that if you later have EF generate queries with joins etc, that they won't be as efficient as they would be with a natural key based repository, but I don't know enough about that area to say for sure either way. 

You can create a user that has certain non-db_owner fixed database roles assigned, and then you can create a user-defined role which contains additional permissions. (See $URL$ So in effect, you can create a user that has all the rights of db_owner except say, managing users and dropping database. But I don't think there's a way to directly deny such privileges, you have to instead add everything but them. Your database creation SP can configure new databases in this way, and youâ€™d have to do it for each existing database. 

UPDATE: Your latest update shows that the problems you're seeing are caused by classic inappropriate parameter sniffing. The simplest solution (if you control the code) is to add OPTION (RECOPILE) to the end of the problematic query. This will ensure the statement plan is recreated on every run, and will also allow certain shortcuts to be taken in creating the plan. The downside is additional CPU and time spent creating the plan on each run, so this solution may not be suitable. Considering the skew of your data (3 mill for one value vs 160k max for others), and assuming that skew will not change much, branching like this may do the trick: 

For whodunnit, there's a profiler trace event "Audit Add Login to Server Role" under security. Presumably it catches ALTER and not just the SPs specified, but that would need to be tested. I'd suspect the guilty once caught would start assigning specific permissions instead so would trace that too. 

Reads under the default isolation level require brief locks to ensure transactional consistency (with caveats -- see Locking Read Committed Behaviours at $URL$ In your case it seems the likely deadlock cause is that the write query needs to update multiple indexes, and the read query has already locked certain rows in the non-clustered index -- or vice-versa. So it's possible that removing the hint that Tanner noticed may resolve it (since the update and select can then queue politely).The hint may not be necessary if the data remains small enough, since after all SQL Server determines by default that a single scan will be more efficient than a bunch of parallel seeks. Some more ways to resolve this: If your select query is ok with inaccurate/incomplete data (and still the occasional crash due to data movement), use read uncommitted isolation, or nolock on the joins. But read this first: $URL$ If Snapshot Isolation is enabled on your database, and your select query is ok with not seeing updates currently being made (i.e. being milliseconds "out of date"), then use that for just this select query if you can. (There are further considerations if the select query is part of a transaction, or if there is more DML after the select.) (And don't confuse this with Read Committed Snapshot Isolation which would also solve your issue but requires comprehensive review before being enabled.) If the problem is relatively rare, perhaps just implement a deadlock retry in the app code where your queries are called. Or if you are using stored procs you can also to it there, see "Retries in TRY/CATCH" at $URL$ -- to which I would add that if you also use SET DEADLOCK_PRIORITY N'LOW' before the retry loop then the deadlock retry need only be implemented for your select statement, which also simplifies the loop code (no rollback etc.). 

Assuming T1 is a explicit transaction (i.e. it has Begin Tran, Commit Tran), I believe running T1 under repeatable read isolation level would protect you from this particular problem. However it may cause blocking or deadlocks more than is necessary. If the one problematic row is the only possible problem, then it may be sufficient to select it just once using the WITH (UPDLOCK) hint, which will prevent other transactions modifying it until T1 completes. However other transactions will still be able to read the row (which may then be updated by T1), unless they also use WITH (UPDLOCK).