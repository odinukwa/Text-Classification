But the algorithm by itself is a little dense. The gist of it is: Briefly stated, the Unicode Collation Algorithm takes an input Unicode string and a Collation Element Table, containing mapping data for characters. It produces a sort key, which is an array of unsigned 16-bit integers. Two or more sort keys so produced can then be binary-compared to give the correct comparison between the strings for which they were generated. You can view the specific Latin sorting rules here: $URL$ or more directly and specifically for MS SQL: $URL$ For the character it shows: 

Update: adding Solomon Rutzky third case, which is trickier because of the space that enables new rules (I chose the "non-ignorable case"): step 1, NFD: 

Indeed, you should not have uninstalled it and re-installed it, as PostgreSQL starts a new database store from scratch, hence empty. BTW it may happen that your data is still there, but in another old directory. See for example this article ($URL$ that shows an upgrade between 9.4 and 9.5 but it may apply in the same way in your case. If you look at the PostgreSQL formula you can see: 

You can test things in this SQLFiddle: $URL$ I am quite confident it could be made generic if needed, but it may be enough already for your needs? However it will not work correctly if you need to store NULL values, and if you need to handle transition such as "some data value => NULL" In such cases there would be a specific need to realy test the existence of each field and just take its value if present, including if it is NULL In passing I would also say like @Lennart that it is better to separate archive data from live data for many reasons, one would be performance, to compute the "next" version you will only have to read one row (the live one) and then doing something as above, instead of having to do things like each time, which will be more costly, even with an index on . 

so should be called only if directory is not already populated, so in your case the database just got initialized in a new directory, empty, while your data is still in another directory. But things have changed recently. Homebrew 1.5, released on January 19th 2018 has this in its changelog: 

If you want a user to be able to read any table and view in your database then you would run the following from SQL Server 2012 onwards 

It depends how complex your spreadsheet is, how many worksheets, whether someone has put all sorts of merged cells, fancy titles etc I've had some success using Apache Tikka as a content extraction tool with basic Linux bash utilities such as grep, awk, sort etc. I've had to do this to determine which spreadsheets might contain GDPR sensitive data. Tikka can extract data from over 1400 file formats and is a JAR file that can be called just like any other Java program. The useful output from a spreadsheet will be tab delimited. The name of the sheet will without leading tabs. Cells will be separated by tabs and the first column in any sheet will be prefixed by a tab. This makes it really easy to grab what output you need and use the MySQL COPY FROM statement to ingest it. 

This is to ensure the distribution statistics of your table are up-to-date and thus help the query optimiser. Make sure you have physical resources for such a command to run, it will hit CPU and disk quite hard Is there some combination of fields in your table that are unique? If so then this will cause the InnoDB engine to use it to create the clustered index for the table. Clustered indexes are very fast when it comes to range scans which will play to your advantage if one of those fields that make up uniqueness is timestamp. A table can only ever have one clustered index. In MySQL the precedence for the DB Engine creating a clustered index is as follows:- 

Let us suppose that you have built a reference data set that has two (or more) fields. For the sake of argument lets call the table geography 

We faced exactly the same scenario. Our solution was to have a SQL Server Agent job with a steps to call stored procs co-ordinating the delete/archive. To make our lives easier we had the application that persisted the data in the first place write a DateTimeCreated field in each related table with exactly the same date/time. For example an ProductEnquiry record and associated ProductResults record would get exactly the same date/time. Our clustered index was on that DateTimeCreated so DELETE FROM ProductResults WHERE DateTimeCreated BETWEEN... made use of the clustered index. As the database grew we found that the purge jobs had to have a pre/post step to disable/re-enable the FK constraints. We also had to start using a loop so we purged 50,000 records at a time and kept going until there were no more qualifying records to delete. When we move to Enterprise edition we started using partition switching which had a dramatic performance improvement on purge activity and massively reduced IO on our SAN. 

Remove the column has it can be computed from other database information. Create a view over with something like: 

step 4 : Compare sort keys: The second value is enough to sort them all, and it is in fact already in increasing order, so the final order is indeed: 

So Amazon added some settings. You will need to contact them directly and ask since their documentation does not provide a result when doing a search, $URL$ gives Your search for "shared_heap_size" returned no results. 

step 3, Form sort keys (for each level, take each value inside each collation array, then put 0000 as delimitator and start again for next level) 

step 4, Compare sort keys: Basically the third value determines the order, and it is in fact only based on the last digit, so the order should be: 

The Unicode Collation Algorithm is described here: $URL$ Have a look at section 1.3 "Contextual Sensitivity" that explains that the sorting can not depend on just one character after the other as some rules are context sensitive. Note also these points in 1.8: 

Second update based on Solomon Rutzky's comment about Unicode versions. I used the data about latest Unicode version at this time, that is version 10.0 If we need to take instead into account Unicode 5.1, this would be: $URL$ I just checked, for all the characters above, the collation arrays are the following instead: 

It is a wrapper around PostgreSQL tool, that can be used to upgrade a database in place, hence not loosing anything, including between major versions. This article ($URL$ can give you a lot of information on how to upgrade PostgreSQL, and what happens when you use . 

It is a very broad question, even more so as you give absolutely no details about your setup and kind of database (volume, type of queries, active connections, size of RAM, dedicated server or not, etc.) You can start by enabling PostgreSQL to log slow queries, see the in the configuration. That will give you historical data that you would then be able to analyze and maybe correlate with other things (like from the list of @VÃ©race) You will then have various tools to help, as described on $URL$ : 

You can write a simple script externally or use the MySQL event scheduler. Both will work just fine. The benefit of the MySQL event scheduler is that it will be part of your database backups. 

Joel, there is a lot of unknowns. If you post your my.cnf, queries, table definitions and queries, EXPLAIN plan output, then you might get more precise answers. You say "disabled query caching and other caching" what 'other caching' are you referring to? What steps did you take to disable QC? Did you restart your MySQL server between tests to flatten any buffers? If no and the KVP table was smaller and fit into RAM then InnoDB might have been able to serve queries fitting a certain criteria from data in the innodb buffer pool. You also refer to memcache. With MySQL 5.6 there is a memcache api. this means you can use memcache calls to access data stored in innodb. This will not be incidentally speeding up your queries. There is also another method of bypassing the SQL layer within MySQL called HandlerSocket. These are a couple of subjects that I would point you at should you continue to desire this type of access to your data with the added bonus of having MySQL persist the data to disk and also be crash safe through InnoDB's crash recovery process. 

You should consider some group ID within this or another relataed table. Marking accounts with the same group ID you can have 1 or more of them as an enhanced account type. Make sense? 

This is an [info] message and displays due to the steps taken by the Debian mysql startup script. It is not an error. 

How large do you expect the data to grow? No harm in keeping older data in the same table. You could plan to use partitioning and implement date range partitions (per year). Using WHERE statements is completely natural in SQL so don't consider this as "messy" at all. 

There are multiple audit logging products available. If you are using Percona or MariaDB flavour of MySQL you have the option of their plugin. If you are using Oracle MySQL you can pay for their enterprise version of audit plugin (as part of Enterprise Edition). There is also an audit plugin from McAfee that will fill this requirement and is generally available cross-alternative and from 5.1+. These products permit you to log both logins and queries. Finally there's a plugin to track logins only from a community contributor. Links to all below. $URL$ $URL$ $URL$ $URL$ 

Personally I would rehearse it within a test environment and expect to do it during a quiet time out of business hours. 

It is possible to generate the GRANT statements dynamically but on any database with a security sensitivity I would be very careful doing so. 

A VARCHAR(8000) column won't be indexed as you can only index up to 900 bytes prior to SQL Server 2016 so you don't need to worry about indexes. You need to consider who and what will be accessing the table when you make that change. As far as SQL Server is concerned such a change is considered a change of data type so will take longer than a change in VARCHAR size which is considered a metadata change. What is happening under the hood is that the data in your VARCHAR(8000) column will be shifted out of row and your record will now have a pointer to the data represented by VARCHAR(MAX). Time to execute will depend on many things such as 

Potential pitfalls to watch out for are the data types, size and collation discrepancies between data_table.field1 and geography.Area_Code. If geography.Area_Code is unique and must always be present make sure you stick a primary key on it. It is probably worth adding an index to data_table.field1. Again, if it is mandatory and unique make it a primary key. 

Failing that try putting an index across timestamp, experiment_id and bucket_label. Again, you had best do this in quiet/down time and make sure you have the physical resource available to do it. On a separate point be very careful using field names that are reserved words such as timestamp. You can get some peculiar exceptions thrown in applications that are very hard to track down. 

For the stored procedure part of your question I would set up an explicit role for stored proc access. I would keep this separate to MyLimitedCRUDRole as the visibility of what MyLimitedCRUDRole is of increasingly high importance in a GDPR world. I would also advise having roles that have clear and single purpose for clarity. 

The technical limit on the number of columns depends on the engine. InnoDB allows 1017 in MySQL5.7 Those 1017 columns can cover a maximum of 65535 bytes. Records are stored on "pages". InnoDB allows the page size to be configured to 4, 8, 16, 32, 64Kb. Your record must fit on a page so you can't stick a 5K record on a 4K page. The problems with having wide records is that when the DB engine retrieves records it does so in pages. You can get few wide records on a page so retrieval performance decreases. DBs pull the results through memory so subsequent retrievals will see if the data remains in memory before falling back to storage. Having many records on a page means that the first physical retrieval of a page is more likely to load into memory records which can be logically (and much faster) read from memory. From a design perspective it depends on what your use case is. In an OLTP system then I would feel uncomfortable with 450+ columns. A database is not a dumb store. It can be used to enforce rules on the structure of information and the relationships between different data entities. This is an incredibly powerful weapon to have in your arsenal. In a data warehouse supporting certain analytical systems 450+ sounds like a lot however I have seen some wide denormalised tables used to feed OLAP cube technologies. If I saw a 450+ column table I should also ask questions about security. When I grant access to that table do I want everyone with access to have access to all 450+ columns? In addition to storage efficiency/performance normalisation can also factor in a security design. Consider performance. Of those 450+ columns which ones get retrieved the majority of the time? Do you really want to have the expense of retrieving 450+ columns if only 32 are used on a regular basis? The answer I have given assumes that InnoDB (the default) is used.