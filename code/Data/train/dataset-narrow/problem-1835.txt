I often like to create backups when testing the software I work on, and will sometimes create a differential backup if I want to be able to get back to multiple previous states. However, sometimes I realize that I forgot one thing I wanted to include in a differential backup, or I no longer need a previous differential backup. Sometimes I simply want to create a new scenario from the original base image and start working with a new series of differential backups. So I'd like to be able to delete some older differential backups so I don't get confused about which ones I'm using. But I can't find any way to delete just the differential backups, selectively or all at once. 

I have found that if I follow the following procedure in one of these virtual environments, running the script takes 9-12 seconds: Test Case #1 

I have run tests on a virtual system with Windows 2003 32-bit and SQL 2005 32-bit and on and a virtual system with Windows 2008 64-bit and SQL 2008 64-bit. I have run tests on a physical system with Windows 2003 and SQL 2005 and on a physical system with Windows 7 64-bit and SQL 2008 R2 64-bit. All the virtual systems I have tried exhibit this slowness and are hosted on the new ESXi environment. All the physical systems do not exhibit this slowness. Can anyone help me understand what's going on here? I fear that similar performance issues are affecting other areas and we should reconfigure something on the host or guest environments. The only thing we can think of so far is turning off hyperthreading in the BIOS of the host machine to match the configuration of another virtual environment and its host where we were not able to see the slow behavior (I didn't observe the test on the other virtual environment&host where it wasn't slow). Could that create such a large performance difference? Edit: After some review of my question and the first answer, I agree that what I managed to demonstrate is probably a difference in performance of I/O latency between our physical and virtual environments. I also realize that I should have provided some other details: these images are using thin provisioning and have two or three snapshots under them. Could this affect that statistic so significantly? The question now becomes, is it normal for this statistic to be so drastically different between virtual environments and physical environments? Should I be able to optimize that in the environment or in the SQL configuration, or is it up to the software itself to be written more optimally for virtual systems with extreme I/O latency? vSphere client reports that the write latency on the virtual disk is 11 to 40 ms with an average of 21 ms. Is that a useful statistic? Is that extreme? Edit: It appears that our hardware (DL380 G6) has performance problems as described at $URL$ and we just need to do some reconfiguration to get the performance up. I'll accept the answer that led us in the right direction of seeing that disk I/O latency was the issue. 

We recently set up a new machine with 8 dual-core CPUs, 20 GB RAM, and 3 1-TB drives set up in a RAID of some sort resulting in 2 1-TB drives we actually get to use (I'm not the hardware guy here). It is set up as an ESXi host and we have a number of test environments set up within it. The current tests are running on Windows 2003 64-bit with SQL Server 2005 Standard 64-bit SP3. From all reports, this system should host environments that perform better than our previous setup, and yet certain tasks are performing much worse. I have found one specific SQL script that reliably runs very slowly under certain conditions, which I can't understand. The SQL script is a simple series of 1700+ UPDATE statements that starts out like this: 

You can set a power policy via the iLO configuration. By setting the CPU power modes to lower settings, the fans may also spin down accordingly. 

The config file or registry key that the settings are stored in may be protected against changes with the new OS' permission model. Try loading the VNC settings shortcut with 'Run as Administrator' option (right-click). The settings should now save. Failing this try editing the configuration file and reg manually: \HKEY_LOCAL_MACHINE\SOFTWARE\ORL\WinVNC3\ %Program files%\UltraVNC 

How are these conflicts occurring? Some kind of simultaneous access by more than one user, or by more than one machine logged on at the same time? If so, see this note on DFS being incompatible with concurrent offline files usage: $URL$ 

31 days doesn't seem unreasonable, but it entirely depends on what's being logged and what the potential usage of the logs would be. The size of the event log depends entirely on how many events have been recorded. You can fix the event log retention at 31 days but you will need to consider: 

Well, what do you want/need to achieve with the script? This'll determine how involved it needs to be. In our setup, our requirement is merely for a few mapped drives on the clients, and a BGInfo on the servers. For that, we have a couple of WSH VB scripts on netlogon, and call them from GPOs attached to the appropriate OUs (servers OU on server startup, and users OU on user login). Scripts are maybe 20 lines a piece. Powershell is a logical next-step, but I'm not impressed with it so far. It feels inelegant and clunky. 

Install OF onto each of the machines and get them to serve their storage out as iSCSI LUNs Install OF onto a bridgehead server and get it to to act as an iSCSI client (this is something that OF can do, but it's not in the GUIs). Once the storage is visible in the bridgehead, server it out to your LAN over iSCSI on the bridgehead. 

You can configure each host to support virtual machines. However each individual VM cannot execute on more than one host simultaneously. Each VM is restricted to the resources available within its host server. You can 'Live Migrate' a VM from one server to another without bringing the VM offline. However as above, at any one time you only have the resources of a single host available to the VM. With the resources you have, you may be better served by taking the RAM and CPUs from two of the hosts and using them to stack the other hosts as fully as possible (2 x dual-processor, 8Gb hosts). Then set those more powerful hosts up to host VMs. This relies on there being enough RAM and CPU slots spare within the hosts to expand them. Hope this explanation helps. If someone does figure out a way to aggregate multiple hosts into a unified VM-hosting platform, I'm pretty sure they'll clean up. It's pretty much a virtualization holy grail ;) 

VMs are not aware of their host system. You may be able to figure out roughly which platform you are on by checking what drivers are installed for your devices (NICs on a VMWare VM will have 'VMWare Accelerated' driver or similar) and seeing if any guest additions/vmware tools are installed on your guest. Beyond that, no. 

You probably don't need roaming profiles for your users - Just setup a logon script or Group Policy preference to map the users drives to the appropriate remote shares. Once you disable roaming profiles you should find that logon drops to a few seconds. If it doesn't, verify that your group policies are as simple as they can be (as too many can extend logon wait times) and check that logon scripts/policies are set to run asynchronously. If you're determined to keep roaming profiles, ensure that users home folders are set to a separate share (and not being synchronised as part of the profile), that temporary files/IE history are always cleared at logoff (to prevent them being synced), and offline files is disabled.