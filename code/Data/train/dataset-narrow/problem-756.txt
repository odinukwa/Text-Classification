If the database fails, you can recover the database with the last full backup regardless of whether it's set to full or simple recovery. Simple recovery doesn't support log backups at all. So basically, you'd lose everything from your last full backup forward. 

8GB doesn't sound like very much RAM for SQL Server. How big are the databases on that server? If the problem isn't VMware, perhaps you should check what's happening in SQL Server itself. Check the Windows event viewer's application log for messages like, "A significant part of sql server process memory has been paged out. This may result in a performance degradation." Also try running some of the queries here, particularly the top 20 resource-intensive queries. If the drops occur at specific times, there might be a resource-intensive agent job running at that time. 

Because this is production and this requires a restart, you'll have to do it after hours. Good luck! 

Pretty much what Edward Dortland said. If I understand what you're wanting, you probably want to create the user, add them to the datareaders role on that database, and manually give them update on the tables you want them to update. 

(If it doesn't let you because that account is explicitly denied, close the window, run the command line as a different user, and try again.) Once you're connected, you should be able to create a new login 

It sounds like no one took log backups on that database for a very long time. Restore the MDF and try: 

Try that. If that works, when you get into the server check the compatibility level of all your databases in addition to running . 

If you're able to insert into documents right now, it's in whatever database is your current context. You can find tables in the future with the undocumented sp_msforeachdb command: 

Go into SQL Server Configuration Manager Select SQL Server Services On the right side of the pane, find your instance and right-click for Properties On the Startup Parameters tab, in the Specify a startup parameter box, type -m and then click Add. (That's a dash then lower case letter m.) Click OK and restart. MSDN suggests launching Management Studio as an administrator for 2014. The traditional way is to use SQLCMD -E. Either way. 

Then set up regular backups for those databases, both full and log backups. This is an article on log backups that might help. This is a more general article on SQL Server backups. Good luck! 

You're using master. Like the error message says, you can't do that in master. Try using the TSQL2012 database. 

(Or you could do it through the GUI. If you right-click on the index and choose properties, there's an included columns pane.) This will hopefully have the effect of replacing your heap scans with nonclustered index seeks, but since your query has a "where in [30,000 items], it might do a index scan instead. Either way, it won't be rooting around in the heap, which I expect will be a good thing. 

This is expected behavior for a log shipping job set to norecovery. The logs are being restored with norecovery, which means that more logs can be restored on top, which is what you want. No other transactions can occur on the target database; it's a copy of the original. If something goes wrong with the original, you bring the target online with . I've never used log shipping with any options other than norecovery, but there's also a standby option that will allow the target database to be used read-only (but the users will all be kicked off during the next restore). If this doesn't suit your requirements, maybe you need to look at something other than log shipping. 

You could do log shipping with the standby option rather than norecovery. The database would be read-only, and the users would get kicked off during each restore, but it might suit your needs better. 

I second Craig Ringer's recommendation of IMAP. It's already got all the relevant standards built in. If you're determined to proceed with creating a webmail client/email server architecture written as a web client with a database server rather than pre-written email server back end, I'd look at how Exchange has historically handled mail databases. Example links include this description of 5.5 (old), this newer description, and this overview. 

This is normal. Max server memory means that your server cannot use more than that amount, not that it must use that amount. Min server memory doesn't mean it must use that amount, either; it means that once the server uses that much it doesn't surrender memory to below that amount. Basically, SQL Server uses the amount of memory it needs until it reaches the upper limit you set, and if it stops using it it might surrender part of it back unless you've set a min memory. (However, in my experience, it's pretty memory-greedy.) In short: This means that you're currently not using 6GB of memory on your SQL Server. 

Check the file permissions on the backup on the second server. It's possible that's your problem, and it's easily fixed. If that doesn't fix it, you should run a on the original database, which will tell you if the database itself is corrupt. If DBCC returns okay, run a on the backup on the original server. If all of that returns okay, go back to the second server and double-check the file permissions. 

Just tell me that you don't have databases on the system drive. :) Seriously, what they said. If your databases aren't in simple recovery, they must have transaction log backups or the log will grow until it eventually consumes the disk. (Which is why I said "not the system disk." One of my lesser colleagues once installed software, including SQL databases, on the system drive, and it ate Windows.) Also, there's no guarantee that those .mdf, .ndf, and .ldf files will attach off your backup, especially if there's any kind of time difference between when they were backed up. (They might, but it's not supported, and they might not.) So... what Thomas Stringer and vonPryz said. Back up through SQL. Run DBCC CHECKDB. (Reindex, even.) Back up the backup files with Acronis. 

Disable the log shipping jobs on both servers. Back up the database on Server A. Restore the database with the backup you just created and on Server B. Re-enable the log shipping jobs on both servers. 

NO. DO NOT DELETE YOUR LOG FILE. You'll probably kill your database and the symptom you're trying to fix is that it won't back up properly. I'm going to recommend this article, "A beginnerâ€™s guide to SQL Server transaction logs." Especially this bit: 

Have you tried running a full backup followed by another transaction log backup? What error messages are you getting? 

Without knowing more about your data I'm not sure about your table structure, but I will say that GUIDs/uniqueidentifiers are controversial for clustered indexes. Traditional logic says that your clustered index should be unique, small, and sequential. GUIDs are unique, large, and random. (There is a newsequentialID() function that you could use as a default value to get around the random bit, however.) An article against random GUIDs as PK/clustering keys: $URL$ Summary: Your data can end up a scrambled mess, and selects of ranges that should be next to each other can be slowed down. Also, it takes up too much space in large tables and indexes. An article for random GUIDs as PK/clustering keys: $URL$ Summary: If you have a LOT of inserts, it might be faster to spread them out instead of having them all be "at the end." Personally, I think that if you have a specific business/logic reason to use GUIDs, then use them. Otherwise, you might want to consider using an autonumber column instead. 

Are you 100% sure that this is SQL Server 2008? This behavior suggests SQL Server 2000. The SQL Server 2000 syntax was 

Run on the database Stop all user activity in the database Switch to the SIMPLE recovery model (breaking the log backup chain and removing the requirement that the damaged portion of log must be backed up) (which you've done, yes) Switch to the FULL recovery model Take a full database backup (thus starting a new log backup chain) Start taking log backups 

You could do it as a case statement, but since @@VERSION returns service pack information it might be providing TMI when all you want is to check for a supported feature. Good luck! 

Like Aaron said, we don't know anything about the application or data (or number of users, or the hardware running the database, or...). I will say that in my experience, SQL Server is much faster and more robust than Access for things like web applications. Whether that means your application will run faster, well. Not enough information. 

Operating system error 2 is a standard Windows operating system error--file not found. Check the permissions on the folder and make sure that the account that owns the agent job has access to the folder and is able to traverse the path to the folder where the backup is trying to write. Unfortunately, this is a Windows error message and not a SQL error. I found something on Microsoft Connect (related to restore, not backups) where they said they were not able to reproduce the problem and confirmed that this is an OS, not a SQL Server, error message. 

Your table is a "heap," which means that it doesn't have a clustered index. The short version of what that means is that your data isn't laid out on the disk in a logical way. A heap really isn't an optimal structure for an actively updated table. Here's an excellent video on heaps. So, my suggestions--please test them before deploying them in production--are: 

Yeah, you need that .ldf file. In the future, don't delete that. If your database is running in full recovery, you need to run log backups regularly. "Regularly" is a term that depends on your particular environment, but hourly during business hours is a common choice. Basically, it only purges the transactions out of the log file when it's backed up. If it's never backed up, the log grows until it consumes the entire disk, as you've seen. In the future, set the database to simple recovery, shrink the transaction log, and then back it up immediately, as NathanC suggested. As for now: I don't know what kind of backups you have beyond the one you restored from, but I agree with NathanC that you need to let it sit and wait for the restore to finish. It's chugging through that log file. Once it finishes, truncate the log and set it to simple like your other databases, or implement log backups.