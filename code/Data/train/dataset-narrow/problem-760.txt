This option is only reasonable if cars and boats have the same properties and that the existance of those properties aren't going to vary based on vehicle type. If cars and boats will have different properties, then they really belong in separate tables. As an aside, some people make modifications for Microsoft's Flight Simulator to fashion a boat simulator. In these mods, a boat is a boat-shaped airplane (look, no wings) that has a maximum altitude set to 0. I am suspecting that if you end up adding properties, you will have strange bugs when "accidentally" setting boat-specific values to things that only relate to cars (oh, the license plate column only applies to cars and should be null for boats) and vice versa. 

SQL Server certs. Start with the relevant exam for the MCTS, that would most likely be 70-432. Then when you know more, take the 70-450 exam. That will give you the certification. You cannot get the equivalent SQL Server 2005 certs because they were retired in June. 

This question gets more complicated when you realize that a number of queries, views and stored procedures use from the table that column is resident in. Then you need to look at the programs that use those results - so you need some scanner/indexer/parser capable of reading source code that may be C#, Delphi, Java, VB, ASP (classic) and so on just to try to hunt down every reference to that column. Then you need to analyze those programs to try and identify if that code is even being called any more. 

No, it will not be the best performing way (for reads in general). Although it will be used when querying only against a, less keys will fit per page in the index as compared to an index only on a and so be less efficient because the keys are longer. This will get worse depending upon the ratio of the size of b to a, i.e. the proportion of the key used for b which is effectively wasted space. However, if you are querying only for a single or very few specific values of a rather than a range, the effect may not be significant, since you wouldn't be going through many pages. I would add the unique index on both to meet data integrity constraints for your problem domain but see if the performance is better for your overall load by adding an index on a alone (note that having to update two indexes will slow performance on writes). 

$URL$ You would still need to set each report to ORDER BY this new column. Probably not a great performer, but who knows without checking the execution plans. 

Well, it seems like all your data is meaningful, since you don't have a lot of it, and it all seems to have a role as a key or useful attribute. If you have PK (by definition unique) on isbn in b_c, then this restricts a book to one class. Is that true? At that point you could argue the design that the class_id should simply then be an attribute of the book table and you don't even need the b_c table. Since you already have a PK on b_c, I don't see the need for a surrogate key. Even if you were to expand to compound primary key on isbn, class_id to be able to allow multiple classes for a book, I don't really see a need for an additional surrogate unique key. In any case, it would only be an alternative unique key, I probably wouldn't make it the primary key and probably wouldn't use it in joins (joining to link tables is not terribly common as a foreign key, since they are usually identified by their parent or child as being part of a collection based on that relationship) This is what I would do: 

I've worked at a place that had a large number of orphaned and semi-orphaned databases. It was hard to tell if they really were orphaned as many tasks were seasonal or annual - so that website only runs for 3-4 months per year (as an example, W2 forms need to be electronically filed 1/31, so the website processing these only ran from mid January to the end of April). What was done was a combination of: * ask every developer if they were using some database or the other (these emails would go out monthly or whenever backups were taking too long). * take the database offline and see who complains. * rename the server to see who complains. Since the pointy haired boss only was willing to allow "full and complete" documentation, a wiki was expressly forbidden, and staff reductions lead to a dramatic decline in documentation that met the standard. If it were up to me, there would be a wiki page per server with contact names for each database (and maybe a brief description of what the database is for). Any database undocumented on the wiki would be fair game for deletion. We had one large financial client that was still using SQL Server 2000 as late as 2009, so we had to keep one SQL Server 2000 instance running until that client finally moved to SQL Server 2005. 

Based on everything you have said in the question and the comment, I don't think you need to worry about hardware yet unless you are trying to just get a ballpark price estimate for feasibility. Since you are a software person, I would build the prototype on commodity hardware like your ordinary laptop, analyze and understand the problem and then spend money once you know more about the profile of the software and the problem space. If your laptop is not up to it, then pick up a refurbished server for a few hundred dollars to get it closer to being able to test. It sounds like a small amount of data and there is no indication of the large amount of ongoing data analysis (which can greatly be affected by the data model - some data models can make analysis many orders of magnitude faster). If you are trying to test at production loads, you will need a production spec machine - so the problem is a chicken and egg one. To have a production machine, by your own admission, you would need an expert. And I know this is relatively off topic from your question, but: The things you need to be looking at on your software design are the rate at which the data is coming in, the amount of processing (parsing, de-duping) you will need to do, the model for the data, the size estimates, the way the reads are going to work (whether you have multiple models - one for writing and one for reading, like data warehousing), and the complexities of the analysis and whether this will be performed by SQL Server in your architecture or by client code (and whether you intend the client code to run on the same server or whether you will also have an application server). This will tell you a lot more about what you want out of your SQL Server than your point 4 (the only thing you have given us which has anything to do with determining the nature of the server configuration you will want in addition to the $2000-$6000 budget). And this would be information that your expert would need to be useful to you. In my experience, the right time to make these kind of decisions is as late as possible - and a decision on this hardware can be deferred. 

Give them logon and view data rights; however to perform DBAly duties, use a separate login with elevated privileges. I know one financial customer that does this - the regular windows authentication based logins were limited in the damage they could inadvertantly do. Restores and running DML required running with the separate SQL authentication login. One government agency I worked with used 2 separate logins for each server/db admin. So if was my domain login (this login would have regular privileges), then would be my separate login. You get into trouble if you use your admin account all the time, but then it lacks permissions to other things (like no email. Oh, you say that like it is a bad thing...). The current government agency I'm working with has each server/db admins having privileges elevated above the standard user, but not quite admin (think of it as the group). Domain admin functions are performed with a shared domain admin account. A common error is restoring the wrong database (like QA restored over the production server), and this isn't going to be solved via restricted rights or multiple logins. Doing potentially destructive things in pairs is one way to minimize the risks. 

My understanding (I haven't worked with this in over a year) is that "merge replication" requires a sql server instance to act as a publisher. If you want to only have an SQL CE database at each end, you need to write your own code with sync framework (if you see articles on "sync services" they are using the sync framework). 

This one U locks the same NCI - for the nested query I guess, then goes to lock the CI for the update. Thus the order produces the deadlock. Easiest solution is to force the two queries to completely block - i.e. serialize. What's the easiest way to force that, just put on the references to the table (one in the first and two in the second)? DDL: Note client has more indexes on this table which should be affected by this update, but are not mentioned in the deadlock graph. 

From my point of view, you need to be careful that you really are talking about only ITVFs. Inline table-valued functions are basically parameterized views, so they are better optimizable compared to multi-statement table-valued functions. However, this limits some of their functionality. I find it unlikely that all your logic for frontend #1 can be managed with ITVFs, but maybe. It's just as likely that they will duplicate logic in forms in Access unless all they are using is the linked tables and not trying to do any forms at all. It's an OK strategy, but I still expect you might have some duplication of code. As a strategy, it is probably sound because ITVFs are intrinsically pretty low on the complexity list - I tend to use the constructs in SQL in increasing order of complexity - trying to solve problems using the least complex structure possible: 

At a previous employer, some of the reports would take hours to run. The month-end and quarter-end reports were the worst (8 and 20 hours respectively). By computing the results, and storing them into a permanent table, the user could look at the results of the report at whim without recalculating the numbers on the fly. The process that calculated the reports was able to restart if it was interrupted, so that also helped: in that part of South Florida, there were multiple second-long power outages each day. While the company had a generator on-site for the bad weather days, not all of the staff had UPSes. Some of the "power users" wanted to be able to access the data and crunch the numbers in Excel, so they had read-only access to the reporting tables. 

In SQL Server, you can take databases "offline" which leaves the database present, but makes connecting to it via code not possible. If a database is "offline" it still remains available and is reversible within minutes. At my last job we had some products that were in operation for several months per year, so turning off, or taking offline, the database for months at a time would not have been noticed by the folks working with that product. As one example, one of the products involved W-2 forms, so 98% of the business happens in January and February (for most companies, the data is not available until the first week in January, and the federal regulatory deadline for filing the information is the last business day in January). The web server was usually turned off from May/June until December. At that company, we had a spreadsheet with the "owner" of the database - one single person responsible for the product. While others could make updates to the structure of the tables, the "owner" was the go-to person when any questions had to be asked. If the owner left the company (rare until last year), someone would be assigned to be the new owner before they left. At other companies, we have taken databases offline for a quarter, if they stay offline with nothing breaking (such as month/quarterly reporting), they get backed up one last time and deleted. This allows someone to later come back and restore the database (which takes a few minutes) for those situations which have stories like "oh, that was for the jones project that we had to set aside while we got the fred project finished." 

You say that the hierarchy gets modified. Presumably while this operation is running, there is some amount of blocking which is taking place then? Even if the hierarchy is changing, are the roots for items changing? Have you looked at the time it would take to just make the mapping table from root to item and index it? I would like to see the execution plan to see what is happening - the CTE should get spooled, but as a manually materialized and indexed table it might perform better in the later steps. Even with heavy activity, it would seem to me that someone has to be blocked if DML operations are changing data which this process is reading. So I'd strongly consider taking a snapshot of the hierarchy. In addition, you have a number of other INNER JOINs - you should review whether it is, in fact, the CTEs at all and whether there are any indexes which are missing to make those joins effective. The execution plan should tell you that. You appear to have quite a few things in the WHERE clause which might help reduce some operations (and determine which indexes might be the best)), but it's hard to tell without looking at the execution plan or the indexes. 

I don't know of a single good comprehensive solution to this. Local development means that developers don't break other people working on their own code against a shared database. However, when you get latest code, you also need to get the database into the right state to match the code changes you've merged in. If two people are making changes at the same time, merging can be difficult since database upgrade scripts can be incompatible. Column order doesn't normally matter in a database, but it can be a bit annoying for databases to be different. There are good tools to compare schema and data and apply changes. I would say schemas should aim to be identical. However, typically you want lookup-type data updated between developers but not regular application data (new customer types, but not new customers). Configuration data you might want updated, BUT sometimes only a subset (new printer options but not file path settings). You would think that ideally, you could JUST rebuild your local database completely once you've each merged changes. If you've set up a bunch of test scenarios through the application for testing (instead of in the build scripts), you now don't have a script to get those changes back into the database. And this is more difficult as the scope of the database schema increases - with surrogate keys and parent-child relationships which all might have complex dependencies. In the ideal scenario for a central database, you would have a developer DBA managed the database interfaces for the app and control that so that the exposed interfaces would consistently evolve over time and all developers about that level would see the same interface at the same time. But then you have two separate groups coordinating their different feature timelines. Which I think goes a long way to showing why people are still attracted to a variety of other approaches which put more emphasis on code and less on the database. Ultimately, I think that just shifts the problem around. 

Several years ago, I tried to build a tool to check similar stuff. The TL;DR answer is that I found it not possible to do with the available resources at that time. 

I've used both and I thought that Red Gate's was a slightly better interface and easier to use (that probably meant I was overlooking something in VS). They both produce reasonably similar scripts. 

Generally, I prefer to add some CLR functions when possible to implement some of the ideas from the book Cryptography in the Database, such as hashing and salting. This way the clients don't need to worry about future maintenance developers incorrectly implementing password (or other data requiring protection) security. I've worked at companies where encrypting stored procedures was done to preserve business trade secrets (frequently from the employees of the firm). Some of these databases would be hosted on the customers' servers, so security of the source code was paramount. If CLR in the database was available back then, they would have proceeded in that direction. Finally, there are times when reporting functions are hard to implement in pure SQL, so a function can be implemented in CLR could allow for more complicated functions to be kept away from the report itself. 

No. The MCSA (and MCSE) are aimed more at administering networks, not SQL Server. These particular 2 are aimed at administering Windows 2003 networks. I think you would be wasting your money taking them as I think these 2 certs will be retiring soon. The Windows 2008 versions use the newer MCTS/MCITP naming convention. In general, the current naming convention progression is MCP (MC = "Microsoft Certified", P = "professional", one exam, of anything), MCTS (TS = "technical specialist", one introductory technical exam), MCITP (ITP = "IT Professional", the relevant MCTS plus one or more "pro" exams). There are MCM (M = "master") certifications, but those will cost between $10k and $20k, so only if your company will pay for them would they even remotely be practical.