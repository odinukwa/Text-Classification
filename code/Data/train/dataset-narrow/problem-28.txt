I think the time and effort it will take to set up another tool that performs this type of work will be about the same as it would be to setup Jenkins. If you are looking into a different tool though, I would checkout Rundeck. It is a great tool for creating scripts and allowing other privileged users to execute them. I'm not sure if it will meet your needs for predetermined fields/dropdowns, but there are plugins available that may help. 

I have a base docker image which is used to run image analysis software. For each container created from the image, there are a set of configuration settings some of which are secrets (encryption keys, customer information, etc.) that are used by the software to analyze and distribute the processed images. How can I safely pass these secrets to a container? 

NOTE: When running the safeRestart command, any jobs set to be executed during the restart will be queued up and executed when the server is back online. Make sure this does not cause any conflicts upon reboot :) 

The most prominent DevOps analogy I can think of is the Pet vs. Cattle analogy on disposable infrastructure. Although, I would argue that is less about the fetching that is associated with the image, and more about how easy it is to understand and relate to. 

I don't see any reason why you wouldn't be able to. In your CI/CD system, instead of referencing your individual source repositories, you will use your main monorepo as the source repository and then reference the specific project directory within it. It looks like as of GitLab version 9.1 this is supported. In my opinion, this wouldn't be something that would happen on commit. You could do this, but I think it would slow down your development speed significantly and use up a lot of resources if you have many commits. The two key points to consider are: 

The ability to manage your artifact versions A central location to access artifacts across infrastructure (which you have accomplished with NFS) The ability to download/recreate previous versions of the artifacts (you could also use a git/svn repository for this, but it would be contain MUCH more irrelevant metadata about the binary artifacts than would be useful to a user) You want to control who/what has permissions to access the stored artifacts 

I have a numerous deploy projects in Bamboo that are used for deploying DevOps/release tools. I would like have all of these projects grouped together for easy reference. Is there a way to group or organize related projects in Bamboo? Ideally, this would be similar to how the build plans are grouped. 

I'm currently using Fastlane to capture screenshots of my application to be used in both the iOS and Android app store. My problem is that Fastlane is only able to capture the loading screen and the username/password authentication screen. Is there a way to pass credentials (possibly test creds with limited access) into Fastlane to allow it to capture other screenshots of my application? If not, what other methods could I use? 

If none of these are applicable in your use-case, I would say that Artifactory would be overkill and the effort to create and maintain it would not out way the value you would receive from it. Although I have used it in the past, and it is a great tool for what it aims to accomplish :) 

I checked out the Jenkins source code, and I believe Jenkins uses the last 3 successful or unstable builds for its estimation. If it out of the last 6 builds it can't find 3 successful or unstable builds, it uses 1 or more of the last completed (not aborted) builds. It then takes the total duration time of the "candidates" and simply divides it by the number of candidates (i.e. mean of the durations). Here is the code I based this off of that can be found in the Job.java file in the Jenkins repo: 

There are a few main reasons why you would use Artifactory (or any other binary repository manager) over a traditional file storage (in your case NFS). 

You can execute the safeRestart command using either the Jenkins Rest API ([jenkins_url]/safeRestart) or you can execute the command via the Jenkins CLI. 

I am looking to migrate a handful of Subversion repositories to Git. Each of these repositories has a development and production branch. When ready to release, a developer will submit a pull request to production. There is a version file that contains the version that has to be incremented in some way. Is there a Git repository manager (Gitlab, Bitbucket, etc.) that allows you to automatically deny pull requests based on a condition? My condition in this example would be the version in a version file is not incremented. I am also willing to consider other methods, such as tagging the repositories instead of a version file, if auto-denying in that way is possible. 

I have a handful of projects that utilize SVN externals. How should these externals be handled in Git? For reference, these externals are shared by multiple repositories and are updated very infrequently, but typically the changes are very impactful (non-backwards compatible). 

Using a BRM is more lightweight than a typical source code repository system such as Git or SVN. Those systems will have much more metadata than is necessary for binary files I have personally used Artifactory by JFrog, but I have also heard many good things about Nexus. 

I'm guessing that your PowerShell script is hanging because of the Invoke-Command command. My best guess is PowerShell is either prompting you for something, you are passing in an argument incorrectly, or you are executing the script in your script block incorrectly. I'm not sure exactly how Script-Block works when referencing paths to PowerShell scripts, but it could be because C:\hg\Update.ps1 does not exist on the remote server. It could also be because of the commands in your "C:\hg\Update.ps1" script. Some debugging tips that I recommend: 

I am currently creating a pipeline for releasing 200+ customer IOS apps. Each application is a derivative of a base app with slight non-code related changes (image assets, configuration files, certs, etc.). What is an effective way to store and release these custom apps? Currently, the customer assets and configuration files are each stored in a an individual directory in an SVN repository. The build server checks out from each customer repo directory and combines with the base app repo before a build. Each application is then individually built and submitted to Apple for review. 

Remove your Invoke-Command line and see if the program executes. Replace the ScriptBlock command with a simpler command, like Get-Culture, command and see if it still hangs. Replace your C:\hg\Update.ps1 with another script that performs a simple command. If all of those options still hang, try using the simple script method, but with different args. 

If your variable is not declared in Bamboo (either by injections, build/deploy variables etc.) it will not inject the variable and will use the string as is. It should only render the variable as an empty string if that is the value you set for that particular variable. As an example, my build plan has the following variables: 

This is a textbook example of server orchestration and is something that Chef inherently is not intended to do. As noted by Tensibai, a server running Chef is a convergent system that achieves its own desired state based upon configuration settings set by recipes, attributes, data bags, etc. Without getting in to specific details about your infrastructure, a few approaches that you might be able to take are: Create independent idempotent operations As you stated in your question, creating a state of operation where your nodes could run repeatedly until all tasks have completed does not scale well. It may be possible however to redesign your nodes so that it doesn't matter. If nodes a and b run tasks to output their logs in parallel, and b completes before a, it could run the task that node a would normally run and vice-versa. Use an external orchestrator for delegation Using a delegator node will definitely scale much better if you intend to have many nodes to orchestrate. However, this could create conflicts with your chef client runs on the nodes being managed by the delegator. It would be very difficult to verify that your node configurations and the delegator node tasks do not conflict with each other. A clever way to manage this could be to incorporate the tasks in the configuration of each node and have the delegator set a value in a data bag or and attribute of the server to signal how it should configure itself (i.e. what tasks it needs to perform). Combine your infrastructure If each node runs it tasks serially depending upon the other nodes, and you have no cost/technical dependencies on running tasks on different nodes, You may want to consider combining your node configurations into one single node. This would eliminate any configuration conflicts you would have between any of your nodes. I imagine there are clear intentions for running your tasks on different nodes, but this is definitely an option to consider (maybe even at the cost of time to rewrite tasks for different nodes). 

As long as your variables and namespaces are unique and descriptive for the plan you are working on, you should not have any issues with name conflicts. If it is replacing it with an empty string, my guess is that you may have that variable created somewhere in Bamboo, but no value assigned to it. 

Your scenario is a perfect use case for a binary repository manager. A BRM's primary feature is to act as a central storage system for your binaries, but there are many other features including: 

Yes, you should definitively run both parts in parallel if you have the resources. This means that your job will run as fast as your slowest running part. This puts emphasis on optimizing your slowest running part. It might be something as simple as running some processes of part 1 ahead of time before part 2 even begins.