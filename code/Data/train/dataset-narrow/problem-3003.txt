When we were drafting the English national curriculum, we found it easier to think in terms of the foundations, applications and implications of computing, all three of which really should be included in any broad and balanced approach to the subject. You can map these to computer science, IT and digital literacy if you wish, although you would need to accept a rather broader definition of digital literacy than that used by the Royal Society Foundations would be about the underpinning principles of computer science (logic, algorithms, data representation, abstraction), as well as their practical expression through programming and more generally in computational thinking. Applications is about skills in using digital technology to get useful work done, including collecting, managing, analysing and communicating data and information and creative work in a range of digital media. Implications is about a critical understanding of the impact of digital technology on individuals and society as well safe, responsible and ethical use. I'd include intellectual property, privacy and security here too. I've an illustration of all three in response to the question 'How does Google work?' Foundations: big data, Page Rank, Big Table / the Google File System (GFS) etc Applications: type your query, click the button (well, these days it starts searching as you type), but also filtering results, advanced queries etc Implications: profile, filter bubbles, advertising, smart creatives, separating costs and revenues for accounting purposes etc 

I, too, have felt exactly the same during lab periods. Walk around, ask questions, sit, repeat. I've come upon two things that help improve productivity: goal-setting and reflection. Releasing students to work as soon as class starts for more time can be counter-productive. Instead, I found it helpful to frame the focus for a particular lab session with a clear objective. That goal may stretch over multiple days, but every class should start with a review of the goal, a discussion of progress made if relevant, and a re-focusing for the day. It does not need to be complex; a simple, manageable goal is effective. Even having multiple small goals rather than one big goal may help students spot their own struggles as they do or do not make progress. Comments can go from "I'm stuck" to "I'm having trouble implementing x." Their involvement with the creation of their own plan for a lab session will also increase their ownership of their work. Plus, it's essentially another way of teaching decomposition. :) To follow that, a goal that doesn't get assessed for progress may as well not be set. I find exit tickets to be an absolutely invaluable tool (thank you, Google Forms!). Even 1 or 2 simple questions at the end of class like "How did today go?", "What went well today? What didn't?", "Do you have a questions for me?", etc., will help you see their progress meaningfully, spot trends among student perception of the lab session, and provide a way to start the next class. 

A variant on the usual random drill and practice test would be to pre-populate with the questions and answers, then remove question and answer from each as they get answered correctly, allowing players to get more practice on the questions they get wrong. Here's an example for times tables. You could try something for an adventure game, building up an inventory of items collected in a list. Another possibility would be an adaptive 20 questions style game, adding additional questions into a database (of sorts) as the player gets to the end of a branch of the tree. 

Some interesting projects for those working with block-based languages (such as Scratch, Snap! and Blockly): For Scratch, check out Dr Scratch, which takes a rubric approach to evaluating how much 'computational thinking' is evidenced by a project. Whilst the analysis might seem a bit reductive, it can be used independently by learners and includes some useful guidance on how to progress. The developers describe their approach in this paper. Dr Scratch is built on Hairball, a Python module which does static analysis of Scratch projects. A more conventional autograder, lambda, is being developed by Michael Ball for Snap! It's already integrated into UCB's Beauty and Joy of Computing MOOC, and I think there are plans to make this more widely available. Michael wrote about this for Hello World #3. Chris Roffey has developed an autograder for Blockly used in the initial round of the TCS Oxford Computing Challenge programming challenge, although I don't think the code for this is shared publicly. 

Getting feedback from students is the best thing you can do to improve your craft as a teacher. I end every week with an exit ticket (and the course as a whole with one), and it is often the highlight of my week reading students' responses. I tell students that feedback from me is the way they improve as students and so to is their feedback the way I improve as their teacher. Understand though that the motivation of your own growth should always be improvement in student learning. Include several straightforward questions about the relevant subject matter. These are ungraded "formative assessments." In other words, these are means by which you can see if your students actually understand what you taught them. It is no good for students to enjoy your class but not actually learn anything, and just because you taught it doesn't mean they learned it. For example, if you are teaching the binary number system, give them a few small sample conversion problems to answer on the form (and yes, Google Forms is my absolute favorite and preferred means for gathering and analyzing this data, but you could certainly do this without them). See if they can actually do what you have been teaching. I would then follow up these short practice questions to test for understanding with questions such as the following: 

I describe programming as a two-step process, of algorithm + coding. I.e. deciding how to solve a problem, and then implementing that approach in a particular language on a particular system. Can you have one without the other? Yes. There are occasions, particularly in the realm of unplugged approaches to computational thinking, when we'll think about algorithm-like solutions without any intention of automating them (e.g. the jam / PBJ / chocolate sandwich thing, or an effective way to play a board or card game). There are other occasions when we'll write in a code that a computer understands without their being any actual algorithm to implement - markdown, HTML, XML, json etc, etc. I think HTML is a great way to get students thinking of a formally specified language, and seeing some immediate effect from their coding, without the additional cognitive load of implementing an algorithmic solution. 

Any programming is a two step process: deciding how to solve the problem, then implementing that as code on a particular system: choosing or designing an algorithm is the first step. There are great ways to illustrate how the choice of algorithm matters. An introductory one might be search - comparing random, linear and binary algorithms to, for example, find a missing number, or a word in a (printed) dictionary, or a book in a (physical) library. Another might be exploring different sorting algorithms, for example bubble sort and quicksort using this CS Unplugged activity. Mathematics provides a rich source of contexts, for example asking students to think of an algorithm for finding the greatest common divisor (i.e. highest common factor) for a couple of numbers. Have them try their algorithms out on paper before coding them and then testing with some big test numbers. 

Thanks to working with Racket recently, I've been spending a lot of time learning about and implementing non-trivial macros. In the process, I have been thinking about ways to apply this skill to my teaching. In particular, I teach students C (in the context of CS50 AP) and would like to show them simple but useful examples of macros in C. I see the potential benefit as two-fold: 1) it would be helpful for discussing the C pre-processor when I teach the stages of compilation, and 2) it might help students write/understand code with information abstracted away in macros. What lesson ideas (with code examples hopefully) do you have for introducing students to macros in C? 

In terms of analogies and to further your own, you could talk specifically about sharing privileges on Google Drive (assuming that is something your students are familiar with). When I pass by value, I in effect give someone the right to copy the original (i.e. "Can view" rights) and make whatever changes they would like. Those changes have no bearing on my original copy, and the other person would now have to explicitly return the favor and give me "Can view" rights to see the work. With passing by reference, I am giving direct access to the original (i.e. "Can edit" rights). You can now modify the original without needing to return anything. The idea of sharing right should be universal enough beyond just Google: the privileges of read-only v. editing should resonate enough. However, I would caution that this might be another area where analogies may not clarify things as much as one might hope. With the C example, it's clear - even before explaining the syntax - that one function swaps values successfully and one does not. Students can now work backwards and problem solve to identify why they think one works and one doesn't. As a result showing a working example where one function passes by value and one does not may be a better teaching tool. This would enable a more Socratic approach where you as the instructor can pose questions about why they think one does something different from the other and in turn they may come to understand the syntax for themselves before you formally teach them anything. If you compare the two functions, the only difference is the use of throughout and the fact that it is called with an ampersand in front of each argument. That creates a teachable moment in terms of syntax. 

We're introducing some aspects of parallel processing quite early on in Scratch. Each sprite has its own script which appears to execute in parallel with those of the others. Scratch has a broadcast and receive message protocol, and support for shared as well as private variables and lists. Children might encounter this in a maze game, perhaps programming a number of similar ghosts to chase the player's avatar. It's also useful for agent-based modelling, e.g. the spread of an epidemic through a population. Of course, it's not true multi-threading, as all of Scratch runs inside Flash, inside the browser, on just the one core, but I doubt those using Scratch will be aware of, or care about, the distinction. This does, though, lead to potential difficulties in 'graduating' from Scratch to a text-based language such as Python - young Scratchers who've been used to programming in parallel in Scratch can find it hard to adjust to doing just one thing at a time in introductory Python programming. 

There are some history topics that link really well with computing - for example a history of communication, taking in writing, printing, semaphore, the telegraph and Morse code etc, through to the internet and the web. The English history curriculum for 5-7 year olds suggests that pupils compare William Caxton and Tim Berners Lee. Another great topic would be cryptography, perhaps starting with the Caesar cipher, mono-alphabetic and poly-alphabetic substitutions, Enigma, Colossus and other work at Bletchley Park, public / private key encryption (with applications to SSL), some of the contemporary issues around privacy and perhaps a look ahead to quantum cryptography. YMMV, but I think context like these may make these topics more engaging than a straight history of computing unit. 

It reminded me of this blog post from Scott Hanselman: "Stop saying learning to code is easy." The entire post is relevant for this topic, but I'll just excerpt this paragraph: 

What I experienced - and what I've seen colleagues experience - is that these three "benefits" cannot always be assumed (obviously). In my view, they really hinge on the first one. Asking students to watch a 45-minute lecture, especially if it's not done by you, can seem like you are phoning it in by outsourcing the instructional work of the course. Also, if students are lost five minutes in, you essentially lose them for their "homework" and the next class meeting because that time for application has to be spent explaining the lecture, i.e. doing what a non-"flipped class" already does. Thus, videos can be a far cry from keeping students engaged; they can just build resentment. However, if you recorded your own lectures, or in this case, your own commentary/follow-up to a lecture, you would demonstrate your own investment in the video content of the course and might increase buy-in that way. Students detect authenticity in us very quickly, so I think it's imperative with this approach to explain how you envision your role as instructor in this pedagogical model. When flipped learning works, those latter two benefits shine. The classroom becomes a dynamic place for interaction among students and between teacher and students. But if #1 fails, the rest become moot. As the article states, 

Harvard's grading policy for CS50 is worth looking into. There are four components for the grade on problem sets (each of which involves submitting code). The overall grade is calculated as scope * (3 * correctness + 2 * design + 1 * style) Scope: to what extent does the code implement the features required by the specification? Correctness: to what extent is the code consistent with the specifications and free of bugs? This is done by the check50 autograder, and it's essentially unit testing. Style: to what extent is your code readable (i.e., commented and indented with variables aptly named)? there's a component for formatting: I think in Harvard's case these marks are awarded by teaching assistants, but basic static analysis or linting might suffice. Design: essentially, is this good code in terms of clarity, efficiency, logic, elegance - again, Harvard use TAs to award these grades, and it's hard to see a machine (or an inexperienced grader) being able to award these marks accurately any time soon. If you were determined to use automatic grading, I guess you could do something with run times for test data or the more sophisticated forms of static analysis. A compromise might be the use of peer-assessment and a detailed, criteria based rubric: peer assessment might have other benefits. 

I think I can answer this via analogy with Python and a surface-level knowledge of Ruby and Sinatra, having skimmed the docs for the latter. From what I can tell the following holds true (although I'd gladly be corrected if it's too reductive): 

Kernighan is great at drawing from real-world examples, examples that would speak to the lives of students. His writing is accessible, and he covers a lot of ground successfully. 

To implement an interface is to promise, to guarantee, that the class will come through on its end of the bargain and implement everything in the interface. The idea of a contract is already a successful analogy, so I wouldn't try to necessarily reinvent the wheel here lest you introduce more confusion with a different analogy. You could have students use the word promise if contractual language does not hit home for them. Putting it in the context of friendship and promises being made might make them understand an interface in a better way than legal agreements and contracts. Ultimately, simple, relevant examples that clearly illustrate why interfaces are of value and how they are central to human-computer interaction will probably be of more value than a different, clever analogy. You may find this since-closed discussion on SO of value.