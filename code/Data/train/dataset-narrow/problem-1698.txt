When this happens, even though there is plenty of unused memory the kernel's heuristics will often cause it to pick the database server to kill (due to its large memory allocation). We had to develop a plugin for our collectd monitoring to see this in action. Our system runs a java process which had a memory leak that did not show in normal monitoring because the memory was never actually used. Hence we did not see memory use grow; however, we did see memory commit grow. We now believe this was the root cause problem. I have seen online references to this problem in java app/db servers, where large numbers of java applications running with a database server can cause the same issue (high overcommit, followed by the kernel killing the DB server). 

And the script to look at all files, and aggregate memory data for all processes of a specific name (again you could use other mechanisms to identify the processes to be analyzed). And again this summed the aggregate memory usage for all processes of a type. 

The script to gather process data via for all processes of a specific name (you could use other mechanisms to identify the processes to be analyzed). This summed the aggregate memory usage for all processes of a type. 

Don't bother with reverse proxy... just set up two instances in Apache. You can use a simple DNS server like dnsmasq on the Ubunutu server. Edit the /etc/hosts file to have two entries (assuming serverpc is 169.254.10.10): 

It wants to know the domain name of the system, so that it can pair that with the hostname to come up with the system's full-qualified domain name (FQDN). See here. I'm guessing this is an installation for your home, where you don't have a default DNS domain, per se. As such, this should probably be asked on Super User rather than Server Fault. The impact this will have is that software that looks for the FQDN will find it, e.g., mail server software.. It will also impact the domain that gets appended to a lookup. For example, if you type "ping systemxyz" it will actually try based on what you entered. Running will also help you understand some of the DNS resolution things that go on behind the scenes of your operating system. 

I found that Unbound meets all my criteria with the configuration directive . It even covers things like SRV records, etc (which I need). And it's super easy for even a simple Windows administrator to update the configuration file. I'm a fan. 

Worst case is I am hoping to get the of the Client List table so I can work with its contents. I can't seem to get any useful data no matter what page. I am not sure if the issue is working with Meraki or my PowerShell approach. I have only done simple scraping up until this point. I realize this is a very specific request but I am curious to know if someone that has Meraki is able or has been able to get this information using these means. 

When I run that command I get some of the user accounts and groups that I was removing while testing. According to this page on TechNet forums though that means the the bin is enabled. 

There are two sections of the script that make updates to Active Directory. First updates their employeeID 

So everything appears to be in check there as well. However in testing deleting both users and containers I am unable to find them in order to recover those objects. The only thing that ever shows up is the deleted container itself 

These are Windows Server 2008r2 machines in an AD environment. Some time in the past a DHCP server failed. An admin replaced it using a different host but the same address. Later on, when troubleshooting a rogue DHCP server, it was discovered that both the old and current servers were list as authorized DHCP servers. Once discovered it was de-authorized. That de-authorized our current productions DHCP server for a short period until that was reversed. Those two, with the same ip address, were listed for almost 6 months. Would that have been causing any issues or merely just an harmless entry in the authorized list? 

I'm showing my age with the L2L-xxx-Local/Remote parlance. That's the way the old 3000 VPN concentrator to used to define each side. :) 

I would start by disabling the Windows firewall (temporarily) altogether. Also, you can do a packet capture on the server to verify that the attempts to tcp/445 are getting there at all... something like assuming that 192.168.1.10 is the client machine and interface is the server NIC in question (run to get a list of NICs on the server). You can get WinPcap and WinDump from Riverbed. 

First off, you really only need one subnet mask entry, since they shouldn't be put on the same subnet if they have two different masks. But beyond that, yes, you AND the addresses with the mask, and if you get the same result, they're on the same subnet. 

In that link you sent, they have a section entitled, Sending mail through corporate mail servers. It describes using a VPN. So you would just relay to a mail server on the inside of your ASA and Google traffic filters wouldn't even see port numbers--they would just see ESP or UDP/500 (ESP over UDP). Concerning two subnets on one tunnel, I can only speak to the ASA side of things. I typically create a one-line access-list that defines two object-groups, then just load up the object-group definitions with the subnets you need to traverse the tunnel. For example: 

I made a script that takes data from an HR database and populates correlating attributes in AD e.g department, title, manager, location. Since people change titles, departements and/or locations on occasion it is important to keep AD up to date since we have processes that depend on the validity of this information e.g. location based dynamic distribution groups. To try and keep the process fast I just run for each users regardless if something changed or not. I had worried that I would be changing the modified timestamps needlessly since it is faster to just make these changes constantly then it is to verify that no changes would be needed. To my surprise it seems that AD is doing something like this for me as most of the user modified timestamps are not matching subsequent script execution times. This seems like a positive for me as AD is doing this for me under the hood. FYI I have 2 DC's that I could be talking to for this and I have checked both times to ensure that I am not just drawing a horrible conclusion. I cannot find an authoritative source that explains this and I am not sure if this is PowerShell doing the job for me or something Active Directory is doing. 

We are seeing a "leak" in "commit" over time, and need to track the offending process(es). RSS ("used" memory") isn't leaking, but the commit is, and its causing the kernel to behave very cruelly (failing to allocate memory when the commit % gets to ~ 200%). I know we can tune this with /proc/sys/vm/overcommit_memory but that's not the point - we'd like to find the leaky process. I've tried various calculations from etc/(proc)/smap and dmap, but nothing adds up across all processes to anything like the commit in /proc/meminfo. $URL$ is sufficiently vague that I'm not clear how to use it. Any suggestions? 

I have a strong suspicion this is due to memory over commit. This is not typically tracked by Linux toolsets: 

Posting to provide how we solved this - not elegant or polished, and not a direct answer (we didn't get the actual over commit allocation of a process), but we identified the offender and found a subtle memory leak. Do apologize for no comments etc, but these were quick one use tools. With that ... We looked at command results and a scan of data for processes, aggregated them for copies of the same process, and loaded them into a spreadsheet for analysis. We took snapshots several times a day over a week, and looked at the trend in memory usage. This showed us, for an aggregate set of processes with the same name, memory trends over time, which allowed us to identify the offending process set. The controlling script (ran a group of commands and organized the results in a file) 

With regards to the SendRequestUsingProxy failed, that should fail. The server does not have access to Microsoft websites so it will be blocked from being able to go there. What I can't figure out is why it isnt getting the updates from the WSUS server directly. We do not use a proxy nor is one configured. On the WSUS Server side of things I see that it get a download failed status for each of the updates. So in short the communication is there but the client is trying to download the updates from externally. It is a 2k16 server and reading the logs with has not proven useful. This is the only external server I have to the network so I do not have any comparison systems to know exactly where the system is. In an attempt to testing connectivity to the server I try to browse to $URL$ which is met with page cannot be displayed on the client server. (That link works fine on the internal network) Why is my Windows Update client not honoring the WSUS path for updates and instead attempting to go externally for Microsoft? 

Can a Windows OS that is automatically managing paging file size for all drives effectively use a drive that is dedicated for that purpose? 

We have a domain, say . We use a cloud-based provider for the authoritative servers. We have some customers with an IPSec tunnel that we need to give some of our internal records to. For that we have "semi-public" name servers that give a different IP address for some records, but if those records are not defined on these "semi-public" servers, the response needs to be forwarded to the "real" authoritative servers. We don't want to duplicate every record (there are somewhere around 1,000) in our "semi-public" name servers. We're currently using Cisco IOS (yes, IOS routers have name server capabilities) with views to accomplish this. It works okay but it's a bit flaky and the TTL is always 10 seconds with no way to change it. It also means that only the network engineers and not the server admins have exclusive access (politics suck). I have it working in the lab with BIND using RPZ (Response Policy Zone). However, an unfortunate consequence of that is that the responses never have the authoritative flag set. I'm nervous this will present a problem for some resolvers. I could skip the RPZ feature of BIND and just create a separate zone for every record, but that wouldn't be fun because who wants to maintain somewhere around 100 zones for BIND, just to have one record in each? dnsmasq would work for this, I believe, because it sets the authoritative flag. But unfortunately I'm constrained to something that will run on Windows Server 2012 R2. So I'm asking if anyone knows of a Windows solution (which I consider BIND to be since I can install it on Windows and it runs fine) that will provide authoritative answers to queries within a zone but forward on if a particular record is not defined? Perhaps I'm missing how to do this in BIND or perhaps there's another solution (e.g., Unbound). 

The windowsupdate.log corroborates this. I would like to try and include only what is required to try and keep the post length down. The client reaches out to the server and see that it has X available updates. However it fails to download those. The log shows entries like this: 

I have a Windows 2008R2 print server hosting about 40 printers. For the longest time we had a Point and Print GPO that allowed the users to install the drivers for these printers without administrative interaction. Administrative Templates\Printers\Point and Print Restrictions: Disabled. This is still in place. Recently though that is no longer working. Take the "sales" printer for example. People have been connected to it for years now and in the last few days, when someone tries to print, their computers (All Windows 7) have been asking to install a print driver. Even new users that have not been attached to that printer before are being asked for admin rights to install the printer. This has affected about half of the printers on this printer server. So some printers users are able to install just fine. So when someone has the issue I hop on their machine and provide my rights so the print driver will install. I am sure I know of the catalyst that caused this but I have no idea how it directly relates. For inventory purposes, I updated the host names of the printers. To clarify I went on the web interface off all the printers and in each of their network IPv4 configurations I updated the host name from its generic Ricoh to be the same as the DNS record I made for the printer. So each printer has a share name, port name on the printer server which are both the same as the physical printers host name e.g. "sales". No changes have been made to the print server hosting these printers. I don't understand how that change would cause this. In the case of the "sales" users it is preventing them from printing. We have to allow the driver to update before they can print. That is how we knew there was an issue and was able to tie it to my inventory update. These users are not all in the same OU in AD and both have the same policies applied anyway. I am testing different GPOs as when you look up network printer driver GPOs there are more things people change than just the one I mentioned above. Any ideas why what I did is causing this issue? Perhaps I am chasing the wrong tail and something else is wrong? 

Many pieces of software can do this. I'm not familiar with Mikrotik, but I have used BIND for this on Linux. You can find an overview here. Windows Server 2016 also has the ability to do it, though I believe the configuration for the particular feature you're looking for is via PowerShell only. Find an overview here. I have done DNS views on Cisco IOS routers (!) as well, so if you have one of those around, you could use it too. One limitation, however, is that the TTL on all records is set to 10 seconds, with no way to override it, so I would not advise that. 

I think some clarification needs to take place. SPF records are for mail being sent out from you. It applies to the zone that mail is being sent from (i.e., the "domain" part of the address). If your domain is and you're sending a message from a "normal mail client" as , you would have a TXT/SPF record for . If you're sending mail from your web server as , that one TXT/SPF record will cover that just fine, provided the IP addressing information you specify is correct (e.g., covers your web server and your mail server). Let's say your mail server is 52.56.221.36 and your web server is 52.56.221.37. You could have two separate entries in your TXT record: and (or if you want to use CIDR, just . You would only need a special "www" TXT/SPF record if you were sending mail from web.server@www.example.com. In that case, I would suggest having only one "main" TXT/SPF record, but just refer to it from your www record. In BIND it might look like this: 

I got the same results from the MXtoolbox. So I guess I understand why my mail gateway thinks it is supposed to send mail to itself. Internic.ca is a CA Registrar. This does not make sense to me. Even less when I try to email them from my google account and I don't get a bounce back. What does this mean? I don't think there is an issue on my side but I don't understand why this means. 

On a SQL 2008R2 box we recently had a number of jobs fails for various reasons that were mostly memory related, including one stating the page file was full. The Windows 2008R2 VM had 16GB of RAM and a dedicated disk for a 6GB page file. For now we moved the page file back to the C: drive and increased its size to 8GB. The long term effects of that are yet to be seen. Our Server Admin, this morning, increased that "swap" drive to 25GB as was recommended by the GUI. What struck me as odd is that the admin also changed virtual memory to be mananged automatically across all drives. This strikes me as waste of space but I don't really understand how Windows automatically manages the page file. Here is a snapshot of the current virtual memory settings to help with the description.