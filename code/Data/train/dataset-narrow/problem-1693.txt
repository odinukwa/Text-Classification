This is logged 32 times during the 16 minute outage I am trying to figure out what is going on here. My concern is that due to deficiencies in my balancer config, that the balancer is repeatedly trying to send requests to the rebooting server (and therefore returning errors to the user). Why is Apache repeatedly telling me that it is "disabling worker"? Does the balancer module intermittently send user requests to the failing node, to try and determine if it is back up, or does it have its own internal health checking mechanism that is invisible to the user? 

A wildcard is the best option, but I'm guessing you didn't buy one of these, and that you have a cert that is specific to "sitename.com" 

Note: in this case the role "Admin" is an example. You need to update that to whatever cross account role you have created. Similarly, "dev" is an example profile in my configuration. Source: $URL$ 

The $1 is everything in the REQUEST_URI, and the QSA flag will append the existing query string (the GET vars) to the new URL. 

I am implementing a reverse http proxy: proxy.example.com This will forward requests to servers based on URI: proxy.example.com/server1 -> server1.example.com When a user requests proxy.example.com/server1, server1 sends a programatically generated (Ruby Devise Gem) 302 response with the following "Location" value: proxy.example.com/users/sign_in I need this to be: proxy.example.com/server1/users/sign_in I have implemented the following config in Apache: 

I'm trying to centralise logging in an environment that using multiple application technologies (Java, Rails and various DBs). We want to developers to bring up stacks with Docker Compose, but we want to them to refer to a central log source (ELK) to debug issues, rather than trying to open shells into running Docker containers. The applications all write to the file system rather than to STDOUT/STDERR, which removes all of the options associated with the Docker logging driver, and logspout too. What we have done is configure the containers to have rsyslog include the application log files and forward those to logstash which has a syslog input. This works in terms of moving the logs from A to B, but managing multi-technology logs in ELK based on the syslog input is horrible (eg trying to capture multine Java stacktraces, or MySQL slow queries). Is there a better way to do this? Should I be running logstash in each container, so that I can apply filters and codecs directly to the log files, so that I don't have to rely on the syslog input? Of is there some way to use the Docker logging driver with application log files that are written to the file system? 

I'll work on the premise here that you are talking about the root EBS volume. ie You have launched an instance, and you now want to detach the EBS root volume, and attach that to another instance. Yes, you can do this. Stop the instance (1). Detach the root volume. Launch another instance (2), for which another root volume will be created. Stop that instance too, and detach the newly created volume. Attach the root volume from instance 1 to instance 2 as /dev/sda1 Start instance 2. No need to re-install anything. You can also accomplish the same effect by making an AMI from instance 1 and relaunching it as instance 2. 

I use a combination of Logstash and the AWS Elasticsearch service to index S3 access logs. The logs are collected in an S3 bucket, processed with the Logstash S3 input filter, renamed after they are processed and then archived in another bucket. I use this method so that the number of access log files that Logstash has to process in each rotation is as small as possible. However, the logs are not being processed in real time. When I look at Kibana or query Elasticseach, the most recent log entry that I see will be the latest log entry from the previous hour. I never see log entries that are < 1 hour old. I can't see anything in the s3 input configuration options to control this behaviour. There is an interval config option, which I have set to 120 secs. This is supposed to instruct Logstash to poll the S3 bucket which contains the logs every 2 mins. I also use this Logstash system to process syslog input from a variety of servers, which does process logs in next to real time. Is this something peculiar to the S3 input filter in Logstash? 

Yes, it is safe to CTRL-C out of the pg_stop_backup(). The build up of files in the /data/wal_archive directory arises from the archive_command on the master. Once the postgres service is started on the slave, these "logs" are consumed (ie written to the DB on the slave) and then deleted. 

Your problem seems to be that you have have given the third party a copy of the private key that is used to access the server. You could just disable access for that private key, but it would seem that other people are also using it, so your Network Administrator doesn't want to do that. You really should have asked the 3rd party to generate their own key pair and provide you with the public key. You could then have permitted that for a short period of time and removed it, which would not have impacted on anyone else. There isn't much you can do. One option is to restrict access to the server by using the limit features of the authorized_keys files. If you know the ip addresses from which access is allowed, you can configure OpenSSH to only allow access from those addresses, even if the correct key is provided. eg $URL$ You could also experiment with the "Match" options in sshd_config to prevent access from users who match certain criteria. Bottom line: never share private keys. Every user should be using their own key pair. 

I've discovered after much suffering that the best way to run elasticsearch on a single server is to change the default setting of: 

The solution to this was quite straightforward, but it throws up an interesting observation re. the use of traceroute to debug routing problems. The source of the problem was that I had not enabled ip forwarding on any host other than Nat Instance 1. ie 

I am currently migrating a DNS zone from one DNS server provider to another. I am trying to estimate how long it will take for the change to propagate, and to understand what the delay might be if I chose to rollback mid-stream. Previously, I thought I could do: 

Boto has a function, update_environment, that allows the user to update options in an AWS ElasticBeanstalk environment. Using the AWS CLI, this would typically be actioned as follows: 

Apache httpd sends request to worker Worker doesn't respond, or responds with HTTP status which triggers failover,and puts member into ERR state Apache httpd starts retry timer (default 60 secs) and doesn't send any more requests until retry timer expires When retry timer expires, go back to Step 1 in sequence 

I would recommend the following considerations: If you creating an IPSEC connection between your corporate LAN and your VPC, use a CIDR that is different than that on your corporate LAN. This will prevent routing overlaps and create an identity distinction for reference. For very large networks, use at least different 16-bit masks in different regions eg 

You can't use backup_add_prefix by itself (the docs suggest you can). You can only use this parameter in conjunction with backup_to_bucket Make sure the IAM account/role you are using to interface with S3 has Write permissions for the buckets you are using (other Logstash can't delete/rename objects). 

I am trying to deploy a Rails application in the Phusion-Passenger Docker image: $URL$ The Gemfile for the application specifies Ruby version 2.3.1, but the image comes with 2.2.5 by default, so bundle install fails. I've read the documentation a couple of times, but it isn't clear to me if it is possible to user a different major/minor version of Ruby with this image. I have tried the following in the Dockerfile 

One thing that is very disconcerting about using the Azure portal when you are familiar with AWS is the frequency with which is spawns new browser windows. As per normal with Microsoft, in trying to make this product idiot-proof, they've made it a horrible experience for people who understand how to work a computer. 

$URL$ If you're paranoid, just update the yum command to report only. You can also rollback changes if issues occur. 

Not sure if your understanding of how Skype works is correct. Skype calls aren't initiated and terminated between users in the same way as a normal TCP conversation would take place. Each Skype client registers its location with Skype's central servers. When one user calls another user, the initiating client asks where the target client is, and the initiating client then starts streaming UDP (or relayed UDP, or sometimes even TCP) to the terminating client. However, because all of this has to involve an independent public Skype host, all of this communication will happen over the public internet, rather than over your VPN. To get Skype to flow over a VPN is very difficult, because once Skype can find a way to the Internet, it will always flow that way, regardless of whether you configure it to use a proxy. You would need to source route Skype traffic on your network device so that it is blocked from accessing the Internet, and then proxy it to a server on the other side of your VPN, and vice versa, which clearly is a whole lotta pain. Bottom line is that Skype probably is never going to use your VPN. Most likely you performance issue is cause by Skype not being able to punch a whole in your firewall to do pure UDP, and reverts to Relayed UDP or TCP. One quick win is to allow all UDP in and out of your firewall, but that still doesn't guarantee pure UDP. That will depend on how congested your Port Address Translation is. $URL$ 

I've inherited responsibility for a 400GB MySQL database that is hosted in Amazon RDS. The DB serves an application that relies intensively on DB operations, so version releases frequently involve running ALTER statements on very large tables. These updates take over 24 hours to run. Typically, we make a snapshot of the production RDS instance, restore it to a temporary instance, run the migration on that and then rename it so that the apps start connecting to it as the Production DB. The obvious difficulty in this is that during the 24+ hour upgrade, we have to keep both DBs in sync, which means data has to be loaded to the application twice, which is a major headache. I'm convinced there is a better way to do this (albeit I'm aware of the limitations of RDS), but I can't see find a solution. The DB can't be live while it is being altered, but the alteration takes forever. Anyone have similarly experience, or should I just learn to live with it? 

I am trying to create a hot-standby replica for a PostgreSQL 9.5 master. The basic process I have used is to start a backup on the master: 

Where pclean.sh is the name of the locally saved script file, and it is saved in the same directory as your working directory (otherwise include the absolute path). With this in place, each time you launch a new instance, regardless of its initial hostname, it will revoke any existing cert that has the same hostname, and generate a new one. Obviously, if you are launching hundreds of instances at the same time, you may have concurrency issues, and some other solution will be required. 

AWS directs traffic to its ELBs via its Edge network, so the ip address that the request arrives at will depend on the ip address from where the request originated at. From that point of view, you can't reliably pin-point which ip addresses your clients requests will go to. You can deploy internal ELBs which exist inside VPCs. These will have fixed RFC 1918 ip addresses (private ip addresses) within a subnet and will only be available inside your VPC. I am not sure what you mean by "VPC Gateway". VPCs have Internet Gateways (routes to the public Internet) and Customer Gateways (routes to 3rd party data centres via VPNs). If you access an ELB in a VPC via a Customer Gateway, you can reliably predict the range of ip addresses of the ELB (particularly if you connect your ELB to a subnet that has a very small number of ip addresses). However, for this to work, your customer will have to establish a VPN to your VPC, which involves configuration work on the VPN termination equipment at your customers site. 

Basically, the IAM documentation is totally unreliable when it comes to doing anything other than set global admin or read-only policies. This is the policy I eventually got to work (for the subnet bit at least): 

Check CloudWatch monitoring to see what the CPU as doing. If the CPU was max'ed out, you would not have been able to make a HTTP or SSH connection. If you're running something like Wordpress on an EC2 instance, prolonged CPU spikes are common. 

Take a look at Dan J Bernstein's daemontools. It can monitor any process and restart it instantly if it fails. Perfect for keeping things like tunnels alive. You can set it up in 5 minutes. $URL$ For a simple HOWTO, check: $URL$ 

I've confirmed that NGinx is preserving all the Environment Variables that I need. The difficultly seems to be with this line (line 11): 

We are in the process of moving our MySQL DB from AWS RDS to EC2. RDS provides us with with a solid HA solution, which we want to replicate in EC2. We've looked at MySQL Utilities and MHA, both of which present problems for us. As a result, we're thing about a DIY solution, which will work as follows: We will have a Master and a Slave replica (Slave 1). We will have a Slave replica of the Slave (Slave 2). Master and Slave 1 will be behind a HA Proxy cluster. We will monitor the Master very closely. If the Master monitor detects a failure, we will run a script that shuts down the Master EC2 instance and stops the Slave process on the replica. Slave 1 will now become the Master, and Slave 2 will become Slave 1. The new Slave 1 will be added to the HA Proxy cluster. We will then rebuild Slave 2 from a backup of Master (backup every 3 hours). We obviously need to be careful to ensure that all steps in the process exit cleanly, and that we have rock-solid error handling. Can anyone offer an opinion as to the viability of our proposed DIY solution?