There is not any single good answer to this question as this thing depends on many factors and one of them is the . As per my experience, I prefer not to remove features as even little information can be very useful. 

As told by @smci, this technique is called Data Imputation. There are several techniques which can be used to deal with the missing data. Some of these are: 

There can be many reasons for this thing but in most of the cases I have observed one common reason. When you split your data using train_test_split or any other method, it is important to note that the column on which you are splitting the data is significant for splitting considering both the training and testing set. For example if I have a 'time' field in my data and I have split the data into training and testing sets on this column such that there is no value in this column of the test set which matches with any of the values in the same columns in the train set. 

are used to control the output of a neural network. They tend to make the output smaller. Suppose the loss function is give as : 

I have a dataframe consisting of some continuous data features. I did a kde plot of the features using seaborn kdeplot functionality which gave me a plot as shown below : 

There are many but some of the resources are awesome. One such resource is Udacity. This is probably the best resource I have encountered yet. 

If you don't define the validation set/ validation split for your model, then it has no way to check for it's performance because you have not provided anything to the model on which it can validate its performance. In this case, the model will run through training examples, will learn a way to minimize the cost function as per your code and that's it. You get a model which has learned a hypothesis from your data but how good the model is, it can only be checked by making predictions for the test set. 

MapReduce: A fault tolerant distributed computational framework. MapReduce allows you to operate over huge amounts of data- with a lot of work put in to prevent failure due to hardware. MapReduce is a poor choice for computing results on the fly because it is slow. (A typical MapReduce job takes on the order of minutes or hours, not microseconds) A MapReduce job takes a file (or some data store) as an input and writes a file of results. If you want these results available to an application, it is your responsibility to put this data in a place that is accessible. This is likely slow, and there will be a lag between the values you can display, and the values that represent your system in its current state. An important distinction to make when considering using MapReduce in building realtime systems is that of training your model, and applying your model. If you think your model parameters do not change quickly, you can fit them with MapReduce, and then have a mechanism for accessing these pre-fit parameters when you want to apply your model. Storm: A real-time, streaming computational system. Storm is online framework, meaning, in this sense, a service that interacts with a running application. In contrast to MapReduce, it receives small pieces of data (not a whole file) as they are processed in your application. You define a DAG of operations to perform on the data. A common and simple use case for Storm is tracking counters, and using that information to populate a real-time dashboard. Storm doesn't have anything (necessarily) to do with persisting your data. Here, streaming is another way to say keeping the information you care about and throwing the rest away. In reality, you probably have a persistence layer in your application that has already recorded the data, and so this a good and justified separation of concerns. If you want to know more... If you would like to learn more about real-time systems that that fit parameters with MR and apply the models a different way here are slides for a talk I gave on building real-time recommendation engines on HBase. An excellent paper that marries real-time counting and persistence in an interesting way is Google News Personalization: Scalable Online Collaborative Filtering Another interesting marriage of MR and Storm is SummingBird. Summingbird allows you to define data analysis operations that can be applied via Storm or MR. 

Hyperparameters and parameters are often used interchangeably but there is a difference between them. You call something a 'hyperparameter' if it cannot be learned within the estimator directly. However, 'parameters' is more general term. When you say 'passing the parameters to the model', it generally means a combination of hyperparameters along with some other paramaters that are not directly related to your estimator but are required for your model. For example, suppose your are building a SVM classifier in sklearn as shown below: 

I will assume that the dataset here is being split into training and validation sets. When you split up your dataset into training and validation sets, you need to take care of that you don't throw away too much data for validation because it is generally seen that with more training data, we get better results in most of the cases. So 50:50 is too bad, 60:40 is fine but not that good. You can make it 80:20 or 75:25 for getting better results. 

So this vector y = [0,0,0,1] is your target to these input types that you feed into the network as labels corresponding to your above input data. 

In general, more training examples means improvement in learning but you can also get a very good (and nearby to the optimal score) if you just fit a good algorithm on a subset of your data set that has enough training examples. Here are a few things you can do in your current case : 

Welcome to the community!! There can be a lot of answers to this question but I would suggest you the approach I took when I shifted from software development to the data science field. 1) Refresh your statistics and probability concepts. You should not go into too much details but you must understand basic things like Gaussian Distribution, Mean, Variance, Probability,etc. 2) Go through the basics of Machine Learning concepts. I prefer Andrew Ng's machine learning course on Coursera. That will help you build a strong foundation and will give you a great start in the field 3) Choose a particular language Python/R for building models. Though it's totally upto you but I prefer python as it has great libraries for machine learning as well as Deep learning. 4) Take part in competitions. We learn by doing not by taking only lectures. I suggest you should join Kaggle and the slack community out there namely, 'KaggleNoobs'. It's a great community. I learn everyday a new thing from there. P.S: Data Science is a vast field. It demands from you various skill set like Data Analysis, Data Visualisation, Machine Learning,etc. So sometimes it can become frustrating too. But once you start enjoying it, you will become a master eventually. 

If you are using a scraping tool that manages a web session for you, you should be able to get most things done using a scraper. You should be able to log in to sites and services and download the content that you are interested in. The biggest difference between scraping directly in Python using BeautifulSoup or something similar is that the basic Python web scrapers will not execute active content on a page. That is, if a page uses Javascript to load some dynamic content and then display it, this content will not be available to you from your scraper. This may not matter for the sites that you are working with, but more and more sites are relying on Javascript to function, so you may find that you eventually need a scraper that can run Javascript. If you do need to execute Javascript from your scraper, you'll want use full headless browser to do your scraping. Some examples are: 

Treat values as categorical by default. Check for various attributes of the data that would imply it is actually continuous. Weight these attributes based on how likely they are to correlate with continuous data. Here are some possible examples: 

There may be differing implementations, but these two terms refer to the same thing. Both convert a generic block of text into a vector similarly to how word2vec converts a word to vector. Paragraph vectors don't need to refer to paragraphs as they are traditionally laid out in text. They can theoretically be applied to phrases, sentences, paragraphs, or even larger blocks of text. Here's one definition of a paragraph vector: 

Treat any columns that sum to greater than 1 as being numerical. Adjust the factors and weights based on testing against different data sets to suit your needs. You could even build and train a separate machine learning algorithm just to do this.