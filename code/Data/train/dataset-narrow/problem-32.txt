We're just starting to push for CI-CD and as a baby step we're going to try updating a stack with the latest green develop once every couple of hours. I am fairly new to Git/Bitbucket, and can't figure out how to ensure the checkout that Jenkins makes gets the last commit to have been marked green by Jenkins, rather than just "the last commit" as a blanket statement. We have the Bitbucket Build Status Notifier plugin installed, so Bitbucket does track which commits are green after our unit tests run. Is there a way to leverage this info to make sure the right commit is picked? 

I have an upstream freestyle job and a downstream pipeline job. The freestyle job is triggered via a Bitbucket hook to run whenever code is pushed to Bitbucket. The freestyle job should capture the commit SHA and branch name and pass both to the pipeline job. It also generates a variable based on the branch author, that should also be passed. In the freestyle job I do something like the following, which works and echos the expected parameters. 

I'm looking to automatically back up Jenkins log files for all jobs running on our master and slaves once a week, moving them to aws' s3 and deleting them from their respective directories when done. My question is, is there an existing plugin that has the capabilities I need, or will I need to do this manually? I've found the Periodic Backup Plugin but it only seems to back up locally, and is focused on config files, not log files (although I could use negative pathing to ignore everything but log files...which would be a massive pain). Likewise, ThinBackup is also config-only. I'm hoping someone might have seen something googling isn't turning up. 

My understanding of Ansible roles is that they are the unit of reusability when implementing processes using Ansible, similarly to what a class or a package is in some computer languages. It therefore seems natural to use roles to make all tasks understanding a given data structure in a specific role, but this yields the question of how to select the actual tasks to perform? Is it a good design to use the file in the task as an entrypoint to select actual tasks according to role parameters? For instance a role could feature: 

It is a common misbelief that “DevOps” is a role in a company. The word merely denotes an organisational shift in software companies, this shift can be shortly described by the “You build it, you run it!” coined by Werner Vogels: 

Feature flags are an engineering device that can be used to avoid long-lived branch and conflicts in product development. Here is how it can be used the context of an object-oriented language to help developers collaborate on a specific product feature while one handle a new version. This solution can also be used in non object-oriented contexts, provided a notion of “interface” exists. (cf. OCaml module system.) For the purpose of illustration, we assume a tool presenting reports about data stored in a database. The code implements a DatabaseClient class used to perform requests. As the dataset grows, it becomes clear that some alternative data layout would improve the application performance. Therefore Alice will develop a new version of the DatabaseClient able to retrieve data from the structures with improved layout, while Bob will maintain the historical DatabaseClient. With the following steps, Alice and Bob can collaborate on short-lived branches while minimising their conflicts. 

Is it possible to launch a jenkins agent from within a script in another job? I'm open to either shell or pipeline options. To be clear I don't mean the agent the job is running on itself. I'm talking about spinning up a completely separate agent. Sometimes our master gets slammed with unit test requests and will make jobs wait in the queue for 15-20+ minutes instead of just spinning up more agents. I want to see if there's a way to spin an agent up intentionally so I can tie it into a job that polls the queue for wait times. 

Run a curl command on the Jenkins REST API to get the info on the last successful build and store it in a temp file Use jq to parse said info and extract the timestamp field, which is in Epoch time. This is stored to a file. The file is archived to the job and then read into a variable. This timestamp is compared to the current time, (gotten at the top of the job), and if the break between them is less than eight hours the job is "failed" out with a success and error message. If the time is over eight hours, the job continues on to build the downstream job. 

TL;DR: Is there a different call I can make that will return some list/file/content that indicates that multiple builds of the same job are currently running? 

It always completes successfully, even with the delay. Has anyone seen this before? Is there anything I can do to prevent the step hanging? 

We just updated our Jenkins to v2.207.1 and our build estimation times on pipeline jobs have stopped showing up. Rather than a blue bar that when hovered over shows an estimate, we are now seeing the blue-and-white striped barber pole and an estimation of N/A. I believe this behavior might be only on pipeline jobs - we have so few freestyle jobs that I haven't seen them run yet. What logic does Jenkins use to calculate those estimations? How many builds will we need before those times fill back in? 

A file added by some docker instruction and removed by some later instruction is not present in the resulting filesystem but it is mentioned two times in the docker layers constituting the docker image in construction. Once, with name and full content in the layer resulting from the instruction adding it, and once as a deletion notice in the layer resulting from the instruction removing it. For instance, assume we temporarily need a C compiler and some image and consider the 

First, while Docker is sometimes seen and used as a ad hoc packaging system, it actually solves a totally different problem: Docker is about running programs. The Docker system allows to describe services, that can be scaled at will and to control swarms of containers. Debian packages are for installing programs and they are able to handle dependencies between software versions. Docker certainly don't qualify as a descent packaging system: each “package” can only have one dependency, the system has no “recursive build” option and does not support complex version constraints! A possible answer would be that, if you are willing to write a Debian package for your application, you can also use Docker to deploy your application. This can be achieved with a configuration script which would look like 

After this, Alice and Bob can collaborate without explicit synchronisation, because code written in their respective iterations is subject to the DatabaseClientInterface. This minimises the risk of a conflict resulting from their concurrent work. Iterations from Alice can be very short, like implementing a test, implementing a method, or even partially doing so, because in production, the code is not selected for use and does not need to be fully functional. The automated testsuite should be configured so that the DatabaseClientInterface always uses DatabaseClient_v1 while Alice can easily toggle to DatabaseClient_v2 when running the testsuite locally – or in a custom CI setup. Once everything is ready, a single commit can perform the change, by updating the configuration value governing the DatabaseClient delegate. 

It depends on what you mean by "log files" I supposed. If you mean the telemetry data you use to verify your system is operating correctly I would say "Don't log sensitive fields." You don't need that kind of information to alert you to high or low transaction rates, response times to your dependent services, etc. If you mean data for billing or audit purposes I would suggest you establish a write-only pipeline where the data is written but can't be read by the writer. Then your billing and auditing pipeline kicks in and you have the controls there to audit who looked at individual records. In the end security comes down to documented processes with their own secure log files, etc. At some point you have to trust someone or some process since all data is written so that it can later be read. 

I would add that a toggle is on/off (show ads on the site, don't show ads) and perhaps augmented by a flag like (West Coast gets ads from provider A, East Coast gets ads from provider B). Toggling turns off all ads. A feature flag might be able to switch from provider A to provider C. I also think a key part is deciding how your switches and settings work - do you have to roll out a new build? or can your control tower determine east coast/west coast and answer "which provider" all on the fly with an easy, almost foolproof interface. 

Sure it is. Production is always different. Real money. Real load. Real users. Real pain. This is why it is so important to put any significant change behind a feature flag. Your deployment should not change anything. Turning on a feature is the only thing that should make significant changes to your site.