The question is why would you want to do this? NTP is designed to ensure systems have the correct time (usualy within milliseconds of atomic clocks). It is possible but not recommended. You can use your local clock as the only other source. Alternatively, you could hack the code to sync with a shadow time with the desired offset from the system time. If you are doing this to show the time as it will be in five minutes, then this may be the wrong solution. It may be better to create a custom time zone with an offset of 5 minutes from the reference timezone. 

I've found that moving the files to the correct directories is the easiest way to move the data. This should work between server of the same architecture. RRD uses hardware formats for binary (numeric) values, so in some cases it is necessary to dump and reload the data. If you do need do this, you need to dump on the original server and load on the new server. Using a shared directory (NFS, SMB, or SSHFS) may make this simpler. You will need to copy (replicate) the munin configuration so it knows which servers to generate files for. The userid munin runs as will need write access to the output directories as well as read access to data (RRD) files. Write access is required to gather new data. Ideally, all files and directories will be owned by the account that is being used to run munin. I would test with data for one server. I believe munin-graph contacts the monitored server to get the configuration for the graphs, although I did find a cache of the data in /var/lib/munin/datafile. If you haven't granted access to the new server yet, this may be causing your issue. 

Choice of backup software/strategy is likely more of an issue than Linux distribution. Oracle has their own distribution which may be appropriate either as the base or a VM. Your software choice may limit your options for Linux distributions. Backing up a running Oracle database is possible, but requires co-ordination between the backup software and Oracle. Do some research on hot backups and cold backups for Oracle. Oracle also has its own RMan software to backup the database to tape or disk. This makes it easier to schedule hop backups. 

Look into clustering support for your server. Properly configured clustering will replicate your session data between the two servers. 

Due to the way it is defined a must be the only RR for the domain. As a result, it is only applicable to apex domains. can be used in place of multiple records when all the sub-domains of a domain should be redirected to the same structure under a different domain. The record does not redirect the domain it applies to. Servers querying a domain redirected by a record will get a response. If both domains are served by the same servers, it would be possible to avoid redirection by using the same zone file for both domains. 

To use SNAT, you would need to make the Linux server the default router for the Windows server. This is not a recommended solution. A common solution for this problem is to proxy the traffic on the Linux server. Have the proxy add a header with the external IP, or rely on the X-Forwarded-For header. This will require the software on Windows to process that header with the external IP. Many applications that are commonly proxied, have a setting to do this automatically, possibly with an application specific header. 

Logrotate is a good tool for log rotation and compression. It is available for most Unix flavors. I find the defaults used on Ubuntu to be a good starting point. When changing rotation frequency, you should change the rotate count. Keep logs as long as they may be useful. Archive to non-disk storage if necessary. For business systems there may be legal requirements directing retention and/or destruction of log data. Depending on the data and availability of data in another log data may be kept for a week, a month, a quarter, or a year. Only duplicated data is deleted after a week. A monthly backup retained for a year will give you a years worth of most log data. A centralized log server generally serves a different function than log retention. Comparing the centralized logs to host logs may detect log file tampering. 

It appears you have a rewrite rule that is appending the domain to whole address rather than the local_part. Try testing your rewriting from the command line: 

Bind support statistics. Cjeck the detail in the Bind 9 Configuration Reference. I think you want to set zone-statistics to yes to do what you want. 

For the servers I have worked with, it has been the responsibility of the application to retrieve the post data. The HTTP protocol provides a clear demarcation between the header and date. The server will process the header and hand-off the socket to the application. Where an application is spawned to service the request the socket is STDIN. It will be the application that retrieves data from the socket by reading STDIN. The application may be started before all data has been received from the client. It should handle cases where the client closes the connection before all data has been received. When the data being transferred is large, there can be issues if the full data is read into application memory. It is often better to read file parts into a file. This would be the application's responsibility. If the post data is smaller than the TCP window size, it is possible the data has been received before the application begins. The data would be stored in the network stack's buffers. 

A lot of bots don't get the hostname right. I suspect that is what you are seeing. Looking at my mail database, I only see one record like this. However, a lot of bots trigger response delays and give up before they get to the EHLO/HELO step. I don't know the Postfix capabilities, but I delay answering the connection for several seconds if the DNS entries for the server aren't good. A proper mail server will wait and connect. Most Bots don't wait around, likely because they want to deliver messages as quickly as possibly, not as reliably as possible. Try doing a host look-up on the addresses which are failing. Valid mail servers will return a pointer to a FQDN (Fully Qualified Domain Name). This FQDN will almost always return the IP address you started with. If not, you likely have a Bot trying to connect. As the name is blank, this is most likely. 

It would be easier to place the VPN in a non-bridged mode, with a separate sub-net. I try to use a separate sub-net for each security zone. This helps address problems like you are encountering. The tradeoff is increased routing complexity versus simpler security setup. I am not sure of the iptables syntax that will result as I don't use bridged mode in my network. But you should be able to specify the tap interfaces in your rule set. If you are running on you may want to look at the Shorewall and Bridged Firewalls documentation. 

Other than the dump on the remote system, this command can use surprising little memory. Mysqldump can page the data files into memory as needed. Indexes are unlikely to be used, so not all the blocks in the data files need to be read. Besides the extra I/O to read blocks in from disk, there may be additional I/O to replace them in buffers. As this is happening on another system, the local impact is only a small amount of memory for network data buffers, and the small amount mysqldump needs to construct the output. 

I find I get a relatively high failure rate on DKIM signed documents from automated senders. This is mainly related to the public keys not being available from DNS. 

It is best to install the highest level package possible. This ensures you get all the components you need. On upgrades you will get any new packages which have been added, and packages which are no longer required get removed. You will end up installing all the components anyway, but it will take more effort. The debian package manager has been designed to ensure you get all the dependecies intalled. It will install any missing packages, and use already installed packages. There can be alternatives, and you get a default package installed. This can be changed by either pre-installing another alternative, or installing it later. To install lamp server, you may need to install mysql-server, but this does not need to be on the same server as apache. Otherwise you will want mysql-client. This is a deployment decision you need to make. Installing php5 will pull in apach2 as a depndency. Any LAMP based packages such as wordpress will bring in the components which need to be installed on the server. 

I did a blog posting on setting up an email server. Other than DNS, I think you have done a good job of covering everything there and more. Legitimate bulk mailers tend to do a poor job of configuring their servers. I don't know why as it is their business. 

Check your error log to see if anything is logged there. I have found the solution there in a few cases where I got directives wrong. 

As James has noted you unlikely to be the administrator for the reverse zone (PTR records). Contact whoever provides your IP address and ask them to make the change. This usually takes a few days. You There will be a further day after they implement until cached values expire on other DNS servers. You could ask them if they support delegating reverse lookups. This is defined in RFC 2317. You will need to make slight modification to the above zone file for it to work. For IPv6, it should be easier to get the reverse zone delegated to you. In IPv4, you usually don't get the reverse zone delegated unless you have at least a /24 block allocated to you. 

You can test your rewrite rules as documented in the Exim4 Specification rewrite chapter with a command like: 

EDIT: Moving your customer's web hosting service has nothing to do with their email service. As long as the destination of the MX record is not changing, nothing will happen to their email delivery. Just change the A records for their web site. It is good practice to reduce the TTL on the records as described in other posts or in my original response. If they are also moving DNS that question has already been asked and answered. There can be issues if their is a period when neither the new nor old DNS is working. Otherwise it can be seamless. ORIGINAL: Keep the old service up until the old records expire. Once you have the new service up, GMail could be configured to forward any new mail to the new services. Prior to changing the MX record, it would be appropriate to shorten the TTL (time to live) on the record to a few minutes. Old cached entries will retain the old TTL value, so adjust your expiration expectations accordingly. Change the setting with the short TTL, test, and monitor for email to start arriving at the new address. Once you are sure the new MX record is working, you can increase the TTL back to what it was. Once email stops arriving at 'GMail', he can retire that account. Email may be lost if you specify the new destination incorrectly. For legitimate email sent to an address which does not respond it is likely only SPAM will be lost. If you incorrectly specify the address of an unrelated mail server, then it is possible that they will correctly bounce the email. Consider adding the new server as a second MX record and adjusting priorities. Use a higher priority (lower preference) initially, then swap priorities once you known the new service is working. Once email start to flow, remove the old MX record. 

It looks like you are missing a definition for the table. This should likely contain an accept rule for your primary server. I don't know if you would need to whitelist any other servers. 

They will time out eventually. Before making changes you should reduce your negative TTL to a reasonable amount. The value is specified in your SOA record for the domain. Query your servers for the SOA record to determine how long the timeout might last. Default value is documented as 3 hours, and maximum value is 7 days. As you have found, it is not a good idea to query your local servers for new services before you know they are available on all your authoritative nameservers. Doing so may prime the cache with a negative answer. Query them first to verify. 

Your problem may well be your DNS configuration. Start with a static DNS address and get reverse DNS working. DKIM and SPF both have DNS components which need to be configured correctly. I don't believe either DKIM or SPF will solve your problem, but do get them working. I have posted on Detecting Email Server Forgery. This is written from the perspective of the recipient. However, it specifies the DNS entries that your need. If you are having problems sending to Gmail and Yahoo, it is likely one of the rules in this document that is causing problem. My post on Securing your Email Reputation with SPF outlines the various SPF records you may want on various servers. You should protect your domain, web domain, as well as your mail server domain. My post on Implementing DKIM with Exim also covers key management and the DNS entries for bind. You will need these if you want your DKIM signatures to be valid.