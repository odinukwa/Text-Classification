$k$-Vertex Cover: Given a graph $G = (V, E)$ where $V$ is a set of vertices and $E$ a set of edges, and an integer $k$, the $k$-Vertex Cover problem determines if there exists a subset of vertices $V'$ of $V$ of size at most $k$, such that every edge of $E$ has at least one vertex in $V'$. There are 2^k possible vertex covers. $\sigma$ is at most $k$ length $0$'s and $1$'s bit string. For each value of $\sigma$, we can generate the vertex set of size at most $k$ and it may become potential vertex cover. For each $\sigma$ we build circuit as follows: $\mathcal{C}_1$ takes input ($\sigma_1$, $e_1=uv$) outputs $u$ if $\sigma_1=0$, $v$ if $\sigma_1=1$. $\mathcal{C}_2$ takes input ($\sigma_2$,$e_2,e_3,\cdots,e_n$, output of $\mathcal{C}_1$) outputs vertex (It finds the least index edge (say $e_i=u_iv_i$) which is not covered by vertex $u$ (say out put of $\mathcal{C}_1$) and $\mathcal{C}_2$ outputs $u_i$ if $\sigma_2=0$, $v_i$ if $\sigma_2=1$) $\mathcal{C}_3$ takes input ($\sigma_3$,$e_3,e_4,\cdots,e_{2n}$, output of $\mathcal{C}_1$, output of $\mathcal{C}_2$) outputs vertex (It finds the least index edge (say $e_i=u_iv_i$) which is not covered by vertex $u$ (say out put of $\mathcal{C}_1$ and vertex out put of $\mathcal{C}_2$. $\mathcal{C}_3$ outputs $u_i$ if $\sigma_3=0$, $v_i$ if $\sigma_3=1$) Each circuit $\mathcal{C}_i$ outputs vertex. This continues un till finds the $k$ vertices. At the end we need to check whether $k$ vertices given $k$ circuits forms a vertex covers or not. I am little confusion about to build each circuit $\mathcal{C}_i$ take only constant. Please let me know what is the circuit complexity of above process? Can it be $NC_1$ or $AC_0$? 

Restricted $k$-set cover: Input: $(U,S_1,S_2,\cdots, S_n, k)$, $U=[n]$ and $S_i\subseteq U$ for all $1\leq i \leq n$. Output: $\bigcap_{i\in I}S_i$ where $I=\{1,i_1,i_2\cdots,i_k\}, i_1=min(S_1),i_2=min(S_1\cap S_{i_1}), \cdots, i_k=min(S_1\cap S_{i_2}\cap\cdots\cap S_{i_{k-1}})$ This problem is Restricted $k$-set cover I want to know the above problem is in L or NL? I trying to show this problem is in NL-complete. But I did not get any problem which is NL-complete connects to this problem. 

$k$-Dominating set: Given a graph $G=(V,E)$ where $V$ is a set of vertices and $E$ a set of edges, and an integer $k$, the $k$-Dominating set problem determines if there exists a subset of vertices $Vâ€²$ of $V$ of size at most $k$, such that for every Vertex $u \in V$, there is an edge $uv \in E$ for some vertex $ v \in V'$. It is easy to see $k$-Dominating set problem for planar graphs in $O(f(k)\log n)$ space. Can we solve the $k$-Dominating set problem for planar graphs in $f(k)+c \log n$ space where $c$ is some constant. Answer to this question is yes[link] Page 11 theorem 2.4. Their proof based on the FPT algorithm for finding the $k$-Dominating set problem for planar graphs [link] page 11 theorem 2.4. Can we get the simple proof or process for finding the $k$-Dominating set problem for planar graphs in $f(k)+c \log n$ space where $c$ is some constant? 

Let $G$ be a directed acyclic graph with $V$ vertices and $E$ edges. Choose some subset of $n\leq V$ "special" vertices $\{v_i\}_{i=1}^n$. How efficiently can we preprocess $(G, \{v_i\})$ so that we can get $O(1)$ reachability queries between any two special nodes $v_i$ and $v_j$? For $n = V$, this is just the usual reachability problem. I want to focus on the case where $n \ll V$. Using $n$ depth-first searches starting at the special vertices, it is straightforward to obtain an $O(nE)$-time algorithm. Can this be beaten? What if we relax to $O(\log n)$, $O(n)$, or $O(\log V)$ query time? EDIT (May 3, 2011). One comment points out that bounds like $O(nE/V^\alpha)$, $\alpha > 0$ imply sub-cubic matrix multiplication bounds. On the other hand, a bound like $O(\max(n^2 V, E))$ for the restricted reachability problem would not imply a sub-cubic method, yet would still be quite useful for $n \ll E / V$. 

We have MPostBQP[0] = BQP, MPostBQP[1] = PostBQP, and MPostBQP := MPostBQP[2]. I'm trying to mirror the Arthur-Merlin classes where AM[0] = BPP, AM[1] = MA, and AM[2] = AM. EDIT (3/27/11 5 PM): There seems to be debate about how postselection should be defined in this context. Obviously, I mean for a definition which does not trivialize my question! :) The definition I have assumed is the following: Postselecting on the kth bit means we project the state into the subspace in which the kth bit is $0$, and normalize. It turns out that in a scheme where we postselect before we do measurements, then we can obtain the final statistics by looking at conditional probabilities in a scheme where the postselections are replaced by measurements. However, I claim that this characterization breaks down when measurements and postselections are interspersed. I think the confusion stems from people using this "conditional probability definition" (which works in the special case which I am generalizing out of) as the definition of postselection, rather than the "forced measurement" definition I just gave, which clearly depends on order because of lack of commutativity. I hope this helps! EDIT (3/27/11 9 PM): I defined postselection in the pure-state formalism already. Niel gave an analysis in the density matrix formalism that disagrees with mine for the 3-qubit example. The culprit is, again, the definition of postselection. Define postselection in the density matrix setting as follows. Given a density matrix $M$, rewrite it as a mixture of separable states $M = \sum p_i \left\vert a_i \right> \left< a_i \right\vert$. Let $\left\vert A_i \right>$ be the result of postselection (on some qubit) using the pure-state formalism I defined above. Define the result of the postselection on $M$ to be $\sum p_i \left\vert A_i \right> \left< A_i \right\vert$. This is a more sensible definition, because it doesn't give us results which say that after we post-select, we alter the statistics of events (measurements) we already watched happen. That is, the $p_i$'s are probabilities of coins we've "already flipped". It doesn't make sense to me to say we are going to go back in time and bias a coin flip that already happened because that would make the current postselection more likely. EDIT (3/28/11 1 PM): Niel concedes that with my definitions the problem makes sense and doesn't trivialize -- but with the stipulation that I shouldn't call it postselection. Given the amount of confusion, I have to agree with him. So let's call what I defined to be selection, which performs a "forced measurement". I should probably change the name of the complexity classes I defined as well (to not have "Post" in them) so let's call them QMS[k] (quantum-measure-select). 

I'm going to try to answer you question by proposing an alternate model for the question. I typically ask more questions than I answer on here, so I hope you'll be forgiving if my answer isn't optimal, although I'm doing my best. I think that the way to phrase the question that would be optimal for allowing game theory to be useful would be to assume a more competitive scenario. I.e., there needs to be competition among a variety of different actors. So, I would assume the following: 

If n is the full input and k is a single parameter of the input, then O(n^(2^n)) would also be a representation of the worst-case complexity of the algorithm. Algorithms with this complexity lie in 2-EXPTIME. On the other hand, the other expression you have described--O(f(k)n^c)--has no bounds given for the function f. Thus, O(f(k)n^c) or O(f(2^n)n^c) could have any finite complexity (assuming that f is a total function and maps to a natural number, of course). There is really no connection between the two functions you have described. If you choose to fix k to be a particular value, then the algorithm associated with the expression O(n^k) will be polynomial time. 

This is a "historical question" more than it is a research question, but was the classical reduction to order-finding in Shor's algorithm for factorization initially discovered by Peter Shor, or was it previously known? Is there a paper that describes the reduction that pre-dates Shor, or is it simply a so-called "folk result?" Or was it simply another breakthrough in the same paper? 

Here's a more or less trivial example: Consider the halting problem for Turing machines that are specifically prohibited (by the definition of the computation model) from accessing an oracle. It is undecidable relative both to no oracle and to a trivial oracle, and yet it is decidable relative to an oracle for the halting problem. (The problem itself doesn't change relative to an oracle because it can't access the oracle, but the (unrestricted) TM that decides the problem becomes more powerful given the oracle.) There are plenty of other examples, too. Just play with the computation model a little and you can find other similar results. 

Again, my question is whether or not this is a reasonable proposal. Obviously, I have no ability to cause experts to adopt what I suggest; however, I'm hoping that experts will read what I've written and decide that it's reasonable. 

Suppose that P = NP is true. Would there then be any practical application to building a quantum computer such as solving certain problems faster, or would any such improvement be irrelevant based on the fact that P = NP is true? How would you characterize the improvement in efficiency that would come about if a quantum computer could be built in a world where P = NP, as opposed to a world in which P != NP? Here's a made-up example of about what I'm looking for: 

Define the computational model MPostBQP to be identical to PostBQP except we allow polynomially many qubit measurements before the post-selection and final measurement. 

In Hardness of embedding simplicial complexes in $\mathbb{R}^d$ it is stated that $\mbox{EMBED}_{3\rightarrow3}$ is at least as hard as recognizing a 3-sphere, which is known to be in NP, but not known to be in P. They continue to say that for all we know, the problem may be undecidable. EDIT: Update. Actually, my answer applies to PL embeddings. For linear embeddings the problem is known to be in PSPACE. I don't know if anything else is known. 

I'm interested in an external memory data structure able to support the following operations on variable length binary sequences: (1) Insert such a sequence. (2) Given a query sequence, find the longest sequence previously inserted which is a prefix of the query sequence, provided one exists. In particular, do we need $O(n)$ I/O's for an operation with a sequence of length $n$? (Say, by directly implementing a radix tree in external memory.) Or is there something nice, where we can get $O(n/B)$ I/O's? 

Define MPostBQP[k] to allow multiple rounds of measuring and postselection before we make the final measurement. Choose indexing so MPostBQP[1] = PostBQP and MPostBQP[2] = MPostBQP and so on. (Update: A formal definition is given below.) Consider Arthur-Merlin games. Perhaps we can simulate them in this model of computation: Postselection can take Merlin's role of producing convincing messages and the intermediate measurements can take the role of Arthur's public coin tosses. This possibility makes me ask: 

This would be philosophically interesting (at least to me) because it would tell us that the "tractable" class of problems for a "postselecting sorcerer" includes (or is) all of PSPACE. EDIT: I've been asked for a formal definition of MPostBQP. (I have updated what follows.) MPostBQP[k] is the class of languages $L \subset \{0,1\}^*$ for which there exists a uniform family of polynomial-size quantum circuits $\{C_n\}_{n \geq 1}$ such that for all inputs $x$, the procedure below yields true with probability at least $2/3$ if $x \in L$, and with probability at most $1/3$ if $x \notin L$. The procedure, which allows for some choices which may depend on $L$ (but not $x$), is defined as follows: 

The least common ancestor problem can be used to solve the reachability problem in dynamic rooted trees, so I imagine you will also be interested in the following: Optimal Algorithms for Finding Nearest Common Ancestors in Dynamic Trees, by Alstrup and Thorup. This paper gives a time bound of $O(n + m \log \log n)$ for $n$ links and $m$ nca queries on a pointer machine. 

Put a uniform distribution on the integers $\{1,\cdots, M\}$. Draw $N$ samples, and sort them as $x_1 \leq \cdots \leq x_N$. A rank query calculates $rank(j) = \mbox{card} \{ i : x_i \leq x_j \}$ for any $1 \leq j \leq N$. What is the most space-efficient data structure that can handle rank queries in constant time (or perhaps $O(\log\log N)$ time)? I think that for $M = N$ it is possible to use an Elias-Fano representation to obtain the structure with $2N + o(N)$ bits of space. Can one exploit the uniform distribution assumption to do better? I am also very interested in the $M > N^2$ case, which is the transition where "birthday collisions" $x_i = x_j, i \neq j$ are no longer expected.