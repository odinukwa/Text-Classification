I believe the only way to guarantee your snapshots are valid against each other, is to shut both guests down, take the snapshots, then power both VMs back on again. 

This may be happening because your new account has a different SID, but the profile folder for your old account still exists. So you new account attempts to login, tried to get into the profile folder that corresponds with it's name, fails (because it's not the SID that has rights to the folder) and falls back to a temp profile. The fix for this is to grant rights to the profile to the new SID, or completely remove the old profile and allow the new account to create a fresh one in it's place. You can delete a profile on a server via My Computer -> Properties -> Advanced -> User Profiles. 

Backup Exec 2010 has just dropped, and I'm about to implement a new BEWS infrastructure, complete with CALs and new central servers. When I specced this up last year, I ignored 2010 and focused on Backup Exec 12.5, since it's a mature product. In previous experience, major released of BE had numerous technical issues and seemed to improve significantly at the first service pack. However, our refresh cycle on the backup infrastructure is slow, the main driver usually being lack of support for some new server type (in this case, ESX has driven our current upgrade need). With this in mind, I'm wondering if Backup Exec 2010 should be my first choice, as it'll last longer under current support than 12.5, which will approach EOL soon. Has anyone got any perspective they could add to this? Right now, I'm leaning towards biting the bullet and going with 2010. 

Not possible ;) If you're not using the host OS for any other purpose, you may be better off loading the server with ESXi. More efficient running just a hypervisor, rather than OS + Hypervisor. 

what PIX version? The feature you want to use is Port Redirection. The basic syntax should be: static (INTERFACE1,INTERFACE2) PROTOCOL IPADDRESS2 PORT2 IPADDRESS1 PORT1 netmask NETMASK so for example: static (INTERFACE1,INTERFACE2) tcp yourexternalip smtp 192.168.0.1 smtp netmask 255.255.255.255 And on your ACL you will need something like this: access-list outside-inbound permit tcp any host yourexternalip eq smtp (Disclaimer: I haven't touched Cisco for a while, but I'm 100% someone who has will be along shortly to confirm or correct!) 

We have far more people and systems than this in practice, but this is an accurate slice of one area where the people and systems can be partitioned off somewhat neatly. You're going to need more people to initially raise these systems from bare metal. If your systems are literally fit-and-forget and build themselves from PXE boot, then your ratios are going to be wildly different to an environment where every server is unique and you're building from DVDs. 

Most of those VLANs make sense to me. It's good to split by function so a VLAN for servers, one for phones, and another for workstations makes good sense. You can then get fine control over the traffic flowing between workstations and servers. What I don't see much point in is having VLANs for workstations on each floor. A single VLAN for all workstations would keep things nice and simple. Spanning VLANs across multiple switches/trunks probably won't be an issue for a network that small. It's also pretty pointless to maintain a seperate VLAN for switch management. They can sit happily on the server VLAN. Nothing magical about VLANs BTW... just separate broadcast network segments with each requiring a default gateway and the appropriate ACL configuration on network ports. 

These are all examples which will cost some money and effort to implement, but the cost is recouped after the changes because your environment becomes much more straightforwards to manage. Not having to dedicate as much headspace to the exceptions and gotchas of running lots of disparate kit is really good for your stress levels, too. 

Obtain the certificate for your server's full DNS name from a trusted root CA On the server open mmc.exe Add the Certificates snap-in, and choose 'Computer Account' Add the certificate into 'Personal Certificates' 

Right now your only option for remote 3D rendering is MS RemoteFX with Session Virtualization. It has to be over a LAN, and your apps must be able to use Direct3d rather than OpenGL. On the server side you need to be stacking the server with Quadro cards on the hardware compatability list (there's only 3 or 4 of them). Be aware the solution isn't particularly cheap, scalable, or mature. 

You can scale in a few different direction here depending on your budget and hardware availability. Consider these steps, probably in this order but whatever suits your situation: 

Cruft - Standardise and reduce as much of your system as you possibly can before you begin migrating. Remove all mail NSFs for users who have left the company, and clear up any instances where managers are still accessing them. Update all database designs to your current version (if you've previously upgraded Domino and still have old templates lurking). Get management to review BES usage and clear off users who don't really need it. The more non-core data and configurations you can remove before you begin, the easier your migration will be. 

Each approach has its strengths. One key thing to remember though is that a monitoring tool is only as useful as its configured to be, and there's no 'magic bullet' for this that'll give you a good blend of 'quiet enough' and 'guaranteed to alert you every time there's a genuine problem'. Unfortunately that requires continuous balancing. 

Most NAS devices (including the one you linked to) only support USB2, so there would be no advantage in doing this. You could purchase an add-in card for your PC that supports USB3: $URL$ Attaching an SSD to a NAS would be a waste of the SSD. 

You will need to contact the host of the FTP server and request that they reset the password for you. They will then issue you a replacement password. If it still does not work, you'll need to go through their support channels to troubleshoot the problem. 

Some disks die in 1 hour, others last 2 decades. If it's not failed or failing (something you can usually establish via S.M.A.R.T. monitoring or performance problems) then the only other reason to throw it out is if it's not large enough or fast enough for your purposes. 

It's important to appreciate that a transaction log holds critical data for your database. The name is somewhat misleading if you're more familiar with the type of logs used purely for human review (which can be deleted and the impact is typically zero). Transaction Logs are different. The transaction log is used in case of system error/failure, to roll back uncomitted transactions in the database. It's also critical to the backup process, to ensure your backup consists of a full, transaction-complete set of data. A backup taken while a half-completed transaction is underway is not a valid backup. Transaction log growth is linked to database activity. The higher the activity on your database, the faster the databases transaction logs will grow. So with this in mind, the long-term solution to this problem is in several parts: 

You can do this at several levels, but usually the best place to do it is on your DNS server (which is indeed usually your Domain Controller). To set this up on your DNS server: 

Microsoft Forefront Client Security uses your existing WSUS infrastructure to issue it's updates. It's an elegant solution since you probably deliver WSUS patching and AV signatures to the same network segments, and can use this to do it all through a single server, on a single port (80 or 443). By contrast, deploying Sophos updates requires clients to be able to report back to the central reporting server on a couple of ports, and to Central Install (CI) nodes via SMB ports. In firewall terms across more than a flat network, the rulebase to support this is a mess. 

Have encountered a similar problem. Windows SQL authentication always takes the credentials of the currently logged in user. There's no 'run as' type method to get around this, as far as I'm aware. So your problem basically boils down to needing some common ground on where both your server can log on, and your remote SQL box can authenticate, using the same username/password. Since they're not on a common logon domain or have any kind of trust, you may be stuck here. Back when I ran into this issue we worked around it by using SQL authentication. One dev at the time did find another way around in the code itself by changing method/library by which the app connected. 

You need to ensure that the domain object for your server is deleted from AD when you remove it from the domain. To ensure this, either use a domain administrator account when asked for credentials as you sever the machine from the domain, or manually jump onto AD Users & Computer, and delete the corresponding computer object from AD. Once you've done this, make sure to wait 15-20 mins so the change gets propogated around any local domain controllers, then re-join the computer to the domain. Also try renaming or new-sid the server before rejoining it. 

Truecrypt has a recovery disk option, which it all-but forces you to complete before encrypting the disk. That CD can be used to recover the partition even if the password has subsequently been changed. Outside of this if you're after a more robust and enterprise-ready solution, PointSec offer full-disk encryption with administrative recovery abilities. 

There are several. For proper analysis you'll want a tool that interfaces with your networking equipment, such as NetFlow Analyzer: $URL$ 

This depends on how your storage is accessed by the virtual hosts. Is it local storage in the hosts drive bays, or remote SAN storage over iSCSI/Fibre Channel? What are the IO speeds you are seeing when benchmarking the physical and virtual instances of these servers? What are the hardware specs and configuration (drives, controllers, cache, RAID levels, SAS, SCSI) of your previous physical servers, and your new vmware host servers? There are some overheads to disk throughput on VMs, but not inherently big. It's much more likely to be a shift in how the storage is being made available, contention from multiple VMs accessing the same storage, or a change/problem in the underlying disk storage hardware (e.g. lack of write cache on the vmware hosts). There's a whole bunch of different ways to improve your IO performance on vmware, but it'll depend on having more info about your current setup. 

The .local domain suffix is not an FQDN and therefore 'non-routable'. This protects your domain somewhat from passing information outside of its perimiter. For example, a user with a laptop on your internal-only acme.com domain plugs into their home network. It attempts to resolve acme.com and talk to its nearest DC. Your external acme.com is suddently batting off AD-related traffic and that traffic is flowing directly across the internet. Worst-case scenario is that you pick an internal domain name, and someone else owns that same domain name externally. Now when your users go off-site, their machine attempts to resolve and contact the domain, only to have their traffic being sent to some random company that owns the mysuperdomain.com name out in Timbuktu. There's also some complications that may arise around internal and external DNS configuration if you choose to use the same name for internal and external domains. Even using a sub-domain of your external domain name (e.g. AD.mycompany.com) can lead to issues with DNS down the line, often in granting internal users access to your resources that are also available externally. Best practice IMO is using mycompanyname.local for your internal domain, and mycompanyname.com (or such) for external. 

Transmit load-balancing is quite simple, you just enable it on the NIC and it should send traffic out through both ports. However receive load-balancing is a bit more involved. In short: 

The mechanisms for doing this are now built into the bittorrent protocol: $URL$ However as mentioned in the article, to date no specs have been released to allow support. 

If I understand correctly from your edit, your users have a shortcut on the desktop that points to the file like \server\share\document.doc and are double-clicking it to open the file. When you're in the office app, you're going file -> Open and manually navigating to \server\share\document.doc to open it. If this is the case, try setting the users up with a mapped drive (e.g. Z:) pointing to \server\share, and re-point their desktop shortcut so it accesses the file as Z:\document.doc A mapped drive should hold onto network share credentials/authentication better than manually hitting the UNC share path (\) when you want to open a file. In the case of office/word, I've experienced odd slowdowns when accessing docs directly via their UNC path. If it's occuring when accessed directly through the mapped drive, too... then you could check into this: $URL$ Some additional troubleshooting steps you could try: $URL$ 

You can work around the 'all explorer windows are slow' problem by enabling 'Launch folder windows in a separate process' option under Tools -> Folder Options. Take a look at the TCP offload settings on both server and clients, as I vaguely remember some issues with SMB browsing if ToE is enabled on the server-side with particular NICs. 

My take, bearing in mind that I can't know all the facts about your particular situation: If you have an imminent physical move ahead of you, then you absolutely do not want to be migrating your systems to different platforms on an immutable deadline of having to be out of the premises. For one, stuff is more likely to go badly wrong under duress and two, if problems occur and timescales slip (and when has that ever not happened?) you'll end up moving halfway through your migration, and still requiring data center space for your kit. Sounds like an expensive recipe for disaster rather than the elegant cost-and-hassle saving panacea the cloud is supposed to provide. No, definitely do not mash 2 projects into one like this if you can possibly avoid it. Cloud hosting (in the sense I suspect your boss is envisaging it) is great and has its place, it may be a good fit for what you do. But right now I think you have enough on your plate with needing to move your infrastructure as-is, so I would push back on your boss with the clear message that you can't just scrap all the hardware and systems at the drop of a hat, but that moving to cloud hosting for at least parts of your infrastructure may indeed be feasible as your next big project, after the dust has settled from the premises move. On the subject of 'The Cloud' itself. You're already running a local/internal cloud solution - Your VMWare/SAN environment. If you move that off to a hosted datacenter where you just rent a rack and continue to manage your own kit, you've now got a remotely served, private cloud solution. Many non-technical bosses live off buzzwords and cloud computing is no exception (in fact at the moment it's probably the biggest offender in the 'bullshit bingo' category). You can turn this to your advantage and steer things in a more sensible direction by tying the things you already do (running a virtual environment) to your bosses idea of the cloud. It can be as simple as referring to your VMs as 'Instances' and ESX hosts as 'Nodes' whenever you're reporting on it.