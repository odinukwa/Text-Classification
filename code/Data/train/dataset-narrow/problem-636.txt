When setting MAX_IOPS_PER_VOLUME in a Resource Pool, what exactly does "volume" mean? Specifically, how many "volumes" would be the following cases: 

Our SQL 2014 server has a blazing-fast tempdb drive (2800 IOPS) but a much slower data drive (500 IOPS). Our application runs a few long-running reporting queries that are I/O-intensive and we'd like to avoid them starving our server for I/O capacity when they run. Ideally we'd be able to limit these queries to 50% of available I/O capacity. Unfortunately SQL Server Resource Pool's IOPS throttling is not percentage-baed nor volume-specific. If I limit to 250 IOPS, then it will unnecessarily slow down performance of queries that make heavy demands on tempdb. Slowing down these long-running queries if the server is busy is OK, but slowing them down by 10x+ if they need lots of tempdb access is not OK. So we're looking for workarounds that will defend other queries from these lower-priority, long-running queries, but without unnecessarily hurting performance of these long-running queries if they happen to use lots of tempdb. It's not practical to change the queries themselves to reduce tempDB usage-- these queries are generated by a custom reporting feature that may sometimes generate really complex query plans that spill results to tempdb. So far the best idea I have is to remove IOPS throttling and instead use the "Importance" of a workload group to defend the rest of the server's I/O capacity from these queries. Is this a good solution to the problem I'm trying to solve? What are the pros and cons of using Importance? Or is there a better way to achieve our goals? 

(Or create a procedure to run that) NOTE: this will NOT work for any sysadmin account as their current_user is always 'dbo' EDIT: Warning, view server state will grant permission to view anyone who is connected, not just people with the current user name so you may want to check around what potential additional security you wish to put in place for that if need be 

Then you can add permissions to that user as it will exist in that database. NOTE: you may want to go into your master database (or whichever database you were running those commands in before) and remove USER1 and USER2 

I cant speak for your compatibility, but general performance and security and longevity is always better with the latest version. (you'll most likely get another 2 years of support for a version 2 years more recent) The best thing I can advise is getting a dev licence (which by comparison cost next to nothing) for 2016 and use that to test your system to see if there are any issues and how easy they are to fix beyond the general these things shouldn't be used any more. There's also the question of time, since you've done a full sweep of 2014 it will be far quicker now to upgrade to that, but there's also you should now know all of the tests that need to be performed, so a second compatability test with a new system should go much smoother (and you've already got the fixed up to 2014 bypassing 2012 which had a lot of changes) so the number of required changes from your prepared system and what you need will be (in theroy) substantially less than the set you have already done. EDIT: SQL Developer edition can now be used for no cost with a visual studio subscription or visual studio dev essentials $URL$ 

Does this same fill strategy apply to tempdb? And does this answer depend on the type of query, e.g. parallel vs. non-parallel? Or is the answer different based on the kind of tempdb I/O, e.g. for temp ables I create vs. tempdb usage by the database engine for worktables or spilling? 

What are the pros and cons of this approach? What can go wrong? Is there a better way to reduce storage & I/O without causing problems with overflow and requiring existing readers to be rewritten? 

Locally attached disk that's split into two partitions E: and F: Software RAID 1 set E: composed of 2 locally attached disks (yes I know software RAID is bad-- adding this case to help me understand SQL Server's definition of "volume", not to design a production setup!) Hardware RAID 1 set E: composed of 2 locally-attached disks SAN disk E: on who knows/who cares how many disks. 1 SQL Server filegroup spread across two locally attached disks E: and F: 

I'm looking to automate a wipe of our test environment to bring it up to be in-line with production at the end of every sprint. As it stands this is currently being completed by the test enviroment grabbing the full backup and doing a restore with move&replace however this is using up most of the space we have. The intention is to create the environment from scratch and populate only the needed tables. I can use the 'Right Click > Tasks > Generate Scripts...' to create the framework of the database. Is there a way to create that script from within a stored procedure so that can be used to re-create the database Also I'm working on this bit as it stands but obviously that script will just create the database at its current size so all of the File Sizes need to be modified once the script has been generated Cheers for any help 

But it doesn't say how SQL Server determines what is a "large table" and "small table" for purposes of this optimization. Are these criteria documented anywhere? Is it a simple threshold (e.g. "small table" must be under 10,000 rows), a percentage (e.g. "small table" must be <5% of rows in the "large table"), or some more complicated function? Also, is there a trace flag or query hint that forces use of this optimization for a particular join? Finally, does this optimization have a name that I can use for further Googling? I'm asking because I want this "use the cardinality of the large table" cardinality estimation behavior in a join of master/detail tables, but my "small table" (master) is 1M rows and my "big table" (detail) is 22M rows. So I'm trying to learn more about this optimization to see if I can adjust my queries to force use of it. 

to generate your static random generator start point the $usersrandomnumber acts as a starting seed see the mysql documentation here: $URL$ it will involve you storing the user's seed in a table somewhere, or cookie /local/server storage but will get you what you're after 

Note, are you wanting those default values in there? would make more sense (in my opinion) to have it as a required field on the execution of the proc so you don't accidentally run it with no values and presume its right 

SQL Server is designed (along with most db engines) with security in mind the main areas you'll look at for viewing users are 

TLDR; Check for conversations being left open completely. In our system we re-use conversations and have a table dedicated to holding these conversations that are usable, however the dev team setup a new service broker without my knowledge ages ago while I was off, didn't set up these conversation points and didn't set any thresholds on the alerting. When the new system has been turned on the conversations are being opened but not closed properly and since there aren't any in the pool it is just creating a new conversation (we reached 7.1 million conversations for one service broker) My steps for fixing was to create and record the 20 conversation handlers that I require for that service broker and record them into our table. This stopped the growth of the tempDB to stop the risk of the DB going down. Then came the long process of closing off all the un-used conversations 

When a detail table contains denormalized data, should denormalized columns be included in foreign key relationships between the master table and detail table? Here's more details: We have a master/detail pair of fact tables: an table with about 1M rows and an table with about 20M rows. To improve reporting performance for date-range queries we've partially denormalized by adding to the and creating a covering index on with the other columns INCLUDEd. There's already a foreign key relationship between the column in both tables. But SQL Server is unaware that the in both tables is the same if the is the same. Should I help SQL Server to know about the relationship? If so, how? Finally, will adding denormalized columns to foreign keys improve cardinality estimates when joining the master/detail pages by telling SQL Server that cardinality shouldn't be reduced when filtering both tables by the same a date range? If not, then what's the benefit of maintaining this foreign key relationship that includes the denormalized column? We're running SQL Server 2014 an are soon upgrading to SQL 2017, if that matters to the answer. 

The method of insert doesn't appear to be the problem, more the data itself The error you are receiving is because one of the columns you have in your database table is too short (see your column 'wikipedia_link') see what length it is and if you can increase it, note it is quite possible to have more errors on other columns that are similar, judging by the data contained in that field the longest field is 128 characters, so I'd make the field nvarchar(130) as a minimum If you load the csv in excel you can find out the max length of a column by using {=MAX(LEN(Q:Q))} (NOTE to get the {} array calculation after typing the query press ctrl + shift + enter) Check all your fields are long enough and try again 

To answer question 2 first 2 A primary Key is NOT always the clustered index, it can be the clustered index and in the majority of cases is the way things are done, but it isn't always the best for your data. The culstered index is the order in which your data is physically stored on the disk whereas the primary key (which can be composite) designates the field to be a unique field and is beneficial for not inserting duplicate values and for foreign key lookups if you wish to do joins (In summary for 2, yes you can add a primary key without a clustered index) NOTE: sometimes if you want an identifiable row you can add a new field to the table to simply act as the primary key (not always advisable, but can sometimes be the a solution to improve performance) 1 Adding a primary key can change performance, however the only true way to know if it's going to improve performance is to test it, if you have a pre-live environment consider adding it to there and running your queries across it. If you have queries that run as joins to this table on ID,AID,BID all together (Sorry I'm not 100% sure how these are all coming together) the Potentially create a composite primary key across all three which means when anything wishes to get data with this table comparing all three of those it can find that row with ease. (Hope this makes sense) 3 Adding a Clustered Index completely depends on your data, once again a pre-live environment would be an ideal situation for testing. A few things to consider when creating a clustered index, what data are you retrieving and what are you inserting (this is a general example)If you're inserting and retrieving data that is the most recent data then a clustered index on the date field sounds the best idea, however if there is a LOT of data going in and out you will have very high contention on the most recent pages in your table, an alternative would be to have the clustered index around a category that those dates are on, eg client, this would mean that the data is grouped by a client which is more likely to have data gathered by, and spreads the read write load across the disk / disks If the data retrieve is very random then a clustered index is quite pointless, if the data you get back has no real order to it then a Heap is completely acceptable. Ultimately there is no be all answer to should I add a clustered index or a primary key because every situation is slightly different and will react in different ways. If you have a pre-live environment (even a cut down version) can help make your decisions. Personally we have tables with primary clustered composite keys, and some tables that are simply heaps. Hope this helps (and makes sense, sometimes I find I ramble) 

I assume the answers to #3 and #4 are "1 volume" and #5 is "2 volumes" but it's #1 and #2 that I'm most curious about. The specific reason I'm asking is wondering if it's possible to increase the Resource Governor's IOPS limit for locally-attached SSD tempdb while having a lower limit for our SAN data storage. So I'm wondering if splitting a single physical disk into multiple partitions might be a way to do this, by putting separate tempdb files on each partition so the total tempdb If #1 above makes SQL Server treat one physical disks as multiple volumes for throttling purposes, this may be an option. I'm assuming that this won't work-- that SQL Server is smart enough to know that 2 partitions is one "volume". But was worth asking. 

Will tempdb I/O from a single query be split across multiple tempdb files? (assuming that tempdb is configured to use multiple files, of course!) For non-tempdb databases, MDSN seems to say that yes, newly-added data will be spread across multiple files in a filegroup: 

Internally within a stored procedure there is no way (that I'm aware of) to determine the calling proc. There are two workarounds that I use 1. This one wont actually be identifiable for a profiler run but can be useful in auditing. When calling any SP call it with (into a value called ), this will mean that it will hold the stored procedure ID (which can get you the schema and name) of whatever stored procedure was first called, by anything (be it a person or automated call (note this can technically be spoofed by someone manually adding in the to the initial call) This is quite a bit of work depending on how many things you want to audit. and generates overheads in your db but will get you exactly what your after just in. 2. The easiest way is each of the different routes into your system has its own login, so you have a login for updating users credentials, a different login for another action. This once again can be a lot of work, but will get you the information you want directly from the profiler, it also will make your system a lot more secure if the logins are stored securely as if one login is compromised then there is only that very specific are is effected someone doesn't have access to your entire system Aditionally depending on how it is being called there is the potential (if you're using php this isnt possible if you have a more complex back end with iss / .net in the background you may be able to set the program_name field for the login, once again this will show up in the profiler and if you have the proc name your calling you can amend that into the program name ( for example) Hope this somewhat helps Ste