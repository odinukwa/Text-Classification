Before considering performance you need to think about normalization. If you can interchangeably concatenate or not concatenate Group A and Group B then it sounds like B functionally depends on A and that means your table is not fully normalized. One example of a problem this could cause is if you don't currently have any products that fit into a given group and sub-group then you have lost the information that those groups and sub-groups even exist. If for example you delete the rows for A2-B2-1 and A2-B2-99 you have lost the information that A2-B2 is a group subgroup. You need to go back to understand the business rules here and if this is a possible use case you need to implement tables for product groups. This analysis must be done regardless of how you decide to cluster. Once you have completed normalization and if you still have this table structure (say you now have a product group, product sub group, and a product group sub group table which parents your Group-Sub Group-Product table), you regardless NEVER want to concatenate columns into a single column. This concatenation breaks 1NF as now you have 2 different date element concepts in the same column. You would be unable to query for all products in a given sub group across groups. From a SQL Server clustering perspective, if this table will always be queried by product group/sub group, and possibly product id, and that is it, you will want to create a clustered index on all 3 columns. That will best support your query as data in the table will always be ordered by the 3 columns and you can seek right to the group/sub group/product you want. If you make random value inserts into the table over time however you will need to tune the freespace you allow in the index as well as periodically reorganize/rebuild the index over time to maintain clustering order. 

In the scenario of customer / employee vs. person / customer / employee the decision as to which to use is based on the subjective perceptions of the business domain. There is no scientific basis to choose one approach over the other. Describing the inheritance approach as "makes sense" the subjective nature is exposed. It makes sense to you, and perhaps to me, but perhaps not to someone else. This is an example of creating a business model at the conceptual level and describing entity types. Either approach can be mapped to R-tables at the logical level which are fully normalized. This is why you cannot find the specific functional dependency which justifies the change - there isn't one. A good primer on normalization is Fabian Pascal's Practical Database Foundation Series, specifically paper number one which describes business modeling and paper number two which describes normalization. Chris Date's Database Design and Relational Theory provides an in-depth look at normalization along with orthogonality with respect to reducing redundancy in logical database designs. 

Your design seems reasonable to me. While you do have to update all subsequent records when new processes are added or deleted that is easy to accomplish. You just issue an update like: 

Not only could it be, it must be if we want to ensure the data stored in the database remains consistent with the rules we have identified in the real world! 

Read Steven Levy's "In The Plex: How Google Thinks, Works, and Shapes Our Lives". This book is a fascinating read about all things Google and does discuss at a high level some of the technology and engineering behind search. Aaron sums it up really well in his answer and Levy's book will give you some more detail about how they do it. 

Employee manages Department - Start Date Department controls Project - Employee assigned to Department - Employee works on Project - Hours Per Week Employee has Dependent - Department has Location - 

I would create a distinct emails table and use a surrogate key to instantiate the reference of the email address to the person and to the user. The model would look like this: 

I have never used the relationships tool for anything other than understanding the relationships between base tables on the back end database. I could see however where it would be very useful on the front end if you have many users of your application who write a lot of ad-hoc queries and you provide them queries stored on the front end database which they can use as an abstraction layer. On the other hand, if the users only interact with the database via forms I would think the work to add the queries to the relationship tool and connect them up would not be worth it. 

We have a number of tables (~1M records) that have a column on them defined as: that gets auto-populated with . We use this ID for synchronizing data across multiple systems, databases, file imports/exports, etc. For this column, we have the following index created: 

We're working on migrating our database (~400 tables) from SQL 2008 R2 to SQL Azure. We're currently in the proof-of-concept stage, figuring out the process we'll be following. We have a process that we think is solid but I'd like to perform some validation diffs on both schema and data to confirm that we have a successful migration. I've never done this before with SQL Azure and am looking for a good way to do this. How can I perform this verification effort on both the schema and data? Ultimately, this is a one-time migration (we'll do it a few times but the real migration will only be done once). 

It only tuned ~9k out of ~530k queries It recommended I drop a ton of indexes (in fact, most of them) It recommended I create 0 indexes 

How can I mix boolean logic with bitwise and/or operators in SQL Server, specifically SQL Azure Database and/or SQL Server 2016? To demonstrate what I'm trying to do, consider the following script: 

We have this complicated query that I'm trying to make "better" until we move it to pull from a data warehouse. I need a solution that's "good enough" for now and I think I'm about 2-3 indexes away from making that happen. I'm stuck on this part, however. I'm specifically targeting this part of my Execution Plan: 

My background: I'm a dev/architect, not a DBA. Sorry! So we have a ~400 table 75GB database. I ran Profiler for ~24 hours ("Tuning" template minus ) and have ~7GB of usage recorded in a trace file. I then ran the Database Engine Tuning Advisor on this trace file (no partitioning, keep clustered indexes, PDS Recommend: Indexes) against our production database (after hours). I gave it ~4 hours to analyze. And here are the summary results I got: 

I'm working with each of the 3 portions of the UNION ALL independent of one another and the other two parts are nice and speedy and executing this third of the unions either by itself or in the union performs similarly (i.e. ~30 seconds). So the UNION isn't a factor but I included it just for thoroughness sake. 

Redgate SQL Compare appears to do this decently well. I'm still nailing down the settings to ignore certain thing (like ) but at a glance, this seems like the tool I want. 

Does this sound right to you guys? I expected it to drop most of my indexes but then to create a ton of new indexes. Also, if it takes 4 hours to analyze 9k queries, is it even feasible for me to get this to consider a normal day's worth of usage? Compared to most large databases, ours is fairly light on consumption (~50 users total). I think I'm either misunderstanding something or am simply doing something wrong. 

However, even after adding this index, it seems the query is still doing the Key Lookup. Am I missing something obvious here or is this the right idea and I just have a problem elsewhere? Note that all statistics and indexes have been refreshed and this isn't THAT highly dynamic of a table but it is approaching ~1M records. A simplified version of this query, focusing on this table of interest, is as follows. Nothing I removed references the PrimaryTableOfInterest. 

(source) Another piece of info I've discovered is that both BlitzIndex and this script are worthless after a few events happening: 

The BlitzIndex tool that @JMarx suggested is working great! However, I'm also finding this additional script to make some good suggestions as well. Not necessarily using all or even most of its suggestions, but cherry-picking from the top is proving very useful! 

The script works fine as-is but the moment you uncomment out that second , all hell breaks loose. I understand this is invalid syntax because I'm mixing boolean logic with bitwise operators. But what is the right way to get this logic in there? Without going crazy with statements, is there some way to do this? 

That table originally had only 2 indexes: the Clustered PK index (that this shows it doing the Key Lookup on) and another FK index on a column not referenced here. Given that this heavy query always needs these add'l columns (DateForValue [datetime], CurveValue [float], BTUFactor [float], and FuelShrink [float]), I thought a covering index was the obvious solution here to remove the (slow) Key Lookup being performed here. So I added the following covering index: 

The short answer here is none. Normalization, more formally called Projection-Join Normalization, is a scientific process in which one can remove redundancies in R-tables due specifically to join dependencies which are not implied by the candidate keys. The join dependencies are exploited by taking projections based on them to create two or more tables from the original table which removes the redundancy. It is important to note that normalization cannot remove all redundancy. Instead, it can remove only redundancies caused by join dependencies not being implied by the primary key. 

Based on what I have seen and read the ordering of columns in SQL Server makes no difference. The storage engine places columns on the row irrespective of how they are specified in the CREATE TABLE statement. That being said, I'm sure there are some very isolated edge cases where it does matter but I think you will have a hard time getting a single definitive answer on these. Paul Randal's "Inside The Storage Engine" blog category of posts is the best source for all the details on how the storage engine works that I am aware of. I think you would have to study all the various ways in which the storage works and matrix that against all of the use cases to find the edge cases where order would matter. Unless a specific edge case is pointed out that applies to my situation I just order the columns logically on my CREATE TABLE. I hope this helps. 

Fabian Pascal recently blogged on the definition of first normal form. Also, his Practical Database Foundation Series includes a treatment of normalization and 1NF. If one purchases the entire series two additional papers are included that specifically address what is and what is not 1NF, the former paper written by CJ Date. I will make an attempt to summarize the key points with regard to your question. For a table to be in 1NF, and thus be a relational table (R-table), certain rules must be followed in its design and population to ensure the table can acquire the properties of a mathematical relation. The rules summarized are: 

Another benefit of the relational approach is that the performance of the system can be managed to a great extent at the physical level without impacting the logical database schema used by applications to work with the data. With the relational approach you can focus on the correct logical schema first and then in most cases let the DBA implement a correct physical design and infrastructure to best support the read heavy workload. This is not the case in navigational systems where the programmer must specify the access paths directly in programs. 

When Codd defined the relational model he defined a set of operators which could be applied to relations. In specifying a relational algebra, much like specification of an integer algebra, we are able to use symbols in place of relations to solve queries. These operators are subject to the same algebraic properties that integer algebra operators (+, -, *, /) are. As a result, we can assume certain laws that always apply to a relation, any relation, undergoing that operation. For example, in integer algebra we know that addition and multiplication are associative in that we can change the grouping of operands and not change the result: 

A surrogate key is a system assigned unique value to identify an entity occurrence. A natural key is what the business uses to identify an entity occurrence. The source systems, as well as your BI/Data Integration database, can use either type to identify the entity occurrence - such as Jim Brown in your example. In the source system we call what the source system uses to identify the entity occurrence a source key. So if you can have 3 different source systems each of which contain Jim Brown, each will have a different source key in addition to the natural key - which you have identified as the SSN + birthdate. The BI staging environment, which the ETL uses, will include a key map table which will map each source key to the assigned surrogate key for the BI database. So for example: 

Great question! Here is an approach using super/sub-typing in the barker-ellis notation style. In this approach super and sub-types are used only for classifying based upon a single, un-changing, fundamental characteristic possessed by all the entities of a given entity type. This is similar to the taxonomy of biological organisms. Using this style, we only sub-type based on who you are, not on the role you play.