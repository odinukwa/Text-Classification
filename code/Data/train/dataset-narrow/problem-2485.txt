You asked whether there exists a finite set of moves which if applied repeatedly from any starting position, yield a solved Rubik's cube. Then you asked a harder question. The former question has a simple solution. Enumerate the group of permutations of the Rubik cube: $G = \{g_1,\ldots,g_{|G|}\}$. Every $g_i$ can be realized as some sequence of moves which we will identify with $g_i$. The following sequence answers your question: $$ g_1, g_1^{-1}, g_2, g_2^{-1}, \ldots, g_{|G|}. $$ 

More generally, you can consider the identity $$ f(g(x)) = g^{(n)}(x), $$ which generalizes your identities, in which $n=3,1,2$ (respectively). For a given function $g$, this identity states that $f$ needs to have given values on $\operatorname{ran} g$, and is free otherwise. The specific value of $n$ doesn't matter, in fact we can put whatever we want on the right-hand side as long as it doesn't involve $f$. Using inclusion-exclusion, you can come up with an explicit formula for the number of solutions, but I'll leave that to you. 

Using the alias method you can make do with $\lceil \log_2 (k+1) \rceil + 2$ bits on average. In preprocessing you prepare a table of length $k+1$ in which each cell is partitioned into two parts by some threshold $\theta_i \in [0,1]$, and the "lower" and "upper" parts are labelled by outcomes. To sample, you first choose a uniformly random cell $i$, and then you sample (mentally) a uniformly random real in $[0,1]$, and compare it to the threshold, choosing either the lower label or the upper label accordingly. If you sample the real bit by bit, then at every point you have a chance of 1/2 to have determined whether you're above or below the threshold, so the total number of bits used is $\lceil \log_2 (k+1) \rceil + G(1/2)$. 

Section 1 of the paper, in which the first definition appears, is introductory. The formal development starts at Section 2. The part of the paper starting at Section 2 is completely self-contained. Therefore, if there is any mismatch between a definition in Section 1 and a definition elsewhere, the latter is the correct one. In theoretical computer science, papers are published in conferences. Conference reviewers cannot be expected to read 50-page strong manuscripts, and so the argument has to be described in an elaborate introduction. This is why there is so much technical content in Section 1. The development there is simplified and more leisurely to facilitate reading. The authoritative part of the paper is Section 2 and forward. 

Regarding the choice of basis: all finite bases are (polynomially) equivalent. That means that if $C$ is a circuit over a finite base $B$ and $B'$ is any complete basis, then there is an equivalent circuit $C'$ over $B'$ of size at most $\alpha_1 |C|^{\alpha_2}$, where $\alpha_1,\alpha_2$ depend only on $B,B'$. This is proved for example in Reckhow's thesis (Section 5.3), though the result itself is due to Spira. What this means informally is that when we're doing circuit complexity, we don't really care whether the basis is NAND or AND, OR, NOT. (That's because in circuit complexity we often don't care about polynomial factors.) NAND was chosen because that's what can be implemented in hardware. AND, OR, NOT were chosen for historical reasons: that's what was used in logic (along with similar bases such as NOT, IMPLIES for which $\alpha_2 = 1$ both ways). Circuits with unlimited arity PARITY gates don't fall under this formalism, since the basis is infinite. Regarding the importance of Håstad's result: This was the first non-trivial bound in circuit complexity. It is still one of the best bounds known for non-monotone models, along with the Razborov–Smolensky methods, and Razborov's slightly superlinear lower bound for switching networks. All other bounds rely on restrictions of the model, usually monotonicity. Parity is computed in real life in an iterative fashion, using tables or simply by "deep" circuits (this concept is of course hard to define exactly, since constant depth is an asymptotic notion). That's a different model. Circuits considered in circuit complexity are good enough for the P vs. NP question (or rather, P/poly vs. NP). At the time, it seems that people thought that the methods developed for small complexity classes would scale up to polysize circuits, but this hope hasn't materialized. Nowadays it is a less presumptuous specialist area. 

You don't need a full look-up table to achieve optimal compressibility. I believe that modern computers using a very reasonable look-up table are able to count the number of constrained Sudokus, which are Sudokus with some digits already in place. Using this, here's how you encode (decoding is similar). Fix an ordering of the squares. Suppose the number on the first square is $d_1$. Put $N_1$ to be the number of Sudokus whose first square is less than $d_1$. Let now $d_2$ be the number of the second square. Put $N_2$ to be the number of Sudokus whose first square is $d_1$ and whose second square is less than $d_2$. And so on. The encoded number is $N = \sum_i N_i$. This method of encoding is known as binomial encoding in the literature. It should enable you to effectively (in a real-world sense) calculate the index of any given Sudoku, and vice versa. You will then require only $72.4$ bits, as alluded to above (this means that you could code several of them with that average number of bits). Edit: The Wikipedia page on the mathematics of Sudoku helps us clarify the picture. Also helpful is a table compiled by Ed Russell. It turns out that if you consider only the top three rows, then there are essentially only 44 different configurations to consider. In the table, you can find the total number of configurations equivalent to any given one (assuming that the top row is 123456789), and the total number of completions of each one. Given a Sudoku, here is how we would compute its ordinal number: 

No, the bombe was very specific. It consisted of a bunch of enigma machines hooked together. It was very limited in its use. A more interesting question is whether the Colossus computer, also used in Bletchely Park, was Turing-complete. When asking such a question, it should be understood that no physical computer is Turing-complete, since it cannot handle arbitrarily large inputs. But even if we abstract this issue away (in any reasonable way), bombes are not Turing-complete. It could be fun to come up with an appropriate model and determine the exact complexity class covered by bombes. 

Here is a different proof, adapted from the monograph The semicircle law, free random variables and entropy. Let $X_i$ be an infinite sequence of i.i.d. variables with distribution $\Pr[X_i = 1] = \Pr[X_i = -1] = 1/2$, so that $ Y_n :=\frac{X_1+\cdots+X_n}{\sqrt{n}} $ converges in distribution to a standard Gaussian. We will show how to compute the limit of $\newcommand{\EE}{\mathbb{E}} \EE[Y_n^d]$ (for even $d$), which is the same as the $d$th moment of a standard Gaussian. If we open $Y_n^d$ up and use the exchangeability of the $X_i$, we see that $$ \begin{align*} \EE[Y_n^d] &= \sum_{t=1}^d \sum_{\substack{\lambda \vdash n\colon \\ \text{$\lambda$ has $t$ parts}}} \binom{d}{\lambda_1,\ldots,\lambda_t} n(n-1)\cdots(n-t+1) \frac{\EE[X_1^{\lambda_1} \cdots X_t^{\lambda_t}]}{n^{d/2}} \\ &\sim \sum_{t=1}^d \sum_{\substack{\lambda \vdash n\colon \\ \text{$\lambda$ has $t$ parts}}} \binom{d}{\lambda_1,\ldots,\lambda_t} n^{t-d/2} \EE[X_1^{\lambda_1} \cdots X_t^{\lambda_t}] \\ &= \sum_{t=1}^d \sum_{\substack{\lambda \vdash n\colon \\ \text{$\lambda$ has $t$ parts}, \\ \text{all $\lambda_i$ are even}}} \binom{d}{\lambda_1,\ldots,\lambda_t} n^{t-d/2}. \end{align*} $$ Since all $\lambda_i$ are even, necessarily $t \leq d/2$. When we take the limit $n\to\infty$, only terms with $t = d/2$ survive. There is in fact only one such term, the partition $\underbrace{2,\ldots,2}_{\text{$d/2$ times}}$. Therefore $$ \EE[N(0,1)^d] = \lim_{n\to\infty} \EE[Y_n^d] = \binom{d}{\underbrace{2,\ldots,2}_{\text{$d/2$ times}}}. $$ But $\binom{d}{\underbrace{2,\ldots,2}_{\text{$d/2$ times}}}$ is the number of ways to pair up the elements of $\{1,\ldots,d\}$, that is, the number of perfect matchings in $K_d$. In the case of free probability, similar arguments involving non-crossing partitions are encountered in the computation of the moments of the semicircle law. 

If $n > \binom{d}{d/2} \approx \frac{2^d}{\sqrt{\pi d/2}}$ then we know that the set is not an antichain by Sperner's lemma, and so the decision version of the problem becomes trivial. But it might be interesting to consider the case where $n$ is close to that value. Friedgut's work on the Erdős-Ko-Rado theorem shows that given the characteristic vector $f$ of a family of subsets of $[m]$, one can find in time $O(m2^m)$ whether $f$ is an intersecting family (every two elements of $f$ intersect). More generally, his method allows us to compute $$ \Sigma = \sum_{x,y \in f} S(x,y), $$ where $S(x,y) \geq 0$ is some (specific) known function which is non-zero only if $x,y$ are disjoint. $S(x,y)$ depends only on the histogram of $\{(x_i,y_i) : i \in [d]\}$, where $x_i$ is the indicator for $i \in x$. (As an aside, we comment that his method also works if we are given two families $f,g$, and are interested in $\Sigma = \sum_{x\in f, y\in g} S(x,y)$. In both cases, we need to compute the $p$-skewed Fourier-Walsh transforms of $f,g$ for an arbitrary $p \in (0,1/2)$, and then $\Sigma = \sum_x T(x) \hat{f}(x) \hat{g}(x)$, where $T(x)$ depends only on the Hamming weight of $x$.) How does all this relate to the problem at hand? Consider the family $$ F = \{ S_i \cup \{x\} : i \in [n] \} \cup \{ \overline{S_i} \cup \{y\} : i \in [n] \}. $$ Every $S_i \cup \{x\}$ is disjoint from every $\overline{S_i} \cup \{y\}$. Since $S(x,y)$ is given explicitly, we can compute the contribution of these pairs to $\Sigma$. Are there any more disjoint pairs? If $S_i \cup \{x\}$ is disjoint from $\overline{S_j} \cup \{y\}$ then $S_i \cap \overline{S_j} = \emptyset$ and so $S_i \subseteq S_j$. So $S_1,\ldots,S_n$ is an antichain iff $$ \Sigma = \sum_{i=1}^n S(S_i \cup \{x\}, \overline{S_i} \cup \{y\}). $$ This algorithm runs in time $\tilde{O}(n + 2^d)$, ignoring factors polynomial in $d$. When $n$ is close to $2^d$, this is significantly better than $\tilde{O}(n^2)$. In general, we get an improvement as long as $n = \omega(2^{d/2})$. Given that we know that a pair satisfying $S_i \subseteq S_j$ exists, how do we find it? Suppose we divide all sets $S_1,\ldots,S_n$ into two groups $G_1,G_2$ at random. With probability roughly $1/2$, the sets $S_i$ and $S_j$ will find themselves in the same group. If we are so lucky, we can run our algorithm on $G_1$ and $G_2$, find in which one do these belong to, and so halve the number of sets we need to consider. If not, we can try again. This shows that with an expected number of $O(\log n)$ oracle calls to the decision version, we can actually find a pair satisfying $S_i \subseteq S_j$. We can also derandomize the algorithm. Without loss of generality, suppose $n = 2^k$. In each step, we partition according to the each of the $k$ bits. One of these partitions will always put $x$ and $y$ in the same part, unless they have opposite polarities; we can test for this explicitly using only $O(nd)$ operations. This gives a deterministic algorithm using $O(\log^2 n)$ oracle calls to the decision version. 

Given $n$, let $M = 2n\log n$ (here $2$ is any constant larger than $1$). Put all numbers from $2$ to $M$ in a linked list. Now implement Eratosthenes sieve, by repeatedly running the following algorithm, until you get $n$ primes. Look for the first unmarked number $p$ (they all start unmarked; start looking at the previous prime), take it as a prime, and mark every integer in hops of length $p$. The running time is roughly $M + \sum_p M/p = \Theta(M\log\log M) = \Theta(n \log n \log\log n)$, where $p$ goes over the first $n$ primes. Since the $k$th prime $p_k$ is about $k\log k$, heuristically indeed $$ \sum_{k=1}^n \frac{1}{p_k} \approx \sum_{k=2}^n \frac{1}{k\log k} \approx \int_2^n \frac{dk}{k\log k} \approx \log\log n.$$ This approximation can be proved rigorously, this is one of the results known as Mertens' theorem. See Emil's comment below or the Wikipedia article for improvements on this algorithm. Edit: Before Emil's comment, I suggested implementing the algorithm using a linked list, but that's a bad suggestion since you can't "hop" in a linked list, causing the runtime to be $\Omega(n^2)$.