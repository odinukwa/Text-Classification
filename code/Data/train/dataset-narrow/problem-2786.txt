The simplest and most effective way to counter 'farming' of scores is to adjust the score received based on the disparity between the players. Bear in mind it's not just one player creating multiple accounts you need to worry about: imagine two friends, one deliberately losing so the others ranking is artificially boosted. Chess and other games avoid this through forms of the Elo rating system. Basically the reward for beating someone of a higher rating than you is high, but the reward for beating someone of a much lower rating is very low. This very quickly makes deliberately losing to advantage another player ineffective. You can tune the rate at which winning against a poor player quickly results in little net gain. A player playing 'properly', i.e. winning against a wide range of similarly ranked opponents, will advance very quickly. NB: it's not enough to just penalise players who lose so that winning against them gets you nothing, because then players can create an account, lose with it deliberately, then create another account. This isn't so much of an issue if there is a barrier to entry (e.g. entering credit card details to create a new account), but for free-to-play games that would work. Also its worth noting that there is a danger if the ratings are used as a measure of actual ability, rather than just as a way to modify any rewards the player receives. If the rating itself is the measure, canny players will use the system to sabotage others. I.e. Player 1 gains a lot of skill/ability through practice, and then creates a new account. Player 2, who is about as good as Player 1, accepts a challenge from the newcomer. Losing to the newcomer will massively hurt their ranking, even though really the new player is of equivalent rank. However if the rating isn't actually used to measure the player, this is actually in Player 2's interest (having their rating pulled down will allow them to advance faster given their actual skill level). 

So for managing 60Hz rendering, the granularity of the time-slicing / Sleep function makes this a not great solution. And it also doesn't take into account the actual refresh rate of the system! 

And let's lay out a timeline in milliseconds, saying rendering takes 3ms to complete, updating takes 1ms, your update time-step is fixed to 5ms, and your render timestep starts (and remains) at 16ms [60Hz]. 

the two triangles are parallel (in which case any point on the triangle is equally close to the other triangle) an edge on one triangle is parallel to a line which crosses the triangle. So you can find two lines of equal length, one contained in the first triangle, one in the second. As those lines are parallel to each other, every point along those lines is equally close to the equivalent point on the other triangle. a single vertex of one triangle is closest to some point on the plane inside the other triangle. 

Essentially what you're describing with your spiral example is a pathological case, where all of the interesting points are co-incident. You could just as readily construct a hypothetical case where every single collision pair is identical. The point of broad-phase culling and use of hierarchical volume trees is not to be a perfect solution for every case, but to improve efficiency in the general case. Given a relatively even distribution of collision objects through the space, hierarchical bounding systems like oct-trees or kd-trees are great at very quickly drilling down to a much smaller subset of objects to check. For a typical game where objects are distributed around a relatively large landscape rather than being localised to one tiny area, spatial partitioning is very cheap and generally very effective. But, when choosing an algorithm for broad phase culling, you are always balancing efficiency of computation against expected results. Even if your spatial determination (the cost of figuring out where in the spatial hierarchy to put an object) only costs a tenth of the cost of a proper object-object check (the narrow phase), then to be more efficient, spatial partitioning needs to cut down the number of comparisons needed in the narrow phase by more than a factor of ten. If it doesn't, then you're better off not broad-phase culling at all. If you know in advance your problem domain is such that the pathological cases are common (e.g. you're simulating a lot of coiled chains), then you should either a) tag those cases such that they skip the broad phase altogether and go straight to narrow phase without incurring the partitioning cost, or b) find a broad-phase culling algorithm that better fits your domain. No algorithm will be perfect in all situations, you are looking for the best average efficiency gain for your particular situation. I can't think of a better solution off-hand for chain collision detection, but I should imagine that you can still use BVH-style culling, you just have to be smarter about how you separate out objects into cullable groups. 

I'm pretty sure on Direct X platforms at least, you're expected to query the platform for a list of available modes (using EnumAdapterModes on the device). That way the platform will already exclude any unavailable resolutions (because the GFX card or OS doesn't support it). You can further sub-filter those modes to exclude any you don't support (for example if you don't support widescreen resolutions), but you certainly shouldn't be defining your own arbitrary list of resolutions and expecting them to map to the set which is actually available. 

If you identify that the 2nd or 3rd cases have happened, you need to do some path-finding to see if 1 and 2 are still connected through their other adjacent voxels. You can get some efficiency here though. If a voxel is entirely internal to a group (i.e. all 8 of its neighbours are part of the same group), then it can be discounted. Why? It's a topology thing. Imagine the 2D case - there are only two possibilities. Either there is a single edge which, regardless of how it twists and turns, still forms a ring of voxels. Or, there are two rings, one containing one voxel, and one containing the other. E.g.: 

There are so many ways of addressing this problem I'd struggle to give a decent and comprehensive answer here. But here are some high level design points. 

Your question is unclear as to the restrictions on context you're working in. The vast majority of textures in 3D rendering are 2D. So if you're just showing a 3D sphere with a 2D surface texture mapped around it, this isn't really a problem. If you can't use 3D rendering, then you have to say exactly what you can use. The basic issue is that you have to render a flat texture over the surface of a sphere, which you get for free with 3D rendering. As the planet rotates, the visible portions of the planet's surface animate in a non linear way (the equatorial parts of the texture move faster than the poles). So I think either you have to distort the image yourself as you map it over the disc, or you do as VirtualVoid suggested, and simply have multiple images which you change between over time. It would be horrible to implement, but if you are able to render the texture, pixel by pixel, then you could basically do the rasterisation calculations for each line of the sphere separately. Let's assume that your surface map texture is flattened out, so that at the equator there are 512px of image. Let's also assume your visible disc is 256 px wide. Now think of each line of the rendered disc as a sliding window on the surface texture. On the equator, the window is 50% of the texture width, and you simply copy each of the 256px onto the equivalent pixel on the disc. The next line down on the disc will be slightly less than 256 px, but because of the distorted surface map, there are still 256px of input surface map data. So you then sub-sample the input surface map data, and render out the resulting pixel. For easy maths, let's assume that 1/3 of the way between the equator and the pole is 128px wide in the output disc. So each of those 128px is going to be the average of 2 neighbouring pixels. When you get down to the pole, you'd be averaging all 256px into only a few output pixels. You could also do it the other way, and have the lines in the source texture be of different lengths. So while the line containing the source data has 512px in it, the line 1/3 of the way down has only 256px, and the line at the bottom has only a few pixels. But each line is double the width of the disc at the equivalent y coordinate. That sort of a texture would be absolutely horrible to have to create though. And would probably suffer from horrible aliasing issues. In both of those cases your animation is simply then incrementing the start pixel x in the input texture, and wrapping around to the start of the input texture line. The more I write about this now, the more I'm convinced that it's a horrible idea, that you'd only implement if you really had absolutely no other choices. And you'd have to be in a very unusual situation to have no other choices. I don't believe I've heard of any solutions to this (rather vague) problem as a specific named technique.