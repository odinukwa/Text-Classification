Usually Shannon entropy is used to prove channel coding results. Even for source-channel separation results shannon entropy is used. Given the equivalence between Shannon (global) vs Kolmogorov (local) notions of information, has there been a study to utilize Kolmogorov complexity for these results (or atleast to replace the source coding part in source channel separation results)? 

It is known that if calculating permanent is easy, then solving hard problems in NP is easy. Is there a transparent example regarding application of say finding independent set or find chromatic number of a graph through the permanent? 

Yuri Manin recently posted an interesting paper on computability of boundary regions of distance-rate trade-offs for error correction codes. $URL$ I am curious if information theory provides any clues to this problem in asymptotic coding theory? 

What algorithms/mathematical techniques are available to exactly/approximately count number of independent sets? Is/Are there a good reference/good references on this topic? I am interested in regular graphs. 

Supposing we know the adjacency matrix $\mathcal{A}_{G}$ of a given regular (or irregular) bipartite graph $G$. Are there good lower and upper bounds to the size of maximum matching from the graph's adjacency matrix's or its Laplacian's eigenvalues (without using an algorithm to calculate explicitly a maximum matching)? 

Any comments and criticisms welcome An approach from compressed sensing seems to provide a range from $69.96$bits to $171.72$bits: 1.)Storing the puzzle implies storing the solution (information theoretically). 2.)The hardest sudoku puzzle seems to have $t(\alpha)\alpha^{2}$ entries for some $t(\alpha)$ that depends on $\alpha$ (For example, $t(3) ~= 2.44444$ to $3$). $URL$ Hence, we have a vector $P$ of length $\alpha^{4}$ that have atmost $t(\alpha)\alpha^{2}$ non-zero entries. 3.) Take $M$, a $\beta \times \alpha^{4}$ matrix with $\beta \ge 2t(\alpha)\alpha^{2}$ and which has any $2t(\alpha)\alpha^{2}$ columns independent and with entries in $\{0,\pm 1\}$. This matrix is fixed for all instances of the puzzle. $\beta = kt(\alpha)\alpha^{2}$ for some fixed $k$ suffices from UUP. 4.) Find $V = MP$. This has $\beta$ integers which on average is bounded by $|\alpha^{2}|$ since entries of $M$ are random with entries in $\{0,\pm 1\}$. 5.) Storing $V$ needs $\beta\log{\alpha^{2}} = 2kt(\alpha)\alpha^{2}\log{\alpha}$ bits. In your case, $\alpha = 3$ and $t(\alpha) ~= 3$ and $2kt(\alpha)\alpha^{2}\log{\alpha} = 69.96k$bits to $85.86k$ bits. $k=2$, the minumum required provides roughly $139.92$bits to $171.72bits$ roughly as a lower bound for the average case. Note that I have hand-waived some assumptions such as sizes of entries of $MP$ and number of entries one has on average in the puzzle. $A.)$Of course, it mightbe possible to reduce $k$ from $2$ since in sudoku the position of the sparse entries are not that mutually independent. Each entry on an average $t(\alpha)-1$ entries each in its row, column and sub-box. That is given, that some entries are present in a sub-box or column or row, one can find the odds of entries being present in the same row, column or sub-box. $B.)$ Each row, column or sub-box is assumed to have on an average $t(\alpha)$ non-zero entries with no-repeating alphabet. This means some types of vectors with $t(\alpha)$ non-zero entries will never occur, thereby reducing the search space of solutions. This could also reduce $k$. For instance, fixing $t(\alpha)$ entries in a sub-box, a row and a column would reduce the search space from ${}^{\alpha^{4}}C_{t(\alpha)\alpha^{2}}$ to ${}^{\alpha^{4}-(3\alpha^{2} - 1)}C_{t(\alpha)\alpha^{2}-3t(\alpha)}$. A comment: May be a multi-user arbitrarily correlated Slepian-Wolf model will help make the entries independent while still respecting the atmost $t(\alpha)\alpha^{2}$ non-zero entries criterion. However, if one could use it, one need not have gone through the compressed sensing route. So applicability of Slepian-Wolf might be hard. $C.)$From an error correction analogy, an even significant reduction may be possible, since in higher dimensions, there could be gaps between the half-the-minimum-distance radii hamming balls around code points with a possibility to correct greater errors. This also should lead to reduction of $k$. $D.)$ $V$ itself can be entropy compressed. If the entries of $V$ are quite similar in sizes, then can we assume that the difference between any two of the entries is atmost $O(\sqrt(V_{max})) = O(\sqrt{|\alpha^{2}|})$? Then if encoding the differences between the entries suffices, this itself will remove the factor $2$ in $\beta\log{\alpha^{2}} = 2kt(\alpha)\alpha^{2}\log{\alpha}$. It would be interesting to see if $2k$ can be made equal or less than $2$ using $A.)$, $B.)$, $C.)$ and $D.)$. This would be better than $89$ bits (which is the best so far in other answers) and for the best case better than the absolute minimum for all puzzles which is around $73$bits. 

Surely without loss of generality we can force $\Sigma = \{0,1\}$. It seems like that $ALL_{\textsf{minCFG}} \leq_T MIN_{\textsf{CFG}}$. That is, we can determine whether a minimal CFG can generate all possible strings, by using the oracle for $MIN_{\textsf{CFG}}$. Let $G$ denote the minimal CFG we are interested. Since $G$ is minimal, there exists a length $p$ such that, for each rule $R$ in $G$ there exists a string $w_R$ with $|w_R|<p$ that can be generated by $G$, but not with $R$ deleted. Construct $G^+_{a_1a_2\cdots a_p}$ where $a_i \in \Sigma$, by adding following rules in $G$: $$ S \to a_1 a_2 \cdots a_pT \\ T \to T0 \ | \ T1 \ | \ \epsilon $$ Now consider what will happen if $G^+_{a_1a_2\cdots a_p}$ is not minimal, thus, there exists a rule $R$ in $G^+$ is dispensable. By our construction it is clear that each rule $R$ in $G$ is indispensable. 

Update. (Nov 5th, 2017) We found that, roughly speaking, we could prove $A_{\textsf{TM}} \leq_T ALL_{\textsf{minCFG}}$ by the same way as one for $A_{\textsf{TM}} \leq_T ALL_{\textsf{CFG}}$, just adding some stuff to show that the grammar we used in proving $A_{\textsf{TM}} \leq_T ALL_{\textsf{CFG}}$ is already minimal. However, I could not assert that this way is quite simple. Comment. @Sylvain gives an elegant proof below, by showing $$ PCP \leq_T FullPCP \leq_m MIN_{\textsf{CFG}}$$ in which I personally thought the key is that context-free language is closed under $h^{-1}$ he defined. 

Therefore, if $G^+_{a_1a_2\cdots a_p}$ is not minimal, we can determine whether $a_1 a_2 \cdots a_p \Sigma^* \subseteq L(G)$. What if it is minimal? In this case we will immediately know $L(G) \neq \Sigma^*$. So we can suppose that all possible $G^+_{a_1a_2\cdots a_p}$ is not minimal. Then by checking whether $a_1 a_2 \cdots a_p \Sigma^* \subseteq L(G)$ one by one and checking those strings of less-than-$p$ length, we can still decide whether $L(G) = \Sigma^*$. 

I am not sure that the below question asked by me several years ago is suitable. $URL$ If we use naive dynamic programming, the complexity will be $\Omega(k n^2)$ (I am not sure whether it is $\Theta(k n^2)$ or $\Theta(k n^3)$). Whereas using the fact mentioned in the post (i.e. the linear recurrence relation), we can solve it in $\mathcal{O}(k^2 \log k \log n)$ time since the order of recurrence is $\mathcal{O}(k^2)$ and computing the $n$-th term of a linear recurrence sequence of order $m$ can be achieved in $\mathcal{O}(m \log m \log n)$ time. 

Trivial result. For $k = n-1$ and $\epsilon=0$, the bound is tight. For a construction one can refer to here. Result from Xitip. I have used Xitip to verify the inequality where $n \leq 5$. I found that when I slightly increase the coefficient ($1/(n-k)$), the program reported that it would be no longer a Shannon-type inequality. This fact suggests that if the bound is not tight, we may find a family of new non-Shannon-type inequalities. 

This post is improved from $URL$ Since the form of the problem is changed in a great extent. I would like to create a new post rather than edit the old one. If it is inappropriate, feel free to edit these two. 

Let us start with some random variables $X_1, \dots, X_n$ and $Y$. Suppose Alice wants to transmit $Y$ to Bob by using $n$ communication channels. She does this by directly sending $X_i$ through the $i$-th channel respectively. Of course, the fact that this coding is correct gives us $$ H(Y|X_1,\dots,X_n)=0 $$ Now, here is an eavesdropper Eve. She can observe random $k$ channels out of $n$. In other words, she will know random $k$ variables out of $X_1,\dots,X_n$. Suppose the coding scheme can guarantee that Eve will not gain too more information about $Y$, by $$ \frac{1}{\binom{n}{k}} \sum_{\substack{A \subseteq \{1,\dots,n\} \\ |A|=k}} I(Y; (X_i : i \in A)) \leq \epsilon $$ Then we can derive a bound about $\sum_{i=1}^n H(X_i)$: $$ \frac{1}{n}\sum_{i=1}^n H(X_i) \geq \frac{1}{(n-k)\binom{n}{k}} \sum_{\substack{A \subseteq \{1,\dots,n\} \\ |A|=k}} H((X_i:i \notin A)) \\ \geq \frac{1}{(n-k)\binom{n}{k}} \sum_{\substack{A \subseteq \{1,\dots,n\} \\ |A|=k}} I(Y;(X_i:i \notin A) | (X_i:i \in A)) \\ = \frac{1}{(n-k)\binom{n}{k}} \sum_{\substack{A \subseteq \{1,\dots,n\} \\ |A|=k}} \left( I(Y;X_1,\dots,X_n) - I(Y;(X_i:i \in A)) \right) \\ =\frac{1}{n-k}(H(Y)-\epsilon) $$ My question: