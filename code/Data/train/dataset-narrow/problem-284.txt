While there is no formal "connection" with UDP there is still a convention that clients send requests and expect to get responses back with the source IP and port swapped with the Destinatoin IP and port. Stateful firewalls and NATs therefore assume that packets with a given combination of source IP/source port/Destination IP/Destination port and the corresponding combination with source and destination swapped form part of a "connection". This allows rules like "outgoing connections only" to be applied to UDP and allows reverse translations to be applied to response packets. Unfortunately the firewall or NAT has no way of knowing when the client has finished talking to the server. So it has to wait for a timeout before removing the entry from it's state tracking tables. That is the timeout you are setting. In principle it would be possible to build a NAT box that used a stateless approach for port forwards while maintaining a stateful approach for outgoing connections but it's simpler to just use stateful NAT for everything and it sounds like this is what your vendor is doing. Unfortunately as you have discovered this sucks for stateless UDP servers serving large numbers of small requests. You end up in a situation where the firewall consumes far more resources than the sever itself. 

Repeaters (hubs) work at a low level, they do not have the ability to buffer frames or to identify the intended destinition of a frame. So if they detect a collision then all they can do is output jam signals to ensure every device in the collision domain sees the collision. Bridges (switches) work at a higher level. They process complete frames and forward them based on destination mac address. If a bridge detects a collision it will perform the CSMA/CD procedure just like an end device would. There is no need to repeat the collision out of other ports. 

Most operating systems will provide a measure of the link speed of the device. They will also provide a measure of how much traffic is flowing. You could subtract the two to get an approximation of the amount of free capacity. How exactly to read these values is outside the scope of this site (probably stack overflow is your best bet there). However there are a few pitfalls with this technique 

Firstly note that there is no standard for router CLIs. It varies both between brands and sometimes between product lines within a brand. Secondly note that home/small buisness routers may not have a CLI at all or at least may not have a documented CLI. As for actually accessing the CLI you can either use a serial cable with a serial console application or a network protocol like ssh or telnet. If the device has a documented CLI then the manufacturer should tell you how to access it. 

Generally frames are first transffered from main memory to a buffer in the medium access controller. The interface used will depend on the application, an on-chip MAC is likely to use a paralell interface. Old off-chip MACs also used parallel interfaces but nowadays serial interfaces are increasingly used. 10 gigabit and faster interfaces typically use multi-lane PCIe which isn't really either a tradtional paralell or a traditional serial interface. The buffer inside the MAC is needed to decouple the timings on the Ethernet interface from the timings on the host interface. The MAC decides when to transmit a frame (using CSMA/CD if needed) and transmits it at wire speed to the PHY. For 100 megabit and 1 gigabit systems the MAC to PHY interface is typically a narrow (2-8 bits) parallel interface, though serial interfaces are also sometimes used. For 10 gigabit and 25 gigabit I belive a fast LVDS serial interface is typically used. For higher speeds I believe a multi-lane interface built up out of fast LVDS serial links is used. The phy takes the data from the MAC and re-encodes it for the wire. 10BASE-T uses manchester encoding. 100base-TX first passes the data through a 4B5B encoding process. This is then further encoded using "MLT-3" to reduce the effective frequency. 1000BASE-T uses multiple pairs in parallel, uses a multi-level encoding scheme to send multiple bits at the same time on the same pair and uses echo and crosstalk cancellation techniques to use all four pairs in both directions at the same time. On fiber at speeds up to 25 gigabit data is sent serially on a single wavelength. At higher speeds a multi-lane approach is used with the lanes of data either being sent down multiple seperate fibers or on multiple wavelengths in the same fiber. 

Fixed layer count models and tunneling don't match up very well. Our stack might look something like. 

However such a low MTU would be extremely inefficient. IPv6 sets a much higher minimum of 1280 bytes and requires links that can't support that MTU to provide a link-specific fragmentation and reassembly layer. From $URL$ 

It can be done and generally works ok. It's quite common to see it in cable TV networks. There are a couple of caveats though. 

Sorry I don't know the exact layout of the code in the linux kernel to tell you where the code implementing this is. 

For a segment to operate correctly in full-duplex mode the devices at both ends of the segment must support full-duplex and be in full-duplex mode (Either through autonegotiation or manual configuration). If for some reason one end of the segment is in full-duplex mode and the other is in half duplex mode then you get a duplex mismatch and terrible network performance. In particular this is likely to happen if a port in autonegotiation mode is connected to a port with autonegotiateion disabled and mode forced to full-duplex. 

It would probably be possible to design a link protocol that solved 2 through 4 while having a lower encapsulation overhead than Ethernet framing or possibly no Encapsulation overhead at all but it would not be a trivial case of saying "just use IP". And then you still have to convince people about 1, lots of pain in adopting a new standard for a relatively small gain. Cranking up the speeds while keeping the framing format the same is the path of least resistance. So it's what happens. 

If the machines on the intermediate networks need access to the internet then NAT will be needed. The NAT implementation must be flexible enough to decide based on source IP whether or not to NAT (linux NAT certainly is, I don't know about other implementations). ICMP replies sent by the routers will be generated with private source addresses and may end up getting blocked. This may cause issues with things like traceroute and path MTU discovery. As long as you don't have any low-mtu segments this shouldn't be a massive issue though. 

Generally in most installation work you are using bulk solid core cable off a spool. So punchdowns are the way to go. OTOH female-female couplers are the way to go if for whatever reason you want to have stranded cables on both sides. 

Note that this example has no firewalling, in a practical implementation you would want to add that. 

The router needs to have a suitably flexible NAT implementation. The traffic needs to actually reach the router The return traffic needs to return via the same router so it can be correctly reverse translated. 

Part 1: background Lets say you have a head office. You build a network in your office, and connect it to the Internet via a firewall, most likely the firewall is also doing NAT. Within your network you deploy some servers. You also subscribe to various Internet services which identify your network through an IP whitelist. But not all your users are at head office. Some are working from home, some are working from small branch offices, some are working on the road. They still need to access your companies services, how do they do it? You could just have them access the services directly over the Internet but this has several problems. 

When a node wants to send a packet it will look up the destination address in it's routing table. Based on that table it will determine the IP address and interface for the next hop. If the destination is on one of the local subnets then the next hop IP address will simply be the destination. Otherwise it will be the IP address of a router (most likely the default gateway). The MAC address (if applicable) for the next hop IP address will be determined by arp (for IPv4) or neighbour discovery (for IPv6) 

Ok with all that in mind you have overlapping subnets and you can't get rid of them. What to do. The first thing you need to do is look at the big picture. Figure out what the network looks like. Figure out what your goals are. 

Afaik with a pure hub a device will either link at the speed the hub is set up for or not link at all. Automatically downgrading the whole network would seem like a very undesirable behaviour to me. Note that there were/are many "dual speed hubs" around. These are logically two separate hubs connected by a two-port bridge. The dual speed hub then negotiates speed on each port and connects it to the appropriate logical hub. At the time This was cheaper to implement than a full switch. 

Indeed it is the setup where multiple carriers can rent the local loop from the incumbent former-monopoly, then run their own equipment at the telephone exchange. The exact details will vary by country. AIUI in the UK there are a number of variants. 

It is possible to do an "outgoing connections only" firewall for IPv6. What any given router actually does is down to the router vendor. Some may well default to wide-open. Note that because IPv6 addresses are typically big and sparse scanning attacks are largely impractical in IPv6. 

It is likely that the client IP address seen by the server will be an IP address of a NAT box rather than the internal IP of the client. some NAT setups may not support connections to the public IP from internal clients at all. 

Don't let perfect be the enemy of good. Have a plan for where you are going and how you are going to get there. 

Plugging in a computer and visiting google is a surprisingly complex process. This answer tries to explain the basic process without going into too much detail. When you plug your computer in to the network it uses a protocol called DHCP to request IP configuration for itself. This configuration will include IP address, subnet mask, default gateway and DNS servers. The DHCP request is sent as a broadcast to all systems on the local Ethernet network and is answered by the DHCP server. In a typical home or small buisness environment the DHCP server will be running on the router. The switch doesn't (normally) know or care about DHCP it just passes the requests and responses around. When you go to visit google.com your browser asks the operating system to convert google.com to an IP address. The operating system generates a DNS request and queues it for sending to the DNS server address. It then looks up the DNS server's address in it's routing table. To determine a "next hop IP address" and interface. By default if the destination is on the local subnet then the next hop IP address will be the same as the destination IP address. Otherwise the next hop IP address will be the default gateway. That is when ARP first comes into play. The "next hop IP address" must be translated to a MAC address. The computer will look in it's ARP table, if it finds an entry then great, otherwise it will make an ARP request to try and locate the MAC address that corresponds to the next hop IP address. What happens next depends a bit on configuration. Maybe the router routes the packet out to a DNS server on the Internet, maybe it performs NAT, maybe the router has a local DNS server. Either way the router will end up with a DNS reply addressed to your client machine. It will look up your machine's IP address in it's routing table and determine it is on the local network. It will then look it up in it's ARP table so it can send it to your machine. your computer gets a DNS reply. So it has an IP address for google.com and can attempt to connect to it. The connection process starts by sending a TCP syn packet. Once again the destination IP address is looked up in the routing and arp tables and the packet is sent to the router. In a typical home/small buisness setup the router will perform network address translation so the packet appears to come from the router and send the packet out towards the internet. When the reply arrives the router will reverse any network address translation and send the packet back to your host. Again it uses it's arp table to find your host's MAC address. The switch is largely ignorant of all of this. It just uses the source MAC addresses to build a table mapping MAC addresses to ports. If it knows which port the destination MAC address is behind it will send the packet there, otherwise it will flood it everywhere. 

No, the speed of the link is the speed of the link. You won't get more than that total. The difference between "leased line" and "broadband" connections is more subtle. 

Typical home/small buisness routers are designed to connect a single local network LAN to a larger network (usually but not nessacerally the internet) usually with NAT (though NAT can be disabled. The LAN ports connect to your local network (whether wired or wireless), the WAN port connects to your internet connection. 

As a general rule hardware is good when you have a relatively simple task that you need to do very quickly/frequently. Software is good when you have a more complex task that is not so speed-critical. Unmanaged switches do pretty much everything in hardware. Their job is a simple one, add MAC addresses to a table, match MAC addresses against that table. It's simple but it needs to be fast and cheap, dedicated hardware is the order of the day. It wouldn't surprise me if there were some tiny processors inside doing initialisation stuff but if there are they aren't visible from outside. Managed switches have a CPU to handle the management side of things. Building the management interface in hardware would be prohibitively expensive both from a development and production point of view. It would also make it impossible to fix bugs or add features after the hardware was produced. A similar principle applies to high end routers, the actual high speed data forwarding is handled by dedicated hardware but the management side of things including talking to other routers is done in software. 

Tecnhnically they can be interconnected by any link layer that can carry the IP packets between the two routers. The interconnections between ASs can be roughly divided into transit (where a provider AS providers "internet access" to a customer) and peering (where two ASs exchange routes for themselves and their customers with each other). There are also possibilities in between conventional transit and conventional peering which I won't go into here. Transit connections will usually be over private links (though some exchange points do allow transit relationships). So will high traffic peering interconnections. The details of these private links will depend on the distance between the routers that need to be connected. For links within a datacenter most datacenters offer cheap and simple cross-connects. For longer distance links a circuit will likely be rented from a local communication provider. The rented circuit may either be a physical fiber, a wavelength on WDM or some kind of virtual circuit (MPLS, Vlan, ATM PVC etc). Exchange points are used where an AS wants to peer with a large variety of other ASs but each individual peering relationship doesn't have enough traffic to justify a dedicated link. The exchange point provides connectivity between many ASs allowing the costs of physical interconnect to be spread across many peering relationhips. The exchange point typically provides an ethernet network over which the peering relationships can be established and each provider connects their router to that network (again this connection may be a simple cross-connect within a datacenter or it may be a longer distance link rented from a communication provider).