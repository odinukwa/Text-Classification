You may be able to use a variation of the following technique - which forces repeated 'MIN/MAX' range scans: Assumptions 

This is not a full answer because it only works if your is less than 4000 chars. The clob version (with plain instead of in the query) fails with - perhaps someone else can explain why exactly? testbed: 

The statement above has three identical subqueries and seems to execute them independently. How can I best rewrite the update to perform the subquery only once? (The idea of this query is to only perform the update if doing so would not cause the sum of pool1_count to exceed the maximum allowed pool1_count and the same for pool2, pool3 ....) 

This could be tidier if Oracle's regex flavour supported lookahead or non-capturing groups, but it doesn't. However this solution does avoid scanning the source more than once. DBFiddle here 

The following two examples should yield the same result by my reckoning, am I missing something obvious? 

Tell your boss what has happened. If you are sure that is the only table that has been dropped, consider recreating it - do a dummy install on another box of the exact same version of Oracle and get the DDL from there. Even if you don't use Advanced Replication, you have no guarantee that there isn't some dependency chain that means something else isn't broken. Don't do any DDL as . Ever. Don't even log in as or unless you are doing something that requires you to do so. Preferably leave the and tablespaces alone - create your database objects in their own tablespaces or the default tablespace. 

On Oracle you basically have a choice between running a hot standby and using RAC. The main aim of High Availability is to remove single points of failure. RAC does this at the server level allowing failure of a server without any service interruption. You will need to achieve something similar at the storage end using ASM, mirroring, and two or more physically independent storage pools (or a SAN). Using a hot standby will mean service interruption in the event of a failure, but is simpler and has fewer "engineering trade offs" Good quality hardware is also essential, eg SAS not SATA, redundant PSUs, UPSs etc. There are also other aspects of High Availability you may need to consider (eg human error) - this white paper from Oracle discusses them in more depth. 

Some of this information will be data in tables, some will be codified logic in views. Something like this perhaps: 

Regarding faster IO, you need to look at hardware or configuration changes. Parallel query was mentioned in the comments, but this could be faster or slower and isn't a magic bullet for IO. Regarding reducing IO, note that your query does not necessarily need to look at the table at all — it could use an appropriate covering index instead: 

Probably not the answer you are hoping for, but have you considered some lower level of snapshotting - LVM for example? 

Conclusion My preference would be option 1 if the number of properties per is likely to be stable, and if you can't imagine ever adding more than a handful extra. It is also the only option that enforces the 'number of properties = 3' constraint — enforcing a similar constraint on option 4 would likely involve adding … columns to the xy table anyway*. If you really don't care much about that condition, and you also have reason to doubt that the number of properties condition is going to be stable then choose option 4. If you aren't sure which, choose option 1 — it is simpler and that is definitely better if you have the option, as others have said. If you are put off option 1 "…because it complicates the SQL to select all properties applied to a feature…" I suggest the creating a view to provide the same data as the extra table in option 4: option 1 tables: 

I would first consider trying to combine the two queries into one. If the second query could be refactored to return in the case where the first query returned zero would you be able to work with that? For example, testbed: 

In addition you can get an approximate answer by querying the catalog - if is zero that indicates no nulls but should be double-checked against 'real' data: 

The question is which of the s for a do you want to use for ordering purposes? If you are happy to use the largest, you could use: 

dbfiddle here Notice that only 21 blocks are touched even though about 100 rows are returned. The degree of clustering defined by can be tuned to balance the speed of query on RECORD_KEY with the speed of INSERTS — you can try different values for the divisor. You may be lucky and find you can simply use for clustering instead. Note that you don't need to use an IOT — you could instead have a covering index leading with (or ). With billions of rows I am making the assumption that doubling storage size will come with a cost — a single IOT to achieve clustering will be the solution that takes the least space. 

I recently use a non-free tool from DBConvert to convert an access database to postgres, and found it well worth the money compared to the amount of time I wasted trying to do it reliably for free. The sell a similar tool for MySQL<->postgres, which I have not used, but may well be worth considering unless you are only interested in command line tools. In case you are wondering I am not affiliated with them in any way :-) 

No, that is not enough - it is true that if "the partition or volume on which the cluster was initialized runs out of space and cannot be extended, a tablespace can be created on a different partition and used until the system can be reconfigured.", but current objects will not automatically spill over into the new tablespace in the sense you mean when you refer to LVM - they need to be moved, eg with : 

Using global indexes goes against the grain of partitioning in the first place — they are marked 'unusable' if you drop a partition. You may find this ORACLE-BASE article a useful read. 

Having read those discussions too, I am not sure why you decided on the DRI solution over the most sensible of the other options you outline: 

As long as you can fence the objects you want to copy into tablespaces separate from the objects you don't want copied over, you can use this feature to move many kinds of objects in one go. Data Pump is also needed but only for the metadata. 

In this case, an makes sense. There are other ways of modelling these business rules but this is the simplest and if it accurately fits the facts that you care about in the real world, I suggest it is perfectly ok 

Indexes can only speed up operations by reducing sorting - this will be more efficient if the index used is the clustered index or at least has the same leading column as the clustered index. In all this I am assuming MySQL has no equivalent of a operation which would usually bypass any benefit of indexes at all - maybe someone else can confirm this. There is a marginal benefit to having a separate index on assuming that is the only column in the clause and neither is the clustered index: the index will be smaller and therefore scanning it will generate less I/O --EDIT-- As an index contains all the primary key fields defined for the clustered index key that are not in the secondary index, An index on will only be smaller than an index on if is not part of the clustered index. 

You might try changing the id generation so inserts are not contending with each other, or consider setting , noting the implications for index maintenance (which are probably only relevant if you are also doing updates) 

Certainly, use integer You will have to decide how many decimal places you are assuming and be consistent at the application level. 

Neither of these potential problems would be resolved by using which allows integers in the range -9,999,999,999 to 9,999,999,999. If it is necessary to constrain the integers to the same range, it would be better to use a , eg: 

Note that foreign key constraints may cause this to fail. If you have circular foreign keys (ie from table A to table B and from table B to table A) then you may need to make deletes cascade or drop (or disable) the constraints before deleting. 

--edit it may be safer to use instead (see @Alex's comment below), but the caveat is that is also changed on grants and revokes. 

You can get a reasonable estimate of the median using pg_stats if you set the number of bins to 2. You need to set it low because it is a maximum, and the idea is to force an even number of bins so the middle bound is the middle of the distribution. Set it to 100 and postgres is free to use 99, 97, ... which is not what we want. 

From what I can tell, "single-threaded" is a bit of a misnomer for H2. The point is that it serializes all transactions (ie does them one at a time). The crucial question regarding whether that is "ok" or not for your application is not "How many users?" or even "How many processes?", but "How long are my transactions going to take?" If all your transactions are sub-second that may be fine, if some take several hours to complete, that may not be fine as all other pending transactions will be waiting for them to finish. The decision about whether that is "fine" or not will depend on your own performance requirements - ie how long is an acceptable wait for my users hitting the database with transactions. --EDIT It seems that H2 doesn't really serialize transactions - just DML. In other words lots of short updates within a single long transaction will not block other updates. However unless you are using the experimental MVCC feature, table locking means this has a similar effect in practice. There is also an experimental "multi_threaded" feature but it cannot be used at the same time as MVCC 

The code you have inherited is broken - and it always has been broken. That re-factoring you'd like to avoid needs to be done. There is no alternative to an explicit to guarantee the sort order of a result set and there never has been. Who knows if the code you inherited always returned rows in the order the original developer 'expected' or not.