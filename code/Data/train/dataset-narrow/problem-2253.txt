Scott Aaronson addresses this particular question on his blog Shtetl-Optimized when considering the timeline of Computer Science. The relevant posts in chronological order: Timeline of Computer Science Top 150 computer science events to be decided once and for all CS timeline voting: the results are in! Most of the events in his list would make good exhibits, although it might be difficult to figure out how to present some of them physically. 

The randomness used in quantum computing comes from quantum mechanics and is postulated to be an inherent 'randomness' of nature (and one of the problems Einstein, for instance, had with QM). If you take a state like $|+\rangle = \frac{|0\rangle + |1\rangle}{\sqrt{2}}$ and measure it in the computational basis, there is absolutely nothing you can do to predict the result (assuming our standard pictures of QM is not horribly wrong). Half the time you will measure $0$ and half the time you will measure $1$. Of course, if you measure in the Hadamard ($|+\rangle$, $|-\rangle$) basis, then your result will always be $+$. I don't think there is a standard notion of quantum randomness, mostly because how random a state looks depends on your choice of measurement. Once you choose a particular basis to measure in, then you might as well start talking about the classical probabilities associated with the results and then you can do your standard classical tests of randomness. Usually when someone talks about the 'quantumness' of a state, they try to capture ideas like entanglement. 

I like Ashley Montanaro's answer, but I thought I would also include a set of functions for which the conjecture is known. A set of functions which is often of interest is functions with constant-sized 1-certificates. This class of problems includes things like $OR$, distinctness, collision, triangle-finding and many other problems (not in the HSP-family) which have been shown to have query complexity separations. For a constant-sized 1-certificate total function $f$, we have $D(f) = O(Q(f)^2)$. 

This is avoiding the obvious cases where you publish incorrect, or controversial results. Also avoiding the case of finite-time: you only have so much time to think and write, so writing a paper might cause you to lose time on another project. An example use-case might be: you are aiming for a position in theoretical computer science, but often publish in non-theoretical areas which might be under the broader CS canopy or maybe even completely unrelated to CS. On the one hand, this can show broad interests and breadth. On the other hand, this could show a lack of focus, opportunism, or lack of commitment to the field. Can you avoid the problem by simply listing 'selected publications' on your CV that tailors to the specific position, or will the hiring committee always google scholar you? If so, when should you consider not publishing or publishing under pseudonym (or alternative spelling of name)? 

There is a recent article on implementing a quantum von Neumann architecture. They do this via superconducting qubits, of course the implementation is very small, with only 7 quantum parts: two superconducting qubits, a quantum bus, two quantum memories, and two zeroing registers. This allows their quantum CPU to perform one-, two-, and three-qubit gates on qubits, and the memory allows (data) qubits to be written, read out, and zeroed. As is mentioned in Peter Shor's answer, implementing a quantum superposition of gates is very difficult, and so the program is stored classically. 

I used to (and to a small extent, still do) have everything scatter across notebooks and binders, but recently I got fed up and moved over to a personal wiki TiddlyWiki. It can be hosted completely locally (it is just one html file), runs in your browser, and after installing a simple plug-in has LaTeX-like math support. I use it to take notes on papers I read as well as to jot down ideas and self-explanations of things that confuse me. 

I don't know what the density of common P-complete problems is, but here is a padding argument that shows how to lower any density below $1/n$: Take your favorite P-complete language $L_n \subseteq \{0,1\}^n$. This language has some density $d(n) \in \omega(1/n)$. Now define $L'_{n + m} = \{x0^m | x \in L_n\}$. In general, $m$ will be some function of $n$, so this might not define $L'$ for all sizes, since we are worried only about the upper density, just make $L'_k = \emptyset$ if $k \neq n + m$. What is the upper density of $L'$? Well, we have \[ d'(n + m) = \frac{|L'_{n + m}|}{2^{n + m}} = \frac{|L_n|}{2^{n + m}} \leq \frac{d(n)}{2^m} \] Now lets use LOG-reductions to build a machine $M$ for $L$ using a machine $M'$ for $L'$. Well, if you are given an input $x$ then simply copy it one bit at a time to the query tape (also use a counter to count what $n$ is), then use a second counter to count upto $m(n)$, adding one every time you add a zero to the query tape (to have log space, we need $m(n) \in poly(n)$ and easily computable). Then query and return the output as your answer. If we want to be sure we are smaller than $1/n$ then just pick $m(n) = n$, and then we will have $d'(2n) \leq d(n)/2^n \in O(1/n)$. 

Details: A certificate for an input $x$ is a subset of bits $S \subseteq \{1,...,n\}$ such that for all inputs $y$, $(\forall i \in S \quad y_i = x_i) \rightarrow f(y) = f(x)$. Then $C_x(f)$ is the minimum size of a certificate for input $x$ and the 1-certificate complexity $C_1(f) = \max_{x | f(x) = 1} C_x(f)$ (The 0-certificate complexity is the same but restricted to $f(x) = 0$). You can show that $Q(f) \geq \sqrt{bs(f)} \geq 2C_0(f)/2^{C_1(f)} + 1$. Then you can use the algorithm presented in Buhrman and de Wolf's survey to show that: $D(f) \leq C_1(f)bs(f) \leq C_0(f)C_1(f)$ 

This is the first time I have read about polynomial delay algorithms, so I am not 100% sure of my answer, but I think something like the following should work. Pick some convention for representing paths that has a natural total ordering $<$ defined on it. (One example would be just to list the vertexes of the path and order lexicographically). Pick your favorite in-place data-structure $D$ that supports logarithmic search and insert (say a red-black tree). Let $G$ be your graph Define an algorithm $F$: 

To elaborate a little on my comment (which is slightly tangential). There is a fun site called eigenfactor that provides nice visualizations of how various parts of science cite each other, and their relative size by publication/citation volume. Unfortunately their primary focus seems to be calculating impact factors for journals and so they operate on the level of journals, not individual papers/authors. However, they have a pretty interesting way of identifying subfields. They use the number of citations from a journal A to a journal B as a weight on the edge from A to B. They then consider a random walk on the resulting graph. Such a walk tends to linger in topic clusters, and thus when they try to create a shortest description of the walk they tend to name the clusters and use a second index inside the cluster (kind of how you would have unique city names, and unique street names in each city, but redundant across cities). I thought it was a pretty nice way of finding clusters, and produces clusters that mimic how most people would divide the topics in science (with the occasional hilarious classification like putting Phys. Rev. Letters in Chemistry instead of Physics) You can read more in their paper or on the mapequation site.