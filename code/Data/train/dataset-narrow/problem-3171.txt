If you are considering the information theoretical point of view, given and then adds no information. However, before we rush into conclusions, one must recall that there are more aspects. The first one is the ability to represent the concept. Let's consider as case in which your features are and and the concept is . The only problem is that I force you to use a decision tree as the model. How can you represent that? Note that even if I allow you to choose any tree, the representation will be bad. The second problem is that the concept is not given in advance and therefor the algorithm should search for it. Most supervised learning algorithms are actually looking for correlations between the concept and the features. A relation like usually cause them problem since the behavior is not linear in x. So, in case that you have ideas for features that don't add information but ease representation or search, adding them might help. You are right asking where should I stop? It might be that there is a different combination of features that will lead to a better model. Unfortunately, we don't have a good solution for that. Consider the problem of building good features, let's say features that reduce the size of the model. We can restrict our self to features that can be represented by trees. However, building a small tree for a data set is an NP-complete problem, which means that we don't have an efficient algorithm for that. More than that, you can claim that given a feature building problem you can reduce classification into it (e.g., build a feature that represents the concept). Therefore, feature building is as hard as classification. 

Usually, people are indeed quite indifferent to the format, as long as there is an easy way in which they can transfer it to their favorite format. CSV is a very common format, since most tools can load it. Note that there are some dataset whose CSV representation is inconvenient (e.g., the text has many commas, data set in which the type is importnat yet hard to deduce). As for the storage of the format most people are even more indifferent to the source too. Web interface is the common option. If you use a relational database you gain few advantages: 

Stop words are common words in a language and they are usually removed when there appearance is not indicative to the analysis goal. Suppose our goal is to text mine and find out if a given text is about sports or politics. If we use a bag of words, "not" is probably a stop word we should remove since it is probably not indicative of either categories. On the other hand, if we would like to differ between "This article will not discuss politics" and "this article will discuss politics", we must not remove "not". The examples you gave look like programming languages reserved words. In most languages you can find this documentation and skip the learning phase. Looking at the frequent tokens will probably also give you the reserved words (and probably plenty of English stop words that will appear in the remarks). However, before utilising such a list, be sure that neglecting them will serve your goal. In case that typing them doesn't differ from typing other words, you might better leave them. This way you will have more data the represent better the user behaviour. 

Scheduling problems might be NP-Complete problems. It is not clear what are the specific details. You might get lucky and have specific constraints that are leading to an easy sub problem or just an easy instance. However, since many variations like On the Complexity of Scheduling University Courses (which might be your case), Job shop scheduling,Multiprocessor scheduling and Open-shop scheduling are NP-complete, you are probably in the same situation. Usually, it is better to treat such problems like optimization problems and not like classification problems. If you'll try to treat the problem like a classification problem you might have severe problems in building a classified dataset that will represent well the time scheduling problem that you would like to solve. There are general technique in order to cope with optimization problem. I also found some work related to your case. I'm not familiar with this specific case but it seems that Solving the Course Scheduling Problem Using Simulated Annealing and the work done here might help you. 

The representation power of the models is different. Consider the case of XOR for many features. Assume the all the features are binary, just randomly sampled fro B(0.5). A decision tree will be able to represent that concept, in exponential size. Any of the features alone is useless (or misleading). A decision stump is restricted to a single feature. Even if the decision stumps will choose the right features, one cannot represent XOR using a linear weight of the features. In simpler words, the reason the XOR is not linear is that there is no weight w such that a xor b = w*a + (1-w)*b. 

How to quickly evaluate a data science project which focus on the data understanding, the value of the insight and the execution. It is a bit abstract. Data Science Project Checklist To Use Before You Start A Project To Convey You Can Actually Get Work Done is indeed a checklist, as the name suggests. Data science done well looks easy - and that is a big problem for data scientists is an interesting reading but general. The “Joel Test” for Data Science is more of what I'm looking for but I don't think that the content of the test is suitable in its current form. Cross Industry Standard Process for Data Mining is a suggested methodology but I don't think it answer my question. 

See mine (and others) answers here regarding how to work with a data scientist. The problem you have with planning is real and severe. We all have a problem in planning research. As Albert Einstein said "If we knew what it was we were doing, it would not be called research, would it?". Though it is hard, we are not the only one having to deal with uncertainty in planning. The common methods of dealing with uncertainty are useful here too. Your idea to break down the task is great. So is performing quick tests (POC like) in order to see is a direction is valuable. The main problem in research that it is very common that good direction will be found to be not beneficial only after a lot of work. You might be surprised from the other side, having great ideas coming when somebody takes a shower. Due to the problem in estimation, people hesitate to do it. Though, they realize that "You'll have it when it ready" is not an answer that a customer will accept. The question is indeed not a pure data science question and so is my answer. Understand that planning is hard, do try to do so and expect your planning to be very inaccurate. 

You need to deal with imbalanced data set when the value of finding the minority class is much higher than that of finding the majority. Let say that 1% of the population have that rare disease. Suppose that you assign the same cost to saying that a healthy man is sick or saying that a sick man is healthy. Provide a model that say that everybody are healthy, get 99% accuracy and go home early. The problem with such a model is that though it has high accuracy, it will probably not what you are looking for. Most of the time you want to find the people with the disease (giving high weigh to false negatives) much more than you are afraid to send an healthy person to unneeded test (low weight to false positives). In a real world health problem the ratio between the weight can easily be 1 to 1,000. The imbalance in the distribution fails most algorithms from finding a proper solution. You are correct that just balancing the distribution isn't the optimal solution. Indeed, an algorithm that is trained on a balanced distribution is not fitted to the natural distribution on which it will be evaluated. My favorite method is adapting it back, as you can see here. For a discussion, see here. Just setting the ratio to some other problem won't work since you will have the same problem. Smote is working in a different way, which didn't work as well when I tried it, but it might fit your problem. 

Interesting question. The way I see it, the justification comes from both mathematics and experimental science. If you suggest a new algorithm, you should state your assumptions and provide a proof, just like in math. However, you should also demonstrate it on real data, showing that your assumptions are reasonable and that your algorithm can deliver value. If you suggest a model, you should justify it based on its performance on the observations, just like in experimental sciences. This is true no matter how did you derive the model. However, examining the model logic itself and the way it was derived will help to gain confidence in it and trace where it needs modification. 

The data structure typically uses is inverted index (e.g., in databases). Please note that matching all ngram is a good heuristic but you might want to improve it. Taking into account the probability of each term and stemming are directions you might benefit from. 

Bounds on the needed amount of samples are very common in PAC learning. When you define a concept class you can compute a minimal size set of sample that will enable learning. However, 

Before evaluating how good is automatic summarization by taking the first sentence, you should decide how to evaluate summarization. In supervised learning typically it is easy to know if the prediction match the concept - they should be identical. Upon that, you can choose a metric that fits your needs (e.g., accuracy, precision, recall) and compare classifiers. The problem of evaluating text summarization is that deciding whether a summarization is good is subjective and prone to error. A possible metric is ROUGE which is a set of heuristics (e.g., common longest subsequence) that compare the summary to the original text. Note that having a good ROUGE score is an estimation but an estimation that will enable you to benchmark your algorithm agains others. See EVALUATION MEASURES FOR TEXT SUMMARIZATION by Josef Steinberger, Karel Jeˇzek for a discussion of metrics and other algorithms. Showing that you get a good score with respect to an algorithm that is based on cue words will be a good results. Another possibility is building a gold standard by comparing the first sentence to the text and manually labeling it for a good summary. While manually labeling will give you good estimates of the performance of your algorithm, it costly in term of time. A more severe disadvantage id that this gold standard is fitted to your algorithm and hard to use for other algorithms. Suppose that the second sentence is just as good algorithm as the first one. The gold standard built for the first sentence won't be able to show that. For a good estimation I suggest that you will use ROUGE for comparison and a gold standard for absolute results. If you have the resources to create a gold standard for the benchmark algorithms, the comparison will become much more robust. 

Do you have a way to evaluate the results? How can you know if a string is indeed a token in this language? In order to get possible tokens, I recommend that you'll try Huffman coding, the compression algorithm. The tree that it builds will contain the tokens. 

First, lets discuss the prediction problem. I think you should treat matching the pairs as a supervised learning problem and not as a recommendation problem. As João Almeida wrote new student won't have any previous relations with alumni. Even the alumnus will have very few previous relations. I would have add to each alumni some features based on aggregations (e.g., the number of past relations, the ratio of past good matches). After that you should build a dataset of the past pairs, using 'Match?' as the concept. It is not clear whether you will be able to learn a good match rule, even if it exists. I guess that your dataset is quite small. If the probability of a match is low, you might have imbalance problem. As AN6U5 commented, height and weight are quite strange features to match students to alumnus. Compute the relations between the features and the concept (e.g. mutual information, Pearson correlation) in order to see if you have useful features. As for the second question, even if you can predict well if a pair will be a good match, you still have an algorithmic problem of which pair to use. Consider a "super alumni" that will be a good match to any student. You wouldn't like to match it to a "super student" but to a student that it will be hard to match to other alumni. Luckily, there matching algorithms that you can use. Build a graph with the students and alumnus as nodes. Create an edge if you predict a good match and run a matching algorithm upon it. 

For the numbers you mention, I think all alternatives should work (read: you'll be able to finish your analysis in reasonable time). I recommend on a design that can lead to significantly faster results. As answered before, in general postgresql is faster than mongo, some times more than 4 times faster. See for example: $URL$ You said that you are interested in improving performance in joins. I assume that you are interested in calculating similarities among the entities (e.g., post, author) so you'll mainly join the table with it self (e.g., by post or author) and aggregate. Add to that the fact that after the initial loading your database will be read only, what make the problem very suitable to index usage. You won't pay for index update since you won't have any and I guess you have the extra storage for the index. I would have use postgres and store the data in two tables: create table posts( post_id integer, url varchar(255), author_id integer ) ; -- Load data and then create the indices. -- That will lead to a faster load and better indices alter table posts add constraint posts_pk primary key(post_id); create index post_author on posts(author_id); create table comments( comment_id integer, post_id integer, author_id integer, comment varchar(255) ) ; alter table comments add constraint comments_pk primary key(comment_id); create index comment_author on comments(author_id); create index comment_post on comments(post_id); Then you can compute author similarity based on comments in queries like select m. author_id as m_author_id, a. author_id as a_author_id, count(distinct m.post_id) as posts from comments as m join comments as a using (post_id) group by m.author_id , a. author_id In case you are interested in tokenzing the words in the comment for nlp, add another table for that but remember that it will increase the volume of your data significantly.Usually it is better not to represent the entire tokenization in the database. 

Please note that even if you are focusing your labelling efforts using active learning, it is still a significant constraint. There are other methods of copying with semi-supervised framework, like co training.The classical reference on co-training is "Blum, A., Mitchell, T. Combining labeled and unlabeled data with co-training. COLT: Proceedings of the Workshop on Computational Learning Theory, Morgan Kaufmann, 1998, p. 92-100" 

I think that you should manage the customer expectations. First, try to define and use a common terminology. You spoke about a low probability of performance while your customer spoke about winning races. Can you tie the terms together? For example, if the probability of performance is the probability of winning a race, once that you have a common terminology you can explain to the customer that the probability of a low performer to win 3 races in not zero and given X horses, you are expecting to see Y such horses, even if the model is perfect. Short term vs. long term issues, should be discussed and incorporated into the concept that you would like to predict. I'm not familiar with horse races but I assume that there are horses that are sprinter and can win a single race while there are horses with more stamina that can win a full match. If there are such differences of interest, they should guide the whole research process. Next, you should explain to the customer that the model being wrong, doesn't means it is not useful. Indeed, what I have in mind in Box's