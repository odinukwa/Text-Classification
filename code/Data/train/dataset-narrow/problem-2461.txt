Sorry this is too long for a comment :) Maybe the complete graph is a trivial graph but it illustrates that e.g. graphs $G = (V,E)$ with large cliques of size $\Omega(\vert V \vert)$ also have exponentially many $0$ obstructed induced subgraphs. And I don't think that $N$ can be easily computed in such graphs. I also think that 3-regularity doesnt help: Let $A$, $B$ be a partition of $V$ each of size $\vert V \vert /2$. The induced subgraph consisting of nodes from either $A$ or $B$ is a cycle and the edges between $A$ and $B$ form a perfect matching. This graph is $3$-regular and every induced subgraph consisting of at least all nodes in $A$ or all nodes in $B$ is $0$ obstructed. 

For arbitrary $b$, Alon, Babai and Itai showed a lower bound on the probability space size of $m(n,\lfloor k/2 \rfloor)$ where $$ m(n,k) = \sum\limits_{i=0}^k \binom{n}{i}$$ which is $\Omega(n^{k/2})$ for constant $k$. They also gave a construction of size $O(n^{k/2})$ in the case of $b = 1$. For $b=1$ there is a paper by Karloff and Mansour which shows lower bounds and upper bounds for arbitrary probabilities, i.e., for $p_1,\ldots,p_n$ with $p_i = P(Y_i = 1)$. E.g., there are probabilities $p_1,\ldots,p_n$ such that the probability space size is at least $m(n,k)$. They also say that $m(n,k)$ is also a upper bound for arbitrary probabilities. I don't known any construction with a better upper bound than $O(n^k)$ which is given by the construction (see here) mentioned by Thomas as a comment. 

There are venues that are interested by elegant proofs of existing results, see for instance the Symposium on Simplicity in Algorithms. So yes, in some cases an elegant proof can be considered as a contribution, especially if it offers new insights. 

During the talk you are not so much interested in, you don't necessarily need all the pre-talk stuff, but always have the paper about the talk open, otherwise you will be lost at some point. Anyway, I think the definite focus would be but not only by listening to talks, also by talking with people, asking questions (to anyone, do not be afraid, usually people are really open to discussion even if they are well-known (might be busier however)). Final advice, be careful, there are many talks so it is easy to lose focus. Some will be much more important for you; it is better not to follow a talk you are not interested in and fully follow one you are interested in than follow half of them (IMHO). 

I'm interested in an computational geometry problem that's sensibly expressed as an infinite dimensional 0-1 integer program. I'm not worried about finding an actual minimum for the objective function, any solution with isn't stupidly big will do. It thus seems natural to apply an approximation algorithm that starts by running simplex or similar restricted to $[0,1]$. I'd expect the solutions usually require only a few hundred dimensions, but any naive restriction of the problem space yields millions of dimensions. As I understand it, good implementations of a linear program solver should be polynomial time in both the dimension and constraints on average cases, but nevertheless this problem chokes GLPK. Should GLPK really choke on a million dimensions? I've therefore started looking for less naive restrictions of the problem space, which lead me to LP-type problems. In particular, there is a claim that Clarkson's algorithm applied to linear programs are equivalent to running the simplex algorithm on the dual problem. In what sense is this true? I find this claim highly dubious with respect to complexity for several reasons. First, Clarkson's algorithm does not exploit any $[0,1]$ solutions with fast average case solutions, but merely randomly chooses pivots. Second, Clarkson's algorithm has running time worse than exponential in the dimension $O(dn + d! d^k \log n)$, which doesn't rule out polynomial time for average cases, but I haven't found that fact yet. As an aside, any nice examples of improving a restriction of an infinite dimensional linear program over time? 

I don't know how you can get or see the bound of $\sqrt{(t+1)(n-t+1)}$ from the original bound $\sqrt{n(n-\vert (2(t-1) -n+1 \vert )}$ but here is the proof that this bounds are asymptotically equal up to a constant factor: First see that (I exclude $t = 0$ because the threshold function is always $1$) $$ n(n-\vert (2(t-1) -n+1 \vert) = \left\lbrace  \begin{array}{ll}  n(2t-1) & 1 \leq t \leq n/2+1/2\\ n(2n-2t+1) & n/2+1/2 \leq t \leq n-1 \end{array} \right. $$ Define $f_1(t) = n(2t-1)$, $f_2(t) = n(2n-2t+1)$ and $g(t) = (t+1)(n-t+1)$. Now you have to calculate the maximal value (according to $t$ within the defined intervalls) of the fractions $f_1(t)/g(t)$, $f_2(t)/g(t)$, $g(t)/f_1(t)$ and $g(t)/f_2(t)$. You can do this with differential calculus or approximation with the help of the graph (with $n$ large enough): $f_1(t)/g(t) \leq f_1(n/2+1/2)/g(n/2+1/2) \leq \dfrac{n^2}{n^2/4} = 4$ $f_2(t)/g(t) \leq f_2(n/2+1/2)/g(n/2+1/2) \leq \dfrac{n^2}{n^2/4} = 4$ $g(t)/f_1(t) \leq g(1)/f_1(1) = \dfrac{2n}{n} = 2$ $g(t)/f_2(t) \leq g(n-1)/f_2(n-1) = \dfrac{n/2}{n/3} \leq 3/2$ This gives you $$n(n-\vert 2(t-1)-n-1\vert) = \Theta((t+1)(n-t+1))$$ and also the wanted result $$\sqrt{n(n-\vert 2(t-1)-n-1 \vert)} = \Theta(\sqrt{(t+1)(n-t+1)}).$$ Is there an easier way to see/get this result? 

Rabie introduced the model of "Rusted Turing Machines" in his thesis: The Power of Weaknesses:What can be computed with Populations, Protocols and Machines (Chapter 7). The idea is that there is a restriction on the number of time the TM can change its internal state because of decay. Rabie introduced the class $Piv(f(n))$, the class of Turing Machines that change their internal state $O(f(n))$ times. An RTM is then an element of $Piv(1)$. In this model, Rabie 

If this is the case, then your problem is NP-hard: you can see it as a generalization of Minimizing total tardiness on a single machine with precedence constraints. Indeed this paper states that for multiple linear chains, it is NP-hard on a single processor. The easy transformation is to take the trees of the form one root, and linear chains starting from the root. However I am surprised because you seem to say that for the case of a single linear chain, you would use Dynamic Programming. I don't see why you would need DP, since it seems to me that when scheduling a single linear chain you do not have much choice because of the precedence constraints: only a single choice. So maybe I misunderstood your problem. 

A Turning machine with insertion and deletion operations can be simulated by an ordinary Turing machine with a quadratic time cost. Do we know how insertion and deletion fit into the polynomial time hierarchy though? In particular, does anyone know a quadratic single-tape Turing machine that cannot be simulated by a linear time single-tape Turing machine with insertion and deletion? I've gathered that separation results are often more powerful, and maybe easier to prove, for the nondeterministic time hierarchy. Is that perhaps an easier place to attack this? If so, that's great because I'm ultimately most interested in rewrite systems anyways. 

We believe code-based public-key cryptography to be post-quantum. In fact, code-base cryptography has the longest history record among post-quantum public-key schemes, but the key sizes seem impractically large, like 1MB in McBits. We use error correcting codes in lattice-based public-key cryptography too, which employ a reconciliation phase like Felipe Lacerda mentioned. In fact, our current best bet for a post-quantum key exchange is the Module-LWE scheme Kyber (lattice-based). 

But I suggest the best will be to check the Complexity Zoo because it has many more information and references on those examples, even Wikipedia Furthermore, as stated in the comments the tight bound when $\alpha=O(1)$, was shown for many problems such as bin packing, machine scheduling (see iris.gmu.edu/~khoffman/papers/set_covering.html). 

Not sure this is your answer (see below) but a bit too long for the comments. I though your problem was something like: $(P|tree;p_i=1|\Sigma T_i)$, where: 

Have you tried to look for what already existed in the field of Scheduling with Communication Cost? If you choose some of the communication cost to be $+\infty$, then it seems to me that it is exactly your problem. A communication cost is defined on an edge between two tasks $T$, $T'$ as: $$ \text{comm}(T,T')=  \left \{  \begin{array}{ll} 0 & \text{if alloc}(T)=\text{alloc}(T')\\ c(T,T') & \text{otherwise.} \end{array} \right. $$ Where the function $c$ is a well-defined cost function (in your case it could be $0$ or $+\infty$, depending on your constraints), and $\text{alloc}(T)$ is the function that states the processor on which $T$ is scheduled. You can read the work of Hanen, and Munier which is more adapted for small communication delays, however since in the general case your communication delays are small, it might be possible to adapt their algorithm. In the worst case you can check on scholar who cites their paper, it may be a good start for bibliographical work. 

There are a wide variety of determent-like constructions. Some like the permanent or immanents are variations on the ordinary determinant for matrices over fields or commutative rings. Some like quasideterminants extend the theory of determinants to non-commutative rings. At least permanents have a rich complexity theory, perhaps the others do to. For any of these constructions, is there a quantum algorithm that asymptotically out preforms the fastest classical algorithms for computing it? I'm asking because some post-quantum crypto systems like $URL$ and code-based crypto suffer from extremely large key sizes due to representing public keys as matrices. A priori, it might be possible to "compress" that large matrix into something like a characteristic polynomial but using some non-commutative analog of the determinant. This approach sounds less viable if quantum computers could compute some analogs of the determinant more quickly. 

There is a linear program for which I want not merely a solution but a solution that's as central as possible on the face of the polytope that assumes the minimal value. A priori, we expect the minimizing face should be high dimensional for various reasons, including that the objective function being minimized is the maximum of many of the constraints : Minimize $\epsilon$ subject to $f_i(\bar x) \leq \epsilon < 0$ with $f_i$ linear and $x_i > 0$ for all $i$ and $\sum_i x_i = 1$. We'd never obtain any centrality-like property form the simplex algorithm of course. Do any of the usual interior point algorithms exhibit such properties though? Do any even guarantee they'll avoid vertices or lower dimensional faces whenever possible? 

I imagine that this can be a lot of work to do from scratch, however it may be possible to have an organization specialized in CS to do this for the conference organizer since it does some advertising for them with not a lot of work. Win-win. I know that this will not be used by every one, but I believe that those using it will really benefit from it (personnal experience, I always had a great time CS). 

Read the summary of the proceedings as soon as it is given to you (bring your laptop, sometimes it is on a USB stick) See which talk may interest you, read the abstract+paper if you are interested Then listen to the talks that you are interested in, after having read the paper (keep the paper open during the talk in case you need to catch up) As much as possible, go talk to people, meet new people, especially people working on similar stuff, ask question. 

I'll concur with JɛﬀE that MS degrees are viewed as "consolation prizes" in the sciences in the U.S. because people usually take them when they fail qualifying exams in Ph.D programs. And who pays to do an MS when they'll pay you to do a Ph.D directly? I'd also concur with David Harris that mathematics might prove the most efficient route to doing serious theoretical work, but this depends entirely upon the program. Ask any math or comp. sci. departments who make offers how they feel about students taking courses outside the department though. I do recommend that you broaden your interests in more applied computer science of course, but do so by reading something. There are mathematically entertaining topics around databases, like Bloom filters, as well as fun applied papers, like the CryptDB articles. 

In fact, I'm probably content with a easy quadratic program that finds the midpoint of the entire polytope since centrality matters more than minimality, just vaguely curious if other linear programming algorithms offer relevant properties. Update : I've reduced the underlying problem to a simple constrained minimization problem solvable with Lagrange multipliers, but the question above remains interesting anyways.