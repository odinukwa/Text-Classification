Otherwise, try to connect to the server local\s12 using SQL Server Management Studio using windows authentication from the same machine that you are running the powershell script. if that fails then there is a connectivity issue. Check firewalls (on the SQL Server machine and on any routers in between the client and server). Check that the TCP/IP network library is enabled on both the client and the server. If TLS is enabled then confirm that the certificates are all correct. 

In SQL Server Configuration Manager, enter the username as MYDOMAIN\Fred, not fred@mydomain.co.nz. You do not need to give the account any permissions at all - Configuration Manager will take care of the user rights and the group memberships and SQL logins and server roles, as well as other tasks like service master key protection. This, however, only works if you use Configuration Manager. Do not use the Services tool to change SQL Server service accounts. Additionally, verify that you are running Configuration Manager with elevated permissions (Run As Administrator). I strongly, strongly suggest you take the account out of the local Administrators group. If anyone somehow compromises your SQL Server then they have just compromised your server. TechNet: Configure Windows Service Accounts and Permissions. 

To correct one common but important misconception: You don't "normalise a database", you normalise a Logical data model. Data design, the process of designing how your data will live at rest, has four distinct phases: Conceptual, Logical, Implementation, Physical. Normalisation is the process of refining the Logical model. Is it required? Yes, if you want your Implementation and Physical models to protect data integrity and to perform well. Tactics that protect integrity can reduce performance so sometimes the normalisation process applied to the Logical model is reversed in the Implementation and Physical models, a process usually called denormalistion. This is a tradeoff - denormalisation means accepting less data integrity protection in the name of better performance. (This also means that questions posted here of the form "is this table normalised?" are meaningless unless the poster includes the conceptual and logical models from which the table was derived.) I suspect the OPs situation is that they learned "a database system" rather than learning "data design". I often see this in people who learned Microsoft Access. They come to believe that the database (the physical model) is the whole data model. 

Free Space (full backup, differential backup, file/filegroup backup, copy only) does not free up space in any files. If anything, it uses up a bit of space in the log file to record its actions. truncates the log, which generally does free up space. A Simple recovery model database truncates its log when a checkpoint happens. File Size None of the backup operations affect the physical size of the files. The only commands which do that are and (in SSMS, right-click a database, choose Tasks, Shrink). 

If it is sqlservr.exe that is listening then the next step is to use SQL Configuration Manager to see what instances of SQL Server are installed. One of them is using tcp1433. 

Short answer: Don't use or or their deny equivalents. They are for backwards compatibility only. Using them will cause issues like the one you are facing. If you want to give principal Alice the SELECT, INSERT, UPDATE and DELETE permissions to all table-valued objects in schema Sales then use the following. 

It is also important to consider what is shared. Failover Clustering uses two or more server nodes sharing one disk array. If the disk array goes down then you lose service, regardless of how many server nodes there are. If the server room where that disk array is located catches fire or floods then you lose service. AlwaysOn Availability Groups and Database Mirroring are a "shared nothing" clustering technology. The database is present on multiple disk arrays in multiple servers. If you have good network links then the multiple serves can be in multiple server rooms, protecting you against fires and floods. 

In addition, anyone working in an Active Directory environment will find it useful to know the basics of Group Policy. As to whether or not you are better off being a "DBA who knows Windows" or a "Windows Admin who knows databases", that depends on the IT industry where you live. For example, I live in New Zealand (a country of 4½ million people). If you want to work in IT here then you have to be a "Windows who knows databases and email and terminal services and printing and troubleshooting and lots of other stuff." The industry is too small for specialists. :-) 

If you want read and write to the local server at all three branches ("multi-master") with site independance (that is, losing the link to a site does not stop that site from operating) then you only have one solution: Peer-to-peer Transactional Replication, an Enterprise edition feature. Merge Replication is a multi-master solution but has a single point of contact - the distributor. If the site containing the distribution role server loses its link then the other two sites will stop replicating. They will continue to serve queries but will not replicate any changes in or out. There are a couple of HADR tools included with SQL Server - database mirroring and availability groups - however with both of these there is only one writable copy of the database. If the site containing the current master loses its link then the failover process will cause that site to stop serving requests to the database and another site to become the new master. 

I can only comment on the SQL Server concept of databases, so this may or may not apply to other RDBMSs. In SQL Server, a database is (1) the boundary for HADR (high availability and disaster recovery) and (2) the boundary for security. Performance? SQL Server shares one buffer cache and one log cache for all sessions for all databases so there is little performance implication of one database vs many databases. If you have 15 megabytes of changes, that means 15 megabytes of traffic to be written to disk, regardless of how many databases it belongs to. Of course, more files does add a very small overhead in management traffic. Connections? One client application makes one connection to the SQL Server database engine service and can then query multiple databases. The client does not need multiple connections for multiple databases. However, since a database is the security boundary then if you want a session to query multiple databases then the login will require permissions set separately in those databases. In summary, usually one database is created for one application. For example, the Payroll database for the Payroll application. 

Here is a higher-level answer, from a BI breakfast I attended two years ago. BI is what people in your organisation are already doing - making decisions based on information. The goal of BI tools is to allow those people to make those decisions faster and with more confidence. Another answer, the one I often use, is that BI tools are there to turn "data" into "information" in a timely fashion. Kimball Group uses the phrases "Deliver data to business users that is easy for them to understand and navigate" and "Deliver fast query performance". 

First - stop using the phrase "Null value", it will just lead you astray. Instead, use the phrase "null marker" - a marker in a column indicating that the actual value in this column is either missing or inapplicable (but note that the marker does not say which of those options is actually the case¹). Now, imagine the following (where the database does not have complete knowledge of the modeled situation). 

¹ I am sure I read somewhere that Codd considered implementing two null markers - one for unknown, one for inapplicable - but rejected it, but I can't find the reference. Am I remembering correctly? P.S. My favourite quote about null: Louis Davidson, "Professional SQL Server 2000 Database Design", Wrox Press, 2001, page 52. "Boiled down to a single sentence: NULL is evil." 

I'm not sure what you are talking about with "gmail profile". SQL Server Database Mail has its own profiles and has nothing to do with any email clients on your machine. When you create a Database Mail profile (SQL Server Management Studio, Management, Database Mail) you must specify the ip address of the SMTP server or servers to use. What you are confiruing is the "outgoing email server" for Database Mail (similar to configuring an Internet email client like Thunderbird or Windows Mail). Those SMTP servers must be configured to allow connection and to allow relaying from the SQL Server machine's ip address, something the email administrators will be able to do. That error message suggests to me that either the above has not been configured or that there is a firewall between the SQL Server machine and the SMTP server that is blocking the traffic. 

The integrity rule we are modelling is "the Code must be unique". The real-world situation violates this, so the database shouldn't allow both items 2 and 4 to be in the table at the same time. The safest, and least-flexible, approach would be to disallow null markers in the Code field, so there is no possibility of inconsistent data. The most flexible approach would be to allow multiple null markers and worry about uniqueness when values are entered. The Sybase programmers went with the somewhat-safe, not-very-flexible approach of only allowing one null marker in the table - something commentators have been complaining about ever since. Microsoft have continued this behaviour, I guess for backwards compatibility. 

I think this is a routing question, not a DBA question. Your router needs to be configured so that if traffic comes in its external interface on port 1433 (for the default instance) or on whatever port your named instance is using then the router sends that traffic to the SQL Server machine on the internal network. This is known as "port forwarding" or "server publishing" or "virtual server", depending on the router (it looks like your router uses "virtual server"). Warning: If you are not familiar with configuring routers then be very careful. Misconfiguring a router can break an entire network. The big steps for what you ned to do are as follows. 

Make sure your SQL Server services and machines are well secured - anything published on the Internet will get attacked. You should have good pass phrases, renamed administrative accounts (if possible), regularly-updated anti-malware software, etc. 

I'm not sure why you've been told that VPN technologies are not generally used. It is correct that they have a performance cost but then again, so does everything! For the amount of data typically moving between an application server and its database server, the CPU overhead of TLS should be negligible, especially on the database server. If that is not the case then look at purchasing SSL-offloading hardware. My preference is two network cards in the application server. The connection between it and the database server goes over a different network than client traffic. If a separate network is not possible then my preference is TLS combined with configuring the firewall on the database server to only allow incoming connections from a small number of computers - the application server (obviously), administrative workstations, operations management servers, etc. 

Nothing should ever access the Staging Database, because that is where partially-cleansed data is stored and no systems should ever access partially cleansed data (except, of course, those processes that finish the cleansing and load the data into the Data Warehouse Database). Now, once the data has been fully cleansed then allowing Operational Source databases to access it can be considered, based on requirements like timeliness of data, speed of access, write-back, and so on. Personally, I think the Kimball one-way flow ( Operational Source → Data Warehouse Star Schema → Data Warehouse Cube → Presentation) is a good idea so it would take a very compelling argument for me to implement Data Warehouse → Operational Source. 

First, the sa password stored plaintext? You should be voting with your wallet. Whoever thinks that is acceptable needs to be put out of business. Here is an anology that might help you explain the issue: Employee Alice needs access to the first floor. Do you give her the master key to the whole building or just the key for the first floor? Answer: You give her just the keys to the first floor. Why? Because it reduces the chance of accidental or deliberate damage. If Alice can't get to the second floor server room in the first place then she will never do anything bad in there. It is the Principle of Least Priviledge. As to why the application needs to use the sa account, that is a question that PerfMon or Extended Events should be able to answer. Create a PerfMon trace using the T-SQL template, maybe filtered by application name. Off the top of my head, here is another argument against using sa: Using the sa account requires the SQL Server service to be in mixed authentication mode. WIndows only authentication is better becase we can leverage all the secure features of Kerberos. 

The keyword in the clause can, as far as I know, only be used to put the results into a table-typed local variable. 

Choosing the number could be automated by using a query and placing the returned number into a dynamic SQL string. This must be placed in a serializable transaction to make sure no other sessions insert new rows while this session is reseeding the value (thanks David). Also, the master must not be changed while reseeding is taking place. One way to deal with this might be to create non-overlapping identity domains. On ServerA, create the identity column as and on ServerB as . That way ServerA's inserts will be 1, 3, 5… and ServerB's inserts will be 2, 4, 6… Of course, if you ever want to create a third server then you are hosed. :-) Better still, don't use in a multi-master situation. It provides no prevention of duplicates. Use a uniqueidentifier or an application-generated identifier instead. 

Short answer: no. Sysadmins are gods. Long answer: Database administrators can use one or more of Server Audit (some editions of SQL Server 2008 and later), Change Data Capture (some editions of SQL Server 2008 and later, very limited information captures), Profiler (almost all editions and versions of SQL Server, including Analysis Services as well), SQL Trace (SQL Server 2008 and later, I think), DML Triggers (only for some operations) and DDL triggers (for SQL Server 2005 and later, and only for some operations). Also Extended Events, as mentioned by Stray Cat below. System administrators can use network sniffing to see everything you do on the network or remote control software to see everything you do on your workstation. All of these can be completely invisible to you. Modern operating systems almost all run on the assumption that you can't hide from sysadmins. If that were not the case then they would be unable to do their jobs. 

First, you can't prevent a sysadmin from doing anything. :-) Any login that is a member of the sysadmin fixed server role operates outside the permissions system - they can do anything. My first thought was to use a DDL trigger to stop the restore. Unfortunately, restoring a database is an operation that does not cause any triggers to fire (see this Connect item). My next idea is to either have an alert on the restore event or have some job periodically looking at the table so you can at least catch the restores quickly and work to fix them. Not ideal, I know. 

Using TCP/IP Sockets, when the SQL client is given a server name SERVER\INSTANCE, for example PRODDB\Payroll, it checks two sockets to work out what port number INSTANCE is listening on. 

Depending on how damaging you consider a power-down to be. :-) This does require xp_cmdshell to be enabled on the server, something that is not the case for the last few version of SQL Server. It also requires that the service account have the shutdown right, which it may or may not have. Enabling xp_cmdshell probably goes outside your 26 character limit. Would you allow multiple injections? 

Foreign Key Constraints A Foreign Key constraint cannot reference a filtered unique index, though it can reference a non-filtered unique index (I think this was added in SQL Server 2005). Naming When creating constraint, specifying a constraint name is optional (for all five types of constraints). If you don't specify a name then MSSQL will generate one for you. 

The actual answer depends on your needs but generally "put data and log on different arrays" is of higher importance than "put tempdb on its own array". Given the number of drives you have available, my starting point would be: 

Microsoft SQL Server Analysis Services uses the term "cube processing". Kimball seems to usually use the term "dimensional model loading". Accordingly, I use the term "ETL" to refer to copying data from the OLTP systems into the staging database (or for copying data from one OLTP database to another) and the term "cube processing" to refer to copying data from the staging database into the OLAP databases. 

Configure both of your SQL Server machines with static ip addresses. Configure your SQL Server named instance with a static port, for example tcp45001, using SQL Server Configuration Manager. The default instance is already using a static port of tcp1433. Open the static port numbers using Firewall with Advanced Protection on both your SQL Server machines. Publish the port numbers on your firewall. Incoming traffic on port tcp1433 should be forwarded to port tcp1433 on the default instance machine; incoming traffic on port tcp45001 should be forwarded to port tcp45001 on the named instance machine.