The point is - TempDB is used in a lot of ways, and it doesn't surprise me at all to see it as one of your busiest, if not the busiest, database. It also doesn't surprise me when I see it as having the highest number of and highest average stalls of all databases at a client's site. It is the nature of its workload sometimes. Looking at some of the things I've mentioned here can certainly help you determine if these numbers indicate a problem and if so, how to go deeper in solving it. 

When you get this message it can be a few things. Some examples: 1.) Read Only Marked Files 2.) Insufficient permissions to the files for the SQL Server Service account 3.) Insufficient permissions to the folders the files live on. You indicated in our chat that this was not read only. You also verified permissions looked like full access. That ruled out these three items. The fact that, from our chat conversation, you were not experiencing this issue when you properly detach a database using sp_detach_db - which actually changes permissions around a bit on the files led me to suggest trying to run SSMS as administrator. Why? Well this article gets into a lot more nitty gritty about some of the impersonation issues here - but my understanding is: 1.) When you just stop the SQL Server Service and move the file around. The owner of the file is the service account that created it. 2.) When you detach the database - you'll potentially notice a permissions change. The owner has changed basically and the file is not really associated with that SQL Server service account the same way. 3.) So it works when you try to attach because the permissions can get assigned, because the security of the file allows it.. But if you just stop the service the security hasn't changed, and you can't attach that if you have UAC enabled unless you are impersonating administrative access. I've probably made the file ownership and access more confusing here but the article I shared probably does a better job :-) But one slight moral is - do a detach next time ;-) Or just be prepared to run SSMS as administrator from time to time when interacting with the OS in various ways. 

So this code may not be exactly what you do, but the point is - you are requiring a positive step to identify where you think you are and checking to see if you are where you think you are. By using a Script you are also requiring yourself to type out items like the database you wish to restore, you are forcing yourself to look things over, pay attention to the SSMS window and see which server, etc. You are also preparing yourself for emergencies, by having a process, having a script and having an ability to use the same approach each time. Now when you have to restore and your CIO is standing behind you rapping her fingers on your cube wall repeatedly saying "is it done yet?" you don't have the added stress of doing the process a different way. 

Obviously not actual code there to create your tables... And your application is somehow joining those tables behind the scenes. Probably on the ClientID. So your user runs a filter on "Acme Company" and behind the scenes SQL Server is doing a JOIN statement. Again - pseudocode 

Is there some component that needs to be reset or restarted after a database restart? Do you have to follow a very specific order of operations for a SQL Server shutdown/restart during maintenance windows? That probably looks like the application or middleware servers going down first, then the database. In a cluster failover you get the DB going down first. What does this mean for you and your company? Do your third party software package vendors support installations on a cluster? They should, it isn't much different but they may have guidance of things to consider during a failover. Do your apps automatically try to reconnect a certain number of times? If not, can they? This may be a good thing to consider in your clustered environment to save some time in the reconnection and getting back to work post failover. 

Out of all of the replication options - Transactional sounds like it is what helps you the most here. It gives the minimal latency and it doesn't need to be (but can be) bidirectional. You aren't merging changes and don't need Peer to Peer. That said - Replication isn't "free" - you need to administer and monitor it and a poor setup could make performance problems worse. You should learn about the best options for performance and the best setup for performance and keep that in mind and keep your replication environment monitored. Transactional replication is "easier" than Merge replication. But that doesn't mean you will always have a smooth sail and can always get away without someone with replication experience helping out. 

This is one of those questions that I love to see. There are a lot of things to add to do to add to migration and upgrade successes and it warms the hear to see the question. You've already covered a lot of things to consider and I'll add a few thoughts but I am looking forward to seeing other answers with more ideas from others. I'm not including things you've already included in the great list you have started and knowing me I'll be editing and adding to this answer as I go. **One disagreement I may have with your approach: Regarding the cluster network name, why not make a new cluster name and just use CNAME to point connections to the old name to the new instance? This lets you make a new name, avoid messing with conflict potential, makes go live night smoother and gives you a clearer and easier rollback. I would do a CNAME approach and change the CNAME once you are sure the old can be taken offline. I may be alone here so I welcome comments supporting that idea or disagreeing. I see a few areas of focus when it comes to migrating. If you make an "outline" of the phases and outline of the areas to focus on comes to mind: Planning, Pre-Migration Steps, Day of Migration, Post Migration. I'll cover some of those sections here and I am hoping others join in with ideas. Planning Thoughts Since you are starting with new hardware. This is a great chance to make sure you are considering all of the best practices that may have been overlooked before. Things like: 

A few thoughts. It sounds to me like you may be able to get away with a couple options if replication is what you end up with. First a word of caution - Replication isn't something that should just be configured in production and enabled/used unless you have some experience with it. This becomes truer with more important workloads and busier systems. Replication can cause headaches if not properly done. I'd invest in mentoring/training/consulting to help you along the path from folks who have been burned by mistakes already. That said. It looks to me like your requirements don't seem to have any 2-way changes that need to be synced up or merged. It looks like you have a bunch of things that need to go one directional. Orders never come in locally, only on the website. Order status never gets generated on the website, only consumed, etc. If that is the case - this sounds like Transaction Replication to me. Merge is more for when you need to have multiple versions that can make changes and receive changes at the same time. The tired old Microsoft example still works - a bunch of sales folk all with a local app that does lead management and a big central server. They can get their leads when in the home office and get data, then they work disconnected and sync up their changes. With Merge you have to deal with conflicts (if the same data is modified in two places, who wins?) Transactional is more what is used when you have changes going in one direction (but you can have it go bidirectional here too even). And you can set replication up to have various tables included or not included in the publication as articles. A couple links to give you a bit more info: Transactional Replication Overview Merge Replication Overview Now if you have data that can be updated on each and needs to be "merged" then merge may be a better answer. When I work with replication - I prefer to stay as close to Transactional as I can, though. It is easier to administer and there is no conflict resolution worries if you don't need it. Depending on your volume - if it is low enough - and your latency - if some lag time is acceptable - you might also consider something more programatic or ETL like. Perhaps staging data and sending it over in batches. Sending messages across as data changes in one side, etc. 

Short Answer: Yes. There are many reasons but the few that stick to mind: 1.) Trust but verify - SQL cares a lot about its environment, the hardware or virtualized system it is on. When I help a company with SQL on VM issues it is normally a misconfigured VM. In many cases the idea of SQL on VM is about to be thrown away. 2.) DBAs should look at memory reservations, alerts and performance conditions, how overcrowded a physical host is ( or isn't) and understand how it works. 3.) DBAs are learning more and more about virtualization. Through excellent blog posts and SQL events, the VMware knowledge in the SQL dba community is rising. An extra set of eyes may help ensure things are well tuned and future proofed. 4.) Prove yourself... If you've done everything right and built a great environment, show it off. Read access to center isn't going to destroy your system and you can show those pesky DBAa everything is up to snuff. And if it isn't? Don't you want to know? ;-) 

So in single_user mode - you would be quite unlikely to have locking problems in that database. It is what it sounds like - single user - and it doesn't mean Single Username - it means one user. So it's used when you as a DBA want to do something that can't be done with others users in. Maybe you are trying to do a repair option of a checkdb. Maybe you are trying to change some object metadata and don't have a better way to kick other users out. Etc. But it really isn't an isolation level "thing" it's really an "access thing" - if you want to eliminate or avoid locking - there are a lot of things to look at such as better performing queries, snapshot isolation levels, improvements to code, etc. 

This is an installation option. In the installer GUI or command line you can leave it as enabled or disabled and automatic or manual. By default in most versions of SQL it is not enabled and started. I always change it when installing - or forget and do it shortly thereafter. 

Pros - It is the natural key, it makes sense and it will likely be searched on, I presume? Cons - The default behavior (which is totally changeable) is for a primary key to be the clustered index. An alphanumeric doesn't make the best candidate because inserts can cause page splits because they aren't set on an ever increasing value like an identity column. The Int identity column will take less space (4bytes) compared to the character data (40+bytes for the unicode) . This makes your other indexes larger since the clustered key is part of them. If you ever change how you identify your customers and make customer codes, this all breaks - going with a surrogate insulates you from those type of changes. In this situation, I tend to optimize for the insert performance and go with an identity column more often than not for the clustered key and primary key. I really like integer clustered indexes. (Now I know your question was not about clustered index, it was about primary key... You could still choose some other column to be the clustered index and make this your primary key, you could also put a unique constraint on this and treat it as a natural key but not make it your primary key). I would at the very least index this with a unique constraint and treat it like a natural key. I just don't know if you really need to make it your primary key. Kimberly Tripp is a trusted resource who has a lot to say about primary keys and (more so) clustered keys on her blog - $URL$ This is all just my opinion - YMMV. 

When there are connection issues like you describe it is normally around something like not actually being on a domain but trying to use windows auth, a password mismatch, not having access to a default database selected for a login or an actual network issue like a firewall. These are two older posts but they are fantastic resources for troubleshooting connectivity to SQL Server. This first link is from the SQL Server protocols team and describes the steps to troubleshooting connectivity issues. This second link is from the SQL Server connectivity team blog describing the various login failure messages and their states and how to troubleshoot each state. 

You should set it to automatic if you intend to use it. I am not sure why Microsoft decides to leave the default to manual after installation but I always change it because I always schedule maintenance. So you are on the right track to start and set to automatic.