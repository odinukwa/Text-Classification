Note that I can't see a convenient way to use a interface. The interface depends on a shared instance of the , so as a result, there needs to be some static data in the Singleton. As a consequence, an interface is the wrong language structure to use. An abstract class may be better. Still, the usage examples for the Multiton above are simple (in some common 'utility' class, or somewhere shared): 

The method seems contrived, and will also do multiple things depending on multiple states. Should the methods just return instead of falling through to the next check. Odd, but the current code could be right, just unusual. The way they call and implies those methods can be private. Now, about the overall state system. It's hard to get a feel for the actual state manipulation. You only have one state, and it is controlled mostly by static manipulations. The actual state changes are 'wrapped' in a class you do not show to us: 

Have you done any testing/metrics that indicate that the current system you have is actually a problem? I would be very surprised if it is. If the server is sending data as infrequently as that, there does not seem to be any reason to 'pool' the objects at all. The overhead of 'cleaning' an instance, storing it, recycling it, repopulating it, and repeating with it is probably just as overwhelming as simply creating a new instance and GC'ing the old one. Unless you can already identify 'specifically' that object creation is a significant part of an existing performance problem, I would suggest that you are looking in the wrong places for performance improvements. EDIT - hypothetically... Answering hypothetically: no, I don't believe your solution is enough... 

I don't like this question of yours. To me it looks like a step backwards compared to your other recent questions. Edit: About Locking Consider some user of your class, they want a stack that is thread safe.... and they need to coordinate between some threads. They need a convenient object to synchronize on. STACK is there... so they use it. Note, I would never normally write code like the following.... but, the following code will never finish... Even though, at face value, it looks fine? You understand now? 

You ask many questions on CodeReview, which in itself is good, but you have to start helping the reviewers actually review your code. You have this habit of dumping code and expecting a review. It does not work that way (very well). For a start, let's review the ideal process for a 'real' review: 

The better alternative is the second alternative, because the first alternative may only write the data to disk when the Garbage Collector clears up the stream. The file will remain open for all that time too, leading to long holds on file handles in the OS. i.e. the first one may produce unexpected results. The second alternative is an improvement, but you should consider using a "try with resources" block, which has been around for the past 6 years in Java 7 and 8... The try-with-resources is a far more elegant way to approach this problem, and it makes the exception handling "trivial"... 

OK, now for the real issue, the storage of the Enums in a convenient-to-access system... First, I recommend that you change your Enum to have two parameters in the constructor: 

Essentially, iterate through the (sorted) items, and if the item is the same as the previous item, ignore it. This is very similar to Brythan's solution (which gets a +1 from me), but note the different way I handle the first element in the list, compared to him. If the list is not sorted, and you cannot have buffer, then the best system to use would be the ListIterator, and not the plain Iterator. You have much more flexibility with that.... 

The style otherwise looks good. The concept you are solving, given the information you have given, is being solved in what amounts to a reasonably efficient manner, all things considered. It is an \$O(nm)\$ solution, if you double either the number of rules, or number of keys, your solution will take twice as long, and if you double both, the solution will take 4 times as long. If you do this just once and then reuse the Mapping, then it should be fine. If you have fewer than say 100 members in each of the rules/keys, then you should be fine. If you have more data than that then you should consider building up the data directly in to the map at the same time as you initially populate the rules/keys. So, you are right to not like the solution because it scales badly, but, if your data is small, be pragmatic and not worry about it too much in this instance. 

Fundamentally, I think, you have solved a problem that does not exist, and your solution does not behave as I would expect. Your use-case could just as easily be solved with a ListIterator Update: Since these lists tend to be small, I would simply leverage the existing toArray implementation in the underlying list, and provide a thin wrapper that is not a general concept: 

With the above code, you do two sorts, and 1 loop through each value. The net result is that your performance complexity is simply \$O(n \log n)\$ where \$n\$ is the longer of the or variables. By sorting each side, you give yourself a significant algorithmic advantage. 

Putting them together, you get an expression that matches any character, if there is one, before an asterisk, as well as the asterisk, and any other asterisks that follow it, and finally any other character, if there is one. See this in an ideone here: $URL$ 

The initialize method on the Cell will calculate, and store, the array of neighbours. This will substantially reduce the number of times they need to be calculated. If you have the one-off initialization then Cell->is_sink can take no parameters again. Apart from the restructuring of the is_sink, and the persistence of the neighbours array, I have a nit-pick about some of your loop-conditions.... you often have code like: 

All in all, the only things i can find to criticize are some naming conventions, formatting, and some pedanticness that is really minor. It's a great solution to the problem. Thanks 

The method would be easy to implement. The method can have the single concern now of the basic computation. Note that Java8 has some nice streaming tricks: 

The concept in your code is fundamentally good, but there are a few things I would recommend you change. Static initializers First up, I like static initializer functions when the static component is non-trivial. So, for example, this code: 

I'm just going to poke at some of your unix conventions and validation. Your general handling looks polished. I like the way that you handle whether this is an interactive terminal, but wonder whether it would simply be better to use the more standard check for or the flag ( reference here ). If you are going to use to tell, you should probably rely on the exit status rather than the text message. The way you find the user directory is great, and is similar to what I normally do too. There are some variations on it, but it is more than good enough. This next one is the one that concerns me most though: 

Notes It took me a little while to track down exactly where the GWT content is stored, about.... 30 minutes to find FireBug, install, learn how it works, track down where things go, understand how GWT loads the scripts from an array of text which it loads separately, etc. A person who is more familiar with JavaScript, the execution model in a browser, and the debug tooling could probably do it much faster. The obfuscated code bases would make that harder, but the concept would still be the same. 

The buffer is static, which makes this code non-re-entrant, and not-thread-safe. you do not reset the buffer before using it. A previous call will invalidate subsequent calls. Use-once-only. You get the backing array at the end with the call. This returns all 64,000 bytes whether they were used or not. 

Your SQL is written to suggest you want it to do the last option, create two in-memory data sets, each of them pre-computed to contain just the name-concatenated values, and then you only perform the cross-product on these tables.... but, just because you wrote the SQL that way does not mean that Oracle will do the process that way.... ... have you done an explain-plan to figure out which option it has taken? My guess is that it has chosen , with a second possibility of . The reasons? 

On the other hand, your variable names are great, and the basic concept is sound. Here's my version of your code: 

Now, you have removed all non-prime values by setting the index value to be false. There are ways to make that faster, but, the important thing is there's no scanning for values, or shifting things either. Reverse Your reverse method converts things to a string, and reverses that. I would prefer an int-based system: 

Focusing on the regex side only.... The use of the UNGREEDY modifier on your pattern, and then internally reversing that, is odd. 

So, it is unusual to make the recommendation I am going to make here, but, in this case, is Python the right tool for the job? I took your code, and looked through it, and also ferada's version. Really, they are pretty good. I suspected that there is a problem with many writes to the file, so I 'batched' the writes up in to 1000-at-a-time IO operations, expecting that to be where the real time is spent. There was not any change in performance (I tested by using just 5 character wide outputs). Using the 5-char wide output, I calculate about 5.778 seconds on my computer. times that for 8 chars, and that's more than 28 hours to run. As an experiment, I did the same in Java. Now, Java is not the fastest compiled language, I know, but the process is compiled, not interpreted, so, what could it do? 

Bottom line is that you have only given a fraction of what is needed for a good review, so, I will take a stab at doing a review with similar feedback.... A good alternative approach for your code is: 

Note though, that the above implementation has horrible handling of issues that may come from invalid input. Still, the entire implementation is significantly simpler than yours. 

Your assumption that the entire dataset is read in to memory is not necessarily accurate. The JDBC clients I have worked with (DB2, MS-SQLServer, Sybase, Oracle, etc.) each default to, or have an option to limit the size of the client-side buffer. You only have a small amount of data on the actual client, and the ResultSet process fetches more data when you iterate through it. There is no need to add another level of buffering. Are you sure the data is all being returned? Have you inspected the configuration options for your JDBC Client? 

the class you are tying to build should have a lot of properties/fields that you would normally want to pass as a constructor you want those properties to be 'immutable' - constant for the life of the car (and this implies that there should not be 'setter' methods for those values) the class is constructed from many places so having a more convenient system is... convenient 

You are not clear in your question what the purpose of your class is... the code suggests that, for an input list of size , that once you have 'pulled' k items from the that the histogram of the results will exactly match the histogram of the input frequencies. Additionally, for every 2k elements you pull, the relative frequency will remain unchanged. Unfortunately, your code does not actually do a very good job of randomizing the values. For example, consider data with the input frequency of (i.e. 1 and 9 values). What your code is supposed to do is: For every 10 times we pull values from the :