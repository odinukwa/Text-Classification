You are right, the covariance matrix should have n^2 elements. However, since cov_{i,j} = cov_{j,i}, there is no need to have a repeated feature cov_{j,i} if cov_{i,j} is already accounted for. Hence there will be only n*(n+1)/2 = 12*13/2 = 78 unique covariances and thus only 78 unique covariance based features (n of those will be variances). 

Visualizing large datasets is a long standing problem. One of the issues is to understand how we can show over a million points on a screen that has only about ~ million pixels. Having said that, here are a few tools that can handle big data: 

This idea will most likely increase the bias in the model. Let's assume that the model has non-zero bias in the model. In this case, when it assumes its predictions to be true, without confirmation from an Oracle as in active learning, the bias of the model increases. In common terms, if the model has some amount of bias in its predictions, and it uses its predictions to learn on, the bias in the model can only increase. This issue does not arise when there is 0 bias in the model to begin with, however, in that case, there is no need to learn any further! Note that this is a highly intuitive answer but I cannot think of an argument against the intuition :-) I will appreciate any feedback on this. 

We can easily quantify given that your model is correct. You just need to run it through linear regression and it will give you the coefficient for *. If you would like to model click through rates, you would have to train a classifier. So you would have to fit a logistic model that models: 

Neither of real analysis or measure theory are necessary for data science. Most of the mathematics you need to know is at the undergraduate level and these courses should suffice: 

The other rule is redundant. However, in the case of a multi-class response variable, we would like to have all the rules written out so we exactly know the likelihood of the rule implying each of the different classes in the response variable. To keep things consistent, this is also done for the case when the response variable is binary. 

There are two straight forward (vanilla) ways without going for any fancy featurization: Clustering: Run a clustering algorithm. Something like k-means should work well with this kind of a dataset. While doing this, I would not feed the day_of_week information into the clustering algorithm. I would suggest running k-means (after normalizing each of the columns). Choose a small number of clusters that is easy to investigate (or you could use the number of clusters that maximizes the BIC). Investigate the clusters to understand membership by day_of_week in each of these clusters. Multi-class Classification: Treat the day_of_week as the response that you would like to predict. Build a decision tree of a fixed depth to predict the day_of_week given the columns. By examining this tree, you can easily tell, which decisions led to a set of leaves being labeled Sunday vs the set of decisions that led to a set of leaves being labeled Monday. These decisions will also help you understand the similarities between different days. 

With increasingly sophisticated methods that work on large scale datasets, financial applications are obvious. I am aware of machine learning being employed on financial services to detect fraud and flag fraudulent activities but I have a lesser understanding of how it helps to predict the price of the stock the next day and how many stocks of a particular company to buy. Do the hedge funds still employ portfolio optimization techniques that are right out of the mathematical finance literature or have they started to use machine learning to hedge their bets? More importantly, what are the features that are used by these hedge funds and what is a representative problem set up? 

Data Science is, as others have noted, a much broader term than machine learning. Applying Machine learning techniques is one aspect of data science. Data Science, more generally, is the science of deriving knowledge from data. The term was coined back in 1960 and kept evolving to describe the flow and interplay of problem definition, data collection, data transformation, data modeling/ analysis, and decision making. So to answer your question specifically: 

Now, using the function/ method from the package in , first create the dataframe (that will be used for final plotting) as follows: 

I have a big data problem with a large dataset (take for example 50 million rows and 200 columns). The dataset consists of about 100 numerical columns and 100 categorical columns and a response column that represents a binary class problem. The cardinality of each of the categorical columns is less than 50. I want to know a priori whether I should go for deep learning methods or ensemble tree based methods (for example gradient boosting, adaboost, or random forests). Are there some exploratory data analysis or some other techniques that can help me decide for one method over the other? 

I have a non-function (not in closed form) that takes in a few parameters (about 20) and returns a real value. A few of these parameters are discrete while others are continuous. Some of these parameters can only be chosen from a finite space of values. Since I don't have the function in closed form, I cannot use any gradient based methods. However, the discrete nature and the boxed constraints on a few of those parameters restrict even the number of derivative free optimization techniques at my disposal. I am wondering what are the options in terms of optimization methods that I can use. 

Note that you can also add with if you would like the lines to be connected based on Location and Variable. 

This is not a regression but a multi-class classification problem. The output is typically the probabilities of all classes for any given test instance (test row). So in your case, the output for any given test row from the trained model will be of the form: 

Data typically exists. What typically does not exist, is ground truth (in the case of classification). Such ground truth is typically always collected manually and crowd sourcing plays an important role. For example, think about Face recognition that Facebook does. Before automatic tagging was available, Facebook allowed users to manually add tags to create a set of labeled data. A more general way of doing this is through Amazon's Mechanical Turk (Amazon's marketplace). See the tasks listed there. Some of them are clearly related to manual generation of labels that will later form the basis of a learning system. Most research in academia is creating methods and you can demonstrate how well it works on existing datasets. However, when a new company is launching, for example, a fraud detection platform, they have to deal with manually labeling transactions as fraudulent or not. Sometimes, this is done when a report comes in from the customer and sometimes by manual eyeballing by analysts. As you would imagine, there has been a lot of academic interest in understanding the quality of results obtained through crowd sourcing and continues to be an active area of research. 

EDIT based on comment Note that if the data does not already exist in the above format, it can be changed to this format. Let's take a data frame provided in the original question and lets assume the dataframe is called . 

For problems where the data represents online fraud or insurance (where each row represents a transaction), it is typical for the response variable to denote the value of fraud committed in dollars. Such a response value might have less than 5% non-zero values denoting fraudulent transactions. I have two questions regarding such a dataset: 

GBMs, like random forests, build each tree on a different sample of the dataset and hence, going by the spirit of ensemble models, produce higher accuracies. However, I have not seen GBM being used with dimension sampling at every split of the tree like is common practice with random forests. Are there some tests that show that dimensional sampling with GBM would decrease its accuracy because of which this is avoided, either in literature form or in practical experience? 

Machine learning aids data science by providing a suit of algorithms for data modeling/ analysis (through training of machine learning algorithms), decision making (through streaming, online learning, real-time testing that are all topics that come under machine learning), and even data preparation (machine learning algorithms automatically detect anomalies in the data). Data Science stitches together a bunch of ideas/ algorithms drawn from machine learning to create a solution and in doing so borrows a lot of ideas from traditional statistics, domain expertise and basic mathematics. In this way, data science is the process of solving a use case, providing a solution as opposed to machine learning that is an important cog in that solution. 

This problem is one of estimating the lag. Once that is estimated, you could create additional features representing the lagged values and move forward with "sequence mining" as you have already suggested in the question itself. For each variable, Var_i, you will have to estimate its lag l_i. This lag can be calculated by estimating the order of a Markov chain with seven symbols (you could use either BIC or AIC to estimate this order; both would require calculating likelihood of candidate orders and pick the order that maximizes either of these criteria). Once you are done calculating the order of the Markov chain for each of the variables, then you could represent your dataset such that each row will have the current value of Var_i and its preceding values, all the way back to its estimated lag l_i. While this methodology is laborious, it pays rich dividends as its automated and parsimonious way of representing the necessary information. 

Tableau: you could use their free desktop tool. Tabplot: built on top of ggplot2 in R to handle larger datasets. See this review for 5 other products that can help you do your job. 

It doesn't seem like they provide the boosting that is described in the paper that you might be referring to. But it should be straightforward to sew in with the idea of boosting using some base tree packages in R like (for example). 

The variable represents the answer to the first question. One straightforward way is to allow for all possible categories in this variable. For example, if there are 5 options in this answer, you will have to treat it as a categorical variable with 2^5 = 32 categories. However, the number of categories increase exponentially with the number of options (check boxes) provided for the answer. In that case, it might be better to restrict the number of categories to, for example, 5. This can be done by leaving the top 4 choices/ options (by count) as they are and treating every other choice as "other". 

The best way to learn data science is through problem solving. I suggest you to head over to Kaggle and work through the for-knowledge problems. To get a good start on Machine Learning problems, acclimate yourself with the tree package in R. This will help you understand how decision trees work, and building upon that, how random forests, gradient boosting machines and other sophisticated tree based algorithms work. Then, there are SVMs and deep learning models. To get an understanding of unsupervised learning problems, learn k-means and employ it for clustering. Other general concepts/ ideas to understand are: 

Feature Engineering is at the heart of Machine Learning and is rather laborious and time consuming. There have been various attempts at automating feature engineering in hopes of taking the human out of the loop. One specific implementation that does this for classification problems is auto-sklearn. It uses an optimization procedure called SMAC under the hood to choose the appropriate set of transforms and algorithm (and algorithm parameters). Note that Trifacta offers a really easy to use tool for data transformation. It has a highly intuitive GUI that allows to set up transformation/ feature engineering maps. There is also a free trial version that can be used for reasonably sized problems. 

cross-validation overfitting, regularization bias-variance trade off dimensionality reduction/ variable selection generalization error ensemble learning 

For both the above problems use an ensemble model. Consider both a random forest and a gradient boosted machine. Both these models will use the independent variables and predict the Hospital time. Additionally, through variable importances, you can obtain which variables are the most important ones and have the most impact in predicting the output. 

The most obvious way of visualizing this is to have the number of computers on the Y-axis and the size of the dots representing the percentages. The categories (or sites in your case) can be represented on the X-axis. The image below shows an example where the Y-axis represents a continuous value (can be mapped to number of computers in your case), the X-axis represents a discrete value (can be mapped to sites in your case), and the size of the dots represents another attribute (like percentage in your case). I have used the package for this. 

Having created the dataset. We will now generate the plot. First, create the bar plot on the left based on the counts of software that represents usage rate. 

All distributions in the package in R are associated with a loss function. For example, when we set , the loss function chosen internally is the logistic loss function. Can anyone explain how multi-class classification works with and the loss function that is being used for it i.e. when we set ? Is it using one-vs-all or all-vs-all under the hood for doing its multi-class classification? 

Your dataset can be viewed as a directed graph. The party's location (latitude and longitude) can be denoted as a node and the directed edge can denote who referred whom. Once the dataset can be viewed as this, the problem boils down to joining co-ordinates with lines. 

Finally, using , you can create the plot such that the colour represents the value, the shape represents the Location and the size represents the Variable. This simple one liner can help you produce such a plot: 

Try the 1998 KDD Cup dataset. Its a regression problem with categorical and integer predictors. For your task, you could either treat integer predictors as categorical or ignore them completely. 

Obtain a predictive model that can be used for prediction. Which variables seem to be the most important ones to be used. 

One way to use both metric1 and metric2 in order to find anomalies in metric3 is to consider residual analysis. In your case, this would require, creating a predictive model with metric1 and metric2 as the predictors and metric3 as the response variable. Then, calculate the residuals for metric3 as its predicted value subtracted from its true value. Now, you can report the all members of the lowest decile [or any other percentile] as one kind of an anomaly and all the members of the highest decile [or any other percentile] as another kind of an anomaly.