Ok since you want numbers, I can only guarantee you one number: 1. That's me. But some things can't be put into numbers, and that's how people feel about certain things. When you release updates for different platforms at different times, and assuming the user are already waiting for this update, the users who have to wait longer get the feeling of being on an second class platform. On the other hand, if you release "premature" updates only on one platform and not on the others, then this community might feel like a test environment, like rats. I personally don't like it to play unfinished/broken games unless I am aware of that and in the mood to do so. I think if you explain the the community that on iOS the validation process generally takes much longer then the desync of releases might have no noticeable affect on the community. Just be honest with the community, and don't lie to them. A different story is it when your game has multiplayer. Then you should make sure that everybody has access to the same version at the same time. It's not nice when somebody can't play with his friends, because he does not have access to the same version. 

don't omit the pushMatrix and popMatrix and use glMultMatrix instead of glLoadMatrix. Otherwise you throw away all your camera information that was used to draw the grid. $URL$ 

write the code, compile it, run it. when it works on all gpu's your planning was correct. If it pleases you: I have my approval that your planning is reasonable and worth testing out. and make shure you always use ther correct active texture unit. That active texture thing (glActiveTexture) is there to confuse people how are new to opengl. you do not pass the texture id to the shader uniform, it is always the number of the texture unit. 

in my experience in games tells me it is far more acceptable to not have a collision when objects intersect, than the other way round. So i would suggest you to put your collision boxes inside of your objects. Then you can also compose your collision of the player of several boxes, eg one for the wings and one for the body. Keep in mind it is not a problem at all when they overlap, just make as few as possible. Then there are also more collision shapes available then just boxes. For your asteroids you could use circles, because they match the shape of your asteroids much more than boxes. 

I am writing a game project in Go and I am using an OpenGl 3.3 core context to do my rendering stuff. At the moment I have different types of renderers. Each renderer has it's own pair of vertex- and fragment-shader, a struct of uniform- and attribute locations. A struct with glBuffers, vertex-array-object, and numverts (int) which contains all data required to render one object (mesh). Last but not least a constructor to create and initialize the attribute/uniform locations a method to load a mesh into mesh data and the render method itself. I am absolutely not happy with this design. Every time I want to create a new simple shader, I have to write all this code, and I haven't found a way to make the overhead of a new shader smaller. All I was able to make with the help of go reflections is to automatically get the attribute/uniform location based on the variable name in attribute/uniform location struct and to automatically set the attribute pointers based on the layout of the vertex struct. another thing that I don't like, is that when I want to implement for example the functionality of the function glClipPlane of the fixed function pipeline, I need to add the uniform to each shader separately and I need to set this uniform in each shader and I need to implement the discard of fragments in each fragment shader Are there some common practices that significantly reduce the code overhead of a new shader? Are there good shader pipelines that you can recommend me to take a look at? Are there some good practices to add functionality to several shaders at once? 

you can do the simple scaling trick. yea too lazy to explain, just look at the image and ask if you don't understand In this example I just scaled by a factor of 2. Higher values give longer shadows. 

you can try to project the screen borders back into the scene. In rendering projection matrices are invertable, because depth isn't thrown away. so you can use (matProjection*matCamera)⁻¹*v to project a point from screen coordinates back to world coordinates. the vertices [1 -1 0 1] [1 1 0 1] [1 1 1 1] [1 -1 1 1] are the quad that is on the right border of the screen. used in the formula above and you a a quad in world coordinates that you can use for collision. 

It sounds like you're after what are usually called "Decals" (not to be confused with this legacy decal shader!). They're typically used for things like bullet holes, posters, or anything you wouldn't want to paint into a texture. To do that, you can instantiate things called billboards on your wall parallel to its surface, slightly away from it so it doesn't Z-fight. Whatever is instantiating them should keep track of the instances, then either deactivate () or destroy them after a certain amount of time (). Alternatively, there is at least one ready made asset in the Unity Store that will do the same thing. Just search for "Decal" 

You can do this using plain mouseup/down tests and raycasting, and the same pattern applies to many other situations, like marquee selection. 

I think you want Dijkstra's algorithm, which is also used for internet routing. In short, you must create a graph where each edge is a path from one node to another. These could be pre-calculated if some of your geometry is fixed. For each starting point, you will order the nodes according to their distance from the destination. Then apply Dijkstra's algorithm through the graph to build routing tables for each one. The first route in the table will be the shortest path. Each route after that will be a variation depending on which nodes are blocked. These extra routes could be calculated as necessary, provided their order is maintained in the table (they should be ordered by the sum of their edge lengths). You'll refer to these routes again when a node is changed (blocked/unblocked). You can then add and remove nodes by splitting edges and moving the new midpoint, then recalculating from the origin node. 

But I prefer not to reinvent the wheel - so is this "correct," or is there some accepted way of doing this that's totally different? 

It depends on whether they have registered the name as a trademark. The liability with respect to similarity ("My Product" vs "MieProduct") in specific instances is a matter for lawyers to advise and courts to decide. 

VI. Use that blend mode to draw brush to canvas, again shown with checkers to indicate the transparent area. 

Updates flow down from the Component Manager, and entity association flows across. So, you'd start from something like this: 

WASD is just a de facto standard for FPS games. If you play an FPS from the pre-Half Life era, you'll end up with all sorts of variations. That said, the analogue from 5DoF to 6DoF is jump/duck, which varies for each game. Case in point, Quake used jump and duck for up/down movement underwater, even though it used arrow keys for forward/back and turning, with alt+left/right for strafe. The original 6DoF FPS, Descent, had flight-sim like controls and used the +/- keys for up/down. 

Okay, here are the settings you'll want to use to additive blending, plus some extra info in case anyone else wants to do something similar. I'm not sure how to get this to work with pre-multiplied alpha (or if that's even necessary, since I'm guessing you plan to do mask-based lighting), so you'll want to make sure that if you have any resource based "brushes" (also known as "cookies") that you set the pre-multiplied alpha property like so: 

OpenAL is an open standard (meaning anyone can read and use it) with several implementations, some of which are proprietary (generally patent protected). OpenAL Soft is one such implementation which is under the LGPL. Creative Technology maintains another confusingly called just OpenAL, which is now proprietary. You must obtain a license from the developer for any proprietary version of OpenAL, but LGPL versions are free as in speech, but not necessarily open source, and that requirement doesn't extend to derivative works. It is always a good idea to include any relevant license information pertaining to your software as distributed, but in this case you must include the LGPL license in the combined work (Section 4 - GPL). This is because the LGPL is based on the GPL (see LGPL Preamble): 

You can add wall-specific information to the Wall class, Tile specific information to the Tile class, and further refine conditions in the "CanGo" method. For example, when a wall is actually a locked door - say, a Door class. In order to draw this, you would start with some arbitrary tile - say the tile in the middle of the current camera position. Then move toward and to the left of the camera according to the size of the tiles. Then do a breadth-first traversal of the IMapFeature nodes, drawing each wall/tile in the order encountered. A* will work on this structure, though you would obviously need some modifications to handle something like locked doors. If you wanted to, you could also maintain a spatial index of the tiles, which would implicitly include the walls, in order to find out which tiles were within camera bounds. You'd still only need to pick a starting tile and a distance to traverse based on tile size.