There are also locality issues since, in a distributed system, each node runs its own instance of a distributed algorithm and has only a local view of the network due to being directly connected to only a small number of other nodes. (Typically you would want a node degree of $O(\log n)$ to make the system scalable.) These issues come into play when maintaining global state such as counting the number of data items, finding the maximum, etc. 

Suppose that $G$ and $H$ are both expander graphs on the same node set $V$ with a second largest eigenvalue of $\lambda_G$ resp. $\lambda_H$. Let $G\cup H$ be the graph on $V$ with the smallest set of edges such that both, $G$ and $H$, are subgraphs of $G \cup H$. 

Kuperberg recently proved that knottedness (of a given knot diagram) is in NP ∩ coNP, assuming that the generalized Riemann hypothesis is true. A knot diagram is close enough to a graph that I think this counts as an answer to your question. 

Tim Gowers is a fan of this kind of thing. See specifically his exposition of Razborov's method of approximations. In his introduction, Gowers references my expository article on forcing, which is a (not entirely successful) attempt to do the same thing for forcing. Forcing is normally thought of as a technique in logic and set theory, but it has found its way into TCS occasionally. It comes up in the study of bounded arithmetic and propositional proof complexity (Krajíček and Takeuti are two researchers who have pursued this connection), and the concept of a generic oracle is related to the concept of a generic filter. 

Suppose that you've implemented a shared memory machine $M$ that only satisfies eventual linearization, defined as follows: in every run $\alpha$ of $M$, there exists some point in time $T_\alpha$, such that linearization holds from time $T_\alpha$ on. Note that there is no upper bound on $T$. (*) (This is an artificial liveness counterpart of the standard safety property definition of linearizability.) Such a shared memory implementation wouldn't be very useful to the programmer: Note that if only eventual linearizability holds, there are no guarantees whatsoever on the consistency of read/write operations in any "early" prefix of a run (before the unknown time $T$). Or, in other words, whatever has happened until now, you can still extend the current prefix of a run to one that satisfies eventual linearizability. (*) If there was such an upper bound, then eventual linearizability would become a safety property. 

In Impagliazzo's imaginary world Heuristica, P ≠ NP but all NP problems are easy on average for any samplable probability distribution. In Impagliazzo's paper, he implies that if you do manage to find a hard instance in Heuristica, it won't take much more effort to solve the instance than it took to find the instance in the first place. 

When you refer to the Crescenzi-Kann compendium, I'm not sure if you're referring to the book or the website. The book is out of date but the authors try to keep the website continuously updated. It would seem that the logical starting point is to approach Crescenzi and Kann with your proposal. 

Finding a maximal independent set in a distributed network of $n$ nodes with maximum degree $\Delta$. There's a known lower bound [3] of $\min(\Omega(\log\Delta),\Omega(\sqrt{\log n}))$ that holds for randomized and deterministic algorithms. The following is a simple randomized distributed algorithm [1] that proceeds in synchronous rounds. (In a round, every node $u$ can perform some local computation and send messages to its neighbors. These messages are guaranteed to be received before the start of the next round.) 

The notion of "timing out processes" refers to the ability of knowing when to conclude that a process must have crashed. If you have a completely asynchronous system, then it does not help to equip processes with perfectly synchronized clocks, as these cannot be used to distinguish a slow process from one that has crashed. 

Minc conjectured and Brégman proved that if $A$ is a 0-1 matrix with $r_i$ 1's in row $i$, then the permanent of $A$ is at most $$\prod_i (r_i!)^{1/r_i}.$$ There is a short proof in Alon and Spencer's textbook The Probabilistic Method, but arguably the "book" proof is Jaikumar Radhakrishnan's proof using entropy (J. Combin. Theory Ser. A 77 (1997), 161-164). It's not at all obvious from the statement of the result that the concept of entropy lies under the surface here. 

The title question arose in the course of discussing a question on MathOverflow. Obviously, from the space hierarchy theorem we know that not only is it false that $\mathrm{DSPACE}(n^b) \subseteq \mathrm{DSPACE}(n^{b/2})$, but there is a proper inclusion in the other direction. But once we limit the time budget of the left-hand side to $n^a$, I'm no longer sure what we can say. If the question can't be answered unconditionally, then can we answer it conditional on some standard hypothesis or relative to some oracle? 

Some important challenges that practically all distributed data structures face, are handling dynamic changes, implementing a scalable design, and being fault-tolerant. This includes finding answers to questions such as: 

The answer is negative. Consider the line $a - b - c - d$ and the MDS $M=\{a,d\}$. The edge $(b,c)$ isn't covered so either $b$ or $c$ need to be added to $M$ to yield an MVC. However, minimality requires that we then need to remove either $a$ or $d$. 

DHTs: To give you some pointers, you might want to look at distributed hash tables (DHT) such as Chord, CAN, Tapestry, and Pastry. Skip Graphs: Since you mentioned skip lists, you might be interested in skip graphs, which is a data structure providing range-queries and $O(\log n)$-time operations for lookups, inserts, etc. The advantage of a skip graph (vs a skip list) is that a skip graph contains an expander as a subgraph with high probability. This implies that routing can be done efficiently (i.e. link congestion is low) and that the skip graph remains connected even if a lot of nodes fail. 

Note that Rabin himself does not use the term "computational complexity" in this paper. MathSciNet also turns up a couple of earlier reviews that use the term "computational complexity," but these seem to be spontaneous and sporadic occurrences. 

To phrase the question another way, what are some exceptions to the heuristic that if can't figure out contradictory relativizations then it is easy to resolve the equality question outright? 

I think that there is a fundamental underlying difficulty with the question you are asking here (and that you asked in your related question about incomprehensible languages). Roughly speaking, it seems you're seeking a language $L$ such that 

It can be shown that this algorithm terminates in $O(\log n)$ rounds with high probability, by arguing that half of the remaining edges are deleted in every round. In contrast, the fastest known deterministic distributed algorithm [2] takes $O(n^{1/\sqrt{\log n}})$ rounds and is considerably more complicated. 

(*) A run of a distributed algorithm is a sequence of configurations. A configuration is a vector of the local states of the processes. Each process executes a deterministic state machine. Any correct consensus algorithm must eventually reach a configuration where every process has decided (irrevocably) on the same input value. A configuration $C$ is $1$-valent if, no matter what the adversary does, all possible extensions of $C$ lead to a decision value of $1$. Analogously, we can define $0$-valency. A configuration $C$ is bivalent if both decisions are reachable from $C$ (which one of the two is reached depends on the adversary). Clearly, no process can have decided in a bivalent configuration $C$, as otherwise we get a contradiction to agreement! So if we can construct an infinite sequence of such bivalent configurations, we have shown that there is no consensus algorithm in this setting. 

Let me respond to your suggestion with a counter-suggestion: Why don't you try setting up a business, acting as a middleman between amateurs and experts? Amateurs pay to have their proofs evaluated. You find an expert and pay the expert to evaluate the proof, taking a cut of the money for your middleman role. Trying to run such a business is the most reliable way of finding out whether your idea is a feasible one. 

This is not a complete answer, but I can point you to some relevant papers and also partially explain why it's not so easy to extract an answer to your specific question from the literature. Let me start by asking, why do you want to know the answer to this question? Typically, the people who find themselves caring about this sort of issue are those faced with actually implementing a high-performance FFT for a practical application. Such people care less about asymptotic complexity in some idealized computational model than about maximizing performance under their particular hardware and software constraints. For example, the developers of the Fastest Fourier Transform in the West write in their paper: