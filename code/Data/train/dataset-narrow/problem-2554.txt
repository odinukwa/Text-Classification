As you suggested, a collision margin gives the physics engine some room for error in detecting contacts and resolving contacts, prior to actual penetration. This helps with the appearance of realism as objects do not visibly poke through the ground, etc. There are nuances here for the calculation of the threshold for when contact constraints apply Baumgarte stabilization during solving to help separate bodies. As DMGregory notes, collision margins may be added to planes or triangles to provide an arbitrary thickness to primitives that are mathematically infinitely thin, effectively turning them them into a volume. Perhaps more important is that many collision detection engines use the Gilbert–Johnson–Keerthi (GJK) algorithm to perform convex-v-convex collision detection. This algorithm is efficient when performing collision detection between disjoint pairs - even if they are very close to one another. The moment the two convex shapes actually overlap each other, the cost of the GJK algorithm increases dramatically. Furthermore, to generate an effective contact point the Expanding Polytope Algorithm (EPA) must be run, which is even more costly. As a result many physics engines (Havok, Bullet) create a shell around the convex hull, which is the usually named something like collision margin or convex shell. 

I'm not sure what you mean 'these equations'. I think you mean the inputs into equations. I would store rotations as quaternions (x,y,z,w) as mentioned. Interpolating from a key-A to key-B would use a slerp or nlerp function ($URL$ Translation is stored as x,y,z and uses a simple linear interpolation. Scale is stored as x,y,z and also use a simple linear interpolation, although in hierarchical animation non-uniform scale is a complicated matter often requiring the squash/stretch rotation to know in what coordinate space the scale is to be applied. Once you have that working, you can investigate various forms of compression and key frame reduction to reduce the cost. For example, keep track of which bones have identify scale (1,1,1) or no translation (0,0,0) and store flags to avoid performing calculations for those bones. This might be per skeleton, or per animation used on a skeleton. If a bone has a rotation that is consistent for the whole animation, just store it once. You can also store data quantized at a lower precision to get better compression. Here's an article about different techniques: $URL$ 

Absolutely - in modern GPUs the overhead of each draw call is very high, so your proposed plan is likely not of great benefit unless the model is exceptionally high polygon. Imagine a case where the arm, head and foot of a model are visible because the model is 'looking into' the frame. You would have three draw calls - one for the arm, one for the head and one for the foot. This defeats any optimizations. Remember that the hardware performs viewport clipping - the triangles are transformed into clip space and potentially rejected there prior to any rasterization. This is relatively efficient. Furthermore, meshes are split on material boundaries, which might be spread all over the model which complicates the value of your approach. Often a single vertex buffer is shared among multiple materials - a collection of shader parameters and textures. Each material must be issued with a unique draw call. Artists often make every effort to use a single material per model to minimize draw calls but it isn't always possible. 

Identify the axis of minimum penetration using the SAT (this defines the reference face) Find the most anti-parallel face on the other shape (this defines the incident face) Clip incident face against the side planes of reference face Keep all vertices below reference face 

For edge-v-edge he suggests: "If the axis of minimum penetration is realized by an edge pair compute the closest points between the two edge segments and are done." 

I don't recommend using TCP for reliable transport in games - the latency is too high, and the overhead of the transport will chew up your bandwidth. Instead you need to implement a reliable messaging system on top of UDP that uses ACKs to handle the sending and resending of messages. Consider the following situation - two machines are sending state information back and forth. Machine A on Seq#100 creates object. In the packet it sends out, labelled #100, it includes the message. Machine B receives the packet, and respond with the ACK of seq# 100. It creates the object. Machine A receives the ACK of 100. Later, Machine A destroys the object on seq# 200 and the same thing happens. A packet goes out with seq#200, Machine B acks it, and Machine A receives the ack. This is great but how does it help us? Well if Machine B never receives seq#100, it can't ACK it, so machine A continues to send all the messages from previous sequence numbers until the ack is received - so seq# 101 has the create in it, seq#102 has the create in it, etc, until the ACK is received. In fact, because of latency, seq#101 is likely created and sent before the ack for 100 could ever be received, which helps decrease latency. Should the ACK from Machine B to machine A be lost, then Machine A keeps sending the message until an ACK is received - it can't assume it has been received so it has no other choice but to do so. Now if there is such dramatic packet loss that nothing is ack'ed since #100 when the object is destroyed, then the outgoing packet will contain both the create AND the destroy data, which is perfect. The messages should be put into the packet in sequence number order, so they maintain their timeline. The key here is to put your messages into a queue, sorted by time, and to send the messages out until they are acknowledged. By having them in a queue, you add new messages to the back, and prune old messages from the front. Adding them is easy because they are always sorted in the queue. Further, the ACK sent from machine B to machine A should be bundled with whatever data that machine B needs to send back - there is no point to make a distinct packet - that's a waste of bandwidth. Note that this is only for reliable messages. Unreliable messages work into this system just fine but are only sent on the seq# they are generated- they are not held onto and resent for multiple frames. What I like about this system is that your create/destroy messages are kept in sync with the packet data for position, health, etc. which would be included in the same packet. In a peer to peer game each peer would send the same data, so the packet layout might look like this: Sequence # Acked Sequence # Number of messages in the packet each message additional payload data such as delta compressed snapshot data Packet size grows as latency increases, but there are some ways to help mitigate that, such as slowing down the frequency you send packets at. I think Gaffer talks about that strategy. I hope some of this makes sense. 

Encapsulate the data into a class, and hide the worldCache & localToParentCache behind accessors. Use setters on the scale, position & rotation to set the dirty flags, Whenever the position, rotation or scale change, to invalidate the caches. When a parent moves, invalidate the worldCache (but not the localToParentCache). On access rebuild the matrices as required. Here are some sample methods: 

This is a perfect example of DRY being a concept that makes people afraid of making the right decision, in some cases. Here repeating yourself makes sense. Each cell should contain a list of objects in it and each object should contain the cell it is in. You will want both to correctly perform queries and operations. For example: given a list of common unit types, find all the cells they are in, or given a cell, find all the units in the surrounding grid cells. The key is to centralize the setting of these values in such a way that they are always synchronized. Using debug asserts in various places to confirm this is true is good engineering practices. 

Are you sure that the matrix operations are causing the slowdown? In C# most Vector libraries implement their types as structures, which are not generally allocated using the garbage collector, but are instead created on the stack. The temporaries in your example don't cause allocations that need to be collected. 

While I hate one word answers I think this one is a "no". Unity is designed to only use floats as far as I can tell. You'll have to convert doubles to floats yourself when using unity functions which likely defeats the purpose of using doubles to begin with. 

One major advantage is that many collision detection operations are more efficient when performed at the origin. A classic example is box vs sphere. When done in a box's local space the tests are very simple axis aligned distance point-plane tests instead of the more costly non-axis aligned planes. Furthermore objects moving through space may not actually hit anything for a frame so there is no value performing the costly operation of moving the shape and all of its vertices each frame. Instead the transform of the rigid body is updated every frame - it has to be to integrate linear and angular velocities, anyways - and then the axis aligned bounding box of the shape(s) is updated. Only when potential intersection exist do the costly tests get performed. If we had to move the vertices every frame it would be far too expensive. Good collision detection libraries are a series of steps that start from simple (AABB overlap) and move towards more complex (specific pair-wise collision agents) in the hopes that the higher level tests will prune out the costlier ones. 

In my experience, the right place to make the API break is on the model data, augmented by providing some cross-API methods to generate dynamic vertex content. I would create a MeshData class - but I wouldn't use virtual methods. As you have indicated, your can only have one API active at a time through compilation. I would simply put a MeshData class in your DirectX class and a MeshData class in your OpenGL class. Client code can hang onto a MeshData instance and access API-specific operations when you need them without arduous forwarding of functions via base classes, or casting. Selection of which header/cpp to include will be done via your makefile and a preprocessor macro - including global/MeshData.h will either include dx/MeshData.h or ogl/MeshData.h - but the #ifdef is then limited to writing it once in the MeshData header. As for performing the rendering, as you have indicated you can either visit the whole scene (if you need to worry about concatenation every frame) or deposit instances of each Mesh in a list from your update. If every object truly needs updating, then I would simply call a function 'QueueForRendering( MeshData *data,params... ) during the update. But that may include a lot of redundant updates on nodes that don't need it. SceneGraph issues are a different topic really, but I would be inclined to keep my list of active 'models' somewhere else so the renderer only has to traverse those models, and it can be written in an API-specific way if there are considerations there (shared VB, material batching, etc). 

This will only do steps when enough time has passed to perform them, and only as many as are needed (in the case of a long lag). 

Given then two dimension array board[][], when a player places a piece, loop over the adjacent elements and check if there is a piece in one of them that matches the player's colour. If so it is a valid move. The difficulty is in the edge cases - you need to make sure if a piece is placed near the edge of the board you don't check outside the board boundary. 

I recently went through this exercise and evaluated both on a couple of platforms. I found that my engine had many moving objects clustering on top of each other. As they moved around the Sweep and Prune (SAP) implementation caused too much sorting and overlap callbacks every frame. It was crippling my platform, which is not too powerful. I did all the tricks - use quantized floats stored as integers to allow for integer compares, stab entries, etc. My research covered reading up on how commercial engines were doing it as well as discussions on a number of message boards. In the end I chose a dynamic bounding tree based on the implementation in Box2D. You can find a great implementation here: $URL$ A number of people have used this version and converted it to 3D with much success. The key is that the objects, when put the tree, have their AABB inflated in the direction of their velocity by some platform and use-case amount. When the object moves, if it doesn't travel outside of the AABB, the tree is not modified. This effectively amortizes the cost of tree updates over multiple frames. This structure is normally paired with a OverlappingPairCache - a cache that stores A/B pairs of objects. Every frame that cache is walked and collisions performed between them. I would encourage you to start with the bounding tree. If I had I probably would never have looked back. Instead I burned a lot of time tinkering with my SAP before giving up and looking elsewhere. This seems consistent with what people are seeing in Box2D, Bullet, PhysX and maybe even new versions of Havok. If you want more details on implementation details, let me know. The thread on the Bullet Physics Research forum I found most interesting is here: $URL$ And Randy Gaul's write is really good, too: $URL$ 

I think you need on the server a generalized subsystem that tracks/accelerates spatial queries for authoritative objects. Loot would be one of these where the loot is registered with the system. Objects would then be replicated to clients in range. Testing would be cheaper because the system accelerates it via spatial structure and multi-frame coherence. The objects on the server deemed near a player could perform authoritative actions and send results to clients as well. From your comments you indicate that the world is a 3D-heightmap. You don't indicate number of players, but here is a basic rundown of how I would build a system: Given that the size of the world is fixed, I would define a grid size (that will need to be tuned later for performance reasons). I would then start with a two-dimensional grid where each cell of the grid is a list of items in the grid. I can imagine that you might have an interface or base type for items in the grid to make things easy to start with. Now we need to keep track of which cells to look at each frame - which cells are occupied by one or more players and need to be searched. There are two strategies here - you could simply iterate over each player and check the cell the occupy. Run the list of items in the cell and update them against the player. The problem I see here is that you would update an item twice if two people are in the cell, so you might need two steps: 1) maintain a dictionary of all the cells and a list of each player in the cell so you can iterate over the dictionary and update each cell regardless of number of people in it. 2) Run the list of players to update their relationship to the items in the cell. I only consider a two-dimensional cell because most heightmaps can't support overhangs, so you can discard the z (height) dimension. There are so many optimizations to be made here but I think starting out that way would help. 

I'm not sure what solver strategy you use but most physics engines including box2d solve islands of bodies that share manifolds. Only the interrelated bodies that can have impact on one another are solved together. Each island can be solved on a separate thread. Since island sizes change based on the number of interrelated bodies a job scheme consuming islands as threads are free is a good strategy. Havok works this way. Box2d is a good place to look for insight for building islands even if solving isn't multithreaded. The other thing you could do is perform batches of narrow phase pair-wise collision in seperate threads since the input data is small and constant until the next step. The outputs are again limited to only the two in the pair.