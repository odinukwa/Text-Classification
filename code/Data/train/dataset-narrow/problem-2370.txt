One version of the sharp or additive space hierarchy theorem is that for Turing machines (and a number of other deterministic sequential computational models) $\mathrm{Space}(f-ω(\log(n+f))) ⊊ \mathrm{Space}(f)$ for space-constructible $f(n)$, where space is measured in bits. Are there reasonable computational models where we have an analogous sharp version of the time hierarchy theorem: $\mathrm{Time}(f+O(1)) ⊊ \mathrm{Time}(f+O(g))$ for $g$-time-computable $f(n)$ with $g(n)≥\log(n+f(n))$? Since I already know one (arguably reasonable) class of models with the sharp time hierarchy, I am including it as an answer, but I am still interested in related results, including - other models with the sharp time hierarchy theorem - whether this model or other models were studied in literature - weaker forms of time hierarchy, especially if stronger than $\mathrm{Time}(f) ⊊ \mathrm{Time}((1+ε)f)$ - whether there is a speed-up theorem for Turing machines with a fixed number of binary tapes that would prevent this form of sharp time hierarchy. Footnotes/References: * The additive/sharp space hierarchy theorem can proved using "Halting Space-Bounded Computations" by Michael Sipser (1978). See also "Improved time and space hierarchies of one-tape off-line TMs" by Kazuo Iwama and Chuzo Iwamoto (1998). * A weak form of additive time hierarchy is discussed in "Computational Models with No Linear Speedup" by Amir Ben-Amram, Niels Christensen, Jakob Grue Simonsen (2012). 

Yes, sliding blocks, Rush Hour, and many other puzzles with reversible moves are (deterministic) linear space complete. By contrast, many puzzles with irreversible moves, including Sokoban, are complete for nondeterministic linear space. Sliding blocks is in linear space because the state has $O(n)$ bits and the moves are reversible (and undirected graph connectivity is in logspace). In the other direction, the above paper first uses a reduction from nondeterministic constraint logic (NCL), and then reduces QBF to NCL. The NCL to sliding blocks reduction is space-efficient (at least if the wiring of NCL is based on a two dimensional grid of cells with $O(1)$ gates per cell, which suffices). However, even if the QBF to NCL reduction is efficient, known simulations of linear space use approximately quadratic size QBF. Fortunately, a later paper offers a different reduction. "[Parameterized Complexity of Graph Constraint Logic]"($URL$ by Tom van der Zanden shows that NCL (even for bounded width corresponding to an nxk board) can simulate H-Word Reconfiguration (which can be used to simulate string rewriting with a finite set of reversible length-preserving rules). While the paper does not say it, H-Word Reconfiguration can be used for space-efficient (up to a constant factor) simulation of reversible Turing machines. In turn, given unbounded time, reversible Turing machines (as a computational model) are space-efficient. (However, it remains open whether some problems in $\mathrm{TimeSpace}(n^{O(1)}, O(n))$ take exponential time for linear space reversible Turing machines.) For appropriate board representations, we even have, for every problem in linear space, a deterministic finite state transducer that reduces instances to solvability of sliding blocks. The transducer gives an nxk board; if we want an mxm board, linear time logspace reductions remain sufficient. 

Regarding oracle separations, there is an oracle with EBPP = BPP = EXPNP, and an oracle with P=⊕P (and hence EBPP=P) and BPP=EXPNP. One construction of BPP=EXPNP oracle (including the one in BPP wikipedia article) is to choose a relativized EXPNP complete problem, and proceeding recursively on the input size (for that problem), fix results for problem instances of that size, and then provide answers to that problem if queried with the input and a filler (of appropriate length) that has not been fixed. For EBPP=EXPNP, instead of almost always giving the correct answers, we can give just enough wrong answers to make the counts exactly right. There is also an oracle in which the zero error analog of EBPP (exactly 1/2 probability of reporting failure) equals EXP (and an oracle with P=⊕P but ZPP=EXP). The P=⊕P and BPP=EXPNP oracle is noted here. In addition to being in BPP and in C=P, EBPP is in ⊕P since we can reduce probability to number of witnesses and then tweak that number. In the unrelativized world, BPP probably equals P, but the evidence is even stronger for EBPP. EBPP depends on the exact number of paths in a way that, unless an unexpected cancellation holds, appears essentially impossible to harness. 

Under basic assumptions about the code and timings, every model of this type satisfies the sharp time hierarchy theorem. The computation of $f(n)$ must erase the worktape at the end (except for the input data if do not use a read-only input tape); we can either define time-constructibility to include the erasure, or use a model where the erasure can be done in linear time. Efficient Example and Implementation For every $k$, there is such a model that can simulate in linear time $k$-tape Turing machines (time depends on the machine simulated), and in turn can be simulated in linear time by a $k+2$-tape Turing machine, which suggests that the full-speed condition is computationally reasonable. (Since a model defines timings, an implementation is an important evidence of reasonableness.) I am not sure if the two tape gap can be reduced to 0. To get such a model, we fix an interpretation and timings of the code such that an efficient universal Turing machine can simulate the code in linear time, with the constant factor independent of the code size. For example, we can interpret the code as a Turing machine, but with the state transition specifying the relative offset $d$ (in bits) and taking $k(d+1)$ time, and with a convention and timings for TimedVirtualize (also, note that without random access tapes, head/cursor position is important). In addition to the tapes in the model, the machine uses a tape storing the timers (and if implemented, space bounds). While at time $t$, we might have $O(t / \log t)$ timers, we only need the antimonotonic case (timers that are set later trigger sooner), so we can use a counter for the innermost timer, and efficiently use subtraction to update the previous timer when TimedVirtualize finishes. TimeSpace Hierarchy If we define space usage in a reasonable way (and allow space bounds in virtualization), we can even get models with a sharp TimeSpace hierarchy theorem: $\mathrm{TimeSpace}(T+O(1), S+O(1)) ⊊ \mathrm{TimeSpace}(T+O(g), S+h+O(\log T))$ for $\mathrm{TimeSpace}(g,h)$-constructible $(T,S)$ with $g(n)≥\log(n)$ and $T(n)≥n$. The need for precise timings makes the space hierarchy less sharp here (at least to the extent we can currently prove the hierarchy). 

If E does not have i.o.-$2^{o(n)}$ circuits, then P=BPP, but this does not tell us about the fine-grained containments between $\mathrm{Time}(n^a)$ and $\mathrm{BPTime}(n^b)$. Are there reasonable computational complexity conjectures that imply a fine-grained relationship between levels of P and BPP?  Same question for approximation problems, including CAPP (approximate probability that a given circuit will accept random input). Also, if for every $ε>0$, $\mathrm{Time}(O(2^n))$ does not have i.o.-$O(2^{(1-ε)n})$ circuits, do we get $\mathrm{BPTime}(O(n^a))⊂\mathrm{Time}(O(n^{2a+ε}))$? If not, what is the best known Time bound here? (If 'worst-case hard' to 'pseudorandom' conversion can be done efficiently, a prng against $\mathrm{Size}(n^a)$ uses a length $\mathrm{lg}(n^{a+ε})$ seed (without uniformity, $\mathrm{lg}(n^{a-ε})$ does not suffice), leading to $\mathrm{Time}(\tilde{O}(n^{2a+ε}))$ to iterate over the seeds.) Below is what I know. Based on circumstantial evidence, a reasonable guess is that - $\mathrm{Time}(O(n^a)) ⊄ \mathrm{BPTime}(O(n^{a-ε}))$ - $\mathrm{BPTime}(O(n^a))⊄\mathrm{Time}(O(n^{a+1-ε}))$ - $\mathrm{BPTime}(\tilde{O}(n^a))⊂\mathrm{Time}(\tilde{O}(n^{a+1}))$ or the weaker $\mathrm{BPTime}(\tilde{O}(n^a))⊂\mathrm{Time}(\tilde{O}(n^{2a}))$. Intuitively, randomness is useful when there is an adversary, and to a limited extent, a problem instance can act as an adversary. If pseudorandom generators (prng) are as easily constructible as possible, we get $\mathrm{BPTime}(\tilde{O}(n^a))⊂\mathrm{Time}(\tilde{O}(n^{a+1}))$: Run the BPTime machine with a prng $\tilde{O}(n)$ times to get (for example) $2^{-10n}$ error probability, and hardcode any exceptions (an infinite number would violate a form of pseudorandomness). Furthemore, in $\mathrm{Time}(\tilde{O}(n^a))$, we can iterate over $ω(\log n)$ seeds, so under a strong derandomization assumption (that holds relative to a random oracle), an adversary needs superpolynomial time to find inputs where the deterministic algorithm fails. (The adversary controls the input $x$ (of length $n$) and can run the prng (which uses $x$), but the prng still appears sufficiently random relative to $x$. Using known cryptographic functions, we have plausible candidates for making $n^a ω(\log n)$ pseudorandom bits in $\tilde{O}(n^a)$ time using the input $x$ (of length $n$) that work for $O(n^a)$ (without the tilde) computations using $x$, despite the polynomially bounded adversary choosing $x$.) As for $\mathrm{BPTime}(O(n^a))⊄\mathrm{Time}(O(n^{a+1-ε}))$, there are three lines of evidence: 

I expect the answer is no, but I could not actually construct a counterexample. The difference is that in $∩_{ε>0} \mathrm{DTIME}(O(n^{2+ε}))$, we might not be able to pick an $O(n^{2+ε})$ algorithm uniformly in $ε$. By a dovetailing argument (for example, see this question), if there is a c.e. set of Turing machines $M_i$ deciding a language $L$ such that $∀ε>0 ∃M_i ∈ O(n^{2+ε})$, then $L$ is in $\mathrm{DTIME}(n^{2+o(1)})$. Given a Turing machine, whether the machine runs in time $n^{2+o(1)}$ is $Π^0_3$-complete. Whether a language (given a code for a machine recognizing it) is in $\mathrm{DTIME}(n^{2+o(1)})$ is $Σ^0_4$ (and $Π^0_3$-hard); whether a language is in $∩_{ε>0} \mathrm{DTIME}(O(n^{2+ε}))$ is $Π^0_3$-complete. If we can prove $Σ^0_4$ completeness (or just $Σ^0_3$-hardness) of $\mathrm{DTIME}(n^{2+o(1)})$, that would solve the problem, but I am not sure how to do that. The problem would also be solved if we find a sequence of languages $L_i$ such that * $L_i$ has a natural $O(n^{2+1/i})$ decision algorithm (uniformly in $i$). * Each $L_i$ is finite. * Not only is the size of $L_i$ undecidable, but an algorithm cannot rule out $w∈L_i$ much faster than $O(n^{2+1/i})$ (for worst case $w$), except for finitely many $i$ (dependent on the algorithm). I am also curious whether there any notable/interesting examples (for $∩_{ε>0} \mathrm{DTIME}(O(n^{2+ε})) \setminus \mathrm{DTIME}(n^{2+o(1)})$ or an analogous relation). 

Multitape Turing machines, 2-tape machines, one-tape and a stack (even with oblivious movement), sufficiently uniform circuits. RAM-based machines (with variations on the cost of a memory access), Turing machines on binary trees, and related models. 

Using $n$ calls to the halting oracle and time $O(n^2)$, you can compute the first $n$ bits of the Chaitin's constant. Using the $n$ bits of the Chaitin's constant and unbounded time, all queries to the halting oracle of length approximately $≤n$ (depending on the representation) can be eliminated. Finally, the result can be obtained in bounded time by querying the halting oracle. Additional Details: In the relativization of $\mathrm{PSPACE}$ here, the oracle tape counts against space usage, so a $\mathrm{PSPACE}^\mathrm{Halt}$ machine can only use queries of length $n^{O(1)}$ (though the number of the queries can be exponential). The digits of the Chaitin's constant can be used to get the number of machines of length $≤n$ that halt, and by simulating all these machines until the right number halts, we can find out exactly which of the (roughly $2^n$) machines halt. The digits of the Chaitin's constant, or just the number of the halting machines, can be computed in $n^{O(1)}$ queries by bisection.