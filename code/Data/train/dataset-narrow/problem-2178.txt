Suppose $H$ has at least two vertices. The family of all $H$-free graphs is hereditary on induced subgraphs and the property of being $H$-free is non-trivial, where a property is non-trivial if it is true for infinitely many graphs and it is false for infinitely many graphs. Thus, the result of Lewis and Yannakakis [1] applies, showing that for all $H$ with at least two vertices, the problem is NP-complete. [1] John M. Lewis, Mihalis Yannakakis: The Node-Deletion Problem for Hereditary Properties is NP-Complete. J. Comput. Syst. Sci. 20(2): 219-230 (1980) 

This does not completely answer your question. I don't believe any upper bounds are known with respect to the average degree of a graph, but here is a result about graphs where the maximum degree is bounded. Björklund et al. (2012) showed that the number of connected vertex subsets of a graph with $n$ vertices and maximum degree $d$ is at most $(2^{d+1}-1)^{n/(d+1)}+n$. For maximum degrees 3,4, and 5, this gives the upper bounds $O(1.9680^n)$, $O(1.9874^n)$, and $O(1.9948^n)$. 

I would simply do as follows: Start with a tree decomposition $T$ of $G$. Each time you would like to compute the number of self avoiding walks on $G$ passing through $u$ and some other node, do dynamic programming on the tree decomposition $T'$ obtained from $T$ by adding $u$ to each bag of the tree decomposition. The width of $T'$ is at most the width of $T$ plus one, and you cannot hope for anything better in general. 

Williams (2009) [arXiv version] gives a randomized $2^k poly(n)$ time algorithm finding a path of length at least $k$ in a graph on $n$ vertices. The paper contains pointers to previous deterministic and randomized algorithms. Many of these algorithms can probably be modified to find a $k$-path whose endpoints are given in the input. 

You are looking for an output-polynomial algorithm for enumerating minimal transversals of hypergraphs (or hitting sets for set systems). According to Golovach et al. (ICALP 2013), 

One example is Maximum Independent Set. It is NP-hard to approximate the problem with ratio $n^{1-\epsilon}$ (Zuckerman, 2007). However, Bourgeois et al. (2011) give a simple $n^{1/2}$-approximation algorithm with running time $O^*(2^{\sqrt{n} \log n})$. Here, $n$ denotes the number of vertices of the input graph and the $O^*$-notation hides polynomial factors. Another example is the Bandwidth problem. Dunagan & Vempala (2001) design an algorithm with approximation ratio $O(\log^3 n)$ and running time $O(n^{\log n})$. To my knowledge, the best known polynomial time approximation has approximation ratio $O(\log^3 n (\log \log n)^{1/4})$ (Lee, 2009); however, no $\omega(\sqrt{\log n / \log \log n})$ lower bound is known for the best approximation ratio achievable in polynomial time. 

Daniel Král and Jiří Sgall answered your question to the negative. From the abstract of their paper: 

It is proved in [1, Corollary 7], that 3-partition is strongly NP-hard when the integers $a_1, \ldots, a_n$ are all distinct. The bounds $B/4 < a_i < B/2$ are not imposed in [1], but this should not make a difference. [1]: Heather Hulett, Todd G. Will, Gerhard J. Woeginger: Multigraph realizations of degree sequences: Maximization is easy, minimization is hard. Oper. Res. Lett. 36(5): 594-596 (2008). DOI 

It turns out that $f(k)=3$ for all $k\ge 3$. Recursive diamond graphs are extremal. See arXiv:1203.4483 for a proof. 

PhD positions in TCS and Discrete Math are sometimes announced on TheoryNet and DMANET. These are mailing lists to circulate announcements and questions regarding conferences, workshops, seminars, books and jobs in these areas. 

As Tsuyoshi points out in a comment below, there is no reason why a solution to the problem has to be a stable matching. So, the approach of this answer does probably not work; especially since I believe that Tomer's answer is correct. 

This definition is a slightly modified version of the definition from Pinar Heggernes' lecture notes. 

There is a recent survey Habib and Paul (2010). A survey of the algorithmic aspects of modular decomposition. Computer Science Review 4(1): 41-59 (2010) that you should check out. 

In other words, $v$ should have degree $k$, instead $k-1$ in your definition. I personally prefer the bottom-up definition, but this is just a matter of taste: 

EDIT: As pointed out by Ryan in the comments, a problem may have a nonuniform algorithm with running time $O(2^{\epsilon n})$ for any constant $\epsilon > 0$ (the algorithm has access to $\epsilon$) but no uniform $2^{o(n)}$ time algorithm. As a SERF reduction is a family of Turing reductions, one for each $\epsilon>0$, I conclude that they can only be used to obtain $O(2^{\epsilon n})$ time algorithms from $O(2^{\epsilon n})$ or $2^{o(n)}$ time algorithms. 

To keep it simple: a non-deterministic machine can optimally choose the outcome of each coin flip (if you like the analogy with a probabilistic machine). You might also imagine that it executes the computation for each outcome of the coin flip in parallel. 

For an instance with $n$ men and $n$ women, the trivial upper bound is $n!$, and nothing better is known. For a lower bound, Knuth (1976) gives an infinite family of instances with $\Omega(2.28^n)$ stable matchings, and Thurber (2002) extends this family to all $n$. 

An early example is the W[2]-hardness proof for Tournament Dominating Set (Theorem 4.1 in [1]). The reduction is from Dominating Set and it constructs a tournament with $O(2^k n)$ vertices, where $n$ is the number of vertices of the dominating set instance and $k$ is the parameter. [1]: Rodney G. Downey and Michael R. Fellows. Parameterized computational feasibility. In P. Clote and J.B. Remmel, editors, Proceedings of Feasible Mathematics II, pages 219-244. Birkhauser, 1995. 

Suppose $G$ is a triangle-free star-cutset-free circle graph. I will show that $G$ contains no vertex with degree more than 2. Therefore, $G$ has at most $n$ edges. Consider a circle representation $C$ of $G$. A set of chords is parallel if no two of them cross but there is a line crossing all the chords. Property 1: $C$ has no 3 parallel chords. Proof. Suppose $C$ has 3 parallel chords. Condider the vertex $v$ corresponding to the middle chord. Then, $N[v]$ is a cutset. This proves the property. For the sake of contradiction, assume $G$ has a vertex $v$ of degree at least 3. Then, the chord corresponding to $v$ intersects 3 other chords. Since these 3 chords intersect one line, they are either parallel or two of them intersect. Due to Property 1, two of them intersect, which means their vertices form a triangle with $v$, which contradicts $G$ being triangle-free. 

This only complements David's answer, who shows that the number of connected induced subgraphs can be enumerated with polynomial delay. Since a complete graph on $n$ vertices has $2^n$ connected induced subgraphs there is no hope to obtain any algorithm faster than $2^n$ for general graphs. For bounded-degree graphs, however, slightly better bounds are known for the number of connected induced subgraphs. Björklund et al. (2012) show that any graph on $n$ vertices with maximum degree $\Delta$ has at most $(\beta_{\Delta})^n$ connected induced subgraphs, where $\beta_{\Delta} = (2^{\Delta+1} − 1)^{1/(\Delta+1)}$. 

So, $N(k)$ does not exist if $k\ge 3$. Král and Sgall also show that $N(2)=4$. Of course, $N(1)=1$. Daniel Král, Jiří Sgall: Coloring graphs from lists with bounded size of their union. Journal of Graph Theory 49(3): 177-186 (2005) 

The fastest known algorithm has running time $n^{O(\log n/ \log \log n)}$ [1]. Your question is reposed as an open problem in [2, page 4]. [1]: T. Feder, P. Hell, D. Král, and J. Sgall. Two algorithms for general list matrix partitions. SODA 2005, pages 870–876. [2]: E. D. Demaine, M. Hajiaghayi, D. Marx. Open Problems -- Parameterized complexity and approximation algorithms. Dagstuhl Seminar 09511 (2009). Available here. 

Is the following graph class known in the literature? The class of graphs is parameterized by positive integers $d$ and $t$ and contains each graph $G=(V,E)$ such that for each vertex $v\in V$, the subgraph of $G$ induced on all vertices at distance at most $d$ from $v$ in $G$ has treewidth at most $t$. It generalizes the concept of locally bounded treewidth, and it seems useful when searching for local structures in graphs. 

Let $\mathcal{G}_k$ denote the set of all graphs that contain two vertices $x,y$ and $k$ edge-disjoint $x-y$ paths. Define $f(k)$ to be the maximum such that for every graph $G\in \mathcal{G}_k$ there are two vertices $x',y'$ with $f(k)$ independent $x'-y'$ paths in $G$. Here, a set of paths is independent if none contains an internal vertex of another. Are any lower bounds for $f(k)$ known in the literature? In particular, I need $f(3)=3$ in an algorithm where I need to find some obstructions in a graph. This is not hard to prove, but I'm wondering whether it is known in the literature, maybe as a special case of a more general theorem. As a different formulation, how large can you make $f(k)$ in the following statement? 

The best known upper bound of the treewidth in terms of the number of edges of a graph is as follows: the pathwidth (and therefore also the treewidth) of any graph on $n$ vertices and $m$ edges is at most $13 m / 75 + o(n)$. Alexander D. Scott and Gregory B. Sorkin. Linear-programming design and analysis of fast algorithms for Max 2-CSP, Discrete Optimization 4 (2007), no. 3-4, 260-287. $URL$ Joachim Kneis, Daniel Mölle, Stefan Richter, and Peter Rossmanith, A bound on the pathwidth of sparse graphs with applications to exact algorithms, SIAM Journal on Discrete Mathematics 23 (2009), no. 1, 407-427. 

The #VC problem of computing the number of vertex covers of a given graph remains #P-hard for 3-regular graphs; see for example [Greenhill, 2000]. To show that the #VC problem remains #P-hard for graphs with at most $c\cdot n$ edges, where $n$ is the number of vertices and $0<c<3/2$, reduce from the 3-regular case by adding a large enough independent set (of linear size). The number of vertex covers remains the same if you add an independent set. Similarly, to show that the #VC problem remains #P-hard for graphs with at least $c\cdot n^2$ edges, where $n$ is the number of vertices and $0<c<1/2$, reduce from #VC by adding a large enough clique component (of linear size). The number of vertex covers is multiplied by $p+1$ if you add a clique of size $p$ to a graph. Catherine S. Greenhill: The complexity of counting colourings and independent sets in sparse graphs and hypergraphs. Computational Complexity 9(1): 52-72 (2000) 

The vertex isoperimetric number of a graph $G=(V,E)$ is $i_V(G) = \min\{\frac{|N(S)|}{|S|} : S \subseteq V, 1\le |S|\le \frac{|V|}{2}\}$. Several academic papers state that the problem of computing the vertex isoperimetric number of a graph is NP-hard, without proof or reference. Can you give a reference where the following problem is shown NP-complete: given a graph $G$ and a number $t$, the question is to decide whether $G$ has vertex isoperimetric number at most $t$? 

Thus, the problem is fixed-parameter linear parameterized by the number of variables. 1) Yes, Integer Linear Programming is "still" NP-complete. The running time of the theoretical result above depends only linearly on the number of constraints, so it scales nicely in the number of constraints. However I know of no actual implementation of this algorithm. 2) Yes, making the variables take binary values is straightforward as you observed. Update. The dependence on $L$ can actually be improved in the running time for Integer Linear Programming. Based on Clarkson (1995) and Eisenbrand (2003) (see the comments below) one can obtain an algorithm with running time $$O(2^nnm + 8^n n \sqrt{m \log m} \log m + n^{2.5n+o(n)}s\log m)$$ where $m$ is the number of constraints and $s$ is the maximum number of bits required to encode a constraint or the objective function. 

The problem is known as r-Set Packing: Instance: A collection $\mathcal{C}$ of subsets of size at most $r$ of a finite set $S$ and an integer $k$. Parameter: $k$ Question: Is there a subcollection $\mathcal{C'} \subseteq \mathcal{C}$ which consists of at least $k$ mutually disjoint subsets? For unbounded $r$, the problem is W[1]-hard by an easy reduction from Independent Set (see for example [1]). However, for constant $r$, the problem becomes fixed parameter tractable (see for example [2]). [1]: Rodney G. Downey, Michael R. Fellows: Parameterized Complexity. Springer, 1999. [2]: Michael R. Fellows, Christian Knauer, Naomi Nishimura, Prabhakar Ragde, Frances A. Rosamond, Ulrike Stege, Dimitrios M. Thilikos, Sue Whitesides: Faster Fixed-Parameter Tractable Algorithms for Matching and Packing Problems. Algorithmica 52(2): 167-176 (2008) 

I agree with the OP: $\mathcal{G} - \mathcal{C} = \mathcal{G}'$. I assume the author of the paper wanted to define $\mathcal{G}'$ as $\mathcal{G} - S_{\mathcal{C}}$. With this modification, the next sentence in the paper makes sense: 

The following Theorem is proved by Chen et al. [2009]. Theorem 2.4. Let $f(k)$ be a nondecreasing and unbounded function, and let $Q$ be a parameterized problem. Then the following statements are equivalent: (1) $Q$ can be solved in time $O(2^{\delta f(k)}p(n))$ for any constant $\delta > 0$, where $p$ is a polynomial; (2) $Q$ can be solved in time $2^{o(f(k))} q(n)$, where $q$ is a polynomial. Taking $f(k)=n$ we obtain that a problem has a $O(2^{\epsilon n})$ time algorithm for every $\epsilon>0$ if and only if it has a $2^{o(n)}$ time algorithm. It is mentioned in the paper by Chen et al. that this equivalence had been intuitively used before but that it was causing some confusion among researchers. 

Adaptive Analysis measures the running time of polynomial time algorithms with respect to a multitude of parameters. For example, you want a sorting algorithm that runs in time $O(n \log n)$, but is much faster when the input is almost sorted. An adaptive analysis of a sorting algorithm would take into account the number of pairwise inversions, the number of runs, where a run is a maximal sorted consecutive part of the input, or the entropy of the input. It looks like the Parameterized Analysis for polynomial-time algorithms, and it seems that the output-sensitive analysis fall in this category. 

The problem is polynomial-time solvable. Say that a vertex is balanced if its in-degree equals its out-degree. Note that a directed graph is Eulerian iff every vertex is balanced and its underlying undirected graph is connected. Now, a directed graph is a vertex-disjoint union of Eulerian graphs iff every vertex is balanced. So, the problem amounts to deleting a smallest number of arcs so that each vertex becomes balanced. In Theorem 2 of the following paper, this problem is solved in polynomial time using network flows. 

It seems that your version of the Marriage problem is equivalent to the Minimum regret Stable Marriage problem with Ties, where everybody ranks the members of the other sex with possible ties, and the goal is to maximize the minimum "happiness". It is shown in [1] that Minimum regret Stable Marriage problem with Ties is not approximable within $N^{1-\epsilon}$, for any $\epsilon > 0$, unless P=NP, where $N$ is the number of men in a given instance of the problem, even if the ties are on one side only, there is at most one tie per list, and each tie is of length 2. [1]: David Manlove, Robert W. Irving, Kazuo Iwama, Shuichi Miyazaki, Yasufumi Morita: Hard variants of stable marriage. Theor. Comput. Sci. 276(1-2): 261-279 (2002). Postprint.