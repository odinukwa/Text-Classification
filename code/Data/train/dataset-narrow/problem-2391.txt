There are different measures of polynomial sparseness in the literature and you haven't been very clear on which one you might mean. However, your optimism is likely not well founded. Take (x-2) / x^32. It's almost maximally sparse by any reasonable measure, but the coefficients in its Laurent series grow exponentially fast. If multiplication is unit cost you can still use the binary exponentiation method to keep up with the exponential growth in sublinear time in the total degree of the series, but the example suggests that things aren't rosy. This is an area littered with hardness results for division and related problems, at least in realistic models of computation where multiplication isn't constant time. Jacques Carette is an expert in this area, so let's hope he posts a definitive answer. 

I'll assume it's available as a library function. You can easily embed its definition directly in the quine, but that would clutter the presentation without good reason. Now the rest is simple: 

Any duplicates will occur in contiguous runs, so they are easily filtered out without keeping a black list. You could also kill them upon insertion in the min-heap. 

First of all, why are observables represented by operators? In classical mechanics an observable is a real-valued function on phase space. It extracts information about values like the energy or momentum from the system but does not affect or interfere with it. If the observer is part of the system then measurement is a physical process and can change the system's evolution. For finite, non-infinitesimal time evolution to be unitary (i.e. preserve total probability) the infinitesimal time evolution must be Hermitian. This is Stone's theorem; it explains why operators in quantum mechanics are Hermitian. If that makes sense, the formula $M\mid\psi\rangle / \sqrt{\langle\psi\mid M^\dagger M\mid\psi\rangle}$ follows from two things: 

Consider the lowly singly-linked list in a purely functional setting. Its praises have been sung from the mountain tops and will continue to be sung. Here I will address one among its many strengths and the question of how it may be extended to the wider class of purely functional sequences based on trees. The problem is the following: You want to test for almost certain structural equality in O(1) time by means of strong hashing. If the hash function is structurally recursive, i.e. hash (x:xs) = mix x (hash xs), then you can transparently cache hash values on lists and update them in O(1) time when an element is consed onto an existing list. Most algorithms for hashing lists are structurally recursive, so this approach is eminently usable in practice. But suppose instead of singly-linked lists you have tree-based sequences that support concatenating two sequences of length O(n) in O(log n) time. For the hash caching to work here, the hash mixing function must be associative in order to respect the degrees of freedom a tree has in representing the same linear sequence. The mixer should take the hash values of the subtrees and calculate the hash value of the whole tree. This is where I was six months ago when I spent a day mulling over and researching this problem. It seems to have received no attention in the literature on data structures. I did come across the Tillich-Zemor hashing algorithm from cryptography. It relies on 2x2 matrix multiplication (which is associative) where bits 0 and 1 correspond to the two generators of a subalgebra with entries in a Galois field. My question is, what have I missed? There must be papers of relevance in both the literature on cryptography and data structures that I failed to find in my search. Any comments on this problem and possible venues to explore would be greatly appreciated. Edit: I am interested in this question on both the soft and cryptographically strong ends of the spectrum. On the softer side it can be used for hash tables where collisions should be avoided but aren't catastrophic. On the stronger side it can be used for equality testing. 

I'm not exactly sure why you say the distribution in step 1 is Bernoulli. It actually doesn't matter if all you care about is the expected running time. Let $P_i$ be the random variable whose value is the halting probability in step $i$ and let $T$ be the running time. The expected running time is then $$\mathbb{E}[T] = \sum n\ (1 - P_1) \cdots (1 - P_{n-1})\ P_n$$ Despite being an expected value, it's still a random variable! To flatten the last layer of randomness, you must hit it one last time with the expectation operator: $$\mathbb{E}[\mathbb{E}[T]] = \sum n\ \mathbb{E}[(1 - P_1) \cdots (1 - P_{n-1})\ P_n]$$ This is as far as we can go without making any assumptions. If the $P_i$ are IID then expectation is multiplicative, so let us assume they are: $$\mathbb{E}[\mathbb{E}[T]] = \sum n\ (1 - p)^{n-1} p,$$ where $p = \mathbb{E}[P_1] = \mathbb{E}[P_2] = \cdots$. This is the expected waiting time of a Bernoulli process with probability $p$, which indeed equals $1/p$. The moral is that everything flattens in a trivial way with nested expectation values of independent variables. 

Use a variation on your shared secret-key encryption proposal: Every packet has an encrypted header for which the plaintext is the connection ID concatenated with the packet sequence number. After successfully receiving a (connection, n) packet, the next packet received on that connection will be (connection, n+1). Store the encryption of (connection, n+1) in a hash table and remove the expired entry for (connection, n). When receiving a packet, do a hash table lookup using the packet's encrypted header word to find out its associated connection. Obviously for this to work, sender and receiver must use synchronized ciphers (e.g. a stream cipher initialized with a negotiated IV and the shared secret key). You can easily extend this algorithm to deal with longer sliding windows by adding hash table entries for sequence numbers n through n+k. The space cost is O(mk) where m is the number of open connections and k is the length of the sliding window. 

This is an old question; I hope thread necromancy is not considered poor etiquette around here. The previous responses have focused on justifying why P and hence NP are natural classes from a complexity-theoretic point of view: it's the transitive closure of the class of linear-time problems under various natural operations, and so on. A completely different angle on the nature of NP comes from logic. In Fagin's paper Generalized first-order spectra and polynomial-time recognizable sets he showed that the languages in NP are exactly those that can be defined by an existential formula in second-order logic. The connection between complexity theory and logic goes way back: Ackermann's function famously cannot be proved total in primitive-recursive arithmetic, and there is a general principle in ordinal analysis whereby the strength of a formal system is related to the growth rate of those functions it can prove total. However, Fagin's analysis is still startling. If there was ever any doubt in your mind that NP is rightfully the centerpiece of complexity theory, his paper should dispel all trace of that. 

You do not say if the bias is known or unknown. The magic of von Neumann's algorithm is that it works in either case. Suppose it is known. The best answer then depends critically on number-theoretical features of the bias. Let's take p = 2/3. Toss the coin twice and map HH to 0 and TH and HT to 1, repeating the experiment if the outcome is TT. Then 0 and 1 are equally likely and the chance of a repeat is only 1/9 instead of 5/9 with von Neumann's algorithm. Or to put it in your terms, you only bias one of the outcomes by 1/9 if your iteration limit is 2. This is all closely related to information theory and coding theory. When p is a fraction with a more complicated numerator and denominator, the best algorithm will require a longer block length than 2. You can use a Shannon-style existence argument to show that for a given bias there is a procedure which is as optimal as you want, but the block length can get very large. Peres in his paper Iterating Von Neumann's Procedure for Extracting Random Bits proves that a version of von Neumann's algorithm can approach the Shannon limit arbitrarily well. A lot of the work in this area seems to have been done by information theorists and statisticians, so I can't think of any paper with a complexity-theoretic slant that would give you a direct answer to your question. There is a fun related problem which asks the opposite: If you have a source of fair bits, how do you efficiently generate a uniform distribution over some non-power-of-two set? The iteration-limited version of the problem that is akin to your question asks to maximize the entropy (i.e. make the distribution as uniform as possible) with n tosses of a fair coin. 

Actually, O(n log n) is a lower bound for this problem in models where sorting is O(n log n). If all permutations are equally likely then the algorithm as a function from random streams to permutations must be surjective. There are n! permutations so in something like a decision tree model there are branches of length at least O(log n!) = O(n log n). This worst-case lower bound still works for non-uniform random generation of permutations so long as all permutations have non-zero probabilities. What changes is the average complexity. It looks like the lower bound on the average complexity in a decision tree model is the entropy of the distribution. With binary decision trees, this lower bound can only be exactly achieved when the distribution is dyadic. The extreme case is when one distinguished permutation has probability $1-\epsilon$ and everything else has equal probability. Then a lower bound on the average complexity should be $O(\epsilon)$. 

No, it doesn't matter. If you use the algorithm I suggested, the effect of allowing composites is that you generate duplicates when unique factorization fails; this happens when two or more numbers in the generating set aren't relatively prime. For generators 2 and 4, the sequence goes 

Warning: This is not yet a complete answer. If plausibility arguments make you uncomfortable, stop reading. I will consider a variant where we want to multiply (x - a_1) ... (x - a_n) over the complex numbers. The problem is dual to evaluating a polynomial at n points. We know this can be done cleverly in O(n log n) time when the points happen to be nth roots of unity. This takes essential advantage of the symmetries of regular polygons that underlie the Fast Fourier Transform. That transform comes in two forms, conventionally called decimation-in-time and decimation-in-frequency. In radix two they rely on a dual pair of symmetries of even-sided regular polygons: the interlocking symmetry (a regular hexagon consists of two interlocking equilateral triangles) and the fan unfolding symmetry (cut a regular hexagon in half and unfold the pieces like fans into equilateral triangles). From this perspective, it seems highly implausible that an O(n log n) algorithm would exist for an arbitrary set of n points without special symmetries. It would imply that there is nothing algorithmically exceptional about regular polygons as compared to random sets of points in the complex plane. 

That's the whole purpose of consistent hashing. When you add a new node, you compute its hash key, find the two nearest neighbors on the hash ring and link the node in between them. When the hash function has good statistics, you expect a uniform distribution of hash keys, resulting in good load balancing. You don't have to redistribute the existing keys. Removal of nodes works analogously. In summary, there's no global coordination to achieve even coverage of the key space. That is an emergent property of the hash function. The only coordination is local in the form of linking and unlinking nodes into the circular list of nodes on the ring. Incidentally, if nodes were only ever added and not deleted, low-discrepancy sequences would be a better choice than hashing (such sequences are particularly simple in one dimension). But in the real world, nodes are added and deleted in an unpredictable pattern. Hashing is better able to cope with those conditions; the trade-off is worse discrepancy in the monotone growth case. Using a low-discrepancy sequence would also require new nodes to coordinate with a key manager, though no existing keys need to be redistributed.