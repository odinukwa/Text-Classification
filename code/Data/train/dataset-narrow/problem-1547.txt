For the option to match, you need to make sure you're looking for the right string - not the extended command name shown by default with e.g. , but what the option shows in combination with e.g. . Compare: 

This has been happening on one of my servers with Ubuntu 14 (PHP 5) and 16 (PHP 7). With the earlier version, there was some delay, but with the latest version, as soon as a single problematic request is served by the FPM process it goes wild. I noticed that the problematic kind of requests do happen to stuff like in $URL$ but I still don't see how that should cause this. It seems to be closely correlated with executing stuff in the background (with the operator). I can't offer an actual solution, merely my fugly workaround script that runs from cron: 

Just for the benefit of others who stumble upon this by googling the Dovecot issue like I just happened to: Dovecot can be made to use if you set it up that way. The documentation is at $URL$ On a typical Debian/Ubuntu system, you can put this in : 

This happened to me, and as I was reading the help screens to find the incantation, it actually changed the status on its own to add the changed p* disk to the u* unit, and start rebuilding the array. It's possible that the controller just has some small amount of back-off time in case you're taking time to seat the drive, and then it kicks into the logical course of action. 

Disregarding for the moment the possible incongruity of the concept of both managing the configuration of a system from a CM system and not doing it at the same time that commenters have already noted... It should be mentioned that for this use case it's usually possible to make use of what has become a convention with modern-day server software - configuration directories which allow insertion of extra config snippets in the right places. That way you may be able to have your CM manage the config for you via e.g. , while at the same time you're still able to fiddle with a particular machine by adding or similar, which would then cause the software to process your overridden settings. Obviously if CM has recipes that aim to control the entirety of you still have no recourse other than to do things within CM. 

It did that set of and calls in a loop, but eventually gave up and did the of its temporary file. The solution was simply to manually remove that backup in . 

The three columns should be separated by spaces or tabs, it doesn't matter. The IP address can be 127.0.0.1 just like localhost, but the hostname part has match the system hostname, and the middle part should be the FQDN, i.e. hostname with a domain suffix. To verify the line is working, simply run and check that it returns your server's FQDN. Finally, restart Exim, e.g. with: 

In addition, on Debian 6 and Ubuntu 12 LTS, the package shipped which notices missing directories on local filesystems and sends daily reminders about it via e-mail, recommending the use of . However, this was removed by the time of Debian 7 and Ubuntu 14 LTS, respectively, because it had become obsolete. 

This happened to me after I made the mistake of editing manually. There's a reason why every manual in existence says - just don't do that :) The edit itself was innocuous - I had renamed an LVM volume group of a mounted logical volume, and needed a trivial update to its entry so that would stop reporting spurious errors. The problem was that my editor left a simple backup file at . All invocations stopped updating , and started taking ~30 seconds to run, which was rather suspicious. Sadly it didn't even print a warning, it took a to figure this out: 

Were you told to apply these settings on the same network interface (e.g. ) or did you receive a separate new one? Either way, you'll just have to configure the new settings but omit the default gateway, and see if it works - if 188.60.240.1 and 188.60.240.129 are both aware that both sets of traffic are legitimately originating from your machine, they'll route your packets regardless of which outgoing address or interface you use. If it doesn't work, you'll have to set up source-based policy routing, so that each piece of traffic goes through its own default gateway interface. 

Shared libraries don't typically produce their own debug output by default - the programs that link them are supposed to take care of that, because they may use various ways to communicate to the user: the console, some sort of X output, syslog, ... Did you try calling from the program you link to the OpenLDAP library? Per , there's no 1:1 mapping between those options and all the library options, and DEBUG_LEVEL isn't explicitly mentioned as available. 

Regarding the question - why is this so - the manual page that explains it is , IOW the one accessible through (not the default one in section 1). The cron daemon does not try to emulate a shell session, rather it sets up a clean, minimal environment for the cron jobs to run in, and then in turn allows the crontab file to set its own arbitrary environment variables. The newer cron daemon shipped with Debian also has several additional provisions for etc. 

If you want to run a specific command using a with sudo, and have config read from its home directory rather than yours, then you have run it with for sudo to update the HOME variable to the called user automatically. In this case that will imply a valid value for AWS_CONFIG_FILE and AWS_CREDENTIALS will be automatically generated. 

The variable is filled according to the fully qualified domain name (FQDN) of the system. On Debian and related systems this is typically formed using the contents of the and files. The hostname file should contain the short hostname (e.g. ), and the hosts file should contain an entry resolving into , i.e. hostname with its domain suffix (normally matching a DNS entry). The output from the command (without any parameters) will return the former, and the output will return the latter. The hostname file is typically filled upon installation, and the hosts file can be edited to have one other record in it beside the default localhost records - it should be of the form: 

The option has existed in Portable OpenSSH since 2008, cf. commit e7140f2. This was released with 5.1p1, made in July 2008, cf. release notes for 5.1, so it exists in pretty much all OpenSSH server installations supported today. 

This happened to me after a failed run. The solution was to make sure the affected volume group was inactive () and then to manually remove all those leftover broken symlinks. Note that deactivation of volume groups may be foiled by bugs such as $URL$ 

It should probably be noted that in relation to the original question, there's another simple solution if format doesn't work with - as it still happened with because of bug $URL$ that was since fixed - and that is to switch to with the file tree external pillar, which is also backed by files on the master. The is documented at $URL$ nowadays. It has existed since version 2015.5.0 so it's newer than the original question and answer, yet it is a solution that is reasonably well available today. Indeed, it is also in the FAQ at $URL$ 

Your system user has a matching PostgreSQL account called , and the rules allow it to authenticate. If you want PostgreSQL to also support users or , create them in PostgreSQL, and set up equivalent rules for them, too. 

means , that is, "No space left on device". There's several lookup commands available for such standardized errors, one is called simply : 

(NB: I originally did this as an edit to the answer from 2009, but it was rejected. The things I was originally fixing are: there was no need to repeat the default localhost record, just explain what the addition should be; explain which IP address can be used; explain what is FQDN; explain how to verify exim itself.) 

Root or superuser access would mean that you're allowed to adjust any parameter of the system, to step outside the beaten path. If you've tested the service and it allows you to do whatever you currently want, then you probably don't need root access. The question is, what happens when your needs happen to change, and you want to add something the service doesn't happen to support? You'd have to figure out how hard is it for the service owners to compromise and either set things up for you differently, or allow you temporary or permanent access so you can do it yourself. And if they tell you upfront that such customizations are impossible, then you have to pay attention to how hard is it for you to uproot yourself (pardon the turn of phrase) and move everything you have there to a different service. 

Judging by the backtrace, this sounds more like some sort of a bug in the network interface driver or the network packet driver . Your userland process is probably the main thing that talks to the network on that machine, so it appears connected, but soft lockups are really kernel space bugs. As a first step I'd suggest examining the changelog for the driver in later kernels, to see if it's worth doing a kernel upgrade. In general, 3.1.x was released a while ago, and it's not marked as a stable or a longterm kernel at $URL$ so you'd be well advised to switch to one that is (e.g. 3.2.x is still marked 'longterm' these days), unless you're paying someone to maintain that 3.1.x kernel for you that is. If you can't find a clear reason to upgrade, find the contact information in the Linux MAINTAINERS file at $URL$ and send in a bug report, and the people there will advise you better.