If is going to be the primary key with values from 1 to 1 million, each will consume between 1 and 7 bytes of space. But I also know from doing the test that, on average, it'll take ~6 bytes (actually 5.89 bytes). Of course, the larger the values get, the more space, on average, each requires. Oracle needs, on average 1.1 bytes per element to store the numbers 1-10, 1.92 bytes to store 1-100, 2.89 bytes to store 1-1,000, 3.89 bytes to store 1-10,000, 4.89 bytes to store 1-100,000, and 5.89 bytes to store 1-1,000,000. So, let's estimate for our example that will require 6 bytes and will require 50 bytes because the average is roughly 50 bytes. So we'll estimate a row size of 56 bytes. The number of rows per block 

will give you the text of the view definition as of an hour ago. You can obviously adjust the flashback time to a timestamp just before the DDL you want to undo. 

It depends but given the data distribution, probably not. The performance benefit of using a single expression like in Oracle is that it becomes indexable with a function-based index. But since there are very few cases where the optimizer would use an index to return 99% of the data in the table, it seems unlikely that there would be benefits in this case. Potentially, a composite index that included and some other columns such that it would be sufficiently selective could be beneficial in some cases but it's relatively unlikely that adding an expression that only adds a tiny bit to the selectivity would be a net benefit. 

Oracle will look in the current schema to see if there is an object named . Since there is no such object, it throws an error. You can fix that by telling Oracle what schema the object resides in by qualiying the table name 

Your table's segment is only 2 MB. The rest of the data is stored in the LOB segments which are physically separate from the table segment. 

In this case, the format picture is the "YYYY-MM-DD". This expression raises the ORA-01830 error because the input string contains more data than the format picture is capable of handling. If, on the other hand, you are doing an implicit conversion, the format picture refer's to the session's which Oracle uses in its own implicit call. 

Alternately, you can create a synonym (public or private) that provides the mapping. As Alice, for example, you can create a private synonym 

What is the plan for this query? Unless you're executing it orders of magnitude more than any other query in the database, which would seem to indicate a separate problem, it shocks me that this is the most expensive query in the database. Is there a composite index on ? Because it should be trivially costly to use such an index for this query. 

In any system, if you make the part of the system that is not the bottleneck more efficient, there is the potential to make the system as a whole less efficient. If, for example, your system's bottleneck is the CPU, making I/O more efficient by increasing the size of the buffer cache may end up causing many more threads to be waiting on the CPU which can make the system as a whole much slower as the operating system is now spending more time swapping between the competing thread, the CPU caches get overwhelmed, and you get the classic hockey stick graph of response time where once you get to the "knee", adding a bit more load adds a lot to the response time of the system. If you have a system where I/O requests are highly uncorrelated, increasing the size of the cache could mean that Oracle will spend more time looking through the cache for a particular block before being forced to do a physical read after all. That's probably relatively unlikely in an OLTP application but it's possible in a data warehousing type system where various reports are asking for blocks that no other report will want to reuse. Of course, whether this is actually the problem that they encountered in the past is still an open question. It is entirely possible that it was simply a matter of having a cold cache that needed to be warmed up or a perception problem where someone drew an incorrect conclusion from an earlier test or a poorly constructed test where, for example, someone inadvertently increased the size of the buffer cache at the expense of the PGA rather than at the expense of the operating system I/O cache. I tend to share your distrust of human recollections of the past that aren't corroborated by some nice AWR reports so I'm not claiming that either of the above explanations are likely merely that they are potential explanations if the human recollections are correct. 

which works out to 59.19 MB. Now, let's test our estimate We'll insert 1 million rows where goes from 1 to 1,000,000 and is a string with a random length between 1 and 100. 

Not to the current session, no. You can use the package to instrument the code so that tools running in other sessions can query and see how far along the process is. This is the same tool that Oracle internally writes to during long-running operations so lots of front-ends like Enterprise Manager and Toad will already read from it. Alternately, you could use autonomous transactions to write to a status table as your code runs and read from that status table in a different session. I can't see why you'd want to, but you could theoretically also use the package to send alerts to other sessions. 

Then, you should be able to create users even with roles disabled and the stored procedure should work. 

That depends on your disk subsystem. A full backup is going to involve a metric crud-load of I/O (that is a technical term). That is going to put a huge load on your I/O subsystem. Depending on how I/O bound your system is, how much spare bandwidth your I/O subsystem has, how fast your I/O subsystem is, etc. the impact will range from "yeah, maybe it's a bit slower" to "everything times out, the system is unusable, users will be visiting the DBA team shortly with flaming pitchforks to express their displeasure." I am hard-pressed to imagine a whole lot of situations where I'd ever want a full backup running on a non-trivial production database during anything close to business hours. If you have a real need to do this, I'd tend to be investigating things like DataGuard that would potentially allow you to offload the load of doing the backup to the standby database. 

It is possible that smaller values would be slightly more efficient. But if your code is so efficient that one of your larger bottlenecks is the size of your integer keys you are way, way, way ahead of the game and have little left to optimize. It is possible that the database you are using will use a different amount of space to store different numeric values where smaller numbers tend to use less space. Not every database will do this but some will (for example, Oracle uses a variable number of bytes to represent different numbers). If this is the case for your application, then rows with smaller keys will generally be smaller on disk which means you can read more rows per I/O request, fit more keys in each index block, and cache more rows per kb of RAM. Potentially, you might find that restarting the key would allow you to use a smaller data type for the key but that is pretty unlikely. All these will have some potential performance benefit. That said, the size of the benefit is likely to be minimal. In most tables, the size of the primary key will be a small contribution to the size of the row so saving a byte or two on a key will probably lead to slightly more free space on each page/ block which would negate the performance benefit. And even if you get slightly more data on each page/ block, it's pretty unlikely that you'd be able to measure the improvement. Yes, you might be slightly more likely to find a row in cache but it would be tough to quantify such a small improvement.