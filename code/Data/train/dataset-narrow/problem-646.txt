Prior to 12c, Oracle made only the base releases publicly avialable on OTN, to download patch set releases, you needed to have a valid licence and support contract. If you have a valid licence and support contract, you can still download 11.2.0.3 from $URL$ Or using this link: $URL$ The file you are looking for is: 

The below commands always connect to just one of the instances. If that instance is down, the connection attempt fails, even though there is another instance providing the same service: 

Static registration is performed by editing and adding the corresponding entries. That alone does not prevent or from dynamically register services in the listener. Dynamic registration can be disabled by setting the to . With this setting, or will still try to register, but the listener will simply not accept these requests. 

Install the first one, that is a combo patch that includes the latest versions of all the others. The others are just the seperate PSU or OJVM patches. 

You do not create regular users in the root container. You create common users, with usernames starting with . You do not put user data in the root container. Create a pluggable database and create a user inside the pluggable database, and place your data inside the pluggable database and not in the root container. 

So? Just copy a dumpfile to a location accessible to database2. is not a directory, it is a directory object defined in the database, that serves as an "alias" to the real path in the filesystem. Overview of Oracle Data Pump File Allocation 

It is not that easy, unless you notice some trivial mistake. Cost is just an estimation. An estimation, based on statistics, that may be outdated or non-existent. Even if you have 100% up-to-date and accurate statistics, the optimizer will have a hard time with nontrivial queries, and it will make inaccurate estimates. Explain plan with bind variables makes it even worse - explain plan will simply ignore your histograms and assume that your data is evenly distributed for an equality filter, and it will estimate based on predefined rules for non-equality filters. About the efficiency and small number of rows - unless you know your data and query well, also not. The database may choose a different access path, join order or join method based on the row sources. What works on a small scale, may not work well on a large scale. If you want proof: test and evaluate. If you have a query, and you know the cost of its execution plan is 15957, you know nothing. If you know the amount of elapsed time, cpu time, disk io time, reads, gets, number of rows processed, executions, memory/tmp usage, etc. per step, those are some results that can serve as a baseline. 

You can start running your report from the application, and follow the contents of the trace file (even , but it will not be human-friendly). After you have finished, disable tracing: 

You do not even need the database to register itself, as you have static registration defined - also in listener.ora 

package has conversion functions as well. Unfortunately there is no support for converting CLOB to CLOB and change characterset in one step, so the data is first converted to BLOB, then back to CLOB. 

Note: this will not affect how NULLs are displayed in the data grid (you can set that from the menu: Tools - Preferences - Database - Advanced - Display Null Value As), but it affects the exported CSV. 

You can find more on the above URL. After you read it, forget everything written there, and use explicit conversion. Relying on implicit conversion is bad practice. 

You can find this information in . Using this view requires the Diagnostic Pack option. For example, sessions in the last 1 day: 

is a local destination that can archive any log in any role. is a remote destination that points to the standby. You should have something like this: 

This has caused some seriously incorrect cardinality misestimates with histograms on GUID columns. This is how this behaves when values are inserted in an increasing order (removed ): 

Starting with 12c, there is a new feature called Oracle Sharding. Sharded Database Management For example you can define the following shardspaces in Global Data Services with : 

1) impdp with network link doest not need the production database to be stopped. This is the slowest method, as this needs to build everything from scratch, I am not sure this would finish in 4 hours. 2) RMAN duplicate fails between different versions because it tries to automatically open the database, and that will fail. But you can manually restore the database on the new host from the RMAN backups, then open it with and run the upgrade scripts immediately. With this method, you need downtime only to upgrade the database on the new host, should not take more than 30-60 minutes. 3) Streams does not require any license. You can just simply replicate the database, and start using the new database whenever you want to. This requires the least downtime, but the configuration and administration is cumbersome compared to other methods. 

Each worker thread uses a seperate dumpfile and requires exclusive access to it. If you have fewer dumpfiles than the degree of parallelism you provided, the parallel threads will not be fully utilized. If you have a single dump file, only 1 worker thread can be active. Tables that contain Basicfile LOB columns are not be exported or imported in parallel. This is a known restriction and documented in: DataPump Export/Import Of LOBs Are Not Executed in Parallel (Doc ID 1467662.1) 

The above will break your replication because of the unique constraint violation. If you ignore the error, you will not have fresh data in the destination database. It sounds to me you want to build a history table, but there is more to that than just ignoring delete operations, have a look at the below support note: Oracle GoldenGate Best Practices: Creating History Tables (Doc ID 1314698.1) 

There is no such thing in the database itself. You can perform something similar in SQL Developer, with the Database Export tool. How to Export Data using SQL Developer With the Tools / Database Export tool, you can specify the objects you want to export, then SQL Developer generates an SQL file for recreating them, with the data as insert statements. 

Finally, do not forget to copy other configuration files from the old home (pfile/spfile, listener.ora, tnsnames.ora, sqlnet.ora, etc.). 

Byte order is reversed, swapping the bytes and the octets: 0x004054c7. Doing this on a non-standby database, with minimal supplemental logging (0x4000000) and force logging (0x1000000) enabled (for GoldenGate replication): 

Get the data, order by . Compute a temporary column: if equals to in the previous row (rows ordered by ), let that column be , otherwise . This column is used for the following: when is the same as in the previous row, we increase the value in by , otherwise we increase the value by . Finally, get , , and the of the temporary column up to the current row (this requires the analytic version of ), this will give us . The above translated to SQL: 

When it comes to Windows, something is always different. works on Linux/UNIX, but not on Windows. The Windows version of is . SQL*Plus Environment Variables 

A similar effect could be achieved by setting the format to the location of FRA, but it will not be the same, as these files will not be accounted in the space usage of FRA. 

Your table is owned by USERGOD, but you are importing as DIGITALNOISE. Yes, you have the IMPORT FULL DATABASE, but you are not doing a full import. Using FROMUSER/TOUSER is one solution. Performing a full import by adding the FULL=Y option is another. 

Too broad. Anyway, in this specific example, the best reasonable index is the PK itself, because its unique, and you get your 0-1 row (block) immediately that you can visit and evaluate the rest of the conditions. Of course, you can create an index as (id, name) which in some cases saves you the table visit because of the short-circuit evaluation, but in my opinion the performance gain with this would be marginal. Sure, you could also create an index as (id, name, lastname), so you would not have to visit the table at all, but there is no point doing that, since you dont have any other columns, so you would index the whole table. It would be better to just create the table as an index-organized table in that case. 

Just select the union of the query and the 'none' constant if the original query does not return any row: 

The ALTER SYSTEM SET clause for this parameter is only valid with the SCOPE=SPFILE option, so you can not change the parameter value with this dynamically. You can change the value for all other sessions, e.g as below: 

B, D incorrect: The join is performed in parallel, there is only 1 serial operation on the picture, only one parallel->serial (P->S) transition, and that is when the query coordinator collects the results. A (, D) incorrect: Parallel SQL uses the producer-consumer model. One set reads the table, the other set builds/probes the hash table, they do not read seperate tables simultaneously. C incorrect: There is only 1 DFO (), the order of execution can be easily recognized by the order of virtual tables or the table queues (). This is not necessarily true for multiple DFOs, but it is inside 1 DFO. comes before , but does not necessarily come before . Read more about this: $URL$ So the first table to be read is , not . E correct: Well, this is how it works, but that is not a real explanation. Easier to prove the other options wrong. 

You should run this with elevated privileges, for example in a command prompt started with "Run as Administrator". 

This is a known bug, and it will be fixed in a future version. Use to generate the index scripts, and create them manually. 

And that is the mistake people usually make, based on a false assumption. Just because you can ping, let's say, even the database server, does not mean at all you will be able to connect or name resolution works properly. Ping and checks work only up to the first entry point in a database connection, and they stop there, these tools alone are not enough for testing database connections. Based on my experience, / is a result of indirect name resolution problems. Even the official description suggests trying IP addresses instead of host names: