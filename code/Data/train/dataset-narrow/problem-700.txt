If you regularly have UAT in sync with PROD, best is to use that dataset to restore PROD - unless you are masking PROD data in UAT. 

This is called PULLING Data as you are executing the query on destination server and pulling data from source server. This will be a lot faster and less resource intensive compared to the former one (depending on how much data is being pulled). In the case of the pull method, using SQL Profiler you will see that a single SQL statement is executed across the linked server (source server), and the resultset is pulled over from source server to destination server which is a huge performance gain over PUSH method. Another point to be noted is : Between Linked server (4 part naming convention used servername.databasename.schema.tablename a.k.a Distributed Queries) and OPENQUERY, generally OPENQUERY will be fast. Why ? For Linked Server - Query optimizer creates an execution plan by looking at the query nomenclature and breaks it into remote and local queries. Local queries are executed locally and data for remote queries are collected from the remote servers, scrubbed locally, combined together and presented to end user as single record set. For OPENQUERY - Executes the specified pass-through query on the specified linked server. SQL Server sends pass-through queries as un-interpreted query strings to an OLE DB data source . Hence, SQL won’t apply any kind of logic on the query and won’t try to estimate what that query would do, it would simply pass the specified query as it is to the target linked server. Open queries are useful when you are not referencing multiple servers in one query. It’s generally fast as SQL does not break it into multiple operations and does not perform any local action on the output received. Excellent reading references : 

Alternatively, you can use SQL Server Maintenance Solution - by Ola hallengren which supports AlwaysON as well. 

You can use tablediff utility to find out the difference in data between your Publisher and Subscriber server. Check How to run TableDiff utility for ALL replicated tables ? article by Chris Skorlinski on REPLTalk. This blog has all the required scripts that will help you out. If you have 3rd party tools like redgate schema and data compare, then your life would become much easier. As to find out what went wrong, you should check if 

Now even if you want to use SQL Server authentication, then log-in using your windows authentication and then create a login with sql authentication and map that to the database that you want to access with ONLY required privileges. 

So, there is no alert mechanism like Mirroring to actually be able to know the oldest unsent transaction. The only way to know is using DMVs. and possibly Extended Events. As a side note, you can use Policy Based Management to monitor your AlwaysON health. Below is the query that I am using to monitor my AlwaysON environment : 

This is called PUSHING Data as you are executing the query on source server and pushing the data into destination server. This will be expensive operation. --- executing in target server 

Thats your best bet - its low impact and works just fine. You have to perform a full backup, ship it to secondary server and restore it with no recovery on the secondary server. Then it will automatically ship transaction logs depending on the frequency that you have set (minimum is 1 min). Make sure you set proper retention period and have a clean up job that deletes the old history from msdb database to prevent it from bloating. Make sure you take adhoc log backups with as adhoc log backup will break log chain ==> it will mess up logshipping. 

Below is a little code that will help you decide if "switching optimize for ad hoc workloads ON/OFF" will be beneficial or not. We normally check this as a part of our health check for in-house and client servers. It is the safest option to enable and is described well by Brad here and by Glenn Berry here. 

Set up logshipping between old server and new server (possibly every 1 mins). When you decide to failover, take a tail log backups first and then restore that tail log backups for all the databases and then restore the databases with recovery on New server. Obviously, you can do some prep-work beforehand to create logins, sql agent jobs, etc. 

Obviously, you can use powershell as well, but I have found the above ones more useful when you have to load large number of trace files efficiently. 

Its complaining about the userSettings section of the Microsoft.SqlServer.Configuration.LandingPage.Properties.Settings file Run below command : 

As a sidenote, its time for migration to something supported by Microsoft sql 2005, sql 2008 R2 or 2012 along with migrating DTS packages to SSIS. Refer to : 

If this is just one time issue and CHECKDB run without any errors and you have good backups, do not bother to investigate it (unless you want to spend time and resources to what is not reproducible). If you are able to reproduce the issue, then I would recommend you to 

Disk space is a concern too if you are not specifying an upper limit on how big will be the file and rollover time. The trace files can get quiet bigger if an upper limit MB is not specified and you dont rollover after certain duration. 

Drop the availability group then using regedit, (first backup the registry of old availability group that starts with ) and then delete that key and Finally create a new Availability Group with new name. 

This way the db can be online and you can start taking log backups (or if point in time restore is not required, then change to simple recovery) to truncate the log file. 

This goes without saying : Why you should not shrink your data files ? If you want to shrink, then after the shrink operation, make sure you do reorg/rebuild of your indexes as well as update stats as required. Below script will help you to shrink datafile in increments 

When you retrieve or load data, SSIS tries to automatically convert it to the correct types. If SSIS can’t implicitly convert the data—and transforming the data within the package doesn't work—you might need to modify the XML mapping files (C:\Program Files\Microsoft SQL Server\110\DTS\MappingFiles -- folder), stage the data so it’s compatible with both SSIS and the data source, create a custom component that can retrieve or load the data, or implement a solution outside of SSIS to prepare the data. SSMS on the background creates an SSIS package. At the end it will even allow you to save it as an SSIS package. 

The C2 Audit Mode uses SQL Trace to capture audit events, which are stored in trace files i.e. they are not stored in database, unless you have a job that loads the files into a database. Note : The C2 security standard has been superseded by Common Criteria Certification. 

best is to use sp_whoisactive -- by Adam mechanic. It can be used to log to a table as described here Depending on your requirement, you can schedule a job to run every 15 mins or so and then you will be able to see how your server is doing. 

Another compliment to @Tara's answer is to use Microsoft's Tigertoolbox (has a lot of scripts which are actively maintained by the CSS engineers) - 

I think I might have a legit answer .. and its a bug. (similar was found when running checkdb in 2014 & fixed in SP1 + CU1) When I run the query as is, SQL Server crashes 

You dont have to use SET. Instead you can do that using DTExec.exe /CONFIG parameter as below: SQL Agent Job --> Steps --> General --> Type (Operating System(CmdExec)) ---> command 

This little code will help for any table that you want to delete records from. It takes care of referential integrity as well ... Below code will generate DELETE statements .. Just specify the schema.table_Name 

Static Port Allocation : If you configure an instance of SQL Server to use a static port, and you have not yet restarted the instance of SQL Server, the registry values are set as follows: 

Instead of detaching a database, its always a wise decision to do backup (and restore to a different server) Another option using PowerShell is to use dbatools - e.g. Below comand forcefully detaches (as a caution, I have used -whatif to not perform any action and just to print out what the command will do) 

If the server is involved in replication (remote Distributor or a combined Publisher/Distributor) then the for will be set to . If the server is not involved in replication, then the for will be set to by default. Note: I confirmed the above statements by checking on the servers with and without replication. I am not sure, where you see a value of . 

I don't see any reasons of NOT using SCHEMABINDING on database objects. Its like safe guarding your objects. Obviously there are pros and cons associated to it. If you are not going to create Index Views, then you dont need SCHEMABINDING for views. Basically, SCHEMABINDING 

A backup when done to a single file or device will use 1 writer thread. So if you are backing up to Multiple files /devices (be that multiple .bak files) will have one writer thread per file/device. 

I guess you have framed your question to somewhat misleading. You want to script out all stored procedures, triggers and functions from one server and create them on the new server. Option 1 : USING SSMS 

From the client application side, your application connection string should use pointing to secondary or mirrored server. 

The problem is with your variable declaration . The is or so define your variable along with proper length to avoid string truncation 

To sync logins, jobs, etc, you can use PowerShell or this utility from SQLSkills (not sure if it works for 2014 !). 

The identity crisis in replication - by Hilary Cotter All about “Identity Range Management” - by Chris Skorlinski 

Using below query, you can easily see the problem of rounding that sql server does when you use data type. 

EDIT : To make my answer more meaningful, I am adding more details Test any scenario that you are going to implement to avoid any surprises !! 

or Note: Above path applies for Default installation. Its a good practice to create a backup of that file, if you are changing any config value. 

QP metrics are always captured in the default group 1 in each respective database and you can access the metrics info using view. So I would suggest you to 

Your database is already encrypted - When the encryption is in progress, you will see the value. When the database is encrypted, the will be 0. you can refer to my script for listing out the databases, encryption status along with other useful information. 

Just to add that you don't have to drop and re-create the replication just to change the "not for replication" bit. You can do it using T-SQL without generating a snapshot or breaking your replication -- sys.sp_identitycolumnforreplication 1 = not for replication 0 = for replication and this causes the problems with Identity colums on subscriber side To change it for all the tables : 

Depending on the edition of sql server, if you are using Enterprise edition then you can do index rebuild as online operation. Also, the script that you have referenced just blindly does rebuild of indexes which is a horrible idea (Blindly rebuild all indexes and update stats ??) Instead use a much proven and intelligent script. I would not put the database in single user mode as alluded by Scott as if somehow e.g. sql agent connects to the database, then its difficult to kill the connection unless you connect using DAC or if is ON then background thread will grab the connection and you will be out of luck ! Instead put database in restricted mode - so that only users with dbo rights on database allowed (e.g. db_owner, dbcreator, sysadmin). This assumes that your end users are not allowed to connect as db_owner or sysadmin. Remember that Index rebuild is an offline operation (unless you are using Enterprise edition with REBUILD = ONLINE option). Command to put database in restricted user mode & back to multi user : 

You can follow a master slave model .. where in you would create a master job that drives the jobs. so create a master job with below steps 

Answering my own question that might be helpful to future visitors .. Aaron's comment made me thinking if we dump the data into temp table would it speed up ? Yes it indeed speed up as due to large number of Availablity groups 53 (I know thats a lot and we have plans to offload them soon), and selects were taking considerably long time. Below tsql runs in 1 min and 03 secs :-)