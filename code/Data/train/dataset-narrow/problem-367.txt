The logic seems to be if there is not contention I will get the data at line rate (32ms) 150 users woudl consume 150*40000*8 = 49,152,000 which is 50 % of 10 meg. If the link is contended and hence policed to 50 meg it will take twice as long as only 50% of the BW is usable 

If your problem is that you have two sites/business parters etc that have separate Internet ASes you can connect them together NOT through the Internet but over a VPN or private link. You have a couple of things to watch for if the address spaces are also advertised via the internet relate to administrative distance and prefix matching. BGP is preferred for many reasons, and you have to use it to talk over the internet. A little more on your use case assuming it is not entirely theoretical would help. At the end of the day there is no law that prevents you from using an IGP. But remember that any network vendor is in the business of selling you rope, what you do with the rope... 

Well you normally need to use virtual links to talk to area 0 via a different non area 0 area or you connect 2 parts of area 0 through a non zero area. So virtual links do not apply. You can have a non contigious area if its not area 0, but as hinted above, not really a good idea. That being said you proposed topology will work without virtual links. 

No other servers in the cabinet are experiencing packet loss. The gateway switch and bad server can ping each other without issue. If I log into another server in the cabinet and attempt to ping the bad server, then I do get the packet loss. The routing table on the bad server is fine -- the default route points to the proper gateway, no other entries exist (except for local IPv4 assignments). Firewalls have been disabled. No VPN setup is in effect (i.e., routing table on the bad server just has the default route). CPU load and network traffic are both very low. Server has been power cycled. Speed and duplex settings are set to auto-neg and are the same on both the switch and server. Forced 100mbit full on both ends, still had the packet loss. There are no port errors (no drops, collisions, FCS etc) recorded on the switch. CPU utilization on the switch is low ($URL$ 

Unless the wifi is intended for administrative access, this would not be a great idea. You will encounter all kinds of interference and increased latency. Your server cabinet is most likely bonded to the ground and essentially amounts to a Faraday cage. Not to mention all the other large metal objects in a datacenter that could cause interference. The datacenter is also very unlikely to allow you to stick antennas outside of your cabinet to improve the signal. Point-to-point wireless bridges such as these work very well and are widely used. However, they are typically deployed on rooftops where unobstructed line of sight can be established and are used for pulling connectivity into a building where fiber is lacking. 

The switch keeps an arp cache like any host. One reason for having an arp cache and using it is when you have non routed subnets (like oracle RAC clusters) so you may not want to put an SVI on the L3 switch (I will run a direct connection between the two switches involved in the NIC team only allow the non routed vlans over it) and put SVIs in the switch for the address space so I can ping the heartbeat interfaces. Then look at the arp table then can find with switch port, easier then getting an ifconfig from the server. It is true that in general the SVI in a switch is just used for management so the only arp entry is for the default gateway, but in certain configurations using multiple SVIs and exploiting ARP can be a great help. 

ASIC can be thought of as a kind of chip. It is normally built in order to do something in hardware that otherwise would be done is software. So Cisco can build an ASIC for anything it wants. Depending on the model of the switch there is 1 or many ASICs. TCAM is a memory design since it is usually found on the chassis systems it is implemented as 1 of many asics. TCAM is used for particular lookup functions like routing (CEF) or ACLS, so if an ASIC does not need to do that kind of lookup it works separately from TCAM. On the other hand ASICs that handle QoS marking work hand in glove with TCAM. The presentation below on cisco live discusses some of the design tradeoffs, and a good place to look to get an understanding of what goes into switch design BRKARC-3466 - Exploring the engineering behind the making of a switch (2013 Orlando) it contains lists of the asics and a lot of general switch design information 

I'm working on testing several FESX448-PREM switches. One of the switches in my test group is known to be bad. It was previously installed as a top of cabinet switch, 42 servers were connected to it, all port lights came on, full duplex, no errors, low CPU, etc but ports 13-24 would not forward traffic. As I understand it, this is due to a bad ASIC that covers port region 13-24. However, I now have this bad switch at my work bench and I cannot replicate the same forwarding issue with port region 13-24. At my work bench, I have port 1 as the uplink and I've been connecting my laptop to ports 2-48 sequentially using a CAT6 cable while running a continuous ping to a public IP. Interestingly, all the ports now work fine -- port region 13-24 no longer has forwarding issues. Does anyone know how this is possible? If there's a bad ASIC for port region 13-24, then I'd expect this problem to occur 100% of the time. I tried a couple other things afterwards. I had the theory that I needed more ports active at once in order to trigger the forwarding issue. So I first took a layer 2 switch and connected it on a bunch of ports with the FESX448. CPU usage immediately went to 100% on the FESX448. I figured something recursive routing was happening with the layer 2 switch. Next, I put the layer 2 switch into boot monitor mode so it wouldn't do any routing. That resolved the 100% CPU issue, but again I'm still unable to replicate the traffic forwarding issues with ports 13-24. Any suggestions on how I can replicate the forwarding issue and effectively test the remaining switches would be much appreciated! 

The tunnel source interface needs to have VRF forwarding enabled You need under the tunnel, to instruct the tunnel to use the specified VRF 

To answer this question, we have to define the term "domain". Easily spoken, the Internet is a network of networks. In BGP terminology, there are many Autonomous Systems (AS) on the Internet, each of them being a network (or domain). One company for example may have a very large network, but is represented by a single AS, hence is a single "node" in BGP. The term interdomain routing describes the process of routing prefixes between Autonomous Systems, whereis intradomain routing refers to routing inside a single AS. Im not sure however what you mean with your last sentence. When two paths to the same destination have the same cost (that is in most basic BGP the length of the AS path, hence the number of ASs the route has crossed). For detailed discussion how BGP chooses paths look for example at this Cisco document. Update As turned out in the comments I might have mis-interpreted the OPs question. The question actually is how load-balancing is performed and where are the differences in load-balancing in common interdomain routing protocols and intradomain routing protocols. Interdomain Routing BGP, being the de-facto standard for interdomain routing, will not perform load-balancing by default. It can however be configured to do so. Two main options exist here: 

Each VLAN creates its own broadcast domain in 1 or more physical switches. If you have a switch taken out of the box it tends to put all ports in vlan 1 so for say a 24 ports the broadcast domain includes all 24 ports. If you were to create a vlan 2 and configure half the ports to be members of vlan 2 you then have 2 broadcast domains each of 12 ports. if you create a vlan 3 and put half the ports that are still in vlan 1 in it you would end of with 3 broadcast domains 2 with 6 ports and 1 with 12. 

I will assume you are running IBGP, you never send a prefix you received via IBGP to an IBGP neighbor, also there would be split horizon (don't send a route back to the person who sent it to you). Since this box only has one neighbor and it is not originating any routes by itself, it will not send any routes, so the policy is implicit, but there. 

You probably want to be polling the CAM/MAC tables, that would have the MAC address of any devices send any traffic through the switch. ARP is L3 CAM/ARP is L2 

Most products require you to configure MAC addresses in DHCP for imaging to, so a normal PC will not use DHCP addresses from the imaging server, this begs the question why have a PXE VLAN? I use one for servers but all production networks are tagged so we can rebuild the server without needing to change the switch, but I would suggest this setup is needlessly complex. Use 2 DHCP servers 1 to build and one for normal operations and have a small scope in the build DHCP server. 

Anyone have any ideas where I should look next? The results of #2, #3, and #11 in particular are really throwing me for a loop... 

I use an iSavi IsatHub for Internet connectivity when I'm in the backcountry out of cellular service. This device is a BGAN terminal and it works by establishing a data link with an Inmarsat satellite and then provides a WiFi network to connect your devices. Data usage is very expensive, so it offers a number of safeguards to prevent accidental usage. One of those safeguards is a firewall. I only need SSH connectivity, so I blocked all outgoing ports with the exception of TCP port 22, UDP 53 and TCP 53. All inbound traffic is allowed. This past week I was in the backcountry far from cell service. I fired up the IsatHub and connected my phone to the WiFi network. Much to my surprise, I started receiving text messages to my Verizon phone. More to my surprise, I was able to reply to those text messages and have a back and forth conversation. The satellite terminal has its own SIM card and SMS capabilities, but these messages were received over my Verizon SIM. I've searched the net for any documentation on network ports used for WiFi texting and calling. I found this and it does include TCP53 and UDP53 (which I assume are for DNS lookups), but it also includes UDP500 and UDP4500 which are standard IPSEC VPN ports. Does anyone have an idea of how this was possible? How the heck was I able to send text messages over WiFi with my Verizon phone when I've blocked all outgoing ports except those needed for SSH and DNS? Is it somehow related to accepting all inbound traffic? Could Verizon be re-appropriating port 53 for WiFi texting? Edit: When I returned home, I connected my phone (airplane mode, wifi turned on) to my home WiFi network, sent a test text message and took a capture of the network ports in use. The only ports I saw in use between my phone and Verizon-owned IP addresses were ports UDP500, UDP4500 and TCP443 (500 and 4500 being used for the IPSEC VPN ports I mentioned earlier). 

In my lab network I have a Juniper MX 80 configured as MPLS router. On one site there is a customer connected (simulated by a single PC) and on the other side is an MPLS network. I use tcpreplay to generate traffic load. The PCAP file I use has been prepared as follows: 

Benjamin Dales answer is certainly correct for switches running standards based spanning tree. For switches using per VLAN STP however: yes, it can very well make a difference. Granted, under rather specific circumstances. As you said, the bridge-id is basically made up of three parts: , where the MAC address is just appended. The lowest bridge IDs wins the root bridge election. That being said, it is obvious that the priority must be the same on all routers in order for the extended system ID to actually make a difference. Suppose a network just consisting of two switches. SW1 has a MAC address of 1 (just going to short that up here, suppose those were actually 48bit MAC addresses), SW2 has the MAC address 2. Both use the default priority (32768). When running only PVSTP (no matter if rapid or not), the VLAN ID of that instance is used as extended system ID. In that case (for the sake of that example we're running in VLAN 1), the bridge IDs would be 

We have a cabinet of dedicated servers with a Brocade FESX648-PREM switch at the top. We own all the hardware and all the MAC addresses are known to us. We also fully manage the servers so we retain administrative access, but our customers have administrative access as well. Currently we use VLANs and subnets to isolate the servers, prevent IP hijacking, etc. However, we want to make more efficient use of our IPv4 allocations. We lose a lot of usable IPs for gateway purposes and we have an abundance of subnets too small for many customer needs. I've considered progressively migrating the servers to a single VLAN and use static ARP to prevent IP hijacking. We'd lose layer 2 isolation, but I'm not sure what sort of abuse that would open us up to. What are the dangers of placing all of our servers on a single VLAN with only static ARP bindings to prevent IP hijacking? In other words, what forms of abuse would be possible with a single VLAN config that would not be possible with every server on its own VLAN? Are there any precautions we can take to prevent the abuse while achieving our goals of more efficient IPv4 usage? 

This ended up being a failing switch. A couple days later we started having issues on ports 37-48. The FESX648-PREM is powered by port ASICs which control port regions. Those regions are: 1-12, 13-24, 25-36 and 37-48. One of the failure modes on this box is that a port ASIC can die and cause forwarding problems. The "bad server" above, was the only server we had in use on the 37-48 region. So when we switched the port and re-tested, we had the same result because the failing ASIC affected multiple ports. We replaced the entire switch and that resolved the issue. 

According to my knowledge, both of them are internal routes (received from an iBGP peer) and both are not synchronized. Hence they should not be considered as candidate for best path and R1 should not advertise that path any further. However, as seen before, it has still selected path #2 as best and advertises the path further to other RR-clients, for example to R4: 

This however does not work (no default route is advertised). Preferred solution is to use a dummy static route like this which is actually working. 

The basic switching functionality will be there right away. This means normally all ports are enabled and assigned to the same default VLAN (usually 1). Further, usually there is Spanning Tree active by default. So long story short: You can use the switch right out of the box, you however have the possibility to add and configure other features (for example VLANs, basic routing, security features, or other things) as required. 

The short answer is: no, you can't (with OSPF), at least not in a nice way. In OSPF the whole DMVPN network within the DMVPN cloud (that is, at least the tunnel interface needs to be in the same area (area 0 in your case), just because your mGRE interface on the hub routers cannot be in different areas. This is the reason why stub areas (there are no stub routers in OSPF) won't help you out. By design you cannot filter LSAs within one area. Further you need to learn the routes of both main offices down at the spokes. Technically you could place an ACL outbound on the tunnel interfaces of the hubs, denying the traffic of the remote main office. At least in this case the traffic is not sent through the hubs, possibly tearing down their Internet connection. My suggestion: do not run OSPF on top of DMVPN unless you really have to do so. Any distance vector protocol such as EIGRP (and yes, even RIP) would be better. You could also consider running BGP (with the hubs running as route reflector), giving you great flexibility. I strongly agree with Ron and Zack in the comments of your question. Ideally you would add a redundant connection between both head offices.