Physically, mass is not directly related to volume. OK, technically mass is related to volume, but based on another property: density. My point is that you could only apply a volume-based mass if the object has a specific density. Which is ultimately little more than just selecting the mass by selecting a density. Though if you have breakable things, having a density is useful as it allows you to compute the mass of the broken object(s). In general, the masses of objects that are physically modeled are usually chosen in a videogame based on the actual results of the physics simulation, not realistic physical properties. Game developers pick what looks good and behaves well, not what is necessarily "correct" in any real-world context. 

The ability to use the same texture for different surfaces. Object-space normal maps can only ever be used on the surface for which they were created. By regularizing normals in tangent-space, you gain the freedom to use the same bump texture for different surfaces. As long as the surface has a tangent-space basis, you'll get reasonable results. The ability to modify the texture mapping, with UV animation for example. Since object-space normal maps store normals in object-space, you can't just add 0.5 to all of the texture coordinates and expect to get proper normals from the texture. You can with tangent-space bump mapping. Smaller component representation. Object-space normal maps must have 3 components; you can't drop one and reconstitute it in the shader. Tangent-space normals will always have a positive Z component, so you can only store the XY of the normal. So you can either get greater precision by using a or formats (32-bits per texel), or employ compression by using an RG-compressed image format (, 8-bits per texel) format. You can try to use S3TC on an object-space normal map, but good luck with what you get back. 

To parody Pokemon, you are not required to copy Pokemon. You simply need to reference it. Traditionally, this is done with obvious knock-off characters. So instead of Pikachu, you might have Zapachu or something like that. Just look at common parodies in fiction; that's generally how they work. Parody, legally speaking, is protected. You're allowed fairly wide latitude to parody things. But be warned: if your parody strays too close to the source material, you could be in for a lawsuit, even if it is a parody. Where that line is depends on who's the copyright holder. Lawsuits are expensive even if you ultimately prevail. So I suggest discretion: try to make it clear that you're parodying it without taking too much directly from it. 

In general, you can assume that if a rendering system is not explicitly advertized as a ray tracer, then it's a rasterizer. OpenGL, D3D, REYES (much of it at least), and others are reasterizers. 

Since we're talking about RGTC, we're in GL 3.x+ land. You shouldn't be using anymore; just use . That way, if you change the type of , everything will still work. But that's minor. Also, is better spelled . But again, minor. The main thing is this: accessing a (non-shadow) texture always returns a 4-component vector. It doesn't matter what the texture's format is; it returns 4 components. If a texture doesn't actually provide all of the components, then the missing components will be filled in by the vector (0, 0, 0, 1). Let's take your format: . This uses the style of compression, and it provides the color component. So it fills in only the red component; every texture access will therefore be (R, 0, 0, 1). If your format was , then you would get the and components; every texture access would provide (R, G, 0, 1). Therefore, if you want the red component from the texture, just ask for it: 

No, but OpenGL might ;) The base Vulkan specification only supports SPIR-V. However, Vulkan does allow for vendor extensions. And NVIDIA is already on-record on this matter; they will be providing a Vulkan extension to be able to shove GLSL into their Vulkan implementation. That shouldn't be taken to mean everyone else will. However, let's not forget that Vulkan is not going to magically appear everywhere, nor will OpenGL instantly vanish into the aether the moment the Vulkan spec ships. First, there are entire classes of users who will gain virtually nothing from switching to Vulkan. Who are getting good enough performance for their problem domains from OpenGL, so they don't need to. Pretty much every 2D game you've ever seen has little to gain performance-wise from Vulkan. Even the CPU power consumption gains from VK are minimal for them. Second, users who would benefit from Vulkan will still take several years to switch over. Now, if you want to understand my cryptic comment, it is entirely possible for OpenGL to start being able to take SPIR-V as its shading language. SPIR-V contains virtually every feature of OpenGL 4.5, with the exception of shader subroutines (a feature so terrible I won't even link you to the OpenGL Wiki article for fear that you might try to use it). Indeed, if SPIR-V were only intended to be used for Vulkan or OpenCL, there would be no reason for it to have so many GLSL-based decorators and such in the language. Vulkan uses descriptor sets, not binding points, so why does SPIR-V allow you to specify binding points for textures/images/etc? The same goes for many other GLSL-isms in SPIR-V. All it would take is an OpenGL extension to allow SPIR-V to be fed to , likely with a dedicated format enumerator for it. Will Vulkan kill GLSL? No. But OpenGL and Vulkan might. Though even then, it'd take a long time for it to die. 

"People" have been declaring the death of 2D games ever since 3D games came into being. Hell, Sony even tried to outlaw 2D games of any kind on the PS1. And what's one of the most well-remembered PS1 games? Castlevania: Symphony of the Night. A 2D game. 2D games aren't dead; they will never be dead. Even if you wiped every 2D side-scroller off the face of the Earth, there would still be puzzle games (Tetris, etc) and other simple 2D games (SpaceChem, etc). These would gain nothing by adding a third dimension. 

Why? That's up to you. There's nothing that says you can't dump the enemy data and reload it in the middle of a game. If you can't do that, it would only be because you did not properly structure your code to make that possible. 

The term "Material" usually refers to a specific set of surface characteristics, which are used by a lighting model to compute the reflectance at a point on a surface. But if it does, then "Material Pass" doesn't really make sense. Therefore, let's assume that by "Material", what you really mean is "a visual effect that I would like to produce, using one or more light sources and arbitrary other data." In which case the term "Material Pass" is a single shader that implements some part of this visual effect. Note that the typical terminology for this "Material Pass" is "Shader Pass" or something of the like, while the term for what you seem to call "Material" is usually "Effect" or "Technique". An Effect can be achieved by one or more Shader Passes. A Shader pass is a concrete piece of shader code, which has a number of parameters (uniforms/constants depending on the language). It will take certain inputs and produce certain output value(s). Multiple shader passes are composed with blending operations or possibly with direct shader feedback through render-to-texture. 

Ultimately, there are plenty of C/C++ tools out there which you can pull together into a game engine. But there is no one-stop-shop for them. There is no one engine that ties them all together in a nice, well-documented package. Which is why serious indie game developers either pick tools they like using and build what they need, or just buy something off-the-shelf. 

That's dealer's choice. However, XML has one significant advantage (beyond being the most prevalent and de facto lingua franca of textual data storage): XML schemas can do integrity testing for you, for simple cases. There are even schema-guided editors that can help prevent users from doing the wrong thing at edit time. I'd use whatever is easier for you as a developer. Pick the syntax you like best; there will be tools for reading it. Personally, I'd use Lua, as I would also be using Lua for scripting, so I would have it handy. 

Yes, you are. Transformation being done in shaders is meant to be literal. "Transformation" in this case being the application of some transform to the various per-vertex attributes. Where that particular transformation comes from is generally irrelevant to the shader. It is given a transformation, and it applies it to the vertex data for the object. 

Again I'm not sure how you copied that wrong, but the "0.2f" for the W component should be "1.0f". The source of your segfault: 

Id Software is not "the industry". They are one company. While they may be influential, they aren't everyone. I've worked on a couple of game engines that date back to 1999, and they used C++. The principle reasons for the adoption of C++ "around that time" are: 

It is possible that this could be the case. Unless you're doing physics for the collision on the GPU, it means squat for that. But in terms of the physics itself... it's possible. If you're simulating the movement of a number of bodies, they tend to move in a very predictable way. Forces and force fields (ie: gravity) are easily predictable. Where things move is quickly computed. Right up until one thing hits another. See, in physics, you have what is called a timeslice; this is the amount of time that the execution of the physics system covers. If your timeslice covers 1/30th of a second (30fps for the physics update), then each physics update moves objects 33.3 milliseconds into the future. When objects don't collide, you can just move them from the beginning of that 33.3ms to the end. The physics for doing so are simple and has been well-known for centuries. You just determine the acceleration from the net forces, apply that acceleration for the timeslice to the object, and move it at its new velocity (note: this can be more complex if you want greater accuracy). The problem is when objects collide. Suddenly, now you have to process physics forces within a timeslice, rather than just once at the beginning. If an object collides twice or three-times within a physics frame, then that's more physics computations you have to redo. If you have a lot of collisions within one timeslice, you can really kill your framerate. However, the chance of multiple collisions within a timeslice decreases as the size of the timeslice decreases. High-end racing sims like Forza and Gran Turismo run their physics systems at incredible framerates. I think one of them gets up to 300+fps on their physics update. Slow-motion is the effective equivalent of that. By decreasing the physics timeslice without also increasing the rendering framerate to compensate, the world appears slower. And therefore, you make it much less likely that you get multiple collisions within a timeslice. That being said, I do doubt that this is why games like this go into slow-motion. In general, it's more for visual flair and dramatic presentation. Those physics systems can generally handle it, performance wise. 

What are the alternatives? Well, one alternative is to use Geometry Shaders, passing a primitive (4 vertices) and having the GS convert this primitive into a triangle strip. However, that's going to be slower to render, because one should assume geometry shaders are always slower to render. A better method is to use indexed rendering. Here, you have two alternatives: and with primitive restarting. The latter will result in a slightly shorter index buffer (one index for every quad). You can reuse the same index buffer for any quad rendering, so you won't need to update or modify the index buffer. I would also suggest using unsigned shorts as the index type. If you need to render more than 16384 quads at once, then make multiple rendering calls, using glDrawElementsBaseVertex to offset the indices. 

If you are going to allow your users to directly modify game data without forcing them to use a specific tool, then you're going to have to accept that users can do the wrong thing. With great power, comes great opportunity to abuse that power. So your options are to "force" users to use a specific tool (which would require some form of encryption on your data file. It won't stop dedicated hackers, but the hoi polloi will be warded off), or to do integrity testing at load time. Or you could split the difference. Let users edit text files however they please, but you introduce a "build mod" step between the text and your game. They have to run some command line tool that converts their text files into your game data. That's where integrity testing goes. You could also optimize your data in such cases, if text parsing is really that big of a turn-off for you. Of course, this makes it a bit more difficult to use, thus slightly slowing down turnaround time between editing the game data and seeing a change. 

If you want Lua to be in charge of this, then this means that Lua be in charge of this. With great power comes great responsibility. And if you want Lua to have the power, then the onus is now on Lua to have the responsibility to use it correctly. If Lua hands some "function" to C++, then it is the responsibility of the Lua script to ensure that this function is properly and transparently updated when scripts change. There are several ways to do this in Lua. One way is to build a simple stub function which searches some global registrar object for the actual function to call: 

I can keep going, but I think my point is clear. If you want to use to draw a text box, all you will get is a static, non-functional text box. Any actual functionality must be implemented by you. Doing text measurement and drawing glyphs? That's the easy part of GUI work. GUI stands for "Graphical User Interface". The last two words, where you take input from the user and do something with it, are the hard part. Game players expect GUI controls to work reasonably. In a PC-style environment (ie: mouse and keyboard), if players see a text box, they expect it to work like a regular text box. They expect to be able to move around in it with arrow keys, jump to the front and end with home and delete, select letters in it, etc. That means you're going to have to implement all of this UI code. You have to test what control was clicked. You then have to do something based on which control was clicked. You have to have the concept of an "active" control, the one that gets keyboard input. Different controls must respond differently to different types of user interaction. Basically, you're going to need a class. Let's say you're implementing a game options screen. So you have your different groups of options: gameplay, graphics, sound, controls, etc. So you have a list of different groups on the left side of the screen. Each group brings up a series of controls that the user can manipulate. What happens when the user presses the Tab key? Well, it depends on what control is active. If one of the group controls is active (was most recently clicked), then it moves to the next group. If instead one of the controls within the group is active, then it moves to the next control in that group. That's how the system is intended to work. So not only must an active control process keyboard input, it must now farm any unprocessed input to the control that owns it. Either that, or the controls must be in some kind of linked list, so that it can go back and forth. That means there needs to be default behavior in each control. This sounds more and more like a retained GUI, doesn't it? The only difference is that... you are the one doing the hard work. Using someone elses GUI is supposed to be a way to do less work. But the effort it takes to make a GUI respond as the user expects is non-trivial. Remember: the UI isn't for you, it's for your users. It's for the player. It should behave as they expect it to. And if it doesn't, then you have failed.