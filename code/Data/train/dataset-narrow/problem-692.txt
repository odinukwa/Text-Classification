I would advise you to drop mysqldump and move to Percona Xtrabackup. It will give you a filesystem level backup that can be made online. Restoring those backups will be orders of magnitude faster. 

One method to address this issue is to use a logical backup tool, such as mysqldump to backup the database. Refer to your my.cnf (usually under /etc/my.cnf) and then ensure you have the setting 

It could be related to indexes. Think about how a constraint could affect inserts. PK is unique by design and you have three columns and with your 3 col PK MySQL needs to check each time a new row is inserted whether that PK is unique. Why can you not simply use the 1st column as your PK? Secondary keys need to be updated when you add new data, ensure that you have expand_fast_index_creation turned on. FK relationships need to be maintained based on their inclusion in the table. What rippling effect does your INSERT have on related tables? On the other hand it might be due to your settings. Check and tune the following: innodb_buffer_pool_size innodb_log_file_size innodb_flush_log_at_trx_commit Some tweaks to these could spin you up a few notches. 

Get advice from the vendor. If you're using the commercial product you're paying for expert advice from Oracle themselves. 

Nismo, It's not the SIZE of the database that matters (no jokes intended), it's the rate of change coupled with the infrastructure available. For example, a relatively static database might perform poorly on 1Gb connection on an overloaded switch with 5400 RPM sata drives. If the rate of change (aka look at your log flush bytes) is less than around 200 MB/sec and you have very fast storage (or a ton of front end cache) and extremely low latency networking then I would say you'll be fine. Let's go over your questions now: 

I didn't bunch this with extended events, even though in reality it leverages the same framework. This is an enterprise only feature, and while it can do this and is targeted for items such as this... I feel most aren't running enterprise edition. $URL$ 

I'm assuming the keys haven't been rotated per your comment. If this is the case you are most likely able to restore these to a test server and restore your TDE enabled databases to said test server without issue. If this is the case, you could re-use these keys assuming nothing else is relying on the DMK/Cert in your current instance. Since TDE works through the SMK (and not by passwords) you're currently "at risk". Sure, it's running but who knows when or if something will happen. I stated before, take new backups. The most straight forward approach would be to remove TDE from the databases on that instance, remove the cert, and remove the DMK (After backing them up first). Then proceed to create the DMK/Cert/TDE enabled. It's going to be disk and cpu intensive for a bit (depending on the size of database and hardware/disk/etc). Backup the new DMK/Cert with a guarded password. 

Slave_SQL_Running_State: Reading event... Suggests that it's working on a large event. If you have MIXED or Row based binary logging and have changed a large amount of data on the master you might end up with a large event or series of large events. Use the mysqlbinlog tool to see what's going on with your relaylog. The master might show a concentration of binary logs over a small period of time (on the filesystem) 

Slave_SQL_Running: No The above indicates that although the replica is connected to the master it is no longer applying SQL statements to the dataset. This is due to the problems exposed in the 'Last_Error' field. You should resolve these issues either by aligning the data or skipping the transaction and syncing the data thereafter. You've likely arrived at this scenario because of data or database object inconsistencies between the nodes. You should explore monitoring the replication health using any of the popular methods for example using Nagios and Percona's Monitoring Plugins for MySQL. There is a broad choice in this space. 

You can write a simple script externally or use the MySQL event scheduler. Both will work just fine. The benefit of the MySQL event scheduler is that it will be part of your database backups. 

Since the certificate and permissions are all included at the database level, there should be no reason to re-create the certificate. Always On will take care of transferring the data to the secondary, it should just be there if it's synchronized or synchronizing but matching log block numbers in the DMVs. If you wanted the certificate to "be" in the "C:\Certificates" folder and constantly use it (not clear as to why from the questions) then it'll need to be on each replica. I'd also say that this would be bad as the keys are on the same server as the databases, which sort of defeats the purpose. 

In general, make a standard and use it. To directly answer your question, if all other filesystems are going to be formatted for 64KB, then why not stick to it - though, regardless, I'd go with the storage vendor's recommendation. Let's look per DB: 

Bjorn, There is nothing out of the box that will force the developers to use non-deprecated features (as they are deprecated, not removed). The most tailored solution that could be created is with DDL trigger(s), but note that they can be tricky... especially if not extremely familiar with them. I would suggest PBM, but it has limitations that won't necessarily capture everything you're looking for. As already discussed, you can use extended events (in the previous link I had) to check for the use of the deprecated items. Here is an example that will stop the use of fn_get_sql() and give a helpful message while stopping the use. Note that multiple checks can be made in the same trigger, this is just a trivial example. 

Joel, there is a lot of unknowns. If you post your my.cnf, queries, table definitions and queries, EXPLAIN plan output, then you might get more precise answers. You say "disabled query caching and other caching" what 'other caching' are you referring to? What steps did you take to disable QC? Did you restart your MySQL server between tests to flatten any buffers? If no and the KVP table was smaller and fit into RAM then InnoDB might have been able to serve queries fitting a certain criteria from data in the innodb buffer pool. You also refer to memcache. With MySQL 5.6 there is a memcache api. this means you can use memcache calls to access data stored in innodb. This will not be incidentally speeding up your queries. There is also another method of bypassing the SQL layer within MySQL called HandlerSocket. These are a couple of subjects that I would point you at should you continue to desire this type of access to your data with the added bonus of having MySQL persist the data to disk and also be crash safe through InnoDB's crash recovery process. 

You should consider some group ID within this or another relataed table. Marking accounts with the same group ID you can have 1 or more of them as an enhanced account type. Make sense? 

In-Memory objects (in this case, tables) are not just pinning data into memory. They have entirely different memory structures and are lock and latch free. There are additional differences in how the data is stored, how it affects database startup, etc. 

Finding What Availability Group Listeners Applications Are Using To Connect Finding Which Connections Have Been Read Only Routed 

Logon triggers are special. Not going too far into it, they must exist in the master database and not any other, thus the answer to your question is: "It can't." $URL$ -Sean 

This is completely possible. In windows clustering, it's a shared nothing philosophy so this will require at minimum two separate instances. 

They need to know information about themselves as well. For example, assume that a full backup was lost but you have a differential based on the newer full backup and you have an older full backup. You could restore the original old backup, logs, then the differential when you finally got to where it needs to be and continue on... but how would you know where that needed to be if you didn't store the values somewhere? How would we as admins know what order items came in if we didn't have these values? 

Availability Groups don't have preferred replica. They have primary and secondary replicas, but not preferred. Windows Server Failover Clustering has preferred owners. Are you looking for who was the primary replica or who was a preferred owner? If it's the WSFC preferred owner you are looking for, then you'll have to scrape the cluster log. If it's the primary and secondary replicas, then nothing exists out of the box - you'll need to write your own. This data can be gathered from a few different sources: 

The "Log" folder SIZE in the SQL server root directory (X:\Program Files\Microsoft SQL Server\MSSQL11.MSSQLSERVER\MSSQL\Log) becomes too big , ~80 GB. When I checked it, I see that there are a lot of SQLDumpxxxx.mdmp /SQLDumpxxxx.txt files in that folder. what to do with them ? can I delete them and if it is good thing to do ? 

The answer is yes ONLY if the files are not in use. USE [master] GO CREATE DATABASE [YourDataBase] ON ( FILENAME = N'Path....\FileName1_Data.MDF' ),-- Data files (mdf/ndf) ( FILENAME = N'Path....\FileName2_log.ldf' ) -- log files (ldf) FOR ATTACH GO 

Transaction Log - when you execute a DML operation (Data Modification Language = INSERTE/UPDATE/DELETE) it is been writing to the transaction log. "select .... from " are not written to the transaction log. If only DML operations are saved, so the question is "why ?" Answer: To give us the ability to Undo/Rollback transactional operation, and in case of server/app failure, if we had a transaction log backup, we can restore it by roll-forward the transaction log . Memory (buffer cache/pool ) - in sql server the CPU working with the memory only. When SQL reads Buffer/Block (Block = in sql server the smallest data unit, 8KB) From the Disk he search it in the Buffer pool(memory) and if it doesn't exist there then he reads from the disk. 

Mysqldump is single threaded so you’re bound by the speed that 1 core can run at to play all your SQL commands in serialised fashion. There are some config variables that can be changed to reduce the amount of disk flushes needed and permit larger memory buffers for speed. Innodb (if that’s what engine you use) variables (plus others) 

The right direction for this is to use an audit logging plugin. There are some choices out there. Please check out the following post for more information $URL$ My personal preference is the Percona Audit Plugin but you should be using Percona Server (MySQL Alternative) for this. 

You should use the MySQL event scheduler to run a a query to update all rows where the age of the row is greater than your threshold and have it execute the update at a suitable interval. 

This all depends on the size of the table. You're doing a full table scan without a where clause so an index isn't going to help. The use of the rand() function for sorting is going to produce a temp table and this will harm performance. If you have a primary key you could generate a random number in the app and perform a query where a single row is selected and can make use of the clustered index. You're likely to be serving this all from memory if you're using the InnoDB storage engine. 

Yes! It's the same error as if you were disconnected for any other reason. Depending on the driver used, the error checking may be different. For example if you're using .Net and the ADO.Net sqldata client you can use the StateChanged event to know when you connect or become disconnected. If your delegate is fired, check the connection. If you are disconnected, attempt to connect again, etc. 

The job started 2:14:40 ago, which is the time of the last known good checkdb - that's true. You started it then, it ran for 2 hours and 14 minutes +/- some agent stuffs. The times look fine to me, it's not like the job ran for 5 seconds and shows a last good of 6 hours ago. I believe the issue is you expect the CHECKDB to show the good value at the end of the run versus being updated when it was executed. That shouldn't be a problem. 

SQL Server and the SQLCMD utility do not, by default, track any of this. If you haven't yet setup customized logging then you won't be able to get any information. SQLCMD is a command line utility and does not keep any history itself. You can setup extra command line logging outside of SQLCMD, but there is no switch or setting to turn on to default log any and all SQLCMD input or output. The closest thing would be to create an extended events session or server audit that looks for connections through SQLCMD... However this would not be extremely worthwhile as application name and information can easily be spoofed. In short - if you don't have anything setup, there is nothing. If you did set something up, there is no guarantee that you'll get everything. If the question changes to "Who is executing what on my SQL Server?" this becomes an entirely different discussion.