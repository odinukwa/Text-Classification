How can I make a progress bar appear in Unity that doesn't make use of the OnGUI functionality in C#? I'm doing something with an Oculus and the OnGUI stuff doesn't work too well in a stereoscopic setting. I'm open to hear suggestions or try code samples as I can't seem to think of a way to display this in Unity and every google search brings up OnGUI examples. 

I'm currently in the process of making a scoreboard for my game. One of things I would like to display is the players accuracy in the amount of hits they had in game. However, I have never done this before and I've no idea how to go about doing this. Is there a commonly used algorithm out there that can help me calculate this, or has someone found a way to calculate this fairly easily? Any help with this would be appreciated. 

I'm trying to implement 5.1 surround sound in my game. I've set Unity's AudioManager to a default of 5.1 surround and loaded in a 6 channel audio clip that should play a sound in each of the different audio spots. However, when I go to run my game, all I get is flat sound coming out of my front two speakers. Even then, these don't play the sound they should (front speaker should play "front speaker" right should play "right speaker" and so). Both speakers just end up playing the entire sound file. I've tried looking to see if there is a parameter that I have missed, but information on how to set up 5.1 sound in Unity is lacking (or my google skills aren't that good) and I can't get it to work as intended. Could someone please either tell me what I'm missing, or point me in the right direction? My audio source is situated at point (0, 0, 0) with my camera also being in the same point. I've moved about the scene but the same thing happens as I've already described. 

I'm trying to make an object rotate up on the Y axis 90 degrees, then stop. I've got the rotating up bit working fine, it's getting it to stop once it hits 90. Some of the things I've tried include the following: 

However, I'm stumped for ideas on how to implement such a method. Can anyone give me some ideas on how I can do it? If you've made a match 3 game, and have implemented a similar method; how did you do it? 

This class also deals with what the XML should do once it has been read in. However, when I build the game and run the exe, this file isn't called. I know that I can store this file in the C drive, but I want to keep everything in one place so when I start to release what I'm working on, the user doesn't need to do anything. Am I doing something silly which is causing the XML not to be read? 

I'm trying to make a target lerp between two objects based on a timer. At the moment, I have the following code: 

I'm trying to create a bunch of different game objects from a settings file I'm passing in at run time. I've checked and my game is reading in settings fine. But when I run the program, I get the following error: 

To control the left thumb stick, I've simply attached the my script to a first person character controller Unity provide. I know my code is updating every frame, but I assumed that it would only update as and when I press the right thumb stick. If that isn't the case, is there a way I can limit it to only increment when I move the right thumb stick? 

I'm trying to have my game spawn enemies when ever the player reaches a way point. Right now, I have this functionality working. When my player gets to the first way, the enemies spawn. He only moves on once he has killed all of the enemies on the screen are dead. However, when he gets to the second way point, no enemies spawn. Right now, in my collision class I call the following line of code: 

But this doesn't work. I've spent all weekend trying to find a solution to this, but can't. Does anyone know what I need to do in order to genereate these dll and .lib files? I've downloaded all of the different SDK options from autodesk, I've searched for the 2012.2 version of the SDK, but got nothing. Failing that; can anyone recommend a good library. or way to implement 3D model loading into my project? I've tried looking at AssImp but found it a nuisance to work with. So I would like to avoid if possible. 

I'm currently trying to make a hand / finger tracking with a kinect in XNA. For this, I need to be able to specify the depth range I want my program to render. I've looked about, and I cannot see how this is done. As far as I can tell, kinect's depth values only work with pre-set ranged found in the depthStream. What I would like to do is make it modular so that I can change the depth range my kinect renders. I know this has been down before but I can't find anything online that can show me how to do this. Could someone please help me out? I have made it possible to render the standard depth view with the kinect, and the method that I have made for converting the depth frame is as follows (I've a feeling its something in here I need to set) 

But, I still get two cameras appearing in my scene. So I'm confused as to why this is happening. Here is the code that is acting on the information received from the settings file: 

My custom size should be at the very bottom, but isn't. Is there something I haven't done, or missed, that will get unity to take in my custom screen size when it comes to running my game through its exe? Oddly enough, inside the unity editor, my custom screen size is picked up and I can have it set to that in my game window: 

I've done a little research and don't know if the following is possible. At the moment I have created a small application in Unity that generates an XML file. This file will be used to help set up my game. It's done in Unity due to it being cross platform with no need to re-write a single line of code. Eventually this will run on an iPad. However, my game will be running on a linux computer and I need to pass over the XML file to the computer that will be running the final game (please don't ask why I'm doing that, it's something I need to do). So what I want to know is the following: Can I generate my XML file on an iPad and have that XML file be saved, and transmitted to a linux machine, without the need to manually copy the file over. If so, how is this possible? 

Found a work around: When I build my project, I need to manually drag my XML file into the Resources folder Unity create in the Game_Data folder when it builds the exe. 

Apart from using a plug in or other third party device, is there anyway Unity can support bluetooth communication between two devices? All of the links I have found seem to relate to third party options which I want to try and avoid. 

I've been looking for the answer to this for some time now, but cannot find anything online that is helpful. What I want to know is the amount of players that the UDK can support on one single machine. An example of this would be golden eye on the N64. On that, you could get 4 players all playing the same game at the same time using split screen. Like in this image: 

This code is in my update function and is attached to each of my objects that I want to move up and down. Each object is able to set their wait time independently of the others, thus i can have 1 move after 5 seconds, another after 10 etc. Then, each target waits a few seconds and moves back down. However, the movement isn't smooth and it tends to jump a set distance. But then, when it gets back to the bottom it goes crazy between the _movedown bool and wont move. Does anyone know of a way I can fix these issues? I do know of the Mathf.PingPong method that constantly moves the object back forth between two points, but that wont allow me to pause the movement at each section. Though, if someone knows a way I can do this, please let me know as well. 

At the moment I'm looking at getting a game engine to run in a CAVE environment. So far, during my research I've seen a lot of people being able to get both Unity and the Unreal engine up and running in a CAVE (someone did get CryEngine to work in one, but there is little research data about it). As of yet, I have not cemented my final choice of engine for use in the next stage of my project. I've experience in both, so the learning curve will be gentle on both. And both of the engines offer stereoscopic rendering, either already inbuilt with ReadD (Unreal) or by doing it yourself (Unity). Both can also make use of other input devices as well, such as the kinect or other devices. So again, both engines are still on the table. For the last bit of my preliminary research, I was advised to see if either, or both engines could do distributed rendering. I was advised this, as the final game we make could go into a variety of differently sized CAVEs. The one I have access to is roughly 2.4m x 3m cubed, and have been duly informed that this one is a "baby" compared to others. So, finally onto my question: 

I'm trying to create a tool using Unity to generate an XML file for use in another project. Now, please, before someone suggests I do it in something, the reason I am using Unity is that it allows me to easily port this to an iPad or other device with next to no extra development. So please. Don't suggest to use something else. At the moment, I am using the following code to write to my XML file. 

Could someone please help me and point me in the direction of a good base light class, or where I could learn how to implement one myself? 

Does anyone know of any good research / books on distrubuted rendering? It doesn't need to be specificly for games, just the topic in general would be very useful Does anyone know if other developers have managed to get Source and the CryEngine to run in a CAVE system? Through all my research I haven't been able to find anything on this, but then my google skills aren't the greatest. 

At the moment I am trying to implement fbx model loading in my DirectX 11 code. This is old code I had create for use with DirectX 9 and I'm simply trying to bring it back up to date. Since I first create the code, the SDK version I used was 2012.2, not autodesk have released 2014.1. With that, I have managed to update all of my code bar one issue; my sdk seems to be missing all of the .lib and .dll files I need. In my old code I needed add the following dependency: 

Whilst this work for the first way point, the second one wont spawn anything as the game object my collision class has been attached to has been destroyed. However, all of my enemies are prefabs and I thought the destroy function would only destroy that instance of the prefab. No matter when you called the instantiate method, it would only destroy that instance. I'm spawning my enemies with the following method: 

Which is exactly what I want. However, each time I press the button that has the WriteXMLFile() method attached to it, it's write the entire lot again. Like so: 

I've been trying to find out the maximum amount of xbox controller Unity3D can handle on one editor. I know through networking, Unity is capable of having as many people as your hardware can handle. But I want to avoid networking as much as possible. Thus, on a single computer, and in a single screen (think Bomberman and Super Smash Brothers) how many xbox controllers can Unity3D support? I have done work in XNA and remember that only being capable of support 4, but for the life of me, I can't find any information that tells me how many Unity can support. 

From what I can see, there is no missing bracket in that statement, and no google search I do seems to yield any result for find a solution. Is there something wrong with my what I have put so far, or I have got it totally wrong? Please help me fix this bug. Thanks. 

So while the middle screen is fine to render, I want to spread the image all around the player. That is both to the side and, the bottom and top of the player. Is such a thing possible? Has anyone ever done something similar? 

I'm trying to make my own first person camera controls in unity that use both thumbsticks on a joypad. I have this working fine. However, when I move the right thumb stick (the one uses to look left / right & up / down the camera begins to constantly update. Here is my joypad movement code: 

At the moment, my game can open up an XML file inside the editor when I run it. In my XMLReader.cs I'm loading in my file like so: 

My experience with Unity in not that in depth. But I am wondering if it possible in unity to split what is being rendered across multiple screens. It's hard to explain, so here is an image: 

I'm creating a game that utilizes off center projection. I've got my game set up in a CAVE being rendered in a cluster, over 8 PC's with 4 of these PC's being used for each eye (this creates a stereoscopic effect). To help with alignment in the CAVE I've implemented an off center projection class. This class simply tells the camera what its top left, bottom left & bottom right corners are. From here, it creates a new projection matrix showing the the player the left and right of their world. However, inside Unity's editor, the camera is still facing forwards and, as a result the culling inside Unity isn't rendering half of the image that appears on the left and right screens. Does anyone know of a way to to either turn off the culling in Unity, or find a way to fix the projection matrix issue? 

I've trying to bring in surround sound audio into my project. I've set my computer up to run in 5.1 and when I play a 6 channel audio through windows media player (it's a test audio that does left speaker, right speaker etc) it works fine. However, when I run it through Unity, all I get is the front 3 channels. I've set it in the Edit -> project settings -> audio to be 5.1 in there. I even set it in code with following: void Start() { AudioSettings.speakerMode = AudioSpeakerMode.Mode5point1; } How ever, when I run a debug line of: print ( AudioSettings.driverCaps); It tells me that Unity is only playing in stereo. Is there something I'm still not doing? I should also add I've ran 10 different tests using the 3D audio pan and spread options. I've set both to either being fully off, half way on and full. Still the same results.