Our company network (which I believe is a windows domain run on server 2008) is painfully slow. A prime example of this is copying files over SMB - listings take minutes, and copying even modestly sized files takes even longer. When approached regarding the issue, the IT manager (who, in spite of whatever other merits he might have, is a very stubborn and not a very technical person) throws his hands up, becomes very defensive and gives excuses instead of listening and attempting to work out the root cause of the problem. Now, recognizing that the human element of this problem will take some time and effort to solve, I am left not knowing quite how to technically rebut his excuses. In this instance, he claims that SMB is the problem, and that it is a "slow" protocol. Is there any evidence for this statement (I only have anecdotal counter evidence)? What is the best way to make headway in this argument? 

On an XP Pro box, using I set up a shutdown script to log something over the network. However, the script was failing, and after some research, it appears that the network is practically the first thing to go while shutting down. Because there will always be a user logged in when the box shuts down (it runs a daemon that cannot be configured as a service), I tried making it a logoff script - and it worked! My dilemma is this: is there ever a time when XP Pro does not run these logoff scripts when the machine is going down (aside, obviously, from a total crash)? Is there a better way to ensure that a certain action is taken for every shutdown? Thanks in advance! 

In trying to set up a highly custom periodic folder synchronization setup for synching a directory on an XP machine to an NFS export on a remote (debian) box, I've become... contemplative, and am second guessing myself (partially because I'm more a developer than a systems guy, and you don't know what you don't know, you know?). The network is assumed to be highly unreliable, and network bandwidth is at a premium and these files can be quite large. I have zero control over the export or the remote machine hosting it (so I can't set up some sort of custom daemon there), but I can do as much computation on the XP side as I want to. The solution I came up with was to compare file size ( L bytes locally, R bytes remotely), and only append transmit (L-R) bytes to each file (i.e., to send only the delta). My questions are twofold: 1) I don't want to reinvent the wheel. Is there a better, standard way that I can perform this operation reliably? Because I'm not sure if I really want the answer to #2 ;) 2) If this sounds like a fine solution, what are some more off-beat edge cases, things to guard, etc.? Thank you in advance! 

RedHat 6.2 Apache 2.2.15 I've installed a new SSL certificate on my apache server and updated the /etc/httpd/conf.d/ssl.conf file to include the new details: 

I have 2 old servers, which I am not using any more, which have been replaced by 1 new system, and I want anyone who tries to go to the old domain names, to be redirected to the new one. So, for example, anyone who tries to go to a.mydomain.com or b.mydomain.com should be redirected to new.mydomain.com What was done was that a member of the network team simply changed the IP address associated with a.mydomain.com and b.mydomain.com names to point to the new server, where new.mydomain.com is being served. This works fine if you go to the top level, e.g. if I go to $URL$ it will redirect to $URL$ perfectly fine. However, if I try to go to a full file URL of the old name, e.g. $URL$ instead of redirecting to $URL$ it redirects to: $URL$ (Missing the slash "/" ) and therefore obviously fails, because that's not a valid url. They said it was a server configuration thing, so I've tried messing around with the virtualhost on the new server. I've tried using the old domains as ServerAliases, i've also tried actually just setting them up as separate virtualhosts to point to the same document root, but I am still experiencing the same problem each time. Does anyone know what might be causing this? And if it's something I need to change on the server, or if it's something the network team need to change in the DNS? (I am using CentOS and Apache) Thanks. Edit: This is what the virtualhost on the new server looked like originally: 

And I even commented out the new.mydomain.com virtualhost, but whichever domain you go to in the browser it goes to the same documentroot. 

Installed service-pack 3? You want to remove/disable services and background apps one at a time to eliminate the one which is leaking connections. This is going to be an application that makes a connection but gets slow responses from the remote host. I would look at the internet connection (I assume LAN or WAN) external link speed and would not mess with the antivirus - its not there, its more likely another application that's at fault. I would start with internet browsers and other remote connection using programs. $URL$ 

never use "more" in powershell. more uses the default formatter (which format-table also uses.) if you had used the Format-list commandlet, you would have found that get-help returns an object with a 'pssnapin' property or a 'modulename' property. 

I assume you are wanting a shutdown event? It is basically impossible to run a script when a shutdown event occurs, logoff, yes, but shutdown no. I have a C# app that listens for the shutdown message, but when the message arrives, it is not possible to spawn any new processes, and the event is often missed. 

Windows sends the WM shutdown message to all top-level windows WM_CLOSE etc - but does to in serial order to all the desktop programs. If you are the last one to get it, you have no time left. Windows has a process load/unload module or dll lock that means when apps are terminating or unloading, that apps cannot load at the exact same instant - its a single threading "shareing-of shared stuff" protection problem. Done to save RAM, but catches us here by slowing things down at a busy period for the O/S. Windows has a internal flag to say don't start any new processes or scripts we are terminating. This means whatever listens, can only write to a remote file or use a TCP connection or similar to let somebody know its closing. You can normally not run any new scripts during shutdown. 

I've tried adding in a new virtualhost to serve the old domain name and test pointing it to a different documentroot, e.g. 

After restarting httpd and visiting the website it is still using the old certificate which expires in a few weeks. (I also tested by running the domain through $URL$ I confirmed that it is defintely using that ssl.conf file, by removing it and then the server failed to load. And then confirmed that it was defintely loading those files, by changing to an incorrect path and the server again failed to load. I ran the following from the terminal: 

And it showed the correct certificates (QuoVadis) I also tried it using the IP address of the server and again it returned the correct certificates. So then I tried using the external IP address and that is where it was returning the old certificates (TERENA), so that is where the problem lies. So I know what the issue is, but I'm not sure how to fix that. There is nothing about SSL in my httpd.conf file, but there are virtual hosts and NameVirtualHosts for http on the internal IP address. All the SSL configuration is in the ssl.conf file, where the virtualhost uses the domain name instead of either IP address: 

We have 2 separate VMWare environments, one is the main environment which has hundreds of virtual machines across lots of sites. The other is a much smaller one installed on one server, just for archiving old systems. What I would like to do is take a snapshot of the current state of one of our live VMs, and use that to copy across to the other VMWare environment and create a new machine there, using that as the archive of that system. Is this going to be possible/easy?