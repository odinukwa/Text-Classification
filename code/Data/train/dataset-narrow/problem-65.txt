Years ago, Pete Warden wrote either a blog post or perhaps it was an entry in one of the GPGPU Gems books where he described how to generate similar patterns. I believe that when he was at Apple, he implemented this as the "Cellular" generator in Motion. It looks like this: 

Given the image you have, there are a few ways you could create something useful from it. You could crop it or extend it so that it has a 2x1 aspect ratio and use it as-is. You'll get a weird seam where the left and right meet, but if you are handy with photo editing tools, you can probably modify it to look reasonable. Note that this will give you the correct image for an equirect projection, but it may or may not look how you want. That purple band would never be visible all at once. It would wrap around the entire field of view, for example. Another option is to make a cube map out of it. You could crop what you have into 3 equal squares for, say, front, left, and right. Then you'd need to generate the other 3 yourself. Generating a star field isn't too difficult either in code or using a photo-editing/compositing tool. Or if you know any artists, perhaps they could help you. 

Usually the resolution of the video is changed on-the-fly to match the size being displayed. So if you have video that has a 16:9 aspect ratio and 1920 x 1080 pixels and you turn your phone into the vertical position, you'll now have a width of 1080 pixels instead of height. So the width changed from 1920 to 1080. Now you have to figure out how to change the height by the same ratio: 

You do it by abstracting away the underlying technology. You know, for example, at a very high level that you'll need to work with images, geometry, and I'm guessing you'll have to write shaders to do your rendering. The textures and geometry can be in the same format for both systems. So from that, you can design an interface for rendering. You'll probably have a scene of some sort that has the geometry in it. The geometry will likely have the textures applied to it. When rendering the geometry you'll need to use a shader. Those might be good places to start with your abstraction. You might need a scene object that holds all the different objects you want to render. Each object is made up of geometry and the images that will be applied to the geometry. The scene object, the geometry objects and the images can be the same between OpenGL and Metal because they aren't specific to either. (I'm assuming here that the geometry is just a buffer of floating point coordinates, normals, etc. and the image starts out as a buffer of RGBA pixels on the CPU.) Next you need to come up with a way to render those things on the hardware you're currently running on with the framework you wish to use. So you create an abstract interface. Perhaps you have a object. It would have methods that allow you to add a piece of geometry with an image to use as the texture. It would allow you to associate a shader with the geometry. And it would have a method to render the geometry to either a texture or the screen. You then either subclass the class (assuming you're using something like C++) or write a new class that implements the protocol (if you're using something like Objective-C or Swift). You have 2 different subclasses or implementations. One that does things using OpenGL and one that does things using Metal. So you might have an and a . The class will implement the method by creating a Vertex Buffer Object and eventually calling to actually upload the geometry to the GPU. In your you'll create a via , and then copying the data into the buffer with . And so on for the other functions. So when writing the main code for your application, you'll code to the interface without knowing whether the underlying renderer that you're actually going to use is an OpenGL renderer or a Metal renderer. (In fact, you could even make it so they can be switched at run time if that would help in some way.) You can have a factory that either decides at run time which to use based on the OS you're on, or you can decide at compile time by only including the appropriate subclass. 

Your fragment shader would then just set the color of the fragment to the interpolated color coming from the vertex shader. Something like this: 

In the olden days, for things like fonts, they didn't do antialiasing. They generally hand made bitmap fonts because it generated the clearest, easiest-to-read results. That said, you could look into error diffusion, halftoning or other dithering techniques for changing continuous-tone images into 1-bit images. These are techniques for reducing the bit depth of images. 

It seems strange that any of these would happen. If that does turn out to be the issue, it would be worth investigating why it's happening. 

In general, non-linear function would mean that an increase in an input does not produce a proportional increase or decrease in output. Mathematically, any function that relates one value to another that doesn't follow a straight line is non-linear. So x raised to the power y is a non-linear function. This matters for colors because our brains don't perceive light in a linear fashion. If you double the number of photons reaching the retina, the brain doesn't interpret it as twice as bright. We have better perception in the mids and highlights than in the shadows. That is, we can distinguish bright values that are close together more easily than we can distinguish dark values that are equally close together. When encoding light values in a computer, we need to choose how precisely to record them without losing too much quality. If we use too many bits, then our hardware becomes more expensive. If we use too few then the image looks bad. Until the last ~10-15 years or so, it was generally considered too expensive to store more than 8 bits per red, green, and blue channel. The problem is that if you encode the light values linearly (where 2x causes your monitor to produce twice as many photons as 1x does), then images stored with only 8 bits per channel will have very obvious banding in bright areas of continuous color. But remember above that humans have a harder time seeing in the shadows than in the bright areas? That means that we can use 8 bits per channel, but not use them linearly. If we instead take each 8-bit color channel value and treat it as a value between 0 and 1, if we raise that value to a power then we have more values to use to represent mids and highlights, and fewer to represent shadows. This is called gamma-correction or gamma-encoding. We just have to remember to do the reverse (raise the value to the 1 / x power) when we want to output the values on a display. So if you're dealing with non-linear RGB, it is probably gamma encoded. sRGB is a popular encoding that uses a power of ~2.2. (In truth, it's a little more complicated than just raising it to a power.) So when someone tells you that RGB is non-linear, that's usually what they are referring to. It is entirely possible to store images in linear RGB, so long as you use more than 8 bits per color channel. There are also formats that use a logarithmic encodings, though they are less common outside of film production. CMYK is more complicated than RGB. Rather than having only a single thing that is outputting the color (the monitor), with CMYK, you have to deal with the medium you're printing on as well as the inks you are printing with. In a typical scenario, you print onto a white surface (such as a piece of paper). Ideally all visible frequencies of light should be reflected off of the paper and into your eyes or camera. When you add ink, the ink absorbs some frequencies of light and reflects others. (And I believe some passes through the ink, hits the paper and is reflected back as well.) The manner in which an ink reflects light is non-linear. Doubling the amount of ink may not cut in half the number of photons reflected back. So CMYK is also non-linear.