EDIT - Disk space Disk space on both servers is sufficient. MySQL is in /data for bdd1 and in /home for bdd2. 

With SSL, one IP = one certificate. If you want several certificates, get an extra IP. The exception to this is multidomain certificates that are signed for several subdomains. 

I have a tricky question for you, well at least seems tricky to me. Here is the setup. I have a linux file server and another linux server. I am looking to use the disk space from the filer server on the other linux server. The only catch is that I would like the data to be encrypted on the filer server to avoid have random people looking into the data. So to sum up, the data needs to be encrypted on the file server but mountable on the other linux server via NFS or some other protocol. The solution needs to be at least free and/or open source. Thank you in advance for your help ! 

You must be able to send out ARP requests for sure. What you can do is ARP all the IPs in your network one by one and see who answers. 

The serveurClient class includes the apache class. This works fine as Apache gets installed and all the configuration gets applied correctly, except the virtual hosts. The configuration relating to the virtual hosts is the following : 

I have actually had to solve such a problem recently. We have 8Mbit/s for 150 PCs. The problem was not so much regular bandwidth use but people who would download big ISO files and kill the bandwidth for everyone else. We handled this by inserting a caching web proxy (Squid on Pfsense) that allows for 2 bandwith limiting parameters. First one is max global bandwidth which is the speed of the line. Second is maximum bandwidth for one host. This is where this gets interesting. We supposed no more then 3 people would try to eat up all the bandwidth aht once. Therefore we set the second parameter at 1/4th (2Mbit/s) of the first parameter. This kept large downloads from killing the internet for everyone yet allowed for decent download speeds. All our problems then vanished. You don't want to set the max banwidth per host too low because it'll also be your maximum download speed. 

For myself, I'm happy with offline by-hand filtering, but I suggested Kemp's service to someone who had an underwhelming experience with Mollom, and I'd like to pass on more reports from anyone who has tried these or other services. 

You can tell iptables to only allow each IP address to connect once to port 110. There are a couple of disadvantages to that: 

Does anyone maintain lists of the most frequently guessed account names that are used by attackers brute-forcing ssh? For your amusement, from my main server's logs over the last month (43 313 failed ssh attempts), with not getting as far as : 

My own experience is that very little comment spam is intelligent, in the sense of getting around filters, in the way that email spam is. 

We see that there is one interaction scree process, 2298, which has /dev/ttyp3 open. Process 2299 also has this tty open, but process 1979 does not access any tty. So you can infer from this output which child processes are talking to which interaction processes. 

Look at twill, which gives you a command-line interface. It doesn't support Javascript, but it does support cookies and forms. The Mozilla project has a more complex offering, XULrunner, which is supposed to support the whole XUL runtime, but I don't know how well this works in practice. My gut feeling is that the semantics of javascript are hard to model satisfactorily with a browser-in-the-middle. 

Steve Kemp (again) has an xml-rpc-based comment filter: it's how Debian filters comments, and the code is free software, meaning you can run your own comment filtering server if you like; There's Akismet, which is from the WordPress universe; There's Mollom, which has an impressive list of users. It's closed source; it might say "not sure" about comments, intended to suggest offering a captcha to check the user. 

If you don't wan't to start doing some fancy networking, you can do Private VLANs. Higher ends Cisco switches can do this. 

OSSEC will also do integrity monitoring and is dead simple to use. It integrates very well with SIEMs such as Prelude if you are using that. Has a web interface available that will make checking the integrity information really easy. 

The setup The setup is very simple and straightforward. It's a pair of Debian servers with a Gigabit link in between. MySQL is stable Debian Lenny version and OS is Debian Lenny. Configuration The dump has been inserted on both nodes and replication has been activated. The "SHOW MASTER STATUS" commands on the master gives the following information : 

I think that is perfectly fine. It wouldn't be a good idea to put their LAN IP because if that ever changes, the DNS resolution may be broken. 127.0.0.1 will never change and will always point to the right ressource so you can just leave this as is. 

We are seeing absolutly no syslog messages related to this process so we're a bit lost... So, I am looking for pointers as to what this process does in general and what could be the potential cause of such CPU usage. EDIT : My /etc/network/interfaces is the following : 

You should really consider using SquidGuard or something similar. There are blacklists available on the Internet that you can download and use easily. 

I could go on for a while. I have switched my personal machines from Debian to OpenSolaris and am not looking back. 

I don't think you're not seeing the entire picture here. You are wanting to connect a file server at 10Gbps speed, which may sound like a sexy idea. The thing you are not seeing is the ability of that server to generate that amount of traffic reading from disks. Getting 1GBps from a file server is, today, a very good achievement. 10Gbps will not only be expensive as you have realized yourself but at minimum 90% useless. Your best option is to start putting in some blazing fast disks in your file servers if it needs to provide such great amounts of IOs. I strongly believe the "affordable" (notice the quotes) path to this is SSD drives in fast RAID configurations (that is RAID10). As for networking, a 4x1Gbps agregate will do the trick fine and you can even add more later. Watch out for the fact that internal buses (read PCI*) are not always capable of handling multi-gigabit speeds. This is especially true if you are not using server-grade motherboards. I believe this is your only "affordable" option here. Infiniband cards are not horribly expensive. I believe you can find some for ~150$ but the switch will be very expensive. 

Use rsync if you can. Rsync allows you to generate diff files that show what has changed, what exists on the target but not in the source, etc. It will make this kind of task much easier. There are several repackagings of rsync for Windows. 

The good news is that running a web server shouldn't look much different, in my experience, after the move, except that you can change some things faster because you don't have to ask anyone. If you enjoy looking after boxes, you will find a VPS much more fun than a shared host. Try running an experimental VPS for a few weeks before you commit anything to it. 

If you do want a host physically resident in the EU, use a European VPS. Go for something based in Amsterdam: that's where the AMS-IX International Peer Connection point, which means it is pretty central for most of the European internet, has as good connectivity to the US as you'll get, and has a huge number of VPS providers. Cf. The ICANN IP hubs map. 

If you're familiar adminning desktop systems with the same OS as your VPS, then you'll be spared the biggest shock that comes with VPS. VPS is to shared hosting what (remote) adminning a system is to having a user account on that system. Things you have to worry about with VPS that you didn't have to with shared hosting: 

You are trying to conserve power: eagerly swapping out will generally increase the amount of disk spinning, since some propertion of swapping out will not be needed; The swap might improve the zippiness of a process whose performance you might not care about, at the expense of a process whose latency is an issue. 

Data swapped out of RAM will exist both in RAM and on the swap disk until the RAM is claimed for another purpose, so are rather like mmap'ed files. As I understand it, if a process then decides to use this data before it is reclaimed, it therefore does not have to be swapped in again. Swapping is usually good, but there are two main reasons why you might not want it: 

I'm going to go ahead and preach the VPN option. All you have to do is setup a small and simple OpenVPN server and the web server. Generate certificates for each iPad, then allow only the IPs from the VPN to access the website in question. OpenVPN can be hard at first but once you start playing around with it, it should rapidly get simpler. The solution mentioned by Arenstar only works if there is no NAT between the iPad's and the server. I believe you do need a jailbroken iPad to install GuizmOVPN. It works really well. My boss uses his iPad to SSH into our servers and he's satisfied. The configuration of GuizmOVPN is not as simple as it should be but the tutorial on the website is rather clear. 

I would like Nagios to execute a Bash command/script when it detects a host down or up. This would allow me to react to down hosts to some degree which would be very interesting. How would I do this ? 

It may also be possible that your backend web servers are refusing too many connections from a single IP. With HAProxy (without tproxy), requests are seen coming from the load balancing node. This could create such problems. 

As I do not know of any tools allowing you to do this, my approach to this problem would be to create a script that goes on every server via SSH and does the modifications you want it to do. This could be a very simple Python script but I imagine other scripting languages can do this rather easily as well. I would use tools such as sed to do the text modifications. A more "rational" approach would be to use tools such as Chef or puppet to manage these configuration files but I have no experience with these (not a good thing !) 

When I started learning how to configure email, SPF existed but there were doubts about whether it was a good thing, and the value of offering SPF records in DNS. Now it seems that it is widely accepted that some form of well-known sender validation is good practice. Is this really true? Am I being a bad postmaster by not supporting SPF/DKIM/whatever? 

If you're using linux, 'iptables' allows you great freedom in choosing a policy for throttling new connections from IP addresses or IP address ranges. Try: 

Not explicitly, as far as I can tell! At least, nothing under /etc or /var/tmp mentions these IP addresses. But says something I can't make sense of: 

That stops a client accessing more than one mailbox; That might stop badly written clients who don't close old connections from making new connections, if they won't clear up their mess. I'm guessing this means that maybe you don't want what you think you want. 

You can tell which screen processes are linked to s by looking at the output of : if a screen client process is connected to the screen interaction processes, then they will share tty devices. So for instance, with: 

The only thing that happens to them is that they are emailed to me; the server does nothing else with them. I can approve comments offline, using Steve Kemp's chronicle. I prefer to have a lean set of filters on my server, to deal only with things that are dangerous. Nearly all are trivial to spot using mail filtering software, and that seems the right place to deal with comment spam. I prefer reviewing spam using mail rather than server logs. 

If you approve all messages that appear on your website, then don't use a captcha. There are comment filtering services out there that can analyse comments in a manner similar to mail spam filters (all links to the client API page, organised from simplest API to most complex): 

The setup We currently have a Freeradius server used to authenticate our Wifi users against our Active Directory server. The link between Freeradius and the Active Directory is done by Winbind. In order for the user to be able to obtain authorization, it needs to be belong to a group in the Activer Directory. This is done by adding an argument to the ntlm_auth command. What we are trying to achieve We are now adding 802.1X to our cabled networks and would like to re-use the existing Radius server to authenticate against the same Active Directory. Everything will be the same except the authorization will need to be based on whether the user belongs to a different one than that of the Wifi networks. What we have already tried I have read many things on freeradius in the documentation and have seen that it is possible to use conditionnals and variables. My plan therefore was to put a variable in the ntlm_auth command that would contain the group SID (as suggested on Freeradius mailing-lists). The group SID would be dependent on the IP of the network device which should be contained in "NAS-IP-Address". This should just be a case of writing a simple conditionnal statement and setting a variable. Nonetheless, I have not been able to do this as Freeradius will not start everytime I try to add a conditionnal to the configuration files. So my questions are : 

If it's web traffic, you're talking about, Squid can do this. It can restrict the max bandwidth for all users and the max bandwidth per user. I implemented this solution to fix this exact problem. If you ever want to use something else, do not use tc. It's a mess. Use OpenSolaris and flowadm which is muuuuch simpler. 

DNS: you probably have to set up and maintain a nameserver; Mail: likewise a mailserver; Securing ports and syslogging: clients on shared host systems have to worry about only a small fraction of these issues. Ssh daemon and user authentication. 

It's not my fault: my VPS providers have overlooked something. What might that be? 81.171.111.1 means I'm happy listening in on ARP requests that I shouldn't be: how do I change this? In any case, what does this mean? I'm looking in completely the wrong place for information on what my image is doing. Where should I be looking? 

The trouble was, as I didn't find out for myself, the ethernet interface was bound to the forbidden IP addresses, although only the gateway address was shown using //. would have listed the missing addresses; using on the two misconfigured ip addresses fixed the problem. I should really learn more about Linux networking — this seems like it should have been obvious to me to check. I found Daniel Weiss' Proxy ARP with Linux useful. 

I agree with Tom that this is only an attempt to subvert your comment system. I get hundreds of these, and similar, each day, and I don't bother trying to filter them on the server, because: 

Your emails are going to get marked as spam regardless of how you send them, if the recipients mark them as spam: GMail, &c, will learn from this, and mark other emails from you as spam. Make sure that people want to receive the emails you send. What perfomance you need will depend pretty heavily on what the social networking software will need, as well as on the number of users and what they do. As a rule, say, PHP sites are less resource intensive than Ruby sites.