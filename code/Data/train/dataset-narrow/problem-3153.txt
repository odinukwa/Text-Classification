With border mode "valid" you get an output that is smaller than the input because the convolution is only computed where the input and the filter fully overlap. With border mode "same" you get an output that is the "same" size as the input. That means that the filter has to go outside the bounds of the input by "filter size / 2" - the area outside of the input is normally padded with zeros. Note that some libraries also support the border mode "full" where the filter goes even further outside the bounds of the input - up to "filter size - 1". This results in an output shape larger than the input. There's a short explanation in.. numpy's convolve documentation: $URL$ 

30 samples probably just isn't enough. The more features you want to use, the more samples you'll need - otherwise you'll get huge model instability and overfitting (especially with bad/useless features). With only 30 samples you'll probably get the "best" results with 1 or 2 carefully selected features. Get 100 or 200 samples, then try again with 5 features. Also make sure you're standardizing your features - for example by removing the mean and scaling to unit variance. SVMs don't like features that are a lot larger than other features. 

I think this has nothing to do with Python but with mathematics. X is a matrix and y is a vector (most of the time). Usually upper case letters are used for matrices and lower case letters are used for vectors. That's why you often see something like this (from sklearn examples): 

Adaboost doesn't care about the order of the features, so no matter how many dimensions your samples have, you can just flatten them. Flattening them gives you a 2D X with shape (n_samples, n_features) which you can feed to the Adaboost classifier. 

It actually has to use a kernel that is 276 rows, 1 column and 3 channels wide since your input images have 3 channels.. it would be 276 rows and 1 column wide if you used grayscale images.. but that shouldn't matter. 

One problem you might run into with a NN (and other classification methods) is that since you've only shown it certain defects, it might not know how to react to completely new / yet unseen defects that might pop up in the future. You want the NN to learn "anything that doesn't look like a non-defective PCB is a defective PCB". But what if it has learned "anything that doesn't look like a defective PCB is a non-defective PCB"? You could try to modify some images of non-defective PCBs by adding a small white spot (or another small perturbation) to them at random locations and have the neural network classify these modified images. It should definitely classify them as defective, right? But it'll probably miss some (or quite many) because it has never seen such defects before. To detect completely new defects, anomaly detection methods / one class classifiers might be more.. trustworty, because they should pick up anything that's never been seen before. As D.W. said, you're just going to have to try both methods and find out which one works better. Just make sure to have a really good test set that also contains completely new defects! 

I think what you're seeing is normal behaviour: With only few samples (like 2000) it's easy for a model to (over)fit the data - but it doesn't generalize well. So you get high training accuracy, but the model might not work well with new data (i.e. low validation/test accuracy). As you add more samples (like 9000) it becomes harder for the model to fit the data - so you get a lower training accuracy, but the model will work better with new data (i.e. validation/test accuracy starts to rise). So: 

Yes, it seems to display unique samples, the others have been duplicated by the bootstrap sampling. There's the 0.632 rule - when you have N items and you take a random sample of size N with replacement (as bootstrap does), you only get 63.2% of the samples from N, the rest are duplicates. That roughly matches what you've seen: 0.632 * (507+545) = 665 unique samples. You can also try it with some Python code: 

"Training error is shown in blue, validation error in red, both as a function of the number of training cycles. If the validation error increases (positive slope) while the training error steadily decreases (negative slope) then a situation of overfitting may have occurred. The best predictive and fitted model would be where the validation error has its global minimum." 2) Parameter selection Your model needs some hyper-parameters to be set, like the learning rate, what optimizer to use, number and type of layers / neurons, activation functions, or even different algorithms like neural net vs SVM... you'll have to fiddle with these parameters, trying to find the ones that work best. To do that you train a model with each set of parameters and then evaluate each model using the validation set. Finally you select the model / the set of parameters that yielded the best score on the validation set. In both of the above cases the model might have fit the data in the validation set, resulting in a biased (slightly too optimistic) score - which is why you evaluate the final model on the test-set before publishing its score. 

Let's say you're working with 128x128 pixel RGB images (that's 128x128 pixels with 3 color channels). When you put such an image into a numpy array you can either store it with a shape of (128, 128, 3) or with a shape of (3, 128, 128). The dimension ordering specifies if the color channel comes first (as with theano / "th") or if it comes last (as with tensorflow / "tf"). The code you've posted contains the following line: 

There are two uses for the validation set: 1) Knowing when to stop training Some models are trained iteratively - like neural nets. Sooner or later the model might start to overfit the training data. That's why you repeatedly measure the model's score on the validation set (like after each epoch) and you stop training once the score on the validation set starts degrading again. From Wikipedia on Overfitting: 

Since no one has mentioned bayesian optimization yet: "Bayesian optimization is a sequential design strategy for global optimization of black-box functions." At the bottom of that wikipedia page are links to several bayesian optimization libraries like: 

By default sklearn.preprocessing.normalize normalizes samples, not features. Replace sklearn.preprocessing.normalize with sklearn.preprocessing.scale. This will center and scale (to unit variance) every feature. Also give it more than 10 epochs. Here are learning curves (log loss) for 5000 epochs: 

While the learning curves on the original / sequential data look like that (unusual because training accuracy is rising over time): 

Besides libraries using bayesian optimization there are also libraries using TPE (trees of parzen estimators), like hyperopt. Here's an image of fmfn's bayesian optimizer optimizing a non-linear 2d function. As you can see, a point close to the global optimum is found after relatively few iterations and the system tests more points in more promising (red) areas than in less promising (blue) areas - making it more efficient than random search: 

I'm not sure I understand your question correctly, but if you have input images that are 276 high, X=500 wide and have 3 color channels, then the following performs column-wise convolutions across the X-axis: 

As the training dataset increases, the training accuracy is supposed to decrease because more data is harder to fit well. As the training dataset increases, the validation/test accuracy is supposed to increase as well since less overfitting means better generalization. 

Since no one has mentioned RAM-efficient methods yet: Instead of loading everything into RAM, you can use online/out-of-core learning. Python's Scikit-Learn for example has the SGDClassifier class. Set its loss function to "log" and you get logistic regression. Using the partial_fit function you can feed it small batches of data that you read straight from the database (or from some CSV file,...). Vowpal Wabbit might also be worth a try. Its made for out-of-core learning - hardly uses any RAM and you won't find anything that's much faster. You could also use Python's Keras library to build a neural network (or in the simplest case just logistic regression), which you can also feed with small batches of data instead of loading everything into RAM. Compared to the other two recommendations a neural network could also learn non-linear dependencies. Besides that, try to start with fewer samples - plot the learning curves with 10k, 100k, 1M samples and see if 100M samples are even necessary to get a good score. 

The output shape of this layer will be (1, 500, 64). I inserted "64" as the number of filters.. you can set that to whatever you need / whatever works best. Note: shapes are specified in tensorflow mode (i.e. the order is (rows, cols, channels)). Depending on your configuration you might have to change those shapes to theano mode: (channels, rows, cols). 

These learning curves seem unusual - the training accuracy should start high and get lower as more samples are added. I suspect that your data is sequential (it has some kind of time dependency). sklearn's learning_curve function does not seem to shuffle the data (should it?), so the training accuracy can change/increase once new structures appear in the data over time. Here's a notebook trying to reproduce the effect: $URL$ Two images from the notebook: The learning curves on the shuffled data look like that (as expected):