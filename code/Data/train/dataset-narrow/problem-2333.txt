I was curious to know whether there has been any progress/work in the direction of getting lower bounds for problems like: Shortest Paths(with/without negative weights), Mincut, s-t Maximum flows, Maximum (cardinality/weighted) matching. Any references related to this are very much appreciated and helpful. Reference [ L92 ] N. Linial, Locality in distributed graph algorithms, SIAM Journal on Com- puting, 1992, 21(1), pp. 193-201 EDIT: As suggested by Robin Kothari in the comments, I am making the question more directed. 

Are there any non-trivial lower bounds on the running time of graph algorithms in RAM/PRAM/ models of computation ? I am not looking for the NP-Hardness results here. Following is a result that I could find [see ref L92]: 

The Vertex updates can be handled using edge updates as follows (Although a bit inefficient as it makes deg(u) calls to edge update function): 

The reductions are all polynomial time, so its easy to see that if M works in polynomial time then we can compute MAX-CLIQUE using M with polynomial time overhead. Using the above reduction we can also see that M is at least as powerful as the machine that solves NP-hard optimization problems. 

I want to find the current literature for the following problem (I have searched on google/asked friends/some Profs didn't get much useful results yet): 

I know this is somewhat related to TSP/Hamiltonian paths but I think that it is significantly different. I came to investigate this problem because I am thinking about solving this problem in a special geometric setting. So, It would be helpful If I could get to know the complexity class to which it is known to belong and any results for planar graphs, grid graphs. Thanks, Rizwan. 

A new approach to dynamic All pairs shortest paths, Demetrescu. et. Al, JACM 2004 Voume 51 issue 6, 2004 Slide of Talk on Dynamic graph Algorithms by Dr. Surender Baswana in "Recent advances in data structures and algorithms" workshop held at IMSc, Chennai. Camil Demetrescu and Pino Italiano, Dynamic graphs, Handbook on Data Structures and Applications, Chapter 36. Dinesh Mehta and Sartaj Sahni (eds.), CRC Press Series, in Computer and Information Science, January 2005. [Draft (pdf)] 

Let machine M solves the $\Sigma_2^p$-complete problem. We can use this machine to solve any "NP-Hard" optimization problem (which includes MAX-CLIQUE and MIN-COLORING) in additional time polynomial in size of input as follows: The solution is described for MAX-CLIQUE but the same idea works for any NP-Hard optimization problem. Let G = (V, E) and O = { $<G,k>$ : the size of largest clique in G is k } D = { $<G,k>$ : there is a clique of size >= k in G } Observation 1: Problem of deciding language O can be solved by making $\log{|V|}$ calls to the machine that decides D [Using Binary search] 

Since it is easy to check whether a graph is one of the graphs allowed by the Theorem, this provides us with a polynomial-time algorithm for the decision problem. Notes: (1) The proof of the theorem is not at all easy. (2) Once we decided that two disjoint circuits exist, it seems less clear how to solve the associated search problem, that is, how to actually find such circuits. The theorem does not give direct advice to that. 

From the common sense point of view, it is easy to believe that adding non-determinism to $\mathsf{P}$ significantly extends its power, i.e., $\mathsf{NP}$ is much larger than $\mathsf{P}$. After all, non-determinism allows exponential parallelism, which undoubtedly appears very powerful. On the other hand, if we just add non-uniformity to $\mathsf{P}$, obtaining $\mathsf{P}/poly$, then the intuition is less clear (assuming we exclude non-recursive languages that could occur in $\mathsf{P}/poly$). One could expect that merely allowing different polynomial time algorithms for different input lengths (but not leaving the recursive realm) is a less powerful extension than the exponential parallelism in non-determinism. Interestingly, however, if we compare these classes with the very large class $\mathsf{NEXP}$, then we see the following counter-intuitive situation. We know that $\mathsf{NEXP}$ properly contains $\mathsf{NP}$, which is not surprising. (After all, $\mathsf{NEXP}$ allows doubly exponential parallelism.) On the other hand, currently we cannot rule out $\mathsf{NEXP}\subseteq \mathsf{P}/poly$. Thus, in this sense, non-uniformity, when added to polynomial time, possibly makes it extremely powerful, potentially more powerful than non-determinism. It might even go as far as to simulate doubly exponential parallelism! Even though we believe this is not the case, but the fact that currently it cannot be ruled it out still suggests that complexity theorists are struggling with "mighty powers" here. 

Remark: Finding the max number of vertex- or edge-disjoint paths is well known to be solvable by network flow techniques in polynomial time. Does the geometric requirement make the problem harder? Edit: Let us assume that the vertices have polynomially bounded integer coordinates, in terms of the number of vertices. Furthermore, assume an oracle is available that can determine for any two edge-curves whether they intersect or not. When the edges are represented by straight lines, this is straightforward, but in the general case it may be hard to decide whether two curves intersect. 

The code on my website also has an initial check that the number isn't zero, which I've just realized is redundant with the v3, v5 zero checks, oh well. As I mentioned, the above method works for the simplified problem, but it really has no chance of working for the general one, because: In the general problem the precise value of every prime's exponent counts for deciding its general size and thus which lengths it has in various bases. This means that: 

The next step is then to re-encode the above in the exponents of a single variable automaton. As the result is pretty long, I'll just describe the general method, but a full version (slightly "optimized" in spots) is on my website. 

becomes . Individual and can first be changed into the combined form. and are unchanged. would be unchanged, except that in our case we still have one final check to do: we need to ensure that there are no prime factors in the number other than 2,3 and 5. Since our particular 3-counter automaton zeros the counters it uses when it accepts, this is simple: just test that the final variable is 1, which can be done by jumping to the code 

So people keep nagging me to post this even though it only solves a simplified version of the problem. Okay then :) At the end of this, I will put some of what I learned from the paper of Ibarra and Trân, and why that method breaks down on our general problem, but perhaps still gives some useful information. But first, we'll look at the simpler problem of trying to decide the set $L = \{ 2^n \mid $ the ternary and binary representations of $2^n$ have both even length or odd length$\}$ Note how this has $2^n$ rather than $n$ as in the original problem. In particular if the input number is not a power of 2, we want to reject it rather than attempt to calculate its length in any base. This greatly simplifies matters: If the original number is written prime factorized as $2^{v_2} 3^{v_3} 5^{v_5} 7^{v_7} ...$, then for all the $v_i$ except $v_2$ we just need to check that they are all $0$. This allows us to solve this simplified problem by using a wrapper around the old method (by Minsky I assume) of encoding the state of a $k$-counter automaton in the exponents of the prime factorization of the single variable of a multiplication/division automaton, which as noted in the OP above is pretty much equivalent to a 2-counter automaton. First, we need a $k$-counter automaton to wrap. We will use 3 counters, named $v_2$, $v_3$ and $v_5$. The automaton will accept iff for the initial counter values, the ternary and binary representations of $2^{v_2}$ have both even length or odd length, and both $v_3$ and $v_5$ are zero. When it accepts it will first zero all its counters. Here is some code for that, in an assembly format similar to the OP (I've just added variables to the instructions). I haven't actually tested it, since I have nothing to run it with, but I consider this a formality: 3-counter automata are well known to be Turing-complete, and to be able to construct any computable function of one of their initial values. 

Note that we require that the parameters are exactly equal to the given numbers, they are not just bounds. If you want to solve this from scratch, it might appear rather hard. On the other hand, if you are familiar with the following theorem (see Extremal Graph Theory by B. Bollobas), the situation becomes quite different. 

Is there a graph class for which the chromatic number can be computed in polynomial time, but finding an actual $k$-coloring with $k=\chi(G)$ is NP-hard? Without any further restriction the answer would be yes. For example, it is known that in the class of 3-chromatic graphs it is still NP-hard to find a 3-coloring, while the chromatic number is trivial: it is 3, by definition. The above example, however, could be called "cheating" in a sense, because it makes the chromatic number easy by shifting the hardness to the definition of the graph class. Therefore, I think, the right question is this: Is there a graph class that can be recognized in polynomial time, and the chromatic number of any graph $G$ in this class can also be computed in polynomial time, yet finding an actual $k=\chi(G)$-coloring for $G$ is NP-hard? 

Access to a $SAT$ oracle would provide a major, super-polynomial speed-up for everything in ${\bf NP}-{\bf P}$ (assuming the set is not empty). It is less clear, however, how much would $\bf P$ benefit from this oracle access. Of course, the speed-up in $\bf P$ cannot be super-polynomial, but it can still be polynomial. For example, could we find a shortest path faster with a $SAT$ oracle, than without it? How about some more sophisticated tasks, such as submodular function minimization or linear programming? Would they (or other natural problems in $\bf P$) benefit from a $SAT$ oracle? More generally, if we can pick any problem in ${\bf NP}-{\bf P}$, and use an oracle for it, then which of the problems in $\bf P$ could see a speed-up? 

I am looking for nice examples, where the following phenomenon occurs: (1) An algorithmic problem looks hard, if you want to solve it working from the definitions and using standard results only. (2) On the other hand, it becomes easy, if you know some (not so standard) theorems. The goal of this is to illustrate for students that learning more theorems can be useful, even for those who are outside of the theory field (such as software engineers, computer engineers etc). Here is an example: 

So let's end with an explanation of the gist of the general method from the above linked paper by Ibarra and Trân (freely downloadable version) for how to prove that certain problems aren't solvable by a 2CA, and how it annoyingly breaks down in our case. First, they modify every 2CA into a "normal form", in which the two counters switch in "phases" between one only increasing and the other only decreasing until it reaches zero. The number of states $s$ of this normalized automaton plays an important role in the estimates. Then, they analyze this automaton to conclude that they can construct certain arithmetic sequences of numbers whose behavior are linked. To be precise (Some of this is not stated as theorems, but is implicit in the proof of both of their two main examples): 

For their own examples they also frequently use the fact that $D,K_1,K_2$ have no prime factors $>s$. To prove impossibility, they then derive contradictions by showing that such arithmetical sequences cannot exist. In our problem, getting a contradiction from this breaks down with the second case. If we have $K_1 = K_2 = 6^k$, where $k$ is large enough that no number between $p$ and $r$ is divisible by either $2^k$ or $3^k$, then there will also be no powers of 2 or 3 between $p + 6^k n$ and $q + 6^k n$, so they are either both accepted or both rejected. Point 1 can still be shown to be impossible, because powers of 2 and 3 mostly grow further and further apart. And I believe I can show the second case impossible if $K_1\neq K_2$ (I've emailed @MarzioDeBiasi the argument). So perhaps someone could use this information to restrict the form of the automaton further, and finally derive a contradiction from that. 

If a number x is accepted by the automaton, without the size $v^x_i$ of the nonzero counter at the beginning of a phase $i$ ever going $\leq s$, then there exists an integer $D>0$ such that all the numbers $x + n D$, $n\geq 0$ are accepted. If a set $X$ contains at least $s^2+1$ accepted numbers such that for each number $x\in X$ there is a phase $i$ such that $v^x_i\leq s$, then we can find $p, r\in X$, and integers $K_1,K_2$ such that