in my opinion this question goes back to the very origins of definitions of computation and was long ago proven rigorously around that time when the church lambda calculus (which highly captures the concept of recursion) was shown to be equivalent to Turing machines, and is contained in the still used terminology "recursive languages/functions". also apparently a later key ref along these lines is as follows 

this paper points out that computing the reciprocal square root value using floating point representation is widespread in CS applications ("very common in scientific computations"); the authors show that a more efficient formula is possible for computing the correctly rounded value if the ABC conjecture holds. [1] The abc conjecture and correctly rounded reciprocal square roots Ernie Croot, Ren-Cang Li, Hui June Zhu, Elsevier TCS 2004 [2] fast inverse square root calculation, wikipedia 

for a 1st cut answer to this question see p281 of hopcroft/ullman 1979. its a nice table of lots of properties/decision problems wrt basic known languages ie regular, DCFL, CFL, CSL, recursive, r.e. on the horizontal axis and basic questions like "w in L, L= $\oslash $, L=$\Sigma *$" etc on vertical axis. now they do not explicitly state how the languages are given, but the assumption is they are using their characteristic form eg regular grammars or DFAs for regular languages, DCGs for DCFLs, CSGs for CSLs etc.; nor do they give reference for all the results in cells of the grid, that would be a big effort because of the large size of the table & the diversity of results. for $L_1 \subseteq L_2$ its decidable for regular sets but undecidable for DCFLs and higher (CFLs, CSLs, recursive, r.e.) [I typed this question from old memory & had forgotten this result, oops. namely, inclusion problem is decidable for regular sets but not for all standard well known larger language classes. however,still do wonder, could there be a language class "between" regular sets and DCFLs for which inclusion is decidable? or possibly some language class not contained in any of those standard classes?] but the case for FSM transducers raised in the question may have a different answer. & I am still not clear on the table, because [as raised in comments] what is the characteristic form for a recursive set? [the r.e. sets are surely given by TMs] sorry for any confusion in the way the original question was phrased, as always there might be a better way to phrase it. 

"r-complete" seems to be a relatively new concept invented by Axelsen and Glück ~2011, possibly not considered much by other authors, and wonder if there is a proof its different than Turing complete. am taking this verbose & circuitous question to ask for basically: 

there is a simple randomized/ estimation algorithm that computes $f(x)$ over random words; am looking for a deterministic one. note that $f(x)$ evaluated on the shortest word in $L$ is not necessarily the same as the global minimum. am also interested in other generalizations of $f(x)$ that are based on similar "simple word metrics". bkg: this question arose in studying & idea in attacking an old open number theoretic problem. 

JS mentions number theory. Fermat is credited with the Fermat primality test, a probabilistic algorithm which dates to the 1600s and serves as a precursor to more modern primality tests such as Solovay-Strassen and Miller-Rabin. [it would take a historian specializing in math & number theory to try to pinpoint exactly what Fermat knew about it versus modern knowledge which is much more complete about the structure of its pseudoprimes (false positives) etc.] 

(it is not always pointed out that coNP $\neq$ NP $\rightarrow$ P $\neq$ NP; this is because P is closed under complementation. dont see even Wikipedia currently stating that clearly.) have not heard of even an older survey that focuses on the NP $\stackrel{?}{=}$ coNP question in particular, it may be that its perceived as presumably tightly coupled to, or "at least as hard" as P $\stackrel{?}{=}$ NP. the subj is touched on in some P vs NP surveys, & eg some mention of coNP in Allender 2009 [2]. as for recent nearby/related results try [1]: [1] NP-Hard Sets are Exponentially Dense Unless coNP ⊆ NP/poly by Harry Buhrman, John M. Hitchcock (2008) [2] A status report on the P vs NP question Allender (2009) 

it occurred to me that 1sthand accounts might be helpful, but those seem to be somewhat rare in CS (as opposed to eg mathematics, biographical/ memoir-like writing etc). here are two online refs that address the question & may be helpful. they are more aimed at PhD student life & CS oriented but most will carry to professor level research (other than teaching, but which also many Phds do). [& in some ways professors can be seen as continually/repeatedly writing many "mini-" PhD theses (research papers) or in some cases "super-" Phd theses (books) over their careers.] there are also several decent books on the subject, will add some if you indicate they would be an acceptable answer for your question. 

am wondering what is the largest language class that is known for which set inclusion is decidable, ie a class such that if $A, B$ are in that class then $A \subset B$ is decidable. am also interested in the same question for what were once called "GSMs", generalized sequential machines, or maybe more modernly, FSM "transducers", where if $f(x)$ is the transducer, $C \subset f(C)$, $C$ in the class. (of course, the problem is also equivalent to determining whether the intersection of a complement is empty.) [simply asking for the "largest known" language is a literature related question. however some kind of proof that there exists a "largest class" I believe is an open question. although, there might be a straightfwd argument of nonexistence via diagonalization...?] unfortunately wikipedia does not have some of this basic info for major language classes. wonder if there is any table, paper, or reference esp online. there is a nice table of decidability & undecidability of basic language questions in [1] but its quite dated at this point. [1] Hopcroft/Ullman, Intro to Automata Theory, Languages & Computation, 1979 

not sure of exact refs but [1,2] are good places to start, an exact algorithm or ref is at least likely cited in these books. [1] Cliques, coloring, & satisfiability, 2nd DIMACS challenge [2] Dimacs vol 26: Cliques, coloring and satisfiability 

tricky question! there is some diverse crosscutting research into this question, and will attempt to outline it, but will in the end take the position here that the question is contradictory/ impossible at heart (and anticipating some this may be a controversial conclusion). here are two key recent references from a physics pov addressing your question. 

it seems researchers might be cautious/ unlikely to conjecture/ speculate solution to this problem would likely/ definitely open up other areas (eg one you list of matrix rigidity is provably nearly equivalent to the extremely hard challenge of proving nontrivial lower bounds on eg circuits etc) but it definitely provably touches many areas. here is a nice recent survey (Jan 2015) by Lovett that lists "many connections" sec 6 along with two other refs that show crosscutting connections/ formulations 

this relatively new book is worth considering as a complete/detailed answer to the question in convenient, extended/collected form and which could be used as supplemental material for an algorithms class. [some of these have already been mentioned; the strong overlap itself is notable.] 

shors algorithm for factoring in BQP. in my opinion/memory, quantum computation was more just a theoretical curiosity until this result in 1994, at which point it seems the literature and research interest into QM computing exploded. its still arguably one of the most important QM algorithms known. awarded the 1999 Gödel prize. it also reveals that factoring in QM computation is actually in a sense somewhat better understood than in classical computing where eg the question of whether Factoring is NP complete is still open. 

in some encapsulated form. arguably other aspects such as inheritance are not fundamental. as is stated in abelson & sussman, what they call "syntactic sugar". [1] structure & interpretation of computer programs by abelson & sussman 

he shows the parameter $\kappa$ estimates number of solutions and measures "constrainedness" and correlates/trends roughly with the clause-to-variable ratio. see p3 fig 4 in particular 

am interested in more technical/ mathematical perspectives that avoid handwaving. (eg something more detailed/ substantial than an assertion that the Blum speedup construction is "contrived" etc.) [1] Is the P vs. NP problem ill-posed? (Answer: no.) Aaronson blog [2] Does program size matter? RJLipton blog [3] Computational complexity: a conceptual perspective / Goldreich 

surprisingly one of the most obvious answers not posted yet. finding a clique of size $c$ (edges or vertices) apparently takes $O(n^c)$ time by the naive/brute force algorithm that enumerates all possibilities. or more accurately proportional to $n \choose c$ steps. (strangely enough this basic factoid seems to be rarely pointed out in the literature.) however a strict proof of that would imply $\mathsf{P \neq NP}$. so this question is related to the famous open conjecture, virtually equivalent to it. other NP type problems can be parameterized in this way. 

(of course a resolution DAG could be either for a proof or a refutation proof. am not specifying/fixing that for this above question. also minimal is not yet/further defined. it could be either "length" (number) or "width" (number of variables) of the SAT instance, or some combination or maybe some other meaningful/natural measure.) it feels like it could be used in a (important?) proof, eg possibly for complexity class separation via diagonalization, somehow, someday. am working on a further/related possibly remarkable construction that requires a "yes" answer ie that all 2-indegree DAGs are possible. (hope to post further on that later.) these do seem to be fundamental yet novel, not-previously considered questions based on the literature. "nearby" refs could be useful. my intention will be to upvote and/or accept any partial but intelligent answers & encourage others to follow. 

here are two other more recent papers that touch on this. as AK cites, cascade correlation algorithms were designed around ~1990 for this purpose of dynamic ANN growth but they seem not to be applied in deep learning experiments so far. the 2nd paper below cites cascade correlation as an inspiration for an incremental/online algorithm that adds feature detectors over time built out of lower-level feature detectors. 

what you mean is uniform cost search. see uniform cost search where the pseudocode matches the book which is not in error. that line's logic replaces the frontier node with a newly discovered path to the same node with less weight. the wikipedia article includes a sample run which unfortunately does not exercise that line (adj that article to do that would be an improvement). wikipedia currently uses the line "if n is in frontier with higher cost replace existing node with n" where the book statement you cite is clearer. "with higher path cost" refers to the higher path cost in the frontier. it seems that both wikipedia & the books pseudocode could be improved and some mention of the logic of that line included to avoid confusion. 

heres another angle turned up on some online investigation. the Birkoff polytope $B_n$ has many deep theoretical properties & relates to eg to perfect matchings on graphs, but volume calculations of it are very hard even for low $n$ eg as in this study by Beck and Pixton. a more direct/remarkable TCS connection arises in that a relatively recent paper proposes a measure of graph complexity based on Birkoff polytope calculations. Birkhoff polytopes, heat kernels and graph complexity by Francisco Escolano, Edwin R. Hancock, Miguel A. Lozano, 2008 

as mentioned in the comments, one great applied case study in this type of big data problem & the relevance of PCA/ dimensionality reduction with major statistical/ scientific analysis is in the Netflix prize for movie ratings prediction. prior to the contest, PCA type approaches were somewhat rarely applied to human ratings predictions. however they were found to be extremely effective in this contest combined with careful tuning/ conditioning. the winning solutions used large "blends" of many different techniques but PCA related algorithms composed many of them. moreover, sometimes mostly PCA alone without further learning algorithms applied on top of this data was enough as an effective/ top performing prediction algorithm. in this case one could say that most of the statistical trends in the data were apparently "identified" by PCA and what was "left over" (the "residual") was either not substantial or noisy. this contest had a $1M prize awarded to collaborating teams and eventually Netflix decided to employ many of these techniques in their live production system. see eg 

unless you carefully define "noncontroversial" in some technical way theres not a precise answer. here's another small machine based on rule 110 proved universal in a sense but my understanding is that it requires infinite periodic input tape formulations (and likewise extraction at the end when the machine halts). havent seen the "periodic vs nonperiodic" tape issue described in the literature although its been discussed on eg math mailing lists [Foundations of Mathematics mailing list] 

current major shift in db architecture is rise of key-value stores which seem to distribute/scale easier over the cloud (or new highly multicore chips) than relational db's & work well in large modern web sites such as facebook. they are now provided/supported on the basic cloud service sites ie amazon/google app engine. see eg: is the relational database doomed? ReadWriteWeb Many-Core Key-Value Store Berezecki,Frachtenberg,Paleczny [Facebook], Steele [Tilera]. a paper examining a key-value db & measuring performance statistics on various multicore chips from 4 (intel xeon) to 64 (tilera) 

suppose that two SAT formulas on different variables $F_1, F_2$ are given on the input that are known to be true and the problem is to build an algorithm that finds a solution to each. the formulas actually are generated by another algorithm which creates/ generates instances $F_n$ for any $n$. the solving algorithm can find/ exploit "similar structures" contained in each by analyzing eg the clause-variable graph, eg looking for graph isomorphisms etc.