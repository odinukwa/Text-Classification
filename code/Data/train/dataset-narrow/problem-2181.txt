Two different questions. How to map your decoded values onto a non-power of two range is really just basic arithmetic. Suppose your target range is the half-open interval [10, 20). (That is, 10.0 <= x < 20.0). Suppose further you have an 8-bit encoding. Let's take the string 01001011. In standard binary, this decodes to the integer 128+8+2+1=139. The total range of values you can represent is of course 0-255 inclusive. 139/(255-0)=0.545098. Think of this as the fraction of the way that 139 is to 255. The target range is only 10.0 however (20-10). Multiplying 0.545098 * 10.0 gives 5.45098. So 5.45098 is the same proportion of 10 as 139 is of 255, right? Now all that's left is to add the lower bound back in so that we're mapping between 10 and 20 instead of 0 and 10. Putting this all together, let X_int = the decoded integer value from your bit string of length L, and let X_min and X_max be your desired range. Then 

Short answer, yes you could build a GA that worked on a 3D model of a turbine with the correct viscosity, and (probably) yes, it would still find good designs. Exactly what those designs would be is somewhat unpredictable -- they might be very conventional blade arrangements or they might be completely novel and weird. However, that's also true of many other search algorithms. GAs are, at heart, just one of a class of hundreds of stochastic search algorithms. All you need for a GA to work is three things: (1) a way of describing what a possible solution looks like, (2) a way of trying to combine good solutions to get more good solutions, and (3) a way of determining when one solution is better than another. The former can be as simple as just a vector of numbers ([number of blades, distance between blades, angle of attack, ...]) or may be arbitrarily complex, allowing for each blade to have an independent length and position, etc. For number three, you need a model that tells you how good a turbine is. This is where you need to get the viscosity of air correct, embed the right equations governing the electrical outputs, etc. That just leaves the genetic operators -- how do you take good designs and produce more or better ones? Again, this can be very complex, but it can also be quite simple. For instance, you have a design with three blades, each five meters long. OK, make a small random change to that. Try blades that are 5.2 meters long. If the model tells you this is better, keep it. If not, throw it away and keep the original five meter blades. It's in this stage where domain knowledge is often invaluable. Having someone who can say, "the number of blades is really important -- taking a good five blade design and removing a blade isn't going to work unless you also change lots of other aspects of the design" allows you to simply say, "OK, my algorithm will avoid making that type of change in favor of making changes that are more likely to help. Like I said, GAs are just search algorithms. They tend to often perform a "broader" search, which can sometimes make them more likely to find these off-the-wall solutions than some other methods. On the flip side, they typically take much longer to find good solutions than some other methods. That's also an approximation; sometimes a GA works great, sometimes it works very poorly. Understanding which cases will be which is one of the great unsolved problems for people working in optimization. Finally, would companies adopt them if better designs were found? As you say, that's a more complex question. It isn't even just a matter of "too complicated". Suppose you find a 5% better turbine design that's completely feasible. You'll still likely have to redesign manufacturing equipment and processes, perhaps find new suppliers, etc. In the real world, "good enough" is often just that. However, there are numerous examples of real-world problems where methods like GAs or other random search methods have found solutions that were adopted. A lot of them are tucked behind corporate walls, but there have been notable successes in things like antenna design and aircraft design, as well as non engineering optimization problems like vehicle routing problems, scheduling problems, etc. 

One nice thing about QAP is that local search methods can take advantage of incremental evaluation of modified solutions, and GAs typically can't. So a pure GA needs O(n^2) time for every fitness evaluation, but a local search can do many fitness calculations in O(1) time, and the rest in O(n). What I've often done for QAP is to employ something like a tabu search method to do much of the search. See Eric Taillard's paper on Robust Tabu Search for a good example of how to apply it to QAP in particular, including how to do the incremental fitness update. You can generate multiple good initial points by running multiple tabu searches from random starting points, or you can integrate it directly into the GA as a hybrid method. 

CHC never has a next generation's population with lower fitness than the current one due to the use of a truncation selection step. It combines the parents and offspring, sorts by fitness, and takes the best $n$ individuals. So if including any child would lower the fitness of the population, it would just select the parents as the next generation and the average fitness would stay the same. In principle, you could indefinitely select the same two parents over and over, continually producing inferior offspring. But generally you're running CHC with a population size of 50 or more, so you don't expect to continually pair the same two parents. Also, it's technically an infinite loop anyway, as most GAs are. In practice you define some stopping condition based on evaluation count or wall clock time or whatever. 

Now we start the loop with i=1. Comparing gray[1] and bin[0], they are equal. But we take the not of that, so we get false, or 0 for bin[1]. And X_int gets updated with this weird += X_int + bin[i] thing. 

If you want to use a standard binary encoding, that's all you need. However, Gray codes often do work better in search algorithms because of the Hamming cliff problem. If we want to use Gray codes, we need to be able to get X_int out of the Gray coded bit string. You typically learn Gray codes by learning how to construct one via the binary reflection trick, but there is a bit of a shortcut here. The reflected gray code of a binary string b is just b xor b logically right shifted by one. We can reconstruct b and decoded it all in one pass through the string. In C-ish pseudocode, 

There's a little more to it than this basic skeleton, as there are things like crossover rates where you might not always do crossover, opportunities for additional operators, etc., but this is the basic idea at least. Most often, the "while not enough individuals in C" can be thought of as "while size(C) < N"; that is, you want the same number of offspring as parents. There are plenty of other ways, but that's a good way to start at least. I'm not sure if this is what you mean by having 20% of the chromosomes in the next iteration or what, but for now, just go with it. So then the question of how to do tournament selection can be addressed. Note that selection is only that one step of the process where we pick individuals out of the population to serve as parents of new offspring. To do so with tournament selection, you have to pick some number of possible parents, and then choose the best one as the winner. How many possible parents should be allowed to compete is the value of I mentioned earlier. 

So you get both the binary string 1001, which is 9 in binary, and the parameter value 9 out the back end. Why this works is not too hard to figure out if you spend some time working out the properties of gray and binary codes. 

Let . Looking at the pseudocode, this yields purely random selection. You pick one individual at random and return it. Let . Now we have a pretty high probability of picking every member of the population at least once, so almost every time, we're going to end up returning the best individual in the population. Neither of these options would work very well. Instead, you want something that returns good individuals more often than bad ones, but not so heavily that it keeps picking the same few individuals over and over again. Binary tournament selection () is most often used. In this basic framework, you can't end up with an empty population. You'll always have individuals in the population and you'll always generate offspring. At the end of each generation, you'll take those individuals and prune them down to again. You can either throw all the parents away and just do (generational replacement), you can keep a few members of and replace the rest with members of (elitist replacement), you can merge them together and take the best of the total (truncation replacement), or whatever other scheme you come up with. 

1000 individuals is typically a fairly large population, so lacking further information, you're probably fine there. You don't say how quickly your algorithm is converging or exactly you calculate the weights for weighted selection, but if you're doing the typical roulette-wheel or fitness-proportionate selection, it's known that that method produces a large amount of selection pressure, particularly early in the run. What happens is that most early solutions are terrible, so the first mediocre thing it finds gets a much higher proportion of the total fitness than it really deserves and quickly swamps the population. The easiest way to avoid that is to use something with a more controllable selection pressure, like binary tournament selection or rank-biased selection. For mutation, the general rule of thumb has always been to mutate each allele independently with probability 1/n. Like any heuristic, that's only a starting point, and you should let experiment guide you in making any adjustments, but if I'm understanding you correctly, you have 192 alleles, so a 4% mutation rate gives an expectation of about 8 mutations per individual, which I would guess is too high. Or do you mean you make a single mutation to 4% of the generated offspring? If that's the case, it's probably too low. I'd start with a method that mutates every single individual by a randomly selected amount, with the expected amount being one flip. Some individuals will get two or three flips, others will not be altered at all. The third idea, having multiple populations and sharing between them goes by the name of "island model" GAs, and has been known to work pretty well for some problems. However, I think in your case there are some issues with the way the underlying algorithm is working that need to be addressed before moving to a parallel model that will only make it more difficult to tease out the dynamics of what's going on. Without more details about exactly how you're encoding your solutions, how your crossover and mutation operators work, etc., we're kind of limited to general statements such as those I just made. Feel free to post any clarifications though and I'll take another run at helping understand what might be going on. 

Without paying much attention to your code, your crossover rate is maybe a touch low, your mutation rate is orders of magnitude too low, your population is tiny, and you're only running 20 generations. You're just not doing enough search for anything to work. If your initial population had a 6 or 7, you find it. Otherwise, you get a handful of crossovers to hope to get lucky and then you're done. You have 4 bits * 6 individuals = 24 bits per generation * 20 generations. At best, you have 480 bits of information here, and your mutation operator expects one flip per thousand bits. You also select parents using fitness proportionate selection, which has pretty severe issues with selection pressure, made even worse with such a small population. Look at it this way, suppose you generate the following six individuals for your initial population (6, 0, 15, 13, 11, 2). f(6) is quite a bit better than every other choice -- the selection probabilities are roughly (0.36, 0, 0, 0.17, 0.29, 0.17). So you'd expect the parent pool to look something like (6, 13, 6, 11, 11, 2). Obviously the exact choices are random, but that fits the distribution. Now if you pair a 6 with a 6 or an 11 with an 11, you're guaranteed to produce another copy of those, as crossover of identical parents can't flip any bit. And you have no mutation at all (practically speaking). So your population just rapidly converges to the best thing you found in your initial random population. Crossover does have some chance of producing better individuals, which is why you see the optimum 30% of the time instead of ~10% that you'd expect just by sampling for the initial population. But that's about it. You just don't have enough search happening to reliably do any better. Again, there may also be bugs -- I didn't look carefully. 

Let's see an example. Take the string 1101. In the reflected binary Gray code, this is 9. So let's decode it and see what happens. First we create a new bit string for the binary equivalent, and set it's leading bit to the same as the leading bit from the Gray code 

The usual course is to feign ignorance and let it happen. However, it is generally bad, so you'd ideally take steps to make it less likely to happen. The brute force solution is of course to simply check for equality and reselect one of the parents if necessary. You can do this, but there's a decent chance that if all you do is throw away the duplicates, the overhead of checking every time may start to limit the returns you get. Another way to minimize these effects is to use a selection operator that is less biased. Roulette-wheel selection has a very high probability of selecting from only the handful of best individuals in many cases (depends on the particular distribution of fitness values of course). You're probably better off starting off with tournament selection as your default choice. It will give you a wider coverage of the parent pool, which keeps more diversity around for longer. Also, you can tweak the knobs a bit by looking at how you do environment selection (replacing old parents by some combination of old parents and new offspring). The more strongly elitist you are, the faster you'll converge, which you usually want to avoid. One of the best black-box GAs out there is CHC (Eshelman, 1990) (always be wary of black boxes, but if you're going to treat the method as one, CHC seems to be one of the more effective ones). CHC is worth looking at, partly because it is a pretty good search algorithm, and partly because it provides a window into the types of things you can do to a standard GA to try and exert a bit more control over what it's doing. CHC takes a very odd approach. It pairs parents completely randomly and without replacement. This means that every parent is selected exactly once (recombination produces two offspring, so each parent is selected as part of one mating pair giving N/2 pairs * 2 offspring/pair = N offspring). Crossover is "HUX" or half-uniform crossover, which produces offspring maximally distant from their parents. Combined, these give a very explorative search. In addition though, it is maximally elitist. Of the N parents and N offspring, the best N of the total are kept. This drives convergence very quickly -- very exploitative. Finally, parents produce no offspring at all if they are closer than a threshold value to one another. The threshold starts at L/4 (L=length of encoding), and is decremented each time zero offspring are produced in a generation. Thus the number of offspring produced in each generation is variable, and this number is used to determine how much the population has converged. At some point, when it has converged sufficiently (when the threshold reaches zero), CHC performs a "cataclysmic mutation" in which all but one individual have 35% of their bits randomly flipped and the process restarts. Unfortunately the original paper describing CHC is only available in dead-tree form as far as I know. However, you can find a few descriptions online and in other papers that reference it.