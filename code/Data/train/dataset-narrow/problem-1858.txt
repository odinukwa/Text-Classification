Within most browsers you can enable a "status bar" at the bottom. This is an option under the "view" menu at the top. Once done it will tell you the current status of a page you are requesting. If it is in fact being delayed by a DNS lookup then you will see something like "Looking up: domain.com" for an extended period. You can also perform some lookups via the command prompt to get an idea of how fast they are being performed. start -> run -> cmd -> nslookup domain.com Mac/*nix: terminal -> dig domain.com 

Are all these computers on the same LAN as the web server? If so attempt to access the servers webpage using the private IP address associated with that server (e.g. $URL$ do this from the browser of another computer on the local LAN. If that doesn't work then we know it's not DNS or router related. It would likely be an issue local to the web server. In which case make sure iptables is initially disabled and read the webserver log files to see if any requests are even getting to the daemon. Also, attempt to telnet to port 80 on the webserver from the other computers. No use in including any DNS related issues or possible routing issues until AFTER you have verified the site is accessible on the local LAN. If it is accessible, then it's like either a DNS issue and/or a port forwarding issue on the router. 

So I figured this out and think it may come in handy for others. What I had to do was remove the <> characters from the "illegal" list within nagios.cfg. This is to be done on the server performing the active checks. 

SFTP is NOT a feature-rich solution comparable to an FTP server like vsftpd. It doesn't support chroots; which is what you are looking for. FTPS (not SFTP) would be the best solution since it supports encryption, chroots, etc. vsftpd supports this and it's easy to setup. In addition be sure to take advantage of the pam_listfile module to explicitly state which users are allowed to login via ftps. 

Most syslog.conf files are setup with wildcard facilities for the messages file (*.info). If this is just a run of the mill app and not some full blown log hogging cow you should probably just log to messages and not a standalone file. Choosing to log to your own file means adding a postinstall step to your software's install packages that adds an appropriate entry in syslog.conf. This also means that if your nice you would add a postinstall step that creates an appropriate logrotate file as well. 

Why doesn't that answer work for you? Each business is different so 3sec load times may be a max for some where 8sec is acceptable for others. In the end "pageviews, queries, visitors" is going to be specific to the application being run and they all add up to load time metrics. Shared will work till you start receiving enough requests per second that the page load times become "unacceptable". Where unacceptable is somewhere over 6sec, when visitors start hitting the back button to google. How many requests/sec the shared host can handle before load times become unacceptable is dependent on the shared servers hardware and configuration. You are at their mercy. I personally don't see why more people aren't using VPS solutions like linode and slicehost. At $20 a month you have root access. What that means is you can run tests on drive IO performance, cpu performance, etc. Additionally, you are more secure than whatever chroot environment a shared hosting provider places your business critical application in. You can recompile php to make use of opcode caching with APC, etc. You can decrease load times by optimizing server performance instead of upgrading to higher tier shared hosting. Basically, from a math standpoint it would seem to me that shared is only a good value to those who wish to do little if any administrative work. The value is with VPS where solid, secure, flexible VPS solutions start at $20 a month. Perhaps I'm biased but this is serverfault.com so I'm in like company. 

I usually place it in /etc/httpd/ if it's used by multiple vhosts, if only used by one domain then one dir above the webroot would do as well. You can place it in the webroot as well just make sure there's a rule denying access to files with it's naming convention(usually in place by default): 

The cURL php extension for windows supports sftp. You should try to find a php script that you can place on the windows server and add to scheduled jobs. Wouldn't be too hard to create your own either. I'm sure python and .net have support for sftp as well. Or, you could install cyqwin (nasty hack) on windows and write a bash script. 

Looking for a host-based IDS comparable to tripwire. Preferably one that allows centralized management. Right now I use tripwire and though it works management and reporting through a central server would be ideal. I'm looking for recommendations that have actually been used and not just google results. Thanks! 

Look at the man pages for check_http and check_ssh. Those two checks are usually ran directly from the nagios server and NOT used in conjunction with NRPE (exceptions exist). The warning and critical thresholds are related to response time. You can also modify what http response codes are acceptable. $URL$ $URL$ Please rephrase your second question as it does not currently make sense. Hope this helps 

I'm evaluating various metric collection systems and appreciate just about everything in Ganglia, except for the less than clean interface. Does anyone know of alternative Ganglia interfaces or additions to the interface that allow custom grouping of hosts a la Cacti's left pane tree view? Could really use some help with this one! Thanks! 

Enable/install APC, the php opcode cache. Enable mysql query caching Install WP Super Cache and serve up static versions wherever possible. 

A job that runs only once and runs forever till you tell it to die is called a "daemon". They are normally started via init scripts in /etc/init.d/. If your job ends at some point and can be considered a one-off kind of job, then you may want to look at the at command. For instance if I wanted to run the find command at 10PM tonight and only this once I would do: 

Was hoping to implement the "options rotate" directive within resolv.conf to have DNS lookups rotate through the nameservers listed in that file. As I understand it that is the point of this directive. It does not work on any system that I have tried to date. It always uses the first nameserver directive in the file and disregards the rest unless of an error. I tested by using a fake DNS python script that mimics a DNS server and always replies with the same bogus 192.168.1.1 address regardless of the request. When placed first in the file it would always go to this server and when placed second it would never go. This is with the "options rotate" directive included in the file. 

Most common solutions are NIS+NFS or LDAP+NFS. NIS is easier to set up than LDAP, but LDAP supports multiple OSes and is more flexible in that sense. I would recommend using one of these two since both are well documented and established in the industry. 

Unfortunately the service name chosen by the program is hard coded. You will most likely have to modify the sshd source and re-compile. The reason they do this instead of just passing ARGV[0] as the service name is for security reasons. If the pam.d/file was chosen based off ARGV[0] (the program name) then at attacker could possibly symlink/hardlink/cp that program to a name of her choosing. One that had the least restrictions within it's associated pam.d/file. Search the source for a string such as: 

You can limit output to results matching the optional argument. Also, limit your range as your example range is 4194304 IPs long. ;) Have people looking for ya with ranges like that. 

Attempt to deliver a message via telnet to the server. If it accepts the email and you still don't see it then the queues may be misconfigured. If gmail can't reach the server you will usually receive a undeliverable notice in that gmail inbox. Telnet SMTP walk through: $URL$ Check the event viewer log files for any service errors as well. 

What is an open source load balancer that allows for hash based balancing? I would like to do hash based load balancing of a URL but first remove the user specific arguments from the URL. Basically want to increase Varnish's cache performance by adding URL to node persistence. Example: example.com/foo/usertoken/bar Where the hash would be based on: example.com/foo/bar. 

There are multiple ways to solve this. You can have a secondary server with just nrpe running. In this way it's acting as a proxy. So the main nagios sends a check through the server running nrpe. Example: From the main nagios server: check_nrpe -H NRPEPROXYHOST -c check_ping -H 10.0.0.3 .... The NRPEPROXYHOST runs the command as if it were the nagios server and submits the results back to the main server. In this setup the secondary server does not run nagios or any bloated daemons. Just the nrpe daemon, the nagios plugins to be ran. This can even be configured on some sort of gateway device and would not necessarily require a dedicated server be deployed. ====== Method 2 would be configuring a second instance of Nagios at the site and having it perform the active checks and submit the results to the main Nagios server. The main nagios server would have all the checks configured with active checks disabled and passive checks enabled. This configuration is a true distributed Nagios as documented on their site. It's quite a bit more robust so if you see yourself having to perform several hundred or thousands checks to these server (every 5 minutes) then this is your best choice. In most instances the secondary server is called a "satelite" nagios instance and the results are usually submitted to the main Nagios server via the NSCA protocol (which is encrypted). The Main nagios server listens for these via the nsca daemon and submits them to the external command file for processing by nagios. The downside is you have to have the config files on two servers and make changes to both sets of configs. You have to have these hosts as passive on the main server and active checks on the satelite server. This is scalable to no end and the preferred solution for installations with tens of thousands of service checks to be performed. Also, look at building the configs on a central server and keeping them in revision control and have a script on the nagios server periodically checkout the new configs and reload nagios. ===== Method 3 DNX, $URL$ an awesome project that patches Nagios so that it can send checks to be performed to "node" nagios servers. To the best of my knowledge though this configuration does not allow you to pick and choose which checks are executed by which node (node affinity), or if they are NOT to be executed by a node. So this solution adds distribution more than it does a proxy into a secondary network. 

I cloned a VM and the ssl certificate has the old hostname. I need to generate a new ssl certificate. My question is what method was used by the server to create the initial certificate? Was this done as part of the post-install for an RPM? I'll use openssl if there's not a generation script that can be run. Thanks! 

Additionally use the debug option to munin-run. Check the plugin file for any hard coded paths and verify they are correct for your system: 

Any way to force the use of a defined host static route when the IP is on the same subnet and disregards the static route entry? For instance I have a host host: 192.168.1.2 with subnet 255.255.255.0 and a default of 192.168.1.1 I'd like to communicate with 192.168.1.3 via 192.168.1.102. 

Investigating inventory management software and was hoping to get some recommendations! Tested OCS so far and am almost hoping that there's an alternative that is extensible and has an API. ------- UDPDATE -------- I want an IMS that can be integrated into our existing systems: cobbler, puppet, powerdns, etc. So we don't have certain host data in one database and partially duplicated in another database. I would like for all provision, configuration, and management systems to drink from the data graciously updated within an IMS. I'm kind of amazed that an IMS doesn't exist in any notable web framework. One that could be integrated along side puppet and other favorites. :( 

If you're not presented with a auth prompt when requesting matching files then you can be certain that the block is not being called due to an earlier break or what have you. 

You could write yet another python script that downloads the python scripts into the proper local dir/s via http or ftp. urllib/2 modules would work just fine. This would work over a LAN or WAN so that may be helpful in the future. To keep the script simple you leave the scheduling logic to windows. Just add the new script to the Windows Task scheduler folder and schedule it weekly or whatever. If they were all on an AD domain I would recommend looking at the package deployment features available through group policy. You would add all the scripts to an msi package and push it out as you wish. 

Fake DNS server pythons script used Also, I used the dig command and the host command. I verified they make use of the resolv library. I tried this on CentOS 5.6 as well as on my personal ubuntu with totally different versions of the related packages. I'm totally stumped here, need some help on this one. 

If you are trying to setup a reverse cache proxy with load balancing...please please read this before you further configure squid. If I misunderstood your needs then just ignore this post. $URL$ 

Transparent proxy that you can enable and disable on the fly would do it. Since some applications do not explicitly support proxies there are pieces of software that you can install that will force a piece of software to use a proxy. They basically intercept any network activity and instead route it over the proxy you configure. For instance some older IM clients do not support a Socks proxy so people use these pieces of software to overcome that limitation. First google result: $URL$ 

Nginx's resident memory usage is trivial and to the best of my knowledge there is little penalty for having 8 Nginx processes (1 per core) rather than 1. I would run apache bench or another utiltiy with high concurrency requesting disk heavy objects. Keep increasing the AB tests till you discover the breaking point. Then experiment with the amount of Nginx procs keeping all other tests equal. Report back so we can all be the wiser. 

I would like to configure snmpd to send traps for disk usage. Currently the file includes these lines: 

I make use of an .ssh/config file to set my username appropriately based on a given portion of a subdomain, e.g. 

Sounds like something on the server has gone haywire and it may have nothing to do with the router or traffic. Perhaps the server doesn't have the resources available to perform it's DNS responsibilities adequately when apache is running. Try changing your DNS information to googles publicly available DNS servers temporarily and see if you can then access the internet with apache running. If you can then it's likely a DNS issue. If you still can't then try running wireshark for windows on the server and look for any weird inbound outbound traffic coming from the server. Google DNS: 8.8.4.4 8.8.8.8 

You could also run a script that periodically dumps the username:passwords into a file and use the pam_pwdfile. There are a slew of choices. $URL$ 

In addition to the wrappers option....I imagine this rsync backup is making use of an ssh key. You can restrict a key to a specific source IP or domain. This would be equivalent to a user-to-IP restriction since only that user is making use of that key (if your smart, which you appear). First line of authorized_keys file: from="trusted.domain.com",no-port-forwarding,no-pty ssh-rsa AAAABasdf