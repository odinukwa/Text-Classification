The book "Gems of Theoretical Computer Science" is a good way to learn lots of different techniques (although you see each of them applied only once): $URL$ 

The answer to this (at least to the concrete question about linearly bounding the solution) is no. This is part of the following paper: $URL$ Theorem 5.1 was the motivation for this question. The counterexample is this: base case: 

Here's one interpretation of your question, which you may or may not have intended, but for which I have an answer. Computers are obviously real physical devices and therefore can be modeled by the laws of physics. But we don't use the laws of physics that would be needed to describe a real computer as a model of computation because it's too complex. To make a model of computation, we define something like a Turing machine that is simple enough to be mathematically tractable. However, now we've untethered the model from the physical world, because we don't say how the Turing machine is built or what forces drive it to run. So can we devise some simple models that capture "computation", but whose fundamental rules are physical in nature? My answer to this would be to check out the Feynman Lectures on Computation: $URL$ He talks about a lot of different simple physical systems that carry out a computation. For example, there is the billiard ball model of Fredkin and Toffoli ($URL$ where the point was to explicitly account for energy requirements and to design a computer that can run for arbitrarily many steps for arbitrarily little energy. In particular, the chapter on reversible computing has a lot of these kinds of examples. We think about this issue a lot in my lab. For example, we've done some work on what it means for chemical reaction networks to do computation: $URL$ and $URL$ We also think about how seeded crystal formation can carry out computation: $URL$ as well as actually trying to make it happen experimentally: $URL$ and some other work based on computing using a physical phenomenon called DNA strand displacement: $URL$ 

If you can compute the raggedness of a line without knowing anything about the other lines, then you can model the problem as finding a minimum-weight $M$-link path in a graph. With concave integer weights for edges, there is an algorithm that solves the problem in $O(N \log U)$ time, where $U$ is the largest absolute edge weight. Another algorithm solves the problem in $N 2^{O(\sqrt{\log M \log \log N})}$ time for any concave edge weights, assuming $M = \Omega(\log N)$. Both algorithms assume that you can compute the weight of an edge in constant time. You could also use binary search to find a line width such that SMAWK uses $M$ lines with it. In some cases, this algorithm does not guarantee a solution with exactly $M$ lines, however. 

I have an idea that might work. We start with a generalized suffix tree for sequences $S$ and $T$. Each internal node with suffixes of both $S$ and $T$ in its subtree corresponds to some common substring of the sequences. Let us call such nodes non-trivial. The common substring is maximal, if the corresponding node has no non-trivial children. If node $v$ is non-trivial, we store the largest string-depth of a non-trivial node in its subtree as $lcs(v)$. If $r$ is the root, then $lcs(r)$ is the length of the longest common substring of $S$ and $T$. Updating the tree after deleting a substring from one of the sequences should not be too hard. We first delete the leaves corresponding to the deleted suffixes, updating their ancestors when required. Then we start processing the suffixes preceding the deleted substring. Let $v$ be the lowest non-trivial ancestor of the current leaf. If the length of the suffix is $k$ (we are $k$ steps from the deletion) and $k < lcs(v)$, we have to move the suffix to its proper position in the tree, updating the ancestors when required. If $k \ge lcs(v)$, we are done, as we are not interested in subtrees with trivial roots. The overall algorithm repeatedly finds the longest common substring of $S$ and $T$ and deletes one of its occurrences from both sequences, as long as the length of the LCS is large enough. There are some technicalities, but the general idea should work. 

For the claim (in bold) about $C_3'$, it is enough to prove that a one-tape Turing machine $M$ with 3 states that always halts either accepts or rejects all strings from $\{1^n;n\in\mathbb{N}\backslash\{0\}\}$. Suppose that a string of the form $1^n$, $n\in\mathbb{N}\backslash\{0\}$, is given to $M$. There are three cases: 1) When $M$ reads 1, it accepts or rejects. 2) When $M$ reads 1, it moves the head to the left. If we want $M$ to halt on this input, it must accept, reject or move to the right on the blank symbol. Hence, it never visits the cell to the right of the initial cell of the tape. If it would, it would run forever on input 1. 3) When $M$ reads 1, it moves the head to the right. It follows that after $n$ steps, the content of the tape is $A^n$ where $A$ is some symbol from the tape alphabet and the head of $M$ is on the leftmost blank symbol to the right of the last $A$. If we want $M$ to halt on this input, it must accept, reject or move to the left on the blank symbol. As in case 2), the head of $M$ will now never visit the cell directly to the left of the rightmost $A$. If it would, then $M$ would run forever on input 1. It is clear that in all three cases $M$ accepts all strings from the set $\{1^n;n\in\mathbb{N}\backslash\{0\}\}$ or it rejects them all. 

The problem becomes easier, if we consider long deletions and substring copying instead of transpositions. Assume that we are using the standard dynamic programming algorithm for edit distance computation, and that an expensive operation of length $k$ increases the distance by $ak+b$, for some constants $a,b \ge 0$. These constants may be different for long deletions and substring copying. A long deletion is the deletion of an arbitrary substring from $x$. Supporting them is easy, if we break them down into two kinds of simple operations: deleting the first character (cost $a+b$) and extending the deletion by one character (cost $a$). In addition to the standard array $A$, where $A[i,j]$ is the edit distance between prefixes $x[1 \dots i]$ and $y[1 \dots j]$, we use another array $A_{d}$ to store the edit distance, when the last operation used was a long deletion. With this array, we only have to look at $A[i-1,j]$, $A[i-1,j-1]$, $A[i,j-1]$ and $A_{d}[i-1,j]$ when computing $A[i,j]$ and $A_{d}[i,j]$, allowing us to do it in $O(1)$ time. Substring copying means the insertion of an arbitrary substring of $x$ into the edited string. As with long deletions, we break the operation down into two simple operations: inserting the first character and extending the insertion by one character. We also use array $A_{s}$ to store the edit distance between prefixes, provided that the last operation used was substring copying. Doing this efficiently is more complicated than with long deletions, and I am not sure whether we can get to amortized $O(1)$ time per cell. We build a suffix tree for $x$, which takes $O(|x|)$ time, assuming a constant-size alphabet. We store a pointer to the current suffix tree node in $A_{s}[i,j-1]$, allowing us to check in constant time, whether we can extend the insertion by character $y[j]$. If that is true, we can compute $A[i,j]$ and $A_{s}[i,j]$ in constant time. Otherwise $zy[j]$, where $z$ is the inserted substring that was used to compute $A_{s}[i,j-1]$, is not a substring of $x$. We use the suffix tree to find the longest suffix $z'$ of $z$, for which $z'y[j]$ is a substring of $x$, in $O(|z|-|z'|)$ time. To compute $A_{s}[i,j]$, we now need to look at cells $A[i, j-|z'|-1]$ to $A[i,j-1]$. Finding suffix $z'$ requires just amortized $O(1)$ time per cell, but computing $A_{s}[i,j]$ with a brute-force approach takes $O(|z'|)$ time. There is probably some way to do this more efficiently, but I cannot find it right now. In the worst case, the algorithm takes $O(\min(|x| \cdot |y|^{2}, |x|^{2} \cdot |y|))$ time, but a better analysis should be possible. The resulting edit distance with long deletions and substring copying is not symmetric, but that should not be a problem. After all, it is usually easier to reach the empty string from a nonempty one than the other way around. 

An easy observation is that if a problem $A$ is decidable by a polynomial-time nondeterministic program using $O(\log n)$ nondeterministic bits (i.e., all witnesses are logarithmic in length), then $A \in \mathsf{P}$. If one then asks the question, "Is it easier to verify a witness than to find one?" for such problems, and one considers all polynomial running times equivalent, then the answer is no, since one can find such witnesses in polynomial time by searching through all potential witnesses. But what if we consider fine-grained distinctions between polynomial running times? I'm wondering if there is a concrete example of a natural problem in $\mathsf{P}$ that has logarithmic-length witnesses that are easier to verify than to find, where "easier" means a smaller polynomial running time. For example, known algorithms for perfect matching in graphs take polynomial time, but more than $O(n)$ time on a graph with $n$ nodes. But given a set of $n/2$ pairs of nodes (a witness), it is easy to verify in time $O(n)$ that it is a matching. However, the matching itself requires at $\Omega(n)$ bits to encode. Is there some natural problem that achieves a similar (apparent) speedup in verification versus finding, in which the witness has logarithmic length? 

along with requiring them all to be nonnegative. You can prove by induction that any real solution must satisfy a_n'' >= a_n + 2^n. We change the "< 0"-inequalities into "≤ -1" because any integer solution satisfies "≤ -1" if and only if it satisfies "< 0". So, the moral is that n inequalities of this form can have the property that all integer solutions have at least one integer at least exponential in n, certainly not linearly bounded as we originally suspected. 

It is a common belief that $\mathbf{P}\subsetneq\mathbf{PSPACE}$, thus (most likely) there are problems that are "harder" for time than for space. But is there a problem in $\mathbf{P}$ with a poly-space lower bound (say for multi tape TM), i.e. is there a space-hard problem in $\mathbf{P}$? Similarly, is there a problem in $\mathbf{P}$ with a good non-deterministic time lower bound? Is there a problem in $\mathbf{NP}$ with poly-space lower bound? ... 

Let $f$ and $g$ be fully time-constructible functions (i.e. there exists a DTM that on input $1^n$ makes exactly $f(n)$ (resp. $g(n)$) steps) and let $f(n+1)=o(g(n))$. The nondeterministic time-hierarchy is many times (superficially) stated as $NTIME(f(n))\subsetneq NTIME(g(n))$. (proof: ask Google for nondeterministic time hierarchy). Well, the hierarchy actualy gives only $NTIME(g(n)) - NTIME(f(n))\neq\emptyset$. We would need e.g. $f(n)\leq g(n)$ for $NTIME(f(n))\subsetneq NTIME(g(n))$. For functions $f,g$ such that $f(n+1)=o(g(n))$, $f(n)\leq g(n)$ is very common. But strictly speaking, nondeterministic time hierarchy is many times stated superficially. To show that $NTIME(f(n))\subseteq NTIME(g(n))$ does not hold for all fully time-constructible $f,g$ s.t. $f(n+1)=o(g(n))$, define $$f(n)=\left\{\begin{array}{ll} n+1 & n \mbox{ odd}\\ (n+1)^3 & \mbox{else} \end{array} \right.$$ and $g(n)=f(n+1)^2$. It is easy to see that $f$ and $g$ are fully time constructible and $f(n+1)=o(g(n))$. From nondeterministic time hierarchy we know that there is some language $L\in NTIME((n+1)^3)-NTIME((n+1)^2)$ over $\{0,1\}$. Define $$L_1=\{0x_10x_2\ldots 0x_n;\ \ x_1x_2\ldots x_n\in L\}.$$ It follows that $L_1\in NTIME(f(n))$. It is easy to see that from $L_1\in NTIME(g(n))$ follows $L\in NTIME((n+1)^2)$, which is not true. Hence, $L_1\in NTIME(f(n))-NTIME(g(n))$. 

Do you mean construction or inversion of BWT? For construction, the best algorithm is probably the one by Okanohara and Sadakane. It takes $O(n)$ time and usually requires $2n$ to $2.5n$ bytes of memory for an input of length $n$. There is an implementation available at Google code. I am not that familiar with BWT inversion algorithms. The papers of Kärkkäinen and Puglisi at ESA 2010 and CCP 2011 might provide a good starting point. 

This sounds similar to superbubbles in bioinformatics. We have a directed graph $G = (V, E)$. A superbubble is an induced subgraph defined by vertices $s, t \in V$ (with $s \ne t$). We have the following requirements: 

The first rule of concurrent data structures is: You do not want concurrency. In the ideal case, distributed/parallel/concurrent computing means that you have a number of completely independent sequential processes. Each process has its own data and resources, and the process is not even aware of any other processes. In the worst case, you have a shared memory system with multiple threads querying and updating the same data structures concurrently. Something has probably gone horribly wrong, if you are seriously considering this. Of course, when we are talking about concurrent data structures, some degree of concurrency is unavoidable. We still want to minimize it. The longer a process can work sequentially without touching mutexes, doing atomic operations, or passing messages, the more likely everything works correctly and the performance is acceptable. Static data structures with batch updates require less synchronization than dynamic data structures. You should try to make your concurrent data structures static, or at least as close to static as possible. If your algorithm requires interleaving queries with updates, try changing the algorithm before resorting to shared dynamic structures. The same design principle also applies to updating static data structures. The more independent you can make the processes updating the structure, the better everything works. 

The proof of the claim (in bold) about $C_3$ follows the same line as above. We take a one-tape 3-state Turing machine $M$ that accepts a string $1^n$ for some $n\geq 1$. Suppose $M$ is given an input $1^m$ for $m\geq n$. We have to prove that $M$ accepts this input. We have 3 cases: 1) When $M$ reads 1, it accepts. 2) When $M$ reads 1, it moves the head to the left. Because $M$ accepts the input $1^n$, it has to accept or move to the right on the blank symbol. Hence, it never visits the $n$th cell to the right of the initial cell. If it would, it would run forever on input $1^n$. 3) When $M$ reads 1, it moves the head to the right. It follows that after $m$ steps, the content of the tape is $A^m$ where $A$ is some symbol from the tape alphabet and the head of $M$ is on the leftmost blank symbol to the right of the last $A$. Because $M$ accepts the input $1^n$, it must accept or move to the left on the blank symbol. As in case 2), the head of $M$ will now never visit the $n$th cell to the left of the rightmost $A$. This is because on the input $1^n$, $M$ does not visit the cell directly left of the initial cell, because it contains the blank symbol and if it would read it, it would run forever. It is clear that in all three cases $M$ accepts all strings from the set $\{1^m;m\geq n\}$.