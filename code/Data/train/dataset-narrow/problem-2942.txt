Some interesting projects for those working with block-based languages (such as Scratch, Snap! and Blockly): For Scratch, check out Dr Scratch, which takes a rubric approach to evaluating how much 'computational thinking' is evidenced by a project. Whilst the analysis might seem a bit reductive, it can be used independently by learners and includes some useful guidance on how to progress. The developers describe their approach in this paper. Dr Scratch is built on Hairball, a Python module which does static analysis of Scratch projects. A more conventional autograder, lambda, is being developed by Michael Ball for Snap! It's already integrated into UCB's Beauty and Joy of Computing MOOC, and I think there are plans to make this more widely available. Michael wrote about this for Hello World #3. Chris Roffey has developed an autograder for Blockly used in the initial round of the TCS Oxford Computing Challenge programming challenge, although I don't think the code for this is shared publicly. 

There are some history topics that link really well with computing - for example a history of communication, taking in writing, printing, semaphore, the telegraph and Morse code etc, through to the internet and the web. The English history curriculum for 5-7 year olds suggests that pupils compare William Caxton and Tim Berners Lee. Another great topic would be cryptography, perhaps starting with the Caesar cipher, mono-alphabetic and poly-alphabetic substitutions, Enigma, Colossus and other work at Bletchley Park, public / private key encryption (with applications to SSL), some of the contemporary issues around privacy and perhaps a look ahead to quantum cryptography. YMMV, but I think context like these may make these topics more engaging than a straight history of computing unit. 

I've always admired the Swiss' education system of teaching kids where they rarely have examinations, but rather through constructive assignments and homeworks to teach students. At a conference I've attended, I heard a quick introduction on gamification of education and I am trying this out with some students. Here is what I proposed: Assesments (excluding exams and final project, due to curriculum and school board constraints) have unlimited re-tests, limited to once a week. 0.5 credit is awarded for 50%+ and 1.0 credit is awarded for 80%+ At the end of the term, their number of credit earned is divded by total number of credits for a "term work" grade worth x% of their final mark The goal here is for students to not worry about a 50, 60, 70, 80, 90 or 100, but rather track their progress through completion of content. I believe the unlimited retries gives incentive for students who are falling behind to realize early and catch up immediately, rather than later. This is to avoid the mentality of giving up because it is "too late" or "wait for next test". To some degree, I believe in the innate competitive nature of CS students transferred from love for gaming I believe this method (with modification to suit your needs) meets what you are looking for. Accurate enough to give a % mark because you track progression. Although I foresee multiple 100% with this method Does not disturb students because they are well aware of their progress, and know they can make improvements rather than blankly stare at an unfortunate poor test 1 Quick and low effort - I just use an excel macro and export to show my class after each week their progress 

A variant on the usual random drill and practice test would be to pre-populate with the questions and answers, then remove question and answer from each as they get answered correctly, allowing players to get more practice on the questions they get wrong. Here's an example for times tables. You could try something for an adventure game, building up an inventory of items collected in a list. Another possibility would be an adaptive 20 questions style game, adding additional questions into a database (of sorts) as the player gets to the end of a branch of the tree. 

Lots of things you can do via role-play, e.g. simulations of how a processor works, how the internet work, how e-mail works etc, but also playing through some scenarios or dilemmas in online safety. Plenty of scope for debating broader moral and ethical issues around CS, including AI: What should Audi's programmers and managers have done? Should end-to-end encryption be available? What rules should a self-driving car be programmed to follow? 20 questions, or something simpler such as guess my number? Lots of fun with hand-drawn graphs, e.g. minimal spanning trees, shortest paths or the travelling salesman problem. 

When we were drafting the English national curriculum, we found it easier to think in terms of the foundations, applications and implications of computing, all three of which really should be included in any broad and balanced approach to the subject. You can map these to computer science, IT and digital literacy if you wish, although you would need to accept a rather broader definition of digital literacy than that used by the Royal Society Foundations would be about the underpinning principles of computer science (logic, algorithms, data representation, abstraction), as well as their practical expression through programming and more generally in computational thinking. Applications is about skills in using digital technology to get useful work done, including collecting, managing, analysing and communicating data and information and creative work in a range of digital media. Implications is about a critical understanding of the impact of digital technology on individuals and society as well safe, responsible and ethical use. I'd include intellectual property, privacy and security here too. I've an illustration of all three in response to the question 'How does Google work?' Foundations: big data, Page Rank, Big Table / the Google File System (GFS) etc Applications: type your query, click the button (well, these days it starts searching as you type), but also filtering results, advanced queries etc Implications: profile, filter bubbles, advertising, smart creatives, separating costs and revenues for accounting purposes etc 

High level: Just like any language that exists in the world today, it has it's own alphabet, syntax and grammar that is for communication. Technical: First off I just want to note that some languages are interpreted while some are compiled, as for their differences I believe it's off-topic. The idea is that your "code" is tokenized based on the language's alphabet and syntax and formatted into a parsing tree. The parsing tree is then translated into some intermediate code. Lastly the compiler translates the intermediate code into source code or machine code that can be executed by the CPU. 

I've found a lot of success giving real world (often times very silly) examples of boolean algebra to give them a more intuitive understanding in addition to the pure algebraic laws. An example would be "If it rains tomorrow, I will bring an umbrella so I will stay dry". This is a simple A -> B: If it rains tomorrow then I will bring an umbrella, I will stay dry (T -> T = T) If it rains tomorrow then I will not bring an umbrella, I will not stay dry (T -> F = F) If it does not rain tomorrow then I will bring an umbrella, I will stay dry (F -> T = T) If it does not rain tomorrow then I will not bring an umbrella, I will stay dry (F -> F = T) Using DeMorgan's we know A -> B = !A V B. We can say A = it will rain tomorrow, B = bringing an umbrella and whether you stay dry or not is the equivalent of the resulting truth table value. You can incorporate students in coming up with these silly examples, and having them figure out how the narrative would look like to reflect the truth table values. In addition, pairs can come up with scenarios and test each other's knowledge. (This was during 2nd year University too! So it's never too old to get silly) Lastly as a remark, I did not see you mention some Laws of Boolean Algebra such as Associative, Commutative, Idempotent, Identity, and Distributive. I think it's worth while to introduce these laws during lesson 1 or 2 because solving boolean algebra down the road is built off of these fundamentals. 

There seems little value in students copying code off a display, but much in watching the teacher model how they think about the task, talking through the problem solving process of coding, as well as debugging (in the case of, ahem, deliberate, mistakes), iterative development and refactoring. I think this works if it's editing a longer program or writing short examples. I'd say good practice would include sharing the code produced, via Github or elsewhere, as well as screencasting the talking through of the development process itself. 

I'm sure there are plenty of others. Not sure I'd use them in every lesson, but they might be a useful incentive towards more purposeful use of the devices they bring. I know one school where in free time pupils are only allowed to play games they've coded themselves... 

Harvard's grading policy for CS50 is worth looking into. There are four components for the grade on problem sets (each of which involves submitting code). The overall grade is calculated as scope * (3 * correctness + 2 * design + 1 * style) Scope: to what extent does the code implement the features required by the specification? Correctness: to what extent is the code consistent with the specifications and free of bugs? This is done by the check50 autograder, and it's essentially unit testing. Style: to what extent is your code readable (i.e., commented and indented with variables aptly named)? there's a component for formatting: I think in Harvard's case these marks are awarded by teaching assistants, but basic static analysis or linting might suffice. Design: essentially, is this good code in terms of clarity, efficiency, logic, elegance - again, Harvard use TAs to award these grades, and it's hard to see a machine (or an inexperienced grader) being able to award these marks accurately any time soon. If you were determined to use automatic grading, I guess you could do something with run times for test data or the more sophisticated forms of static analysis. A compromise might be the use of peer-assessment and a detailed, criteria based rubric: peer assessment might have other benefits. 

Let's start with the term "Loop Invariance". It is a property of a loop that is true before and after each iteration, thus in-variant, non-changing. So then, what is the purpose of the loop invariance in proving algorithm correctness? That is, it is a predicate about what the loop is supposed to do. Thus with proof by induction on this predicate shows the correctness of this algorithm. I know this is still very theory heavy so let's break this down even more. A simple insertion sort. The purpose of insertion sort is to sort an array. Therefore the loop invariance would be that after each i-th iteration, the array is sorted up to the i-th element. The magic here is that instead of looking at the nested for i, j, loops of the algorithm you are choosing the loop invariance that contributes to the goal of the algorithm. To answer (1). There is no sure guarenteed way to choose the correct loop invariance unless you are very experienced in algorithm correctness through countless examples. The best approach is to choose the segment of code that is actually doing what the algorithm is trying to do. Such as the example above, sorted up to the i-th element. (2). I believe this has to do with the proof itself, rather than understanding the loop invariance. Structurally, to prove that the algorithm is correct, you would have to use proof by induction (either simple, or complete) to pove the loop-invariance and the fact that the algorithm actually terminates. Usually proof of termination is a 1 liner, such as when i > array.length, loop will terminate. 

Any programming is a two step process: deciding how to solve the problem, then implementing that as code on a particular system: choosing or designing an algorithm is the first step. There are great ways to illustrate how the choice of algorithm matters. An introductory one might be search - comparing random, linear and binary algorithms to, for example, find a missing number, or a word in a (printed) dictionary, or a book in a (physical) library. Another might be exploring different sorting algorithms, for example bubble sort and quicksort using this CS Unplugged activity. Mathematics provides a rich source of contexts, for example asking students to think of an algorithm for finding the greatest common divisor (i.e. highest common factor) for a couple of numbers. Have them try their algorithms out on paper before coding them and then testing with some big test numbers. 

We're introducing some aspects of parallel processing quite early on in Scratch. Each sprite has its own script which appears to execute in parallel with those of the others. Scratch has a broadcast and receive message protocol, and support for shared as well as private variables and lists. Children might encounter this in a maze game, perhaps programming a number of similar ghosts to chase the player's avatar. It's also useful for agent-based modelling, e.g. the spread of an epidemic through a population. Of course, it's not true multi-threading, as all of Scratch runs inside Flash, inside the browser, on just the one core, but I doubt those using Scratch will be aware of, or care about, the distinction. This does, though, lead to potential difficulties in 'graduating' from Scratch to a text-based language such as Python - young Scratchers who've been used to programming in parallel in Scratch can find it hard to adjust to doing just one thing at a time in introductory Python programming.