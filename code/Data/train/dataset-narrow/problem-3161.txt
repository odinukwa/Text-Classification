Supervised learning refers to learning a concept from examples. Those examples typically require work from a human, which is usually "expensive". In fact a huge part of practical ML-work is to come up with smart ways of obtaining training data (think of reCaptcha's for instance). Unsupervised learning is learning without labeled data, that is all there is to it. Clustering is one example, PCA is another and autoencoding is the newest hottest thing (imho). In some sense unsupervised learning try to compress the data (by finding subspaces of high dimensional representations along the lines of which the data is ordered, or manifolds): You are trying to formulate concepts that describe the data on a higher level, this involves finding smarter descriptions. It might be somewhat of a leap from K-means, but consider this: I could easily describe a photograph to you using a set of abstractions (fi: I see a dog with a bone -> 23 bytes). This would give you a picture of the scene without the need of me sending you the full dataset (fi: 1080 * 768 points ~ 10^6 bytes). Conceptually this would be somewhat similar to: 4 examples of cluster one and one example of cluster 4, instead of sending 5 complete instances of, say, 10.000 features. 

Is there any reason you have to use Naive Bayes? While Naive Bayes does handle multi-class modeling quite nicely, sometimes the "naive" assumption that each word in the text is independent of the others is too naive, especially given the size of your training set. While you may be able to increase the accuracy a little bit by doing more text processing, I wouldn't expect to see significant improvement, especially given the training size. Since Naive Bayes itself doesn't have much parameter tweaking, if you want to try more processing of the data itself to improve performance, you can try things like text count vectorization or TF-IDF vectorization to represent the text data more holistically than just keyword flagging. If you are able to implement other models, I would take a look at the scikit learn multi-class models for ideas. Tree based methods are also inherently multiclass, and have more parameters to tweak, so implementing something like Random Forests may suit you better. As for the training size itself, that does seem a little small. Especially if you are using text data, the more features you include, the more data you need to be able to stand a chance of detecting a signal in the data. 

KNN is instance based so it will store all training instances in memory. Since you are using images this will add up quickly. KNN on untransformed images might not perform that well anyway, you could look into filter banks to transform your images to a bag-of-word-representation (which is smaller and more invariant). However if it is accuracy you are aiming for I would recommend skipping all that (it is very 2012 anyway) in favor of using deep learning, fi: construct an auto-encoder and determine similarity on the encoded representation of an image (which could in turn be done using knn btw). 

In the basics a good visual split is a good starting point. And yes, it is smart to keep in mind how the algorithms divide the space. A good strategy, I personally like to apply is to start of with simple learners to learn how your data is structered. Hoe well does NN work, are there hints of local behavior? How well does Naive Bayes work? Is the concept complex or do individual features hold information? Etc. As for selecting the features: You could try ranking your features on methods that compare their use (such as information gain), or simply write a scheme that tries all combinations of two on your two methods (it's only 9 * 8 runs). If the space was a little bigger I would suggest a combination of the two. You might also want to try combining features (fi: PCA). 

Given just one line of the data, it's a little hard to go off of, but I'm assuming you're trying to get at the number after each colon, and the number before it refers to the column name? If so, you can use read_csv with a little tweaking: 

Part of the problem lies in how much data you have. To create a second level of complexity, you ideally want to use a holdout data set to decide the right combination for the model predictions. If you use the training data from the models themselves to also combine the model output, you risk over fitting your final model. If you have a small data set, trying to build an ensemble on top of it might lead to worse performance in deployment since the model is only memorizing the training data. In that case, a simple average might do you better than trying to anything more sophisticated. But, assuming you have more data to use, or if you want to use the data you trained on anyways, you can either design a weighted average or create a second model. If you have an idea of which model performs better, you might want to try manually assigning weights for each model's output. A simple average assigns an equal weight to each output, but you can experiment with shifting the weight around a bit. For an example, given two models, a basic average yields 

It could be a case of padding in combination with convolution strides: if you would pad the first layer with 2 zeroes on either side and use a stride of 2, you would end up with an 18 * 18 * x. The 3 channels on the input are most probably RG&B, which are fairly commonly scaled up to 32 feature maps. 

@Emre's response and @elias-strehle's implementation are correct. I have made a similar implementation using an elegant (imho) hashing function I borrowed from this tweet The class hashes each word to the product of all it's letters, in which each letter is mapped to a prime number. Products of prime numbers only collide when using the exact same numbers. It is quite fast (as requested), around 200k anagram lookups per second (on my machine) See here for a full example 

Perhaps another solution to your question: There are datasets that contain road surfaces, such as the (Dutch) BGT-database. 

You can infer from the total in the first row that your model predicts Class 1 81% of the time, while the actual occurrence of Class 1 is 94%. Hence your model is underestimating this class. It could be the case that it learned specific (complex) rules on the train set, that work against you in the test set. It could also be worth noting that even though the false negatives of Class 1 (17%-point, row 2, column 1)) are hurting your overall performance most, the false negatives of Class 2 (4%-point, row 1 column 2) are actually more common with respect to the total population of the respective classes (94%, 6%). This means that your model is bad at predicting Class 1, but even worse at predicting Class 2. The accuracy just for Class 1 is 77/99 while the accuracy for Class 2 is 2/6. 

The problem type you're dealing with is referred to as multiclass classification. Not all algorithms are suited to handle it, but tree based methods and neural networks are popular choices. If you need it to run quickly and probability calibration isn't too important, Naive Bayes also works quite well for some data sets. To see an example of a dataset of this type, check of the Kaggle Spooky Author Identification competition. The published kernels give some good examples of feature engineering and modeling choices. (I'm assuming the category label is unique. If there can be more than one label per record, it's called multilabel classification, which I would handle by building a separate binary model for each of the 39 labels in your data set. For an example dataset, check out the Kaggle Toxic Comments competition.) As for modeling with your data in specific, the structure you have it in now seems a bit odd to feed to a model. A training data set should have each row represent one record, and each column represent a feature with the value in the column describing the record, whereas in your format, any cell phone should have information regarding each value in the corresponding mobile Attribute_Names. When you get the raw input 'Samsung Galaxy On Nxt 3 GB RAM 16 GB ROM Expandable Upto 256 GB 5.5 inch Full HD Display', how is that transformed into a format that can be fed into the model? Also, where are the Attribute_Names, Attribute_Values coming from, are they manually specified? If so, that is limiting the performance potential from a model since there could be additional words in the data the model could detect if left to generate features on it's own. For a good modeling flow, the training data should contain the inputted raw text, then process the text to generate features, then feed into the actual model to output a label. So one row of raw training data set would be: 

I am assuming that you mean a vector representation of words, not to be confused by the vector representation produced in a bag of words approach that represent a document in vector space. Word2vec is an approach in which you train a model to represent words as a function of the provided context. The answers that follow are: 

Either format will work with R and Python, though you'll need a library for MS Access, which is the least common option. If any of these formats is the native format of the data, I'd go for that, which will avoid weird transformation artefacts. If neither of the formats is, I'd choose CSV, which would allow me to use a text editor if needed. However, if one of the columns contains user generated texts, it might be worth the trouble to choose Excel, that would limit the chance that the export would get botched. It is way easier to get your hands on a CD-reader. Your data doesn't sound that impressively large that it warrants using tape.