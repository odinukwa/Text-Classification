Now for some references. For typed-term generation for testing purposes, you may be interested in "Making random judgments: Automatically generating well-typed terms from the definition of a type-system" by Burke Fetscher, Koen Claessen, Michał Pałka, John Hughes, and Robert Bruce Findler, 2015. 

I would think you can define the and by turning this into a three-place judgment. Only showing one half: 

No, to my knowledge there has been no work on formalizing TeX of the kind you are interested in. (What follows is a subjective and personal commentary). I think it is an intriguing and well-posed idea, and your motivation of using it to perform optimizations sounds reasonable -- another related question is whether you could define a bytecode format for it to speed up interpretation. On the other hand, the idea has two downsides. First, it is not clear to me that there is a large potential for optimizations (for example, what kind of program-preserving transformations could one perform to speed up computation?), as it may be that the language semantics is intimately related to parsing the character flow, and thus not very accommodating to the design of optimization-friendly intermediate representations. Second, the need for improvements in TeX interpretation speed is not well-established: the speed of batch speed building has remained reasonable thanks to hardware improvements. Cases where speedups could be welcome are complex graphics package (beamer presentations can take quite some time to build), packages embedding rich computations (but then another language may be more appropriate), and use cases requiring fast rebuild for instant user feedback (but then incrementality, rather than optimization, may be the point; a formal semantics would certainly help reason about incremental implementations as well). That is to say: this sounds like a fun, instructive topic, but it is not clear to me that the practical justifications for doing the work are strong. If someone was interested in doing it out of curiosity, that sounds like an excellent adventure, but otherwise there may be other ways to employ the same skillset whose impact would be more sought-after by end-users. 

(The two halves, and , can be factorized with a parametrized presentation where is and is .) The problem here are the mu-unfolding rules: 

"Resumable exceptions" are indeed a well-known idea in some programming language circles. In particular, Common Lisp has had resumable exceptions for a long time (so they are really not confined to research language; Common Lisp could be considered "mainstream", it has been widely available and relatively widely used for a long time). For a discussion of resumable (or "restartable") exceptions, see for example this blog post by Manuel Simoni, which links to this dylan mailing-list post by Chris Double and the following Lambda-the-Ultimate discussion: Common Lisp exception handling. You made the insightful guess, that resumable exceptions are not a good idea in all contexts. They can make resource handling harder and more generally anything related to side-effects must be considered very carefully when you add new control-flow entry points to a block of code. It is possible for the exception raising code to decide on whether it should be resumable or not. This is natural if you see resumable exceptions as a combination of usual exceptions and continuations: you raise an exception and, in the attached data, decide whether or not to include a continuation to the rest of the computation; the handler then decide whether or not to invoke that continuation, that is "resume" the exception. Effect handlers ( block, -like resource handler assuming lexical lifetime) can get confused if you re-enter the computation. Combining handlers and rich control flow is difficult and gave way to a lot of different approaches in the Lisp/Scheme communities (keywords: , ). You are certainly adding non-trivial complexity to the language. I speak of "effect handlers" because, once you have such rich forms of control flow, you get very close to the ability of presenting arbitrary effects (exceptions, backtracking, but also logging, mutable state and input/output) on top of it. See for example Andrej Bauer and Matija Pretnar's work on Eff a language based on such "effect handlers". 

(Technically these interact with the induction/coinduction presentation: you may not use an hypothesis in Gamma without using one of the coinductive/productive rules first.) I guess that those could be adapted to: 

There has been interesting work recently on making the relation between proof net and focused calculi tighter, using "multi-focused" variants where you may have several simultaneous left holes, and studying "maximally focused" proofs. If you pick the calculus right, maximally-focused proofs can correspond to MLL proof nets or, in classical logic, to expansion proofs (The Isomorphism Between Expansion Proofs and Multi-Focused Sequent Proofs, Kaustuv Chaudhuri, Stefan Hetzl and Dale Miller, 2013) 

See also Yann Régis-Gianas PhD thesis work with François Pottier: A Hoare Logic for Call-by-Value Functional Programs (MPC'08). This work was extended to cover the usual ML side-effects by Johannes Kanig and Jean-Cristophe Filliatre in 2009: Who: A Verifier for Effectful Higher-order Programs. 

You should have a look at Uniform Proofs as a Foudation for Logic Programming by Dale Miller, Gopalan Nadathur, Frank Pfenning and Andre Scedrov, 1991. The idea of this work and the rich area that has seen spawn from these ideas is to understand the execution of a logic program as proof search in simple and expressive logics, such as intuitionistic logic or linear logic. A particular class of formulas with good proof search properties are the Hereditary Harrop Formulas, which nicely generalize Horn Clauses. 

One vision of the limits of strongly normalizing calculi I like is the computability angle. In a strongly normalizing typed calculus, such as the core simply-typed lambda calculus, System F, or Calculus of Constructions, you have a proof that all terms eventually terminate. If this proof is constructive, you get a fixed algorithm to evaluate all terms with a guaranteed upper-bound on the computation time. Or you can also study the (not-necessarily-constructive) proof and extract an upper-bound from it -- which is likely to be huge, because those calculi are expressive. This bounds gives you "natural" examples of function that cannot be typed in this fixed lambda-calculus : all arithmetic functions that are asymptotically superior to this bound. If I remember correctly, terms typed in the simply-typed lambda-calculus can be evaluated in towers of exponential : ; a function growing faster than all such towers won't be expressible in this calculi. System F corresponds to intuitionistic second-order logic, so the computability power is simply enormous. To seize the computability strength of even more powerful theories, we usually reason in terms of set theory and model theory (eg. what ordinals can be built) instead of computability theory. 

In Extending Type Theory with Forcing by Guilhem Jaber, Nicolas Tabareau and Matthieu Sozeau, 2012, intuitionistic forcing is presented as an internalization of the presheaf construction, implemented as a type-preserving translation in the style of Bernardy and Lasson's parametricity translation. This means that you can define terms in your usual type theory, and then "translate them" into a "forcing layer" where they are interpreted as translations at a different translation type. For example, the translation induced by indexing over decreasing natural numbers lets you use your usual terms in a post-translation theory where a modality is definable. This sounds rather close to your idea of working internally in the topos of trees. It seems that they have a new, simpler Coq plugin implementing these ideas at CoqHott/coq-forcing, and in particular SI.v builds this forcing translation for step-indexing. Unfortunately, while it does the work of building the model, there is no example of using it for step-indexed definitions in practice (the only thing translated instead of defined in the forcing layer is , which isn't terribly informative). You could try to experiment to see how (in)convenient this is to use. 

The "Reduction strategy" wikipedia article is entirely extracted out of a particular edit made by an anonymous IP to the "Evaluation strategy" article. The view that it represents is not consensual, in the sense that I suspect relatively few people of the field will spontaneously give this answer if you ask them "would you distinguish the names 'reduction strategy' and 'evaluation strategy'?". I have only heard it from Matthias Felleisen, which is adamant about the importance of this distinction -- and I assume this point of view is shared by others that had the chance of taking time to discuss these points in details with him. My current understanding of this point (but I have not yet studied the technical details to their full justice) is about the following: this is about whether you use "big step" versus "small step" semantics -- this distinction is standard and understood by everyone in the field. Small-steps semantics define one atomic step of reduction, and the result is in general still reducible. Big-step semantics define one "big" step of reduction that goes all the way from the starting program to its value (or some richer "answer" type if your language has other observable effects than returning a value, eg. input/output or mutable state). If you define both a big-step and small-step relation, you can check that the big-step semantics is included in the transitive closure of the small step relation, and that the small-step relation does not reduce to other stuck terms than those reached by the big-step relation, or diverge if the big-step reduction is defined. This is the expected coherence relation between both. I think that the wording of the article can be more or less described, in modern terms, as "evaluation strategy is the big-step relation", "reduction strategy is the small-step relation". Do note that the discussion made in the "Reduction strategy" article is mostly about articles and research (and, more importantly, eloquent viewpoints formed during their reading and writing) between 1973 and 1991, at a time where those notions were just born, and probably not as well-understood as they are today. (big-step semantics was emphasized by Kahn in 1987, and one of the most important works on small-step semantics is Wright and Felleisen, 1992) For the more opinionated side of why Felleisen insists on the importance of this difference (that is, why there may be more to it than just "small-step vs. big-step, meh"), my current understanding is the following: the point that is being made is that the small-step semantics should be viewed as an implementation detail. The semantics, according to this argument, is the abstract function that maps each program to its value/answer, and the rest are implementation devices designed to approximate it (or reason on the equivalence induced by this semantics). When we say big-step today, we think of a system of inference rules of syntactic nature, but the "reduction strategy" that is being discussed above is in fact its abstraction as a mapping. (I don't think this gives more expressivity or strength to the notion in practice, but it makes it more abstract.) So I think that what this wikipedia page, and Matthias Felleisen, are saying is something like: "Define your evaluation in whichever way you like, but in the end of the day the thing that matters is how your programs are mapped to their values/answers/behaviors, and this is what should be called 'operational semantics' and reasoned about.". Note that this position goes somewhat against the current distinction (which I think is rather consensual, but it may be a cultural bias on my part) between "operational semantics" and "denotational semantics", where the former is seen as more syntactic in nature (defined as a reduction relation), and the latter is typically characterized by the fact that computationally equivalent programs have the exact same denotation (so the denotation is oblivious to the actual computation mechanism). Under this latter view, what is proposed as an "evaluation strategy" or "operational semantics" in the articles and my explanation above would rather be seen as a denotational semantics -- but admittedly of a more concrete nature than most: values/answers/behaviors are closer to syntactic objects than many semantic domains. References: to understand this point of view, it is probably useful to go back to its proclaimed source, which is the article by Gordon Plotkin in 1973. You may also have good luck trying one of the latter articles cited on wikipedia; I found for example that "Parameter-Passing and the Lambda Calculus", by Crank and Felleisen, 1991, gave a very clear overview of their position on the matter in the first few pages. 

One first reason to reject axioms is that they might be inconsistent. Even for the axioms that are proved consistent, some of them have a computational interpretation (we know how to extend definitional equality with a reduction principle for them) and some do not -- those break canonicity. This is "bad" for different reasons: 

Let be the Turing machine that writes on all positions of its output tape (it doesn't terminate, but that's not an issue). Let now be the machine that computes the Syracuse/Collatz sequence for all successively, and for each writes in the -th output position if the sequence goes back to a . Now test if is equal to , and you've answered the Collatz conjecture. Similarly, you can check the halting problem for any turing machine by just turning it into the machine that writes on the whole output tape after it normal function has terminated, and then comparing that to . 

Clearly there should be a rule that allows to build a (mu a. t3) on the right-hand side; I think the rules above are probably not complete. Some presentations of recursive subtyping have rules of the form: 

Implicit Complexity has taught us that (some) complexity classes can be classified by type systems, in the sense that there are type systems that only accept polynomial programs, for example. One more practical-minded offshoot of this research is RAML (Resource Aware ML), a functional programming language with a type system that will give you precise information of the running times of its programs. It is more precise, in fact, than big-O complexity, are constant factors are also included, parametrized by a cost model for the base operations. You may think that this is cheating: surely some algorithms have a complexity that is very hard to compute precisely, so how could this language easily determine the complexity of its programs? The trick is that there are many ways to express a given algorithm, some which the type-system will reject (it rejects some fine programs), and some, maybe, that it accept. So the user doesn't get the world for free: he or she doesn't have to compute cost estimations anymore, but need to figure how to express code in a way that is accepted by the system. (It is always the same trick, as with programming languages with only terminating computations, or provable security properties, etc.) 

Why are you looking at dependent type theory to represent OOP? Can't we model OOP in a satisfying way with non-dependent calculi? I have an informal model of what OOP looks like, say, when translated to System F (or Fω if you want to support generics), and I don't see where the type-value dependency would come into play. Dependent types can be used, for example, to give a lower-level meaning to algebraic data types. You could probably do such a low-level encoding of OO features, but I'm not sure that's better than adding algebraic datatypes to your modeling language. Maybe you want to give a finer static semantics to OOP constructs that are currently untyped, such as followed by a . I can see dependent type hackery being useful to statically reason about such programs; but I'm not sure it would "model" those operations that concentrate on the dynamic angle, it's something more.