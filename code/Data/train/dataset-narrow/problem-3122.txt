If you want to stick with vanilla Machine Learning, SVMs hav been known to work really well on text classification problems. You can experiment with various kernels. Alternatively, you can try neural networks with word embeddings. However from your evaluation metrics, it seems you have a data imbalance problem, hence the low precision and recall. Accuracy becomes completely meaningless here. Before you try anything else, I would suggest that you try to balance your samples by getting more data, if that doesnt seem to be possible then try bagging. 

Yes, you can run a regression using the features you have, and predict the revenue but you will need more features to run an effective analysis. Maybe adding features for the genre and wether the music is trending or not. To turn it into a binary classification problem you will have to decide on a cut-off and label everything above that as 1 and below it as 0, the median might be a good cut-off but solely depends on your data. This entire problem will boil down to feature engineering, which in my view is one of the hardest thing to do. Also you will need a lot of data for this. You can create features wether the song was featuted on the billboard. See, try to think of it like a human and add such features to your model. Like as a person I think the music that will generate more revenue will be : 

Seems right. However, establishing causality will not be as simple as extracting keywords and noticing the differences. And I would suggest not to divide the posts, instead club them together run LDA, extract the keywords, then analyse the differences. By separating you are introducing quite a huge bias into your model. 

You could use K-NearestNeighbors to do this. Nearest Neighbors is what you are probably you are looking for. You can try out different distance metrics. Cheers. 

It is definitely possible, but obviously to a certain extent. What you need, is a sequence to sequence model trained on questions and answers data of a domain. Denny Britz has this amazing blog post on impelementing a retreival based chatbot trained on ubuntu dialog corpus using tensorflow. Go through the post to understand the difference between retreival based and generative model chatbot. The efficiency and efficacy of such a bot, is another question all together and rigorous research is going on to improve such bots. IBM watson does it pretty well as mentioned in the comments. 

You are right. Mainly any network with more than two layers between the input and output is considered a deep neural network. Libraries like tensorflow provide efficient architecture for deep learning applications such as image recognition, or language modelling using Convolutional neural networks and Recurrent neural networks. Another thing to keep in mind, is the depth of the network also has to do with the number of units being used in the layer. Mainly, as your non-linear hypotheses get complex you will need deep neural networks. 

You can use the first approach to run quick tests and check the efficacy of the approach and then move on to the second one if you found it fit to your use. 

Firstly your data is very less for any kind of analysis, so if it was posssible to get more data then that would be best. Secondly as you mentioned that your data was imbalanced then the accuracy metrics you have posted loose all meaning, since 140 samples are of the same class, the algorithm is predicting that class for every sample. So for better evaluation calculate precision, recall amd f-score. Thirdly, since your data is already less don't undersample, instead oversample using the SMOTE (Synthetic Minority Over Sampling Technique) implementation. Use a stratified KFold, and a Random Forest will mostly be your best bet here. But remember with this less data, it would be impossible to achieve a model without underfitting or overfitting. 

Why go for something so complicated? You can do it quite simply with a big enough dataset using convolutional neural network. There are tons of material available discussing the use of CNNs to image classification, MNIST digit recognition. Hope this helps. You can use PIL to extract pixel values. Though you will need labelled data. 

Logits is the unnormalized final scores of your model. You apply softmax to it to get a probability distribution over your classes. 

The goal of ID3 is to get the purest nodes possible ( ironically that is what contributes to its problem of overfitting), so 50% is not pure at all, the data under that node is equally likely to be in one of the classes which makes peedicition tricky, it would be better to grow the tree further and find nodes which are more pure than atleast 50%. 

If you are doing a classification task, use random forest classifier. If you want probabilities use predict_proba or you can use predict directly to get classes. You might be getting correct results but understand that the random forest regressor works on a different cost function, and is not constrained to give outputs between 0 and 1, so what you are getting out of the regressor are not really probabilities. If you run it on enough datasets it might start giving you outputs greater than 1. Hence, bottom line stick to classifier. Also I will recommend reading up a bit on the differences of regression and classification tasks. 

Okay, so from what I understand you need a scoring scheme rather than the scores themselves. I would suggest you to look into the TextRank algorithm paper here. It is mainly used for keyword extraction and works like pagerank, however the transition probabilities are defined on a lot of metrics, including synonyms which would solve your problem of counting indirect occurence. It would return you a ranked list of words along with the scores, you can use the scores of the words that you have already extracted. Normalize the extracted scores so that you can get better relative importances. Another simple approach is to replace all synonyms with one common token and then use the TF-IDF scores as the needed scores. You can use wordnet to extract synonyms. However, understand that these approaches are frequency based and no other information other than synonym information is being used. So they will not be pefect for your application but they will do. If you need a better system then you need human scored data and then running some kind of regression on a set of extractef features for words, as to what makes a word relevant and what doesnt. 

Use stratified K-Fold cross validation, it tries to balance the number of positive and negative classses for each fold. Kindly look here for the documentation and examples. If it still doesnt solve your problem of imbalance please look into SMOTE algorithm, here is a scikit learn implementation of it. 

Since you are going to use TF-IDF representations, you already have a feature matrix. To calculate cosine similairty between all vectors, you can use: 

What you need to use is mini-batch gradient descent or stochastic gradient descent. You will need to shuffle your samples and make draws from it of the batch size you are aiming for, also you will have to make sure all of the samples in your data are included which will constitute of one epoch. Train for a few epochs depending on the data. Here is a good blog post on mini-batch blog post. 

#Use numpy arrays To begin clustering, you can use K-means algorithm to begin with, and use cosine similairty as the distance metric. Here's an example from scikit-learn itself on clustering documents. Further things to try: If you find the above methods not working to your expectations, look into word2vec and doc2vec, and instead of using tfidf, which isa Bag of Words approach, use word vector representations. Here is a good blog explaining the concept. 

For details look into this RNN tutorial by Denny Britz, this is from scratch. If you are of the deep learning persuasaion then check out Denny Britz's CNN text classification tutorial where he uses tensorflow and trains his own word embedding. The first blog will give you all the information you need on how to prepare your text dataset. 

Looking at your code, I dont believe there is any mistake in it. The Stanford NER tagger is modelled by a Conditional Random Field Classifier and is hence bound to make mistakes. If you are able to check for a large number of samples and calculate an accuracy metric on the basis of that you might get a good idea if the pre-trained model is useful for you. Else, I suggest that you train your own model using. The stanford NLP group provide an implementation for training, you just have to give it the data in the required format. Look at this blog to get a good idea on how to do it. 

Stuff like this. So you wilk have to find features that quantify these things. Try to find some open source datatset for this kind of problem and work from there. 

I hope this gives you a rough idea of how the implementation can be done. You can easily find much more efficient and scalable ways of doing this. If you are not familiar waith algorithms like SGD, I would recommend to get familiar with them because online learning is just a one sample mini batch gradient descent algorithm. 

Yes, it will work. Basically by creating the encoding, scikit-learn's label encoder does the same thing, you are creating such more features each representing the presence or absence of that level in your sample. To represent d levels you need d-1 variables, so in your case that is what is exactly happening. However since you said, the number of levels can be massive it will not be a good idea to use a vector that massive. It would be a good idea to run some preliminary analysis and check if some levels are totally useless and dont relate to the response as well, some might have the exact same effect and hence those levels can be combined.