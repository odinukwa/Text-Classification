Consider the following cases: 1) One-way permutations (OWP) exist but trapdoor permutations (TDP) do not (i.e. we are in a variant of Impagliazzo's "minicrypt" world). In this case you just take the OWP that is guaranteed to exist, and you know that it doesn't have a trapdoor. 2) Both OWP and TDP exist. Here you have two options: (a) Every OWP has a key generation algorithm G that outputs the function's "public" description f along with a sampled trapdoor t. In this case, consider a modified key-generation that only outputs f. This gives you a OWP, and moreover it is infeasible to find t given f (as otherwise you have an efficient way to invert f). This should also hold for a non-uniform variant. (b) There exists a OWP f such that no algorithm G can output both f and t so that t enables inversion of f(x) for a random x. In this case f is a OWP that doesn't have a trapdoor. One of the comments in the thread above seems to suggest that you question is actually whether the existence of OWP is known to imply the existence of TDP. This has been shown not to hold wrt black-box constructions/reductions, and is open in general (see my comment in the thread above). 

One paper that deals with the issue is Games and the impossibility of Realizable Ideal Functionality by Datta et al, but it does't seem to address the issue in full generality. I am not aware of any general statement that guarantees simulation-based security of the whole protodol based on specific game-based security properties of its sub-protocols (unless of course the game-based definitions imply simulation based security, in which case the generic composition theorems by Canetti should apply). However, there do exist specific instances in which one can obtain simulation-based security from game based security. The most telling example in my view is a constant-round zero-knowledge (ZK) argument for NP by Feige and Shamir (see e.g. Feige's PhD), which builds on witness indistinguishabille (WI) and witness-hiding (WH) sub-protocols. Both WI and WH are (arguably) game-based definitions, and yet the entire protocol can be proved to be ZK (which is the mother of all simulation-based definitions, at least for interactive protocols). 

I am interested in the problem of packing identical copies of (2 dimensional) rectangles into a convex (2 dimensional) polygon without overlaps. In my problem you are not allowed to rotate the rectangles and can assume that they are oriented parallel with the axes. You are just given the dimensions of a rectangle and the vertices of the polygon and asked how many identical copies of the rectangle can be packed into the polygon. If you are allowed to rotate the rectangles this problem is known to be NP-hard I believe. However, what is known if you cannot? How about if the convex polygon is simply a triangle? Are there known approximation algorithms if the problem is indeed NP-hard? Summary so far (21 March '11). Peter Shor observes that we can regard this problem as one of packing unit squares in a convex polygon and that that problem is in NP if you impose a polynomial bound on the number of squares/rectangles to be packed. Sariel Har-Peled points out there is a PTAS for the same polynomially bounded case. However, in general the number of squares packed can be exponential in the size of the input which only consists of a possibly short list of pairs of integers. The following questions appear to be open. Is the full unbounded version in NP? Is there a PTAS for the unbounded version? Is the polynomially bounded case in P or NPC? And my personal favourite, is the problem any easier if you just restrict yourself to packing unit squares into a triangle? 

You can solve the problem in a fixed number of dimensions by extending the linear time original solution of Bird from 1977 $URL$ (subscription needed sadly). The general idea (in 2D) is in step 1 to build an Aho-Corasick automaton of the rows of the 2D pattern and then feed in the rows of the 2D text one by one. You will then find all the positions that the pattern rows match in the text. To finish you now only need to do a 1D search for the (labels of) the rows of the pattern in the right order in a column in the output of step 1, using KMP say. This all takes linear time. Using the same method you can reduce from any dimension d exact matching problem to a dimension d-1 problem. In this way you get a linear time solution for any fixed dimension d. 

Do you mean construction or inversion of BWT? For construction, the best algorithm is probably the one by Okanohara and Sadakane. It takes $O(n)$ time and usually requires $2n$ to $2.5n$ bytes of memory for an input of length $n$. There is an implementation available at Google code. I am not that familiar with BWT inversion algorithms. The papers of Kärkkäinen and Puglisi at ESA 2010 and CCP 2011 might provide a good starting point. 

This sounds similar to superbubbles in bioinformatics. We have a directed graph $G = (V, E)$. A superbubble is an induced subgraph defined by vertices $s, t \in V$ (with $s \ne t$). We have the following requirements: 

If you can compute the raggedness of a line without knowing anything about the other lines, then you can model the problem as finding a minimum-weight $M$-link path in a graph. With concave integer weights for edges, there is an algorithm that solves the problem in $O(N \log U)$ time, where $U$ is the largest absolute edge weight. Another algorithm solves the problem in $N 2^{O(\sqrt{\log M \log \log N})}$ time for any concave edge weights, assuming $M = \Omega(\log N)$. Both algorithms assume that you can compute the weight of an edge in constant time. You could also use binary search to find a line width such that SMAWK uses $M$ lines with it. In some cases, this algorithm does not guarantee a solution with exactly $M$ lines, however. 

I have an idea that might work. We start with a generalized suffix tree for sequences $S$ and $T$. Each internal node with suffixes of both $S$ and $T$ in its subtree corresponds to some common substring of the sequences. Let us call such nodes non-trivial. The common substring is maximal, if the corresponding node has no non-trivial children. If node $v$ is non-trivial, we store the largest string-depth of a non-trivial node in its subtree as $lcs(v)$. If $r$ is the root, then $lcs(r)$ is the length of the longest common substring of $S$ and $T$. Updating the tree after deleting a substring from one of the sequences should not be too hard. We first delete the leaves corresponding to the deleted suffixes, updating their ancestors when required. Then we start processing the suffixes preceding the deleted substring. Let $v$ be the lowest non-trivial ancestor of the current leaf. If the length of the suffix is $k$ (we are $k$ steps from the deletion) and $k < lcs(v)$, we have to move the suffix to its proper position in the tree, updating the ancestors when required. If $k \ge lcs(v)$, we are done, as we are not interested in subtrees with trivial roots. The overall algorithm repeatedly finds the longest common substring of $S$ and $T$ and deletes one of its occurrences from both sequences, as long as the length of the LCS is large enough. There are some technicalities, but the general idea should work. 

Generally spealing, the definition of a cryptographic protocol consists of two different parts: syntax and security. The syntax specifies the functionality of the protocol under legitimate use (dealing with issues such as key generation, correct decryption, valid signature verification, or more generally some desired output). A protocol can be totally "insecure" and still satisfy the syntax of the cryptographic task at hand. The security part deals with guaranteeing that the protocol can be used safely in a cryptographic context. It specifies the access that the adversary has to the protocol, as well as what it means for the adversary to break it. It is desirable that a security definition carries some "semantics" with it, in the sense that if a protocol satisfies this definition then the user is convinced that the security guarantee is meaningful (e.g. having a security definition that allows any protocol is certainly "legitimate" but it clearly doesn't guarantee any security). The biggest conceptual contribution of modern cryptography is to develop a methodology for coming up with security definitions that are extremely meaningful and at the same time realizable (see Goldwasser Micali, Goldreich, Goldwasser, Micali and Goldwasser, Micali, Rivest for prime examples of this methodology). Following the works mentioned above it has become common (some would say mandatory) practice to define both syntax and security and to prove that a given protocol satisfies the given definitions (usually under some widely accepted intractability assumptions). The precise definitions to be satisfied depend on the cryptographic task at hand, and are evaluated in light of the intended application. As Sadeq points out in his answer, the general syntax of protocols is defined via interactive Turing Machines (by Goldwasser Micali Rackoff). This definition allows to model players that "keep state" between messages that are sent and received. The GMR paper is also the first to rigorously define security for interactive protocols, and in particular what it means for a protocol to be zero-knowledge. More general security requirements are given in later papers on secure two and multi-party computation. For references to these see Sadeq's answer. 

I suggest you look at a survey by Oded Goldreich, called A computational perspective on sampling. In that survey he presents some basic facts on expanders (along with pointers to more extensive material). These facts seem to be sufficient to at least understand the "Security Preserving Amplification of Hardness" paper. In particular, in appendix A he surveys random walks on expanders, and in C.4 he presents the expander hitter, which is what is essentially done in that paper. 

The first rule of concurrent data structures is: You do not want concurrency. In the ideal case, distributed/parallel/concurrent computing means that you have a number of completely independent sequential processes. Each process has its own data and resources, and the process is not even aware of any other processes. In the worst case, you have a shared memory system with multiple threads querying and updating the same data structures concurrently. Something has probably gone horribly wrong, if you are seriously considering this. Of course, when we are talking about concurrent data structures, some degree of concurrency is unavoidable. We still want to minimize it. The longer a process can work sequentially without touching mutexes, doing atomic operations, or passing messages, the more likely everything works correctly and the performance is acceptable. Static data structures with batch updates require less synchronization than dynamic data structures. You should try to make your concurrent data structures static, or at least as close to static as possible. If your algorithm requires interleaving queries with updates, try changing the algorithm before resorting to shared dynamic structures. The same design principle also applies to updating static data structures. The more independent you can make the processes updating the structure, the better everything works. 

The problem becomes easier, if we consider long deletions and substring copying instead of transpositions. Assume that we are using the standard dynamic programming algorithm for edit distance computation, and that an expensive operation of length $k$ increases the distance by $ak+b$, for some constants $a,b \ge 0$. These constants may be different for long deletions and substring copying. A long deletion is the deletion of an arbitrary substring from $x$. Supporting them is easy, if we break them down into two kinds of simple operations: deleting the first character (cost $a+b$) and extending the deletion by one character (cost $a$). In addition to the standard array $A$, where $A[i,j]$ is the edit distance between prefixes $x[1 \dots i]$ and $y[1 \dots j]$, we use another array $A_{d}$ to store the edit distance, when the last operation used was a long deletion. With this array, we only have to look at $A[i-1,j]$, $A[i-1,j-1]$, $A[i,j-1]$ and $A_{d}[i-1,j]$ when computing $A[i,j]$ and $A_{d}[i,j]$, allowing us to do it in $O(1)$ time. Substring copying means the insertion of an arbitrary substring of $x$ into the edited string. As with long deletions, we break the operation down into two simple operations: inserting the first character and extending the insertion by one character. We also use array $A_{s}$ to store the edit distance between prefixes, provided that the last operation used was substring copying. Doing this efficiently is more complicated than with long deletions, and I am not sure whether we can get to amortized $O(1)$ time per cell. We build a suffix tree for $x$, which takes $O(|x|)$ time, assuming a constant-size alphabet. We store a pointer to the current suffix tree node in $A_{s}[i,j-1]$, allowing us to check in constant time, whether we can extend the insertion by character $y[j]$. If that is true, we can compute $A[i,j]$ and $A_{s}[i,j]$ in constant time. Otherwise $zy[j]$, where $z$ is the inserted substring that was used to compute $A_{s}[i,j-1]$, is not a substring of $x$. We use the suffix tree to find the longest suffix $z'$ of $z$, for which $z'y[j]$ is a substring of $x$, in $O(|z|-|z'|)$ time. To compute $A_{s}[i,j]$, we now need to look at cells $A[i, j-|z'|-1]$ to $A[i,j-1]$. Finding suffix $z'$ requires just amortized $O(1)$ time per cell, but computing $A_{s}[i,j]$ with a brute-force approach takes $O(|z'|)$ time. There is probably some way to do this more efficiently, but I cannot find it right now. In the worst case, the algorithm takes $O(\min(|x| \cdot |y|^{2}, |x|^{2} \cdot |y|))$ time, but a better analysis should be possible. The resulting edit distance with long deletions and substring copying is not symmetric, but that should not be a problem. After all, it is usually easier to reach the empty string from a nonempty one than the other way around. 

I have been wondering about comparison versus RAM models and have some very basic sounding questions. Is there a name for the subset of P (or FP really) that is computable by comparison RAM algorithms? For the avoidance of confusion we can think of the comparison RAM model as just being the normal RAM model where the input symbols are given in terms of rows/columns of a ternary matrix that tells us if pairs of symbols are bigger, the same, or smaller. That is we never get to find out or use the value of the input symbols. What are the biggest known gaps for the time complexity of RAM and comparison RAM algorithms for natural problems coming from this subset of P (or FP)? We can imagine unnatural examples where comparisons are constant time but the RAM model has to spend a lot of time inputting the bits. Those are not the ones I am interested in. Sorting is the simplest example with a log gap for poly size inputs but is there anything bigger? Related to the last question, is there some reason to suppose a maximum gap between (natural) RAM and comparison RAM problems? I suppose one route would be to say that you could simulate one with the other efficiently. 

Like many things in life, there is no one definitive definition. For an algorithm to run in real-time, some people on the theoretical side say that this means it will take constant time per 'something.' Now you have to decide what a 'something' is but let me give a concrete example. Let's say that the input arrives one symbol at a time and you want to output the answer to a query as soon as a new symbol arrives. If calculating that output takes constant time per new symbol then you might say the algorithm runs in real-time. An example of this is real-time exact string matching, which outputs whether a pattern matches the latest suffix of a text in constant time per new symbol. The text is assumed to arrive one symbol at a time. However, an engineering answer will be less worried about "constant time" and more worried about it happening fast in practice and in particular fast enough that the result can be used by the time it is needed. So for example in robotics, if you want to play ping-pong it is useful for the robot to be able to work out where the ball is and move to hit it as the ball arrives, and not after the ball has passed. The asymptotic time complexity of the underlying algorithms will perhaps be of less interest there than just the observation that the code works out the location quickly enough. To give another example, if you want to render video and can do it at 25 frames per second then it is reasonable to say that the rendering is happening in real-time. So basically you have two answers. One for the theoreticians/algorithmists and one that just says that you are doing the work as you need it on the fly. EDIT: I should probably add that one extra feature one should require of even a constant time algorithm is that the time complexity is not amortised. In this context, real-time == unamortised constant time.