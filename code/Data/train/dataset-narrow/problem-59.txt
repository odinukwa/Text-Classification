We can implement light->SampleLi however we want; we can choose the point uniformly, or importance sample. In either case, we divide the radiosity by the pdf of choosing the point. Again, to satisfy the requirements of Monte Carlo. If the BRDF is highly view dependent, it may be better to choose a point based on the BRDF, instead of a random point on the light. But how do we choose? Sample based on the light, or based on the BRDF? Why not both? Enter Multiple Importance Sampling. In short, we evaluate $BSDF(p, \omega_{\text{i}}, \omega_{\text{o}}) L_{\text{i}}(p, \omega_{\text{i}})$ multiple times, using different sampling techniques, then average them together using weights based on their pdfs. In code this is: 

It would be prohibitively expensive to try to model the light particle's interaction with the molecules of the material. We instead, make some assumptions and simplifications. 

First, we add "color += throughput * SampleLights(...)". I'll go into detail about SampleLights() in a bit. But, essentially, it loops through all the lights, and returns their contribution to the color, attenuated by the BSDF. This is great, but we need to make one more change in order to make it correct; specifically, what happens when we hit a light. In the old code, we added the light's emission to the color accumulation. But now we directly sample the light every bounce, so if we added the light's emission, we would "double dip". Therefore, the correct thing to do is... nothing; we skip accumulating the light's emission. However, there are two corner cases: 

You've said that "... bilinear filtering on in the texture ...". It seems that you are interpolating the depth values of the shadow map. The correct way of using interpolation with the shadow map is to apply it over the outcomes of the shadow tests (as far as I remember, OpenGL supports that). You might even combine the interpolation of the outcomes of the shadow tests with PCF, which will deliver much better results. However, as you might have noticed, aliasing is a plague that always pursue the shadow mapping :) Although I understand that you are looking for solutions regarding shadow mapping (even because it is quite simple to implement), have you ever considered the use of shadow volumes? It is much more intricate to implement, but does not suffer from aliasing at all, and I think would fit nicely your purposes. 

This question is somewhat related to this one. As Alan has already said, following the actual path of the light ray through each layer leads to more physically accurate results. I will base my answer on a paper by Andrea Weidlich and Alexander Wilkie ("Arbitrarily Layered Micro-Facet Surfaces") that I have read and implemented. In their paper they assume that the distance between two layers is smaller than the radius of a differential area element. This simplifies the implementation because we do not have to calculate intersection points separately for each layer, actually we assume that the intersection points are the same for all layers. According to the paper, two problems must be solved in order to render multilayered material. The first one is to properly sample the layers and the second is to find the resulting BSDF generated by the combination of the multiple BSDFs that are found along the sampling path. Sampling In this first stage we will determine the actual light path through the layers. When a light ray is moving from a less dense medium, e.g. air, to a more dense medium, e.g. glass, part of its energy is reflected and the remaining part is transmitted. You can find the amount of energy that is reflected through the Fresnel reflectance equations. So, for instance, if the Fresnel reflectance of a given dielectric is 0.3, we know that 30% of the energy is reflected and 70% will be transmitted: 

Can you use a fixed probability (cut-off), and then redistribute the 'lost' energy. Is this unbiased? 

Finally, the pdf. In monte-carlo integration, we need to combine the pdf's of each integration we do. In path tracing, we can integrate over many many things. For example, the general rendering equation integrates the incoming light over the hemisphere, depth of field can be treated as an integration over a focal distance, etc. For next event estimation, you explicitly split the rendering equation into two integrands, direct lighting, and indirect lighting. Standard rendering equation: $$ L_{\text{o}}(p, \omega_{\text{o}}) = L_{e}(p, \omega_{\text{o}}) \ + \ \int_{\Omega} f(p, \omega_{\text{o}}, \omega_{\text{i}}) L_{\text{i}}(p, \omega_{\text{i}}) \left | \cos \theta_{\text{i}} \right | d\omega_{\text{i}} $$ Next Event Estimation: $$ L_{\text{o}}(p, \omega_{\text{o}}) = L_{e}(p, \omega_{\text{o}}) \ + \ \int_{\Omega} f(p, \omega_{\text{o}}, \omega_{\text{i}}) L_{\text{i, direct}}(p, \omega_{\text{i}}) \left | \cos \theta_{\text{i}} \right | d\omega_{\text{i}}\ \ + \ \int_{\Omega} f(p, \omega_{\text{o}}, \omega_{\text{i}}) L_{\text{i, indirect}}(p, \omega_{\text{i}}) \left | \cos \theta_{\text{i}} \right | d\omega_{\text{i}} $$ In simple naive forward path tracing, everything is is treated as indirect light. In next event estimation, we directly calculate the direct lighting and add it to the indirect lighting. And if we hit a light, we ignore the contribution, since we're calculating the direct lighting. Since we have two integrations, each will have its own pdf. Aka: $$L_{\text{o}}(p, \omega_{\text{o}}) = L_{e}(p, \omega_{\text{o}}) \ \ + \ \sum_{k=0}^{\infty } \frac{f(p, \omega_{\text{o}}, \omega_{\text{i}}) L_{\text{i, direct}}(p, \omega_{\text{i}}) \left | \cos \theta_{\text{i}} \right | }{pdf_{direct}}\ \ + \ \sum_{k=0}^{\infty } \frac{f(p, \omega_{\text{o}}, \omega_{\text{i}}) L_{\text{i, indirect}}(p, \omega_{\text{i}}) \left | \cos \theta_{\text{i}} \right | }{pdf_{indirect}}$$ If you want to see how this is implemented, you can check out my implementation here. Note: my light sampling is a bit more complicated, since it does multiple importance sampling. But it can be as simple as: 

According to the code above, the path branches recursively if the ray has bounced up to two times. After two bounces, however, RR is used to select the path to be followed. This is also Ok for me. What is a bit confusing is the fact that the radiance returned by both possible non-branching paths (refraction and transmission) is scaled. I understand that there are different probabilities regarding reflection and transmission. However, if for instance Re = 0.3 and Tr = 0.7, and 100 rays strike the surface, about 30% of the rays will be reflected and 70% of will be transmitted due RR. In this case, I understand that there is no path termination neither energy loss, so there wouldn't be anything to compensate for. Thus, my first two questions are: why are these radiances scaled? Should they be scaled, or would it work without scaling at all? My third question is related to the scaling factors: Why the author has used P, RP and TP instead of Re and Tr? Any indication of a good reading about this topic is also very welcome!! Thank you! 

As can be seen, TIR or Fresnel reflectance might keep some rays bouncing indefinitely among layers. As far as I know, Mitsuba implements plastic as a two layer material, and it uses a closed form solution for this specific case that accounts for an infinity number of light bounces among layers. However, Mitsuba also allows for the creation of multilayer materials with an arbitrary number of layers, in which case it imposes a maximum number of internal bounces since no closed form solution seems to exist for the general case. As a side effect, some energy can be lost in the rendering process, making the material look darker than it should be. In my current multilayer material implementation I allow for an arbitrary number of internal bounces at the cost of longer rendering times (well... actually, I've implemented only two layers.. one dielectric and one diffuse :). An additional option is to mix branching and RR. For instance, the initial rays (lower deep levels) might present substantial contribution to the final image. Thus, one might choose to branch only at the first one or two intersections, using only RR afterwards. This is the case with smallpt. An interesting point regarding multilayered materials is that individual reflected/transmitted rays can be importance sampled according to the corresponding BRDFs/BTDFs of the current layer. Evaluating the Final BSDF Considering the following light path computed using RR: 

For more info, I suggest looking up how some window managers do compositing. There is a decent explaination of how WPF works here. And an excellent post about how Chrome accelerates their browser rendering here. 

If the energy that would be lost by terminating a ray without redistributing its energy is eventually lost anyway (as the rays to which it is redistributed are also eventually terminated), how does this improve the situation? 

So input[n], with output[n] results in spectral[n] The ASTM file is a flattened version of the MATLab data file, with one major difference: the input and output vectors are in spherical coordinates, specifically the traditional physics notation: 

Another optimization in ray tracing to to trace rays from the camera into the scene, rather than tracing them from light sources and hoping they hit the camera. Therefore, the first rays we shoot out are the ones that go from the eye through each pixel on the virtual screen. 

There are a number of optimizations / improvements you can do in these functions, but I've pared them down to try to make them easier to comprehend. If you would like, I can share some of these improvements. Only Sampling One Light In SampleLights() we loop through all the lights, and get their contribution. For a small number of lights, this is fine, but for hundreds or thousands of lights, this gets expensive. Fortunately, we can exploit the fact that Monte Carlo Integration is a giant average. Example: Let's define $$h(x) = f(x) + g(x)$$ Currently, we're estimating $h(x)$ by: $$h(x) = \frac{1}{N} \sum_{i=1}^N f(x_i) + g(x_i)$$ But, calculating both $f(x)$ and $g(x)$ is expensive, so instead we do: $$h(x) = \frac{1}{N} \sum_{i=1}^N \frac{r(\zeta, x)}{pdf}$$ Where $\zeta$ is a uniform random variable, and $r(\zeta, x)$ is defined as: $$r(\zeta, x) = \begin{cases} f(x), & 0.0 \leq \zeta \lt 0.5 \\ g(x), & 0.5 \leq \zeta \lt 1.0 \end{cases}$$ In this case $pdf = \frac{1}{2}$ because the pdf must integrate to 1, and there are 2 functions to choose from. In English: 

When light hits a conductor or a diffuse surface, it will always be reflected (being the direction of reflection related to the type of the BRDF). In a multilayer material, the resulting light path will be the agregate result of all those possibilities. Thus, in the case of a 3-layer material, assuming that the first and secong layers are dielectrics and the third layer is diffuse, we might end up with the following light path (a tree actually): 

We can evaluate the total amount of radiance $L_r$ reflected by a multilayer BSDF considering each layer as a individual object and applying the same approach used in ordinary path tracing (i.e. the radiance leaving a layer will be the incident radiance for the next layer). The final estimator can thus be represented by the product of each individual Monte Carlo estimator: $$ L_r = \left( \frac{fr_1 \cos \theta_1}{pdf_1} \left( \frac{fr_2 \cos \theta_2}{pdf_2} \left( \frac{fr_3 \cos \theta_3}{pdf_3} \left( \frac{fr_2 \cos \theta_4}{pdf_2} \left( \frac{L_i fr_1 \cos \theta_5}{pdf_1} \right)\right)\right)\right)\right)$$ The paper by Andrea Weidlich and Alexander Wilkie also takes absorption into consideration, i.e. each light ray might be attenuated according to the absorption factor of each transmissive layer and to the distance traveled by the ray within the layer. I've not included absorption into my renderer yet, but it is just a real coefficient computed according to the Beer's Law. Alternate approaches The Mitsuba renderer uses an alternate representation for multilayered material based on the "tabulation of reflectance functions in a Fourier basis". I have not yet dig into it, but might be of interest: "A Comprehensive Framework for Rendering Layered Materials" by Wenzel Jacob et al. There is also an expanded version of this paper.