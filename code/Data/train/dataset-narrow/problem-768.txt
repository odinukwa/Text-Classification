Honestly, the easiest way to do this will be to upgrade your local instance to 2008 R2, or install a second local instance of 2008 R2. Then it's just a matter of making a database backup from the server, copying it to your local machine, and restoring it on a local instance. If you really need to transfer the database to 2008, for development or troubleshooting on that version, you'll have to do all sorts of unpleasant things like scripting out all database objects (including indexes, security, etc), transferring the data (bcp or SSIS), and hoping that your database isn't using any features that are new to 2008 R2. It's much less hassle to just upgrade. 

At the end of this, contains the query for the current request. Also, you could just as easily use a table variable instead of a temp table. 

You'll have to grant them appropriate permissions at the database level to indicate what they should be able to do. Then grant ALTER permission (or make them the owner) on the schemas in which they can do these things. 

It's certainly not required for any functional reasons, but it's not a bad idea for some quick and dirty documentation. It sounds like they're confusing it with the Access relationship designer, which is where you actually declare your foreign keys. The SQL Server database diagram designer looks similar, but has a completely different purpose (documentation rather than DDL). 

Now, as for whether declaring a PRIMARY KEY instead of a plain CLUSTERED INDEX (which 2014 lets you do) will result in different query plans? That I can't say authoritatively. This contrived test was still a clustered index seek: 

Make a log backup. This marks the active log space for reuse. Shrink the log file. This will shrink it down to the end of the last virtual log file in use (which will probably leave it larger than what you wanted to shrink it to). Since the end of the log file will now be in use, given that's where it stopped shrinking due to a VLF being in use, SQL Server will wrap back to the beginning of the log file for further log writes. CHECKPOINT to make sure dirty pages for active transactions are flushed, then make a backup again. (The CHECKPOINT might not be necessary - you can try without first.) Shrink the log file again. You should be able to bring it down to where you want. 

Yes, you have two clauses with a multi-table statement. This is essentially just an anti-join turned into a deletion. 

If a package is using 32-bit components (e.g. OLEDB libraries), you will need to choose the "32-bit runtime" option on the Advanced tab when running the package on a 64-bit server. 

So I'm assuming you're reasonably safe. Of course, that's not to say you couldn't do something wacky like having a stored procedure testing for membership in those roles, and implementing its own security functionality based on the results. 

There are times when the indexes on underlying tables just aren't enough to satisfy a particularly big query with lots of joins/applies/subqueries/CTEs. For those times, temp tables and table variables can be just what you need to improve performance. By using a temp table to store intermediate results, you can create any indexes or statistics that you need to improve further processing. I run into this now and then when we need to build an occasional-use report on top of a bunch of OLTP data. Table variables can be used to similar ends, but with the restriction that they're not nearly as flexible with indexing (up until 2014, at least, which began to remove much of this restriction). But they are a bit lighter-weight, and more importantly they are scoped to the module rather than session, so you don't have to worry about naming conflicts like you would with temp tables. There is overhead associated with creating and writing to temp tables, of course, to say nothing of the impact on code maintainability, so if the query can be written cleanly as a single query making use of set-based operations without sacrificing performance, then stick with that approach. 

The relevant thing to know about SQL Server transaction log files is that they are written to in a circular fashion. This is going to over simplify things, and leave out some details like virtual log files (how it's actually handled internally), but it's a reasonable facsimile of reality. Imagine you create a database and give it a 100 MB transaction log file. Transactions will be written starting from the beginning of the file. Suppose you then have 50 MB of transaction activity. You now are using the first 50 MB of the 100 MB transaction log. Then you take a transaction log backup. Now you're using nearly no space in the transaction log; practically the whole thing is available for (re)use. But what happens if you try to shrink the log to, say, 20 MB? It'll shrink it down to (roughly) 50 MB. This is because the active portion of the log happens to be sitting around that 50 MB mark, and SQL Server won't shrink the log past where it's currently active. Think of it kind of like the play head on a cassette tape, and SQL Server won't/can't do anything with tape that's already on the take-up reel. This is why shrinking the transaction log is frequently a two-step process: you do the first backup and shrink (chop off all the empty tape after the play head), then you generate some transaction activity so that SQL Server is forced to wrap back around to the beginning of the log file (rewind the tape), and finally do another backup and shrink (chop off more empty tape to get to your target size). Note that there are some other situations that can prevent you from reusing (and thus shrinking) logs. Usually you can find out the reason by looking at the column in . Transactional replication, database mirroring, and AlwaysOn availability groups are common culprits. 

I have a user account - let's call it 'wordpress' - that I need to allow to access a few catalog tables in another e-commerce database on the same server. I've configured the user with three host masks it's allowed to connect from: 'localhost', the IP address of the web server, and the hostname of the web server. No problems there. The 'wordpress' user also has full access to its own database, granted via the Schema Privileges section in MySQL Workbench. Here, it shows the host is '%', which is what I want, since I don't want to manage three duplicate sets of privileges for the same user. If I look in mysql.db, I see these privileges, with '%' in the Host column. So now I want to grant SELECT permission on a handful of tables in another database - let's call it 'store'. So I try this: 

Will Management Studio 2012 install and run side-by-side with, say, Management Studio 2008? I vaguely recall not being able to do this with 2008 and 2005. Does Management Studio 2012 fully support connecting to and administering SQL Server 2008? Does Intellisense still work against older versions? 

It's not as sophisticated as what full-text indexing does, and it only accounts for matching on a single column, but it gives decent enough results. 

Go to Tools, Internet Options. Go to the Security tab, click Local intranet, and click the Sites button. Click Advanced to open the list of sites. Enter the full name ("server2.newdomain.net") into the appropriate box, and click Add. 

Another point worth adding: If you ever take a differential backup, it will use whichever full backup was most recent as its base, meaning you would potentially have to determine which system to retrieve the full backup from in order to restore a differential (and hope that said system is still operational). You can use the option to take a full backup without resetting the differential change map, but you can't use that copy-only backup to restore any differentials. It's best to have one official sequence of full/differential/log backups being taken for recovery purposes, and use for doing things like restoring test copies of data. 

Extra write overhead. Snapshots are copy-on-write, so this will obviously impact performance. A snapshot will share pages from the old database on disk, but it uses its own memory in the buffer pool. If you have a large table, it will end up consuming memory for both the OLTP database, and the snapshot, even for the unmodified pages. You can end up with a lot of additional memory usage because of this, which can lead to more disk I/O if your server doesn't have loads of free RAM. 

I don't know if this will work with PDW specifically, but I've got an awful, dirty hack I use with user-defined functions, which also don't allow RAISERROR. Just attempt to cast a varchar literal containing your error message to int. 

Note that service names may vary if you aren't restarting the default instance. Also, the user account that the scheduled task runs under will need to have permission to restart the service. I don't see any reason why this wouldn't work with a recurring schedule, as opposed to the one-time runs that I do. 

Summary: stick with , , or , and make sure you're using the one that returns what you actually need. 

Create a new query in the query designer. In the Query Type section of the ribbon (looking at Access 2010 at the moment), click the "Pass-Through" button. Open the property sheet for the query, then in the "ODBC Connect Str" field, click the little ellipsis button. You'll have to choose an ODBC connection, much like setting up a linked table. Enter the query exactly as you want it passed to SQL Server. 

They won't perform major functional changes to user databases, but there may be internal structural changes to the databases to support bug fixes, changes in database engine functionality, etc. If you've ever restored a backup from an older patch-level of SQL Server onto a newer one, you'll see a bunch of messages about it performing upgrade steps on the database that was restored. This same thing happens when patching SQL Server to a newer version. Thus you will not be able to move that upgraded database back to an older build of SQL Server. 

You're using Enterprise edition, so you should be able to use online indexing. Add the option to your index DDL statement. The operation will still take just as long (possibly longer), but SQL Server will do some magic to keep the table accessible throughout. Here's some documentation with assorted restrictions, but they mostly only come into play if you've got LOB columns in the table: $URL$ This won't help you with altering the column data type, of course. If it's constantly being inserted, but not necessarily updated or read (and you can live with such operations being unavailable during the transition), you can rename the original table, create a new one with the desired structure and permissions, move all the data over, then recreate the indexes on the new table. This may be slightly more complicated if you have an identity column and want to preserve values, but it's still doable. 

Day (Dimension Key) - post_date Month - post_year, post_month Quarter - post_year, post_quarter = Year - post_year 

Are the substring ranges always the same? If so, you could turn that into a computed column, and index it. 

Using parentheses allows you to logically evaluate the inner (required) join before the optional outer join, preventing undesired filtering of records. 

It's possible that the corruption is a total fluke (either a software bug, or a hardware error that went undetected), but if it hits again, and corrupts a table/clustered index, rather than just a nonclustered index, then you could have a bigger problem on your hands. 

The issue you'll run into here comes from the fact that SQL Server doesn't have the "FOR EACH ROW" triggers that Oracle does. You have to write your triggers to handle multi-row changes, i.e. the INSERTED or DELETED virtual tables can have more than one row in them. If such an update were to happen, your trigger would fail, since would return multiple rows, and subqueries have to return a single value to be used in a direct comparison. The trigger would probably have to look something like this: 

I've had to resort to a bit of guesswork, but as far as I can tell, I haven't done anything overly permissive. I started with a non-privileged test account that's a member of the AD group I created, and I was able to navigate as far as the FileStream directory for the database (), but nothing was visible in there. So, I tried the most obvious approach first: 

I'm trying to get Apache installed on our main database/application server, and I'd like to give it its own IP address on a secondary NIC so I don't have to use a nonstandard port. This requires that I ensure everything else listening on port 80 is bound specifically to the primary IP address, rather than "all". This is no problem for the IIS sites, and I've already configured them appropriately. The Reporting Services Configuration Manager seems to only give you the option of binding to a hostname or IP address, but not both. Is there a way around this via command line tools or direct editing of the config data, or will I need to add a third IP address and dedicate it to Reporting Services? 

So if you're trying to make reporting less intrusive on only a single server, you're probably better off just querying the OLTP database and using WITH (NOLOCK) hints, and then reigning things in with the resource governor if needed. Beyond that, look into some kind of scale-out system (log shipping, mirroring, etc.) 

EDIT: One other speed-up I found is wrapping all the executions in a single transaction (presumably this eliminates the overhead of constantly locking and releasing locks). Do before instantiating your SqlCommand, and after it's done. Here are the run times on my development machine: 1:08.14 - Original version 0:33.05 - Changed to open the connection only once, and reuse the SqlConnection 0:11.93 - Changed to use a single transaction 

If you want to omit those specific SQL statements, you could probably put them into the TextData NOT LIKE filter for the trace. Escape the underscore with . 

Main form bound to . Subform linked to main form via primary key of (e.g. parent form = subform ). Form inside subform bound to query that joins to (). 

This sounds like a great situation to use indexed views. These are pretty much what they sound like: a view that physically stores its results in the database, and is updated automatically when the base tables are updated. A few cautions: 

If that still doesn't work, check DBCC OPENTRAN, and the log_reuse_wait column in master.sys.databases. 

Or you could combine the two; look for earliest matches first, and within those, prefer shorter values. 

I'd probably stick with triggers, for the same reasons gbn described, but if performance becomes an issue, you may want to consider using Service Broker to perform the work asynchronously (if business requirements will allow that). The typical design would be to have a trigger call a stored procedure that sends a Service Broker message, and the Service Broker queue handles the work where latency is occurring. Here's a short (noncommercial) article I wrote that might help keep any triggers from getting out of hand: Writing Well-Behaved Triggers 

If you COALESCE the individual columns first in order to replace them with zero, you will likely get the result you're expecting: 

In the absence of any other input, here's what I was able to deduce. Short story: this appears to work fine. As far as I can see, enabling TDE for a database is a logged operation, but the actual data encryption is not logged (otherwise the transaction log would grow considerably, which it doesn't). I assume there's something like the differential change map, or a flag in the page headers to indicate if a page is encrypted or not, and the storage engine just chews through everything until it's fully encrypted. When I enable TDE at the primary database, the secondary database ends up encrypted too (I have verified this with a hex editor). While encrypting is in progress, the view sys.dm_database_encryption_keys indicates percent_complete for all the secondary servers is 0, and this value never increases. The primary server updates this column normally. All the replicas tend to finish encrypting at their own pace, at which point encryption_state changes to 3. Seems like everything works normally once it's finished.