A perfect chess player will always force a win when they can force a win and force a draw when they can force a draw. Of course, at any point if they can force a win, they can also force a draw. Also when ever one player can't force a win, the other player can force a draw. Chess without the 50 move rule or 3 fold repetition rule might not be as hard to solve as you think. It can be shown that adding in the 3 fold repetition rule makes no difference to whether a player can force a win or a draw. The number of possible ways a game can go after n moves keeps growing exponentially with n. The number of states that can occur after n moves on the other hand doesn't keep on growing exponentially because it can't exceed the total number of possible states that can occur in a legal game. According to $URL$ there are about 10^47 states that can occur in a legal game of chess. Chess can be solved as follows: take a set of states that we can prove contains all states that can occur in a legal game of chess without the 3-fold repetition rule or 50 move rule. Two different states could have the same arrangement of chess pieces and differ by whose turn it is, whether you have the right to capture by en passant, and whether a given king or rook has the right to ever castle again. Next, take all states where the minimum number of moves white can force a win in is 1 which must occur on white's turn. Next take all states where the minimum number of moves white can force a win in is 2, which means it's black's turn and no matter which move they can make, white can force a win in 1 move. Next take all states where the minimum number of moves white can force a win in is 3, which means white has a move that will give them a forced win in 2 moves but can't force a win in 1 move. Next take all states where the minimum number of moves white can force a win in is 4, which means it's black's turn and no matter which move they make, white can force a win in 3 moves but white can't currently force a win in 2 moves. Once we get to a number such that there are no states where the minimum number of moves white can force a win in is that number, we've already found all the states that white can force a win in. We can find all states that black can force a win in in a similar way. All the remaining states are ones where both players can force a draw. Since there are about 10^47 states that can occur in a legal game of chess, it would take more than our lifetime to use brute force to build a computer that will play chess perfectly no matter how it's opponent plays. I believe it hasn't been proven that there's no much shorter algorithm that can tell you how to play perfectly no matter how your opponent plays. For instance maybe only a small fraction of states that can occur in a legal game can occur in a game where you play the way that algorithm tells you to play so that algorithm works even though it only tells you how to play perfectly in all states that can occur when you have always followed that algorithm since the beginning of the game but not in all states that can occur in a legal game. Maybe in addition to that, that algorithm is a complex algorithm that for each state that can occur in a game where you have always followed it, takes way fewer steps to compute an optimal move than the number of states that can occur in a game where you have always followed it. According to $URL$ the evolutionary learning laboratories are planning to solve complex problems. Maybe some day, they'll figure out a complex strategy for playing chess perfectly. Maybe even if an algorithm that's very short and takes very few steps to compute an optimal move in any state that can occur in a game where you have always followed that algorithm doesn't exist, that still doesn't stop a human from being able to learn how to play chess perfectly. Maybe a human could continuously figure things out and retain what they figured out figure more things out from what they previously figured out and retain them by some complex method, be able to figure out from the pieces of information they previously figured out how to play perfectly with a 90 minute base time and 30 second increments in any state that can occur in a game where they play the way they play after learning way fewer bits of information than the number of states that can occur in a game where they play the way they do which they can learn in their life time especially if the technology to live 6000 years gets invented. It's probably even simpler for a player to have a strategy that ensures that if their opponent plays perfectly, they will also play perfectly. I suspect both players have a forced draw from the beginning of the game. It's probably simpler to have a strategy that forces a draw than a strategy that guarantees that if your opponent gives you a forced win, you will not lose it. A strategy that forces a draw is also a strategy that ensures that if your opponent plays perfectly, you will play perfectly. If they play perfectly, they will not give you a forced win in the first place so you will not lose a forced win after they give you one. 

Take an enumeration $(\mathcal{M}_i,\mathcal{R}_i)_{i\geq 1}$ of pairs of polynomially time-bounded non-deterministic Turing machines $\mathcal{M}_i$ and polynomial reductions $\mathcal{R}_i$ (we can enumerate polytime machines and reductions by enumerating all machines and polynomials, and augmenting them with a polytime "stop clock" that enforces timely halting). For each pair of a machine $\mathcal{M}_i$ and reduction $\mathcal{R}_i$, the let $\mathcal{M}'_i$ denote a machine that does the following on input word $w$: 

A class of languages $C$ is recursively presentable if there is an effective enumeration of Turing machines $\mathcal{M}_1,\mathcal{M}_2,\ldots$ such that $C=\{L(\mathcal{M}_i)\mid i=1,2,\ldots\}$. Uwe Schöning considers this notion in his elegant generalisation of Ladner's theorem ("A Uniform Approach to Obtain Diagonal Sets in Complexity Classes", Theor. Comp. Sci. 18, 1982). In a previous answer on this site, Ryan Williams argues that the set of all P-hard languages can be recursively presented "similar to how Schoening does it for the NP-hard sets". I would like to know how this works, since I cannot find this in the paper. Schöning works with Turing reductions and shows that the NP-complete sets are recursively presentable. We can adapt his argument to NP-complete sets under polynomial many-one reductions as follows: 

After some further thought, partly triggered by writing a long question, it seems clear that the set of P-hard languages is not countable, and therefore not recursively presentable either. To see why, consider languages $L$ that satisfy $w\in 2SAT$ iff $ww\in L$. 2SAT can be log-space reduced to any such language by definition. However, there are countably many words that do not have the form $ww$, and the languages might differ on their membership status. Hence we obtain uncountably many P-hard languages of this form. Moreover, it does not seem possible to recursively present all decidable P-hard languages either, since there is no effective enumeration of all Turing machines that are guaranteed to halt (this seems to be the minimal requirement for applying Schöning's construction). 

The required enumeration is $\mathcal{M}'_1,\mathcal{M}'_2,\ldots$, since each of those either decides $\mathrm{SAT}$ (up to finitely many cases) or some other language in NP to which $\mathrm{SAT}$ can truly be reduced. I do not see how to apply this idea to recursively present all P-hard languages. One could readily recursively present, e.g., all P-hard languages in ExpTime (which would suffice as an answer in the other thread), but I don't see why the P-hard languages in general (including undecidable ones) should even be countable. 

Here is another attempt at a more comprehensive answer. Your question already contains the formal definition of FO-rewritability, which at its core says that you can reduce a query answering problem: 

Several noteworthy things are happening here. The original problem is a logical entailment question. We are asking if the Boolean query (=a special form of logical sentence) is a logical consequence of a set of facts $D$ together with a logical theory $\Sigma$ (consisting of certain rules, called tuple-generating dependencies). If you would want to answer this question directly, you might try to compute a universal model of $D\cup\Sigma$. This can be viewed as a "completed database" which is obtained from $D$ by adding (recursively!) all the facts that need to hold true for $\Sigma$ to be satisfied. In easy cases (e.g., if the rules $\Sigma$ do not have existential quantifiers), the computation of a universal model (usually called chase in this area) may lead to an exponentially large database. In harder cases, the universal model could be much larger or simply infinite (making it impossible to compute it to answer queries). This is why the original question $D\cup\Sigma\models Q$ is hard and undecidable in general. Now for FO-rewritable $\Sigma$, we can reduce this hard problem to a question $D\models Q_\Sigma$. You can also view this as a logical entailment problem as before, but the more natural view is to consider this as a model checking task. Indeed, $D$ is a finite structure (model) and we are merely asking if the sentence $Q_\Sigma$ is true in this structure. This problem of first-order model checking is the logical version of what practitioners know as "SQL query answering" (for basic SQL queries without complicated features). Indeed, every first-order formula $\varphi$ can be written as an SQL query $S_\varphi$ such that $D\models \varphi$ is true if and only if the query $S_\varphi$ matches the database $D$ (with facts stored in relational tables in the obvious way). This means that, in principle, the reduced problem $D\models Q_\Sigma$ can be solved by any RDBMS, without any universal model computation. If you want to learn more about the relation of first-order logic and SQL, you should have a look at the standard textbook of Abiteboul, Hull, Vianu: Foundations of Databases, freely available online $URL$ In terms of complexity the data complexity of answering queries over FO rewritable fragments is indeed $\text{AC}^0$, a very low, highly parallelisable complexity class that is one of the few which is known to be strictly smaller than $\text{P}$. This is what you get when disregarding the size of $\Sigma$ and $Q$ as neglectable. If you consider them, then the resulting combined complexity is usually dominated by the effort of computing the rewriting (typically exponential). Some small remarks are possibly helpful for you to get a better idea of the overall picture: 

The obvious N2Exp problem is of course the word acceptance problem for 2exp time bounded nondeterministic Turing machines. Using this might be as hard/easy as 2exp tiling, because the simulation of such a Turing machine computation in essence also requires you to define a double-exponentially large grid (2exp many configurations of memory tapes of length 2exp each) that is then filled in a non-deterministic way. In practice, showing N2Exp lower bounds often boils down to constructing such a grid (and making sure that it is not a tree or something else of insufficient structure). The "N" (nondeterminism) is often an inherent part of the problem and not so difficult to get once you have a large enough grid (if not, one would maybe shoot for 2exp at first). Another practical N2ExpTime-complete problem is reasoning in expressive description logics (DL). In particular, the DL $\mathcal{SROIQ}$ that is underlying the W3C OWL 2 Web Ontology Language standard is N2ExpTime-complete (Yevgeny Kazakov: RIQ and SROIQ Are Harder than SHOIQ. KR 2008: 274-284). Now this is probably not a problem you want to use in reductions, since the definition of the logic is a bit unwieldy because of its many features. The actual lower bound proof for $\mathcal{SROIQ}$ has also been done by reduction to 2-exp tiling. However, depending on your problem, the construction given for $\mathcal{SROIQ}$ could be inspiring to see how to craft such large grids. The tiling also shows another general pattern: N2Exp really is like NP, you just need to find a way to encode even larger problem instances very efficiently. In principle, you could try to scale up any NP problem. The reason why tiling is nice is that you only need to scale the size of the grid in this case (which is rather uniform). On the other hand, if your problem is possibly only 2ExpTime-complete, then you could get away with an exponentially space-bounded alternating Turing machine simulation. If you have troubles building a 2exp grid, but you can get to exponential sizes, then this might be worth a try.