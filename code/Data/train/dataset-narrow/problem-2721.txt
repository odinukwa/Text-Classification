Yes, this behavior changed from DX9 to DX10+, don't know much about the real story behind it, but I suspect that DX9 had to remap registers between VS and PS at runtime anyway (when linking shaders) which is inefficient. In DX10+, when you compile a VertexShader and a PixelShader, you can see which register will be affected to a particular semantic (see output of fx.exe compiler), and these registers (which will almost map to an hardware register) should match between the output of a stage to the input of the next stage. Note that the input of a stage could have less semantics mapped, as long as the order is the same and there is no gap in the mapping. If you are using some legacy FX file (techniques/passes) with DX10+, the compiler will check input/output signatures for you. But if you are compiling separately VS and PS, you will have to double check any signature mismatch (or you can use D3DCompiler API to verify this yourself). There is one exception for the input of the VertexShader that is coming from the VertexBuffer and the InputLayout. The InputLayout is able to specify the byte offsets of each vertex elements and how it maps to the semantic in the VS shader, so you can have a "sparsed" mapping, the input layout can have more vertex elements/semantic bindings than the VS input, as long as all semantics from the VS are covered by the InputLayout declaration. This is handy when you have a model that has several vertex elements, and want to use the same vertex buffers for several vertex shaders that are expecting different input layouts: you can map the layout of the vertex buffer to the layout of the vertex shaders (as long as the mapping is covering all the input of the VS shader). 

Not exactly: texture arrays are declared in HLSL as for Texture2D and not as an array of texture, so it is quite different. They are almost acting as a 3D texture, where the z is a slice of the 2D Texture (in terms of uv, it is the w dimension). The difference with 3D texture is that they are not supporting trilinear interpolation between the 2D slices but you can still select dynamically a Texture2D slice with a z/w component (unlike an array of texture), the z/w component is rounded to the nearest integer to select the z/w slice. Concerning the memory, I believe this is equivalent, for performance, not sure they give a huge boost even accessing a set of Texture2D compare to an array of texture (but they support dynamic indexing). Using is also easier as you only need to bind it to a single slot. 

ROAM stands for "Real-time Optimally Adapting Meshes." It is a level of detail algorithm for rendering large terrains. It's somewhat complicated so I'll link to some more in depth explanations: Here is the paper: $URL$ The following is a slightly less academic explanation: $URL$ 

Those two noted, I consistently come back to the following article almost every time I write a generic shader: 

I'd say match it to the way you want your game to be perceived. If you want it to come off as a casual game, name them something like casual, normal, tough, hard, etc. If you want the game to come across tougher, name it like gears of war--casual, normal, insane, etc. It's really up to you. Keep in mind that how you name your difficulties will also somewhat affect how the player feels after finishing them. It's much more rewarding to complete "insane" than "hard". 

Where each state on the stack references its sub-items. When a button is clicked, it's corresponding state is pushed onto the stack, and becomes the top-most item which is drawn and interacted with. Usually, you would only draw that top state, unless there is some kind of transparency between layers, in which case you would draw from the bottom of the stack up until you draw the active state. 

It's not necessarily a common program's output at all. It could just be an arbitrary extension chosen by the developer for files that hold data about sprites. You can try opening the file in notepad to see its contents or as suggested above check the first four bytes to see if it is a normal file format saved under a different extensions. 

Most modeling packages allow you to add data to objects (color, texture, etc) so you can use these properties to your own properties. All of the major modeling packages also support you adding arbitrary data to objects so you wouldn't have to stretch yourself trying to finding ways to encode data on your objects. You also get the benefit of editing right on top of your level instead of making little adjustments to marker positions until they are in the right spot. Other than that, there are a number of free or cheap level editors that output to a neutral scene file, like $URL$ Exploiting your modeling software is probably the easiest way other than building your own editor however as you don't have to add another set of tools and file formats to your pipeline. 

Exactly, the actual values in the resulting matrix is just the coordinates of the view direction (), up and right vectors next to each other; 

GL (and most other graphics acceleration libraries) specifies power of 2 size so that sampling operations just need shifts and additions using any other size will replace that with a floating point multiplication which is much slower, what you can do is resample the images when they load so they become a power of 2 size, this may introduce artifacts in the image so it is better to start from a power of 2 size 

Then during the rendering you fill the second VBO with , set the attribute pointer as if they where one for each vertex and set the attribute Divisor to 1; draw with 

Different platforms may result in different scores even with the same skills. For example because you were forced to have less enemies on the Android game due to performance issues. This (along with the control difference) is why console and PC games generally never play together (either multiplayer or through leaderboards). 

but if size*size>255 then you need to upgrade the indicesList to contain a short or int, otherwise they just won't fit. 

Why not just move the blending to the skinning phase? You just need to add the base matrices and the interpolation qualifier to the uniforms (or a static texture for the matrices if you really want to); you have a lot of space for uniforms. Also passing and slerping quaternions is cheaper than passing and slerping matrices. and applying a quaternion rotation to a vec4 is very straightforward. 

can be gotten with . Then will be similarly . is a convenience function that will compute also take into account the quadrant of where the point is and deals with properly and generally goes from to . 

you don't render the same game state 4 times but interpolate the state between 2 physics frames also 15 (visual) FPS just feels laggy you need at least 25ish to feel smooth 

Minor issue: don't perform any GPU interaction in the update method but only in Draw (in fixed time step, this method can be called several times per frame). The correct way to implement the micro-synthetic test is to do it like this: 

The effect is automatically managing to update the constant buffer, upload it if it changes since last apply...etc. Though you can still update the constant buffer directly as you did. 

Instancing requires to change the InputLayout of vertex buffers and pass an instancing buffer along the mesh vertices. Unfortunately, there is nothing automatic to do this with Toolkit models, though possible but would require to dig into the internals to do this yourself. Before even trying to do some instancing with models, you should start with a basic instancing sample with your own raw vertex buffers/index buffer and effect. You will see exactly what needs to be changed and how to setup/use instancing. With this proof of concept working and with the source code of the Toolkit available, you should be able to figure out how to use existing toolkit Model data to turn it into instancing friendly. 

Use Device.UpdateSubresource (requires texture to be declared with Usage.Default) or Map/UnMap (requires texture to be declared Usage.Dynamic).In the case of the swapchain, I guess that only UpdateSubresource will work. Keep in mind that you are refering to the low level Direct3D11 API in SharpDX, so there can't be any high level methods like "SetData". If you want to have XNA equivalent API, you have to use the Toolkit which is available from the 2.5 dev package. If you want to stick with plain Direct3D11, you will have to dig into all the details about how Direct3D11 is working. 

Using this format when declaring a depth stencil buffer, you should be able to copy the depth buffer to another R16_FLOAT/R32_FLOAT...etc. texture. On a side note, it is often not recommended to read back data on the CPU because of the latency that will be introduced. Current techniques - on Windows Direct3D11 - tend to perform typical CPU computation on the GPU with DirectCompute. 

look at the security framework in java, which is the same framework that should prevent java web applets from deleting for the scripting engine you can then decide exactly what the script has access to, do be careful because opening holes can have unintended consequences 

If You subdivide the line into CE and ED and the projection of the 2 lines combined will be the projection of the full line. Using that we split the line where it intersects the screen then EC' will be the resulting line and you ignore the ED part of the line. Current day graphics pipelines will do this automatically using frustum culling with the near plane. 

lets rephrase it a bit you have a point and a direction , find the smallest such that has one of its coordinates on a whole multiple of cell size in the same dimension. then it is easy take p.x and find how much you need to go in the x direction so that we are on a multiple of width then you find how many you need to add take to get there, if d.x is negative then replace with and negate v.x then you do . repeat for each of the axis and take the minimum of the s 

DirectX is a windows API with a corresponding DLL somewhere in your C:\windows directory, similarly with opengl32.dll. Check with dependency walker in you don't believe me. The rest is probably handled through scripts which don't have to be DLLs and just reside in the rest of the assets. 

will result in dangling pointers (with undefined behavior if you dereference it), after you assign the char* to the variable the temp std::string (and backing array you just got a pointer to) gets deleted. 

You don't need to call for the indexes; the indexes are found from the bound during the call (which is saved in the VAO state IIRC). However indexes start from so the contents of your index buffer are flawed they should be: 

In your keyPressed and keyReleased you can use a to map the to Make a new enum with the actions you want to be controllable 

One way to do this is to add "markers" in the scene geometry file itself. You would use a specific naming convention on these markers (which are just pieces of geometry) to represent various things. For example: 

Well as far as C++, C#, and C# with XNA are concerned, Visual Studio has excellent debugging tools--pause the application in real time at any point in the code, see the values assigned to variables at any time, step through function calls, analyze the call stack, and more. In addition, tools like Pix and the CLR Profiler make development with DirectX (Pix) and CLR based languages (CLR Profiler) great. In addition, one especially useful part of working with games is that we have this giant area to write debug text, graphics, etc to. Realtime graphs of memory usage, number of objects/vertices drawn, framerate, etc are common things to draw to the screen. 

I'm sure there are more ways I didn't even think of as well. As for sharing, that's really up to your team and you but as this is an open source project and not something built in a corporate environment it would presumably be split evenly amongst you. 

Unfortunately it's really hard to stop people from cheating, which is why software like Punkbuster was created to try to catch some of them. For an MMO, the biggest thing you can do is to just not trust anything the client sends you. The client should not be able to tell you where the player has moved to, how much damage they have dealt to whom, etc. Instead, you should send control input to the server, validate that it is practical (pressing the "attack" button 1000 times in 1 second, for example, is not valid), then update the simulation on the server. You would then send the relevant parts of the new state of the game back to the client. While you are validating user input, it is also possible to look for trends in their input. If you don't allow macros, you could catch them here by noticing that the user has clicked the same button at the exact same pixel coordinates in perfect 1 second intervals for the last 30 minutes. The client itself is the main place people can cheat, so programs like Punkbuster can cut down on wallhacks, aimbots, etc. Other than that, not trusting the client and validating anything sent to you by the client will help cut down on cheating. 

just generate and keep all blocks within a X radius of the player in the hash table, and then frustum filter for drawing, same way minecraft does it it's cheaper to keep more sectors in memory and then filter than keep just enough and generate 

Start generation early; if a player gets within 3 screen widths of the edge of the generated map start generating more chunks on that edge 1 chunk at a time. Focus generation on the area the player is likely to go. That means follow the walkable terrain and defer sky and underground for later (when they return to the location or linger for a while). Multi-thread the generation. You can do that easily by doing the expensive creation in the other thread and then just paste the data into the level data in the update loop. 

method 1 can be expanded to use arrays to abstract out the number of matrices in the code: (assuming java) 

And then you go over each of the marked cells and update them in the same way, (marking the neighbours of changed cells as you go): 

If you have a method of getting the quaternion of the rotation matrix then just get the lookat matrix and use that method. otherwise you can get the rotation from (0,0,-1) to the look vector, This results in a lookat transformation with an arbitrary up. then find that arbitrary up with and then find the rotation between that and the resulting up with the look vector as the rotation axis. Concatenate the results and that will be the final transformation. 

you are in a voxel engine which means that each box should have a discrete x, y and z coordinate thus: if you have a box at index (x, y, z) then you can get all neighbouring boxes by getting the boxes at index , and edit: after some more info... just detect the collisions for a line between the center of the current cube and the center of the potential neighbouring cube, thus instead of intersecting with the bounding box of the cube you intersect with 3 lines strechting from to and to and to