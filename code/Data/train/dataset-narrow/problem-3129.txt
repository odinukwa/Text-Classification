As mentioned it can be a good idea to repeat CV a few times and average the results to obtain a more reliable estimate If you find many parameter constellations that are within one standard deviation (or in that neighborhood) of the best performing model, it can make sense to choose a model with a slightly worse CV performance, but with a simpler decision boundary (e.g. shallower decision trees, smaller gammas in an RBF-kernel SVM, stronger regularization parameters etc.) - something that was suggested for example here. 

I'm looking for resources that talk about best practices or simply some examples (at specific companies or in general) on how tech companies that heavily use machine learning like Twitter, Facebook etc. set up their data science workflow to make it easy to take models from exploratory to production level. In 2012 Twitter published a paper on what their analytics stack looks like. They heavily rely on Pig. Since this is already a few years old, my quesiton is: Is there a more recent version or something similar to that available for one of the big tech companies using data science? A recent blog entry from Slack describes their architecture, but it says little about how it is used more specifically for machine learning purposes. 

You should only undersample the data that is used for training. Test data should represent the true distribution. Then you have at least two possible baselines. One is ZeroR (your second point), which always predicts the majority class. Given a true distribution of (0) 10% and (1) 90%, this would give you a recall of 100% for (1) and 0% for (0) and 90% accuracy. If rows represent the true labels and the columns the predictions, the confusion matrix looks something like this: $\begin{bmatrix}90 & 0\\10 & 0\end{bmatrix}$ The other would be to flip a coin with probability 90% for class (1) (your first point but without undersampling), which gives you a recall of 90% for (1) and 10% for (0), and 82% accuracy, which is a little more balanced: $\begin{bmatrix}81 & 9\\9 & 1\end{bmatrix}$ If your classes are strongly imbalanced then recall and accuracy have the same problems as you noted above. F1 might be a better choice, but the best metric depends on your application. 

Most machine learning around ICD codes deals with auto-encoding documents or NLP to extract ICD codes automatically from documents. Once you have them, most applications I have seen use them as is. One simple alternative would be to assign the codes to relevant categories to reduce the number of levels and make them more interpretable. For example 

In scikit-learn the feature importance is the decrease in node impurity. The key is that it measures the importance only at a node level. Then, all the nodes are weighted by how many samples reach that node. So, if only a few samples end up in the left node after the first split, this might not mean that J is the most important feature because the gain on the left node might only affect very few samples. If you additionally print out the number of samples in each node you might get a better picture of what is going on. 

If the problem is that you don't know all the categories in advance and different records have different categories, then the hashing trick might help. 

One advantage of is that it can operate on values other than integers (so you don't need the ) and returns a DataFrame with the categories as column names. Also, you can conveniently drop one redundant category using . One advantage of scikit-learn's lies in the scikit-learn API. OHE gives you a transformer which you can apply to your training and test set separately if you specify the total number of categories. This doesn't work with ,for example, if the training set misses categories present in the test set. You can still delete categories by simply deleting columns from the resulting numpy array (e.g. using or to see which columns correspond to the same feature). Some models work regardless, for example tree-based models. Also, L1 regularization can often set redundant features to zero (see Lasso regression). 

For all $2^3=8$ possible labelings we can find a hyperplane that separates them perfectly. However, we cannot find any set of 4 points so that we could classify all $2^4=16$ possible labelings correctly. Instead of a formal proof, I try to present a visual argument: Assume for now, that the 4 points form a figure with 4 sides. Then it is impossible to find a hyperplane that can separate the points correctly if we label the opposite corners with the same label: If they don't form a figure with 4 sides, there are two "boundary cases": The "outer" points must either form a triangle or all form a straight line. In the case of the triangle, it is easy to see that the labeling where the "inner" point (or the point between two corners) is labeled different from the others can't be achieved: 

The idea of random forests is basically to build many decision trees (or other weak learners) that are decorrelated, so that their average is less prone to overfitting (reducing the variance). One way is subsampling of the training set. The reason why subsampling features can further decorrelate trees is, that if there are few dominating features, these features will be selected in many trees even for different subsamples, making the trees in the forest similar (correlated) again. The lower the number of sampled features, the higher the decorrelation effect. On the other hand, the bias of a random forest is the same as the bias of any of the sampled trees (see for example Elements of Statistical Learning), but the randomization of random forests restrict the model, so that the bias is usually higher than a fully-grown (unpruned) tree. You are correct in that you can expect a higher bias if you sample fewer features. So, "feature bagging" really gives you a classical trade-off in bias and variance. 

All labelings can be achieved through the classifier $h$ by setting the parameters $a<b \in R$ such that 

However, research in machine learning is mostly found in journals and papers. It will be hard to find one or a few books that cover everything you want to know. 

As you said, "find informative projections" gives you the best pair of features ("score plots") to explain the target variable. The two best features will be on the x- and y-axis, while your target variable will be the color (hue for numeric, categorical otherwise). Here is an example for the Iris dataset. Where "iris" is the target --> color, and petal length and petal width are the most informative features, followed by petal width and sepal width, and so on. 

How does scikit-learn handle it when there are multiple objects that can have the argument? For example: Are there some combinations that make (no) sense, e.g. set both to ? 

I think in the original paper they suggest using $\log_2(N +1$), but either way the idea is the following: The number of randomly selected features can influence the generalization error in two ways: selecting many features increases the strength of the individual trees whereas reducing the number of features leads to a lower correlation among the trees increasing the strength of the forest as a whole. What's interesting is that the authors of Random Forests (pdf) find an empirical difference between classification and regression: 

This only makes sense if you look at the time series as vectors with same length. In that case, there is no need for a package. Just use, for example, . 

First of all, data only comes in so many forms that it might make sense to stick to a more "concrete definition". Data Science is necessarily practical. But here are a few other books with a more theoretical grounding. Others will certainly know many more... 

You need to pass an array containing the class-labels (or whatever the criterion for stratifying is) as an argument to . In your case, the answer is probably . 

It seems pretty clear from looking at the data when an event starts and ends(basically whenever there is a sequence of positive values). So, instead of starting with some complicated models, I'd suggest calculating a few simple features (like length of the event, total amount of water, amount/seconds, time to previous event, time of day in seconds from start of recording) for every event and then try some clustering algorithm on that new data. k-NN might even produce something meaningful. But a statistical summary of the features can probably already give you a better idea of how to further approach this. EDIT1