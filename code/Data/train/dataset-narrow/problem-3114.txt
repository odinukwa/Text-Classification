That's fine, but changing the momentum-coefficient will require us to also re-adjust learning rate. A cleaner version will be: 

Putting aside things applicable to neural networks such as dropout, l2 regularization, new opitmizers - what are the cool things I should be adding to my Reinforcement Learning algorithm (Q-Learning and SARSA) to make it stronger, in 2018? So far, I know these: 

Ok. Maybe we can get the benefit of training "session A", "session B" etc on separate machines? But then how is this different to simply training with the usual minibatches in parallel? Keep in mind, was mentioned in point 2: things are worsened by sessionA predicting gradients which are vanishing anyway. Question: Please help me understand the benefit of Synthetic Gradient, because the 2 points above don't seem to be beneficial 

I need to debug custom network through overfitting & thus uncovering the inconsistencies Assume we have 1000 fully-connected layers (really exaggerating, to get my point across) in a feed-forward network (bad idea, but fits well for my overfitting task), with 10 neurons in each layer. $layer0 {\rightarrow} layer1 {\rightarrow} layer2 {\rightarrow} .... {\rightarrow} layer1000 {\rightarrow} output$ Because the Rprop algorithm looks only at the signs, the magnitudes of the gradient vectors won't vanish or explode. Also, outputs from $layer0$ are the arguments to the functions represented by a neurons from the layer directly above. With that many layers, adjusting the weights from $layer999$ will have minimal effect. On the other hand, adjusting weights from $layer1$ will have a huuuuge change on the future output (same neurons serves as an argument to many-many future neurons), especially if we use the same value of learning rate for $layer999$ and the $layer1$. I have a feeling that there a formula, to scale down the gradients on deeper layers, generated by Rprop, or something else that must be done? The Rprop is too bold in adjusting these early layers, the algorithm escapes any optimum and becomes chaotic the more layers we have to adjust. If the slower learning rate coefficients are chosen, then the front layers aren't learning fast... 

When using each element in your memory during the Correction session (one element after the other), you need to: 

Having tried the first approach (I still used standard weights on BlockInput but no recurrent weights in that place) seems fine but a little odd - network converges well, but very rarely jumps over local mimima, as if too high learning rate. Personally I like the second approach (recurrent weights on all 4 entry points), which one do I use? 

The LSTM seems to converge fairly quickly on any task 0.95 bothered me, so I manually compared NumericalGradient array against the BackpropGradient array, side by side. All of the gradients match sign, and only differ in size of each entry. For example: 

The question is very simple, yet I can't find a quick confirmation on the web. It might seem obvious - by design, will Dropout always result in stochastic-looking gradient descent? (SGD) I've built a system which converges nicely with momentum at 0 and learning rate 0.01, even with 100 layers stacked. With dropout, the error decreases, but jumps up and down due to dropout knocking-out certain neurons? Is it usual to see fluctuations of error during backprop with dropout, similar to SGD? would that imply I can train on huge batches instead of minibatches? 

SG speed up the training speed, by allowing multiple forward passes (with immediate weight adjustments), since DNI will already predict the future gradient. However, we will loose time "experiencing" a few hundreds of training sessions only to find a optimal structure of DNI with which it will predict gradient the best way. By that time we could have already finished our training with an oldschool Backprop through Time. 

As you can see, I got s[3] at the start and s[4] at the end. ...But we've just agreed s will only have 3 entries {-1, 0, 1} and this means out of bounds error. And, I can't really understand how what we've done would "shift" the entire $w^g$ into $w_{new}$ once we do it for all 5 entries (we've done the above just for 3rd entry). Can someone give me the intuition for it as well? 

However, in practice, it will be fine after approximately 10 timesteps - and so there is no real need for the bias correction in Momentum, but it should be done if we are using combination of Adam (a combination of Momentum & RMSProp) 

Why in LSTM we calculate gradient w.r.t weights, but not w.r.t the cell state? Is it theoretically possible to correct the contents of the cell state, and what would it result in? I understand that weights are like a "set of skills", so that network can respond correctly to the input, even gazillions of iterations later. The cell-state is an understanding of what's going on in the past, up to the start of the current minibatch. So why not to correct the value stored in the cell state? It would be very useful if we carry the cell state forward, between minibatches. $URL$ 

It would still be a thrill to find out how the math formula wasn't going to reach out of bounds, it clearly says $s_t(i-j)$ which would go out of bounds. 

Question 1: I realize 7 is possible, because we have a very good understanding of how network will generalize with the help of step 6. - Is this assumption correct? Question 2: Is reverting back to the initial $W_0$ a necessity, else we would overfit? (revert like we do in step 4. and 7.) Question 3, most important: Assume we've made it to step 7, and will train the model using ENTIRE data. By now, we don't intend to validate it after we will finish. In that case how do we know when to stop training the model during step 7? Sure, we can train with same number of epochs as we did during Cross validation. But then how can we be sure that Cross Validation was trained with an appropriate number of epochs in the first place? Please notice - during steps 3, 4, 5 we only had $K$'th chunk to evaluate Training vs Validation loss. $K$'th chunk is very small, so during the actual Cross-Validation it was unclear when to Early-Stop... To make things worse, it will be even more difficult in case of Leave-One-Out (also know as All-But-One), where K is simply made from a single training example 

As stated in the last edit of my question, the issue indeed was to do with the softmax function. As clarified here We shouldn't apply softmax directly to the result of the last LSTM. Notice, LSTM will produce a vector of values, each of which is bounded between -1 and 1 (due to the tanh squashing function that's applied to the Cell). Instead, I've created a traditional fully-connected layer (just additional weight matrix), and feed result of LSTM to that layer. This "output" layer isn't activated - it feeds into a softmax function, which actually serves as an activation instead. I modified the back-prop algorithm to supply the gradient generated by the softmax to the Output layer. Of course, if you used a cross entropy Cost function originally, then such a gradient will remain $(predicted - expected)$. It's then pushed through the weights of that Output Layer, to get the gradient w.r.t. LSTM. After this the backprop is applied as usual and the network finally converges. Edit: There is slight improvement to momentum. 

run a forward pass, and get the error Apply the scaled previous momentum, () to the weights run a backward pass using the error of each pass. set , but don't apply it to the weights yet repeat forward pass, from 1. 

Minibatch is a collection of examples that are fed into the network, (example after example), and back-prop is done after every single example. We then take average of these gradients and update our weights. This completes processing 1 minibatch. Because of that, we can run several minibatches in parallel, granted we have enough memory for temporary storage. I read these posts 

Specifically, in deep networks (with 8+ layers) this effect starts to become apparent, as the earliest layers are adjusted. This has a massive effect on the outcome, and the error jumps around. The noise also happens in a network with just 2-3 layers. However, it is only apparent when the error reached 0.00, and the iRprop+ keeps running. In some cases it will suddenly cause a very abrupt change, and the cross-entropy cost function will produce error larger than 1 000 000 I've built a custom LSTM in c++ and experiencing this noise during debugging of overfitting Perhaps the learning rates from each layer should be initialized with a different magnitude, depending on the layer's depth? [link] 

For any layer in my neural net, should I apply dropout onto an entering vector, or on the pre-activated vector? In other words: $$\vec q=W\cdot \vec x$$ $$\vec h = activate(drop(\vec q))$$ or: $$\vec q=W\cdot (drop(\vec x)) $$ $$ \vec h = activate(\vec q)$$ I think the second variant is smoother (none of our current vector is directly dropped out) and is therefore better. 

Edit: Somewhat solved, - I simply forgot to turn-off "average gradients by number of timesteps" during backprop. That's why my gradients were always smaller in the "backpropped" column. I am now getting descrepancy of 0.00025 for LSTM Edit: setting epsilon to 0.02 (lol) seems like a sweet-spot, as it results in descrepancy of 6.5e-05. Anything larger or smaller makes it deviate from 6.5e-05, so it seems like a numerical issue ...Only 2 layers deep though, weird af Someone had this precision before? 

Set N=0 perturb the weight upwards foward prop at a particular timestep N perturb the weight downwards forward prop at a particular timestep N get single delta, store it away increment N until N not equal 4 return to step 2) sum the 4 deltas to get a total change in Cost 

As you can see, the signs do indeed match, and the numerical gradient is always larger than the back propped gradient Would you say it is acceptable and I can blame float precision? 

Do I miss a part of the puzzle, or it really has to be done like this? $$$$ my understanding of Batch Norm: Batch norm (link to original paper) makes the search space more even & simpler to navigate during forward & backward passes. It is usually applied to the sum $Z$ going into activation function (although there is a lot of debate (1) (2)). This is done for every neuron in a layer. in usual settings, the neuron's output on layer $l$ is $$H_{i} = \phi(W\cdot X +b)$$ where W is a vector of weights leading into our $i$'th unit, $X$ is a vector of outputs from previous layer $l_{-1}$ and $b$ is a bias With Batch norm, we still have our usual non-activated sum for neuron $i$: $$Z_i = W\cdot X$$ but also have our mean for that neuron across all minibatches (Just for that neuron!) $$\mu_i = \frac{1}{m} \sum_{p}^m Z_{ip} $$ Similar, for our Variance as: $$\sigma^2_i = \frac{1}{m}\sum_{p}^m (Z_{ip} - \mu_i)^2$$ Our non-activated normalized sum for neuron $i$ (here, $\epsilon$ is a tiny number, in case if variance is too small, to avoid division by zero): $$Z_{norm} = \frac{Z_i - \mu_i}{\sqrt{\sigma^2_i + \epsilon}}$$ We give a network a chance to learn the actual distribution it needs, so it's different from default distrib. We introduce 2 learnable parameters, $\gamma$ and $\beta$, and network can use it to 'squash and shift' the distributions as it needs (even canceling-out the batch-norm if required). $$Z_{adjusted} = {\gamma}Z_{norm} + \beta $$ Finally neuron's output is computed as: $$H_i = \phi (Z_{adjusted}) $$ Bias isn't included, because we already have $\beta$ for the whole layer $l$.