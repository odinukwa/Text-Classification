Well as a simple hack on your current technique, you could choose between two up-vectors based on whether your view vector is tending towards parallel with one. It'd just be an abs, compare, and select in shader terms, so not much overhead - especially considering the sampling and blending... 

My generally feeling is always that learning to use something in the wrong context is not a valuable exercise. 

You don't need collision detection, but you will need to give your enemies a bit of intelligence, and have them avoid running in to each other. Collision detection without that will just make your enemies look stupid anyway - people avoid each other, they don't generally collide. Look up "flocking" for some simple behaviours. The basic idea is that things should head towards the player, but away from a close obstacle (each other, walls, that kind of thing). 

It's easier to model because there's separation between the limbs and body, and the bits that need to stretch when animated are already stretched - so they get modelled and weighted correctly. Same reason you'd model a hand as open and fingers spread, rather than modelling it as a fist. Oh, and as far as programmers are concerned, we don't care. It'll still look like something from "The Thing" when we get the skinning code wrong at the first attempt. 

Difficult to tell from the video exactly what you mean, but the simplest method of doing a sprite-based motion blur would just be to render the sprite several times, with some form of translucency. You could buffer the previous positions for use as the trail, or you could use a small time-step in the animation to render sub-frames. From the video in question I'd suggest a basic additive blend would work, which means you don't need alpha either in the texture or even in the vertex - just darken the sprite for the "trail". Over-writing the same pixels makes them brighter, for the glow. If you have even a very dark glow around the object in the original image, you'll get a glowing halo if you additively blend a bunch together. Alternatively you could use a basic HDR-like hack as a post-process. Take the image without any glow, threshold it so you only have very bright pixels, blur it a lot, and add it back in to the original. TBH I don't think that's necessary here. 

which is pretty neat because exp(...) has infinite continuous derivatives - exactly what we are looking for (and probably why you settled on that scheme in the first place!) So that's pretty awesome! But you can go deeper and deeper. Note that this is the case for the focus position P being constant - what happens when the focus changes/is moving? Is that a function of analog stick position? A binary function of a key being pressed? A cut in a cutscene? And so on. By unrolling the complexity of what's driving your motion and understanding what is actually causing what to happen, you can add and adjust constants and factors and functions, normally in the service of 'smoothness' (but sometimes for other reasons like 'keep the player on screen' or 'avoid ringing' or 'avoid framerate dependance') to achieve enjoyable results. 

?? are typically rotation matricies or unit quaternions, Compose is matrix or quaternion multiplication, and F and invF the associated conversion functions. As an example in quaternions: 

If you are sure your has a uniform scale and no skew components, then the non-translation part of the matrix can be expressed as M_33 = R * (s * I), where R is the an orthogonal rotation matrix, and s is the uniform scale. This is vaguely annoying so solve, but in 3d comes out to be: 

This form also assures the transform has the appropriate form and epsilon-ish errors don't sneak in, while maintaining composability. 

or something else that suits your use case better. For the best results, a good grasp of the underlying calculus (of both the curves as well as the externals causing the end point changes) can go a long way to crafting a formula to produce the desired effect. 

The problem you are talking about is often called 'broadphase collision detection' and your solution is a 'swept volume', not really a hack, just how it's done (simply take tha AABB of the object including both start and end of the motion and use that for collision - though things get a little tricky with rotations). Then the go to broadphase collision detection algorithm that makes this fast is called 'Sweep and Prune.' 

The result will be that the object's vertices will first be scaled (along the local XYZ axes), then rotated (around the local origin), and then translated into "world" space. Then the world-space co-ordinates will be translated such that the camera is at the origin, and finally everything will be rotated around to the correct view direction. This is probably what you want for a basic scene. As Richard Fabian says, you generally want to consider the camera transforms as an inverse, though you might equally replace the camera transforms with a lookAt() function or similar, to directly construct an appropriate view matrix. If you push the matrix stack after setting up the camera, you can pop/push for each object without having to set the camera up again. 

You could search in both directions - from the current tile to the nearest neighbour, but also looking to see if another tile has the current one as nearest in the opposite direction. Move to the closest of those two if different. 

I find it useful to build profiling in. Even if you're not actively optimising it's good to have an idea on what is limiting your performance at any given time. Many games have some kind of overlayable HUD which displays a simple graphical chart (usually just a coloured bar) showing how long various parts of the game loop are taking each frame. It would be a bad idea to leave performance analysis and optimisation to too late a late stage. If you've already built the game and you're 200% over your CPU budget and you can't find that through optimisation, you're screwed. You need to know what the budgets are for graphics, physics, etc., as you write. You can't do that if you have no idea what your performance is going to be, and you can't guess at that without know both what your performance is, and how much slack there might be. So build in some performance stats from day one. As to when to tackle stuff - again, probably best not to leave it too late, lest you have to refactor half your engine. On the other hand, don't get too wrapped up in optimising stuff to squeeze out every cycle if you think you might change the algorithm entirely tomorrow, or if you haven't put real game data through it. Pick off the low hanging fruit as you go along, tackle the big stuff periodically, and you should be fine. 

While this question is a little abstract, I can share a few tips/realizations I've had over the years. First, 'smooth' normally means 'more continuous derivatives.' If you are moving at speed X, you are all good. your position is v*t (continuous), the derivative of that is v (continuous), and all derivatives of that are 0 (continuous). However, if you suddenly stop at time T, your position is v*t -> v*T (continuous), but your speed is X -> 0 (discontinuous at T). So maybe to solve this you 'slow down' when close to T rather than coming to an immediate stop. Some other side notes - the 'farther away' the motion is from the player, the more tolerant they'll be of these discontinuities. Closest: camera, next: avatar, farther: enemies/ais. A super rough guideline - the camera likes to have inf continuous derivatives, the avatar can survive w/ continuous movement and velocity (but then have instant acceleration, but continuous acceleration is better - that's why analog controller input 'feels' better than digital or keypresses), enemies can be a lot crazier. Ok, now let's analyze your situation. 

Hard to tell from just the code, but a common issue with quaternion interpolation is that 2 (antipodal) quaternions represent the same rotation. If matrix -> quaternion ever returns the 'wrong' one in one of the two cases, your lerp is going to put you at 0 which will lead to all sorts of nonsense. Also your scale calc looks bogus as Sam mentioned ;) 

So, let's analyze where the discontinuities could be. The first obvious point is d = D, where we switch from moving to not moving. What cases would this be continuous and what cases would this be discontinuous? So on one side pos is constant and the velocity == 0, so on the otherside we need our velocity to approach 0 as d -> D. Ok, what's the simplest function that does that? What about f(d) = 0, hah! Of course this works, but that just means the camera never moves! Ok, next simplest function f(d) = k. Now we get there, but definately have a discontinuity at D == d. Ok, next simplest: f(d) = k * d. That has some nice features that we'll get to in a sec but immediately we notice that k*d is non-zero for any D != 0. But we can fix that by subtracting out a D: f(d) = k * (d - D). Now f(d) = 0 when d == D. Awesome, now we are continuous across d, what about when d > D? Then we are strictly in the case 

This aliasing effect is definitely apparent in datasets like the one above (a raster plot of a signal) Here is a second example that shows it a lot more. 

Vertices It would appear that the vertices going into the vertex shader are correct. For example, in reference to the first image the data looks like this in the VBO: 

With that said, all I am left to believe is that I am munging up my actual projection. So, I am looking for any insight into maintaining the 1:1 pixel-to-world-unit projection. 

My suspicion is that this is an artifact of the minfilter when there are many texels per pixel. What the heck is going on here!? 

Issue For sufficiently large geometries (often 10-100x larger than the viewport) this causes some annoying artifacts that cause the texture to "shimmer" effectively changing the visual representation of the data. Examples Here is an example where the camera is panning down the Y-axis (down) in very small values relative to the viewport dimensions and you can see the texture actually appear to change: 

Problem In the screenshot below (1:1 scaling) the grid spacing is 64x64 and I am drawing the unit at (64, 64), however the unit draws roughly ~10px in the wrong position. I've tried uniform window dimensions to prevent any distortion on the pixel size, but now I am a bit lost in the proper way in providing a 1:1 pixel-to-world-unit projection. Anyhow, here are some quick images to aide in the problem. 

When this seemed off place, I went about and did the base case of 1 unit. Which seemed to line up as expected. The yellow shows a 1px difference in the movement. 

I am looking for some insight into a small problem with unit translations on a grid. Update and Solved I solved my own issue. See below for details. Everything in this part of the post turned out to be correct. If anything it can act as a miniature tutorial / example / help for the next person. Setup 

What I Want Ideally moving in any direction 64-units would output the following (super-imposed units): 

would move 1-unit with the aspect ratio factored into the equation. So: . Movement did not fit within the grid. Forced a 512x512 window with a matching orthogonal projection. Tried various magic numbers and tried to draw correlations between the two. 

Vector3 vT = v2 + headingNorm * 3; Be careful though, if v2 and v1 happen to be closer than 3 units away this will put you on the far side of v1. Maybe you want this to make the unit step back to make room for the attack. But then again be careful, because that means as you approach that attack point you will overshoot then correct and overshoot the otherway over and over again! 

interesting, we have a first order differential equation, which i'll skip the hairy details but that means the solution is in the form 

For transforms for game objects (that tend to have requirements like 'uniform scale'), it's often easier to store object transforms in their components, and compute the full matrix when necessary, as opposed to storing the full matrix and then having to extract the components. 

This is actually why people bother to use quaternions (or matricies), instead of axis angle, because they compose well. Notes: You could go directly from lat,long to quaternions rather than needlessly converting to AxisAngle rep, I did so in the example just to build off of what you already had done, and show that the piece that is missing is 'how do i compose rotations' - which you alluded to in your question. (2) I might have the order wrong argument in Compose(). 

Here are random-but-not-completely random techniques I use. 1) Shuffle bag. Put all valid outcomes in a bag, choose one randomly and remove it from the bag. Repeat until the bag is empty then refill the bag. To add back in randomness, refill the bag when it only has X items remaining. 2) Progressive percentages. First roll X% chance, if fails second roll is X+Y%, then X+2Y% etc. Reset to X% on success. Both the start value, and progression can be adjusted (even non-linear). 3) Internal cooldowns. Roll X& chance, if success, do not roll for next N seconds or M attempts or whatever. Can be combined with other methods. To add in more randomness have multiple event checks with different coodlowns, ie make 2 rolls at 25% with 3 and 7 second internal cooldowns as opposed to 1 roll with 50% with a 5s icd (not exactly the same probability, but you can calculate and match if important). 4) Pre-rolled spacing. Rather than checking vs a percent each event, simply choose when the events happen. For example, "this boss crits every 1d4+4 attacks." Works well when the want something to happen relatively consistently, and somewhat random, but you don't want back to back events. In someways a special case of methods (2) plus (3). Again for more randomness you can have 2 or more overlapping sequences.