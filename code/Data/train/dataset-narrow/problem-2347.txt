My question is whether we know explicit examples, at least for $B_r(f)$ with bounded $r$. Of course, we have several lower bounds for monotone circuits, even for unbounded $r$. But do any of them works also for matroid functions? Note that for $r=1$, the hands of circuits are extremely tied: boolean functions computed at inputs of an And gate cannot then share a common variable. So, at least for $r=1$, we don't need the entire power of Razborov's method of approximations:lower bounds for read-once monotone circuits should come much easier. 

Usually, in applications, only large thresholds do work: we usually need thresholds of the form $2^{n^{\epsilon}}$ for $\epsilon > 0$. Say, if $F:\{0,1\}^n\to \mathbb{N}$ counts the number of $s$-$t$ paths in graph specified by the $0$-$1$ input, then for $t=m^{m^2}$ with $m\approx n^{1/3}$, the threshold-$t$ version of $F$ solves the existence of a Hamiltonian $s$-$t$ path problem on $m$-vertex graphs (see, e.g. here). 

One "obvious" obstacle is finite vs. infinite domain issue: boolean circuits work over finite domains, whereas Turing machines work over entire set $\{0,1\}^*$ of $0$-$1$ strings of any length. So, to derandomize probabilistic boolean circuits, it is enough to take the majority of independent copies of a probabilistic circuit, and to apply Chernoff's inequality, together with the union bound. Of course, over infinite domains, this simple majority rule won't work. 

Remark 1: If the graph $G=(L\cup R,E)$ is bipartite, then the vertex-edge incidence matrix of the inequalities $x_u+x_v\leq 1$ for all $(u,v)\not\in E$ is totally unimodular, and one can solve the clique problem on induced subgraphs of $G$ via linear programming. Thus, for bipartite graphs $G$, $CLIQUE(G,k)$ has a small (albeit non-monotone) circuit. 

is a closed term on the shape . is on the shape , where is the only variable. and have no lambdas within applications. 

An answer to the traveling salesman (and similar) problems can be easily verified on light lambda-calculi. Also, if I understand correctly, the light lambda-calculi can compute every polinomial-time computable function. That way, if one can prove that the traveling salesman problem can't be encoded on the light lambda-calculi, that would also prove the problem can't be solved in poly-time, which would also prove P!=NP. Is that correct, or am I confusing some concepts? 

It is known that interaction combinators can implement any interaction net system efficiently. Now, let us define a modification of interaction combinators, which, instead of two types of fan cells, has an infinite number of fan-cells, labelled with unique integers. That system works exactly like the previous: two nodes with identical labels annihilate; two nodes with different labels duplicate. In other words, interaction combinators are a subset of that system, with only 2 possible ids. How can you compile that new system to the former, without losing efficiency? 

The church-encoding for natural numbers is a natural mean of implementing addition, multiplication and so on on the lambda calculus. Interaction nets are said to be an alternative universal computation system, yet, nothing is published as to how one could encode simple data structures and algorithms on the interaction nets directly. What are the natural encodings of numbers and numeric algorithms on interaction combinators? 

Let and be lambda terms in the normal form, such that is intensionally different from - that is, their string representation using bruijn indexes isn't the same. Is there any choice of and such that, for any that is also a lambda term, ? 

What, say, about DP algorithms that only use $\min,+$ or $\max,+$ operations in their recursion equations? Actually, I am not aware of any proof of BPP = P in any "non-pathological", at least "somewhat interesting" but uniform model of computation (whatever these "non-pathological" and "somewhat interesting" should mean). 

We know (for now about 40 years, thank Adleman, Bennet and Gill) that the inclusion BPP $\subseteq$ P/poly, and an even stronger BPP/poly $\subseteq$ P/poly hold. The "/poly" means that we work non-uniformly (a separate circuit for each input length $n$), while P without this "/poly" means we have one Turing machine for all possible input lengths $n$, even longer than, say, $n$ = the number of seconds to the next "Big Bang". 

Note [added 27.11.2017] My question actually is: can we capture THE barrier for derandomization in the uniform setting? After Adleman's theorem, one serious "barrier" for extending it to circuit (or decision trees) working over infinite domains $D$ (like $\mathbb{R}^n$, instead of $\{0,1\}^n$) seemed to lie in the infinity of the domain. Adleman's theorem simulates probabilistic circuits by majority vote of about $\log|D|$ deterministic circuits (Chernoff bounds then suffice). But this (infiniteness of $D$) turned out to be no barrier: it is then enough to replace $\log|D|$ by the Vapnik-Chervonenkis dimension of deterministic circuits. 

The "uniform convergence in probability" results from statistical learning theory yield the tool for derandomization. 

I know explicit polynomials $f$ (even multilinear) showing that the circuit-size gap "computes/counts" can be exponential. My question concerns the gap "counts/decides". 

The following lemma is due to Pudlák and Rödl; see Proposition 10.1 in this paper or Lemma 2.5 in this book for a direct construction. 

When applied to a church number evaluates to normal form quickly in several existing evaluators, including naive ones. Yet, if you encode that term to interaction nets and evaluate it using Lamping's Abstract Algorithm, it takes an exponential numbers of beta-reductions in relation to . On Optlam, specifically: 

A class of lambda terms can be evaluated using Lamping's abstract algorithm - that is, converting them to interaction nets and applying a set of rules. In order to get the result, you have to read back lambda terms from normalized interaction nets. For example, this net: 

Consider the problem of higher order unification - that is, finding a substitution for the equation , where and are open terms on the lambda calculus, such that and have the same alpha-beta normal form. That problem is undecidable in general. Now, consider the case where: 

Now, suppose we take the elementary affine logic and extend it with dependent types and . It is easy to prove that language is consistent and normalizing as a consequence of EAL. It can, I believe, express all the type families from Coq, with lambda-encodings and since it has . A simple expression of recursive algorithms is possible with church-encodings for iteration and scott-encodings for matching. And it is obviously simple, as it only has , and . Moreover, such language would have the very comforting property that it can be reduced optimally using the abstract fragment of Lamping's algorithm. All things considered, such language looks like a perfect candidate for the role of a small functional core that serves as an universal code-interchange format, as proposed by Gabriel. Is my suspicion correct, or is there any problem with this reasoning?