So after a lot of testing and diagnosing I've found the solution, but I still don't understand why. If anyone can explain this to me I i'll award the answer. The problem is was with the /boot directory. As it's installing under UEFI, the ESP gets setup under /boot/efi, in Grub this worked fine, i could read this ok. However, some of the /boot directory, which contains the required kernel was not readable. When i attempted to manually load the kernel and boot from grub, the kernel loaded fine, but when attempting to load the initrd, i received error: error: attempt to read or write outside of disk hd0 From what I understand this is because /boot is on the main 4.5TB partition, the files can end up anywhere on the drive, and in this case, and my many test cases before this, the files in /boot are too far up the drive for Grub to read. Creating a dedicated /boot partition before the ESP partition has resolved this. This is the same issue as documented here: (according to $URL$ What I don't understand however, is that from my understanding, under UEFI the full 4.5TB should be readable. Ubuntu should boot fine under it's default partition layout with only and ESP partition? This is confirmed as I've managed to install Ubuntu without a /boot partition on 3 other identical hardware and bios settings servers. It's just this one server which couldn't read inside some of /boot.a I've ensured the disk was booting under UEFI in the BIOS. 

This sounds like you potentially have issues with both your DNS and bindings. For the first issue, not being able to hit the site by IP, this is likely because you've only bound the site to a host name, you don't have a default binding for the IP address. You'll need to add an additional binding for the site that includes the IP address but not a host name, and stop any other websites that have conflicting bindings (if they exist and are running) and you should then be able to hit the website by IP. Not being able to hit the site by domain name from the web server most likely means that it can't resolve the DNS for that domain name. Try an nslookup from the command line to see if this is indeed the case. 

You have the right feature, it's just not terribly clear how to use it to do what what you need. You can indeed use the IP Address and Domain Restrictions feature to restrict access to a single folder, there just isn't any UI that make it obvious that's what you're doing. First, you highlight the folder that you want to restrict access to (admin, in this case) in the IIS Manager, and then any rules you add using the IP Address and Domain Restrictions feature will apply only to that folder. So, once you've highlighted the folder, set the feature default to deny all, add an entry to allow your subnet, and you'll be set. 

Symptoms When high disk write access occurs (when a vm is being spawned for instance), the OS and subsequently virtual machines experience high i/o wait, to the point of becoming unresponsive, the OS becomes very laggy. Normal i/o is about 10MB/sec read or write (according to iotop). When simulating using dd: 

We're in the process of migrating Windows Server virtual machines between cloud infrastructure. In certain circumstances (HyperV Gen2 UEFI) servers, the system will fail to boot unless we install the storage driver ready for post migration boot up. While this works when doing it offline using (eg the disk attached to another server and injecting the driver) : 

Also, should i look to make the conntrack settings on the DNS server more aggressive to close connections faster, these are the current settings on the DNS server: 

I'm looking at moving our XenServer cluster (150 VMs) to OpenStack on KVM. After extensive reading it looks like virt-v2v will do this. However I'm confused about it's usage. I was going to copy the VHD file and then run virt-v2v on this, then import into Glance, and start an instance. However it appears this isn't the process virt-v2v uses. Could someone explain the overall process, how to use virt-v2v or any other tool(s) that I will need to convert VMs from XenServer to KVM and import into OpenStack. The two 'clouds' are separate hardware, over the internet - so i would like to avoid shared storage between them if that's possible, however if it makes it too complex we can sling up a VPN between. 

I hate this kind of software, but webmin does what you ask :-) Edit : I didn't find any module in webmin for tomcat 

All computers have a proxy.pac file that indicates which proxy to use or whether to connect directly. Computers have access to just a local DNS (no name resolution for google.com for example.) By the way ... The company does not respect the RFC1918 internally and uses public addresses! (historical reason). The use of internet proxy explicitly makes it possible to not to have problem. What if we would migrate to IPv6? 

is a great tool if you need to perform simple tests. If your site is static and contains only one html document, then you can rely on . But I do not think this is the case. If your site is dynamic, it contains files css, javascript, images, ajax, headers cache, etc. ... So does not correctly simulate users and your benchmark is not relevant. How to do a relevant benchmark? You can use Jmeter for example. Jmeter offers a proxy that can record the traffic on your browser. You can then play it X times to simulate a nomber of users. There are of course also many commercial solutions (appliances, cloud, etc.) 

The above triggered the mentioned symptoms almost immediately. RAID shows healthy, however I'm unsure if I have the optimum caching configuration enabled: 

Ubuntu default guided partition layout, non LVM, completely unmodified. Confirmation of partitions, including ESP is present. 

What's next? Updates: Further reading shows that this problem would be simple if the drives were not attached to a raid controller (all be it in passthrough mode), as then smartctl would provide the information required. Using mfiutil: 

To streamline the process we would like to install the driver while the machine is online. Unfortunately dism is requiring the disk to be offline, returning the error "This command can only be used with an offline image". We have tried pnputil to install the driver but this does not work, we believe this is because pnputil is for plugged in hardware with no driver currently. Is there a way to install a driver into an online image? 

Attempting to install Ubuntu 16.04 Server onto a newly initialised disk. Using UEFI, and a 4.5TB / partition. After installing without issue, on rebooting, the server will not get past Grub. Hardware: 

In the end i couldn't find a way to map the mfiutil to a device. I'm sure there is a way but it escapes me. In the end i rebooted into the raid controller bios and luckily as the drive was completely dead it was showing in the controller. I think if i had studied megacli i could have realised this without rebooting, which would have been better. But in the end mfisyspd5 actually mapped to E1:S9, serial 1EJ49HWH But overall, if you want to run ZFS, don't use a RAID controller even in passthrough, just get a HBA. Will save you hassle in the long run. 

Based on the comments, it doesn't look like you have a particular reason to run this application below the root, and you don't have anything else running at the root of the website, so the correct solution would be to point the root of the website at the /MyApp/ folder, and call it job done. 

Are you really hitting your IIS site? There's not a lot of information about your configuration here, but what appears to be happening is that the router that is port forwarding is expecting to find the site at 192.168.0.2, and now that you've un-bound your site from that IP, it's likely finding the default site at that address, and that is what is returning the 403s. You should be able to verify this by checking the logs, which should show that some site other than the web site you're trying to configure is taking the requests and responding to them. As to the correct configuration, it might behoove you to seek the advice of the CuteFTP people, and ask them for their instructions for implementation in a NAT environment. I'm sure it's a configuration they're familiar with, as it would be required for any load-balanced site. 

You can create two virtual host (and it is possible to use the same port) in combination with mod_rewrite. 

Step 2 : IPv6 AND IPv4 in internal network And why not full IPv6 network directly? Because there is always the old servers that are not compatible IPv6 .. Option 1 : Same architecture as in IPv4 with a proxy pac This is probably the easiest solution. But is this the best? I think the transition to IPv6 is an opportunity not to bother with this proxy pac! Option 2 : New architecture with transparent proxy, whithout proxypac, recursive DNS Oh yes! In this new architecture, we have: 

You try tu use an old (an inexistant !) repository (etch) with the release Wheezy. Clean your . Then, try again . If you yant to install libapache-mod-security, just try : 

Step 1 : IPv6 internet access Internet access in IPv6 is easy. Indeed, just connect the proxy in Internet IPv4 and IPv6. There is nothing to do in internal network : 

is a platform-independent shared data directory (and hadoop is in java...) and is mostly host specific system and application configuration files. Then, to configure hadoop, just use the following command and answer yes, yes, yes... !