Please read about the security implications this has: $URL$ In short, don't use ownership chaining when you can't trust the db_owner of the source database. For a good in-detail explanation including examples see $URL$ 

You didn't do anything wrong. Unfortunately, referencing calculated columns in the field list is just not possible. The usual workaround is to use self joins, joins to subselects or anything like that. But that would only qualify for more complex calculations. In your case, the solution would be to just write it the way Ryan mentioned, even if that turns the stomach of every programmer. 

I am currently building a databse scheme for a small company. But at the point doing it I figured out that my attept will be unacceptable due to the ability that one action will affect multiple unwated records. To explain this. Currently I have one table addresses that links towards person, company, order etc. The issue comes when we have a person that is linked to address ID 1, and a company address is linked to ID 1. And now when we want to change the company address - just one field like the building number from 5 to 6. The change will affect both the company and the person address - most likely all other attached to ID 1. How should it be solved proffesionally - as the pro's build such scheme. My attept would be multiple address tables person_address, company_address etc. All would have the same fields which is not a optimal attempt I assume. Thanks for any hints. 

There're such cases. In Domain Driven Design we have things called "Value Objects" which don't need an ID. These are objects with naturally posess no identity at all and thus you needn't store one for them. It's also considered a best practice to keep them immutable. But before you create tables which store the same value object multiple times think about normalization. Often times it's better to have a key (and be it surrogate) and just store the amount of items you have. Think about your design very well before you decide to leave out a PK. So just because you don't necessarily need them does not always mean you should not have them. PKs make life oh so much easier. 

I recently came up with this and I am not sure if it is even possible to do in Mysql. The idea is to create a forecast/prediction of expenses based on last 3 months also adding some conditions for the values. Currently I created two working queries, but I am unable to transform my vision into the query for the prediction table. The questions are : Is it possible to achieve this? And is this possible to make it one query. Any hints will be appreciated. I am willing to use this in my PHP script, but I want to reduce the query amount and preferably have it later translated into a SP. The query logic. 

Let me help you to understand how a "standard" index works. Most databases indexes are just B-Trees (not to confuse with a binary tree). Simply speaking, when you query an indexed column, a binary search will be performed. A binary search performs generally in O(log(n)) and thus you can find individual rows quite fast, even if there are many of them. The database uses B-Trees instead of loading and sorting the table as the indexes don't require as much memory and less disk reads are required. Now imagine that you try to binary search for a value, but you don't know the exact beginning or ending of the value. A binary search is basically not possible and you have to traverse nearly the whole tree to find every possibility. Of course, there're cleverer techniques than this. MySQL is not that dumb, it uses a Boyer-Moore algorithm for this problem, but that doesn't mean you don't suffer a performance impact. A Fulltext search index will of course help. It uses entirely different datastructures (Tries, Suffix-Trees). Reading the manual of MySQL, I also get the impression that fulltext searching is quite easy with it. However, on most systems fulltext searches require some maintenance/housekeeping by the admin to keep up good performance. Often times fulltext indexes keep "mapping/index" tables for the tokens of the indexed texts. These tend to fragment which can impact query response time, as the index grows unecessarily larger. So from time to time, they should be defragmented and optimized. You might want to look into that. 

EDIT as op stated that Enterprise is not an option: When using Standard Edition, you can fall back to using Partitioned Views. The main difference from the usage side is that you have to maintain the partitioned tables for yourself and you also have to make sure that every new member tables gets added into the view. The idea builds on your scenario 2 but you don't have to care about where to insert the data, as this is determined by the Partitioned View based on CHECK-Constraints in the member tables. For a good example, see 2 As for all partitioning scenarios you have to make sure that the partitioning columns you choose fit your query-and-insert behaviour. 

Desired result I would like to merge thous queries into one, My final query seems to be ok but there is a thing missing, How can I get all the where clauses in the final version? Perhaps the Join's should be a subquery? or is there a smoother solution 

I am struggeling with the following issue. I have a table executives and supervisor, quite similar in the structure. Both of the executives can have a Car one or more. Should I make a table executive_has_cars or executive_cars + supervisor_cars, or a table cars with fields ID,Person_ID,Type{1/2 - executive/supervisor}. Or is there an other possible option. I need it to be scalable and fast, so would be good to have the best solution on the schema. And where should I put the indexes. Thanks for tips 

I have written a query utilizing Oracle's with clause. I have identified one particular with clause in a larger query to be really, really slow. Here are three versions of the same query, they all have the same execution plan! Version 1 (takes 30 seconds): 

Here's the difference between the two statements: The first statement calls the levenshtein function only on the retrieved resultset. The second statement calls the levenshtein and the length function as a so called "predicate" for every row in the table to retrieve the resultset. Even if levenshtein is only evaluated if the first condition is true, then there's the overhead of doing additional comparisons. Thus, the second query is naturally slower. IMHO this is a CPU bound problem. To make this query faster, either supply more predicates that can be evaluated (means: add more restrictions to the where clause), add more CPU power and try parallel selects (if mysql supports that). 

The idea is to get all data for a time frame We want to do a query that shows everything related with a employee for this month. There is a total of 7 queries to merge into one pivot result. 

But as we have some entries in the table The calculations should be made as following for February ( since we have month : 2 in the table ) 

We first get the average expense cost of last 3 month, and each forecast month should be increased by a given percentage 

The object(building) need's to have also some extra field's like electrical review/sctructural review and several other types. On top of that they can be a yearly based review or 5 years review. The question is how to make a quite simple and scalable schema for the table that will keep the reviews. Reviews should cosist of the following fields as base :: [ each building can have multiple reviews ]