Are you using DHCP on your LAN? If you do, and you are setting up the VMs to get their configuration from DHCP, ensure that the virtual NICs have different MAC addresses. If you don't have DHCP on your network, or you do but want to configure the boxes manually, ensure that you use different IPs for each box (and that they do not overlap with other boxes in the LAN or the DHCP pool). If possible, and esp. if you have control over DHCP, I would suggest using DHCP. 

SFTP + Winbind? It will let the users log in as their domain user via SSH and thus SFTP, and get the right permissions. 

Most probably your VPS didn't came with Apache installed via apt-get. See if you can find the binaries in /usr/local, which is a telltale sign of it being compiled from source. I can't find a good reason for doing this... and I don't like the sound of it... 

Yes, deploy a directory. You can also push a new .war file and Tomcat will do the right thing without the need for restarting Tomcat. IIRC, you can modify files of an exploded war file, and it will work. But it will cause problems (the war will be reexploded on restart, for instance, and overwrite). 

I want users to login to my mail server using: u: user1 (NOT user1@example.com) p: password I am using Samba4 on Zentyal for authentication. How can I configure such that users do not have to add the domain name as username when logging in? 

Create the script to take the relay mails offline from the "LAN" in the perception of the mailstore (M), so that it falls back to its fall back relay Create a script on dns server (D) {or in etc hosts on M, whatever is better, but essential DNS for M}, that alters mail-relay.ourdomain.com to point to the server mail relay A or B that has internet access with a TTL small enough (say 5 seconds) 

My question aims to clarify whether I can get a setup running, where Server A on 1 site receives email as is received at the same time by Server B on site two. The requirement is to have Server B be failed to in case Server A is inaccessible, or down; but at any given moment, users will be using one of the two. The setup I know is that one server can temporarily receive email for the other whilst its done, but then forward, using two or three MX records with differing priorities e.g. MX 10 mail.example.com MX 20 mail.example.com but how do you make it that each and every email is sent to both servers, and one server is used as sending server by users? Take note, this is platform open, meaning I would also allow answers suggesting platforms, but so far these are my platforms 

Better scaling of capital costs. Most datacenters aren't fully populated immediately after being built, which means that a lot of money gets tied up in things like the building shell, power distribution, uninterruptable power, etc, that sits idle. These container units are largely self-contained, so the costs don't hit until shortly before they are deployed. Portability is a big deal. In addition to making it easier to chase lower energy costs, as others have mentioned, they can simplify disaster recovery. In addition, they let massive computing power be deployed to a location relatively quickly and inexpensively. I had a friend working on a project that required crunching huge datasets. It was better for them to set up a container in the parking lot outside his office building than to have to push the huge datasets over a WAN to a big centralized datacenter somewhere. 

I don't know, but if I were you, I'd ask myself whether it is worth the hassle of getting it working on an unusual configuration. The overhead of Apache compared to running PHP apps in FastCGI is much less than most people realize (just looking at top and adding the resident memory consumption of each apache process doesn't cut it). You can still get many of the benefits of Nginx by setting it up to serve static files and reverse proxy everything else to apache. Make sure to enable buffering of proxied requests, it will allow apache to move on to the next request while nginx deals with feeding the result back to the client, which makes much more efficient use of the memory apache+php takes up. Plus, you get to draw on all the experience people have with deploying PHP apps on apache. I'm using the combo to host wordpress on my VPS 

All these can be factors, you just need to ask yourself question 1 above, or understand the ssh configuration made on that PC to see if its not one of the factors I mentionedfor question 2 and 3. Good Luck. 

Is it in a LAN, or publicly (internet) visible. If in a LAN, you can't connect to that machine when you are at home because the machine is in a different LAN, unless you get a link to the public domain. Is ssh enabled for the user e.g. root (or the one you connecting with) in the ssh config file of the machine you want to connect to (you did not specify the OS on the 'school computer': if its Linux then follow the guide to "refuse remote ssh for root" on google? Lastly, is ssh disabled outside the LAN. 

Update IN WHAT WAY (preferably using Zentyal setup above) can I setup Server B to get the same email that Server A gets (even by forwards, or whatever TRICK): if this is possible, or can be achieved, and in what way. I have researched and can't find clarity on this matter if this is a STANDARD or you have to achieve it by the TRICK. What I want is whether its possible, if not WHY. ULTIMATELY, the feel should be that when Server A is down, Server B is connected to by both users sending email, and for receiving email into the mail store (Which can be explained if it is one or more, however ONE still leaves the problem of a failure with that mail store), and users connect to it for IMAP and POP3 

It's pretty common to see advice to disable the write cache on individual disks used for databases because otherwise some disks will acknowledge writes that haven't yet made it to the disk surface. This implies that some disks don't acknowledge writes until they've made it to the disk surface (Update: or that they report accurately when asked to flush the cache. Where can I find such disks, or where can I look for authoritative information on where to find such disks? I'm setting up some DB servers that would really benefit from using write cacheing, but the application is price sensitive and I'd rather not double the cost of my disk subsystem for some caching RAID controller because I don't have enough information to know whether I can trust the cache in each drive. 

Unless you expect all your data to fit in the cache, I'd look at the ratio of cache hits to cache misses. You get diminishing returns. Doubling the cache will probably halve your miss-rate, which is a big win when half your requests are uncached, not so much when 90% are. 

Whatever POSIX says. I'm not up to stuff on this, but I would guess scripts using ps won't be very portable anyway. 

You might want to use bootchart to see what are the boot time hotspots. There's also readahead: $URL$ , which I haven't tried. 

You should say how are you mounting the volume. I guess you are using some kind of GUI tool, such as Gnome's. There might be a way to make that work using your system, but I recommend changing the method of mounting the volume to something more "scriptable": 

Subscribe to debian-announce. No, really, you don't want to automate version upgrades, as by definition, things might break. 

When the cost of switching to a dedicated server is lower than that of optimizing the code to get the required performance on your current system (when you have reached the limit of your hardware, it goes to infinity). 

Unless you have great bandwidth between the nodes, I don't think this is going to fly. You could set up a distributed block device with drbd, for instance, and run a RAID setup over several boxes, mount the fs on a single node and run your database server. But the performance is going to suck unless you have LAN-level communications performance. What are you storing in your database which does not fit a single server? Is it really cheaper for you to buy multiple boxes vs. one big server? Have you looked into sharding? Are you storing files in your database? If you are, could you split those off? 

Both these options work for the most part, what I need is why they would not work (any danger of using one of them)? 

I work an 8am-5pm job in a server room partition, separated by closed glass from the actual room where the server racks are mounted, from where the continual buzz sound of running servers, approximately 12 -15 of various sizes (mostly mini computers or small business servers, no main frames), is audible all shift long. Recently, my co workers have been complaining about the static charge i discharge onto them when i greet or exchange pleasantries physically with them, mostly as soon as i leave the room for up to something like 5 minutes. It doesn't always happen when i have touched any of the servers, sometimes its just being in the presence of the room i work in that gets me charged up. Where i sit, the temperature ranges from 18 to 30 degrees Celcius, depending on whether i have my air conditioner on. There are two issues that now make me think of their harm: 1. The buzz sound & 2. The static electricity Can this be cancerous? And are there any ways this can harm me if i continually manage servers in such an environment? What best practices can i take to ensure the safety of both the servers and myself? 

If you can't experiment, all anyone can do is blind guessing. Which might work. And you might have a particularly lucky and accurate blind guess. You should profile and examine your running system. Someone "guessed" above that you are hitting swap, and that might be a good guess. First use top, vmstat, sar to get a picture of what's the box doing. Do you have your CPU pegged? Are you doing massive IO? Are you swapping? These will give you a reasonable idea of what's the problem. Your problem might lie anywhere between lighthttpd, PHP, memcache, MySQL. The usual suspects there would be: 

Which I don't really understand. I've dug around using dig(1), and have noticed they've set up a SOA record for foobar.es: 

Well, it goes both ways. You might have concurrency issues, typically locks. Write operations typically lock other writers (and sometimes, writers block readers, or even readers block other readers). If you have locking, concurrent users can slow others. On the other hand, you can have scenarios such that one users is performing I/O while another one is doing CPU work; these could be parallelized and you would be using your resources more efficiently. Most of the time, concurrency hits you, though.