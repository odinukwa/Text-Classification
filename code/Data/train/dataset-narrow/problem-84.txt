If you have a 1D function $f(x)$ and you want to integrate this function from say 0 to 1, one way to perform this integration is by taking N random samples in range [0, 1], evaluate $f(x)$ for each sample and calculate the average of the samples. However, this "naive" Monte Carlo integration is said to "converge slowly", i.e. you need a large number of samples to get close to the ground truth, particularly if the function has high frequencies. With importance sampling, instead of taking N random samples in [0, 1] range, you take more samples in the "important" regions of $f(x)$ that contribute most to the final result. However, because you bias sampling towards the important regions of the function, these samples must be weighted less to counter the bias, which is where the PDF (probability density function) comes along. PDF tells the probability of a sample at given position and is used to calculate weighted average of the samples by dividing the each sample with the PDF value at each sample position. With Cook-Torrance importance sampling the common practice is to distribute samples based on the normal distribution function NDF. If NDF is already normalized, it can serve directly as PDF, which is convenient since it cancels the term out from the BRDF evaluation. Only thing you need to do then is to distribute sample positions based on PDF and evaluate BRDF without the NDF term, i.e. $$f=\frac{FG}{\pi(n\cdot\omega_i)(n\cdot\omega_o)}$$ And calculate average of the sample results multiplied by the solid angle of the domain you integrate over (e.g. $2\pi$ for hemisphere). For NDF you need to calculate Cumulative Distribution Function of the PDF to convert uniformly distributed sample position to PDF weighted sample position. For isotropic NDF this simplifies to 1D function because of the symmetry of the function. For more details about the CDF derivation you can check this old GPU Gems article. 

You are on the right track but what you need to do is to calculate u/w and v/w, and also 1/w for each vertex, which you interpolate linearly in screen space in your rasterizer. Then for every pixel you divide the interpolated u/w and v/w coordinates with the interpolated 1/w to get perspective correct uv-coordinates for the pixel. The same applies to all the vertex attributes, e.g. if you need to get perspective correct color or position of vertices per pixel, then you interpolate c/w and p/w and divide those values by 1/w per pixel. Even if you don't implement software rasterizer this can be quite useful for example in ray marching of screen-space reflection implementation. I have seen people doing the ray marching for SSR in world space because they don't know how to properly interpolate position in screen space. What old software rasterizers used to do is to perform this 1/(1/w) division only every X pixels and linearly interpolate values in-between because the division was relatively expensive operation to perform every pixel. 

When you perform regular Monte Carlo integration over a hemisphere using $N$ samples, each sample represents $\frac{2\pi}{N}$ steradians. So the Monte Carlo integration for Lambertian BRDF is: $$\frac{2\pi}{N}\sum_{i=1}^N\frac{\rho}{\pi}L_i*Cos\theta_i$$ For path tracing, you only take one sample per path segment, so because $N$=1, the above sum becomes: $$2\pi\frac{\rho}{\pi}L*Cos\theta = 2\rho L*Cos\theta$$ 

Atmospheric scattering in Bruneton model is composed of Mie scattering and Rayleigh scattering. Mie scattering is mostly wavelength independent strongly anisotropic scattering from particles that are quite a bit larger than the wavelength of light (water droplets & pollution in the atmosphere). Mie scattering results in effects like halo around the sun. Rayleigh scattering on the other hand is wavelength dependent scattering from atoms in the atmosphere whose size is close to the visible light wavelength range. Rayleigh scattering is responsible of effects such as the blueish sky at noon and blueish tint of objects in the distance. 

The cosine term in the rendering equation is to account the amount of light reaching the surface, and leaving it out from the rendering equation is what he refers as "complete nonsense", which is right. The original Phong model didn't have the cosine term in the rendering equation, so if you want to model the original nonsensical Phong with proper rendering equation, you need to hack it by adding the cosine denominator to the Phong BRDF. 

The alpha of the color wheel is constant (i.e. same for all colors in the wheel) The beige background extends half way through the color wheel, and is constant for top and bottom halves of the image The background in the center is constant for all colors The alpha blending equation is the standard $f=c*\alpha+b*(1-\alpha)$, where $f$ = final color, $c$ = color on the wheel, $b$ = background color, $\alpha$ = alpha of the color wheel 

Instead of using cubic Bezier spline, you should use cubic Hermite spline. For cubic Hermite curve you specify the segment end points and tangents, which is what you then control in the tool. Hermite spline is almost the same as Bezier spline but with different $B$ matrix, so if you already understand Bezier, then understanding Hermite is a trivial step. So in the Adobe Illustrator pen tool (which based on the video I take is the same as Adobe Photoshop pen tool that I have used) you keep adding more Hermite cubic spline segments to your spline. By default the Hermite start tangent matches (with opposite sign) the end tangent of the previous segment and in GUI they are linked together (changing one changes the other). So in that configuration the curve appears smooth. However, you can unlink the tangents to cause C1 discontinuity in the spline and control them separately. This is purely GUI work though and how you control the Hermite spline tangents & positions. 

You can't perform this transformation by applying constant transformation matrix to the ladder model since it's not linear transformation like joojaa said in the comments. What you would have to do instead is to define the transformation as a function of height as you have done in both of your proposals. So both of your proposals are kind of correct, except for the typo in second one where you should use $R$ instead of $T$ that you raise to power of y. However, you might have difficulties raising a matrix to non-integer power (requires expansion of Taylor series, AFAIK), so your first proposal is much more practical. 

Since the cube is a convex object, you could first find the silhouette edges of the cube, by using dot product between cube face normals & view vectors. Then you can sort the edges by the smallest screen y-coordinate for each edge and rasterize the cube a scanline at the time by filling the span between two active edges for each scanline. For each scanline you progress the two edges by adding the edge slopes (x1-x0)/(y1-y0) to the current x-coordinate of the edges. 

I wouldn't really call it OpenGL SDK but OpenGL API. There's a common OpenGL API which is basically a set of functions you can call to control the GPU. This API is hardware agnostic (barring extensions), so you can use it on NVIDIA, ATI, etc. GPU's. What the OpenGL driver does is to turn these platform agnostic function calls to hardware specific commands that the driver then sends to the GPU. For example when you tell GPU to draw a triangle via OpenGL API, then the driver converts these calls under the hood to specific commands for the specific GPU you are using. This abstraction is important so that you don't have to write different code for every GPU. To explain things a bit in lower level, the graphics API implementation operates in user mode, while the graphics driver operates in kernel mode. User mode is "protected" mode your program runs in, in the sense that you can't mess up your entire device but only your own app and it's also relatively fast to perform the API function calls in this mode. Kernel mode on the other hand is "unprotected" and performing kernel mode calls requires an expensive mode switch. For OpenGL the IHV's provide both the OpenGL API and driver implementations and thus IHV's can provide optimal switching between the modes. This is in contrast to DirectX where API implementation is provided by Microsoft and the driver by IHV's. 

The short answer: no, but if you are interested in details, please keep reading (: About lighting units Light “brightness” is indeed quite poor/ambiguous layman’s definition for brightness of a light source. Below is a list of different lighting properties/units commonly used in lighting calculations that define the light “brightness”. I listed both radiometric and photometric [in brackets] properties/units. Radiometric units are Human Visual System (HVS) neutral "physical" units defined based on Watts (W), while photometric units are HVS weighted units defined based on Lumens (lm). Photometric units are commonly used in rendering because practically all consumer image input and output devices operate in these units as they are designed & optimized for image acquisition & consumption by normal tristimulus (LMS) HVS. Proper conversion between photometric & radiometric units isn’t simply a matter of multiplication by some factor (like between kilometers & miles or kilograms & pounds), but different projection from Spectral Power Distribution (SPD) of visible light spectrum (explained later). Some people use constant luminous efficacy of 683 lm/W to convert between the units, but this has very little touch with reality and results in unrealistic values. 

A common approach for real-time radiosity in games is to assume static geometry, which can be used to pre-calculate visible surfels (position, albedo, normal) for a point in space (light probe). This approach avoids the need for run-time rasterization of the geometry for different viewpoints. The surfels can be then dynamically re-lit in run-time and used to gather luminance for the probe, which is used to approximate the indirect illumination. 

There are couple of ways this is usually done: 1) put the textures for the house to a single texture are just UV-map the object accordingly. If you would need to use different BRDF etc. for different parts of the object, you can use uber shader and switch in shader and use another texture to control which BRDF to use in which parts. The benefit is that you can render the entire object with a single draw call reducing CPU overhead with the expense of some GPU overhead due to the uber shader. 2) split the object to different sub-objects (at triangle level) based on the material and issue separate draw call for each. This keeps the shader simpler, but has higher CPU overhead due to more draw calls. 

You need to perform integration of $P$ over the hemisphere to calculate the solution. There doesn't seem to be closed-form solution as the solution requires incomplete gamma function: $$2\pi(e^{-\alpha d}-\alpha d\Gamma(0,\alpha d))$$