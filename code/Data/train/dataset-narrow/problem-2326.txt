Write down ILP, convert to LP by relaxing the integer constraints and round the solution Write down the ILP, convert to LP by relaxing the integer constraints, write it's dual and solve it 

Finite automata and regular languages Pushdown automata and context-free languages Turing Machines Undecidability and TMs Complexity classes (P, NP etc.) 

What's an intuitive proof that shows that the conditions of complementary slackness are indeed true: 

What are some good references, not very dense, to understand the underlying math and computability aspects of the notion of "functional programming"? I'd like to have something that talks about it from a math point of view and if/when I learn a functional programming language I can have the necessary "aha" moments when seeing the ideas being implemented. I've read SICP, but that's Scheme specific and is a dense read. I don't really "get" functional programming after reading it and was looking for something more sublime and closer to the underlying math and computability points of views. 

From a purely abstract math/computational reasoning point of view, (how) could one even discover or reason about problems like 3-SAT, Subset Sum, Traveling Salesman etc.,? Would we be even able to reason about them in any meaningful way with just the functional point of view? Would it even be possible? I've been mulling on this question purely from a point of self inquiry as part of learning the lambda calculus model of computation. I understand that it's "non-intuitive" and that's why Godel favored the Turing model. However, I just wish to know what are the known theoretical limitations of this functional style of computation and how much of a hindrance would it be for analyzing the NP class of problems? 

It seems from the excerpt that the reason was to develop the "philosophical foundations of type theory" - I thought this foundation already existed (or maybe I assumed it did). Was this the main reason then? 

Why is the second approach considered 'better'? What makes it the preferred way of approximating the solution to an ILP? What is that #2 has that #1 doesn't? In Vazirani's Approximation Algorithms book he approximates set cover using both of the methods above, but I'm unable to discern the underlying concept of choosing #2 over #1? What "IS" the intuitive 'aha' to help me understand this? 

Let us say we have some abstract context-sensitive grammar in the Kuroda normal form, which is where all production rules are of the form: $AB\rightarrow CD$ or $A\rightarrow BC$ or $A\rightarrow B$ or $A\rightarrow a$ There is also the one-sided normal form or the Penttonen normal form for context-sensitive grammars (described in the same wikipedia article), which is where all rules are of the form: $AB\rightarrow AD$ or $A\rightarrow BC$ or $A\rightarrow a$ Question: how to generally convert a rule of the Kuroda normal form $AB\rightarrow CD$ into bunch of rules in the Pentonnen normal form? In particular I am stuck with understanding how to deal with an intermediate rule of the form $AZ\rightarrow WZ$. 

The Context-Free tree grammar has rules of the form: $A\rightarrow t$ or $A(x_1,\dots,x_n)\rightarrow t_x$, where $A\in N$, $t\in T(N\cup T)$, $t_x\in T(N\cup T\cup \{x_1,\dots,x_n\})$, $T(Z)$ means a set of all possible trees with labels from $Z$. where $N$ is a finite unranked set of non-terminals and $T$ is a finite unranked set of terminals, $x_i$ are free-variables. It is clear, that this form of rules is definitely context-free. The thing which I doubt about is: would the following form of rules $x_1(A)\rightarrow x_1(A_1,\dots,A_2)$, where $A_i\in(N\cup T)$, be context-free? (More generally: $x_1(A)\rightarrow x_1(t_1,\dots,t_2)$, where $t_i\in T(N\cup T)$). (Obviously it is not context-free according to the definition, but why not?). I don't see any context here. Such kind of rules may be useful for describing changing of the branch without growing tree down. $X$ here is a free-variable, and it points to the arbitrary parent node of the terminal $A$. The only one theoretical objection here against "context-free-ness" of this form of rules, is that this kind of rules implies, that non-terminal $A$ is not a root of the current derivation tree. From other hand, conventional rules of the form $A(X_1)\rightarrow\dots$ can not be applied for the case when $A$ is a leave in the current derivation. UPD: For those who asked me for an example of Context-free tree grammar, please refer this link: $URL$ Though it describes CFTG for ranked trees, obviously the same rules in the same semantics can be applied for the case of unranked trees. 

Complementary slackness (CS) is commonly taught when talking about duality. It establishes a nice relation between the primal and the dual constraint/variables from a mathematical viewpoint. The two primary reasons for applying CS (as taught in graduate courses and textbooks): 

And similarly for the dual variables $y^*_i$ and constraints in the Primal. Where $x^*$ and $y^*$ are the optimal solutions to the Primal and Dual respectively. What's an intuitive proof as to why this is the case? Staring at the equations makes sense algebraically, but I wish to understand it at a more visceral level. 

Given today's computing power and polynomial algorithms for solving LPs is CS still relevant from a pragmatic viewpoint? We could always just solve the duals and address both the points above. I agree that it's "more efficient" to solve the dual with the help of CS but is that it? Or is there more to CS than meets the eye? Where exactly is CS useful beyond the above two points? I've commonly seen texts alluding to the concept of CS when talking about approximation algorithms but I fail to understand its role there. 

I'm just reading up on lambda calculus to "get to know it". I see it as an alternate form of computation as opposed to the Turing Machine. It's an interesting way of doing things with functions/reductions (crudely speaking). Some questions keep nagging at me though: 

If $x^*_j > 0$ then the $j$-th constraint in the dual is binding. If the $j$-th constraint in the dual is not binding, then $x^*_j = 0$ 

Currently, our ToC (Theory of Computation) courses are designed with the following progression of topics: 

I'd go for Combinatorial Optimization: Theory and Algorithms - Korte & Vygen. It will go you a good overview of algorithms with a constant focus on optimization. This book is intended for those with a heavy math inclination IMHO. This would go well with Algorithms: Dasgupta & Papdimitrou, I believe. 

If one were to redesign the ToC course where TMs in #3 and #4 above would be replaced by $\lambda$-calculus how would the rest of the course look like? That is, what would we teach in place of #1 & #2 above to progressively lead to $\lambda$-calculus? Also, could there really be chapter on complexity classes with $\lambda$-calculus as THE model of computation? 

The lambda calculus is a rewriting system and Turing complete. Which are the rewriting systems corresponding to the other levels of the Chomsky hierarchy? E.g. what is the functionally computing system for finite state machines and so on? 

If a function $f$ is understood as its graph, i.e. a set of pairs $\langle x,y\rangle$ where $x$ is input and $y$ is output, then the empty set $\emptyset$ is a valid function, and for any set $A$, we have the empty function in $\emptyset\to A$. This makes $\emptyset$ the initial object in the category of sets. By set extensionality, there is only this empty function and this also fits with cardinalities, where we like $|A^{\emptyset}|=|A|^{|\emptyset|}=1$. In logic, we usually take that form absurdum follows everything, ex falso quodlibet, and in the propositions interpretation for types, if $\bf 0$ denotes the empty type (or bottom type), we want a function ${\bf 0}\to \tau$ for all types $\tau$ too. I've been told in some programs this would be done by defining alla definition exfalso (a : Type) (x : 0) : a := "case x of -emptyspace-" Now I try to construct the terms for function types involving the empty type over the standard theories you'd find in books, but I can't quite reproduce the good properties, see below. I wonder 

To understand Applicative, as induced by a monad, I want to point out the following construction: The Yoneda lemma implies that there is an isomorphism between $FA$ and $\mathrm{nat}(\mathrm{Hom}(A,B),FB)$. In the category of Haskell types, this is the mapping $$a\mapsto\left(g\mapsto F(g)(a)\right)$$ of type $$FA\to B^A\to FB.$$ Evaluating at $FA$, then applying the functors arrow map to the resulting function - if the components of the natural transformation make sense as arrows - and abstracting $FA$ again, we obtain a mapping of type $$FA\to F\,B^A\to FFB.$$ Now if the functor comes with a monadic 'join', mapping from $FFB$ to $FB$, and if we switch around the lambda abstractions and thus the first two argument slots, we can we can obtain a function of type $$F\,B^A\to FA\to FB,$$ which one denotes by <*>. (This is also what you get via $\mathrm{LiftM2\ id}$ in Haskell.) 

What would be a good informal/intuitive proof for 'hitting the point home' about LP duality? How best to show that the minimized objective function is indeed the minimum with an intuitive way of understanding the bound? The way I was taught Duality only led to one understanding which I am sure is shared by a LOT of people I know: For every corresponding minimization problem there is an equivalent maximization problem that can be derived by inverting the inequality constraints. Period. This "conclusion" of duality is what seems to stick but not "why is this so" (i.e. how/why is there a bound on the optimal solution). Is there a way of playing with the inequalities just to 'show' the lower/upper bound on the optimum which could be a motivation for the proof? I've gone through Chvatal's book as well as a few others but found nothing that could be understood by absolute noobs to LP. The closest that I got was from Vazirani's book on algorithms, where he talks about 'multiplying the inequalities with some magic numbers that show the bound' - I'm not sure how to reproduce the effect for an arbitrary LP. 

I'm aware that functional programming languages are based on lambda calculus but I'm not considering that as a valid contribution, since it was created much before we had programming languages. So, really what is the point of knowing/understanding lambda calculus, w.r.t. its applications/contributions to theory? 

Outside of academia, what are the uses of my 'powers'? What can I do other than teaching and publishing papers? Where all can I apply my powers? For the sake of argument: Please assume I have a PhD in algorithms/TCS and have learnt a great deal of 'stuff' and made have come up with breakthrough bounds on existing algorithms etc., And I also have a strong footing in algorithmic analysis, approximation/randomized algorithms, mathematical programming etc., Rationale behind question: Curious about non-academic career options for folks in this area and to possibly motivate some students that it's "just not theory" and there are potential uses in the outside world in essence. PS: Please don't answer stating there is a lot to learn and you may want to try topic XXX. I am curious from a career/professional development point of view. Operations Research (OR) seems to be the only good fit, IMO. What other options exist?