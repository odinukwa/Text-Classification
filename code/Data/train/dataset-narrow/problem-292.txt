...and there's "public" area, where CPU is using the buffers as they're punted towards it and need servicing. The "Rx"-level buffers are part of the shared buffer to service all interfaces (on either old, legacy switches like 2950 or newest 2960S/3560X/etc) or a subset of interfaces, belonging to specific port ASIC (like 2960 or 3560/3750/3560E/3750E). On the 4500 and 6500 in particular it gets messy, as there is a number of pools that packet can go through - input interface (ASIC) buffer, the pool at linecard level (on the 6500 at DFC), at the switch-fabric level and at the end the buffer at Supervisor level. They don't have to be physically separate memory pools, but are often mapped in different commands to different names to ease the troubleshooting process (at which step of the packet walk-through was the packet dropped for example). 

For better utilization of the physical ports, I'd recommend you take a look at the WS-X6748-GE-TX card, that also has 48 10/100/1000 ports but has also two 20Gbit/s fabric connections. Those 20Gbit/s fabric channels are split between ports 1-24 and 25-48, so you get still oversubscription, but only 24Gbit/s over the 20Gbit/s supported by channel, not 8Gbit/s over 1Gbit/s as in 6548 (so, effectively, 1.2:1 oversubscription in 6748 vs 8:1 in 6548). This should give you space to burst traffic over the link from the sending station and distribute it across the system. 

Router will decide on input what kind of packet it receives based on Ethertype - with IPv4, IPv6 and MPLS all having distinct values. As for destination - the destination IP in FIB will contain instruction which label to push to the packet before forwarding. On surface, (3) is fastest, because you can search single label space quite efficiently in one lookup. For RIB/FIB you're usually dealing with hierarchy/tree, so it usually requires multiple lookups (with a lot of work during last 10-20 years on the construction and optimization of such search trees). Only in theory however - current generation FPGAs/ASICs are capable of doing multiple lookups at the same time, and what's even more important - are doing it anyway. Why? They need to check for various services assigned to the packet/FEC - like QoS, filtering and so on. 

ACL 102 is showing - are you sure you want to encrypt ALL traffic? And you need to show us debug that Gareth asked for, as the negotiation fails with ISAKMP ( and ), and it's not yet the problem on the traffic/data plane level. 

Well, this node BGP session to ISP is down so you don't get any routes over 222.222.222.57 and that's why all the traffic goes to the other BGP neighbor: 

Because RFC4271 says so? The limit was present in BGP 3 as well. Given the small internet at the time when BGP was created on legendary napkin, I'd say it was choosen exactly the way number of bits in IPv4 was. 

If the platform is Cisco Catalyst as reflected in the tags for the question, that may be the case. Some platforms reflect only punted (soft-switched) packets in the "show ip access-list [...]" output, while others reflect none. For example, for Catalyst 6500 and Sup720 You'd need to use , while for other Catalyst platforms You'd usually be able to see aggregated statistics for ASIC or whole box via . 

That said, for the inter-vlan routing in most of the scenarios the L3 switch will perform better and cause less problems, as small/edge routers are still using CPU to route the traffic and performance may be not ideal. So propably you'll want L3 switch and edge router. 

The configuration you're trying to use is based on router capabilities, not switch capabilities. While it's true 2960 can do shaping, it's configured in a different way and works in a slightly different way. Please take a look here to read about the idea, and then when you have configuration get back with any other, specific questions: $URL$ One example of configuring sub-rate shaping on interface is proposed here: $URL$ 

Because in some VoIP protocols signalling (setup of connection, but also DTMF tones) can be carried in separate session from voice payload. If the voice payload can't traverse both ways (and it's often the case with NAT/firewall gateways) you'll get one way audio, but the connection still will be made and can provide some additional services (like your tones). 

There's a different path taken by each of those protocols in the network - due to things like traffic engineering, different routing policies or whatever is between you and the destination host. There's a box in the middle (like firewall or something similar) that's modifying the TTL field because either that's a policy (and for one protocol it's in place, for the other it's not) or it's doing something in addition to just processing the packet from one side to the other. 

Well, You can extend the LANs with L2TPv3 and protect the setup with IPsec. Here's example configuration: $URL$ And here's another one, modern, using FlexVPN - unfortunately, depending on your IOS version, you may not be able to use it: $URL$ I did some testing for that kind of setups, bear in mind transporting all the L2 traffic over IP will cost you bandwidth. If you need any help in setup/troubleshooting, let me know. As for more obvious problem - connecting 891's to ISPs - just point default network to them as gateway. From this setup the routing itself is typical in terms of configuration. 

If you can't find 12.3 image that fits in the 8MB flash partition, you'll need to repartition the flash with in the configuration mode. Then format it as one big partition, which should give you 16MB of space, and finally put some 12.3 IOS image on it which should then work fine. Ah, and BTW, the IOSes tend to reserve some space, and if you're fighting with pushing every available bit to use, erase the flash in this manner: 

Why IP needs to be there? Remember that IS-IS is L2 protocol and L3 information (IPv4 or IPv6 address information) is overlay information for IS-IS. It's not needed for IS-IS itself, but needed to manage neighbourship at the L3 level for given IPv4 or IPv6 routing nodes. 

With sufficient CPU power, you can run link state in Internet, and you'd do probably fine. But that's neither elegant nor scaleable. BGP was invented to provide scale - by limiting number of unnecessary things internetworking protocol would need to deal with and focusing on things, that on the internetwork scale are important (like AS_PATH, which shows on a Internet scale how AS interconnect). Link-state protocols for example, are focused on links, not prefixes or AS_PATHs. Why would you like to keep link state information across all links in the Internet? Can you do it? Yeah, probably. Will it run on processors that were available 20 years ago? No. Would it be really wise to do it today from engineering perspective? Again, probably not - at least not on every node. In the era of SDN controllers, more and more horse power will be available to provide large-scale calculations across or even for entire Internet. That doesn't however mean this should be executed on a per-node basis. 

On the ISR G1/G2 routers you can use packet capture feature, where you use ACL to match the traffic and store it into memory during capture, then dump to .pcap compatible format if you need it offline: $URL$ On the Catalyst 4500 with newer Supervisors you can actually run wireshark. 

So first of all shows fabric utilization, not CPU utilization. Fabric doesn't have CPU component per se, and you can go all up to 100% of fabric utilization without any adverse effects similar to what CPU causes when nearing to the full load. Next, the WS-X6548-GE-TX is 8Gbit/s card, so "old" fabric attached LC with 8Gbit/s channel. Internally, it shares buffers per 8 ports on card, so given you're getting 'overrun' errors that typically point to a problem with receiving traffic in timely manner and handing it over to other ports, first thing I'd do is separate incoming 8-port group on the card to separate group. In other words, if there is specific port/group of ports receiving high-volume multicast traffic, I'd move it to separate group on the card - and please rememeber, each consecutive 8 ports is one "group": $URL$ This means, among other things, that the 8Gbit/s interface to fabric is statically sliced in 6 groups of 8 ports, out of which each group gets 1Gbit/s maximum. So, if in any given port group (of 8x 10/100/1000 ports) you have ports receiving traffic over 1Gbit/s you'll hit exactly problems you're encountering. That's why my proposal is to move any other ports out of the 8-port group apart from the one interface receiving massive amounts of multicast traffic (it seems it's in your case). You can find this information literally stated in the release notes: $URL$ 

Yes, both transform-sets like ACLs to select interesting traffic can be reused in different crypto maps and different crypto map entries of the same map. 

Router will respond with ICMP and drop the packet if it can't fragment the packet further, or it's prohibited to do so with the Don't Fragment bit set. And yes, as you pointed out, PMTUD often breaks in real world, as people do filter ICMP messages - either in transit, or at their internet edges, shooting themselves in the foot. Here's a great writeup on PMTUD and fragmentation for your reference: $URL$ 

There are specific solutions (usually proxies) both commercial and open source that can address this and are specialized to fight with data leakage prevention. On the protocol level, limiting traffic to some artificial values (be it 1000 or 10000 bytes per day) doesn't make sense as it will break TCP or any other higher-level protocols. 

Again, doing that kind of testing with simulated environment just pushes it too far, I'd go back to some hardware setup and test/lab it to reflect real scenario. Even if you hit success, you may not be able to repeat it in predictable manner later on. 

It won't interfere per se, but question is - what you're trying to achieve? First of all, it's hardware dependent - you may be silently ignored using this command on software platforms. Essentially, you should drill down to supported/unsupported notes for your gear to check if it's available and working. Some platforms can tune up/down delays separately. If you're trying to achieve fast convergence, use one protocol - by approaching the problem with the KISS principle. BFD seems to be best fit in case multiple protocols are used and they need information about the neighbor loss ASAP. BTW, will only work on interfaces, that will signal up/down situation properly. Connections to shared media (like Ethernet switch) or to media that doesn't properly propagate up/down signal (some SDH/SONET/ATM gear) will cause problems anyway, so you should fall back to something that YOU control. 

Your "host 10.25.0.0" construct is wrong - you're referring to network here, not single host, so use "10.25.0.0 0.0.255.255" (inverse mask, without 'host' prefix) in 100, 200, 300 and 400. 999 is good.