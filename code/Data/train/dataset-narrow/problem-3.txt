Answered on Slack, the likely first issue here is that the output from is not a cert, it's a CSR. But there are some other lingering issues here too. It is recommended to use something like Certstrap to do internal cert generation rather than raw commands as the modern standards are quite fiddly. 

The above answer covers part of it but misses one of the important elements: convergent design. I wrote some words a while ago about this in the context of Chef at $URL$ but the short version is that a bash script is a set of instructions, while an Ansible playbook (or Chef recipe, Salt state, etc) is a description of desired state. By documenting the state you want rather than the steps you want to take to get there, you can cope with a lot more starting states. This was the heart of Promise Theory as outlined in CFEngine long ago, and a design which we (the config management tools) have all just been copying since. tl;dr Ansible code says what you want, bash code says how to do a thing. 

Is there an elegant way or a best practice when it comes to deleting old images from the Docker registry? I see a lot of requests/issues here: $URL$ but didn't find a good/popular solution for it. So, is there a tool or a technique which would help me do that? Also, is there any best practices which you follow while doing it? 

I use for starting Ansible playbooks inside the Master which is spawned via boto3 from AWS Lambda. This playbook creates multiple workers, make them do some tasks, and terminates them. So, I want to design/build a reliable monitoring system for the same. For now, I am writing the logs of to a file inside master, and pushing it to a store(for now, S3) once the Ansible plays are completed. But, I wanted to know whether there is a much more elegant (and/or) better method for doing the same? And also, whether there are good tools which would help me in the process? 

Talking specifically about the image packaging piece of Docker, not the container runtime there are a few minor bits. The biggest is that a Docker image is more like a chroot, which means you are prevented from accidentally depending on shared system state since every file in use must be explicitly included in the image while a system package might pick up dynamic links you didn't expect or otherwise get more intertwined with other packages. This can come up with complex C dependencies getting loaded without your knowledge, for example OpenSSL. Additionally using deb packages don't de-duplicate shared bits in the say Docker's storage system does. For some this might be a good thing, better I/O performance and fewer moving pieces, but for others it might be a problem. 

You can find more details about and friends at $URL$ With cookbooks based around custom resources instead of recipes, things can get more complex but the specifics depend a lot on the how exactly the cookbook you are extending is written. 

Example, I have 3 tasks, which are registered , and . runs when fails, and runs when fails. Now, I have another task below it which is : 

[Converting my comment to an answer] One way to do it would be to write the logs to some external file, and then having a task after it which makes use of failed_when condition, and remove the log file, if the previous task was successful. Something like this should help you. 

Also, having failover mechanism would help too, like how the answers in this post describe. However, I can't afford to have failover queues in a distributed architecture, as it would increase the complexity. So, the above workaround was a better idea for my team. 

Note: This might not be the best way to handle your problem. But, this was a hack which helped me do my logging and monitoring. 

Test Kitchen has a kitchen-ansible provisioner plugin for testing of Ansible code. It isn't as deep as the Chef integration but it does get the job done for most cases. There is also the more recent Molecule project which is a dedicated Ansible testing system. 

There are a lot of specifics but the overall pattern we use is "wrap and extend". The general idea is to make a cookbook that depends on the community cookbook, usually named , and then make recipes in that which call but with either more stuff added before/after or with calls to things like to change resources. Avoid when possible since it leads to brittle code, but it is there if you need it. You can also use wrapper cookbooks to set attributes, subclass or wrap custom resources, and so on. For the specific case of "I need to tweak a template in a community recipe" it would look like this: 

Also, would the routing mesh act as an application load balancer? Like for example, if I am having a Docker cluster in AWS, how does the ELB added to it, work? And how would that be different from what the internal load balancer is doing? Or, did I get the concept of the routing mesh completely wrong? 

I went through the available metrics in the Cloudwatch dashboard, but haven't found anything related to AWS Config. 

So, a lambda function can be put into a private subnet of a VPC, but not inside a public subnet. And for being able to create instances and ssh'ing into them, from this Lambda, add the private subnet to the security group of the public subnet [or another hacky way, is to add the VPC's IP range to the security group]. 

We faced the same problem in the infrastructure which we're building. So, we had style blocks to set the bid price, depending on the on-demand price of the instance. AWS has an API for getting the on-demand price of an instance. We used this Python wrapper for the purpose. So, once we got the on-demand price (let's say ), we plugged in style blocks, which are , , , , which means we are trying for a bid price in the range 40%, 60%, 80% of the on-demand price. If all fails, then we are falling back to creating on-demand instances. Also, as this is independent of AWS's current spot pricing, we never pay a price above the on-demand price. But, if you're looking for a way to do it on-the-fly, then Hashfyre's solution should be the way to go. 

In general git allows concurrent operations because one of them will eventually fail. Some specific database updates will have file locks (like updating the index files) but multiple receives can be in flight at once I think. In this case whichever verification completed first would be allowed in, and the second would fail due to having an invalid history (i.e. not being a fast-forward push). In general this kind of operation is not well suited to git's server-side hooks since if something does fail, the person will then have to do some surgery on their branches to get things back in a valid state. These kinds of checks are generally run on the workstation side, though then you run into the "fun" that is distributing git hooks as they can't actually be checked in directly. Another common solution is to have a CI service like Jenkins running verifications (tests) and updating a second branch or a utility tag showing which commits are acceptable. 

I was trying to provision spot instances via Ansible yesterday, and almost all my requests failed, even when I put my spot price == the on-demand price of that instance. So, when I had a look at the spot pricing graph, I found something very interesting: 

Velocity Conference is one of the most popular Devops conferences, and also is an O'Reilly conference. From it's website: 

The spot price of the instance in us-east-1a is more than the on-demand price, which confused me. [in fact, ~5x times higher] Aren't spot instances preferred for the low cost? If yes, then why is the price higher than the on-demand price? According to AWS's docs: 

As using SQS queues are an option for you, you can make CloudWatch alarms which can monitor for activity in the queues, and link them up with SNS, which can be used as a trigger for LambdaB. So, your flow can be: 

For a purely EC2-based environment, the easiest solution is to use AWS IAM roles and S3 bucket policies. Some formulations of this pattern also include KMS for encryption and DynamoDB instead of S3 for storage but I've not found a hugely compelling reason to use either. Check out $URL$ it is specifically aimed at Chef users but the underlying pattern works for anything, you just might have to make your own helper for doing the actual download from S3. 

The specifics depend on the exact use case. If the value is only used as a property on another resource, you would use the helper method. If you don't need the output, you would use the resource. In some more complex cases you might use a resource or write your own custom resource. 

"Serverless" mostly just means you've got relatively simple microservices, generally just a little webapp or a single function that is automatically connected to a REST frontend. The same concepts apply as you would use for a more traditional web services: usually some mix of remote syslog and ElasticSearch writers. Networked or remote syslog has been around for a long time and has a fairly robust set of tools around it. You would have to run the central syslog server(s) but the protocol is very simple and there are pure client libraries in every language that you can use for sending logs. One common problem with remote syslog is that it has traditionally been based around UDP. This means that under heavy load, some log messages may be lost. This could be a good thing, helping avoid a cascade overload, but it is something to be aware of. Some newer syslog daemons also support a TCP-based protocol, but client support is less unified so just do your research. More recent but very popular is logging to ElasticSearch. This is mostly useful because of the Kibana dashboard and Logstash tooklit (often called ELK, ElasticSearch+Logstash+Kibana). Amazon even offers a hosted ElasticSearch option, making it somewhat easier to get started. ES uses a relatively simple REST API, so any language with an HTTP client (read: all of them) should be okay with logging to ES but make sure you are careful with blocking network operations in cases of partial system outages (i.e. make sure your app won't get stuck in a logging call that will never succeed and stop servicing user requests). More complex logging topologies are bounded only by your imagination, though these days you'll see a lot of use of the Kafka database/queue/whatever-you-want-to-call-it as a nexus point in very complex log distribution systems. On the "serverless" side, you'll generally want to integrate with these systems directly at the network level, so sending log data directly to syslog or ES from your service/function, rather than writing to local files (though maybe echo to those too for local debugging and development). 

I understand how containers work and the concept of containerizing. However, I am having trouble understanding how Container Cluster Managers work. I was reading through this excellent blog post, which blew my mind about Kubernetes. So, I wanted to know how exactly do these systems like Kubernetes, Docker, etc work? I tried reading this Wikipedia article, but I wanted a much more basic-level explanation. Are these similar to how Zoo-Keeper work? (like the Master-Worker architecture)? 

where is any of the previous tasks. So, how do I set here? cause, Any of my 3 tasks can run. So, it can be or or . So, how do I dynamically write that in the adding task after them? 

I have also asked the AWS Support folks for help too, as Lambda's security was crucial for us [HIPAA compliancy]. This was their response: 

Also, does this mean that people are bidding over the on-demand price? If yes, then why so? Aren't they better off with an on-demand instance? Or did I understand the concept of spot instances wrong?