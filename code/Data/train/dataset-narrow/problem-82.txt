When I saw an implementation of normal mapping that computed the TBN matrix in the vertex shader and converted everything (in particular the view vector and light vector) to tangent space at that stage, it seemed like the proper way to do. This way there is no matrix-vector multiplication left to do in the fragment shader. Enters image based lighting: for each fragment we want to fetch from an image, typically one or more cube map textures, the light intensity coming from certain directions. I assume this data is in world space. My question is: is it possible to use IBL while retaining the vertex shader transform optimization mentioned above? 

Appendix Let's describe the problem at hand. As an input, we have an environment map: it's a set of six textures forming a cube, that represents the light coming from all directions. The light is assumed to be coming from sources far enough that, at our scale, it's as if they were infinitely far. An environment map typically looks like the left figure of this illustration from the GPU Gems article: 

Without looking at the code, from the look of the pictures, my intuition would be that somewhere the code is using the inverse of the matrix it should. Looking at the code seems to confirm so, but there are some other problems too. 

It is fairly common to set texture coordinates dynamically in the vertex shader, so you should be able to do so too. 

If the compiler can do such optimizations, how good are they? Can we explicitly tell the compiler to pack or not, and when should we? 

If you organize your scene in a spatial structure (the usual way being a Bounding Volume Hierarchy), you can use a sort of virtual scene (I am making up this term, in reference to virtual textures). A memory manager would keep only a limited number of bounding boxes loaded at a time, and abstract the operation consisting in retrieving one. This way, a box would be loaded only as needed: when a ray hits a bounding box, the box get loaded to resolve the collision. Later on when another box needs to be loaded, the unused one is deleted to make room for the new one. With all these boxes getting loaded and deleted, the ray coherency would be a major factor in speed. I suppose a further improvement could be to defer loading, by reordering rays to treat first the boxes that are already loaded. 

Monte Carlo methods rely on the law of large numbers, which states that the average of a random event repeated a large number of times converges toward the expected value (if you flip a coin a gazillion times, on average you will obtain each side half the time). Monte Carlo integration uses that law to evaluate an integral by averaging a large number of random samples. Using a uniform distribution would break the algorithm because the law it is based upon wouldn't apply anymore. 

In fine, how relevant is this parameter, if at all? Do drivers actually take it into account, and if they do, in your experience how much does it impact performance in practice? Do you have data to share? I have written a thin graphics API abstraction layer meant to be implemented as either of the existing APIs, and it is tempting to just ignore this parameter altogether and hide it from the exposed abstraction. 

I don't see any problem with having a big area light. That said, it also depends on the scale of your scene. If the light is large compared to it, shadows will tend to be more diffuse, like under an overcast sky. 

It depends. On a desktop, if the function is always called with the same arguments, the shader compiler will probably optimize it so it's evaluated only once. On a mobile platform though, the compiler has less time to do optimizations and might miss it. Either way, you can confirm by looking at the generated assembly. Note that Aras Pranckevičius of Unity wrote a GLSL optimizer to address the problem of sub-optimal compilers on mobile: $URL$ That being said, regarding your sample code, I would definitely at least expose as a uniform. 

Perlin noise is just a base block, not very interesting by itself. You don't need to modify it, but to combine and filter it in interesting ways. Look at how to make fractal Brownian motion (fBm) with it for example, which combines octaves based on few parameters to get a richer texture. The question of terrain rendering is a difficult one and a topic of active research. Many generated terrains have this artificial and uninteresting look. This is exactly what the paper "Modeling Real-World Terrain with Exponentially Distributed Noise" tries to address, by proposing a formula based on statistics measured from actual terrain, which yields more convincing shapes. Another source of valuable information is Iñigo Quilez's website, where he mentions procedural generation a lot, including tricks to fake erosion. 

The layout declared in the shader must match the one declared through the OpenGL calls. The number of vertices is irrelevant on the shader side, but it must be consistent on the OpenGL declaration side. Unless you are using different indexes for the vertex attributes, you should have as many positions, normals, texture coordinates and face types. The following snippet from your question seems to be declaring face types for only three vertices though: 

First render the scene into a color buffer, without the distortion effect. Because it's not possible to both read from and write to a same frame buffer at the same time, copy it as is (blit) into another frame buffer. From now on, new draws will go into that new frame buffer. In the new frame buffer, draw a mesh with the shape of the effect, using a shader that will fetch the texture of the first color buffer, with an offset depending on the normal. Also note that: 

I am not familiar with , but from what I read, it seems to be limited to instanced rendering. You can find a simple example of binding in the examples of Dear ImGui. 

A fundamental assumption of deferred shading is that there will be only one surface, and therefore only one depth, at a given pixel. An effect that contradicts that assumption will require some sort of special handling in a deferred renderer. Translucency, because it allows to see through multiple layers, is such an effect and therefore will need its own rendering pass, or use some sort of compromise. 

In this context, "perfect mirror" refers to a perfectly flat surface with a 0% (pure black) diffuse color. It is still possible though to have a specular color: for example a Christmas tree ball behaves like a perfect mirror, but still tints the reflection. If you replace the 99% white color with a red tint, you will get the red Christmas ball look. In SmallPT, spheres have two colors: the emissive color and . The trick is that has two different meanings which depend on the material type : 

As an output, for a any point of a given object that we're rendering, we want to know the diffuse irradiance affecting it. What does that mean? A parenthesis Models describing how the perceived color of a surface varies depending the light reaching it, typically consist in integrating all incoming light over an hemisphere, with a weight that depends both on the direction of the light and of the observer. For opaque materials, it is common to consider two components contributing to that color: the diffuse and the specular. The diffuse is due to light slightly penetrating the surface, getting partly absorbed, then getting out in random directions. The specular is due to light bouncing off the surface without entering it. 

I see mainly three ways of computing normals for a generated shape. Analytic normals In some cases you have enough information about the surface to generate the normals. For example, the normal of any point on a sphere is trivial to compute. Put simply, when you know the derivative of the function, you also know the normal. If your case is narrow enough to allow you to use analytic normals, they will probably give the best result in terms of precision. The technique doesn't scale too well though: if you also need to handle cases where you cannot use analytic normals, it may be easier to keep the technique that handles the general case and drop the analytic one altogether. Vertex normals The cross product of two vectors gives a vector perpendicular to the plane they belong to. So getting the normal of a triangle is straightforward: 

Pick a size that is roughly the typical distance between two points. Use that size as a particle diameter. Render the point clouds using such large particles, to a depth buffer but not to the color buffer. Maybe offset that point cloud a little, so it's a bit farther from the camera. Render the point cloud using points this time, and using a depth test "closer or equal". 

1 For each texel of the generated diffuse cube map (which has $6$ faces times $32 \times 32$ texels), the diffuse equation combines all texels of the environment map (which has $6$ faces times $64 \times 64$ texels). Thus $32 \times 32 \times 6 \times 64 \times 64 \times 6$ as stated. Since the article considers SH with $9$ coefficients, and each coefficient needs to combine all texels of the environment map, that's $9 \times 64 \times 64 \times 6$. 2 "Bruteforce" doesn't necessarily refer to graph theory. In this article it refers to the naive approach of going through every single texel of the diffuse map and each time combine every single texel of the environment map. 

This method should be a lot simpler to implement than A, because there is no need to triangulate anymore. The result will not be perfect though: some points that should be visible might disappear, and some that shouldn't be visible will remain visible. Tweaking the particle diameter and offset will yield different results. Idea C: As a very cheap trick, just render the point cloud with a fog function. The farther points are from the camera, the less visible they are. This method won't hide any point, but still help reading the volume. 

Another possibility would be to define the orientation by having more information in the curve, but I haven't tried that approach as I was already satisfied with the result. 

The fact that a photo is stored in gamma space doesn't have to do with PNG or JPEG, it has to do with the fact that it's a photo. The camera detects a 50% intensity but is going to save it as a 187 instead of 127, because that is more efficient to keep the color information that matters to the human eye when you have only 256 different possible values. And so does Photoshop if you tell it so. For normals or displacement maps, you don't have this need so there is no reason to use gamma space for them (using gamma for them would actually be a mistake, since it would affect how precision is distributed). Thus when you decode a normal texture or a displacement texture, you don't need to apply gamma. If the normal map is encoded in a different space though, as can be the case for a G-Buffer for example, then you have to apply the corresponding correction. 

If multi-texturing is supported, texture coordinate generation will affect the active texture unit (quote from the documentation, emphasis is mine): 

As far as I understand, in a BRDF the Fresnel term is telling us the probability for a photon to be reflected or refracted when it hits a surface. The reflected photons will contribute to the specular term, while the refracted ones will contribute to the diffuse term. Therefore when determining, in a physically based manner, the contribution of a light to the color of material, I feel tempted to just write: 

Create a mesh from the point cloud. Render that mesh to a depth buffer but not to the color buffer. Render the point cloud using a depth test "closer or equal". 

An effect like that is most likely done as a second pass, after storing a copy of the color buffer. When looking carefully, you can see some of the vertices from the shape of the effect above the character. So this scene in particular probably does something like this: 

The edge between the sphere and the background is actually the one that is incorrect; you need to initialize your normal texture with a unit normal to get correct results. The two pixels thickness is a limitation of the Sobel based edge detection and other 3x3 convolution filters: you can only detect edges twice as big as your pixels. The Robert operator uses a 2x2 convolution filter with weights making a cross, so it yields thinner edges but they are still thicker than one pixel. If you absolutely want one pixel width, you could use filtered texture sampling and sample every half pixel when applying the Sobel operator, but the detection will probably miss some features. 

Although this part seems to be correct as far as I can tell, I would try to make sure there is no bug introducing a bias. For every bounce, it checks if there is a ray hitting the light first, and if so it breaks out of the loop: in case the code is incorrect and it happens to hit the light more often than it should, the shadows will disappear. Moreover, in the first if: 

(here it is on ShaderToy) Noisy, but not nearly as dark as your picture. It is very different scene, but considering the that the shape is similar (a box with one side open and only one light source), I would expect to get a similar luminosity/noise appearance. The rest of this answer assumes your code to be correct.