If it is a categorization problem then you should look for a classification algorithm, not a regression technique. The simplest classification algorithm is Logistic Regression. But by the looks of it, seems like you do not have a labelled data-set and if that's the case you should look for Clustering techniques. Clustering is a part of Unsupervised Learning Technique in ML which create clusters or groups of similar data points. 

Like Word2vec is not a single algorithm but combination of two, namely, CBOW and Skip-Gram model; is Doc2Vec also a combination of any such algorithms? Or is it an algorithm in itself? 

Without going into technical jargons, Collaborative Filtering Approach is of two types -- User-based Collaborative Filtering & Item-based Collaborative Filtering. In User-based Collaborative Filtering, let's say user/client A bought two items X and Y and user/client B bought the same items X and Y. So these two users are similar to each other based on their purchase history. Now if the user/client B purchases a third item Z, Z will also be recommended to user/client A. In Item-based Collaborative Filtering, let's say a user/client A bought items W, Y and Z. User/client B bought W and Y. User/client C bought only Y. What will be recommended to user/client C? Answer is item W because item W was co-purchased with item Y by user/client A and B. These comes under Nearest Neighbor techniques of Collaborative Filtering because we are trying to classify a user or an item by determining its nearest neighbor. An algorithm known as kNN (k-nearest Neighbor) is used to do the task. There is one another technique known as Matrix Factorization as well. Hybrid Based Approach - Hybrid Based Approach, as name says, is a combination of Content Based Approach and Collaborative Filtering Approach. It is used largely used by Netflix and Amazon to give best recommendations. 

In order to draw conclusions from this one building's data on other buildings on the campus, you would need a set of parameters shared between the building you know about and the buildings you want to predict for. For example, if the monitored building is of a male dormitory type, perhaps you can infer something for male dormitories; if the building is at location X, perhaps you can infer something for buildings at or close to location X. You'd need as many as possible of such additional columns to extrapolate knowledge to other buildings. As for visualizing/predicting resource usage of that single building, I suggest you try data mining tool Orange, which I find phenomenal for prototyping machine learning workflows. It also has a time series add-on, which I can't vouch for, but it does seem to have some forecast capabilities. 

It appears those numbers are Unix timestamps. The numbers you quote correspond to the following human-readable dates in ISO format: 

You can use with . It will return a a matrix some models can work with. Your matrix will indeed be 100e6 columns wide, but not densely populated won't take a lot of RAM. 

Indeed is a parameter for Tree Booster. There are 4 choices, namely, , , and . The default is set to which heuristically chooses a faster algorithm based on the size of your dataset. Approximate algorithm () is chosen in your case since your dataset is huge. To change the tree construction algorithm, you have to pass in the function as follows: 

Popularity Based Approach - Popularity Based Approach will recommend items, in your case, mutual funds, based on the total count of purchases. It will rank the mutual fund with maximum purchase in the past at the top. The item at the top will be recommended to every user/client. 

I have a data set which is distributed across various web services. The problem statement given to me is 1) Is it possible to determine which pregnant women are prone to an abortion? 2) Is it possible to determine which pregnant women are least likely to go to a hospital for delivery? 3) Is is possible to determine which pregnant women are most likely to give birth to an under weight child? I have made a comprehensive list of all the web services and the attributes within and it is given below: 

Perhaps have a look at MDS (multi-dimensional scaling) widget, Line Chart widget (from Timeseries add-on), or Network Explorer widget (from Networks add-on). 

Don't compress files that are already compressed (like how JPEG/PNG images or video files are). Their inherent compression is usually good enough, and you only trade some 5% in lower compressed size for a much lengthier and often non-seekable decompression that results in using twice the disk space. If you need to batch the files together, just use tar. 

Not only the naming has changed, also the individual widget settings and layouts are completely different. Some Orange 2 widgets are not even available in Orange 3 yet, or have been consolidated into other ones. I'm afraid Orange 2 workflows are just not compatible with Orange 3. 

I don't think Orange was ever intended to be used that way, but if you can convert your data into , you might be able to instantiate the widgets you need with it. See the block at the bottom of most widgets for some example. 

Popularity Based Approach Content Based Approach Collaborative Filtering Based Approach which composes of Nearest Neighbor and Matrix Factorization Hybrid Approach 

I'm building an NLP question-answering application using Doc2Vec technique in package of Python. My training questions is very small, only 20 documents and I am getting very inaccurate and different similarities even for same document while running at multiple instances. Almost all the sources which I referred trained data set containing thousands of documents. So I infer the reason behind my model's inaccuracy is the size of my data set. Is there any way to improve the similarity between documents, maybe by changing parameters or feature engineering? If yes, what are those parameters and by what ratio should I change them? If no, what are other ways or perhaps other neural network models to tackle the problem? 

I am trying to implement Doc2Vec model to convert a corpus into vectors using a pre-trained model (). I want to return the resultant vectors and save it in a text file. This is my code: 

This is not Orange-specific, but IIUC, you could preprocess your data (e.g. in Python or Excel) to have each of the 10 coughs pertaining to a patient on the same patients line. Thus you would have: 100 rows of patients with each row (10*170 + other patient data) attributes wide. 

In Orange documentation - Loading your Data, it says to prefix your column with to be marked as instance weights column. 

If you can trancode the data into a regular CSV file, then Orange can open it. Orange is a visual programming tool for data science. It includes several classication and regression algorithms which you can use to easily predict any unlabeled data you might have. There are also widgets for clustering and cluster analysis, which you may find useful (e.g. find if there is a group of products that better fits together than rest). 

Python package Orange3-Associate, which contains functions for mining association rules and seems to be what you are referring to, should be able to be installed on Anaconda's Python distribution with Python's internal command, i.e.