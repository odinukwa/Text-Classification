What you describe is a non-stochastic version of the "functional multi-arm bandit problem": you know you have an unknown function from some class C (does not have to be randomly selected), and you have query access to this function. The goal is to find the element which maximizes the function. As you say, depending on the class C, this may or may not require learning the function. This paper by Amin, Kearns, and Syed exactly characterizes the query complexity of the problem: $URL$ 

To avoid the possibly sticky issue of the max-cut taking value $0$, I will allow that $\sum_{e \in E}w(e) > 0$, and/or be satisfied with algorithms that result in small additive error in addition to a multiplicative factor approximation. 

Let $G = (V, E, w)$ be a graph with weight function $w:E\rightarrow \mathbb{R}$. The max-cut problem is to find: $$\arg\max_{S \subset V} \sum_{(u,v) \in E : u \in S, v \not \in S}w(u,v)$$ If the weight function is non-negative (i.e. $w(e) \geq 0$ for all $e \in E$), then there are many extremely simple 2-approximations for max-cut. For example, we can: 

At some level all NP-complete questions are "equivalent" (sure they have variously different approximation hardness properties!) but it seems intuitively a bit unobvious that the optimization versions of each of them can't be written in this form though one of them can trivially be! 

$$min_{N} [ \{ N \vert Pr_{\{N\}} [ \vert p(D) - p'(D,N,A) \vert \leq \epsilon ] ] \geq 1- \delta \} ]$$ Has such a thing been defined or computed or studied? (...maybe one would very optimistically also want to put in a ``$min_A$" over the whole expression if in any restricted scenario that too is possible to account for and hence making the quantity truly intrinsic to $D$!..) (..I can imagine such a question coming up with say "topic modelling" where one can sample only the "word" random variables (the "views") and under certain conditions one can recover the joint distribution of the topics and the views..) 

What you want to do is called "Private Set Intersection". You can think of Alice and Bob as each holding sets (the indices for which their strings are "1"), and they want to compute the intersection (the bitwise AND) so that neither of them learns anything about the other's set except what is implied by the intersection itself. This problem is well studied. See, for example, Freedman, Nissim, and Pinkas: $URL$ 

Hm, its tough even to think of examples of claims about TCS that make it to the popular press. One thing I have seen occasionally is the claim that factoring is NP-hard, when explaining cryptography. This is related to the less innocuous error of claiming that quantum computers can solve NP hard problems, but restricted to the context of cryptography, this is a relatively mild error. The point is just that we (users of cryptography) seem to believe that there is no efficient algorithm for solving the problem. The particular conjectures we use to justify this assertion are besides the point. 

Not sure if its immediately related but putting this out here : Is there any obvious reason why this very famous paper $URL$ does not deal with Max-Clique? Isn't it true that the complement of the Max-Independent-Set integrality gap instance does not give the integrality gap instance of Max-Clique? 

Not a direct answer but something : He is mentioned 19 times in these lecture notes of Scott Aaronson, $URL$ That says something, I guess? :D 

Is there any sense in which one can call Ramanujan graphs to be the "optimal" spectral sparsifiers? (Reference : $URL$ ) (..At least w.r.t $K_n$ can one say that given any $d$ and $n$, any $d-$regular Ramanujan graph on $n$ vertices, say $R_{(d,n)}$, gives the smallest $a$ s.t $x^T L_{K_{n}}x \leq \frac{n}{d} x^TL_{R_{(d,n)}}x \leq a x^TL_{K_{n} }x , \forall x \in \mathbb{R}^n$?..or some such similar inequality?.. ) Does the usual Cheeger's inequality become sharper if restricted to Ramanujan graphs? Or do we otherwise know of graphs which either saturate the Cheeger's inequality and/or maximize any of the combinatorial notions of expansion? The only kind of connection I know of between the spectral gap of an expander to its combinatorial expansion are statements like Theorem 4 and 6 here in these notes, $URL$ But even this is a lower bound and not an upperbound. One can put in $\lambda = 2\sqrt{d-1}$ in these two theorems but that somehow looks too weak and naive. I wonder if something sharper can be said about the combinatorial expansion properties of a Ramanujan expander : may be some sense in which one can justify calling them optimal from this point of view too? 

In addition to some of the great people listed in the comments, Gregory Chaitin independently developed much of Kolmogorov complexity while he was a highschool student in New York city. 

Along the lines of Jeff's answer, two algorithms are similar if the author of one of them expects that the author of the other one might be reviewing her paper. But joking aside, in the theory community, I would say that what problem algorithm A is solving is rather tangental to whether it is "similar" to algorithm B, which might be solving a completely different problem. A is similar to B if it "works" because of the same main theoretical idea. For example, is the main idea in both algorithms that you can project the data into a much lower dimensional space, preserve norms with the Johnson-Lindenstrauss lemma, and then do a brute-force search? Then your algorithm is similar to other algorithms that do this, no matter what problem you are solving. There are some small number of heavy-duty algorithmic techniques that can be used to solve a wide variety of problems, and I would think that these techniques form the centroids of many sets of "similar" algorithms. 

The closest thing I have found in this regard are things like Theorem 1 (page 12) , Theorem 2 and Theorem 3 (page 14) in this paper, $URL$ But then I am not understanding as to why these would be called "sample complexity" because these theorems don't seem to tell me any lower bounds on the number of samples needed to learn the tensors they are looking at. What one can do at best is to take the RHS of the equations proved in these three theorems and ask for that to be upper-bounded by some error tolerance and hence invert that to get a lower-bound on the number of samples needed. But this is a-priori not the same thing as finding out the minimal number of samples needed to get that accuracy. Am I missing something? 

The relationship between these sets is: $$PN \subset MN \subset CE \subset CCE$$ We can consider the price of anarchy over any one of these solution concepts: the worst case social welfare for any profile in the set, divided by the optimal social welfare: $$POA(S) = \max_{s \in S}\frac{COST(s)}{OPT}$$ So, by the above containments: $$POA(PN) \leq POA(MN) \leq POA(CE) \leq POA(CCE)$$ My question: are their known bounds on how fast this quantity can grow? It is possible to have a game with $POA(PN)$ finite, but $POA(CCE)$ unboundedly large. But if I know $POA(PN)$ is finite, does $POA(MN)$ also have to be finite? $POA(CE)$? How much larger can they be? 

Yup. Everything follows from duality. (I am only half joking). A partial list: Boosting The Hard-Core Lemma Online Learning The ability to actually solve LPs efficiently A large fraction of approximation algorithms results. Much more To develop algorithms, you often need a constructive or algorithmic version of the duality theorem (which is essentially equivalent to Von-Neumman's min-max theorem in game theory -- the applications in game theory are also huge). This is essentially what the multiplicative weights algorithm is. See Arora/Hazan/Kale for the best survey on this topic: $URL$ 

In the other situation when in your notation $k>>d$, in one of our papers we did analyze one such autoencoder in details to be able to say something about what you are asking. Does our Theorem 3.2 here, $URL$ help? It took us about 15 pages of laborious calculation to get this - hopefully its helpful! :D The "??" in the statement is a LaTeX error. It was meant to refer to the previous Theorem 3.1. 

Not sure if this is the kind of thing you want but here is something I find fascinating : This particular breakthrough paper's proof technique is in a sense "random matrix theory", $URL$ (..I dont know if before this paper anyone had guessed that random matrix is the way of think of this question..) I feel it is a deeply important question to be able to (a) come up with a version of this which shows this existential result restricted to only simple graphs and (b) come up with a deterministic polynomial time algorithm (or prove that such a thing can't exist!) which makes this existential argument constructive in the case of simple graphs. This isnt clear for even just bipartite graphs! 

I am wondering if the following problem has a name, or any results related to it. Let $G = (V,w)$ be a weighted graph where $w(u,v)$ denotes the weight of the edge between $u$ and $v$, and for all $u,v \in V$, $w(u,v) \in [-1,1]$. The problem is to find a subset of vertices that maximizes the sum of the weights of the edges adjacent to them: $$\max_{S \subseteq V} \sum_{(u,v) : u \in S\ \textrm{or}\ v\in S} w(u,v)$$ Note that I am counting edges both that are inside the subset and that are outside the subset, which is what distinguishes this problem from max-cut. However, even if both $u$ and $v$ are in $S$, I only want to count the edge $(u,v)$ once (rather than twice), which is what distinguishes the objective from merely being the sum of the degrees. Note that the problem is trivial if all edge weights are non-negative -- simply take the whole graph!