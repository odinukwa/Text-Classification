Here are a few possible solutions to your problem: Option 1: Don't rebuild that index That's a 30 GB index you have there. What measurable performance problem are you trying to solve by rebuilding it? Especially at 5% fragmentation, this seems like an incredibly expensive operation (in terms of system resources and locking) for very little gain. You can read some very well-founded opinions on why you might want to give up on the index rebuild here: 

Plain select is on the left at 332 ms, while the nolock version is on the right at 383 (51ms longer). You can also see that the two code paths differ in this way: 

Modify that script so that it points to the desired location of your primary data file (mdf) and your deploy should go much more smoothly. Note that is set by the settings of the target instance: 

Note: I wrote this answer before I realized this was on Azure SQL DB (David's answer seems to indicate this is legit buggy behavior in Azure). Leaving here as the OP was also looking for an explanation about what that error meant, and maybe it will be helpful for others in an on-prem setup with this error. 

If your mdf and ldf files are in a non-default location (or have a non-default prefix / name), you need to add that information to the SSDT project. This will help the schema compare operation (that occurs when you do a publish) to see that the files are in the right place and don't need to be dropped / recreated. Right-click your project -> Add New Item -> File Group File You'll get something like this: 

Next Steps Since it seems like the disk is reasonably fast (per the benchmarks you shared), I think it would be a good idea to log the contents of before and after the nightly batch job you mentioned. This will tell you how much I/O is happening on tempdb during that process. This is important, because maybe there really is more I/O than the disk can handle. So here's what you do: 

But according to the failover docs, a sync mode AG guarantees zero data loss during a manual failover 

If this is a default (not named) instance, you can ignore the part. If the files you want to be deleted are not in this folder, and don't have a .trn extension, then they won't be deleted by the cleanup step. Something is "touching" your log backup files The extended stored procedure date parameter deletes based on the "modified date" metadata. If some other application has "touched" these files, causing Windows to update the modified date to a time inside the cutoff, they won't be deleted. There is a permissions issue The account that's running SQL Server Agent Services should have full control on the folder where backups are to be deleted. Make sure this hasn't been changed. Validation checks are failing or other problems occurred I noticed you have - you should check to confirm there are no error messages being reported during these times. 

Fantastic job adding those details to your question, that was really helpful. It sounds like what you were running into is what Shawn Melton mentions in his answer here. In short, events (queries, in this case) are only written to that file when the (defaults to 30 seconds) or (defaults to 4 MB) is reached. For what it's worth, I have experienced that is not always enforced (i.e., if the buffer limit is not reached, the file is not updated - even if it's been longer than 30 seconds). However, as Shawn mentioned, stopping the event session flushes the entire buffer of events into the file. 

Make sure to give some thought to the "target size" you use with SHRINKFILE if you do this. You say it's only 10 GB of used space, but I wouldn't immediately set it to 10 GB - you are putting some data into this database, right? 

Two solutions were mentioned in the comments to your question. Let's review both of them. Use a READ_ONLY cursor 

So it has to do the key lookups to check and see if the rows you've read in have been modified. The solution proposed in the blog post, and by Jacob H, is to use a more restrictive cursor type (such as READ_ONLY) in order to avoid these extra key lookups entirely. Optimize the index 

This shows that there is some branching in the method based on the isolation level / nolock hint. Why does the branch take longer? The nolock branch actually spends less time calling into (25ms less). But the code directly in (not including methods it calls AKA the "Exc" column) runs for 63ms - which is the majority of the difference (63 - 25 = 38ms net increase in CPU time). So what's the other 13ms (51ms total - 38ms accounted for so far) of overhead in ? Interface dispatch I thought this was more of a curiosity than anything, but the nolock version appears to incur some interface dispatch overhead by calling into the Windows API method via - a total of 17ms. The plain select appears to not go through the interface, so it never hits the stub, and only spends 6ms on (a difference of 11ms). You can see this above in the first screenshot. I should probably run this trace again with more iterations of the query, I think there are some small things, like hardware interrupts, that were not picked up by the 1ms sample rate of PerfView 

So the difference is 51ms. This is pretty dead on with your difference (~50ms). My numbers are slightly higher overall because of the profiler sampling overhead. Finding the difference Here's a side-by-side comparison showing that the 51ms difference is in the method in sqlmin.dll: 

then by all means, shrink it. Maybe there was a typo in the filesize when this database was first created (someone tacked a 0 onto the end of "37" - poof, 370 GB database file). Maybe there was a one-time data load / deletion that will never happen again that caused the DB to grow to this size. 

If you used that template, it should definitely be logging the queries you ran from SSMS. Extended Event sessions don't start by default when you create them - you need to tell them to start. Perhaps you did that on the other server, but not in this second case? You can check to see if the session is running by look at the results of: