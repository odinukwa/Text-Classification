Find a low-dimensional emdedding of the images using a nonlinear dimensionality reduction method like autoencoding, then select the image closest to the centroid in the embedding space. 

I would use a regular clustering algorithm and replace the objective function, which is usually the MSE, with a differentiable loss function of your choice. Another way is to learn an embedding that optimizes your similarity metric using a neural network and just cluster that. If you would rather do similarity-based clustering, here are some papers: 

Hot encode the categorical variables and use Bernoulli naive Bayes. Hot encoding is usually the trick one uses in representing categorical variables. 

I removed the gradient noise (which did not seem to help, at least as you did it) and replaced your momentum optimizer with Adam using the default hyperparameters and it just worked. After 10,000 epochs I got a loss of ~8 with bcdefghijklabcdefghijklabcdefghijklabcde (actual) vs ccdeeffgghhiccdeeffgghhiccdeeffgghhiccde (predicted). The moral is that optimizing neural networks is still an art. 

You should look more into the infrastructure side of things if you don't like maths. The lower you go in the software stack, the further away you get from maths (of the data science sort). In other words, you could build the foundation that others will use to create the tools that will serve analysts. Think of companies like Cloudera, MapR, Databricks, etc. Skills that will come in handy are distributed systems and database design. You are not going to be become a data scientist without maths; that's a ridiculous notion! 

In natural language processing, this first task is called stop word removal. You can identify them by looking at the words' frequency over the documents; uninformative words appear very often. 

Yes, that is a reasonable approach. Also try neural network based representations such as doc2vec. I suppose you know how to do the classification part? 

The shared depth between foo1 and foo2 and 0, discounting the root. For foo1 and foo4 it is 2. For foo2 and foo3 it is 1. You can define the similarity as a transformation of this through the function $f:x \to 1-\exp(-ax)$, where $a$ is a parameter you can use to tweak the clusters. For implementation, in python, you can try sklearn. General You can featurize a path string by creating a set from its parent folders, which can then be represented as a sparse bit string by hashing the set elements. For example, "a/b/c/d/foo1" becomes ("c", "c/d"), if we let "a/b" be the root, as before. (The notion of a root is not strictly necessary here except to ensure that the baseline similarity is zero.) The path similarity is simply then the set similarity or, after conversion to bit strings, the $L_p$ distance. 

Let's define the centrality of a vertex as proportional to the sum of its neighbors' centralities. If you write it out and incorporate the adjacency matrix, the eigendecomposition emerges immediately with the proportional constant as the reciprocal of the eigenvalue. The relevance of the eigenvector is that the centrality is defined through it: the score of a vertex is the corresponding entry on the first eigenvector. We had to choose the first eigenvector because the adjacency matrix is non-negative and we want the centralities to be so too, due to the Perron-Frobenius theorem (see these lectures notes for details). So if our centralities are intrinsically related to the transition matrix' eigenvectors, how do we find them? By using the power method, which relies on their fixed point nature! If you transform an eigenvector, you get a collinear vector (the property you wanted to relate), so why not do this with a random estimate of it until convergence, normalizing as we go along? Furthermore, if we slightly reformulate the problem to use stochastic matrices, the scores are directly interpretable as probabilities of random walks terminating at the corresponding vertex! If you're really curious, there's a monograph: Google's PageRank and Beyond. Welcome to the site. 

I haven't seen anything like this before but it seems quite feasible. You need an ontology to separate the main concept into its subconcepts, then you need a classifier to distinguish between your broader categories; description, methodology, classifier, application, and example. That is, I would manually label some transcripts at the paragraph level. If you don't have paragraph-segmented text, smooth the classification probability over the sentences so they share the same label. Or use a paragraph segmentation model as in Automatic Paragraph Segmentation with Lexical and Prosodic Features. The classifier could be a CRF or an RNN. The modern way to induce the ontology would be through word embeddings; cf. e.g, Learning Semantic Hierarchies via Word Embeddings. Formerly I would have recommended a hierarchical topic model, such as hLDA; cf. e.g, Non-Parametric Estimation of Topic Hierarchies from Texts with Hierarchical Dirichlet Processes or Unsupervised Terminological Ontology Learning based on Hierarchical Topic Modeling Welcome to the site and good luck! 

I would use a parallelized Gaussian process regression model. The next points to query in this framework are given by the posterior extrema or those of maximum expected improvement. Here's an implementation. 

Business intelligence is perfect for you; you already have the business background. If you want to become a bona fide data scientist brush up on your computer science, linear algebra, and statistics. I consider these the bare essentials. I don't know about Scandinavia, but in the U.S., data science covers a broad spectrum of tasks ranging from full-time software development to full-time data analysis, often with domain expertise required in various niches, such as experimental design. You have to decide where your strengths and interests lie to pick a position on this spectrum, and prepare accordingly. Useful activities include participating in Kaggle competitions, and contributing to open source data science libraries. 

This is a great problem. Rain affects the image through a localized lensing effect, so I'd partition the image into patches big enough to capture a raindrop, then run a classifier on each patch. Training data is not an issue since you say you have video. I'm not sure how to capture this lensing effect, but my intuition would be to use a 2D FFT or wavelet decomposition to extract the raw features your CNN will run on top off. If this does not work well enough, I suggest asking about what kind of features to use to detect lensing on physics.SE. 

Treat the installed software as categorical variables, and train a binary classifier such as logistic regression using training data, if you have it. If you don't, there is nothing you can do. You could create derived categorical variables from the company and type of software product, etc. 

Apache Spark can do it, using the new MLLib library. Here's a presentation, and here are some benchmarks. Bindings are available for python, scala, and java. 

In high accuracy regimes (>0.9), I look at the error rate -- the complement of the accuracy -- or its reciprocal; the mean time between errors. This captures the intuition that an accuracy of 0.99 is ten times as "good" as an accuracy of 0.9. 

Any difference in regression models can be reduced to differences in the latent model (e.g., linear vs. exponential), regularizer (e.g., $L^p$ norm), and loss function. So you can have subtle differences by keeping some of these three parameters fixed while modifying the rest. My understanding of a BI trend line is that it assumes an affine latent model without saying anything about the regularizer or loss function (though I'd assume it's the MSE unless stated otherwise). In the data science world, you should also state what loss function and regularizer you used if you want to be clear. 

I don't see the problem. All you need is a learner to map a bit string as long as the total number of contestants, representing the subset who are taking part, to another bit string (with only one bit set) representing the winner, or a ranked list, if you want them all (assuming you have the whole list in your training data). In the latter case you would have a learning-to-rank problem. If the contestant landscape can change it would help to find a vector space embedding for them so you can use the previous embeddings as an initial guess and rank anyone, even hypothetical, given their vector representation. As the number of users increases the embedding should stabilize and retraining should become less costly. The question is how to find the embedding, of course. If you have a lot of training data, you could probably find a randomized one along with the ranking function. If you don't, you would have to generate the embedding by some algorithm and estimate only the ranking function. I have not faced your problem before so I can't direct you to a particular paper, but the recent NLP literature should give you some inspiration, e.g. this. I still think it is feasible. 

They are not related in any meaningful sense. Sure, you can use them both to extract features, or do any number of things, but the same can be said about a many techniques. I would have asked "what kind of neural network?" to see if the interviewer had something specific in mind. 

Your problem is studied under the rubric of "image (Quality) assessment" and, more specifically, "document image assessment". Here are links to and abstracts from some relevant surveys: Document Image Quality Assessment: A Brief Survey 

Data engineering is infrastructure work; maintaining "big data" pipelines from ingestion to output. Today you might be expected to know things like SQL, Hadoop, Spark, Docker, and AWS. Data science is an umbrella term, so it can mean a lot of things, including data engineering. But it can also mean pure data analysis without any production work. It really depends on who's using the term; read the job description and ask the company for details. 

You could learn about word embeddings. These will provide you a natural path to topic models, and many other NLP tasks. Look up word2vec. I think it is better not to learn two new subjects (NLP and ML) at once. I would start with ML on its own; take this class perhaps. A good place to start is linear regression, or binary classification. 

Fork sklearn and implement it yourself! The linkage function is referenced in cluster/hierarchical.py as 

You do want to model the traffic, at least over a work day, otherwise it wouldn't matter what time you traveled! Absent any data, I'd assume there isn't much variance over the working week, but that's one thing the data will quickly confirm or refute. If it is varying, you can use a different model for each day. You have two variables; the departure times from home and work, respectively. Let's call them t_h and t_w. Let's call the commute time T_c(t), where t is the time of day. You can estimate this function from the data, so I'll assume it is given. You want to maximize c t_h - (1-c) t_w subject to the constraints t_h + T_c(t_h) < 9.5 and t_w > t_h + T_c(t_h) + 8 where c is a constant you can set to adjust the relative importance of leaving home early relative to leaving work early. You should be able to solve this numerical optimization problem with Mathematica, MATLAB, or something similar. I would not recommend Java; it's not meant for this. The only tricky part is estimating T_c. You know that it's a non-negative function, so you could use the standard trick of estimating it's logarithm (say, with kernels) and exponentiating. For implementation with Mathematica see Smoothing Data, Filling Missing Data, and Nonparametric Fitting and Constrained Optimization.