Install a temporary copy of 1.5.4 or later (or e.g. set up a Fedora 10 VM to use the copy there), use from 1.5.4 to back up the repository then load this into a new repository using with the 1.4.2 tools. This may be slow, and you'll also have to copy over any hooks etc. You'll also lose the server-side support for svn:mergeinfo attributes. Install a newer version of subversion on your machine. You can either build this yourself - getting a Fedora SRPM is a good start, although it will usually need a few fix-ups - or you can set up your system to update from RPMForge, either in its entirety or just for subversion and its dependencies - and install their up-to-date builds of 1.6.x. You'll get all the improvements of 1.6 and keep the svn:mergeinfo attribute support, but you'll no longer have a strictly-RHEL-5-compatible system (if you're bothered about that). For future note: as in the discussion below you must restart apache, e.g. , after updating the installed subversion. 

in /var/log/asterisk/full, and possibly on the asterisk console too - I forget. Here abcdef is the name of the inbound route you've configured and FROM_DID is our line number. Even if you don't get the FROM_DID, are all you numbers set up for the same incoming IAX2 route - can you distinguish by route? 

to build a script that'll hopefully generate the file for you. Untested, sorry - don't have apache to hand. You'll need to create the password file first. 

Try upgrading all your subversion clients to 1.6.13 or later - according to the issue ticket that's when this was fixed Patch your subversion server to permit OPTIONS at the repository root for all users. Here's my old patch to do this which permits the OPTIONS only and doesn't leak information about other project directories (I don't think). I didn't get accepted to subversion - I wasn't seriously proposing it, though - but we've run it on our SVN server for several years now. 

Unfortunately CentOS 5's subversion is quite old, 1.4.2, whereas Fedora 10's is newer, 1.5.4. The fsfs format must have changed between the two (look for a version file in the repository directory) and your older subversion on CentOS can't read the newer version generated on 1.5.4. You have two options: 

You probably want , or you can go to the clock control panel (e.g. right click on the clock in the task bar) and go to the 'Internet Time' tab. I think you need to edit the registry to alter the update frequency, though: 

You do not need to recompile anything. In fedora you can get the sqlite php module by installing the php-pdo module. The following should do the trick. 

This might already be what your doing but, here is a thought. If the contents of sample are only to be accessed via FTP then you can move that directory outside the web server's document root and use your 0775 with owner=user and group=nobody plan. The php script will be able to write, user will be able to use FTP and the outside world will not be able to get at sample through the web server. 

You should try the Django work without any restarting of apache. Most sever side environments work fine while doing development work without any need for stopping and starting the web server over and over again. But, yes you can run any number of apace instances as long as you make sure your second server's config file is pointing at different resources. Like: 

Port 25 is the standard port SMTP traffic runs on. If you intend for you system to be an email server than those might be legit servers trying to send you or your users email. If you do not intend your system to be an email server, figure out how to get port 25 turned off. Historically email servers would be configured to politely send on email for other servers. Today this is bad, bad, bad. It's called being an open email relay. It would be wise for you to verify that you are not doing this. But, don't go to far and try to block port 25 traffic if you do mean to accept email from the outside world. 

I'm assuming, possibly incorrectly that your users are sitting in from of the windows boxes and need access to some linux gui applications. VNC works very nicely. Most Linux distributions have a the server sides vnc x-server included these days. Get a compatible windows vnc client and you should be good. If you need some additional security look for the ssl-enabled vnc stacks that are starting to crop up. Getting a windows side xserver like the one in cygwin will also works but is more complicated and will be much harder to explain to windows users. If your users are already linux savy this won't be to bad, otherwise go with vnc. 

About serving static files. Yes, you can use a lighter web server to do this. But, before you go to the effort be sure that it will do you any good. Is apache really using resources you need elsewhere? Maybe just configure apache to not start quite so many child processes. Be sure the added complication will pay for itself, because down the road it will almost assuredly confuse somebody when they try to figure out how everything is working. 

into the alphabetically last subdirectory of . Try anywhere to help visualize this. Now, judging by the other comments you made, it appears you already shuffled stuff around further. Since the you ran is unlikely to really destroy any data, you should be able to move stuff back in place. If you have no statically linked rescue shell like available, you'll have to boot into a rescue system. From there you should be able to locate , , , etc. If you manage to put these back into your system should be bootable again. Think before taking further steps, it seems your recovery attempts so far only worsened the situation. 

If your setup is not working as intended after that, the output of will probably be helpful for further debugging. 

No idea what your sysadmin is trying to achieve with a loop mounted file for (and then using a journaling filesystem for it). There's plenty of space on your root partition , so I guess if you just you'll be fine until he's back. 

In a network where one of the authoritative nameservers sits on the border of the internal network, I use views and the directive: : 

If you're not modifying HTTP headers or similar in your application, you might be able to make the switch to fcgi without any code changes. What you gain by this is a much cleaner separation of the various elements that take play in this setup. The will be a lot more robust against any bugs (performance-, but also security-wise) that third party modules inflict. 

In the original DNS specification (RFC 1034/1035) there were two steps of cache invalidation that needed to take place, before a zone update was globally visible. Additionally to the already mentioned TTL expiry of caching resolvers around the world, you first needed to wait for (all) your secondary name server(s) to refresh the zone data from the primary's zone. Only after DNS NOTIFY (RFC 1996) was specified in the year of 1996, there was a standard way of promptly notifying all authoritative name servers about zone changes. So maybe the original phrase of "change propagation" was more appropriate at the time, since it was a two-step process. 

This is a bit of a shot in the dark, but: You're using which will only limit the heap portion of your memory. Since your process grew to ~ 50 GB memory usage, this could have been caused by a huge stack. So if your application spawns a lot of threads, heavily relies on recursion or stuff like that, this will be your cue. If that's the case, look into things like GC logging, , , , to analyze the situation and into , JVM parameters and ThreadPool sizes to remedy the problem. (Not entirely sure about the parameters, Java 5 was the last version I had to do with professionally.) 

The reason your seeing php as having been built without sqlite is so fedora can split the php package and thus not force a big string of dependencies on people who don't want them. For instance you need sqlite and thus likely you do not need postgresql. If fedora was to build the main mod_php application with all --with's turned on you would end up installing postgresql without needing or wanting it. This helps people concerned with both security (only install exactly the software needed) and people concerned with package download bandwidth. 

This might not be correct in your case. But, typically the answer to this question for a database application is that the database needs vacuumed, optimized or some such as is appropriate for the database in question. From what I can find it looks like mysql has an command. I'd look into that. 

I'd suggest using the expensive raid controller to do the bulk of the raid work. LSI cards and the software they come with works quite nicely. When properly configured, they will send you email when intereting things happen to the array. Like when disks fail. There is nothing wrong with either of the two linux software raid options, but you've gone out and purchased a somewhat fancy raid card. Let it do the work. Configure the disk array to expose one big device to Linux. If you would like to break up the final device into small volumes use lvm for that. One big physical volume, one big volume group and cut the volume group into whatever number of logical volumes you need. 

Nothing requires that ping be possible between two hosts. It might be that somebody between you and google is dropping ICMP packets. If everything else is working I'd not worry much about this. If you are particularly worried check with whoever runs your networking equipment or firewall and see if they are letting ICMP traffic through. Also check to see if you can ping anybody else in the outside world other than google? 

Be sure to try generating lots of different types of failed login attempts. Try from X (gdm or kdm or xdm), try from the console, try from ssh, try from sudo and try from su. Different subsystems can (and will) be configured different ways. It's not uncommon for ssh to be configured to use an internal login command that cuts around the /var/log/btmp business. Try the command as well. You might look in /var/log/secure to see if your failed login attempts are being stored there. But, I'm afraid I don't know the structure of the Debian log directory. Try on anything in the /var/log directory. It's quite likely that ssh is logging something someplace.