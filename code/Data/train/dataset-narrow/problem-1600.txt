No, there is no method for doing this. This is specifically why Zone transfer is to be disabled to untrusted DNS peers - To prevent discovery of the entire DNS topology. 

I'll echo what others are responding with, that proper high-availability isn't cheap and that if you're trying to cut corners with this, evidently HA isn't really important enough to your business. If it was, you'd be laying out the appropriate investment for the appropriate return. If you're a small shop and need this on less than 3 ESXi hosts per site, investigate the VMWare Essentials pricing model. It's very cheap (relative to the regular socket licences), and vMotion, FT, DRS and HA are available within the scheme. There are also license-up options if your requirements grow in the future. That said, you can still leverage your application/OS level clustering technologies inside VMs running on ESXi, as you would do with physical machines. I'd recommend you pursue clustering within the OS or apps to solve your problems. You're reasonably unlikely to find a hypervisor-level solution to your problem with ESXi, because VMWare specifically requests that vendors do not ship solutions for the free version which conflict with the licensed versions of ESXi. Classic example - Veeam used to support the free ESXi in their backup tools. VMWare contacted them and requested that they stop doing this, as the vBackup APIs are only exposed once a host is licensed. Veeam complied, and ESXi free support isn't available in Veeam backup any more. 

If you're after database performance in Amazon, you may find this article useful: $URL$ The general vibe from my own research (have been reading about this for a few weeks) is that EBS is the faster option. 

Given your requirements, NTBackup to staging disk space on the backup server, then backing up on that server directly to tape. Since you're backing up remote machines, you'll need to get enough staging space on the backup server to host the backup files. Streaming directly from the remote servers to tape won't be feasible because data throughput won't be high enough to keep up with the tape speed. 

Once you've figured out your tool chain/process for this, you may find you can schedule it to run without your intervention. I would imagine many other people would be very interested in a writeup, too! 

$URL$ offer a fully encrypted web connection for a monthly fee. They also have a free trial period to check it out. 

There are sites out there that serve 30,000 concurrent connections from a single web host, and there are sites that need hundreds of servers for the same load. You need to drill much deeper in terms of specifics and testing before you can answer this for your scenario. Read this: $URL$ It's asp specific but with the info you're currently running with, it applies equally. 

Ensure your choice of iSCSI target is on VMWare's HCLs. I've seen all kinds of problems from using OpenFiler to serve iSCSI to ESX. It doesn't work correctly and shouldn't be used. You may have more success with a Windows 2008 R2 server running MS' iSCSI target, although I'm not sure if the free version of that is on the HCL, either. 

To accurately plan capacity for this, you need to investigate the characteristics of the application and the load it'll place on the terminal. First off, if this is a critical line-of-business app that will lose the business money if it's not available for a few hours, you want to be looking at 2+ terminal servers running in parallell. Basic load-balancing of users across multiple terminal servers is actually really easy, you just do a round-robin DNS. The benefit here is that if one TS goes down for any reason, users that urgently require access can continue to access the system. My recommendation would be to look at 2 or 3 servers and ensure you have enough capacity in the environment to withstand the loss of one of the servers. As for capacity/load, check the amount of memory that a user's session occupies on the TS when they're running the app. Multiply that by the number of users you expect to accomodate, add maybe a Gb for the system's own use, and then add another 20% on top for comfort. That's how much RAM you need, as a starting point, to support that number of users running that app on your TS. You must calculate based on an actual user connected to an actual example TS session, because each user will occupy extra RAM for other user processes in addition to the app itself. Those extra little processes add up. Next, check your processing load and the characteristics of the app. Do users tell the app to run reports, which can peg the CPU at 100% for a short while? If so, big problem. Scaling that up to 60 users (even on a 16-core machine) means you'll have peak times where several people are trying to run reports and everyone's suffering. Also consider any extra apps required by the users of the system. It's fairly common for users to want to output from business apps to office applications like Excel. Are they going to be able to shuffle things via shared drives to accomplish this, or is there a requirement to run office on the Terminal Server itself. If so, you need to be aware that a) office is licensed entirely differently in a terminal environment and regular versions will not install. b) A couple of Excel sessions soon eat up all your RAM. tl;dr scale out across multiple servers rather than up within a single box 

Yep this is do-able. You just need to change the compatability mode for the installer to 'Windows XP'. No guarantee that you're not digging yourself into a hole with this, though, as future patches to your 2003 server are going to be layering on top of an unsupported state. If you're having issues connecting from 2003 to newer RDP servers, there are settings you can alter on the newer RDP hosts that will allow them to accept older clients that run lower levels of channel security. This would be the supported solution, if that's your issue. 

On the device look under Options -> Messages for 'Auto More'. However there used to be a 'hard' limit of around 40Kb on messages sent to a device. If the messages are over that amount, I'm not sure there's a fix. 

Option 1 seems the best move if you just want more available IPs. If you're doing this to segregate your network, Option 2 is the way to go... and I'd advise vLAN-ing off the new subnet + gateway IP, then migrating port-by-port into it. 

It's a dangerous for a real network because it's applying a narrow metric when you need to take a holistic approach. In graph theory, reliability is best when you can remove the most paths on the graph, and still retain a valid path between all vertices. Hence, a network where each vertex connects to all others with a dedicated paths is the most reliable. In real networks, that's just one of many factors. Reliability is best when your servers and workstations can happily communicate at acceptable speeds come rain or shine and that's determined by a lot more than simply 'how many links to other things do I have?' In real networks, you must take into account: 

We have some physical servers that we cannot virtualise because they are connected to analog modems for alert paging. We need to retain this functionality, but virtualise the servers and reduce the pile of individual modems down to something that can be rackmounted in a minimal number of U. So I guess I'm seeking a stack something like this: 

The reason I asked about the hardware above is because many GPS devices (stratum 0, the 'root' source) connect to the computer that then acts as the NTP server via a serial link. Serial connections often have 1-5ms jitter on the line due to signalling overheads/interrupt waits. Hence, I'm guessing your NTP source is reading from a serial source. There are some tweaks you may be able to perform on the serial connection to reduce jitter. Primaily, disabling FIFO may gain you decent results. $URL$ $URL$ 

As with any important system, you need to have a solid backup. If you get in trouble with this (or any other suggested process), you're going to need that backup. 

Can you outline some usage scenarios? The kind of box you need will differ depending on your usage. If it's host hosting SMB file shares the kind of array you require is going to be very different to hosting VMFS VMWare LUNs for 300 users... If you want a very customisable solution that scales well, you can build a SAN system from components, like so: 

The most reliable method of maintaining time in a virtualized domain environment is to keep your PDC physical, increase the sync frequency and drift tolerance on all your virtualized DC and domain members, and disable host-guest time synchronization for all VMs in question. Here's a pseudo-explanation of the problem (it's been a long time since I looked at this): When a machine (workstation, server, VM) starts up, it reads the time from the RTC (battery backed clock chip on the motherboard/BIOS), and works out the duration of each CPU tick. The OS then counts the number of clock ticks that have occurred since the initial reading was taken, and adds the time from that to the original time reading taken at boot. This gives you the current time. Problem is, hosts obfuscate the true clock cycles occurring from the VMs. A VM may have seen 100 clock cycles when 500 clock cycles have actually occurred on the host. So this method of calculating time breaks down, and time drifts out of whack on the VM. Host-Guest time synchronization via the installed vm tools/enhancements packages on vSphere and Hyper-V go some way to curing this, but they're not perfect (in some setups they can pull a VM forwards if it's drifted behind realtime, but they don't jump a VM forwards if it's drifted ahead of realtime). This complicated further by the way clock cycles are counted on multi-core setups (the timing counter is essentially emulated on each core) and on setups that can change clock speeds on the fly (I have no freaking idea how this is maintained). Factor in the idea that a VM can execute one clock cycle on one core, then jump to a different core on a different CPU for the next cycle, and it gets really horrible. So back to the original point: Domain time by default starts at the PDC, then trickles down to the other DCs, then out to the member servers and workstations from there. So if you ensure your PDC is a truly reliable time source (by keeping it physical), and disable host-guest sync on all other domain members, you'll ensure a stable and relatively accurate time infrastructure. Note that running your PDC as a physical server then enabling Hyper-V on that server and adding some guests to it is probably not a suitable fix either, as I believe that when you enable Hyper-V the 'base' OS actually becomes a virtualised OS as well (silently). So keep one physical server as a PDC, and keep Hyper-V off the box. Interesting side point to note: Microsoft's official stance on Windows Time Synchronization, even using the compliant NTP service that's been built into Windows since XP/2003, is 15 seconds. In practice, you can get it down to sub-100ms, but all they'll support is synchronization to within 15 seconds. Kinda makes sense, the only time-sensitive key component at the core of most MS environments is Kerberos, and by default that'll operate happily as long as you're within a 5 minute tolerance.