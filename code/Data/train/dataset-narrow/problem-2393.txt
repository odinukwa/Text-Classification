I do not think that the part of Wikipedia you quoted is talking about space complexity. It simply states that unlike non-tail calls, tail calls do not have to store the return addresses in the stack. However, it is not correct to state that tail calls do not touch the stack at all, because you still have to clean up local variables allocated on the stack and possibly push arguments passed to the function onto the stack, hence the parenthesized note. 

Edit: In revision 1, I wrote an embarrassingly complicated answer. The answer below is much simpler and stronger than the older answer. Edit: Even the “simplified” answer in revision 2 was more complicated than necessary. 

The question is somewhat open-ended, so I do not think that it can be answered completely. This is a partial answer. An easy observation is that many problems are uninteresting when we consider additive approximation. For example, traditionally the objective function of the Max-3SAT problem is the number of satisfied clauses. In this formulation, approximating Max-3SAT within an O(1) additive error is equivalent to solving Max-3SAT exactly, simply because the objective function can be scaled by copying the input formula many times. Multiplicative approximation is much more essential for the problems of this kind. [Edit: In earlier revision, I had used Independent Set as an example in the previous paragraph, but I changed it to Max-3SAT because Independent Set is not a good example to illustrate the difference between multiplicative approximation and additive approximation; approximating Independent Set even within an O(1) multiplicative factor is also NP-hard. In fact, a much stronger inapproximability for Independent Set is shown by Håstad [Has99].] But, as you said, additive approximation is interesting for the problems like bin packing, where we cannot scale the objective function. Moreover, we can often reformulate a problem so that additive approximation becomes interesting. For example, if the objective function of Max-3SAT is redefined as the ratio of the number of satisfied clauses to the total number of clauses (as is sometimes done), additive approximation becomes interesting. In this setting, additive approximation is not harder than multiplicative approximation in the sense that approximability within a multiplicative factor 1−ε (0<ε<1) implies approximability within an additive error ε, because the optimal value is always at most 1. An interesting fact (which seems to be unfortunately often overlooked) is that many inapproximability results prove the NP-completeness of certain gap problems which does not follow from the mere NP-hardness of multiplicative approximation (see also Petrank [Pet94] and Goldreich [Gol05, Section 3]). Continuing the example of Max-3SAT, it is a well-known result by Håstad [Has01] that it is NP-hard to approximate Max-3SAT within a constant multiplicative factor better than 7/8. This result alone does not seem to imply that it is NP-hard to approximate the ratio version of Max-3SAT within a constant additive error beyond some threshold. However, what Håstad [Has01] proves is stronger than the mere multiplicative inapproximability: he proves that the following promise problem is NP-complete for every constant 7/8<s<1: Gap-3SATs Instance: A CNF formula φ where each clause involves exactly three distinct variables. Yes-promise: φ is satisfiable. No-promise: No truth assignment satisfies more than s fraction of the clauses of φ. From this, we can conclude that it is NP-hard to approximate the ratio version of Max-3SAT within an additive error better than 1/8. On the other hand, the usual, simple random assignment gives approximation within an additive error 1/8. Therefore, the result by Håstad [Has01] does not only give the optimal multiplicative inapproximability for this problem but also the optimal additive inapproximability. I guess that there are many additive inapproximability results like this which do not appear explicitly in the literature. References [Gol05] Oded Goldreich. On promise problems (a survey in memory of Shimon Even [1935-2004]). Electronic Colloquium on Computational Complexity, Report TR05-018, Feb. 2005. $URL$ [Has99] Johan Håstad. Clique is hard to approximate within n1−ε. Acta Mathematica, 182(1):105–142, March 1999. $URL$ [Has01] Johan Håstad. Some optimal inapproximability results. Journal of the ACM, 48(4):798–859, July 2001. $URL$ [Pet94] Erez Petrank. The hardness of approximation: Gap location. Computational Complexity, 4(2):133–157, April 1994. $URL$ 

Note that each output bit depends on at most five input bits. I omit the proof of the correctness of the reduction, but the key idea (which I borrowed from [Dur94]) is that if φ is satisfiable and input bits x1, …, xn are set to a satisfying assignment of φ, then the m output bits z1, …, zm are constrained to have the even parity, and therefore the circuit cannot be a permutation. On the other hand, if input bits x1, …, xn are set to a non-satisfying assignment of φ, then output bits z1, …, zm can be set to anything; because of this, if φ is unsatisfiable, then the circuit is a permutation. Tractability On the tractable side, your problem is in P in case of NC02 circuits. This is shown as follows. In general, each output bit in a Boolean circuit for a permutation is balanced; i.e., exactly half of the input strings set the output bit to 1. However, every balanced Boolean function from {0,1}2 to {0,1} is affine; i.e., a copy of a single input bit, the XOR of the two input bits, or the negation of them. Therefore, we can first check that each output bit is balanced, and then check the bijectivity by the Gaussian elimination. I do not know the complexity in case of NC03 circuits or in case of NC04 circuits. References [Dur94] Bruno Durand. Inversion of 2D cellular automata: some complexity results. Theoretical Computer Science, 134(2):387–401, Nov. 1994. DOI: 10.1016/0304-3975(94)90244-5. 

Since you count games as an example of “physical intuition” while I cannot see anything related to physics in games, I assume that your emphasis is not on “physical” but on “intuition.” I argue that part of the purpose of study (education or research) in theoretical computer science is to develop the intuition for the abstract notions related to computation. Intuition is acquired by studying and getting familiar with the concept. I do not expect that there is a nice shortcut. For example, undergraduate students will be surprised by undecidability of halting problem (probably because the mere existence of an undecidable language is already surprising). But learning the fact, its proof, some related results and the wide applicability of the proof technique makes this surprising result less surprising and in fact very natural. I believe that the same is true for more complicated results. As for the specific result, I do not agree that there is no simple intuition for MA⊆AM. (Warning: I am currently studying this and related results myself, and I may say something incorrect.) In an MA system, Merlin has to give a single answer which fits most of the random sequences used by Arthur. We change the system so that Arthur sends several (polynomially many) random sequences to Merlin and Merlin has to give a single answer which fits all of them, which seems to me like a natural thing to try. Proving the soundness of this AM system is a simple application of the Chernoff bound. I do not think that anything in this result is conceptually difficult to understand. Marginally related: Your question reminded me of a beautiful blog post “Abstraction, intuition, and the ‘monad tutorial fallacy’” by Brent Yorgey, where he explained the difficulty of communicating the intuition by a fictional non-explanation “Monads are Burritos.” If the above explanation of how the proof of MA⊆AM works does not make any sense, I might be demonstrating the same fallacy. :(