Apparently the second of the above statements is either equivalent to or implies that if the $\lfloor \log (\frac {n}{2} )\rfloor-$bit Boolean function picked is such that it indeed cannot be matched on $(\frac{1}{2}+\frac {\epsilon}{3})-$fraction of the inputs by any depth $2$ linear threshold circuit of size $s \leq \frac{c \epsilon^3 n^{\frac {3}{2}} }{\log ^3 n}$ then with probability at least $1-\frac {\epsilon}{3}$ this function's truth table is the same as an $\frac{n}{2}$-bit string picked uniformly at random. I don't understand why! 

I have seen instances of how the technique of epsilon biased sets can be used to construct hard functions against a circuit class - like how in the recent paper of Kane-Williams this was used to construct a hard function for the class of circuits of the form Majority-of-two-layers-of-Threshold-gates. I was wondering if there is any past work where this object epsilon-biased-sets have been used as a proof technique for showing circuit lower bounds. (..I would be happy to get any reference along these lines that may strike your mind in relation to this...) 

The Razborov-Sherstov paper does use the Minsky-Pappert function as a tool in their final proof on page $16$ whereby they use its pattern matrix to create another matrix which is a submatrix of high sign-rank sitting inside the pattern matrix of their hard function from the "Main Result" quoted above. Its not clear to me if this is somehow implicitly also proving a high sign-rank for the Minsky-Pappert function itself. Is it? 

I was going through Les Valiant's seminal paper and I had a tough time with Proposition 4.3 on page 10 of the paper. I cannot see why is it the case that if there is a generator with certain values for $valG$ with a basis $\{(a_1,b_1) \ldots (a_r,b_r)\}$, then there exists some generator with same $valG$ values for any basis $\{(xa_1,yb_1) \ldots (xa_r,yb_r)\}$ ($1^{st} kind$) or $\{(xb_1,ya_1) \ldots (xb_r,ya_r) \}$ ($2^{nd} kind$) for any $x,y \in F$. Valiant points out the reason in preceding paragraph - namely the $1^{st}$ kind of transformation can be achieved by appending to every input or output node an edge of weight $1$. The $2^{nd}$ kind of transformation, Valiant says, can be achieved by appending to input or output nodes chains of length $2$ weighted by $x$ and $y$ respectively. I have not been really able to understand these statements. Maybe they are already clear, but still I cannot really see why the above construct helps achieve any realizable $valG$ values with one basis with the new basis which is one of the above types. Please help illuminate them to me. On a different note, are there some tensor free surveys for hologaphic algorithms available online. Most of them use tensors which, sadly, scare me :-( Best -Akash 

For a function $f : \{-1,1\}^n \rightarrow \mathbb{R}$ one can ask for its $l_0$ norm in the indicator basis i.e the number of vertices on which the function is non-zero. Does this sparsity parameter have any bearing (or the otherway) on how large can the largest non-zero term be in its Fourier expansion? 

(I am hugely editing the question. My initial question was if lowerbounds on threshold circuits say anything about P/NP and it seems that they dont. Irrespective of P/NP its an independently true fact that there exists Boolean functions which are exponentially hard for threshold circuits just that we have never seen them.) Let me ask my followup question in $3$ parts which I guess are related, 

If one looks at the space of all polynomial sized threshold circuits (at constant depth or not) then do we know of any natural complexity class in which they sit? The closest I know of is that the class of depth 3 threshold circuits with no weight restriction are known to be in $NP/poly$. What happens at higher or non-constant depths? Is there any class of circuits against which we know that there cannot be exponentially hard functions if $P=NP$? (Threshold circuits are clearly not of this type.) Are there reasons to believe that for each depth $d$ there exists a Boolean function which is easy for depth $d+1$ but exponentially hard for depth $d$? And if such functions are found then would it have any implications (separations) for the other usual complexity classes? 

You could have a look here EDIT I should have added that these are the lectures by Amitabha Bagchi at IIT Delhi 

Recently, I was going over an introduction to Holographic Algorithms. I came across some combinatorial objects called Pfaffians. I do not really know much about those at the moment and came across some surprising uses they can be put to. For instance, I came to know that they can be used to efficiently count the number of perfect matchings in planar graphs. Also, they can be used to count the number of possible tilings of a chessboard using 2*1 tiles. The tiling connection seemed very curious to me and I tried searching for more relevant materials on the web but in most places I merely found just one statement or two about the connection and nothing else. I just meant to ask if someone could suggest some reference to relevant literature as that would be really great and I am looking forward to study some related materials. 

How about the fact that computing permanent is #P-Complete but computing determinant - a way weirder operation happens to be in the class NC? This seems rather strange - it did not have to be that way (or maybe it did ;-) ) 

The most general theorem for PAC learning of Boolean functions that I am aware of is the theorem in section 3.4 of Ryan O'Donnel's book where its basically shown that Boolean functions whose Fourier weights are concentrated on some modes can be PAC learnt in polynomial time. Also it trivially follows that in $2^n$ queries one can learn any Boolean function on the $n-$Hamming cube. My question is two fold, 

If one wants to say minimize a function $f : \{-1,1\}^n \rightarrow \mathbb{R}$ on its domain then a degree$-d$ Lasserre relaxation of it would be to solve the problem of $\min \mathbb{E}_\mu [f(x)]$ over degree$-d$ pseudo-distributions $\mu$. 

My question is particularly about the set-up in section $8$ (``Analysis of the KV Max-Cut instances") of the paper, $URL$ 

Consider an iterative algorithm of the form $x^{t+1} = x^t - \eta g(x^t)$. (..if necessary feel free to assume that a function $L$ is explicitly known such that $g = \frac{\partial L}{\partial x}$..). Suppose that now I want to show that there exists a point $x^*$ within some pre-determined ball which is a fixed point for this iterative algorithm. 

This should really have been a comment, but for the lack of space I am posting this as an answer. Thanks for the answers and comments everyone. Recently, I came across another survey by Robin Thomas. You can find it here $URL$ Other than this, I would also add one statement about the tiling connection (which was pointed out to me by Prof Dana Randall). If you take the dual lattice, then 2x1 domino tiles are just edges. Therefore, a perfect tiling is precisely a perfect matching in the dual. Then, the theory of Pfaffians can be used to count perfect matchings in planar graphs. This means that you can just primarily focus on counting perfect matchings in the graph - the rest just follows trivially. 

Recently, I started (independent) learning of the theory of metric embeddings from the Fall 2003 course offered at CMU . I had a very basic question from the very first lecture of this course which I would like to get more intuition about. On page, $5$, the notes say that this technique can be used in a straightforward way to give an $\alpha$ approximation algorithms for problems like TSP if (say) the following hold. (i) The metric embeds into a tree. (ii) The embedding has distortion at most $\alpha$ What I am not sure about is whether the solution generated by using the embedding is even valid - because for all I know, it could be that the TSP solution on the tree uses only from among those edges which were contracted. To be more precise, I feel more comfortable accepting that if we have got a mapping $f$ from the original space $(X,d)$ to the tree metric $(V, d')$ which expands all the pairwise distances, then I can use the TSP solution on this tree as an approximate solution to the TSP problem on the original metric with approximation factor same as the expansion of the mapping $f$. I am not sure about how approximation factor can be the same as (or even related to) the distortion of the mapping $f$. Thanks -Akash 

If this is off-topic I will remove this. I guess people can just comment here rather than write "answers" so that I can eventually remove this question. 

Given a $f: \{-1,1\}^n \rightarrow \mathbb{R}$, I want to compute this quantity, $\sum_{ \hat{f}(S) \neq 0, S \subseteq 2^{[n]}} \vert S \vert $ i.e the sum of the sizes of the subsets of $[n]$ corresponding to the non-zero Fourier coefficients of the function. 

Do we know of instances of $Max-2-Lin(\mathbb{Z}_2)$ which have a integrality gaps w.r.t to high degree (> 4) SOS relaxations? 

I wanted to know as to how often are the submissions for ECCC cleared? Or do we know when is the next set of uploads going to be made on that site? Unlike arxiv where I know when my paper will go online, on ECCC there seems to be no way to know what is the timeline of my article. Last time I had submitted something on ECCC it took about 3 days for it go online. This time I am wondering as to why its taking longer. (..I had gone by this previous experience of 3 days to plan that I would refer to this online index number as reference for an upcoming talk of mine but now it seems I can't do that because the timeline looks indeterminate..) 

I am not sure if the following question falls within the scope of this site; if it does not, I will request the moderators to take appropriate action I have been going through Jin-Yi Cai's expository paper on Holographic algorithms. I had a question. Immediately after introducing Grassman-Plucker Identities for Pfaffians and the definitions for matchgates, recognizers and generators, Cai introduces signature tensor for matchgates. My question arises from the following statement. He states that a generator $\Gamma$ with $m$ output nodes is assigned a contravariant tensor $\mathbf{G} \in V_0^m$ of type $\binom {m} {0}$. I looked up contravariant tensors on wikipedia but did not find any reference to the type of contravariant tensors which Cai talks about. Maybe I am not searching properly, but I was unable to find a proper reference which defines what I am looking for. I would be glad if anyone could help me with this. Thanks -Akash 

Recently while working on a problem, I had to go through some of the literature on nested dissection. I happen to have one (maybe two?) questions related to the same. First, I will define a few relevant terms. These terms come up when we study the process of Gaussian elimination graph theoretically. Say, we have got a matrix $M$. With this matrix, we associate a directed graph $G_M = (V,E)$ where we have $V = \{v_1, v_2, \ldots, v_n\}$ where $v_i$ corresponds to row i and variable i and $(v_i,v_j) \in E$ iff $M_{ij} \neq 0$ and $i \neq j$. The elimination process may create some new non-zero elements in locations in $M$ which contained zeroes to begin with; the edges corresponding to these elements are called fill-in. In graph-theoretic terms, removing a vertex $v$ calls for addition of the following set $S_v$ of edges. $S_v = \{(u,w) | (u,v) \in E, (v,w) \in E, (u,w) \not\in E\}$ In order to make the elimination process efficient, we can target minimizing fill-in. By a result of Yannakakis, we know that fill-in minimization is a NP-Complete problem. It is easy to see that the value of fill-in depends on the ordering of vertices which leads to definition of a related parameter. An elimination ordering is a bijection $\mathbf{\alpha \colon \{1,2,\ldots, n\} \to V}$ and $\mathbf{G_{\alpha} = (V,E, \alpha)}$ is an ordered graph. Basically, this represents the order in which the vertices will be picked for deletion in the corresponding directed graph representation. Corresponding to different orderings, we get different values of fill-ins. The ordering which minimizes the fill-in size is called the minimum elimination ordering. And again we (of course) have that computing the minimum elimination ordering is NP-Complete. My question 

Is there any class of non-convex objective functions for which (stochastic) gradient descent can provably get to a local or a global minima? (..maybe in the approximate sense like a point such that the spectral norm of the Hessian is bounded by some epsilon..) 

A terminology issue about what is ``low degree" : Around page 13 here, $URL$ when this was shown for the Boolean hypercube graph the author claimed that this is a degree $4$ SOS certificate. This terminology is a bit peculiar to me. Because as one sees in the proof it uses non-negativity of the pseudo-expectation of polynomials of the form $(fg)^2$ where $f$ and $g$ are degree at most $d$ polynomials in $n-1$ variables. So I think this should have been called a degree $4d$ SOS certificate. But somehow they want to see this as a degree $4$ SOS proof in the space of Fourier coeffients because the product $fg$ is obviously degree $2$ over Fourier coefficients. I am not sure if this way of thinking makes sense! It would be helpful to know why this way of counting makes sense! 

I was and still am surprised by Euclid's algorithm. To me, it is a testament to power of human thinking - that people could conceive of such an algorithm so early (around 300 BC if I trust my memory). Fast forwarding, there is mind numbing literature on the subject. I think Scott Aaronson's list should be helpful in this regard - though, as Aaronson himself says its not complete (and not entirely theoretical) 

I do not know if this is a video lecture that everyone should watch, but I have decided on watching these to learn some Algebraic Topology. Seems pretty good to me so far. Algebraic Topology Lectures EDIT (Added Later). Another nice set of video lectures Here is another wonderful set of lectures by Erik Demaine. The course is called Planar Graph Algorithms and Beyond and is being taught by Erik Demaine, Shay Mozes, Christian Sommer and Siamak Tazari. They are also using some sections of Phil Kliens draft on Planar Graphs. I feel certain that it will make a great book after having seen the first two lectures.