A non-deterministic protocol for the inequality function is a protocol that behaves as follows: Alice and Bob get strings $x,y\in\{0,1\}^n$ respectively, and an untrusted prover is trying to convince them that $x \ne y$ by sending them some proof string $\pi$. The protocol accepts if both Alice and Bob accept $\pi$, and rejects otherwise. The prover should succeed in making the protocol accept if and only if indeed $x \ne y$. It is well-known that in every such protocol, the string $\pi$ must be of length at least $\log n$. The easiest way to see it is to observe that if this lower bound did not hold, then there would be a deterministic protocol for the equality function that transmits less than $n$ bits. This lower bound is tight, since the prover can choose $\pi$ to be a coordinate $i\in [n]$ suh that $x_i \ne y_i$. My question is whether the non-deterministic complexity of solving $k$ independent instances of inequality is at least $k\cdot\log(n)$? In other words, is there a direct-sum theorem for the non-deterministic communication complexity of inequality? Relevant work: Feder et. al. [FKNN95] proved a general direct-sum theorem for non-deterministic communication complexity. Unfortunately, this theorem only applies to functions whose complexity is $\gg log(n)$, and therefore does not apply to the inequality function. Karchmer et. al. [KKN95] gave an alternative proof for this theorem, which suffers from the same limitation. References: [FKNN95] Tom√°s Feder, Eyal Kushilevitz, Moni Naor, Noam Nisan, "Amortized Communication Complexity". SIAM J. Comput. 24(4): 736-750 (1995) [KKN95] Mauricio Karchmer, Eyal Kushilevitz, Noam Nisan, "Fractional Covers and Communication Complexity". SIAM J. Discrete Math. 8(1): 76-92 (1995) 

Then, clearly the communication complexity must be at least $\log(W/w)$ (since there must be at least $W/w$ monochromatic rectangles). The theorem you stated is just a different way to look at this argument. 

Here is my point of view, which I learned from Guy Kindler, though someone more experienced can probably give a better answer: Consider the linear space of functions $f: \{0,1\}^n\to\mathbb{R}$, and consider a linear operator of the form $\sigma_w$ (for $w\in\{0,1\}^n$), that maps a function $f(x)$ as above to the function $f(x+w)$. In many of the questions of TCS, there is an underlying need to analyze the effects that such operators have on certain functions. Now, the point is that the Fourier basis is the basis that diagonalizes all those operators at the same time, which makes the analysis of those operators much simpler. More generally, the Fourier basis diagonalizes the convolution operator, which also underlies many of those questions. Thus, Fourier analysis is likely to be effective whenever one needs to analyze those operators. By the way, Fourier analysis is just a special case of the representation theory of finite groups. This theory considers the more general space of functions $f:G\to \mathbb{C}$ where $G$ is a finite group, and operators of the form $\sigma_h$ (for $h\in G$) that map $f(x)$ to $f(x\cdot h)$, The theory then allows you to find a basis that makes the analysis of such operators easier - even though for general groups you don't get to actually diagonalize the operators. 

It is true that there is "no randomness" in the sense that the protocol is not randomized and is supposed to work on all inputs. However, that does not mean that we are not allowed to use probability distributions in the analysis of the algorithm. This theorem says that when you wish to prove a lower bound, then for the purpose of the analysis, you can take whatever distribution $\mu$ you like, and the communication complexity would be at least $\log(1/\delta)$, where $\delta$ is the largest probability of a monochromatic rectangle. If you do not like thinking about it in terms of probability, you can also think about it as a counting argument. Suppose you could prove that every monochromatic rectangle contains at most $t$ pairs. Then, this would imply that the communication complexity must be at least $\log\left(\frac{|X \times Y|}{t}\right)$ by simple counting. Now, the foregoing argument is exactly equivalent to applying the theorem you stated with $\mu$ being the uniform distribution and $\delta$ being $\frac{t}{|X \times Y|}$. The theorem generalizes this argument to work with probability distributions other than the uniform. In the case the distribution is not uniform, this is equivalent to augmenting the counting argument with weights. Suppose you assign each pair some weight such that 

Here are a few examples: There are the expander codes of Spielman (not to be confused with Sipser-Spielman): $URL$ There are the linear-time codes of Guruswami-Indyk: $URL$ There are the expander codes of Zemor: $URL$ 

My only background on this subject is a course I took as a grad student, so take my answer with a grain of salt: The reason that some methods transform the DAG to a tree because some algorithms work on trees but do not work on graphs with cycles. For example, belief propogation is guaranteed to converge on a tree after every node received two messages. On general graphs, on the other hand, no general convergence theorem is known, and we know that convergence occurs only in some special cases. Intuitively, the reason is that on a general graph, a message that goes through a cycle can loop in it indefinitely, each time changing the information stored at the nodes. Regarding under what circumstances it is recommended to transform the graph into a tree: In the process of transforming the graph into a tree, every node of the new tree is identified with a set of nodes of the original graph. The size of the maximal set with which a node is identified (in the best decomposition) is called the tree-width of the graph. Now, the complexity of belief propagation, as well as other algorithms, is exponential in the tree-width, so you want to transform the graph to a tree only if the tree-width is sufficiently small. Unfortuantely, it is $NP$-complete to compute the tree-width of a graph exactly, but there are some approximation algorithms. You can read more about it in Shiva Kintali's blog: $URL$ 

Also, this paper is somewhat older, but it is interesting and can help understanding the context to some of the previous papers. In addition, if I may recommend a paper of my own, I think this paper contains open problems that might be useful. 

In PAC learning, there is the "Occam razor" principle, which says that learning is qualitatively equivalent finding a succinct hypothesis that is consistent with the training samples. My question is whether there is a similar principle in Angluin's model in exact learning with membership and equivalence queries. In this model, the learner can either ask for the label of a sample of her choice, or ask for a counterexample for a hypothesis of her choice (if no counter-example exists, the verifier is told that the hypothesis is correct). I am not sure how an "Occam razor" for Angluin's model could even be defined, but ideally it would say that learning is equivalent to the ability to find a succinct hypothesis that is consistent with training samples, in some sense. 

It depends on what assumptions you are willing to make. Under certain hardness assumptions, namely $E \not\subseteq SIZE(2^{\varepsilon n})$, you get that $P = BPP$. This in particular implies that $BPP = ZPP$, and therefore that every language $L \in BPP$ is accepted by a Las Vegas machine (see "P=BPP unless E has Subexponential Circuits: Derandomizing the XOR Lemma", by Impagliazzo and Wigderson). You can also make a milder hardness assumption, namely, that $ZPE \not\subseteq \rm{io-DTIME}(2^{\varepsilon n})$, and get that $BPP = ZPP$ (see Lemma 46 in "In search of an easy witness: Exponential time vs. probabilistic polynomial time" by Impagliazzo, Kabanets and Wigderson). 

Such a bound is not possible. Consider the case where $f$ is the distribution that is uniform over some set $S$ of size $2^{\delta \cdot n}$, and let $\tilde{f}$ be the distribution that with probability $\delta$ outputs a uniformly distributed element of $S$, and otherwise outputs a uniformly distributed string. It is not hard to see that you can get from $f$ to $\tilde{f}$ you only need noise of at most $(1-\delta) \cdot 2^{-\delta \cdot n}$. However, $H(f)= \delta \cdot n$ while $H(\tilde{f}) \approx (1 - \delta + \delta^2) \cdot n$. Thus, you get a difference of $(1 - \delta)^2 \cdot n$ for arbitrarily small $\delta$ for an extremely low noise. In particular, you can set $\delta = \frac{\log(1/\varepsilon)}{n}$, and obtain noise $\varepsilon$ and entropy difference $\approx n - 2\log(1/\varepsilon)$. 

Suppose Alice has a distribution $\mu$ over a finite (but possibly very large) domain, such that the (Shannon) entropy of $\mu$ is upper bounded by an arbitrarily small constant $\varepsilon$. Alice draws a value $x$ from $\mu$, and then asks Bob (who knows $\mu$) to guess $x$. What is the success probability for Bob? If he is only allowed one guess, then one can lower bound this probability as follows: the entropy upper bounds the min-entropy, so there is an element that has probability of at least $2^{-\varepsilon}$. If Bob chooses this element as his guess, his success probability will be $2^{-\varepsilon}$. Now, suppose that Bob is allowed to make multiple guesses, say $t$ guesses, and Bob wins if one of his guesses is correct. Is there a guessing scheme that improves Bob's success probability? In particular, is it possible to show that Bob's failure probability decreases exponentially with $t$?