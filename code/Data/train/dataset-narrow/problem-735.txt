Increase innodb log files (innodb_log_file_size) - you can go up to 4GB or so for your logs (esimating based on zero information given on the amount of writes), Increase pIOPS capacity for the data drive Increase innodb_io_capacity (up to ~70-80% of pIOPS capacity) 

It just works! Seriously though, if you got a master with two replicas environment, you probably want to do occasional consistency checks with pt-table-checksum. Replication does not guarantee consistency (and writes can happen on replicas too). Otherwise: 

Since your commits are stalling, it's pretty safe to assume you're running with (see #4 here), so one obvious change you could do is switch to , however that may not be the right choice for you - see the previous link. In any case, don't usualy stall due to other activity on the server (like queries), unless that other activity locks some kind of mutex for far too long. If I had to guess without any further evidence, I would say that your writes are so intensive, that MySQL is unable to flush fast enough using adaptive checkpointing, so it exceeds 75% of the log capacity and starts sharp checkpointing (which blocks server activity for a short period). There are few quick ways to make MySQL handle writes better (besides one already mentioned above): 

Otherwise it would be really useful to see disk activity from AWS, amount of data server currently writes (you can calculate it using delta between two consecutive runs) There are other ways too, but for that, I'd need a lot more details on the actual workload. 

Certainly you are person who knows a lot of technology tools etc. But I will just point here, that two things are probably worth of research: 

Storage of the files on Linux filesystem. From my experience cames that even opening and listing directory with 100 000 or more files is a task which Microsoft cannot do in efficient way. Just using more efficient operating system here may give you at least some ergonomic gains. Maybe you should research etl tools like Apatar. It works with many files by GUI, so in some cases you may have a visualization of your process, which probably makes it easier to automate or analyse. It may be surprising in relation to this problem, but on the market there are very efficient data collection tools. Maybe thinking outside the box, and using clustered tools for system log acquisition, like Graylog log server, will be interesting choice. It has pipelining system, which takes data from network port, or through remote agents monitoring particular text files on remote system, and crunch it through various regex collectors, then has ability to divide that streams into output depending on given regex results. Output may be defined by you, and sent to another network device. Even on single machine acquisition of several hundreds of thousands records takes a seconds. It is very, very efficient. I believe at least part of your work may be automated here, for example it may test patterns and produce stream with wrong format entries ( or with good ones). And it gives you some basic statistical informations like dashboard with graphic representation of error types etc. 

Then you can use this view instead of the table in the query above. (And life would have been a bit simpler if you had picked some other value than NULL for the default district_id). 

JSON_SEARCH will give you the path to the feature called 'display'. To find the value of the same feature, we replace '.en' in the path with '.value'Â and use JSON_EXTRACT to look up the value at this path. (JSON_UNQUOTE is needed to convert the JSON string from JSON_SEARCH to an SQL string to be passed to JSON_EXTRACT). 

Join ordering is left to right. That is, in the above example a nested-loop join is performed by a full table scan of CUSTOMER followed by index look-ups into ORDERS and LINEITEM. (The meaning of ALL and ref are defined in the user manual for traditional EXPLAIN.) Note that Visual EXPLAIN is significantly updated in MySQL Workbench 6.1 including more descriptive text for the table operations. 

If overlaps are only partial, (i.e., a range may partially overlap another, but no range is a subset of another range), I think the following query will do what you want: 

It is difficult to say why MySQL Optimizer is choosing a different join order for 5.7 without seeing the optimizer trace for the query. However, you can force the same join order as 5.5 by using STRAIGHT_JOIN instead of INNER JOIN. That will tell MySQL to process the tables in the order they are listed. 

It is troublesome in every day use, as it is quite common some operations have to change databases records even in the past, at least for short time during years change It is very hard to use such systems in external analytics, based on normal tools: instead writing queries on database with time periods as filters, you have to manage several database connections and aggregate statistical data outside, probably using temporary tables or even additional data warehouse Some reports may be required, for time periods which not agree with years change. For example for accounting purposes. Exporting duch data into other systems, for example when changing HR platform to other products, is nightmare in such scenario. Upgrade. Usually is performed on present database. Will you upgrade historical ones as well? If yes - what I advantage of having it separately? If not - you will end with several incompatible database schemas. Maintenance. It may be a lot of effort put to maintain several databases instead of only one. It is known issue. 

It really depends how your HR/, Payroll application presenting it to the users. I can imagine that can be done in completely ergonomic way, so basically it would be only integration problem for some kind of external analytics. And it probably may have advantage, past year databases may be put on read only mode, and that current year databases would be smaller than big, aggregate database.. But saying this, I would like to say, I've never seen any good system for such kind of database model. Some random thoughts: 

Verify your backups to make sure you can actually recover (most of our data recovery customers thought they have backups, but they were actually not working). Query reviews - definitely worth learning how to do that so you can review them proactively. Do check error logs - any corruption errors will show up there. 

ANSWER 1: Internally, in case of InnoDB, full table scan is the same as using primary key because InnoDB clusters data by primary key. And it doesn't use any other index because there's no WHERE condition on the first table. ANSWER 2: In reality, adding won't make any difference because the execution plan will essentially remain the same (and results returned will be the same too). That's again due to the fact that InnoDB clusters data by Primary Key i.e. rows are always internally sorted by primary key. So if it makes you feel better to not see those queries in the slow query log, do add the order by. Finally, I know you didn't ask this, but - I really would recommend against using as it makes you focus you on the wrong metrics. Here's my recommended way to go about query analysis: Advanced MySQL slow query logging video tutorial. Good luck! 

It's a fairly safe bet that some of the tables have (including system privilege tables) have structure that MySQL 5.5 can't recognize. MySQL 5.5 and 5.6 is a major version difference (regardless it looks minor) and they are not backwards compatible. Try adding to to see if the problem is only with system tables (when you add , privilege tables won't be used at all and anyone will be allowed to connect from anywhere with any password). If MySQL would start, that means you have a broken privileges tables and should help you load the blank ones. If it doesn't help, you may have InnoDB structure changes that MySQL 5.5 can't recognize. In that case, you'll have to use logical backup (mysqldump or mydumper) for such downgrade. BTW, you can use any of the working slaves for it. BTW, if you're using on the master, there's no need NOT to use it on slave. In fact, it doesn't make much sense to. 

Since MySQL 5.6, MySQL may automatically convert IN-subqueries into a JOIN query. This is called semi-join transformation. By converting the subquery to a join, the MySQL optimizer may be able to process the tables in a different order than for traditional subquery execution. For your query, the amount of data that need to be accessed, will be much less if the tables of the subquery is processed first. However, as described in the manual, semi-join transformation will not be done if subquery contains UNION. In your case, it seems straight-forward to avoid the union so that semi-join transformation can be done. AFAICT, this query should be equivalent to yours: 

For the first problem, AFAICT, there is no way to use SQL to project certain properties of a JSON object. I think you will have to do that in the application layer. For the second problem, how to search for products with a certain feature: I assume you have a table products with one row for each product. The table contains a column of type JSON which contains the features of the product as you have shown. If so, I think the following query should find the products where the value of the feature called "display" in English has the value of 5.5": 

I do not think it will be worthwhile to change the type. You may save a byte or two for the length field of each column value if using TEXT or MEDIUMTEXT, but that will be insignificant compared to the size of your data. VARCHAR and TEXT columns are handled the same way in InnoDB, so there is no reason to switch to VARCHAR either.