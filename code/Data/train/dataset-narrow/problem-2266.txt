The article you want is C++ Templates are Turing Complete by Todd Verhuizen. It explains how things work in sufficient details. If you can faithfully encode a Turing machine and its behaviour in C++ templates, then this is sufficient to demonstrate Turing completeness. 

Finally, there's the work on iteration theories, Iteration theories: the equational logic of iterative processes by Stephen L. Bloom and Zoltán Ésik, which focusses on iteration (e.g., Kleene star), but from a more general perspective, where regular languages are just one thing that falls under the theory. 

One possible ranking scheme is the so-called Australian Ranking of Computer Science Conferences: At the bottom of this page you'll find various lists containing the data organised in different ways. For example, this one is ordered by acronym, and is probably the most useful. If the conference is not in the list, it is probably new and thus neither established or necessarily high quality. Regarding the quality of the publication, the SCI index can tell you that sort of information. But this is unreliable for conferences published in LNCS, as some volumes are included and some are not. (There are tools and websites providing SCI data.) Other things to look for are the number of editions the conference has had, whether it is a conference, symposium or workshop, whether it has ACM, IEEE, or other such sponsorship. None of these are particularly reliable metrics, though. Finally, ask your colleagues in the field. 

The Barabási-Albert Model is used for constructing scale-free networks using the preferential attachment technique. The essence, as I understand it, is that nodes are incrementally added to a graph by adding an edge to an existing node selected with probability proportional to the number of edges connecting the node. The algorithm starts on some initial clique, $G$. Now let's assume that newly added nodes are labelled $0, 1, 2, \ldots$. Define a construction path to be a path $l_0l_1l_2\ldots l_n o$ in the generated graph such that $o$ is a node from the initial clique $G$, $l_n<\cdots<l_2<l_1<l_0$ and $l_0$ is maximal. Clearly, at one extreme, such paths will be of length $1$, when all newly added nodes connect to one of the initial nodes. At the other extreme, the length of the path will be equal to the number of nodes added to the graph. Both of these cases are extremely unlikely. I'm interested in the average case. Are there any known results about the expected value of the lengths of construction paths? NB. Construction path is a name I invented for this question: please let me know if such paths go by another name. 

The conclusion, jumping to the punchline, is that if a zombie infection ever occurs, we're screwed. An agent-based framework for performing such simulations has also been published: 

Single-instruction, multiple data (SIMD) and Multiple-instruction, multiple-data MIMD architectures have one or more streams of instructions operating on more than one stream of data. These architectures have been realised by machines such as the old Crays, Vector processors, and Connection Machines, which differ significantly from Von Neumann machines as they break away from the single instruction sequence and a single data stream model. More exotic architectures have been proposed, inspired by biology such as the cellular architecture and systolic arrays, named after analogy with the regular pumping of blood by the heart. Many of the ideas of these architectures have been incorporated into modern hardware, if you consider multicore architectures with hyper threading. Formal models analogous to some of these include PRAM, the parallel random access machine, and cellular automata. The less general purpose these architectures are the more difficult they are to program, except for a special class of problems; for problems in this class, highly efficient implementations are possible, as the "abstractions" of the problem match the topology and other features of the architecture. 

The basic tactics either run an inference rule forwards or backwards (for example, convert hypotheses $A$ and $B$ into $A\wedge B$ or convert goal $A\wedge B$ into two goals $A$ and $B$ with same hypotheses), apply a lemma (~ function application), split up a lemma about some inductive type into a case for each constructor, and so on. Basic tactics may succeed or fail depending upon the context in which they are applied. More advanced tactics are like little functional programs that run the basic tactics, perform pattern matching over the terms in the goal and/or assumptions, make choices based on the success or failure of tactics, and so forth. More advanced tactics deal with arithmetic and other specific kinds of theories. The key paper on Coq's tactic language is the following: 

Q1: There are many notions of program equivalence (trace equivalence, contextual equivalence, observational equivalence, bisimilarity) which may or may not take into account things such as time, resource usage, nondeterminism, termination. A lot of work has been done on finding usable notions of program equivalence. For example: Operationally-Based Theories of Program Equivalence by Andy Pitts. But this barely scratches the surface. This should be useful even if you are interested in when two programs are not equivalent. One can even reason about non-halting programs (using bisimulation and coinduction). Q2: One possible answer to part of this question is that interactive programs are not algorithms (assuming that one considers an algorithm to take all of its input at once, but this narrow definition excludes online algorithms). A program could be a collection of interacting processes that also interact with their environment. This certainly doesn't match with the Turing-machine/Recursion theory notion of algorithm. 

Grothendieck's impact can be felt in type theory and logic. For instance, Bart Jacobs' 700+ page volume Categorical Logic and Type Theory gives a uniform treatment of various type theories ($X$-type theory, where $X\subseteq \{ \text{simple},$ $\text{dependent},$ $\text{polymorphic},$ $\text{higher-order}\}$) based on the categorical notion of Grothendieck fibrations (also called a cartesian fibrations). Similarly, the notion of Topos, also due to Grothendieck, plays a heavy role in providing categorical semantics to logics and type theories, which is of interest to logicians and theoretical computer scientists alike. 

If you intend to publish it is essential that you learn how to write academic papers. Even if you can already write well, it still takes effort to get the style, focus, and, in particular, the quality right. Academic writing is highly compressed, rather formal, and precise. Theorems and so forth must be written in a particular way. Writing proofs is an art. There are even (implicit) standards for referencing the literature. Get any of these wrong and your article looks amateurish, which lends itself to rejection irrespective of the quality of the contents. Here are some general tips that will help in this direction: 

Improving the writing always helps. Better, more intuitive examples. Are the ones you first thought of the best ones? Adding/improving empirical validation, including an implementation with performance evaluation or user studies, if applicable. On the other hand, if the paper is empirical in nature, strengthening the theory will improve the story. (Maybe this isn't so relevant for TCS papers.) Ensure that your notation is consistent, both internally and with the "standards" of the community. 

Perhaps not the simplest example, but Xavier Leroy has done a lot of work in this area, such as a formally verified C compiler. He gave a summer school presentation using a small imperative language IMP, which is an accessible introduction to the more advanced work. 

Rather than making a very precise analysis at the level of pixels, which is surely undecidable, you could design a program analysis that computes bounding boxes of either the set of pixels that may have changed or the set of pixels that definitely have not been changed. Then the given set of boxes (or their complement) are the ones that need to be redrawn. There might be some work in abstract interpretation that deals with this topic, but a quick google search didn't reveal anything. 

Buy a book such as this one, implement the algorithms, and find out some example or small project to work on from the exercise section. Here and here are lists of many project ideas. Google ought to reveal many others. Pick one that sounds fun and go for it. 

Here's a link describing how to do it in Haskell $URL$ Other languages have variadic functions, too: $URL$ 

The key role of types is to partition the objects of interest into different universes, rather than considering everything existing in one universe. Originally, types were devised to avoid paradoxes, but as you know, they have many other applications. Types give a way of classifying or stratifying objects (see blog entry). Some work with the slogan that propositions are types, so your intuition certainly serves you well, though there is work such as Propositions as [Types] by Steve Awodey and Andrej Bauer that argues otherwise, namely that each type has an associated proposition. The distinction is made because types have computational content, whereas propositions don't. An object can have more than one type due to subtyping and via type coercions. Types are generally organised in a hierarchy, where kinds play the role of the type of types, but I wouldn't go as far as saying that types are meta-mathematical. Everything is going on at the same level – this is especially the case when dealing with dependent types. There is a very strong link between types and category theory. Indeed, Bob Harper (quoting Lambek) says that Logics, Languages (where types reside), and Categories form a holy trinity. Quoting: 

Of course what will really help in the real world is a good design course and, I believe, exposure to many different programming languages. Getting a grasp on concurrent and distributed programming concepts, problems and solutions will also ready you for the future developments in the real world. 

Naturally, the course has multiple goals, where one of them is attracting potential graduate students. In the coming years the course may be expanded to a regular course, which will need more content. Given that the background of people here is quite different from mine, I would like to know what content you would include in such a course. 

A zipper, in general, is a data structure with a hole in it. Zippers are used for traversing/manipulating data structures, and the hole corresponds to the current focus of the traversal. Typically there is also an element of the data structure under consideration, so that one has a (list) zipper and a list or a (tree) zipper and a tree. The zipper allows the programmer to efficiently move around the data structure, even replacing the element at the focus. The pair of the zipper and the element in the focus satisfy the constraint that placing the element at the focus in the hole gives the original data structure. Zippers can be generalised to arbitrary inductive data types. The concept can be defined in type-indexed fashion (See type-indexed data types). They are also related to the idea of the derivative of a data structure, and has been studied from a Category Theoretic perspective. 

In proof systems for classical propositional logic if one want to show that a certain formula $\psi$ is not derivable one simply shows that $\neg\psi$ can be derived (although other techniques certainly are possible). Non-derivability follows essentially from the soundness and completeness of the proof system. Unfortunately for non-classical logics and more exotic proof systems (such as the rules underlying operational semantics) no such direct technique exists. This could be because the non-derivability of $\psi$ does not imply that $\neg\psi$ is derivable, as is the case with intuitionistic logics, or simply that no notion of negation exists. My question is given a proof system $(\mathcal{L},\vdash)$, where $\vdash\;\subseteq\mathcal{L}^*\times\mathcal{L}$, (and presumably its semantics), what techniques exist to show non-derivability? The proof systems of interest could include operational semantics of programming languages, Hoare logics, type systems, a non-classical logic, or inference rules for what-have-you. 

At present I teach a small course (Four two hour lectures at the Masters level) on Logical Methods in Security, though the title Formal Methods in Security might be more apt. It covers briefly the following topics (with associated logical methods): 

Programming language theory is fun for young and old. It's applying logic to the real world. Come join the parade!! More seriously, for me, programming language theory is interesting for the following reasons: 

The referees report contains at least 4 main elements: A summary of the paper and its contributions; points in favour of accepting the paper; points against the paper; major comments, including points to be addressed; minor comments (typos etc). You should always give an indication of acceptance or rejection or the degree of revision required. Comments to the editor could include: a short, perhaps blunt assessment of the paper; any statement of doubt; details of possible plagiarism or parallel submission of the paper; ... 

This can vary. Sometimes it will be your fault, sometimes it will be theirs. Use your judgement. Maybe ask a colleague to take a look at the paper. If you totally cannot understand it and it's not due to poor formatting, then perhaps contact the PC chair and explain that this is the case. In any case, this should be reflected in your confidence score. If the paper cannot be understood due to poor writing or language or because it has been prematurely submitted, then this should be written in your report. 

I don't know of any survey of the semantics of various language features, though there are many books dealing with the semantics of programming languages. Here are a few of the more comprehensive ones, which also deal with less standard constructs: 

Think about the hypothesis: It rains in Belgium. This is true. Assume that the proof of this fact does not contain the assumption Cows eat grass. What the $\to$ introduction rule states is that I can nonetheless conclude Cows eat grass $\to$ It rains in Belgium. Classically, this is true whether or not Cows eat grass is true. Constructively, if I have a proof of Cows eat grass, I can simply throw it away and return the previous proof of It rains in Belgium, as implicitly this is what the meaning of $A\to B$ is: it takes proofs of $A$ and converts them into proofs of $B$. 

You might want to look at the old programming language NESL which took these ideas seriously. The language is based on operations on collections, essentially lists and lists of lists and so forth, but I think trees and graphs were also considered, via tricky encodings. Some of the research done in that project (in the mid 1990s) could help answer your question. The PhD thesis (available as a book) Guy E. Blelloch. Vector Models for Data-Parallel Computing. MIT Press, 1990 may provide some answers. It was some time ago since I looked at it. Work done on the Bird-Meertens Formalism (BMF) falls into this category too, as did the language Charity. From the Charity wikipedia page it says that the language is not Turing complete, but can express Ackermann's function, which means that it's more than primitive recursive. Both BMF and Charity involve operators like folds and scans (catamorphisms, anamorphisms, etc), and they have their roots in Category Theory. I short, imprecise answer is that you can express quite a lot. 

Many such titles are available. Another option is to find suitable material online: a good starting point, which has lots of links, is $URL$ The other option is to google "Discrete mathematics" and such like, find .pdf files and skim through them to see whether they are at your level. Also the notes of many first year university courses covering discrete mathematics and such topics are available online. Again, google is your friend. 

The programming language Charity offers a solution to your problem (though it may be polymorphic). They key is to introduce folds into the language as combinators rather than using general fixed point recursion. Folds are very much like the recur primitive that you mention. Other such combinators are possible. Such combinators are known by strange names such as catamorphisms, anamorphisms, hylomorphisms, and paramorphisms (also known as Bananas, Lenses, Envelopes and Barbed Wire. Other work on recursion schemes expand the class of functions that can be expressed. Now folds are required to pull a data structure apart – building one up you need an unfold. Much of the work cited above, or cited in the work above, or building upon the work above, also deals with combinators or recursion schemes like unfold. (Caveat: I'm not sure that all of these schemes are strongly normalising – the ones in Charity supposedly are.) Returning to your example. You are essentially stating that this example cannot be encoded using a folds. I suspect that it can be, because, for instance, a filter can be encoded as a fold, and your example is a bit similar to a filter on trees.