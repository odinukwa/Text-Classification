If your model needs to be interpretable, you might be forced to make it simpler. Make sure to also remember Occam's razor. If your model is not "that much" worse with less features, then you should probably use less features. 

Shuffling data serves the purpose of reducing variance and making sure that models remain general and overfit less. The obvious case where you'd shuffle your data is if your data is sorted by their class/target. Here, you will want to shuffle to make sure that your training/test/validation sets are representative of the overall distribution of the data. For batch gradient descent, the same logic applies. The idea behind batch gradient descent is that by calculating the gradient on a single batch, you will usually get a fairly good estimate of the "true" gradient. That way, you save computation time by not having to calculate the "true" gradient over the entire dataset every time. You want to shuffle your data after each epoch because you will always have the risk to create batches that are not representative of the overall dataset, and therefore, your estimate of the gradient will be off. Shuffling your data after each epoch ensures that you will not be "stuck" with too many bad batches. In regular stochastic gradient descent, when each batch has size 1, you still want to shuffle your data after each epoch to keep your learning general. Indeed, if data point 17 is always used after data point 16, its own gradient will be biased with whatever updates data point 16 is making on the model. By shuffling your data, you ensure that each data point creates an "independent" change on the model, without being biased by the same points before them. 

(Assuming you are talking about supervised learning) Correlated features will not always worsen your model, but they will not always improve it either. There are three main reasons why you would remove correlated features: 

I ended up finding the paper you were referencing, maybe next time, add it to your post. From what I can understand in this paper, label-dropout means that you are dropping the real labels and replace them with others. The section of the paper you're referring to is explaining everything in great detail, so I'd try to read it carefully. In short, instead of using the ground-truth distribution (i.e. one 1 and a bunch of 0s) in the target layer, they are adding some uncertainty. That way, the model becomes less confident, and therefore is less prone to overfitting and becomes more adaptable. 

How to approach the solution using a NN ? Given your data, it doesn't look like an RNN/LSTM is the most sensible choice. It looks like you have two features and a binary output. RNN/LSTM are useful with variable-length input, otherwise, they are a bit overkill. Also, make sure that you have a reasonable amount of data before using a deep learning approach. They usually have many parameters to train, and if you don't have enough data points, this might be difficult to do. If you want to stick to NN, I'd say a regular multilayer perceptron might do the job. If you are open to other options, I would try decision trees for this particular problem. How do I convert the categorical value of OUI to numerical entities You could use one-hot encoding. There are plenty of implementations out there (i.e. scikit-learn), or you could do it yourself. 

Again, this is a difficult question as it will depend on the data, the architecture of the networks, the training time etc. I haven't done empirical research on this, but I'd say that your best guess would be time series that are rather short (less than 100 time steps for instance). If the time series is short, you might not need to model such an intricate relationship through time, which a regular NN could perhaps do as well as an RNN. 

The keyword being harmful. If you have correlated features but they are also correlated to the target, you want to keep them. You can view features as hints to make a good guess, if you have two hints that are essentially the same, but they are good hints, it may be wise to keep them. Some algorithms like Naive Bayes actually directly benefit from "positive" correlated features. And others like random forest may indirectly benefit from them. Imagine having 3 features A, B, and C. A and B are highly correlated to the target and to each other, and C isn't at all. If you sample out of the 3 features, you have 2/3 chance to get a "good" feature, whereas if you remove B for instance, this chance drops to 1/2 Of course, if the features that are correlated are not super informative in the first place, the algorithm may not suffer much. So moral of the story, removing these features might be necessary due to speed, but remember that you might make your algorithm worse in the process. Also, some algorithms like decision trees have feature selection embedded in them. A good way to deal with this is to use a wrapper method for feature selection. It will remove redundant features only if they do not contribute directly to the performance. If they are useful like in naive bayes, they will be kept. (Though remember that wrapper methods are expensive and may lead to overfitting) 

I was thinking about this lately. Let's say that we have a very complex space, which makes it hard to learn a classifier that can efficiently split it. But what if this very complex space is actually made up of a bunch of "simple" subspaces. By simple, I mean that it would be easier to learn a classifier for that subspace. In this situation, would clustering my data first, in other words finding these subspaces, help me learn a better classifier? This classifier would essentially be an ensemble of each subspace's classifier. To clarify, I don't want to use the clusters as additional features and feed it to a big classifier, I want to train on each cluster individually. Is this something that's already been done/proven to work/proven to not work? Are there any papers on it? I've been trying to search for things like this but couldn't find anything relevant so I thought I'd ask here. 

In the example above, the two centroids (X) at the top clearly both have points for which they are the closest centroid. But the centroid at the bottom is never the closest centroid for any instance. How do I deal with this? 

Since you only have 5 groups, you should probably look at distances instead of clustering. Now the question is: which distance should you use. Of course you could use Euclidean, Manhattan, or cosine distances and be done with it. In this case, I'd pick cosine distance because it reduces the impact of a single dimension on the overall distance and since you have 2000 features/interests, it might help. Now, I'm guessing that the interest groups are somewhat correlated. In an extreme case, 1999 interests are very correlated, and 1 isn't. If this happens and you use regular distances, it means that two groups who only agree on the 1999 interests will be considered much closer than the ones who only agree on the 1 interest. Even though, you know that you're only dealing with two interests here. So you might want to use some form of weighting to compute your distances using the correlation between the interest groups. Perhaps the more unique groups should matter more than the groups who are very similar to other interests. To do that, you could use dimensionality reduction techniques like PCA. PCA will "remove" redundant interests and "group" them into one. Once you have reduced the dimensionality of your data (let's say that now you are looking at 20 interests instead of 2000), you can compute your distances. Of course, distances are subjective and you have to define how much agreeing on specific interests matters. Perhaps agreeing on sports matters more than agreeing on books. If you have this prior knowledge, you'd have to manually input it into your weights. Once you've decided on how to compute your distances, you could compute multiple distance matrices by sampling subsets of the features (interests). That way, groups that are far away from each other will remain far away most of the time, same thing with groups that are close to each other. You could then look at the average distance to decide how far away the groups are from each other. 

There is a subtle nuance that is important to grasp when talking about class imbalance. Namely, is your data imbalanced because: 

Intuitively, your neural network will itself learn which feature is important or not by learning weights. 

Perhaps you could use word embeddings to better represent the distance between certain skills. For instance, "Python" and "R" should be closer together than "Python" and "Time management" since they are both programming languages. The whole idea is that words that appear in the same context should be closer. Once you have these embeddings, you would have a set of skills for the candidate, and sets of skills of various size for the jobs. You could then use Earth Mover's Distance to calculate the distance between the sets. This distance measure is rather slow (quadratic time) so it might not scale well if you have many jobs to go through. To deal with the scalability issue, you could perhaps rank the jobs based on how many skills the candidate has in common in the first place, and favor these jobs. 

Then, what you can do is perhaps use the distance between the two closest/furthest elements in the groups with label x and the group with label y. You could also create an average vector for each label, and then simply get the distance between these. 

Every neural net gets better in theory if it gets deeper. For a regular NN to model time connections properly, you could use the last n time steps as your input and the n+1th time step as your target. This will generate your training set and depending on your data, you could be able to model your time series fairly efficiently. All of the most obvious pitfalls of this approach are actually addressed by RNNs/LSTM. 

I always find the notion of false positive and negative confusing, especially when it comes to multi-class problems. A good rule of thumb that I came up with is the following True positive: "I predicted that you were a certain class, and I was right" False positive: "I predicted that you were a certain class but I was wrong" True negative: "I predicted that you weren't a certain class, and I was right" False negative: "I predicted that you weren't a certain class, but I was wrong" So in your case, there are 31 true positives with regard to the first class since you predicted 31 times that something was class 1 but you were wrong. 

Difficult question, which would probably require some empirical results to check that theory. Also, with neural nets, sometimes, it's more about how fast it trains rather than how much training data it has that will make the biggest difference in performance. To me, the main difference is that your regular NN will need a fixed-size input, whereas your RNN will be able to learn with input "up to" a certain size, which can be a big advantage to model the entire time series well. 

K-prototypes computes the distance between instances by combining the Euclidean distance between the numerical features and the hamming distance between the categorical features. 

Intuitively, scaling features can make your training faster because your "path to convergence" will probably be shorter if your features are scaled (right picture). It also makes numerical sense to scale your features because if you have very large and very small values, some of your weights might drop very low and this could cause some numerical issues and hinder the performance of your model. 

All clustering methods use a distance metric of some sort. And remember that distance is essentially a dissimilarity measure. So if you normalize your similarity betwen 0 and 1, your distance is simply 1-similarity As for algorithms that do not require a number of clusters to be specified, there are of course hierarchical clustering techniques, which essentially build a tree like structure that you can "cut" wherever you please (you can use some perfomance metrics to do that automatically) X-means is a version of K-means which tries a certain number of K and picks the one that maximizes some evaluation function. Mean shift also "finds" a natural number of clusters but is sensible to other parameters such as the bandwith for instance. 

I would definitely checkout this question first: K-Means clustering for mixed numeric and categorical data In case it doesn't help, here is my explanation: In the case where you have mixed data types (i.e. numerical and categorical), you have several options: 

First off, yes there are different Naive Bayes algorithms. But they are all based on the same principle, namely Bayes Theorem where the features are assumed to be independent. Here is a short guide on when to use which for spam detection (or document classification in general for that matter): 

Theoretically, you could use n-grams to model text sequences. But there are some good reasons why it's not often mentioned in textbooks. Intuitively, modeling sequences is learning that after a, b, and c comes d. So if your sequence is: "Hello, my name is Bob, what a lovely day today, how are you?" It technically doesn't matter if a = "Hello", or "H", or "Hello, my name is Bob". To your model, it will just learn that after a thing, comes another thing etc. So yes, you could use n-grams. But the problem is that you need to make sure that the way the ngrams are split is meaningful (i.e. "Hello, my" is a probably worse split than "my name is", which probably happens more often) But don't worry, the whole point of using LSTM is that the memory part of the network will mimic using n-grams, as long as it helps with the overall performance. This means that if it needs to take more words into account to predict the next one, it will do it up to the depth of your network. 

Intuitively, it's like trying to infer rules such that: if the reasons are X and Y, the diagnosis will likely be Z. The most common algorithm is Apriori, which is easy to implement. 

In some cases, one class occurs much more than another. And it's okay. In this case, you have to look at whether certain mistakes are more costly than others. This is the typical example of detecting deadly diseases in patients, figuring out if someone is a terrorist etc. This goes back to the short answer. If some mistakes are more costly than others, you'll want to "punish" them by giving them a higher cost. Therefore, a better model will have a lower cost. If all mistakes are as bad, then there is no real reason why you should use cost sensitive models. It's also important to note that using cost-sensitive models is not specific to imbalanced datasets. You can use such models if your data is perfectly balanced as well.