There are multiple reasons why this could happen. The first, and probably the most important one is that while both of the algorithms are tree based ensemble methods, they are very different from each other. Second of all, these feature importances are just some heuristic that do some calculations on where the features are used. I assume these heuristics are very different for both the algorithms In this case however the differences might just be too big to only point to these two reasons. What could have happened is that your features are highly correlated. If two columns are highly correlated then the algorithms become kind of indifferent to which one to use, which means the two algorithms might both pick a different one each time they come across this choice. You could take a look if column 0 and column 7 are very correlated. Another option is that some of your columns are categorical and that the implementations deal with this in a different way. Python doesn't work cleanly with categorical values in Scikit-learn. One-hot encoding is not a nice representation for undeep trees but if you believe this may be causing it you could preprocess them like that to see if the feature importances become more similar. 

So at time $t$ we want to predict values $y_{t+1}$ up to $y_{t+6}$ correct? You should use $x_0$ up to $x_t$ as inputs and use 6 values as your target/output. Then when you get new information, you add $x_{t+1}$ and use it to update your cell state and hidden state of your LSTM and get new outputs. The problem with feeding predictions is that errors will accumulate, although this does happen with sequence generation models (like seq2seq). What you would do then is during training use two types of training, one where you use current predictions and one where you use your known training data, both a fraction of the time. This is called 'teacher forcing'. If we use my first suggestion, here is an explanation how to prepare your data. To train your model you will need to use lags to prepare your targets. Let's say we have $x$ as one-dimensional and we are trying to predict the next 6 values of $x$, you would get as $x$ and $y$: 

I have a number of features X and a target which is the revenue generated by this interaction. More often than not no revenue is generated, but if there is it can differ considerably, it has a big right tail. I've made a model to estimate the expected revenue generated by a new x. Due to the weird distribution of the revenues I have split the model up in two parts, first estimating the probability of revenue generation and then conditionally on the fact that somebody will book, predict the amount of $log($revenue$)$. The first model is trained on a dummy variable which indicates revenue and the second one is trained on a subset of the data which contains all interactions that generated revenue. After I have these two models P (probability of generation) and R (expected revenue generated conditioned on > 0) we can calculate the expected revenue for a new x using: $$f(x) = P(x)e^{R(x)}$$ I've done a reasonable amount of cross validation on both the models individually and they work decent, however when they are combined they severly underestimate the amount of revenue generated. I have a good enough number of rows for the dimensionality of my data and I'm using Logistic Regression for the probability (linearity assumption is somewhat reasonable, however unbiased probabilities are very important) and Gradient Boosting for revenue estimation (got the best results for the second portion). What should I do about the underestimation of the revenue? What are alternative unbiased probablity estimators, should I train a neural network with a hidden layer for the log-loss function? Why does the combined model underestimate? Is there a good alternative to do it in one model? Which would not increase small errors into huge errors due to the multiplication and exponent? 

I doubt either of your proposed approaches will work, these convolutional layers learn all kind of relationships between the colors which will be non-existent in grayscale, upsampling it to RGB makes your input space very different from the input space that ImageNet was trained on and averaging these filters of the trained ImageNet model makes a whole lot of assumptions that are unlikely to hold. The second approach does seem more promising than the first, so if you want to test an approach I would go for the second one. It's possible that it is at least a better initialization than randomly. I think you will end up needing a lot of data and computation anyway, so it might be better to start off with your own network shape and start from scratch. 

Stability of the clusters is highly dependent on your dataset, for clear cut cases running it multiple times is a waste of resources. I think that is the rationale behind the default value of 1. But I agree that for most smaller cases setting it much higher makes a lot of sense. 

For my current use case, I have a high number of (noisy) samples that I do binary classification on. I use a neural network to approximate this with a sigmoid layer as output and log loss as loss function. My question is about the loss function, I understand that we use log loss to penalize probabilities that are far from the truth because of the log likelihood. The problem (or maybe it is not a problem) is that I want to aggregate my results to get the expected amount of positive samples in big groups. If we look at it from that perspective, looking at it as a mean as opposed to a probability also makes sense. In that case, having 2x 0.4 and 1x 0.7 is the same mistake as 1x 0. and 2x 0.75. Does it still make sense to use the log loss or might it be better to use L2 or L1 regression instead? 

You can take a look at Latent Dirichlet Allocation. In my experience this does very well without too much effort. You need to remove words that don't help like stopwords (and in your case Twitter handles and probably URLs) before feeding it to the algorithm. The only really important parameter that you need to give it is the number of topics. This will depend on your population (are these random tweets, or only tweets from a specific subgroup/hashtag?) and you need to compare some settings. What you can do is print the most important words per topic and see if they indeed do belong together. If there are different languages in your tweets you need to deal with that beforehand, maybe classify them on language and only keep the English ones for example. 

Another alternative is to use Jupyter notebooks, if it supports kernels of your programming language. It allows for normal markdown, LaTeX, output shown in the web document and interactivity. You can share the true, runnable document if it's hosted on a normal server or a HTML dump for easy sharing. 

Without making any underlying assumptions you will not get anywhere. That said, there are multi-arm bandit strategies that try to optimize the rewards, there is a ton of research on this field. It comes down to sampling from a distribution of your options (in your case two) and adapting this distribution based on the rewards. $URL$ Once you know that the reward distribution from each bandit comes from a specific distribution, you can deduce optimal sampling strategies. Once you have at least some prior information you can do fairly well although not always optimal. Regardless, most strategies will do better than normal A/B testing if the strategy is not super greedy. 

My estimate is that there are around 5.4b numbers in your dataset, all in the range of 0-50 according to your comment. I doubt there are much faster ways to do this on all the data than you are currently doing. However if you would just take every file and get random samples of 0.1-10% of each file and then combine these, you will still get an ecdf that is visually almost exactly the same while reducing memory issues and computing power necessary. 

You can turn it into a one-hot encoded feature with an added class of 'Missing', depending on the cardinality (how many categories are there). If the cardinality is too high, you will need to use other techniques for high cardinality features but you can still have 'Missing' as an additional category. 

Accuracy is a metric meant for classification problems, look at the mean squared error instead. Your network is too small for a highly fluctuating function that you want to learn, if you divide your x by a smaller amount it would be easier to learn. Second of all, adding another layer and having an identity activation at the end will help quite a bit. Also taking batches bigger than 1 will make the gradient more stable. With a 1000 epochs I get to 0.00167 as mean squared error. 

I would look into the Python package BeautifulSoup. It parses HTML documents into a tree structure and allows for all kind of filtering and manipulation of the tree. I've taken a look at the Gear Patrol website and I would advice these steps: Scrape the front page looking for articles. By taking a look using a web inspector like the one in Chrome (Ctrl+Shift+I) you can look at the HTML structure. Looking there we can see that the links to the articles are in: 

The CV stands for CrossValidation, meaning it will split up your training set in a number of folds (in this case 3), train on n-1 of those folds and test on the remaining one. This is why your training is now done on 32 instead of 50 samples. Crossvalidation is useful for estimating how well your model (including specific hyperparameters) does on unseen data. 

In the wrapper function you can pass scalars or keras tensors like additional inputs. In your case you would add the weights in the top wrapper function and reference them in your inward function. 

Here are some Python implements on Wikibooks. The algorithm to compute these distances is not cheap however. If you need to do this on a big scale there are ways to use cosine similarity on bi-gram vectors that are a lot faster and easy to distribute if you need to find matches for a lot of words at once. They are however only an approximation to this distance. 

You are going in the right direction. Dates can contain a lot of information depending on the task you want to learn. A problem with your suggestion of using the day of the year straight up is that the last day of the year and the first day of the following year are very close to each other, while in your representation they are the furthest away. An alternative to this could be using the fact that this is cyclical and map the feature onto a circle and use the coordinates on this circle as features to represent this. A lot of the important features regarding dates are cyclical, like the day of the week, the day of the month, the hour of the day etcetera. An alternative which allows for easier non-linear relationships would be to one-hot encode them as classes. This has the advantage of a more free representation, but the downside that it will cost more features and it cannot generalize based on the fact that two options might be next to each other. With regards to additional domain knowledge, like knowing that being close to christmas, or close to salary day might be relevant, it's almost always beneficial to add these features to your data. This could be done by calculating absolute distance to christmas, or days before pay day etcetera. Some of these features can be work intensive to create, for example localized holiday features, but they can significantly influence people's decisions and if that is what you are trying to predict, adding these will benefit your performance. 

For a project I want to use recurrent neural networks, however my knowledge on this subject is still somewhat limited. I do have some experience with convolutional nets and traditional neural networks. I need to predict a probability distribution over one of the inputs of the next step. Most of my sequences are relatively short, with some big outliers in there. There is more than enough data so that should not be an issue. I'm using Keras for this task, using either LSTM or GRU layers. My input exists of sequences of categorical input $x_A$, $x_B$ and a few numerical values $x_1$ to $x_{11}$. I'm trrying to predict $x_A$ of the last step that is not trained on. For the high cardinality categorical features I'm using an embedding layer to map them to a dense space and then merge the three different types of input into one layer. My issue is merging them together before going to the softmax layer to do the one-hot encoding prediction of the last step. I cannot seem to get the inputs to match. I pad the shorter sequences to 10 steps plus the label and truncate the longer sequences cutting off the start. I've attempted both the Graph and the Sequential interface. 

Yes, sequence 2 sequence models attempt to do this. This can be used in a number of domains, from typo fixing to machine translation. They are encoder -> decoder based, which means you have a part that encodes your input and then a decoder that generates a new sequence based on this encoding (and usually some attention). In this case your encoder would likely be two recurrent neural networks of which the output would be concatenated and then a decoder that takes this concatenated output and turns this into a new sequence. If you want to use attention you need to adapt the standard attention a bit because you have two textual inputs, but if you understand how it works this would not be too difficult to adapt. 

You are correct, there is more overhead to process the same amount of data because you do more weight updates and maybe preprocessing the batches in your generator will take more time as well. However, since you are doing more updates and if your batch size is big enough, the gradients of your mini-batches will approximate the gradients of your full batch fairly well and you will have gotten much closer to the optimum of your full dataset by splitting your full batch into mini batches than you would have gotten by doing one non-stochastic weight update. 

The firs solution sevo proposes is not feasible because of a third problem that was not mentioned. The first layer only learns a first representation of the input, which is used in later layers. Even if the absolute weights of $x_1$ might be very big, if the later layers have small weights connected to these neurons the importance goes down. This is exactly why neural networks are considered to be difficult to interpret. The rest of the answer is useful, I just wanted to add this. 

A classic optimization problem! You can use Linear Programming/Optimization to find a good split. Every of the n samples s $\in S$ has weight f(s) and we want to divide them into m folds. You can use a trick to linearize the L1 objective or you can use Quadratic Programming for a L2 objective function. The Quadratic Programming model is easier to define in this case. Let's define $x_{ij}$ as a binary decision to put sample i in fold j and $\mu$ is the ideal, mean weight per fold. Then this is our objective function: min $\sum_{j=1}^m(\sum_{i=1}^nx_{ij}f(i)-\mu)^2$ Under the following constraints: $\sum_{j=1}^mx_{ij}=1$ for all $i\in \{1..n\}$ to ensure exactly one assignment per sample $x_{ij} \in \{0, 1\}$ to turn it into binary decision varibales Depending on the size of your dataset and the solver you use this can be a heavy optimization, but there are a lot of greedy heuristics that will get you close fairly fast. 

Just taking the mean seems better, that way the distances between the groups is better represented than just indexing them by order. With the target you should be a bit careful not to include the current row for the mean, since for the test set and the actual predictions you cannot use that target either (for obvious reasons). To get even more information about the category you try to get rid of is to also take the variance or standard deviation of the target within this group and take the mean and variance of important numerical features for these categories. If you have multiple categories you can also take the expected value for features and targets conditioned on multiple categories at once. This is a nice compromise between keeping enough information on high cardinality while keeping the dimensionality low. Here's an example where the c is the categorical variable, n are numeric and y is your target. We want to get rid of c (because c could have 1000s of values). 

There are multiple options, depending on your problem and the algorithms you want to use. The most promising (or closest to your original plan) is to use a generator to prepare batches of training data. This is only useful for models that allow for partial fits, like neural networks. Your generator can just stratify examples by for example generating a batch that includes exactly one of each target. One epoch would be when you served all the samples from the biggest class. Downsampling is not a bad idea but it depends on the difficulty of your task, because you do end up throwing away information. You could look at some curves depending on the amount of samples for your model, if it looks relatively capped this wouldn't be a big issue. A lot of models allow for weighting classes in your loss function. If we have 10,000 of class A and 1,000 of class B, we could weight class B 10x, which means mistakes that way count much harder and it will focus relatively more on samples from class B. You could try this but I could see this going wrong with extreme imbalances. You can even combine these methods, downsample your biggest classes, upsample your smaller classes and use weights to balance them perfectly. EDIT: Example of the batch options: We have 4x A, 2x B and 1x C, so our set is: A1 A2 A3 A4 B1 B2 C1 Regular upsampling would go to: A1 A2 A3 A4 B1 B2 B1 B2 C1 C1 C1 C1 But this will not fit in our memory in a big data setting. What we do instead is only store our original data in memory (could even be on disk) and keep track where we are for each class (so they are seperated on target). A: A1 A2 A3 A4 B: B1 B2 C: C1 Our first batch takes one of each class: A1 B1 C1 Now our C class is empty, which means we reinitialize it, shuffle them (in this case it's only one example). A: A2 A3 A4 B: B2 C: C1 Next batch: A2 B2 C1 B and C are empty, reinitialize them and shuffle: A: A3 A4 B: B2 B1 C: C1 Next batch is: A3 B2 C1 And our last one of the epoch would be A4 B1 C1 As you can see, we have the same distribution as the full memory option, but we never keep more in memory than our original ones, and the model always gets balanced, stratified batches.