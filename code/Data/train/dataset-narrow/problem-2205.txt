I'm trying to draw up a taxonomy of algorithms for transforming regular expressions into automata so as to perform some empirical tests of their complexity properties in specific domains. I'm aware of several of the 'bigger' names, e.g., 

Chapter 5.1 in "Data Structures and Network Algorithms", which is ref. 43 in the paper you cited, seems like it might have the answer: $URL$ 

Perhaps the Menagerie of Monoids diagram will be helpful? $URL$ from page 5, "On the Representation Theory of Finite J-trivial Monoids", by Denton, et. al. (2010). A detailed description is also given in Section 2.1, 

I tried searching for related follow-up papers in both English and Japanese but was unsuccessful. My guess is that the approach proved unsuccessful, or the professor got busy with something else (looks like this was around when he switched universities). I think that your best bet at this point, assuming you want to follow up the rest of the way and get a concrete answer, is to write professor Kunihiro directly (in English!) 

I think you are confusing zero knowledge with soundness. Fix some language $L$. [Adaptive] soundness for NIZK says, roughly, that when $\sigma$ is chosen at random a prover cannot find a $y \not \in L$ and a proof $\pi$ for which a verifier will accept. Zero knowledge says that, for any $y \in L$, a simulator can output $(\sigma, \pi)$ that is indistinguishable from $\sigma$ being chosen at random and then an honest prover generating a proof $\pi$ for $y$. It makes a huge difference whether $\sigma$ is chosen first, or whether the simulator gets to choose $\sigma$. 

Katz and Lindell were recommending against using LFSRs by themselves as pseudorandom generators. However, it might be possible to construct a pseudorandom generator using an LFSR in conjunction with other mechanisms. (In particular, PRGs based on LFSRs must include some non-linear component.) 

I'm not sure the solution you have given works. However, it can be adapted to work -- both for NIZK proofs and proofs of knowledge -- by using a coin-tossing protocol where "secure" here means in the sense of general secure two-party computation. This can be done based on one-way functions in polynomially many rounds, or based on stronger assumptions in constant rounds. A good reference is Lindell's paper on coin tossing. 

This is the oldest reference that I know (1970), but Jean-Yves Girard's thesis work was around the same time and also talked explicitly about neutral ("simple") terms in connection with proofs of strong normalization. How far back does the idea go? Did, say, Church or Curry discuss it? 

I suppose some of the above points might sound negative, though I wouldn't call them "cons" but merely "realities" of intersection and union types. On the other hand, from a language design perspective, one reason for making the effort of supporting intersections and unions (and for getting them right!) is that they allow more precise properties of programs to be expressed in a fairly incremental way, requiring a much less drastic transformation than, say, dependent type theory. A brief reading list: 

$\newcommand\iddots{⋰}$In "A simple proof of a theorem of Statman" (TCS 1992), Harry Mairson gives a simple proof of Statman's result that deciding $\beta\eta$-equality of terms in simply typed lambda calculus is not elementary recursive, i.e., cannot be done in time $O(2^{\iddots^{2^n}})$ for any finite tower of exponentials. It's a nice proof, that works by showing how to encode arbitrary formulas of what Mairson calls "higher-order type theory" as simply typed terms, such that a formula is valid iff the corresponding term evaluates to the Church boolean $true = \lambda x.\lambda y.x$. The phrase "higher-order type theory" sounds ambiguous to me, so for the purpose of this post I will just call these higher-order quantified boolean formulas (HOQBF) — please let me know if there is a more standard terminology. HOQBF can be seen as a generalization of QBF, where quantifiers can range not just over the set of booleans $\mathbb{B} = \{ \mathbf{true}, \mathbf{false} \}$, but over sets of booleans $P(\mathbb{B})$, sets of sets of booleans $P(P(\mathbb{B}))$, and so on. In other words, the language of formulas is $$ F ::= \forall x^k.F \mid \exists x^k.F \mid F\wedge F \mid F \vee F \mid \neg F \mid x^k \in y^{k+1} $$ where the $k$ are natural numbers, and where variables $x^k$ are interpreted as ranging over $P^k(\mathbb{B})$. (Again, is there a standard name for such formulas?) Mairson cites 

It is easy to show (by probabilistic argument) that there exist functions that require circuits of size $O(2^n/n)$. This, in turn, can be used to prove a non-deterministic hierarchy theorem showing (roughly) that if $2^n/n > T(n) \gg t(n)$ then there exist functions that can be computed by circuits of size $T$ but not by circuits of size $t$. What is known if we are instead interested in approximating functions rather than computing them exactly? This question can be asked in many ways, but for concreteness let us say that $f \in approxSIZE(t)$ if there is a circuit family of size $t$ that (for each input length) computes $f$ correctly 3/4 of the time. What can be said about existence of hard functions here? What kind of hierarchy theorem is known? Does the answer change if we require the circuits to compute $f$ correctly $1-1/n$ of the time? Note (added after Ryan Williams's answer): In fact, the answer to my first question follows from a simple modification of Shannon's lower bound, since a circuit on $n$ input bits can only $\epsilon$-approximate $\sum_{i=0}^{\epsilon \cdot 2^n} {2^n \choose i} < 2^{H(\epsilon) \cdot 2^n}$ functions, where $H$ denotes the binary entropy function. A hierarchy theorem should follow from this in the usual way. 

In any case, as glossed in the abstract of the original tech report, the idea of the proof is to show that HOT is a "hierarchy by definitional level". That is, he defines a notion of rank for a HOT combinator, and a family of combinators $Hn$, such that each $Hn$ has rank $n+1$ and is not $\beta$-equivalent to any combination of HOT combinators of rank $n$. This implies that HOT is not combinatorially complete, because if the $S = \lambda x.\lambda y.\lambda z.(xz)(yz)$ combinator could be derived from a combination of HOT combinators of rank $n$ for some $n$, then so could any other combinator, in particular the combinator $Hn$ of rank $n+1$. 

I don't want to make a statement about "all linear lambda calculi" since it's hard to make that precise, but for pure linear lambda-calculus the answer is yes. One way to enforce linearity in pure lambda calculus is by trying to type application and abstraction using the linear implication $A \multimap B$, $$ \frac{\Gamma \vdash t:A\multimap B\quad \Delta\vdash u:A}{\Gamma,\Delta\vdash t(u):B} \qquad \frac{x:A,\Gamma \vdash t:B}{\Gamma\vdash \lambda x.t:A\multimap B} $$ together with the linear identity and exchange rules: $$ \frac{}{x:A\vdash x:A} \qquad \frac{\Gamma,y:B,x:A,\Delta \vdash t:C}{\Gamma,x:A,y:B,\Delta \vdash t:C} $$ But if you just want to check that a lambda term is linear, types are overkill, and it suffices to check syntactically that every free or $\lambda$-bound variable is used exactly once. A way to make that a bit more precise is to define the set of linear lambda terms $\Lambda_1(\gamma)$ with a given list $\gamma = x_i,\dots,x_1$ of distinct free variables. This is really a family of sets (indexed by $\gamma$) generated by the following rules (which can be seen as an abstraction of the typing rules above): $$ \frac{}{x \in \Lambda_1(x)} \qquad \frac{t \in \Lambda_1(\gamma,y,x,\delta)}{t \in \Lambda_1(\gamma,x,y,\delta)} \qquad \frac{t\in \Lambda_1(\gamma)\quad u \in \Lambda_1(\delta)}{t(u) \in \Lambda_1(\gamma,\delta)} \qquad \frac{t \in \Lambda_1(x,\gamma)}{\lambda x.t \in \Lambda_1(\gamma)} $$ Then a term $t$ of pure lambda calculus with free variables $\gamma$ is linear just in case $t \in \Lambda_1(\gamma)$. Note that these rules (like the typing rules) also have the effect of performing scope checking, and thus ruling out certain syntactic expressions which might look linear but violate scoping (e.g., an expression like "$\lambda x.x(y)(\lambda y.\lambda z.z)$", where the variable $y$ is used before it is bound). Finally, pure linear lambda calculus has the remarkable property that every term is typable (which is related to the fact that it is strongly normalizing). So, once we know that a lambda term is linear it is always possible to go back and give it a type. Even more remarkably, the principal type of a term uniquely identifies its $\beta$-normal form, so type inference is in fact equivalent to normalization. Harry Mairson had a nice paper discussing some aspects of these remarkable properties in Linear lambda calculus and PTIME-completeness (JFP 14:6, 2004). 

There are a couple of different things going on here, and you need to define your problem more clearly. For starters, let's just look at a simple case where what is being stored in a database (for each user) is either $H(pw)$ or $s, H(s, pw)$ where $H$ is a hash function, $pw \in \{1, \ldots, N\}$ is a password, and $s \in \{0,1\}^\ell$ is a salt. To attack a single user without a salt, the attacker can pre-compute $H(pw)$ for all possible values of $pw$ yielding a table of size $N$. Without knowing the salt, however, the attacker has to compute $H(s, pw)$ for all possible salts as well, thus requiring a table of size $N \cdot 2^\ell$. On the other hand, if the attacker does no pre-computation but instead just waits until it compromises the database and then obtains $s^*, H(s^*, pw)$, then we are back to the previous case where the attacker just has to compute $N$ values of $H(s^*, pw)$ to learn the password. Thus, in the single-user case, the salt increases the attacker's off-line computation but does not increase the on-line computation needed. Before continuing, let me note also that (in the case without the salt) the attacker can use to obtain various time-space tradeoffs. Use of salts makes rainbow tables less effective as well. Salts also help, in a somewhat orthogonal way, in the multi-user setting. To see this, note that if the attacker got the database of hashed passwords in the unsalted case, then using $N$ work he gets the passwords of all users. But in the salted case, assuming each of $M$ users is assigned a different salt, the attacker must do $M \cdot N$ work to recover all passwords. 

This is a question about the correct reference for a result that seems to appear frequently in the literature on planar graph isomorphism. In "A $V \log V$ Algorithm for Isomorphism of Triconnected Planar Graphs", Hopcraft and Tarjan mention that a 3-connected planar graph has an essentially unique embedding into the plane (up to mirror symmetry), and attribute this result to Whitney (1933) "A set of topological invariants for graphs". I've noticed this citation reappearing in various places (for example here, here, and here), but I couldn't find the result in Whitney's article itself -- am I just overlooking something? The paper is dense, and builds on a series of other articles he published in succession... Still, this result sounds a lot like Theorem 11 of Whitney (1932) "Congruent Graphs and the Connectivity of Graphs", which states that a 3-connected planar graph has a unique dual. Was that the intended reference? 

The short answer is "to verify additional properties of existing code". Longer answer follows. I am not sure "implicit" vs "explicit" is good terminology. This distinction is sometimes called "structural" vs "nominal" subtyping. Then there is also a second distinction in the possible interpretations of structural subtyping (described shortly). Note that all three interpretations of subtyping really are orthogonal, and so it doesn't really make sense to compare them against each other, rather than understanding the uses of each. The main operational distinction in interpreting a structural subtyping relation A <: B is whether it is witnessed by a real coercion with (runtime/compiletime) computational content, or whether it can be witnessed by the identity coercion. If the former, the important theoretical property that has to hold is "coherence", i.e., if there are multiple ways to show that A is a substructural subtype of B, each of the accompanying coercions have to have the same computational content. The link you gave seems to have the second interpretation of structural subtyping in mind, where A <: B can be witnessed by the identity coercion. This is sometimes called a "subset interpretation" of subtyping, taking the naive view that a type represents a set of values, and so A <: B just in case every value of type A is also a value of type B. It is also sometimes called "refinement typing", and a good paper to read for the original motivation is Freeman & Pfenning's Refinement types for ML. For a more recent incarnation in F#, you can read Bengston et al, Refinement types for secure implementations. The basic idea is to take an existing programming language that may (or may not) already have types but in which types do not guarantee all that much (e.g., only memory safety), and consider a second layer of types selecting subsets of programs with additional, more precise properties. (Now, I would argue that the mathematical theory behind this interpretation of subtyping is still not as well understood as it should be, and perhaps that is because its uses are not as widely appreciated as they should be. One problem is that the "set of values" interpretation of types is too naive, and so sometimes it is abandoned rather than refined. For another argument that this interpretation of subtyping deserves more mathematical attention, read the introduction to Paul Taylor's Subspaces in Abstract Stone Duality.) 

"Applied Combinatorics on Words", by Lothaire, 2004 Is far and away my favorite. Loads of examples, and also builds up from the absolute basics all the way to some pretty interesting automata applications like Automatic Speech Recognition with Weighted Finite-State Transducers, and topics in bioinformatics. Best of all, it's free to download, and also includes solution sets: $URL$ 

and their distinguishing properties (epsilon-free-ness, determinism, size, minimization, etc.) but I know this is not an exhaustive list. I'm particularly interested in algorithms which present either significantly different time complexities to those listed above, and/or significantly different topologies. If you know of others, a link to a paper which describes the construction algorithm in detail would be greatly appreciated (read necessary if I'm going to implement it!) Edit: Added some references as per requested. 

Thompson "Regular Expression Search Algorithm", Thompson, 1968 Glushkov "A New Quadratic Algorithm to Convert a Regular Expression into an Automaton", Ponty, et. al, 1996 Antimirov "Partial Derivatives of Regular Expressions and Finite Automata Constructions", Antimirov, 1996 Follow "Follow Automata", Ilie, et. al, 2003; "Computing the follow automaton of an expression", Champarnaud, et. al, 2002 Hromkovic "Translating Regular Expressions into Small e-Free Nondeterministic Finite Automata", Hromkovic, et. al, 2001 

In particular, he showed PTIME-hardness by reduction from the circuit value problem. My question is about the restriction of the decision problem to linear lambda terms which are planar in the sense that they can be typed without using the exchange law (e.g., $B = \lambda x.\lambda y.\lambda z.x(yz)$ is planar, but $C = \lambda x.\lambda y.\lambda z.(xz)y$ is not). Is this an open problem? I would suspect that it has lower computational complexity, since Mairson's encoding of boolean circuits uses non-planarity in an essential way to distinguish "True" from "False". I'm specifically interested in $\beta$-equality of planar lambda terms, but the problem is related to deciding equality of proofnets for non-commutative multiplicative linear logic, so I'd also be interested in any results in that direction. 

[Expanding the comment into an answer.] First, just a clarification about counting bound variables in a combinator (= closed term) $t$. I interpret the question as asking about $$ \text{the total number of distinct bound variable names in }t $$ so that for example the term $t = (\lambda x.x(\lambda y.y))(\lambda x.\lambda y.yx)$ counts as having two bound variables, despite having four binders (i.e., lambda abstractions). This way of counting was initially a bit strange to me since it is not invariant under $\alpha$-conversion: for example, $t$ is $\alpha$-equivalent to $t' = (\lambda x.x(\lambda y.y))(\lambda a.\lambda b.ba)$, but $t'$ has four distinct bound variable names. However, this is not really a problem, because the minimum number of distinct bound variable names needed to write a closed term $t$ is equal to $$ \text{the maximum number of free variables in a subterm of }t $$ and the latter notion is invariant under $\alpha$-conversion. So, let $\mathcal{C}$ be the collection of all combinators which can be written using at most two distinct bound variables, or equivalently the collection of all combinators whose subterms have at most two free variables. Theorem (Statman): $\mathcal{C}$ is not combinatorially complete. It seems that the original proof of this is contained in a tech report by Rick Statman: 

A direct product theorem, informally, says that computing $k$ instances of a function $f$ is harder than computing $f$ once. Typical direct product theorems (e.g., Yao's XOR Lemma) look at average-case complexity, and argue (very roughly) that is $f$ cannot be computed by circuits of size $s$ with probability better than $p$, then $k$ copies of $f$ cannot be computed by circuits of size $s' < s$ with probability better than $p^k$. I am looking for different types of direct product theorems (if they are known). Specifically: (1) Say we fix the probability of error $p$ and are instead interested in te size of the circuit needed to compute $k$ copies of $f$? Is there a result that says that if $f$ cannot be computed by circuits of size $s$ with probability better than $p$, then $k$ copies of $f$ cannot be computed with probability better than $p$ using a circuit of size less than $O(k \cdot s)$? (2) What is known with respect to worst-case complexity? E.g., if $f$ cannot be computed (with 0 error) by circuits of size $s$, what can we say about the complexity of computing $k$ copies of $f$ (with 0 error)? Any references would be appreciated.