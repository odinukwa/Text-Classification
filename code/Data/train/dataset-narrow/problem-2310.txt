This answer makes an assumption that it is possible to generate probabilities of the necessary form, which is not necessarily granted in the question. See the answer for Peter Shor for why this can be a problem. 

The answer is trivially no. If you apply the inverse of each gate in reverse order you compute the inverse of the function. Assuming a model where gates act on a fixed number of qubits at a time, then the inverse of each elementary reversible gate can be applied in constant time. Hence such a function is as easy to invert as to compute (up to a multiplicative constant), and hence is not a trapdoor function. 

If I am understanding the problem correctly, it would seem to amount to public flipping a $k$-sided coin. There seem to be lots of ways to do this if you assume bit commitment. One example would be having each party generate a random integer between 0 and $k-1$, using bit commitment to publicly commit to that bit string. Then once each agent has committed, they all publicly reveal their secret integer. The winning agent is then the one indexed by the sum of the integers modulo $k$. If even one agent is honest, then the flip must be random. Of course one problem with this is that it requires bit commitment. Information theoretic schemes for bit commitment are impossible for both classical and quantum computing (though Adrian Kent recently proposed a scheme exploiting relativity). However, secure bit commitment can be achieved with computational assumptions. 

Not really an answer, but an extended comment on András's answer. András's answer contains a nice intuition, though I do not believe it is a rigorous calculation of the expected number of steps. I think it is perhaps a good approximation to an answer, but it does not seem to properly deal with cases where the bin below the highest occupied bin becomes empty before that the upper bin is emptied downwards. Still, this might be a reasonable approximation to make (I'm not sure). His calculation contains an error which affects the scaling. I'm going to take exactly the same starting point, and redo and expand the calculation. It misses a factor of p inside the summation, as the probability of randomly choosing the correct bin is $\frac{p}{n}$ rather than $\frac{1}{n}$. As a result we have $\begin{eqnarray*} n + \sum_{p=1}^n \sum_{k=0}^{\infty} (k+1) \frac{p}{n} \left(\frac{n-p}{n}\right)^k & = & n + \sum_{p=1}^{n} \frac{p}{n} \sum_{k=0}^{\infty} (k+1) \left(\frac{n-p}{n}\right)^k \\\\& = & n + \sum_{p=1}^{n} \frac{p}{n} \cdot \frac{n^2}{p^2} \\\\& = & n + n\sum_{p=1}^{n} 1/p \\\\& = & n (1+H_n) \end{eqnarray*}$ where $H_n = \sum_{p=1}^{n} 1/p$ is the nth Harmonic number. To approximate $H_n$ we can simply replace the summation with an integral: $H_n \approx \int_{1}^{n+1} \frac{1}{x} dx = \log(n+1)$. Thus the scaling is $n (1+\log(n+1))$ or approximately $n \log(n+1)$. While this scaling does not match the scaling of the problem exactly (see simulation below) it is out by almost exactly a factor of $\log(2)$. 

I agree with most of what Peter and David have said, but I thought I'd contribute an answer anyway, so that you can have a few different perspectives on this. I should mention that the criteria people use for determining what to upload is not uniform across the community. Some (perhaps most) people only upload finished papers, while others upload lecture notes or journal club notes, or minor results not intended for publication. For me, the general rule I use for determining whether I am going to upload something to the arxiv is whether or not I intend to submit it for publication somewhere. This is basically a practicality: I have a finite amount of time to spend writing stuff up (which is by far my least favorite part of the job), and so it makes sense to spend it writing up the results I consider the most important. As you progress through your research career, you tend to accumulate a lot of minor results. I tend not to bother writing these up, but often save them in case they might make an interesting starting point for working with a new student. However, the rule I mention above I use simply for practical reasons. There is a much more important rule to bare in mind: Don't put something on the arxiv you are not happy to have people see with your name attached to it. People will see it. Within our community it is far more common for people to read the arxiv mailings than to read any particular journal. Also, it is essentially impossible to totally remove a paper from the arxiv. You'll notice that even papers people withdraw remain there. I'm not sure whether it is of any use to you, but since we are in the same field you may find it useful to hear a breakdown of my own uploads. All but one are intended for publication (either have been published, are in review, or are waiting to be submitted somewhere, possibly in an updated form). The one which will likely never be published was initially intended to submit to PRL, but got superceded by a much more detailed version I wrote for a CS conference. Also, one of my uploads is a review paper which has essentially zero original research in it. Most of my papers are on the arxiv, but there are 4 or 5 which for various different reasons don't appear there. So where does this leave your paper? Well, as you are already distributing it via your own website, I would certainly say my second rule of thumb doesn't apply. Also, clearly you have already written it up, so the first rule doesn't apply in this instance. With that in mind, I would think there is no reason not to upload. If you get good feedback, you might even consider making a small paper out of it. 

If you are interested in multiplying two matrices and getting back the full classical result, then Martin's response is probably a definitive answer to your question. However, if you want to calculate something like $v^\dagger X Y v$ then you can do this extremely efficiently. Harrow, Hassidim and Lloyd have an algorithm (arXiv:0811.3171) for calculating $v X^{-1} v$ which is only logarithmic in the dimensions of the matrix $X$ for sparse matrices. It seems relatively straight forward to adapt this approach to calculate products rather than inverses. 

Beigi, Shor and Watrous have a very nice paper on the power of quantum interactive proofs with short messages. They consider three variants of 'short messages', and the specific one I care about is their second variant where any number of messages can be sent, but the total message length must be logarithmic. In particular they show that such interactive proof systems have the expressive power of BQP. What I want to know is whether there are analogous results for the multi-prover setting, either for classical or quantum verifiers. Are any non-trivial complexity results known for multi-prover interactive proofs where the total length of all messages is restricted to be logarithmic in the problem size? 

Not a full answer, but hopefully close. This is very close to Luca's comments above. I believe the answer is that at least there do exist constants B∈ℕ, a>0, and 0<s<1 such that Gap-3SATs is NP-complete even for a 3CNF formula which is pairwise B-bounded and consists of at least $an^{2-\epsilon}$ clauses, for any constant $\epsilon$. The proof is as follows. Consider a GAP-3SAT$_s$ instance $\phi$ on $N$ variables in which each variable appears at most 5 times. This is NP-complete, as you say in the question. Now we create a new instance $\Phi$ as follows: 

Certainly in the context of quantum computation systems of dimension 3 or more have been looked at. These are known as qudits in general, and qutrits for 3 level systems. The motivation here has mostly been the available physical systems, rather than any expectation of a significant change in computational power. Indeed in quantum systems it is very easy to see that there is no such advantage: Evolution in quantum systems is described by unitary operations which can be associated with the Lie group su(D), where D is the total dimensionality of the system. Given entangling gates between subsystems, together with individual control over these local subsystems, it is known that it is always possible to approximate any such operator, independent of the dimensionality of the local systems. Obviously, if you can reach all unitary operations on D dimensions, then you can reach all of the ones on $d<D$ dimensions. So, you can use, for example 2 qutrits to replace 3 qubits, or 4 qubits to replace 3 qutrits, with only constant overhead. With this in mind, the relationship between qutrits and qubits versus trits and bits is similar, and the level of interest on a theoretical level has been fairly similar. It is really in the context of physical implementations that qutrits attract interest, since quantum computing has not really settled on a dominant architecture yet, and there are plenty of systems with local dimensionality > 2. You may want to have a look into what are called continuous variable systems, which are a type of system considered in quantum computation which have an infinite number of local dimensions (position is an example of a continuous variable quantity). Such systems do actually exist, and are fundamentally different from analog computers due to the quantization of energy levels. 

The answer to the general question about $R^d$ would seem to be that it is NP-hard. The proof is fairly simple. We simply take a 3SAT instance on $d$ variables and associate each variable with a dimension. Think of the space as a space of possible assignments of variables: we only consider points between -1 and +1 in each dimesnion, and associate locations $<0$ with an assignment of 0 for that variable and $>0$ with an assignment of 1. Each clause excludes a region given by a $1 \times 1 \times 1 \times n \times n \times n ... \times n$ hypercuboid. If the union of these cuboids fills the space (and so contains a $2 \times 2 \times ... \times 2$ cube), then there is no satisfying assignment of variables for the 3SAT instance. If however the largest cube contained is $1 \times 1 \times ... \times 1$ or 0 (for no clauses), the only other possibilities, then a satisfying assignment of variables exists. 

So here is the idea. We have 2 players, A and B. B will supply the input, while A will directly implement a Turing machine. The decks will be composed almost entirely of land, but also the Gemstone Array card to void mana burn. A will have 3 types of land: Islands, Mountains and Forests. The basic idea is to use tapped land to represent a 1 and untapped land to represent a 0. Islands will be used to represent the state of the tape, Mountains to index the current position along the tape and Forests to represent the internal state of 24 state 2 symbol Turing machine (I believe there is a universal one due to Rogozhin). The decks are ordered as follows: A's deck: Gemstone Artifact; 6 Forests (since $2^5 = 32 > 24$ plus an additional forest); For m=0 to infinity: $2^{m+1}$ Islands followed by 1 Mountains. Note that the number of mountains (which can be either tapped or untapped) is always the number required to index every island, plus a halting state. B's deck: Gemstone Artifact; 6 Forests (since $2^5 = 32 > 24$ plus an additional forest); For m=0 to infinity: $2^{m+1}$ Input Lands followed by 1 Mountains. Note again that the number of mountains (which can be either tapped or untapped) is always the number required to index every island held by A, plus a halting state. The Input lands are taken to be Plains (to represent a 0 in the input string), Swamp (to represent a 1 in the input string) and Islands (which are used after the end of the input string has been reached. Strategy: A and B both play one land a turn in the order in which they are drawn. When each has drawn 4 forests they play Gemstone Artifact. Note A goes first, so already has an Island when B draws plays his first input card. A and B simply continue to place their cards in order until B has exhausted their Plains and Swamps and plays their first Island. On his next go, A for all i taps his ith Island iff Bs ith Input Land was a swamp. A initialises his turing machine by tapping his first Forest and Mountain. If he has tapped an odd number of cards he taps his extra forrest, and uses all this mana to add tokens to Gemstone Array. From here on the play proceeds as follows: B uses their turn to simply mirror the state of A's mana. B taps his ith Input Land iff A's ith Island is tapped. Similarly B taps his ith Forest(Mountain) iff A's ith Forest(Mountain) is tapped. As A always taps an even number of cards, so does B, and the mana is used to add tokens to Gemstone Array. On A's turn, all of A's mana becomes untapped, so A looks at the state of B's mana, represents the state of A's mana on the previous turn. A applies the transition rule according to the universal (24,2) machine to B's state to obtain his new state. Play proceeds in this manner until the turing machine halts. At this point, A puts his mountains into the reserved "finished" state (the all untapped state). If the Turing machine halted in an accepting state, B copies the state of A's mountains, but taps all their remaining land neglecting to use Gemstone array, thus beginning the process of suicide by mana burn. On A's turn, if B's mountains are in the "finished" state, and all B's other land is tapped, A simply does nothing (note that his mountains are automatically in the "finished" state). If A's mountains are in the finished state, but nothing else is tapped, B continues suicide by mana burn. This is repeated until B is dead. If however, the machine finishes in the reject state, B leaves all their cards untapped. If all B's cards are untapped, A taps all his cards, beginning the same process of suicide by mana burn. If A's non-Mountain cards are all tapped, and the mountains untapped, B leaves all their cards untapped. This will lead A to continue the mana burn suicide until he loses the game. This should satisfy the criteria asked for in the question, and hence when this ordering is allowed, I believe the game is Turing complete in the sense described in the question. 

The other answers are excellent, and this is not meant to replace or contradict any of them, merely to offer some intuition for why P=BQP does not necessarily imply equality between quantum and classical interactive proof systems (for fixed rounds etc.). We do now, however, know that QIP=IP thanks to the work of Jain, Ji, Upadhyay and Watrous, so I am certainly not trying to claim that such equalities never happen. If we only assume that P=BQP then we learn something only about the which decision problems can be answered by the quantum and classical models. It is not the same as implying that the models are actually the same. The main difference is that quantum computers can process states in superposition, which means that their input and output need not be restricted to classical states. This is a very important difference between quantum and classical models, since the quantum input/output makes it possible to query oracles with superpositions of classical states or to communicate quantum states (which may potentially have exponential classical descriptions) between a verifier and prover. Indeed, oracles do exist which separate BQP from P, and quantum communication leads to reduced communications complexity for a number of problems. Thus, the quantum model is strictly more powerful than having a classical computer supplemented with an oracle for BQP: A quantum computer can do everything a classical computer can do, but a classical computer cannot do everything a quantum computer can do. For this reason, the question of whether P=BQP is not the deciding factor in whether quantum and classical models are equal in situations which make use of communication/oracle queries. 

Basically, most sets of operators which generate only some small subspace of $SU(2^N)$ tend to be simulable, while any that generate $SU(2^N)$ are as hard as general quantum simulation of N qubits. It seems very unlikely that quantum mechanics is efficiently simulable, and so such a dequantization program will likely be impossible in general. There is however a regime where this has worked, which is with interactive proofs. Several different kinds of interactive proof systems with quantum verifiers have been shown to have the same power if the quantum verifier is replaced with a purely classical verifier. For an example of this, see Jain, Ji, Upadhyay and Watrous's proof that QIP=PSPACE (arXiv:0907.4737).