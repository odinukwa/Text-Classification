A good response should be fact based, and any subjective subjects like difficulty should be based on an explained experiences with the product. 

The equivalent to Navigation Meshes for 3D spaces is Navigation Volumes. Havok AI implements both navigation volumes and a volume pathfinder as shown in their GDC 2011 demonstration. The principle of A* in a volume is the same as A* on a navigation mesh. Since A* will find a path over any graph it doesn't matter if the graph is represented by a point to multiple points, a polygon to multiple polygons, or a volume to multiple volumes. The algorithm will still find a solution if one exists. Some slight nuances that are different with paths found on navigation meshes is how you determine path points at the edge of line segments, at the ends, or maybe at the middle? The same can be true of of navigation volumes, to determine the cost to traverse to the next volume you'll typically have to pick a point within the volume, midpoint/edge/etc. This all essentially boils down to the heuristic part of the A* algorithm you must supply yourself, or use a basic Euclidean distance algorithm. Path Following is not Pathfinding How your AI determines to follow this path is something completely different and is referred to as Path Following. The typical strategy for Path Following is to allow your AI to look ahead of where it's traveling to see if it can short cut the path to make more natural curved movements. Havok AI Demo at GDC 2011 

That is a 4x4 column-major matrix, and from the looks of it, a view matrix. The first 3 columns define direction of your basis vectors (up, left, forward as you called them), and the last column defines translation of the eye point. Put them together and you can describe the orientation of your camera, and more importantly you can use this matrix to transform points into a coordinate space known as "eye space", "view space" or "camera space". Those are all synonyms for the same coordinate space. Unfortunately you have to learn all of the synonyms when dealing with computer graphics because different books and people will call them by different names. Most coordinate spaces have multiple names. By the way, the three columns in your view matrix are generally orthogonal, that is they form right angles to one another. This is not required, but it is a very common property when constructing a traditional camera. 

So typically Havok works best with normal human sized objects with a gravity of 9.8m/s^2 and dealing with everything in meters. In my Game though there will be a large variety of scales from millimeter sized objects to meter sized objects. Typically this rules out running a standard Havok setup and dealing with things in a meter scale. Would it be best to increase everything by 10x or some scalar like this and increase gravity likewise? I understand this would also decrease how far objects can be away from the origin but for the sake of this question assume everything is relatively close to the origin, a hundred meters max. Issues of concern Object penetration - Havok usually allows some object penetration in the 2-3cm range in the meter scale. Floating point precision - Issues dealing with distances of objects etc, typically physics breaks down at the kilometer scale in Havok. Unseen - other unseen issues that may result with such large differences in scale. 

Last but not least, here is how a packed texture atlas would be built that would replicate behavior in the presence of a texture filter: (Subtract 1 from X and Y in the black coordinates, I did not proof read the image before posting.)    Due to border storage, storing 4 256x256 textures in this atlas requires a texture with dimensions 516x516. The borders are color coded based on how you would fill them with texel data during atlas creation: 

On the DirectX end of things, prior to Windows Vista issuing a draw call would invoke an expensive kernel-mode context switch. This was never an issue in OpenGL, because it always had a user-mode component to its driver, but D3D only gained a user-mode front-end with the introduction of WDDM. D3D10 also shifts the responsibility of resource validation from usage to load-time, so draw calls are even less expensive when you move from D3D9 to D3D10. 

The Mario AI Allows you to implement an AI Agent to control Mario. Different levels of map details are available to allow a simple implementation or implementations with near engine level map details. The API is a server/server type implementation using Java. Additionally a Level Generation API is provided for creating user generated levels. 

While doing a BS in CS at a California State University there was only one game development course which was group based where each group was to deliver a complete game from scratch in 10 weeks. Each group consisted of 4 programmers. This single game was worth 100% of the grade. It was straight C++ and OpenGL with weekly deliveries from all groups. One of the hardest classes I've ever had but at the same time we learned everything about how game engines really work. Rarely do students learn this anymore since most are spoiled with engines or frameworks that abstract all the "hard" stuff away. My professor published a paper about the class in 37th ASEE/IEEE Frontiers in Education Conference 2007 Student Teamwork: A Capstone Course in Game Programming The game my group created Images from my Portfolio Video of the game from another teammate 

Tells OpenGL that when you fetch something from this texture, rather than returning the actual value, what it is supposed to do is fetch a value and then perform a comparison against the coordinate in your texture coordinates. What coordinate, you ask? Precisely! Using with a texture that performs comparison produces undefined results because you sample a with 2D texture coordinates. There is no 3rd coordinate to perform any comparison using. 

Getting to your question about which function is valid, I am leaning towards neither. This is because some other part of your application may have changed the currently bound texture between the time you called and . At the very least you need to bind your texture before attempting to issue . There is an extension called Direct State Access that avoids the nightmare of having to bind most OpenGL objects to modify their state. This is probably too advanced for you right now, but it is something to consider in the future. OpenGL's bind-then-modify design is quite frustrating at times. 

Sergio you might want to aim more toward a Game Development math book like Essential Mathematics for Games and Interactive Applications, Second Edition: A Programmer's Guide Instead of the classical Linear Algebra you would learn in college. Also like Ron Warholic said, stating what your math comfort level is would better help us taylor a specific book. 

Open Dynamics Engine is another semi-popular open source middleware solution for physics and collision. $URL$ PhysX is another popular collision/physics middleware from NVIDIA. Binary available. $URL$ Last but not least is Havok which is the gold standard of collision/physics. Binary available. $URL$ 

Since there was a little bit of confusion in the comments regarding what is required for a uniform to be active, I will try to explain it here. The first thing you need to understand is that in OpenGL, GLSL programs are built in two phases: 

The proper way of doing this is to implement the alpha test yourself in a pixel shader and any pixel that fails. This will discard the pixel and no depth or stencil values will be written to the framebuffer, without having to write anything to a separate color buffer and perform an extra test afterwards. As discussed in comments, arbitrarily deciding during pixel shader execution to discard a pixel is going to disable some hardware depth buffer optimizations. However, the complexity of your depth-only shader is such that it probably will not affect performance too much. Early Z benefits you most when you can skip complicated pixel shaders (such as those used in lighting) for occluded primitives. This shader basically amounts to a single (probably cached) texture lookup and comparison between 0.0 per-pixel; not complicated at all. 

Ai Game Dev Excellent source of Game Ai information from videos, to interviews, to articles, etc etc. The best content has to be paid for but is worth it since there are a lot of interviews with Game AI developers about techniques that aren't published anywhere else. On top of that it allows you access to their AI Sandbox application for testing out your own AI implementations. 

You can kill two birds in one stone. USC Interactive Media Division or USC Game Development The Princeton Review rated USC as the number 1 Game Design school, Digipen rated 2nd. $URL$ 

Because the variable is the only variable in the fragment shader stage that contributes to an output, it is the only active code path. This means the uniform is part of an active code path and will be assigned a uniform location. , on the other hand, is used to compute something in the fragment shader, but that something is never output. This variable effectively dies inside the fragment shader. If you trace its path back to the vertex shader, you will see that was used to set this value, but does not belong to an active code path. Thus, that uniform is not active and is not assigned a uniform location.