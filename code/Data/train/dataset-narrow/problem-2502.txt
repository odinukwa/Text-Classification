I am trying to understand exactly what the lower bounds for the query complexity of statistical algorithms imply for convex relaxations for the planted clique problem ? A recent paper by Feldman, Perkins and Vempala builds the connection between Statistical Algorithms and Convex Relaxations by showing that for detecting planted k-CSPs, lower bounds on the complexity of Statistical Algorithms can be translated to lower bounds on the dimension of canonical convex relaxations. I was wondering if a similar thing can be said about Planted Clique lower bounds for Statistical Algorithms (which were proven here). In particular what kind (if any) of lower bounds on Convex Relaxations do these imply? 

I am asking the question on a slightly abstract level and it may depend on the specifics but it would be great to have related references or ideas. Consider the random graph model $G_{n,p}$ where its a random graph on n vertices and each edge is selected with probability $p$. I want to prove that a certain property $P$ is satisfied by graphs in this model with high probability ($\rightarrow 1$ as $n \rightarrow \infty$). I know/can prove the following two facts 

Let $f:\{-1,1\}^n \rightarrow \{-1,1\}$ be any boolean function. Let $Maj_n$ represent the majority function. Let $\langle f,g \rangle = E[f(x)g(x)]$ and $\mathcal{I}(f) = E_x[\# i, s.t. f(x)\neq f(x \oplus e_i)]$ be the total influence of a function under the standard definitions. Then does the following identity always hold? $$ \langle f,Maj_n \rangle \leq c*\frac{\mathcal{I}(f)}{\mathcal{I}(Maj_n)} $$ where c is some universal constant. I believe it might be true but have not been able to prove it. I have tried some standard functions and they seem to work. I may be mistaken as I am very new to this area. Could you please suggest a way to prove it ? or some references regarding this Thanks in advance 

(This is about the problem in which $|\phi(\Phi^{-1}(i))|\geq 2$ instead of $|\Phi(\phi^{-1}(i))|\geq 2$. Read it too fast. On the bright side Dave fixes it in a comment to this message ) What about saying it computes a proper edge coloring of a regular graph ? This problem is NP-Complete, and amounts, given a graph as an entry, to find a partition of its edges into matchings. Vizing's theorem says that to do that Delta (the maximum degree of your graph) or Delta + 1 colors are required, though deciding which is NP-hard. In your case, I think setting $N$ to the be number of edges, and setting K to Delta would do the trick. You then want to split your $N$ edges into $K=\Delta$ classes, and N is a multiple of Delta (for example in 3-regular graphs, for which the problem is still NP-hard). In order to ensure that the answer is a proper edge coloring, one can let $\phi_v$ (for each vertex $v$) be the function equal to $0$ when edge $e\in [N]$ is adjacent to $v$, and 1 otherwise. If each color class has at least two different images for each $\phi_v$, it means that each color class contains a edge incident to each vertex. As the graph is Delta-regular, it also means that all the edges around a vertex have different colors as there are only Delta edges around each vertex, and if a class had two it means one would have none. If it's true it would mean that finding one answer to your question is hard, and sampling solutions too :-) Nathann 

Conditioned on the event that the graph obtained from the sample is regular, the probability of the property being satisfied is very high (something like $1 - 1/n^c$) Given any graph G and any other graph G' that can be obtained by removing edges from G. If G satisfies the property then so does G'. 

I am looking for a way of getting a good estimate of the eigenvalues of random bipartite d-regular graphs. The literature has very precise values the proofs of which are very involved and since I am looking to extend the estimates to a more general scenario I dont want to get into very involved techniques. The kind of bounds I am looking for can have constant (or even log factors) thrown around. One way I have in my mind is to use the matrix Bernstein inequality by expressing a random bipartite d-regular graph as a sum of d independent random matchings and then black box the Matrix Bernstein Inequality result. This gives satisfactory answers for me with the caveat that summing up d random matchings does not necessarily produce simple graphs (edges can get repeated), however I feel that the estimate that we get from Matrix Bernstein should hold for the random regular graph case too. Is there an easy way to get around this difficulty? Thanks in advance 

The best is probably to read the (unfortunately) long list of methods related to graphs... $URL$ (I wrote a short tutorial there : $URL$ ) Nathann 

I'd be surprised if there existed a graph library recognizing them all (there is a lot of them), but most of the algorithms you will find in the litterature are focused on a very small amount of classes... For example chordal graphs (easy to recognise), or treewidth-bounded graphs (much harder.. hopefully the algorithms given for these classes are almost useless in practice), planar graphs, ... In Sage ($URL$ we are trying to build a large graph library, with recognition algorithms and methods to solve NP-Hard problems. For instance, we have the following methods : 

You main gain significant time by first computing the fractional chromatic index, which would tell quickly if your graph is class 2. Then Vizing's algorithm would probably do. In Sage -- even though that's probably not the best solution -- we solve it by LP. If you guys have anything that could help us to solve edge coloring, please please tell me :-) $URL$ The function computing the Fractional Chromatic Index will be available in the next release. 

There is "Encyclopedia of Algorithms" from 2008, which surveys a lot of different problems (1160 pages of it) $URL$ 

I wanted to add this as a comment but it was too long. I am not sure if this completely answers the question. For bipartite graphs for instance we can possibly get a simple first cut bound from a simple trace method. Lets look at the adjacency matrix. In your case we want to show that $\lambda_k$ is not close to 0. So consider $Trace(A^2)$. For a d-regular graph this is always $\geq dn$. Therefore $$\sum \lambda_i^2 \geq dn$$ which implies that $$2d^2k + n\lambda_k^2 \geq dn$$ Now for $k << n$ and $d$ a constant one can essentially get a $\Omega(\sqrt{d})$ bound for $\lambda_k$. So indeed they cant be very close to 1 in the normalized laplacian. I am not sure whether the parameters work out in the way you want them to. 

I have seen a bunch of results concerning Matrix Completion, PCA, Compressed Sensing where a common theme has been to relax the Rank constraint/objective by replacing it with Nuclear Norm. I was wondering if there is a survey of some sort which collects these results, compares them and presents the basic underlying technique. I havent read the original papers yet so they might be the best reference but the purpose of the question is to know if there are other easier to understand references to get started on this topic with. Thanks in advance 

One million nodes is far too much for any exact method that I know. This being said, I expect your graph does not have a very large number of edges, so the best is to begin by applying repeatedly this algorithm : If you have a vertex in your graph that has not outgoing edge, or that has no incoming edge, remove it from your graph as no cycle can go through it. Take care, as removing such vertices can lead to remove other vertices that had edges in both direction before ! This should reduce dramatically the size of your graph (look at what is called "topological sort"). Once this is done, you can compute the different strongly connected componets, as your cycle is included in one of them. Then, you can do the computations separatedly. Perhaps the splitting of your graph will have created new vertices that you can remove. Iterate through these two algorithms until you can do no other operation in your graph. When it is done, come back here and ask your question again giving the new number of vertices. When you do so, do not count the number of vertices having exactly one incoming edge and one outgoing edge. They do not change the complexity of the problem. If after all this your graph is still 1M vertices large, it begins to really be desperate :-) Nathann 

Can the above two statements be enough to make a general statement about the probability of the satisfaction of the property in $G_{n,p}$ Any references would be very helpful? 

Can we prove a sharp concentration result on the sum of independent exponential random variables, i.e. Let $X_1, \ldots X_r$ be independent random variables such that $Pr(X_i < x) = 1 - e^{-x/\lambda_i}$. Let $Z = \sum X_i$. Can we prove bounds of the form $Pr(|Z-\mu_Z|>t) < e^{-t^2/\sum (\lambda_i)^2}$. This follows directly if we use the variance form of chernoff bounds and hence I believe is true , but the bounds that I read require bounded-ness or have some dependence on bounded-ness of the variables. Could someone point to me to a proof of the above ? 

Are there results/techniques pertaining to the analysis of squares of random matrices ? More specifically, let $A$ be an $n\times n$ matrix such that each entry is $1$ or $-1$ independently and with equal probability. Now if we want to analyze for any $u,v \in \{-1,1\}^n$, we can make a case for concentration of the value of $u^TAv$ using chernoff bound arguements. However suppose now we want to analyze the value of $u^TA^2v$. This time due to a lot of dependencies among the variables a chernoff type arguement becomes difficult or at least I cannot see it straightaway. Could someone point me to an analysis for this scenario ? 

You may also like to know that this "sum of squares of the degrees" is also called "First Zagreb Index". When you type it in Google you get results like this one : $URL$ 

You should try Sage's implementation. It uses LP, but I don't think that you would get something so large in less than 100milliseconds. Greedy probabilistic would be nice in this case I guess. $URL$ 

I like to think this way too, and I link it with the $P=NP\cap co-NP$ conjecture. It is mentionned there or there (I am having a hard time finding pages about this conjecture as I can only refer to it with math symbols that Google does not like). A problem is in NP when you can give a certificate for a "True" answer (example : a hamiltonian cycle for the TSP problem or a valid assignment for the SAT problem). It is in co-NP when you can give a certificate for a "False" answer (example : a "proof" that the SAT formula is not satisfiable, a bad cut in a graph that prevents the existence of a hamiltonian cycle, etc...) Actually, those may be the "obstructions" you may refer too. And this conjecture is about saying : a problem which is in NP is polynomial when I know what the obstructions are. Now, the "obstructions" can take many different forms. For the matching problem (which is polynomial) it can be a partition of the graph (see Tutte's characterization of graphs admitting a perfect matching), or it can be the certificate given by Farkas's Lemma for linear programming (and graph problems that can be reduced to it). It can actually be a great many things, and so I usually "use" this conjecture in one direction : When I can find no obstructions, I do not deduce that my problem is NP-Hard. Some obstructions are really hard to find... But when I have a complicated polynomial algorithm for a problem, I am sometimes convinced that "there must be an understandable set of obstructions, which would be easier to understand that the complicated algorithm". Well... My two cents :-) Nathann 

I am interested in the following problem which seems like an extension of the Kruskal-Katona Theorem. Let $A_k \subseteq \{0,1\}^n$ be a subset of the hypercube such that every element in $A$ has exactly $k$ ones. For any element $x \in \{0,1\}^n$ let $N_l(x)$ be the set of elements obtained by flipping one of the 1's in x to 0. (Generally referred to as the lower shadow of X) Let the majority upper shadow of $A_k$ referred to as $M_u(A_k)$ be the set such that for each $a \in M_u(A_k)$ number of ones in $a = k+1$ and $|N_l(a) \cap A_k| \geq (\frac{k+1}{2})$. That is more than half of a's neighbours are present in $A_k$. Given the size of $A_k$ can we put an upper bound on the size of $M_u(A_k)$. Has this problem been studied and are there results are relevant to the above. Note that in case $|A_k| = \binom{n}{k}$ we of course have that $|M_u(A_k)|=\binom{n}{k+1}$. In general I am looking at the size of $A_k$ to be $\epsilon\cdot \binom{n}{k}$ where $\epsilon$ is a small constant. Could you also refer to me a good survey of the Kruskal-Katona Theorem in general , one that surveys recent results in this setting ? Thanks in advance 

I am quite new to the area of metric embeddings so this question might turn out to be extremely easy. Consider a metric supported on the edges of a boolean hypercube. By supported I mean every edge of the boolean hypercube has a non negative distance associated with it and the metric is defined by the length of the shortest path according to the distance function between any two vertices. Can we put upper bounds/lower bounds on the distortion when we embed such a metric into $l_1$ ? Any references would be highly appreciated. 

If you ask questions like that, I feel that the only answers you could get are problems that "reduce to the maximum clique". That would be a mistake. There are many problems in practice where finding a maximum clique is thought of as a subroutine, as it is not the most expensive part of the algorithm. Finding cliques is known to be NP-Hard, and depending on the instances it is or it is not. NP Hard does not mean anything in practice. My (reduced) own experience tells me that I would prefer 1000 times to deal with a maximum clique problem that with a TSP, or worse : an integer multiflow. Max clique is for me one of the "kind" NP-hard problems. And it means that I sometimes reduce problems to Max Clique because it can be done this way, and because I know that I will get the result fast in practice (and also because I already have all the functions available, so there is nothing new to code). If you are interested in pratical problems that can be formulated immediately as a max clique problem, I am afraid you will not find many. Practical problems have their own aspect. However, I solved one thousand time "Max Clique" problems, often on optimization problems which would not have required it, because Max Clique is, somehow, an easy problem, and because it is so general of aspect. I guess the last time I computed a maximum independent set (which is exactly the same) was in order to obtain a decomposition of a graph (a partition of its edges into copies of a given graph). The graph had hundreds of nodes, and solving the max clique problem on it was very far from being the bottleneck. Hoping it helped,