Disclaimer: I have never built a Teradata system, so I can't claim this from first-hand experience, but I will explain the reasoning. I think that Teradata will be able to produce this view efficiently. From what you say, it appears to do little more than join some very small dimension tables against a fact table. The join operations will be relatively efficient. Unless I misunderstand your requirements these columns are allowing your application to select various rollups of data from a multi-grain fact table. Even though Teradata is a shared-nothing system, I can't see any requirement for the view to push large semi-joins across nodes or anything like that. Beyond that, all I can suggest is that you suck it and see. If you don't have anywhere to experiment you could download the express version of Teradata off their web site and see if you can prototype this structure to see what the query plan actually is. 

If you script the procedures out to a file, the search/replace can be trivially dome with a sed script along the lines of 

Actually I was a bit underwhelmed with VS2010, to be honest. I think an old-school create table script and files for stored procedures are easier to work with. If you need schema management then you can get Redgate SQL Compare Pro for a few hundred dollars. If you really need a database modelling tool then Powerdesigner or even Erwin does a much better job, although they're not particularly cheap. Although I prefer SSMS I've used both. Some pros and cons: 

The vendor does offer a forum and a knowledge base of sorts for the tool. Plus, there is also the manual. I don't really understand what you mean by 'everyday database admin concepts' with respect to a modelling tool, though. Could you elaborate on that? However, asking specific questions on data modelling tools is certainly on-topic for DBA.SE. If you get to 'How do I do xxx in Erwin?' type of questions then it's fairly likely someone here is familiar with Erwin and can answer the questions. 

2NF deals with 'the whole key' - if you have a composite key and some members of the relation are dependent on a part of that key then they should be split off into their own relations. In your case: 

I have a requirement to read reference data from several access databases for an ETL job. The data owners have produced a file with a series of linked tables that consolidates them all into one place. However, if one of the base files is open with a lock (.ldb) file the OLEDB provider will refuse to open the linked table. The SSIS package is set up with several data flow tasks reading from OLEDB sources. To clarify, these are reference tables, and not necessarily being updated at the time - the .mdb file has a lock (.ldb) file. Is it possible to configure the MS jet OLEDB driver to force-open the linked tables, ignoring the locks? A couple of alternatives that we've looked into are: 

Create a cube with Date, Product and Reseller dimensions and a SalesTrends fact table with a single metric 'Price'. The MDX query below will produce counts of products for which the price is increasing, static or decreasing in a given period. 

The value depends very much on the individual organisation and its requirements. Depending on the level of sophistication required, a B.I. role might fall into a few different categories: 

I am a completely certification free zone, although some might describe me as certifiable. Hasn't hurt my career in the slightest, but I have worked for a couple of MS gold partner companies where there was a requirement to have a minimum level of certified staff in order to keep the gold partner status. 

A lot of vendors try to keep their database schemas close to their chest. As often as not, it's about keeping a lid on their dirty little secrets like abject lack of data integrity or obviously poor database design. Other reasons include: 

I was a bit underwhelmed with Oracle OLAP when I evaluated it circa 2005, mainly as it had poor support from front-end tools at the time (Discoverer 'Drake' had no drill-through support, and there was practically no support from third party tools). In the end that project went with MS Analysis services. @Ali's post suggests that it does have support from OBIEE now, so if you have licenses for that you can probably put a reasonably frendly front-end on the cube. While somewhat exuberant, the points he makes are fundamentally sound. In answer to your question: 

Automation will probably get you 95% of the way with some manual intervention and the scripts aren't all that hard to write. It's not a 5 minute job, but it's certainly possible. You could programatically generate SSIS packages to do the load, but I think that generating bcp control files is probably easier. Another option would be to just structure the scripts so they copy the data from the queries into another shadow database. This can then be backed-up/restored to wherever you want. It's not a trivial undertaking but it's certainly not beyond the wit of man. As a bonus, if you can configure your script so the starting keys can be parameterised, then you can make a generalised utility to copy subsets of your application database. This will be quite useful for rolling out test environments. 

With spinning-platter disks you want to have the logs and data on separate drives as random access data disrupts the sequential log write operations, making the logs a performance bottleneck. SSDs do not have this issue as they lack the performance constraints imposed by the mechanical action of conventional hard disks. If you're getting SSDs for a DB server, get ones designed for a long life span, such as Intel S3700s. You're probably better off getting two of those than four cheaper ones, and 2x100GB units + a hot spare (which is probably enough to keep you going for a few years at your current rate of growth) should cost around £200 each according to Google Shopping. Mixing logs and data on the same disks isn't such a big deal any more. However, you may get some resiliency out of having separate log drives. If you want to do this then 5 disks (enough for two pairs + a hot spare) should still see you with change out of £1,000. Try two good quality drives in a RAID-1, with a hot spare for a starter. Your growth suggests you probably won't have transaction volumes so high that this setup can't keep up with your application workload. Back up your database onto spinning disks - a couple of enterprise-grade SATA disks and a hot spare should cost you a few hundred dollars. If (and only if) you have performance problems with that lot then you might look into adding more SSDs, but I suspect just one pair will be fast enough to keep up with your transaction volumes quite comfortably. 

MDX Drillthrough allows you to specify a limit for the row set returned. As far as I am aware it doesn't have a 'show how many rows this would produce' facility. You could fake it by having a count measure for each of your fact tables and selecting this for the slice in question before you execute the drillthrough. I'm not sure whether Excel 2010 has much in the way of facilities to manage this out of the box. Back when I were a lad Excel 2000-2003 didn't do drillthrough out of the box so you had to roll your own. This is an example of a VBA script to do a drillthrough action. You can hook the menu on the pivot table fairly easily to provide access to this sort of script. This would at least allow you to provide some sort of validation. Introspection into the pivot table through the API is possible if a little fiddly. You can get the dimension and slicer state out. Back in the SQL Server 2000 days I did a 'drill across' this way that allowed you to open another cube on the same slice (think insurance policies going across to claims). Now SSAS supports cubes with multiple fact tables (measure groups) this is less useful. However, if you could intercept the (IIRC) double click event that would kick off a drillthrough and knock that on the head then you could use the introspection to formulate a count query that could guard against over-large drillthroughs. This would then pop up a dialog box along the lines of 'You're going to drill through into 1,347,961 rows.' 

The answer is: yes, there is a benefit to doing it. Reports on on operational database will use a lot of resources and will interfere with the performance of the operational system. Remember that database performance is subject to mechanical constraints (disk heads moving back and forth and rotational latency as we wait for the right sector to make its appearance under the head). You have two broad options for a reporting strategy: 

The basic technique is quite straightforward. When you read the record you take a note of the version or timestamp column, e.g. 

I'm not quite sure what you're asking, but I think you want to do the following (correct me if I'm wrong): 

Business Intelligence Edition Business Intelligence edition has some useful features, like Master Data Services and non-additive aggregations (i.e. anything but sum/count). EE has partitioning and the rest of the large database features. The EE features are mostly relevant to users with large data volumes. If you have less than (say) 100GB of data then you can probably get by with BI edition. B.I. edition also has a limit on the number of CPU cores and memory that can be used by the database server, although this does not appear to apply to Analysis Services or Reporting Services. A more detailed breakdown of the S.E., B.I. and E.E. features can be found here. Some things that will be fine with B.I. edition 

Query tuning 101 There is no magic silver bullet to query tuning, although I can give you some hints and tips. The first thing to do is to understand what's actually going on behind the scenes. Get a good internals book like the third Guru's Guide book. Poorly performing queries tend to come in two basic flavours: Transactional queries that take too long, and grinding batch jobs (or reports) that take too long. One good sign of a query with something wrong with it is a single item in the query plan taking 99% of the time. Transactional queries On most occasions a poorly performing transactional query is one of a few things: 

I think it's probably appropriate here to note that relational algebra <> SQL. Relational algebra (the theoretical underpinnings of relational databases described in Codd's paper 'A relational data model for large shared data banks') is not Turing complete. The model has the property of Godel completeness, which makes it equivalent in expressive power to first order predicate calculus - ordinary logical expressions to you and me. However, most SQL dialects have various constructs including recursive CTEs and flow control in stored procedures which make them effectively Turing complete. A Turing complete language can express any computation that can be described algorithmically. It's worth noting that the strict definition Turing completeness requires infinite storage, which is not physically possible. However, this requirement is often relaxed informally when describing programming languages as Turing Complete. 

If you're talking about identifying candidate keys for relational synthesis you need to know the dependencies. If you have the functional dependencies you can use the relational synthesis algorithm; a synopsis of which can be found here. Note, if this is a homework question please mark it as such in the tags. 

I believe that it can, and the API wraps a fairly well documented web service interface called XML/A. The SQL Server 2008/2008R2 management software will work with SSAS 2005 cubes as well. 

I did something like this once with a document parser tool I wrote. It would strip out specially formatted comments from a DDL statement and produce a data dictionary. This was renedered with LaTeX. However, I still had to reverse engineer the database into Visio to make E-R diagrams. For a limited subset of SQL needed to parse create table statements it wasn't too hard to write the parser. I had it working in a couple of weekends. 

cmd.exe actually has significant pipelining capabilities, although not as rich as a unix shell. You can get Win32 ports of tee and grep from the GNU Win32 collection, or from Microsoft Services for Unix (SFU). The GNU Win32 ones are better because they use msvcrt.dll and understand native Windows paths. SFU places an emulation layer over the base O/S in much the same way as cygwin, so it emulates style paths that behave in a more unix-like manner. An alternative to this would be to use WSH, powershell or even a small .Net console app to implement a small utility to read the output from the log file and present your prompt to the user. Reimplement the process in a way that allows the control to be automated. If you can agree business rules for this control it can be run automatically and simply log the result, generating a status report when the process has finished running. It seems quite strange that you are developing a server-side process that requires interactive input from an operator. If this process has to be run interactively, is there any reason why the interactive parts could not simply be run from somebody's PC? This would probably allow you considerably more flexibility in installing third party components or bespoke client-side code.