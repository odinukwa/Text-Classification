I use an iSavi IsatHub for Internet connectivity when I'm in the backcountry out of cellular service. This device is a BGAN terminal and it works by establishing a data link with an Inmarsat satellite and then provides a WiFi network to connect your devices. Data usage is very expensive, so it offers a number of safeguards to prevent accidental usage. One of those safeguards is a firewall. I only need SSH connectivity, so I blocked all outgoing ports with the exception of TCP port 22, UDP 53 and TCP 53. All inbound traffic is allowed. This past week I was in the backcountry far from cell service. I fired up the IsatHub and connected my phone to the WiFi network. Much to my surprise, I started receiving text messages to my Verizon phone. More to my surprise, I was able to reply to those text messages and have a back and forth conversation. The satellite terminal has its own SIM card and SMS capabilities, but these messages were received over my Verizon SIM. I've searched the net for any documentation on network ports used for WiFi texting and calling. I found this and it does include TCP53 and UDP53 (which I assume are for DNS lookups), but it also includes UDP500 and UDP4500 which are standard IPSEC VPN ports. Does anyone have an idea of how this was possible? How the heck was I able to send text messages over WiFi with my Verizon phone when I've blocked all outgoing ports except those needed for SSH and DNS? Is it somehow related to accepting all inbound traffic? Could Verizon be re-appropriating port 53 for WiFi texting? Edit: When I returned home, I connected my phone (airplane mode, wifi turned on) to my home WiFi network, sent a test text message and took a capture of the network ports in use. The only ports I saw in use between my phone and Verizon-owned IP addresses were ports UDP500, UDP4500 and TCP443 (500 and 4500 being used for the IPSEC VPN ports I mentioned earlier). 

We have a cabinet of dedicated servers with a Brocade FESX648-PREM switch at the top. We own all the hardware and all the MAC addresses are known to us. We also fully manage the servers so we retain administrative access, but our customers have administrative access as well. Currently we use VLANs and subnets to isolate the servers, prevent IP hijacking, etc. However, we want to make more efficient use of our IPv4 allocations. We lose a lot of usable IPs for gateway purposes and we have an abundance of subnets too small for many customer needs. I've considered progressively migrating the servers to a single VLAN and use static ARP to prevent IP hijacking. We'd lose layer 2 isolation, but I'm not sure what sort of abuse that would open us up to. What are the dangers of placing all of our servers on a single VLAN with only static ARP bindings to prevent IP hijacking? In other words, what forms of abuse would be possible with a single VLAN config that would not be possible with every server on its own VLAN? Are there any precautions we can take to prevent the abuse while achieving our goals of more efficient IPv4 usage? 

Some people are fussy about terminology, but I've usually heard "rack" for place to put 19-inch mounted equipment and shelves, and usually "cabinet" when it has doors. If it's both then it's a matter of emphasis or local custom. How you organise your servers and patching and routers is up to you and depends a lot on what you've got and what changes are likely. How open or closed they are depends on security, money, thermal and noise. For example, many servers are rather deep; most networking equipment is not. For networking things, especially patch panels, I normally like to have the vertical rails quite far from the front, so the patch cables have room when the door is closed. I don't really distinguish between fibre and copper cabling, just how far it's going. Fibre obviously needs more room though. I've often found it beneficial if the patch panels are all in one rack as they're going to be a pig to move. I like sides on racks and proper routing for inter-rack cabling. In a couple installations I put all power and interrack cabling out the top so not a single cable touched the floor -- was a basement and yes when there was flooding we didn't much care. Big wheels on the bottom (with brakes) will be a lifesaver when, eventually, they have to move. Finally, don't get racks taller than your doors! Just some thoughts. EDIT: I can also hugely recommend UPS-per-rack (at the bottom) and inter-rack patch (at the top). With good wheels you can keep your systems up during a move if you want to. 

A host (Windows or any other) tries to keep its ARP table accurate by as many means as it can. Normally there is a timeout for the cache entry and it's just discarded after a period since it had confirmation of this address (typically in the 60 to 1500 second range) -- but this is just to deal with the worst case scenario. It also will update it more swiftly under many other conditions, in particular any incoming packet from that other host. In the case you describe, a PC and a router, if the router suddenly changes ethernet address in mid-traffic, there are typically a number of TCP connections communicating. One of them is likely to be a packet from the router, with the new ethernet address, to the PC. At this point the PC udpates its cache, and starts the timer. If the next packet is outgoing, the PC can invalidate its cache entry when it does a TCP retry, and perform normal ARP request. (Whether a given OS does this is a topic for elsewhere.) If the outgoing traffic is send-only (perhaps syslog over UDP) and there really is nothing coming from the router, then the PC won't know anything about the change, and will expire this cache entry after timeout since we last heard from the router -- which would have been the last ARP reply. The normal case is that when any host changes ethernet (or IP) address, it sends ARP announcement packets to tell all the other hosts of the new details. The worst case situation is that the PC keeps sending something to the old ethernet address until the cache times out, then issues a new ARP request and packet delivery resumes. The ARP RFC 826 is really a recommended read. The address conflict RFC 5227 covers a lot of good casees too. Just for a postscript, it's terribly rare for routers to change their MAC addresses except if a) it's actually a different router, or b) there's some reconfiguration of some kind. To solve the single-point-of-failure problem for hosts without good routing protocols, hot-standby router protocol and similar provide methods for an ethernet address to be shared by several routers. (HSRP RFC 2281, VRRP RFC 5798). 

I have a Brocade FastIron FESX648 and I'm attempting to increase the ARP cache timeout on a port that is connected to an IXP. According to the Brocade docs, I can increase the timeout up to 4 hours with the setting . I've tried applying this setting globally, specifically to the IXP-facing port and variety of other combinations (lower and higher timeout values), but nothing seems to work. Whenever I run I can see that the ARP entries for IXP hosts are continually refreshed and never age higher than 4 minutes. Has anyone faced a similar problem with Brocade FastIron gear and have any config suggestions or possible workarounds? 

Anyone have any ideas where I should look next? The results of #2, #3, and #11 in particular are really throwing me for a loop... 

I'm working on testing several FESX448-PREM switches. One of the switches in my test group is known to be bad. It was previously installed as a top of cabinet switch, 42 servers were connected to it, all port lights came on, full duplex, no errors, low CPU, etc but ports 13-24 would not forward traffic. As I understand it, this is due to a bad ASIC that covers port region 13-24. However, I now have this bad switch at my work bench and I cannot replicate the same forwarding issue with port region 13-24. At my work bench, I have port 1 as the uplink and I've been connecting my laptop to ports 2-48 sequentially using a CAT6 cable while running a continuous ping to a public IP. Interestingly, all the ports now work fine -- port region 13-24 no longer has forwarding issues. Does anyone know how this is possible? If there's a bad ASIC for port region 13-24, then I'd expect this problem to occur 100% of the time. I tried a couple other things afterwards. I had the theory that I needed more ports active at once in order to trigger the forwarding issue. So I first took a layer 2 switch and connected it on a bunch of ports with the FESX448. CPU usage immediately went to 100% on the FESX448. I figured something recursive routing was happening with the layer 2 switch. Next, I put the layer 2 switch into boot monitor mode so it wouldn't do any routing. That resolved the 100% CPU issue, but again I'm still unable to replicate the traffic forwarding issues with ports 13-24. Any suggestions on how I can replicate the forwarding issue and effectively test the remaining switches would be much appreciated! 

Unless the wifi is intended for administrative access, this would not be a great idea. You will encounter all kinds of interference and increased latency. Your server cabinet is most likely bonded to the ground and essentially amounts to a Faraday cage. Not to mention all the other large metal objects in a datacenter that could cause interference. The datacenter is also very unlikely to allow you to stick antennas outside of your cabinet to improve the signal. Point-to-point wireless bridges such as these work very well and are widely used. However, they are typically deployed on rooftops where unobstructed line of sight can be established and are used for pulling connectivity into a building where fiber is lacking. 

I was curious about the views of the many very experienced network people here on the use of "IP" for "IP address". Obviously it follows the very common English mechanism where adjective-noun is shortened to adjective when frequently used: "laptop computer" to "laptop", "Hoover vacuum cleaner" to "Hoover" (much to the chagrin of intellectual property lawyers), and has a certain familiarity ("best friend" to "bestie", "hometown boy" to "homeboy" to "Homie"). Wikipedia editors commonly use "IP" for "IP address user" (ie identified by IP address rather than username). But of course IP is a thing of itself (subject of RFC 791), and we speak of "internet protocol numbers", ie 17 being UDP. How many experienced engineers use "IP" to mean address? I confess it still jars a little and makes me ask every time if someone means address, and was wondering if I should stop sticking my heels in and accept it as what they meant? (per Cratylus, or perhaps Humpty Dumpty.) 

[EDIT: updates at bottom] It turns out to be to do with the encryption algorithm, and thus indirectly related to the version of Cisco IOS. The router on the bench is just to help me debug the problem. The real issue is with a moderate amount of installed CPE routers in a global telemetry network. The configs of the tested routers are almost identical as regards SSH access, and vary somewhat as regards interfaces. [EDIT: of course none in the field accept telnet, only on the bench to help with debugging.] [EDIT2] After further testing, the differentiator is the encryption negotiated between the two sides. A survey found this client works against with AES128-CBC and fails with AES128-CTR. A survey of some available routers shows that Cisco SSH server works with default configuration on 15.2(4)M3, 15.2(4)M6, 15.3(2)T2, 15.3(2)T, 15.3(3)M; and fails with 15.4(3)M1, 15.4(3)M3, 15.5(3)M. We are using 867VAE-K8 + 15.5(3)M, and the following minimal pair config change makes it fail/work: 

I've used read-only cables for similar purposes: invisible syslog sinks. My understanding is that in 100baseT only autonegotiation is done with the "fast link pulse" FLP, while link-connectivity is still done with the "normal link pulse" NLP. The suggestion of using a third interface, to prop up the link, should work. I'd love to try this: 

Consider the directed broadcast coming from outside a local network. As the routers forwarding the packet don't have the network mask (of the far end) it is not possible to identify a directed broadcast IP address. For example is a broadcast address for a /28 but not /24 network. A router connected to 192.168.0.0/28 might or might not accept the incoming packet and broadcast it. See for example RFC 2644 on this topic. Also remember that there are critical layer matters in broadcasting, from the point of view of switches and media transmission in general. See for example my answer here. 

I presume you don't mean a site-to-site VPN such as from a small office to HQ, which would normally be WPA2 to the access point, then encrypted in the tunnel to HQ. A VPN on your laptop will be tunnelled before the wifi, so what the eavesdropper sees is lots of WPA2 packets from your laptop to VPN peer. It will be encrypted ONLY if the VPN is set to use encryption. Most are, but many are just for addressing convenience. In many kinds of laptop-based VPN, encryption is an option, which you need to make sure is enabled. While I'm here: many don't know that one kind of "encryption" available in HTTPS is "NULL" -- all the trappings of SSL are there but there isn't any encryption. (Eg look up aNULL and eNULL in Apache SSL configs.) Most web servers are set up to use good encryption only, but if they made a mistake, and your laptop negotiates NULL encryption then the data is naked. So you could have WPA2, a VPN from your laptop and be on an HTTPS web site and still have no (practical) encryption. The truth is often an empirical matter: packet sniffing on your laptop before and after the VPN is sometimes the only way to tell.