From a signal processing point of view, you're sampling a continuous-domain signal, and you need to filter it to get rid of frequencies beyond the Nyquist limit. It's that filtering that leads to integrating over the pixel area—or more generally, integrating over the support of your antialiasing kernel (which need not be a box). Consider your rendering function that takes a sample point $x, y$ in screen space and returns back the color found at that point. (Let's ignore any issues of random sampling for the moment, and assume it returns a "perfect" rendered color for that specific point.) This function effectively defines a 2D continuous-domain signal. Or to put it another way, it defines an infinite-resolution image, since nothing prevents this function from having features at arbitrarily small scales. In terms of frequency domain: the function is not band-limited; it can include components of arbitrarily high spatial frequencies. Now you want to convert it down to a finite number of pixels. Much like digitizing an audio signal, when you sample it, you'll get aliasing unless you first eliminate frequencies beyond the Nyquist limit imposed by the sampling rate. In other words, you have to get rid of features smaller than the pixel grid. To do this, you apply a low-pass filter. The ideal low-pass filter is the sinc function, but for various reasons of practicality we use other filters (that don't perfectly eliminate frequencies beyond the Nyquist limit, but they at least attenuate them). Low-pass filtering is done by convolution. If $f(x, y)$ is the rendering function and $k(x, y)$ is a filter kernel, then we can define the low-pass filtered image as: $$f_\text{filtered}(x, y) = \iint f(x', y') \, k(x' - x, y'- y) \, dx' \, dy'$$ Then the image is safe to sample, so the final pixel values are gotten by just evaluating $f_\text{filtered}$ at the pixel coordinates. If $k$ is a box filter, which looks like $k = 1$ within the pixel box and $k = 0$ elsewhere, then this simplifies to just integrating $f$ over the pixel box. But as noted, the box filter isn't so great and there are better choices such as tent, bicubic, and Gaussian filters. Anyway, now that we have an integral, we can use Monte Carlo for it and combine it with any other integrals we might like to do—for lighting, motion blur, and so on. We can even apply importance sampling to the $k$ factor in the integral, by generating samples for each pixel that are distributed around the pixel center according to $k$. 

Using for colors is massive overkill, but using for internal representation of colors is common for all the reasons above. When working with GPUs, (16-bit float) is also common, as they support that format in hardware. 

The 8x MSAA image (which is not quite a grid but still has a repeating pattern) still plainly has jaggies in it, although they're antialiased jaggies. Compare to the TXAA result, which both has a higher effective sample count (due to temporal reuse) and uses a Gaussian rather than box filter for accumulating the samples. On the other hand, random sampling produces noise instead of aliasing. There is no pattern to the sample locations, so no pattern to the resulting errors. Both aliasing and noise are errors due to not having enough samples to form a clean image, but arguably noise is the less visually-objectionable artifact. On the other other hand, perfectly random sampling (in the sense of i.i.d. random variables) tends to exhibit a certain level of clumping. Purely by chance, some areas in the domain will have denser-than-average clumps of samples, and other areas will be lacking in samples; those areas will be, respectively, over-represented and under-represented in the resulting estimate. The convergence rate of the Monte Carlo process can often by improved by using things like stratified sampling, low-discrepancy sequences, or blue noise. These are all strategies to generate "de-clumped" samples that are spaced out a bit more evenly than i.i.d. samples, but without creating any regular patterns that might lead to aliasing. 

PIL's function performs dithering by default when you convert the image to 1-bit—not simply thresholding. That's what creates the noise along the edges of the shapes; the antialiasing isn't the problem. Unfortunately, at such a low resolution, the dithering does more harm than good. I grabbed your original image from your other question and did a straight threshold at 128, and it comes out looking a lot better: 

Sigma and kernel size of Gaussian filter Regarding how to choose the sigma and the kernel size (pixels) of the Gaussian: you set the sigma based on how wide of a blur you want (judging it visually) and then choose the kernel size based on the sigma. It's a game of finding a kernel size that captures enough of the (mathematically infinite) bell curve to look good, while not being too expensive. As a rule of thumb, the kernel size should be at least 6 times the sigma. Then the kernel extends out to 3 sigmas in each direction, which is enough to cover almost all the weight of the mathematical bell curve. It's also nice to round up the pixel size to the next odd number so the filter will be symmetrical. So for example, with sigma = 1.5 px, you'd use a 9-pixel filter; with sigma = 2.0 px, use a 13-pixel filter, etc. Alternatively, if you have performance limitations that control how big a kernel size you can use, then divide by 6 to get the maximum sigma you can get away with. (Note that sigma can be fractional, as it's just an input to the bell curve equation.) By the way, a trick to get better performance on large Gaussian blurs is to downsample the image, perform a smaller blur, then upsample. For instance, to get an effective 13-pixel blur, you could downsample the image by 2x (using bilinear filtering), then perform a 7-pixel blur, and then upsample (using bilinear filtering). It will look almost as good as the 13-pixel blur, but will be considerably faster. Creating a good-looking glow filter A common trick with these is to use multiple Gaussians of different radius, added together. It makes a better glow effect than any single Gaussian by itself. For example, here's a mockup in Photoshop created with three layers additively blended together. The three Gaussians are of size 3px, 5px and 11px (corresponding to sigma = 0.5px, 0.83px, 1.83px respectively). 

Without explicit light sampling, I'd expect very slow convergence. Any path that doesn't happen to hit the light source before being terminated will be useless (contributes zero). Since you have a small light source, the vast majority of paths will not hit it. It would be interesting to add some statistics to your tracer to see how many paths return zero vs nonzero. I'd guess you'll find something like 99–99.9% of them are zero, so you're only getting one useful sample out of every 100–1000 or so paths traced. Naive path tracing is more reasonable with e.g. a sky light, which almost every path can be expected to hit. 

Vertex formats are specified in D3D11 using the input layout. An example input layout description for your vertex might look like: 

The value (the third parameter to these calls) indicates which descriptor slot to use. You have all of them set to 0, which won't work. They need to be distinct. For example, they could be 0, 1, and 2. 

Real-time graphics deploys a variety of approximations to deal with the computational expense of simulating indirect lighting, trading off between runtime performance and lighting fidelity. This is an area of active research, with new techniques appearing every year. Ambient lighting At the very simplest end of the range, you can use ambient lighting: a global, omnidirectional light source that applies to every object in the scene, without regard to actual light sources or local visibility. This is not at all accurate, but is extremely cheap, easy for an artist to tweak, and can look okay depending on the scene and the desired visual style. Common extensions to basic ambient lighting include: 

Use the Gram–Schmidt process. This works as follows: first, normalize $u$. Then, for each additional vector, subtract off its projection onto the previous vectors, and normalize what's left over. For the 3D case this looks like: $$ \begin{aligned} u' &= \text{normalize}(u) = \frac{u}{|u|} \\ v' &= \text{normalize} \bigl( v - (u' \cdot v)u' \bigr) \\ w' &= u' \times v' \end{aligned} $$ Note that the notation $(u' \cdot v)u'$ means take the dot product of $u'$ with $v$ (giving a scalar), then multiply that scalar by $u'$. This gives the vector component of $v$ that is parallel to $u'$. When this is subtracted from $v$, the remaining component is perpendicular to $u'$. Finally, $w'$ is set to the cross product of $u'$ and $v'$ to ensure the basis is right-handed. Since $u', v'$ are already orthonormal, we don't need to normalize $w'$—it comes out already normalized. (This isn't strictly part of the Gram–Schmidt process.) By the way, the Gram–Schmidt process works for making an orthonormal basis in any number of dimensions, not just two or three. The other way, which only works in 3D, is to use cross products. $$ \begin{aligned} u' &= \text{normalize}(u) \\ w' &= \text{normalize}(u' \times v) \\ v' &= w' \times u' \end{aligned} $$ Here, $w'$ has to be normalized because $u'$ and $v$ are not orthonormal. But then $u'$ and $w'$ are orthonormal, so we can calculate $v'$ from them without a normalize. This gives the same results as the previous process, so it's up to you which you prefer to use. 

Yes, all vectors used for lighting need to be in the same coordinates. If you do lighting in eye space, the normals would need to be transformed to eye space as well. Lighting calculations can also be done in world space, or any other space that's convenient. You're right that normal vectors shouldn't receive translation; likewise other vectors that represent directions or displacements rather than points in space. If you represent transformations as 4×4 matrices, it's simple to skip the translation by just leaving off the last column. Also, one subtlety that comes up is that if your transformations contain non-uniform scaling (different scale factors along different axes) or shear, then you need to use the inverse transpose matrix for normals. This is to ensure that the normals stay perpendicular to the polygons. (The inverse transpose will be the same as the original matrix if neither scaling nor shearing is present.) 

The polar coordinate system commonly used in BRDF definitions is set up relative to the surface being shaded, i.e. in tangent space. The $\theta$ angle measures how far you are from the surface normal while $\phi$ is the azimuth around the plane of the surface relative to some reference direction (which doesn't matter unless the BRDF is anisotropic). So if you want to convert to these coordinates, you have to first get your vector in tangent space (with the origin at the intersection point and two of the axes aligned with the surface), then apply the usual Cartesian-to-spherical transformation. However, normally you can and should evaluate BRDFs without using polar coordinates, trigonometric functions, or angles at all, but just using vector math primitives such as dot products. This is usually more efficient and it's more robust, as you don't have to deal with angle wraparound, factors of pi, out-of-range arguments to inverse trig functions and so on. For instance, you probably know that the cosine of the angle between vectors can be obtained by just dotting the (normalized) vectors. The sine and tangent can be obtained through trig identities from the cosine (i.e. from the dot product). Fabian Giesen wrote an article on this topic, Finish your derivations, please, that refers to the exact Oren-Nayar article you linked and gives an alternate, trig-free form: 

In a real GPU, instead of having multiple cores trying to read/write the same region of the depth buffer and attempting to synchronize between them, the depth buffer is divided into tiles (such as 16×16 or 32×32), and each tile is assigned to a single core. That core is then responsible for all rasterization in that tile: any triangles that touch that tile will be rasterized (within that tile) by the owning core. Then there is no interference between cores, and no need for them to synchronize when accessing their part of the framebuffer. This implies that triangles that touch multiple tiles will need to be rasterized by multiple cores. So, there is a work redistribution step between geometry processing (operations on vertices and triangles) and pixel processing. In the geometry stage, each core might process a chunk of input primitives; then for each primitive, it can quickly determine which tiles the primitive touches (this is called "coarse rasterization"), and add the primitive to a queue for each core that owns one of the affected tiles. Then, in the pixel stage, each core can read out the list of primitives in its queue, calculate pixel coverage for the tiles the core owns, and proceed to depth testing, pixel shading and updating the framebuffer, with no need of any further coordination with other cores. 

Since you want to downsample the image by a factor of 2 along each axis, a simple and easy thing to do is just average a 2×2 box of source pixels to generate each destination pixel. In pseudocode this would look like: 

In order to ensure that the pattern shapes are always either wholly present or absent, never cut off, it's necessary to ensure that the same value is used for all texels within the shape. In your example of circles, all the texels in a given circle need to agree on . I assume that you have some way of evaluating at a given point on the surface (whether it is looked up from a texture or calculated from some function). Then one way to ensure a group of texels all get the same value is to ensure they all look it up from the same point. The UVs of this evaluation point could be stored in extra channels of the pattern texture. For instance, you could have the red and green channels store the UV coordinates at which to evaluate , the blue channel store the threshold at which to turn on that pattern element, and the alpha store the antialiased gray level of the pattern to display. The UV+threshold data could also be in a separate, secondary texture if desired. To generate this UV+threshold texture, starting from an input pattern texture, you could programmatically find connected components (e.g. by searching for black pixels and flood-filling). Set the evaluation point for all texels in each component to the UV of the component's center, and generate a random threshold for it. Then, when rendering, use a pixel shader that first samples this texture, then looks up at the given evaluation point and compares it to the given threshold. That way, each pattern shape will see a uniform value and threshold, and will either turn on or turn off fully. As increases, more of the shapes will pass their threshold and appear, giving the impression of a continuously varying density. 

As far as I can tell, the main advantage of half-edge is that traversal can be a bit simpler due to a guarantee of edges having a consistent orientation within each face. Consider the problem of iterating over all the vertices or edges of a given face, in counterclockwise order. In the half-edge structure, this can be done by starting with an arbitrary half-edge of that face and simply following the "next" pointers until you get back where you started. In contrast, doing this in a winged-edge structure is a bit annoying, because the edges are oriented arbitrarily; any given edge that you encounter might point either clockwise or counterclockwise relative to the face you're trying to iterate around, so you have to do an extra conditional check at each step to see if you should traverse the current edge forward or backward. Other kinds of connectivity queries behave similarly: the half-edge version lets you follow links in a consistent sequence while the winged-edge version requires orientation checks at each step. Whether the conditionals are actually a performance problem for winged-edge will probably depend on other factors. For a "naive" implementation with pointers every which way and data scattered across memory, I'd expect cache miss overhead to swamp any effect of the conditionals. On the other hand, if you have a tightly packed data structure with everything hot in the cache, you might see some effects from the conditionals due to incorrect branch predictions, etc. It's hard to say. Leaving performance alone, I'd prefer half-edge just because it seems easier to reason about and write correct code for, even if it results in a slight memory overhead. By the way, there are a couple of other design choices with mesh data structures that often seem to get confused with this one. A commenter mentioned single vs double linking, but naturally you can do either single or double linking with either half-edge or winged-edge. (Although I don't see how single linking with winged-edge would even work, since as mentioned above, you might have to traverse edges either backward or forward in the course of a query.) Also, there's the question of whether the vertex and face structures store a list of all their edges, or just one (and require you to traverse the edges to find the rest). Having a variable-length list of edges per vertex/face considerably complicates the logic if you want to do it efficiently (i.e. not having a separate heap allocation per vertex/face), but again this is independent of whether the edges are half-edge or winged-edge.