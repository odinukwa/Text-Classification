They show how to take an input state $|\psi\rangle$ and output two qubits such that the reduced density matrix of either output qubit (let's call it $\rho$) satisfies $\langle\psi|\rho|\psi\rangle=5/6$. This was later shown to be optimal by several authors: 

EDIT: Suresh points out that there is a short definition of leaf languages in the Wikipedia article. I'm copying it below. Several complexity classes are typically defined in terms of a polynomial-time nondeterministic Turing machine, where each branch can either accept or reject, and the entire machine accepts or rejects as some function of the branches' conditions. For example, a non-deterministic Turing machine accepts if at least one branch accepts, and rejects only if all branches reject. A co-non-deterministic Turing machine, on the other hand, accepts only if all branches accept, and rejects if any branch rejects. Many classes can be defined in this fashion. 

Before that question can be answered, we have to make the model precise. We can think of Grover's algorithm as solving the following problem: $\exists x\; \varphi(x)$? In this model you only have access to $\varphi(\cdot)$ through an oracle who will evaluate it at any input you like. So another way of thinking of the same problem is that Grover's algorithm evaluates the following formula: $\varphi(1) \vee \varphi(2) \vee \ldots \vee \varphi(N)$. And this can be done in $O(\sqrt{N})$ queries, where it would need $N$ queries (deterministically) and $\Omega(N)$ (bounded-error) classically. Similarly, $\forall x \exists y\; \varphi(x,y)$ can be written as a formula using only AND, OR and NOT over the variables $\varphi(i)$. In fact, it is a formula in which every variable appears exactly once (such formulas are called "read-once"). A very general result in quantum algorithms says that any read-once formula over the gate set {AND, OR, NOT} over $N$ variables can be evaluated in $O(\sqrt{N})$ queries. This result has a long history, beginning with generalizations of Grover's algorithm to handle bounded-error inputs, a breakthrough result of Farhi, Goldstone, and Gutmann (Scott Aaronson's blog post), a beautiful connection with span programs, an SDP characterization of quantum query complexity, etc. The final $O(\sqrt{N})$ bound for any read-once formula over that gate set is from "Reichardt, B.: Reﬂections for quantum query algorithms. In: 22nd SODA. (2011) 560–569" This almost answers the quantum part of your question, unless I've misunderstood it. However the classical query complexity of read-once formulas isn't well understood, so I can't tell you whether you always get a provable quadratic speed up or not. Sometimes you do get a quadratic speedup, as in Grover's algorithm. If the formula is balanced and each gate has fanin 2, i.e., if it looks like $\forall x_1 \exists x_2 \forall x_3 \ldots \varphi(x_1, \ldots x_n)$, then the best classical algorithm, which happens to be a zero-error randomized algorithm, needs $O(N^{0.753})$ queries, so it's not really a quadratic speedup. 

Yes, there is an oracle $A$ such that $\oplus P^A \not\subseteq PP^A$. In fact, there is an oracle $A$ such that $\oplus P^A \not\subseteq PP^{PH^A}$. You can find the result in the following paper. 

I'm not sure if we should be answering such questions, since it is at the level of a homework question, and not a research level question. So instead I'll just give a hint to the OP so the OP can figure out the answer with some effort. (Other posters may choose to give out the explicit solution if they wish, of course.) The concept you refer to, i.e., reducing the search problem (or optimization problem) to the decision problem, is called self-reducibility. It is known that all NP-complete problems, and some others like Graph Isomorphism have this property. See this powerpoint presentation on self-reducibility. It explains in detail how SAT, GI and NP-complete problems in general can be shown to be self-reducible. If you read those notes and understand those examples, you should be able to do the same for TSP using similar ideas. Hope that helps. 

This is Assignment 3, Question 5 in Richard Cleve's ongoing intro to quantum computing course. (Seems like this assignment was due today.) While we're not supposed to answer homework questions on CSTheory, fortunately the assignment answers all your questions. It also takes you through the construction of the quantum algorithm. I strongly recommend reading it. 

I like this post by Scott Aaronson, which explains complexity theory as quantitative theology. Here's an excerpt: 

If it were known that it is NP-hard to approximate VC better than a factor of 2, even if the UGC is false, then we would unconditionally know that approximating VC to a factor better than 2 is NP-hard. This also works with a factor smaller than 2. If it were known that if UGC is false, then VC is NP-hard to approximate better than a factor of (say) 1.5, then we would know that it is (unconditionally) NP-hard to approximate it better than a factor of 1.5. Since we don't know that, I guess the best we can say if the UGC is false is that it's NP-hard to approximate it to within a factor of 1.3606 by the result of Dinur and Safra (2005). (I think that's the current best-known inapproximability result.) 

The strongest result I am aware of is that for all k, there is a problem in $S_2^P$ that requires circuits of size $\Omega(n^k)$. $S_2^P$ is a class contained in $ZPP^{NP}$, which is itself contained in $\Sigma_2^P \cap \Pi_2^P$. (The complexity zoo has more information about this class.) The result follows from the strongest version of the Karp-Lipton theorem due to Cai. A quick proof of how this follows from the K-L theorem: First, if SAT requires super-polynomial size circuits, we are done, since we've exhibited a problem in $S_2^P$ that needs super-polynomial size circuits. If SAT has polynomial size circuits, then by the strongest version of the Karp-Lipton theorem, PH collapses to $S_2^P$. We know PH contains problems such problems (by Kannan's result), and thus $S_2^P$ contains such a problem. 

The paper shows (or gives a citation for) an oracle separation between almost every pair of classes that you might care about between P and PSPACE (e.g., it has classes like P, RP, BPP, UP, FewP, NP, MA, AM, other levels of PH, PH, IP, PSPACE, etc.). For example, Theorem 8 shows an oracle problem in coRP that is not in NP. Since (relative to all oracles) coRP is in BPP and NP contains P, we get an oracle problem in BPP that is not in P. As I mentioned in my comment, showing an oracle for which $\text{P}^A = \text{BPP}^A$ is easy. Let A be a EXP-complete language or a PSPACE-complete language. 

Another way to generate problems that are outside P but not P-hard is to take complete problems for classes incomparable with P. Say a class X is incomparable with P, in the sense that neither is a subset of the other. Then a X-complete problem is necessarily outside P (otherwise P would include X) and is not P-hard (otherwise X would include P). I tried to think of some classes incomparable with P, but P is a pretty robust class, so there aren't too many such classes. For example, RNC and QNC might be incomparable with P. DSPACE($\log^2$) might also be incomparable with P. PolyL is incomparable with P, but doesn't have complete problems under logspace reductions. 

Here's an example that might be of the type you're looking for. The parameter is not an integer though, it's a pair of numbers. (Although one of them can be fixed to make it a one parameter problem.) The problem is to evaluate the Tutte polynomial of a graph G at coordinates (x,y). We can restrict the coordinates to be integers. The problem is in P if (x,y) is one of the points (1, 1), (-1,-1), (0,-1), (-1,0), or satisfies (x-1)(y-1)=1. Otherwise it is #P-hard. I got this from Wikipedia's article on the Tutte polynomial. 

It's NP-hard. Here's a reduction from the feasibility version of Binary Integer Programming (BIP), which is NP-hard. The problem is to decide if there's a feasible solution to the constraints $Ax \leq b$ and $x_i \in \{0,1\}$. It's easy to convert this to a problem with the constraints $Ax \leq b$ and $x_i \in \{-1,1\}$. Now consider the following optimization problem: $\max \sum_i x_i^2$ subject to the constraints $Ax \leq b$ and $-1 \leq x_i \leq 1$ for all $i$. This problem has objective value $n$ (the total number of variables $x_i$) if and only if the original BIP problem was feasible. 

Indeed, as wwjohnsmith1 said, you can get a square root speed-up over Schöning's algorithm for 3-SAT, but also more generally for Schöning's algorithm for k-SAT. In fact, many randomized algorithms for k-SAT can be implemented quadratically faster on a quantum computer. The reason for this general phenomenon is the following. Many randomized algorithms for k-SAT that run in time $O(T(n) \mathrm{poly}(n))$ (where $T(n)$ is some exponentially growing function of $n$) actually do something stronger. At their core, there is a polynomial-time algorithm that outputs a satisfying assignment, if one exists, with probability at least $1/T(n)$. From this it is clear that if you repeat this poly-time algorithm $O(T(n))$ many times and accept if any of the runs returns a solution, you will get a randomized algorithm for k-SAT that runs in time $O(T(n) \mathrm{poly}(n))$. Now instead of running this algorithm $O(T(n))$ times, you can run amplitude amplification on this poly-time algorithm. Amplitude amplification is a general quantum algorithm that can decide if another algorithm accepts with probability 0 or with probability $1/T$ using only $O(\sqrt{T})$ uses of this algorithm. Applying amplitude amplification to such a k-SAT solver will immediately yield a quantum algorithm for k-SAT with running time $O(\sqrt{T(n)}\mathrm{poly}(n))$, which is quadratically faster (ignoring the poly(n) term). 

Both these classes exist. I know that Quasipolynomial-time algorithms arise frequently in many areas. A google search will reveal several known quasipolynomial-time algorithms. Some of these may actually be QPLIN algorithms. One of the advantages of QP over QPLIN is that it's more robust. For example, it is closed under composition, so if you have a quasipolynomial-time subroutine that takes an input of size $n$ to an input of size $n^{\log n}$ and now run a quasipolynomial-time algorithm on this new input, you will have an algorithm that runs in $O(n^{\textrm{polylog}(n)})$ time. 

The question makes sense, and the short answer is that it's an open problem. Here's the long answer: Depending on how you define constant-depth unbounded-fanin quantum circuits, you might get different classes. QAC0 is usually defined to have unbounded fanin Toffoli gates and single-qubit gates. QAC0wf is the class where we also allow a "fanout" gate, which copies an input bit to many outputs. (It implements |a>|0>...|0> --> |a>|a>...|a>) This class is really powerful since it contains, besides PARITY and AC0, also ACC0 and TC0. So the obvious question to ask is whether PARITY is contained in QAC0, and this is an open problem. It is equivalent to asking whether QAC0 = QAC0wf. I guess that the belief is that PARITY is not in QAC0. Further information can be found in the survey Small depth quantum circuits by Bera, Green and Homer. 

On a side note, it's not clear that EBPP is a robust class. For example, if instead of allowing the algorithm to flip an unbiased coin, if it were given an unbiased 3-sided coin, or a 6-sided die, it's not clear that you get the same class. BPP remains the same if you change these details. Anyway, your primary question is whether EBPP is equal to BPP or not. It seems to me that EBPP is closer to P than it is to BPP. Consider the query complexity or oracle version of these classes where they have access to a large input string and have to make queries to learn bits of this string. If you have a P algorithm that computes a function $f$ with $Q$ queries, then there exists an exact representing polynomial of degree $Q$ for $f$ over $\mathbb{R}$. (This is the usual polynomial method argument.) On the other hand, if you have a BPP algorithm, then you get a degree $Q$ polynomial over $\mathbb{R}$ that approximates $f$ in the sense that its value is close to the value of $f$ at every input. Given an EBPP algorithm for a function $f$, we can construct a polynomial that outputs 1/4 when the answer is NO and 3/4 when the answer is YES. By subtracting 1/2 and multiplying by 2, you can get an exact representing polynomial, just like in the case of P. This suggests to me that EBPP is closer to P. This observation can also be used to show an oracle separation between EBPP and BPP. Consider the promise-Majority problem where you're promised that the input has either more than 2N/3 1s or less than N/3 1s and you have to decide which is the case. This is clearly in BPP. Using the polynomial argument described above it can be shown that this function requires $\Omega(N)$ queries for an EBPP machine. But note that you can also prove an oracle separation the other way, between P and EBPP. So maybe oracle results don't say much for this problem? Or maybe what they say is that it will be hard to show equality in either direction. 

Since $G$ is a minor of $H$, $G$ can be obtained from $H$ by deleting edges, isolated vertices and performing edge contractions. It's also easy to show that we can insist that the subgraph operations are done first, i.e., we can first perform all the edge and vertex deletions and then perform all the edge contractions. Moreover, let us restrict the definition of "edge contraction" to disallow contracting edges where one of the vertices has degree 1. Contracting such an edge is the same as deleting it, so this will not change the definition of graph minors. Let $H'$ be the graph obtained from $H$ by performing all the edge/vertex deletions first. $H'$ still contains $G$ as a minor. If we show that $H'$ contains $G$ as a topological minor then we're done, since the definition of topological minor also allows edge/vertex deletions. Since $G$ can be obtained from $H'$ by edge contraction only, $H'$ and all intermediate graphs must have maximum degree 3 since there is no way to decrease the maximum degree of a graph by performing an edge contraction. (This would have been possible if we had allowed the contraction of edges incident on a vertex of degree 1.) So consider any step in the conversion of $H'$ to $G$. The only types of edges we can contract are those with both degree-2 vertices or one degree-2 vertex and one degree-3 vertex. (All other combinations don't work. For example, edges with two degree-3 vertices will give rise to a vertex of degree 4 when contracted.) And now we're done, since if $H_1$ is obtained from $H_2$ by contracting an edge with two degree-2 vertices, then $H_2$ can be obtained from $H_1$ by performing edge subdivision on that edge. Similarly for an edge with one degree-3 vertex and one degree-2 vertex. Thus $H'$ can be obtained from $G$ by performing edge subdivisions only, which means $G$ is a topological minor of $H'$ and thus $H$. 

(Note that the problem is equivalent to checking if $U_1U^\dagger_2$ is the identity matrix, which is called the identity check problem.) 

I have no idea how one would disprove a claim like that, but I doubt that it's true. We do have other exponential speedups by quantum algorithms that don't rely on the Abelian HSP. Moreover, Abelian HSP is not known to be BQP-complete. On the other hand, problems which are known to be BQP-complete are problems like computing Knot invariants, other manifold invariants, partition functions and doing Hamiltonian simulation. With an oracle for any of these problems, BPP would be as powerful as BQP. Finally, I'm sure one can construct an oracle separation between the two classes you mention, but that would not be a fair way to compare them since one class can make quantum queries and the other cannot, so the separation would merely reflect this fact. 

In the other case where we're promised that they're not too close when unequal, the problem is indeed coQMA-complete. 

Some observations: First, let's take a different parameter called c. The problem is to find a minimum weight connected subgraph which connects at least c vertices in S. Now c=2 is the easy problem you refer to, and c=|S| is the Steiner tree problem. I think for c=$|S|^\epsilon$, the problem is still NP-hard. The reduction would basically add lots of useless terminals connected to the rest of the graph with very high edge weights, so that these vertices will never be picked in the min weight tree, and the min weight tree will only have the vertices we wanted. For example, an algorithm to solve this problem for c=|S|/2 can be used to solve the Steiner tree problem by just taking the instance of Steiner tree, and doubling the number of terminals and connecting them to the rest of the graph with large edge weights. As you mentioned, the c=2 case is in P. Similarly, for any constant c', the c=c' case is easy by a brute force search.