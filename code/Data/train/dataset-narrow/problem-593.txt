I found no other relevant log entries in the Error Log or Event Viewer. The closest error that happens in the Event Viewer is: 

I am currently experiencing a situation where an application is sending several identical queries at once (I found this in SQL Profiler). This application is buggy, it is probably in loop somewhere and the responsible team is working to solve this problem. But the same application also provides other services that are essential to the company, so I can't just disable it. I have to live with it, until the team solves the problem. So far so good. But this SQL Server instance also maintains other applications, whose performance are being penalized due this occurrence, some of them timeout their operation which makes this situation a DoS, in practice. My question is: what can I do, in the DB side, to minimize the impact to the other applications? I don't care if the buggy app gets penalized, but I would not like to penalize the other ones. (I can't just disable it, but I can slow down their queries results, if possible.) 

I am analyzing the size of some tables in Oracle, and I noticed that the primary key occupies almost the same size of its related table. To be clear, my statistics are the following (approx.): 

The scenario: One of our heavy used production databases, which runs several ETL jobs, and long running table batches, entered in Recovery Mode and became inaccessible for some time. This happened three times this week (this server is on for ~2 years, and we didn't notice this issue in the past). Looking into the errors logs what happened was clear: the Transaction Log was full, the database needed to rollback the transaction, the rollback failed, the database shutdown, and started in recovery mode. The DBA defends this as normal behavior of SQL Server. That is, according to him, every time the transaction log gets full and a transaction needs to rollback the database will enter in Recovery Mode due to the lack of log space. After the rollback (that can only be done in Recovery Mode according to him), the database will become available again. I found no reference for this info. So I strongly disagree. I would really appreciate if someone convince me that I am wrong. My point: As far of my knowledge, a DBMS is built to manage/run queries. If it lacks space, the query will fail. Simple as it is. And I am not talking about performance of anything else, but availability only. It makes no sense for me to accept that a DBMS needs by design to shutdown itself to rollback any transaction. In my understanding, it does not matter if I am running tons of queries or if the queries are bad designed. The bad queries should fail and life continues. Doesn't it? My guess is that something else is making it fail, and I need to track what is happening. Is my understanding wrong or this is really how SQL Server is designed to work? Supposing I am not wrong, what else can I do to track the source of this issue? 

This error happened about ~18 minutes before the database start the recovery process, and repeated sometimes during the beginning of the recovery. It is somewhat related with the DBA user, but I really don't know what it is (I had no time to ask for the DBA yet). 

I am facing a situation that it is being somewhat hard to address. I need help to understand what is happening. TL;DR: Every time the Transaction Log gets full in SQL Server it needs to shutdown the database to enter in Recovery Mode and rollback the offending transactions? Is this always done by design or this only happens when something bad happens? 

I've seen suggestions to set the "Use 32 bit runtime" option, but this had no effect. I've seen suggestions to set something similar in BIDS, but I didn't use BIDS to generate this package. I've seen options to use the 32-bit version of DTExec but I don't think the 32-bit version is installed on the server. SQL Agent Job definition: 

You need to avoid casting your backup size to varchar or you will not be able to sum the values. If you need to then you can do that conversion afterwards. Other than that it is just a matter of wrapping the whole lot up as a subquery, grouping the results by database name, and doing the sum (You may be able to do it other ways but that seemed the least amount of re-working for me). 

I'm looking to set up SQL Server 2012 installation with an Always On Availability Group, where the 'passive' replica will be hosted at another site on the WAN and using synchronous data commit - the idea being that we will have a hot standby with no loss of data in the event of a failure at our primary site. One potential problem that I foresee is that the secondary site has slower storage than our primary site. I don't care about that in the event of a failure, we can live with slow speeds for a period of time until the primary site is restored. My worry is that, because we are using synchronous commit, that the slower disk speed at the secondary site will affect performance at the primary site during normal operation. Is this a valid concern, or is it likely that the slower speed will be offset by, for example, the disk not having much read activity in comparison to the primary site? 

If you've tried everything else then perhaps it would be possible (making sure you have a good backup first!) to detach the database, rename the log file (so SQL Server cannot find it) and then re-attach the database. I believe this will force SQL Server to create a new log file. Whether it will also stop thinking that the database is replicated I have no idea, but it seems at least possible. 

leave them with the data warehouse database put them on the server that will have Reporting Services installed on it create another server specifically to host those two databases. 

EDIT #2 I just completed a backup using the GUI database backup in case my script was wrong somehow - This gave the same result of a damaged backup. -- This only started happening last night. The only significant change that I am aware of is that I changed the 'auto-shrink' option of the database (it was originally set to true). Is anybody aware of a problem in this area with SQL Server 2012 (specifically I'm on version 11.0.3128)? It could just be coincidence or related to something else entirely but that seems the most likely cause at the moment. In addition does anybody have any advice on what to do in such a situation? The database is functioning fine (as far as I can determine), but I don't care to be without backups for very long...! 

I have a database that seems to be functioning fine with no apparent errors except that any full backup taken is broken - attempting a restore fails with an error "RESTORE detected an error on page (18469:-1767164485)" DBCC CheckDB on the database completes without errors. EDIT #1 The backup is created with the following command: 

Some of the mystery is solved - The reason that some sites could connect and some couldn't is that there was an additional connection string stored in the machine.config file that two of the sites were using. The site that worked when a port number was specified did not use this connection string, so worked. The other sites didn't work when port number was specified because I hadn't added the port number to this connection string in machine.config. Adding the port number to that connection string as well as the ones stored in web.config allowed all the sites to work. Additionally, the reason that iisreset had an effect is that once IIS knows the port number that SQL is using, it seems to 'remember' the port number, and so it no longer has to rely on using port 1434. Once I did an iisreset it 'forgot' what port number was used, so had to make a request on port 1434, which for some reason doesn't work. So my question has gone from a very confusing 'works sometimes' situation to a basic 'cannot communicate on port 1434' situation. 

I'm drawing up a proposal for upcoming infrastructure changes. This will include a production server and reports/data warehouse server, each with Always On. To keep hardware and licencing costs down, is it possible to run in a configuration of Server-A running Prod-AG Primary and Rep-AG Secondary, and Server-B running Rep-AG Primary and Prod-AG Secondary? I presume each server would need 2x of the following WSFC instances,sql instances, AG's, listeners, DNS names/ports. I hope this makes sense, here's a diagram of what I think it will looks like. 

I finally got the right balance of performance and accuracy. The below will return the top 100 queries based on average total elapsed time. Its for SQLCMD mode, you'll have to change that if you don't want to use it in that setting. 

If this is a one off and this isn't production, you could put the database into simple recovery mode, do all the deleting you need to do. Shrink the log file at this point if you don't want it to be 500GB. Then put it back into its previous recovery mode. 

I believe this is to do with the plan cache. Just ran a little test on my end and I can replicate it. To remove the table from tempdb works, obviously target the plan handle for the stored procedure. You could do this but it would just come back again and freeing the cache all the time generally isn't the best idea. If you're concerned about it taking up space you can run this query I wrote for a monitoring tool that should work. On my tests it always shows 0KB after the SP has finished. This leads me to believe it just keeps the shell of the table there for some internal use. So it seems harmless to me. 

Microsoft documentation mentions they're removing it, it says "Next Version" but i assume this was written for a previous version. They advise to use maintenance plans in the future. Microsoft Documentation for sqlmaint Edit: Last update on that article was 03/14/2017. But given all the examples point to it being originally written with SQL Server 2008 in some and edited with later versions as examples (it's all over the place to be honest), it's probably safe to say the note at the top is wrong and it's actually removed already. 

In the case of a fail-over on either node, the workload/business need isn't that great that running off the same server for a couple of hours would be a major issue. I've only found a couple of mentions of a similar setup that kind of worked but no definitive information from Microsoft or anyone who's successfully ran this setup. SQL Edition will be 2017, most likely standard, I don't think we'll be approved for Enterprise. OS will be Windows Server 2016 Core. 

how about going the OUTER APPLY route. You can modify the inner query to handle nulls differently, i've just disregarded them but if you want to handle them you could use ISNULL or CASE in the future. 

I has this issue, this tutorial did the trick for me. $URL$ Should be noted an easy work around for me was just to change the account to NETWORK SERVICE in config manager, if you're not fussed about using a dedicated account for security reasons it will likely work for you also. 

Thanks to @SQLWorldWide who's suggestion about reading the DB from the xml, that added alot of time (3mins) but while i was playing around within dm_exe_query_plan i noticed the value column was equal to the Database ID. it might not be 100% encompassing of all traffic but its fast (1 second) and works for what i need.