If I'm using immutable servers/containers, do I need tools like Chef, Puppet, Ansible, or Salt? Those config management tools are designed to establish a configuration and then maintain it. If I'm deploying immutable servers, should I use configuration management tools only for initial provisioning? 

I'm ITIL certified (though it has been a while.) I agree with Tensibai: ITIL and DevOps aren't incompatbile, but that doesn't necessarily make them great friends. The argument can be made that the processes in ITIL must happen in some way, especially for larger organizations. Successful integration of DevOps practices, where ITIL is already practiced, requires careful planning, communication, and execution. Then again, that's true of any DevOps Transformation. For a "greenfield" transformation where neither ITIL or DevOps are in place, I'd craft a combination of both using "mapped" terminology as you have described. As long as everyone in the organization is on the same page, using the same language, ITIL and DevOps can add value when combined. 

What is an artifact repository? If it's just a place to store files, can't I just use a source control system? 

I'm trying to make an Windows Nano Server container that runs an ASP.NET Core app. I'm building it on Windows Server 2016. I can make it work, but there is one odd problem. The only way I can make it work is to build the image, run the container, then start an interactive Powershell session and use . Once I do that, the app is visible from other servers (I can't browse the container locally from the Win2016 Server due to a known NAT bug.) Here's my Dockerfile: 

According to the documentation, memory storage is intended for testing purposes only and should never be used in production. The only practical purpose I can think of for this might be state testing, such as with Serverspec or Inspec, to validate configurations. Using memory storage for this purpose might be a little faster and use less disk storage. I'm not sure those benefits are worth the effort to implement. 

I don't think there are any "universal" DevOps KPIs. For example, velocity is great, unless it's not a key driver for your business. Amazon cares a lot about velocity because they have a massive retail operation. That's less important for a small app with 100 users. This begs the question: how do you select the best KPIs relevant to your business? That's a research and discovery process that involves your entire Enterprise. What do you care about? 

What keeps your business stakeholders up at night? What determines whether you make money this quarter or not? The list above might include some of those things, or it might not. Make your list, then figure out how to align incentives across every department to achieve them. Incentives drive behavior, so decide collaboratively on SMART goals. Pick two or three items off your brainstormed list, and start a measure/fix feedback cycle for each. Don't pick too many at once- you're more likely to succeed by focusing hard on two or three things. 

The two tools I've seen for this are InSpec and ServerSpec. Serverspec is a Ruby-based tool that builds on RSpec. InSpec is inspired by RSpec and ServerSpec. I've used ServerSpec. It's cool, but maybe not 100% stable. I've had problems with testing for specific versions of software on Ubuntu. I've read the InSpec docs but haven't dug in deep. It does essentially the same thing as Serverspec. Judging by the Github commits, it looks like work on ServerSpec has tailed off somewhat, whereas InSpec is just getting ramped up. 

A value stream is a process that adds value to a collection of things that have less inherent value. The canonical example there is an assembly line. I have little interest in a disconnected collection of parts (steering wheel, shifter, gas pedal.) On the other hand, I'll pay good money for a car. Car manufacturers add value to the parts in the form of facilities, skilled work, supply chains, etc. Parts go in one end of the stream, cars come out the other. Enterprises use complex processes to deliver their products and/or services. Value stream mapping is a process used by companies to figure out how and where they deliver value. The purpose of value stream mapping is to develop a clear understanding of the steps, time, and resources required to support a business capability that delivers value. Value stream mapping is useful in kaizen, or Continuous Improvement. The "systems thinking" employed in DevOps and Lean encourages small, incremental improvements made throughout a complex process. A value stream map can expose areas of waste, such as unnecessary or inefficient steps in a value stream. A complete map tends to expose those areas of waste. This helps prioritize improvements. Value Stream Mapping can be applied at large scope to a business process, or to a smaller scope within software delivery. The software delivery process itself mirrors an assembly line. Code is written, reviewed, tested, integrated, and tested again. It is delivered from one environment to another (in manufacturing this is called a work center) until it is finally delivered to Production. 

Store environment-specific configurations on a mounted volume, and make the volume mapping a deployment variable Use configuration service such as in the External Configuration Store pattern 

Cloud technologies are very hot right now, but they can be expensive. What are the best strategies for learning/trying cloud services without racking up a huge bill? 

"Hiring a DevOps" will be tough because DevOps isn't a role. IF you can find an engineer with software development and engineering skills, you're on the right track.. However, Your hiring process will be complicated by a few factors: 

I Googled "Push on Green" and the first link was: $URL$ This was representative of almost the entire first page. It looks like this term originated in Google's SRE group and has been taken up by the industry at large. You are correct- "push on green" means that deployments are automatically executed when all tests pass. This could be considered synonymous with "Continuous Deployment." The accepted method for automating deployments to environments is to automate testing. If your automated testing is rigorous enough, you can deploy any code for which the tests pass. In very mature enterprises (the DevOps unicorns of all shapes and sizes) this can lead to Continuous Deployment all the way to the production environment. My personal experience is that reaching this level of automated testing is a challenge in any enterprise. I am currently in the process of pursuing this level of maturity in my current position. Our first major milestone will be "push on green" deployments to any environment beyond "Integrated Development" (our "lowest" environment.) This challenge is both technical and cultural. As an IT organization, we owe it to our business stakeholders to prove that our testing is in fact rigorous enough to serve as the sole gateway to a deployment. Once we have done that to our own satisfaction, we have to convince those stakeholders to let us try Continuous Deployment to the next environment (for us, this is QA.) 

In my experience, the purpose of the "DevSecOps" (or whichever order you prefer) has mostly to do with including Security as a first-order concern of software delivery. Security is commonly treated as a "bolt-on" (often meaning optional,) non-functional requirement. DevSecOps attempts to describe a process where security concerns are folded into the delivery process at every step of the value stream. 

I recently had an interesting discussion with a colleague. We have different perspectives on what is required to develop Docker containers. I won't say who believes what, to keep from prejudicing the responses. Here are two competing assertions: Assertion 1: It is not necessary to install Docker on development workstations. Container build is a CI task that can be performed by a CI tool, then containers can be troubleshot/debugged in a DEV environent. Assertion 2 It is critical for developers to run Docker locally so they can smoke test and execute containerized processes before they are committed to SCM or CI. Containerization is a core developer skill, which is specialized by development platform. Is it critical, nice-to-have, or not necessary for Docker to be installed on developer workstations where containerized workloads are developed? 

Automation of installations like this can often be done with config management tools. GitHub and the public repos for these tools is a good place to start. There is a Puppet installer for TeamCity here: $URL$ Here's a Chef cookbook: $URL$ You can also use tools like Packer to generate your image(s). Packer can substitute for a Dockerfile in generating a Docker image. Packer will also run Provisioners (including Chef/Puppet/Ansible) to configure your image. 

It's important to note that the terms "checkin" and "checkout" have different meanings depending on the type of SCM system. Centralized systems like TFVC, Subversion, and Clearcase use "exclusive" checkouts. This is like Pierre's book borrowing metaphor, where only one user can have a file checked out at one time. Distributed systems like git have a "checkout" command, but it means something completely different. is used to switch between branches when working with a local repository. 

I've seen this same "sawtooth" pattern in other systems, in particular a Java-based data tool. Based on your description, I think you're looking at .NET garbage collection (assuming this is a .NET app.) Java and .NET are both memory-managed languages and frameworks that use garbage collection. A memory leak is typically found in frameworks that lack memory management, or in a program on a memory-managed framework that is overriding or confusing the garbage collector. The fact that these are your highest-traffic servers makes sense. You're seeing the .NET framework allocate memory as needed, then the garbage collector kicks in on a regular cycle and reclaims unused memory using the garbage collection algorithms. Unless you're tracking specific performance issues I don't think this memory usage pattern is a problem. 

I would say that DevOps is a peer of Agile, not a child. Agile methods apply largely within software development. DevOps, by contrast, applies the same Lean Manufacturing/Lean IT concepts across the software delivery lifecycle, which begins with the business and ends with business value delivered (code running in Production.) I believe, after some thought on the subject, that there is also a dependent relationship between Agile and DevOps. DevOps is intensely focused on automation. There is limited benefit in automating a process that executes infrequently. Waterfall development methods are a perfect example of a slow, infrequent process. Agile methods are focused on maintaining a constant flow of work through Development. This is a perfect, natural fit for DevOps, which keeps that flow going all the way from the business to the end state, which is code running in Production. 

I see some interesting parallels in this story and The Phoenix Project. (Spoilers ahead for the book, if you haven't read it read this anyway :)) I take it the title for "Notes Day" comes from the term of art in Hollywood, where "notes" are constructive criticism about a work. In this story, Catmull plays the part of Parts Unlimited CEO Steve Masters. Catmull accepts that he can learn from his employees how to do a better job, as Masters does in Phoenix Project when he does his public mea culpa and apologizes to Bill Palmer. In a sense, Catmull is also Brent- he's a major constraint. If his time was so precious, it meant that Pixar was struggling to delegate authority. It seems Catmull was critical to every decision, rather than able to set large-scale goals and leave it to employees to figure out how to achieve them. An event like this might work well at some companies, less well in others. I think it's the key idea behind Notes Day that is most valuable: accountability and transparency from the highest levels of management to the last employee. That value can be spread throughout a company in a variety of big and small ways. 

My particular use case is for .NET web applications, where the web.config is a file stored with the application. Both options appear to have potential pros and cons. Is one of these options better, or am I missing a potential best option? 

You're omitting the parameter from your command. If you include it, prompt should go away. I've built an Ubuntu image with Packer as well. Here is the shell script I use to perform the update: $URL$ This is derived from a great, well-maintained library of Ubuntu Packer builds: $URL$ 

(I reviewed this question but the one answer doesn't offer practical solutions.) The general wisdom when it comes to containers appears to be: "Make them immutable and identical." I take this to mean that every container for a given application function (a web server, for example) should be bit-for-bit identical. The code, the internal config, and the file system should be built the same way every time for a given version. This begs the question: where to store the config that makes a given app container a DEV, TEST, QA, or PROD container? If the environment config is stored inside the container, that makes them different. This seems to run contrary to the identical/immutable goal. A better model would be to somehow keep environment-specific configuration outside the container. There are two methods I can think of to do this: 

I wouldn't describe Jenkins jobs as "fire and forget." They are designed to run automatically without the need for human interaction, however, that doesn't mean the result of a job is not visible. Quite the opposite- detailed logs of all activity are available for jobs ranging from command-line tools to custom deployment jobs. The testing results from Jenkins jobs are routinely displayed on large-screen TVs all over the Development floor of many companies I've seen. There are whole plugins devoted to making this type of information available to developers, Ops, and management in a dashboard format. 

In short, you cannot prevent your customers from modifying containers they run in their own infrastructure. Containers are not like binaries that can be obfuscated; they are runtime environments. The code you distribute inside the container, however, can be obfuscated. Your question alludes to a third-party support issue: clients modifying software they run in their own environments. If you provide tools to operate the containers you supply (e.g. monitoring and logging) then the clients should agree (as part of a software license) not to make unauthorized modifications to them. This applies to all types of third-party software, not just containers. Depending on your situation, you may also have the option to provide your app as Software As A Service (SaaS) running in a cloud infrastructure. If your client requires your containers be run on their infrastructure, and refuses to abide by modification restrictions, then you probably don't want to try to support their use of your software. 

Feature toggles are a common practice in high-velocity development because they de-couple development from release. Dev teams can "soft-release" a new feature to production, in a disabled state. This allows the feature to be released any time. If the feature is dependent on other work or preparation, it doesn't have to wait for a major release to go to production. As far as "convincing" developers to use them, that's an exercise in making the case for the freedom it offers. My experience is that it's not a tough sell to developers. It's management that tends to be reluctant to try new things. Try this: