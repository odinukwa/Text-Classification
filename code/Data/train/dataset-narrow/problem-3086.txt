It should return a function . (That function returns a number.) Essentially we are pulling out a dimension or two. In theory of course it gives us nothing, it just postpones some calculation. (In practice this small specialised object is very useful in some production applications.) 

Is there a machine learning framework that supports partial evaluation? For example: We train on . Today we call 

Note: this is more frequently called canonicalisation than harmonisation. URLify and URL-safe are also terms that can help you in your search. whitespace Combining the built-in functions + is a solid way to canonicalise intermediate and trailing whitespace, including tabs, carriage returns and so on. non-ASCII chars ASCIIification is actually somewhat contrary to canonicalisation. That is, you should think hard about whether you really want to be converting 'München' to 'muenchen', rather than 'muenchen' to proper 'München'. If you do want to go in that direction, you should give more details on the use case and the desired conversions. Here are some examples of possible input: 

The we compute is not based on the feature $x$, which we don't know, but based on the known label/target of the sample. Let say we are doing a regression and trying to minimize the mean square error, and let say we have the following: 

As pointed in the comments, there is a distinction to make between the importance of a feature for the model, i.e., how many times it is used, and the importance for the accuracy of the prediction. These two points might be unrelated. For example, consider that you want to predict a continuous target $y$ from two features, $x_1, x_2$, where the ground truth is $y = x_1 + x_2$. Assume that $x_1$ can either be $0$ or $100$ and $x_2$ is a continuous variable ranging from $0$ to $1$. Training a model that fits exactly your training set will have only one split for $x_1$ and as many splits on $x_2$ than distinct training values of $x_2$. In this case, you can make $x_2$ arbitrarily more important that $x_1$ by increasing the number of distinct training points, but $x_2$ is mostly irrelevant to the accuracy of the prediction when compared to $x_1$. This problem makes it impossible to judge the importance of a feature based on the number of time it is used, in the general case. I don't think there is a definite answer on how to rank features with respect to the accuracy either, as the information contained in one feature can be a duplicate from another one, or be useful only if you have another feature to complement it. One simple method to approach this problem is the reverse of iterative feature selection, which you can read more on Feature Selection (Wikipedia). Given a model with 10 features, you can build 10 models with 9 features, removing a different one each time, and compute their loss on a validation set. If the 2nd model out of your 10 models performs the worst, then the 2nd feature is the most important. If the 9th model, without feature #9, is still as good as the full model, then feature #9 is not important, and you can use the difference in your loss metric to measure the importance to accuracy. However, it might be the case that feature #9 becomes useful if you add a new feature and that feature #2 can be built by a combination of other features, which you'd catch if you allowed your model to be more complex. 

At human check I see that as a result, all predicted labels are a constant sequence. What are the possible ways of improvement? The training data seems OK (at least as far as any pile of handwriting could be). 

Now, I want to apply clustering to this latter set. I guess that I could get better results if I downweight the dummy variable columns (proportionally to their number) because with equal weights the distance based clusters will be distorted. My question is, how could I get the following weight vector from the new data set: 

I have studied the usual preprocessing methods for Machine Learning but I couldn't cope the following specific problem. I apply the "usual" preparation for modeling (dummy variables, normalization, PCA etc., of course in the necessary cases) to the training data. So far, so good. But when I get the to-be-classified new data to make prediction the model constructed above, it's evident that I must apply these preparatory steps to this new data set as well. And the problem arises here, because if I simply apply the preparatory steps for my new data in turn again, these doesn't take into consideration the characteristics of the training data. So, if I convert the new data factors into dummies, then it takes only the existing factor levels in the new data into account; if I min-max normalize the new data, it will be normalized according its own min-max values, disregarding the values in the training data; and if I use PCA, then the resulting components from the new data will be totally independent of the training data. So essentially my point is that applying the same conversions separately to the train set and the new data set(s) (which could be only one observation as well), then the two resulting transformed sets will have nothing in common, so the prediction will be baseless. I found some traces that in some cases there is some "learning" step in the training phase in these transformations as well and apply this "knowledge" to new data (caret and Sklearn, for instance, with "predict" could transform to the new data with characteristics learned from the training data), but generally speaking this inconsistency remains unmentioned otherwise. What is the correct practice here? 

Sentiment analysis, sentiment detection and opinion mining all cover a set of problems, and can generally be considered to be one and the same. The term sentiment analysis seems to be more popular in the press and in industry. In practice, as of 2015, it is mostly about giving a score, to text, between 0.0 and 1.0, for negative to positive sentiment. (Strictly speaking that is only a subproblem and one of many possible formulations.) But whenever any of the terms is used, you should define or ask for definition of the exact problem. 

Even though the answer in reality is always or , you can make your class attributes not labels but floating point numbers, ie 1.0 and 0.0 (or 100 and 0). That is, you can frame it is a regression problem, not classification problem. Then the predicted output will likewise be numbers on that spectrum, ie probabilities not labels. Your current implementation is essentially equivalent to a regression model with a threshold of 0.5. With such an output, you or your can client can define a threshold that is acceptable (eg 0.3). Of course there will be more false positives then, but for some applications, like detecting cancer, that is optimal. Oversampling (the positives) or undersampling (the negatives) are also ways to fix this, but must be done thoughtfully, can sacrifice accuracy, and still sacrifices the control to move the threshold after creating the data and training. 

create_model is a function that builds the Neural Network Model. The fitting (last row) gives a long error message: 

I have started to study ANNs with Tensorflow and Keras. Now I want to find a solution to use ANNs over Hadoop. I have learnt that Spark 2.0 does have a Multilayer Perceptron Classifier, but as far as I can see it is quite "primitive" in comparison with TF and Keras (there are only feedforward types and hidden layer/output layer activation functions are hard-wired), there is no wide variety of optimizers, cost functions, architecture types etc. Is there any competitive alternative for large scale neural networks? Of course, I could use Amazon with very powerful, GPU-driven machines but these are not parallel frameworks either... 

I have a factor variable in my data frame with values where in the original CSV "NA" was intended to mean simply "None", not missing data. Hence I want replace every value in the given column with "None" factor value. I tried this: 

Data augmentation is very standard for annotated image datasets for tasks like image labelling. Images are flipped, rotated, pixelated and so on, to add more training data and make the system robust to noise and not overfit on irrelevant features. How are for example speech recognition training datasets pre-processed? Are they augmented with background noise or other mutations like image datasets are? Has there been any work in this direction? 

the instances where "capacity" occurs, then "capacity" can be guessed to be a label of number-like things. Even the nature of these relationships (eg, whether it should be before, after, capitalised) could be learnt from existing known labels. This can be done with Google BigQuery using the public tri-gram sample data. But I assume that's out of scope and impractical here. A practical approach, might aim to classify these strings as , or or even more classes, and then do human review of the latter. (There are some very tricky cases, for example, "capacity" is numerical, but "capacity type" would not be.) As a starting list you can use: wiktionary:Category:en:Units_of_measure wiktionary:Category:Symbols_for_SI_units wiktionary:Category:en:Mathematics Note that they are in the singular. For your domain considering finding other such lists. If a string is a full match with one of those labels, you can consider it a . Your next concern is tolerant, fuzzy matching. You could treat '.' as a wildcard (so "in*" matches "inch"), or find the actual abbreviations of units like "inch". These you can label as and then review. Likewise if the word is simply contained in the string, eg "arch length" contains "length". I think once you have done this you can make some refinements and add some special cases. Without seeing even a sample of your data, it is difficult to say more. If most of your strings are numerical units, then it may be easier to identify categorical units instead. 

The procedure is described in their paper, section 3.4: Sparsity aware split-finding. Assume you're at your node with 50 observations and, for the sake of simplicity, that there's only one split point possible. For example, you have only one binary feature $x$, and your data can be split in three groups: 

The algorithm will split based on $x$, but does not know where to send the group $M$. It will try both assignments, $$(B, M), C \qquad \text{and} \qquad B, (C, M),$$ compute the value to be assigned to the prediction at each node using all the data, and chose the assignment that minimizes the loss. For example, if the split $(B, M), C$ is chosen, the value of the left node will have been computed from all the $B$ and $M$ samples. This is what is meant by 

Note that even if the value of $x$ is missing, we know the value of target of the sample, otherwise we could not use it to train our model. In this case, the split would be made on $(B,M), C$, the value assigned to the right node containing samples $C$ would be $10$, and the value assigned to the left node containing samples in $(B, M)$ would be the mean of the target for the whole group. In our example, the mean would be $$\frac{|M|}{|B| + |M|}\text{mean}(M) + \frac{|B|}{|M|+|B|}\text{mean}(B) = \frac{10}{30}0 + \frac{20}{30}5 = 3.\overline{3}$$ 

How could I compute similarity taking semantic distance into account? Shall I use word2vec representation instead of TFIDF? 

I face with a special NLP (?) problem. I have a sequence of program development steps with code (actually a Git repository with multiple commits of build sources) and a full list of test cases (which I haven't any more specific closer description about apart from some quite general ones like id, timestamp etc.) of which subset that have been run against in a specific build test runs. Based on this historic data I have to build a machine learning model which predicts/recommends relevant test cases for any future build of the given software. Apart from the extremely large amount of data points (several hundreds of program files with several hundred megabytes of gross length and several thousands of potential test cases) I cannot see any promising way to extract features from the code base for the prediction. My first idea is to trace the changes in respective code files per build runs and the executed test cases (so the independent variables are the IDs of the files with binary values if they are modified or not in the current development stage, and the goal is a multilabel classification with as many labels as the possible test cases are) but it's evident that the source file change is a very poor guess for a necessary test case to involve. Is there any practice to a proper feature extraction for this problem? I'm aware that program code handling with ML/DL does exist but usually in a form of code generation and syntax checking which are not my cup of tea.