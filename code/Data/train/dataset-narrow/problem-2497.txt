Imho this is one of the few key points in (TM) algorithmics that you have to accept, as it is a very dry fact with little to understand. You could try to derive a contradiction from the students' wrong premise. By the same reasoning they apply to KNAPSACK, integer factorisation is in polynomial time and even reasonably fast. Why is it that common encryption methods can rely on factorisation being hard? 

There certainly are many, many more examples; in fact, every time you use regex as a programmer and do not use backreferences/groups (!) you define a regular language for some purpose. 

Cranky Sunday morning idea that might or might not be correct: Wlog, let $P_1$ be the partition with more sets, $P_2$ the other. First, assign pairwise different names $n_1(S) \in \Sigma$ to your sets $P_1$. Then, find a best naming $n_2(S)$ for the sets $P_2$ by the following rules: 

If I understand correctly, you are clear about converting functions that contain no other function calls but to themselves. So assume we have a "call chain" $F \to F_1 \to \dots \to F_n \to F$. If we furthermore assume that $F_1, \dots, F_n$ are not recursive themselves (because we have converted them already), we can inline all those calls into the definition of $F$ which thusly becomes a directly recursive function we can already deal with. This fails if some $F_j$ has itself a recursive call chain in which $F$ occurs, i.e. $F_j \to \dots \to F \to \dots \to F_j$. In this case, we have mutual recursion which requires another trick to get rid off. The idea is to compute both functions simultaneously. For example, in the trivial case: 

The difference is visible in the types: $\alpha \to \beta$ is the function type, but $\alpha \Rightarrow \beta$ is the handler type. In operational semantics there is also a difference: we must apply a function to a value, and we must apply a handler to a computation. Thus, instead of writing 

In this context the duality refers to taking the least fixed point in one case and the greatest fixed point in the other. We should try to understand in what sense $L = \forall X . (F(X) \to X) \to X$ and $G = \exists X . (X \to F(X)) \times X$ are the "least" and "greatest" solutions of recursive equation $F(X) \cong X$. First of all, $L$ and $G$ are indeed fixed points (under certain technical assumptions which restrict the nature of $F$) because the comparison maps $v : F(L) \to L$ and $w : G \to F(G)$ given by $$v \, x \, X \, g = g \, (F (\lambda h : L \,.\, h \,X\, g)\, x)$$ and $$w (X, (f, x)) = F (\lambda y : X \,.\, (X, (f, y))) \, (f x)$$ are isomorphisms. Notice that we used the fact that $F$ is a functor, i.e., it is monotone, when we applied it to functions. Suppose $Y$ is any solution to $F(Y) \cong Y$ with a mediating isomorphism $u : F(Y) \to Y$. Then we have canonical maps $$\alpha : L \to Y \text{ and } \beta : Y\to G$$ defined by $$\alpha \, f = f\, Y\, u$$ and $$\beta \, y = (Y, (u^{-1}, y)).$$ Therefore, $L$ is least because we can map from it to any other solution, and $G$ is greatest because we can map from any other solution to it. We could make all this more precise by talking about initial algebras and final coalgebras, but I want my answer to be short and sweet, and cody explained the algebras anyhow. In practice the least solutions are eager datatypes and the greatest solutions are lazy datatypes. For example, if $F(X) = 1 + A \times X$ then in the first case we get finite lists of $A$'s and in the second finite and infinite lists of $A$'s. 

By far the nicest procedure I have seen is the one mentioned by Sylvain. In particular, it seems to yield more concise expressions than others. I wrote this document explaining the method for students last summer. It directly relates to a specific lecture; the reference mentioned is typical definition of regular expressions. A proof of Arden's Lemma is contained; one for correctness of the method is missing. As I learned of it in lecture I don't have a reference, sadly. 

I do not know when it came up, but the recursive solution for Towers of Hanoi is frequently used as introductory example. The problem originated before formal approaches on computation. 

I can recommend the ANTLR Reference. It explains many issues with language parsing (without the depth of the Dragon Book, though) and introduces you to a powerful (free) tool, namely a compiler generator. For both reasons, it might be a good starting point for your studies. 

In 1936, Konrad Zuse developed what was for all intents and purposes the Z1 the first computer in the modern sense. This fact is little known but has since been acknowledged even by his international competitors, e.g. IBM. While the Z1 was not very reliable, later models (still developed during WWII) actually worked. Shortly after the war, Zuse's company began building (universal) computers for multiple major universities in Europe. Zuse's motivation was not to gain mathematical insight, although he did develop a formal, universal programming language called PlankalkÃ¼l. He primarily wanted to do away with repeated, mechanical calculations often seen in engineering -- surely a machine could perform such mindless manipulations of symbols! Note how Zuse's early work happened concurrently and, due to different background and the political situation, mostly independently of the better known work in the US. 

(where $H$ is the binary entropy function $H(p) = p\log(1/p) + (1-p)\log(1/(1-p))$, and $\delta_n = \epsilon_n/n$.) Here's a quick plot to show the idea. We have the binary entropy function in blue and the Binomial pmf (for $p=0.5$) in green. So we can see that the expectation of $H(k/n)$, when $k$ is distributed binomially, will always be below one but should be approaching one. The question is how fast. 

(Comment --> answer) The inequality unfortunately fails to hold, a counterexample is $$p = (1,0,\dots,0)$$ and $$q = \left(\frac{1}{2},\frac{1}{2n},\dots,\frac{1}{2n}\right),$$ where the support size is $n+1$ and $x=2$. Then $\sum p_i^r = 1$, but $x^{r-1}\sum q_i^r \approx \frac{1}{2}$: \begin{align} \sum q_i^r &= \frac{1}{2^r} + n\left(\frac{1}{2n}\right)^r \\ &= \frac{1}{2^r}\left(1 + \frac{1}{n^{r-1}}\right) \\ &\to \frac{1}{2^r} \end{align} as $n \to \infty$, so $x^{r-1}\sum q_i^r \to \frac{2^{r-1}}{2^r} = \frac{1}{2}$. 

The idea is that, when we get to i, the previous elements are perfectly random, so let's leave them there and recursively shuffle the rest. In contrast, the naive algorithm will sometimes "reach backward" to make a swap. Now I'm noticing that in the naive shuffle, elements which are below the current index i can only ever move forward from now to the end of the algorithm. Okay, so in particular, the last element only has two ways to end up in the first position: Either the first element can swap with him, and then nobody else ever swaps with him (this has about a $\frac{1}{n}\frac{1}{e}$ chance), or else nobody ever swaps with him until the last round, and then he swaps with whoever's in first (this has the exact same chance). So the last element should only end up first with probability about $\frac{2}{en} \approx 0.74\frac{1}{n}$ instead of $\frac{1}{n}$. I simulated this in python with n=100 and 100,000 trials, and got 734 trials in which the last element ended up first, versus 992 trials using Fisher-Yeates. 

There are also commands for moving, copying, branching, resolving conflicts, ... but as long as you do not try funny stuff, you are fine with the above. If anything breaks or seems to, backup your edits, delete the whole folder and checkout the whole thing anew. That's for svn like rebooting for Windows. Addendum: I see you seem to be concerned about "smart merges". I assume that you refer to have to different versions of a file merged, with the assumption that two people added things in disjoint parts of the paper. As far as I know, svn would treat that as a conflict, and probably rightly so. I don't think that there is a general procedure that ensures you get what you want after two people manipulated the same source. There are graphical svn clients that visualise such conflicts and help you resolving them; they are pretty much diff-viewers were you can choose which version to keep for every conflicting line). It will require work, though. 

I think we can just start with some base language $L$, then take $L_0 = L$ and $L_{s+1} = L_s \cup \{0,1\}^{s+1}$. That is, each $L_s$ is the union of $L$ with all strings of length up to $s$. Each $L_s$ is at least as hard as $L$ but is no harder (in an asymptotic sense), assuming we can count to $s$. I also thought about the opposite "limit", so each $L_{s+1}$ is contained in $L_s$, and $L = \cap_s L_s$ is easy while each $L_s$ is hard. But I think we could just start with a hard (but countable) language $L_0$ and just remove one word at each step; the intersection should be empty (every word is eventually removed). 

I can attempt a long-winded version of Thomas' comment and others can correct me where needed. Algorithms have been around a long time (e.g. Euclidean algorithm for GCD circa 300 BC), and even computational machines are somewhat old (Babbage's design for the analytical engine in the 1830s). However, most wouldn't consider theoretical CS to have started until we had the first formal, mathematical definitions of what an algorithm can be. Algorithms were formalized in three main ways, to my understanding, in the early 1900s. There were the $\mu$-recursive functions, which Godel used in his incompleteness work in 1930. However, these were thought of as mathematical constructs and not necessarily mechanical or computational. I would guess that the same is true for Alonzo Church's lambda calculus (early 1930s), the second way. Although one could view them as being closer to computation, they were mainly mathematical and I don't think there was a clear connection to the capabilities of machines and "programming" (using modern terminology). The third way that algorithms were formalized was of course Turing's Machines in 1937. Most would say that this paper ("On Computable Numbers") founded the field of CS. The reason is that his formalization of algorithms was mechanical. He described how to build "dumb" machines that implemented these algorithms. Recursive functions and the lambda calculus did not so immediately relate to machines (though it was soon shown that all of these are equivalent). The paper was, however, primarily theoretical and mathematical and he proved some interesting/foundational theorems already in the first paper. So again, the key innovation was to define a model instantly recognizable as "mechanical" or dumb (implementable as a machine), yet capable of expressing anything we would consider to be an algorithm; and then proving theorems about it. Many other researchers were involved around that time and soon afterward (I haven't mentioned Kleene or Post yet for instance). But for this reason I think most would point to Turing and his 1937 paper as the start of the field. 

Kristoffer's solution can be used to show that, assuming reals are represented so that we can compute limits of sequences of reals which are computably Cauchy. Recall that a sequence $(a_n)_n$ is computably Cauchy if there is a computable map $f$ such that, given any $k$ we have $|a_{m} - a_{n}| < 2^{-k}$ for all $m, n \geq f(k)$. The standard representations of reals are like that, for example the one where a real is represented by a machine that computes an arbitrarily good rational approximation. (We can also speak in terms of computing digits, but then we have to allow negative digits. This is a well known issue in computability theory of the reals.) 

That's three questions. Can handlers be decomposed? No, a handler cannot always be decomposed because the operations may interact. It is false that you can decompose and into separate handlers because they have to handle a shared state. You can try these examples online in Eff. First, the ordinary state handler: 

A professor at my university does this in his algorithms course. His language of choice is Modula. I don't think the particular choice of language matters, though. Just stick to one (per paradigm) that fits your level of abstraction best. 

I would be interested in integration of GPU processsing (wether CUDA does not matter) into standard compilers. Can you figure out where application makes sense programmatically? Can you efficiently use GPUs? 

This reduces computational complexity to measures known from elementary school; who has not wondered then why 1000m give 1km, but 1000mÂ² do not give 1kmÂ²? Exponential growth can best be explained by brute force: we start with a single bacterium. This kind separates every six hours and never dies. Make a table or better, pictures: After one day, there are 16 bacteria. After two days, 256. And so on. After 18 days, our colony weighs ten million tons. That should make an impression. You can make it a search problem again: assume our start bacterium had a marble in its body that is passed along to one offspring. After 20 days, how long do you have to search for the marble if you can only examine one (or ten, hundred...) bacteria at a time? Another thing often used in magazines for children is folding a piece of paper: how often do you have to fold to reach the moon? (not too often) Also popular: the guy with 1, 2, 4, ... grains of rice on the chess board. Creating a full table of a binary function with 3, 4, ... parameters is telling, too.