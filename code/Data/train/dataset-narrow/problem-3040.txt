If you have a probabilistic cost function (e.g log-loss), I'm pretty sure backprop is an estimator for the MLE. Consider the derivation here: $URL$ If you have articles on hand, I'd be interested to see an example of an experiment where they discussed fitting a network via MLE but explicitly were not using backprop. Otherwise, I think you can assume that the authors were describing backprop when they said "MLE". Maybe they thought "MLE" sounded more academic? 

Your intuition that the distributions of words in the subject and email body are likely to be different has merit. Regarding and "out of the box" solution, the easiest approach would be to construct two separate classifiers -- one each for the subject and body -- and then ensemble them together. A very simple ensemblification approach would be to just use the probability scores outputted by those two models as inputs to a logistic regression classifier. 

Your problem here isn't in choosing an appropriate clustering algorithm, its defining an appropriate similarity metric. Edit distance and jaro winkler distance will cover a lot of ground for you, but you should still anticipate needing to do a fair amount of pre-processing and customization here. Also, as great as text-based metrics are, you have much more information here that can be leveraged. Your data is obviously going to have implicit clusters in it already based on documents that are contained in the same folders, and going further those folders exist in a hierarchy which also implies certain groupings. You should make sure that your clustering and/or similarity scoring incorporates these topological features in addition to any text similarity you do. Your first step is going to be understanding your data more. I'd recommend constructing a tree visualization of the folder hierarchies you're going to be dealing with, taking a sample of 5-10 filenames from within each folder so you can better understand what your dealing with. From here, start trying to understand what kinds of naming conventions are in place that you can take advantage of. There are probably lots of files with dates at the beginning or end, maybe commonly occurring client names, words that are suggestive of document classifications like "report", "newsletter", "resume" etc. The more of these you can capture and deal with directly, the better. Next, you may start seeing some patterns that suggest ways you can further tokenize filenames. spaces, hyphens, and underscores are probably good places to start (after dealing with dates/timestamps, obviously), and CamelCasing would be worth looking out for as well. Also, different filetypes might have different naming conventions. For example, *.png files are probably more likely to have all numeric names starting with dates (i.e. someone dumped their camera to a folder). If you want to just get really quick and dirty with it, something you could try would be to parse each filename into n-grams (e.g. all sequential 3-letter sequences that occur in a filename) and then score pairwise filename similarity based on the jaccard distance of the n-grams that appear in each filename. Once you've figured out a couple of different approaches you want to try, you should start thinking about how to evaluate your results. Obviously you're going to start out evaluating things qualitatively, but that doesn't really help you compare the strengths/weaknesses of different approaches. One thing you could try would be to use the naming conventions learned by a particular method to try to predict whether or not randomly sampled filenames appear in the same folder or not, and score your methods based on how well the resulting classifiers perform. Ultimately, your going to have to custom tailor the solution to what you see in your clients filenames. Hopefully, I gave you a few ideas to work with here. 

I haven't worked on many time series problems, so take my answer with a grain of salt. Having said that, it's not clear why you can't you just treat all predictors the same, in other words treat previous values of the univariate target series and independent predictors as regular predictors. I don't see what is special about the element of time that makes it especially difficult to train/learn like any other type of ordinary predictor. If time is truly important, then the neural network will demonstrate that importance by weighting your multivariate lagged variables accordingly during training. This sort of reasoning would apply to any type of model, not just neural networks. For example, the same could be said of gradient boosting. 

Yes, most software implementations of trees will allow you to predict a continuous target variable with all binary predictors. This is because the predictors are only used as splits, and the prediction comes from the average value at a given terminal node. The predictions will not be truly continuous across all terminal nodes in the same way that linear regression is continuous, but in practice, this is generally not a problem. If your tree is under-fitting (not continuous enough) you can always add more terminal nodes. Also, one-hot encoding should be sufficient. 

Because of the encoder-decoder structure. The encoder reads the input sequence to construct an embedding representation of the sequence. Terminating the input in an end-of-sequence (EOS) token signals to the encoder that when it receives that input, the output needs to be the finalized embedding. We (normally) don't care about intermediate states of the embedding, and we don't want the encoder to have to guess as to whether or not the input sentence is complete or not. The EOS token is important for the decoder as well: the explicit "end" token allows the decoder to emit arbitrary-length sequences. The decoder will tell us when it's done emitting tokens: without an "end" token, we would have no idea when the decoder is done talking to us and continuing to emit tokens will produce gibberish. The start-of-sequence (SOS) token is more important for the decoder: the decoder will progress by taking the tokens it emits as inputs (along with the embedding and hidden state, or using the embedding to initialize the hidden state), so before it has emitted anything it needs a token of some kind to start with. Hence, the SOS token. Additionally, if we're using a bidirectional RNN for the encoder, we're definitely going to want to use both SOS and EOS tokens since the SOS token will signal to the reversed-input layer when the input is complete (otherwise, how would it know?). 

I'm building a remote-controlled self driving car for fun. I'm using a Raspberry Pi as the onboard computer; and I'm using various plug-ins, such as a Raspberry Pi camera and distance sensors, for feedback on the car's surroundings. I'm using OpenCV to turn the video frames into tensors, and I'm using Google's TensorFlow to build a convoluted neural network to learn road boundaries and obstacles. My main question is, should I use supervised learning to teach the car to drive or should I provide objectives and penalties and do reinforcement learning (i.e, get to point B as fast as possible while not hitting anything and staying within the road boundaries)? Below is a list of the pros and cons that I've come up with. Supervised learning pros: 

Your best bet is to conduct focus groups or conduct market research, and that will probably be very expensive. You are not likely to find anything useful on Twitter, Facebook, or the news. No amount of text mining will solve your problem. This is not the answer that you want to hear, but your project is not set up for success, and you should look for/propose a new project rather than potentially waste more of your valuable time on this project. 

There are definitely ways to process your data to make categorical data compatible with sklearn (e.g one-hot encoding). An alternative you can look into is h2o, which supports categorical features natively (although it doesn't offer the breadth of models of sklearn). 

The deconv layers are probably to blame. Check out this distill article for a fairly in depth discussion about how deconv layers create checkerboard artifacts. The gist is that deconv striding creates interference patterns which can cancel out if you're careful, but are more likely to be worsened as you add more deconv layers. The authors suggest using nearest neighbors for upsampling followed by a convolutional layer, which they call resize-convolution upsampling. 

Err... sort of. This is definitely the most popular formulation of kmeans, but a more appropriate formulation would be: $$J(X,Z) = min\ \sum_{z\in centroids}\sum_{x \in data} distance(x,z)$$ In your formulation, you're concretely defining distance as euclidean distance, i.e. the L2 norm. But you can swap out L2 for any distance kernel and apply kmeans. The caveat here is that kmeans is a "hill climbing" algorithm, which means each iteration should always be at least as good as the previous iteration, and so it must be the case that this improvement will be true for both the E and M steps. For most common distance metrics (L1, L2, cosine, hamming...) this is the case and you're good to go, but there are infinite possible distance metrics and if we're going to be technical about it, the probability that a random distance metric will satisfy this criterion is almost surely 0. So, to circle back to your question: does the objective function as you formulated it imply the distance metric is euclidean? Yes. But does kmeans only apply to euclidean space? No, absolutely not. Use whatever distance metric you want and throw the EM algorithm at it and bam: you've got yourself a non-euclidean kmeans. Generally, when people say "kmeans", they're talking about euclidean kmeans. But Kmeans is super easy to modify with a different distance metric and I think only the most pedantic people would argue that you shouldn't call it "kmeans" after such a modification. Although it's generally always described with the objective you posted (which, yes, does imply euclidean distance), you can really drop in pretty much any useful distance metric, throw EM at it, and it'll work. Some people might call it "___ kmeans" depending on the distance metric, but really it's all kmeans. I think part of the reason kmeans often isn't described this way isn't formulated like this is because its often compared with gaussian mixture models (GMM). With a euclidean norm, kmeans is equivalent to a GMM with diagonal covariance (and hard cluster assignment), and is consequently often described as an easy way to fit a GMM with spherical clusters. But this comparison fails if we use a different distance metric. I suggest you check out the CrossValidated discussion on this topic. Full disclosure: the highest voted answers disagree with me, but as you probably guessed I think they're being fairly pedantic. 

Thomas Cleberg's approach sounds reasonable, but another very simple approach would be to explicitly code an "undefined" category. This is common when dealing with text datasets where a word might be new or too rare to stand on its own. With a large enough collection of websites, surely there are website that are not cleanly classified in one of your categories. Worst case you could simply search for such websites manually and augment your dataset. This approach wouldn't require any changes to how you train your models. 

I used R for several years but have since moved to Python and so I have a hard time understanding output.OLS's data hierarchy. Nonetheless, here are my thoughts. In-Memory Databases: If you're struggling to fit your R object in memory, then I'm guessing the memory requirement is too great for your laptop (i.e., the problem isn't due a 32-bit installation of R). If that's the case, putting an in-memory db, like MongoDB, on your laptop will only transfer the memory problem to a new technology. In other words, 6 GB in R will be 6 GB in Mongo, so if your laptop simply can't handle 6 GB, then it doesn't matter how you put it there. You could set up a MongoDB on AWS, which could hold 100s of GB, but this can be expensive and also difficult if you don't have AWS experience. On the other hand, MongoDB is good at storing hierarchical data like lists and hash tables / dictionaries (I forgot what R's term for that is) and is fast if you have enough memory. Relational Databases (RDMS): I would recommend going this route and creating a wide and deep table. You can still get value out of an RDBMS even if your machine has limited memory because everything is efficiently stored on disk. If you create table indexes on the columns you expect to use/filter/select most, you can very quickly query the data that you want even if your data is multiple GB. If you don't create table indexes, your queries will be horribly slow, and you'll probably abandon the RDBMS effort entirely (so make sure to index). A long time ago my go-to package for accessing MySQL through R was RMySQL. MySQL has been around for many years, so you'll be able to find most answers to your problems on Goggle, but MySQL user interfaces like Sequel Pro (for Mac only) have sort of fallen behind on maintenance, so setting up tables and databases can be a little more challenging than they used to be. You'll now have to go the command-line route. Postgres has now replaced MySQL as the open-source DB of choice, but it lacks good user-interface support and so you'll probably have to set everything up via the command line. I started using Postgres after I stopped using R, so I never used a Postgres R package, but it looks like there is one here. Whether you go with MySQL or Postgres, you'll probably have to use the command-line outside of R to set up the database, but after that creating, dropping, joining, or querying tables can all be done through R. RDBMS systems are also great because you can guarantee data quality (no duplicates) by creating primary keys -- I found this feature very helpful for my R modeling. In any case, SQL is a very basic language that every data-oriented person should know so setting up an RDBMS and possibly learning SQL are skills worth learning. 

I think you can interpret autoencoders as essentially performing kernel PCA, but with an extremely complex kernel. Like, really, stupidly complex. The kinds of things you can accomplish with autoencoders are spectacular. For example, consider BicycleGAN. If kernel PCA is a step above PCA, autoencoders are miles away. If you have an application where PCA makes sense but isn't working out, try kernel PCA. If you have an application where condensing your input into an embedding of some kind would be valuable but PCA is unlikely to be very helpful, use an autoencoder. 

Yep, that's a thing. It's called a "Generalized additive model (GAM)": $URL$ You may also be interested in "multivariate adaptive regression splines (MARS)": $URL$ EDIT: Regarding demonstrating that these are actually used, I'm not sure what you're looking for. I've never seen a survey to try and gauge the popularity of specific models, and I'm not sure how useful such a thing would be. I could speak from my subjective experience, but I don't consider that particularly meaningful either. If you just want examples: 

EDIT: I managed to find the code that was used to build the chart in that paper. I searched the paper for "we used" and found this in the acknowledgements: 

Home field advantage is incredibly important in sports. If you always put the home team in the first column, your model will adjust the bias term accordingly. I've been doing this successfully for NBA and MLB models for several years now. Making the first team the home team also eliminates the need for an explicit home indicator variable and you'll be able to use "half" of the data like you've described. For rare neutral games like the Superbowl, you would need to create a mirror image of the data. In other words, present two rows for each game (team-A,team-B;team-B,team-A). When it's time to score you model, you can just arbitrarily pick one of the two records because they represent mirror opposite of the same thing. I've done this before for March Madness basketball games (all neutral courts), and this year (2016) my March Madness model was in the top 10% in Kaggle, so the technique works. Home field advantage is just one of the variables you'll need to make a good predictive model, but that's an answer for another question. 

I have a tremendous amount of experience training supervised machine learning models. However, I recently became a data scientist at a small financial services company, and I've been asked to build some sort of model that assesses portfolio-level risk. I'm not sure how to go about this task. Is this a supervised learning problem? It doesn't sound like one. I used to work as a software/data engineer at a different financial services company, and while there I was briefly exposed to the concept of Basel credit risk management. Is Basel used for assessing portfolio-level risk? As far as I could tell (I could be wrong), Basel seemed to aggregate charge-off predictions by running every single client through each of the company's relevant credit risk models (which were trained in a supervised fashion). If my memory is right, how does this aggregation work? Is it effective? 

The discrimination boundary will shift towards the negative class. This will manifest as an increasingly negative intercept term. The slope of the logistic curve will steepen, having the effect that a unit change in the inputs will have a larger impact on the outcome. If we give the model enough perfectly separable data, the curve will approach a step function and the model will output probabilities of 0 and 1. This manifests in the magnitude of the coefficients. With just three observations, the curve is fairly flat and changing the values of the inputs doesn't have much effect on the outcome probability, so the resulting class assignment is essentially determined by the intercept alone. 

Here's an easy solution. The sort order changes, but that shouldn't be difficult to address if you really care: 

If you are asking how to reproduce those plots, the following technique works to visualize the decision boundaries for any model: 

Let $N$ denote the number of observations in your training data $X$, and $x_j$ denote the specific observation whose prediction, $\hat{y}_j$, you want a CI for. Let $K$ denote some number of resampling iterations (Must be $\ge 20$ for a CI with coverage $\ge 95\%$) For $i$ in $K$, draw a $N$ random samples from $X$ with replacement. Denote this $X_i^{*}$ Train a model on $X_i^{*}$ and use this model to form a prediction on $x_j$. Call this $\hat{y}^{*}_{ji}$ Estimate distributional parameters for $\hat{y}_j$ from your sample. A $100 - \alpha$ CI is given by the $\frac{\alpha}{2}$ and $100 - \frac{\alpha}{2}$ percentiles of $\hat{y}^{*}_{j}$.