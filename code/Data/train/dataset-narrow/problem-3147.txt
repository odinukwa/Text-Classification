What you are describing is a normal multidimiensional linear regression. This type of problem is normally addressed with a feedforward network, either MLP or any other architecture that suits the nature of the problem. Any neural network framework is able to do something like that. The key to do that is to remember that the last layer should have linear activations (i.e. no activation at all). As per your requirements, the shape of the input layer would be a vector (34,) and the output (8,). Update: the usual loss function used for regression problems is mean squared error (MSE). Here's an example of multidimensional regression using Keras; the network is not an MLP but it should be Ok to illustrate the idea. 

The answer to your questions depend a lot on the nature of the data represented in the time series. You should ask yourself some questions to better understand what might or might not work, e.g.: 

In general, a safe default option for recommender systems is to recommend the most popular product, so two options for the recommendation: 

Keras is used in academia (see google scholar citations of Keras as a proxy for academy adoption) and hobbyists (see github stars or google results for keras in www.kaggle.com as proxies for hobbyist adoption). It was recently bundled together with Google's tensorflow. It is also used in industry, at least for prototyping (I know because I use it!), but I have found no source to back this statement. 

From the formulation of the question, I assume that there are no "examples" of anomalies (i.e. labels) whatsoever. With that assumption, a feasible approach would be to use autoencoders: neural networks that receive as input your data and are trained to output that very same data. The idea is that the training has allowed the net to learn representations of the input data distributions in the form of latent variables. There is a type of autoencoder called denoising autoencoder, which is trained with corrupted versions of the original data as input and with the uncorrupted original data as output. This delivers a network that can remove noise (i.e. data corruptions) from the inputs. You may train a denoising autoencoder with the daily data. Then use it on new daily data; this way you have the original daily data and an uncorrupted version of those very same data. You can then compare both to detect significant differences. The key here is which definition of significant difference you choose. You could compute the euclidean distance and assume that if it surpasses certain arbitrary threshold, you have an anomaly. Another important factor is the kind of corruptions you introduce; they should be as close as possible to reasonable abnormalities. Another option would be to use Generative Adversarial Networks. The byproduct of the training is a discriminator network that tells apart normal daily data from abnormal data. 

In neural networks applied to natural language processing normally each possible word (or sub-words) is handled as an individual token, and normally the vocabulary (the set of supported tokens) is around 32K. These tokens are typically one-hot encoded. Therefore, there is not inherent problem in having one-hot encoded vectors with thousands of components. However, the amount and variety of training data should support the dimensionality of the model for it not to overfit. Note that in NLP, these one hot encodings are used as expected outputs of the model, over which to compute maximum likelihood (categorical cross-entropy) together with the output of the model (a softmax). Depending on how you are using this portfolio information, another option would be to have an embedding layer, which embeds discrete values into a continuous representation space (whose dimensionality is a hyperparameter of the model). This would be possible if this information is used as input to the model. In those cases, you input an integer number, which is used to index the embedded vector table. 

It really depends on the outcome you want from your analysis. The most straightforward approaches might be resampling: 

These are random variables. Let's refer to them as $V_d$, where $d$ is the week day, e.g. $V_{monday}$. With those data, one option is to perform a hypothesis test. Depending on the distribution of $V_d$, you may do different things. If they are normal (which can be also tested by means of a hypothesis test) (so that $V_d \sim \mathcal{N}(\mu_d, \sigma_d)$), you can compare the means of two days, having a null hypothesis $H_0: \mu_{wednesday} = \mu_{sunday}$ and the alternative hypothesis $H_1: \mu_{wednesday} > \mu_{sunday}$ and choose an appropriate two sample test (depending on the variances of the distributions, sample size, etc). If your distributions are not gaussian, you may use a Kolmogorov-Smirnov test or a Mann-Whitney U test. To know how to choose between them, you can check this. 

As you can see, the implementation is . Note that they reuse the output of the sigmoid itself , therefore they write . Maybe in your example they did the same and that's the source of the confusion. UPDATE: OP has included the blog that is the source of the confusion. There, we can see that function is misleading: 

A possible approach would be a denoising autoencoder. It is like a normal autoencoder but instead of training it using the same input and output, you inject noise on the input while keeping the expected output clean. Hence, the autoencoder learns to remove it. This kind of autoencoders are also described in the blog post you linked to. In your case, you could just train your denoising autoencoder injecting to the inputs spikes of the height you expect to be removed. About what kind of architecture (e.g. fully connected, convolutional), only actual tests can tell you what is appropriate and what is not. 

Ways to "determine feature importance" are normally called feature selection algorithms. There are 3 types of feature selection algorithms: 

it is referring to the fact that, at a conceptual level, classes are not expected to overlap. An example of non-overlapping classes are "cat" and "dog". An example of overlapping classes are "Elephant" and "African Elephant"; this type of problem is called multilabel classification, because instead of classes, you have labels and each individual can be assigned many labels. Update: with the new information about the problem, we can tell that the problem being faced is not a classification one, as the desired output are specific probability values. This implies that the problem is a regression one. As the outputs are probabilities, it is appropriate to use softmax to ensure they add up to 1. As it is regression, it would be appropriate to use MSE as loss function. 

Google recently included in tensorflow's nightly builds its Eager mode, an imperative API to access tensorflow computation capabilities. How do tensorflow eager compare to PyTorch? Some aspects that could affect the comparison could be: 

You might want to have a look at the reddit post comments. If you want to fully understand them, you can go ahead with the 90 page-long appendix of the arxiv preprint. They got a lot of attention when they were presented, but I think they have not delivered up to the expectations, as no one seems to be talking about them lately on the internet. 

ConvNets work because they exploit feature locality. They do it at different granularities, therefore being able to model hierarchically higher level features. They are translation invariant thanks to pooling units. They are not rotation-invariant per se, but they usually converge to filters that are rotated versions of the same filters, hence supporting rotated inputs. I know of no other neural architecture that profits from feature locality in the same sense as ConvNets do. 

With tensorflow, currently the most straightforward and easy way to get persistence for your model is to use a . You just need to use it instead the normal that is frequently used. This an illustrative Python snippet: 

From the question, I understand that both filter and wrapper approaches are suitable for the OP needs. A classic article that covers both very well is this one by Kovavi and John. Here you can see an overview of scikit-learn feature selection capabilities, which includes examples of the three aforementioned types of variable selection algorithms. 

The evolution of the improvement of the performance of the network if you keep training it depends on: 1) the surface of the loss function with respect to the network parameters. 2) the learning rate, which is the scale factor you apply to the gradient in order to compute the next set of weights. If the learning rate is too high (with respect to the shape of the loss surface) and you keep training, you may escape a local minimum you just reached. The problem is that you don't know the loss surface and so we cannot choose the perfect learning rate for it. There are a lot of heuristic tricks to try to improve the exploration behaviour of SGD, like momentum. So the answer is: it depends. The network could stabilize around a local minimum and so the weights would only suffer small oscillations, or it might ruin everything by escaping the local minimum. 

About the tips regarding plot cost vs. iteration, they are generally applicable to gradient descent approaches, including deep learning, where hyperparameter tuning (e.g. learning rate) is crucially important. About the proper input scaling, it is not only related to the machine learning approach, but to the specific problem under consideration. Sometimes machine learning algorithms rely on distances to compute the similarity between individuals. Scaling changes some of these distances. In these cases, the resulting distance after scaling should be assessed to check whether it is more appropriate than without scaling. Here you can find examples for clustering. For some machine learning algorithms, you need standardized features, e.g. regularized linear/logistic regression. For most optimization-based machine learning algorithms, it makes sense to have feature scaling. On the other hand, there are problems where scaling doesn't even make sense (e.g. discrete input problems, like token-based natural language processing). 

Your are mixing two different beasts. Despite both having encoder and decoder parts, the way in which a normal feedforward image transformation network (i.e. the autoencoder) and an autoregressive model (i.e. the seq2seq) are actually used is very different: 

These 2 things do not work well together on their own, because you cannot propagate gradients through discrete stochastic units. There are 2 main approaches to deal with this: the REINFORCE algorithm and the Gumbel-Softmax reparameterization (also known as the Concrete distribution). Take into account that REINFORCE is known to have high variance so you need large amounts of data to get good gradient estimations. As an example of REINFORCE for textual GANs you can check the SeqGAN article. An example of Gumbel-Softmax you can check this article. Another completely different option is not having a discrete stochastic unit as output of the generator (e.g. generating tokens deterministically in embedded space), hence eliminating the original problem of backpropagating through them. 

Most ensembling methods are independent from the internals of individual models, e.g. something like majority voting, bagging or model stacking can be directly applicable to any model. Note: this was originally a comment to the OP. 

You may reparameterize $x_i$ as $x_i = sigmoid(\alpha \cdot z_i), z_i \in \mathbb{R}$, with $\alpha$ being a constant such that $\alpha \gg 1$, and then proceed with your favorite optimization method for finding $z_i$, and from there $x_i$. This, of course, may present problems, as $x_i$ are no longer bounded in $\{0, 1\}$, but may suffice your use case. 

What you describe sounds a lot like Scaled-Exponential Linear Units (SELUs), that are the core of Self-Normalizing Neural Networks, which where presented at NIPS 2017. A short summary from here is that: 

Normally, the answers to those questions are that series are not perfectly aligned and that variations in scale are also fine as long as the shape is similar. For these scenarios, the classical measure is Dynamic Time Warping (DTW). There are lower bounds for DTW that are very efficient computationally. The research of Professor Keogh might be interesting if you need theoretical foundation for it. Also, normally euclidean distance and Manhattan distance are not very appropriate for time series due to their sensitivity to signal transformations (e.g. shifts), but actually they are often used in practice. 

Both PyTorch and Tensorflow Fold are deep learning frameworks meant to deal with situations where the input data has non-uniform length or dimensions (that is, situations where dynamic graphs are useful or needed). I would like to know how they compare, in the sense of paradigms they rely on (e.g. dynamic batching) and their implications, things that can/cannot be implemented in each one, weaknesses/strengths, etc. I intend to use this info to choose one of them to start exploring dynamic computation graphs, but I have no specific task in mind. Note 1: other dynamic computation graph frameworks like DyNet or Chainer are also welcome in the comparison, but I'd like to focus on PyTorch and Tensorflow Fold because I think they are/will be the most used ones. Note 2: I have found this hackernews thread on PyTorch with some sparse info, but not much. Note 3: Another relevant hackernews thread, about Tensorflow Fold, that contains some info about how they compare. Note 4: relevant Reddit thread. Note 5: relevant bug in Tensorflow Fold's github that identifies an important limitation: impossibility to do conditional branching during evaluation. Note 6: discussion on pytorch forum about variable length inputs in relation to the algorithms used (e.g. dynamic batching). 

The example you linked is from version 0.12 but the current tensorflow version is 1.2. The example underwent a lot of changes from 0.12 to 1.0. Actually, the very line you are referencing is no longer there in the last version. This way, it's possible that you are using a version of tensorflow that is no longer compatible with the APIs used by the example. Ensure that the version of tensorflow you are using is the same as the example. 

One option is to create a directed acyclic graph (DAG) from the pairwise probabilities, where the nodes are the items and the direction of the connections are driven by the pairwise probabilities (the connection goes from item A to item B if , else the connection goes from B to A), and then compute the topological sorting of the graph. This would give you a sequence of nodes that respects the pairwise orderings derived from the probabilities. The python code to implement topological sort can be implemented from the algorithm, but there are python packages like toposort. 

For most clustering approaches, first you need to choose a similarity measure. Some common default ones for raw time series are Euclidean distance and Dynamic Time Warping (DTW). When you have computed the similarity measure for every pair of time series, then you can apply hierarchical clustering, k-medoids or any other clustering algorithm that is appropriate for time series (not k-means!, see this). Update: if the number of time series (along with their size) makes it computationally not acceptable to compute pairwise distances, then one option can be to extract features from each time series, and then use such features as proxies for the time series in the clustering process. Some examples of such features are maximum value, number of peaks, mean value. There are libraries like tsfresh in Python that are meant to easily extract such kind of features from time series. With these features, then any clustering approach like k-means can be applied. 

This kind of setup is normally addressed with a single neural network that has an output sigmoid layer of $N$ units (where $N$ is the number of possible fixes). Having combinations of fixes represented with their own label does not make much sense; instead, let the different labels take the expected value: if 2 fixes are fine, let both expected outputs be 1. Having $N$ networks each one generating a single fix-nofix is equivalent but does not benefit from the potential gains of the multitask learning obtained when training everything together and obtaining a shared internal representation. 

You may want to provide more information if any of the assumptions above is not correct. Now: If only A and the output are observable, we are talking about a latent variable model. Latent variables are best suited for Hierarchical Bayesian methods. If the relations between variables are linear, you can model this as a bayesian linear model, e.g. hierarchical bayesian regression to model your variables and inject your prior knowledge as prior distributions over the weights. Here you can see the application of hierarchical bayesian regression to a latent variable model. This kind of methods are simple and interpretable, they handle prior knowledge graciously and they handle uncertainty. If all variables are observable, you can define a combination of different extremely simple "neural network" models to approach your problem: Network 1 would receive A and output C; Network 2 would receive A and output B; Network 3 would receive A, B and C and generate the output. If the relationships among variables are linear, Network 1 and Network 2 are simple multiplications. You can then initialize their weights with your prior knowledge. The training would adjust the weights for everything to fit, but will not give you insight about the relationships except the fitted weights. I suggest you think about whether bayesian hierarchical models can fit your problem.