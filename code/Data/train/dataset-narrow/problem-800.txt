As spaces can be freely used in SQL, maybe you have some non-printable characters in your code. This problem usually relates to using Windows and/or word processors for programming. How (using what program) did you write your code? Always use an editor designed for programming and always save in 'text only' mode or something similar. If you are in Windows, you can use Notepad (until you get a better editor which is out of scope here), copy-paste your code there and save it. 

Depending on your needs, could be unique key, so would be only foreign key for linking group. In this case can be dropped from . 

Make "Entity" an entry in and treat it as any other attribute. Also, do the same for "Category" (whatever your categories might be) and drop table 

It can be installed to a server or you can buy it as a cloud service. As your organization has an IT department, you should ask their opinion first. 

(As non-native English speaker, "expanse" is not immediately clear to me) I'd join the four tables (customer, supplier, employee, expanse) to a single table 

No. For ibdata files you have variable you can specify one or more but not for individual table spaces. 

Innodb is rather advanced and complex engine than myisam. It is a default storage engine for reasons... you said the system is upgraded from 5.1 to mariadb 10 (eqv to 5.6). So the upgrade path was mysql 5.1 -> maria 5.5 -> maria 10? 

Also if not and you're looking to create slave of RDS slave then, there is an old update blog which says it's possible. Hope this helps. 

Thus it appears that your table have more column than the file. You will have to specify the column-names explicitly. Also the remaining columns should be nullable. (You might want to share table definition if below command doesn't work) Try this: 

See the MySQL documentation or a Step by step replication setup blog if you need to review. If these doesn't help please share the output of (as such it appears connected) and also output of of master/slave. Mainly you need to worry about server-ids, replication filters (replicate-to-db/table etc) 

Consider the source database name is databaseA and destination is databaseB. Step-1. Take dump of tables to be loaded to new host: 

Likely you have some data that doesn't fit the datetime column somehwere in the file. Insert to a staging table with a varchar or navachar filed for the column and look at the data. You may need to adjust some of it (or null out bad records) before loading to the real table. 

First - you must be able to backup and restore a database. You must know how to set up a recurring schedule to backup both the database and the transaction logs. You should know what other maintenance is required periodically such as updating statistics. You should understanding indexing - how to create them and when to create them and when not to create them. You should understand how to read query plans - execution plans or explain plans depending on the db backend. You should understand datatypes and why using the correct one is important. You should also understand why every table needs a PK and how to set up PK/FK relationships and you should never allow application developers to think this stuff should be handled by the application and not the db. You should be familiar with database normalization. You shoud read about performance tuning and database internals for your particular database and be proficient in advanced SQL. You should know how to monitor performance of your database. You should know how to set up new users and use roles based security. You should be able to install the database on a new server. I'm sure there's more, but this is a starting point. 

Updated answer as per updated question: I'd try to understand your question. You have following table: 

Notes: - Here HOSTNAME is the hostname/ipaddress of the destination Just to add, if your databases are on same machine, (which is not the case here, but still saying) you can use RENAME operations to move tables: 

but as @greenlitmysql has mentioned you might see performance issues later on as the data grows (and length of column)... 

So you know now that another master-master setup is not possible as DRMaster cannot replicate from two masters. I doubt multi-source replication is a solution here as it will create more trouble than help. You need to make sure to keep your DRSlave, "master-ready"... setting auto_increment% and binlog + logslaveupdates settings. 

If this condition fails, you can conclude your backup is a failure... Another mode is to using which should tell you "Backup completed successfully" So if only worry is if-it-was-successful, then you can use above methods to determine. Xtrabackup is physical while mysqldump is logical, which is mostly a decisive factor in choosing one over other. In case you need to restore individual table/database, with mysqldump it is possible but not with xtrabackup (unless you do complete restore). Physical backup is fast comparatively but also asks for frequent restore tests like data corruption is not easily identifiable (I think so) for hot backups. There are more pros and cons you can consider looking around but if you think this time is too much and you have identified your restore requirements fit in with Xtrabackup, go with it. Here is holland-xtrabackup setup steps if you'd like to configure it that way. If you're worried about speed and still want a logical backup, go with mydumper. If you're fine with physical backup go with Xtrabackup. 

The only time I would ever consider a delimted list is if there is no chance you will want to look at the data separately. Then indeed it is faster. This is a very rare case, however. And in what you are doing, there is an approximately 100% chance you will want to look at individual payments separately. Delimited lists are harder to query (and updating if someone made a typo, ugh) for individual elements (and generally slower for this than a related table would be) and harder to maintain the data integrity. Databases can easily handle many millions of records in tables that join with correct indexing and design. If you need more performance at that point typically, you partition the database. Sit down and read some books on database design and performance tuning. The rules for good design are different in databases than applications. Please take the time to learn them. 

When you are building complex queries, you should build them in stages checking the results as you go, not build one whole huge query and then try to figure out what is wrong. Here is what I do. First I list all the columns I want on a spearate line and comment out all the columns except those in the first table. Then I add the from clause and any where conditions on the first table. I note how many records are returned. Then I add each table one at a time and check the results again. I am especially concerned when the number jumps up really high unexpectedly or goes down unexpectedly. In the case of the number jumping up high, you may have a one to many relationship that needs further definition. This is especially true if the field or fields you are getting from the table are almost always the same. You may need a derived table or a specific where condition to resolve. You might even want to do some aggregation. Now I'm not saying it always bad if the record counts go up, only if they go up when you didn't expect them to or when the result set looks suspicious. In the second case, you generally have an inner join where you need a left join. The number of records went down because you did not have a matching record in the joined table. I often check each inner join with a left join to ensure that I return the same number of records. If it does then the inner join is appropriate, if it doesn't then I need to decide if it is filtering records I should be filtering or if I need a left join. When I am not sure why the record counts are off from what I expect, I use a select * (temporarily) just to see all the data, so I can determine why the data is off. I can almost always tell what the problem is when I see all the data. Do not ever use select * in a multiple join query like this or you will be returning much more data than you need and thus slowing the query as at a minumum the join fields are repeated (plus it is truly unlikely you need every field from 20 joins!). To find you issue, you are going to have to repeat this process. Start with just the from clause and the where condtions on it and add tables until you find the one (s) which are causing the query to eliminate all records. Don't stop with the first find, it is possible in a long query like this that you have multiple problems. 

Not sure if you just want to create a read replica clicks away from you? If not then I guess you need to restore from a dbsnapshot. You can: 

Note that APP2 will read eventually consistent data and it will have to handle writes to DB1 itself. (I do not tend to like master-master replication setup and writes to both ends due to past experiences but it is also an option.) You said no links but below this link includes steps for setting replication, you might want to refer. 

Considering all the slaves were in-sync with master while you issued reset-master, it is safe to issue on slaves with newly generated first bin-log. 

--> use --no-create-info to dump only data. (onlydata.sql) --> Have standard table definitions ready and intact for dev server. (definition.sql) --> Refreshing dev => load definition.sql and then onlydata.sql 

I donot think there is --ask-pass in xtrabackup! You might want to write a wrapper shell script which will 

"I know its different for every application" 15G is not a large data-set unless you tell me it's a single table! Your data can be contained in innodb-buffer-pool and performance should be better. Importantly, you should make sure your queries are well written and tables are indexed correctly. (log-queries-not-using-indexes) I'd start with 1 Sec and review the amount of queries getting logged and work on improvements. long-queyr0time is dynamic and you should be able to play with it depending on load it generates. 

You don't want simpler, you probably need more complex. Invocing is a complex field that requires a good bit of accounting knowledge (knowledge of internal controls to prevent fraud in particular and how those would be implemented in your database). But lets start with a simple example, customers do not have only 1 address (or ship to only one address), so you should have a separate address table. You should have a separate phone table. You should have lookup tables for phone types and address types. You probably need product lookup tables. I can't say for sure in mysql, but float is generally an inexact datatype and should not be used for any number you intend to do calclutions with. Not unless you like dealing with rounding errors. Or losing money. Accountants tend to hate that kind of thing. Shipping address should be linked to the invoice not the customer. I may ship one order to a customer in Maine one day and another to a differnt person in Washington the next. I may ship to any one of my three offices or to my home or to my parent's home. How are you planning to maintain those updated by and updated date fields? They shoudl be in a trigger or they are useless. Normally products have some sort of product details that can vary such as color or size or no of items in the package. You have a description field for this but 80 chazractesr seems way too small to me. Truly you need to learn normalization before you even attempt to design a database, let alone one with serious legal implications. Taxes can be quite difficult depending on how many states or countries you are selling to and the typesof products you sell. Without knowing more detatils it is imposooible to design a good set of tables for taxes. There is no way under the sun that I would consider deigning such a thing without consultations with a good experienced accountant. Preferably one with auditing experience. There are many many professionally designed accounting packages that do invoicing, it would be cheaper and far better from a legal standpoint to buy one. 

PostgreSQL documentation is the standard form of run automatically. If you insist on freeing unused space from tables, then do manual . 

C could be a fire insurance for a house (B) or traffic/collision insurance for a car (B), in which cases C links to B, or a life or health insurance for the person himself (A), in which case C links to A. In this scenario, some kind of type field of C would indicate to data model or application logic, should it link to an A or a B. 

SQL standard does not specify in which order the rows are returned, if the query has no ORDER BY. Some implementations return rows in an order which might seem deterministic, but unless some order is guaranteed in documentation, you can't trust next version of same software or a different data set would behave same way. 

Since the primary key (page_id, user_id) is same for all roles it would be better to put all roles in a single table, the one you named pages_users. The question on using role_id and a separate roles table is a matter of personal preference. I would use a separate table, at least if I'm going to populate an UI component list based on the choices. 

So in anycase statement/row, you don't need to be worrying about the data on slave and it should match the master. but still if you want and can fit-the-logic on an event, this might be a possibility on slave at the risk of inconsistency!! As you said this is dw slave, would you consider a separate process to do the task you're looking to do, say a procedure? 

AFAIK it isn't correct way to pass argument. It should not be accepting that argument. Never done that; importantly it isn't even necessary for default location!!! 

InnoDB is performing well and it does have fulltext as well (though performance is a bit of something being worked over there.) InnoDB is even a default storage engine in latest MySQL versions. If you really really need performance from FULLTEXT go with MyISAM (Note that, you have FULLTEXT in InnoDB and you can try it.) is (default) good choice. There is one more reason to avoid InnoDB in favour of MyISAM is possibly due to "Storage requirements" (it takes 3x space) but that shouldn't be a reason actually. There are more reasons to avoid MyISAM than to use it. Go with . (As every one here seems to comment) 

The data will likely long outlive the application code. If the rule is critical to the data being useful over time (like foreign key constraints that help keep the integrity of the data), it must be in the database. Otherwise you risk losing the constraint in a new application that hits the database. Not only do multiple applications hit databases (Including some that might not realize there is an important data rule) but some of them such as data imports or reporting applications may not be able to use the data layer set up in the main data entry application. An frankly, the chances of there being a bug in the constraint are much higher in application code in my experience. In my personal opinion (based on over 30 years of dealing with data and experience with hundreds of different databases used for many different purposes) anyone who doesn't put the contraints in the database where they belong will eventually have poor data. Sometimes bad data to the point of being unusable. This is especially true where you have financial/regulatory data that needs to meet certain criteria for auditing. 

Active-Passive master-master is a good setup but I have seen SUPER (humans)users writing on slave without setting sql_log_bin. (Though super_read_only in 5.7 will change things around this.) Anyways, following is possible and works. 

A1. How large are the databases? If you need logical backup, you can use mysqldump (--single-transaction, --master-data are some options you need to review and use). For speed, you want to go with mydumper/myloader tool. It will also make sure of taking individual dumps. Advantage here is you can grab the database you need to restore without having touch other databases of the instance. If you choose to go with mysqldump, you still can use mysqldumpsplitter to extract the tables/databases of your choice to export from full dump. Alternatively, you might want to use the physical backup using Percona's Xtrabackup which provides hotbackup. Here is a post to setup Xtrabackup using Holland framework. A2. Backup from Active Master? No! If we have replication failure on read-only master than priority is to get it fixed and then take the backup. But try to avoid backsups from Active Master. It's also advised to make sure your backups are stored on a different location other than read-only master! A3. Binary log backups provide point-in-time restore. You can store binary logs to a separate partition and have it backedup/snapshot or scripted to copy files to a separate location. Check out these two articles: 1 & 2 Hope this is helpful!!