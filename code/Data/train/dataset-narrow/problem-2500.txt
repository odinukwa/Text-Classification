Initialize $T[u,v] \gets \{S\}$ for each edge $u\to v$ with clobber set $S$ in the graph, $T[u,u] \gets \top$ for each vertex $v \in V$, and otherwise $T[u,v] \gets \bot$. Initialize $W$ to contain the set of pairs $(u,v)$ such that there is an edge $u\to v$ in the graph together with the set of pairs $(u,u)$ such that $u$ is a vertex in the graph. While $W \ne \emptyset$, do: a. Pop an arbitrary vertex-pair $(u,v)$ from $W$. b. For all edges $v \to w$ (with clobber set $S$) out of $v$, do: 

Use repeated-squaring, along the lines you mentioned. Factor $n$ using a state-of-the-art algorithm, then use the Chinese remainder thoerem. If $p$ is prime, you can compute $a^{b^c} \bmod p$ efficiently by computing $b^c \bmod p-1$ using fast exponentiation, call the result $d$, then computing $a^d \bmod p$. If $n$ is a product of primes, you can combine the results using the Chinese remainder theorem. 

You'd have to fill in some details, but I bet you could expand this sketch/outline to get a protocol that would solve your problem efficiently and securely. 

Given all of this background, we can now use induction to find a solution. We start by forming a solution that works for $i=0$. At each step, we increment $i$, until we reach $i=\alpha$. Each step can be done in 64-bit arithmetic semi-efficiently (with a few thousand basic arithmetical operations), so the whole computation should be semi-efficient. 

My guess is that these are free parameters of the algorithm that you must set somehow. For instance, you might evaluate different choices of these parameters on a set of training data to see which performs the best. You could also use cross-validation or other techniques to evaluate different settings of these parameters. Note that p.46 of the thesis says "This threshold value is based on the predicted ratings that are achieved after running the item-based collaborative filtering algorithm." That sounds consistent with my interpretation above. In general, I would recommend reading the original, published, peer-reviewed research papers on algorithms for this problem. I would expect them to generally be of relatively high quality, and to be more likely to explain the algorithm and methodology in full (or cite a source that does). 

In practice, if you want to extract randomness from a source $S$, you don't use an extractor. Instead, you use a cryptographic hash function and hash the data from the source. (This is how cryptographic-strength pseudo-random number generators distill randomness from many non-uniformly distributed sources of randomness.) This can be proven safe in the random oracle model. Of course, if you care mostly about theoretical bounds, extractors are better because they provide provable bounds without requiring cryptographic assumptions, so theoretical work will naturally focus on extractors -- but if you want to do this in practice, that consideration is less important. If you want to do this in practice, I recommend you use a cryptographic hash function, as the assumptions are reasonable and you get something that does not waste any entropy. (And before you reject the idea of depending upon cryptographic assumptions: Hey, if it's good enough to protect e-commerce and banks and classified documents, it's probably good enough for your purposes.) If you use a cryptographic hash function, the bottom line (essentially) is that all of the entropy of the source is preserved (up to the security level of the cryptographic hash function). So, quantifying the amount of extracted randomness comes down to quantifying how much randomness is present in the source. There is no good black-box way to determine the min-entropy of the source distribution. So, in practice, you don't try to determine the min-entropy of a source through observational methods (e.g., observing many samples and then doing something). Instead, you need to have a model of how the source works, and to derive estimates of its entropy based upon that model. That model might be based upon the physics of the source or other domain-specific assumptions. This is not a question that algorithms can help you with much. Why is it hard to estimate the min-entropy of a source? Consider two sources: $S_1$ outputs a random $1000$-bit value $x$ (distributed uniformly at random); $S_2$ picks a random $128$-bit value $k$, then uses AES in CTR mode (a secure pseudorandom generator) to stretch $k$ to a $1000$-bit value $y$, and outputs $y$. Notice that, assuming AES is secure, there is no feasible way to distinguish source $S_1$ from source $S_2$ by just observing their outputs: any algorithm for distinguishing the two sources would require something like $2^{128}$ steps of computation. However, the min-entropy of $S_1$ is very different from the min-entropy of $S_2$ ($1000$ bits vs $128$ bits). This example shows that computing the min-entropy of a source is a difficult in general, so you should not expect to find any efficient algorithm to do that for you based solely on observing some outputs from the source. For a more rigorous treatment of the complexity of estimating the min-entropy, see e.g. the following paper: 

For each edge $u \to v$ with clobber set $T$: a. If $T \subseteq S$, return immediately (without adding any edges). b. If $S \subset T$, remove the edge with clobber set $T$. Add an edge $u \to v$ with clobber set $S$, and mark it. 

Use the pigeonhole principle. Let $w$ be the shortest differentiating word. Suppose its length is larger than the number of states of the DFA. Consider the sequence of states traversed on input $w$. What can you say about them? (Use the pigeonhole principle.) 

For symmetric-key encryption: If Alice and Bob know a proof that $D_K(E_K(x))=x$ for all $x,K$, then they should be able to prove this fact in zero knowledge. In particular, they can produce boolean circuits for $D_\cdot(\cdot)$ and $E_\cdot(\cdot)$, publish a commitment to each boolean circuit, and then turn their proof into a zero-knowledge proof. In particular, verifying that $D_K(E_K(x))=x$ for all $x,K$ is in co-NP, and thus in IP, and there are zero-knowledge proofs for all of IP (and all of PSPACE). Now if Alice wants to encrypt a message, she can give a zero-knowledge proof that she has encrypted some message correctly by doing the following: publish commitments $C(x),C(K)$ to her message $x$ and her key $K$; publish a commitment $C(y)$ to her ciphertext $y=E_K(x)$; and now give a zero-knowledge proof that all the commitments are consistent (the ciphertext corresponding to $C(y)$ is a correct encryption, under the boolean circuit for encryption that was committed to earlier, under the key corresponding to $C(K)$ of the message corresponding to $C(x)$). Whether this is a useful thing to prove is another matter entirely -- it's not clear exactly what you are looking for or how you would use a solution to your problem, but hopefully this will give you some ideas. For public-key encryption: See Sadeq Dousti's comment. The exposition above can be readily adjusted to fit the public-key encryption setting, along the lines Sadeq Dousti suggests. 

Of course, I realize that the worst-case coverage, taken over all possible DFAs, might be very poor. But I would be satisfied with any approach that one can argue is close to optimal, in any meaningful sense. For instance, maybe one metric might be to use a regret ratio, where the regret of strategy 1 compared to strategy 2 is the maximum, over all DFA's $M$, of the benefit of strategy 2 on $M$ divided by the benefit of strategy 1 on $M$. Or maybe you can suggest some other way to formulate this in a principled way that admits an interesting solution. I'm open to suggestions about how to formulate the problem in a way that permits analysis. If it's helpful, I'll promise that the alphabet is small (say, $|\Sigma| \le 10$), the number of states is not too large (say, $|S| \le 1000$), and the diameter of the graph is small (say, at most 10). I'm also interested in the variant where $M$ is a NFA instead of a DFA, but I thought I'd start with the simpler case. This is an attempt at an application of theory to practice. The application is randomized testing of UI-driven applications, where we think of the application as a DFA and the inputs represent user actions (e.g., tapping on a particular portion of the screen). The problem above is an idealization of the problem where we don't get any feedback as we go (which is motivated by the fact that it's not trivial to observe anything reliable about the state of the application). 

I think you can finish off your computation using inclusion-exclusion. If we fix a set $S$ of $k$ elements, let's count the number of ways to choose the $p$ sets so that they all contain $S$. Call this $n(k)$ [note that this number depends only on $k$ but not otherwise on $S$, justifying my notation]. You correctly showed how to compute $n(k)$, namely, $$n(k) = \prod_{i=1}^p {N-k \choose m_i-k}.$$ Now you should be able to use inclusion-exclusion to count the number of ways to choose the $p$ sets so that their intersection is of size exactly $k$. For example, if $m=\min(m_1,\dots,m_p)$, then the number of ways to choose the $p$ sets so that their intersection is of size exactly $m$ is $$I(m) = {N \choose m} n(m)$$ [as you correctly noted in your question]. As another example, the number of ways to choose the $p$ sets so that their intersection is of size exactly $m-1$ is $$I(m-1) = {N \choose m-1} n(m-1) - m {N \choose m} n(m).$$ Here we've counted each set $S$ of size $m-1$ once; but then that overcounts things, because each set of size $m$ got counted $m$ times (once for each of its subsets of size $m-1$), so we account for the overcounting by subtracting off $m$ times the number of ways of getting an intersection of size $m$. The number of ways to choose them so their intersection has size exactly $m-2$ is $$I(m-2) = {N \choose m-2} n(m-2) - (m-1) {N \choose m-1} n(m-1) + {m \choose 2} {N \choose m} n(m).$$ Here we've accounted for the overcounting by subtracting something related to sets of size $m-1$; but then that over-corrects and now undercounts some sets of size $m$, so we correct for that by adding an appropriate term. It's easier to re-discover the formula than to explain it, but hopefully you get the idea. You should be able to take it from here. 

You can turn it into CNF using the Tseitin transform. Some SAT front-ends will do the transformation for you: for instance, the STP solver has native support for all of those operations and thus can handle the transformation for you. 

Here's a trivial answer: assuming $L \ge \lg |N|$, then you need to know the value of $f$ at all $|N|$ points to uniquely determine $f$. Therefore, the approach you sketch doesn't help you at all, unless you somehow know that the length $L$ of the program is extremely short: much shorter than $\lg |N|$ bits. Consider the family of functions $F=\{f_i:i\in N\}$, where $f_i$ is defined to be the function $f_i(x) = 1$ if $i=x$ and $f_i(x)=0$ if $i\ne x$. Notice that the Kolmogorov complexity of computing $f_i$ is about $\lg |N|$ bits, since you can hardcode the value of $i$ in the source code and then all you need is a simple conditional statement ($O(1)$ extra). However, you cannot distinguish $f_i$ from the all-zeros function unless you test it at the input $i$. You cannot distinguish $f_i$ from $f_j$ unless you test at the input $i$ or $j$. Therefore, you'll need to evaluate $f$ at all $|N|$ inputs, to uniquely determine which $f_i$ we are dealing with. (OK, technically, you need to evaluate it at $|N|-1$ inputs, but whatever.) 

With some basic arithmetic, we can reduce this to a question about a two-variable Diophantine equation. Based upon your comments, it sounds like $d$ was intended to be given as input. Now the question is just: can we choose $x_1 \ge 1$ and $n$ such that $x_1 + (n-1)d < K$ and such that the arithmetic sequence $x_1,x_1+d,x_1+2d,\dots,x_1+(n-1)d$ sums to $K$. It is easy to compute the sum of this arithmetic sequence. By the Gauss formula, it sums to $$n x_1 + dn(n-1)/2.$$ So, given $K,d$, we want to find a solution to the equation $$n x_1 + dn(n-1)/2 = K$$ where $x_1,n$ are positive integers and where $x_1+ (n-1)d < K$. You can view this as a diophantine equation in two variables $x,y$ of the form $$2xy + \alpha x^2 + \beta x + \gamma = 0,$$ where $\alpha,\beta,\gamma$ are integer constants given in advance and there is an additional linear inequality on $x,y$. (Here I made the replacement $x=x_1$, $y=n$.) I don't know if this problem has a nice solution. One thing we can say is that $n$ must be a divisor of $2K$, since we know $$2nx_1 + dn(n-1) = 2K,$$ and the left-hand side is divisible by $n$. Therefore, if you know the factorization of $2K$, and if it doesn't have too many different prime divisors, you could try enumerating all divisors of $2K$ as the candidate values of $n$, and then for each candidate value of $n$, solve for $x_1$ and see if it yields an integral solution. However, in general this will not be efficient: it gives an algorithm that is efficient in the average case but runs in exponential time in the worst case.