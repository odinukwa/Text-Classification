When someone says Cook-Torrance, they usually mean a microfacet BRDF where the distribution (D) is Beckmann, which I think is what the original Cook-Torrance paper was about. GGX is really just a different distribution (D term). GTR is another one. The names are often used as a shortcut to mean a BRDF using that distribution. Now all of these assume specular reflection (or refraction) on the microfacets, hence the fresnel term. If you take a similar idea but use diffuse (lambertian) microfacets, you get the Oren-Nayar BRDF. I think that one is based on a gaussian distribution of the microfacets but I'm not familiar with the details. 

Just don't consider that a general replacement for a proper bessel function. However, it's fine for computing the window as far as I could tell. As the second wikipedia page I linked shows, that can be used on a symmetric filter with $N$ weights by using $x = \frac{2n}{N-1}-1$, where $n$ is $0, 1, 2 \dots N-1$. Computing the weights of the sinc filter itself is another thing entirely as it depends on filter size, how much you're resizing, etc. I suggest posting a separate question if you don't know how do to that. 

Specular surfaces which use MIS are not perfectly specular like a mirror. They have a small amount of blur, otherwise there is indeed no point in sampling the light as all the samples will evaluate the BRDF as 0. In fact, you would only need to trace a single reflection ray. A small amount of blur means that a given camera ray will see a small area of the lightsource (assuming it hits where the specular highlight is visible). Thus, a small fraction of the light samples will get a BRDF which is not 0 so it will eventually converge. The less blur, the more samples it will take. You are correct that such a case will converge faster if you sample only the BRDF. Likewise, a very diffuse BRDF will usually converge faster if you sample only the light. But this is not true if your light is very large and your surface very close to it1. Where MIS shines is that it handles those cases as well as the BRDFs which are neither very diffuse nor very specular. If you choose to ignore it, what you have to do is a simple integral. There are a few ways to go about it but if you start from MIS equations, just use 0 as the PDF of the BRDF since you're not generating those samples. And be prepared for some cases which won't converge well. 

I believe this is where you went wrong. If you draw the 2D version of this on a sheet of paper, it will be obvious that the edge of your triangle which touches the side of your square goes through the sphere. More so as the camera comes closer. That line needs to be tangent to the sphere. So your large similar triangle will have its right angle at the contact point with the sphere, not at the sphere's center as you did. If my trigonometry is not too rusty, this gives $\frac{R}{\sqrt{\lVert V\rVert^2-R^2}} \times \left(\lVert V\rVert-R\right)$ 

It depends how you define easy and what kind of constraints you have. The general case of this is rendering caustics but that's probably not what you're looking for if real time is your target. If your mirrors are always flat as in your demo and you only want to support a single bounce, the easiest I can see would be to reflect your light sources on the other side of the mirror's plane, before you start a frame. Let's call each reflection a virtual light source. Then for each of these, test if the ray between whatever you're shading and the virtual light source hits inside the mirror (simple plane/ray test). If it does hit then you trace the actual shadow ray to the mirror and the rest of the shadow ray from the mirror to the actual light source. This obviously won't scale well to many mirrors. I think it could be adapted to 2-3 bounces if the mirror and light count is low. The number of virtual lights could quickly grow out of hand though. 

The general idea for sampling half vector based distributions is that you generate $H$ and then compute $w_i$ by reflecting $w_o$ about $H$. This is so $H$ will be the half vector of your $w_i$ and $w_o$ pair. It is standard reflection: $$w_i = -w_o + 2(w_o\cdot H)H $$ How you generate $H$ depends on the specific distribution. Generally, it is done in polar coordinates, with the angle from your distribution's center being picked using some specific function. Then the azimuth will be a uniform distribution, unless your distribution is anisotropic, in which case it gets more complicated. For example, with Cook-Torrance, it goes something like: $$\tan \theta = m \sqrt{ - \log \xi_1 }$$ $$\phi = \xi_2 2\pi$$ Then, given an orthonormal basis made of vectors $N$, $a$, $b$: $$H = a\sin\theta\cos\phi + b\sin\theta\sin\phi + N\cos\theta$$ As for problems in your code, replacing this: 

No disadvantage that I can see but what you have is not a kd-tree if you have axis-aligned bounding boxes in every node. A kd-tree only has a splitting plane in every node, so there is no AABB to shrink. The way to deal with empty space in them is to split the node further into an empty node and a smaller node with the same objects as its parent, when justified by whatever heuristic is used. It is just one more possible split to consider when picking how to split a node. 

The calculations involved for the texture lookup would be trivial as everything is 2D, in a plane of the world coordinates. It also does not need programmable shaders which IIRC were not around yet at that time. What you would need as texture coordinates is simply the (x,z) location of the vertex (assuming y is toward the sky here), along with an offset equal to the (x,z) location of your character. In OpenGL, I believe that could be achieved by along with a texture transformation matrix. The reason for the artifacts is that this really does make a "character-silhouette shaped tube", as you put it. Looking from above, the shadow is painted on every surface regardless of it being in front of or behind the character (above or below, from the camera's view). It's like you're dropping black ink from the sky inside the outline of the character and it goes through everything. 

The assumption is that the light samples all have a similar contribution to the final result. Even with very good sampling of the light, this must assume that the BRDF will be similar for all light samples. At least I don't know of any direct light sampling formula which takes the cosine term of the diffuse surface into account. Pushing this to the extreme, you can build an example where most of the light is behind the surface, so most of the light samples will contribute nothing at all to the final result. BRDF samples are obviously sent only on the front side of the surface so they might converge faster, even if only a small % of them hit the light. 

Section 6 of Microfacet Models for Refraction through Rough Surfaces has a good description of how they did it to validate their own model. That may not be everything needed to build a full database (eg. more automation) but it's an interesting read. 

The classic method is to uniformly sample the disc at the base of your hemisphere and to project your samples upwards on the hemisphere (eg. compute z from x and y). This yields a cosine weighted distribution. As the projection preserves stratification, you need only use stratified sampling of the disc to get a stratified cosine distribution. 

They must define it as a surface of revolution of a line segment. From the comments, they are probably the a and c coefficients of the canonical hyperboloid equation. With a surface of revolution, a == b so no need to store b separately. It's clearly related to figuring out ah and ch from the line segment. I can't say more precisely without working out the math involved. I'll leave that to someone with more free time. 

That looks like multiplying the front image by the luminance of the shirt behind it. There are multiple definitions of luminance but for a start, I think you could assume your data is sRGB and follow the transformation to CIE to get the CIE Y component. 

This is the second version. Remove the to get the first version which is more like Lanczos. The modified Bessel function of the first kind seems to be a C++17 / boost thing. If you have neither, this quick hack will do: 

I think dilation would be close enough to what you want. It is likely that font processing software would have more advanced algorithms to avoid rounding all the corners, among other things. Fonts are a whole separate world with a lot of rules about how things should be done to look right. No simple image processing algorithm will respect all those rules. 

GIF animation compressors normally do this automatically, using transparency in a given frame to avoid storing what has not changed from the previous frame. The reason it's not working in your case is that your input is bad. The flat part of the parchment is not really static. It is moving slowly upwards, at a rate of about 1 pixel every 20 frames. Fix your animation so the non moving parts are perfectly identical between frames and you should get a much smaller file. You would also get a smaller file if you avoided the noise in the unfurling part (most visible in the shadow). Such noise barely compresses at all. This looks like it came out of some 3D software so crank up the quality settings until the input is noise free. If dithering, which is really added noise, is used when producing the images, make sure it does not change from frame to frame. Or just turn it off entirely. You might get a little banding but the file will be smaller. You'll need to decide which you prefer. 

Having not worked on these games, I can only speculate... but from your description, it sounds like a crude shadow map with orthogonal projection and no depth information. It probably has (or had) a specific name but the way I suspect they did it is: 

It looks like distortion because the trapezoids are rendered as two triangles by the GPU. This leads to incorrect UV interpolation. Try this: draw a horizontally symmetrical trapezoid with the top being much shorter than the bottom. Then draw one of its diagonals to split it into two triangles. Place a mark at the midpoint of that diagonal. It should be obvious the mark is not at the horizontal center of the trapezoid. Yet that mark is where the middle of the interpolated texture coordinates will be. Your border will go through the mark but also through the center of the top and bottom edges. You should not have that problem when producing a final image with any decent renderer. 

Most of this is entirely subjective of course. I think it's fairly good but falls short of the state of the art in many areas. 

This is not really a CG issue but you appear to have an integer division problem. Assuming is an integer, is always 0 and is 0 for all threads but the last one, where it is 1. So one thread is doing all the work. 

Actually, it would change if you changed units in the $\textit{pdf}$ definition. The fundamental reason is that the $\textit{pdf}$ is defined as the probability per steradian. That's what the density part means. You could very well redefine it as the probability per hemisphere and end up with a $\textit{pdf}$ of $1$ for your example. 

Gives me a much nicer plot. Your remapping was incorrect as it was done on a vector which includes z and is no longer in 0,1 for x and y because of the normalization. This explains why the center was not in the center of the frame and the whole thing was leaning. Removing (a transformation which makes no sense) and using instead fixes what I think you were calling the hard transition in the 4 quadrants. That was caused by squaring the components of the vector. The remaining glitch in the center appears to be numerical issues. Fixing that will require reworking the code to better handle some cases (likely where you have the constant for a sin and the for a cos). 

I was curious so I tried it. I used the Kaiser Window from Wikipedia: $\frac{I_0\left(\alpha\sqrt{1-x^2}\right)}{I_0\left(\alpha\right)}$ This page has a slightly different formulation using $\pi\alpha$ instead of $\alpha$. Given the comments in the page you linked about the Kaiser window being very similar to Lanczos and how they even overlap in the graph, I suspect they also used the former formula. You can see how close they are if you plot both on Wolfram. This left me wondering if that's what Mitchell originally meant as it's not "noticably better". I could not see the difference at all with my test texture (it's there, just not noticeable). However, using the second formula, the difference in the plots is much more striking. And there is a noticeable difference in the resulting images. Whether it's better is a matter of interpretation and depends on the filter size. With $x \in \left[-1,1\right]$ , I used this C++ windowing function: