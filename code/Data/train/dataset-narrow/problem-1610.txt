I`ve done some more tweaking and tinkering and have gotten to a good spot in the last couple of days. So I will write up a summary of what I got to work. To refresh the topic: I wanted to create a network behind a single (LAN) interface of my pfsense box. The requirements for the network was to provide workstations in the network with working ipv6 addresses. The network shouldn`t allow the global network to identify devices on this network, but should provide an option to write firewall rules based on addresses in this network. I want all hosts in this network to be able to "anonymously" browse the web and stay unidentified, and I want all hosts in this network to have access restrictions for other networks that are directly attached to my pfsense box. So I need to identify each of the hosts and write per host (or per employee role) firewall rules that restrict some access to inside resources. To my knowledge there were 3 ways to achieve this. 1) Use ULAs (Unique Local Addresses) for inside communications and have access restrictions on the static ULAs, but use NAT66 for global communications, protecting the identity of hosts. This is agreed to be bad. 2) Have multiple per employee role (access level) VLANs in the network. That way the firewall rules can be written based on VLANs ignoring the addresses and every host could have GUAs (Global Unique Addresses) for both global and local communications. There could be temporary privacy addresses to help protecting the identities from the global network. This was not an option, since I want to do this with a single VLAN. 3) Still keep having a single VLAN with GUAs, but assign a specific address range for each host in which it can rotate its addresses. This way I can write firewall rules based on these ranges and protect identities with the rotating addresses. This is what I wanted to achieve, but found is impossible in pfsense (opposed to some commercial solutions). Now I have found a fourth way to achieve this. 4) Each host in the LAN has 2 different network addresses: one ULA and one GUA. There can actually be multiple GUAs, since GUAs are autogenerated. I could have the the Router Advertisements taking care of GUAs by advertising the global prefix to hosts in the network. These GUAs would then be used for reaching the global network and hosts could use privacy (rotating) addresses so that they stay unidentifiable from the global network. As for the ULAs: the DHCPv6 server would take care of these addresses providing a statitc per DUID LUA for each host. These addresses could be used for the mentioned inside communications and have firewall rules written based on them. To make this work I need each host to only have a single address in this local network, so I have to disable RAs (Router Advertisements) for this prefix. By default pfsense generates the /var/etc/radvd.conf configuration file with all the RA listed prefixes and always adds the DHCPv6 prefix to RA as well. This way all the hosts get multiple addresses which would cause them using privacy addresses as source and my firewall rules would be of no use, so I commented out the part that generates the DHCPv6 prefix clause for the radvd.conf file from /etc/inc/services/inc file, and the DHCP network no longer gets advertised with Router Advertisements. This way each host only has a single address within this local prefix. In addition to that I have to make sure that hosts will always use the ULA to reach inside resources and GUA for global resources, which happens thanks to the 2nd rule in source address selection protocol per RFC6724 ($URL$ which says that a source address with the same scope (global, local, link local) as the destination will be preffered. Thanks to this and inside resources having local addresses as well I have everything working the way I want it to. The only hack is to change the /etc/inc/services.inc file to comment out that one part. Furthermore I also have to set up an IP Alias type Virtual IP for the LAN interface with an address from the second prefix of the network. So one address gets set to the interface (in my case the local one) and the other (global address) gets set to the virtual IP. So this fourth way is working as expected and hasn`t required more VLANs or NAT66 or a feature pfsense does not offer. 

My first post so be gentle please! So I have an installation of graphite up and running on Ubuntu 16.04 server. (version 0.10) I am sending in metrics and everything works fine, but I am sending in multiple timestamps at once. As in I am sending in data once per minute with per 10s timestamps. Every minute new data comes in. It can happen, that a key with a corresponding timestamp (same as before) is resent a minute later. So the new value is being saved and last one lost, because of the way graphite works. So I put a carbon-aggregator in front of carbon-cache thinking it will aggregate the values per timestamp. It does not. It aggregates all values received in a time interval ignoring the timestamps. I found that statsd has the same functionality. What I want is for the aggregator to aggregate values per timestamp. As in keep data for lets say 1 minute and see if a datapoint for the same key and timestamp is received. If so sum them. If no such data came, forward the original data to carbon-cache. Is there a way to do this with graphite or statsd, or do I have to write my own little buffer in front of carbon-cache (instead of carbon-aggregator), that does this? It could be a linked list of self made structures in C resulting in about 150 lines of code, but would take precious time. Someone has definitely had the same problem, but has anyone found a solution? Bump: has noone really encountered this issue? Thanks in advance! 

A Bastion host is a machine that is outside of your security zone. And is expected to be a weak point, and in need of additional security considerations. Because your security devices are technically outside of your security zone, firewalls and security appliances are also considered in most cases Bastion hosts. Usually we're talking about: 

You can use powershell group policy commandlets. You can get the users information using Get-ADUser. You can then follow this up by using Get-GPInheritance on the users OU. After this you can run Get-GPOReport on the resulting GPO's. I haven't tried this before, but it should be possible with the new commandlets. However this will be quite a bit of work and research to get going, and perhaps a bit much for the answers you're looking for. Depending on how large your environment is, and how many group polices you have, it might be easier to follow Nitz's answer. 

Update 3: I've found that the very first time the is opened after a fresh install, and the folder is clicked the following error appears in the application log: 

The issue was resolved by Microsoft support, and we received the root cause analysis a week later. Apparently the day before, a new version of the API was released. Microsoft ended up rolling back the portal API, which solved the issue for us, and several other customers. So in our case, this was not a solve-able problem, the only recourse was to contact Microsoft. 

My lack of comprehending these steps and simply copy pasting from the internet caused my mistake. When it reaches the second pipe, it sets the not yet updated securitygroup (So I'm simply pushing the group exactly as I pulled it) and gives me the success message. After which I the new group in my cache, which is a local copy of the rules without publishing it. So a correct way to do it is: 

Cloning an Azure VM is done by using normal operations for preparing a machine for imaging. Windows machines will need to be sysprepped and then captured. There are ways to do this either through the GUI or through Powershell / CLI. But whatever method you choose, you will need access to the VM in question. Linux machines will need to be prepared by the azure linux agent, waagent. The linux agent will deprovision your machine, and prepare it for capturing. 

The thumbnail photo is not really related to the users profile as such. It is a property that was added with Exchange2010 to allow for a central, manageable repository for user pictures within outlook. As it was quickly determined that you don't want your IT department to be responsible for hundreds of user photos (you're bringing back bad memories of my first 'sysadmin' job here!) there are quite a few tools out there that allow for the users to upload their own image. This requires some minor permission tweaking in AD (nothing that could pose a security issue). You might want to google "AD photo upload". 

Sounds like you're using a private virtual switch as your hyper-v switch for that guest. You can review the different types of switches in hyper-v and their effects here. If this isn't the case, try running a tracert and adding the results here so we can have a better idea of what's going on. For this particular instance however the issue was caused by the ICMPv4-In rule being disabled. In other words the guest OS firewall was the cause. 

We ended up trying to remount the databases, restart the services (SQL Server and Virtual Disk services). But in the end the only solution was a restart of the server. What happens during the VSphere Snapshot process that could cause this chain of events? If this is related to the VSphere Snapshot, why would a reboot fix it? 

Why do you want to put one pair of huge disks in there, when you have a server running a database? If you want to look at performance, try putting your database on different disks than your OS / web application. Databases are often bottle-necked on I/O. In stead of purchasing 2x 2TB disks, you could buy 2x 320GB disks for the OS / web app, and 2x 128GB SSD for the database. And I don't think it needs much repeating but backups are important! Don't put them on the same disk as your database etc. 

We are currently experiencing failures on our SQL Server machines. It appears that once a server on the same (VMWare) host as our SQL Servers starts a maintenance procedure. The SQL Servers will balloon, loose nearly all of their memory. What I would expect next is for the SQL Server to receive back its memory after the maintenance window. However this never happens, the SQL Server will remain at bottom memory until we force a reboot. 

The problem: Our staging environment does not have spanned disks, it has one single volume (1.99TB, as the staging is only 1TB large). I can't replicate the move with anything of equal size, without spending a lot of time and resources recreating the current setup. I'm looking for either any kind of documentation that shows that moving a spanned disk from one VM to another will correctly recognize them as being a foreign disk group. So far I've only found this article. Or better yet, a cleaner solution for moving the databases across servers. Would it be safe to replicate this without the 3TB disks, simply with 250GBx3 disks in a spanned volume? Or can I expect additional issues with the larger disks that simply can't be tested without increasing the size over 2TB (the single volume max size).