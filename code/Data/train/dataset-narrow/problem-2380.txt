Khot proved that no PTAS exists under plausible complexity assumptions; there is an $O(n^{1/4+\epsilon})$ approximation to the problem by Bhaskara, Charikar, Chlamtac, Feige and Vijayaraghavan; a polynomial integrality gaps result by Bhaskara, Charikar, Guruswami, Vijayaraghavan and Zhou. Therefore I believe your question is hard to approximate in the most general case. 

Intuitively operations that preserve connectivity will not decrease the eigenvalues. For example, adding edges to the graph does not decrease the connectivity. In general, if H is a subgraph of a graph G, by interlacing we know that the i-th largest Laplacian eigenvalue of H is no larger than the i-th largest Laplacian eigenvalue of G. A proof can be found in Proposition 3.2.1 of the book "Spectra of graphs" by Brouwer and Haemers. Note that the definition of Laplacian used in the book is not normalized; it has node degrees on the diagonal and -1 (or 0 if there is no edge) in the off-diagonal entries. 

Here's another interesting example, raised in induced subgraph detection: A theta is a graph with non-adjacent vertices $x,y$, three paths $P_1, P_2, P_3$ from $x$ to $y$, where any two paths induced a cycle with length greater than 3. A pyramid is a graph with a vertex $x$, a triangle $y_1,y_2,y_3$, and paths $P_i$ from $x$ to $y_i$ for each $i=1,2,3$, with at most one path with length one. Finally, a prism is a graph with two triangle $x_1,x_2,x_3$ and $y_1,y_2,y_3$, and paths $P_i$ from $x_i$ to $y_i$ for each $i=1,2,3$. It is easy to describe in figures: 

I wonder if there are constant degree expanders that, after removing an appropriate number of edges, can be decomposed into two disjoint union of spanning trees. 

The resource bounded measure theory developed by Jack Lutz is a great area for people who have background in analysis to work on. The original paper 

It seems that the problem can be reduced to the matrix multiplication problem on the adjacency matrix $A$ of the graph, by replacing multiplication with EQU (that is, NXOR) and addition with AND. So if there is a pair of twins in the graph, then the resulting matrix $AA^T$ will not be the identity matrix, and the indices $(i,j)$ where the value $a_{i,j}$ is not zero are exactly the twin pair nodes. To my best knowledge the matrix multiplication problem can be solved in $O(n^\alpha)$ time with $\alpha \approx 2.376$ by the Coppersmithâ€“Winograd algorithm. If practical solutions are needed, any matrix multiplication algorithms works well in practice are nice. 

First we prove the theorem is false for all k > 1, K > 1, and any n satisfies ${n \choose k}$ > K > $({n-1 \atop k})$. In order to construct a counterexample, for any large N and A = [N], we have to construct a coloring function f such that for all n-subset A' of A, if B' consists of all k-subsets of A', some of the K-subsets of B' have different colors. Here we have the following observation: 

This is known as the rectilinear crossing number $\overline{\mathsf{cr}}(G)$, which is the minimum number of crossings among all possible straight-line drawings of the graph $G$. Compare to the normal crossing number $\mathsf{cr}(G)$, one can see that $\overline{\mathsf{cr}}(G) \geq \mathsf{cr}(G)$. And your question is essentially as the same as asking whether $\overline{\mathsf{cr}}(G) = \mathsf{cr}(G)$ if $\mathsf{cr}(G) \leq k$ for some constant $k$. In the paper Bounds for rectilinear crossing numbers, Bienstock and Dean proved that 

It's good to see a fellow undergrad in pursue of this great problem, with such an enthusiasm. Allow me to offer you a piece of advice from my own experiences. $ P \neq NP $ is a very interesting problem. The implications of the answer are immense, especially in the case that the two classes are equal. The reward is great in many levels, from the altruistic scientific one to the materialistic money award. That leads many young people that encounter the problem in trying to solve it, with no or limited knowledge about it. Perhaps most theory students go through that phase. You will have an idea and think it is right, but it is almost certain that you are wrong. Some people never get through that phase and embarrass themselves by being too stubborn to admit their errors. In FOCS 2010, Rahul Santhanam compared the $ P \neq NP $ question to a mythical monster. It would take many sacrifices and courage to even try to defeat this monster. After all, it may be the most difficult problem ever. To have a fighting chance, you will have to study a lot about this problem and complexity in general. You'll never know what the "monster's weakness" will be. So my advice is this: Take your time in knowing the problem. Every time you figure out a solution, assume you are wrong somehow and try to find the problem with it. That way you'll learn much. As for references, I would recommend Sipser's book as well. After finishing it, I would recommend "Computational Complexity:A modern approach" by Arora and Barak, a more complexity-oriented book, that requires a good understanding of the concept of computation. 

Although your reasoning is flawed, I believe it is educational to see why. First of all, I have a rule of thumb when I am trying to prove something that seems extraordinary or difficult: "Assume that your approach is wrong and try to find out why". Arguing against yourself is not an easy task, but it is very important. One technique I use very often is trying to apply the same argument to different situations. In other words, I'm trying to find a counterexample. Assume that your reasoning is perfect. Then, apart from computability, you are disproving a good part of mathematic knowledge, which is based on contradiction proofs. For example, the statement that there is no natural number such that no other natural number is greater than it, uses contradiction. This result is much more intuitive than the undecidability of halting. Perhaps the notion of contradiction could be better understood if you think of every person trying to prove a statement as a little God: Assume that you have a theory, i.e. a number of axioms and theorems. Assume also that you have a universe that instead of physical laws, obeys only this theory. Now , if you wanted to prove that statement S is false,you could work in the following way (an analogy to the physical world and a statement such as the conservation of energy is highly educational): 

Distance preserver is also known as an emulator; many related work can be found on internet by searching the term spanner, which requires H to be a subgraph of G. But in my applications we can use other graphs as well, as long as H preserves the distances between T in G. 

I'm interested in but not familiar with this topic. Searching for "Average case complexity theory", I found a thesis written by Tomoyuki Yamakami: 

The pathwidth of 3D-grids has been studied by Ryohei Suda, Yota Otachi and Koichi Yamazaki in the paper Pathwidth of 3-dimensional grids, IEICE Tech. Report, 2009. It is claimed in the abstract of the paper that 

Dyck languages $\mathsf{Dyck}(k)$ is defined by the following grammar $$ S \rightarrow SS \,|\, (_1 S )_1 \,|\, \ldots \,|\, (_k S )_k \,|\, \epsilon $$ over the set of symbols $\{(_1,\ldots,(_k,)_1,\ldots,)_k\}$. Intuitively Dyck languages are the languages of balanced parentheses of $k$ different kind. For example, $(\,[\,]\,)\,(\,)$ is in $\mathsf{Dyck}(2)$ but $(\,[\,)\,]$ is not. In the paper 

This is called the edge-contraction heuristic, which an upper bound $(k-1)/(k+1)$ on the approximation ratio can be shown. See section 3 of the work by Kahruman et al. for reference. Imagine whenever we combine two nodes u, v into a group, instead of forming a group we contract the edge (u, v) between these two nodes, and updates the weight of all the edges incident to the new formed node. Repeat the procedure until there are only k nodes left. Lemma 3.1 in the above paper shows that the sum of weights of the first i edges being contracted (denoted as $W_i$) satisfies the following inequality: $$W_i \leq \frac{2iW}{(n-1)(n-i-1)} \text{,}$$ where $W$ is the sum of weights of all edges. This can be derived from $$W_{i+1} \leq W_i + \frac{W-W_i}{({n-i\atop 2})} \text{,}$$ since we always contract the lightest pair, the weight of the contracted edge cannot surpass the average weight. One can see that the cut formed by this algorithm has weight $W_C = W-W_{n-k}$, and by observing that $W \geq W^*$ where $W^*$ is the weight of the optimum cut, we have our desired approximation ratio: $$W_C \geq W-\frac{2(n-k)W}{(n-1)(k-1)} \geq \frac{k-1}{k+1}W^* \text{.}$$ For k=2, the algorithm gives a 1/3-approximation to the Max cut problem. 

$\mathsf{UL}$, the unambiguous logspace, is the class consists of problems that can be solved by an $\mathsf{NL}$-machine with additional constrain that there are at most one accepting computation path. KNOWN: 

A stronger form which covers a collection of $\mathcal{F_1}, \ldots, \mathcal{F_m}$ that is used in practical situations can be found in the survey. There are also some variants that may reduced the use of random bits, or derandomized the lemma to get a deterministic weight assigning algorithm when the size of $\mathcal{F}$ is not too large. Problem After a search of literature, it seems that all the research on this topic were all focus on isolating a unique subset of $U$. I want to know whether there is any obvious reason that we don't have a weak isolation lemma, in the sense that the number of minimum weighted subsets is bounded by a particular bound, and the requirement for the weak lemma is less than the original one. For example, do we have a lemma that only isolates $O(\log n)$ minimum weighted subsets? How about linear or polynomial? Since the size of $\mathcal{F}$ is at most $2^n$, when the bound is set to $2^n$, the result becomes trivial since no isolation is needed. 

I don't know about the most widely used, but I believe I know of the oldest usage (for computer science anyway). In the 1965 paper by Hartmanis & Stearns "On the computational complexity of algorithms", Corollary 2.1 is: 

To increase confidence to the intuition that $\mathbb{PH}$ does not collapse, by showing that it implies results that we believe to be true (or equivalently by contrapositive, that unlikely results imply a collapse). To establish a web of results that are true, if one accepts $\mathbb{PH}$ does not collapse, without the need to wait for a proof of that result. i.e. to establish conditional results. 

In the general case, it is impossible to create an algorithm that confirms whether an algorithm is equivalent to a specification. This is an informal proof: Almost all programming languages are Turing-complete. Therefore, any language decided by a TM can be also be decided by a program written in this language. The problem of determining whether two TMs accept the same language, known as $Equivalence/TM$ is undecidable. As a consequence of Turing completeness, the same holds for the programs of the given language. In other words, it is undecidable to know whether the inputs you want your program to accept and the inputs it really does are the same. Furthermore, $Equivalence/TM$ is not even recursive enumerable. That is because $Non-emptiness/TM$ (whether a TM accepts at least one input) is an acceptable language, since you can iterate over all possible inputs. If the TM is non-empty, eventually you will find a word that is accepted. Therefore $Emptiness/TM$ is unacceptable, otherwise $Emptiness/TM$ would be decidable (which we know it is not). However Emptiness/TM can be reduced to $Equivalence/TM$, therefore $Equivalence/TM$ is also unacceptable. Therefore you can use an algorithm whether or not two machines are not equivalent, but you cannot be sure if they are equivalent, or you haven't given your algorithm enough time. However, this is only for the general case. It is possible that you can decide whether or not the specifications are equivalent to the program, by solving a more relaxed version of the problem. For example, you might examine only a number of inputs or say that the two programs are equivalent with some uncertainty. This is what software testing is all about. As for the rest of your questions: Note: This part has been edited for clarification. It turns out that I did the mistake I was trying to avoid, sorry. Let $TrueR$ be the collection of languages that we know to be decidable. Of course $TrueR \subseteq R$. One can then prove the following: $ProvableR = TrueR$. It is easy to see that $ProvableR \subset TrueR$. However, it is also $TrueR \subset ProvableR$ . The proof is very easy: Let $A \epsilon TrueR$ . By definition , in every input $A$ returns either YES or NO. Therefore, $A$ halts on every input. It follows that $A \epsilon ProvableR$ . Informally , this can be summarized as: You don't know that a language is decidable until you have proven it to be. So if in a formal system you have the knowledge that a language is decidable, this knowledge can also serve as a proof for that. Therefore, you cannot simultaneously have the knowledge that a language is both decidable and it cannot be proven so, these two statements are mutually exclusive. My final point is that our knowledge is partial and that the only way of studying $R$ is through $ProvableR$. The distinction might make sense in mathematics and we can acknowledge it, but we possess no tools to study it. Since $ProvableR \subset R$ , we are bound to never completely know $R$ in its entirety. @Kaveh summarizes it best: Provable always means provable in some system/theory and does not coincide with truth in general. The same holds for any other complexity class: To determine membership you need a proof first. This is why I believe that your second question is too general, since it contains not only complexity theory, but also computation theory as well, depending on the property you want the language to have.