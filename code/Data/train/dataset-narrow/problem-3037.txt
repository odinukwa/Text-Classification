I have my own project that works similarly (but much simpler model), and it takes me about 0.1s to run my predictions in real time. You did the right thing by re-using the session -- that's what I did too. My guess is that your bottleneck is the size of the model. As far as I'm aware, the Inception model is huge. There will always be a tradeoff between model complexity and runtime prediction speed. The Inception model is accurate, but no one ever said it was fast. 

This sounds like a pretty standard supervised learning problem. In this case, your records would be businesses on site X and their actual category on site Z. Your predictors would be tags/categories for a particular business on site X, and your target variable, y (i.e., what you're trying to predict), would be the category on the other website. As far as the code goes, you have a variety of options depending on your preferred language. You could use the caret package in R, the scikit-learn library in Python, or the Weka library (maybe even Spark's ML lib because of its simplicity) in Java/Scala. Side note, in your question I think you meant to say "logistic regression" instead of "logical regression". You don't need to use logistic regression (although it wouldn't hurt). You could also try algorithms like Random Forests or Naive Bayes. Also worth noting: your target variable will have many classes (ie every possible category for the site you're trying to predict), so don't get alarmed if it seems like there are a lot of classes. That's normal for a problem like the one you've described. 

Random forests don't suffer from correlated variables like linear regression models do. Random forests randomly pick from a subset of variables at each split (hence the "random" in "random forests"). This means that correlated variables are less likely to show up together when the trees are being trained. But even when correlated variables show up in the same random subset of variables, it's still not much of an issue because the variables aren't assigned coefficients. Correlated variables are mostly an issue for linear models that try to hold all other variables constant when calculating coefficients during training. The variable selection process is much simpler for trees and tree-based algorithms like random forests and gradient boosting. When a random forest is being trained and a tree's split is being evaluated, the algorithm will simply pick whichever feature most reduces error on that particular split of the tree. Once a variable is picked, there is no coefficient, just a greater-than/less-than split point, so the problem of "exploding coefficients" doesn't apply. 

Please note: I know that there are a lot of other questions on here about what to study to become a good data scientist, but this question is subtly different. I personally disagree that knowledge of traditional CS algorithms is relevant for most day-to-day data science work and that knowing these things makes you a better data scientist. Most machine learning algorithms (gradient boosting, random forests, linear models, SVMs, neural nets, etc) are available in easy-to-use libraries like caret (R); scikit-learn, and TensorFlow (Python); or H2O, and MLlib (Scala/Java). There are also easy-to-use tools, like Spark, to make these algorithms scale. I feel that understanding how the learning algorithms (like gradient descent) work is relevant, but I don't agree that recursion or dynamic programming is relevant. Am I wrong? Should I be more open minded? 

Yes, you're correct -- it's that C and C++ are harder to use and are more burdened with boilerplate code that obfuscates your model building logic. When you build models, you have to iterate rapidly and frequently, often throwing away a lot of your code. Having to write boilerplate code each time substantially slows you down over the long run. Using R's caret package or Python's scikit-learn library, I can train a model in just 5-10 lines of code. Ecosystem also plays a big role. For example, Ruby is easy to use, but the community has never really seen a need for machine learning libraries to the extent that Python's community has. R is more widely used than Python (for stats and machine learning only) because of the strength of its ecosystem and its long history catering to that need. It's worth pointing out that most of these R and Python libraries are written in low-level languages like C or Fortran for their speed. For example, I believe Google's TensorFlow is built with C, but to make things easier for end users, its API is in Python. 

If your model makes a prediction 6 months into the future, then it doesn't make sense to judge its performance before 6 months. If only 2 months have passed, then possibly 2/3 of the true positives have yet to reveal their true nature and you are arriving a premature conclusion. To test this theory, I would train a new model to predict 2 months out and use that to get an approximation of live accuracy while your wait 4 more months for the first model. Of course, there could be other problems, but this is what I would try first. 

Deep learning (and in particular deep convoluted neural networks) is very popular right now because of recent, substantial improvements in various machine vision / object detection challenges. Deep convents in particular have been used to reduce ImageNet classification errors from 28% in 2010 to 7% in 2014. Deep learning is certainly overhyped but that shouldn't detract from the fact that there have been meaningful improvements in the field of machine vision. 

For each tree you randomly select from the variables that you can use to split tree nodes. Generally you randomly select 1/3 of the variables per tree. 

I've been building web scrapers for over 5 years, and I have to say that a web scraper is rarely "relatively straightforward" simply because of how idiosyncratic each website is. It usually takes a minimum of 10 hours per site to code your scraper if you know what you're doing. Whenever you have to interact with the page that you're scraping, I recommend Selenium. It's open source and works with most major languages, including Python, Java, and Scala. Moving a mouse around is possible, but I think in your case it might be easier to directly call the javascript that is triggered by the mouse movements. Your web scraper would iterate over all of the hoverable html elements and call the on-hover javascript on each element. However, the devil is in the details, and you're going to need to post a lot of questions on Stackoverflow and do a lot of Googling before you get to a final solution. I've heard that Google has an API for their maps capability. That might be a lot easier. 

If you're new to both machine learning and programming, try taking a look at this guy's tutorials: $URL$ I've been using them for a while, and I think they're great because of how visual, hands-on, and practical they are. For example, he has facial recognition and handwriting recognition lessons. I'm using what I learned in his tutorials to build a self driving remote control car (similar to this: $URL$ 

No, it's not problematic. Most data scientists do not need or use deep learning. Deep learning is very popular right now, but that does not mean it's widely used. Deep learning can lead to substantial overfitting on small to medium datasets (I'm arbitrarily going to say that means less than 2 GB), which are the sizes that most people have. Deep learning is primarily used for object recognition in images, or text/speech models. If you're not doing either of these two things, you probably don't need to use DL. 

Background: I'm working on a binary classifier that tries to predict when -- if ever -- a user goes bad, a terminal state from which a user cannot recover. This phenomenon is tricky becuase a user might start off good, go bad suddenly and only get caught after a long time. Currently I've defined my target variable at the user-month level, so for each user there is one training record for every month that the user existed. Question: Should I only include a user's first bad month, all bad months, or something else? Additional Context: The problem I'm facing is that some users go bad and remain undetected for many, many months. If I include all of a user's bad months, a small percentage of the users dominate and the model focuses on them at the expense of other bad users. When I include all bad months, I have one feature in particular that performs extremely well (causes the model to have a high AUC value). When I include only a user's first bad month, this same feature has little to no importance and many of the dominant bads go undetected. I know that today there are some users that exhibit the same signs of the dominant bads; and I suspect that they are in fact bad. If I train my model on only the first bad month and deploy it into production, I'm worried my model will likely miss the obvious. What's the best way to tackle this problem? I'm also open to using something other than client-months, e.g., "first bad occurrence in the next 6 months". 

I recommend using a 3D convoluted neural network. Traditional 2D convoluted neural networks excel at identifying objects in images, and 3D convolution is used for identifying objects or movements also across multiple frames (i.e., time). Tensorflow offers this capability. 

I would try experimenting with recurrent neural networks: $URL$ Recurrent neural networks can output sequences of variable length given inputs of variable length. In your case a recurrent neural network might output a sequence like the following when given a user interface: click a button, select a field, type some text, hit enter. For another interface, the network might output only: click one button, click another button, and that's it. This would be useful for you because the sequence and length of actions from interface to interface might change a lot. You could also experiment with reinforcement learning and build an algorithm that has an objective (reach some final page in as few actions as possible). The algorithm would start by doing random things (like clicking the same button a bunch of times), and then gradually learn over time to take appropriate actions. If you go that route you could use deep learning and Monte Carlo Tree Search (MCTS) like what Alpha Go did. In either case you're going to need a framework that can train an algorithm quickly because you're likely to have to go through a lot of iterations. TensorFlow ($URL$ is one option (I've started using it recently, and I like it a lot because of its easy of use). TensorFlow is capable of building both recurrent neural nets and deep neural nets. 

There really is no wrong answer here, but I recommend predicting flight cancellations (#22) and/or delays (25-29), since this is how I often see this data set being used. It could also have practical significance to you if you should ever find yourself flying to or departing from one of the worst offending airports/airlines. I'm not sure if you have a choice (perhaps your employer requires it), but don't use Map Reduce -- it's incredibly difficult to learn/maintain, it's slow, and on top of that it has become obsolete. Use something like Spark's ML lib ($URL$ It's much easier to use and is much more current. 

The accuracy should be the same. Yes, the data is distributed, and yes on each node there is only a subset of the data, but that does not mean that the learning algorithm (for example gradient descent for linear regression) happens on only one node for only one of the subset/local copies of the data. One common technique is to perform gradient descent on each node's local subset of the data and then pass the updates back to a master node. The master aggregates incoming updates from across its workers and then sends out the updated weights to workers for continued gradient descent. When repeated numerous times, the error aggregated across all nodes eventually reaches a global minimum. 

Summary: Go with Scala. Most data science work is prototyping, and Scala will help you work through prototypes faster. Spark ML is probably good enough for your needs, and Scala is much better for Spark than Java. 

Jump right in by joining a Kaggle competition: $URL$ You'll learn a lot by doing, and the most valuable skills are those that you can apply. The theory will come naturally from experience. Also, if you do well enough in Kaggle, you can win some serious money. If not Kaggle, I recommend finding some personal project that interests you and will keep you motivated enough to get through the tough parts. For example, I'm building a self driving remote control car so that I can learn more about convoluted neural nets. The car is cool, but the neural nets are difficult. In any case, it will probably take you 3-4 years to be truly comfortable with machine learning. The 6 month estimate you've provided sounds good for one project though. 

With a few exceptions, you can pretty much use any machine learning algorithm for your model. The beauty of most machine learning packages is that the interface for each model is mostly the same (although the tuning parameters will differ), and it takes just a few lines of code to try out each model. There is no reason you should artificially constrain yourself to trying certain models. Some exceptions to this rule are algorithms that might only work for classification or only work for regression. It sounds like you're trying to predict a continuous target variable that you'll then use for ranking. If that's the case, then you won't be able to use an algorithm called Naive Bayes because it can only output probabilities. In other rare cases like Deep Learning models, the run time can be very long (hours or days) and in those cases you wouldn't want to use an algorithm like that unless you had a good reason to do so (e.g., face recognition in images). You should be able to use nearly every algorithm in MLlib though: gradient boosting, random forests, etc. 

Year-over-year earnings growth (relevant) Percent chance of rain (irrelevant) Humidity (irrelevant) Temperature (irrelevant) 

However, I suspect that what you're really asking is how to get more recurring customers, which is a much more difficult and open ended question to answer. One thing you could do is build an explanatory linear regression model to see which types of product categories are most associated with your frequent customers and then focus your efforts on optimizing customers' experience with those products. In any case, I don't think a predictive model will help you here. From the raw data it should be clear who your most frequent/recurring customers are. Edit: Your comment below clarified your question. Split your data into two parts: a training dataset and a validation data set. Create a binary dependent variable that corresponds to whether someone has made a transaction 12 months after some pre-specified date. Feed multiple combinations of predictor variables into a gradient boosting model (GBM). Gradient boosting will capture interactions between your variables and will enable you to get a quick proof of concept working because most gradient boosting implementations can handle missing variables, so minimal data cleaning will be necessary. I noticed in your profile that you use R. R has a great predictive modeling package called "caret": $URL$