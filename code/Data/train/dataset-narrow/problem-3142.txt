That totally depends on what you're trying to answer. The smaller granularity will be more sensitive to noise and other anomalies. The lager granularity will be able to answer questions more confidently, but loose some of it's usefulness. For example, if you're trying to see when people start looking up venues to weekend plans to know when to launch marketing campaigns for a new night club, you'll want to be looking at daily data, if not smaller. If you're looking at the general trending of night clubs to figure out who you want to invest in, then monthly would probably be better. 

Data becomes "big" when a single commodity computer can no longer handle the amount of data you have. It denotes the point at which you need to start thinking about building supercomputers or using clusters to process your data. 

Short Version The difference is in the trade-off between complexity of model and how hard it will be to train. The more states your variable can take, the much more data you'll need to train it. It sounds like you're reducing the feature space through some front-end processing. Example Context: Say you're using NBC to model if you go outside given the high temperature that day. Let's say temperature is given to you as an int in Celsius which just happened to range between 0 and 40 degrees. Over one year of running the experiment - recording the temperature and if you went outside - you'll have 365 data points. Internal Representation: The internal structure the NBC uses will be a matrix with 82 values (41 values for your one input variable * 2 because you have two states in your output class). That means each bin will have an average of samples. In practice, you'll probably see a few bins with lots more samples than this and a many bins with 0 samples. A Pitfall Say you saw 8 cases at a temperature of 5 C (all of which you stayed inside) and nothing at a temp of 3 or 4 C. If, after the model is built, you 'ask it' what class a temp of 4 C should be in. It won't know. Intuitively, we'd say "stay inside", but the model won't. One way to fix this is to bin the classes 0,1,2,... into larger groups of temperatures (i.e., class 0 for temp 0-3, class 1 for temp 4-7, etc). The extreme case of this would be two temperature states "high" and "low". The actual cut-off should depend more on the data observed, but one such scheme is converting temperatures within to "low" and is "high". Discussion This front end processing seems to be what you're talking about as the "wrapper" around the NBC. The result of our "high" and "low" binning scheme will result in a much smaller model (2 input states and 2 output classes gives you a number of classes. This will be way easier to process and will take way less data to confidently train at the expense of complexity. One example of a drawback of such a scheme is: Say the actual pattern is that you love spring and fall weather and only go outside when it's not too hot or not too cold. So your outside adventures are evenly split across the upper part of the "low" class and the lower part of the "high" class giving you a really useless model. If this were the case, a 3-class bin scheme of "high", "medium", and "low" would be best. 

If you want to just mine for seasonal patterns, then look into autocorrelation. If you're looking for a model that can learn seasonal patterns and make forecasts from it, then Holt-Winters is a good start, and ARIMA would be a good thing to follow up with. Here[pdf] is the tutorial that got me off the ground. 

you can follow along with some mooc or series of tutorials to spoon feed you some basic project. It is a start. You can first try to break into being an analyst first. Could be a much shorter path. Best case scenario, analysts can turn directly into data scientist. Worst case scenario, you get stuck not quite living your dream. If you take this route with the intention of eventually becoming a data scientist, be verrry wary of the tools you use. Highly automated analyst tools keep you locked into what the tools can do. And if the tools don't have any statistics or machine learning baked in, then you'll get stuck. The hard way. If you're serious about it, then know you need to learn both overlapping languages - math and programming. Ask around and do your research. Find what programming languages will work best in the niche you expect you'll most like. A part of you "do I really want to be a data scientist?" should includes the types of things you want to do as a data scientist. Out of your answer, pull a few examples of actual projects. Of those projects, pick the easiest and learn the math behind it. Like financial stuff? Perhaps some simple time series prediction on some open financial data. Like marketing? Perhaps start with looking up A/B tests. Yes, this one may include also doing #1, but with the very important extra layer of sincerity around why you're doing it. And a LOT of the job of a data scientist is being able to take a question nobody's asked yet, and figure out how to apply your tools to solve it. Many moocs rob you of that most-necessary layer of first framing the problem before trying to solve it. 

Short Answer: Practice. You already see it's both math and programming, so start practicing your math and your programming and eventually you'll be there. Long Answer: Data science happens at the intersection of math and programming. A lot of people start with some intense math (biotech, physics, engineering, or if you're lucky statistics) and thus have to learn the programming from scratch. A lot of others start with being some professional developer, and then have to learn the math. But as you said, you know neither. So check your motivations. Do you really want to be a data scientist, or are you just caught up in the buzz that still surrounds it? You really have to love it to endure the massive learning curve. And if you love it, figure out why you're only now getting started. Alright. Still on board? If so, you really have three paths as I see it. 

"Sample" as a noun usually refers to a single data point. "Sample" as a verb is the act of extracting a subset of data points from some larger body (reality or a larger dataset). The only way to be less ambiguous is use more specific words than "data" or "sample". Example: Say you collect 1MM data points from four different sensors in the field giving you four sets of 250k data points. Say this data is too big for some demo of a model you're testing or an analysis you're running, so you pick 100k data points evenly split across the four sensors (giving four sets of 25k data points). In this example, you're sampling twice. First, to gather your 1MM data points sampling from reality. Second, you sample again to decrease the size of your data set to something more manageable. 'Data' or 'sample' could refer to reality, the 1MM dataset, the 100k dataset, or any of the sensor-specific subsets. To make it less ambiguous, establish a unique name as soon as possible for each possible definition you'll be working with. ('reality' for the set of all possibly observed samples. 'the complete dataset', something derived from the source of the dataset, or even X for the full 1MM dataset. 'our trial dataset', or even Y for the small 100k dataset. What you actually do comes down to context and what's appropriate for your intended audience, but the general answer is to use more specific words. 

You can't. In physics, a comparable idea is the Nyquist frequency. The general idea is that you can't add more information than what you already have present in your data without bringing in more data. Given only the day someone ran a query, how can you tell what time of day that query was ran? You may be able to make some inferences, but the only way to answer the question is to directly or indirectly bring in more information to the system. There are things you can do to make informed guesses at the daily state of monthly variables (as gchaks mentioned, interpolation), but your data is still fundamentally monthly data stretched to look daily. 

Think of it in terms of navigating a landscape. The land you move across is created by your error function (that is, the realtion between your model and your data), and the way you move across the landscape is your training function. Especially in Neural Networks, falling in 'local minima' is a big problem. In fact, Neural Networks have been theoretially able to approximate any function for... 20 years? See Universal Approximation Theorem. But the problem has always been learning how to train a given network. More specifically, in your case comaring MAE and MSE, the difference is in the 'square' part. The steepness of the 'fitness landscape' will be much steeper where error is greater with MSE compared to MAE. 1*1 = 1. i.e., squaring does nothing at unit-error. 10 * 10 = 100. Absolute error of 10 will become a squared error of 100. That is looking at one sample, and has huge implications around falling in local minima where the error is great. Looking across the whole dataset, another huge implication is how the NN balances the error it sees. With MSE, samples with high error become samples with very high error. Thus, a NN trained with MSE will 'care more' about learning samples that are very wrong at the expense of small improvements to 'easier' samples. 

As Neil said in the comments, split out a test set (data you don't use in training either model) and see how each trained model performs on the test set. 

Aggregation. For example, you have the number of time people searched for 'widgets' every day. Add up the daily totals for a month to get monthly totals. I would need to see more specifics about the actual data collected at each granularity to give you a more complete version. 

Top level: The rule is to chose the most simple network that can perform satisfactorily. See this publication and it's pdf. The Methodology: So do your proposed test (training many networks at each number of hidden nodes) and plot the results. At the minimum number of nodes, you'll see the worst performance. As you increase the number of nodes, you'll see an increase in performance (reduction of error). At some point N, you'll see the performance seems to hit an upper limit and increasing nodes beyond this will stop giving significant performance gains. Further increases may start to hurt performance a little as training gets more difficult). That point N is the number of nodes you want. How it worked for me: The first time I used this methodology, it created a beautiful almost-sigmoid-looking function with a very clear number of nodes that were needed to achieve good results. I hope this works for you as well as it worked for me. 

It is possible that the other variables you're feeding into the NN are simply bad at predicting sales. Sell prediction is a notoriously hard problem. Specifically the addressing of mapping a multi-state categorical variable to the NN's {0,1} input range: Another idea is to change that one, 5-state variable into five boolean variables. Rather than {0,0.25,0.5,0.75,1.0} on your one variable, make each of the five boolean variables represent a single day and make [1,0,0,0,0] equal Monday, [0,1,0,0,0] equal Tuesday, etc. I've personally had more success both with training good networks and introspecting the network itself when spreading out states of classes like that. Other hacks you can try: * Take out the the 'day' column all together and see if any of the other variables get used. * Plot the distribution of spend as a function of day. Even if nothing else comes of this current model, it sounds like you've found one interesting insight already. * Consider also trying different models. 

This is the fundamental challenge to all data modeling. We don't just want to memorize the the link between a given input and a given output (otherwise you wouldn't be modeling data, you'd be memorizing 1:1 connections with a dict / hash / relational database table / etc). We want to capture the underlying pattern in the data from only looking at the training data. Let's expand a little on your gravity example. You have your 10 training samples showing the start and ending position of an object dropped. For consistency, let's say the object was dropped the moment the object's location was initially recorded and the ending location was recorded at some precise time interval later (but before the object hit the ground). Let's also say the model (neural network in this case) managed to precisely learn the expected change in location since it just comes down to subtraction in one axis. You can show it another 10, 100, 1000 examples that all leverage the connection found and your model will keep performing well. Why not keep going to 10k, 100k, or even more samples? Theoretically, if you managed to isolate the connection and run the experiment the same way each time, your model will always work. But realistically, something is going to eventually change in the system. You hire a new lab assistant who tends to press the 'record location' button well after having dropped the object (giving the object more initial velocity, which you won't notice having only recorded location). Maybe you lost your initial ball and had to use something else which is lighter and catches the wind more (so it goes slower). .... the longer you run the experiment, the more small changes will creep into your system. Eventually these changes will alter the connection enough to make your initial model wrong. When modeling data, we want to capture the underlying patterns and acknowledge that the model only matters as long as those underlying patterns stay relevant. It's not really about the number of samples. It's about the connections / the model itself. The number of samples just happens to be one of the better proxies we have - the more samples you use, the more confident you have some underlying pattern. 'Statistical validity' is one stab at solving this, though it's validity is still up for question in the era of big data. There is plenty of work done trying to solve for how to gain confidence in good generalization in neural networks specifically, but it's still very much an open question. For a different example, if you're looking at user behavior, you'll see differences between day and night; weekdays and weekends; summer and winter; year of a person's life; culture a person grew up in... even if you prove you found a pattern in your initial sample, the system will eventually change and it's up to luck whether the connection(s) you found are a part of the system that changed or a part of the system that didn't. 

Regarding the being new to decision trees and wanting to get off the ground, I wrote a tutorial on decision trees that will help. Regarding methods to avoid overfitting: The game for any model is to limit its complexity to what is reasonable given the data you have. Complexity in decision trees is manifested as adding new decision boundaries, so any limit in complexity is a limit in the decision boundaries it can draw. Two common ways to do this is to place constraints on when a new decision can be created (a minimum of data in a leaf, significant increase in information, etc) or more simply to limit the max depth of the tree.