If nothing else, it may inspire you... If I've understood correctly, it will remove redundancy and code... 

I've only seen it once with an bad app that had badly indexed heaps and had heavy ETL.This was rubbish and luckily not mine. Otherwise, there is no reason. If you are getting statistics updates at inappropriate times then it means you are doing incorrect index/stats maintenance or have massive deletes/loads that hit the threshold. With SQL Server 2005+ you can defer the stats update anyway. See "When to Use Synchronous or Asynchronous Statistics Updates" It'd be interesting to see what article they've followed or read about to make this choice... 

The problem with your current model (as shown) is the Pets table: it adds no value currently. Of course, your simplified design now hides information that probably makes it useful... I'll throw in some ideas with assumptions though. Note: This is correct design, whether EF can deal with it or not. That is, you design the model, implement the database, then make your client work with it Option 1: Assuming an owner can have 0..n dogs and/or 0..n pets, then you'd need 3 object tables and 2 link tables. You don't need Pet. 

Note that sp_change_users_login is deprecated. Now, if your SQL Logins already exist, then the names match but the sid values are different. For this you use ALTER USER use the LOGIN option. Finally, the SQL Login passwords can be recreated if you have a backup of the "old" master database. If you restore this as, say, FixLogins then you can use this 

This should give you a list of the objects that are in the WarehouseDatabase databse but not in the NewWarehouseDatabase database. 

You are missing the schema name in your query. It should be [LINKED_SERVER].[DB_NAME].[SCHEMA_NAME].[OBJECT_NAME]. So in your case, [12.34.56.78].TESTDB.[HERE SHOULD BE YOUR MISSING SCHEMA].test_table It's a public IP address. Hopefully it is a firewall's address that routes the traffic to a SQL Server! If it has a DNS name assigned to it, then you could use it. Or you could create a SYNONYM and forget about typing this long name. 

Here is the rule to trigger auto update the stats Statistical maintenance functionality (autostats) in SQL Server: 

MBR cannot handle partitions bigger that 2 TB, thus if your partition is bigger than 2 TB you have to use GPT. There is no performance gain as far as I know. It's all about the capacity! 

There is nothing wrong! This is how DBCC CHECKDB works. You can read Jonathan's post explaining this behavior and a workaround if you're on the Enterprise Edition -- DBCC CHECKDB Execution Memory Grants – Not Quite What You Expect The only strange thing is that usually this happens with servers with larger amount of RAM! Here is also a really good article on Understanding SQL server memory grant 

Check this article -- Reversing Log Shipping! Now what you need to do is to take the log backup with NORECOVERY on the primary and restore this log on the secondary with RECOVERY. This will preserve the log chain. 

Note that only "highest consuming" statements are retained in AWR. So if the query you're interested in isn't the slowest, most executed, most disk access, etc. it may not be in the AWR data. You can get around this by "coloring" sql_ids of interest. This ensures they stay in AWR for as long as your retention period is set. You can do this by running: 

If this returns anything, move the objects to another tablespace if you want to keep them before dropping the tablespace. 

As you're on 11gR2 and suitably licensed (make sure you check!), I would recommend taking a look at the SQL monitor. This gives a detailed breakdown of all SQL statements that took more ~5s to execute in the past 24 hours. Using this, you can see the duration of each step in the execution plan, along with I/O and wait details. This will enable you to see why it's taking so long. The quick SQL executions won't appear by default, but you can add the hint to force them in. You can get a graphical view of the plan using Enterprise Manager or SQL developer. You can also get a text version using SQL. Have a read of this oracle-base article for more details on using the SQL monitor. It's possible that the reason for the differences is because the SQL is flipping between two execution plans. You can spot this by looking in the AWR to see this. To do this you'll need to find the sql_id for the statement(s) that are causing you issues. You can do this with: 

Even thought the KB point to 2000, it's still true up to 2012. Run through this scenario and see for yourself. STEP#1 

Yes, as soon as you pass the threshold of 20% + 500 from the total rows. The auto update will trigger. You can run though this scenario by re-running STEP#1, but then modify STEP#2 by running these queries: 

When I need to do something like this, I just use sys.objects. After you restore the NewWarehouseDatabase databse, create a Linked Server on the instance where you have WarehouseDatabase to point to the instance where you have the NewWarehouseDatabase databse. Then write an EXCEPT query using sys.objects. Something like this: 

Perhaps, if you see a big IO Queue on your SAN, you have a lot of IO bound queries on top of your memory pressure. Check the PLE and Memory Grants Pending counters on this server. If you PLE is very low and you have lots of pending memory grants, your server might benefit from additional memory. Also look if you need to add any indexes (review missing indexes DMV) -- Are you using SQL's Missing Index DMVs? 

We tried to use ODBC for QB few years ago but gave up on this idea because it was painfully slow! So our developers created an extract application using QB API to export the data we need out of the QB to a CSV file which we import into a table on our SQL Server. This process have it's own drawbacks like when a QB client gets updated to a newer version everybody needs to get on the same version otherwise the export application fails. Again, we developed this few years ago and tested with, perhaps, an old ODBC driver. Also have only handful of QB clients that need to be maintained. This has been working for us all these years. Here is a good starting point if you'd like to start developing an application using QB SDK -- QuickBooks Desktop HTH 

Committed transactions are never rolled back. It's how all RDBMS operate, on ACID principles Now, there are some different cases where it may look like this rule has been broken. But it hasn't. Before we look at these cases though, different user sessions do not share a connection. Each user/client has one connection to the SQL and all are isolated from each other. Connection pooling does not affect this. Savepoints You can SAVE a transaction and rollback to this savepoin, That is, you can partially commit/rollback if you use savepoints but I've never seen anyone do this in real life code. I won't expand more. Nested transactions You can nest transactions but they don't really mean anything Simply put, SQL Server does not really have nested transations even if @@TRANCOUNT can be higher than one. 

Because the return values can be large or a decimal number. Double precision will accept a wide range of values Looking at other numeric types, you only have decimal which will have overhead: you don't know the return scale or precision needed beforehand so it would have to wide 

By default, a DB is not ready to use. It stays "restoring" ready to receive log restores. Simply run this to bring the DB on-line 

However, I would consider doing this in my report layer (say in SSRS): do you need all levels of grouping at the same time? 

I haven't tried this myself so don't have any tips. However, an indexed view is the only way to have any index span tables. 

Datafiles are allocated to tablespaces, not users. Dropping the users will remove the tables etc. from the tablespace, but the underlying storage is not affected. Don't delete the dbf files from the filesystem directly, you'll get your database into a mess. To remove them, find which tablespace the files belong to using the following statement: 

This appears to be a straightforward connect by where you're getting the next person based on the prior manager. The only difference being you want to get the top-level (root) person listed for each row it applies to. This can easily be found using the function (docs), which will return you the value of the column listed at the root node in each hierarchy. This gives you a query like: 

Whereas you can have multiple unique keys in a table and the referenced columns may allow null. By declaring a primary key, you are saying "this is the candidate/surrogate key that should be referenced by foreign keys". This is well understood by database developers who are unlikely to apply a foreign key to a unique constraint when a primary key is available. Unique constraints can also be used to enforce rules such as "a customer can only have one default contact number". A primary key can't be used for this purpose, as customers may have multiple non-default numbers, so the constraint can't uniquely identify all rows in the table. 

Oracle is unable to do partition pruning when a function is applied to the partitioned column. From the docs: 

It depends on the session's execution plan. If a session executes a query with a serial plan then yes, you can say 1600 queries, but if it runs a query with a parallel plan, then a session can have multiple worker threads per session. Tasks, Workers, Threads, Scheduler, Sessions, Connections, Requests – what does it all mean? 

Using RCSI on the subscriber(s) is a common way to design this type of a scenario to avoid blocking. RCSI will allow readers and writes to play nice together but won't solve writers blocking writers. Since the report queries are readers and the transactional replication is a writer, this feature is a good fit for this. You just need to make sure that your TempDB is configured to support your workload for the versioning. Also remember that enabling RCSI adds 14 bytes to each inserted/updated row which might cause internal fragmentation. 

No update yet because the threshold is 24,796.4 - 20247 = 4549.4 but we inserted only 4548 rows for ID 8. Now insert this one row and double check the histogram: 

Now the histogram show the missing ID 7 and the execution plans show the right estimates as well. Query #1: 

Look at the table and histogram! The actual table has ID = 7 with 20247 rows but the histogram has no idea that you've just inserted the new data because the auto update didn't trigger. According the the formula you need to insert (20247 * 6) * 0.2 + 500 = 24,796.4 rows to trigger an auto update for stats on this table. Thus, if you look at the plans for these queries you see the wrong estimates: 

1: In some cases it may be worth including a column in an index if this means all the columns in your query are in the index. This enables an index only scan, so you don't need to access the table. 2: If you're licensed for Diagnostics and Tuning, you could force the plan to a skip scan with SQL Plan Management ADDEDNDA PS - the docs you've quoted there are from 9i. That's reeeeeeally old. I'd stick with something more recent 

To help you fix this, if the query is static (doesn't change) and you've licensed the tuning pack, I'd advise looking at the SQL tuning advisor. With this you can create an SQL profile locking the query to a good plan. Hopefully the advisor will find the good plan for you automatically, but if not you may have to create it manually. Some links: Using the tuning advisor package: $URL$ Manually creating SQL profiles: $URL$ If you're not licensed for the tuning pack, then I think you're down to restructing your query, adding indexes and fiddling with hints. Finding out which changes will benefit will be a bit of trial-and-error; it's difficult to say exactly what you should do without access to the actual datasets. 

You also have standard functions to extract the various components of the time (hour, minute, etc.): 

Yes, it is possible to perform an SQL injection attack without supplying quotes in the parameter. The way to do this is with an exploit to do with how numbers and/or dates are processed. You can specify at the session level what the format of a date or number is. By manipulating this you can then inject with any character. By default in the UK and US, a comma is used to indicate the thousands separator in numbers, and a full stop for the decimal point. You can change these defaults by executing: