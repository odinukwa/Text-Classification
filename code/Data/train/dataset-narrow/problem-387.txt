I'm having issues with Oracle 12.1 rman - specifically the MAXPIECESIZE parameter not being honored by my Level 0 backups. This value is set in rman as 

I have an Oracle Reports server 10g (10.1.0.4.2) that for the last few years has been running okay with the rwclient.sh client application residing on the same host. Due to some performance issues caused by long running/heavy reports we are looking at trying to separate the reports server from the application server. From memory rwserver.sh should be able to accept connections from a client on the same LAN, but not on the rwservers localhost (but I'm going back many years since I saw that sort of setup and I was just a developer, not an admin, so I could be misremembering it). When I try and submit a report across the network I get: 

but I can't see any options in there that would override the MAXPEICESIZE To be clear - I'm NOT concerned about the location just the file sizes. Is there another setting somewhere that I am missing? Do I need to remove the db_recovery_file_dest parameter maybe? Thanks Backup summary of the latest backup: 

So I have a database (on a SQL Server 2008 R2 SP2 instance) that is 2TB uncompressed, with a .bak file that is 500GB compressed. As I have been toying with the fastest way to restore this database after a series of tests, I've tried a couple of things: 

However I would like to have that integer replaced with a variable that is the RunID of the row updated (which is causing the trigger to fire). How can I do that? 

I'm not sure where else to check, I know I have a lot of moving parts here but I'm getting the feeling I'm just missing a max pool setting somewhere. 

So the first thing I did was to make sure that the instance is accessible via SSMS (it is), and that my CMD prompt is Administrator (it is). Then I checked to make sure the instance has TCP enabled (it does) and allows remote connections (it does). What am I missing? The commend I use, minus the script, is: 

This is in addition to other IT concerns you'd want to keep in mind like maintainability, support contracts, and your space/power requirements. 

On the other database on this server (that is behaving as I think it should), using identical config/scripts the recovery area usage looks like 

If I run a crosscheck archivelog all, this picks up only the recent archivelog files that I expect to be there: 

Due to diskspace concerns, a second disk was added and I tried to move rman to use this disk by changing the channel device eg 

The incremental backups them selves seem to be being created fine and in the correct place, it's just the recovery that is failing. Is there any way to see what/where RMAN is looking for the datafiles during the recover command? Or is there a way to force it to look in the new location? If needed, my RMAN config below: 

I've enabled tracing on the reports server and can see requests on the server coming from the external clients (i think), but I only get one line per report run attempt. (But consistently get this same line so I think the rwclient is at least seeing the rwserver): 

(The java processes above are all for ) I assume the 0.0.0.0/127.0.0.0.1 multiple ports things is why the server can see the initial probe, but the client then fails to connect properly. I suspect there's got to be an option somewhere that determines what interface the server tries to bind to but I can't find any option in the report server .conf to enable this. Questions: 

I am trying to use SQLCMD to remotely detach a database but for some reason it can't find the server. From the workstation, I can connect to the database instance (which is the default instance) using the Host Name through SSMS successfully. The error is: 

After a test completes (on a SQL 2008R2 system), the system updates a row in the TestRun table to include an end time. I am creating a trigger that will take action after that update, and do some post-test analysis for me. Right now it's very simple. 

That exception usually doesn't refer to the SQL Server itself running out of memory, but instead the client workstation that is processing the results running out of memory. You will generally see this if you are outputting a very large result set into the Output window. To troubleshoot the issue (and possibly resolve it) allow SQL to write the results to a file rather than trying to put it all in the output window. 

The way to gauge hardware performance is to be take a look at your current workload. So I would begin by quantifying that: 

I've got two databases running on the same 12.1.0.1.0 oracle home on the same machine, on the same disks. Each database has multiple schemas of approximately the same size. On database A, the datapump exports take on average of about 2 minutes. On database B, the exports take on average, about 20 minutes. These exports get run twice a day, and the performance has been consistent for at least as long as I've been paying attention. I've also tried different times of day incase it was related to system load, but that seems to make no difference. The configuration for the databases are largely the same, the only differences I can see are file locations, and the pga limit/target, both of which are big enough for the respective databases, and the log_buffer (which I don't think should impact exports, but maybe?) I've added METRICS=y and LOGTIME=all to the backup scripts - that revealed some interesting timings: Database A (fast) 

Looking at the output of , there seems to be number of ports opened by the reports/java process - most of them open to 0.0.0.0, but a couple are bound to 127.0.0.1: 

I have 2 instances of SQL Server installed on one server: SQL002. One instance is a default instance, SQL2008 R2, and the SQL service is generally not running except for specific tests. The other instance is a named instance, SQL 2014 (so SQL002/SQL2014). This is generally running all the time. Today I just had a developer tell me that when they connect to SQL002 in SQL Server Management Studio it will connect successfully. I replicated this from my machine and also see a successful connection, however when I look at the properties of the connection it is reporting that it connected to the named instance. What is going on here? Is there a redirect I'm not aware of? 

You're right that SQL Server rarely bottlenecks at the CPU, but it's not impossible and you can tell whether you're seeing a CPU bottleneck by looking at some perf counters. Then you'll know whether or not you'll see a performance improvement simply by updating the CPU to a faster one or one with more cores. If your SQL isn't written to take advantage of parallel processing, it won't matter if you throw more cores at it. Additionally, there's more things I would want to consider in buying a system: 

I'm running a 12.1 SE database on Oracle Linux 7. I am getting nightly errors/warnings from my rman scripts about missing datafiles when trying to recover. I have been running rman to a local drive using the following script: 

As mentioned, this is the same oracle home, same server, same disks etc - the two databases should be performing at least roughly the same I think. Any know what could be causing this? Or where I should be looking to see what could be causing the slow down? Thanks 

(Weekly I run a level 0, but the script is other wise unchanged) The backup list looks as I'd expect (had to purge the backups on the 5th due to space constraints), with the tagged level 1s and 0s interspersed with the Full autobackups. 

Then from within the actual exports, for Database B, the times reported by the LOGTIME parameter all give a time of a few seconds per table, but the datapump reported times are all 0 seconds: 

I have two 12.1 databases on the same server that I've set up with rman and have been running okay for the last year or so (with numerous restores to a seperate test server - so at least restore/recovery wise rman is set up okay.). These databases are both around 100G in size, with the recovery area (originally) sized at around 200G. (Currently bumped up to 300+G to give a little more lee-way time whilst I work out what's going wrong). Just recently one of the databases has started filling up the db_recovery_area with backup pieces - normally this has been fairly static increasing over the day and then dropping back to zero when I run the rman level 1 backup over night (or level 0 over the weekend). However, over the last couple of weeks, the recovery space has trended upwards eventually getting to the point where I need to delete all backups/archivelogs out of rman to avoid a production database freezing due to lack of space. There's been no configuration change or upgrades on this server for some time and I can't work why the behaviour has changed - but only for one of the databases. Looking at V$RECOVERY_AREA_USAGE the only thing consuming a substantial amount of space is the backup pieces. 

Currently I run when I want to wipe information out of the buffer pool between running SQL queries. However, I was reviewing this Technet article referencing . What caches does wipe that doesn't? 

I'm gathering wait types on a load test environment, and the second biggest wait type is . The stats are coming from sys.dm_os_wait_stats, but I can't find any documentation that explains what that particular type means. I'm running SQL Server 2014, SP1 on a physical Windows 2008 R2 instance (no VM involved). 

However I can't figure out how exactly I need to escape the quotes. It always wants to treat the as a separate column. How can I escape the comma properly? 

Currently this takes something like 3 minutes to run. Is there a better way to get the information I'm looking for? 

I have a group of about 30 tables, and I want to know the physical size on disk of all of these tables (plus indexes). Is there an easier way of doing this than through the GUI in SQL server 2008 R2? 

Sometimes our developers will write a query that uses cursors, but doesn't close them out explicitly. I'm trying to generate a list of active objects in my production database that use cursors but don't explicitly close/deallocate them. To do this I've written a simple statement that does the job, but is extremely slow: 

(in hindsight, symlinks may have been a better option but at the time I preferred to make the location of the backups on a seperate drive explicit/obvious). This channel change didn't seem to have any effect. Initially I suspected because I have a 7 day retention period set and the old image were still valid, however after 7 days datafile images were still being created in the old location. As diskspace was becoming increasingly tight, I changed the backup script to use a different TAG to try and force a new set of backup images to be create on the new disk. This seemed to work - the new set of backup images were created okay on the new disk that night, but subsequent recovery operations seem to be failing. 

After the same number of level 1 backups, the backup piece usage is sitting at a few meg, instead of almost 100G as is the misbehaving database. Why are the backup pieces on this database sitting in the recovery area eventually filling it up completely? Is this normal behaviour with the other database being the abnormal one? This is not a high traffic database - should I need to have the recovery area set to more than 3 times the size of the database? Thanks 

When I restore after dropping the database, the fastest I can get a restore going is 4 hours. This is when I put the .bak on a RAID 10 logical drive, with the data files being written to a separate RAID 10 logical drive on the same server. The log files go to yet another RAID 0 on the same server. However, if I restore with Replace, the restore process only takes 56 minutes (similar setup). Did I find some sort of turbo button? This is so fast that I am worried. 

I'm running SQL Server 2008 R2 SP1, on a Windows Server 2008 box. I have a .NET script running from Visual Studio 2010 that does the following: 

I found the answer elsewhere on the Internet, courtesy of Grant Fritchey. "When you look at the fragmentation percentage, you're looking at external fragmentation. It means the percentage of pages that are out of order showing an inneficient storage mechanism." $URL$ So it's how many pages out of the whole are out of order. 

The total number of times it will iterate is 150, however it is stopping at 100 connections and I can't figure out why. I could adjust my script to just use a single thread, but I'd prefer to know where I'm missing a max connection setting as that will be more useful to know for future reference. Here's where I've checked so far: 

My understanding from the documentation is that this is normal on the first run (when there is no datafile image copy), and normal on the 2nd run (when there is a datafile copy, but no incrementals), but on the 3rd and subsequent run there should always be a datafile copy and an incremental to apply to it. This has now been failing for the last 6 nights. From $URL$ 

Is it actually even possible to connect to the reports server from external clients? and if so; Where abouts do I need to configure the reports server so it listens for external connections? 

Where are these files likely to have have come/why weren't they eventually expired and deleted like the rest of the archivelogs? Why does crosscheck archivelog all not detect them and add them back? is it because there is a 4 month gap between them and the next most recent files? Is it safe to just delete these files off disk? 

I have tried explicitly setting the CHANNEL DEVICE as part of the backup script (immediately before the BACKUP INCREMENTAL line) - this made no difference. Also maybe related, the location does not seem to match what I am specifying above - vs - this seems to match up with db_recovery_file_dest set in the database: