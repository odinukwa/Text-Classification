The reason NB is called "Naive" is that is makes the assumption that the predictive variables are all independent. This assumption usually skews the model scores (which, under the above naive assumption are unbiased probability estimates) towards 0 or 1. In your case, e.g., the presence of words and indicate category, but, because the presence of these words is not independent (if one is present, the other is likely to be present too), the model will over-value their appearance. Taking the extreme case, if words and appear only together, then 

I would view my points as elements of $\mathbb{R}^{n+1}$ and use PCA to project my points to $\mathbb{R}^3$ and then scale them to $S^2$. 

and thus one should multiply by the odds ratio of once, not twice, as the Naive Bayes algorithm requires. Your remedy is to use calibration (a separate model which maps model scores to probabilities). 

After a half year of working as a data scientist, I found myself tend to clone my previous work (in machine learning, like predictive modeling, EDA, data preprocessing etc.) in the new project and the work becomes a repeated style. I need new directions to lift my work with some innovative pieces. That is a key incentive to my work and also make me feel fresh and confident. Could anyone put forth advice for me? Cheers 

My task is to create a model to predict the defaulted rate for each cohort in a time window like 360-day, i.e. how about the defaulted rates of each cohort after 360 days given its input into the model? I have tried some ARIMA models but the AR order is too large, e.g. AR(100) and the computation take long time to run. I wanna try a machine learning method which allow me to create a dataset not only with the time labels but also some other features which can contribute to the prediction of the defaulted rate. Please advice how can I do that or please recommend some papers/textbooks for reference. I know some concepts about the sequential learning but not to the executable level. Many thanks 

To me, this $-Q(s_t, a_t)$ term at the very end is redundant. ...If we set gamma to $0.8$, the future rewards will decay anyway. Yes, if instead we now set $\gamma=0$, then the $-Q(s_t, a_t)$ term will drag our value down - is there a case when it would be useful and what would the result be? 

Why a second matrix is needed during training: We cannot put softmax directly after hidden layer, because we are interested in summoning a mechanism to convert into an embedding. That's already a responsibility of a first matrix E. Thus, we need an extra step (an extra matrix) that will give us enough room to now form a conclusion at the output layer about a different but similar (context-wise) neighbor-word During the runtime you throw away the second matrix. But don't delete it permanently in case if you need to come back and continue training your model. 

So I just wanna confirm that both methods are valid and then which one is better in random forest application? 

I use Python to run a random forest model on my imbalanced dataset (the target variable was a binary class). When splitting the training and testing dataset, I struggled whether to used stratified sampling (like the code shown) or not. So far, I observed in my project that the stratified case would lead to a higher model performance. But I think if I will use my model to predict the new cases which would highly probably differ in the distribution of target class with my current dataset. So I inclined to loosen this constrain and use the unstratified split. Could anyone can advice to clarify this point? 

I face a data which records the default rate of loans by cohort.e.g. My company currently hold a portfolio comprising personal loans whose starting date was in a range from 2014 Jul till now. The loans were divided into each monthly cohort like loans drawn in 2014 Jul, in 2014 Aug, ... in 2015 Feb,... in 2017 May etc. Therefore each cohort had a fixed loan population but the defaulted amount was increasing as time goes. So we record the defaulted rate (defaulted amount / total loan amount) of each cohort on a temporal axis (daily basis.). I create a sample data which is isomorphic to my real dataset. The first column is the cohort and the other columns are for dates on the time axis. 

I've worked-through the back-propagation for the Phased LSTM (Daniel Neil, Michael Pfeiffer, and Shih-Chii Liu 2016) and would like to show the notes. It was a relatively difficult task, so I post it here, to help if anyone struggles with derivation. I assume you know how to do the back-prop for the usual LSTM, and so will show only the most critical gradient-pieces. Keep in mind the gradient flows in to opposite direction of the arrow. The Phased LSTM uses Peepholes. Also, notice, the authors skip some timesteps by referring to the "most recent update time" as $t_j$ but I am using just $t$ for everything. That's because during training we need to consider every timestep - there is a "leak" in the time gate. However, as the paper advises, the leak is turned off at runtime, and you can then use $t_j$ and skip any 'non-j' timesteps to save power. Note - I still haven't checked it in code, there might be an error in the notes, although I've checked it thoroughly. Please let me know if you spot one. Here is a link regarding partial derivatives through the modulo operator Tools to be used: The quotient rule says: $$\frac{\partial}{\partial{a_i}} \left( \frac{g(a_i)}{h(a_i)} \right) = \frac{\frac{\partial{g(a_i)}}{\partial{a_i}}h(a_i) - g(a_i)\frac{\partial{h(a_i)}}{\partial{a_i}}}{h(a_i)^2}$$ and the the Chain rule says: $$\frac{\partial}{\partial{a_i}} [ g(h(a_i))] = \frac{\partial{g(h(a_i))}}{\partial{h(a_i)}}\frac{\partial{h(a_i)}}{\partial{a_i}}$$ as a bonus,product rule says: $$\frac{\partial}{\partial{a_i}}g(a_i)h(a_i) = \frac{\partial{g(a_i)}}{\partial{a_i}} + \frac{\partial{h(a_i)}}{\partial{a_i}}$$ 

Suppose a person (John Doe) uses different user names (handles) in different circumstances (for simplicity, let us limit the conversation to a single social network, e.g., ). E.g., he 

What you described is the more-or-less standard approach, with a couple of caveats. Leakers Remember that we live in a stochastic world, so a perfect model is immediately suspect of using leakers, i.e., variables which have a causal connection with the target. In your case, e.g., it would be the presence of a service contract for the machine (which would indicated that the company actually does own one). This, building an interpretable model and examining the most powerful variable for such causal connections (and eliminating them) would be a good idea. Better Approach However, one could improve on this approach in a very powerful way by replacing the current state of the company with its historical state at the time of the machine purchase. This way one eliminates the leakers automatically and also gets a better model because the predictor values which are more relevant: after all, you are trying to predict which company is ready to buy the machine now rather that which company bought it in the past when its conditions were very different. In fact, your approach works precisely because it is an approximation of this approach (approximating the company state at the time it bought the machine with its current state) and the absurdity/uselessness of the perfect model that you mention is the artifact of your approach being an approximation of The Right Thing. 

I am quite new to machine learning and python as well. I faced an imbalanced dataset and wanna use cross validation and oversamopling like the figure shown. 

I trained a model with results as below. It is a stacking model with base learners of random forest and gradient boosting. The mega model is a GLM. The dataset is imbalanced in the target class as shown in the confusion matrix on the right top. The target class is a default status of a single loan (Positive: default; Negative: non-default). The AUC ROC score is quite high but the f1 score is still only 0.53. My concern is both of recall and precision were approximately 0.5, meaning that the model can only distinguish half of the bad cases and only half of bad cases it diagnosed were truly bad. If I adjust the probabilistic threshold, recall will increase but precision will also be sacrificed to some extent. Due to the highly imbalanced situation on the positive cases, a low precision may lead to a large proportion of False Negative among the predicted positive cases. e.g. under a lower precision, if a model tells 10 bad cases, there may be actually only 2 truly bad ones and more fake bad ones predicted which is not wanted in practice as well. In predictive task on an imbalanced dataset, does f1 score matter and if so how can I further improve the score? (add new (composite) features, cost sensitive methods?) 

I've been reading about word2vec and it's ability to encode words into vector representations. Coordinates (probabilities) of these words are clustered together with their usual context-neighbor words. For example, if we have 10k unique words in our dataset, we will feed 10 000 features into the network. The network can have for example 300 hidden neurons, each with linear activation function. The output of the net are 10 000 neurons, each with a softmax classifier. Every such a "soft-maxed output" represents probability to select the appropriate word from our 10k dictionary. 

Using full-batch gradient descent, stacking 100 layers and using alpha 0.0001 results in steadily decreasing error. However, after I implemented Batch Norm, the same scenario results in fluctuations. My implementation was verified by several people, so now I am wondering - why batch norm adds this stochastisity effect? In fact, I am no longer able to stack 100+ layers, only approximatelly 10 layer before stochasticity becomes very apparent and hard to control.