I would encourage you to think of error correction not as entanglement-swapping, per se, but the stimulated dissipation of excitations from a carefully engineered Hamiltonian — not the Hamiltonian which governs the unitary evolution, but a different Hamiltonian in whose ground-state manifold one thinks of trying to keep the state of the system, by dissipating excitations into a system of low temperature. Then we can describe fault tolerance as the threshold at which the rate of stimulated dissipation is such that the probability of an uncontrolled spontaneous dissipation can be efficiently suppressed by any desired factor. Against the role of entanglement — per se — in error correction The picture of 'entanglement swapping' which you describe brings to mind the question of coherence, and the issue of the state of the computation being isolated from the environment: in the quantum circuit model, we would quite like (short of what transformations are being applied) for the state of the computation to be essentially closed off from the outside world. This is the classic catch-22 behind the engineering problem of designing a quantum computer: to be open to the environment only when you want it to be (because you are part of the environment), and to control when it is open to the environment (from within the environment). It invites us to think of the problem in terms of a double-bind. Furthermore, not all errors necessarily would manifest as entanglement with the environment: an environment which deterministically performs Pauli operations on each of our qubits in some order would not be entangling and would be just as bad from an error perspective as anything which did involve entanglement. Error correction as a form of Hamiltonian engineering An alternative view is that the error correcting process is a sort of dissipative process. 

Elaborating on Joe's earlier answer: note that $\textrm{FACTORING} \in \mathsf{NP \cap coNP}$. The latter is the second lowest class in the "low" hierarchy: which is to say that $\mathsf{NP^{NP \cap coNP} = NP}$. This implies in particular that $$\mathsf{P^{\textrm{FACTORING}} \subseteq NP^{\textrm{FACTORING}}} \subseteq \mathsf{NP}.$$ We may make similar remarks for $\mathsf{coNP}$ and $\mathsf{BQP}$, to show that at least on a coarse-grained level, $\mathsf P^{\textrm{FACTORING}}$ has the same complexity bounds as the problem $\textrm{FACTORING}$ itself, which is to say $$ \mathsf{P^{\textrm{FACTORING}} \subseteq NP \cap coNP \cap BQP}.$$ 

not to compound the cost of transforming many amplitudes in parallel (so that the cost of performing a single-qubit gate does not depend on whether you consider it to "act" on many qubits when you take the tensor product with the identity), and to be independent of what other gates are used in the circuits (we don't assess the cost of any gate depending on whether the circuit uses more exotic gates as well). 

Let $~{\mathrm{PRESARITH}}$ denote the decision problem of the truth of statements in Presburger Arithmetic. As you note, [Fischer+Rabin 1974] (PS manuscript) show that the nondeterministic time complexity of $\mathrm{PRESARITH}$ is at least $2^{2^{\Omega(n)}}$. In particular, $\mathrm{PRESARITH} \notin \mathsf{NEXP}$. However, we have the containments $$ \mathsf{ BPP \;\subseteq\; BQP \;\subseteq\; PP \;\subseteq\; PSPACE \;\subseteq\; EXP \;\subseteq\; NEXP } $$ Thus, it directly follows that $\mathrm{PRESARITH} \notin \mathsf{BQP}$, and in particular it is not in $\mathsf{BPP}$ either. 

This is also a good thing to do, to compute a more explicit representation of $P'$ from $R'$, once your interest in $P'$ becomes dominated by testing $(i,j) \mathbin{\in?} P'$. This will take time $O(r^2)$, as there is essentially unit cost to reverse each edge $(i,j) \in R$, and to record each of the descendants of any $i \in [1,r]$ throughout the traversal. If $s = |S|$ and you use trees for collections at each index $(i,j)$, constructing the representation for $S$ takes time $O(r^2 s \log s)$, with the $\log s$ factor being saturated in the case where many elements of $S$ share many pairs $(i,j)$ in common. This works best for partial orders which are sparse and essentially unrelated, which would reduce both the expected overlap for pairs $(i,j)$ and the number of entries $(i,j)$ for which any relationship is recorded in the traversal of its reduction. Having this representation of $S$, you can test incompatibility of $P$ with $S$ (where $P$ is given by a reduction $R$) as follows. 

Context. The proposition above is a standard result which allows decision problems about the rank of a matrix A to be reduced to those about the characteristic polynomial of B. A convincing proof of the proposition for the case that R is a finite field can be found in Mulmuley's article A fast parallel algorithm to compute the rank of a matrix over an arbitrary field. Because the degree of the formal indeterminate x can be bounded in det(B − t I), we can reduce this problem to a problem of polynomials over R alone. While Mulmuley's proof breaks down for any ring that has nilpotent elements (such as 2 in the rings ℤ2k ), Allender + Beals + Ogihara claim (in their article The complexity of matrix rank and feasible systems of linear equations) that this proposition (or rather a result which I've paraphrased in the result above) holds generally for any ring, and is shown in the book chapter 

Question. I'm looking into certain algorithms for linear algebra which lie in NC2. Does anyone know of alternative references for the proof of the proposition just below, relating rank of matrices over R to characteristic polynomials over R(x)? 

The problem MIN-3CNF-DELETION you refer to is better known as MAX-3CNF-SAT (or simply MAX-3SAT for short). Presented as a decision problem, it's the problem of determining (for any input value m) whether there exists an assignment which satisfies at least m of the CNF clauses; presented as an optimization problem, it's the problem of determining the maximum number of clauses which any assignment may satisfy. This presentation as an optimization problem is clearly complementary to the problem you describe, and (unlike MIN-3CNF-DELETION), it makes sense to consider the feasibility of approximation versions of the problem with multiplicative error because the most extreme value isn't zero. In fact, MAX-3-SAT cannot be approximated to within a constant factor better than 7/8 (unless P = NP). Furthermore, Karloff and Zwick demonstrated that there does exist an approximation algorithm achieving that upper bound. Edited to add. The business with the approximation factor in Karloff and Zwick's result seems to be somewhat more complicated than it appears. Many papers in the literature describe their result simply as a 7/8-approximation algorithm. However, Zwick himself writes the following in a footnote on page 1 of this paper: 

Barring any advances in derandomization, it seems to me as though the requirement that the Las Vegas Machine makes no mistakes is crucial, so that there is little to no benefit to having randomness at all in this case. For a BPP language $L$ decided by a suitable algorithm $A$, which acts on inputs $x \in \{0,1\}^n$ and a random string $r \in \{0,1\}^{N(n)}$ representing its random choices, the zero-error criterion implies that the Las Vegas machine must ascertain for certain which of the two cases $$\Pr_r(\text{$A$ accepts $(x,r)$}) \geqslant \tfrac{2}{3} \quad\text{or}\quad \Pr_r(\text{$A$ accepts $(x,r)$}) \leqslant \tfrac{1}{3}$$ holds. If we are given no further information about $A$, then this is essentially an oracle promise problem: given an oracle $A'$ computing $A'(r) = A(x,r)$, and given the promise that $A'$ yields one output $a \in \{0,1\}$ for at least twice as many inputs as the opposite output $1-a$, determine which output is more common. Although the Las Vegas Machine may use random techniques, if we are indeed forced to treat $A'$ as an oracle, we can see that the only strategy available to a Las Vegas machine is to take a relatively thorough (though not exhaustive) survey of the random strings $r$, to see what answer is given for each. It can only be sure if it finds more than $2^{N(n)}\!/3$ distinct strings $r$ which all give rise to the same output; otherwise, with small (but non-zero!) probability, it may be unlucky and obtain a non-representative sample of the possible outputs. To obtain zero error, it must sample at least $2^{N(n)}\!/3$ inputs $r$. Because the Las Vegas machine must inspect at least a constant fraction of all of the possible random strings $r$, asymptotically we're no better off than if we deterministically tested all possible random strings. We get no asymptotic advantage in simulating BPP algorithms randomly in a zero-error setting, beyond what we can do deterministically by brute-force. Note that this same argument gives rise to an oracle separation between BPP and ZPP, i.e. there is an oracle $A$ such that $$\mathsf{ZPP}^A \subsetneqq \mathsf{BPP}^A$$ because the ZPP algorithm takes exponential time, while a BPP algorithm can solve the question about the oracle in a single query and succeed with bounded error. However, it doesn't tell you any more than what you suspected already (that the simulation overhead may be worse than polynomial) nor that the asymptotics are just as bad as a naive deterministic simulation. 

[Revised.] I have revised my response based on your revisions to your question, I've retained the content of my original response, but made it shorter. The more elaborate description of the "simulation" process has been replaced, but I suppose that it can be seen by viewing the edit history of this post. Most people will understand "postselection" in the sense of a conditional probablity. Indeed, the current version of the Wikipedia article on PostBQP describes it that way; and viewed as an operation on density operators (in which one applies a completely-positive trace-non-increasing map Φ, such that Φ2 = Φ, and then renormalizes the trace) one recovers this definition. Given this definition of postselection, your defintion of an MPostBQP[k] algorithm can be simulated by a PostBQP algorithm, by deferring the post-selections and performing them simultaneously, in a suitable way. This is noted more-or-less explicitly on page 3 of Aaronson's paper Quantum Computing, Postselection, and Probabilistic Polynomial-Time which introduces the class PostBQP. This can be shown explicitly by noting that, for a sequence of bits P1 ,  P2 ,  ... to be postselected (e.g. in the state, which is usual), there is no difference between conditioning on them being in the middle of the computation and conditioning on them being at the end of the computation, so long as the values of these bits are not changed in the interim. Then, rather than post-selecting on each of them individually being , we can compute their logical AND before post-selection and then postselect on that conjunction being . Furthermore, computing the AND can be performed at any point between the last transformation of the bit and its post-selection. This will in no way affect the joint statistics of any of the properties of the state. Thus, using the common definition of postselection in terms of conditional probabilities, we would have MPostBQP[k] = PostBQP for all k > 0.