So, we instantiate like normal and get a reference to the created game object. Then we check to make sure the result wasn't null for some reason. If that went okay, we can get a reference to our script using GetComponent. Using the script reference, we can call the method with want. Also note in Unity 5 a generic version of instantiate was added. This means the second line can be changed to: 

And that's it. This will build all your bundles tagged in the editor and put each one's dependencies in the bundle with the same name as the output folder. We can specify some options here such as no compression, force rebuilds every time etc. The last parameter we give is the target platform. You should build asset bundles for each platform you release on. Only asset bundles that have changed are rebuilt when calling this method. That's fine, but what if you want to set asset bundles in scripts? You have a couple of options: 

The built-in Unity networking would be fine for this. Sure, someone has to be a "server", but this doesn't have to mean much at all depending on how you program it. Unity does give you complete freedom on how you distribute the work across the players, and how they're all synchronised. Of course the only problem here is the master server. You do need a master server, but it's only purpose is to help find games. I don't see how this would really get in your way, you can host your own master server as well. Really, the Unity networking is what you make it. Photon on the other hand is VERY limiting. 

Loading assets from an asset bundle is done explicitly. You give the bundle the name of the asset to load, then it returns it( either now or later. ) However, loading dependencies is done completely automatically. How is this done? Is the asset bundle completely removed from memory after the required objects are loaded? Are all the bundle's assets loaded, or just the necessary ones? How can these assets be accessed outside of the reference that caused the dependency in the first place? I ask this because I am trying to collect some type information about the assets loaded in asset bundles. I need to be aware of all assets of a certain type that are loaded into the game. 

Yeah, this is how spritebatch is meant to be used. It takes a view matrix in the begin parameter and uses that throughout the begin/end pair. You could argue that spritebatch would be more flexible with matrices if you could pass a view matrix in for each .draw call, but evidently the XNA team decided against this for one reason or another. To answer your question, having two seperate begin/ends won't make THAT much difference overall. Tutorials often make a note of using as little begin/end as possible to make the reader knows that it should not be called once for every draw call or nested somewhere in a class' .draw method. The biggest problem I've found this leading to is when you have a .draw method in another class that relies on a spritebatch being begun. If that is the case, this method can only draw with one view matrix. This can be a bit of a problem, but shouldn't be too hard to get around. Spritebatch .begin changes a lot of the state with the GPU. This is pretty expensive, but two calls per frame is very reasonable and shouldn't be a problem. If your 2D application is lagging on a decent modern computer, you should look for issues elsewhere. 

There is still no change to the Transform inspector. How can I define my own custom editor for these components? 

Where both these use texture coordinate 1, we need to copy texture coordinate 1 so there are two instances to it. We'll end up with: 

This line is passing the target vector into the method to create a rotation. That's probably not right. If you want to rotate about the axis shown in your picture, you'd want to pass it the global up vector, like you do in your LookAt line. The second problem is: 

Sprite holds a reference to a texture. It is not a texture itself. Therefore, if you have 20 sprites using the same texture, it will only load the 1. Textures are resources, and should only be referenced, not copied. $URL$ Read about it here. 

"My goal was to teach them the fundamentals of C++ using OOP", following this, I think you're making the right choice. I find the OpenGL API doesn't lend itself to OOP( in the programming sense, anyway ) very well. I'm surprised you're even exposing 12th grades to C++ and OpenGL. Most university courses I've seen avoid both of these like the plague. So, yes, I would suggest keeping the OpenGL API out of learning focused on OOP. The OpenGL way of creating and binding objects really doesn't go well with RAII or many C++ concepts. That should be a course of it's own. 

As you can see we have 4 vertices at each corner. This makes our quad and we texture map it with the sprite we want. The important thing to note here is that the origin is in the center of the quad. What does this mean? If you've done a bit of 3D programming you'll know that any rotation , scale or translation will be done to the center of the quad. So, if we rotate, we'll rotate about the middle, and if we scale, we'll scale inwards. The vector ( -1, -1 ) scaled by 0.2 will result in the vector ( -0.2, -0.2 ). The quad becomes smaller from all directions. You may have been expecting the opposite: 

I'm trying to get my character to perform a "dash" where their x velocity increases rapidly and then falls over time. However, I'm also maintaining a constant x velocity so the character appears to be running at all times. The problem is trying to figure out when the dash should be considered finished. It can't be finished when the x velocity is equal to the running speed again, because other forces may be acting on the character. It feels like all the information I need to figure this out is gone because it's all been calculated into the velocity vector, and the actual forces and where they come from are thrown out. I've considered the following: 

You can store ALL transformations in one matrix, rather than a matrix and a vector. Also, matrices are associative. This means a translation, then a rotation, is not the same as a rotation and then a translation. The translation could be affected by the previous rotations, or scales. This could be useful. Also, when multiplying a 4*4 matrix with a 4 component vector, we could make the forth component of the vector 0, meaning it'll ignore the translation. A vector with a forth component of 0 means it's normalised. We don't want a normalised vector to me translated. Yes, you could do all of this without matrices. It's just simpler and faster this way. You especially wouldn't want to manipulate a game with half matrices and half vectors. 

In Unity shaders, #include, #define, and other preprocessor commands seem to not be able to be used outside the CGPROGRAM section of the shader. Since these are required for compiling multiple shader variants, and a large portion of the shader is non CG, this would be incredibly useful to reduce shader duplication. Do these commands exist outside CG? 

However, The actor that is handling this event is not involved in the overlap. How can I access the actor that owns this delegate? Assuming I can't store a reference to this elsewhere. Is there another delegate that takes that parameter? Is this reference stored in OtherActor somewhere? 

From what I understand, resources that are local to a particular scene( not a prefab, or texture, material asset etc ) are embedded in the scene file itself, while external resources are just referenced and loaded if needed. The game I'm working on requires all scene be loaded from an asset bundle. Which means by the time the level is loaded, all data should be in memory. In order to show relatively accurate load progress, I sum of the total bytes of all the needed asset bundles to load the level and divide that by the (approximate) number of bytes loaded so far( calculated using the file size of the asset bundle in bytes multiplied by Unity's AssetBundleRequest.progress value ). However, this isn't possible with Application.LoadLevel since there's no way to weigh how long it might take relative to the time it took to load the asset bundles. So, my question is, does Application.LoadLevel take long enough to be an issue if ignored? The presence of an async version of this method leads me to believe there is some need for this. That might only apply for scenes that were built into the game rather than into an asset bundle though. 

For some reason lambdas like this don't work too well when accessing list elements through either index or iterator. It might have to do with how the lambda captures the variable. 

That's obviously very minimal, but it'll do for this example. This is where DoMove moves in some way by speed amount. This is really the same as the delegate you were using. So, now we have a common interface, but now what do we do with it? Well, we make more specific components. But then we'll face the same problem with parameters as delegates had, right? Well this is where the Unity editor comes in. Let's define a simple Follow component that implements IMovable. 

Seeing this, you can compose different game objects. I apologise if you are already doing this, your post just made it seem as if you're not. Now, there is a huge problem with this. The things I listed above describe only one state. We don't want each component to check what state the object is in because then it ties that component( which is meant to be independent ) to a particular game object and destroys it's portability between objects. So, how do we communicate state changes? What I've done in a project I've been working on is create a state machine component. This component simply holds a dictionary of states with the key being the state name. What does a state hold? A state holds a list of all components that will be active while that state is active. So, when we transition from one state to another: 

In my game, I keep a list of all clients currently connected to the server and a list of all actors in the game. Each actor is "relevant" ( currently replicating ) to 0 or many clients, and each client has has 0 or many actors that are relevant to it. In some situations, I want to do something like this: 

We can see how these are scaled. Let's take one of the top vectors and scale it by the up vector * 5: 

This will build our bundle structs. NOTE: dependencies will only be recorded within the same BuildAssetBundles call. This means if you use two calls to build two asset bundles, dependencies between them will not be considered. If things will have dependencies you have to build them in the same call, or make your own dependency file. This should be enough to build your bundles. I haven't gone into too much detail regarding variants because I've never really used them. I know they're a way to have the same bundles( names and types must be identical ), but different assets. Think swapping out HD and SD UI textures without having to load different assets entirely. Now that's done, let's load our asset bundles. All dependencies are stored in the manifest bundle. This bundle is named the same as our output folder, and is also stored in the output folder. Let's load that first: