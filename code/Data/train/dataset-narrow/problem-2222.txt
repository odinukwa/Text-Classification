So, solving for $n=N(k)$, we have $k \approx \sqrt {2 \log n}$ and $S(k) \approx \sqrt {2 \log n}\, 2^{\sqrt {2 \log n}} = \exp(O(\sqrt{\log n}))$. This takes care of all positions $n$ in the set $\{N(k) : k\in\{1,2,\ldots\}\}$. For arbitrary $n$, trim the bottom of the solution $P(k)$ for theÂ smallest $k$ with $N(k)\ge n$. The desired bound holds because $S(k) / S(k-1) = O(1)$. QED 

Here is an explicit proof that a standard Chernoff bound is tight up to constant factors in the exponent for a particular range of the parameters. (In particular, whenever the variables are 0 or 1, and 1 with probability 1/2 or less, and $\epsilon\in(0,1/2)$, and the Chernoff upper bound is less than a constant.) If you find a mistake, please let me know. Lemma 1. (tightness of Chernoff bound) Let $X$ be the average of $k$ independent, 0/1 random variables (r.v.). For any $\epsilon\in(0,1/2]$ and $p\in(0,1/2]$, assuming $\epsilon^2 p k \ge 3$, (i) If each r.v. is 1 with probability at most $p$, then $$\displaystyle \Pr[X\le (1-\epsilon)p] ~\ge~ \exp\big({-9\epsilon^2 pk}\big).$$ (ii) If each r.v. is 1 with probability at least $p$, then $$\displaystyle \Pr[X\ge (1+\epsilon)p] ~\ge~ \exp\big({-9\epsilon^2 pk}\big).$$ Proof. We use the following observation: Claim 1. If $1\le \ell \le k-1$, then $\displaystyle {k \choose \ell} ~\ge~ \frac{1}{e\sqrt{2\pi\ell}} \Big(\frac{k}{\ell}\Big)^{\ell} \Big(\frac{k}{k-\ell}\Big)^{k-\ell}$ Proof of Claim 1. By Stirling's approximation, $i!=\sqrt{2\pi i}(i/e)^ie^\lambda$ where $\lambda\in[1/(12i+1),1/12i].$ Thus, $k\choose \ell$, which is $\frac{k!}{\ell! (k-\ell)!}$, is at least $$ \frac{\sqrt{2\pi k}\,(\frac{k}{e})^k} { \sqrt{2\pi \ell}\,(\frac{\ell}{e})^\ell ~~\sqrt{2\pi (k-\ell)}\,(\frac{k-\ell}{e})^{k-\ell} } \exp\Big(\frac{1}{12k+1} - \frac{1}{12\ell} - \frac{1}{12(k-\ell)}\Big)$$ $$ ~\ge~ \frac{1}{\sqrt{2\pi\ell}} \Big(\frac{k}{\ell}\Big)^{\ell} \Big(\frac{k}{k-\ell}\Big)^{k-\ell}e^{-1}. $$ QED Proof of Lemma 1 Part (i). Without loss of generality assume each 0/1 random variable in the sum $X$ is 1 with probability exactly $p$. Note $\Pr[X\le (1-\epsilon)p]$ equals the sum $\sum_{i = 0}^{\lfloor(1-\epsilon)pk\rfloor} \Pr[X=i/k]$, and $\Pr[X=i/k] = {k \choose i} p^i (1-p)^{k-i}$. Fix $\ell = \lfloor(1-2\epsilon)pk\rfloor+1$. The terms in the sum are increasing, so the terms with index $i\ge\ell$ each have value at least $\Pr[X=\ell/k]$, so their sum has total value at least $(\epsilon pk - 2) \Pr[X=\ell/k]$. To complete the proof, we show that $$(\epsilon pk - 2) \Pr[X=\ell/k] ~\ge~ \exp({-9\epsilon^2 pk}).$$ The assumptions $\epsilon^2pk\ge 3$ and $\epsilon\le 1/2$ give $\epsilon pk \ge 6$, so the left-hand side above is at least $\frac{2}{3}\epsilon pk\, {k \choose \ell} p^\ell(1-p)^{k-\ell}$. Using Claim 1, to bound $k\choose \ell$, this is in turn at least $A\, B$ where $A = \frac{2}{3e}\epsilon p k/ \sqrt{2\pi \ell}$ and $ B= \big(\frac{k}{\ell}\big)^\ell \big(\frac{k}{k-\ell}\big)^{k-\ell} p^\ell (1-p)^{k-\ell}. $ To finish we show $A\ge \exp(-\epsilon^2pk)$ and $B \ge \exp(-8\epsilon^2 pk)$. Claim 2. $A \ge \exp({-\epsilon^2 pk})$ Proof of Claim 2. The assumptions $\epsilon^2 pk \ge 3$ and $\epsilon\le 1/2$ imply (i) $pk\ge 12$. By definition, $\ell \le pk + 1$. By (i), $p k \ge 12$. Thus, (ii) $\ell \,\le\, 1.1 pk$. Substituting the right-hand side of (ii) for $\ell$ in $A$ gives (iii) $A \ge \frac{2}{3e} \epsilon \sqrt{p k / 2.2\pi}$. The assumption, $\epsilon^2 pk \ge 3$, implies $\epsilon\sqrt{ pk} \ge \sqrt 3$, which with (iii) gives (iv) $A \ge \frac{2}{3e}\sqrt{3/2.2\pi} \ge 0.1$. From $\epsilon^2pk \ge 3$ it follows that (v) $\exp(-\epsilon^2pk) \le \exp(-3) \le 0.04$. (iv) and (v) together give the claim. QED Claim 3. $B\ge \exp({-8\epsilon^2 pk})$. Proof of Claim 3. Fix $\delta$ such that $\ell=(1-\delta)pk$. The choice of $\ell$ implies $\delta\le 2\epsilon$, so the claim will hold as long as $B \ge \exp(-2\delta^2pk)$. Taking each side of this latter inequality to the power $-1/\ell$ and simplifying, it is equivalent to $$ \frac{\ell}{p k} \Big(\frac{k-\ell}{(1-p) k}\Big)^{k/\ell-1} ~\le~ \exp\Big(\frac{2\delta^2 pk}{\ell}\Big). $$ Substituting $\ell= (1-\delta)pk$ and simplifying, it is equivalent to $$ (1-\delta) \Big(1+\frac{\delta p}{1-p}\Big)^{\displaystyle \frac{1}{(1-\delta)p}-1} ~\le~ \exp\Big(\frac{2\delta^2}{1-\delta}\Big). $$ Taking the logarithm of both sides and using $\ln(1+z)\le z$ twice, it will hold as long as $$ -\delta\, +\,\frac{\delta p}{1-p}\Big(\frac{1}{(1-\delta)p}-1\Big) ~\le~ \frac{2\delta^2}{1-\delta}. $$ The left-hand side above simplifies to $\delta^2/\,(1-p)(1-\delta)$, which is less than $2\delta^2/(1-\delta)$ because $p\le 1/2$. QED Claims 2 and 3 imply $A B \ge \exp({-\epsilon^2pk})\exp({- 8\epsilon^2pk})$. This implies part (i) of the lemma. Proof of Lemma 1 Part (ii). Without loss of generality assume each random variable is $1$ with probability exactly $p$. Note $\Pr[X\ge (1+\epsilon)p] = \sum_{i = \lceil(1-\epsilon)pk\rceil}^n \Pr[X=i/k]$. Fix $\hat\ell = \lceil (1+2\epsilon)pk \rceil - 1$. The last $\epsilon pk$ terms in the sum total at least $(\epsilon pk-2)\Pr[X=\hat\ell/k]$, which is at least $\exp({-9\epsilon^2 pk})$. (The proof of that is the same as for (i), except with $\ell$ replaced by $\hat\ell$ and $\delta$ replaced by $-\hat\delta$ such that $\hat\ell = (1+\hat\delta)pk$.) QED 

A lower bound of $\Omega(\log N)$ (times the radius of the first circle). A matching upper bound of $O(\log N)$. 

Theorem. The problem in the post is NP-hard. By "the problem in the post", I mean, given a graph $G=(V,E)$ and integer $k$, to choose $k$ edges to raise the capacities of so as to maximize the min cut in the modified graph. The idea is to reduce from Max Cut. Roughly, a given graph $G=(V,E)$ has max cut size $s$ if and only if you can increase the capacities of $n-2$ edges so that the resulting graph has min cut size $s$. The idea is that $n-2$ edges are just enough to force the resulting graph to have only one finite-capacity cut, and that can be any cut you choose. This idea doesn't quite work because to get a given cut $(C, V\setminus C)$, you need the subgraphs induced by $C$ and $V\setminus C$ each to be connected. But you can work around this with an appropriate gadget. Proof. Given a connected graph $G=(V,E)$, define a connected cut to be a cut $(C,V\setminus C)$ such that the subgraphs induced by $C$ and by $V\setminus C$ are each connected. Define Max Connected Cut to be the problem of finding a connected cut (in a given connected graph) maximizing the number of edges crossing the cut. We show that Max Connected Cut reduces to the problem in the post. Then we show that unweighted Max Cut reduces to Max Connected Cut. Lemma 1. Max Connected Cut reduces in poly time to the problem defined in the post. Proof. Given a Max-Connected-Cut instance $G=(V,E)$, let $k=|V|-2$. To prove the lemma, we prove the following: Claim 1: For any $s>0$, there is a connected cut $(C,V\setminus C)$ in $G$ of capacity at least $s$, IFF it is possible to raise $k$ edge capacities in $G$ to infinity so that the resulting graph has min cut capacity at least $s$. ONLY IF: Suppose there is a connected cut $(C,V\setminus C)$ of capacity at least $s$. Let $T_1$ and $T_2$ be subtrees spanning, respectively, $C$ and $V\setminus C$, then raise the capacities of edges in $T_1$ and $T_2$. (Note that $|T_1|+|T_2|=|C|-1 + |V\setminus C|-1 = |V|-2 = k$.) The only finite-capacity cut remaining in the graph is then $(C, V\setminus C)$, of capacity at least $s$, so the resulting graph has min cut capacity at least $s$. IF: Suppose it is possible to raise $k$ edge capacities in $G$ so that the resulting graph has min cut capacity at least $s$. Consider the subgraph formed by the $k$ raised edges. Without loss of generality, assume this subgraph is acyclic. (Otherwise, "unraise" one edge from a cycle of raised edges and instead raise some unraised edge that joins two connected components from the subgraph. This only increases the min cut in the resulting graph.) By the choice of $k=n-2$, the subgraph of raised edges has two connected components, say $C$ and $V\setminus C$, so the only finite-capacity cut in the resulting graph is $(C,V\setminus C)$. And this cut has capacity at least $s$, as it did in the original graph. This proves the claim (and the lemma). (QED) For completeness, we show that Max Connected Cut is NP-complete, by reduction from unweighted Max Cut. Lemma 2. Unweighted Max Cut reduces in poly time to Max Connected Cut. Proof. For any integer $N\ge 1$, define graph $P(N)$ to consist of two paths $A$ and $B$, each of length $N$, with edges from each vertex in $A$ to each vertex in $B$. We leave it as an exercise to the reader to verify that the max cut in $P(N)$ ($A$ on one side, $B$ on the other) has size $N^2$, and no other cut has size larger than, say, $N^2-N/100$. Here is the reduction. Given any unweighted Max Cut instance $G=(V,E)$, construct a graph $G'=(V',E')$ as follows. Let $n=|V|$. Let $N = 100(n^2 + 2n)$. Add to $G$ the graph $P(N)$ defined above (with its two paths $A$ and $B$). From each vertex $v\in V$ add an edge to one vertex in $A$ and another edge to one vertex in $B$. This defines the reduction. To finish, we prove it is correct: Claim 2: For any $s\ge 0$, there is a cut $(C,V\setminus C)$ in $G$ of capacity at least $s$, IFF there is a connected cut in $G'$ of size at least $s+N^2+n$. ONLY IF: Given any cut $(C,V\setminus C)$ in $G$ of capacity at least $s$, consider the connected cut $(A\cup C, B\cup V\setminus C)$ in $G'$. This connected cut in $G'$ cuts at least $s$ edges from $C$ to $V\setminus C$, plus $N^2$ edges from $A$ to $B$, plus $n$ of the $2n$ edges from $V$ to $A\cup B$. IF: Suppose there is a connected cut in $G'$ of size at least $s+N^2+n$. $A$ and $B$ are on opposite sides of the cut. (Otherwise, since the second largest cut in $P(N)$ cuts at most $N^2-N/100$ edges in $P(N)$, the total number of edges cut is at most $N^2-N/100 + |E| + 2|V| \le N^2 - N/100 + n^2 + 2n = N^2$.) Let $C$ denote the vertices in $V$ on the side of the cut with $A$. Then there are $N^2$ edges in the cut from $A$ to $B$, and $n$ from $V$ to $A\cup B$, so there must be at least $s$ from $C$ to $V\setminus C$. This proves the claim and Lemma 2. (QED) By Lemmas 1 and 2, since unweighted Max Cut is NP-hard, the problem in the post is also NP-hard. 

EDIT: The argument that I had answered with was not wrong, but it was a bit misleading, in that it only showed that the upper bound had to be tight for some $n$ (which is actually trivial, since it has to be tight when $n=2$ and the bound is 1). Here is a more precise argument. It shows that if the upper bound of $\log_2 n$ is loose for any particular $n$, then for all $n$ the number of oracle calls required is $O(1)$. (Surely it is not $O(1)$, so the upper bound is never loose! But I don't actually prove that here, and given the other answer to the problem, it doesn't seem worth pursuing.) Consider the problem of computing the maximum output: 

The algorithm returns a $(1+O(\varepsilon))$ approximate solution in $O(|x^*|\log(n)/\varepsilon^2)$ iterations, where $n$ is the number of elements and $x^*$ is the optimal fractional set cover (trivially $|x^*|\le n$). (A similar algorithm appears in the paper Chandra mentioned. Vertex Cover is of course a special case.) (Remark: Note that the iteration bound does not depend on the number of sets, just the number of elements. Thus, the algorithm can be used with an implicitly defined set system, as long as, given weights on the elements, one can efficiently find a set of maximum (or near-maximum) total weight. This kind of oracle is the same as the separation oracle required to apply the ellipsoid algorithm to the dual problem. For packing problems such as set packing, you need an oracle that, given weights on the elements, returns a set minimizing the total weight. For problems such as multi-commodity flow, you might, for example, need to find a path minimizing the sum of some given edge weights.) Here's a sketch of the proof of the performance guarantee. In each iteration, the partial derivative w.r.t. the chosen $s$ is at least $1/|x^*|$, where $x^*$ is the optimal fractional set cover. (To see why, recall that the gradient of Lmin$(Ax)$ with respect to $x$ is $(g(Ax))^T A$. If we were to choose a set $s'$ at random from the distribution $x^*/|x^*|$, the expected value of the partial derivative with respect to $x_{s'}$ would thus be $(g(Ax))^T \,A x^*/|x^*|$. Since $Ax^* \ge 1$, this is at least $|g(Ax)|/|x^*|$. Since $|g(Ax)|=1$, this is at least $1/|x^*|$. Thus, there must exist some $s$ giving partial derivative at least $1/|x^*|$. Since the algorithm chooses $x_s$ in each iteration to maximize the partial derivative, it achieves a partial derivative of at least $1/|x^*|$.) Then, the step size $\varepsilon$ is chosen just small enough so that no coordinate of $A x$ increases by more than $\varepsilon$. Thus, because of the smoothness of Lmin, increasing $x_s$ to $x_s+\varepsilon$ increases $\mbox{Lmin}(Ax)$ by at least $(1-O(\varepsilon))\varepsilon/|x^*|$. In this way, the algorithm maintains the invariant $$\mbox{Lmin}(Ax) \ge (1-O(\varepsilon)) |x|/|x^*| - \ln n.$$ (Note that Lmin$(\overline 0)$ equals $\ln n$.) At termination, in the invariant, the $\ln n$ term is $O(\varepsilon)$ times the left-hand side, so by calculation one gets $\min_e A_e x \ge (1-O(\varepsilon)) |x|/|x^*|$. After the normalization in the last line of the algorithm, this implies $|x| \le (1+O(\varepsilon))|x^*|$. FWIW, the inequalities involved in proving the invariant are essentially the same as those involved in proving the Chernoff bound. (In fact, this algorithm can be derived by applying the method of conditional probabilities to a randomized-rounding scheme that repeatedly samples sets from the distribution $x^*/|x^*|$ (with replacement), increasing $x_s$ for each sampled set $s$. This derandomization of that gives the algorithm: the underlying invariant is just that the pessimistic estimator stays below 1. The exponential penalties in the pessimistic estimator come from the using the Chernoff bound in the analysis of the rounding scheme. This basic idea is explained further in the paper Chandra mentioned.) Fractional Weighted Set Cover (and general fractional Covering) To handle problems such as Weighted Set Cover efficiently, we modify the algorithm to use non-uniform increments (an idea due to Garg and Konemann). The LP is $\min\{ c\cdot x : (\forall e) \sum_{s\ni e} x_s \ge 1\}$, where $e$ ranges over the elements, $s$ ranges over the sets, and all variables are non-negative. To present the algorithm, first rewrite the problem as a general covering problem. Let $A_{es} = 1/c_s$ for $e\in s$ and $A_{es} = 0$ otherwise. Then (with a change of variables, scaling each $x_s$ by $c_s$), the LP is $\min\{ |x| : A x \ge 1; x \ge 0\}$, which we can view as a general covering LP. Here is the algorithm: 

Each city is assigned to some center: for each $v$, $\sum_c X_{vc} = 1$. Any assigned center must be open: for each $c$ and $v$, $X_{vc} \le Y_c$. Exactly $k$ centers are opened: $\sum_{c} Y_c = k$. All variables non-negative. 

(EDIT: Note that this is not the same as the "variant 2" at the end of the original poster's question.) Here's a proof (more or less) of his conjecture (it is bounded in this case). Lemma. For the variant suggested by David, the total distance traveled by the bug is always $O(d_0)$, where $d_0$ is the maximum distance between the bug and any point in the first circle placed. proof. Assume WLOG that the final resting place is the origin $o$, and that the bug starts at distance 1 from $o$. For exposition assume the bug starts at time $0$, travels at unit rate (one inch per second), and stops only when it reaches the last disc placed. Note that (as explained further below) as the bug crawls its distance to the origin strictly decreases. Partition the unit-radius disc (centered at $o$) into infinitely many rings by drawing concentric circles of radii $1, 0.99, 0.99^2, 0.99^3, \ldots$. 

Also, we sketch a lower bound (Lemma 3): for a certain class of so-called well-behaved solutions, Lemma 1 is tight (up to constant factors in the exponent), and no such solution using poly-log space can reach position $N$ in time $O(N\,\text{polylog}\, N)$. Lemma 1. For all $n$, it is possible to reach position $n$ in $n$ moves using space $$\exp(O(\sqrt{\log n}))~=~n^{O(1/\sqrt{\log n})}$$ Proof. The scheme is recursive, as shown in the figure below. We use the following notation: 

Initialize all $x_j = 0$. Let $N=2\ln(m)/\varepsilon$. While $P x < N$: 2.1. Choose $j$ so that the partial derivative of Lmax$(Px)$ with respect to $x_j$ is at most the partial derivative of Lmin$(Cx)$ with respect to $x_j$. (Explicitly, choose $j$ such that $$\frac{\sum_i P_{ij} \exp(P_i x)}{\sum_{i}\exp(P_i x)} \le \frac{\sum_i C_{ij} \exp(-C_i x)}{\sum_{i}\exp(-C_i x)}.)$$ 2.2. Increase $x_j$ by $\delta$, where $\delta$ is chosen maximally such that no constraint $P_i x$ or remaining constraint $C_i x$ increases by more than $\varepsilon$. 2.3. Delete all covering constraints $i$ such that $C_i x \ge N$. Return $x/\max_i P_i x$. 

Initialize all $x_e = 0$. Let $N=4\ln(n)/\varepsilon$. Repeat $n\,N$ times: 2.1. Choose $u$ uniformly at random from $U$. 2.2. Choose $w$ such that $(u,w)\in E$ minimizing $\sum_{e\ni w} x_e$. 2.3. Increase $x_{uw}$ by $\varepsilon$. Return $x/N$. 

By geometric arguments similar to those in the lower bound, for some $\epsilon$, each of these triangles has two long legs and one short leg, and the ratio (for each triangle) of the short leg length to the long leg lengths is $\Theta(\epsilon)$: $$\frac{|bb'|}{|ab|} = \Theta\big(\frac{|ab|}{|pa|}\big) = \Theta\big(\frac{|pa|}{|bo|}\big) = \Theta(\epsilon).$$ This equation and the assumption that $|bo|$, which is the circle radius, is in $[1/2,1]$ imply that $|ab| = \Theta(|pa|^2/|bo|) = \Theta(|pa|^2)$, and then that $|bb'| = \Theta(|ab||pa|/|bo|) = \Theta(|pa|^3)$. Now we focus on the bug's distance to $p$. Call it $d$ before the step, and $d'$ after the step. (Note $d=|pa|$, $d'=|pb|$, and $d-d' = |bb'|$.) In this step, this distance $d$ reduces by $|bb'|$, which by the above observations is $\Omega(d^3)$. Thus, the number of additional steps required to reduce the distance by a factor of 2 (to at most $d/2$) is $O(1/d^2)$. Changing variables, if $d=1/2^k$, the number of additional steps required to bring the distance below $1/2^{k+1}$ is $O(4^k)$. Since the sum is geometric, the total number of steps required to bring the distance below $1/2^{k}$ is $O(1/4^k)$. Changing variables again, after $n$ steps, the distance to $p$ will be $O(1/\sqrt n)$. Finally, recalling the displayed equation several paragraphs up, in the $n$th step, the distance that the bug travels, i.e. $|ab|$, is $O((\mbox{the current distance to } p)^2) = O(1/n)$. Thus, the total distance traveled in the first $N$ such steps while the circle radius is in $[1/2,1]$ is at most $$\sum_{n=1}^N O(1/n) = O(\log N).$$ By scaling, we conclude that, for any $k$, the total distance traveled while the circle radius is in the range $[1/2^k, 1/2^{k+1}]$ is $O(\log(N)/2^k)$. Summing over $k$, the total distance traveled is $O(\log N)$. QED