There is a more complicated theory of duality for SDPs that is exact: there is no 'extra condition' like Slater's condition. This is due to Ramana. (For another take on this involving SOS, see [KS12].) To be honest, I've never tried to understand these papers and would be happy if someone dumbed them down for me. One notable consequence of this work is that the problem of testing whether a given SDP is feasible is in NP if and only if it is in coNP. (However, I think the experts expect the problem is in neither. The best upper bound known is PSPACE.) 

In light of the Ding--Sly--Sun verification of the 1-step Replica Symmetry Breaking picture for kSAT (when k is large enough) I think experts would now be pretty surprised if the MPZ/MMZ-conjectured formula for the 3SAT satisfiability threshold (approximate value: 4.2667) is incorrect. 

I think a good test case would be to generate random uniquely satisfiable 3XOR instances (planted instances) with $\Theta(n)$ constraints and then convert them to 3SAT instances. 

I'm not an expert in this area, but I think the random SAT / phase transition stuff is more or less completely unrelated to the industrial/practical applications stuff. E.g., the very good solvers for random instances (such as $URL$ are based on the statistical physics methods (belief propagation etc.) I believe, whereas the very good 'general' solvers (such as $URL$ are using unrelated techniques (more like what Kaveh was talking about) I believe. But, as I said, maybe don't take my word for it; this is coming from a definite non-expert. 

I'll provide a naive approach which give $O(m^2 \cdot max_f)$ running time but shows the problem is P. First turn the graph to undirected graph. Suppose one edge is in first graph and the other in the second graph, contract both of them and name them $s,t$. We can find a minimum edge cut that separates s and t in time max_f which is a running time of best maximum flow algorithm on unit weight graph. We run this algorithm for all possible pair of edges. But why this works? trivially there is no smaller cut and the two sides are not edgeless. On the other hand if a graph G is connected minimum edge cut C between two vertices s, t always cuts the graph into two connected subgraphs. 

Parametrised complexity: kernelization, iterative compression, bidimentionality, ... Approximation algorithms: randomised rounding, scaling, ... In graph theory, based on structural hierarchies of graphs like bounded tree width graphs/excluded minor graphs/ bounded expansion graphs / nowhere dens graphs, there are different tools (like decompositions) and meta theorems (like Courcelle theorem). 

I wonder how to find the girth of a sparse undirected graph. By sparse I mean $|E|=O(|V|)$. By optimum I mean the lowest time complexity. I thought about some modification on Tarjan's algorithm for undirected graphs, but I didn't find good results. Actually I thought that if I could find a 2-connected components in $O(|V|)$, then I can find the girth, by some sort of induction which can be achieved from the first part. I may be on the wrong track, though. Any algorithm asymptotically better than $\Theta(|V|^2)$ (i.e. $o(|V|^2)$) is welcome. 

Perhaps you want what's sometimes called "Chang's Lemma" or "Talagrand's Lemma"... called the "Level-1 Inequality" here: $URL$ It implies that if $1_S$ has mean $2^{-d}$ then the number of linearly independent Fourier coefficients whose square is at least $\gamma 2^{-d}$ is at most $O(d/\gamma^2)$. (This is because an $F_2$-linear transformation on the input does not change the mean, so you can always move linearly independent Fourier characters to degree-1.) 

It might be conjectured that the Learning Parity with Noise Problem (LPN) at constant error rate requires time $2^{n^{1-o(1)}}$. The fastest known algorithm (Blum-Kalai-Wasserman) uses time $2^{O(n/\log n)}$. 

To somewhat restate what Ryan Williams wrote in his last paragraph: The Moshkovitz-Raz theorem shows that there is a function $T(n) = 2^{n^{1-o(1)}}$ such that if Max-3Sat can be $(7/8 + 1/(\log \log n)^{.000001})$-approximated in time $T(n)$ then the decision version of 3Sat is in time $2^{o(n)}$. It is commonly believed that the latter is impossible (this is the Exponential Time Hypothesis), in which case the former is impossible too. To put it not quite accurately, you can't beat $7/8$ for Max-3Sat in anything better than full exponential time. 

I believe that the smallest such classes known are $S_2P$ (Cai, 2001), $PP$ (Vinodchandran, 2005), and $(MA \cap coMA)/1$ (Santhanam, 2007). All of these are indeed known to not be in $SIZE(n^k)$ for each constant $k$. 

A useful survey on the topic, slightly out of date: Philip Bille. A survey on tree edit distance and related problems. Theoretical Computer Science, Volume 337, Issues 1–3, Pages 217–239, 2005. A recent paper on one of the versions of the problem: Tatsuya Akutsu et al. Exact algorithms for computing the tree edit distance between unordered trees. Theoretical Computer Science, Volume 412, Issues 4–5, Pages 352–364, 2011. 

A very good question. I don't know the full answer and would like to know it myself. However, you may find the following interesting. If, instead of the group $S_n$, we consider its 0-Hecke monoid $H_0(S_n)$, it has a representation on a certain class of integer matrices which acts by tropical $(\min,+)$-multiplication. This has a lot of interesting applications in stringology, via multiple-source shortest paths in grid-like graphs. For details, see my technical report: A. Tiskin. Semi-local string comparison: Algorithmic techniques and applications. $URL$ 

As pointed out in the answer by @ilyaraz, the question is easy for 2D points and halfplanes. It becomes much more interesting for 3D points and halfspaces. An $\epsilon$-net of size $O(\frac{1}{\epsilon})$ is still possible in the 3D case, but the proof requires much more work. Even the latest "simple" proof is actually surprisingly complex, bearing in mind the simplicity of the 2D case: Evangelia Pyrga, Saurabh Ray. New Existence Proofs for epsilon-Nets. SoCG 2008. $URL$ This paper also gives references to earlier proofs by Matousek et al. 

bit representation of $i$ is smaller than bit representation of $j$ (w.r.t. the definition of $\le$ in the question), and bit representation of $j$ has exactly one additional $1$ compare to $i$. 

Thor Johnson, et al, in their paper: Directed Tree Width, introduced a definition for directed grid $J_k$, and they conjectured: 

First of all I think you mean a maximum clique, not all cliques (and even not maximal cliques). As otherwise e.g in $K_n$ there are $2^n−1$ cliques. If the question is the one that I said, then there is no polynomial time online algorithm for update (unless P=NP). If there is such an algorithm $A$, given a graph $G$, we can find a maximum clique of $G$ in polynomial time. Just start from a graph on $n$ vertex without any edge and add edges of $G$ one by one and update information in each step by $A$. After adding all edges we have a maximum clique. Note that if we use exponential memory at some steps we can do this, but to write on exponential memory we need exponential time as well. Maybe there are some heuristics. 

Consider a partial order $P$, a series-parallel order $Q$ and a total order $R$, such that $P \subseteq Q \subseteq R$. Given $P$ and $R$, we are asked to find $Q$ of minimum length. An $O(n^3)$ dynamic programming algorithm suggests itself, solving the problem in increasing intervals of $R$, starting from empty intervals and terminating with the whole $R$. Is it possible to solve the problem in subcubic time? 

More generally, fast matrix multiplication can be done on $p$ processors in $O(n^2/p)$ memory per processor. However, the communication between processors is then suboptimal. Optimal communication can be achieved by using more memory. As far as I know, it is not known whether optimal communication and optimal memory can be achieved simultaneously. Details are in $URL$ 

The answer is certainly yes. Assume for simplicity that the two strings are of equal length $n$. If the "induced" alphabet of each string is also of size $n$, then both strings are permutations, and the problem degenerates to the classical longest increasing subsequence (LIS) problem: it is easy to see that the edit distance between a given permutation $\pi$ and the sorted permutation $1,2,\ldots,n$ is $n-LIS(\pi)$. The LIS problem was studied as early as 1930s by Erdos and Szekeres, and by Robinson. The best algorithms for LIS run in time $O(n \log n)$ or $O(n \log\log n)$, depending on the computation model. Such algorithms were (re)discovered dozens of times (or perhaps hundreds of thousands, considering that the problem makes a good programming exercise), often with various twists. It is easy to find the relevant papers, since most of them cite previous work. The most recent result I am aware of, and a good point to start exploring the references, is the $O(n \log\log LIS(\pi))$ algorithm by Crochemore and Porat: M. Crochemore and E. Porat. Fast computation of a longest increasing subsequence and application. Information and Computation, 208(9):1054--1059, 2010. $URL$ The same paper also describes how to interpolate between this extreme case and the general edit distance problem, taking the total number of character matches as a parameter. This is relevant when your "induced" alphabet size is large, but not exactly equal to $n$. For smaller "induced" alphabet sizes (say $n/2$), it is easy to construct examples that are essentially binary over a significant part (say, one half) of each input string, so a large "induced" alphabet over the whole string won't help more than by a constant factor. 

I think Sparsest cut or edge expansion, is good enough for you, because deals with both edges and nodes. The task is to find a cut, with $S\subset V$, $|S|\leq {|V|\over2}$ in one part such that: $$\alpha(G) = \min \{ {E(S,S^c)\over |S|}\}$$ is minimized over all possible set of $S$. You may look at Sparsest cut and bottlenecks in graphs by Matula,Shahrokhi for proof of NP-Hardness (proof is by using max cut). Also P. Bonsmaa et al, shown that it's NP-Hard even in uniform case (means all edges are of weight $1$). 

In general, if the problem is in such a way that needs more than one dimension for dynamic programming and also those dimensions are depended to each other then the problem has potential to be hard in graphs of bounded tree width. We can see this pattern in both of problems in the question as well as for the sparsest cut problem. (In the first problem we want to keep previous coloring on the other hand keep coloring as small as possible, in the second problem obviously there are two functions which are dependent to each other)