When you run this file with , it will look up your host list (as simple as an ini file, but can also be a dynamic list written in any programming language) to determine what servers exist and what groups they're in, and then run the defined tasks on the servers that you tell it to. Now, you specified that you wanted to stay in the Javascript ecosystem. However, while Ansible is a new thing for you, it doesn't have all the complexities of another programming language, since configurations are written in YAML. It will also allow you to do much much more later, and unlike several of its competitors, uses ssh to communicate, so there's very little setup involved (nothing to install on the remote machines). 

Since you haven't yet implemented anything, you might reconsider this. Using a system like Ansible vault has a number of security downsides: 

Most repository managers have some way of implementing this. GitHub (and GitHub Enterprise) has the idea of statuses on a commit; you'd set up a webhook that pings your tool's webserver when a commit is pushed, and then it would use the status API to set whether or not it passes the version check. Bitbucket, when self-hosted, supports plugins to integrate nicely with it, including ScriptRunner, which can be used to do all sorts of things similar to what you want. I'm not familiar with GitLab, but it probably has some similar method. You will need to do some programming to implement the checking logic, but these pieces should allow you to kick that off when appropriate and use the results to block the merge. 

My coworker is trying to attach IAM roles to EC2 instances and doesn’t have permissions. I’m trying to work out which permissions to give him. My question is: What is the AWS user permission that allows attaching and detaching IAM Roles to instances? 

There is a great discussion of the Cattle vs Pets distinction from Randy Bias here. Martin Fowler talks about a SnowFlakeServer. In a series of talks, Adrian Cockcroft talks about how they moved toward Cattle Servers for solving a scalability problem at Netflix. The challenge with this distinction is always managing persistent state. Does it make sense to treat your database servers as Cattle? It does if you (a) manage the state outside of your cattle model (external volumes for your docker containers), or (b) use a distributed database like Cassandra that allows for individual nodes to fail, but still maintain state in the cluster. I get that you can get very close to the 'disposability with persistent state' of Docker containers mounting a shared volume, with launching AMIs to machine instances mounting a shared drive. You can get closer this this idea of scheduled cluster management by having an autoscaling group that recreates machines that you've blown away. To me - the machine instances lack the granularity of a docker container. They gravitate more towards the 'pets' end of the spectrum. My question is: Does the "cattle not pets" distinction apply as equally to machine instances as to containers? 

The way we handle this is to run our alerts through Sensu first. You can configure a Sensu check to require multiple failures to alert, and then also configure the check's notify to get sent to PagerDuty. This way, by the time PagerDuty hears about it (and pages you), it has already passed the "N failures in a row" criteria. If you need to check not for number of failures in a row, but rather N failures across a time range, then that's a good use case for adding elasticsearch or graphite or similar into the mix. Whenever there's a failure, log an error or increment a metric, and then your check can simply look over the time range and see if the aggregate is above the limit. 

There are two general strategies for dealing with traffic surges: increasing capacity and reducing cost. Increasing capacity means auto-scaling, which everyone was very excited about when public clouds first became available. In its most basic sense, this will boot up more webservers for you based on load and add them to a load balancer, but since can be a pain to manage, there are more automagic solutions as well, like Elastic Beanstalk. The trouble with automated capacity expansion is that its also automated bill expansion - 10x normal traffic means 10x servers means 10x money you have to pay. That's why, while it's a useful strategy to keep in mind, I think you should always start by seeing how much you can cheat. By cheat, I mean cache, which rests on the idea that most of the time you can give users slightly out of date data and they won't notice, and that can save you tremendous amounts of time. Imagine that you have a page that you decide it's ok if it's five seconds out of date, and it gets 20 req/s. Without caching, you're running that calculation 1200 times a minute, whereas with caching it's only 12. You can see how this can make a tremendous difference. There are of course many types of caching, and a successful website will use several of them. But for your use case, there are two pretty good and easy options. The first is to make the site completely static. This assumes that you can do so, but if you can, then you just have Nginx serve up the html directly, and it can serve tons of requests with no sweat. If you need some level of dynamicity, then doing some full-page caching is a good option. Nginx has some capability to do this, but I really like Varnish because of its flexibility. Whatever option or options you go with, make sure you do load testing to validate that you've set it up properly; sometimes fixing one spot exposes a new bottleneck. 

I'm having a discussion with a friend about use cases for Docker. One guy in the team wants to use Docker for everything - like a kind of universal unix process wrapper. The other thinks that Docker should only be used for stateless applications like Microservices and AWS Lambda style apps. We've engineered proof of concepts for both. On our docker cluster we have a shared drive that gets mounted when the Docker host is mounted, and if a Database in a container is mounted, it simply mounts a volume to the shared drive. My friend still sticks to his position, despite being shown the contrary evidence. (He also argues that Docker adds unnecessary risk by adding complexity to the stack.) I'm trying to listen and understand his point of view, both in an act of empathy, but also to better reason with him. (We all get on quite well - so this is a mix of in-jest and serious discussion). Kind of the question behind the question is: are databases cattle? This comment suggests that a good automated backup and retrieval strategy for your database is indistinguishable from a cattle server. My question is: What are the reasons Docker should not be used for databases? EDIT: People have asked me to clarify my terminology. I was assuming that the database application was in the container, and the storage was in the volume. What I meant was, the RDBMS is in the container, and the database storage is in the volume. Some commentators have suggested that the docker volume drivers aren't going to work with database writes very well. (Or something to that effect). Could you please expand on that? 

I am not sure if you are open to commercial solutions, but Sumo Logic is an excellent ingest-type solution. I also tried Logly and LogDNA which were nice but not as powerful and feature rich as Sumo. Let me also say am not a shill for this company. I’m just a DevOps guy who hates dealing with crappy and incomplete log collection. After dealing with the complexity of ELK, which is admittedly is very powerful and flexible so I have read, ElasticSearch is not for the faint of heart, especially when you walk into a shop and their ELK stack is 4 years old. It’s important to know right right off that Sumo is a cloud solution, so as long as you’re okay with shipping your logs and metrics over HTTPS to their servers, it’s a good way to go. Then you can focus on just feature configuration without having to maintain the core service. It offers most of what I remember that Splunk does without being prohibitively expensive. As you might have guessed, I just completed a project where I moved from ELK to Sumo for production log data and metrics visualization and it is a world of difference from Kibana as far as how much easier it is to get useful output. I found statistical capabilities similar as well, but Kibana does seem to be more actuarially adherent. It’s easy to set up alarms, integrate with RESTful APIs for tools like like Slack, perform complex data correlation, and make purdy dashboards for management. It’s comprehensive as it is an entire stack. Also, there is a very well-developed chef cookbook so installation and configuration is very manageable once you get your ingestion configured along with rulesets. That can be time consuming but as far as cookbooks go I’d say it’s medium complexity. Data ingestion is incredibly flexible- I have not hit any limitations with what it can do, but I did give up trying to ingest directly from logstash as getting metadata to map was more trouble than it was worth. While slurping text logs is what they are geared for, pushing logs and metrics is also an option. We continue to use logback for our app logging, but now Sumo collects pretty much all our logs now as well as graphite data. It does Windows Event Logs nicely as well. We are starting to integrate fluentd from k8s but we’re having some challenges keeping the log volume tamed. They have their own pre-built support setups for a large amount of OSs, app frameworks, and infrastructure elements which can make initial setup that much faster, but some vendors like Fastly even have advanced prebuilt Sumo Logic implementations that are downright fantastic so our CDN logs create a remarkable dashboard giving an excellent picture of site health, load, as well tons of other useful data. There is definitely a learning curve but nothing like ELK. Getting all your regex just right so things like exceptions in app logs are handled properly can take time, but you can get a lot going with very little effort as a lot of tedious problems like automatic timestamp detection and multi line detection are built in if you want them. Ingestion can be as sophisticated as you want it to be. I found set up kind of evolutionary over the last year where I started with a very basic setup where I did all my pattern matching in my queries, but eventually I had ingestion rules that sifted all my fields out for me which made things more user-friendly for other users. Also, support and documentation are very good. As far as paid services go, this one seemed quite worth it unless you’re already and ace with ELK. Boy, that was probably a lot more info than you bargained for, but it’s a fair breakdown of Sumo Logic. (I hope!) 

I've been evaluating Netflix Ice - as a billing tool - but it appears to only work at the Machine instance level - not at the container level. I'm looking for a tool to help generate billing reports for using docker containers across a cluster of docker servers on EC2. (Non ECS) My question is: How to track (non ECS) container costs on EC2? EDIT: I have different groups each running their container on the same host. So I want a way to split instance costs by container usage. 

I'm trying to wrap my head around the Azure confidential computing offering. It appears that that AWS does not offer encryption at the application level (see diagram for what I mean by this:) 

I've got an ec2 autoscaling group that starts in the morning and finishes in the evening. I'd like to create a 'button' that people can click to warm up the autoscaling group to run jobs in the middle of the night (only on an on-demand basis). My question is: What is the ec2 cli command to modify the minimum number of nodes in a scaling group? 

The benefit of unikernels is managing a large number of applications in a protected space. (Some might say all the benefits of Docker without all the overhead). (Please don't get me wrong - I'm a huge fan of docker and use it 20 times a day in my work - I'm using this question as a a way to explore an idea). The following commentator writes: 

The mean time to recover has an implied subject - the mean time to recover what? Defining this is key to using the metric effectively. Are you recovering the general availability of your production website? Are you recovering the functionality of a particular feature that has a bug in it? Once you know what you're actually trying to measure, it's much easier to measure it! The general thrust of your question seems to actually be surrounding the competing goals of shipping features and maintaining reliability, which is an ages-old battle. Traditionally it is developers' jobs to implement new things, and sysadmins' jobs to prevent things from breaking, and this leads to departmental conflict, as change tends to cause breakage. One of the philosophies oft associated with DevOps is the idea that developers and ops engineers should work closely together so as to ease this tension. You may also be interested in Google's approach to that problem, which is to have "error budgets" for development teams to spend; once they've penalized stability too much, they must spend the rest of the quarter only working on stability. Along with this, the site reliability engineers have available goals, and if they over shoot, they are encouraged to let more changes through; the idea here is that their goal must not simply be to maintain reliability as high as possible, as then they'd be motivated to fight change in every situation. 

In our work environment we have a standard Corporate Intranet with Active Directory. We've been granted limited access to an AWS VPC. Our connection allows outbound (from the Intranet to the VPC) but not inbound. That is - if we run a webserver in the AWS VPC, then a client in the Corporate Intranet can connect and browse to it. But a client in the AWS VPC cannot connect to a webserver in the Corporate intranet. Note that this 'outbound connections only principle' applies to all ports, not just http port 80. An associate has suggested we need to replicate our Active Directory down to the AWS VPC AD. I think it is not possible to do a one-way replication. My question is: Can you replicate Active Directory from a Corporate Intranet to an AWS VPC where there is an outbound-only link? 

Now to me, a server application written in C++ (not memory managed) vs Java (memory managed) has no impact on the utility of a unikernel. In both you get the isolated protected way to manage your application lifecycle. Perhaps I'm missing something. My question is: What does it mean that "outside of memory-managed code execution runtimes (JVM, Go, etc.) the usefulness of unikernels starts to rapidly decline"? 

Google, probably the most famous monorepo user, developed Piper to handle their needs. But you aren't Google, and so their solutions are probably not yours. One of the key advantages to a monorepo is that you can make globally-atomic changes (i.e. you don't need to version many things because you can change the caller and the callee in the same commit). To power this, you really want to have a unified build system that tracks dependencies across the entire repo. Bazel is an open-source extraction of Google's build system, Blaze, and it attempts to do that (although it's young and immature and missing a lot of features that are necessary for non-Google use). Pants is a similar system out of Twitter. If you're building tons of code when you make such an atomic change, then you probably also want a build farm that allows you to do that not on your local machine. Similarly, you'll need a powerful CI system to handle running tests across everything as you update. 

Generally recovering from this sort of mistake involves editing the boot loader to force a root shell. Since you don't have physical access to the (non-physical) machine, things are a bit different. If this is an EBS boot drive, you can try detaching it, fixing it on another machine, then reattaching. This is a good opportunity to learn why it's useful to use configuration management tools for everything: if you were doing this, you could replace the instance in just a few minutes and one command.