Historically, F5's Application Delivery Controller has been the enterprise-grade industry solution for doing this. With this solution you can: 

Though I have not yet read it (it is on my reading list!), the description claims that the book includes 

No, it's actually really good security practice to sudo when you need higher level privileges. This is the reason that most distributions prohibit default login and actually force you to instead of just typing - they want to encourage you to use sudo for it's security benefits. There are few reasons why: 

NixOS and GuixSD both bill themselves as "declarative" and DevOps-friendly. How and in what ways are these declarative distros different than Kickstart and/or a Linux distro managed by a configuration management tool? What advantages and/or disadvantages are there to this style of declarative linux configuration as opposed to these currently existing technologies? 

If you go this route, just make sure to standardize on a language that plays well with REST most DevOps stuff uses this interface. And finally, 

Many in the DevOps apply the cattle-not-pets mentality by implementing immutable infrastructure and redeploying when changes are needed (instead of modifying). Configuration management has a similar principle of idempotence. What are the comparative benefits, similarities and drawbacks of immutability vs. impotence and which is more efficient? Can these be used synergisticly (eg, periodic deletion and redeployment of VMs or Docker containers using configuration management?) 

This also does not work in old versions of salt and is only supported with pillar.get as of salt version 2016.3, so you may be in need of an update. 

The answer is that you can't directly. You have to set a grain on the minions first by doing something similar to: 

This however should not be construed to mean that decision paralysis or analysis paralysis isn't a problem. It is! Fail-fast just isn't the best way to combat it. There are other, much more tried and true ways to combat this without comprimising good design principles. 

The subsequent bullets then expand on this question. For example, Redis is a caching system and stores key-value pairs. Similarly, MongoDB is a type of NoSQL database which also supports key/value pairs and caching, so it is a little silly to have both Redis and MongoDB. It seems that sirex007 feels that this graphic illustrates the point that and as there are a lot of trendy tools appearing in the graphic, which is making the point that the implementer doesn't seem to understand the toolkits in use in his or her stack well. This is a fair point: with the DevOps goal of rapid iteration and deployments, the appearance of new tools on a frequent basis and the changes within those tools could certainly seem like they are moving at a breakneck pace. I would argue that this is evidence of the success of the DevOps model however. If these projects are able to successfully iterate rapidly using DevOps, it is proof of the model's successes - though perhaps the cost is trying to keep up with that rapid pace. 

The key difference between DevOps in the context of Avionics and, aircraft/spacecraft merely has to do with deployment velocity due to FAA certifications, Aircraft re-grooms and the way in which the Airline industry releases their changes. But ultimately, DevOps is not exclusive to High Velocity delivery methods (like Agile). And can be done in the context of Waterfall methods or other similar methods. In some ways, the airline industry is often factory oriented, which aligns well with Kim, Behr, and Spaford's depiction and conceptualization of DevOps. Development on LRUs and racks can easily be done using an Agile methodology using a DevOps mentality. These many small changes can be stored up and released periodically to undergo final rack testing and certifications (such as AS9100). Airlines could be doing much more to leverage automation for software deployment to fleets and have yet to capitalize on these developments. At the Avionics company I worked for for several years, we were seeing several successes and challenges with implementing DevOps. I simply can't emphasize enough however that challenges were internal and political, not technical or methodological challenges. These are also not systematic issues resulting from industry regulations, policies and standards. Avionics companies who are able to innovate and provide better value, higher reliability and cheaper costs will easily be able to out-innovate their competitors and gain traction and market share. The Avionics industry is ripe for this kind of shakeup and DevOps is well suited to be a cause of changes. 

Red Hat used to brand Spacewalk as "Red Hat Satellite 5" and decided to scrap it and build a new toolset from the ground up. Red Hat then based what they now brand as "Red Hat Satellite 6" on the Katello plugin for The Foreman. In addition, the Foreman also has plugins for Docker, Puppet, and several other related systems. But to be honest, the system is a bit rough and needs some polish. I frequently bump up against bugs and have to expend quite a bit of time making all the pieces work together properly - though it is still very cool and powerful. You may, however, have an issue with any client networks that have strict security requirements. At my employer, we are not allowed to directly connect to the internet and are barely able to use proxies which require a rigorous security review and then we are still quite limited, so that might need to be a consideration for your updates model - You will be a no-go on those types of customers. On the other hand, that's part of the advantage of the "As-a-service" model. You don't have to worry about it or maintain it, so you will pick up more small-and-medium business as customers. 

If yes, then the answer will be yes. If no, then the answer will be no. ELK is nothing more than a system for processing and reporting metadata and generating structured data from plaintext logs, so it will do whatever you configure it to do. 

While this will not explain or recommend how to do DevOps on a technical level, it will make and explain the business case and method for doing so, thereby taking you from beginner to intermediate. The authors have also published a followup tome, the DevOps Handbook which bills itself as a how-to on "Creating World-Class Agility, Reliability, and Security in Technology Organizations," however one reviewer, FA Calkins cautioned, 

Do what you can to merge these together into one place. It will make your life a heck of a lot easier. 

Use a configuration management/automated deployment system. This is what these were designed for. Things like Kubernetes, Puppet, Chef, Ansible and Salt Stack are designed for just this purpose and can be utilized with Golden Master templates, kickstart scripts, Amazon AMIs or just a Docker container. This allows you to use default base templates and then layer on additional roles. This will ensure builds are documented completely (as code) and it will be fast and easy to deploy to production, and will be deployed exactly identically to what was designed or deploy additional instances when the need for scalability or redundancy arises or something breaks. Changes/updates can also be integrated this way. Just as you release software updates, you can release updates to your configuration and the configuration code can be managed just as your software code is managed - in the same repos and with the same workflows. And when upgrade time comes, there is no mystery how the thing is built, just look at the script. One way that configuration management systems do this is through heavy use of templating for your configuration files. For example, there are generally many lines that will be the same or similar in your environment. SaltStack utilizes jinja templates and puppet uses Embedded Ruby templates. Using AWS as an example, you may need to set a, api key, IAM role, region (or randomly select a region from a list of regions), a VPC, etc that is all the same across all instances. But then you need to have your functions and outputs unique. Or better yet you could write a puppet module or salt formula which defines "contracts" and use those contracts (api definitions) for both inputs and outputs saving you from having to configure your microservices two or three places. SaltStack for example recently had a meetup to discuss this particular scenario. Furthermore, SaltStack is also able to manage and deploy docker containers natively. (Puppet also has a module for Docker) Likewise Ansible has playbooks and docs for working with serverless deployments and Docker containers. Also, if you wish to continue with your serverless theme/paradigm, Puppet, Ansible and saltstack all are masterless or support a masterless mode, if you wish to continue this theme. 

The biggest problem with database backups is that you cannot simply snapshot them. You first have to make sure the DB is not mid-transaction when taking a snapshot, otherwise you will restore to an inconsistent state. In the case of windows, you can leverage the Microsoft SQL VSS writer to snapshot your database. You will want to be sure that when you take a snapshot of the VM, HyperV is capable of leveraging this VSS writer. If you have a particularly large dataset, it may be better to leverage your storage filer to use delta-clones if your filer has this feature and it can help to address the size/time issue you are experiencing with backups. In terms of deploying the new application to QA there are several ways to do this. One is to have an automated build system such as Jenkins, Travis CI or a similar tool automatically trigger a backup and deployment upon a code commit. This can be done by leveraging a configuration management tool such as SaltStack, Puppet or Chef (which all run on Windows) that can automatically set up and configure a SQL server for you and deploy your application if you are able to script a SQL install with PowerShell. Personally, I would recommend SaltStack as it's Reactor will allow you to broadcast a beacon when your backup/snapshot completes and trigger a build (Puppet lacks this capability). One option for spooling up a VM each time is to leverage a system like The Foreman which has a HyperV plugin. You could then use SaltStack (vis-à-vis Reactor. This is probably the best way if you backup system blocks until the entire backup is complete, which I doubt) or your automated build system (Jenkins/Travis) interact with The Foreman's REST API to request a host be spun up. Once this comes up, it would automatically check in with your configuration management system to install SQL server and restore the snapshot in the new VM. If you combined this with an automated testing battery, you might even be able to implement a "Push on Green" system to repeat this process into your next environment if all the QA tests pass. The advantage to using a piece of middleware like The Foreman is that it will allow you to deploy into any virtualization system or even the cloud and prevent you from being locked into HyperV and make your environment more portable. I am sure there are other ways to get there, but this is one way I have used in the past to implement similar deployment workflows.