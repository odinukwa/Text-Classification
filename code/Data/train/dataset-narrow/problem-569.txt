The job started 2:14:40 ago, which is the time of the last known good checkdb - that's true. You started it then, it ran for 2 hours and 14 minutes +/- some agent stuffs. The times look fine to me, it's not like the job ran for 5 seconds and shows a last good of 6 hours ago. I believe the issue is you expect the CHECKDB to show the good value at the end of the run versus being updated when it was executed. That shouldn't be a problem. 

Part of this I explain above but let me go a little further. First, thank you for the comments explaining a few things about what you did and the environment. Sadly, since this was production, immediate and drastic measures were taken. Also, there was no hint of any errors in the errorlog, which is odd. What I believe happened is the database was brought online and Query Store was initialized. There was a failure of some sort (which we won't know now, why) on the initialization of Query Store which was suppressed from the errorlog (again, no idea why). Since the initialization fails, the queries are never signaled and thus you get into the state you are in. Again, this is my assumption of what happened based on what I looked into. The interesting item is that there should be a timeout period, where if Query Store isn't loaded within said timeout - everything should just continue on and not wait any longer. You should have easily hit this timeout - which makes we not exactly sure this is what happened... again, though, not too much to go on. There are a few extended events that we can monitor to see if this truly was the case or not. 

This is going to depend on the environment. I, personally, hate finding a server someone setup using a local account and asking to get access to network resources some time in the future, among other issues. 

Yes, it is possible but will require SQL Server 2017 (I'd also apply the latest CU, which at time of posting is CU4). Please note that without a domain, SQL Authentication is the only available authentication type and there is also the additional endpoint setup of using certificates. If you'd like to read more about read-scale availability groups (Not for HA or DR) the Docs articles are a good start. I've also blogged about it when it comes to things like read only routing. 

The values can be found in the XML plan as Max has proposed in the comments. I can confirm it has been this way since at least SQL Server 2008. Kudos to @Max Vernon and @Joe Obbish. The MSDN BOL Technet Docs description isn't super helpful. 

Overall, there is no need for a cluster IMHO. If you need HA at the DR site, then do it - but if you don't, I don't see a purpose for it as the stand alone will work just fine. The reason I say this, is that they will be two distinct clusters which makes it a little hard to justify. 

Its pretty easy to do this but I wouldn't recommend it. To implement a single "document" table for all possible documents and parents, you just abstract the id into a "parent_id" column, and add a "parent_table_name" column. There are a lot of issues with this approach. Some are: 

Yes there are significant benefits to data normalization if you are willing to do the work to achieve and maintain it. The two fundamental benefits to normalization are: 

Not only could it be, it must be if we want to ensure the data stored in the database remains consistent with the rules we have identified in the real world! 

This was created using Oracle SQL Developer Data Modeler - a free download - which is a great tool if you want to continue the process into database design and creation. Primary Keys You have identified primary keys. Technically, an ERD does not require you even consider keys. If you think of the ERD as a tool to model the business, its not important to determine at this point exactly how you will uniquely identify an occurrence of each entity. Instead, you can assume that at a later point you will figure this out, and instead focus only on the entities, their relationships, and their attributes. It is a good idea to note which attributes are unique for each entity as this will help choose keys. Once you complete the diagram and iterate upon it with the business, you can go back and complete it by choosing the right keys. In my diagram I only noted that the two numbers mentioned as unique in the requirements were unique on the diagram using a U beside the attribute name. Entity Roles Now to address the dark green highlighted supervisor I mentioned earlier. The requirements stated that "a department has an employee who manages the department." They go on to state "We also keep track of the direct supervisor of each employee." So what we have here is an entity - a person - who is playing different roles. The employee might be a manager. The employee might be a supervisor. So how do we resolve this? First, we need to decide if the manager and the supervisor are the same thing! Is it the case that the manager of a department is also the supervisor of the employees assigned to it? If yes, the solution is simple. A new entity type called Assigned Manager can be created with a one to many relationship from department to it and from employee to it, with a single attribute of the start date when that employee started managing that department. Then, each employee's supervisor is assumed to the the manager of the department they are assigned to. If no, then we can add a recursive relationship from employee to employee to represent the the supervisor and the employee. This is the simplest approach but does introduce a mixing of roles in the same entity type. A better approach is to add a new entity type called Assigned Supervisor with two one to many relationships from employee to it - one to represent the supervisor and the other to represent the supervised. If there is a date when the assignment began and ended this could be added as well. Conclusion Hopefully this, coupled with the description of the process used to develop it, will be a good learning tool for you to understand how you get from a description of a business process to a draft ERD. I say draft because it is just a starting point. Once completed, it serves as a communication vehicle to validate the requirements, find holes in them, uncover new ones, and so on, before any programming or even system design begins. Remember that doing a great job building a system that implements requirements that were mis-understood will be a failure and it will be much cheaper to modify an ERD than a working system! References Two really good reference for ER diagramming are Steve Hoberman's Data Modeling Made Simple, which is a great overview, and David Hay's Enterprise Model Patterns, which gives a great in depth look at common patterns you find when analyzing organizations. Both of these references give much more detail on describing relationships using verb and prepositional phrases, identifying and non identifying relationships, and strong and weak entities - concepts I didn't address. Fabian Pascal's Practical Database Foundation Series is also excellent, and has a great first paper that is the perfect compliment to the books as Fabian describes the entire process of determining data requirements. Remember that ER diagrams can only show keys and references, whereas in fact there are many more kinds of business rules that can be explored and implemented in the ultimate database. 

Again, depends. Will this ever be failed to? Maybe just in the instance of a true regional DR event where RTO trumps RPO? Will there be any reporting on this new replica? If so, does the reporting require a specific data freshness SLA? Synchronous will obviously have an overhead and your round trip time (rtt) latency will play the largest factor for long distance AGs, followed next by disk. Asynchronous will obviously not have those problems in terms of reducing the throughput of the workload but will still feel it in terms of data freshness if it is used for reporting. Remember, if it is a planned downtime you can change the availability mode before performing any activities. 

That depends, but my gut instinct with the data you've given is - no. Sure, you'll potentially save some of that space as a plan stub will still take memory just not all that much (compared to your 1 MB plans). So you'll net memory, we get that. However, we don't know how many of those plans were executed a single time, and then some point later while still in cache executed again. This brings up the question about compilations/recompiles and the cpu utilization to go along with it. If you have a good bit of headroom then it may be a trivial issue (pun intended). If your server isn't under memory pressure, I would not expect to see too much of an improvement in terms of "performance" depending on how you want to classify that. If you're swapping and having some slight memory pressure this could alleviate it for a few moments - though upping the VM memory would have the same effect at a much faster implementation without negative side effect cost. 

Present the disk to all possible owners of that disk (nodes) Bring the disk online from the first node that will own it Bring the disk into the cluster - it'll now reside in available storage Move the disk into the resource group for SQL Server Set the SQL Server resource to DEPEND on that new disk Double check sys.dm_io_cluster_shared_drives to see that SQL "sees" the disk. If it doesn't, one of the steps above was not successfully completed. 

It'll be branded stationary if, like you've already stated, that 10% or more of the inserts aren't ascending. If 100% of your inserts were as you say... then you might not have this problem, until of course you delete but then it would go back to unknown. Here is a repro of your issue: 

Unfortunately (or fortunately) there is nothing built in to do this. I don't want to give you any ideas but you'd need to have a middle layer between the application and the database server. This intercepting application would need to understand TDS (which is the protocol used with SQL Server) and allow you make changes to (or, again, not) the TDS packets to change the queries on the fly. If you made this, you should sell it :) 

Aside: One thing I will gloss over in the interest of space but that is extremely important is truly understanding all of the candidate keys. In this example we have RESPONDENT_ID as the PK. Are there other data elements that uniquely identify a respondent? Just designating a surrogate key upon which we declare all the other now "non-key" columns functionally dependent does not change the functional dependencies of those non-key columns on some other column or set of columns which also form a candidate key. Another Aside: There is a CURRENT_IND column. That makes the PK of RESPONDENT_ID very suspicious. If this means what it usually means, then the table could potentially have multiple rows per RESPONDENT_ID, many of which are not current, and one of which is. Then the real candidate key is something like RESPONDENT_ID plus CREATE_DATE. This makes the example too complex to analyze though so I will ignore it. But in reality this is a real problem that implies you really have two types of things here - current respondents and former respondents. This gets into temporal concerns which are complicated to address. Let us assume the following functional dependencies for the second table: 

I would create a distinct emails table and use a surrogate key to instantiate the reference of the email address to the person and to the user. The model would look like this: 

Department - Number, Name Employee - Name, SSN, Address, Salary, Sex, DOB Location - ??? Project - Number, Name Dependent - First Name, Sex, DOB, Relation 

In looking at the table in the exercise these conditions are met. Once a table is normalized, it can be further normalized as a way to eliminate certain redundancies which occur due to functional dependencies between the columns. By functional dependency we simply mean that the value of one column always determines the value of the second. This is based on the mathematics of algebraic functions. For example, the value 2, when plugged into the algebraic function 2x + 3 will always yield 7. 2NF is defined to mean that each non-key column in the table is fully dependent on the entire key - thus no partial key functional dependencies. The Exercise When inspecting the example table we can easily see is dependent only on and not on . Each time for example we see a value of 295 for we see a value of Miracle Holidays. The repair is exactly as you have done - split and out into their own table where there can be a single unique row for each with the corresponding . Now is fully dependent on the key - . simply stays in the original table and there is no need to redundantly split it out into a third table. A Caution The exercise is asking you to infer the functional dependency of on based solely on inspection of the data. While in this simple example it is obvious, in the real world you cannot simply assume a functional dependency exists based on the existing data. What appears to be a functional dependency might turn out to instead be coincidental. Because of this, in the real world you would always ask the expert in the domain of knowledge the table represents what the functional dependency should be. Getting More Information Normalization is a very complex topic and I have glossed over many important concepts. CJ Date has written an entire book with respect to it called Database Design and Relational Theory: Normal Forms and all that Jazz. While definitive, it is hard to grasp all the formalisms. An excellent reference that presents the formalisms in language more easily understood by common practitioners is Fabian Pascal's Practical Database Foundation Series. Studying both of these references - first Fabian's and then Date's - will give you all the information you need to master normalization as a repair procedure.