The script creates certain objects, and it will put them in the current schema. Oracle wishes that these objects be created on SYSTEM schema, hence the requirement. There are two Oracle schemas, SYS and SYSTEM, because they have two purposes. In short, SYS truly is the "special" user in respect to database internals, while SYSTEM works normally. The SYS is the owner of the database, the only user with access to X$ tables, the only user which acts as SYSDBA (internally), that bypasses logon triggers, and that, for some obscure reason, cannot obtain read-consistent view of data. As a schema it holds crucial Oracle stuff, mainly the data dictionary. The SYSTEM is fairly normal schema with DBA privilege (database administrator, not the same as SYSDBA), only that it is built-in and contains some additional (but also quite important) Oracle stuff. You shouldn't put your own stuff in this schema, if not instructed so by Oracle. 

Verify here you have had only a single control file and not two or three of them. If you have had more than one control file, then this instruction is not for you. 

XE does not support Streams, as officially documented. SE and SE1 support Streams but without redo capture. EE and PE (Personal Edition) fully support Streams, and also support Logical Standby. 

This is a full database import which also includes tablespace definitions. Find out from the original Solaris database what are all the target tablespace names and sizes. Pre-create the tablespaces manually using the datafile paths of your choice () before performing the import. You will see which you can ignore. 

The next thing in terms of cost is an OS (machine) and its IP address. You cannot afford a separate system for every TNS name. So crmdb.mydomain.local is not the only name for the IP address; the same IP address would have more names, like financedb.mydomain.local. Your OS admin would decide how to do this best, and how to determine the main hostname of the OS. They have the same problem with many other systems - multiple names referring to one OS - so they should have a solution at hand. The only people who are confused now are DBAs and OS admins, they see multiple hostnames leading to the same IP address. But users don't care about that and are not confused by that. (By the way this approach is coherent with SCAN. ) The next thing in terms of cost is either one of the two: an Oracle instance or "administrative cost of separating schemas out of instance". The tradeoff is for you to decide. 

I fixed the problem by changing folder permissions on the Postgresql ODBC driver folder and giving read/execute access to Network Service. Because MS DTC runs under network service, which I confirmed by looking at the service properties. That got rid of the error! 

I have installed postgresql 9.3 on ubuntu 14.04 using "apt-get install postgresql". Everything was going well until I discovered that I do not have access to the "pg_ctl" commands. Installing the postgres-xc literally breaks postgresql server. The package links become broken. I cannot re-install and have had to revert my VM and keep working without pg_ctl. My question is, to get access to this package, should I start from scratch or uninstall and re-install postgresql? What should I have installed in the first place? Alternatively, is there something wrong with my path? (I am new to postgresql and have been tossed into it for work purposes) 

I am changing a database's compatibility mode from 90 (SQL Server 2005) to 110 (SQL 2012) and I am wondering at what point would I see errors from breaking changes if there are any. I ran Upgrade Advisor 2012 which spotted several Stored Procedures that needed updating. An example was: "In SQL Server 2005 or later, column aliases in the ORDER BY clause cannot be prefixed by the table alias." However, when I run this stored procedure in SQL 2012 (without having made changes) it doesn't show any errors. Also, when I changed the database compatibility level from 90 to 110 there were also no errors. Nor when I restored the database. If I am not seeing errors anywhere, I am hoping that Upgrade Advisor caught everything. Is there any other ways I should check for compatibility errors? Also, how is it possible that this stored procedure ran successfully even though Upgrade Advisor told me it would fail? Thanks :) 

What is a good way to manage developers' data changes? I am using RedGate SQL Source Control to monitor changes to tables and stored procecures, so that when we do a production build, I can include all the changes in a SQL update script. My question is: what is a good way to consolidate and monitor changes made to the data? (As this is something RedGate Source Control cannot track) For example, new records included in lookup tables? And new records included in tables used for routing procedure calls. Thank you! 

Beware the OS authentication is done by client machine, not by the database server. I think that within reasonable pessimism to Oracle tools you could expect that it boils down to this: database server receives a TCP connection from whichever IP address that might pass the network path, and that connection just claims "I promise I've done OS authentication and, trust me, this connection is made on behalf of OPS$JohnSmithCEO." Neither the database client or database server is given OS password for additional OS authentication! And you can't even trust that this connection came from a valid Oracle software. It can be man-in-the-middle in reality. It's worse than telnet. Whatever "Windows-specific" checks are done by Oracle, they seem contrived and you can't really trust they are complete and secure. 

No. If the datafile has been occupied by a partition of a table, you need to ALTER TABLE EXCHANGE PARTITION with some empty dummy table. Then drop the exchanged table. Then rebuild the invalidated indexes. This works if you don't have foreign keys to the table. This is quite a troublesome operation. Then you are free to drop the corrupted datafile. 

You don't want your test database to retain the same internal id number as the production database. Hence it would be best to use RMAN's command DUPLICATE, because it sets a different id (and also a new database name). This command is specifically designed to do what you require. 

Because for some strange reason, although regexp_like looks like a function on a first glance, it is not a function in reality, but is a condition (an integral part of SQL syntax). But, more confusingly, it is a function if used in PL/SQL: 

Jay, this isn't officially documented, so I'm speaking only from my own experience. In RMAN, the command is synonymous with . Also the command is synonymous with . I'm not sure about ; it might be also a synonym of , although I've never tested the latter. In your scenario, the former would work as expected. In particular, neither or require all the datafiles to come from "a single backup of database" (from a single run of ). Actually, RMAN doesn't even have a concept of "a single backup of database". 

In some versions of Oracle, you cannot put space between the beginning-of-line and the WALLET_LOCATION keyword, and you must put space between the beginning-of-line and the definition of a wallet. Your snippet indicates that you failed at one of these things. I think they removed this silly requirement starting from some Oracle version, but better safe than sorry. Good: 

The DLL is in fact that location, so the registry seems to be pointing to the right file. The ODBC driver is 64bit and so is my OS. "File=%2" is pointing to something on the d drive, which doesn't make sense to me, since d drive is a DVD. MSDTC is running... what am I missing? I have toggled Linked Server Properties "Enable Promotion of Distributed Transactions for RPC" to both "True" and "False" and this doesn't change the issue and does not produce a different error. Otherwise, scouring the Internet has brought me nothing. Last thing to point out, my query isn't actually doing any updating - it is just pulling data. So I'm not sure why MSDTC get's invoked in the first place... 

I currently have Mirroring set up between two production SQL Servers. One of these servers is a hardware SQL server (principal) and the mirror is a VM (on different server). I need to migrate these servers to Availability Groups (HA cluster NOT using shared storage). None of this is my choice. I would personally prefer two VMs, or completely identical systems. My question is: has anyone come across issues for AG/clustering between a hardware SQL Server and a VM SQL Server? Or is there any issues that you can imagine with that setup? I have had no issues with Mirroring between a hardware & VM. As long as the servers are very similar (ie: same version of SQL Server, same RAM given, data files are named & stored in the same locations, etc). My concern is AG utilizes windows failover clustering - an unfamiliar territory for me - so I'd like to know if there's anything additional to look out for. 

I am trying to understand best uses of PostgreSQL replication and how it works so I can troubleshoot in a production environment. I am having a hard time understanding the differences between these 2 types of replication in terms of (1) Configuration (2) How the 2 servers Master/Slave perform in each scenario Replication on PostgreSQL (9.2+) is essentially XLOG files of 16MB in size (depending on frequency settings for creating each file) are being created on Master and sent by some method to the Slave. My Setup (for purposes of this question) Configuration of Postgresql.conf on Master archive_command= 'rsync -av %p postgres@[SlaveIP]:[wal_archive_folder]/%f' Configuration of Recovery.conf on Slave to read log files restore_command = 'cp [wal_archive_folder]/%f \"%p\"' primary_conninfo = 'host=[MasterIP] port=5432 user=postgres' My question is what part of this configuration makes this "streaming" replication versus "log shipping"? My master is configured to use rsync to send logs to the slave (is this log shipping?) My slave is configured to be able to connect to the master in recovery.conf (is this streaming?) Second part of the question: What is happening? I understand there is another protocol on PostgreSQL via WAL_sender & WAL_receiver. But I am not clear if this is used for streaming only and if so, how is the rsync being used in the Master? :) Thank you!! And sorry if this is an obvious question. I've been doing a bunch of reading blogs/books but having a hard time understanding. Postgres wiki is so in depth that it takes a long time to get through it all (and I have deadlines) 

I'm trying to get a linked server to ServerA created on another server, ServerB using "Be made using the login's current security context" in a domain environment. I read that I'd need to have SPNs created for the service accounts that run SQL Server on each of the servers in order to enable Kerberos. I've done that and both now show the authentication scheme to be Kerberos, however, I'm still facing the error: 

Is there a "best" way to automatically restart SQL Server on a regular basis? I read that I could create a batch file to net start, net stop the agent and service in a batch file and schedule that, but I was curious if there was a better way of doing this. Please nevermind how this is indicative of a larger problem of operation, I've been arguing this for a month and we've ended up here. Thanks :). 

I actually found a resolution to this. When setting up the linked server to Server B, I used a Windows authenticated account as the remote login (although MSDN says it must be a SQL Server authenticated login) and I was able to successfully connect to linked server B, while connected to server A from a local workstation, while NTLM was set as the authentication mode. 

I was recently tasked with moving some databases to a new server. I created backups then restored the databases to the new server and it looks like the database users were retained, however there's no corresponding logins, prohibiting the users from accessing the new server. All of the users are domain users. Is there any way to iterate over the existing users in a database and create a login for each? 

I'm creating a linked server from ServerA to ServerB, both of them are SQL Server 2008 servers. I need to create the linked server using a domain service account so that anyone will be able to utilize the linked server. ServerB only accepts Windows authentication, so I've added the service domain account to ServerB with proper permissions. I've verified the permissions by logging in with the service domain account to ServerB and everything looks great. But when I create a linked server on ServerA with the script below, I get a "login failed for user". Is this script incorrect, or am I missing something else? 

In SQL Server, I have a user in a particular database and I've been asked to grant them access to all of the non-system views of the database only. I believe this can be done by editing securables of type view and granting select on each one, but there are many, many views. Is there a more efficient way to accomplish this?