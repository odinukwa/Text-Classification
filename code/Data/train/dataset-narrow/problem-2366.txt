Motivation: a boring metro trip, and a poster that shows the "locations" of the stations at the resolution of one minute of travelling time. Here we are minimising the error that people make if they use the poster to look up the travelling time between stations $i$ and $j$, averaging over all pairs $i<j$. 

Something to consider: try to figure out if you want to present your work at a scientific conference, or if you would prefer to publish it in a scientific journal. Pros of conferences: 

There are 65536 integers of type X. There are 4096 integers of type X + Y. These are precisely those integers of type X that begin with sequence '0000...' 

Given two strings $x$ and $y$ and integers $k$ and $K$, I would like to solve the following problem: 

Above, I have focused on questions that are specific to distributed computing. There are also open questions in distributed graph algorithms that have nontrivial connections to open problems in theoretical computer science in general. For example, non-constant lower bounds for the congested clique model are a big open question in distributed computing; it was recently discovered that such lower bounds would also imply new lower bounds for ACC. 

Is it possible to construct sequent calculus for nonmonotonic/defeasible logic? If it is possible then those logics can be encoded in proof assistants which require sequent calculus for logic to be encoded (Isabelle, Coq). I am aware of the several papers from the late nineties by Ana Teresa Martins et al. about sequent calculus for paraconsistent logics. the labelling of propositions (given and derived) is used in those papers. But those papers seems to be unpublished, although they contain bright ideas, the quality of language may be a bit higher. It seems to me that those ideas are not caught by others and developed futher. 

Hoare logic can be used for proving program correctness (e.g. for deriving correctness statements for the whole program from the statements for the individual commands or constructions; good summary is $URL$ The question is - is there any line of research where Hoare-style reasoning can be made for programs that creates and deletes objects. One thought can be that this reasoning can be simulated by defining large pool of objects which have (for the purposes of correctness analysis) additional state attribute with values from the enumeration {not-created, created, destroyed}. But I guess that better approach should exist. Are there any references or keywords for furher search into this matter. 

There is no clearly visible pattern, but it does not have a uniform grey colour. Part of it is most likely caused by the imperfections of the grey card, but I would assume that most of it is simply noise produced by the scanner (thermal noise in the sensor cell, amplifier, A/D converter, etc.). Looks pretty much like Gaussian noise; here is the histogram (in logarithmic scale): 

Fairly often, the interlibrary services of university libraries can provide copies of papers published in hard-to-find books and journals. As Peter Shor suggested in comments, you can also try to contact the authors. Usually, authors will be happy to send you copies of their articles – after all, researchers write papers in order to make their results as widely known as possible. 

While exploring different techniques of proving lower bounds for distributed algorithms, it occurred to me that the following variant of Ramsey's theorem might have applications – if it happens to be true. 

Conference submission = arXiv ID. Nowadays, a conference submission is usually a PDF file. Instead of asking people to submit PDF files, we could easily ask them to just enter the arXiv identifier of the work that they want to submit. [Of course the arXiv paper must be formatted like a conference submission: approx. 10 pages of text + all missing details in an appendix.] This would be relatively straightforward to implement, and it would have numerous benefits: 

There is known connection between classical and modal logics and type theory (lambda calculus), but are there connections between nonmonotonic logics (e.g. defeasible logic) and type theory (lambda calculus)? Maybe HoTT provides some generatlization where such connections can be found? If such connections can not be established then widely available proof assistants (Isabelle, Coq) are not useful for nonmonotonic logics, aren't they? 

I am trying to translate code from one programming language into another (to be specific - from RuleML to Drools, but other pairs can be expected as well) and it would be nice to know - whether there exists more general workbenches for this. E.g. one can formally specify programming language, that is clear. But are there any tools that can be used for capturing the formal semantics for formally specified language. It is obvious that the operational semantics is the most suitable for industrial programming languages. And - if semantics is specified then the translation from one programming language into other can be done at the meta-level. I guess, that this maybe too hard for C-to-Java translation, but it should work for more modest, recent languages that are created by academia. I have heard about Maude, but I guess that there can be something more. My experience is that Google can not replace bibliographies or suggestions from community. Thanks. 

Let's look at the future some 30 years from now. Let's be optimistic and assume that areas related to machine learning keep developing as quickly as what we have seen in the past 10 years. That would be great, but then what would be the role of traditional algorithmics in such a future? Here by "traditional algorithmics" I refer to the usual process that we follow in TCS: formalise a well-defined computational problem, design algorithms for solving the problem, and prove formal performance guarantees. Now what are the application areas in which we must use traditional algorithm design and analysis also in the future, and it is highly unlikely that any advances of machine learning will make traditional algorithmics mostly irrelevant? At first this may seem like a silly question: Of course we will need to be able to do sorting, searching, indexing, etc. also in the future! Of course we will need to be able to do Fourier transforms efficiently, multiply large matrices, find shortest paths, solve linear optimisation problems! But then again, once you start looking deeper at the applications in which we traditionally use the algorithms that we design, it is not at all clear that the traditional algorithm design and analysis is the right answer to such problems: In applications related to search, usually we are interested in finding something that is a close match for a human being in some vague ill-defined sense (e.g. semantic similarity), not something that is optimal in some mathematical sense (e.g. minimum edit distance). In applications related to route planning, usually we are interested in finding routes that are good based on examples (e.g. other people prefer it), not routes that are optimal in some mathematical sense (e.g. shortest distance or cheapest price). And once you have some vague, ill-defined human component in the picture, it might be the case that we are better off trying to teach the computer to produce good answers based on examples, instead of trying to let a TCS researcher to come up with a formal computational problem that we can tackle by means of traditional algorithm design and analysis. So what are the application areas (preferably real and direct industrial applications) in which it is absolutely clear that what we have been doing in algorithmics in past is also going to be the right way (and the only possible way) of making progress in the future? Algorithms that are used as subroutines in machine learning techniques looks like an obvious future-proof candidate, but this heavily depend on the particular machine learning technique that we use, and as we have seen in the past ten years or so, this might rapidly change. 

There are lot of applications of Horn clauses (notable examples include use of rules in cognitive architectures and knowledge bases, as well as use of rules in business rules programs). Are there formal methods that can help specify and verify Horn clauses. Is there semantics of Horn clauses. One can perceive Horn clauses as something similar to programming code of traditional programming languages and therefore one can expect the denotation, operational and similar semantics as well for Horn clauses. 

It is known that many logic problems (e.g. satisfiability problems of several modal logics) are not decidable. There are also many undecidable problems in algorithm theory, e.g. in combinatorial optimization. But in practice heuristcs and approximate algorithms works well for practical algorithms. So one can expect that approximate algorithms for logic problems can be suitable as well. However the only reseach trend along these lines I have managed to find is the max-SAT problem and its development was active in nineties. Are there some other active research trends, workshops, keywords, good references for the use and development of approximate methods for modal logics, logic programming and so on? If automated reasoning is expected to gain prominence in the future applications of computer science then one will have to be able to go beyond undecidability constraints of logics and approximate methods or heuristics can be natural path to follow, isn't it so? 

Edit: Now a natural question is whether there is an approximation algorithm – for example, can one always find a feasible solution $s$ that is within a constant factor of the largest feasible solution. However, this does not seem to be the case. It seems that it is actually NP-hard to find any non-trivial solution (i.e., a solution $s \ne \emptyset$), at least for certain values of $p$. The construction is a bit messy, though, but I can try to work out the details if needed. Anyhow, it seems that to get anything positive (with provable performance guarantees), you need to relax your constraints a bit. 

Typically, whenever we discover a nontrivial reduction $A \to B$, it falls in one of the following categories: 

While it is not a typical core-TCS journal, it seems that ACM Transactions on Database Systems sets a very good example: 

If you take this perspective, then it often turns out that in order to model distributed systems, it does not really matter that what kind of computational power your nodes (or processors or computers) happen to have. Typically, you can simply assume that each node is just a state machine (often it is enough to have a reasonably small number of possible states, such as $O(n)$). The machine changes its state based on the messages it receives. Usually you are not that interested in how the machine changes its state. It might be a Turing machine, but this is not really that relevant. For example, if you take a (reasonable) graph problem $X$ and study the distributed complexity of solving $X$ (e.g., the number of communication rounds required to solve it), the way you model computation at each node does not usually affect the answer. If you analyse it first by using Turing machines, and then by assuming an arbitrarily powerful oracle, the answer is typically the same. You can add non-uniform advice and it does not change anything. The "bottleneck" is that you cannot gather information quickly. In $T$ communication rounds, no matter what you do, each node can only have information regarding its own radius-$T$ neighbourhood. You could have an arbitrarily powerful processor at each node, but what good does it do if the processors do not have any information to process! Hence using Turing machines as the starting point in order to model distributed systems sounds a bit unnatural to me: if this is an irrelevant aspect, why build everything on top of it? On the other hand, in parallel computing this would be natural (except that the model is usually something like PRAM instead of Turing machines). 

Proof nets are interesting essentially for three reasons: 1) IDENTITY OF PROOFS. They provide an answer to the problem "when are two proofs the same"? In sequent calculus you may have many different proofs of the same proposition which differ only because sequent calculus forces an order among deduction rules even when this is not necessary. Of course, one can add an equivalence relation on sequent calculus proofs, but then one has to show that cut-elimination behaves properly on equivalence classes, and also it is necessary to turn to rewriting modulo, which is quite more technical than plain rewriting. Proof nets solve the problem of dealing with equivalence classes by providing a syntax where every equivalence class is collapsed on a single object. This situation is anyway a bit idealistic, as for many reasons proof nets are often extended with some form of equivalence. 2) NO COMMUTATIVE CUT-ELIMINATION STEPS. Cut-elimination on proof nets takes a quite different flavor than on sequent calculi because commutative cut-elimination steps disappear. The reason is that in proof nets the deduction rules are connected only by their causal relation. Commutative cases are generated by the fact that one rule can be hidden by another causally unrelated rule. This cannot happen in proof nets, where causally unrelated rules are far apart. Since most cases of cut-elimination are commutative one gets a striking simplification of cut-elimination. This has been particularly useful for studying lambda calculi with explicit substitutions (because exponentials = explicit substitutions). Again, this situation is idealized since some presentations of proof nets require commutative steps. However, their number is much smaller than in sequent calculus 3) CORRECTNESS CRITERIA. Proof nets can be defined by translation of sequent calculus proofs, but usually a system of proof nets is not accepted as such unless it is provided with a correctness criterion, i.e. a set of graph-theoretical principles characterizing the set of graphs obtained by translating a sequent calculus proof. The reason for requiring a correctness criterion is that the free graphical language generated by the set of proof net constructors (called links) contains "too many graphs", in the sense that some graphs do not correspond to any proof. The relevance of the correctness criteria approach is usually completely misunderstood. It is important because it gives non-inductive definitions of what is a proof, providing shockingly different perspectives on the nature of deductions. The fact that the characterization is non-inductive is usually criticized, while it is exactly what is interesting. Of course, it is not easily amenable to formalization, but, again, this is its strength: proof nets provide insights that are not available through the usual inductive perspective on proofs and terms. A fundamental theorem for proof nets is the sequentialization theorem, which says that any graph satisfying the correctness criterion can be inductively decomposed as a sequent calculus proof (translating back to the correct graph). Let me conclude that it is not precise to say that proof nets are a classical and linear version of natural deduction. The point is that they solve (or attempt to solve) the problem of the identity of proofs and that natural deduction succesfully solve the same problem for minimal intuitionistic logic. But proof nets can be done also for intuitionistic systems and for non-linear systems. Actually, they work better for intuitionistic systems than for classical systems. 

The answer to such question can have practical applications as well - e.g. it can help to define some kind of formal semantics of the "business rules" (declarative programming paradigm that is used more and more in business applications, e.g. IBM ILOG, JBoss Drools, Oracle Business Rules, PHP and Python also have their own rules engined) and going further - this can be useful for cognitive robotics as well. 

I am acquinted with the basics of such notions as logic programming, monotonic and non-monotonic reasoning, modal logic (especially dynamic logic) and now I am wondering - does logic programming provides anything new to any logic? As far as I understand, then (at least in dynamic logic) the logic programming refers to the formalization of state transitions (by actions, i.e. - logic programing can be understood just as logic about actions). But the same notion "logic programing" seems to be in use even in domains, where there is not state transition, just exploration of one state (or set of states - in case then the initial set of premises are vague enough to describe more than one state). It seems to me that some authors simply use the notions "logic programming" for describing the procedure how to evaluate the query (I am reading currently about defeasible logic programming). But such procedure (although practical indeed) does not add anything new to the underlying logic. E.g. there is notion of "rational closure" (e.g. used for adaptive logics; just the consequence set for some set of premises) which should contain all the possible knowledge about state and therefore all the possible results of "logic programing" (if it is indeed perceived just as state exploration, derivation). So - the question is - does logic programming provides anything new to the logic and does every logic (to be completely understood and readied for applications) need to have its own logic programming? Maybe I am just missing the point... Just for reference I find the following works interesting about this subject, if there are more along this line, then it would be great to hear!