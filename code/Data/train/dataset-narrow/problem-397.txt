If you want to switch to a different database just issue . oraenv (if /usr/local/bin is in the PATH) and give it the ORACLE_SID of the database that you want to manage. Most databases will be accessed using a client from an other machine/VM. In that case you also might want to start the listener: 

Oracle redo files are written in a sequential fashion. This is what gives the best performance because the write head is always on the correct location to write again, as long as the disk is not shared with other files. As soon as the disk - or more precisely - the write head is shared with a datafile, the head has to make more movements to satisfy the random IO pattern that is mostly found for datafiles. The cylinders numbering of disks start at the outer edge of the disk. See Oracle Solaris 11 System Administration: Administering Storage Devices for some info. While formatting the slices you can point them their location on disk. If you really want high performant redo files, give them their own dedicated disks and stripe them for best performance. Don't forget that the archiver is also going to read the redo after it has been completed .... Managing files on separate disks is something of the past. Since years Oracle preaches SAME. Stripe And Mirror Everything. See Optimals Storage Configuration Made Easy Add the disks to an ASM diskgroup and let ASM handle the file placement. 

I would suggest that you indexes to job_id assuming there are many events for the same job and event_date assuming that there are many events on the same date assuming that there are queries which use these columns for filters. However see answer by @ypercube below which has a better explaination for indexes PS: Updated to remove erroneous suggestions which are left below for reference: b) In the query I can see that you are restricting by a date and time yet event_create_date is a date field, you can improve the comparison by using TO_DAYS(event_create_date) = TO_DAYS('2012-12-18 00:00:00') c) Additional advice on indexes - add indexes to those columns with a lot of repetitive data, indexes are not very effective if the data varies widely ... (That is why @Rohan requested for more details on the data) 

In addition to the above changes to mysqldump, I would like to suggest that you try Percona Xtrabackup www.mysqlperformanceblog.com/2012/02/27/announcing-percona-xtrabackup-1-9-1/ which seems to offer more backup options, including parallel streaming which brings up to par with other mainstream databases. 

We maintain database scripts as part of our application codebase which is maintained under version control. However we use different "processes" for development and production code Development we maintain the following scripts: 

Since your plan is to re-use schedules I would recommend a change in your thinking as follows: a) Each service location can have one or more schedules - called location schedules (basically this is an association between a location, a predefined schedule and other data) b) Each location schedule can have a priority (which allows you to create a custom schedule that overrides the default schedule) c) Each location schedule has a start date and an end date when its active - which allows you to define a custom schedule which is active for only a specific period of time d) A schedule has a name and description e) A schedule has one or more schedule details - day of week, opening time, closing time I have included an explanatory data model (MySQL) 

With these and some additional databases/tables and jobs and time you can build out a basic monitoring system (but it isn't pretty) these are tools for DBAs; unless you are good at BI stuff you will struggle to find time to produce useful business friendly stuff from it, though the Ozar sp_blitz app is pretty dang cool. After spending around a year doing the free thing and resolving plenty of issues (but not getting much buy in) I was able to make it clear, after a major issue, that perf monitoring software was a priority, and we were going to buy it come hell or high water. After demoing the previously mentioned clients, I chose DPA because management could easily consume the results, though I definitely have client licenses for SQL Sentry Plan Explorer Pro (1000% worth the money) and really liked using the server version, it just didnt grab them the same way. I also tried getting SQLNexus working at one point but I ended up working a lot than I was interested in, it may suit your needs. 

To explore a bit more about the question and comment, I want to note that generally when normalizing your goal (among others) is to reduce duplication. A naive example of your table might be everything you have but adding things like the CSV list you mention, this ends with a very wide table that has multiple areas of duplication and which will require you to manage and clean your data (and is generally bad for performance.) A solution to this problem is to simply project the data into another table and provide the keys to join back to the original. Eg: 

Normally for a straight forward data migration it is enough to know that all data is complete, valid and accessible for the application. Typically subtle differences may pop-up with some data-types like those with date, time, binary fields and long text fields. In a straight forward data migration, data from table a field a goes to the new system in the same table, the same field. A good check to validate this could be done by computing a MD5 check-sum on all fields of all tables, both in source and target tables. Any difference here shows there is a problem in the migration procedure or software. Make sure to have the same formats in place for the MD5 computation; for example, make sure that the date formats are the same on both sides. The application has to be tested, that's for sure. What the strategy should be is hard to tell, every application has it's own challenges. If all logic is known and standardized, it could work by making a simple test application that accesses all different field types and see how the interaction goes. In the end a regression test will be needed. 

Looks like a serious error. What is writte in the alertlog file? First thought is that the file is damaged. How exactly did you drop the tablespace[s]? how exactly did you perform the export? how exactly did you re-create the tablespace[s]? how exactly did you re-create the schema? how exactly did you perform the import? You should have dropped them using 

Do you have an index on account_id? The second problem may be with the nested sub-queries which have terrible performance in 5.0. GROUP BY with a having clause is faster than DISTINCT. What are you trying to do which may be better done through joins in addition to Item #3? 

I think the question of duplicated data depends on the meaning of the relationship between the two users. For example if the relationship is who is following who, then User 1 can follow user 2, but that does not mean that User 2 is following user 1. However if the relationship is where there are no duplicates, for example if the two belong to a team or work together, then the above model would not necessarily work since you need to relate them through another entity, team in my example 

a) Attributes - define the attributes for all devices (anything goes in this table) columns: id, name, description b) Item Attributes - defines the allowed attributes for a specific device - itemid, attributeid c) Item Definition - defines an item say Black Berry Torch 4500, Iphone 4S, Iphone 3S etc - id, name, description, categoryid (if you want to add categories like mobile phones, switches etc) d) Devices - the individual devices - id, itemid, inventorydate, deactivatedate, serialnumber ... (basically all other attributes for a device) If you want to track any other information on device transcations then you can add more tables linked to the device as you need. 

The database[s] and software are on SAN/NAS storage. In that case you can remount them on the new server with the same path's. Because the OS version is different issue a relink for the Oracle binaries. Don't forget to also copy /etc/oratab, /etc/oracle and /usr/local/bin files etc. Backup and restore are not bad at all but assuming there is some pre production environment with similar setup, you can use that to verify the procedure before the production migration. This is the quickest option (remounting). 

If you happen to be using the same platform (cpu/OS) it is fairly simple to get this up and running, if you know from which version of Oracle this backup is AND the backup is taken in the correct way. In your case, since all you have is ctl and dbf files, it should have been a COLD backup. If all of the above is not in place: forget it. If all of the above is in place: 

IF you just want to be able to restore a few tables and the volume is not to high, why not just make copies of them? You do need to set up good auditing and security around all your tables to prevent data loss, in the event that users did manual updates on tables that are going to be restored because of a process failure or something similar. It could be that despite that failure, the user modifications are still needed and valid. Best would be to prevent any user changes in the period where processes run that could cause the need for a restore. 

The easiest method is (probably) to uninstall and this time run the command line installer again with a different set of flags and it will install to a different location unless you are talking about components not listed below. From: $URL$ Proper Use of Setup Parameters Use the following guidelines to develop installation commands that have correct syntax: 

Edit:Adding a little flavor text as requested. Basically what is happening is you are choosing to aggregate one of the values (in this case the counts of the salary) so that it "rolls up", and the group by generally indicates which value you want to do the rolling up by. It makes some sense to say "I want to group by the number of employees" but you are actually trying to express "I want to return the number of employees grouped by department." 

Simple answer, you grouped by your sum. Solution is simply to remove that from your group by statement, eg: $URL$ 

You are grouping by the thing you are counting, not by the department name. Change your group by to: 

If you find yourself saying "nullable columns" before you have written code I generally think you need to normalize more(as always, it depends.) If you want one system to maintain, the last option seems the most relevant, but "splitting them into two tables" is not really where you would want to take it. if we want to track basic things like Players, Games, and Sport type you would simply add intersection tables between the relevant things you want to store. 

Add a movie genres table - as a movie can have one or more genres Split up the movie_showing table as follows: 

Instead of object_id and object_type columns, why not use a primary key for each object type to be commented on, but maintain the object_type (for partitioning later) so that you have question_id, comment_id, and answer_id. For each row of data, two of the columns will be null, but it is easy on your ORM (single table inheritance with a type qualifier). Your queries will also join to the correct FK, I am not sure of the space usage for this, but I expect it to be very performant since the joins do not need additional filters on the where clause for different types as well as complex "SQL join acrobatics" 

I follow the following rules for primary keys: a) Should not have any business meaning - they should be totally independent of the application you are developing, therefore I go for numeric auto generated integers. However if you need additional columns to be unique then create unique indexes to support that b) Should perform in joins - joining to varchars vs integers is about 2x to 3x slower as the length of the primary key grows, so you want to have your keys as integers. Since all computer systems are binary, I suspect its coz the string is changed to binary then compared with the others which is very slow c) Use the smallest data type possible - if you expect your table to have very few columns say 52 US states, then use the smallest type possible maybe a CHAR(2) for the 2 digit code, but I would still go for a tinyint (128) for the column vs a big int which can go up to 2billion Also you will have a challenge with cascading your changes from the primary keys to the other tables if for example the project name changes (which is not uncommon) Go for sequential auto incrementing integers for your primary keys and gain the inbuilt efficiencies that database systems provide with support for changes in the future