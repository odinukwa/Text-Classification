Fields such as bioinformatics and computational biology could fundamentally change the way we live since it may lead to revolutions in drugs manufacturing, disease prevention and treatment, and possibly synthetic biology. Currently, there is no formal language in TCS that explains how DNA code controls the functions of biological systems. There is no theory in TCS that determines the functions of some DNA segment. Research progress in those fields would have huge impact on medicine, biology and therefore society. 

The article, The Chaos Within Sudoku is published (11 October 2012) in Nature by the same authors you site in your question. An excerpt from the abstract: 

I'm always intrigued by the lack of numerical evidence from experimental mathematics for or against the P vs NP question. While the Riemann Hypothesis has some supporting evidence from numerical verification, I'm not aware of similar evidence for the P vs NP question. Additionally, I'm not aware of any direct physical world consequences of the existence of undecidable problems (or existence of uncomputable functions). Protein folding is NP-complete problem but it appears to be taking place very efficiently in biological systems. Scott Aaronson proposed using the NP Hardness Assumption as a principle of physics. He states the assumption informally as "NP-complete problems are intractable in the physical world". 

As for question 2, there are at least two examples of $NP$-completeness proofs that involve computer-assistant. Erickson and Ruskey provided a computer-aided proof that Domino Tatami Covering is NP-complete. They gave a polynomial time reduction from planar 3-SAT to tatami domino covering. A SAT-solver (Minisat) was used to automate gadgets discovery in the reduction. No other $NP$-completeness proof is known for it. Ruepp and Holzer proved that pencil puzzle Kakuro is $NP$-complete. Some parts of the $NP$-completeness proof were generated automatically using a SAT-solver ( again Minisat). 

The result of Khuller and Vazirani shows that finding the lexicographically first four-coloring for planar graphs is $NP$-hard. 

Graph Isomorphism is natural problem which is most widely believed to have intermediate complexity between $P$ and $NP$-complete. GI can be thought as deciding the existence of an isomorphism between two sets of node pairs. I'm trying to develop a notion of isomorphism between sets of triples. We are given two hyper-graphs $G1$ and $G2$ such that the set of hyperedges ($E_1$ and $E_2$) consists of triples of 3 nodes $\{t_1, t_2, t_3 \}$. We say that Hyper-graphs $G1$ and $G2$ are isomorphic if there is bijection $f$ from $V_1$ to $V_2$ such that pair $\{u,v\} \in E1 $ if and only if $\{f(u),f(v) \} \in E2 $. We say $\{u, v \} \in E $ if there is a triple (hyperedge) $\{ u, v, z\} \in E$ for some node $z$. Is it $NP$-complete to decide the existence of such isomorphism between two hyper-graphs $G1$ and $G2$? 

One major impact is that quantum computing gave us a polynomial time algorithm to solve the integer factoring problem which is not known to be efficiently solvable on classic computers. This partially motivated the development of public key cryptography that does not depend on the hardness of factoring problem. Efficient quantum algorithm for an NP-complete problem would have the greatest impact on TCS since it would give us insights into the fundamental differences between the computational power of classical and quantum algorithms (assuming classic deterministic algorithms can not solve NP-complete problems in polynomial time) . 

For unweighted problems, The Nearest Codeword problem in coding theory is known to be very hard to approximate. It is NP-hard to approximate to within a factor $n^{ \Omega(1)/ \log \log n}$. Another hard to approximate problem is the longest path problem in directed graphs. Björklund, Husfeldt, and Khanna showed that longest path in directed graphs is hard to approximate to within better than a $n^{1-\epsilon}$ factor for any $\epsilon \gt 0$. Also, Approximating Closest Vector in a lattice to within almost polynomial factors is NP-Hard. Finally, minimum independent dominating set of a graph is not approximable within $n^{1-\epsilon}$ for any $\epsilon \gt 0$. 

This problem related to Mandelbrot set seems to be $NP$-complete: $M=${$(c,k,r) |$ In the sequence $P_c (0),P_c (P_c (0)), P_c (P_c (P_c (0)))...$ of first $k$ complex numbers, there is a subset $T$ of complex numbers such that the sum of the real parts $\gt$ $r.k$ and the sum of imaginary parts $\gt$ $r.k$} $r$ is real number and $k$ is an integer in unary. Here is a geometric interpretation, since each $P_c^i(0)$ is a vector in 2D, we want to find the maximum size square obtainable by the summation of a subset of two dimensional vectors. A promising reduction is from (optimization) number partition problem where each partition has the same number of elements (in the optimization version, we minimize the difference between the two sums). 

For two computational problems $A$ and $B$ in complexity class C (let say $NP$), the existence of a reduction $A <_m^L B$ computable in class (say $L$) implies that $A$ is not computationally harder than $B$ and $B$ is at least as hard as $A$. I am interested in information theoretic characterization and implications for the existence and non-existence of $<_m^L$-reductions between computational problems. I'm aware of concepts such as computational entropy and hardness amplification but my limited understanding of these notions, which I guess originated from cryptographic applications, does not seem to shed light on the information theoretic characterization of $<_m^L$-reductions. What is known about information theoretic characterization and consequences of the existence and non-existence of such reductions? Can notions such as computational entropy and hardness amplification help in answering my previous question? 

Some problems have variants that appear to be harder. For instance, Graph Automorphism (GA) problem has quasi-polynomial time algorithm ( by Babai's GI result). However, the fixed-point free GA problem is NP-complete. Also, Factoring decision problem has sub-exponential time algorithm and it is not believed to be NP-complete. Meanwhile, a variant of factoring problem is NP-complete. Subset-sum and partition problems are weakly NP-complete problems since they have pseudo-polynomial time algorithms. I am interested in their variants that are strongly NP-complete. 

This paper, Computational complexities of Diophantine equations with parameters by Tung, proves co-NP-completeness of a variant with parameters over natural numbers. 

This is a partial answer $ABS$ is the class of NP-optimization problems solvable in polynomial time to within an absolute additive error from the optimal solution. The following two problems are in $ABS$. -A famous problem that has additive error approximation: 3-coloring of planar graphs is $NP$-complete while every planar graph is 4-colorable in polynomial time (by the four color theorem). -Every cubic graphic is edge 4-colorable in polynomial time but edge 3-coloring is NP-hard. Maximum independent set problem is not in $ABS$ unless $P=NP$ 

No, there is no known sparsely certified $NP$-complete languages. The class that you are describing is known as $fewP$. It is widely believed that $fewP \ne NP$, So, No $NP$-complete problem is known to be in fewP. (It is impossible unless $fewP=NP$). 

Arvind, Köbler, Mundhenk, and Torán introduced the notion of time-bounded nondeterministic instance complexity. Based on a quick reading, It seems they use Kolmogorov complexity measure that depends on the size of shortest nondeterministic TM. They were able to prove the existence of hard to prove Tautologies under a notion of hardness based on nondeterministic instance complexity. Vikraman Arvind, Johannes Köbler, Martin Mundhenk, Jacobo Torán, Nondeterministic Instance Complexity and Hard-to-Prove Tautologies, 

$P \ne NP$ if and only if worst-case one-way functions exist. Reference: Alan L. Selman. A survey of one-way functions in complexity theory. Mathematical systems theory, 25(3):203–221, 1992. 

According to this blog By Reza Zadeh, training a neural network to produce correct output even for just two-thirds of the training examples is computationally hard: 

A star system is a family $F$ of n subsets of n-elements set $S$. A star system is graphical if there is some graph $G(V,E)$ such that $F$ is the family of vertex neighborhoods in $G$. It is $NP$-complete to decide whether a given star system is graphical. 

The input is a cubic graph. The problem is to determine whether it is a cycle permutation graph or not. 

A counter intuitive result from complexity theory is the PCP theorem: Informally, states that for every $NP$ problem $A$, there is an efficient randomized Turing machine that can verify proof correctness (proof of membership in $A$) using logarithmic number of random bits and reading only constant number of bits from the proof. The constant can be reduced to 3 bits. Therefore, the randomized verifier needs to read only three bits from the proclaimed proof. 

This paper may be useful for you: Andrade, Lucena, and Maculan, Using Lagrangian dual information to generate degree constrained spanning trees 

References: 1- Deep learning and the information bottleneck principle, Naftali Tishby and Noga Zaslavsky 2- Opening the Black Box of Deep Neural Networks via Information, Ravid Shwartz-Ziv and Naftali Tishby 3- Conference talk video: Information Theory of Deep Learning by Naftali Tishby 

Paul Lemke Steven S. Skiena Warren D. Smith, Reconstructing Sets From Interpoint Distances, gave backtracking algorithm that runs in time $O(n^n \log n)$ for the beltway reconstruction problem. As far as I know, this is the best known. The exact complexity of the problem is not known. It is not known to be in $P$ and neither known to be $NP$-complete. 

Your problem is NP-complete. Two-colorable perfect matching (which is NP-complete even when restricted to cubic planar graphs) is reducible to your problem. Take $H_1$ and $H_2$ to be perfect matchings each of $|V|/2$ nodes. Then your problem is to find 2-coloring of $V$ such that each color class induces a perfect matching. 

There could other types of failures. For example, some of the processors (e.g. under broadcast or multicast protocols) may become overloaded and would not be able to process all incoming messages. This results in making the processor appear offline to some processors in the distributed system. 

A decision problem has good characterization if it is in $NP \cap coNP$. Many natural graph problems have good characterizations. For instance, Kuratuwski's Theorem gives good characterization of planar graphs. Konig's Theorem gives good characterization of bipartite graphs. Tutte's Theorem gives good characterization of graphs that have perfect matching. Euler's Theorem gives good characterization of Eulerian graphs. All these recognition problems have polynomial-time algorithms. Is there a natural graph problem that has good characterization but not known to be in $P$? A pointer to a survey of such problems would be appreciated. 

The famous Isomorphism Conjecture states that all NP-complete problems are isomorphic via polynomial-time computable and invertible bijections (reductions). Padability is the only property that I know which can be used to show P-isomorphism between two languages (The most direct way is to present the reduction). My intuition suggests that two p-isomorphic languages are two different labelings for some language and should be related via permutations.