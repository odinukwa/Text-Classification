I have a two node RAC database server hosting 8 databases. I have 3 scan listeners running. 2 in node 1 and one in the 2nd node. All the 8 databases are running for different applications. Are these 3 listeners sufficient to manage the work load for different 8 databases? So only the service names will be different for the databases for the respective applications to talk to right? Are there any best practices to have more scan listeners run on different ports or or is it recommended that way? 

I need a drop a database which is a 2 node RAC configuration and I will also be drooping the disk groups as well completely. What is the right way to drop the database + diskgroups? Will it be ok to shut down the database and then drop the diskgroups including contents thereby dropping the database as well? In this case, the cluster will still have entries of the database which is dropped? 

AC does not require any changes on my application code. This is one of the reasons i'm going with AC because i'm using thin drivers where I cannot use TAF and I do not want FCF as that requires application code changes making it RAC aware. I believe all that is required is to only create a service for AC on my RAC server. 1.(a) Does enabling FCF add advantage in addition to AC? 

Should the service-name mentioned in TNS and the service which I create for TAF using srvctl be the same so that client need not make any changes in tns at their end? 

Chances are that the reason your script runs from the command line but fails from cron is that your DB2 environment is already initialized (i.e. via your .profile) for the interactive session, where it is not when called by . Here's a suggested modification: 

If you are asking if there's any way to determine whether existing rows that have a value were updated as a result of an existing trigger, then the answer is no. There is nothing inherent to triggers that would allow you to identify them as the source of the data in the column. The trigger could contain some logic to identify itself as the source of the value, but that would be stored in some other column or table. 

The message you include from your db2diag.log shows that there is a potential space problem (i.e. the file system holding your data is filling up). The monitoring data here shows that you are ranging between 89% and 94% over a 90 minute period with increases and decreases – this is probably associated with system temporary tables in the database. It may be worthwhile to increase the size of the filesystem(s) holding the data since you're getting very close to running out of space. This space utilization, however, has nothing to do with how much I/O activity (IOPS) the database is generating. Generally when you have an I/O problem where you see a much different usage pattern than normal. You mention that normally there the system is generating 300 IOPS, but lately it's been generating 5000 IOPS. Assuming that the I/O activity can be traced to the file system(s) holding DB2 data, then you need to monitor the database during these periods of high I/O to find out what queries are active and are causing lots of physical I/O, and then do some investigation to figure out why. You can use the utility as well as many other monitoring tools to figure this out. At that point it's a tuning exercise. Are your bufferpools too small? Was an index dropped? Or is the cause a new query that needs to be reviewed / tuned? 

Is it possible to restrict sensitive information from users having sysdba role? Does Data redaction help us in achieving this? I know we can restrict particular schemas to prevent from viewing from sensitive info by using redaction.. but what about sys user? Can that user still view everything? 

I'm having a Recovery catalog database 12c version. While I'm trying to register the target database (11g) with the catalog I get the following error: 

I have run mysqlcheck on all the databases as well which returned 'OK' for all the tables. Not sure where to start debugging. Any ideas? 

I have MySQL community version 5.5.50 running in linux having 4-5 databases on it. Requirement is to upgrade to Percona Server 5.6.31-77.0. What are the pre-requisites that I need to check before upgrading? Are there any common errors that i'm likely to face? I'm simply planning to backup the databases. Remove the MySQL binaries. Install the new Percona Server and restore the databases. 

I'm new to SQL anemometer. I need to know whether this tool would be able to query to the source database and analyze the slow query logs on real time or not? 

The process ID is getting changed and mysql instance is continuously getting restarted. my.cnf entries: 

Oracle 12c database comes with default Unified auditing enabled even if we set it to FALSE which has ORA_SECURECONFIG policy enabled by default. Apparently, the audit options for that particular policy captures many default options such as Alter and create statements. My question is, by default for any new installation, mixed mode is enabled which means that the traditional auditing is enabled and also Unified auditing is enabled as well both capturing the same audit_options. Why is it that way? What is the point of Mixed mode? When by default tradionally Oracle audits certain user actions, why should ORA_SECURECONFIG should also audit the same default user actions and write to AUDSYS schema? And if I need to keep only Unified auditing, then how to disable traditional auditing? Should I change audit_trail from DB to NONE? 

I'm also not quite sure why you are posting this without just attempting it, but yes, this will work without problem. The only issues you might have are: 

You don't specify what "first" means (or provide any table structure or example data), but let's assume it's the earliest completion timestamp: 

There was no change that I am aware of in Fixpack 5 to the requirement that the registry variable must be set to for the functionality to work. (Unsetting it in FP5 still results in an error if you try to use / ). You may want to look to see if is set in the global registry. Notice that running alone does not show : 

This example adds a calculation showing how much the logrecordnumber has grown since the week before. Doing the rest of the calculation (extrapolating to when LSNs might run out) is left as an exercise for the OP. 

This is really an odd question – it's hard to image a scenario where you would need to create multiple databases simultaneously. Are you coming from a different DBMS (like MySQL or SQL Server), where the concept of a database is more like a schema in DB2? 

Sounds like you are not properly deallocating objects like , , etc., so your application is holding open statement handles in the database. This will eventually exhaust "CLI Packages" in the database, resulting in the error. The proper solution is to make sure that you close these as soon as you're done reading them. You may also find documents suggesting that you rebind to increase the number of CLI packages at the database level, but keep in mind that this is just a band-aid. 

I have a SQL server A and B which is production databases. The DR of production 'A' is production db 'B' and DR of production db 'B' is production db 'A'. DR feature that is used is log shipping. All in the same location. Now, what are the very critical issues that I could face in case of disaster? 

I'm sort of new to SQL Server so bear if the doubts are silly. I have configured log shipping for my DR purpose. SQL Server 2012 is the version. Today I encountered the following error in my log shipping instance. 

I have 4 servers running MySQL 5.6. A,B,C and D. A and B are Master and slave whose default port is 3306. C and D are running in non-default ports 3360. A and C are in Master-Master replication mode. C and D are in Master-Slave setup. Now I need to change the ports on A and B from 3306 to 3360, what commands I need to run and in what order to have B,C and D back in replication? Is the below steps right? 

I have enabled MySQL master-master replication with 2nd master acting as read-only. Recently I checked the out the size of the databases in both master and slave and I found that database size in two databases is 1mb less than the Master. Does it mean that the setup which I made for master-slave was not done properly or the replication is not happening properly? Also, how can I make the changes so that it remains the same in both master and slave? Thanks! 

Online backups require that the database be enabled for rollforward recovery. However, this is not the default when you create a database. In order to do this, you need to set the database configuration parameter. Once you have done this, you'll need to take one offline backup (i.e., no users can be connected). Once you've completed these steps, you'll be able to run online backups as you wish. You may want to spend some time reading the Data Recovery section of the DB2 Database Administration guide to help familiarize yourself with DB2 Backup/Recovery. 

You are correct in your assumption – there is nothing done at the database level that would track the source of a row. You would need to modify your history table definition and trigger logic to add this kind of tracking. 

For the second problem you may want to look at using the function so you can calculate the delta in the log record: 

No, it's not possible to specify a default location. The best way to do this would be to write a shell script (which specifies your location of choice) to handle your backups. You can have DB2 remove old backups (using the database configuration parameter with proper values for and ), but that will only occur after a successful backup. So you'll need to ensure that there is enough space for the backup to complete successfully. 

This is the classic question / fight over whether monitoring should have a monitoring agent installed on each server, or if it should be "agentless". With an agent installed on each server, a process/script/etc. wakes up every so often, collects data, and reports it back to a central location (i.e. monitoring server). With an agentless solution, the central server polls each monitored server/database/etc on a schedule to collect the information. Cacti and Nagios typically use this solution. There are pros and cons to each method. There is not necessarily a best practice for which method to use, discussion usually just results in a holy war (similar to Emacs vs. vi, DB2 vs Oracle, ...). 

Is it recommended to have the MySQL database location as the default one which is /varlib/mysql ? Are there any risks in having the databases in the specified location? 

I would like to store the database size for a particular database on daily basis on to a table which would help in understand the growth of the database over a period of time. 

With the above setting, I know I can get to know what action has been performed (alter,create,update,etc) but will I be able to fetch the SQL statments (all DML operations) with the exact SQL statement fired from that schema from the audit records? Let me know how? If the above is not possible, what is required to fetch the exact SQL statement that is run by the schema. 

I have two node RAC servers hosting 4 databases running on 11.2.0.3 version in RHEL OS. The underlying LUNs assigned to the ASM is not currently multipathed, meaning the disk sequence would change when my server node reboots causing my ASM not to mount the diskgroups and therefore I have planned to create new disks using ASMLib with newly added LUNs which would be multipathed. I got new LUNs and I created new disks using ASMlib as well in node 1. Once created in node 1 and scanned the same in node 2 for disks to be discovered in node 2. The disks are discovered in both the nodes. After creating diskgroups when I try to mount them, in node 1 it is getting mounted, but in node 2 it fails giving 

The first trigger is necessary to handle the outer join (i.e. where a user hasn't made any calls yet). The second trigger inserts the calls, and also deletes the records inserted by the first trigger (as they are no longer necessary once a record in PHONECALL exists). You would need to decide how you want to handle deletes in either table (either through triggers or foreign keys with cascading deletes). Again, I want to stress that this is a possible solution, but I would only use it as a last resort. Databases are designed to perform joins efficiently, but this might require some proper design and tuning. The table structures above should absolutely not require this kind of solution, but I'm working on the assumption that this is not your actual schema. I am only describing this solution because I can imagine a few edge cases where something like this solution might be necessary. 

There is an API called that you can use to read transaction logs and collect information about deleted records. IBM also sells a product called Recovery Expert that will help you do this and generate undo SQL statements. 

If you are asking if you can determine if the table in question has any triggers defined, then yes - you can either 

The reason for this behavior is that DB2 must have enough space for the largest possible LOB locator in the column for the row in the table. INLINE LENGTH is not like a VARCHAR where space is used only according to the amount of data. It's like CHAR – the size you specify is reserved in every single row of the table. From the documentation (scroll down to description of ): 

Currently I have set log_bin_trust_function_creators=1, and it doesn't take in effect after restart. 

I have two servers. One is DB server another is RMAN Catalog DB server. From my RMAN Catalog, I connect to my remote DB server and I run the backup. Where will the backup files be stored? Will it be in my RMAN catalog db server or the source db? 

After each day's full backup gets over, I make sure that the above mentioned path only contains that day's backup pieces in that path and the older backups will be moved to another folder 

I have two physical servers, hosting 6 two node RAC databases running in 11.2.0.3 version (both grid and database). I'm told to have the grid software in 11.2.0.4 version and the databases in the same 11.2.0.3 version since there could be future plans to upgrade the database to a higher version. Plan is to create new grid software (11.2.0.4) and create a new ASM instance with new luns and create diskgroups with the new luns, create a new database and then export/import from old database to this new database. So that the new database will have the Grid and ASM running in 11.2.0.4 and database in same 11.2.0.3. Perform the same activity for all 6 databases, post that uninstall 11.2.0.3 software and binaries, resulting in the required result wherein only one ASM instance will be running (11.2.0.4). Is the above mentioned activity feasible? I'm not in a situation to upgrade 11.2.0.3 grid to higher version as I cannot afford downtime. But then the above mentioned activity will have two ASM instances running at the same time for some period. I can add/clear doubts if the mentioned activity is unclear.