What approaches have people found work best to getting the AI to vary the paths they take, look convincing and surprising? 

I'm just getting started build my first game with XNA (I'm experienced with C# but not games). I'm building a pretty simple top down 2d shooter. I read this tutorial on using a color based collision system and it sounded really cool to me. $URL$ It means I can quickly make levels just using any graphics program and not have to define my scenery (walls, trees etc) in terms of collision boxes etc right? However I can see that going down this path means that perhaps the calculation for determining whether fast moving objects like bullets intersect walls etc becomes more difficult potentially because you cant' do basic geometry intersection type calcs. Is that right? Am I going to regret going in this direction if my game gets more complex over time? Worth just investing in the creation of a level editor to define my scenery in terms of geometry? Any advice for a noob very much appreciated! 

When one enemy spawns and generates a path to the center, temporarily increase the cost of all the nodes on that path, then slowly decrease them back down over time. Then the enemy AI that spawn later will be forced to take a wider path. The above approach will lead to AI just taking a wider and wider path though and still be very predictable. So I thought I'd also introduce a number of intermediate goal nodes around the map. When the AI spawn they randomly pick one of the intermediate goals and head there first before heading to the center of the map. Combining this with the above approach of increasing the costs might look pretty good? 

Render the skeleton line segments into a volume Expand the volume belonging to the skeleton by iteratively filling voxels whose neighbours belong to the skeleton Extract the resulting surface using the marching cubes (or marching tetrahedra) algorithm 

Firstly, in your call to you pass . This should be one of , , or (depending on the type of ). I imagine you would get an returned from . Secondly, in the line: 

I found that a loop summing each element of two separate arrays and storing the result in a third performed exactly the same as a version where the source data was interleaved in a single array and the result stored in a third. I did find however, if I interleaved the result with the source, the performance suffered (by around a factor of 2). If I accessed the data randomly, the performance suffered by a factor between 10 and 20. Timings (10,000,000 elements) linear access 

I suspect your vertex data is being converted to float (and back again). You should use to upload integer vertex data without converting to floating point. From the documentation: 

Call You can then write to the built in variable in your vertex shader. This affects the size of the quad that will be created for the point. If you have a vertex attribute to specify the size of each point you can simply set from that. The point size is specified in device pixels. 

I want to create some fairly complex 2d predefined paths for my AI sprites to follow. I'll need to use curves, splines etc to get the effect I want. Is there a drawing tool out there that will allow me to draw such curves, "mesh" them by placing lots of points along them at some defined density and then output the coordinates of all of those points for me? I could write this tool myself but hopefully one of the drawing packages can do this? Cheers! 

I have a top down 2d game where the AI spawn at the edges of the map and run towards the center. I'm using A* and a node mesh to do the pathfinding. Right now, the AI spawn at a point on the edge of the map and all take the same path which is the shortest route to the center. Now I want them to be more surprising and interesting and take different paths to each other. I can immediately think of two ideas for doing this but wanted to know if there are other ways or better ways that people often use? 

I'm currently working on AI and, as stated above, I'm having trouble figuring out how to keep the AI from getting stuck on each other. More specifically, I'm working on two different behaviors for the AI: 

I've started rewriting some of my code (npcs, navigation and such) and I need some help with my new navigation system. Basically the way it works is: the game generates a random dungeon. For example: 

I'm working on a small maze puzzle game and I'm trying to add a compass to make it somewhat easier for the player to find their way around the maze. The problem is: I'm using XNA's draw method to rotate the arrow and I don't really know how to get it to rotate properly. What I need it to do is point towards the exit from the player's position, but I'm not sure how I can do that. So does anyone know how I can do this? Is there a better way to do it? 

I've been looking for articles or tutorials on this and I can't seem to find any, so I thought I'd ask here. I've created a bullet class in my game that is added and removed through a list. If I'm going to make these bullets work the way I want them to I need to be able to do two things: One, I need to check if the bullets are colliding with each other and then remove the two bullets that are colliding. I've tried doing it by updating the bullet list with a for statement and then checking for collision with another for statement, but that occasionally crashes the game. Two, I need to be able to check how far away every other bullet in the list is from the current one. So how can I go about doing this? 

I think you would be better off re-sizing your textures to the nearest power-of-two. You won't need to adjust the texture coordinates as they are relative to the size of the texture image (although you may need to set v = 1 - v to account for the different origins). The problem with wrapping the texture coordinates yourself is that filtering will not work correctly at the "seams" where a value goes suddenly from 1 -> 0. 

The second parameter should be: (where is whatever you have declared as. Generally it is a good idea to check after more or less every GL call. You can do this using a macro that logs any errors in debug mode, but does nothing in release mode. 

This is non-conforming behaviour by the Nexus 7 (Adreno GPU). You say "uniforms are not meant to be randomly accessed", but according to Appendix A of the spec: 

Persuade your image editor to multiply your colour channels by the alpha channel (it may then not display properly in the editor), or otherwise pre-process your image files to the same effect. Multiply the colour channels by the alpha channel when you load the image. Multiply the colour channels by the alpha channel in the fragment shader. Switch to your old blending factors when using those textures 

I'm in the process of moving a 2d top down game I've been working on into a proper rigid body physics engine like Farseer. Up until now, I had just hacked together my own physics code where needed. I'm trying to learn the proper way of doing things here. What is the proper way to make your AI follow a set path once you have made them rigid bodies inside of the physics engine? If I have a path of navigation nodes on my map that I need the AI to follow, previously I would just move them along the path manually by calculating the next position they should be at for the next time step and manually setting them to that position. But now they are rigid bodies and subject to collisions and any forces that may hit them and knock them off path. So to make the AI move I believe I should now be applying impulses/forces to them? I should no longer be manually setting their position each frame. So I think I need to go from a deterministic world where I force the AI to strictly follow a path to a non-deterministic world where they could get knocked about in any direction if hit and I simply nudge them towards the next node in the path to make them move. Is that right? Is that how other people do it? This raises some questions about how to then avoid your AI getting stuck on corners of scenery now that they aren't walking a precise path, how do you guys handle that type of thing? Or is it better to somehow mix the two and still have your AI follow a fixed path by setting their position manually, and only react to other forces under certain circumstances you can easily control? Thanks for any advice guys. 

The AI form a group of four, determine which one of them is the strongest and bravest (whichever has the highest number), make that one the leader and then snake behind the leader. This doesn't workout because the leader ends up getting stuck on the others as they attempt to walk through him to form a line. The AI moves from one room to another. This doesn't work out because the one changing rooms will get stuck on ones that are doing other things or even on other room changing AI. 

When it finishes that, it goes through the nodes and connects the nodes that have a clear line of sight to each other. Like this: 

Okay, through some trial and error, I've come up with a system that works. The ai chooses a target node, then uses this method: 

Blue is walls, white is floors, orange is doors and green is the designated "center" of that room (usually used only for A* pathfinding.) I kind of already had an idea of how I could find the available space, but decided to ask because I wanted to know if there was an simpler way of doing it. 

In addition to bogglez answer (you don't need to re-render shadow maps which haven't changed between frames), there is also the possibility to render several lights at once if each light has its own shadow map. This is more of an issue with a forward (i.e. non-deferred) renderer where you want to avoid churning through all the geometry in your scene for each light. You can also group lights together according to which geometry objects they intersect and re-use a small pool of shadow maps if you can't afford to have a shadow map per light. 

You are now using blending factors which assume pre-multiplied alpha (usually a good idea), but some of the textures don't have pre-multiplied alpha. You can either: 

No (at least not necessarily). The cache controller should, in most cases, be able to deal with reading from more than one contiguous array efficiently. The important part is to try where possible to access each array linearly. To demonstrate this, I wrote a small benchmark (the usual benchmark caveats apply). Starting with a simple vector struct: 

So the solution is to check the voxels which share a face with the voxel containing the sphere's center first, then those voxels which share an edge, then those which share just a corner. This ensures that e.g. moving across a flat floor the entity will always be pushed up before it collides with the hidden edges of voxels making up the floor. If this seems like an arbitrary hack, ask yourself if it is possible for a sphere to collide with the corner or an edge before it collided with a face of a neighboring voxel. Your pseudocode above should work if you add a step to sort the voxels found in by Manhattan distance from the voxel containing the entity's center. 

I'm working on a dungeon generator and I'd like to be able to connect rooms through other rooms. What I currently have set up is two dictionaries and a map (an int array.) The first dictionary holds room templates separated by height, width and type (so: .) The other dictionary holds rooms that have already been added to the map and uses rectangles as keys. I'd like to be able to find the maximum rectangular area that could fit into an empty space, then select a template room accordingly. So what would be the simplest way for me to do that? Edit: Admittedly, I was a little vague before so here's a little more information. My current method for creating the map follows certain steps: 

Load in room templates from an xml file. Create the start room in the middle of the map using a start room template with a random size. 

With the black representing walls. After the map generates, it goes through the map and finds all the outward corners and places a node on them. The end result is something like this: 

So it works like this: enemies use the nodes to navigate their way around the map. When they want to do things like wander, they choose a random node (one they can see) and go to it. Every node has a list of the nodes it's connected to, so from there they choose another node and head towards that one. The enemies keep a list of the previous three node locations, so they don't wander in circles (if there's only one node connected to the current node, they head back.) The whole thing is based on line of sight, basically. However, there are some limitations, namely: movement. Creatures in my game have tile based movement and can only move in cardinal directions. I also need enemies to try to avoid each other and change direction based on stimuli (such as a change in "bravery" level, causing them to rush the player, retreat or stand their ground.) Given all that, how can I calculate the ai's next move? Edit: To be more clear: I want to know how I can have the ai calculate the direction it will move in next. So I need to know what kind of formula the ai needs to determine which tile is the best to get it to it's destination. Something that takes into account the target, other enemies (if there are any) and the bravery level of the ai (which will change player interactions and make them group with other enemies when their bravery is low and they're alone.) Edit 2: Okay, let me change the question and add some information. My game is turn based, so I need to make steering behaviors work in a system where it can only move once per update (one tile length) and can only move up, down, left and right. I'm not sure how to do this, can you help me?