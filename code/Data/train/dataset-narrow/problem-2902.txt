It is generally assumed that the mass does not matter and they bounce up to the same height. This is because the coefficient of restitution, which lets you calculate the velocity change after the collision, does not depend on mass. The velocity right after the collision determines the height that the ball will move up to, independent of the mass (just like how mass does not affect free fall duration). So, for games, it's safe to assume that they will end up at the same height. However, in real life, it can be hard to make the two have the same contact properties and balls with different masses may end up bouncing to slightly different heights. This paper can provide further insight. 

The best way to increase your chances of being employed as a game developer is to get your hands dirty and actually work on the development of some small games. You say that you are "interested in programming games", which makes me believe that you haven't developed any games, yet. I advice you to stop thinking and start doing. However, the first order of business will not be to specialize in any of those fields. Let me elaborate. When I interview you for a job, if you tell me you are interested in programming games and your specialty is game AI, I will immediately ask you which games you worked on. If the answer is "none", I don't care about how many books you read or how you believe you are an expert in game AI. You won't be hired, period. To obtain game development experience in a specific field such as AI, you need to seek out others that are developing games and try to get into their teams. You want specialization, remember? A generalist can develop the whole game, but a specialist needs to work in a team. However, you will again be asked for previous work for someone to trust you with the AI of their game, let's say. Even if you will work free of charge. So, it's a chicken and egg problem right there. Basically, you can't just specialize in anything with no prior game development experience. Here's what you need to do: you need to forget about specializing and start contributing to the development of a game in any way you can. You can develop a simple game using a game engine such as Unity, or ask someone else developing a game and they will tell you what they need done. Then you'll figure it out and that will give you experience. Once you have something to show, not only you will have a better chance of joining a team, but also you'll have a better idea on fields that you can specialize in or that you don't like to work on. Trying to make a decision right now without that experience is like marrying someone without getting to know them. You cannot know if you will hate coding game AI unless you have some relevant experience. 

As others have already explained, your question is based on a false premise: that most 3D games don't allow you to move. I think what you actually meant is "Why do you control movement in most VR-first games with a controller, rather than with your body?". The answer is not to do with technical limitations, but human factors. For most games, it's completely unnecessary - and indeed makes for frustrating gameplay - to control your character with your body rather than a controller. Nobody wants to clear out their room to play a VR FPS only to get annoyed with wires and obstacles. There would be too many compromises needed (unless you literally had a huge hall to play in) to make it so that the controls don't interfere with the game. That doesn't mean that there aren't games where you can at least move a bit by actually moving your body. The controller is your primary mode of ambulation, but you can still look around corners by moving your head etc. On that front, the commenters on your question are right; most games actually do let you do that. Regardless, your reasoning is still wrong. Think of a VR headset as just a set of double monitors; anything that can be done on your actual screen can be done here too. IMU input from a headset is equivalent to WASD + mouse or a controller. It has nothing to do with caching a scene in VRAM or limitations of your graphics card or anything of the sort. The actual UX pitfalls with VR headsets are resolution, FOV, and latency. People might decide to limit how you control your player based on those for example, to prevent motion sickness among other things. 

Try placing this code in FixedUpdate() instead of Update(). Physics motions happen in FixedUpdate() and Update() is synched with draw calls. The two don't always happen at the same frequency. Most of the time this is the cause of the jittery motion. So, if you are going to affect the motion of a physically-simulated object, you need to do it in FixedUpdate(). Now, why does it happen only when you do the normalization? Probably it's because the extra processing time required by Normalize() causes the Update calls to lag more behind FixedUpdate. The fact that it acts differently in different computers also supports this idea. How much out-of-sync Update and FixedUpdate get depends a lot on the available CPU cycles, which tends to be nondeterministic. 

Unity animations do not animate changes to mesh details. Animations are only pos-rot-scale of nodes. Unity's skinned mesh renderer uses animations of nodes to deform meshes. How Unity deforms your mesh can be slightly different than Blender's deformation. Unity is trying to do the right thing given the armature animation and the mesh weights for the nodes of the armature. How are you animating the normals in Blender anyway? The armature in Blender isn't supposed to deform normals like the first screenshot you gave. 

First of all, when you say "creating" a tileset, I presume you mean that you want all these tile transitions pre-baked into textures. I'd recommend against that as the number of transitions you need increases quadratically as your number of tile types increases, and isn't worth any extra efficiency usually. What you want to look into is Texture Splatting and do this all runtime. I can't think of anything in particular that would give you extra efficiency when it comes to cell types, except the usual voxel optimizations like storing voxels in an octree and optimizing that, occlusion culling, frustum culling, LOD, optimal mesh generation etc. On that last one, as the saying goes, premature optimization is the root of all evil, but if you end up having some extra FPS, you could try rendering your cells to look more "detailed" with algorithms like marching cubes / marching diamonds (think the 3D Worms games' destructible terrain). Edit: Here's a good read on voxel terrain meshing. 

1) Yes your observations are correct. 2) The standard global XYZ coordinate system makes sense when you think in terms of a first person shooter, when you are looking through the eyes of a character in the scene with a blank(identity) transformation matrix. Like it would when you draw a coordinate system on a piece of paper, X points to right and Y points upwards. According to the right hand rule (x->thumb, y->index finger, z->middle finger), Z points towards you. 3) It wouldn't be wrong, but it would be a diversion from standards. There are three problems that I can think of at the moment: (a) Let's say one day you want to use a physics library that uses the standard coordinate frame. If you did not follow the standard, now you have to think about the transformation that takes you from your world to the physics world. Can get annoying when you want to fix a bug. (b) When you want to share code with someone, or bring someone over to help with development, they have to get used to your convention. (c) When using standard 3D models, you always have to have a transformation above them to prevent them from looking sideways. Now to add to question 2, it is sooo useful to think of X, Y, and Z as not just three letters, but as right, up and backwards. Every character in the scene has a local coordinate system attached to them, and in their local coordinate frames X is always right, Y is up and Z is backwards. Once you have this, now you can make sense of vectors that you print out, or write your algorithms in a way that makes sense. Let's say you have two characters A and B, and you want to do something if one of them is facing the other. You can simply find B's location in A's coordinate frame (Ta^-1 * p_b), look at the vector you get and see if Z(backwards) is negative and X(right) and Y(up) are small, because that vector tells you how much backwards, right and up B is with respect to A. 

The next step is animation. To do this, you just need to play around with the "r" value and change the radius. You don't need to worry about clamping color values between 0.0 and 1.0 because that happens automatically anyway. All together it would look something like this: 

Yes, step 1 and 2 are correct. Here's proof. Step 3 and 4 can be done in a couple of ways. Following your steps to the letter, you can convert the RGB color to HSV, modify the values, and convert back to RGB. The first conversion is unnecessary though, since the image is grayscale, so we know the only thing that is affected is the "V" in HSV. So you can use your image as V, and use those constant H and S values to get something similar to what you want. Subject to tweaking and experimentation of course. Converting back and forth between RGB and HSV isn't the best idea for something like this though; perhaps in this case - if all you want is a slight blue/cyan tint - then you could instead just slightly tweak the blue and green channels and get the same result. 

I know exactly what's going on and it's a tricky one:) In time, since one orbits after the other (the update functions do not happen at the same time), their distance increases or decreases little by little. Right when the planet orbits a bit, you want your moon to do exactly the same motion so that their distance does not change. Otherwise you'll make an orbit around a slightly different radius. Your planet, before doing its orbit step, can remember where it was and where it went to, and tell the moon to move in the same direction with the same amount. To show you how to do that, I would need access to your planet code as well. However, it's simple subtractions and additions of transform.position values, you can also figure it out. In the meantime, below is a hacky fix that should remedy the situation if both the planet and the moon are orbiting around Vector3.up. I wasn't sure about how you use the orbit angle, so I changed that a bit. This works for me here: 

This does not necessarily have to be as computationally expensive as you imagine. First of all, as you hinted at, you don't have to check every entity; just the ones that are moving. As such it might be wiser to have individual entities update their grid cell in their update method. As for checking coordinates, there are a few optimizations you could make. For example, say your grid is made up of cells that are 64x64. To figure the coordinates of which cell an entity is in, all you have to do is and (try it). This is faster than, say, a grid with cells that are 100x100, where to find out the cell coordinates you would need to do and and floor the result (or cast into if it's not already). Thus it's more efficient on a larger scale to pick cell sizes that are powers of 2 for example. Although I doubt that this will be a bottleneck (remember premature optimization is the root of all evil), you could also update zones less frequently than on every game tick and maybe even make the rate depend on the maximum/expected speed of any given entity. 

1) Since speed is a concern, you may want to take a look at approximate nearest neighbor algorithms. I've used ANN in the past and it performed very well for around 12 dimensions. It lets you adjust desired precision so that you can have a trade off between speed and precision and find what works best. 2) Since your visual occlusion is a black-box one (I'm assuming unpredictable moving obstacles), I'm not sure if you have much of a choice other than doing occlusion tests on the points that the NN algorithm returned. 3) I don't believe ANN supports points changing, but I'm not sure since I didn't need that. It seems Cgal and Pastel support dynamic sets, but in terms of insertion/removal of points. Perhaps the papers here would also provide some insight. I don't know if you need this advice, but I found that reusing libraries for such problems almost always is a better idea. There are so many pitfalls one can fall into while implementing the details. Good luck!