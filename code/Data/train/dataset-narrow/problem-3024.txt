As @Emre suggested, if you already have the distribution of topics in each document, you can represent each document as a vector $x_d \in \mathbb{R}^N$, where $N$ is the number of the unique topics in your collection. For documents, not exhibiting specific topics just fill the specific cells in each feature vector with zeros. Then, you can use some clustering algorithm such as nearest neigbors, using those feature vectors. Example usage code in python below: 

If you follow the example, the output for the wanted student (with $student_{ID}=3$) with opinions: {Trump -1, Net Neutrality -1,Vaccination 1, Obamacare -1} will give you two other students with the same opinions and their ids. You can modify the script to fit your needs accordingly. P.S.: Sorry for the messy code, it was written rather hastily. Also, i tried it with Python 2.7. 

A very simple approach would be to find some kind of centroid for each cluster (e.g. averaging the distributions of the documents belonging to each cluster respectively) and then calculating the cosine distance of each document within the cluster from the corresponding centroid. The document with the shorter distance will be the closest to the centroid, hence the most "representative". Continuing from the previous example: 

When use any sampling technique ( specifically synthetic) you divide your data first and then apply synthetic sampling on the training data only. After you train you use the testing set ( which contains only original samples) to evaluate. The risk if you use your strategy is to have the original sample in training ( testing) and the synthetic sample ( that was created based on this original sample) in the testing ( training) set. 

This is interesting !?! The way you think about it is as solving a linear system equation, which is not exactly the case. Imagine if you have thousands of data points , and you try to find the set of weights that solves ( satisfies) these points, what if an exact solution is not exist. What if there is no set of weights that exactly solves all these points. It becomes an optimization problem. What is the set of weight that minimizes the error. In this case the error is the difference between the actual output and the output you get by using the current weights. In fact, in current machine learning technology, it is difficult even to construct such a linear system. We use optimization techniques to solve this and find the optimal set of weights 

To my understanding you should be looking for something like a Gaussian Mixture Model - GMM or a Kernel Density Estimation - KDE model to fit to your data. There are many implementations of these models and once you've fitted the GMM or KDE, you can generate new samples stemming from the same distribution or get a probability of whether a new sample comes from the same distribution. In python an example would be like this:(directly taken from here) 

You can change the scatter plot attributes for marker-size, colormap etc. according to the documentation to match your tastes. 

This takes 4 documents with $N=7$ unique topics, fills missing values with zeros and creates a similarity matrix between all documents. Then querying for documents 1(Science=0.7) and 2(Art:0.5) the most similar other document in the collection, we surely get documents 3(Science:0.8) and 4(Art:0.6) correspondingly. You can try more sophisticated approaches regarding clustering and other distance metrics. 

I agree mostly with what was already said regarding feature engineering and just to provide you with more material this post has a nice analysis regarding different stages in feature engineering, with many links and references to papers and specific challenges. I think it's worth checking out. Also, to some extend you could automate the task of finding "good" compound features, using kernel methods. You could use some of the standard kernel methods implemented in many libraries (e.g. in sklearn) as feature extractors and feed these higher-level features as input to a random forest model that inherently uses feature-importance while training. Then keep only most-informative of these higher-dimension features, as scored by the trained model, as compound features alongside the raw data. 

Having more data may not mean that you have online learning. Above answers are correct if you assume that the coming new data samples will not have any concept drift. Meaning the true data generating distribution may change in a way that your current ( most recent model) is in a point that is very far away from the optimal minimum for the new data distribution. I recommend you to read more about “concept drift” and understand if your data will have any concept drift. Try to search for how NN deals with concept drifts. Unfortunately I am not aware of any technique or method can make NN adapt to concept drift. If you do not expect any concept drift, then saving the check point and continue training will be good solution if you are using stochastic gradient descent 

Usually removing noise is a process depends on the type of noise. Noises can be high or low frequency , you are using low frequency filters ( average and median) . 1. You did not describe the source of noise, we can not tell if it is a high frequency or low frequency . 2. Try to use a higher order low pass filter 

There are many ways in which you could create an ensemble from your base models. Some resources to take a look at are the following: Firstly, I would point your attention towards this question, that has a lot of answers and interesting ideas. Regarding implementation: I have used in the past the brew module that has extensive implementations of different stacking-blending techniques etc, while using different combination rules and diversity metrics. It is also compatible with sklearn models. Another interesting project is xcessiv, which I haven't used myself but provides a helpful gui for managing your ensemble's details. Finally, regarding theoretical details, I would suggest you take a look into this survey that focuses on ensembles in regression tasks. 

You could use a 3d-scatter plot, where each class would correspond to one axis and the color-intensity of the point would indicate the score value(e.g. for a grayscale colormap, the whiter the closer to 1 the value of the score). Using the above format: 

As a fast answer, you can represent each student as a vector with $K$ elements (where $K$ is the number of topics) and values $\{+1, 0, -1\}$, denoting positive/non-existent/negative opinion about this topic. Then, a simple measure of agreement between two students is the element-wise product between two student-vectors. That is the product will be: $similarity = \sum_{i=1}^{K}st_1[i]*st_2[i]$, where $st_1,st_2$ are the student-vectors. Obviously, only the topics where both students have aligned opinions will boost the total [e.g. $1*1=1$ and $(-1)*(-1)=1]$, while misaligned opinions will decrease the sum. If any of the two students haven't expressed an opinion about a topic, then this topic won't matter in the sum. In that sense, you can find the most like-minded students to a specific student, as the ones with the highest $similarity$. If what you really need is a number of agreeing students for each unique student, then a threshold on the $similarity$ score can be set. The value of the threshold can be decided empirically from your data. This is easily implemented and if you are comfortable with coding, I could post a sample script in python. One thing to consider though, is in what format is the bipartite graph (a .csv, a graph file of some kind etc.). EDIT: MINOR EXAMPLE. Fetch example .csv file used from here. 

I have an answer now for my question. I will share briefly the main steps / technologies I used to deploy the model in production. I am using Python programming language. After training and generating valid models I wrote a restful api using Python programming language and flask. Using flask you can write a restful api. Three important points: 1- It is very important to give attention to where you will define the model architecture/initialize the parameters/ define the session. Avoid doing this each time you call the restful api. this will be very expensive. 2- Flask provide a very good mechanism to run servers in production environment. Read about flask + wcgi Avoid runing the server code (the resful api) directly, in this case you will not have direct and full control. 3- Watch the memory and the cpu usage, make sure to limit the maximum number of instances that can run in parallel. Remember these models can take a lot from the memory. unfortunately, I can not share codes with public. But hopefully my answer can give an idea about how to do it in production. 

In the end the model, could be used for sampling new data points or predicting the probability of a new sample to have been generated from this distribution. You should play around with different kernels in KDE models or number of base distributions in GMMs, along with other parameters to get optimal results for your data. 

I would use a two-step approach, using the idea of the $\hat{c_4}$ class you mentioned. In the first step, use a binary classifier(trained on the whole dataset) to decide if a sample belongs to the class $\hat{c_4}$ (i.e. in any non-interesting class). For this, step you could also take a look in outlier detection methods, if the samples belonging in the "interesting" classes are much different than the rest. If the result is negative, move on to the next step, a new classifier trained only on samples belonging in the classes $c_1,c_2,c_3$ and use that prediction as your final one. I think that even using a simple clustering approach as a first step (e.g. 4-clustering k-means using as initial centroid values the average centroid $cent_j = \frac{\sum\limits_{x_i\in D: y_i=j}x_i}{\sum\limits_{x_i\in D: y_i=j}1}$ for each $c_1,c_2,c_3, \hat{c_4}$), would still be useful. 

You need to understand to which of these cases your problem belong. if you have very severe data imbalance + very few number of samples + wide variation within the majority class and similarities between different classes. regular oversampling or down sampling techniques will not help you as well as most of the synthetic oversampling techniques designed specifically to deal with the data imbalance but the assumption is to have enough number of samples. Try to focus more on ensemble techniques that designed mainly to deal with data imbalance. SMOTE-Boost RUSBoost SMOTEBagging IIIVote EasyEnsemble 

It depends on the expected size of neural network you are thinking about. If the DNN consists multiple layers with large default input and many fully connected layers then you need a GPU with large memory. Memory is the most important factor , without enough memory, you GPU is useless. To get an idea about the size of any DNN try to think about the number and size of fully connected layers. Of course the number of cores is important but again memory is more important 

I think what you should be looking for is ordinal regression/classification in order to utilize the ordering of the classes. Example python/matlab implementations, with some extra resources. For fast integration with your system, you could create a new ordinal regression model using the outputs of the already trained network as input to the new ordinal model and the true ordinal value as the output. For another quick/dirty solution you could try simple Mean Absolute Error or Mean Squared Error, according to this study, between the predicted and target classes of your system, as it is right now, instead of the categorical cross-entropy error function. 

I think the answer also depends on your usecase. If you just want to detect these kind of strings I would focus on heuristic rules that suit my needs, rather than creating a system that learns to recognize the patterns of the strings. However, if your aim is generating similar kind of strings based on the current patterns or finding new strings in a stream of text, you should look for regex generators. There is this list of resources regarding regexes in general and you should focus on the generators for the task at hand. For a similar task in the past, I had used an online, free tool that will generate a regex (if possible) that satisfies as many of your sample strings as possible. So you could give it a try, as a fast solution. Either way, I would first do some data exploration(e.g. are numbers and letters also intermixed or do numbers always come up after or letters/symbols etc.), in order to get a better understanding for the problem at hand. Also, this could lead to simple heuristic rules that could suffice as a crude solution or at least a simple baseline system against which I would "pitch" a machine learning model. 

I have worked on several practical projects. In these projects we designed classifiers to deal with practical problems. Before I started working on practical I used to be very interested in : overall classification accuracy, sensitivity and specificity. In practical life, usually they care in two values: Positive predictive value " precision " and positive percentage PP "how many samples are predicted as positive". The pp value measures the usefulness of this practical solution while the ppv value measures the quality of your solution. Based on my understanding if the pp value is zero the ppv value is not defined and the solution itself is not valid 

You should use the testing set without any change, as answered by others. But it is very important to understand the difference between average accuracy and overall accuracy. In overall accuracy you find ( number of samples predicted correctly/ total number of samples) in average accuracy, you find the overall accuracy per class and then you find the average of these overall accuracies. When you know that you are working with imbalanced database, where all classes are important, you should use the average accuracy To understand what this means: imagine you have two classes, class A and class B , and the ratio is 90 to 10 . If you are sampling randomly for the training and testing, then the ratio is still 90:10 in the testing set. If your model is very biased , that predicts all the samples to be class A , then: Overall accuracy = 90% Average accuracy = 50 % ( 100% for class A + 0% for class B) / 2 The overall accuracy is really high but it does not reflect the actual quality of the model. The average accuracy gives you a better indication of the quality