I don’t know where you’re getting this particular classification scheme from, (continuous, categorical, nominal), but it’s worth mentioning that this isn’t a great scheme for classifying data types. You’ve already encountered an example of a data type (count data/natural numbers) that doesn’t fit neatly into them. It is also worth mentioning that a lot of people will use “nominal” and “categorical” interchangeably to describe a discrete data type with no natural ordering. Probably the most widely-accepted term to describe discrete data with a natural ordering is ordinal data. In this case, "number of rooms in a flat" has an unambiguous ordering. In a very real and intuitive sense, a five-room flat has more rooms than a two-room flat. A one-room flat has fewer rooms than a three-room flat. It makes sense to talk about one flat having a greater or lesser number of rooms than another, and if we want to know if that ordering matters in some way, we need to preserve the ordering in our analysis. 

The images in the link you provide, of the severed sphere and its lower-dimensional representations, go some way towards explaining the difference. The severed sphere is a set of points in a three-dimensional space, but we want a two-dimensional representation of it. The objective of manifold-learning is (shockingly) to find a manifold: a subset of that three-dimensional space which (a) closely fits all the points that make up the severed sphere, and (b) can be described with a two-dimensional coordinate system. If you look at some of the other lower-dimensional representations of the severed sphere, it's like they're taking it and flattening it out into a rectangle so it'll fit in two dimensions. It's taking the severed sphere and figuring out a new coordinate system that maps as closely as possible onto all the points that make up the severed sphere. The MDS lower-dimensional representation, though, is more like a shadow that the severed sphere casts on a wall. Rather than finding a new coordinate system that closely fits the sphere, it's just "forgetting" whichever of the dimensions it thinks it can most afford to lose while maintaining the same distance to and from all the points. A good analogy would be maps of the earth. A good map of the earth makes a new coordinate system that fits a sphere onto a 2D surface. To do this it has to distort the relative distances between places, but you end up with effective 2D coordinates that relate well to places on the globe. Instead of doing this, you could just take two photos of the earth from above the north and south pole and glue them back to back. You'd still have a 2D representation of the earth, but it doesn't work so well as a coordinate system. This isn't to say that MDS is "bad". It's just doing something different. You probably wouldn't use MDS for dimensionality reduction prior to carrying out some sort of statistical procedure, but if you're trying to produce a graphic that gives some idea of how close multidimensional points are to one another, it might be a good choice. 

Consider how cosine similarity is calculated. Cosine similarity takes the dot product of two real vectors, and divides this value by the product of their magnitudes. By the Euclidean dot product identity, this is equal to the cosine of the angle between the two vectors. The upshot of this is a value between 1 and -1. When the value is 1, those vectors are pointing in exactly the same direction. When the value is -1, the vectors are pointing in exactly the opposite direction (one is the negation of the other). When the value is 0, the vectors are perpendicular to one another; in other words, when the value is zero, these two vectors are as unalike in the feature space as it is possible to get. The dot product is the sum of all the element-wise products of your two vectors. The bigger those numbers, the more they contribute to the cosine similarity. Now, take any feature in your vector. The fifth, say. If you set this to zero in one of your vectors, the fifth element in the element-wise product of the two vectors will also be zero, regardless of its value in the other vector. When you sum up all these element-wise products, the fifth element will not have any impact on the summation. As a result, setting a value in your feature vector to zero means it doesn't make any contribution to the cosine similarity. This is why setting a value to zero in a feature vector is equivalent to not including the feature in the calculation of cosine similarity, and does not does not distort cosine similarity. 

Treat the examples from the first as labels for training; you want to learn "what is an exception" and you need to have an expert answer that question for you. Treat the suggested features as new features for your classification. If you cannot get the experts to answer the questions you make, then try to infer them from their interaction with your reports: how many of those reports were downloaded, how many times are they mentioned, how are they ever used in decision-making ... Try to separate the important ones from the uninteresting ones, and there you have your labels. 

In order to build those profiles, you need to map the "categories" and "keywords" that you have, which are too plentiful, into the features you think are relevant. Look into topic modeling or semantic similarity to do so. Once that map is built, it will state that all dollars spent on webs with keywords "gadget", "electronics", "programming", and X others, should all be aggregated into our first feature; and so on. Do not be afraid of "imposing" the features! You will need to refine them and maybe completely change them once you have clustered the users. Once you have user profiles, proceed to cluster them using k-means or whatever else you think is interesting. Whatever technique you use, you will be interested in getting the "representative" point for each cluster. This is usually the geometric "center" of the points in that cluster. Plot those "representative" points, and also plot how they compare to other clusters. Using a radar chart is very useful here. Wherever there is a salient feature (something in the representative that is very marked, and is also very prominent in its comparison to other clusters) is a good candidate to help you label the cluster with some catchy phrase ("nerds", "fashionistas", "aggressive moms" ...). Remember that a clustering problem is an open problem, so there is no "right" solution! And I think my answer is quite long already; check also about normalization of the profiles and filtering outliers. 

What you are asking about is, in my view, the main problem of implementing a lambda architecture. Here are some suggestions on how to solve it. The combination of Spark and Spark Streaming largely supersedes the original lambda architecture (which usually involved Hadoop and Storm). Read here an example of how to use a and a separate to produce different s, one for batch processed results and another for real-time results. Once you have replicated that in your system, you still have to think about how to query both kind of s. The trivial case would be to just both of them: 

There is a worked out example in the Wikipedia page for "simple linear regression" Just for the sake of it, let me plug in your example into the formulas: The fitted model should be a straight line with parameters $\alpha$ (value at $x = 0$) and $\beta$ (the slope): $$f(x) = \alpha + \beta x$$ The values for these parameters that minimize the distance between line and data points are called $\hat{\alpha}$ and $\hat{\beta}$. They can be computed out of the data point values by using these formulae, derived here: $$\begin{align} \hat{\beta} & = \frac{ \overline{xy} - \bar{x}\bar{y} }{ \overline{x^2} - \bar{x}^2 } , \\ \\ \hat{\alpha} & = \bar{y} - \hat{\beta}\bar{x} \end{align} $$ where an expression with an overline $\overline{xy}$ means the sample average of that expression: $\overline{xy} = \tfrac{1}{n} \sum_{i=1}^n{x_iy_i}$. Here are the values I find for the datapoints you have listed in your question: $$ \begin{align} \overline{xy} &= \frac{1}{4} \sum{<(1 \times 4000), (2 \times 10000), (3 \times 22000), (4 \times 30000)>} \\ &= \frac{1}{4}(4000 + 20000 + 66000 + 120000) = 52500, \\ \overline{x} &= \frac{1}{4} \sum{<1, 2, 3, 4>} = 2.5 ,\\ \overline{y} &= \frac{1}{4} \sum{<4000, 10000, 22000, 30000>} = 16500 , \\ \overline{x^2} &= \frac{1}{4} \sum{<1^2, 2^2, 3^2, 4^2>} = 7.5 , \\ \overline{x}^2 &= 2.5^2 = 6.25 \end{align} $$ and the fitted line should be: $$ \begin{align}\ \hat{\beta} &= \frac{52500 - 2.5 \times 16500}{7.5 - 6.25} = \frac{41250}{1.25} = 33000 , \\ \hat{\alpha} &= 16500 - 33000 \times 2.5 = - 66000 , \\ \Rightarrow f(x) &= - 66000 + 33000 x \end{align} $$ Therefore, the model would predict, for a house with 10 rooms, a rent of: $$ f(x) = -66000 + 33000 \times 10 = 264000 $$ 

The example given in your textbook proposes a multiple linear regression with 100 predictors, all of which have a "true" regression coefficient of 0. In other words, your independent variables have no statistical association with your dependent variable. When you calculate the $p$-value of an individual coefficient, you're looking at the magnitude of the coefficient, the standard error of the coefficient, making some distributional assumptions about it, and asking the following question: "what is the probability of seeing a coefficient value this extreme if the true value is actually zero?" If our distributional assumptions are correct, any given coefficient with a true value of 0 will report a <0.05 $p$-value approximately 5% of the time. That's not so much of a problem if we only have one predictor, but by the law of large numbers, if we have lots of predictors, we'd expect 5% of them to report a $p$-value this low. This makes the $p$-values in high-dimensional regressions hard to interpret. The $F$-test is different. Instead of evaluating every single coefficient for statistical significance, it applies a single test to the entire regression. So instead of having 100 chances to throw up an erroneous $p$-value, it only has one chance. This makes the $F$-test useful for evaluating whether or not there is a regression effect for high-dimensional regressions. 

If you have a large enough sample size, you can indeed carry this out the way you propose. For five events, you have 120 ($^5P_5$) possible permutations of the order of events. This allows you to run a logistic regression with 120 dummy independent variables, each of which corresponds to a permutation of your order of events. The F-test of this regression will function as a significance test to see if there is any difference in frequency of your outcome between different orderings of events. This does require a large sample size, however. A good rule of thumb is at least 20 observations per independent variable in a General Linear Model, so if you have a few thousand samples, we'd expect this model to fit reasonably well. This does assume you have a relatively small number of events. Five seems manageable, but as your number of events increases, you quickly run into problems as your number of independent variables grows factorially. 

There is Matrix Calculus, (and I would recommend the very useful Matrix Cookbook as a bookmark to keep), but for the most part, when it comes to derivatives, it just boils down to pointwise differentiation and keeping your dimensionalities in check. You might also want to look up Autodifferentiation. This is sort of a generalisation of the Chain Rule, such that it's possible to decompose any composite function, i.e. $a(x) = f(g(x))$, and calculate the gradient of the loss with respect to $g$ as a function of the gradient of the loss with respect to $f$. This means that for every operation in your neural network, you can give it the gradient of the operation that "consumes" it, and it'll calculate its own gradient and propagate the error backwards (hence back-propagation) 

I have a population, each unit of which exists in one of several states that change over time. I am using first-order Markov chains to model these state transitions. My population can be segmented into various subpopulations of interest. I've obtained the transition matrices for each of these subpopulations, and would like to know if these subpopulations differ from the general population in some principled way. I don't know of any principled way of comparing transition matrices in this way. Comparing the transition matrices row-wise to that of the general population seems like one approach, but I'm not sure how to go about interpreting this. Another approach might be a spectral/eigendecomposition approach, which is much more readily interpretable to me, but might be harder to squeeze insight/stylised facts from. I've had a cursory search of the literature without much luck. Any suggestions? 

Right now, I only have time for a very brief answer, but I'll try to expand on it later on. What you want to do is a clustering, since you want to discover some labels for your data. (As opposed to a classification, where you would have labels for at least some of the data and you would like to label the rest). In order to perform a clustering on your users, you need to have them as some kind of points in an abstract space. Then you will measure distances between points, and say that points that are "near" are "similar", and label them according to their place in that space. You need to transform your data into something that looks like a user profile, i.e.: a user ID, followed by a vector of numbers that represent the features of this user. In your case, each feature could be a "category of website" or a "category of product", and the number could be the amount of dollars spent in that feature. Or a feature could be a combination of web and product, of course. As an example, let us imagine the user profile with just three features: 

You do not need a wrapper for DBPedia, you need a library that can issue a SPARQL query to its SPARQL endpoint. Here is an option for the library and here is the URL to point it to: $URL$ You need to issue a DESCRIBE query on the United_States resource page: 

Or maybe you can create a new , similar to in the linked example, where some keys are kept for real-time results, and others for batch results. 

Is installing Python locally a good practice? Yes, if you are going to develop in Python, it is always a good idea to have a local environment where you can break things safely. Is there value in setting up a Python "server"? Yes, but before doing so, be sure to be able to share your code with your colleagues using a version control system. My reasoning would be that, before you move things to a server, you can move a great deal forward by being able to test several different versions in the local environment mentioned above. Examples of VCS are git, svn, and for the deep nerds, darcs. Furthermore, a "Python server" where you can deploy your software once it is integrated into a releasable version is something usually called "staging server". There is a whole philosophy in software engineering — Continuous Integration — that advocates staging whatever you have in VCS daily or even on each change. In the end, this means that some automated program, running on the staging server, checks out your code, sees that it compiles, runs all defined tests and maybe outputs a package with a version number. Examples of such programs are Jenkins, Buildbot (this one is Python-specific), and Travis (for cloud-hosted projects). What are the hardware requirements for such a box? None, as far as I can tell. Whenever it runs out of disk space, you will have to clean up. Having more CPU speed and memory will make concurrent builds easier, but there is no real minimum. Do I need to be concerned about any specific packages or conflicts between projects? Yes, this has been identified as a problem, not only in Python, but in many other systems (see Dependency hell). The established practice is to keep projects isolated from each other as far as their dependencies are concerned. This means, avoid installing dependencies on the system Python interpreter, even locally; always define a virtual environment and install dependencies there. Many of the aforementioned CI servers will do that for you anyway.