Here is a grammar that should meet your specification, though it generates the very simple language $a^+(b+c)$. (A simpler grammar has been added below) $S \rightarrow ab \mid aBb \mid ac \mid aCc$ $B \rightarrow a \mid aB \mid aBB$ $C \rightarrow a \mid aC \mid aCC$ How was it built: I started from the grammar $\{S \rightarrow S S \mid a\}$ which is very ambiguous, so that trying all parses of a string $a^n$ takes exponential time. I put it in Greibach Normal Form, so as to remove any left recursion, giving $S \rightarrow a \mid aB$ $B \rightarrow a \mid aB \mid aBB$ Then I introduced $C$ to duplicate the possibilities of derivation, with $b$ or $c$ as end marker to discover at the very end whether I should have been using $B$ or $C$ as non-terminal in my derivation, i.e,, whether the first rule to apply was $S \rightarrow aBb$ or $S \rightarrow aCc$. But if the choice of rule made at the very beginning was wrong, the backtracking process will have first to try all parsing possibilities for the string of $a$, which has exponential cost. Since the recursive descent has to try first one of the two rules, there will always be a string (requiring the other rule) that takes exponential time to parse, because of the useless backtracking with the wrong non-terminal. Memoisation brings this down to cubic time. Naive memoisation will make it $O(n^4)$, but a bit of currification can correct that to cubic time. 

This problem is known to be $\mathsf{NP}$-hard by [Dow81]. However, Yao [Yao76] presented an algorithm which on any input produces an addition chain of size $$ \log N +c\cdot\sum_{i=1}^p\frac{\log n_i}{\log \log (n_i+2)}\le \log N +\frac{cp\log N}{\log \log (N+2)},$$ where $N=\max_i(n_i)$ and $c$ is a constant. In [Cha05] it is now claimed that this is an approximation algorithm for the general addition chain problem of ratio $$O\left(\frac{\log\left(\sum_i n_i\right)}{\log\log\left(\sum_i n_i\right)}\right),$$ i.e., Yao's algorithm produces for each input a chain which is at most by this factor larger than a smallest addition chain. Now my question: Is there a lower bound on the approximation ratio of Yao's algorithm, e.g. is there a family of inputs such that Yao's algorithm produces addition chains which are by the described factor larger than a shortest addition chain? $ $ $ $ [Dow81]: P. Downey, B. Leony, and P. Sethi, "Computing Sequences with Addition Chains", SIAM J. Computing, vol. 11, pp. 638-696, 1981 [Yao76]: A. C.-C. Yao, “On the evaluation of powers”, SIAM J. Comput., vol. 5, no. 1, pp. 100–103, 1976 [Cha05]: M. Charikar, E. Lehman, D. Liu, R. Panigrahy, M. Prabhakaran, A. Sahai, A. Shelat, "The Smallest Grammar Problem", IEEE Transactions on Information Theory. 51 (7): 2554–2576, 2005 

It is is worth thinking about WHY intuistionistic logic is the natural logic for computation, since all too often people get lost in the technical details and fail to grasp the essence of the issue. Very simply, classical logic is a logic of perfect information: all statements within the system are assumed to be known or knowable as unambiguously true or false. Intuistionistic logic, on the other hand, has room for statements with unknown and unknowable truth values. This is essential for computation, since, thanks to the undecidability of termination in the general case, it will not always be certain what the truth value of some statements will be, or even whether or not a truth value can ever be assigned to certain statements. Beyond this, it turns out that even in strongly normalizing environments, where termination is always guaranteed, classical logic is still problematic, since double negation elimination $\neg\neg P \implies P$ ultimately boils down to being able to pull a value "out of thin air" rather than directly computing it. In my opinion, these "semantic" reasons are a much more important motivation for the use of intuistionistic logic for computation than any other technical reasons one could marshal. 

Regarding topics There are different type of issues that may be considered theoretical computer science. The important word here is "theoretical" (as we all have some idea of what computer science deals with). Understanding the word theoretical is not so obvious. For a long time I took it to mean mathematical, as opposed for example to "hacking". I learned better from people in linguistics: theoretical for them clearly does not meant mathematical, but based on a theory which may be somewhat informal (though it may be also mathematical), and is an organized body of knowledge and concepts that structure understanding of observable phenomena and hopefully allow some deductive and predictive use of the acquired knowledge. It also reduces the amount to learn and to teach by reducing the number of primitive concepts from which the rest can be deduced. Theoretical can be opposed to practical, which is how this knowledge is used to actually run computing engines, to build systems, etc. I can also be opposed to applied which is the use of this knowledge to address problems in other fields of science and human activities. But I doubt there are clear-cut boundaries. This said, theoretical computers science covers diverse domains, and I will try to give some, while I am sure I forget others, and also that other people may disagree with this organization. 

The paper "Algorithmic Meta Theorems for Circuit Classes of Constant and Logarithmic Depth" (Elberfeld, Jacobi, Tantau) gives a nice balanced tree decomposition based on tree contraction in $TC^0$: $URL$ Another nice $TC^0$-decomposition is based on so-called TSLPs in the paper "A Universal Tree Balancing Theorem" (Ganardi, Lohrey): $URL$ 

An addition chain of size $n$ for given integers $n_1,n_2\dots ,n_p$ is a sequence of integers $k_1,k_2\dots ,k_n$ such that 

Let $G$ be an acyclic, context-free grammar over a fixed alphabet $\Sigma=\{a_1,\dots,a_k\}$ with the restriction (without loss of generality) that $|w|=2$ for each rule $A\to w$ in the grammar. Acyclic means that if $N$ is the set of nonterminals, then $$\{(A,B)\in N^2\mid A\to xBy\text{ is a rule in }G\text{; }x,y\in(\Sigma\cup N)^*\}$$ is an acyclic relation. So $L(G)$ is finite. Let in this setting the size of a grammar be defined as the number of nonterminals My question Let $\#_w(i,j)$ be the number of different subsequences of $a_ia_j$ in $w$. For example $w=a_1a_1a_2a_2a_1$ yields $\#_w(1,2)=4$, $\#_w(2,1)=2$, $\#_w(1,1)=3$ and $\#_w(2,2)=1$. Now I am looking for the complexity of: 

How can inherited attributes be simulated with synthesized ones: an example. The way of doing it is to postpone the evaluation of any attribute that uses directly or indirectly an inherited attribute, by abstracting it into a function that takes the inherited attribute as argument. There may be several such arguments. Here is a fragment of a simple example, on the grammar rule $T\rightarrow T\; v$ for a term in an expression, which is to be type-checked. Right hand side symbols are subscripted to identify occurrences, so we actually write $T\rightarrow T_1\; v_1$ We use the following attributes: 

The semantic rule for type checking is: $type(T)\gets if\; type(T_1)=symtab[name(v_1).type]\; then\; type(T_1)\; else\; typefail$ meaning that the type of the subterm $T_1$ and the type associated with the symtab entry of the $name$ of the variable $v_1$ must be the same, and that the name of this common type or $typefail$ is the value for the synthesized attribute $type$ for $T$. You replace that semantic rule by the following one: $type(T)\gets \lambda\; symtab.\; if\; type(T_1)(symtab)=symtab[name(v_1).type]\;$ $\qquad\qquad\qquad\qquad\;\;\;\;\;\;then \;type(T_1)\; else\; typefail$ The synthesized attribute $type$ is now a function rather than a type name. When called, it takes a symtab as argument and checks whether the expression $T$ is well typed with respect to this symtab, calling recusively other such functional values of the attribute to actually do the type checking for subexpressions. You no longer need to know the symbtab where you do the type checking, so you do not need the inherited attribute $symtab$. When higher up in the tree you get a synthesized attribute that is the actual symtab, you simply apply the type attribute with that symtab as argument to get the type checking done. Well, it is a bit more complicated with scope rules. This can be completely formalized, but I do not have the right papers handy, and I have no courage or time to redo the complete formal presentation. But I am sure there are books and papers on this It is a kind of continuation passing style for processing the inherited attributes. A proof that inherited attributes are not necessary Hypotheses: 

Just because you can always embed a lower-order type into the higher-order universe doesn't imply that the reverse is true. The higher-order universe is always strictly larger than the lower-order one, so there is no problem with respect to predicativity. 

I would recommend investigating the field of Finite Model Theory and more particularly its sub-field Descriptive Complexity. It can be used to model such sorts of problems. 

I'm wondering if anyone knows of a formalization (even limited) of any part of finite model theory in any of the major proof assistants. (I'm most familiar with Coq, but Isabelle, Agda, etc. would acceptable.) Especially of interest would be any of the results in descriptive complexity. 

If you are having trouble with the concept of least fixed point, I would recommend spending some time getting a background in more general order theory. Davey and Priestley, Introduction to Lattices and Order is a good intro. To see why the transitive closure is the least fixed point, imagine building up the closure from an empty set, applying the logical formula one step at a time. The least fixed point arrives when you can't add any new edges using the formula. The requirement that the formula be positive ensures that the process is monotonic, i.e. that it grows at each step. If you had a negative subformula, you could have the case where on some steps the set of edges would decrease, and this could lead to a non-terminating oscillation up and down, rather than a convergence to the LFP. 

Given an alphabet $\Sigma$ of size $k$ and two strings $w_1,w_2\in \Sigma^n$ of length $n$. The longest common substring problem asks for a longest string in the set $A(w_1,w_2)$ of all common substrings of $w_1,w_2$. We can define $A(w_1,w_2)$ as follows: $$A(w_1,w_2)=\{s\in\Sigma^*\mid w_1=x_1sx_2,\;w_2=y_1sy_2,\;x_1,x_2,y_1,y_2\in\Sigma^*\}$$ In genereal, this substring $s$ is not unique. Therefore, I am looking for the maximal number $m(k,n)$ of different longest common substrings for an arbitrary pair of strings of length $n$. Formally: $$m(k,n)=\max\limits_{w_1,w_2\in\Sigma^n}\left|\left\{s\in A(w_1,w_2)\mid\forall x\in A(w_1,x_2):\;|s|\ge |x| \right\}\right|$$ Natural upper bound: It is easy to see, that $m(k,n)\le n$ since the maximal number of substrings of the same length for a strings of length $n$ is $n$ (for substrings of length $1$). For $n\le k$ we even achieve $m(k,n)=n$ (take a string $w_1$ which consists of different letters and $w_2$ the reverse string of $w_1$). For $n> k$ we conclude that $m(k,n)<n$, but from now on it seems to be difficult. Lower bound: Based on De-Bruijn sequences it is possible to deduce a lower bound as follows: As a conclusion of a paper of Lin et al, for each $m$ there are orthogonal De-Bruijn sequences $B_1(k,m)$ and $B_2(k,m)$ of length $k^m$, which means that the longest substring of $B_1(k,m)$ and $B_2(k,m)$ is of length $m$. The special property of (those) De-Bruijn sequences is, that each string of length $m$ is actually a substring (some of them by reading the De-Bruijn sequence cyclic). Therefore, both $B_1(k,m)$ and $B_2(k,m)$ containing $k^m-m+1$ different subtrings of length $m$ (acyclic) and for that reason there are at least $k^m-2m+2$ different longest common substrings for $B_1(k,m)$ and $B_2(k,m)$. So, for $n$ as a power of $k$, we get $$m(k,n)\ge n-2\log_kn+2$$ I'm quite sure we can achieve a very close result (caused by some rounding-issues) for each $n$. But my question is, how tight are these bounds? I could imagine that orthogonal De-Bruijn sequences are already (asymptotic) worst case examples, i.e. $$m(k,n)\in n-\Theta(\log n).$$ But I am neither sure about this nor able to show it. Any help is welcome!