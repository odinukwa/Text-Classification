It is heartening to see that you are concerned about normalization. That will serve you well through your career. Stick with it. All real-world implementations have their limitations, however. Even 128 cores and 1TB RAM will struggle with petabytes of data. The trick, then, is to do the best with what you've got. Start with the queries. Are they returning the minimum amount of data? Doing more work than required is slower. Have you got indexes in place? Do they match the query predicates? Is the optimizer using them? Some DBMS allow WHERE clauses on index definitions. This limits both the work on write and on read. Is clustering defined appropriately? There are different technologies for storing data. Traditional engines are optimised for disk. Recently memory-optimized engines have become common. Without the IO and with different concurrency techniques they run much faster. Traditionally data has been stored row-wise. Now column stores are available. They are especially good for aggregate queries. There are engines that specialise in the kind of rollup queries you need called OLAP. Some are updated in batch, some synchronously. Most need a separate query language. Not sure if this is overkill for homework. None of the above will break normalization. By denormalizing data you hold a value more than once. The problem is to keep the copies synchronised. This task can be delegated to the application, which is OK for small, short projects. Better to have it in the DBMS, though. Triggers are one approach. A write to a low-level value will be duplicated to the next higher level and so on. Be aware that the top-level Tables get very "hot" and may become a bottlneck. If a small amount of variance is acceptable caching values in the client helps performance greatly. Perhaps only do the nation-wide query every 30s, say, and perform it asynchronously so the app's latency is not affected. Generaty speaking, view results are not cached separately. When a query referencing a view is parsed and compiled the view's definition is substituted into the query's SQL and that is executed. Materialized views, as mentioned in another answer, do store the results separately. 

The only thing that I can think of is to split the date into day/month and year - having one or the other as NULL as necessary. This is one of the only times I consider a split year/month/day structure, with separate fields, to be OK. 

If you know for sure that these 35 programmables are the culprits a combination of Profile/Events for elapsed time and cached plans, plus sys.dm_exec_query_stats should get you a long way to understanding where about the pain lies. @Paul's comment about recording a baseline is important, though. 

SQL Server does not de-escalate the lock. I investigated using a "Numbers" table with 100,000 rows. Empirically, updating 5,000 rows produced a corresponding number of RID locks in . Updating a further 10,000 rows caused escalation to a single table lock. This was consistently reproducible. To minimise the objects involved the table was a heap without indexes. I used Extended Event tracing to capture and events. For ease of analysis I used separate traces during the UPDATE and ROLLBACK phases. Two sessions (SSMS windows) were used - one for the DML statements and one for the trace DDL. I could not use as single session as I wanted to stop and start traces while the transaction is open and this is not permitted. The isolation level throughout is READCOMITTED. The result of is 

Well, you know how much you've transferred (previous question) but since you've no strong conversion rate or wastage you cannot know how much is actually in their workshop and how much has been scrapped. For that the labourers would have to report scrap and wastage directly. 

You will be able to avoid a lot of your race conditions by performing many of your steps in a single statement. By using a cluse will be able to set the flag on at most one row. By using the cluse you can return this to the application automically. I define a simple test table and populate it: 

If all apples are green and all bananas are yellow a table would be appropriate, with your data table having a foreign key to it. If you have yellow apples and orange bananas (yea for genetic engineering!), but only certain combinations are permitted you will need with the latter two as FKs to your option 2 tables above and your data table having a FK to FruitRainbow. If any fruit can occur in any colour, and you don't want to limit those combinations in advance, your option 2 is fine. If your query is really about resource optimisation rather than relational integrity then you'll have to decide what you want to gain and what you're willing to trade to get it. By using integer FKs instead of natural name you get a smaller disk footprint at the cost of runtime load. There are no free lunches. Pick your problem and solve it in the knowledge of the compromises it will entail. 

If you can apply partitioning to the table(s) in question you can swap out whole partitions then drop the swapped partitions. This is extremely fast and resource-light. It requires minimum versions and editions, however, and your question's tags don't specify this. It also imposes some prerequesites on your tables' keys. 

Yes, deleting the unused images will free up space inside your DB files. To actually reduce the size of those files you will have to them after the images have been deleted. Here's a link to the MS Technet site for this process. Shrinking the files will most likely introduce internal fragmentation into your tables and indexes. A thorough would be appropriate afterwards. On a 10GB database, once a year, this shouldn't be an undue burden. 

The CASE statement will work. It returns a value depending on a condition, so you can assign each invoice's value to exactly one age bucket. To be able to roll up the values at whatever granularity required use a sub-query to put values into appropriate buckets, then in the super-query perform the required grouping. 

If you rename the existing, underlying table and use the table's current name for the new view you shouldn't have to change any SQL in the application (except for references to the column, of course). Whether you should have the column at all depends on the application's needs and your operational constraints. If you don't need this history for audit/ reporting/ whatever then get rid of the column and rows when they are no longer useful. Smaller databases are better databases! If you must retain old records moving them to an archive table is a good solution. It keeps the operational table small and access fast (did I mention that small is good?). You may have foreign key constraints in place; that will complicate matters. A from the operational table and an to the archive table will be more work than a simple to the column and hence slower. If this is a problem, marking them inactive, then moving a short time later in a batch process could work. 

For interests sake, here's the estimated costs against Geoff's sample data (thanks for that!) after various changes: 

Of course you'll have to choose a working language for the keys. In a graph DB, add each word from each language as a node. Label the nodes with the language. Alternatively have a separate node for the language itself and connect the word-node to the language-node. Then connect word-nodes in one language to the corresponding word-node in another language. Note that English has a lot of words imported from other languages so decide whether you have one word-node connected to several languages or separate word-nodes. In a relational database I would have a table for the languages, one for word lists and a third for equivalences 

Name your relationships and things will become clearer. Use specific, strong words that are common in the problem area. "Garth Brooks ... has a song" is ambiguous and weak. He wrote the song, published the song, owns copyright on the song, recorded the song, has custody of original manuscript of the song: these are strong, meaningful statements about the artist's connection to the song. Each will produce a separate relationship in the data model with its own cardinality and membership. 

As for the second part -- "Is it possible to enter the records with keys in a different order to have a tree of less height?" With 5 keys and two keys per node we need at least 3 leaf nodes to hold all values and a height of 3 to form the tree. So my arrangement is optimal for the given data, sequence and algorithm. The book uses a very different pointer arrangement to what I use, and a different page split arrangement. This will be significant, leading to part-full pages. That there is a section on page 42 called "Data Loading" that shows how fuller pages can be achieved by loading out of key sequence supports my hunch. However, I hope I've given you sufficient pointers and you'll be able to use the book's pointer structure to work that out for yourself.