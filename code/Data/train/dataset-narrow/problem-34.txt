I don't see a point. An external attacker (who has access to the host) can easily find where the files are - he only has to look at the last file system layer and will have them presented right there for him. An internal attacker (who gains access to the container through a misconfigured part of the app or whatever) will also rather easily find them by looking through the process table at and noting the CWDs. Attacking the image might be easier with a fixed location, as the attacker only has to add another FS layer, overwriting that directory with his own application. But then, if you did not secure your image store, it does not matter anyways what else you do. So... I'd say the convenience of a common location is good, and little to no security would be gained; and if, then only security by obscurity, which does not count. 

When people talk about running a database in Docker, they do not mean to store the data in a container; they are talking about having a docker image with the DB software, and mounting the data as a volume (a bind volume, not a container volume). Volumes are an essential part in Docker, and are not something that is flakey or just tacked on. Docker is not just made for stateless (micro)services. Wish as I might, I cannot find a technical reason not to run a database in a Docker, so unfortunately I'll pick the other side of the argument and hence maybe not give you the answer you are looking for. (I'm using Oracle as an example because I'm familiar with it, both bare metal and dockerized, and because it's quite a notorious beast for being just a bit non-trivial to operate if you go past default settings.) 

Note that in your case you want to make it hard for a thief who knows that he is doing something illegal and already has committed to doing a crime. He will not respect any "weak" protection (like rules/laws) anyways. 

All in all, this would probably not be that much more effort than using Jenkins - with Jenkins, you'd write the content of your scripts somewhere in the job definition, anyways. Obviously, you do not get all the fluff that Jenkins or other solutions give you - statistics, reports, GUI, load-balancing the worker nodes etc., but as long as you do not need those, why not. 

I believe DevOps is orthogonal to your question, i.e., it changes nothing compared to a "classical" approach (or to hiring any team at all, not only for software development). You identify what your key needs are (for example, an "architect" who is able to structure large software systems; some "hacker" who is able to fix a Kernel driver if needed; a "tester" who likes to test and such; and finally maybe a "DevOps engineer" who excels in creating good CI/CD tooling). But all of them need, to stay with your image, to be able to kick the ball. I.e., they all need to work together in the context that the team is working in. They all need at least some basic understanding of what the other team members are doing; if you have a strict CI/CD pipeline, then they all need to be able to develop in that frame; and so on. 

Even with the rationale you edited in, I don't really see a point. DevOps is an intersection of Dev, Ops (and more, like Security and maybe Testing) in a cultural sense. But on a technological sense, you will always have people in a mixed team who are good at one of those but not the others, and that is fine. For example: you might deploy something like Kubernetes or OpenShift; and then you will for sure have ops-sided people who are expert at running the low-level stuff, tuning your NAS, adapting your orchestration to a cloud provider of your choice, or to on-premise bare-metal servers and so on. You will not, usually, have all of your developers do the same; they may use those things provided by the others. The ops guys might never see a piece of application code at all; and you would still call it "DevOps". What your devs will be able to do is manage the full lifecycle of their applications, from source code to production, using the tools provided by the ops guys. When one says that devs do jobs that ops used to do in the past, one means that the automation makes it so that the environments (dev, test, int, prod...) are mostly the same; that deploying to them is as automated as humanly possible (without requiring manual ops, and most importantly without requiring a handover of binaries etc. between devs and ops); that there is a complete, unbroken technological chain from source to prod; including things like virtual networks automatically springing into existence and so on. One does not mean that devs troubleshoot hardware or networking on a low layer. The same goes for your ops guys. They will most certainly not try to change the application code proper after a few seminars on it, unless you are talking about really small companies with 3-person teams who need to do so out of sheer necessity. Note that I am not saying that it is not possible to be one person who does both development and operations work. Is is an absolute gold mine to have such people available; and some companies foster that by, for example, rotating devs into low-level ops groups occasionally. But those guys are not your target audience. By the topics you listed in your question, you are targeting ops guys who have never in their life coded anything ("hello world"). If you actually want your ops guys to grow into coding instead of just clicking around in GUIs, modifying or creating config files and such, then here are some suggestions: 

We have a brownfield project which was recently struck by disaster (many people leaving for unrelated reasons, too many new customers, much too large backlog for much to few people, etc.). Management support is still fine. Thank bob the product owner (decades of knowledge) is still here. The devs have seen some glimpses of DevOps benefits from a very nicely-received DevOps colleague who is doing practical work as well as evangelisation, so there is no resistance on that front, either. At the moment, we are bringing in new people from all over the place (i.e., split across different cities and even countries, though all speak the same language), and getting the know-how under control etc. We are also focusing a lot on automation before starting actual development again, because everybody saw what happens when we don't. Also, we have bundled everybody together - devs, ops, testers, to make use of every last bit of knowledge and avoid future silos. So it is a very dynamic situation. My main objectives are: 

Is it possible to have several brokers behind a single TCP/IP port? I.e., offer a single endpoint for Kafka clients, and having a scalable amount of brokers behind that? In my company, some people are configuring a (scaling) Kafka server for us, and they insist that this is not possible, and that we need to configure every single broker as a separate endpoint... This is in a OpenShift context; it would be nice to just be able to scale the Kafka pods up and down as needed, while using the usual loadbalancing to offer a single port to the outside world. EDIT: I am familiar with HAProxy; the question is about Kafka, not OpenShift/HAProxy. Is there anything special to be configured (in the Kafka broker image) so they are all synchronized amongst each other? Are there examples for that somewhere? Googling for "openshift kafka" etc. brings up plenty of examples, but all that I found seem to have a different service for each kafka broker (i.e., explicitely running 3 separate Kafka deployments inside Openshift, with separate OpenShift services, one each). This is what I wish to avoid. I want to horizontally scale the number of Kafka processes behind HAProxy, and have one single external IP. 

All of these tools do much more than what you need, but all of them give you a way to do your job incrementally. Vagrant, Ansible and Docker are pretty easy to learn, as far as I'm concerned (as long as you are in Dev/Test mode, the "interesting" parts start when you go to production). Ansible is very minimalistic and needs nothing except a ssh connection. Vagrant and Docker might not be feasible in your infrastructure, you will quickly see. 

I view Jenkins one step "lower" than managing releases. In "pure" CI/CD, the final stage, there would be no releases anymore as every commit would directly and in real-time end up on production anyways. At the other end of the spectrum, you have something like 3 releases per year, each one of them late by 2 months ;). Every project will be somewhere along that spectrum, as dictated by circumstances (not the least of which are your stakeholders and the available money - running a very fast CI/CD is not free). 

So there you go. By all means do dockerize your DB, at the very least for your developers (who will be eternally thankful) and your testing environments. On the production, it will come down to taste, and there at least, I would also prefer the solution that sits best with the specialized DBA/Ops - if they have decades of experience working bare metal DB servers, then by all means trust them to continue so. But if you are a startup who has all IT in the cloud anyways, then a Docker container would just be one further piece of onion in the whole picture. 

Well, the name "integration" test means to test different pieces of the system together, I would not call a test that only uses one component an "integration" test. You'd wire them up like in production (in a testing environment, of course), provide test data and let them do their thing. The components themselves do not need to know - they should not know - that they are under test. The "wiring up" should be as similar to the production environment as possible, re-using configuration code where possible, to test those bits&pieces as well. Don't put them in the same repo just because you do integration tests. To see whether the components work together correctly, you would have a test driver that exercises several of them together. It will set up the test data and look at the end results. If everything is green, then the APIs have, by definition, been used correctly. This means that there should be plenty of integration tests - if in doubt, I'd err on the side of having too many integration tests rather than too many unit tests. Obviously this depends a lot on what you are actually doing... 

EDIT2: I am still investigating. The basic form of the Dockerfile (with the 3 lines) is not the culprit. Consider: 

Plain, default, simple, works. The sources are on a shared/synced directory, i.e., they are usually edited on the Windows host and then compiled and ran inside the VM. Unfortunately, I prefer to use Linux development tools myself. Right now, I have this setup: 

I have never heard of a common name for what you describe; and rarely, if ever, have I seen a notation where someone references to something which has a "begin" and "end" version. It just is not that applicable in a general fashion; this is probably why there is no such name. Also changelogs usually don't contain this information except for extremely bad breaks that need some kind of notice about which span(sic) of versions one should rather stay clear of. The closest you could get to a common name (popularized by Jira, for example) is probably to call 1.3 in your example the "affected version", and 1.5 the "fixed version"; possibly allowing multiple values there. But they also have no name for the span as an entity. Note that it is not really a problem if you have multiple old maintenance releases like you are describing. Those would just lead to multiple spans distributed over the branches. Regarding your edit: 

You should be able to use (or maybe ) to use a specific source address. For , it's etc. Start the curls in the background ("&") and they will run more or less concurrently - or look for some test driver (different question) to run them for you. This will give you lots of requests from separate IP addresses, but all conveniently running on one machine. Docker, trivially With docker: just make an image, as small as humanly possible, which connects to your server. You can use without any special options. Then fire up your 1000 docker containers, just like you usually would. Each will automatically get an IP address on the internal docker network (172.x.0.0/16 by default). $URL$ indicates that it is possible with hardware that is not too uncommon today; a naive 1000 docker containers used roughly 3 GB of RAM for them. That question also has some hints on options you can use to decrease the memory needs. Aside from memory, there won't be too much overhead, i.e. CPU wise it should comparable. Docker, with special networking opens up manually created networking for you. You should be able to cobble something together (just check the individual pages, it's all pretty self explanatory), and combine that with the approach explained above. This would mean that you only need one container for your s, and start 1000's of them inside that container. I would try to just run your 1000 containers, first, though, unless you are limited in the RAM department, for ease of use. About the ... Whatever you do, don't just start a single in each instance, but have each runner be an endless loop of back-to-back calls. This way, you avoid long gaps inbetween where your individual jobs are being started and stopped; this is obviously especially important for the "1000 docker containers" variation. In the answer linked above, they note that it took them some good amount of minutes to start up those thousands of containers, so you do not want to do that over and over again. And you might as well get rid of altogether and create small networking app in the language of your choice (Perl, Ruby, C, Java...) to avoid process startup overhead.