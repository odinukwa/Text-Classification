The function $\Psi$ is randomized, using some (public) random sequence $r$. Solving $\Psi(\mathcal C)$ is typically as hard as solving $\mathcal C$. If a solution $x$ is found for $\Psi(\mathcal C)$, then a solution $\Psi^{-1}(x)$ can be efficiently computed for the original challenge $\mathcal C$. Knowing a solution for $\mathcal C$ does not help in finding a solution for $\Psi(\mathcal C)$. 

Let us fix an encoding of Turing-machines and a universal Turing-machine, U, that on input (T,x) outputs whatever T outputs on input x (possibly both running forever). Define the Kolmogorov complexity of x, K(x), as the length of the shortest program, p, such that U(p)=x. 

The private coin (one- and two-way) randomized complexity of ANY function is at least log log |size|, so e.g. in your case at least log log {U \choose m}, which would be log log U if m is small, which can give a better lower bound. This result is mentioned in Yao's seminal paper on CC, you can find the proof in my master's thesis, lemma 3.8 and around: $URL$ Of course this is just a lower bound, maybe their is a matching upper bound like m + log log U. 

Depends on your definition of advanced, and what sort of AI you want to study. Many problems in AI are provably intractable-- optimal solutions to POMDPs are provably NP-Complete, optimal solutions to DEC-POMDPs are provably NEXP-Complete, etc. So, absent some unexpected breakthrough in complexity theory, the more one knows about approximation algorithms and their theoretical underpinnings, the better. (In addition to the measure theory, etc, needed to truly understand the Bayesian probability that underlies the POMDP model.) Multi-agent artificial intelligence, in particular, intersects with game theory; so knowing game theory is helpful which in turn depends on topology, measure theory, etc. And likewise, many problems in game theory are intractable. Some are even intractable under approximation and even understanding when it is possible to usefully approximate takes a considerable amount of mathematics to work out. (I note that the game theorists have been having a pretty good run in the Nobel Economics field, for the past few years, and that's heavily mathematical in nature. I predict in twenty odd years, today's algorithmic game theorists will be in about the same position.) 

Edit 5th of Jan: In fact the One Heap Game described below is a special case of the problem, i.e. when the numbers follow each other in a specific way such that the first group is bigger than the second group which is bigger than the third etc. and the numbers in each group are increasing. E.g. 8, 9, 4, 5, 6, 7, 2, 3, 1 is such a permutation. So I propose to solve this special case first. Disclaimer: I no longer claim that the below proof is correct, see e.g. the comment of Tsuyoshi which shows that deleting a number from a permutation will give a diagram not obtainable by deleting a square from the diagram of the permutation. I left the answer here to show that this approach does not work, plus since it contains another simple game. The game has a very simple other formulation thanks to Young Tableaux. I am sure that it can be analyzed from there as other games and it will yield a linear time algorithm. First define the following game on Young Diagrams: At each turn, if the current diagram is horizontal (all squares in one line), the current player loses and the other player wins; otherwise, the current player removes one of the bottom-right squares, and play passes to the other player. Now order the sequence of numbers into a Young Tableaux. The main claim is that the winner of the original game is the same as the winner as the diagram game starting with this shape. To see this, notice that whenever the players delete a number, the diagram of the new sequence can be achieved by deleting a bottom-right square of the diagram. Moreover, any such diagram can be achieved by deleting the number from the respective bottom-right square. These statements follow from standard Young Tableaux theory. Although this diagram game is simple enough, it is trivially equivalent to the following game, which seems more standard: One Heap Game: The players are given some heaps with some pebbles in each. At each turn, if their is only one heap left, the current player loses and the other player wins; otherwise, the current player removes a pebble from a heap, and play passes to the other player. If there is a simple solution to the heap game (and I strongly believe there is one) we also get a solution to the original game: Just put the sequence in a Young Tableaux, and transform its diagram into heaps. Unfortunately I do not see which heap positions are winning/how to determine the Spragueâ€“Grundy values. I checked a few cases by hand, and the following are the losing positions with at most 6 pebbles: one heap; (1,1,1); (2,2); (3,1,1); (2,1,1,1); (1,1,1,1,1); (4,2); (3,3); (2,2,2). Anyone can solve this game? Edit: Peter Shor can, see his answer! 

It is easier for me to think of applications of computer science (techniques) to game theory, than the other way around. There is a very active field of algorithmic game theory which focuses on the development of efficient algorithms (or complexity results) for, e.g., Nash equilibria, Shapley values, and other such standard game theoretic concepts. Often, these concepts are easy to define, but prohibitively difficult to compute directly from the definitions. This work extends at least as far as mechanism design, where we attempt to manipulate the rules of auctions in order to guarantee agent behavior (e.g., we would like them to report truthful bids) or overall results (e.g., we would like to guarantee maximal revenue.) Noam Nisan, Yoav Shoham, Tim Roughgarden, and many others have some fascinating papers on the subject of mechanism design from a theory point of view; Vince Conitzer has applied AI techniques to the problem to develop automated mechanism design. On the more applied side in artificial intelligence, it's difficult to think of multi-agent systems without thinking of them as games. The Partially Observable Stochastic Game (POSG) framework is often used to discuss to multi-agent settings; under the right reward function criteria it becomes a DEC-POMDP. 

The 0-1 principle says that if a sorting network works for all 0-1 sequences, then it works for any set of numbers. Is there an $S\subset \{0,1\}^n$ such that if a network sorts every 0-1 sequence from S, then it sorts every 0-1 sequence and the size of $S$ is polynomial in $n$? For example, if $S$ consists of all sequences where there are at most $2$ runs (intervals) of 1's, then is there a sorting network N and a sequence that is not ordered by N if all members of $S$ are ordered by N? Answer: As can be seen from the answer and the comments to it, the answer is that for every unsorted string there is a sorting network that sorts every other string. A simple proof for this is the following. Let the string $s=s_1\ldots s_n$ be such that $s_i=0$ for ever $i<k$ and $s_k=1$. Since $s$ is unsorted, after sorting $s_k$ should be $0$. Compare $k$ with every $i$ for which $s_i=1$. Then compare every pair $(i,j)$ such that $i\ne k$ and $j\ne k$ many times. This leaves the whole string sorted, except possibly for $s_k$, which is unsorted for $s$, and for certain other strings that have more $1$'s than $s$. Now compare $s_k$ for $i=n$ downto $1$ except for the place where $s_k$ should go in $s$. This will sort everything but $s$. Update: I wonder what happens if we restrict the depth of the network to $O(\log n)$. 

Antenna design has already been mentioned, and it is an extremely rich domain. (It is, very directly, what started my motion from electrical engineering to computer science (in the late 90's) and more specifically to bio-inspired computation and artificial intelligence (in the last five years or so.)) In the same vein, I'll add antenna array optimization, especially for phased array optimization, which is all of the headaches of antenna design, and more. There are opportunities in the entire field of electromagnetic device design, really: Antennas, antenna arrays, microwave filters, optical gratings, metamaterial device design, all off the top of my head. A dated survey is Electromagnetic Optimization by Genetic Algorithms, and a more recent survey is Genetic Algorithms in Electromagnetics. (I should really buy that second one. I've seen a lot of good papers on non-electromagnetic circuit design as well: GAs coming up with competitive op-amp or other integrated circuit designs, GAs "learning" to take advantage of the analog imperfections in FPGAs to implement analog functions like clocks, etc. Even something as simple as dumb, discrete element filter design can be a target for GAs: I've seen one that factors in q-factors, tolerances, discrete values and soldering parasitic models to get good, manufacturable filters from the parts you have at hand. These often involve some novel (to me, anyway) circuit representations to get the genetic operators to fit the paradigm, as well as variable size chromosomes. 

$NEXP^{NP}$ is (probably) bigger than NEXP, as we can ask questions of exponential length from the oracle. That NP in the power is practically a NEXP there, so eg. co-NEXP is contained in $NEXP^{NP}$. 

Also not an answer, but you can decide st-connectivity in $O(n\log n/ k)$ non-deterministic space with $k$ passes. Just guess the first $n/k$ nodes of an $st$ path and check that they are connected in the first round, then continue from the last of these $n/k$ vertices and check the next $n/k$ from the $st$ path and so on. 

So I give a sketch why the problem is NP-complete. It is very sketchy, which you can take as a sign of trust that you're a smart guy, and not at all a sign of laziness on my part. We will reduce a variant of PLANAR-SAT, where we also require that the edges connecting the variable to its negated and unnegated occurrences form adjacent intervals in the rotation of the vertex of the variable; e.g., if each variable occurs at most once negated. The matrix will have a small top-left corner that will contain the important information, and many additional rows and columns to impose a structure on this part. In particular, I claim that with properly chosen additional rows and columns, we can achieve that instead of arbitrary permutations, we can restrict the problem to permutations that do not change the rows and can swap only given pairs of adjacent columns, or otherwise the number of components would be larger than $k$. If we can achieve this, then in the top-left corner we "draw" the graph of our PLANAR-SAT, such that at the heart of each vertex there is a pair of swappable columns. Every other row is constant on these two columns, so only the neighborhood of the vertex is effected. And at this vertex the negated clause-edges come from one side, the unnegated from the other, the swappable column decides which ones are connected to some main component. Therefore, the CNF is satisfiable if and only if all clauses can be connected to the main component. Since I didn't provide any details about the additional part, it is not clear how $k$ depends on $n$. Can $k$ be kept constant?