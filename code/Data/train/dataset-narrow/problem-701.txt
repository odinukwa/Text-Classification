The servers are using Dynamic quorum which is the default setting on Windows Server 2012. You can check the current votes of the cluster by running 

then create a secondary index for the { VALUE | PATH | PROPERTY } you are querying with Xpath $URL$ and $URL$ 

If you are using Windows Authentication in the linked server setup it will not work using the SQL Server agent even if you have Kerberos correctly setup. You can configure the linked server to use SQL authentication for those connections but I would recommend to create a SSIS package for this as the authentication will be easier to configure. Can you please post the linked server configuration for further information? 

You are running into a known bug - There is a fix for it but update to SP2 rather than installing the hotfix. The errorlog contains and you are using availability groups. The memory leak is in the MEMORYCLERK_SQLLOGPOOL which is used for transaction log activity. 

After you run the Mofcomp tool, restart the WMI service for the changes to take effect. The service name is Windows management Instrumentation. Then you might need to register two .dll's In your case: 

For each file which is in SIMPLE recovery and not having wait_desc = "active transaction" you can simply do 

So in your case. Do a Partitial backup of of the Primary filegroup and the Read/Write filegroup and restore that with recovery. The RO file will be recovery pending and then you can restore that filegroup with recovery to get the database up to date and then reload the data. 

You need to check the actual size of the database files which you can check by running This will give you information about the files contained in the backup and their sizes in bytes You need the same space available on the server as the database file size, the backups can be compressed. Now you need to free up more space on c: on the server but I would recommend that you add drives to your server and that you would look at storage best practies 

Create an XML index on the column, you will have to create a primary index and then a secondary index on the path or value you are querying. 

A backup is a full copy of all the database pages, or a dump of the database if you like, starting at page 0 and written sequentially to the backup device. The Data-Tier application is a set of scripts that will create the database objects along with insert statements that will insert the data into the table objects afterwards. From a backup perspective the Data-Tier application is a snapshot of the database in a certain state, you can install the application in that state but you can not restore additional data but have to copy the data from another source. The backup is a consistent state of the database and you can make log or differential backups that you can use to restore the database to a point in time. 

You can skip the filecopy step and just insert the data directly to SQL Server. (use EPEL repository) and then use freebcp to insert the data into SQL Server from the linux box. 

Which creates a linked server named db_instance2 and tells the server to use the logged on user credentials to connect to the linked server. If you have configured Kerberos correctly for DB_INSTANCE1 users with permissions on both the servers will be able to select data from the other server: As stated before, for this to work Kerberos has to work and therefore you will need to setup SPN's and do some active directory magic. The Kerberos configuration manager will help you and your AD administrator to set up everything needed It also means that users that connect with database logins will not be able to connect to the server - For that you will have to create user mappings either in the GUI or using TSQL 

and for each page find to which object it belongs When you have that information you can hopefully copy the non damaged data from the database into another. 

Use the SQL Server configuration manager to change the SQL Server Agent user account. Change it to run as local system, apply and restart and then back to the correct user, dont add any privileges to the user in the operating system (esp. not add it to the server local administrator group). The Configuration manager will set all the correct permissions for the account. 

The DMV's themselves are the same between same builds of SQLServer and give you a snapshot or a cumulative counter of the status of your server at any given time. You cannot restore the database to a new server with the metadata from the old one. You can make snapshots of some of the performance metrics into a table but you cant restore the state of the DMV's. You can however grab the query plans from your production server and use plan guides to have the same behavior in test, if you want to mimic specific behavior of a single query that is not behaving in production environment but runs ok in test. You can then of course use the Management Data Warehouse to collect information about performance of queries and the system to find which queries to optimize or third party tools that can collect query plans and wait statistics on the server. Here is a way to collect the query plans using sp_whoisactive 

Now also make sure that the tempdb files are created such that you don't need auto growth and that they are all the same size with the same growth parameters not in percentiles. Your tempdb is now ~140 GB so that is about the size you need to provision for. Create 8x15-20 GB datafiles and a single log file (as transaction logs are used sequentially) set all the files with exactly the same growth parameters, something that makes sense on your storage 512mb is fine if you have instant file allocation. As your transaction log is just about 5GB create a single 6-8GB file for the transaction log and set autogrowth to be in MB. Log growth is not affected by instant file allocations so make that parameter smaller than the one on the database files. If you still see contention then go for 16x 8-10 GB files and dont change the transaction log parameters 

If the instance name is .\SQLEXPRESS you have an instance of SQL Server Express but not localdb. The default install will give builtin\administrators permissions on the database so you will have to start the Managment Studio (SSMS) by right clicking the shortcut and select run as admin to have an administrative token and be able to create databases. When running the SSMS as admin you can add your user account to the sysadmin role in the database server and after that run things using your user account without elevation. 

But for this to work you will have to make sure that you made a backup of the original certificate with the private key. 

Both of these are user processes and are part of an application, you can check the outer query for the cursor with the following 

As Explained at WindowsItPro " When there are only two nodes left, one of the nodes loses its DynamicWeight so only one of the nodes now has a vote (this is chosen randomly). This assures that if the second node crashes (the node without a vote), the first node can stay active (giving you a 50/50 chance of surviving an unplanned failure of a node). If the first node was taken down cleanly, then the second node would be given the vote and the cluster would stay online." 

To create a certificate for SSL encryption for SQL Server you need a certificate for Server authentciation. That is [EnhancedKeyUsageExtension] OID = 1.3.6.1.5.5.7.3.1 If you want to manually create a certificate for this using Windows Certificate services you need to do it like this. ($URL$ Create a file named cert.inf 

Managed service accounts are supported in SQL server 2012 onwards to run it's services. There is no 'correct' way but managed service accounts on the application server are normal domain users as the SQL Server is concerned so as long as you can use them for the application they are a nice option. You might have to type the name in (domain\appuser$) as older versions of SSMS don't know the object type but you can add them manually To minimize the configuration changes you can do the following: 

This will dump the page header which you can use to decipher what is on the page. From the error message posted there seems to be errors in the GAM/SGAM/PFS for pages 72792-80879 so you can look at which object is stored there by dumping the headers and check the object_id. The syntax for DBCC PAGE is dbcc page (database_id,File_id,PAGE_ID,0); The zero is for dumping the page header but you can dump the whole page by changing that last flag 

The destination path for the LSCopy job has to be an UNC Path for all practical purposes, to make things simpler to debug I would reccomend not to use the default \d$ shares as you will have to change the permissions on them but connecting to \localhost\c$ is a network connection. Now the error 

In both cases you end up with a CRT that you can import to the machine store of the SQL Server and then use the SQL server configuration manager to encrypt the connections. You will have to trust the root certificate in the latter case though and in the former case, if you are not running on a domain you will have to install the root certificate for the windows CSA on both the machines