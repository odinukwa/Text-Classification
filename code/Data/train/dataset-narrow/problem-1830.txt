According to your firewall logs, you've forgotten to allow dns queries. Allow port 53 udp and your setup will work better ! 

Which user is running mysqld? It is likely that the user who runs mysqld, usually mysql, does not have write access on /home/site/public_html/mysql-logs/ Therefore, it cannot write log files. Try creating a group for both /home/site/public_html owner and mysql, then chmod 0775 directories and chown directories 

I'm also just shooting blindly here: Maybe ovh has enabled some kind of "network protection" to prevent servers being compromised by brute force attacks. To figure out, try to run run sshd on a different port eg 6022 and see if the problem persists. 

"what happens when server #1 is under such heavy load that he cannot handle the requests properly" Usually clients get "connection timed out" error message. Don't worry about the load balancer : with modern hardware, they can handle thousands of requests, especially if you use cache for static content. Most of the time, the bottlenecks are : 

There is another solution to your problem. Instead of making wget convert those links to the new domain name, you can make your webserver rewrite links on the fly. with apache, you can use mod_sed to rewrite links. eg : 

Depending on your monitoring system, you should be able to write a script that parses the output of ifconfig and displays an alert if the number of errors of dropped packets is too high. 

You have to recompile ioncube or reinstall it correctly. To have a hint about whet went wrong or what dependency is missing, just type ldd /usr/lib64/php/modules/ioncube_loader_lin_5.6.so 

Your issue was solved by rebooting the box. Therefore, we won't be able to determine the root cause of the problem. 

You can try to type mount -a just after system has booted. If this workaround works, you can set up a script whose content is just something like "sleep 60 && mount -a" and make it executed at boot time (via cron, systemctl or any other mean). It's really dirty, the good solution would be to investigate why some fs does not mount correctly. 

If i understand your question correctly, you want to receive all mail for test@example.com, but none of the mails sent to joe@example.com 

Howerver, it is likely to log both to syslog and a logfile. If so, you can keep on playing with configuration file to achieve your goal... Hope this helps 

We have to find a string, let's say "foobar" among several webapps. However, some webapps contain zipped files, eg log4j.jar. Therefore, grep -IR "foobar" /pathto/tomcatroot/ won't work, because of compressed files. unzip -c /pathto/tomcatroot/libdir/log4j.jar |grep foobar can solve the problem, but only for one file Is there a way to achieve this for a whole directory? 

It looks like your dhcp server is broken. According to your question, the dhcp server does not provides the correct gateway to your client. Try setting a static IP adress by ediiting /etc/network/interfaces (you'll find correct syntax on the internet). 

sender_canonical_maps (default: empty) Optional address mapping lookup tables for envelope and header sender addresses. The table format and lookups are documented in canonical(5). Example: you want to rewrite the SENDER address "user@ugly.domain" to "user@pretty.domain", while still being able to send mail to the RECIPIENT address "user@ugly.domain". Note: $sender_canonical_maps is processed before $canonical_maps. Example: sender_canonical_maps = hash:/etc/postfix/sender_canonical Source : $URL$ 

"Only option left after reading quite a number of forum is to make the ssh key same on all the server. " I think there is another option : you can use ssh certificates. 

I have no idea how it works with storcli64, but I just had to replace a raid1 failed drive (slot 0) with megacli64 : 

It looks like a dns issue. To figure out, try in cmd.exe : ping myhostname It should not work. Then you have two option : 

" node uberlamp3 advertised itself as uberlamp.14.by instead." What happens if you type "hostname" on the client? Have you checked munin client configuration? 

It seems that everything is okay but your telnet command tries to open port 23 and not 10000. ( "connecting to ...could not open connection to the host, on port 23: connect failed") try 

apt-get install libssl-dev You must install openssl development files in order to compile php $URL$ Also, once you compile php successfully, don't forget to change apache module to the right directory for php.so There must be a line looking line LoadModule php7_module modules/libphp7.so in apache configuration 

mxtoolbox has a pretty comprehensive blacklist checker. I know you already checked but this might be useful to others who come across this post as well. Your best bet - as mentioned by caelyx - is to implement DomainKeys and be done with all this nonsense. Yahoo and Gmail both green light dkim signed email till they have reason to do otherwise (users tagging it spam). If this is at all business related it's a pretty obvious choice ROI wise. An hour or so of configuration and testing equals much fewer issues with spam filters. 

For a quick change do: "MaxRequestsPerChild 4096" to something like: 700 will help. The longer an apache process lives the more resident memory it's going to consume due to mod_php and the like. Also, enable keepalive and place aggresive timeout settings for it: 

Does the originating/source IP show up in that log output? If yes does that IP show any valid requests in the http logs? Perhaps a monitoring system of some kind is checking http on your server, since you said it was in consistent intervals. Just throwing stuff out there. 

We have outgrown the use of a single server for monitoring our network so we are looking to add another monitoring server. Unfortunately, we were shortsighted when we initially configured the hosts being monitored. We configured the hosts on our network to only allow monitoring from a single IP. To add another IP to all the hosts firewall rules is not logistically possible. I won't go into why but suffice it to say the servers have a mix of owners and cannot be updated at will. So initially we were thinking of just having an OpenBSD box doing NAT with PF in front of the two servers. We're not thrilled about the additional complexity or, most especially, the single point of failure. So, can we do some magic to allow both servers to use the same internal IP address without adding another host? Also, the system needs to continue working if either one of these hosts go down. So doing the NAT on one of the two hosts and routing the second server through the first isn't really enough. We basically need failover source NAT in front of the shared IP...without adding two additional boxes. 

If in fact it isn't what Frenchie said, which it most likely is, it may be helpful to look at that users mail. Cron mails output from cron jobs to the user account of that crontab. That is why you often see STDOUT and STDERR piped to /dev/null, so they wont be mailed output they don't care about. You can use the mail command as that user to check for mail with helpful output. Also, the /var/log/cron file may include helpful information. 

Sounds like your on a tight timeline, so you should probably get some help with this one. If you do choose to go it alone you should know that most VPS accounts are a bit stingy on RAM. It's common to use a web server besides apache for this reason. Lighttpd, Nginx, etc are good choices. Stick with Ubuntu or another distro, they will have all the packages you need available in the default repo. I run CentOS but I end up adding additional repositories and setting repo priorities. Since you aren't all that familiar with such and are on a timeline you should probably stick with a debian based distro at first. Work on getting the site up under an alternative domain such as dev.domain.com so you can test it before the switch date. Only change one thing at a time and test it fully. Don't add APC, additional mysql options, ssl, all at once. I honestly believe that this could be a great learning experience in the long run but disastrous in the short. Everything I know about hosting is due to three years trial and error..not three weeks or so. Good luck. 

From the check_http man page: -k, --header=STRING Any other tags to be sent in http header. Use multiple times for additional headers $URL$ 

Not only could you still perform your job function, but probably better than if you were using a Linux destop manager(on the server that is). As a Linux SA for the past three years I have yet to use Linux as a workstation and have never once needed to use X or any window managers on a server. I have seen a few servers running a full blown desktop manager before; always assumed the tech who installed it was clueless. I use mac as my workstation and keep a few virtual centos instances in parallels for trial and error. I run macvim as my editor and the rest is standard. 

Backups per continent in the server and a separate S3 instance and the worse case scenario the dev-server is the final backup. Now, think an image folder like a big "image-basket". So, how can the "image-basket" be commonly attached to EVERY website? All the servers will use (i think it would do the trick easy) per 15to30mins in case to re-syncronize themselves for core files like the "HTML/CSS files etc." Do I have a big overhead here? Is there any other solution to "keep the servers updated all the time" ? Of course, one will be the primary, and as I believe the major-first-primary server will be the developer server that hosts the essentials like administration panels etc. So, after the "push" the dev-server will "push" or "rsync" everything to all the three Regions-servers. As you can understand, it is going to be a huge issue if I "rsync" the /image/ folder. We have to talk about 500+ /image/ folders of aprx. (right now) 20 GB each. So we need 500*20GB = 10.000GB extra for the images.. This is not a case as you can understand. 1) One "dirty" solution might be, to host in one server all the images, and every website using an HTTP request to get the correct images. Pretty bad solution because you might DDos your own application in high volume traffic hours. 2) In my opinion and point of view, the /image/ folder has to be in a separate instance or "whatever-is-that", to be used rapidly on every single server, in other words, to every single website. Is there any service out there that can be bind 3 at least webserver in different regions? I didn't find anything close to that yet. At least, do you think any solution for the "sharing-image-basket" issue? There was a question with something similar: Reliable file sharing but as I see and understand, his case is different. UPDATE-Clarifications 1) A request for (e.g.) /image/foo.png should always return the same file regardless of which site is being accessed, and regardless of which region. 2) What are the geographic references of the regions: 3 different continents. So, three different "private networks" if we are talking about Digital Ocean, or three different VPCs if we are talking about AWS, etc. 3) How much traffic is there: Traffic is not related to the solution. Think a big increasing volume of traffic and requests. It doesn't make sense to have a solution for low traffic and when the traffic is double+ to "design it" or trying to upgrade the CPU/RAM/STORARE-GB. That's why I propose "local" load balancer for a region, with a "replica" machine to distribute and bypass the big volume of traffic. 

As you can understand the images are GEOtargeted, but the names remain the same across all these websites for quicker managing. However, they didn't "design" it well, and the latency between the servers and the management of the images are a complete disaster after a while. So, in a nutshell, 3 Continents with one server and one dedicated MySQL server per continent. One developer server with MySQL in the office. On top of them, a Cloudflare load balancer to distribute the traffic correctly per region and per "server's health". 

As you can see, the curl returns a 404 error. BUT the a URL is live and working like a charm. The problem is on cloudflare I thought. So, I "purge everything" when I curled before but the 404 still exist. I added a custom rule to bypass caching on cloudflare on a spesific url but it didn't work at all. Also, I thought that was some mistake on my .htaccess and I erase everything from there. I restart the apache but the 404 are still exist. As a result, every search engine and every other bot see a 404 header. The problem is everywhere, except on the /index.html and on the wordpress blog which is on a /folder-wp-blog/ folder. This WP wasn't changed at all. So, every other html and php file which is in root is flaged as 404... Any ideas? solutions? 

Be aware, that it might be a case to put a second server in one region depending on the traffic volume. So, the "local" structure will be 

I have just updated a (dynamic) website by sftp/ssh and all the new pages have been considered as 404 error not found. Actually, all the pages, including the old pages which was overwritten. Also, the website is using cloudflare. Let me show you a curl of a url: