We have a cabinet of dedicated servers with a Brocade FESX648-PREM switch at the top. We own all the hardware and all the MAC addresses are known to us. We also fully manage the servers so we retain administrative access, but our customers have administrative access as well. Currently we use VLANs and subnets to isolate the servers, prevent IP hijacking, etc. However, we want to make more efficient use of our IPv4 allocations. We lose a lot of usable IPs for gateway purposes and we have an abundance of subnets too small for many customer needs. I've considered progressively migrating the servers to a single VLAN and use static ARP to prevent IP hijacking. We'd lose layer 2 isolation, but I'm not sure what sort of abuse that would open us up to. What are the dangers of placing all of our servers on a single VLAN with only static ARP bindings to prevent IP hijacking? In other words, what forms of abuse would be possible with a single VLAN config that would not be possible with every server on its own VLAN? Are there any precautions we can take to prevent the abuse while achieving our goals of more efficient IPv4 usage? 

This ended up being a failing switch. A couple days later we started having issues on ports 37-48. The FESX648-PREM is powered by port ASICs which control port regions. Those regions are: 1-12, 13-24, 25-36 and 37-48. One of the failure modes on this box is that a port ASIC can die and cause forwarding problems. The "bad server" above, was the only server we had in use on the 37-48 region. So when we switched the port and re-tested, we had the same result because the failing ASIC affected multiple ports. We replaced the entire switch and that resolved the issue. 

We are in our second week of school, now, and starting yesterday we started receiving complaints about slow wireless internet. After some investigating, we found out that our entire wireless network is being sapped with ICMPv6 neighbor solicitation packets. I captured a wireshark packet captures for 1 minute on multiple occasions, and it is pretty clear that this traffic is the culprit. During times where wireless is running flawlessly, about 1-2% of the total traffic is ICMPv6. When the wireless is bogged down, ICMPv6 accounts for 40-65% of the network traffic. I'm running out of ideas as to how I can fix this. It looks like an IPv6 denial of service from inside. I can't pinpoint it to a single machine, as there are multiple IPv6 source and destination addresses, along with multiple source and destination MACs. I have posted both a good and a bad sample of the traffic at $URL$ We thought we had this fixed after about 11:00 AM this morning, when we found an access point that was causing the issues. We replaced the AP with a new one and everything was fine for three hours until now. Our wireless system is Unifi, by the way. 98% of our laptops are using Realtek 8188CE wireless NICs, and the other 2% are using a Centrino 7000 series NIC, not sure on the exact model. I forgot to mention two things at the initial time of posting. 1) I have two buildings, only one of them is experiencing the problem. The networking equipment is identical between the buildings, and the only thing that is different is concurrent users (only about 70 less than the failing building), and laptop hardware, but the wireless cards are still the same except the working building is 100% Realtek 8188CE. 2) It will work during certain periods of the school day, then it will shut down for entire periods, which we thought pointed to malicious user activity, but any time we thought we were able to pinpoint it, the flood would start again. UPDATE AS OF 8/28/14 11:14 AM It currently appears that a piece of software installed, LAN School, used to monitor the activity of students, may have been the issue. We updated the version of this on every machine at the problematic building, and icmpv6 traffic has since maxed out at .5% and is often at 0%. I'm going to give it another day's time to verify, but it appears to be fixed. 

I am troubleshooting a bizarre case of packet loss. We have a cabinet of servers with a top of cabinet switch (Brocade FESX648-PREM). That switch runs BGP sessions with our transit providers. We have one server (referred to below as the "bad server") that's experiencing 50% packet loss. The server is running Windows Server 2012 R2 and it's been running for months without issue until this morning. At this point, I suspect something might be wrong with the switch itself, so I'm turning to this community for help with additional troubleshooting rather than ServerFault or SuperUser for server-related troubleshooting. This is what I've checked so far to rule out the cause of the packet loss on the bad server: 

I have a Brocade FastIron FESX648 and I'm attempting to increase the ARP cache timeout on a port that is connected to an IXP. According to the Brocade docs, I can increase the timeout up to 4 hours with the setting . I've tried applying this setting globally, specifically to the IXP-facing port and variety of other combinations (lower and higher timeout values), but nothing seems to work. Whenever I run I can see that the ARP entries for IXP hosts are continually refreshed and never age higher than 4 minutes. Has anyone faced a similar problem with Brocade FastIron gear and have any config suggestions or possible workarounds? 

I use an iSavi IsatHub for Internet connectivity when I'm in the backcountry out of cellular service. This device is a BGAN terminal and it works by establishing a data link with an Inmarsat satellite and then provides a WiFi network to connect your devices. Data usage is very expensive, so it offers a number of safeguards to prevent accidental usage. One of those safeguards is a firewall. I only need SSH connectivity, so I blocked all outgoing ports with the exception of TCP port 22, UDP 53 and TCP 53. All inbound traffic is allowed. This past week I was in the backcountry far from cell service. I fired up the IsatHub and connected my phone to the WiFi network. Much to my surprise, I started receiving text messages to my Verizon phone. More to my surprise, I was able to reply to those text messages and have a back and forth conversation. The satellite terminal has its own SIM card and SMS capabilities, but these messages were received over my Verizon SIM. I've searched the net for any documentation on network ports used for WiFi texting and calling. I found this and it does include TCP53 and UDP53 (which I assume are for DNS lookups), but it also includes UDP500 and UDP4500 which are standard IPSEC VPN ports. Does anyone have an idea of how this was possible? How the heck was I able to send text messages over WiFi with my Verizon phone when I've blocked all outgoing ports except those needed for SSH and DNS? Is it somehow related to accepting all inbound traffic? Could Verizon be re-appropriating port 53 for WiFi texting? Edit: When I returned home, I connected my phone (airplane mode, wifi turned on) to my home WiFi network, sent a test text message and took a capture of the network ports in use. The only ports I saw in use between my phone and Verizon-owned IP addresses were ports UDP500, UDP4500 and TCP443 (500 and 4500 being used for the IPSEC VPN ports I mentioned earlier). 

The most recent update on my question was, indeed, the solution. It seems we had a few bad installs of LAN School on some (maybe all) of the laptops, and as soon as we updated it to the newest released version the problem disappeared entirely. Any time we see ICMPv6 traffic, sure enough, there is another laptop without the most up to date version of the software. Immediately upon updating it to the newest version, the ICMPv6 packets disappear again and we end up back to 0 ICMPv6 packets. @RickyBeam Another update. The issue went away for a while, then just came back again yesterday afternoon. After more testing, it is a UBNT issue. We are putting our old HP Procurve wireless back in tonight. It remedies the problem entirely. This is the second time we have had to remove the Unifi product from our network. 

I have a star topology consisting of 12 switches. Recently, we began running low on available IP addresses (192.168.100.0/24) and I was asked to open up a new scope (192.168.50.0/24) on the DHCP server. The scope has been created, I tied it to the same VLAN 50 on all switches, tagged and untagged the appropriate ports and everything seemed to be fine. The clients correctly grab their IP address from the newly designated scope, and they have full network access. The problem we are seeing is that when a host tries to communicate with a host on the other VLAN, there is anywhere between 0-10% packet loss. If I run a constant ping across the VLANs and it isn't dropping any packets, I will get 3-4ms per attempt and then a 30-40ms packet every 15-20. Any host that communicates with its own VLAN is flawless. The switches are mostly HP Procurve 1920-24G, with a couple of 1910-24G models that haven't been replaced yet. I have checked on multiple occasions to make sure that there isn't a loop in the network, because the switch logs all seem to show ports intermittently changing their status to DOWN and back to UP right away over a period of a few seconds. Sometimes this will happen every 90 minutes or so, and the port is not always the same. It just seems quite strange that there are only issues when the communication crosses over to the other VLAN. I can upload any configs or output, if needed. 

Anyone have any ideas where I should look next? The results of #2, #3, and #11 in particular are really throwing me for a loop... 

I'm working on testing several FESX448-PREM switches. One of the switches in my test group is known to be bad. It was previously installed as a top of cabinet switch, 42 servers were connected to it, all port lights came on, full duplex, no errors, low CPU, etc but ports 13-24 would not forward traffic. As I understand it, this is due to a bad ASIC that covers port region 13-24. However, I now have this bad switch at my work bench and I cannot replicate the same forwarding issue with port region 13-24. At my work bench, I have port 1 as the uplink and I've been connecting my laptop to ports 2-48 sequentially using a CAT6 cable while running a continuous ping to a public IP. Interestingly, all the ports now work fine -- port region 13-24 no longer has forwarding issues. Does anyone know how this is possible? If there's a bad ASIC for port region 13-24, then I'd expect this problem to occur 100% of the time. I tried a couple other things afterwards. I had the theory that I needed more ports active at once in order to trigger the forwarding issue. So I first took a layer 2 switch and connected it on a bunch of ports with the FESX448. CPU usage immediately went to 100% on the FESX448. I figured something recursive routing was happening with the layer 2 switch. Next, I put the layer 2 switch into boot monitor mode so it wouldn't do any routing. That resolved the 100% CPU issue, but again I'm still unable to replicate the traffic forwarding issues with ports 13-24. Any suggestions on how I can replicate the forwarding issue and effectively test the remaining switches would be much appreciated! 

I have a Force10 S25 switch in each of my two buildings. One building uses VLAN 200 to work with 10.2.0.0 /16 traffic, the other building uses VLAN 400 for 10.4.0.0 /16 traffic. Our wireless guest network in the VLAN 200 building is segmented off onto VLAN 20 on the network 172.16.20.0 /23. I am trying to add this VLAN to the Force10 switches in order to allow VLAN 20 to pass guest network traffic from the VLAN 400 building as well, since our ISP connection is located in the VLAN 200 building. I have been given a lot of conflicting information between tagged and untagged functions of the VLANs. Force10 told me to set all ports that are NOT trunk ports to untagged, and trunk ports to tagged. ProCurves, however, are the opposite, as well as Netgear switches. Having the ports set as untagged for the host connections and tagged for the trunk ports works, but should I switch that setup? Or is Force10 just backwards? As far as the Guest VLAN goes, if I go to tag the host ports necessary, I get a "Tagged flag mis-matched gig 0/23" message. If I try to untag the ports, it tells me it is already untagged on another VLAN, which makes sense. I also read that the port needs to be in hybrid mode in order to tag multiple VLANs on it. If I try to put a host port in hybrid mode, it says it can't because it is a layer 2 port. If I try to put a trunk port in hybrid mode, it says it can't because it is a layer 3 port. That leaves no port mode available to enter hybrid mode. Is my current setup truly backwards? And if so, is it causing the issues with trying to tag multiple VLANs on a port and preventing me from entering hybrid mode on any other ports?