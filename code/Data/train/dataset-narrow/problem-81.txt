I'll also review the code for non-essential points - but I'll leave this until after the essentials. Probability Density of a path when using Next Event Estimation Looking at the paper on which the code you are following was based it seems that the novel shift mapping described in section 5.2 is defined in terms of the reflective properties of the surfaces found at the vertices of the path. I must emphasise that I don't have a full understanding of this, but it suggests that Next Event Estimation may not require a change in this approach, as the surfaces encountered will be the same. Hopefully once the other problems are cleared up it will be easier to judge whether the image looks correct. Note that section 5.2 of the paper already mentions (just below Figure 10) that they take into account sampling the emitter "either using BSDF or area sampling". The difference with Next Event Estimation is that the area sampling happens at every vertex of the path, but it isn't obvious to me that this should cause a problem. The fact that your scene uses only diffuse surfaces means that the offset path should in most cases rejoin the base path at the second vertex, so you would only need to recalculate the area sampling for the first vertex of the offset path. The cause of the incorrect lighting direction In reading through the code to familiarise myself with how it works, I noticed that is calculated, but then not used. A text search revealed that the only other occurrence has a different case: . Here are the two variables in context (first and ninth lines of this excerpt): 

The lit area seems a bit (how much?) larger than for the spotlight. SampleCmpLevelZero An other strange observation is that using a depth value of 1.1 for my spotlight's PCF filtering () always results in a 0 value as expected: 

I obtain the following images after visualizing the shadow factor (this is not the light contribution of the spotlight) (and some linear fog as well): 

This is clearly wrong. All six shadow maps seem kind of stretched. Without the stretching, the curtain's shadow would still be in the close vicinity of the curtain itself and the leaves' shadow would be much smaller (equal to the leaves' shadow for the spotlight). Furthermore, the circular shadow at the end of the pillar's shadow is associated to the buckets in front of the curtains. Any ideas what goes or could go wrong? For clarity, the following two images show the shadow factor of the cube map face corresponding to the spotlight, for the omni light by adding: 

where $$ f = min(A_{s0}, 1 - A_d) $$ So it seems that the reference documentation is misleading for this particular case. However the specification is clear: there is no clamping or conversion for floating-point color buffers. What if Ad is negative floating value? My reading of the spec is that nothing special happens if Ad is negative. The operation will still produce results, but I don't see how they would be useful. I've never used myself, but the way I understand it, with , it is meant to accumulate color values in the color buffer until the accumulated alpha value equals 1. When the alpha value of the color buffer is 1, no further color contribution from new pixels is allowed. This assumes that you use alpha values between 0 and 1 and use an initial alpha value of 0. Otherwise everything falls apart. In fact, I think majority of the blending factor mechanisms were designed with alpha values between 0 and 1 in mind and make little sense outside of that range. 

I obtain the following images after visualizing the shadow factor (this is not the light contribution of the omni light) (and some linear fog as well): 

No MakeFile or Visual Studio Solution file is provided. So you will need to do the setup yourself in the Visual Studio IDE. Note that you could just drag and drop the files (.h and .cc are known extensions to Visual Studio) in a new console application if you are not/less familiar with Visual Studio. The code itself is just plain C++ code. No special stuff. So it does not matter which platform or IDE was used during development. In fact the code is pretty self containing except for 

I don't have a definite authoritive answer for you but I'm very confident that the answer is "yes". Here's why: When using framebuffer objects, stencil attachments are usually combined with the depth attachment in a single texture ( or ). In fact, until recently, a combined depth and stencil was the only way to get a stencil buffer. It is now possible to create a stencil only attachment for an FBO. This OpenGL wiki article on FBOs states (emphasis mine): 

This doesn't handle the case where a side is smaller than the rounded corner radius. You could reduce the rounded corner's radius in this case based on the segment's length. 

Here, the hit position in light projection space coordinates is calculated as follows from the hit position in camera view space coordinates (shading space): 

A Transform structure can now be allocated on the heap with 16 byte alignment, since our custom will now be invoked: 

They all seem to be used (in the literature, in the animation industry and in the gaming industry) in the format corresponding to your second option. All the D factors in my enumeration contain an explicit $\frac{1}{\pi \alpha^2} $ with $\alpha \equiv \text{roughness}^2$ (See Equations). Edit 2: A recent presentation deriving and explaining the division by $4$ instead of $\pi$: Earl Hammon: PBR Diffuse Lighting for GGX+Smith Microsurfaces, GDC 2017. To make a long story shorter, option 2 is the only correct specular term (of the three options provided). 

I have worked with this specific formula for the OVER operator but not with additive blending. I'll use the paper's nomenclature in the following discussion: $$ C_f = \frac{\sum_{i=1}^{n}C_i \cdot w(z_i, \alpha_i)}{\sum_{i=1}^{n}\alpha_i \cdot w(z_i, \alpha_i)}(1 - \prod_{i=1}^{n}(1 - \alpha_i)) + C_0\prod_{i=1}^{n}(1 - \alpha_i) $$ This is not explicitly stated in the paper, but the term $$C_i$$ is the premultiplied-alpha color (i.e. ) As described in the paper, the term $$ C_0\prod_{i=1}^{n}(1 - \alpha_i) $$ provides the "revealage" of the background color. If all the transparent surfaces are transparent, the product will be 1 and the background will be fully visible. The rest of the equation provides an approximation of the result of sorting the transparent surfaces by distance and using the OVER operator with pre-multiplied alpha colors. However, the equation for additive blending (using and ) without weights is: $$ C_f = \sum_{i=1}^{n}\alpha_iRGB_i + C_0 $$ This equation is already order independent! In order to add weights to the transparent surfaces with a normalization step in the end, the equation can then be simplified to: $$ C_f = \frac{\sum_{i=1}^{n}C_i \cdot w(z_i, \alpha_i)}{\sum_{i=1}^{n}w(z_i, \alpha_i)} + C_0 $$ If you'd rather keep the same equation as before, you can achieve the same result by changing the shader outputs in listing 3 to: 

No additional memory is required as no particles are being used. We're simply swapping pixels at random, with no destination locations in mind. The key is in choosing that apparently jumbled initial image to have the pixels in exactly the right places such that applying a million random swaps will happen to leave all the pixels in exactly the right places to give the desired text. Setting up the initial jumbled image To do that, this preparatory jsfiddle takes an image as input and outputs a jumbled image that is precisely arranged to work as input for the main jsfiddle. This one takes more memory, but is still a very simple algorithm: 

Since you're working on CAD software, you probably want some precise results. Here an algorithm that could work: For each side: 

All this strongly hints towards an accelerated storage format for stencils similar to the one used by depth buffers. Also, empirically, over the years many rendering algorithms (e.g. stencil shadows) using the stencil buffer would have been prohibitively slow if stencil buffers did not use some form of accelerated structure. The NV_packed_depth_stencil extension (2001) spec also provides interesting insight: 

Have a look at Improved Alpha-Tested Magnification for Vector Textures and Special Effects [Gre07] (pdf). The gist of their method is to store distance field information in the font texture instead of bitmap glyphs. This information is then used to build much higher quality up-scaled versions of the glyphs. They even describe a rendering path without programmable shading that gives ok results. 

Even if I use a value equal to 100000.0, I'll notice lit areas? Depth Biasing I use (to prevent shadow acne; note that I use 16bit depth maps) for all my Rasterizer states. PCF filtering I use the following sampler comparison state for PCF filtering (my shadow maps have no mipmaps) for both spotlights and omni lights. 

In the SIGGRAPH course: BURLEY B.: Physically Based Shading at Disney, SIGGRAPH 2012 Course: Practical Physically Based Shading in Film and Game Production, 2012. it is mentioned that some BRDF models include a diffuse Fresnel factor such as: $$(1-F(\theta_l)) (1-F(\theta_d)).$$ The Disney BRDF itself uses the following diffuse BRDF component (using Sclick's Fresnel approximation): $$f_d = \frac{\textrm{c_base}}{\pi} (1 + (F_{\textrm{D90}} - 1)(1-\cos\theta_l)^5) (1 + (F_{\textrm{D90}} - 1)(1-\cos\theta_v)^5),$$ where $$F_{\textrm{D90}} = 0.5 + 2 \text{roughness} \cos^2\theta_d. $$ Where does this come from? My attempt... If I evaluate $(1-F(\theta_l)) (1-F(\theta_v))$ (instead of $(1-F(\theta_l)) (1-F(\theta_d))$?) with Schlick's approximation, we get: $$\left(1-(F_0 + (1-F_0)(1-\cos\theta_l)^5)\right) \left(1-(F_0 + (1-F_0)(1-\cos\theta_v)^5)\right)$$ $$\left(1-F_0 + (F_0-1)(1-\cos\theta_l)^5\right) \left(1-F_0 + (F_0-1)(1-\cos\theta_v)^5\right)$$ If we substitute $F_{\textrm{D90}} = F_0$, we get: $$\left(1-F_{\textrm{D90}} + (F_{\textrm{D90}}-1)(1-\cos\theta_l)^5\right) \left(1-F_{\textrm{D90}} + (F_{\textrm{D90}}-1)(1-\cos\theta_v)^5\right)$$ This looks similar except for the 2x $-F_{\textrm{D90}}$? Is my reasoning completely wrong or where do I make mistakes? Or am I not aware of some further (common) approximations? 

I don't know any shader languages. I've heard of GLSL and HLSL, and I'm interested in learning one or both. Are there significant differences between them that would make one or other better in certain situations? Is it useful to know both or would either cover most needs? I don't want vague answers indicating personal preference. I'm looking for specific measurable differences so that I can decide for myself which will suit me best. I don't have a specific task in mind - I'm hoping to discover whether there is one or other that I can learn and then apply to any future tasks, rather than having to learn a new language for each new task. If there are other shader languages which I have not mentioned I would be interested to hear the comparison for those too, provided they are not dependent on any particular GPU manufacturer. I want my code to be portable across different graphics cards.