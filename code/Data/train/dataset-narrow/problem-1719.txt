If you do not even see the outgoing packet, then there is an internal problem with the name resolution config on your server. 

while you attempt to download a file. If you don't get any entries then its a firewall issue, if you get something like: 

As you can see it is a "mostly free format human readable date", it will pick up the timezone from anywhere it sees fit. It is very easy to test it. For example: 

Then anything you type on either end should be reflected/visible on the screen. If its not then your port is being blocked along the way. 

But to answer your question, I have seen this behavior many times (at both the ISP level or the individual router level). If I read you correctly, your existing sessions still work during the "incident" but new ones do not. This is caused by either your ISP placing a limit on the amount of sessions an end user can establish, or because the router simply choked on the high amount of sessions established. It could also be the ISP's modem/router having a limit on the amount of sessions per IP. That would explain why your 'computer 2' with another IP continues to work fine. ISPs place these session limits to prevent malware like bots/trojans/viruses spreading like wildfire and bringing a network to its knees. Just imagine a home user with a 50Mbps connection opening hundreds of thousands of sessions all over the place. Then multiply that by X users and you see how this cascades down into total chaos. I suggest you retest under a more controlled environment. Maybe just one PC and in safe mode to prevent any strange software from loading that is opening the sessions (maybe a bot/virus/trojan). Then put a permanent ping (no -c or -w). I would be pretty confident that you would not see the loss of connectivity every 15 minutes. UPDATE Here is a new test for you to narrow down the issue. Since you say the failure lasts 40 seconds, it should give you plenty of time to do the following. As soon as the failure is detected, change the wan IP of your router (from 192.168.0.10 to maybe 192.168.0.20). If the connectivity is restored, it will prove to you the ISPs modem is limiting sessions based on IP. 

This is not nginx caching file content, this is your filesystem (Unix?) changing the inode when you update the file. Since file descriptor information is cached in nginx, inode update is not propagated to it, thus continuing reading the old inode. On cache invalidation ('s parameter) or revalidation (), file descriptor information gets respectively removed (will be up-to-date on next opening) or updated. This behavior is specific to each filesystem, this is why using file descriptor cache on NFS, for one, is advised against. 

merely does an internal redirect to the last parameter if no file specified before is found. Therefore, you should not notice any change to the URI. TeroKilkanen asked for a complete configuration. You should provide a minimal working configuration reproducing the alleged behavior. Regex s support PCRE, thus why not? If you encounter trouble with PCRE, you should specify your question. 

That should do it. Now, where does that empty PID file comes from is an interesting question one could ask him/herself... Have you played around with it? Multiple instances of nginx does not apply for it, you should have something there! 

Use to validate your configuration Monitor your error log file while you issue to catch any error that might happen at runtime which are undetectable at configuration time. 

I guess you misunderstands nginx' design. This Web server is not following the one-process-per-connection classical scheme as Apache, for one, does. Thus, there is little to no use in adding more workers than you number of CPU cores, a number of worker process being equal to this machine property being the most efficient configuration. Put another way: in Apache times, you had to add more workers to handle more traffic since 1 process <=> 1 connection. Now, this nginx, only the relation 1 connection => 1 process remains, but 1 process => 1 connection is not true anymore. The event-driven design of nginx allows each worker to accept many connections instead of waiting (remaining idle) until he is totally busy (100% CPU on the core-s- being used). The baseline is: you do not need to change the number of workers of your nginx instance if it is properly configured (cf. , being usually enough). If the workers are all saturated, then you will have a machine property being the bottleneck (CPU, I/O), thus adding new workers won't do much but add to the problem. Now, for the part in which you want to 'exchange workers one by one', nginx' master allocate sockets to workers, so you would not know which requests/clients you are affecting. Think of if at a group which spawns (and dies) together. You could have several binary version and/or several configuration loaded on several masters at the same time by using the on-the-fly upgrade capability of nginx. You can then switch between different configuration seamlessly and almost instantly by communicating with the masters through signals. I strongly suggest you read on nginx' design to have a clear idea of what you are dealing with and why what you asks falls flat. You have not specified what your real intent is, so I was stuck with providing you with the exact questions you asked. There is probably a simpler set of questions/answers if you told the community what you wanted to achieve. 

Your sendmail.mc looks ok. Apart from stopping and starting sendmail again (which I would guess you have already done). I would look at the actual configuration file which would be called sendmail.cf. Make sure you see the lines: 

if $TZ is unset and /etc/localtime is UTC then why are you using timezone T (Tango) in the date command? On my system I have a localtime of EDT. 

As you can see the first parameter ($1) is the device interface, for example it could be the wifi interface. It also resets interface ifb0 which is an alternative to tc filters for handling ingress traffic, by redirecting it to a virtual (ifb0) interface and treat is as egress traffic there. The idea is that you can shape egress traffic but not ingress traffic, thereby if you can make all traffic egress then you can shape it. This line creates a scheduler (qdisc) for the traffic on the interface supplied in paramter $1: 

The reason your are getting the mydomain.com error is because the operating system is resolving the DNS host name lookup 127.0.0.1 to mydomain.com. That is not a mysql issue but a networking issue. 

Full info of components of Linux Traffic Control can be found here. UPDATE: by the way, in a regular linux box you can see the status of the traffic shaping queues by using tc -s. For example you could try to issue the following commands in your phone and they should work too: 

You could either have a firewall rule blocking access or Your /copos directory does not have full permissions. You should be able to figure it out by doing a: 

This has nothing to do with domain names, but rather at what defines a network socket: a duple. In your case, it is implied all involved subdomains point to the same duple as the main one. You configured nginx to listen on this socket, thus handling any incoming connection there. nginx then selects the best virtual server for the request, defaulting to a default one if no best match is found. I suggest you read on how nginx serves requests. In your case: 

According the official documentation, the only way to manipulate environment variables in nginx is through the use of the directive, only available in the context (ie not dependent on the protocol such as , thus not its inherent s). That means, the variables will be set for the whole nginx environment (even though it seems you can change it worker-based, which are independant processes). To do what you wish, I would suggest either: 

As for the unknown directive "location", that is a totally unrelated error simply indicating your configuration is not valid (thus won't be loaded/applied). You made a syntaxic/lexicographic error while modifying your configuration. Check your changes. 

Start by deleting the stanza from your file since that triggers the now unwanted behavior of including content at level. Now you could use some locations such as: 

To update content in Development, use the account or any tool using that user. To update content in Production, use the git tool rightly configured to push data at the corretc location, using the user. 

It appears you have a Centos 6 machine but have configured the Epel repository for Centos 7. That is not going to work. Remove it and install the Centos 6 Epel Repository at: 

So what you need to do is add mydomain.com as a permited host in the users table, mysql database. For example: 

Make note that in my example the secondary IP is not controlled by NetworkManager, hence the NM_CONTROLLED="no". You don't need to tie it up with a another MAC address, it should work fine without the HWADDR line. 

the important thing to note here is that it is lightning fast! That is because huge number of ip networks can be represented by a single hash instead of hundreds or thousands of lines of iptables rules. For blocking countries see this example: 

The first thing to check is to see if the DNS request is actually leaving for the right destination. Open 2 terminal sessions. On one type: 

The first thing I would do before proceeding further would be to test port 25 end to end with a tool like netcat. The tool comes standard with most linux distros. For example on CentOS I would stop postfix to release port 25. Then I would start netcat like this: 

That is a traffic shaping (also known a throttling) script. It appears to take 3 arguments. The interface, the receive limits and the transmit limits. This first part just deletes everything: 

I think you can see the point. If you are not familiar with military timezones here is the list: $URL$ You can also try to use zdump to see if your zone file is not really what you are expecting: 

RFC 2818 states the separation of ports between HTTP/HTTPS comes from the fact the first data sent is different, since HTTPS starts the TLS session before sending HTTP over it. There is no way for the server to understand you if you are not talking the right protocol. Moreover, RFC 7230, updating the previous one, states: 

As a supplementary piece of advice, you might want to have a look at your regular expression to replace the generic catch-all characters by more specific classes or sets, so its behavior is better defined. 

1. Handling arguments Despite what Tero Kilkanen says, nginx is perfectly capable of handling arguments: this is done by using the and variables from the core module. 2. context As Glueon pointed out, is usable in , and contexts, as the documentation states. is to be avoided at all costs, so I won't be going inot details about it here. Now should you use in or ? In Blocks of s will be considered sequentially, and the first matching will be used. Your configuration thus depends on the order of directives, which is bad (it is one of the things which are wrong with Apache). See for yourself: 

On a setup using with , I noticed the following errors occurring frequently in the nginx' errors log: 

Note: I suspect the configuration snippet you provided is not working, since the URI prefix of the directive does not start with the mandatory character. 

nginx is able to talk with any backend listening on a FastCGI socket. That includes PHP-FPM and HHVM, provided both frontend and backend are configured to find each other. Thus, it not efficiant to use 3 layers while you could only have 2 layers . You would need to decide which Web server you would prefer to use. I do not understand what might be unsafe, though, provided you use a setup suitable to be a server, up-to-date and properly configured. Security covers a wide range of different scopes, starting from network design, passing through OS configuration, up to individual Web server locations configuration. Provided you use stable software and an easy/automated method for maintenance/updates, I do not see why it would not be suitable for production. Anyway, the baseline is: you only install the bare minimum of stuff you need on a server. Security is helped by simplicity/cleanliness.