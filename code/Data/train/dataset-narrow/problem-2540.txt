Yes. The standard OpenGL headers are all written for C, although the book is written in C++ so that they can use their supplied classes. Most windowing frameworks, including SDL and GLFW, as well as GLEW for loading extensions, are also written in C. Even SFML has a set of C bindings. 

Basically, you have to store the two most recent "snapshots" produced by the update step. These snapshots have all the information needed to render the scene, such as the positions and rotations of all your sprites. To properly interpolate, the rendering system time lags behind real time by the interval of an update, so if your game updates at 50Hz, the rendering system is behind 20ms. This ensures that the rendering time lies between the snapshots. To render the scene, you interpolate between the snapshots. Be wary that objects that are orbiting something extremely quickly will be rendered incorrectly if you interpolate between their positions and not their polar coordinates relative to the center of their orbit. You could also integrate acceleration and velocity to more accurately interpolate, but if you're going to go this far you might as well just run your update step more often. 

This type of logic should be done in systems. I'd add an extra layer of abstraction, so the system only needs to decide the "purpose" of the animation. The animation data stored in the component maps "purpose" to the given animation. For example, the "run left" purpose could map to a series of frames that show the sprite running left, but this data could be different for each sprite. 

The control systems - there is one for each control component type (keyboard, gamepad, network, AI, etc.); they set fields of the input component The action systems - there is one for each action component type (walk, jump, attack, interact, etc.); they look at the fields of the input component and modify the physics state, weapon component, etc. The physics system - looks at the physics data and simulates a step 

All operations that are performed over a large set of data, with each operation working independently from one another will benefit from parallelization. Here are some common examples of such tasks: 

The ideal algorithm would run in a compute shader, one execution path per voxel. If you wanted to get fancy, you could also store the water's velocity within that voxel for additional simulation effects: for example, attempting to distribute an amount of water proportional to the velocity to the cells "pointed to" by the velocity vector. This velocity data could also be used to move the waves, draw rapids, etc. 

The colored areas in this image each represent a different shadow volume. The red volume is the smallest and thus has the highest texel resolution, then green, and so on. This page is more in-depth and should provide you with a pretty thorough understanding of the concept. 

The normals would be generated based on the gradient of the density function at the same time that you get the intersection points between the edges and the surface. If it's something simple and closed-form like a sphere then you can calculate the normals analytically, but with noise you'll need to take samples. You have the next steps in the wrong order. First, you generate a vertex for each cell that exhibits a sign change. The QEF you are minimizing is simply the total distance to each of the planes that are defined by the intersection point/normal pairs for that cell. Then you walk through the edges that exhibit sign changes and create a quad using the four adjacent vertices (which are guaranteed to have been generated in the last step). Now, my biggest hurdle in implementing this was solving the QEF. I actually came up with a simple iterative solution that will run well on (for example) a GPU in parallel. Basically, you start the vertex in the centre of the cell. Then you average all the vectors taken from the vertex to each plane and move the vertex along that resultant, and repeat this step a fixed number of times. I found moving it ~70% along the resultant would stabilize in the least amount of iterations. 

The advantages of this design are not limited to the flexibility you get. Data-oriented programming is much friendlier in terms of cache performance, and it's easy to multithread any independent operation like in UpdateEntityMovement(), so DOD is very scalable in terms of current-gen hardware developments. 

You can implement this by writing two sets of shaders, one for the object, one for the outline, and then rendering the same geometry with the same world transform, bones, etc. once per set. 

EDIT: It seems to be that, for some odd reason, my shader stopped outputting at all when I added the framebuffer support. 

I solved it by creating a singleton called GlobalEventManager. It derives from the local EventManager class and I use it like this: 

Is there an algorithm that takes a set of triangles (concave) as input and outputs a number of sets of triangles (convex)? I'd like it to not fill in the holes between parts of the original mesh. I've already come across a small idea: find all the concave edges, and split the meshes along the edge loops. Am I on the right track? How could I implement this? 

Some tasks, like pathfinding, are better suited to the CPU because algorithms like A* involve a lot of dynamic branching. When you are performing particle simulation or joint pose interpolation, you can offload this work to the GPU because each operation is identical except for the data. 

A good base would be a function like , since it passes through the origin and exhibits a horizontal asymptote. 

The density field is used to smooth out the curves. If you populate the field with simple true/false values you get blocky, Minecaft-like terrain. A voxel can be a floating-point value or a boolean value, just like a pixel can be a floating-point value or a boolean value (such as in masks). A pixel is a picture (2D) element; a voxel is a volume (3D) element. Example using boolean values: 

In a totally immersive, simulation-type role-playing game, the player should not look at bars or numbers to keep track of his or her stats, so another method of showing these stats is needed. For example, the player needs to know when to eat, because games (currently) cannot incite a real feeling of hunger. The player also needs to know if they are hurt, so they can take appropriate measures. Somehow, these feelings must be represented visually or audibly. I'm not a fan of coloring and blurring the screen to show feelings like how most first-person shooters show that you have been shot. What are some ways to represent these feelings immersively? 

You are looking for a technique called deferred shading. The basic premise is to reduce the number of lighting calculations done on a render while avoiding the expensive sorting used in the reverse painter's algorithm. The complexity of the whole procedure is . It takes steps, where is the number of light sources. The first step renders each object to an intermediate buffer called the g-buffer. This wide buffer stores the depth, world-space normals, albedo (color), and other material parameters; all per-pixel. It is a quite cheap shader because it really just does vertex transformation and texture reads. The second step runs once per light source and adds to a buffer called the la-buffer (light accumulation). The shader reads in the data for each pixel from the g-buffer and performs lighting calculations for the corresponding pixel in the la-buffer. The la-buffer is then either used directly as the output or run through a post-processing step. This step can be rendered with light geometry (sphere for point light, cone for spotlight, full-screen for ambient light) to avoid doing unnecessary light calculations. The advantages of deferred rendering are that it scales much better to scenes with many dynamic light sources and that it only needs to do at worst one light calculation per pixel per light. However, it uses up a lot of memory for the g-buffer and is dependent on the fill rate of the GPU. The good news is that this problem will disappear as hardware gets faster and has more memory. It is also more difficult to implement transparency, but there is a method published to work around that problem. It also constrains you to essentially one lighting equation per light source, but this is generally a non-issue. Here is an image describing the basic pipeline. 

I've implemented a game loop similar to that in Glenn Fiedler's Fix Your Timestep! article. I have an update step running at 50Hz and a render step running at 60Hz (monitor refresh rate). To keep things smooth, the update step produces snapshots that the render step can interpolate between with a constant latency of 20ms. These snapshots contain all the information needed for the rendering step to build skeletons and other transformations, but not anything involving physics, AI, or networking. Actually interpolating between the snapshots is pretty simple, but something I've forgot about until now is how to deal with renderables being created or destroyed. What would be an elegant method to go about interpolating between snapshots when a renderable may only exist in one of them? 

My game has first-person melee combat that is based on directional attacks that are controlled with mouse gestures. For example, a downward slash is most effectively blocked by a shield held up high. I'd like to add a short "focus" mode while the player is holding down the mouse button to use their weapon/shield. It should last no more than a second or so. The problem is how to make this work when there are other players in the vicinity. Nearby players shouldn't be slowed down or notice that the other player has slowed down time - it should look completely normal to them. The players also need to be synchronized in "game time". How can I do this? 

There are countless others, but these are some of the most widely-used. They are more high-level, and are more suited to faster and simplified development rather than total control and performance. They are good enough for indie games if you don't want to spend a lot of time on writing your own rendering back-end. If you value being able to do whatever you want with your engine without needing to work around the limitations of high-level frameworks, low-level APIs like OpenGL and Direct3D are for you. 

The one big caveat is that it is not as simple to parallelize for the GPU, if you are interested in that. PS: You may get stuck trying to figure out the QEF solver - all it is is an algorithm that finds the point with the lowest sum-of-squared-distance-to-planes. I found a particle-based approach with a fixed number of iterations, which can be unrolled in a GPU kernel/shader, worked quite well. 

It's an Unsigned NORMalized float, which is a floating-point value between 0.0 and 1.0. EDIT: To clarify, the Depth component is accessed as a float, and the Stencil component is accessed as an unsigned integer. 

I'm going to be using Newton in my networked action game with Mogre. There will be two "types" of physics object: global and local. Global objects will be kept in sync for everybody; these include the players, projectiles, and other gameplay-related objects. Local objects are purely for effect, like ragdolls, debris, and particles. Is there a way to make the global objects affect the local objects without actually getting affected themselves? I'd like debris to bounce off of a tank, but I don't want the tank to respond in any way. 

There may be some errors, my C# is a little rusty... EDIT 2: In response to your edit: Basically, just prefetch the images within a certain radius of the visible region of the collection. If you configure the system such that a CardImage that hasn't finished loading yet (in a background thread) can be "drawn" without any errors, the user won't care that the image is suddenly popping in as they scroll over the card. They're probably used to using the internet. 

In a similar way, directional light shadow maps use an orthographic projection, so the direction of the light is uniform across the map so as to approximate a very far away light source (the sun). The frustum in this case is built unsing: 

will draw from the currently bound vertex attribute arrays, which are "created" and bound themselves with and , which do use the currently bound vertex buffer. It doesn't matter if the vertex attributes are all from one buffer or multiple buffers, and you don't need any particular vertex buffer to be bound when drawing; all the functions care about is which vertex attribute arrays are enabled. 

This is the exact same thing that you would do in a generic collision detection algorithm - narrowing down the list of candidates through a gauntlet of cheap (and usually parallel) early-out tests. Indeed, you could implement the building's broad phase query as a collision test against the circle/sphere that bounds its range, which might be useful if you're using a 3rd-party collision/physics library. 

I think that Entity-Component Systems lend themselves much better to data-oriented design rather than object-oriented design, as long as you're willing to use components only for storing data, in which case I would choose the term "properties" over "components". You can think of all the data that the entities hold as being in a big table. Each row of the table stores all the data for a single property for every entity. Each column of the table represents an entity as a whole. 

What you need is a separation of the collision resolution and specific logic steps, so the collision process is broken up into three separate steps, each of which is well-defined. Here's a simple diagram to show how it would work: 

You say that you can't use a sprite sheet because the textures are different sizes. That doesn't really matter. You can use something called a texture atlas, where different sprite images are put together in a non-uniform manner. I've never used the technique myself, but here is a tool I found to put together a texture atlas. Since you probably have too many sprites to fit on a single texture atlas, you should group together sprites that will show up with each other more commonly. You will still need to use multiple VBOs, one for each texture atlas, but it is more efficient than using one draw call per individual sprite. 

Based on what I know from reading, your best bet is fully orphaning the buffer each frame and filling the entire thing. As far as how you architect your rendering engine, it depends on the scene. To use your example, if all you're drawing is 100,000 circles, you're best off having D be stored in a format that can be written directly to B and used for instanced rendering. In any case, you should be trying to reduce the time it takes to translate D to B as much as possible. I would suggest architecting D in a data-oriented way - contiguous and using a structure-of-arrays design rather than array-of-structures - so that you can boil it down to a few straight 's. 

I think simplicity is key when designing these systems. What I do is associate a bitmask with each entity where each component type corresponds to a bit; the bit is switched on if the entity contains a component of that type. I give the entity/component manager a couple of helper functions to change the bitmask when adding and removing components. If you associate each system with a component bitmask as well, it is trivial to build the set of entities to be processed by a given system. You can then do one of two things: 

Different locales and profiles can be loaded simply by choosing the right file, for example between and . Because the table is in global scope, it's easy for other Lua code to reference. If a script needed to access a configuration variable or throw an error, it would just have to look up the value using something like or . The difficulties come when native code (C) has to work with these values. There are basically two options that I can think of: 

Also, you shouldn't be representing individual voxels as classes. C# implements these as reference types, meaning a lot of overhead for something that should be plain old data, i.e. a struct. 

But what about when it comes to specific information like character status, AI behavior modifiers, and NPC descriptions? Only a handful of entities need this kind of information: who would expect a barrel to give out quests? It needs to be discrete for each entity, so flyweighting is ruled out. 

Each voxel datapoint in my terrain model is made up of two properties: density and material type. Each is stored as an unsigned integer value (but the density is interpreted as a decimal value between 0 and 1). The density values are used to generate a mesh with the marching cubes algorithm. My current idea for rendering these different materials on the terrain mesh is to store eleven extra attributes in each vertex: six material values corresponding to the materials of the voxels that the vertices lie between, three decimal values that correspond to the interpolation each vertex has between each voxel, and two decimal values that are used to determine where the fragment lies on the triangle. The material and interpolation attributes are the exact same for each vertex in the triangle. The fragment shader samples each texture that corresponds to each material and then uses the aforementioned couple of decimal values to interpolate between these samples and obtain the final textured color of the fragment. It should work fine, but it seems like a big memory hog. I won't be able to reuse vertices in the mesh with indexing, and each vertex will have a lot of data associated with it. It also seems pretty slow. What are some ways to improve or replace this technique for drawing materials on a voxel terrain mesh?