Proposition 15 states that there exist two mixtures of $k^2$ Gaussians on the line whose $L^1$ distance is exponentially small in $k$. Since the construction is careful and the distribution parameters are well-behaved (polynomially-sized), this dependence on $k$ is real, and thus any algorithm whose performance depends on statistical distance will run into trouble. For evidence that few dimensions actually forms a bad regime, please see the following very recent work: 

The other answers look very nice. I'd like to share a comment Russell Impagliazzo made years ago in a lecture, which has stuck with me ever since. 

Cybenko's result is fairly intuitive, as I hope to convey below; what makes things more tricky is he was aiming both for generality, as well as a minimal number of hidden layers. Kolmogorov's result (mentioned by vzn) in fact achieves a stronger guarantee, but is somewhat less relevant to machine learning (in particular, it does not build a standard neural net, since the nodes are heterogeneous); this result in turn is daunting since on the surface it is just 3 pages recording some limits and continuous functions, but in reality it is constructing a set of fractals. While Cybenko's result is unusual and very interesting due to the exact techniques he uses, results of that flavor are very widely used in machine learning (and I can point you to others). Here is a high-level summary of why Cybenko's result should hold. 

This is treated extensively in the statistics literature, under the topic of regression. Two standard references here are Wasserman's book "all of nonparametric statistics" and Tsybakov's "introduction to nonparametric estimation". I'll talk briefly about some of the standard stuff, and try to give pointers outside of statistics (this is a common topic and different fields have different cultures: prove different kinds of theorems, make different assumptions). 

Many of the known properties of coordinate methods are captured in umbrella theorems for more general descent methods. Two examples of this, given below, are the fast convergence under strong convexity (hold for any $l^p$ steepest descent), and the general convergence of these methods (usually attributed to Zoutendijk). Naming is not standard. Even the term "steepest descent" is not standard. You may have success googling any of the terms "cyclic coordinate descent", "coordinate descent", "Gauss-Seidel", "Gauss-Southwell". usage is not consistent. The cyclic variant rarely receives special mention. Instead, usually only the best single choice of coordinate is discussed. But this almost always gives the cyclic guarantee, albeit with an extra factor $n$ (number of variables): this is because most convergence analyses proceed by lower bounding the improvement of a single step,and you can ignore the extra coordinates. It also seems difficult to say anything general about what cyclic buys you, so people just do the best coordinate and the $n$ factor can usually be verified. 

(Kernel regressors, sometimes called the Nadaraya-Watson Estimator.) Here you write the function at any point as a weighted combination of nearby values. More concretely, since this is in the statistics literature, you typically suppose you have some examples $((x_i,f(x_i)))_{i=1}^n$ drawn from some distribution, and fix some kernel $K$ (can think of this as a gaussian, but zero mean is what matters most), and write $$ \hat f(x) := \sum_i f(x_i) \left(\frac{ K(c_n(x-x_i)) }{ \sum_j K(c_n(x-x_j))}\right), $$ where $c_n\to\infty$ (you are more sensitive to small distances as $n$ increases). The guarantee is that, as $n\to\infty$, a probilistic criterion of distortion (expectation of sup-norm, high probability, whatever) goes to zero. (It hardly matters what $K$ looks like---it matters more how you choose $c_n$.) (Basis methods.) A similar thing is to choose some family of "basis functions", things like Wavelets or piecewise linear functions, but really anything that forms a (possibly overcomplete) basis for the vector space $L^2$, and determine a weighted linear combination of scaled and translated elements. The techniques here differ drastically from (1.); rather than plopping down basis functions centered at data points, you carefully compute the weight and location of each in order to minimize some distortion criterion. (Typically, their quantity is fixed a priori.) One approach is "basis pursuit", where you greedily add in new functions while trying to minimize some approximation error between $\hat f$ and $f$. To get a sense of the diversity of approaches here, a neat paper is Rahimi & Recht's "uniform approximation of functions with random bases". Perhaps I should say that the grand-daddy of all of these is the Fourier expansion; there's a lot of good material on this in Mallat's book on Wavelets. (Tree methods.) Another way is to look at a function as a tree; at each level, you are working with some partition of the domain, and return, for instance, the average point. (Each pruning of the tree also gives a partition.) In the limit, the fineness of this partition will no longer discretize the function, and you have reconstructed it exactly. How best to choose this partition is a tough problem. (You can google this under "regression tree".) (Polynomial methods; see also splines and other interpolating techniques.) By Taylor's theorem, you know that you can get arbitrarily close to well behaved functions. This may seem like a very basic approach (i.e., just use the Lagrange interpolating polynomial), but where things get interesting is in deciding which points to interpolate. This was investigated extensively in the context of numerical integration; you can find some amazing math under the topics of "clenshaw-curtis quadrature" and "gaussian quadrature". I'm throwing this in here because the types of assumptions and guarantees here are so drastically different than what appears above. I like this field but these methods suffer really badly from the curse of dimension, at least I think this is why they are less discussed than they used to be (if you do numeric integration with mathematica, I think it does quadrature for univariate domains, but sampling techniques for multivariate domains). 

Now I'd like to say a few things about Kolmogorov's result. While this result does not apparently need the sort of background of Cybenko's, I personally think it is much more intimidating. Here is why. Cybenko's result is an approximation guarantee: it does not say we can exactly represent anything. On the other hand, Kolmogorov's result is provides an equality. More ridiculously, it says the size of the net: you need just $\mathcal O(d^2)$ nodes. To achieve this strengthening, there is a catch of course, the one I mentioned above: the network is heteregeneous, by which I mean all the transfer functions are not the same. Okay, so with all that, how can this thing possible work?! Let's go back to our cubes above. Notice that we had to bake in a level of precision: for every $\epsilon>0$, we have to go back and pick a more refined $\tau >0$. Since we are working with (finite) linear combinations of indicators, we are never exactly representing anything. (things only get worse if you include the approximating effects of sigmoids.) So what's the solution? Well, how about we handle all scales simultaneously? I'm not making this up: Kolmogorov's proof is effectively constructing the hidden layer as a set of fractals. Said another way, they are basically space filling curves which map $[0,1]$ to $[0,1]^d$; this way, even though we have a combination of univariate functions, we can fit any multivariate function. In fact, you can heuristically reason that $\mathcal O(d^2)$ is "correct" via a ridiculous counting argument: we are writing a continuous function from $\mathbb{R}^d$ to $\mathbb R$ via univariate continuous functions, and therefore, to capture all inter-coordinate interactions, we need $\mathcal O(d^2)$ functions... Note that Cybenko's result, due to using only one type of transfer function, is more relevant to machine learning. Theorems of this type are very common in machine learning (vzn suggested this in his answer, however he referred to Kolmogorov's result, which is less applicable due to the custom transfer functions; this is weakened in some more fancy versions of Kolmogorov's result (produced by other authors), but those still involve fractals, and at least two transfer functions). I have some slides on these topics, which I could post if you are interested (hopefully less rambly than the above, and have some pictures; I wrote them before I was adept with Hahn-Banach, however). I think both proofs are very, very nice. (Also, I have another answer here on these topics, but I wrote it before I had grokked Kolmogorov's result.)