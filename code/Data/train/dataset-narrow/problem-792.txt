Your colleague is an idiot. The solution won't be scalable, the UDF isn't concurrent (same reason as this). And how do you deal with multi-row inserts: this would require a UDF call per row And migrating to other RDBMS doesn't happen often in real life... you may as well not use SQL Server now and use sequences on Oracle and hope you don't migrate away. Edit: Your update states that moving data is for refreshing non-production databases. In that case, you ignore the identity columns when refreshing. You don't compromise your implementation to make non-prod loading easier. Or use temp tables to track the identity value changes. Or use processes: we refresh our test system every night from production which avoids the issue entirely. (And ensures our prod backup can be restored too) 

Note though, the error you have is caused by a linked server call from your SQL Server. Depending on how you set this up (say via a stored procedure) you may need to change the stored proc or view, or set the values in the code. Hard to say as it stands. 

Why insert into tblusers directly at all? I always use staging tables. You can use SSIS of course for the same result at with greater complexity 

The safest way for SQL Server in this case would be to use standard constraints such as unique and foreign keys. I can't see why you check the folders table for a constraint on the files table though Edit: to prevent a file and a folder having the same name in a given parent folder only, use an indexed view. Duplicate files or duplicate folders requires table level uniqueness. 

Have you ever connected? Changed any settings? Have a backup of master? What other errors are there in the event log? Anyhow, restart SQL Server with the -f option and ensure the the 2 settings above are sensible. 

TL;DR How can I diagnose the source of a very inconsistent, un-reproducable 'String or Binary data would be truncated.' error, when I am quite positive the problem isn't related to user data being to large to fit into SQL objects? 

I have a SQL Server instance that runs 5 scheduled tasks each night, each of which run SSIS packages. These packages have been running for years and the associated steps run via a Proxy Account (PackageExecutor). PackageExecuter is associated with a SQL Credential that was a former domain admin account. Soon, the domain associated with this admin account is going to be shutdown. I have to use a new account, on a new domain, as the admin account associated with my roxy, PackageExecutor. When I created a new Credential for the new Admin account and associated it with PackageExecutor, I started to get the following error when I tried to run one of my SQL jobs as a test: 

If you are needing simple access to tabular data, from the web, then Google Docs has a spreadsheet document option which should suffice. Granted, this isn't a database but it's easy to use and does not involve any coding. Also, Google Docs are stored on your "Google Drive". Therefore you will need to have a Google account and you may have to accept a series of terms and conditions for multiple accounts before you can begin using Google Spreadsheets. 

I manage a SQL Server machine (with SQL Server 2000 and 2005 installed) that was first created on an Active Directory domain that we are transitioning away from and are about to shut down. All of my databases have File Owner's from the old domain. New accounts have been created that have the correct permissions to run the SQL Server services and execute all necessary SQL Server processes. My question is, what benefit is there to updating the associated property on each database? Do I need to assign a new AD account to each database, or can I leave it as it is since there are already accounts setup that have full admin rights for all of my databases? 

This will make a full backup of all the changes made since mysql-bin.0001 till let's say mysql-bin.0007. Flush logs will flush the logs and switch the current file and all the changes till mysql-bin.0007, mysqldump will be aware of. --master-data option will print two lines to indicate the point in time recovery beginning time and master log position. My confusion is at the following command (printed as it is from the docs) 

The biggest table size is around 97 GB, for now, which is expected to grow more and more. I was asked to look into some compression options for all the 362 tables in order to reduce the size of the database. I took the biggest table which is 97GB in size and has 208 columns. On Oracle out of the 208 columns 142 columns were VARCHAR2(500) and 50 columns were VARCHAR2(4000) and when the database was migrated, because of row size limit imposed in MySQL, a lot of VARCHAR2 columns were converted to TEXT. I don't if the database was designed properly or not because rest of the tables don't have so many columns in them. When I tried to compress this table (on test server) using , there was no difference in the size of the table. KEY_BLOCK_SIZE=8 didn't work, it threw an error regarding row size limit, hence I chose KEY_BLOCK_SIZE=16. I took another table, which doesn't have any TEXT columns, and it got compressed to 50% and it doesn't have too many columns in it. I don't understand what could go wrong with the 97GB table. Is it because of the TEXT column or could it be anything else? Well I read that the TEXT and BLOB columns are best candidates for compression. Is there any other method to compress the TEXT columns? Thanking you. 

I am Avinash, I have a slight confusion about incremental backups. I am going through the MySQL 5.7 Docs to study the backup process using mysqldump. I am only willing to have Point in Time Recovery capabilities. I am aware of the xtrabackup tool, but I want to use the logical backups as well. The Docs explain how to make a full backup of InnoDB tables 

I need to create a couple of snapshot tables that will export a large amount of data. While running a few tests, I need to create the table, import data, inspect the results, update the export query and then delete all the data in the table and start over. The table, when finished, will be moderately large but dropping and reloading the data again and again is making the transaction log file MASSIVE. What I would like to do is cut off transaction logging for these files until I'm satisfied with my results, or clear the transaction log for the table. To be clear, I do not want to cut off logging for the entire database. This is a shared, developer DB and we need to keep the log running. Can this be done? I've tried to Google this multiple ways and multiple ways but I can't find anything related to table-specific transaction log settings. 

If I'm understanding this reasonably explicit error, what it's telling me is that the Credential accounts, associated with my proxy is in correct. How do I validate this? I know that this account is legitimate-- I've already associated it with every associated server group, I've made it a sysadmin user on the server. What could be causing this problem? To be clear, I haven't mis-typed the account name or the password associated with the Proxy Credential. However, when I entered the account name and clicked the Check Names button, SQL Server automatically transformed the User ID to the fully-qualified version. I'm not sure if this has anything to do with this problem. I'm at a bit of a loss. I've given my credential account full access to everything that I can think of. What might I need to do to get this to work? UPDATE Sorry, one more quick mention. I've found this MSDN kb article. Resolution method #1 is what I've been doing for years. The others don't seem to apply, or I'm missing something. Any tips or clarification would be beneficial. 

I am working on installed on RHEL7. The database has been migrated from Oracle to MySQL and I don't know how it was migrated. 

in the first command the author uses only --master-data and in the second command he/she uses --master-data=2. I don't understand the difference. Could you experts please help me out? Does --master-data=2 indicate that it's an incremental backup of level 1? Does it indicate anything related to master-slave configuration? What command should I use to make incremental backups? The second command again mentions backup of Sunday itself, that's why may be I am getting confused. The author uses Sunday as full backup (in the first command) and under incremental backup (for Monday) explanation (second command) also he uses Sunday in the dump file name. Thanking you. Regards, Avinash 

I am Avinash. I am willing to install an additional MySQL instance on a UAT server for testing purposes. I am using MySQL Community Server 5.5.58 installed on RHEL7.2. I am going through the MySQL Docs $URL$ , which tells me that I would require certain unique operating parameters such port, socket, pid, datadir and log files related parameters. To achieve better performance, it also recommends to use different tmpdir. This is well understood. I can either use different binaries or the same binary for this purpose. If I am using same binary, I'll have to use mysqld_multi and use the same config file. If I use mysqld_multi, I'll have to put different sections for mysqld daemon such as mysqld1, mysqld2 and so on. All using unique values for above mentioned operating parameters. Please help me out here. Am I correct about mysqld_multi? My second question is about bind-address variable. $URL$ Here, it explains that My question is that will I need an additional NIC and an additional IP address for the second MySQL instance? For the existing instance, I have disabled the bind-address option. Do I need to add the bind-addresses for both the instances. Also, the second instance will be used only by me (for now). Thanking you. 

This is more of a comment, but since this is a lengthy response that's worth mentioning, I'm posting it as an answer. Your database looks sufficient for your needs. (Well, as best can be assumed from the diagram.) I assume that the table has a one-to-many relationship between the four tables , , and (from here on, the V-B-V-M tables.) Next, I assume that the table has a one-to-one relationship with each link between the V-B-V-M table entries. With this information, I would recommend moving the from each V-B-V-M table, to the . Next, it seems that is always associated with one of the four V-B-V-M tables. Considering this type of setup, I would recommend creating an additional table-- . 

Next, update the table and each V-B-V-M tables to include a field. the table should include one entry, for each type of asset. In this case, , , and , each with a unique description and ID. You may want to do this for a couple reasons-- First, you can now query the table and discern what type of asset is stored in a specific record, without having to link the data to your four other V-B-V-M tables. This will speed up your querying of the data, when all of the information from your four tables isn't needed. Second, this also provides an easy means of linking in a textual description of the type of asset associated with the entry. Again, without having to link to the V-B-V-M tables to figure that out. Last, you may want to consider handling the barcode data differently. If barcode is a simple look-up, you can add it to the asset table. However, if barcode values have to be unique, keeping it in a secondary table is necessary because you'd have to put a constraint on the database. You could do this if it was integrated in the table, however, your vehicle data would cause problems because vehicles do not have barcodes. To my knowledge most, if not all, major database engines won't allow you to define a unique field, that also allows values because, then, the data is no longer unique. You might be inclined to think that you could combine the field with the . Since VINs are unique and Barcodes are unique, and neither should be the same, why can't they be combined into one field on the table, which allow you to drop the additional table. I wouldn't recommend this because I think it gives the data less "focus". Yes, it is a form of unique object ID but barcodes are barcodes and VINs are VINs-- the concept behind both is completely different and consolidating the two can become confusing with time. Furthermore, if you expand your project in the future, it could cause collisions. Technically, I don't see a big problem with that (in your case) but I'd still highly recommend avoiding that option. Also, I would also add the to the table, for the same reasons you added it to the table.