If you have a managed switching infrastructure that supports netflow or sflow you could always monitor the traffic generated when you start your game to see if it within an acceptable range. Like Bill Nye says, "Try it!" (and then let us know what your results were). 

A bit of explanation is in order: All our SCCM clients belong to a Collection that gets assigned a Default Maintenance Window that only occurs once and is in the past. This prevents Collection membership changes and untimely client policy requests from causing clients that have held off actions from immediately performing them. However, since the Maintenance Windows are "union-ed" the Weekly Maintenance Window should apply... at 20:00. On a hunch I dumped all the Collections this client was in and then went and checked to see if they have Maintenance Windows assigned to them: 

Long story short. They don't. What results did you expect? I really expected to see a Collection that had a Maintenance Window applied to it that started at 19:00 and unless my SQL is bad and I missed it I guess that is not what's going on here. The fact that it is one hour early really also inclines me to think it could be an issue with timezones or clock skew but that looks to be expected as well. I still think both my hypotheses are decent and have not been refuted but I do not know how to gather more information to make a determination about them. Any ideas on how I should proceed with troubleshooting? Is there something else I should consider? What other things could cause this? 

Because that's how it was configured by a previous administrator and while it makes conceptual sense that a single subnet is "mapped" to a single VLAN there's no technical constraint that I am aware of that makes this have to be the case. 

There's always Debian GNU/kFreeBSD which ports the FreeBSD kernel the Debian/GNU userland. This would allow you to use pf as your firewall. Unfortunately, as of current (Debian 6/Squeeze) GNU/kFreeBSD is considered a "techninical preview", which means not ready for production. 

When you say "the .50 address is the server", I am assuming you mean the Hyper-V server. It is very likely that the Hyper-V server is configured with a either a dedicated External NIC or that the NIC that has been connected to the same External Virtual Switch that your BackTrack install is using has been selected for Management Traffic ('Allow management traffic to share network adapter' has been selected). In either case you are seeing traffic from the parent partition on that Hyper-V server on your guest. This is probably OK and by design. It has been a while before I have worked with Snort but you should be able to ignore all the TCP and UPD traffic with something like this: 

"XenServer Free" as in Citrix's XenServer? Last I recall, Citrix's XenServer was just dom0 implemented on CentOS. If that is still the case you should be able to change the MAC address of your dom0's interface by editing and adding/changing the setting. Executing a should bring up your interface with the new MAC address (providing the drivers for that interface support modifying the MAC address... they probably do). Refer to the CentOS/RHEL or Xen.org documentation for more information. 

HP's specification doesn't specify what version of SSL the management interface is using but Google Chrome disables SSL 2.0 by default. Try enabling all possible versions of SSL under Options - Under the Hood - Security - Manage Certificates and try again. Using wireshark could also verify if there is a SSL version mismatch. 

The Access Token the user or process ends up contains the more restrictive ACLs and not just a union of then. This is the only sane way to do it otherwise you have to maintain and troubleshoot your ACLs in two separate spots with two separate set of tools. That way lies the path of madness. 

The offending process is SVChost.exe which is wrapping the DHCP Client (dhcpcsvc.dll), EventLog (wevtsvc.dll) and LMHOSTS (lmhsvc.dll) services. I'm certainly not a Windows internals expert but I could not seem to find anything especially amiss when viewing the process with Process Explorer other than it appears the EventLog is triggering a ton of RpcBindingUnbind calls. 

The HP2600 series can only filter inbound traffic on interfaces. Should I change my filter to deny any to 192.0.2.0/24? 

Wait WUT? That line was from another client's ServiceWindowManager.log and it certainly believed that 19:00 was the appropriate time to start. I checked a few others. Guess what. Not a single mention of starting at 20:00... but if I look at the what is listed in the database AND in the Configuration Manager Console the Thurs. Night Maintenance Window is listed as starting at 20:00. Zoinks! It's not a mystery maintenance window! It's a masked maintenance window! It looks like that for whatever reason is configured to start at 20:00. The Configuration Manager Console reports that and so does the database but if I look at a handful of clients some go 19:00 and others at 20:00. Two WAG (Wild Ass Guesses): 1) We have old Machine Policies hanging around from a previously implemented ConfigMgr 2007 Site. or 2) the Maintenance Window policy got changed from 19:00 to 20:00 at some point and the not every machine got the news. Whatever. I have no idea what I'm doing here. Resolution I created a new Maintenance Window to replace and assigned it to the appropriate Collection. I waited an hour or two for the clients to do their policy pulls and guess what: 

We noticed our Automatic Deployment Rules for Software Updates failed to automatically download and apply this month's patches from Microsoft although they are correctly listed in the Catalog. 

You didn't specify what your environment is but if you're using Unix I think a combination of dig and grep should work. should be the hostname of your nameserver, is the domain your host is part of, and HOST is the host that you want to find all the CNAME records for. That's actually a tab character in the grep command, not literally (you may have to adjust the grep string). Also your nameserver needs to to be configured to allow zone transfers, the particulars of which will be implementation dependent. 

Is there any reason why an USB-based install won't work? It will be much simpler and faster. If it all possible, I would try to go that route. PXEboot can be... er... troublesome. 

I am working on a migration from a VMware vSphere environment to a Hyper-V Cluster utilizing Windows Server 2012 R2. The setup is pretty small, an EqualLogic PS6100e and two Dell PowerConnect 5424 switches and handful of R710s and R620s. The SAN was configured as a non-RFC1918 network that is not assigned to our organization and since I am working on building a new virtualization environment I figured that this would be an appropriate time to do a subnet migration. I configured a separate VLAN and subnet on the switches and the two previously unused NICs on the PS6100's controllers. At this time I only have a single Hyper-V host cabled in but I can successfully ping the PS6100 from the host. From the PS6100 I can ping each of the four NICs that currently on the storage network. I cannot connect the Microsoft iSCSI Initiator to the Target. I have successfully added the Target Portals (the IP addresses of PS6100 NICs) and the Targets are discovered but listed as inactive. If I try to Connect to them I get the following error, "Log onto Target - Connection Failed" and ISCSIPrt 1 and 70 events are recorded in the Event Log. I have verified that access control to the volume is not the problem by temporarily disabling it. I suspect the problem is with the Portal Group IP address which is still listed as Group Address of old subnet (I know, I know I might be committing the sin of the X/Y problem but everything else looks good):