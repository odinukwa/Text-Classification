I'm having trouble saving a large (relative to spark.rpc.message.maxSize) Spark ML pipeline to HDFS. Specifically, when I try to save the model to HDFS, it gives me an error related to spark's maximum message size: 

You definitely have interval data, that is, data which takes on discrete values, as opposed to continuous data, which takes on values along a continuum. It may be of value to additionally determine if the data is ordinal, meaning that the order of the values is important, for example if [0, 1, 2] signifies [small, medium, large] or some analogous system. In the case of ordinal data, it may be best to keep the data as exposed to the SVM training process in integer form, as the integer representation encodes some information about the relationship between the categories. This approach would also be more reasonable if the values that the variable could take on in a production setting could expand beyond the values you've already observed in the training set- a categorical approach would be less able to handle new values in that context. If there are no ordinal relationships and you suspect all of the possible values are enumerated in the training set,treating the variable as categorical would be approriate. 

In order to set the number of threads used in Theano (and, therefore, the number of CPU cores), you'll need to set a few parameters in the environment: 

Absolutely you can create an approach that forces high-precision class tagging algorithm (at the natural cost of recall). What's more- you can do this with (at least) any method that provides a percentage calue for predictions, which is the vast majority of classifiers. The key is, as you mention, to find the minimum acceptable value of precision and cut the predictions at that value. If a minimum precision is your only constraint and your solution is not sensitive to the recall (getting all or the highest possible proportion of the websites correctly classified), this is a very simple matter. Some lower percentage of your observations will be classified, but those that are will be more likely to be correctly classified. For example- if your Precision floor is 70%, your cut could look something like this: 

Euclidean distance -by which in this application, I assume you mean the euclidean distance in an $n$-dimensional space defined by the distribution of document contents among $n$ topics considered, is a valid measure to use in comparing the topics represented within two documents. What you're doing by applying this method is quantifying a topic frequency difference within this newly defined space, and so interpretation of these quanta will require analysis of the space. For example, what euclidean distance indicates that documents are relatively similar? In distiction, the normalized result of something like the hellinger distance provides an easily interperable framework by which to evaluate the results- a score of 0 indicates no overlap in the distribution over the topics in question of the two documents, and a 1, perfect overlap. For the efficiency concerns, it's not clear to me why you couldn't truncate your topics considered to the crucial topics and then calculate any of the metrics on the distributions over ony those topics, rather than the entire universe of considered topics. 

Depending on what exactly you mean by framework, I would argue that there is. Using a REST interface to serve a production model at inference time is pretty close to a general serving framework at this point. However, there are a potentially infinite number of use cases that would diverge from this standard even if one categorically existed. Some mobile apps using image recognition models on personal data that the user doesn't want transmitted to the cloud are pushing the envelope of ways to compress formerly huge neural network architectures into ones that easily fit into mobile-sized memory, for example. If you're talking software framework, it shouldn't be surprising that a field of programming practice (or, rather, plethora of fields) that find needs for a variety of languages and even ecosystems within languages would have difficulty forming a consensus around a single aporoach. It's likely that deployment frameworks have yet to build in enough benefit to justify a reduction in general flexibility to justify their use. 

Stationarity is an important factor in determining the model structure for the ARIMA family of models. Some of the reasons for this are discussed in relative depth in this question. Briefly, the reason that you have to consider the stationarity of the series explicitly when developing a model in the ARIMA family is because the model has to have descriptive power over changing conditions, which you have to make choices to enable. In contrast, an LSTM is not constrained on this dimension- i.e. a sufficiently trained LSTM with a sufficient architectural descriptive base can determine the changing nature of the time series without the modeler making explicit choices based on that feature of the data. You WILL want to understand stationarity in some form regardless of this feature, as it will help inform the useful life period of your model as well as lifecycle concerns - i.e. how often it will need to be retrained, what monitoring is necessary in production, etc. 

Abstractly, if you've already considered decision trees as decomposable into directed acyclic graphs, then one example of you're looking for is, straightforwardly, a Markov Chain. Markov chains can, indeed, model sequences of arbitrary length. Additionally, markov chains containing cycles are possible-- usually referred to as hamiltonian-embedded markov chains. 

If we, however, take the model trained on a single sin curve and further train it on the larger range, we begin to see progress after 1000 epochs: 

A chord diagram would allow you to see interactions and co-occurrences of likes between each of the heroes directly, including relative magnitude of the effects intuitively and immediately. You could also include other properties of the character (universe of origin, gender, time period of introduction etc) by use of color and/or positioning of the hero in question on the circumference of the diagram. As chord diagrams are a graph-based approach, you'd want to transform the individual observations you have into what would be a (this has to be a first for this term) hero-like-cofrequency matrix formatted as follows: 

To compare two LDA topics, you're really trying to compute the distance between two probability distributions. One such measure that's commonly used in these circumstances is the Hellinger Distance. To find the closest match for $x_1$ in the topics for $y$, you would calulate the Hellinger Distance between $x_1$ and each $y$ topic, then take the lowest one. Keep in mind that there's no guarantee whatsoever that the "most similar" topic in this sense would be remotely, subjectively similar. 

Possible ways to expose these categorical variables as part of a time-step to be fed to an RNN include producing One_Hot Tensors, Hash Table Representations, or Embedding Layer Outputs from the categorical fields and concatenating all of the feature tensors at each time-step to feed to your RNN. If, instead of discrete categories, you're looking to interpret the strings that make up the fields as sequences themselves, you could encode them numerically at a word- or character level and pass them through a subsidiary RNN to produce the internal representation at each timestep. This would be appropriate only for cases where you have a very large variety among strings in those fields. Apologies for the Tensorflow links if you're not using that framework.