Yes. A Huffman code is optimal. If a less frequent symbol had a shorter code length than a more frequent symbol, then you could get a code with better performance by exchanging them. 

This problem is NP-hard. You can get 3-SAT with the gates min(x,y), max(x,y) and 1−x. What we want is to reduce a 3-SAT problem to a circuit for which you can get 1 if all the variables are satisfiable, and you can only achieve something strictly less than 1 otherwise. We can force all the variables to be either 0 or 1 by taking a minimum of a lot of expressions, and make these expressions include max(x,1−x). Now for every clause in the 3-SAT problem x&vee;y&vee;z, we put the expression max(x,y,z) in the minimum. I don't know what the optimal value is for a non-satisfiable 3-SAT problem, but it will be strictly less than 1. 

Each edge has a unit capacity. And if you have $x$ units of flow from A to B and $y$ units of flow from B to A, you can route it as long as $x+y \leq 1$. This is the gadget referred to in Andreas Björklund's comment. 

The Stanford Encyclopedia of Philosophy article seems to be missing a very important point. There were no computers when Turing wrote his 1936 paper. I believe he was thinking about them already, but to explain his theories to the mathematician on the street, who had never dreamt of computing machines that had capabilities beyond the relatively limited office machines built by companies like IBM, he had to frame them in terms of an effective procedure carried out by a human. Gandy's paper "Church's Thesis and Principles for Mechanisms," in The Kleene Symposium (1980) states that the Church-Turing thesis does not apply to machines. It then gives what purports to be a proof of it for a very limited class of machines. Among the things that Gandy claims is that the original Church-Turing thesis did not take parallelism into account. Gandy's machines don't take into account the possibility of randomness, of non-mechanical physics, of action at a distance, of asynchronicity, of continuous variables, and other things that might be used to build actual physical machines. So was the original Church-Turing thesis intended by Church or Turing to apply to machines? Andrew Hodges has a paper considering this question, in which he quotes Church's review of Turing's paper: 

Tsuyoshi, great observation in your comment! I think this nearly solves the problem. Consider the following two questions 

This problem is known as the question of realizability of pseudoline arrangements by straight lines. An arrangement of pseudolines is an arrangement of $n$ curves in the plane, such that any two curves intersect exactly once. For pseudolines, you can define the same labeling as the OP does. Pseudoline arrangements are related to oriented matroids, and you can use these to give a polynomial-time algorithm for telling whether a labeling of regions is realizable by a pseudoline arrangment. The question of whether a pseudoline arrangement can be realized by straight lines exhibits Mnëv universality, which means that it's at least NP-hard (and probably strictly harder). In particular, it is equivalent in complexity to the existential theory of the reals, which is known to be NP-hard, and to be contained in PSPACE. There is a very nice AMS feature column on oriented matroids. You can read Richter-Gebert's paper "Mnëv Universality Revisited" for a nice discussion and simplification of the proof of Mnëv universality of line arrangements. Hyperplane arrangements also exhibit Mnëv universality. I don't know offhand of a good reference for this, but it's clear that they have to be at least as hard as line arrangements, and so they are also equivalent to the existential theory of the reals. 

Do you have a tame physicist (preferably one who is familiar with Quantum Monte Carlo techniques) that you can talk to? I don't think it's actually all that hard, but if you try to understand it by reading about it, I strongly suspect that you'll find the books all assume you understand some moderately advanced quantum mechanics/quantum field theory (time-ordered integrals, some non-obvious properties of operators) which they don't explain, and if you don't understand these, you'll find their explanations completely incomprehensible. I also see from Google that there are lots of different variations on the Quantum Monte Carlo method, some of which will be harder than others, and deciding which one to use for a particular problem is very likely a skilled art. 

Positions where $t \geq s-1$ containing an odd number of stones. Here, if $s>0$, remove a stone from a heap with a single stone. If there's only one heap left, we've won. Otherwise, we now have $t \geq s$. If there are no heaps with a single stone, remove a stone from a heap with at least three stones. (Since there were an odd number of stones, this is possible). Since $s = 0$, we have $t \geq s$. Positions where $t \leq s-1$ containing an even number of stones. Here, if there are any heaps with at least two stones other than the largest heap, remove a stone from one of them. If this heap has three or more stones in it, $t$ decreases by one. If it has exactly two stones in it, $s$ increases by one. We now have $t \leq s-2$. The last case is when all the heaps except one consist of single stones; in this case, it is easy to check the first player wins if there are an even number of stones. 

No such matrix exists. The Desnanot-Jacobi identity says that for $i \neq j$, $$ \det M_{ij}^{ij} \det M = \det M_i^i \det M_j^j -\det M_i^j \det M_j^i $$ so using this, we get $$ \det M_{12}^{12} \det M = \det M_{1}^{1} \det M_{2}^{2} - \det M_{1}^{2} \det M_{2}^{1} $$ But your requirements force the left-hand-side to be 0 (mod 2) and the right-hand-side to be 1 (mod 2), showing they are incompatible. 

Strassen's statement needs to be put into context. This was an address to an audience of mathematicians in 1986, a time when many mathematicians did not have a high opinion of theoretical computer science. The complete statement is 

If we have $a|0\rangle + b|1\rangle$, there are two quantities we need to know to specify the qubit: 

Pick two runs at random. If you can exchange them and still have a legal sequence, do so. Pick two adjacent runs. If you can exchange them and still have a legal sequence, do so. Pick two runs of the same colour. Redistribute the balls in them randomly among legal possibilities (so if the maximum number of balls in a single run was 3, and you had 5 balls total in the two chosen runs, the first one is equally likely to get 2 or 3 balls; if there were 3 balls total, the first is equally likely to get 1 or 2; if there were 4 balls total, 1, 2, and 3 are all equally likely). Pick some colour $C_i$ at random. Consider the sequence $S'$ of balls with all balls of colour $C_i$ removed. Now, pick at random two points in $S'$ where adjacent balls of different colours touch. a. If there are two runs of colour $C_i$ at these two points in the original sequence $S$, and neither is maximum length, move a ball from one to the other, with each direction chosen with probability ½. b. If there are two runs of colour $C_i$ at these two points in the original sequence $S$, but one is maximum length and the other is not, move a ball from the maximum length run to the shorter one with probability ½. c. If there is just one run of colour $C_i$ at one of these two points in $S$, with probability ½ move one ball from the run to the other point. d. If there is no run of colour $C_i$ at either of these points, or if there are runs of maximum length at both of these points, do nothing. 

Yes, they can. Recall that any reversible classical function can be computed in superposition. Now, generate the state $$ \frac{1}{\sqrt{n!}}\left(\sum_{i=1}^n |i \rangle \right) \left( \sum_{i=1}^{n-1} |i \rangle \right)\left(\sum_{i=1}^{n-2} |i\rangle \right) \ldots \left(\sum_{i=1}^2 |i\rangle \right) \bigg(| 1\rangle \bigg) . $$ This is the superposition of all sequences where the $i$'th position contains a number between 1 and $n+1-i$. It's easy to generate because it's a tensor product of $n$ registers. Then, find a reversible classical mapping from these sequences to permutations. One way of doing this is as follows. If there is an $i$ in the $t$'th position, take the $i$'th unused number available for the permutation. For example: suppose we have the sequence 24211. This gets mapped to the permutation as follows: $$\begin{array}{r|l} 24211 & \ \ \ \emptyset \\ 4211 & 2 \\ 211 & 25 \\ 11 & 253 \\ 1 & 2531 \\ \emptyset \ \ \ & 25314 \end{array} $$ To illustrate, the third element of the permutation is $3$ because at the third step, the available numbers are $1$, $3$, $4$, ($2$ and $5$ are already in use) and the second of these is $3$. You can easily check that each of these steps is a reversible computation, and so implementable in superposition on a quantum computer. 

Your question (1) is essentially the difference between Turing machines that recognize languages and Turing machines that compute functions. This difference is essential for proving theorems about complexity classes like NP. And in fact, if we look at Cook's 1971 paper The Complexity of Theorem-Proving Procedures, which proved the Cook-Levin theorem, we find 

P=NP is a well-defined mathematical question which does not depend on space-time geometry. The question "which problems can be solved by computations that are tractable in this universe?" may depend on physics, and the answer does indeed appear to change in hyperbolic space or with quantum mechanics (e.g. quantum computing). However, this doesn't affect the P=NP question. In fact, one of the first reactions of a theoretical computer scientist would have to your reference is: "What complexity class can be computed by a cellular automaton in hyperbolic space?" If you redefine complexity classes when you change to hyperbolic space, it becomes much harder to talk about this question. And if you look later in the book, the author defines P$_h$ as problems solvable in polynomial time on a hyperbolic cellular automaton, and proves P$_h$=PSPACE (and NP$_h$=P$_h$=PSPACE), so even the author isn't really taking complexity classes to change in hyperbolic space. 

The Jacobi symbols $(\frac{a}{n})$ which are $\pm 1$ are exactly those for which gcd$(a,n) =1$, and thus the number of these is $\phi(n)$, where $\phi$ is Euler's $\phi$-function. Now, if $n$ is not a perfect square, exactly half of these are $+1$ (this can be seen using the Chinese remainder theorem). Thus, if you know |$J_n^{+1}$|, you know $\phi(n)$. And given $\phi(n)$, you can factor $n$. So there is a polynomial-time algorithm for |$J_n^{+1}$| if and only if factoring $\in$ P. 

Take 3 points A, B, C on an equilateral triangle and add 3 more points D, E, F, in the center. It's clear you want two of A, B, C on one side of the cut, so let's say the cut on these three points is (AB;C). Now, each of the points D, E, F has to go on the C side of the cut, so the optimal cut is (AB; CDEF), and the ratio is easily checked to be 2/3. Now, move each of the points D, E, F slightly away from the center to form a small equilateral triangle. It doesn't matter in which direction, as long as they're symmetric around the center. If you move them a small enough distance, the optimal cut still has to be (AB; CDEF). Consider the length of this cut. The edges (AC, BC) form 2/3 of the total length of the edges (AB, BC, AC). By symmetry, the total length of the edges (AD, AE, AF, BD, BE, BF) are 2/3 of the length of the edges (AD, AE, AF, BD, BE, BF, CD, CE, CF). But none of the edges (DE, EF, DF) are in the cut. So the ratio of this cut is strictly less than 2/3. You should be able to optimize this construction to find a configuration where the optimal cut is significantly less than 2/3. Trying it, I get that if you take six points arranged in two equilateral triangles having the same center, with the smaller one $(\sqrt{6}-1)/5 \approx .2899$ the size of the larger one, then the max-cut becomes $.6408$ the total weight instead of $2/3$. 

Isn't this problem NP-complete? Start with a graph $G$. Make $x_i = 1 \ \ \forall i$, where $i$ is a vertex of $G$. Now, for each edge $(i,j)$ of $G$, make $x_{i,j} = -3$. Otherwise, make $x_{i,j}=0$. Give weight 0 to all three- and four-level marbles. To maximize your objective function, you want to pick as many marbles as you can so long as you don't take two marbles connected by an edge. This is the maximum independent set problem, which is NP-complete. 

We wrote a paper related to that. Perfect packing theorems and the average-case behavior of optimal and on-line packing, by Coffman, Courcoubetis, Garey, Johson, Shor, Weber, and Yannakakis. See Theorem 1: 

I'm a little baffled by question (1). An unextendable product basis exists in $H_1 \otimes H_2 \otimes \ldots \otimes H_n$ if $n\geq 3$ or if $n=2$ and $\dim H_1, \dim H_2 \geq 3$. In all of these cases, it is straightforward to find one. For question (2), the question is equivalent to checking whether there is a tensor product state in the subspace which is the complement of the space spanned by the basis. Leonid Gurvits has shown that checking whether a general subspace contains a tensor product state is NP-hard, so I suspect that it is hard in this case as well. 

Although families of circuits aren't usually defined this way, you actually could require this for non-uniform families of circuits; i.e., require circuit families to be consistent. There still would be circuit families for non-uniform languages, and P/poly would still contain all the same languages with this definition (P/log would change, though). Then you could indeed leave out any finite number of circuits, and the function would still be well-defined. You wouldn't cause any paradoxes. One question you could ask is: are there any advantages to this alternate definition of P/log? My guess is that there are. 

Compare the polynomial hierarchy with the hierarchy for interactive proofs. If for some fixed k, you have k alternations in an interactive proof -- IP(k) -- the resulting complexity class has no more power than what you get with two alternations -- that is, IP(k) = IP(2) = AM (assuming k≥2). However, if you allow a polynomial number of alternations, you get the complexity class IP = PSPACE, which is believed to be much bigger than AM, a class is contained in Π2P, at the second level of the polynomial hierarchy. So this phenomenon actually happens (although, not so far as we know, with the polynomial hierarchy). This happens because the reduction which takes a problem of size n in IP(k) and turns it into a problem in IP(2) blows up the problem size, so that while for any specific IP(k) the problem remains polynomial-size, if you let k vary, the resulting reduction doesn't give problems that are polynomial in k. 

Step (2) is always possible, since after step (1) the sequence ends with a 0. Except for step (1), which adds at most two bits to your encoded sequence, this cannot do worse than double the string size, and generally does better. For example, you encode 0110100101 → 0.11.0.10.0.10.10 → ababbbababbababbabb, taking 10 characters into 19. If I did my calculations right, starting with $n$ random bits, this encoding gives an expected length of 11/6 $n$ bits, which is only slightly more than the optimal value of around 1.813 $n$ bits. 

This is reversible, and can theoretically be performed using an arbitrarily small amount of energy. Restore to one is the operation 

Linear algebra is very useful for certain areas of graph theory (including some fairly advanced linear algebra). It can also be very useful in practice -- linear algebra and graph theory are two of the things which make Google work. And elementary linear algebra is used in enough places that you might want to learn it in any case. 

I've been thinking about your problem a little more, and I think separating the Turing-universality from the isolation part may be quite hard. For example, consider a Turing-universal CA with states 0,1,2, where the Turing-universal computations never use two 2's in adjacent cells. You could also have arrange the rules so that if there are two 2's in adjacent cells, they will never be changed. I am fairly sure that one can devise such a Turing-universal CAs so that if you without the state 2, the CA is not universal. You then can't just drop the state 2, as required by property 1. I think this would also evade your new property 2, although maybe not if you required the CA to use all the rules. But it seems like it might be quite difficult to come up with a reformulation which would evade all examples such as this. 

The 2-OPT heuristic for finding TSP tours is fairly close to what you're asking about. It's a local improvement algorithm which looks for sections of a TSP tour which, when reversed, will yield a shorter tour. The 3-OPT and the Lin-Kernighan heuristics work along the same lines, but use more complicated moves. Looking for the optimal rearrangement of some subpath of a TSP tour is going to be inefficient unless it only involves a short section of this tour, and then I imagine it will be inferior to more global rearrangement algorithms such as 2-OPT (which can reverse a large subpath of a tour). As far as I know, the concept that you're asking about has no name.