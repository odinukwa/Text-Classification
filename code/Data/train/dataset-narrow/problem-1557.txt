blocks (possibly with plain text above each block showing what certificate the block contains). If you split each of those blocks into its own file so you end up with and you should be able to run on each of them to get their respective certificate subject lines. Assuming that has a subject line of you should then be able to run and that should, hopefully, connect without giving you the self-signed certificate error. 

To apply this to arbitrarily nested directories in the cvs hierarchy the matching local directories must also exist. To find the directories available under (for example) the following appears to work. 

and tell you what sasl plugin types are available. I would assume those would return errors or empty lists if sasl support was not enabled at build time (though possibly not). You could also check the output of and see if it links to a sasl library or not (though it might be possible for postfix to be built with sasl and not link to a sasl library if it supports it via a plugin or something, I don't know whether it does or not). 

If this is a new environment I would just create the new git user like normal and then run through the gitolite setup for that user (and forget the gitolite3 user). If you really want to keep the current gitolite configuration/repositories/etc. you can probably just copy all the relevant files to that user once you are done (and make sure the ownership information is updated) and it might Just Work (check the gitolite docs on moving the repos to a new machine in case it lists any extra gotchas about this process). 

This appears to be something that the javascript on the google page is doing. I don't see it in firefox with noscript enabled and stop seeing it in Chrome on Windows if I disable javascript. I don't know what specifically as I haven't dug any deeper than that. 

Here is an illustration of my prose description above. After removing things that are not pertinent to the issue, the Monit configuration is like this: 

Building a configuration using external tools is the way to go. I've used + . I use a custom Python script in one of my projects. The choices are plenty. 

If a file named exists one level above the root, then the server will be in maintenance mode. (This is the test). Except that if there exist a cookie named with the value , the check described above will be bypassed and nginx will serve the data as if the server were not in maintenance mode. (This is the test.) 

(Emphasis added.) Going by this, unless is responsible for rotating the log, then will be ignored. I thought about maybe configuring logrotate to perform rotate operation that effectively does nothing but I'm not seeing anything among the configuration options that would easily allow that. 

Since you have not mentioned anything about your cache configuration in Nginx, I'm going to assume you did not set a cache, and this would explain why your header has no effect for dynamic responses. When it comes to static resources, Nginx has a really easy way to determine how to handle : it compares the time in the field with the time the file was last modified. No problem there. When you want Nginx to do the same with dynamically generated responses, there's nothing for it to compare against, unless you turn on caching. By default, Nginx does not remember the responses it has served. When you turn caching on, Nginx has a way to compare an incoming request with a response previously given, and thus has a way to use . I've found this article really useful to learn the finer details of setting an Nginx cache. 

Assuming you mean then yes, that should work. Check that the file has exactly two lines and that there aren't any extra spaces or anything in the key line for the second key. Also check that you have added the correct lines for each desired key. 

You need to paste the rpmfusion.repo (or whatever) file from your /etc/yum.repos.d directory. That being said that error either means the file is incomplete or the mirrorurl that is configured is broken (i.e. not returning any values) or no longer exists (assuming yum wouldn't give a different error for either of those conditions). Given the contents of that file it appears as if you have misconfigured the repo. You can see this by trying to load in your browser and inspecting the output. Notice the error indication and the fact that all lines are commented out. You need to adjust the mirrorlist entries in those repository configuration blocks to match your system. You probably want to use something like and friends. 

I believe you have run into this bug. Which I just duplicated on a CentOS 5 machine and EPEL 5. That's a pretty bad bug to have in a shipping RPM if you ask me. Luckily the workaround is not too bad. Either create the symlink in the normal place (which I don't generally recommend) or create a directory and put the symlink in there and use LD_LIBRARY_PATH to point there. (I'm not sure why you have LD_LIBRARY_PATH set in your environment already unless that was a debugging attempt.) 

I would suggest trying bind mounts (like /Database already is) rather than the unlink/relink games that page has you play. Try something like in for /lib/modules. Alternatively, I assume your problem replacing the /lib symlink was that commands started to fail immediately after the /lib symlink was removed. That's likely because the replacement was not atomic. You might have better luck with as that should be more atomic if not, in fact, actually atomic (I don't know offhand). 

I'm using Monit to monitor various processes that need to be up and running as a group for a web site to work properly. To bring up or bring down the site, there's a definite order by which the processes must be started or stopped. The dependencies are as follows. (The names have been changed to protect the innocent. I use more descriptive names in the real configuration.) 

The site is always started or stopped through Monit so as to avoid the possibility of race conditions, or Monit working against me. (e.g. I stop a service and Monit keeps starting it back.) The problem is that it takes much more time than necessary to bring the whole site up. If I instruct Monit to start the site, then once Monit has figured the dependencies, the sequence of actions on Monit's part is: 

I would also have to change the length of the polling cycle to something smaller so that a cycle is less than 2 minutes. However, I don't want Monit to always poll these services more frequently. I'd like Monit to only poll services more frequently when it is in the midst of waiting for a state change. Say, if Monit has started a service and another service depends on it, poll at a 5 second interval rather than 2 minutes. I'm not seeing any way to configure Monit to do this, but maybe I missed something. 

This service has the and method defined. You do not specify such methods for your service, so they are undefined and Monit cannot do anything if somehow it is requested to , , or (which is a third method) the service. You don't need to define them if Monit is not actually going to be tasked with starting or stoping the service. I have a disk space test where the methods are not defined, and it works just fine. 

This is known as pinning. Ubuntu has some high-level documentation available here which recommends reading a man page for further information about specific syntax for the configuration file in question. 

Entry capitalization matters in the ifcfg-eth* files. Those files are read into a shell script context and looked up by known name and is not the same variable as . 

Drop the awk if that doesn't show what you expect. The output might differ slightly from what I got. If you are speaking the cvs wire protocol directly you can do a similar thing without the directory hackery I believe but I'd have to dig a bunch more to sort that out again. (I believe Zend/Eclipse does that when it detects an older cvs server version.) 

If you configure apache with no listening configuration on port 80 and ensure that you configure it to listen on port 443 with ssl correctly then yes, your server will not listen on any non-ssl ports. 

According to that documentation it looks like anywhere on the ISO you want as long as you point the path to it correctly in the preseed/url parameter. The example documentation puts it in the root of the ISO filesystem. That being said an ISO is a not a zip file and extracting and recreating one is not as simple as a similar operation for a zip file (though there are plenty of tools that should allow you to recreate the ISO as needed). It might be simpler, if you have an http/ftp server to stick the file on briefly, to just use that for this. 

I'm going to assume that the -f argument expects an absolute path to the file and interprets the path as relative otherwise. Are you running those commands from in /etc/openldap? Does using work? Are you running the commands as a user that can read that file? 

Starts . Sleeps for 2 minutes. Detects that is running, so start the two workers. Sleeps for 2 minutes. Detects that the workers and redis are running, so start . [Sleeps for 2 minutes] [Detects that is running.] 

I've bracketed the last 2 steps because they are practically moot since the site is effectively up and running before the last 2 minute interval. The 2 minute sleep is the default polling interval that Monit uses to check on services. I know that I could reduce this interval so that these services are always polled more frequently. For instance, I could do 

All incoming connections are required to use HTTPS so the cookie never goes in plain text over the Internet. I've also set the server to which nginx forwards the requests to display a big fat warning if the cookie is set so that I don't mistakenly forget that the site is in maintenance mode. 

It looks to me like the issue is that the upstream server is just not sending response that contain an expiration date () or a cache validator (for instance, ). (The cookie expiration time has nothing to do with caching.) The HTTP 1.1 spec says: 

Looking at my own monit logs, I see this happen if for some reason Monit is trying to start a service for which no method has been declared. Here's an example from the documentation: 

(Emphasis added.) So it looks like your Django setup does not have caching turned on. I suggest you turn on per-site caching. If you do this, Django will generate in its response the headers that Nginx needs for caching. Another option would be to use the option in your Nginx configuration. I used a setup similar to yours and replicated the behavior you got, then I added which tells Nginx to cache responses that have any status code () for 5 minutes . Once, I did that, I got cache hits. 

The other possibility, in general, is that individual verification checks can be disable on a per-file or per-directory level in the spec file itself. So, while not true in this case, it is entirely possible that a packager could disable MD5 sum checking for files that are known to change for one reason or another. 

The following hackery should work with 1.11.* client and server versions of cvs. I imagine it will work with others but those are the versions I have locally to test with. 

I would suggest that you simplify your life tremendously and avoid going down this route any farther and instead see about finding a package for the 6.2 version and either using that directly (if possible) or building a package from the source package for it. If all else fails you should be able to take the current (5.9) source package and replace the version bits (and possibly patches) and try to build a package from it with the version you want (6.2). Using a package (yours or someone else's that are built correctly) will avoid the large amounts of pain you will inevitably encounter trying to do this all from source when you aren't comfortable with this stuff. 

That's a linker failure. It looks like you may not have the gd library installed (or more likely that you have an incorrect version of the library installed). Does report any missing libraries? (I don't expect it will.) What version of libgd do you have installed? 

openssl ciphers will tell you what openssl will translate your cipher spec string into. Use to see verbose information about the ciphers listed. So for your listed cipher string on my CentOS 6 system I get: 

If you are already using yum to install your packages then the RPMs already exist and you could just grab the RPMs you want and store them locally for later use... or you could get a bit fancier and actually set up a local repository with your desired packages (or even the entire distribution) and use that for your servers (though if you go this route you need to be careful about getting security updates and the like if these are things that need a reasonable security status).