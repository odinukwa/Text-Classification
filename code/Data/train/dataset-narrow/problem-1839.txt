Cheers, I have the situation that I want to apply a service to a host, but only for hosts that a certain service (with certain variables). So essentially something like: 

The Apache module will do exactly this for you. You enable the module, and the config file should look something like 

for the is probably what you want to have a look at. The documentation snippet reads like gitolite doesn't ship with anything for that, but rather relies on external factors. You can set up sssd to authenticate against the LDAP server and then use , for example. Documentation should be easily findable. 

"Caching nameserver" is what you're looking for, I assume. Just add a Forward instruction to your bind configuration, as explained in many tutorials and setup instructions, and you'll be fine. 

Exactly so. You check whether the mail server who just talked to you is not pretending to be someone else by corelating the forward DNS query (the A record of the domain) with the reverse query (the PTR record of the IP address). If they match, bravo. If they don't, boo, hiss. This, of course, implies that any server with multiple host names on their IP address should just use the address specified in the PTR when sending mails. 

You should have specified which operating system you are using. Under Linux, you just use , as the manpage can easily tell you. 

is there any native way of actually retrieving the current check results from an Icinga 2 instance? I can't find it documented. If not, are there accepted, known-good command line frontends to query Icinga/IDO/API? 

The problem can't be easily summarized, but the main problem is that vendors are always reluctant to invenst time, and thus money, into developing drivers where they are not sure that they will achieve the necessary return value. In short: if they don't think there's too many 64bit users, they won't develop for them. Same goes for applications. Most applications are just written for 32bit operation and rely on upwards compatability through padding to work. But depending on what kind of arcane magic is performed in the source code, this can end up pear-shaped, even though it is not too common - for the most part, only complex applications like games really suffer from compatability problems. The OS itself is rarely at fault for doing bad stuff with the architecture - most implementations of 64bit are sound. As to when 64bit OSes will be a majority, well, it's probably going to be a slow, creeping process. The easiest way would be for a chip manufacturer just to wholeheartedly dump 32bit processing compatability. This would force OS developers to maintain 32bit application compatability layers, and these would be implemented so that it's okay to run old software this way, but actively discourages any users trying to write new software to go 32bit. 

Just to be on the safe side - you haven't accidentally set a mail address for the user in /etc/email-addresses? 

I'm not quite sure on this one, but most likely the system variables will be available all the time, wherease the Default User variables only apply as long as the user has not overwritten them. 

Maybe you need to beef up your virtual memory on your system hard drive, where low hard drive space could have caused said problem. Also, from a logical point of view, the server does not actually need to store a file in memory when copying from file system to file system; it just allocates a buffer in memory the file passes through. Depending on how you copy files, though, some applications will first store the file completely in memory, and then write it to the disk. Try to use a protocol like FTP - if it still happens, you should probably look into some networking problems. The interesting question here would be how the server actually stores the files - as you can see, the I/O load is just way down, which means it's not actually writing the file anywhere, just buffering it in memory. 

Storage / Rates: "The GB of storage billed in a month is the average storage used throughout the month. This includes all object data and metadata stored in buckets that you created under your account. We measure your usage in “TimedStorage-ByteHrs,” which are added up at the end of the month to generate your monthly charges." FTP: Because it would be silly, as the storage model does not work this way. 

Usually, a responding client should receive a sigterm and off itself, thereby terminating all its forked child processes. If, on the other hand, the client is not responding, a normal termination signal will probably evoke no response. So you then proceed to send a SIGKILL, which tells the system itself to Make That Process Stop, Now - which it can do most of the times, except when the process is in a state of ininterruptible sleep, like waiting for some I/O input or similar. In that case, even the SIGKILL will end up doing nothing to the process; the child process will probably in a similar state of retardation. There is a border case where the parent process might terminate just nicely, but the child process is not killable. Then you will end up with a zombie process that cannot be cleaned up since its father is gone, but it is not responding to anything. The only way to get rid of those is a reboot; in most cases, though, they are quite harmless, as the only thing they do is take up a PID while whatever socket they tried to use has withered away already. 

You will probably want to just take the next logical leap and look in the direction of the so-called Host-Based Intrusion Detection Systems (HIDS), which, as one of their features, offer setting up file monitoring. I can't recommend any HIDS for Windows, but you should be able to find something to suit your needs. 

Have a look at DeNIC's Nameserver Predelegation Check. Short version: your DNS servers for don't actually allow querying . 

You could upgrade your PPTP connection to EAP-TTLS, thus relocating the problem from storing a password to storing a certificate. Nevertheless, encrypted file systems can only be recommended for such affairs. 

Environment variables aren't Tomcat properties. AFAIK you cannot use the properties inside the actual code, just in e.g. server.xml. If you want to use environment variables that you can retrieve with System.getenv, you'll need to define them with your operating system; that quite depends. A classic Linux example would in or in the systemd service. 

You just need to set aside a partition or the Windows, at best giving it the appropriate type number when creating the partition. Ideally, you would have your own hardrive which you can just throw Windows on to, but that option might not be available. What, at least for previous Windows', was necessary, is that the partition ideally be a primary partition and bootable. You will also need a USB boot medium or your installation CD to boot your Ubuntu in the case Windows tries to overwrite your MBR with its own bootloader. Just reinstall your own boot-loader and add the Windows entry (with etc) if desired. The Ubuntu you can of course partition like whatever you feel like. 

According to a paper by Mathur et al. (p. 29), e2fsck time grows linearly with the amount of inodes on a filesystem after a certain point. If the graph is anything to go by, you're more effective with filesystems of up to 10 million inodes. Switching to ext4 would help - under the condition that your filesystem is not loaded to the brim, where the performance gain (due to not checking inodes marked unused) has no discernable effect. 

I'm still looking for something like a free and open source replacement for the prodigial Evernote, since the lack of maemo and S60 support sort of spoils the beans for me. Thus far, I mostly use my rather good memory and a plethora of tabs with Google results and similar I open up when trying to find an answer to something that interests me. This can lead to a dozen or so tabs open. I don't use it in the case of server administration knowledge, but I actually also use a dokuwiki just to jot down anything ideaish and have it accessible; this is a step up from my previous, studenty paper mess with written notes just about anywhere. 

The problem is that your server might bugger up when the client is trying to use passive FTP. With normal FTP, your client tells the server what IP and port to send the data to. With passive FTP, it's the other way round - you ask the server for an IP and port, connect to that, and get the data. Now, the problem here is that when generating the passive request, the server needs to send its IP. If the server is behind a home router, it will most likely not have a public IP, and thus the only IP it knows of itself will be 192.168.1.64. When it transtmits that IP to the client, though, it won't work, since those IPs are not routable and will up ending nowhere or someplace totally different. With linux, there's a module for the NAT setup that 'rewrites' these calls, but most home routers won't have those. So, what you have to do is give the FTP server your external IP address. The most hassle-free way can be achieved if you can give a hostname to your server. Then you just need to sign up for a service like DynDNS and give that hostname to the server. If that is not an option, you will have to manually give the server its new IP and erstart it after each new internet connection. 

You can only have an SSL certificate for a single IP address. You will need to give the subdomain a different IP address and a new SSL certificate tuned to that address. There is also the option of using multiple domains on your certificate, if your CA supports this. Then you'd just create a certificate for all the domains on that IP and use that. This might confuse some older clients and be viewed as bad style, though. 

Streaming just implies that it can offer you a constant bitrate above a certain threshhold when transferring the data, as opposed to having the data come in in bursts or waves. If HDFS is laid out for streaming, it will probably still support seek, with a bit of overhead it requires to cache the data for a constant stream. Of course, depending on system and network load, your seeks might take a bit longer. 

Error in assumption; the opposite of encryption is decryption, and signing does not operate transitively on either. Signing a message has no effect whatsoever on the encryption. 

If you want to be really wicked, you could chain ssh and tar, something like , but this can run into all hands of problems. The easier way would be just to set up an SSH tunnel with the built-in methods of SSH; look at the switch in the manpage and just forward some port to the other server's ssh port. 

I assume the tomcat is spawned by the apache. The problem could be that when the Apache closes its logfiles, the tomcat is trying to access said logfile and when it suddenly finds it has disappeared magically, it just keels over and dies. The other explanation would be that the Apache server is also resetting tomcat while rotating logs and reloading the configuration files (which SIGHUP initiates), and some bug or other prevents tomcat from restarting at that time. 

From the client, you can't be absolutely sure if you're connecting via the squid. You might have the X-Forwarded-For header present. Also, try to access the site directly. Is it fast, then, or is it just your browser itself being slow due to the whole JavaScript business? 

I'm assuming that it might just be that your kernel could be re-ordering the drive names, as it usually just assigns the names in the first come, first serve manner without something like udev to do the renaming. My suggestion would be using an initrd which comes with udev, so you can ensure that the drives are named correctly. It would be even more failsafe if you use udev (or something similar, custom-built) and then just mount the drives via /dev/disk/by-uuid/, as they usually don't change. 

Most mail servers support rewriting addresses or changing delivery method base on some metric over the input mail. You could either rewrite all outgoing addresses with a local user, or just change the delivery method to dump it unceremoniously into a file. If you're using postfix, for example, there's a rather thorough documentation available, even though it does not specifically list your case. 

Sounds like you have some kind of timeout in your authentication chain. Check how sudo tries to authenticate and watch for bottlenecks. 

Squid specifically has a feature supporting hierarchic organization of proxy servers. The squid manual page has excellent documentation on that topic. 

So if I had services with , with and with , I'd get two services, one each for and . As I just noticed, I'd probably also need some way to get a unique service name derived from the conditional services. Is it folly to go for it like this or should I just use variables for the host and derive all services from those? 

Using won't help you along. You need to put an in there so it actually has some effect. If it is followed by the line with , it should work out just as you wanted. 

Question first off: does it actually need the bind accessible to the outside world? If not, just block ingoing traffic on the DNS ports, and you're all set. But yes, indirectly this is part of an 'attack', as your mail server is probably trying to bounce back "user not found" mails to bogus servers. And do you have spamassassin running on your machine? If you're hit by a spamwave and the perl spamassassin is trying to handle all the mail, it might take down your system on unlucky configurations.