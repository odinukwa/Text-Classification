Its not a good practice for production, but for reporting and staging you can use, Even though SAN and iSCSI are related to a network share. As Dan Guzman(who commented below question) Microsoft recommends keeping the files on SAN, Local Server or iSCSI. 

And I found this error on master. found a zombie dump thread with the same UUID. Master is killing the zombie dump thread(7). On Master: 

One of my production mysql is in 5.6. So I want to upgrade it to 5.7. So before upgrade that I need to know the best way to achieve this. 

Instead of using single transaction and flush logs, you can pass This will help you to get the correct table contents and . Once the dump is completed you can see the bin log file name and its position inside the dump. 

PG_pool1 and PG_pool2 should send all the connections to Master Server, If master server goes down then the PG_pool1 will promote Slave as master and the pg_pool1 and PG_pool2 will send the traffic to Slave(now its promoted as master) Is this possible one? Or any other things to archive this(I tried Pacemaker, DRBD, But this could be a simplest one) 

Im using mariadb 10.0.29, I need to change the innodb file size to 256M, default size 50M, My config file is located on 

Im using AWS MySQL Aurora, the current wait_timeout is 28800 which is the default one, and interactive_waitimeout is also 28800. But I can see more than the default value. 

Then created a new ndf in the same file group called sec_file.ndf and created few tables, inserted 1000 rows on each table. After an hour took a file backup of my MDF and NDF files 

Restart your PostgreSQL or Reload it. you should enter the below command to connect PostgreSQL via root user 

As dezso said, its easy to change the data directory. But I did a small PoC for this. So its possible to use existing physical files to another Postgresql. Limitations 

Recently I have also done this process with Postgresql.But I didn't use any third party tools. Here is my process. 

Grant full access to SQL server service account for M:\UserLogs. (Which must be a domain account) Manually restore the database. While adding the db to AG choose JOIN ONLY. 

While installing this cluster, its asking to remove the existing How can I install this without remove anything? 

I want to map Linux's root user to Postgres user, like if I enter psql in root user it should go the PostgreSQL cli without asking any authentication. I tried to add an entry in pg_ident.conf and make the necessary changes in pg_hba.conf, but it didn't work. 

Now my question is I have enabled the trigger after restored the DB. It took 4 hrs. During the Restore, there were so many rows get inserted/Updated/Deleted. So is my target DB how will get these changes? Because I have enabled the trigger after the restoration. 

Native doesn't have a feature to ignore columns. But you can do this by exporting that specific tables as CSV then import it to your staging. Example 

Then, repeat the same step. Its my main production table and its always busy, so no locks, blocks, and deadlock. Can anyone suggest a better approach for this? I have already asked a question about archival, but here I don't want to partition the table. 

The table has 10000000+ records, so I want to update first 1000 rows, once it is done then update 1001 to 2000 rows. I need to make it as a loop. 

Im using the below query and it took min . I tried to create single column and multi-column index. But still I can't able to increase it's performance. Row count on each table: 

I don't why its happening. AndI tried to create users like `'user'@'localhost, this is working, But I don't know why the user has '%' is not working. (Im trying access the user login on the same server where the MySQL is installed) 

How the unique column has 3 duplicate values (value 2)? While restoring why its throwing this error? 

Then I need to replicate the . Just take the binlog position and set replication on Master1 will work, but the problem is binog file and position is changing frequently. So before execute the Change master command on Master1 the binog get changed. How can I achive this without downtime. 

Im using Master slave streaming replication in PostgreSQL. Repmgr is configured to handle automatic failover. Once the failover happened I need to make the old master as a slave without doing any basebackup, also the new primary requires some changes to act as a master like need to enable wal logs and other things. How can I avoid these changes in Secondary during replicate to Old master and sync Old master without basebackup? 

Im using MySQL replication with binlog_format=mixed on both master and slave, but still Im getting error 1032. 

Im using a Postgresql-9.2 on Centos 6.7. I tried to upgrade the PostgreSQL using --link. After that, I took a table backup and tried to restore this on Dev server. But Im getting an error like, 

I have MySQL Database and its 1.5TB. I need to setup replication for this. I have 32core CPU and 250GB Memory. The problem restore is taking more time even I used mydumper/myloader to backup and restore. Here is my MySQL Setting: 

Basically, ibdata1 contains the tables and ib_logfiles are having . So the important thing is to being your ibdata1 first, the only way use any recovery software, or if you have MySQLdump then restore it. 

As of my knowledge, DRBD is for Block level replication (works on storage) and Pacemaker is Service Cluster (works on service level). So which is the best one to use and best HA solution? And another question is Im using AWS EC2, So how can I manage VIP (I mean if the primary goes down how the PGpool will switch the VIP to Standby also its and in AWS)? 

As per my knowledge, PostgreSQL will create a Primary key on column and Unique index on . So the EID is the unique column. --On both servers 

IOPS limit is based on your volume size if you are using General Purpose SSD. And IOPS are your reserved IOPS. 

You can restore any database to anywhere. It'll help to refresh you Dev database. It'll help in PITR. You can restore any specific table from the specific database backup. Finally it will save a lot of time while restoring a single Database. 

Try to use SSIS which is the simplest method. Or generate the script for each table in a separate .sql file. then create a batch file to execute one by one. 

I have a master-slave setup, Slave is not in read only mode.(we can do all writes). I have created a user. 

Im using backup on secondary only.Even in few outages, I tried to use copy only full backups and log backups to recover them. 

Create a read replica. Export the data from Read Replica as CSV format. Create the tables with appropriate column names. Then import CSV files to tables. Make shell script to automate this. 

You already know the answer for this, it is FAST. Here are very basic and known benefits of using SSD. 

I have already asked here about PostgreSQL HA setup, and I got many possible answers. But the bad thing is, even though if I use any third party software like PGpool, Repmgr, pacemaker are asking for timeout(If master is unavailable for n seconds). So there will be a minimal downtime. Is there any best option to overcome this and setup a zero downtime cluster? 

You have to create the index in the below manner. This query should check this first. This covers the 

Is there any upgrade command to do this? OR Do I need to install 5.7 and move the data files to 5.7's directory? OR Take dump from 5.6 and restore it on 5.7(DB size 1.5TB) 

Set complex password. Use validate_password plugin. Disable remote root login. Disallow incoming traffic apart from your app and report server. Always create users with the proper host(don't use '%'). Rotate password at every month. Restrict MySQL data, log and backup directories. Try to implement SSL communication between the server and the clients. Restrict SSH access to the server where MySQL installed. Prevent my.cnf from other users. 

As of now no issues, tested the replicatin. Then I did a restart both nodes. After that mysql won't start. Server1: 

Im doing a replication with 2 PostgreSQL servers with Bucardo. I just want to know I have started to restore the data only to Server 2. Once it is done then I started the Bucardo triggers. Initial Restore: 

Hope, this has been fixed, in future I recommend to use mydumper for take backup of huge database and restore it. Native mysqldump and restore is pretty slow, because it's single threaded, but in mydumper you can use upto 62 threads. $URL$ 

If you have any Log backups job please disable it. Avoid taking log backups while enabled log shipping, or just manually run the logshipping backup job for log backups. 

Please never mind, I found a solution on this blog. Still the mysql data directory points to $URL$ Then did chmod 700 binlog. 

Also disabled the wal log, cleaned the wal log files and reset the xlog. I just wanted to why happened and how to fix them? 

Its good to see that you are working with Alwayson. Here are the answers for your questions: Do I need the same config for all the nodes? Unfortunately its YES, becuase alwayson is helping us to make the business always up. Once the Primary goes down, then Alwayson will failover to another node and make that as the new primary. So it must be have the same config to get the same performance here. Luckily this is the only case you must have same config. If you are using Availability group for Readonly purpose then make the reader with less config. Alwayson is for DR? NO, Human errors are also considered as DR. In this case if someone deleted a table, then this statement should immediatly replicated to Seconday. So there is no delyed replica here. Multiple Availability Groups? I didn't understand this question, but as per my understanding, You can have more than one availability Group, but you can't add one database to multiple availability Group.