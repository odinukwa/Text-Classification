Then you can see all the queries being executed in the file. 2# Another option is to use slow logs with . This you can manage on session level: 

As @jkavalik wrote you're executing the update with a dependent subquery which effectively runs it 3861 times making it highly ineffective even if the query itself runs fast. An updated version of the query could be: 

If possible I would also take a look at your table structure and improve. Having a ~100 byte primary key with many secondary keys is not really advised. You have four secondary keys and some of them are superfluous for example index is a subset of 

Changing doesn't need you to nuke ib_logfiles. This could easily be the reason for you had to restore the database (*). Just stop mysql -> change the settings in my.cnf -> start mysql. 

As @jkavalik mentions in comment the literature you want to read up on is database normalization and normal forms. Here is a good, clear explanation on them: $URL$ Generally speaking you should thrive for at least 1NF. So in your case if user has only one phone number you can still store it together but try to avoid , column creation which pattern unfortunately can be seen in many places. is most likely unique so there's no need to split that from your table. On this page you can find a lot of sample database models: $URL$ You can browse and find which resembles to your problem the most but it's also good for learning about different solutions as well. 

You can also include which will give you the possibility to order by relevance. To read more about this you can take a look on a benchmark I did with MySQL Full text search alternatives: $URL$ Option #2 You can use some search engines which provides great full text search capabilities out of the box like Solr or Elasticsearch. A very simple solution for what you want is to use the facet and facet filter with parameters like this: 

Auto Increment ID is "replicating" to the slaves. Otherwise it would be hell of a job to promote a slave to a new master if the status of auto increment needs to synced. So to answer your question it would start from 8. If you do not write actively to both master at the same time you don't need to do any settings trickery. These settings are meant to avoid situation where the two master picks the same ID for new rows inserted concurrently on both masters which would lead to conflicting IDs while applying changes from the relaylog. 

Â Alternative solution You can make the composite index the and have a single key on the other column: Pros: 

Your CPU is spending most of the time in user space. That is most likely due to locking issues or very inefficient queries. What I would do is go through the following steps and check after each if the problem is still present. Proceed only if it is. 

You probably will need an index on as well for efficient lookup. See how your queries will perform and add if necessary. 

The list is kept for MVCC ($URL$ as @jkavalik mentions in his comment. This is required for providing isolation for transactions ($URL$ A record cannot be deleted under you as long as you're in a transaction. When you start a transaction (depending on your isolation level) you get a "snapshot" of the current state of the database. This is implemented in MVCC by TRX_ID which is being checked against your actual transaction id. If it's higher you cannot see that because it means it was updated after you started the transaction. Deleted (and also old versions of updated) records are picked up by a purge thread in the background as of MySQL 5.6. It used to be in the master thread and now the number of threads is configurable also ($URL$ 

Innodb_buffer_pool_size the maximum you can have. If this is a dedicated box to mysql something around 16Gb should be a good start. innodb_log_file_size depends on the amount of write you have. Run the following commands in your mysql console while database is being used: 

If it is 2 (or incremented by 2) than you have the variable. If it is 1 (or incremented by 1) you don't. That should work on every version. 

Then the old master should start to replicate from the new master and catch up. A couple of things to keep in mind: 

copy current version of row update as requested append new current version to a chained list of version 

MAX(id) can have different values in case you have concurrent users. Even in a transaction you can read different values in a READ-COMMITTED and READ-UNCOMMITTED isolation level. Just write exactly what you described with limiting the update to a reasonable subset of rows. For example running this after your insert will always set the correct values. 

Partial indexes make sense when they match the query pattern. In your case if you would have queries with condition then the index would be used otherwise it's simply not useful. Cannot use the partial index: 

Do you actually need this? You should be able to drop this to a much lower level and like 8MB or 16Mb and monitor the status variable. 

You didn't write which PG version do you have. A better approach is to have LATERAL JOINs (available since 9.3): 

This will only update originalid which haven't yet been set and with the correct value. Also please note that you can have the last insert id by the function LAST_INSERT_ID function. $URL$ So this would also work (and you don't need an index on originalid to be effective) 

MySQL is using a relatively simple (simpler than other RDBMS) cost model for planning queries in which filtering your dataset has quite high priority. In your first query with the merge index it is estimated that scanning ~9000 rows is going to be necessary while the second with the index hint will require 18000. My bet would be that this weighs in the calculation enough to move the scale towards the merge. You can confirm this (or find other reasons) by turning on, run your query and evaluate the results. 

So depending the logic what you consider expired the query changes a bit. I've included both version. In comment the date and active condition as per the example. Using Lateral join: 

In the background this history is being cleaned up depending on how old your oldest transaction is. If you have long running transactions the history can grow pretty big. 

You also need to have execute otherwise you cannot "CD" into the directory nor MySQL will be able to. Run this to fix directories recursively: 

It's fine to write to the master directly. A database does much more for you than just persist your changes to disk. It manages transaction, enforces integrity, etc. If you introduce a queue you introduce a new cog in the machine that can break and a lot of complexity that you need to manage for yourself and that is actually not required in most cases. I worked with MySQL master servers having thousands of connections all the time and handling thousands of inserts/updates per second without problem. InnoDB is quite efficient if your tables are well designed and you have your configuration properly optimized. There are many ways how tables and my.cnf can be optimized for writes if that's your main concern. Some general tips: 

I usually consider things like this best to be caught as high in the stack as possible on Nginx or Apache level. Wordpess is doing it's job of serving a 404 page for a url that doesn't exist. The lower you get in the stack the harder it gets to distinguish between usual requests and this behaviour. From MySQL it's just a normal legit request from php. I have this in many of my sites Nginx config: 

You're right. The more selective column should come first unless it happens to be in a range condition. Having in the index will provide absolutely no benefits on the performance when you query for . It will however put some extra overhead on your indexes which may result in actual degradation of performance. If is always in the query with this cardinality I wouldn't bother having in your index at all. Especially if is unique. 

You can always check if a variable is dynamic or not with on $URL$ Or looking at the parameter directly: $URL$ If it is dynamic you can: 

You can always check MySQL manual if a variable is dynamic: $URL$ Unfortunately is not. So you have to restart MySQL to make this variable picked up. 

tl;dr running is sufficient Because is complete superset of . If you run you don't need to run separately. See the discussion on the mailing list archive. Analyze is an additional maintenance operation next to vacuum. It is supposed to keep the statistics up to date on the table. 

You use the inner query to enumerate the results by date. Here you probably want to apply some heuristic where condition to filter the results to optimize your query. For example if they in average play every second day then you can limit for the last one month or so. Then you can count the lost games with the given conditions per team. @update to show teams with 0 loss 

You probably want to build indexes if you want to have some lookups on them. Also a primary key may be beneficial. 

Any of these will only reports current data. Unless you have a consistent high load with many long open transaction where the purge thread is unable to keep up you will mostly have free pages. It may be beneficial to plot this over time so you have an understanding of what your server is doing. Fix Shrinking ibdata is not possible. The only thing you can do is to dump and restore your data via mysqldump (or any other equivalent tool of your choice). Other considerations Since 5.6.3 it's also possible to separate your undo tablespace from the main ibdata file ($URL$ There may be some interesting thing for you. In 5.7.5 innodb_max_undo_log_size parameter was introduced. 

That returns you the amount of MBs written to the log file in an hour which is the rule of thumb for smoothing out write IO efficiently. To my.cnf: 

For apache with htaccess you can put a shared favicon.ico (an empty image or something) and this should work (I didn't test this): 

Then having internet and TV would be 110 = 6. But this is going to be hard to search for and requires some magic to do bitwise operations to extract the features. A middle ground would be to use SET and store the features which the real estate has aka. values that are set to yes. The downside is that each time you need a new feature it requires schema change and 

Since MySQL 5.6 import tablespace is possible ($URL$ Since it's quite cumbersome I would only do that though if you need to 'merge' production schemas from multiple physical servers into a single dev database. Otherwise as Rick wrote replication, snapshotting or xtrabackup are all better options. 

Therefore it does make sense to move such "hot column" to a separate table. Especially if the row size is significantly bigger than the size of the "hot columns". Be aware though it has some implications on the select performance: 

Further down the road If you want to have better HA and/or more automated way of operation you can look at the following solution. (this list is not complete only some pointers to give you a start) Percona XtraDB cluster With XtraDB cluster you can ensure consistency and have true HA with even under 100ms failover time (depending on your failover technology). MySQL Fabric It's Oracle's in house HA solution. Requires client libraries so make sure your application can handle it properly. MySQL MMM One of the old player on the MySQL HA market but worth to mention. I hope this helps.