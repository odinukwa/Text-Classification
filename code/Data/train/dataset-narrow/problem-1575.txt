A quick way to test SQL Server connections directly is by creating udl files, if your web host is willing you could do a remotion session and test some different configurations using a udl file. 

You shouldn't be restarting servers to fix sluggishness / latency if you can help it. If you think the problem is related to the database being the bottleneck, then test for that. There are some pretty great free or not so free tools out there that can help you diagnose sql server bottlenecks. However when it comes to finding performance bottlenecks in general, there are books on the subject, and you should probably be utilizing your google-fu to find out where to start. Whatever I can write here will probably not be enough. Usually to shut down servers you plan maintenance windows. Please stop rebooting during office hours to fix performance issues. You don't know what might be going wrong behind the scenes, and you don't want to end up corrupting your payroll system because it isn't coded properly and you're half committing something. 

The reason the GPO is applied, is because the client upon refreshing its group policy's queries active directory to find the gPLink attributes of the objects in active directory that it is a member of. While it does not yet have the security group set up locally, the link does exist in Active Directory. And when the client queries Active Directory to find out which group policies it should process, it gets the relevant group policies pushed depending on its active directory membership, regardless of the local knowledge. You can find an explanation of the gPLink attribute on a relevant technet blog. Together with more information on how group policies are applied. 

You're focusing a bit much on a single metric! Diagnosing a bottleneck to a single component rarely ends with one counter giving a full explanation. There are quite a few great guides for using perfmon to diagnose performance problems on SQL Server. And unfortunately your Admin could be right, the counter you choose does indeed depend on the underlying hardware. However I can't find any documentation stating that Azure is based on a 6 disk raid. So perhaps focus on other counters? 

There are also some cleaner ways using XML filtering. But personally I haven't had a need for them yet, and content matching the message has been sufficient so far. 

You can use group policys to set a logon script that runs a powershell script to see if the program is installed. Powershell Reference However, why are you reinventing the wheel? You could just use Symantec Antivirus Management Console... 

We have several M$ CRM instances running. Each one has its own dedicated SQL Server instance. The permissions are set up in accordance with the Microsoft recommendations for setting up MS CRM. However, I can't find any documentation regarding minimal required permissions to run CRM. Currently we have the following two permissions that I would like to remove: 

The directory tab is a way to define security per directory under the virtualhost. You can find the full documentation on the apache.org site: $URL$ 

Vmware tools are actually a very important part of the VMware software packet. They contain the device drivers, and act as an API to the host for the ESXI. Upgrading them is a big part of maintaining stability and performance of your vSphere environment. A full list of features the vmware tools provide should be able to demonstrate quite quickly that they are an important part of vSphere. I don't have immediate proof, and a quick google search showed no results. But I have anecdotal experience with CPU managment on an older VM acting erratic after an update to 5.5. And updating the VMWare tools resolved the issue. 

Moving the spanned volume to the other server. This would involve detaching the volume from the current server, and attaching it on the new server. We've done this on the staging setup, however that setup did not have a spanned volume, and information on moving spanned volumes across vm's is sparse. Copying everything from the spanned volume to a new, not spanned, volume, and moving that disk to the new server. This has the advantage of not having to move the spanned volumes. However copying all the data will take a long time, and will mean a long er maintenance window. 

Mixed Environments with Windows NT Server 4.0 and Active Directory Domain Controllers Inputting a recent password does not increase the bad password count 

You can then deallocate the resources, and generalize + capture the machine. From the azure documentation for capturing Linux VM's: 

A pretty good idea of your average performance A history of performance long enough to be able to map trends A small enough repository of metrics that you don't end up with too much data 

Moving the WSUS db: This is the same as moving the files of any normal SQL Server database. With the added caveat that the connection string uses . You can either use backup and restore for this (recommended as it is safer) Or detach / attach (faster but generally more prone to errors) 

I assume this is because the actions value can not be empty. I'm not sure what I should/could safely add in the actions, and at this point it's starting to look like I'm putting together something complex for what should be quite simple. So how should I set up/create roles to allow for a user to see and utilize only a single subnet within a larger VNET? 

It looks like my error is firewall related. I'm unsure why the one command () encounters the error while the older version () does not. 

I've recently had an issue where MSDTC was unable to start up. The error in the command line when running indicated that there were configuration issues. I went into the registry and set the configuration settings to the same values as a default installation of MSDTC. Afterwards, MSDTC was able to start up, however the local dtc seems to be missing in the component services. 

So to summarize, as bad password attempts are prioritized and every bad password attempt is also retried at the PDC emulator, your account will be locked out by any properly replicating domain controller. There are however a few exceptions that might allow you more than your allotted amount of logins: 

Yet when I copy the exact value of the string, and use get-counter -counter Values of $string it works fine... Could someone advise me on a way to get get-counter to work with either an array or a string with a list of counters? 

For 4672 (Special logon events): This comes from anything requiring special privileges. Running a scheduled task with administrator privileges, an application that has run as administrator ticked on, or just logging on with an administrator account,... You could review these and see what's running with special privileges and whether or not it should be. 

Eight hours later, the next backup and snapshot kicked in, and succeeded. However, the machine itself was completely unresponsive, and the SQL Server that was installed on it was throwing I/O errors. On the machine itself the following warning occurred every 30 seconds for the next 12 hours: 

Depends on the version of SQL Server and the version of Windows Server 2012. From further down the link you yourself posted. But as far as I can tell, all versions of Windows Server 2012 support all versions of SQL Server 2016. The full list as in the link above: 

And the steps here. Which is the similar procedure as above only with also deleting the registry configuration. I've also put back a registry backup and rebooted. And rebooted after reinstalling. So far no matter what, the local dtc is hiding from me. Does anyone know what might be wrong here, and how to reinstall the local dtc? UPDATE: As I was trying to find more ways of installing the local dtc, I found out that the Powershell Commandlets are actually entirely missing. If anyone has a solution/cause for this, that might help me along. 

While I've never seen that on our hyper-v servers before, I have read of something that sounds similar. You might be looking at numa spanning happening. Your physical host does not have enough memory in his NUMA nodes to accommodate your 112GB (keeping in mind the way NUMA nodes and memory works) As such it is splitting up your 2 processors across 3 physical processors, to allow for the amount of memory you requested. While not really a bad thing (as it allows you to create a machine with the settings you want) it is bad for performance. Most notably you might see performance changes between reboots, which will have you scratching your head. 

As for when to start... After you need it is too late, so start today. If your customer cares if it performs badly, get a baseline. 

So you did nothing wrong with your NOIP settings, you just picked the wrong service for what you want to achieve. 

You're best suited using powershell. There are a few good posts out there already about how to use powershell to update AD user fields. $URL$ 

I'm currently learning how to create azure environments using powershell Resource Management cmdlets. The classic way of working had no problems, I could use and it would allow me to access my subscriptions. However, with the new Resource Manager cmdlets, when I use I get either one of two errrors. When using stored credentials using , and then logging in using I receive the following error: 

The thing to remember is that when you create a zone, you are not creating an entry in an existing database. In fact, you are creating a new database as said in the documentation: When you create a new entry, the DNS server checks if a relevant storage database exists. In your case a relevant, all be-it empty and inactive database existed. The DNS server then reactivates the database, and puts your new entry in there. Why it does this, I guess you'd have to ask Microsoft. 

When I grant the SQL Server Agent account permissions on the drive that the Proxy Account is trying to access, everything works. This despite the fact that the Proxy Account already had permissions on the relevant drive. As per the definition of proxy accounts the security context of the job step I'm running should be defined by the Proxy Account. The documentation and articles about Proxy Accounts that I was able to find so far all indicate that when running the job step that has a Proxy Account defined, the SQL Server Agent should impersonate the credential, and run it with the Proxy Accounts security context. So why is it that the Agent account also needs permissions on the relevant drive? Am I misunderstanding something about the workings of a SQL Server Agent Proxy Account? 

I've been trying to push Azure NetworkSecurityGroup rules through powershell. Using the console I seem to be able to create what I want, however using powershell I am having little success. Using the following syntax: 

It looks like something is wrong with your master database. Several of your system stored procedures appear to be missing (judging by the look of the ssms prompt) You may want to Restore your master database from backup. 

Your question has a flaw in it, which immediately makes you jump to the conclusion that your situation is something normal that bigger companies also struggle with. Running a batch job on power-on for the purpose of executing scripts remotely is a very round-a-bout way of managing your servers. Most server farms don't actually shut down their servers, as they are virtualized machines with limited resources that are shared when not in use. And even if a server had to be quickly spun up to do some larger jobs (such as resource intensive ETL jobs). Then most sensitive scripts or packages will be password protected, encrypted. Or if scripts and batches need to be triggered remotely, there are plenty of ways to do so. With the most common way being to just put in a scheduled task that runs as system. Now for your primary question, is a user without a password a security flaw? Yes, it is. You just created another hole in your security. It doesn't matter where it is. If it gets out through social engineering that your servers run with users without passwords, and through some fluke you get compromised you just opened a nice additional entry. 

Normally Symantec Endpoint blocks this by default, and a more common request is to allow for the behavior instead of deny it. At any rate, you can use the steps in this article. Simply replace Allow Traffic with Disallow Traffic. 

Open the SBS console and select Backup and Server Storage. Select the Server Storage tab. Click the Move Windows Update RepositoryData Wizard. Select the new drive location and click Move. 

then make sure you have the correct version of SSMS installed on your workstation. You can follow the steps explained in this MSDN reply about SSMS versions and SQL server Agent. Summary: 

EC2 While amazon had some issues with blacklists in the early days of EC2, it has since become a reliable platform to host your mails on. As for recommendations, you would probably want to choose an image that has the basic necessities already installed. To save yourself some complexity and extra work, choosing an image with postfix already installed can help speed you up. For Google Compute Engine Google, fearing abuse, has blocked some of the email related ports. As such you will need some more knowledge on the subject of email hosting. And will most likely be using a third party email relay. However, just like Azure, google has partnered with sendgrid. And most of the tiers contain a certain amount of free emails relayed.