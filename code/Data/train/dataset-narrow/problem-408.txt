It should not make a difference or reduce compile locks or cause less recompilations to declare a variable half way down the stack or at the top. I happen to do this at the top for readability more often than not. To get at the "what is my DBA thinking" part of the question, the only thing I can come up with (other than Nick's point that they are thinking of how something used to be) is perhaps they were talking about Parameter Sniffing (See Option 2 at this link on simple talk) About your blocking --> If you are seeing true blocking, that isn't the type of compile lock contention that your DBA is talking about most likely. While it is true that there are certain things that affect this (not schema qualifying tables, not schema qualifying your stored procedure calls, for instance) this is not the cause of your high reads certainly and likely not the cause of your blocking. You should definitely do all you can to avoid these compile locks. But I would look at tuning and optimizing the rest of the stored procedure code as a more important task than worrying about where the variables are. You can also read How to identify and resolve compile locks if you want to verify you aren't experiencing issues here. Post those before/after examples and we'll see what the DBA is driving at here. 

The problem here isn't the fact that you are using a Union, as indiri said. The problem is simply the size of the tables and the number of rows you are likely bringing back. Inside that query plan there are a couple missing index suggestion. Not all missing index suggestions are created the same and quite often they go a bit too far - but have you tried that? I would even just look to the equality columns - don't make the "mini-clustered indexes" that the optimizer is suggesting with all of the included columns. How many rows come back? Most of the rows in the table? They may not help, but less than 25% or a lot less? Maybe they will help. Are your statistics up to date? One of your tables is a heap table - no clustered index at all, is there a great clustered index key candidate there? Do you need to grab all this data for the calls to this view? I would be spending a bit more time looking at the business problem, the response time needs and look to other solutions, maybe intermediate steps, maybe better indexing, maybe trying to bring back less columns if all columns aren't needed, etc. But again the Union All isn't the real pain here from the paste the plan like indiri said in the answer I upvoted above. 

No way to do this natively through the log shipping GUI or command line with logship.exe You can roll your own "poor man's log shipping" scenario. Basically all you are doing is backing up, copying, restoring with standby or norecovery and building some alerting. An older post but an example of what I mean. 

Queries can start to slow down over time for a few reasons and you rebuilding the indexes can be fixing the problem a few ways. I'll share some of the more common reasons in my experience but there could be other causes as well. My guess is you are suffering from one of these issues.. I've also asked some questions as a comment to your question to see if we can get more details. But a few thoughts: Statistics Getting Stale SQL Server maintains column and index statistics. These essentially tell the Query Optimizer how your data is distributed. This information is critical to the optimizer in choosing the right access method for data (Seek vs Scan) and then choosing the join method being used. If you have auto update statistics enabled (default setting in SQL.. At the database level) these get recomputed, but only when "enough" data changes. So if you have some inserts into your table but never manually update statistics and the inserts/updates are not enough to trigger an auto stats update you could be suffering from poor plans for your data distribution... Rebuilding your indexes also recomputes your index statistics I would create a job to manually update statistics on a regular basis, this is a best practice anyway - and the next time this happens try and just run in your database and see if you notice a difference Query plan issues You could be suffering from parameter sniffing - basically the first time a query runs one value is passed in - the query gets optimized for that value. When you next run it with a different value that would benefit from a different query plan, it suffers with the original query plan resulting in a slow query. When things run slow for the app - are they also slow if you run the same query in SQL Server Management Studio? If it is fast in SSMS but slow in the app - that can be a good sign pointing towards parameter sniffing. If it is consistently slow across the board over time for all queries and regardless of parameters, then I wouldn't look here. This article talks quite a bit about parameter sniffing. Not enough memory/too many ad hoc plans It sounds like you are sending ad hoc SQL to SQL Server. This can bloat your plan cache sometimes, especially if you have a separate plan for each execution of a query. Depending on the memory on your server, this can also lead to the issue. How much memory is on your server? Check out this link on the problem with single use plans. You don't have a lot of great solutions in SQL Server 2005 for this problem, if you have it. If you can recreate this problem in a non-prod environment, I would suggest running in your non-prod environment if this happens again. Please note! This is an instance wide setting, if you do this on production - any stored query plans in cache for any database will no longer be there. It means you have to "pay" for compilations again. If you have high concurrency and a busy system, this could prove to cause issues. If this is the only real database and you are suffering from performance issues anyway, it doesn't hurt to try this in production.. If you have other Databases and just want to do it for this database, this blog post explains how to approach a clear for just one DB. Index Fragmentation - It is possible that index fragmentation is the actual issue here, but I'm surprised it gets so bad so quick. If your tables are clustered on a key that causes fragmentation quickly and you have a lot of inserts, this could be the case. It would be made much worse if you were underpowered in terms of memory and disk IO. Setting up a job to rebuild/reorganize your indexes on a regular basis would be good. Based on your answers to some questions in the comments above there may be other things to do to minimize the impact of this. 

(local) will work with any programs that understand to change or or into the machine's network location. I would imagine that is the issue with the debugger. But as long as the program you are using (i.e. SSMS) understands that translation, there is no behavior difference to worry about. I would imagine for most production servers, most connections will not be happening from a local machine as you would have a separate database server for security, availability and performance. So for that reason, it may be better to refer to the server as you will from other machines, but most of the time when working locally I just connect to or since I hate typing ;-) 

The Short Answer: I misread your question a little bit. The below discussion still applies and explains my answer, but I would suggest for your index. Especially because of your partitions and the fact that is included in every query. The question of fragmentation is a good one, and I imagine you'll see some in either of your two choices, but the performance benefit would be best with this way, I feel. The Previous Discussion: This is one you'll likely get a few answers on and I've asked a couple questions in the comments. That said a few quick thoughts to get started (ignoring the multiple system question and the way the IDs are being used): 1.) Presuming you are talking about SQL Server partitioning, a partition aligned clustered index is generally best. So in my mind that is a good vote for . 2.) You indicate that filters will always (is that really always? or figuratively always?) have . In my mind that is another good vote for . 3.) Because you'll be having IDs coming in at both a high range and a low range, you will experience some level of index fragmentation on ID. Assuming is always increasing and you aren't entering data from dates all over the place, you may actually experience less fragmentation going with . You also didn't indicate if this is a fact table or dimension. Assuming it is a fact table, that is another good argument for date in my book. If you are always querying filtering on a date or date range, then the clustered index being on that range scan value should help your queries. The fact that it would be partition aligned would also likely help. If this is a dimension and you will sometimes join on ID to pull data out of it, then there could be a stronger argument for going with , though I'd argue that the unaligned nature of that index to your partition could cause you more grief in the future performance wise. I would not bother adding to the index. Unless you include it enough as a filtering predicate and could actually see an improvement in performance of most queries. Yes, the date column alone wouldn't be unique, but if this table is a , the 4 byte uniqueifier SQL Server adds to the clustered index to make it unique would actually cost less in terms of storage space than that column. These are just my thoughts and they are quick thoughts. This is one of those things you should really test, try a couple ways and see how it works out. Though partitioning sometimes forces your hand a bit. So I would probably end up with the date column only as my first choice and date plus ID as my second choice. Others will likely have alternative ideas on that one, too. And more information edited back into your question will help.