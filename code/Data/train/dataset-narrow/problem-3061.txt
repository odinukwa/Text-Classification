Your approach is a good one. This way to extract features can lead to good results. But before moving forward I recommend answering these two questions: Is there is any correlation between the behavior of the user today and yesterday ... Etc " temporal correlation/ temporal dependency" Is there is any kind of dependency between the users? Answering these questions can measure the quality of your feature ? Sometimes there is no need even to do calculations to answer these questions , if you tell us what kind of actions or activities you are looking at we may help in better way . For example : if user 1 used his car today , there still a high prob to use it tomorrow, but if user 1 went to barbershop today there is a high probability that will not go tomorrow 

I beleive that the quality of my feature is not very good. I think the nonlinear case experinces a kind of overfitting. 

I think the best way to deal with this problem is to use blending (although it is not a bad idea to start with random forest just to get sense about the problem you have). If you are not familiar with the word blending you can check this page: $URL$ But let me explain what is the idea: I believe that there is a kind of relation between the features you have: you can group all the features related to Weather together, The day, time ..etc together. Each set of features can be used to train a random forest. The output of these random forests can be combined by either linear or non linear regression blending techniques. If the database you have is very large it is better to read more about blending techniques and do some experiments before going to your problem to make yourself familiar with it 

In general, people think about overfitting as a function of the model complexity. Which is great, because model complexity is one of the things that you can control. In reality, there are many other factors that are related to the overfitting problem: - number of training samples - number of iterations - the dimension of the input ( in your case , I believe this is the reason why you are not overfitting) - the difficulty of the problem:if you have simple problem, linearly separable, then you do not to worry much about overfitting. There is a visual demo provided by google tensorflow that allows you to change all these parameters. $URL$ You can change you input problem, the number of samples, the dimension of your input, the network, the number of iterations. I like to think about overfitting as Overfitting = large models + unrelated features 

This question is very common in automation, when machine learning used to perform specific tasks. Guaranteeing the quality is always a must. Evaluating the model while it is in production is not an easy task. the reason, why? In order to evaluate the model in production you need to have the ground truth. This ground truth is not available (if it is available no need to have the model). Getting the ground truth (by using human for example) is not a good solution: 1- it is very expensive, 2- again if you will generate the ground truth for data in production then no need to have a model. BUT how to deal with this problem in reality? I have worked recently on a prediction model that is used to predict the vehicles (Makes, Models), since every year we may have new models, makes, it is a good question to ask how often do I have to repeat the training process? Three different ways I used to answer this question: 1. I analyzed the changing on the training data I have year by year. based on this variation I can estimate the number of new makes and models appear every year and the number of makes and models disappear every year, and thus i can estimate the expected degradation in performance. 2. I did several experiments using the data from 1990-2014 to predict 2015. Using Data from 1991-2015 to predict 2016. This helps me a lot in understanding how much my model is invariant from year to year. 3. Instead of scanning all the data from production, you can randomly sample. The used distribution can be adaptive, such that the number of sampled records increases gradually by time. The reason why the distribution is adaptive, because we are expecting that the model performance will deviate from the the expected performance with time. 

SMOTE is an algorithm used to generate " synthesize" new samples from the real samples. It selects randomly one of the k-nearest neighbors, find the distance between these two pints , synthesize new point by modifying the sample considering the distance and a random number between 0 and 1. SMOTE algorithm does not use samples from majority class only samples from minority. It synthesizes new samples It expects a high density minor class with small variation within the class Anything else is related to the implementation, many implementation return only the synthesized samples 

To understand what overfitting means and how it affects the accuracy of the model, you need to understand the bias - variance tradeoff. Under-fitting as well as overfitting are two different problems that are directly related to biased- variance problems. It is always important to understand the relation between three different factors and how these factors are connected to bias-variance ( overfitting- under-fitting) problems: 1- the size of the model. Number of parameters 2- the amount of data available for training. Number of training samples. 3- the number of iterations. training iterations. Making a direct connection between any of these factors to overfitting- under-fitting problems without looking at the others will always lead to wrong conclusions. Because of understanding these factors and linking theme by using mathematical equations to avoid overfitting and under-fitting problems is a difficult task, more over it is a task dependent, people use simple methods to discover and avoid overfitting. The easy way is to divide the data into three different parts, training, validation and testing. Testing should not be touched. Use training set to train the network and validation set to test the network after each iteration or a number of iterations. Theoretically, you will see that the error on the validation set decreases gradually for the first N iterations and then will be stable for very few iterations and then starts increasing. When the error starts increasing, your network starts overfitting the training data and the training process should be stopped. Note: the value N is very related to the three factors I listed above. It is always a good practice to have a demo training set and test with different models, training data. You will see that the larger the model - the less training data the smaller the N. The smaller the model - the more training data the larger the N. Note: be careful when using small models of having under-fitting problem. 

There are many algorithms that can be used to perform classifications ( many to the point that it is difficult to mention all of them ) I suggest you to have a look at this $URL$ Making the decision which algorithm to use is a function of the problem you are working with, mainly: 1. The number of classes 2. The number of samples 3. The variations within classes and similarities between classes 4. Data imbalance 5. The dimension of your feature And many other parameters In general, CNN is very popular for two reasons: They can lead to high performance in very challenging problems and they are general solutions in the context that you need to understand their architectures the strategies and tricks to perform training only, after that you do not need to change anything , no parameters to play with . 

I use c++ to perform training / testing and develop machine learning tasks. Then to interface with c# you need to use managed code to build you dll ( c++ cli/ clr) then you can call this dll from c# I am not sure if it is possible to call R from c# But if it can be called from c++ then you can go to c# 

SMOTE is not designed to work with severe data imbalance specially if you have wide variation within the minority class Try borderline SMOTE Or SMoteBoosting 

It is valid to use the k-means to initialize the EM for Mixture of Gaussian modeling. As you said, the mean of each component will be the average of all samples belong to the same cluster (it depends on the used clustering algorithm, some times the centroid is not the average of the cluster but is one of the samples). for the weight you can use the following: the weight of cluster x = the number of samples belong to cluster x divided by the total number of samples. thus, the cluster with the highest number of samples is the cluster with the highest weight. for the variance: just find the variance of all samples belong to the same cluster. 

I am working on a binary classification problem. My data contains 100K samples from two different sources. When I perform the training and testing on data from the first source I can achieve classification accuracy up to 98% and when perform training and testing on the data from the second source, I can achieve up to 99%. The problem is when mix both of them, the classification accuracy goes down to 89%. Any idea how to perform the training to achieve high accuracy. Knowing that one of my features is related to the source 

based on your description, it looks like different models have different biases. two important questions: do you have any data imbalance problem? what kind of models you are using? using stacking based classifier is beneficial if you have different biases. Try to use a simple stack based classifier. for your level-1 classifier, use different models (e.g. SVM-L, SVM-NL, DT, RF, ... etc). For your meta-data, use probabilities and for the meta-classifier use Random Forest. if you have data imbalance problem using stack based classifier is a little bit more challenging. 

Do you have data imbalance? The answer is yes. Is it a problem? It depends. Your data imbalance ratio is quite high (1:10), is it a problem or not, it depends on three things: - what is the used classifier? - are these separable classes or not ? - are you interested in overall accuracy or average accuracy ? The impact is to have bias classifier! If you have a classifier that predicts all the samples as class one , you will have overall accuracy of ~ 70%. But it is a biased classifier the results in average accuracy of ~ 33% 

First of all, you can do this in automatic way by setting the layers to be trainable or not. By selecting this hyper parameter to be false you freez the layer. we know that the first few layers are features extraction and the assumption when you perform fine tuning that the original problem and your problem are correlated, thus the features should be also the same. What is the right thing to do? It is very difficult to find a direct and absolute answer, it depends mainly on the original problem and the new one. In fact, I prefer to fine tune the whole network using different learning rates. You can use smaller learning rate for the convolution layers and larger rate with the fully connected layers. 

Based on the image you are sharing, the training accuracy continues to increase, the validation accuracy is changing around the 50%. I think either you do not have enough data to use neural network or the network is small to capture all the information, in both cases I feel there either under fitting or over fitting problem. Can you give us some information about the database so we can help more 

my question is how to explain this behaviour?!! does what I am thinking in make sense?!! I am thinking in using Adaboost to perform the training, is it a good idea or not? 

What you just said is dividing the data into three parts: training, validating and testing. This is a very common practice in machine learning. We use validation to help in selecting hyper parameters. Going with this option vs. Leave one out vs. cross validation depends mainly on how many number of samples you have. If you have a lot of samples, then going with the option of splitting the data into three parts can be more efficient. I am not sure about your statement of unbiased. You need to be careful when you make judgment 

smote algorithm depends on the data set you have. If you have severe data imbalance, like the one you have in your case smote algorithm may not be able to help if the variations within the minority class is very high and the similarities between the two classes is very high. But How to know if this is the case. Try to duplicate samples from the minority class train a non linear svm and check the results if the classification accuracy is very low then this the case. Smote use knn to create new samples but if the variation within the minority class is very high then using smote will use samples that are not real neighbors even. To be honest with you , there is no clear solution for this problem but i can suggest the followings: 1. Try borderline smote : it is a modified version of smote algorithm 2. Try smote boosting : it is a modified version of adaboost where adaboost algorithm is augmented with smote 3. If you can modify the smote boost to consider borderline smote instead of smote 

The theory behind cross validation ( v-fold cross validation) has been addressed in many papers. There is a proof for that in a set papers published from 2003-2007. Please refer to : - oracle selector. 2006 - super learner 2007 - super learner in prediction 2010 - unified cross validation 2003 

Data imbalance problem ?? In theory, it is only about numbers. Even if the difference is 1 sample it is data imbalance In practical, saying this is a data imbalance problem is controlled by three things: 1. The number and distribution of Samples you have 2. The variation within the same class 3. The similarities between different classes The last two points change how we consider our problem. To explain this let me give an example: Class A = 100 samples Class B = 10 000 If the variation within class B is very low Then down sampling will be enough , there is no data imbalance problem If the variation is very high within class b , then down sampling may lead to information loss And it is dangerous to apply down sampling Another point, having a lot of samples ( mainly for the minority class) will relax the data imbalance problem and make it easier to deal with E.g. 10 : 100. Vs. 1000 : 10 000 

from my own experience: In one case, I have worked with real database that is very small (300 images) with many classes, severe data imbalance problem and I ended up with using 9 features: SIFT, HOG, Shape context, SSIM, GM and 4 DNN-based features. In another case, I worked with very large database (> 1 M images) and ended up with using only HOG feature. I think there is no direct relation between the number of instances and the number of features required to achieve high accuracy. BUT: the number of classes, the similarity between classes and variation within the same class (these three parameters) may affect the number of features. when having larger database with many classes and large similarity between classes and large variation within the same class you need more features to achieve high accuracy. REMEMBER: the quality of used features is more important than the number of used features.