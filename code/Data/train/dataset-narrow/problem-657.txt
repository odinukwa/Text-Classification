Visual Studio 2010 is no longer supported for SQL Server Business Intelligence tools. You will not find a download link unless Google or some other site might have cached it somewhere. You will need to download Visual Studio 2012 SSDT-BI or 2013, these support BI projects for SQL Server 2012 and 2014 respectively. 

You will need to adjust your script to call out the property returned by the AD command. So the following adjustment should work: 

The recovery phases of a database are logged to the log(s) for SQL Server. However since by default there are 6 logs for a given instance you will have to iterate through each one to find the information you are looking for, or more based on the configuration. A restart will cause a new log to be generated so if multiple restarts occur (which I have had this occur) you may get by only checking the last few logs instead of all of them. This may also depend on how often you are running this check. You can query to determine when a service last started, or check the created date of . A restore can be checked via . An attach would be via the logs, don't think this is logged in the default trace but it might. 

If you want full copy for your local instance, just get the latest full backup. If you need to make a backup, make sure you use to not affect recovery for production. 

To expand the reason you can't generally use the PowerShell step with another module is the SQL Server provider. It does not play nice and there is a connect item out to get MS to fix this. If you add to the your job step you will see the full output of why the command is erring: 

There is really no other way than to use a case statement if you want the plain English version of all the integer values used by SQL Server (at least to my limited knowledge of T-SQL). This is a common thing needed by a lot of DBAs and there are plenty of scripts out there on the subject. I found the script below from Michelle Ufford to be quite useful in getting a good view of the schedule of jobs running. Since maintenance plans end up as SQL Agent jobs this will show you that as well. The output you get from the script will give you the frequency, subFrequency, scheduled time and next run time and date. I just took her script and put it as a stored procedure on my server so I can easily run it when needed. You could also put it as a view if you wanted too. Since other folks can come behind me and add jobs and such I like to check it every-now-and-then to see what is going on. 

You pretty much just work out writing this as a script for each server, then pull in that bit of the summary log (or just the whole log) to a central spot so you can go back and determine the status. Option 2 would be to just include a few lines of code to connect to each instance and verify the build number. 

If you want something that will be useful for the future I would probably steer clear of trying to search the registry. The hives for SQL Server have changed a bit over the years and it can be troublesome to keep up with. The method with the is flaky at times and although I will use it, not concrete evidence that instances are on the network. I believe it depends on SQL Browser Service as well, which most of the time I find disabled. I will utilize the WMI class . I use this because it offers up more information about the service than cmdlet does. I write everything as functions generally because you can use this to actually just do daily check or verification of the service for troubleshooting. 

As it states in the message you have to restore the log backups up to the point in time of the filegroup backup. In order to bring the database online it needs to play the log back to the same point in time in order for the database to be consistent. However restoring just the primary file group is a special situation. I believe in order to restore the primary filegroup you pretty much have to do a full restore, not just a filegroup. (I'm not positive on that but believe the below text points to that requirement.) Understanding How Restore and Recovery of Backups Work in SQL Server 

The import/export wizard is not showing because it is not available for Azure SQL. Even if you downloaded the latest version of SSMS, you shouldn't see it. You are in a different environment that how on-premise SQL Server databases work. In that Azure SQL does not support using cross database queries. So doing an are not going to work. Depending on how big the table is, your easiest thing to do is using the script object wizard and just include the data. Then just run that under the destination database. If you need to do this on a routine basis then look at the data sync services Azure offers. 

EDIT As pointed out you can utilize the Attention event in profiler with the events for capturing the T-SQL statements. It does not necessarily specifically state what the attention event is when I tested it so I guess the fact that it follows the event sequence you can estimate those queries that have an issue. I did not get a chance to full test it out with code and all. I did however come across an exact example with Extended Events that can be used to find timeout queries, and this example is with SQL Server 2008. It is from Jonathan Kehayias: An XEvent a Day (9 of 31) – Targets Week – pair_matching 

In most cases where you hit this issue you will just need to follow the manual uninstall steps Microsoft provides here. That will, in cases I have had to, get the system to a point where you can successful reinstall SQL Server to a working state. 

You will most commonly see this if you do not create a user in the database for every person that can modify data. A better option is to change the trigger to capture so you obtain the login that made the change, not just the user. I have seen cases where the user was mapped to a different login name so your auditing records can be flawed if that is not managed properly. 

It is fairly straight forward in using the UI via SSMS to create them but for purposes of this post, this would be the T-SQL to do the same thing: 

Without actually backing up the tail log of a database (don't have an test instance to try this on) you could logically conclude that the value returned in the column mentioned would be the first LSN of the next log backup, in your case the tail. So executing the following will return the value I believe you are looking for: 

While I do not work directly with PostgreSQL, your SQL code would have to handle outputting the records changed. PowerShell or the method does not usually contain that value on most DBMS drivers. You can check this answer on SO to see how it is collected via SQL code. 

Yes. You would need to write your own custom web app to interact with each SSRS instance. You would do all of this using the .NET library around SSRS, it includes methods and such for interacting with reports in SSRS. A small/basic example: $URL$ 

I'm guessing you are referring to a file system level backup of your data folder? I am not familar with Rackspace for performing such a task, but have worked with other backup products that allow you to take snapshots of active or inuse files. Which it would allow you to grab a picture of the data and log files of a SQL Server database. The problem with that is when you take a backup of those files in this manner you want to make sure the database is in a consistent state so it can be recovered, or restored. When you take a snapshot of those files while a transaction is being written to the log file, or committed to the data file, it will not necessarily have a consistent state. When SQL Server attaches or starts a database it will read in the log file and either roll forward or back any active transactions that have not been committed to the database. If it hits an inconsistent transaction it will mark the database as suspect and you have to clean it up manually, and hope you can get it online. Another problem that will come up with not using SQL Server native backups can depend on your recovery model. The recovery model basically dictates how your log file is managed for reuse of space. If you are in full recovery model a native full backup helps keep the log under control, taking filesystem level backups will bypass that process so you could end up with a log file out of control. NOTE: When you determine your backup strategy for your database, you must be confident that you can use it to restore your data. No matter what suggestions you get, always try your backup plan and do a test run on your restore. If you can restore with that backup plan and it meets your recovery needs, then go with it. 

A bit of a hack method would be to run your script as a CmdExec step, then run sqlcmd and set your connection timeout. (Untested but I would think would work if no other option is presented.) 

I tend to favor Google or Bing as my script repository. A few folks also have dedicated scripts that are used for mutliple things when it comes to DBA tasks. One is Adam Machanic and his sp_whoisactive. Then there is Ola Hallengren and his maitenance script. Both of these are probably the most used tools a SQL Server DBA can have available. They are both kept up-to-date with each release of SQL Server as well. 

Getting specific to SQL Server would require there be any cmdlets within SQLPS that actually use the ByValue or ByPropertyName binding. I only know of a few like those for the Backup or Restore, but don't really use them. I think the ones around Azure SQL might use them as well (e.g. ). So in the end this is really nothing to do with SQL Server itself, just the way PowerShell works...but still worth learning about I think. The best way for you to see how they work is just use to look at the binding metadata as PowerShell does it. A simple example would be piping something to a cmdlet like or . I am by no means an internal's guy when it comes to PowerShell. Up front from what I can find, is not actually used or maybe just has a lower precedence than . If you look at use of this command on my local laptop: 

If you are using sequential processing you will never know when one particular server is going to have a load from the index maintenance or stats maintenance. As well, it could also vary each time it runs based on other processes running on a given instance. Something to consider is if your maintenance window is large enough to handle that fluctuation. 

If you want to see pain points of a query capture the actual execution plan via SSMS when you run it. Viewing this within SQLSentry Plan Explorer (free!!) is a bit more easy on the eyes than what SSMS will show. There are some good blog post and articles scattered around the Internet on how to read an execution plan if you need it. Capture Actual Execution Plan: 

You cannot use any backup statement to get a backup since the database is still restoring. At most the only option would be to use mode but this would only provide you read access to all the data. You can get it to by using the following command: 

I tend to use strictly because on active instances trying to set a database to and be the single user that connects is not always consistent. If you set it offline that is it, it is offline. I will also add that a better practice to do the restore is to have the application stopped or block access from the server. This will save from your error log filling up and prevent connections from occurring once the restore is complete. 

This will open up SSMS as the system account that has sysadmin access to the instance of SQL Server. This is done during installation of SQL Server, however I have heard that this will not be so with SQL 2012. 

Based on that I would run this query against sys.master_files to see what the physical locations are supposed to be for the data and log file: 

Whether you run that code while you are on the primary or secondary is irrelevant. When your path is you are technically running it against that server, it does not matter if you ran the command from the server or a random desktop in the domain. The issue with it showing as secondary though is local. I do not get that on my servers. If, you had PowerShell and the provider opened on "Server1" when you failed it over then I would suggest opening a new PowerShell prompt and see if it shows the same thing. If it does then try from another computer all together. 

The SSMS November 2015 preview works against the SQL Server 2016 CTP versions. I actually use this version of SSMS (November 2015 preview) against my local 2012 and 2008 R2 instances and don't have any problems. 

There is no tombstone in Availability Groups. However, one thing you need to consider is the mode your AG is in for the replica that is being taken down for 4 days, and how much log is going to need to be kept for that period. An AG syncs up other replicas by sending the data written to the log of the database, if you are going to stop a replica for 4 days that could be ALOT of log space needed. You are basically suspending data movement to that replica by taking it offline (or shutting it down). That is acceptable for a few hours on moderate loads, but when you get into days you need to remove that replica from the AG altogether. This is going to prevent the need to keep all the log data in the database log files for any database in that AG. 

When it comes to PowerShell and SQL Server there is always different ways of doing or handling this scenario. I ended up just splitting them up as suggested by Aaron in the comments. I don't want to delete this question so I am just answering it in this manner in case someone wants to use the scripts provided. If a moderator feels that the question is better of removed, by all means go right ahead. 

Whichever transaction requires additional log space will grow the log file within that transaction, so it can complete. So if is performing a bulk load of data and uses up all available space in the log file, you will see a growth event for the log being called by . 

Based on the BOL article you referenced to, the date is not going to change unless a Service Pack or CU creates new system objects. However the date returned may not necessarily coincide with the date you install that CU or SP. 

I ended up changing around how I am going to use SSDT with this database project. In the end I will be fixing all the issues with case sensitivity on the stored procedures. The caveat to my project is I do not know all the objects that are actually being used in the application, so I only care about those objects actually being used. I am simply starting with an empty database project and adding in the objects the developers are going to be using in the application as they redesign it. I should then have a database project with "clean" objects in order to do a schema compare. 

There are plenty of questions on DBA.SE regarding and how to resolve problems when errors are returned. My specific question is on actually getting notified that returned errors. Most all DBAs know that you can automate the command and should run it often. I came across this article by Cindy Gross, which has some very good notes. In it she mentions use of SQL Server Agent that if it finds errors from the execution of the command it will fail that step (or job depending on configuration). She points to Paul Randal's blog post on the topic here. Now I am curious if anyone knows that the Check Database Integrity Task in a maintenance plan would do the same thing? MSDN does not mention that it will and I have not truthfully been an environment where it has come across a corruption issue; so can't say that it does. This would be versus simply setting up a SQL Agent Job with multiple steps that runs the specific command against each database, as Cindy suggested. Thoughts? Obviously proof is in the pudding so providing more than just a guess would be helpful...