No, there is not. Geometry shaders, like most shader stages, are separate from one another. What you seem to want, since you specified DX11, is tessellation. That allows you to take a "patch" and subdivide it. A "patch" could be a string of points of some arbitrary length, which you would "subdivide" into multiple subdivisions. 

Only SDL version 1.3 provides support for creating an OpenGL 3.3 core context. So you should upgrade. And if you're having a problem upgrading, that's something you need to get worked out. However, if you want to use a 3.3 compatibility context, all you need is recent drivers. Well, that and 3.3-capable hardware (anything advertised as DX10. Except from Intel). 

Resist the urge to rant on Sirlin... resist the urge to rant on Sirlin... OK, with that out of the way, yes, you would be perfectly capable of developing a competitive fighting game. As long as you pay attention to what you're trying to make, rather than whether you personally like it. Most competitive fighting games were never designed to be competitive games. Or at least, they are descended from games that were never designed that way. Take the Street Fighter combo system; that is derived entirely from a game bug, yet it is the cornerstone of competitive SF play. Pop Quiz, hot shot: as a game designer, you are presented with your players (or if the game is still in development, testers) having found a game bug which they have exploited. This bug, when properly exploited, allows them to deal far more damage than your game design normally allowed. Which means that a move that you could punish with one hit (maybe 10% damage at most) can be punished by 30 or 40% damage instead. What do you do? Do you remove the bug, thus eliminating the problem entirely and allowing your original vision of the game's design to show through? Or do you instead allow the bug to persist and effectively balance the game around the bug? The real essence of this Pop Quiz isn't the answer; it's your justification behind your answer. If you want to remove the bug, why do you want to remove it? Is allowing more punishment really that bad for the game? What is it exactly about the bug that you find objectionable? If you want to keep the bug around, why do you want to keep it? Does it detract from the challenge, because exploiting it is too easy? Does it make the game one-dimensional? Will it always be one-dimensional, or is this simply a temporary metagame moment that will get smoothed out as people learn to deal with it? 

The true source of this information is in the various OpenGL specifications. They contain a list of what functions there are, what their definitions are, what enumerators those functions take, and so forth. For each OpenGL version. If you want what was removed (deprecated means "present but not advised to use". When something is removed, it is removed), the 3.1 spec section E.2.2 has a list of the removed functionality. However, if you simply want to avoid non-core functions, you should use an OpenGL loading library that has headers that exclude removed functions. GL3w can work in this capacity, but it doesn't load extensions. The only one I know of that does this is my own (which is why I wrote it; to get rid of the non-core cruft). 

Let's assume there was; I have no idea one way or the other myself. But if there was, it would map the buttons differently. So installing a browser plugin would also install this driver, which would override the current driver and screw up the button settings in every program that uses the controller. I don't see a lot of people appreciating you doing that to their systems. It would be better for your plugin to simply detect what OS its running on and adjust the inputs accordingly. You should only install drivers if the device would not work at all without them. 

I actually fill the texture with data (rather than passing NULL to glTexImage2D) as a debugging aid. It ensures that everything was working prior to starting to use the texture as a render target. Also, notice that I provide a BASE_LEVEL and MAX_LEVEL. I always do that with my textures immediately after creation. It's just a good habit, as OpenGL can be picky sometimes about texture completeness and the mipmap pyramid. Rather than remembering the rules, I just set them to the correct values religiously. Here's the main drawing function: 

So you need to assign the locations appropriately. Given the names of the textures you used, you should use the following: 

Can it? Yes. OpenGL defines functionality, not performance. You can indeed make things much slower. Or you can make things faster. It all depends on how you use it. The OpenGL Wiki has a good article on how to properly stream data. 

GPUs are very good a parallel tasks. Which is great... if you're running a parallel tasks. Games are about the least parallelizable kind of application. Think about the main game loop. The AI (let's assume the player is handled as a special-case of the AI) needs to respond to collisions detected by the physics. Therefore, it must run afterwards. Or at the very least, the physics needs to call AI routines within the boundary of the physics system (which is generally not a good idea for many reasons). Graphics can't run until physics has run, because physics is what updates the position of objects. Of course, AI needs to run before rendering as well, since AI can spawn new objects. Sounds need to run after AI and player controls In general, games can thread themselves in very few ways. Graphics can be spun off in a thread; the game loop can shove a bunch of data at the graphics thread and say: render this. It can do some basic interpolation, so that the main game loop doesn't have to be in sync with the graphics. Sound is another thread; the game loop says "play this", and it is played. After that, it all starts to get painful. If you have complex pathing algorithms (such as for RTS's), you can thread those. It may take a few frames for the algorithms to complete, but they'll be concurrent at least. Beyond that, it's pretty hard. So you're looking at 4 threads: game, graphics, sound, and possibly long-term AI processing. That's not much. And that's not nearly enough for GPUs, which can have literally hundreds of threads in flight at once. That's what gives GPUs their performance: being able to utilize all of those threads at once. And games simply can't do that. Now, perhaps you might be able to go "wide" for some operations. AIs, for instance, are usually independent of one another. So you could process several dozen AIs at once. Right up until you actually need to make them dependent on each other. Then you're in trouble. Physics objects are similarly independent... unless there's a constraint between them and/or they collide with something. Then they become very dependent. Plus, there's the fact that the GPU simply doesn't have access to user input, which as I understand is kind of important to games. So that would have to be provided. It also doesn't have direct file access or any real method of talking to the OS; so again, there would have to be some kind of way to provide this. Oh, and all that sound processing? GPUs don't emit sounds. So those have to go back to the CPU and then out to the sound chip. Oh, and coding for GPUs is terrible. It's hard to get right, and what is "right" for one GPU architecture can be very, very wrong for another. And that's not even just switching from AMD to NVIDIA; that could be switching from a GeForce 250 to a GeForce 450. That's a change in the basic architecture. And it could easily make your code not run well. C++ and even C aren't allowed; the best you get is OpenCL, which is sort of like C but without some of the niceties. Like recursion. That's right: no recursion on GPUs. Debugging? Oh I hope you don't like your IDE's debugging features, because those certainly won't be available. Even if you're using GDB, kiss that goodbye. You'll have to resort to debugging... wait, there's no on GPUs. So you'll have to write to memory locations and have your CPU stub program read them back. That's right: manual debugging. Good luck with that. Also, those helpful libraries you use in C/C++? Or perhaps you're more of a .NET guy, using XNA and so forth. Or whatever. It doesn't matter, since you can't use any of them on the GPU. You must code everything from scratch. And if you have an already existing codebase, tough: time to rewrite all of that code. So yeah. It's horrible to actually do for any complex kind of game. And it wouldn't even work, because games just aren't parallel enough for it to help. 

Why do you call this? OK, I know why you're calling it. You've somehow taken a tutorial that's supposed to be showing you how to do modern, shader-based OpenGL, and are adapting it to fixed-function stuff. If you are, then you need to adapt it properly. is for setting up generic vertex attributes. You cannot use generic vertex attributes with fixed-function rendering. If you're going to do things in fixed-function, you need to actually do them in fixed function: 

Get a greyscale image of a card. Make it fuzzy around the edges, possibly expanding its size. This is your shadow card image. Note that this is a single-channel image. When you access this texture in your shader, you should make sure that you put the single value in the alpha component. This can be done in your shader or somewhere else. A value of 0.0 in the image means that there is no shadow. A value of 1.0 in the image means total shadow. That is, completely pitch black. You probably want a value of 0.5 or so as your darkest color. Clear the screen such that the alpha is set to zero. Before rendering anything (or at least anything that will be rendered "under" the table), for each card, render the fuzzy texture, offset from the actual position of the card (but with the same orientation). The color output by the shader should be The blend configuration for this should be (in OpenGL parlance): 

There are literally hundreds of libraries out there that handle all kinds of aspects of game development. From loading images, to opening and managing windows, to rendering, to sound, to physics, etc. What there are not are very many "one-stop-shop" solutions, where you get everything all at once. That's one of the benefits of XNA. You generally have to pull together many libraries to build an engine. This gives you great flexibility, as if one component is lacking, you can (relatively) easily swap it out for another. But at the same time, it requires more work. Really though, why are you so hung up on what programming language you want to use? It seems to me that you really like XNA and want to use it for game development. So go do that. 

You should have both versions, but not in the same place. A function that takes a simple object like a vector generally should not modify it. Why? Because then you couldn't do this: 

No it doesn't. It means that you can only use 16-bit indices (assuming it even means that, as I'm fairly sure D3D9 allows 32-bit indices. That's what the INDEX32 index buffer format means, right?). Which means that a single draw call can only access a 16-bit range of indices. But you are not limited to a single draw call for the entire scene. That's one reason why has the BaseVertexIndex parameter, which allows you to bias the index for a rendering command by a fixed offset. You don't have to draw everything in one draw call. So even if you were limited to 16-bit indicies (which again, you are not), you could still make several draw calls to render what you needed. 

Well, now we have Direct3D 12, Apple's Metal, and Vulkan on the way. So it seems that some form of middle ground was discovered. And with the more low-level memory capabilities of D3D12/Vulkan, Carmack will apparently get his wish. 

In general, the way this works is very simple. Your characters have a list of bones, which are animated by the animation system. You renderer takes these animated bones and uses them to draw the mesh via skinning or some similar technique. So you simply add an extra bone to the animation system. This bone is not weighted to any vertices; it's there solely to position objects your character carries. Typically, this bone is either a child of the animation root or a child of the hand. When you execute your animation system and get the bone-to-object space matrices for each bone, you get one for the "extra" bone as well. Just use that as the transform for the object you want the character to "carry". If the animators and modelers did their jobs (animation must position and orient the bone correctly, modelers must build the mesh to fit the hands and center the object on the place where the character will hold it), then the object will appear in the character's hand. 

The most important thing you can understand is this: you cannot satisfy both of these at once. You must pick one: smaller GPU memory, or smaller disk space/load time. So let's look at both cases. Case 1 Your possibilities here are the various "Block Compressed" types, using the D3D 10 language for them. I'll assume you know what BC1-3 are, since they are formerly known as DXT1, DXT3, and DXT5, respectively. BC4 and BC5 are for 1 and 2-channel formats respectively. These two can be either unsigned or signed normalized. BC5, 2-channel compressed, can be used for storing tangent-space normal maps, with the third coordinate (Z) reconstructed in your shader. It does a reasonably good job. BC6H and BC7 are quite new. BC6H is a compressed, floating-point format. BC7 is a more accurate way to compressed RGBA color data. You can only get BC6H and BC7 support from DX11-class hardware. The OpenGL internal formats for these are as follows: 

Vertex attrib binding brings with it entirely new ideas, the specific performance of which are not entirely clear. However, since vertex attribute binding is taken almost verbatum from Direct3D's concept of vertex formats and streams, it is useful to look to that API for guidance. There, a vertex format is an immutable object. You create a format, and that's the end of it; it cannot be changed or modified. This suggests that there is a significant cost associated with changing vertex formats. Indeed, when NVIDIA was coming up with bindless vertex transfer, they took the opportunity to do something very similar: separate the format from the buffer that provides it. Therefore, a reasonable suggestion is to follow D3D conventions: set the format once for a VAO and don't touch it. Just change the buffer binding with (the equivalent functionality in D3D is mutable).