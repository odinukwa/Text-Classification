Denormalization to the 'Vehicle' table is still option, technically, but based on the responses in the comments I would probably sub-class it as well. So, you're at the options I mentioned in my last comment. There's no magic bullet. It's string concatenation for dynamic SQL or separate statements. One way or another you're going to have to change your app code. 

That's a big 'it depends.' Depending on how your statistics have been maintained and the options you specify you could end up running full table/index scans and thrashing your I/O and buffer pool. Depending on the characteristics of your hardware and databases that could be very bad. Also, rebuilding statistics invalidates execution plans, which means you could see a CPU spike and slower performance while SQL Server re-compiles queries. Best practices dictate updating statistics during off-peak hours to minimize impact. Otherwise, take due precautions to minimize load on the system such as rebuilding statistics on only the tables that require it over a period of time. Check books online for more information: $URL$ $URL$ 

Either way could be correct. Choose 1 table if you want to make it difficult to change the number of reviews possible. It may be counter-intuitive to want to make something difficult, but if it would actually be a bad idea to make it 1:n, then building in some pain to make the change has some logic. But, you almost certainly want to go with two tables for long term flexibility. If it was me, I'd put a trigger on there to enforce the 'one review per methodology statement' rule. If that changes, just modify or drop the trigger. One question to keep in mind, though: if you go to multiple reviews, how would you resolve status codes that conflict? For instance, what if you had one code 4 and one code 5 for the same methodology statement? UPDATE: ...or you can do the much better thing and use a unique constraint as @Max Vernon suggests. 

I think source1 has the answer. He (Aaron) states that even when you get an "accurate" count, you could be blocking users trying to write to the table. As soon as you put your eyes on the result, it could be immediately inaccurate as writers were trying to put data in your table while you blocked them for the accurate count. So, I believe using partition_stats works fine for what you want. I wouldn't be afraid to test it though and it might be an interesting test at that. You could create a db of 1000 empty tables. Then a routine that constantly inserts and deletes data in random tables. Then run your partition query and run your count query and compare. I think what might work best is to use partitions and accumulate your data in a table and over periods of days or weeks, any table with consistently zero results is unused or abandoned. 

The stored procedure sp_addsubscription allows for you to specify that you want all articles (i.e. tables) or just one. If you want a subset of articles in a publication, you just need to run the sp_addsubscription once for each article that you want. It looks like the sp_addsubscription defaults to @article = 'all' My suggestion is to go through the New Subscription Wizard and have it generate the script only. Then you can go in and modify the script as you need it. You would need to add multiple sp_addsubscription lines. 

You need a tally table or calendar table that includes all years (that you want to report on). Call the table Allyears or something like that. As a field, you could put something like yearstart datetime and populate it with January 1 of each year (consider yearend as well). Then simply join the Allyears table to TestDates and display the years you want. Note, you have to decide if they were a customer on January 1, do you count them for the year? Or did they have to be a customer on Dec 31st to be counted. That will determine how you should create the table and join it. Apparently, I'm the only one who has trouble posting Sql code, the site or my company's security doesn't allow the edits when they contain code (sometimes). 

This is your best option. It maintains atomicity while at the same time encapsulating and decoupling the data access from your app. Do this. UPDATED: To clarify my point about the Party model. The following are my major beefs with the approach: 

This is nasty from a model perspective. If you go this route you have attributes that apply only to organizations intermingled with attributes that are only for users. It becomes quite a mess and you just 'have to know' what is right and wrong. It is a significant impediment to clarity. 

It's difficult to make a clear recommendation without knowing a lot more information about your environment. I've used most of these methods to varying degrees of success. Note that most places end up building out more robust data marts / data warehouses for reporting and analytics, so you'll probably end up with the ETL route one day. Oh, and make sure you have licensed the secondary server ;) 

How about another answer? Get rid of the Party table entirely. I'm a passionate hater of the Party model as it causes vastly more problems than it solves. However, since it's unlikely you're in the position to do such a thing: 

There's nothing per-se wrong with this as long as you do your transaction control in your app. It's not what I would suggest though. 

Using mirroring alone won't be sufficient since the mirrored secondary is not available for querying. You have to create and maintain snapshots, which can be annoying. Your options are, in no particular order: 

An alternative to @Jon's answer gets you around some of the nastier referential integrity and database design issues with a meaningless abstraction like 'Entity': You already have University, Program, and Course tables, so create tables called University_LearningContext, Program_LearningContext, and Course_LearningContext. That way you can enforce, at a database level, data and relationships that only apply to certain 'entities.' Abstracting everything to 'Entity' means you end up enforcing all kind of data rules in app logic (or, worse, you don't enforce them at all!), which is really not the way to go. It appears more complex because you're creating additional tables. However, from an overall architecture perspective it very clearly describes and enforces data integrity rules at the proper layer. 

You could also try at the beginning of the proc, setting isolation level to SNAPSHOT. More info available at: $URL$ You will incur some cost in tempdb for the row versioning. 

You can only reference servers that are listed under Server Objects -> Linked Servers as well as the local server via what you get back from @@SERVERNAME. Four part naming does not trigger a NETBIOS / DNS lookup. If you are referencing the local machine anyway, why not just use three part naming? 

The short answer is no as they would need the database master key and it's password. Here is a good overview of certificate encryption: $URL$ And one for master keys: $URL$ 

I've got a SQL 2005 SP4 server that connects to a 2008 SP3 instance via linked servers using the SQL Server server type. Every once in a while, one of those linked servers will start throwing login timeouts. To rule out firewalls, I can RDP to the server and run sqlcmd and get in just fine, even making sure to use the same login. I'm thinking that SQL has somehow cached something that prevents it finding the right address. The remote servername is defined in that machine's host file. So far, only a reboot fixes the issue. 

You're pretty much going to have to use networking tools to do the monitoring. Another option is to query the size of the tables involved directly on the linked server and that will give you a rough estimate. If you're joining across linked servers, all the data must be brought over, then filtered down, so you're probably transferring a lot more data than you think. 

The LDF is not a log backup, so I think you're stuck with restoring the db as it was during the last full backup. If you had transaction log backups since the last full backup, you could restore those as well and get you up to your most recent one. Unless someone answers that knows of some tricks to pull, afraid you've lost 15 days worth of data. 

I don't know PHP or MySql but the answer is definitely not creating new tables for each room. I'd have one table for rooms and one for messages and have the messages refer to the rooms (via foreign key). You probably want to consider archiving and/or purging as the data will probably grow quickly. Look into MySql's ability to partition tables and if there is an ability to SWITCH (this is possible in Sql Server) data in a partition from one table to another VERY quickly - it's just a meta data change. This could move Gbs in seconds vs having to physically copy and delete data from one table to the other. 

You feel like they should be separated because that makes perfect sense in the relational world. But as you said in the NoSql world and in MongoDB in particular, you want to group like-items together. I've not done extensive research on Mongo, but have spent some time with the online classes and I believe the answer is to store them together. You probably realize there is no such thing as a join in Mongo and therefore if you wanted to get 100 rows and get their corresponding images, you'd have to get the IDs for the 100 rows and then get 100 rows by their identifier (object_id or whatever). In other words, you have to do manual joins. 

You should break the quantity ordered into two columns or at least one integer column (but I like two). MinQuantity and MaxQuantity. Note I don't usually allow for abbreviations in tables or columns, but I'll make exceptions if it's universally accepted like Min and Max. Second, name your table appropriately, something like PriceBreak is more descriptive in my opinion than Exception (that could mean anything). Once we have proper table names, column names and data types, then the solution becomes much easier. You will use the itemname and itemquantity to join to the proper row (itemquantity between minquantity and maxquantity). One of the reasons that it's a good idea to name everything well is that the code becomes self documenting and is easier to create and troubleshoot. In that scenario, you won't even need a case statement, just a LEFT JOIN and an ISNULL. 

Have you tried using Adam Machanic's sp_whoisactive? There's an option to get the outer command to see if it really is within a proc. It could be the application is holding open a transaction instead of committing it. Try looking at DBCC OPENTRAN as well. 

We are in the process of removing a previous dba login and he owns all the endpoints and event notification objects. Endpoints were easy to change; Event notification objects not so much. I found this thread about changing the owner of an event notification object (you have to drop and recreate). I don't want to go through this process again if I can avoid it. I doubt it's possible, but outside of logging in as another user, can you create an event notification that runs as sa, etc.? 

The simplest solution is to buy something like Red-Gate's SQL Doc. There are a few people who have written scripts that you can find for free. 

If your database is in FULL or BULK_LOGGED recovery mode, you need to backup your database and log files on a regular basis. If your database is in SIMPLE recovery mode, then you only need to backup your database on a regular basis. Please read the following articles for more info: 

You could get a quick estimate via the following WHERE clause since phone numbers shouldn't have alpha characters... unless you allow phonetic numbers, ex. 1-800-ANT-FARM. 

Both Microsoft and Amazon offer SQL databases in the cloud. GAE isn't an RDBMS, it's NoSQL in the cloud. If you just need an object data store, well someone has probably written a wrapper in the language you want, otherwise there's Python and Java. If you need an RDBMS, I suggest you check out either Microsoft's SQL Azure or Amazon's Amazon RDS. 

@JulienVavasseur is correct in that you need to format your queries using Ansi92 join syntax. If nothing else, it will make it easier for us to read and help you with your questions. However, looking at the problem, it seems as easy as adding a Distinct to the count of session.id to reduce it from 7882 to 752. When you ask for a count, you're going to get a count of the rows, not a distinct count of the sessionid (unless you ask for distinct). 

You are correct, the only way to "rearrange" the order of the columns is to create a table with the new structure and push the old data into it, drop the old table and then rename the new table (or some variation of that). It requires copying all the data in the table and some drops and renames. 

There is an excellent blog post $URL$ that explains what's happening. SQL Server allows for a set number of compilations based on their complexity. It groups them into small, medium, and large. For large compilations, there can be onlyi one compiled at a time, so let's say all of your procs are considered large, then each one has to be compiled serially. That could account for the blocking. I think there may be several approaches to the problem - consider more resources (more CPUs will allow more small and medium queries to be concurrent or may up the threshold for what is considered medium). Also, more memory may solve the problem. If you're like most of us, that might not be possible. Another option might be to review the ADO calls and see if the number of calls can be reduced or spread out so that not all calls happen at the same time. Reducing the number at any given time should reduce your wait time. If that doesn't work, consider fixing the 'compilability' of the stored procs. Maybe break them down into smaller chunks which might reduce them to the small or medium buckets and allow more parallel compilations. Or determine why the procs need to be recompiled each time. See if they can be rewritten such that they don't need to be recompiled. Finally, I'd consider using Plan Guides. These will allow the procs to be precompiled and may save some time. Hope that helps 

I would say consolidate your indexes in this case as for every insert, delete, and appropriate updates would require index maintenance * n indexes. Also, you can use the WITH (DROP_EXISTING = ON, ONLINE = ON) to reduce outage if you have Enterprise Edition. 

You could further normalize and have a row for each unique combination of game, team, & inning. This would allow you as many innings as the InningId datatype would allow. 

If either $FreeSpace or $Size -eq $null, then it won't properly complete the query string. Either use command parameters just as you would in .NET (best method) or check for $null before insert. 

Will not handle out of SQL references, but you might want to check out Redgate's SQL Dependency Tracker. It's a nice visualization tool. 

If your table will have a non-trivial amount of rows, you might want to try a FULLTEXT index instead. It will be much faster and will match just on the exact word. 

It depends on your environment. I would setup a test using both methods and see which works best for you. Personally, I would page on the server. Less data over the wire and less data in the client's RAM, the better. If you can control the client machine specs, all traffic is over a nonsaturated LAN and clients always page through multiple pages quickly, then you might want to page at the client. 

Both of these guys authored several books, but I'm going to link to their blogs in case you want something more immediate. Louis Davidson: $URL$ Paul Nielsen: $URL$ 

I have a 2 node cluster (NODE-A & NODE-B) with 2 SQL instances spread between them. INST1 prefers NODE-A, INST2 prefers NODE-B. INST1 started generating errors then, failed over to NODE-B. Migrating INST1 back to NODE-A generates the connection errors after it logs a "Recovery is complete." message. Win 2008 R2 Ent. SQL 2008 R2 Ent. Errors from the Event Log after first failure: