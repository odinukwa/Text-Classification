Edit: Fix the input representation to be the $N \times N$ adjacency matrix of the underlying $N$-vertex symmetric simple directed graph, with the rows listed consecutively to form an $N^2$-bit string. 

The complement of a vertex cover is an independent set. Your question is therefore equivalent to asking whether counting independent sets is #P-complete on trees. The answer to this question is NO, it is not #P-complete: there is actually a polynomial-time algorithm to count the number of solutions. 

Database theory is a sprawling field providing many applications of logic. Descriptive complexity and finite model theory are closely associated fields. As far as I can tell, these areas all tend to use algebraic styles of logic (following in the footsteps of Birkhoff and Tarski) rather than proof-theoretic. However, some of the work of Peter Buneman, Leonid Libkin, Wenfei Fan, Susan Davidson, Limsoon Wong, Atsushi Ohori, and other researchers who were working at UPenn in the 1980s-90s, did seek to unite programming language theory and databases. This seems to require being comfortable with both styles of logic. The same goes for more recent work by James Cheney and Philip Wadler. In terms of specific references, the standard textbook is available online for convenient reference: 

Suppose a graph $G$ with $n$ vertices is presented as a stream of $m$ edges, but multiple passes are allowed over the stream. Monika Rauch Henzinger, Prabhakar Raghavan, and Sridar Rajagopalan observed that $\Omega(n/k)$ space is necessary to determine whether there is a path between two given vertices in $G$, if $k$ passes are allowed over the data. (See also the technical report version.) However, they do not provide an algorithm to actually achieve this bound. I assume that an optimal algorithm would actually take $O((n\, \log\, n)/k)$ space in a realistic computing model, since one has to distinguish the $n$ different vertices if one cannot index memory using constant size pointers. 

So yes, the question is sensible, and it has also essentially been answered. (Thanks to @user834 for the preprint links.) 

There is clearly an algorithm, since the finite search space can be explored using brute force. This takes a long time... Instead of considering $f \colon \{0,1\}^n \to \{0,1\}^n$, one can look at the Boolean function $f' \colon \{0,1\}^{2n} \to \{0,1\}$ defined by $f'(xy) = 1$ iff $f(x) = y$. No polynomial-time algorithm is known to find a minimum-size circuit for a Boolean function, which is known as the Minimum Circuit Size Problem (MCSP). However, MCSP is in NP. If MCSP were in P, then one could factor in average time $2^{n^\epsilon}$ time (for any $\epsilon > 0$) numbers that are products of two primes, each of which is congruent to 3 modulo 4. These numbers are quite hard to factor in practice, so this would be quite surprising. On the other hand, if the MCSP is NP-complete, then this would imply strong circuit lower bounds for the class E = DTIME($2^{O(n)}$), which we don't currently know how to do. 

There are large classes of queries which are "easy", even in the worst case. In particular, if the class of queries contains conjunctive queries only and each query has bounded width (for instance treewidth, treewidth of its incidence graph, fractional hypertree width, or submodular width) then the query can be answered by using something like a join tree, together with brute force enumeration for the local parts of the query that deviate from the tree. This requires polynomial time, with the degree of the polynomial determined by the width parameter. It seems that many queries encountered in practice are both conjunctive and have small width. So the polynomial runtime has low degree in this case. Dániel Marx presented a paper at STOC 2010 on submodular width recently, the full version of which includes a nice summary of the various notions of width and how the CSP formulation relates to the database formalism (the conference version lacks this). 

In more detail: Let $\Gamma$ be the class of finite simple graphs, and $\Gamma_0$ be the class of finite simple graphs which may have self-loops. (Clearly $\Gamma \subset \Gamma_0$.) Deciding if a connected input graph $G$ has factors in $\Gamma_0$ can be done in polynomial time for the Cartesian and strong products, and also for the direct product when $G$ is non-bipartite. Deciding if $G$ has factors in $\Gamma$ is in polynomial time for the Cartesian product, but is unlikely to be in polynomial time for the lexicographic product. I do not know the status of deciding if $G$ has factors in $\Gamma$ for the direct and strong products. Relevant results from Imrich and Klavžar: 

Cipu recently announced an asymptotically better bound of $2^n$. (Note that the smallest possible bound is $2^{n-1}$.) The argument builds on a bound on the determinant of a matrix, due to Waldi. 

Harvey Friedman showed that there is a neat fixed point result that cannot be proved in ZFC (the usual Zermelo-Frankel set theory with the Axiom of Choice). Many modern logics are built on fixed point operators, so I was wondering: are there any consequences known of the Upper Shift Fixed Point theorem for theoretical computer science? 

So deciding whether a graph is prime with respect to the lexicographic product is equivalent to GRAPH ISOMORPHISM, with respect to Turing reductions. The case of the direct and strong product having factors without self-loops seems to be absent from the references I have looked at. I would appreciate any pointers to papers that do discuss this case, or a hint why it is uninteresting. 

Maximum Bipartite Matching is the version where the required size of a matching is given as part of the input. By Chandra-Stockmeyer-Vishkin, this is equivalent to BPM via $\textsf{AC}^0$-reductions, and these problems are hard for $\textsf{NL}$ via $\textsf{AC}^0$-reductions. 

If you really do want SAT and would like to avoid constraint satisfaction, then you can translate the constraint formulation into SAT, either manually or using an automated translation tool like Sugar. Note that Chapter 2 of the recent Handbook of Satisfiability claims that it is common to first express one's problem as a constraint satisfaction problem, and only then to translate this formulation to SAT. So constraint satisfaction should probably be considered even when one wants to work with SAT. There is one big advantage of staying within a more expressive paradigm. The constraint satisfaction framework allows the use of interval and numeric constraints. These often occur in computer vision, but can lead to SAT instances that are infeasibly large. Specialised constraint solvers may be greatly preferable to SAT solvers in such cases. (A final aside: I have heard that the computer vision community tends not to be aware of constraint satisfaction approaches, and that this has led to the reinvention of well-known constraints techniques and results. I do not know if this is a correct assessment, but if it is, then it would be worth translating constraints results to the terminology and specific concerns of computer vision.) 

I'm interested in the status of the following well-studied decision problem, in particular whether it is known to be in $\textsf{NL}$: 

The current spurt of theoretical analysis of the semiring model of data aggregation was kick-started in 2007, in the context of provenance. Provenance is a fancy term for annotating data. Since any database tuple can be seen as annotations applied to some unique tuple identifier, aggregation of data can be seen as just combination of annotations. Provenance is therefore a generalization of the idea of aggregating data, and it has explicitly been argued that the right theoretical model of combining annotations is a semiring. The most general semiring, of provenance polynomials, actually allows one to keep track of the entire history of how a piece of data was obtained from constituent parts. As an example, a p-value in the analysis of a clinical trial can keep track of how it was calculated from each of the individual trial results. If some of them turn out to be wrong (or fake) then one can simply recalculate without the bad data. 

Let $L$ be a language in P/poly. There is then a deterministic polynomial-time Turing machine $M$ with polynomial-sized advice that decides $L$. Consider the language $A(M)$ of all advice strings given to $M$ for instances in $L$. 

Details Of interest here are $v$-vertex graphs encoded using a sequence of edges, each edge being a pair of distinct vertices from $\{0,1,\dots,v-1\}$. Suppose $(M_v)$ is a sequence of two-way finite automata (deterministic or nondeterministic), such that $M_v$ recognizes $k$-Clique on $v$-vertex input graphs and has $s(v)$ states. A general form of the question is then: Is $s(v) = \Omega(v^k)$? If $k = k(v) = \omega(1)$ and $s(v) \ge v^{k(v)}$ for infinitely many $v$, then NL ≠ NP. Less ambitiously, I am therefore stipulating that $k$ is fixed, and the $k=3$ case is the first nontrivial one. Background A two-way finite automaton (2FA) is a Turing machine which has no workspace, only a fixed number of internal states, but can move its read-only input head back and forth. In contrast, the usual kind of finite automaton (1FA) moves its read-only input head in one direction only. Finite automata can be deterministic (DFA) or nondeterministic (NFA), as well as having one-way or two-way access to their input. A graph property $Q$ is a subset of graphs. Let $Q_v$ denote the $v$-vertex graphs with property $Q$. For every graph property $Q$, the language $Q_v$ can be recognized by a 1DFA with at most $2^{v(v-1)/2}$ states, by using a state for every possible graph and labelling them according to $Q$, and transitions between states labelled by edges. $Q_v$ is therefore a regular language for any property $Q$. By the Myhill-Nerode theorem there is then a unique up to isomorphism smallest 1DFA that recognizes $Q_v$. If this has $2^{s(v)}$ states, then standard blowup bounds yield that a 2FA recognizing $Q_v$ has at least $s(v)^{\Omega(1)}$ states. So this approach via standard blowup bounds only yields at most a quadratic in $v$ lower bound on the number of states in a 2FA for any $Q_v$ (even when $Q$ is hard or undecidable). $k$-Clique is the graph property of containing a complete $k$-vertex subgraph. Recognizing $k$-Clique$_v$ can be done by a 1NFA that first nondeterministically chooses one of $\binom{v}{k}$ different potential $k$-cliques to look for, and then scans the input once, looking for each of the required edges to confirm the clique, and keeping track of these edges using $2^{k(k-1)/2}$ states for each of the different potential cliques. Such a 1NFA has $\binom{v}{k}2^{k(k-1)/2} = (c_v 2^{(k-1)/2}/k)^k.v^k$ states, where $1 \le c_v \le e$. When $k$ is fixed, this is $\Theta(v^k)$ states. Allowing two-way access to the input potentially allows an improvement over this one-way bound. The question is then asking for $k=3$ whether a 2FA can do better than this 1FA upper bound. Addendum (2017-04-16): see also a related question for deterministic time and a nice answer covering the best known algorithms. My question focuses on nonuniform nondeterministic space. In this context the reduction to matrix multiplication used by the time-efficient algorithms is worse than the brute-force approach. 

There is a large and still growing body of theory about classes of fixed-template constraint satisfaction problems that have polynomial-time algorithms. Much of this work requires mastery of the Hobby and MacKenzie book, but luckily for those of us who are more interested in computer science than universal algebra, some parts of this theory have now been simplified enough to be accessible to a TCS audience. Each of these problems is associated with a set of relations $\Gamma$, and can be expressed as: given a source relational structure $S$ and a target relational structure $T$ with relations from $\Gamma$, does there exist a relational structure homomorphism from $S$ to $T$? An NP-hard example is when $\Gamma$ is the set containing the relation formed by the back-and-forth directed edges of a complete graph with at least $k \ge 3$ vertices: this expresses Graph $k$-Colouring. An example in P is when every relation in $\Gamma$ contains the tuple $(0,0,\dots,0)$: in this case mapping every element of $S$ to $0$ in $T$ is a (trivial) solution. It is believed that there is a dichotomy for these problems, which can be expressed roughly as: the problem for $\Gamma$ is in P precisely when the algebra associated with $\Gamma$ has a Taylor term (note that I am leaving out some crucial conditions for the sake of exposition). This extends Schaefer's Dichotomy theorem to the case where the set used to construct the relations in $\Gamma$ contains more than two elements but is still finite. The dichotomy is known to hold for the important case where the relations in $\Gamma$ are conservative; this means in practice that the class of problems contains all the successively simpler subproblems considered by a constraint solver, so the process of constraint solving avoids generating "hard" intermediate instances while solving "easy" problems. In one important case that falls into P, methods that are similar to Gaussian elimination can be applied. This works if $\Gamma$ is obtained from a system of linear equations over a finite field. More surprising, this also works for a range of problems that do not appear at first glance to have anything to do with linear algebra. They all rely on the relations in $\Gamma$ being closed under "nice" polymorphisms. These are functions that are applied componentwise to collections of tuples from the relation to yield another tuple, which then has to be in the relation. Examples of "nice" polymorphisms are Taylor or cyclic terms. The results to date seem to indicate that there should be a kind of general powering transformation of an underlying reachability state space that can turn such problems into ones with a constant tuple in each relation, like the example above. (This is my personal interpretation of ongoing research and may well be completely wrong, depending on how the ongoing search for an algorithm for algebras with cyclic terms pans out, so I reserve the right to recant this.) It is known that when there isn't such a transformation then the problem is NP-complete. The frontier of the dichotomy conjecture currently involves closing this gap; see the open problems list from the 2011 Workshop on Algebra and CSPs. In either case, this probably deserves an entry in Scott's list. A second class in PTIME allows local consistency techniques to be applied to prune possible solutions, until either a solution is found or no solutions are possible. This is essentially a sophisticated version of the way most people solve Sudoku problems. I don't think this reason currently features in Scott's list either. It is interesting that Libor Barto's PTIME algorithm for the conservative case in the presence of cyclic terms is nonconstructive: if there is an absorbing subalgebra then there is an algorithm, but no way is known to decide whether a set is an absorbing subalgebra of an algebra. Contrast this with the Robertson-Seymour setup, where there exists an algorithm but it relies on knowing the finite set of forbidden minors, yet no way is known how to decide whether a finite set of graphs is the list of forbidden minors of a graph class. Barto's algorithm relies on knowing the absorbing subuniverses of all subsets of the algebra associated with $\Gamma$, and of the existential nature of the algorithm he says "I love it"... Finally, there is also much exciting work initiated by Manuel Bodirsky for the case of infinite domains. Some of the algorithms look quite strange and may ultimately turn out to lead to more entries in Scott's list. 

Edit: to clarify, resource bounds is my focus here. With unbounded resources this devolves into a question about definability and large ordinals, as discussed by Aaron and Kaveh. The idea is that each interesting number should be definable by a sentence that can be checked given a bounded amount of resources. (It might also make sense to allow some finite set of sentences as separate building blocks instead of making them into single sentences via conjunction.) 

I'd appreciate clarification of this statement; it seems to imply that $\text{AC}^0$ contains even undecidable languages. 

SQL and (original) SPARQL are based on fragments of first-order logic, without fixed-point or transitive-closure operations. The basic idea is that first-order formulas can only express things that are "local" in some sense, with the reach of locality increasing with the length and complexity of the formula. (The definitions of Gaifman and Hanf locality make this precise.) Transitive closure and graph connectivity require either a more powerful way of talking about arbitrarily long paths (like property paths in (new) SPARQL, or a fixed point operator), or arbitrarily long formulas. So one can't capture path expressions of arbitrary length without explicitly adding support for them to a first-order-based query language. If you want to know more, have a look at the above textbook, or at the paper: 

Using a hardness result seems to be weak evidence for a separation between the classical and quantum worlds, since it is possible that the class of matrices in the specific quantum setup will all be of special form. They might have complex entries, but may still possess a lot of structure. There could therefore exist an efficient sampling procedure for such matrices, even though the general problem is #P-hard. 

there exists a linear-bounded automaton that recognizes SAT, from which one can extract a CSG, so that the language defined by the CSG is in NP due to some feature of the grammar (and not because we already know it is in NP)?