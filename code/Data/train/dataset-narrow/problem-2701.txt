Yes. That's why it's called a "memory vs. performance tradeoff." You spend memory to make things faster. You're doing 2D rendering. Do you honestly think that making a proper array is going to tax your GPU's memory significantly? Even if your tiles are 4x4 pixels in size, and you're rendering at 2560x1600, that's only 256,000 tiles for a single screen. With each tile being 32 bytes in size (four vertices, where each vertex has 2 signed shorts for the position, 2 signed shorts for the texture coordinate), that means your vertex data takes up 8,192,000 bytes of memory. Is ~8MB of storage really going to be a problem for you? Even on cards with 128MB of graphics memory, that's less than 10% of the space. And that's the worst-case scenario; for more reasonable tile sizes like 8x8 or 16x16, you're looking at 2MB or 512KB, respectively Also, instancing for tiles is more likely to kill your performance than to help it. Instancing works most optimally when: 

Of course, this only works when the GL_EXT_texture_filter_anisotropic extension is available, but since every graphics card has supported this is about the year 2000, I wouldn't worry about it. 

Do not discount the utility of this so easily. You will never understand how productive you will be until you take away the recompilation step. The "flow" is a fairly well-understood psychological concept when it comes to work. The flow is that feeling you get when you're focused on an activity, when you're analyzing and solving problems almost without thinking, etc. You are at your most productive when you are "flowing". Compile times screw all of that up. It's hard to stay in the flow if you have even a 10-second compile between testing something. When you are developing gameplay, what you usually have is a "tight loop". You have an idea, you code up a test to see if it works, and then you try it out. If it doesn't work, you modify it and try again. The "code-to-test" time is very important for maintaining flow. Getting it as small as possible is crucial. What Lua (or any embedded scripting language) allows you to do is to test changes, not just without "compiling", but live in the game. Depending on how you build your game, you can run a command that will restart the game with new scripts without having to stop and reload data and so forth. Not only do you not have to recompile, you don't have to re-run. The ability to do this, given the proper engine support, can dramatically increase productivity. 

If you're storing a color, a normal, and a depth, then I can only assume that you are doing some form of deferred rendering-type thing here. That is, the color is the diffuse reflectance, not the light radiating from the background. Given that, 10-bit colors are pretty much overkill. You don't need that level of color accuracy for a diffuse reflectance value; you can get the same level of accuracy from an sRGB color value. HDR is all about the lighting; you can do HDR just fine with lower color-depth. Once you're dealing with regular sRGB colors, you can start using standard image compression techniques. 

Personally, I'd establish (and enforce) a standardized convention for attribute indices. GL index 0 is the position. GL index 1 is the color. Index 2 is a normal, with 3 and 4 for tangents and binormals (if needed). Index 5-7 are texture coordinates. Maybe 8 and 9 are for bone weights. 10 can be a second color if needs be. If you're not able to use or GL 3.3+, then you should also establish a standardized attribute naming convention. That way, D3D has conventions and OpenGL has conventions. So the user doesn't even have to ask what the index of a "position" is; they know it's 0. And your abstraction knows that 0 means, in D3D land, . 

This will only call five times. I'm guessing you wanted to call it 6 times. But I can post a working answer. Note that this code is designed to be run alongside my tutorial series, so it makes reference to code that isn't present. But this is mainly things like creating meshes and so forth; nothing truly important. Here are the salient points. The shaders for the main sphere object. Vertex shader: 

You cannot really expect two different physics engines to get binary identical results. They all do their computations differently, and thus they will come up with different results. 

is what creates an OpenGL context on Windows, but it isn't the only way anymore. There is also , which is an extension function (and thus paradoxically means that you need to have created an OpenGL context already). This is used to create OpenGL 3.2+ core contexts. I don't know what you intend by "load my library," but if you're trying to hook into all of OpenGL's stuff, to get every OpenGL call, then glIntercept is probably the place to look. 

Lua is not magic. Lua cannot magically associate itself with any application and interfere in any function call. You can't even do that from C. Yes, with Win32 DLL injection, you can interpose a DLL within a DLL boundary. But if there is no DLL boundary to inject your code... you can't do anything. And I'm guessing most games don't expose the "spawn entity" function across a DLL boundary. In general, most games put the main game code either in a DLL by itself or within the executable itself. The graphics system, sound system, etc, may be DLLs. But even if the game code is a DLL, internal game processes (create entity, manage entity, etc, stuff you want to hook to) would all be internal within that DLL. And therefore not injectable. Oh, you can do this (not with Lua directly). But you would have to decompile the application, reverse-engineer the flow of functions and such, and manually edit the compiled binaries to add your external hook functions. 

That right there is indicative of your problem. You cannot take a color value of (0.25, 0, 0), and add it to itself and get anything besides more red. It will not magically become yellow, then white. It will always be red, and it will always have zero green or blue (which are necessary to achieve yellow and white). Blending is just math; nothing more, nothing less. Math on colors is no different from math on things that aren't colors. If you want adding "red" to eventually produce yellow and white, then you're going to need to have some blue and green in there. Furthermore, you're going to have to use some form of tone mapping and high-dynamic range rendering to make this work out in a reasonable way. 

This is highly unnecessary. Why does the have to do this? Yes, conceptually, it is the creature's attack that causes items to be removed. But why does the object have to deal with that? Here's how I would do it. The would have one or more s. An is an object that, when successful, can cause one or more effects. Effects include: 

The sprite needs to not be one piece. Split it into two sprites and render them that way. The top and bottom halves can animate independently sometimes but dependently other times. See Contra. You need special frames of sprite animation for walking and shooting simultaneously. 

You're thinking about this the wrong way. You're thinking in terms of characters, background, etc. You need to be thinking in terms of a world, which can be viewed from any location. Or at least, a world that has a bounded area from which it can be viewed. You should be able to position your camera in any arbitrary location you wish, within restrictions (like world boundaries). Your camera is what determines the offset for the "background". For example, let's say your world starts at (0, 0). And it advanced to the right by 400 units, and up by 200 units. That's the boundary of your world. Now let's say that your camera can show a 20x15 rectangle of this world. All entities have a position in the world. For example, maybe the player starts out at location (10, 5) (remember; positive X goes right, and positive Y goes up in this example). That's the player's world-space location. When you render the player, you don't render them in world-space. You transform the player's position into screen-relative coordinates. To do that, you must define a camera, which represents a particular view of the world. Let's say we want to center our camera (which is 20x15 in size) on the player's position, as much as possible within the boundaries of world space. So the bottom-left position of the camera, given a player at (10, 5), would be the camera at (0, -2.5). That is half the camera width/height subtracted from the player's position. But since that is outside of the world, we clamp the Y value to 0. So the bottom-left of the camera is (0, 0). Now, let's say the player moves to location (30, 20). Well, the bottom-left of the camera should be (20, 12.5). That's still within the world, so there's no need to clamp. All of those camera positions we computed? Those are used to transform entities from world-relative coordinates to screen relative coordinates. This is done by simple subtraction. If your camera is at (20, 12.5), and there's an entity somewhere that is at (30, 16), then the actual position of that entity on the screen is the entity's position minus the camera's position: (10, 3.5). This is the location you render him at (again, assuming bottom-left orientation. Most 2D renderers try to use top-left. The math is ultimately the same either way though). So there needs to be a separation between "where the character actually is" and "where the character gets rendered on the screen". 

Your point being? Google barely tolerates people using NDK; it's clear that they want everyone to use Java. The only reason NDK exists is because there are certain important developers who simply will not use the platform without it. Thus, NDK exists to service them and their feature needs. Yes, C++ is a bigger, more complicated specification to implement. So is Java. So is any language except C. Also, what do you mean "all the latest and greatest features"? If you're talking about C++11 stuff, well, nobody fully implements that yet. The spec is barely a year old now. Also, I don't know if NDK support C11 either; yes, C also has "latest and greatest features" that aren't supported everywhere. Which brings up an important point: if you want your pure-C application to compile on Visual Studio, then it needs to conform to C89, and nothing higher than that. So fragmentation with C already exists. Some platforms support only C89. Some support C99. Some support C11, to varying degrees. Etc. If you want to use C, then use C. The fact that other people didn't make that choice does not mean your choice is wrong or that their choice is wrong. You don't need to justify yourself to them, and they don't need to justify themselves to you. 

You should always give the player the ability to change their key assignments. That is how it is "usually handled": let the player change them. Some players will set their keyboard to QWERTY when they play games because that's what most games expect. Some will leave them set to their current keyboard and rely on the ability to change the keys. 

In general, unless it's a 2D game, people don't use the painter's algorithm. There's just no point to it; depth buffer is not only cheap, but faster than what you're trying to do. Yes, really. With early depth tests, fragments can be culled before having the fragment shader even execute. This saves time with scenes that have heavy overdraw. With coarse front-to-back sorting, there's just no reason to not use a depth buffer. Thus, once you commit to the painter's algoithm, you've already given up quite a bit of performance. What you might save from optimized calls is irrelevant next to what you lose from the massive overdraw your scenes will have. Most important of all, until you have profiling data that suggest your number of calls is too great, you shouldn't much bother. Or if you are going to bother, make sure that your sorting criteria is something you can change, so that you can adjust what you sort based on.