Any GUID for an identity column will be an issue due to the size of it. This leaves less space in each page for actual data which means your data density is lowered and you are likely to see more page splits and fragmentation. The problem when you use non sequential GUIDs on an id column with a clustered index is fragmentation. If you had 100 rows in the table with batches of 20 with sequential GUID values as the id and then wanted to add a random id value the engine may need to find space in the middle of your table to insert that record instead of just adding it to the end. Depending on your fill factor level this could cause page splits which cause index fragmentation. Lower the fill factor of the index can help with fragmentation but not as much as always sequential values. This way each new record can be added to the newest page or a new page can be added at the end without the need to reorganize existing data. Check here for more info. 

You should your backups to another SQL Server instance to check that the backups are OK and that they can be restored. Doing this everyday will soon become a pain so automating the process with a SQL Agent job or some PowerShell would be ideal. This restored version of your database also gives you a great place to run consistency checks without adding extra load to your production system. 

The following SQL will show you which file groups your tables and indexes are in which will make it easy to see if there is data in any file group that shouldn't have data. 

If you can group your data in a way that makes row 1-8 group 1 and the other rows group 2 then you can do a page break at the end of each group. 

Option 3 sounds like your best option as it gives you all the flexibility you need, fits into a RDBMS perfectly and follows normalisation. Option 1: requires schema changes when new alerts are added to the system. Option 2: More joins = slower queries. Option 4: This does give you the flexibility you need but if you were going to do it this way then maybe consider using a document store like MongoDB. 

If all the databases are on the same instance you could have a central database (named reports, admin or whatever you like). Your report could use that database for your reports datasource. You could then have a stored procedure that made use of dynamic SQL and a parameter to determine which client database the query should read from. So your report connects to the reports database and runs the stored procedure with the parameter of 'ClientA', this is passed into the dynamic SQL and executed. If the databases are in separate instances you would need to use linked servers for this method to work which could cause some security issue so I would look into that before enabling it. 

I would only recommend this approach if you have not, can not or do not want to install SSIS. If you are planning on regularly importing the contents of this file you will be better off using SSIS as you have things like error handling and data transformations that may be easier for you to configure in SSIS instead of writting them from scratch in T-SQL. The OLEDB solution may be the quick solution and require no extra applications other than the Jet OLEDB driver, but SSIS will be a better long term solution. I would recommend you research SSIS and try to make a package to insert the data in a test environment. 

I'm not sure if what you are building is a good idea or not but the code below should get it working. You basically need to include the statement to separate the script into individual batches. Using a temporary table to store the original settings values allows access to the values by each of the batches. You can't include the GO statement in a stored procedure so if you are looking to automate this process you will need to separate the script out another way (Agent job steps, SSIS, PowerShell, etc) 

Right click the databases folder in the tree on the left in SSMS and select restore database. The restore task will remove the existing database and then create the restored database. Remember to select overwrite on tick box on the options tab. 

We are doing something very similar at my place and this very question was in my head for a while a few weeks back. After noticing that logs were consuming 98% of our DBs in SQL Server I did some research on how we should handle this problem. It all depends on the structure of your logs but our logs are not really relational so storing them in a RDBMS and consuming all the resources built for the RDBMS does not make sense. I started looking into NoSQL stores and found DB Engines Ranking to be very useful to compare the feature sets of each DB engine. Personally I found that an eventual consistency solution worked for me as it has good sharding, availability, and fast reads on huge data sets. Check the CAP Theorem wiki entry to learn about the trade offs on different types of NoSQL solution. We are currently testing a Cassandra deployment for our logs with MapReduce jobs. 

You would need to build a custom backup script which uses dynamic SQL to build the name of the .bak file. The maintenance plan could execute this with the 'Execute T-SQL Statement Task' item. The T-SQL would look something like this 

Example: I start updating all the rows in my Orders table at and the transaction takes 5 minutes to run. This creates a row in the history table for each row with as . All the rows in the current table will have a of . If someone were to execute a query at (while the update is running) they would see the old values (assuming default read committed isolation level). But if someone was to then use the syntax to query the temporal table as it was at they would see the new values because the their would be . To me this means it doesn't show those rows as they were at that time. If it used the transaction end time the problem wouldn't exist. Questions: Is this by design? Am I missing something? The only reason I can think it's using the transaction begin time is that it is the only 'known' when the transaction starts. It doesn't know when the transaction will end when it starts and it would take time to apply the end time at the end which would invalidate the end time it was applying. Does this make sense? This should allow you to recreate the issue. 

The MIN value is useful when you have a server with multiple instances. You can use it to prioritise memory usage per instance. But mainly it is to guarantee that SQL with have at least that much memory to work with. This stops people running something on the server and stealing all the memory. The default setting basically leaves memory allocation to SQL Server but it's best practice to set a static value so that there is never an issue of SQL taking memory away from other processes or the OS. This is a must if you are running SSIS, SSAS, SSRS or any other serverice / application on the same server.vSo your setting needs to leave enough memory for the OS + services + applications running on the server. 

A separate AD service account for each service on each instance is the most secure model and if you have some good password management software it shouldn't be a problem. The work involved in creating all the service accounts and entering them into your password software will be quite time consuming depending on how big your environment is. Some people sacrifice some security by using the same service account for all the services on a SQL instance. The problem with this is if the account gets locked out then all the services including the SQL service account will be locked out. This problem becomes even worse if you use the same AD account across SQL instances as one instance can be affected by another. Also obviously if an account is compromised, the more services it is used for then the more services that are compromised. 

From reading your question I think you are building the SQL code in your application and then sending it to the server as one long script with multiple statements. If this is the case you can separate your statements with GO to create batches. The following is valid SQL: 

I don't think you need a sub query here. Does this give you what you need? If so it should be more efficient. 

Instead of SSIS I think you might want to look into SQL Server Service Broker. This lets you send messages asynchronously. See here or here for a start. MSMQ or RabbitMQ may also be options you should consider. 

I would suggest moving all clients to the domain and removing the SQL logins as the AD route is much more secure. If this is not an option then I would probably remove the AD logins and have RDP users use SQL logins as having two logins per user doesn't make much sense. If for some reason you decide you have to go the two login route I would write a stored procedure to replicate the settings from AD users to your SQL logins. This could be triggered with a . This way you would only have to update one set of users. I would also be careful in using AD groups if you are not the only person responsible for the administration of the domain. It's not clear from the database who has the permissions specified for the group as users be dropped in and out of the AD group without your knowledge. 

It can help query performance by employing partition elimination. This means large sections of big tables can be ignored when looking for values which means much less IO. Index alignment needs to be looked into when partitioning. See details here You can break your backups by partition. This can be useful if you are struggling to complete your backups in time. See here for details Index rebuilds can be done at the partition level instead of the whole table. Large inserts can be done with partition switching. Queries can also run parallel across partitions. 

The second approach is much more normalised and what you would expect to see in an OLTP application database. The first approach is more of a de-normalised approach that you would likely see in a data warehouse for reporting purposes. The first approach would probably be faster as the less joins you have in a query the quicker it generally is but this can be fixed with indexes that work much better with integers. Also you would be duplicating a lot of data and increasing the size of your database with the first approach. 

Without the schema this is a little difficult to test but I think this will work. Even if I do make some syntax errors you will be able to see what I am getting at. 

I have assumed the ID field that would be used to join the two tables. Let me know what field you would use to join these tables and I will update the answer. You should also specify the correct alias in your @columns variable. I also assumed the UserRegistration table schema to be dbo. 

Dynamically creating multiple triggers for individual rows sounds quite nasty. This is not likely to scale well. After reading your update I would suggest you implement 1 trigger that fires for all CRUD operations on the table. This trigger checks to see if the modified rows of are in the table. If they are send a notification, if not don't send notification. 

UPDATE I made a mistake in my code above. It is not possible to alter the encryption algorithm and the certificate used in one statement. This was not required anyway so the following code is sufficient: