source: Wikipedia. Now consider the following, this paper gives an algorithm where one can check in quantum polynomial time whether a function satisfies Simon's promise. Merlin can therefore check this, for Arthur we don't know, let's assume it can't. Question: Given the above mentioned conjecture, if Arthur sends (through a message) Merlin the function to be computed/checked in the first pass, Merlin can check whether the function satisfies Simon's promise or not, if yes, then we know the answer, if no, then is checking whether a function one-to-one in polynomial time? Also if Arthur can't check whether the function satisfies Simon's promise can the probabilities above mentioned change, since Merlin knows the answer already? I also think this question might remain open until we know whether BPP != BQP, are there any indirect techniques that can let one evade having to rely on conjectural support in my argument? Thanks! 

Context: Ed. Witten recently wrote a potentially revolutionary paper where he showed that under certain conditions, a Chern-Simons path integral in three dimensions is equivalent to an N = 4 path integral in four dimensions (this is the standard d=4, N=4 super Yang Mills theory) Speculation: Witten had shown that the Chern-Simons topological quantum field theory can be solved in terms of Jones polynomials. A quantum computer can simulate a TQFT, and thereby approximate the Jones polynomial. (source: Wikipedia and this paper) Now I haven't completed reading Witten's paper and I wouldn't understand much of it anyways. But the idea is that if a quantum computer can simulate a path integral (or a Chern-Simons TQF) and since now Witten has shown in both of them to be dual descriptions in some sense, a quantum computer, atleast theoritically might be able to simulate a QFT. Also by the extension of that, Maldacena proposed that the specific field theory that Witten is using to be dual to type-II B string theory in AdS/CFT so then it may also be possible (only theoritically) to simulate a string theory. Question: What are the technical constrains that a quantum or classical computer faces while simulating a QFT? Also my speculations only partially complete, could experts suggest a better description? Thanks! PS. Also thanks to Mitchell Porter who brought up that paper before. 

It is not known if there is an $\varepsilon > 0$, $c > 2$, and $k > c$ such that $(c,k)$ hyperclique is in $n^{k-\varepsilon}$ time. Note that the case of $k \leq c$ is trivial. For years I have communicated this problem to many people, and taught it in cs266 at Stanford, due to its connection to solving $k$-Sat. (Several open problem sessions at workshops probably recorded this.) Here are a few things I know: I proved several years ago that solving $4-cycle$ on $n$ node graphs in $n^{2-\varepsilon}$ time implies $(3,4)$ hyperclique in $n^{4-\varepsilon}$ time. Haven't published it. If you can solve $(3,4)$ hyperclique as indicated above, then Max-3-Sat can be solved in strictly less than $2^n$ time. Similarly, solving $(k,k+1)$ hyperclique would yield a faster $k$-Sat algorithm. So if you believe Strong ETH then there is an obvious limit here. The reduction is a natural generalization of the reduction from Max-2-Sat to triangle finding ($(2,3)$ clique) from ICALP'04 and my PhD thesis. You can solve $(c,k)$ hyperclique in $n^k/(\log n)^{\Omega(k)}$ time by generalizing the paper Efficient Algorithms for Clique Problems. 

NAUTY "colors" nodes with constant depth neighborhood canonical forms. Babai's new algo does likewise with log size neighborhoods. The kicker is that in a random graph the diameter is about log n, so you end up gobbling the whole thing. Definitely worth doing for sparse graphs, can really cut down the state space you need to search. Also when you have to go brute force, only check repeated prime cycles, not all k! $URL$ 

Read Feynman's letter. Very good advice. Always be curious and solve little problems you are good at. Eventually you will have enough solved for a major publication. 

If you are storing random permutations with probability ${1\over2}$ then you are going to need $log_{2}(n!)$ bits per permutation, Kolmogorov complexity dictates it. If the distribution is non-random it depends. To understand the state space it might help to look at $URL$ , the size of any min dominating set over $S_{n}$ using a monogenic inclusion relation between permutations (ignoring the identity which is in all subgroups). You can encode the relevant prime order permutations in $log_{2}( OEIS\_A186202(n) )$ bits each. That will give you some savings over the usual $log_{2}(n!)$ needed for a random permuation. 

Speculation: Let's take Arthur-Merlin protocols (I actually don't know in much depth about these protocols so please pardon any inconsistencies that may arise) where one can restrict Merlin's power to BQP instead of unbounded resources and Arthur has BPP resources. Now I realize that there are probabilities to: 

I will try to answer my own question here, I found some new stuff :) John Preskill recently at Holographic Cosmology v2.0 gave a talk on some problems with this simulation and I just watched it again one more time. Joe mentioned some of the similar problems such as truncating the Hilbert space. The way one might approach the simulation of a quantum field theory is through some indirect methods, one possible approach is as follows. This paper describes how it is possible to simulate a lattice gauge theory, a gauge theory is simply an invariant version of a field theory where the Lagrangian has some specific constrains. The paper I mentioned above gives a technique whereby one can generalize their implementation to other gauge theories under some constrains however, so generalizing their work to the gauge-gravity duality or its more specific case the AdS/CFT correspondence might be the way to go. In lattice gauge theory, the spacetime is Wick rotated into Euclidean space and discretized into a lattice, so once the work is done in the Euclidean space, oen can wick rotate back to AdS. Unfortunately, I don't nearly have the necessary required math or complexity theory knowledge to go deeper into it. 

2^32 is only 4 billion; and doubtful you will saturate even a small fraction of that if you are watching IPs. Just store it all. A radix tree will compact as it saturates since you can flag entire subdomains as saturated. 

In Spivak's book on page 192 he gives an example of using colimits to create transit maps. Also, his Application 5.2.1.2 discusses applying Liquibase like patches to a database schema over time then using the colimits to reason between old and new data in universal manner. 

Every integer has an associated Kolmogorov complexity; the shortest program that prints that integer. There are $\approx {x \over ln(x)}$ primes up to $x$ so primes have lower Kolmogrov complexity than composites on average; $\approx ln({x \over ln(x)})$ vs $\approx ln(x)$. As a side effect you have to have some large gaps between primes; otherwise you could encode every number as the previous prime plus some small number of bits. 

I am trying to track down the name of this digraph and some references: You take all members of the transformation semigroup on $n$ elements, $T_{n}$. For two members $x$ ,$y$ ; if $x$ is in the semigroup generated by y then you put an arrow from $y$ -> $x$ . You would read this as "$x$ is in the semigroup generated by $y$". Alternatively $y^k = x$. What is the name of this digraph? References would be great. If you wanted to visualize it, the symmetric group $S_{n}$ would be one of the connected components. The sinks are idemptent transformations like (0,1,2,3), (1,1,1,1), ... 

The sum of all degrees $d(x)$ in a graph is at most $2e$. The sum of all $d(x)^{\alpha}$ is at most $(max degree)^{\alpha -1}$ times sum of all $d(x)$. Together these give the upper bound. 

In fact the answer is no. (It would be that $\Sigma_{i:h (i) =1} x_{i} \mbox{mod } 2 = 1$ holds with probability at least $1/2-\varepsilon$, if we were working with an $\varepsilon$-biased hash family, and indeed using $\varepsilon$-biased hash functions gives a way to improve the parameters of the construction. But pairwise independence is not necessarily $\varepsilon$-biased.) It seems they are missing one additional step here. To apply Valiant-Vazirani directly, you would need to also randomly choose the range of the hash function. Rather than picking random pairwise-independent $h : [2^k]\rightarrow\{0,1\}$, it seems you should pick random $\ell \in \{2,\ldots,k+1\}$ and then pick random pairwise-independent $h : [2^k]\rightarrow \{0,1\}^{\ell}$. (Here I am deliberately using Arora-Barak's statement of Valiant-Vazirani, found on page 354.) Let $s$ be the number of $x_i=1$. Valiant-Vazirani says that when you have chosen $\ell$ such that $2^{\ell-2} \leq s \leq 2^{\ell-1}$, then the probability that $\Sigma_{i:h (i) =1} x_{i} = 1$ (over the integers!) is at least $1/8$. So by picking random $\ell$ and picking random pairwise independent $h: [2^k]\rightarrow\{0,1\}^{\ell}$, then you have probability at least $1/(8k)$ that $\Sigma_{i:h (i) =1} x_{i} \mbox{mod } 2 = 1$. To simulate the random choice of $\ell$ in the circuit, you could simply take the $OR$ over all possible $\ell$ (their number is logarithmic in $2^k$, after all), so the probability of success becomes at least $1/8$ again. So rather than having $O(k\log s)$ hash functions with range $\{0,1\}$, you'll want $O(k)$ different sets of hash functions (each set having a different range), with $O(\log s)$ hash functions in each set. 

Worst case you have a Kolmogorov complexity issue where you have chosen half of the $k^l$ words at random. Since it is random your CFG has to take $O(k^l)$ space since it cannot compress. 

I came up with a result the other day that arbitrary length Roman numeral evaluation can be modeled as a monoid: $URL$ 1) Is this a known result? 2) If not, any suggestions of a niche journal that might be interested in such a submission? 3) Any known results on the space complexity of finite monoid elements? I have yet to come across a monoid representation with efficient parallel computation that took more than O(log N) space, with N being the number of elements being "added"/"multiplied". Useful monoid data structures seem to be a constant number of counters or a member of a transformation semigroup of constant size; i.e. a fixed length array of size K with elements in 0...(K-1). 

Simple observation, lets use {Animal}{Mineral}{Vegetable} S = A,B,K = {horse,cat,dog}{gold,silver,quartz}{corn,tree, peach} $S_{0}$ ={horse}{gold}{peach} $S_{1}$ ={cat}{gold}{peach} $S_{2}$ ={cat}{gold}{tree} ... Your $n$ can be as big as $|A|$*$|B|$*$|C|$* ... *$|K|$. That's not tractable. An adversary can just start generating you random elements in S. Kolomogrov complexity implies you will be forced to store them all without any compression until you get to that intractable number; making your storage intractable. 

Here are a few additional references. More can be found by looking at the papers that cite these. Duris and Galil (1984) give a language in $P$ which requires $T^2 S \geq \Omega(n^3)$ on one-tape Turing machines with any constant number of read-write heads. Karchmer (1986) showed that the same lower bound holds for the element distinctness problem. Babai, Nisan, and Szegedy (1989) give a very natural language (generalized inner product) that is solvable in $O(n)$ time and $O(1)$ space on a $k+1$-head one-tape Turing machine, that requires $T S \geq \Omega(n^2)$ on any $k$-head one-tape Turing machine. Ajtai (1999) shows time-space tradeoffs for deterministic random access machines computing element distinctness. In particular if $S \leq o(n)$, then $T \geq \omega(n)$. Subsequent work by Beame, Saks, Sun, and Vee (2000) proves time-space tradeoffs for randomized computations. Santhanam (2001) showed that $TS \geq \Omega(n^2)$ holds for multitape Turing machines solving SAT, building on Cobham's analogous lower bound for PALINDROMES. 

Let CNF-SAT be the problem of determining whether a given CNF formula is satisfiable (no restrictions on the width of clauses). 

Overlaying a point cloud on a two-dimensional probability distribution such that the local point density corresponds to the local probability density is a good way to define the cell generators of a centroidal Voronoi tessellation (CVT). CVTs don't loose their quantization precision by translation or rotation in concert with the underlying probability distribution. Not sure what your question is. 

The optimal centroidal Voronoi tessellation for the infinite 2D grid is a hexagon tiling. You can get one of these by placing points at even coordinates on even rows and odd coordinates for odd rows. Depends on the number of points you need. Start with that then run Lloyd's algorithm for a few iterations to tidy up the corners and edges. I would compute it offline on a unit square to good precision, then dilate it to your chosen rectangle with a simple linear transformation. 

Yossi Shiloach, Uzi Vishkin. An O(log n) Parallel Connectivity Algorithm. J. Algorithms, 1982: 57~67 -- One of my favorite papers. It would be interesting if you could do it in O((nlogn/k)/p) space with p processors in $k$ rounds where each round each processor is only allowed to read in O(n/p) of the edges.