I know that value objects should be embedded in the table of the object they belong to but what if the value object is more than a simple object and contains many fields/columns or even sub-objects? I currently have a table named which in short stores rich text. Since I don't always parse the content I've added a caching system. The table also has a 1:n-relation to another table which contains a list of all the links in the table. 

Putting multiple relationships into one table can be useful if those relationships have the same attributes and/or if you want to aggregate data over multiple relationships. It is necessary if the types of relationships are defined by the user at runtime. However this is rarely really the case. In your example the relationships don't share attributes, the relationships even referening two different tables. This makes it hard to enforce constraints and the design is also less intuitive. I would only chose that design if creating tables literally costs money. 

I've defined a unique constraint for two columns: A bigint and a VARCHAR(256) COLLATE utf8mb4_unicode_ci Then this error occured (which I'm already familiar with): 

is to distinguish between real persons, etc. is the name by which the person is mostly referenced. In the case of a real person it is the firstname and lastname. is the name inclusive all academic degrees (if any). A sample for a foreign key: 

Limit does have an impact on how many rows are selected/affected. 543 is just the number of rows which match the joins and the where clause. 

If I would put this data in the orginal table I would follow the rules of database design but many it would make work harder. 

I know that one-to-one relationships can be used to split data into multiple tables for performance or security and that it is used to create a is-a-relationsship. But aside from that things, what is the real-world prupose of an optional one-to-one relationship, expecially one where both sites are optional? 

In my application stores both real and fictional persons (from movies, etc.) as well as stage names and alter egos. These entities have almost the same properties and are often referenced via foreign key regardless if the are a real or not. I'm still concerned if it is a good idea to store this information in one table even if otherwise the model would be more complex. The table : 

We have changed the proc to be more efficient and now its executing under 1 second, but I cannot let go as to what is causing the existing SP to be fast in 90% of the situations, but seriously slow in the others. Especially across 2 servers. Due to the speed and consistency for months, I am baffled as to what might have caused this. I have looked at database settings using sys.database comparing one to another. The only differences I see are Slow setting / Normal Database setting 

I have a stored procedure that is installed on 30 databases across 4 servers. We encountered an issue last night where 3 of these stored procs are now taking over 8 minutes (one 18 minutes!) to execute while all of the others are under 1 second. 2 of the SP's are on databases under the same server, the other is on another server. I have looked at the following and seen no improvement or rational explanation why. 

sp_recompile update statistics review indexes fragmentation compared with successful databases but recreated anyways. change proc for param sniffing problem/solution Reviewed execution plan for anything shocking (estimated rows was WAY off but came back to normal after #2 above drop and recreated procs with no changes watch temp db, as proc uses temp tables nothing jumped out 

Could this have to the with the above settings? The proc (which I can provide if needed, along with execution plan) reads data from tables and inserts into a couple of temp tables and returns a dataset. from existing tables it only performs selects. The tables are no bigger then 250,000 rows. Some databases where the proc is running quick are in excess of 800,000. So the 3 in question actually have a small overall data set. Any ideas of what to check ? What else can i do to figure this out? 

Should I drop all the non-clustered indexes and create them after the process is done? Should I copy the table perform the actions and then replace the existing table with the copy? My hesitation is we have replication and data backups etc. that need to be considered. Should I run with ONLINE mode off, would that make a sizable difference in the length of time? What other steps should I do to prepare for this? I am identifying records we can remove and any duplicate or unused Non clustered indexes. 

Because your hierarchy models is Group A -> Group B -> Group C, it's enough to store in [user_groups_data] table just the user_id, the last group id (in the groups hierarchy tree), and the info So, data in the tables can look like: 

Why not recreating the primary key as (nid, vid), and creating a new index on the column "order" (just for fast retrieval / ORDER BY clause). --> The worst that can happen is to have 2 ingredients with the same "order" value, but the order-change-logic should be already correctly defined in the application. 

Just for fun, and the sake of your question, if are using MySQL, this would be a stupid hack to check if the hourly file has been received on the server: 

This can and will work (theoretically :) ...), but you will need a recursive function (in application or in the database) that can return the parent group id (ex. B-1), or the top group id (ex. A-1) for any group id value given as an input parameter (C-1). But ... like I said: The more info you give, the better responses you get. Hope that helps. 

you can partition the table by certain criteria. Because your using the table for a "weather station", and your data is time series values, your best option would be to partition by [station_id] then by [created] Create a Table_A_Archive, where you can move data that would be to old to keep in Table_A. How long do you intend to keep data in Table_A ? would it make sense to delete old rows that become obsolete for you application 

Your free space in the current innodb file (ibdata[1] ?) is decreasing. It doesn't necessarily mean that your website is slow. Maybe the number of users (concurrent users) has increased in the last few days ? (also causing lots of inserts int he DB?) There is no error in the print screen. it's just your monitoring system, reporting a value that have reached the critical threshold. 

* I'm not an ORACLE Dev, but the function should look something like the one above... Once you create the function, you will create a functional index that will use the function to look for the rows you need: 

In the first use case, you (or the application) do a select to retrieve some metadata about a file; In the second one, you enter an ID (select query) and the application does the archiving (insert statement into table_b); What exactly do you need to automate here ?