Just adding this comment, so it shows up as "answered", but all the credit goes to Martin Smith: Thank you, Martin! Installing SQL2012 SP1 CU8 update did the trick for me! Note that hacking around with files as suggested here did not help 

I have created a small SSIS solution to illustrate my problem. Unfortunately it looks like I can't attach it here, so I will try to explain. I can send the solution to people individually. SSIS SQL Server 2008. I have a package called "Parent_Package" with a single variable: "User::Log_Path". During design time I assigned this value to it: "C:\temp\Log_Design_Time". The package has a log provider configured (simple text file). The connection string of the log file has an expression: @[User::Log_Path] + "\" + @[System::PackageName] + [expression for timestamp] + ".txt". As you can see, I would like to create a new log file for each package execution. If I now run this package, I will have two log files generated! One will have a timestamp from around the time I created the package. This file only has the header fields and no other other information. The second file has the correct timestamp and correct output information: 

You can check the execution plan of the queries and verify that this index is actually being leveraged. The index helps, but since I have to make changes to dozens of publications at once, it still was not enough. So the second part of the fix was to simply introduce a random delay. I added up to 3 min delay to my deployment script and this scattered the queries enough to significantly lower probability of deadlocks (but not eliminate it completely :( Something like this: 

I have done a fair number of traditional data warehouse implementations where the loads were done in a batch-oriented manner, i.e. the data is refreshed nightly or, at most, every few hours. I now face a challenge of creating a system, where the data in the data warehouse needs to be maintained close to real-time (a few minutes delay is OK, but no more than that). I have done a lot of reading and it seems that getting close to real-time has been a trend in DW over the last few years. However, I am having trouble finding specific examples and concrete information about the available tools, which support this kind of "trickle-feed" ETL. It seems that the right tool would be able to read the database transaction log and send those changes over to the data warehouse, while allowing to do some data transformations in-flight. Does anybody has experience with real-time data warehousing and can recommend a good tool or point out a good reading on this subject. Here are a couple of relevant links: $URL$ $URL$ Thank you! P.S. I work in a Microsoft shop, so the source databases are on Microsoft SQL Server. I do have a good handle on SSIS, but it doesn't seem to fit here. 

I have a transactional replication setup between the two servers and I noticed that if a run a statement similar to this: UPDATE mytable SET mycolumn = mycolumn the replication somehow knows to ignore this transaction and it does not get applied on the subscriber. I have confirmed it by running SQL Profiler and also by adding TIMESTAMP column to my subscriber table (it does not change). I suspect there is some sort of mechanism, which enables this kind of "smart" behavior and I was wondering if anybody could shed some light on it. Thank you! 

What's going on? Finally, if I simply remove the original "Log_Design_Time" directory, my child package will start failing complaining that it cannot find the path specified: 

I am looking for suggestions/ideas to the following problem. We have a number of data warehouses for each of our clients (on SQL Server 2014). These data warehouses are maintained fairly fresh with continuous data loads every few minutes. When DW needs to change, this will, more often than not, require a complete data re-load. The challenge is that we can't afford to take DW offline for extended period of time and, consequently, make any applications, which use this data unavailable. It is acceptable if the data is stale, but it has to be available. Therefore, we are looking for a solution, which will enable us to maintain a secondary copy of the data warehouse. Let's call these copies A and B. The secondary copy (B) needs to be kept in-synch with the primary copy (A). When the time comes to make significant DW changes, we would like for the following to happen: 1) stop synchronization between A and B 2) point our applications to the secondary copy (B) 3) perform our upgrade/maintenance/re-load of the primary copy (A) 4) point our applications back to the primary copy (A) 5) re-establish synchronization between A and B (we understand it will take some time) So basically B acts as a temporary "fill-in" while A is being upgraded and reloaded. The solution has to be automatable, meaning, we need to be able to do everything from a command-line/powershell. We looked at some of the existing HA/DR technologies, for example SQL Server Always On, but it seems that breaking and re-establishing synchronization (as described above) may not be a good fit for it. It seems that what we need is a 3rd party software, possibly capturing disk activity at the system level. I appreciate any ideas, recommendations or experiences. Thank you! 

Visually we can tell that, for example, we can schedule projects 1, 2, 3 to run at the same time because they do not share any employees. Let's call this group of projects "Group 1". After that we could schedule projects 4, 5, 6. Let's call it "Group 2". Finally, in our example we have project 7 left and this would be our "Group 3". My question is, how can I write a T-SQL query to perform such grouping of projects? Thank you! 

In real-life scenario, these could be very large tables with millions of rows and I am concerned that this will kill my performance in production. Should I be worried? BTW, all of this acrobatics is done as part of performance tuning exercise for EF queries, so I am prevented from using many of the normal performance tuning techniques. 

Obviously it's still looking for the path specified earlier, but why? Again, I set "Delay Validation" = True on all my connection managers and the package itself. I appreciate any help on this. Thank you! P.S. Here is a complete expression for log path: @[User::Log_Path] + "\\" + @[System::PackageName] + "_" + (DT_STR, 4, 1252)DATEPART("yyyy", @[System::ContainerStartTime]) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("mm", @[System::ContainerStartTime]), 2) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("dd", @[System::ContainerStartTime]), 2) + "_" + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("hh", @[System::ContainerStartTime]), 2) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("mi", @[System::ContainerStartTime]), 2) + RIGHT("0" + (DT_STR, 2, 1252)DATEPART("ss", @[System::ContainerStartTime]), 2) + ".txt" 

I would like to ask for help coming up with a query, which can identify groups on non-overlapping records. Here is a sample scenario (admittedly contrived). Let's say I have employees who are assigned to work on various projects. While an employee can be assigned to multiple projects, he/she can only work on one project at any given time (don't you wish we all had this luxury :). I need to find out which projects can be scheduled to be worked on in parallel because they do not share any employees. Here is some code to setup sample tables and data. 

I am trying to use sp_publication_validation stored procedure to validate that my replication setup is working correctly. This procedure is pretty straightforward and is described here. I am running this command: 

Once again, thank you to Vladimir for providing the formula! The correct way to accomplish what I need is this: 

I just wanted to share what I did to work-around this problem in case someone else runs into it. Once again, the fundamental issue is caused by simultaneous changes being made to multiple publications, which share the same distribution database. After analyzing the code of the Microsoft queries, which are reported by the deadlock trace (see my original post above), I found out that MSsubscriptions table is missing an index, so I went ahead an added it: 

Why does it take up so much space on the disk? I looked, but I couldn't find any way to control the size of the memory-optimized files created. Here is T-SQL Script used for creating in-memory table: 

For my next test, I now would like to change the location of the log files, so I modify the value of the [User::Log_Path] variable in the parent package to "C:\temp\Log_Run_Time". If I run now, I will have four log files! Two for the parent package in the "Log_Run_Time" directory and two for the child package - one in "Log_Design_Time" and one in "Log_Run_Time" directory: 

I would like to close the loop on my own question. We worked with Microsoft support on this and the bottom line is : "it's by design". I strongly believe that this is a flawed design and should be changed. The Microsoft support engineer opened a feature change request for this. In addition I created a "MS Connect" item about this. Please up-vote it if you agree with the request: $URL$ 

It appears that the first file gets created during validation (even though I set DelayValidation = True for the log file connection manager). So my first question is why is this happening? It doesn't seem like the right behavior. Now to take this to the next step, I create a child package "Package_Child". I add an execute package task to the parent package to run this child package. In the child package I add a variable by the same name as above: "User::Log_Path". Next, I add a configuration to the child package, so that the value of "User::Log_Path" is obtained from the parent package variable by the same name. Finally I add log provider to the child package and set it up the same way as described above for the parent package. Let's now run the parent package again. This time I will end-up with three log files: two for parent and one for child: