Open the console In the left pane, navigate to "IP Routing" -> "NAT/Basic Firewall" In the right pane, right-click your WAN interface, and choose "Properties" Go to the "Services and Ports" tab, and check the box for "Web Server (HTTP)" and/or "Secure Web Server (HTTPS)" as appropriate. Put in the server's local IP address when prompted. Click "OK" in the properties dialog, and close the Routing and Remote Access console. 

This appears to be a known issue for WSUS servers that use an authenticated proxy server. Microsoft has release a hotfix for this issue that you can apply to your server. According to one person who tried it (linked above), you must perform the following steps to reset the WSUS content repository after installing the hotfix. This is because WSUS "thinks" in the SQL database that all updates have been downloaded when in fact they failed. To do this perform the following steps: 

where contained . However, I still could not access the web interface. The only way I could find to resolve the issue was to allow outside access to port as well - then access to port worked. However, this means that the web interface is now available from outside on two ports, the default and the one I want. What is the correct configuration for doing this so that the web interface is available internally on and externally on ? 

Note that, despite the name, the option does not transform the schema into a form that is guaranteed to work in the database you chose. All that the option does is remove things that are unique to MySQL so that you can more easily finish the transformation yourself, either by hand or using some kind of script that you have built. From the documentation: 

I manage a large number of PHP applications using front controllers and the following htaccess file: 

AutoSSH does exactly what you need. On MacOS, it can be installed through Homebrew with . See the README for usage instructions, or here and here for some examples. 

That article about WinCache is over seven years old, and is only about PHP 5.2.x and 5.3.x, which did not come with a built-in OpCache that worked on Windows. Now that PHP comes with its own OpCache, you should be using that one. Additionally, the official release notes for WinCache version 1.3.5.0, the first version to support PHP 5.5, say: 

If there are a few specific commands that you need to execute, you can set up sudo (via the file) to allow a particular list of users (and/or groups) to execute those commands without a password. 

(Note: Do not confuse this with Microsoft's program of the same name) 5. You can use a management tool CCK2 is a third-party management tool for Mozilla products. See its documentation for more details. 

I installed MongoDB on a Debian VM using from the 10gen repository. Some time after the install, I wanted to enable authentication in the configuration file, but when I ran , it stopped and refused to start again. I am able to reproduce this reliably: every time I run the server starts. However. when I run , it says 

I would have thought that the path is relative to the .htaccess file in which it is configured, but it seems that it is actually relative to the requested URL. Is there a way to have act the way that I expected? 

There seems to have been some kind of filesystem-level or OS-level permissions caching, because the issue appears to have resolved itself after a complete system reboot (even though restarting IIS () and recycling App Pools really should have resolved this). 

Vagrant's built-in provisioning features should be a perfect fit for this. There are a whole bunch of ways you could do it, but the simplest is probably to take the guts of your Upstart script and make them a plain shell script. Then, put one of these blocks in your : 

I have no idea if the last line is actually needed or not, but users were waiting so I didn't try to experiment. 

To complicate this, there is absolutely no reason that either of the things you mentioned in your question is a "hostname" in the first sense of the word. In fact, many places do not have a computer with the name "webmail", instead having that name point to another machine with a different name. For example, in my old office, "mail.example.com" actually could be on a machine named "mailserver2.office.example.internal". As you can see, the "hostname" in the first sense of the word is not even a publicly accessible address. For your example of "mail.google.com", GMail actually runs on many servers (hundreds or thousands, though no one outside Google knows for sure) so there is no computer with the name "mail.google.com". Google does not publish the actual names of the servers that "mail.google.com" points to. Even when you see "googlemail.l.google.com" when you look up the where it points to, that is also not the real name of the server, but a name that points to many servers. For example, there could be a server named "webmail01-newyork342-version1.googlemail.datacenter-newyork.google.private" which is publicly accessible as "googlemail.l.google.com". Google uses DNS round-robin, region-based DNS partitions, and IP anycasting to make all of those servers respond to the same name. In smaller setups, it is often true that the leftmost portion of the full address is the name of the machine that you are connecting to, but that is rarely the case when dealing with any large company. 

If you want to embed PHP scripting in your Delphi application, just include a system call to the PHP executable, or embed the PHP interpreter using php4delphi. This sounds like a really bad idea, but you could also use a PHP script to call the Delphi CGI, strip out the returned CGI headers, the content, and reset the headers in the PHP output. Only do this is you are very confident that you understand the risks of using . 

Is there a way to get Horde to use the IMAP login for the calendar in CalDAV the way that it works for the web interface? 

In the end, I had to switch the main Horde authentication to also use IMAP directly instead of passing it back to IMP. Here's what I added to : 

4. Mozilla's executable You can create a login script that runs to add the certificate to the user's profile. This forum post has an example script (for Firefox), of which the important part is copied below (with modifications for Thunderbird): 

Open the menu and click 'Options' Go to the 'Advanced' tab Click 'Config Editor' Click 'I accept the risk!' if prompted to do so. Search for Double-click to set it to true. 

I can use it the way it is if that's the answer, but it would make it a lot easier to manage the sites that I have if this can be done. The sites use the same basic code with different themes and database connections, and the way it works now I have to modify the htaccess file every time we deploy a new version of the code (because it is checked into Git). If we can make it that someone doesn't have to remember to make this modification every time, that would be really great. 

Using an "inline" script. Vagrant will copy the contents from the into a script in the directory and execute it. 

Close any open WSUS consoles. Go to Administrative Tools – Services and STOP the "Update Services" service. In Windows Explorer browse to the WSUSContent folder (typically or ) Delete ALL the files and folders in the WSUSContent folder. Go to Administrative Tools – Services and START the "Update Services" service. Open a command prompt and navigate to the folder: . Run the command 

Yes, that is the entire file (for each application)! However, a few of the sites are in subfolders, necessitating the following change: 

I am running Gitlab reverse-proxied through an Apache 2.2 server for my work group. Recently, we wanted to upload some group logos to use as project Avatars. The uploads all completed successfully (confirmed in a directory listing through SSH), but the images never showed up - instead we just got a 404. We are using Apache's mod_proxy to connect to a Unicorn backend. 

This successfully authenticates users against DreamHost's IMAP server so they can log in to Horde. Once logged in through IMP, they can also access Kronolith (calendar component). I am now trying to set up CalDAV support for the calendar, but I can't get the authentication to work. As a simple test, I take the calendar URL from the Horde that looks like this -- -- and I try to use CURL (with option ) to retrieve the calendar. Horde returns the following response: 

It turns out that the Gitlab sample Apache configuration that we originally used (over a year ago) was missing the check for whether files exist before it would go back to the Unicorn server and was also missing the directives to not proxy certain directories (like ), but the Unicorn server wasn't configured to serve these files. Updating the Apache configuration to match the newest sample version fixed the problem. 

is a MySQL-only option (other Database software uses different commands -- for example, PostgreSQL uses a special column type called ). Therefore, when you export with it will not be included in the dump. You can either stop exporting with the PostgreSQL compatibility flag, or you can manually add the option. In either case, if you actually plan to import this schema into PostgreSQL, you will have to modify the dump file to change the column type to for any column that you want to auto increment. 

The name of the computer. This can be in many different formats, including single words, full domain names, and anything in between. The full domain name of a computer, whether the computer does or does not have that full name set on the computer as its host name. 

I would run a script on the VyOS machines that checks the state of the upstream connection and then does / on the other side as needed. The simplest way to do this is putting scripts in (check if the interface that went down is the updated side and bring down the other side of it is) and (check if the interface that went up is the updated side and bring up the other side of it is). Alternatively, you could run a monitoring script once a minute with cron (or Systemd Timers, or any similar scheduler), or you could write it as an infinite loop that sleeps for a few seconds to get sub-minute checking. 

In most cases, the 2.4 equivalent of is to enclose the directives in a block. The equivalent of would be a block. For more information on the changes and equivalents, this presentation (PDF) by Rich Bowen might be helpful. 

The most likely reason for this error is that your server cannot communicate with itself. This could be because your router does not have NAT reflection so your server cannot communicate with itself on its public IP. Try adding a pointing to in your file (or your internal DNS if you have) so that the server knows to contact itself instead of going out on the Internet. Alternatively, this could be because your server is not allowing the WebDAV HTTP Verbs through to PHP. See the OwnCloud setup guide for instructions on making sure these settings are set up properly. 

I edited to remove the argument from the call to and got an error with a stack trace. I can't post the full stack trace (because I can't scroll back in the terminal window to get it), but the last function call before the error was some function that has to do with getting the system language. I had previously set in order to build some ruby gems. I ran and now I am able to start MongoDB. (The interesting thing is that I then again and restarted MongoDB and it still worked.) 

This is a very old question, so I'm assuming you've likely either found a workaround or abandoned the attempt, but maybe it'll help someone else. In order for the WMIC commands listed in the question you are looking at to work, the certificate needs to be in the "Local Computer -> Personal" store, not in the "Local Computer -> Remote Desktop" store. Since you said you put the certificate in the "Remote Desktop" store, the server cannot find it when you run the WMIC command. 

You could also use a provisioning method that works through a "real" deployment and provisioning system, but depending on your requirements that may be overkill. 

You can automate this by deploying a config file to the computers. 2. Deploy a default profile You can add the certificate to your own profile, then copy your profile's file to the main program folder. Any new profile that is created on the computer will then use that version of . Unfortunately, this will not help for any user who has already opened Thunderbird because their profiles have already been created. See $URL$ for more info. 3. Deploy a config file As mentioned and linked above in option #1, Mozilla products support deploying a configuration file in . You can put a script in that file which adds your certificate to the certificate store. Here's an example: 

However, I know that there is very little running on the machine, and nothing else should be trying to bind to that port. What could be causing this? 

I stopped RabbitMQ for a short time on my Windows (2012) server. However, when I tried to restart it, it would shut down again within about 30 seconds. The Windows Event Log had the following message: 

Note that the script here is inside a "heredoc", which means that you need to be careful to make sure that the token which ends the heredoc is in the right place - i.e. on a line by itself, without extra whitespace. (The link about heredoc above does show a way to allow whitespace, but you need to be careful anyway.) Pointing to a script file. Vagrant will copy this file into the directory and execute it. 

Because Thunderbird is based on the same platform as Firefox, you should be able to use the same tools as Firefox would use. There are several options of tools you can use, in order of simplest to most complex: 1. It may be built-in! Thunderbird has experimental support for auto-importing certificates from the OS Certificate store. Here's how to enable it manually: 

On my Windows 8.1 machines, the so-called "April Update" from KB2919355 was installed automatically by Windows Update, as expected. However, on my 2012 R2 server, the update was not automatically installed, and Windows Update says "no updates are available". I know that I can download and apply KB2919355 from the Microsoft Downloads center, but missing this update makes me worried that this machine may be missing other updates as well. The server is updating directly from Microsoft, not from WSUS, and there is nothing else that I know of which could be blocking the update. The machine does have the prerequisite update from KB2919442. How can I find out why this update is missing? What can I do to make sure this doesn't continue to be an issue with other updates? (I wish I had access to another 2012 R2 server to confirm whether this is an issue specific to this machine or not, but my other Windows servers are running 2008 R2 or 2012 original, so this update doesn't apply to them.) 

Note that the path here is relative to the location. However, the file is not executed from that location, so if you need the file to be in a certain directory, you should follow method #3. Using an "inline" script that points to a script file. This way, you can execute a script in a chosen location (in the example below, that's in after you have done prerequisites like or setting environment variables. 

I am setting up a PHP application on IIS 8.5 on Windows Server 2012 R2. I have successfully set up this application before on several Windows Server 2008 R2 and Windows Server 2012 machines, but this is the first 2012 R2 machine I am using. The PHP application has and directories that it needs to write into. On all previous machines, I have set up the PHP site to run in the Default Application Pool, and then given permission to the group to read the entire site and to write to those directories. So far, this has worked on all of the previous servers I have used. However, on this new server, PHP still says it does not have permission to write to the needed locations. I checked in Task Manager to be sure that the IIS process (and the PHP CGI process) run as the expected user and find that they run as , as expected. However, the permissions don't seem to have worked. I tried stopping and restarting IIS, recycling the App Pool, and adding the user permissions directly to the folders, none of which have helped. What is wrong here? Why does this not work when it worked with no trouble at all on four older servers?