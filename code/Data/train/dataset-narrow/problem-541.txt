But this seems useless to me. I mean if I set serialized to a client connection then I would want all currently running transactions to be sequential, right? Otherwise what's the point? 

id is the PK of the table. These statements execute without blocking each other. Makes sense since they refer to different rows. Next: 

Can someone please explain why the delete shows a different number of rows than the count? I am using the same join. Are they not equivalent? 

When using triggers, if an update is done to a table then a trigger is executed. This is very convenient. But what I would need is to execute an external script. Is it possible to configure MySQL somehow so on a trigger/change of a value in a table an external process/scrip is executed? 

What I am doing is something like: This way I pick up each request one at a time. Then I do (the id=123 assume it is from the SELECT request). This works but I am not sure if this is another approach. What would be better? E.g. doing a LIMIT 100? My concern is that the order by is not a good idea as the data grow. Does it make an impact if I always process e.g. per 100? I still have to order by right? 

I can not understand or visualize how this is formed in the HD. I think that 4 * 5 = 20 physical files are created. Is this correct? And what data are stored together? Is there a "clustering" of 2008 dates for all customers with the same hash? I am confused on that. Also I am confused how the data are fetched. Is the customer_id hash used first to determine the range partition or the opposite. What are the exact steps to fetch the data? UPDATE Another example. I did the following: 

Is there a reason for that? Composite PK might have performance hit, specially if you are using InnoDB engine, and if you have many updates on the table (insert/update/delete) 

Here is a way to get the name of the month in French, then you can use CONCAT to construct the date however you want: 

It looks like 'page swapping' is the problem here. When you deal with one table at a time, its indexes and parts of the table are loaded into the RAM. If you keep dealing with the same table, it will be fast as the table already exists in the ram. On the other hand, if you keep changing the table that you are dealing with, then the data in RAM has to be written to the disk, and the second table's info will be loaded to RAM. This is one of the MOST EXPENSIVE operations. Solutions!! Not straight forward though: You may distribute your DB, or run your scripts sequentially. Hardware upgrade may make the process faster. 

since is defined to be unique, then IS unique, so you only have to define it as , not . [This is more helpful while inserting though] Having lots of values is not recommended. It is not good for performance. If possible, add instead. However, this is highly dependent on your requirements. If there are lots of (parent_id, child_id) = (NULL, NULL), then create a new table with these two fields only, and link it to this table. 

I read that I can have different isolation levels per connection and in the server. The default isolation level is REPEATABLE-READ. So if I have a transaction that issues a to the client connection, how does it interact with other transactions from other client connections? Do they all end up being serialized? From set transaction seems not: 

When having a table in SQLite that has a data type, is there any implication if the length of the contained string differ significantly? 

Let's say I have an entity which has a location as part of its attributes Now a can have N of other that are close by. So seems to be a self referencing relationship 1-N (optional) If we know before-hand which exact are close by each other what would be the best way to represent that? Since we have a self-referencing 1-N relationship I think another table would be needed In this case we would store e.g. etc to show that pizza store with id 1 has 22, 23 and 78 near by etc Now in order to get these rows back in-order I would need to create a PK and query based on that. I was wondering would an auto-increment guarantee the insertion order? Or would I need to use a float representing the distance e.g. (where 2.04 is the distance in miles) I was also thinking is there a better way than this? We know that if is close to then is also close to right? Is there a more efficient way to represent this information? I think it would suffice to store just the row to capture that is close to and that is close to . But this way we are losing the order Update: All the answers are very useful. One thing though that perhaps is not clear. I already have the order of the entities based on the distance on insert time. I.e. I have a set of ids of objects already sorted based on distance and I want to insert this set in the database in a way that I can retrieve the rows back in the order I have inserted them. I want to avoid sorting on distance on retrieve since I already had the order at insert time. These ids would be used to join to another table (they are ids) and I want to retrieve the rows in the insert order 

Instead of , try selecting the fields you need (unless you need all of them!): Try to have the fields you are selecting in the used index. This way you will have a , so the data will be read once only from the index If you need many fields to be returned, you may first select the IDs using the query you already have, then send another query like this: [In total, this may not be much faster, but try it to see if the difference is significant] 

I think this is due to the partial index. i.e. the index has only 8 char of It estimated that the number of matching first 8 char will lead to a second read from the table to compare the full value from first table with the full value in the other table, so the optimizer decided to skip the index, and deal with the table directly. 

It is the big number of indexed fields in each index. Remember that the update will update the table and also the index tables that hold affected rows, and in your case, all indexes will be affected. As a side note, your indexes are 'bad'. 

Therefore, for replication purposes, slave's binary files are not needed. And, yes, you can disable binary logging or purge these files if not needed for other purposes like "backup". 

My recommendation is that you talk to your admin and work both as one team, not as two separate teams. HTH 

OR do I need any transactions or locks for the DELETE vs INSERT/SELECT? It seems not but I was thinking I may be misunderstanding something Note: My question is not about transactions and their benefits.My question is if 2 statements from different processes on the same table SELECT/DELETE need for any reason to be synchronized 

I know that transactions are meant to group several operations as one. But if for example in the same table one transaction does 

Databases use the file system to store the data. As far as I know it is not possible to delete a record from a random access file. So does that mean that when we do a the size of the table i.e. the file that stores the table never decreases? So databases essentially keep growing and never reduce in size? 

I read somewhere that InnoDB can store all the tables in a single file or in separate files. Is this a configuration option? What is the default setting? A file for all tables or a file per table? What is the recommended setting? 

If you have a big table that you want to partition and you have 2 candidates as the partitioning key (in the sense that 50% of the queries use one column and 50% of the queries use the other column and almost no query uses both in the same SQL query unless all the code is rewritten to do that) how can one determine which key is the best candidate to be used? Does it matter for example if the "entity" that one key represents is "fixed" and the other is constantly increasing? E.g. as an example of what I mean "growth" (taken just as an analogy of what I am asking) if you have decide between using the doctor id or the patient id as the partitioning key where we can have only so much doctors but infinite number of patients 

Create the result table, , without index, and when it is populated, add the desired index. Make sure you have an index on (name, type) in both tables. i.e. a composite index It is not a good idea to have primary key on all fields. This is especially true if you are using InnoDB engine. Instead, alter the table to add an integer auto increment field as primary key, and if needed, add a composite unique index on the fields you want to be unique. 

One way to do it is to update and select in the same statement; and add a condition to the statement so that (if selected, don't select again) Here is my way: 

IMPORTANT: drop the other index on (The name of the index you should drop will be displayed if you run "") Other factors that could affect the performance: 

Replication may fail because of different factors. You don't want to shot down the site each time replication fails. Instead, you should think of a solution for this situation: when replication fails, how to keep the site up. It seems that you have master-master setup for the replication. I suggest to let users (application) connect to the closer server when they are reading, and perform all writing operations on one master, call it "active master". This way, you gain the speed of reading, and you know that the replication is going in one direction. If the replication fails, you fix it while the site is still up. On the other hand, if the master fails, you switch masters: becomes the , and you fix the failed one, then resume the replication. Having said that, there are some situations where you may want to shut down the site. HTH 

Now if this snippet runs concurrently by 2 transactions, T1 and T2 what will happen? I know that in MySQL by default the isolation level is . So let's say that amount initially is 500. So when the 2 transactions finish will the final amount be 300 or 100? I mean when they start both read from table which I guess is a shared lock. When one updates the table it gets an exclusive lock right? So what happens to the other transaction? Will it proceed with the amount it "view" when it started i.e. 500? 

What happens to a transaction when the client connection is lost. Example the client is a web application code that starts a transaction to a remote server instance. The client sends the sql code to the server and waits for the transaction to complete. What happens if the network connection fails? Does it depend on the point we are in a transaction? Is it irrelevant? Is it aborted? 

These block each other but not always! I can not understand this. These refer to different rows why do they block each other? 

I have a rather basic question on transactions. Assume a table with a column amount. Now we have the following code: 

I have noticed that even on huge tables with millions of records when I do a the index is created immediately (I get the console back in nsecs). Why is this? Doesn't it create the B-Tree with the table data at that point? I can only assume not due to the immediate return of the . But then how is the index created? On each access? 

You may use REPLACE INTO. The disadvantage of it is that it creats high IO, as each existing record will be deleted and then inserted (as opposed to being updated). Try loading the new rows' IDs into a separate table on the destination server, then run a delete on the destination joining this new table with the existing table using the ID. After that you run your ETL with s only 

In MySQL, or almost any other RDBMS, it is common to have an ID for the row (object), so you use it in other tables to make a link between the tables (Relations, or PK and FK if you prefer). This is part of the normalization process that is recommended for most types of DB design [there are exceptions though]. In MongoDB however, the general way or design is to denormalize. i.e. the document (row) is stored with all its related objects. To make it more clear: In MySQL you would have two tables: and . In cars you only store the ID of the type of this car. In Mongo DB, you would have one collection called s, and the document in this collection would have a sub-document that contains the type's info. (You still can have a separate collection for , but you don't store only in car's collection) HTH 

I have the following way to get the IDs of the rows that have at least two not null values; however, I am afraid this is not efficient when it comes to few million rows per table. I'd like to share it anyway: