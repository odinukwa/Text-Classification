The first answer was $T^{2}(n)$ , where $T(n)$ is the running time of the TM being simulated, with an improvement quickly provided by Hennie and Stearns to $\log T(n)$, which is the best current answer to the best of my knowledge. This problem is still open and an improvement to it would improve many results, with the most important perhaps being the gap in the deterministic time hierarchy. However, research on the subject suggests that the $\log T(n)$ gap is necessary. 

Also for multi-head machines, from "Linear time simulation of multihead turing machines with head — To-head jumps" by Walter J. Savitch and Paul M. B. Vitányi : 

Different cases make more sense when talking about algorithms, not problems. On the other hand, complexity classes are about problems , not algorithms. Therefore, the complexity class always being a worst-case for any algorithm is due to the definition. In complexity, your goal is to know the number of resources needed to solve any instance of a given problem. Therefore, you know that for any given instance and algorithm, you are going to need those resources and nothing more. In algorithm analysis, your goal is to ensure your algorithm has an upper bound for a resource, in any instance of the problem. A trivial bound is the complexity class of the problem, since no useful (one that does make uneccesary steps) algorithm takes more time than that. However, you can improve that bound given the specifics of the algorithm. For example, let's say you are analyzing mergesort. Given the solution, you can confirm it in polynomial time, therefore SORTING is in NP. However, by analyzing, you can lower this down to $\Theta$(n*logn) As for best case, it is trivial for every problem to find the least number of resources needed. Assume that the input is of length O(n) and the output of length O(m). Then the following TM M always runs in O(n)+O(m) for the best case : M{Input,Instance,Solution} 

Let's take this step by step, so we are not confused by that small tower of oracles. $X \in NP^{PosSLP}$. Since $PosSLP \in P^{PP^{PP^{PP}}}$, we derive that $X \in NP^{P^{PP^{PP^{PP}}}}$. We want to prove that $X \in NP^{PP^{PP^{PP}}}$. Since $NP \subseteq NP^{P}$ and due to oracle properties, adding the power of the same oracles to both hands of the relation preserves it. Therefore we have: $NP^{PP^{PP^{PP}}} \subseteq NP^{P^{PP^{PP^{PP}}}}$. So yes, the problem is in $NP^{P^{PP^{PP^{PP}}}}$. 

There are other models I know of, but I think they simply expand on the ideas I've presented here or are pure mathematical constructions, so they are more like "neat tricks" than something that could disprove the Church-Turing thesis. 

Note: It is also not unusual that papers assume that $\mathbb{PH}$ does not collapse in addition to some other hypothesis, e.g. the (generalized) Riemann hypothesis. Then, the contrapositive simply shows that at least one of the hypothesis is false. 

Definitely not. We do not need yet another academia alternative. Certain people have already provided those alternatives, providing both amateur and professional researchers and society with false hopes of a high-tech future (I have a particular venue in mind but I'd rather not mention it). The academic system was forged after centuries into what it is. I see attempts to skip the peer review system as simply "skipping the line". If an amateur researcher does not have the skill to understand why her proof is wrong, either alone or with some comments from reviewing, then there is a simple solution and that is that she must study the subject at hand harder. This is why educational institutions such as universities exist, to certify the knowledge of a certain person. If that person happens to be a genius that grasps the knowledge of the field instantly, fine, award that degree as soon as possible. But the vast majority must and should undergo training for their own sake and for the sake of society. What would you think if this line of business was taken from mathematics and was put on medicine or another field where the consequences are more immediate? How long until an untrustworthy businessman on purpose alters the amateur papers so that they are hard to review or can fool the reviewer? Fooling the customers-amateur researchers is bound to happen, as with all businesses that rely on hard to get knowledge. You can see that my point is that there are functional problems before even delving into the technical details. And since I am making an engineering argument, "programming" is a good example of what happens when amateurs are trying to do serious work with little or no training. The last thing any scientific community needs is the media glorifying the next "self-taught theoretical computer scientist who challenges the foundations of the whole scientific community from his garage", let alone a fragile science as ours, which still tries to get the recognition it deserves. 

First of all, Cook actually showed that the problem of whether a logical expression is a tautology is $\mathbb{NP}$-complete under Cook reductions. The proof however works by replacing them with Karp reductions to show that $SAT$ is $\mathbb{NP}$-complete, in the modern sense of the term. As for whether Cook understood the significance of a $\mathbb{NP}$-complete problem not being in $\mathbb{P}$, going through the actual paper shows that he did. However, I believe it wasn't until Karp's list of 21 complete problems that the practical significance of Cook's result began to be understood. 

So you have discovered that TMs cannot solve every problem! The very first step Turing took and is highly logical (although not trivial if you consider the state of computing at that time) was oracles. Informally, you are adding to your machine a new black-box module that can "somehow" solve the problem your machine cannot, let's say the halting problem. Of course, oracles are just a mathematical abstraction and there is no secret behind their inner workings. Personally, I don't see any way that an oracle can be used to discover a model that disproves the Church-Turing thesis. 

Any natural problem with a lower bound of $DTIME ( f(n) )$, where $ f(n) = \Omega ( n^{2} \log n) $. By the DTIME hierarchy theorem, it requires $\omega( n^{2} )$ time. I believe there exist a handful of these. Any natural problem with a lower bound of $NTIME ( f(n) )$, where $ f(n) = \omega ( n^{2} )$, by using the NTIME hierarchy. I am not aware of any such natural problems. Any natural problem with a lower bound of $SPACE ( f(n) ) $ , where $ f(n) = \omega ( n^{2} / \log n) $. This is justified by the TIME-SPACE separation. I believe that 

Let's see two different cases why two different persons would like to prove a problem $NP-complete$: a) You are working on a software project. Having specified your system , you are starting to define the architecture of your application. This includes breaking down the large problem/need the application serves to smaller problems. Let's say that you have been given the task to find an efficient (we don't want our application to be slow!) algorithm for one of those smaller problems. After struggling for some time, you cannot find a polynomial algorithm. Then you might think: maybe this problem is very hard, so it's very difficult (or even impossible) to find an efficient algorithm. By proving that the problem is $NP-complete$, you have some evidence for this conjecture of yours and you should start considering an alternative approach (e.g. altering the problem so it becomes easier). b) You are researching complexity theory. By definition, you want to characterize problems (or classes of problems) according to the number of resources needed , i.e. the difficulty to solve them. By proving that a certain problem is $NP-complete$, you gain some insights: i) You know have a vast knowledge of the problem. Instead of working on a single problem, you can work with the class of $NP-complete$ problems, which might lead you to new insights. ii) You allow researches that want to prove $P=NP$ another way to do that. Maybe your problem is easier to attack than $3-SAT$. iii) On the reverse direction, you can use your new problem as a representative of $NP-complete$ problems. By studying it, you can perhaps understand why this problem is so difficult to solve (= does not have an efficient algorithm) and apply this knowledge to all other problems in the class. (that's what Deolalikar tried to do with the $CLIQUE$ problem) Summarizing, characterizing a problem allows you to use common techniques. By studying the class it is related to, you can think in an abstract level, without bothering about the specifics of this particular problem, which is common in mathematics and science in general. Working with classes instead of individual members allows you to use known techniques and furthermore, apply your insights to a larger number of objects, instead of only one. 

By definition, $\mathbb{P} = \mathbb{NP}$, since any $\mathbb{NP}$ problem could be solved by answering the question "Is the # of accepting paths non-zero", which by assumption, can be calculated in $\mathbb{P}$. As a consequence, the polynomial hierarchy collapses as well. You also get other results by using results or their contrapositives. 

My argument is not the most sound one, I would call more of a proof idea. My idea is a variation of Ladner's theorem, where instead of padding we use alternations of quantifers. All levels of $PH$ have complete problems with a constant number of quantifier alternations. Consider now quantified formulas with at most $f(n)$ alternations where $f(n)\in o(1)\cap \omega(n)$, i.e. a sublinear non-constant function. For every such $f(n)$, let $f(n)-SAT$ be the language of satisfiable quantified formulas with at most $f(n)$ alternations. It is apparent that $f(n)-SAT$ is in $PSPACE$ as it is a special version of $TQBF$, however I believe it is not in $PH$ and it is not $PSPACE$-complete. Suppose that for some constant $f(n)$, $f(n)-SAT$ is $PSPACE$-complete. Then by definition,TQBF is Karp-reducible to $f(n)-SAT$. Thus we have a reduction that can limit the number of alterations from at most $n$ to at most $f(n)$. The same reduction, perhaps applied repetitively, could be probably used to reduce $f(n)$ to a constant number of alternations, thus it would be in $PH$. That would lead to a contradiction of the hypothesis that $PH \neq PSPACE$. Therefore, $f(n)-SAT$ canoot be $PSPACE$-complete and since such a reduction does not exist, neither in $PH$. One weak point of this idea is that perhaps the reduction can reduce the alternations to a small function, that is however in $o(1)$. 

I believe an appropriate term is "a Turing Machine physical implementation". The main problem with any implementation is how to provide "infinite tape" or in a more abstract level, infinite memory. An easy solution to this problem is to use a special symbol to indicate the last tape square. When a Turing Machine reaches it, it enters a special state which requires user intervention, who is supplying extra tape. Then, the TM can continue its operation. Unfortunately, such implementations being "physical" involve physics. If the universe is finite and due to the Planck scale, there is a finite amount of tape available. This is where problems arise that perhaps cannot be answered by computer scientists but by physicists. Note that physicists have not reached a conclusion on those matters, which are considered major open problems of the magnitude of $P \neq NP$, so it would be unlikely that a computer scientist would resolve them. You can read more in Scott Aaronson's paper NP-complete Problems and Physical Reality , especially in the Analog and Relativity computing section. You can also find a lego implementation (with finite tape) in the following page: $URL$ 

Scheduling algorithms. They are used by every user device and server worldwide. A number of variations is being used, I found a lot of references for "multilevel feedback queue" 

About your second question. No, that would not imply $P \neq NP$. Hierarchy theorems are mostly useful to determine the amount of a single resource needed by a TM so that additional problems can be solved. For example, we know that $DTIME(n) \neq NTIME(n)$. Let $f(n) = n$, $g(n)$, $h(n)$ such that $f(n+1) = o(g(n))$ and $f(n)log(f(n)) = o ( h(n) )$. From the hierarchy theorems it follows that $ DTIME( f(n) ) \subsetneq DTIME( g(n) )$ and $NTIME ( f(n) ) \subsetneq NTIME( h(n) )$. Under those assumptions, $ NTIME( g(n) ) \subseteq DTIME( h(n) )$ is possible. The hierarchy theorems can be used to determine relationships between resources, given an equality between them. For example , assume that $ NTIME(2^{n}) = SPACE( n )$. We know that $NTIME( g(n) )$, for $g(n)$ such that $2^{n+1} = o(g(n))$, cannot be equal to $SPACE(n)$ , due to the NTIME hierarchy theorem.