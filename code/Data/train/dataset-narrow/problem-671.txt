is not going to see the not-yet-inserted row, which is why it returns true on this second INSERT in your example: 

So I wouldn't trust it to be a great source of advice on PostgreSQL in particular. Every RDBMS can be surprisingly different! I'm a little confused about your original question, but here's an example showing that section of the book is not 100% correct. To avoid further confusion, here's the whole relevant paragraph, you can see it in Google Book Search. 

Because you have given two trailing options to this command: and . Since neither the nor option were given, assumes the database name should be the first argument and the username should be the second argument, per the documented Synopsis of the command. Second, to answer your other question: 

You are presumably looking at the raw return of , which is not a 32-bit value like the XID Limits you are looking at. Instead, it is 

According to Luis Carvalho, one of the developers of PL/Lua, PL/Lua can use LuaJIT. However, you may find there are many more factors which affect the overall performance of the PL code you write, including the overhead of the language's bindings in PostgreSQL, data type conversions, familiarity of your developers with Lua vs. other languages and their ability to write performant code, and many more. There's some more discussion about performance of the PLs you may find interesting. PL/Lua did do well in a benchmark done by Pavel Stehule. 

You may also put the line in the file , however it seems ~/.psqlrc directives are honored only in interactive mode, not for commands passed in via . More great tips for non-interactive psql usage from Peter Eisentraut. 

What happens when a hits is that dirty buffers held in are guaranteed to be written (i.e. fsync'ed) to disk. The WAL files must have already been fsync'ed to disk at COMMIT time, assuming synchronous_commit=on and fsync=on, etc. Your question of: 

(awesome for making sure that schema migrations go in either all-together or not at all.) You said, though: 

Sounds like your terminal's pager is taking over the output: you can tell psql not to send its output to the pager when you're using the invocation via: 

For Question 1: using md5 auth for local connections is particularly helpful when you have a multi-user machine, or if you might add additional user accounts for friends/family/coworkers/whoever someday. If you are and will remain the only actual user on your system, there is less benefit, but it could still be useful: suppose some bad guy manages to break into some system account like 'www'. He may not be able to do a whole lot from that account without root privileges. But if you've left your pg_hba.conf open to 'trust' all local users, the bad guy can do anything to your database, probably including taking over the 'postgres' user account as well, and moving on to attempt to root your system, steal your data, or whatever from there. For Question 2: You could use a .pgpass file with, say, only a 'readonly' user's credentials, so if a bad buy took over that account he wouldn't have superuser-level access to the database. If you do store the password for the postgres user in your ~/.pgpass, then yes, it would be "game over" for your postgres instance if a bad guy took over that local user account. Again, that documentation was written with the possible use-case of a multi-user machine in mind, e.g. different local users might have or use various Postgres user accounts, not necessarily superuser accounts. 

It's not clear why monitoring pg_database_size(your_database_name) doesn't give you the information you are after. Plot the size of all your databases daily, fit a curve to the plot, and you should be able to come up with an estimate of when you need to buy more disk space. What else do you need to know, and why? For this part of your question: 

See this post, which links to a big query summarizing all the indexes which may not be pulling their weight. 

If you want to see the race condition in action, run that same SQL through two psql sessions at the same time. You'll see that both sessions will think that the user has enough coins to purchase an item, and will both record a purchase. (Note, the exact same problem would exist with a CHECK constraint on the table with transactions.) How to fix this race condition? Read up on transaction isolation levels, you probably want to be using for important accounting code like checking account balances and recording purchases. Another option is to use SELECT ... FOR UPDATE to obtain a row-level lock on, say, the table while you're checking the account balance, which will ensure that two transactions can't be performing that same delicate check at the same time. 

If that doesn't work for you, it would be helpful for you to post a lot more information such as a minimal testcase and EXPLAIN ANALYZE showing how slow the query is for you. 

Guess I don't need to tell you that this is a seriously awkward data model. Anyway, I think this query would do what you're looking for: 

You don't really need to concern yourself with how many rows are in each table or the precise structure of the table. Just query and you have the answer, in bytes. 

There's probably a more concise way to write the above, perhaps using DISTINCT ON and ORDER BY to save having to fetch the price for the MAX time in a separate subquery, but I'll leave that as an exercise for the reader. EDIT Alright, here's a simplified version which I think should work out to be equivalent but much faster. 

This is Postgres 9.3 by the way, but I believe the results would be roughly similar on 9.1, although it wouldn't use an "Index Only Scan". Edit: I see you've clarified your original question, and you are apparently wondering why Postgres isn't using an index in a simple example like: 

Looks like you've figured out question 1 for yourself already (short answer: yes, use the latest 9.1.x release, and make sure the compile-time options are the same between the version on the old and new machine to be sure the data directory, and the machines should ideally be as similar as possible in order to be binary-compatible, e.g. both x86-64, similar glibc versions, etc.). But about question 2: 

In your example, the you have set of 256MB should be seen by the subsequent command, because you have changed this GUC inside your session. In fact, the docs suggest bumping up (as you showed) for just such a purpose. 

has some logic by which it determines whether it is safe to keep and reuse the password you entered initially, see the logic about in command.c. Presumably when you are able to use the meta-command to reconnect from within an existing session, is preserving your initial password and reusing it -- either that, or your pg_hba.conf rules allow a connection without a password with the given user/database, e.g. a trust or ident rule. 

Well, very roughly speaking, at steady-state (i.e. after you have a standby server initialized with a basebackup and in-sync with the primary), the amount of bandwidth needed to keep a standby in-sync will be roughly your primary's WAL volume throughput. Now, what is your primary's WAL volume throughput? Basically, how many 16 MiB WAL files your primary server produces per unit of time. You can either poke around in your primary's directory and see how many new files are being churned through. Or here's a nifty shell command using , credit of depesz that you can use: 

If you must construct that SELECT query separately (e.g. because the actual query you are comparing against is significantly more complicated than just ), I would put your query against in a CTE, so that it only needs to be evaluated once, something like this: 

H/T to Erwin for this tip. Remember that these XID values wrap around at 2^32. For what it's worth, I think that blog post you linked to is excessively complicating this topic. To watch out for XID wraparound, you really just need to check: 

The comments so far are roughly correct, but to give an authoritative answer from looking at src/backend/tablecmds.c: If you're only performing , then will be invoked to handle the , and it uses to perform a WAL-logged block-by-block copy of the table. However, if you were to specify additional actions to the command which require table rewriting, then the new copy of the table should be built (and compacted) in the new tablespace via . 

And if you don't have to write that as a separate CTE, you could of course just LEFT JOIN directly against instead of . 

Interesting question! Short answer: no. Long answer: there does not appear to be any existing way to get a list of savepoints defined. Even worse, it doesn't seem possible to create a PostgreSQL extension which would let you do this: looking at src/backend/access/transam/xact.c, you can see that functions like RollbackToSavepoint (which is where that "no such savepoint" error message you mentioned comes from) rely on the variable CurrentTransactionState, which is declared static to xact.c, i.e. would not be visible globally to extension code. Now, if you were daring and quite desperate to generate a list of defined savepoints from the server-side (as opposed to just having your client remember...), you could add in a helper function to xact.c that would display this information for you. In fact, here is just such a patch. That's a very rough patch for illustration purposes only, and just elogs the savepoint names, it should really be returning those names as setof text. As to why this feature is missing, I surmise that there is simply no plausible use case for a client needing to fetch a list of defined savepoints from the server. What would the client do with this list -- just choose one at random and to it? to the last one blindly? AFAICT savepoints are only useful if a client remembers what savepoints it has defined and where they were in order to be able to make use of them. 

if you had the server running. I don't know of a trivial way to determine the database name from those OIDs without having the server running, but at least you know how many databases there are and how big they should be. And you should be able to figure out creation time too from checking . In my case, "1" was for "template1", "12292" was "template0", and the rest were various other databases. 

You've got to be really careful in cases like these. The default transaction isolation level PostgreSQL uses is , and in that mode you can easily get nasty cases where two transactions are performing checks like this: 

Try adding the "-w" flag to your command, so that should only return once the server is up. If that worked, the problem was that your command was running too quickly after , before Postgres was actually up and accepting connections, when those commands were executed in a script as opposed to manually. (Re-posting comment as an answer, as requested.) 

Well, one immediately obvious downside of the table design you posted is that there are no constraints on or ensuring that you don't have more than one such row in each of these tables for each user. And even if you remember to add that, you will still have the problem that it will be difficult or impossible to enforce the rule "every user must have a role (or other critical attribute)". Where it typically makes sense to break out metadata like this is when you have either many-to-one relationship of this metadata (e.g. a single user may have multiple roles), or you have a lot (i.e. many columns) of optional metadata that is unwieldy to cram into a single table. 

If you'd like a snapshot of your primary database refreshed nightly, you could do this with a cron job restoring RDS snapshots every night. I don't think RDS has a button to do this automatically for you, but it shouldn't be too hard to script up a nightly create-db-snapshot + restore-db-instance-from-db-snapshot using the AWS CLI, or boto, or whatever interface to AWS you like. You could even maintain a Route53 entry which would always point to the most-recent instance, and leave the old instances lingering for a day or so before being killed off, so that sessions running against existing instances overnight wouldn't be interrupted. 

Each of those directories with an integer as the directory name represents a database inside my PostgreSQL cluster. The integers (OIDs) in the directory name match the you would see from a query like: 

You'll have to create individual columns (e.g. ) and have a trigger function enforce that the column is set correctly to achieve this. A more flexible and useful approach is usually constructing an audit table, with a trigger on the "user" table saving the changed columns for every UPDATE, or the contents of the whole row for every DELETE. This is where json, jsonb, or hstore types come in handy, if you'd like a single audit table capable of dealing with several tables, or capable of dealing with many columns from a single table without needing to know the structure of the table. 

And, in theory at least, any of super1's privileges would be transferred over to super2. However, I'm not sure your question makes much sense, since superusers generally are allowed to override any privilege checks. As the documentation explains: 

So canceling that , either through or a Ctrl-C issued from the controlling psql prompt, will have a similar effect as if you had done 

I'd be interested to hear what privileges your existing superuser role had which were not automatically granted to whatever new superuser role you created by default. Edit: and if you're interested in copying over the per-role configuration parameters (i.e. those documented under configuration parameters), then you could use a function like this (demo only, you may need extra error handling, security considerations, etc. for production use):