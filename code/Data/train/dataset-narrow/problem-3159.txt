Models with direct regularization on their weights benefit from this. These regularizations add a prior to the model and punishes high weights. If your variables are not in the same range then the regularization is not the same for each input. Search for weight regularization. Another form is where in neural networks the weights are initialized in a way that they expect the inputs to be normally distributed. If they are far from that, the impact in the later layers can be fairly big, greatly impacting convergence and numerical stability. On the other hand, tree based methods don't care at all usually. 

Interesting theoretical question. While I cannot answer this with 100% certainty, my own intuition and experience say: 1) I would be very surprised if this is not the case. If we look at different problems, where we have one very important feature that holds most of the information but some other features with some signal in there the model generally still improves by adding these lower value features. That is exactly what you are doing here, you have the MNIST pixels that individually hold little value but combined tell you quite a bit. The value of these pixels goes down quite a bit by adding such a strong feature, but the value is absolutely not reduced to 0. I think in theory this should lead to a strictly better model. 2) I think it will take around the same time to converge on average. Fitting to the one-hot encoded feature will be very fast which means most of the time will be spent on fitting to the features, which is a similar problem as without the feature. However, instead of going from 10% to 99% accuracy, we are now kind of going from 95% accuracy to 99.7% accuracy. I'm less certain about the second question but I think this is what you would find if you do the (interesting) experiment. Let us know if you do implement it! 

Probably because their forecasting algorithm or method requires that data is inputted in a specific format, like most forecasting or classification algorithms do; different algorithms will have different tolerances to data issues like missing values or outliers. So you process a piece of data that is unstructured with the goal of using it, usually by adding it to a data model or inputting it as data for a report or as a feature for a predictive model. PS: I'm curious if anyone knows a case where you just throw unstructured data at a predictive algorithm (e.g. a neural network) and get something meaningful or useful out of it. 

Let's start with your problem definition: "a good strategy make the relationships first and then count the occurrences". That is, roughly, the basic strategy that market basket analysis algorithms use. However, algorithms like Apriori or FPGrowth are specially designed to analyze such datasets (at scale) and infer the inherent association rules between items across all baskets. My recommendation would be to use one of these to gather the relationships between items purchased, instead of reinventing them; especially because you'll face a lot of the hard problems these algorithms already solve (namely the large search space when generating combinations of basket items). You can use any of several libraries or languages to do this, namely R, Python, etc. Doing this in Spark would be pretty simple using MLLib, your workflow would be something like: 1) choose an algorithm, e.g. FPGrowth; 2) prepare your data to fit the format required by FPGrowth (each transaction should be an array of basket items); 3) run FPGrowth and output its frequent itemsets. There's a good example of this at Spark's website: 

You sample a random mini-batch as opposed to the full dataset. This means that you get a stochastic approximation of the true gradient of your loss function with regards to your dataset and weights. 

If you want multiple things out of your network you need multiple output nodes. In the case of multiclass classification you want multiple outputs, one for each class. These represent the probability distribution over the different classes. For binary you can get away with only one output because the other class has as the probability. You could say the same for multiclass that you need one less, however this is commonly parameterized with a softmax which needs all values for the denominator. Other cases might be for example one output per pixel, for example for image segmentation. 

There are multiple ways you can do it. The first two options you propose would be valid but I'm not sure if the log-loss allows you to use probabilities as target as opposed to just binary labels, but if that is the case they will work for what you want. I have another suggestion. Let's say there are at any point n possible moves available (In case of a 2x2x2 there are three dimensions and in every dimension 4 moves I think, so that would be 12 options) you could train 12 classifiers or in the case of a neural networks just twelve sigmoids in the output layer. Then you can represent every move as a probability, represent the training targets as a vector of 1s and 0s with 1 as being an optimal move and then during classification pick the one with the highest probability. An extension of your first option would be to add all the optimal moves as state. Say move 2 and 4 are optimal you could just add the same state twice with different labels. EDIT: That said, I think your second option is the best one if the math works out (I'm uncertain about this and don't have time to figure it out right now) but the others will work as well. EDIT2: According to Neil Slater the second option will work if it's implemented properly and I think that is the cleanest solution so I would go for that if possible, otherwise I would go for either go for the multiple samples for multiple optimal moves or the 12-way binary classification. 

I would not recommend doing this in general as it will likely lead to overfitting. While a particular reordering might improve fit on your dataset, the goal is to create a model with good predictive power. If you received never before seen data from an independent second, third, one hundredth Titanic, would the ordering still be the same? I doubt it. You'd probably be incorrectly using the somewhat arbitrary ordering fixed on the first, real Titanic on new data where the ordering is no longer the same. I think the story goes that the young and the elderly got preferential treatment and this is why their survival rate was higher. Therefore your AgeBand curve should be approximately U-shaped. (Which it is, approximately.) If you insist on linear regression, I don't see how you could ever (directly) include this nonlinear (U-shaped) effect. The fewer assumptions (particularities) go into the choice of features, the better. It would then make more sense to me to reorder such that you pick bands alternately from the two ends (starting from the middle): (or in reverse order, starting from the ends). Or, even better, pool the age bands into coarser groups: . I think these are justified by your exploratory data analysis. When I look at the range of values in the AgeBand plot ($0.33$ to $0.55$), this variable does not actually explain as much as Pclass, Sex, FareBand. In the end you might end up dropping it from your variables anyway, so once again, the more attention you pay to it, the more you're at risk of overfitting. 

Just removing x_2 loses a lot of information and would create a significant bias towards higher targets. 

I don't think the second picture necessarily has more noise, it's just less linear than the first one. If you are doing regression (which is the case in both your problems) you can use the same loss function to optimize your family of functions that you are using to fit your data. The only difference is that with certain function families gradient descent might not get to a global optimum but will get stuck in a local one. The loss function only determines how to penalize certain errors (based on the residuals). 

First of all you have to realize these kind of problems have large amounts of noise compared to signal, because predicting what someone will buy based on a very small window of information is difficult. That said, you are throwing away a lot of information with your current approach. Temporal aspects include a ton of information, for example the sequence in which items were bought etcetera. While this is a lot more complicated than what you are describing now, you could look into recurrent neural networks where you feed history up to the point of prediction as a sequence and predict the item they will buy next as softmax classification. This will depend on the amount of products that you offer whether this is feasible or not. Another advantage is that so-called 'out-of-core' training is relatively easy with neural networks due to the iterative training of batches. Multi-label is also clean, you can just add a number of labels at the end of your graph if necessary.