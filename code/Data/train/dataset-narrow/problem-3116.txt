I think, even before doing LDA, you should remove words which appear in more than "x" percent of your documents. Try different "x" starting from 80% and then going down. The logic is that if the word is common for many documents, it does not distinguished those and should be neglected. 

No. There is no similar mechanism for continuous variable. If it worries you, that , you can 1) demean the price, that is subtract mean price from all price values. Then negative values will clearly show below-average and positive above-average prices. 2) after demeaning you can divide values by the standard deviation of the price. (1) and (2) together is called "standardization". Alternatively you can 3) rescale your price to the range of values you want. Usual choice is (0,1) range. If you do this for one feature (price in your case), then it makes sense to do the same for other features. Whether or not this will help to get better prediction results depends on the model. Some models, a typical example would be SVM, do require such transformation. 

For an intuitive explanation I also found this IMHO, for your problem with n=10 you can use d<=2, for n=1000 d=3 (maybe 4, at most 5). Why d=3 for n=1000? Roughly speaking this would correspond to 10 points along each dimension (10^3=1000), which is reasonable to fill the 3D space. For d=5 it is like 4 points in each dimension, which is not so good but not a disaster. IMHO, you should try to reformulate your problem and significantly reduce dimensionality (maybe try to use SVD or PCA). This may automatically solve your problem of noisy data. 

converts strings to integers, but you have integers already. Thus, LabelEncoder will not help you anyway. Wenn you are using your column with integers as it is, treats it as numbers. This means, for example, that distance between 1 and 2 is 1, distance between 1 and 4 is 3. Can you say the same about your activities (if you know the meaning of the integers)? What is the pairwise distances between, for example, "exercise", "work", "rest", "leasure"? If you think, that the pairwise distance between any pair of activities is 1, because those are just different activities, then is your choice. 

I would say, the choice depends very much on what data you have and what is your purpose. A few "rules of thumb". Random Forest is intrinsically suited for multiclass problems, while SVM is intrinsically two-class. For multiclass problem you will need to reduce it into multiple binary classification problems. Random Forest works well with a mixture of numerical and categorical features. When features are on the various scales, it is also fine. Roughly speaking, with Random Forest you can use data as they are. SVM maximizes the "margin" and thus relies on the concept of "distance" between different points. It is up to you to decide if "distance" is meaningful. As a consequence, one-hot encoding for categorical features is a must-do. Further, min-max or other scaling is highly recommended at preprocessing step. If you have data with $n$ points and $m$ features, an intermediate step in SVM is constructing an $n\times n$ matrix (think about memory requirements for storage) by calculating $n^2$ dot products (computational complexity). Therefore, as a rule of thumb, SVM is hardly scalable beyond 10^5 points. Large number of features (homogeneous features with meaningful distance, pixel of image would be a perfect example) is generally not a problem. For a classification problem Random Forest gives you probability of belonging to class. SVM gives you distance to the boundary, you still need to convert it to probability somehow if you need probability. For those problems, where SVM applies, it generally performs better than Random Forest. SVM gives you "support vectors", that is points in each class closest to the boundary between classes. They may be of interest by themselves for interpretation. 

With (d=256, n=10) as well as with (d=16000, n=1000) you are under the curse of dimensionality. The essence of the curse (quoted from Wikipedia) 

The size of each tree depends very much on its depth. Thus, change the maximal depth (). Try to set it to finite number (as opposed to the default "None") and then try to reduce this number. In addition (or as alternative) try to increase or . You can also analyze you features and keep only important ones. The simplest way would be to have a look at the of your forest. (In general, finding important features is an art and science on itself.) Exclude non-relevant features and rebuild the forest. 

Recall depends on the quality of your model (and thus also on the quality of your training data) and on the chosen probability threshold. By decreasing probability threshold, you improve recall but worsen precision. Which tradeoff to choose is again a business decision. 

does this for you. To get intuition of the grid-search process, try to use To extract scores for each fold see this example in scikit-learn documentation 

rpart is a decision tree model and as such is very much interpretable. You should visualize your decision tree. See the examples here, one of them is 

The question 'is this a good model?' is essentially a business question. There is always a precision/recall tradeoff and you decide based on your business goal what model to choose. Data science can help you with the question on how to compare classifiers. Let us start, that each (or nearly each) classifier can predict not only the outcome (Class2 or Class3), but also the probability of this outcome. Your and all your metrics are suitable for making business decision, but, from data science point of view, have two problems 1) they do not take into account the agreement by chance 2) they are based on the threshold probability 0.5, which is not necessarily the optimal probability for deciding about the class. To deal with (1), have a look at Cohen's kappa To deal with (2), use metrics based on probability. The simpler one is Area Under the ROC Curve, see also here. Even finer, but not so straightforward to interpret is Logarithmic Loss 

in common English, the comma symbol doesn't appear in numerical data too often, or it is not required. E.g., one million would be 1000000. we don't need to commas, but programes like MS Excel show them just for readability, e.g. 1,000,000. in other languages, such as German, the comma symbol uis actually used as an English 'point' or 'dot' to separated decimals from integers, e.g. ("one point five") in English is written ("one comma five). This means that people working exclusively in German don't always used commas as a separator (but might still used the suffix!). probably should have been reason #1 - CSV files are actually still human-readable. Anyone with a text editor can open it up and get the gist of the data. That is a huge timesaver. The following point optimised for something else... People do use other seperators - check out this intersting blog post about using the Icelandic thorn symbol. The idea is to basically use a separator that does not appear anywhere else, so you can parse any file with pretty much zero doubt that something unexpected comes out. The thorn has some downsides e.g. that it doesn't fit into a single byte. There are other symbols that you could go for, such as the vertical bar , the ampersand or even the tilde . Have a look here for a little more discussion with respect to databases. Using whitespace symbols can work well. People use tabs, because they don't usually feature at all in a dataset as information themselves and they also leave the file in a readable format. The newline symbol is of course reserved for newlines (again, to point out the obvious). One could try using something like five spaces, claiming that it wouldn't appear in the English language (or any other natural language), but then the user would need a fancy regular expression (a.k.a. regex) to parse it, so perhaps not the most user friendly. 

execute the search download the file navigate to the next search results page repeat from step 2. until finished 

Because you use the word horizon, I will assume you mean that you would like to predict 10 days into the future at a given time step. There are a few ways of doing this. With this kind of time-series problem, it is common to make the assumption that only a certain history will influence the next few time steps (neglecting seasonal effects). Example in words: So in your case, you might use e.g. the previous 60 days, and predict the next 10. Taking your 100 rows of data as an example, this means you can actually make predictions, each prediction of 10 time steps ahead (we will need these 31 predictive_blocks later). From 100 rows we lose the first 60 to fit the first model. Of the remaining 40 rows of data, we can predict 10 steps ahead (rows 61-70), then we shift the whole thing one row further and repeat. The last prediction of 10 future points would be for rows 91-100. After that we cannot predict 10 steps anymore, so we stop - and this is why we have to subtract that extra 9. [There are of course ways to continue making prediction, as to use all the data] Example with a thousand words: Let me paint the picture; to help explain the idea of a shifting window prediction. For each train set (e.g. from to in red - train set 1), you want to predict the following H time steps (corresponding to t=6 in orange - test set 1). In this, your horizon is simply one i.e. . 

One can of course attempt to frame it as a pure machine learning model, and there are many ways of doing this; it is optimisation, after all. I think the first step, whether using some machine learning algorithm or a linear optimisation construction, is to understand your data, the effects of each factor and their relationship to your target variable to be maximised/minimised (profit/loss, respectively). So, as I was told back when learning about this myself: "The first step is to translate words into linear inequalities". So you could think about the factors you have and understand which ones effect price. If there are constraints/limitations, these should also be considered. For example, if you know that you are not allowed to sell more than 10,000 tickets on one day due to capacity constraints of your system. There are some interesting points made in this thread over on Cross-Validated. Now you understand the data a little better, you could think about which models to try out. Without knowing more about your data, I can't really offer more guidance as to which models might be worth trying, but hopefully this answer helps you along that path. 

Thought #2 Of course, you are in essence just mapping some input to some outputs. Almost any machine learning algorithm will have a good go at it and - depending on the complexity of the internal workings of your magical machine - will be better or worse. Algorithms such as boosting (e.g. via gradient descent) generally require you to put some kind of information into the model e.g. by specifying a regression equation, mapping inputs to outputs. A good implementation is the package in R. There is a great tutorial to help decide what might be important. 

Note: there is ambiguity in the dimensions where I highlight it. A colour image will have three dimensions: (height, width, 3). Black and white will have only two dimensions: (height, width).