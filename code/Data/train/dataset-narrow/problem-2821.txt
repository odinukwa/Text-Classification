Create a camera Set the view matrix of the camera to identity so that there are no view transforms made to the children Set the projection matrix of the camera to be of an orthogonal matrix with width and height equal to 1 Set resize policy on projection matrix to be fixed Set render order to be (I previously had this as and that didn't work. I still have to dig into why). Create a textured quad with width and height of 1 unit Add the textured quad to an instance of and add that geode to the camera Extract the first window from the camera. Set the graphics-context and viewport of the camera Add the camera to the scene and you have a background image 

I'm trying my hand at using OpenSceneGraph for graphics in my game. I'm having a hard time trying to figure out how to get a background image up. The relevant piece of code that I have can be found in this gist. Does anyone here have an idea of what I'm doing wrong/missing? BTW: 

I was unable to find a good working code for this simple thing. Luckily I got this working. I hope this helps someone else who's trying to do the same. P.S.: If you want to displace the background image on the screen, think of doing that in relative values; [0, 1]. So if you want the background image to be on the top-right of the screen, create the like so: 

I was able to solve my issue. The second revision of my gist has the solution. In a nutshell, this was what I did: 

The matrix calculations, as long as they are constant for a frame being rendered, should be calculated by the CPU and sent to GPU. Multiplying matrices is not an expensive operation for the CPU as you might think. But if you pass it to the vertex shader, the same matrix result gets calculated n times, where n is the total number of 3D vertices being rendered for that frame, which is totally inefficient no matter how fast that might be. That is unless you have a method of calculating the matrix in vertex shader just once. If you do, then its great. Otherwise just 'keep it simple'. Calculate them in CPU and set them as constants to the shader. 

You only truly need a separate class if you're multithreading, so you can have separate constructors, one ran in the render thread. Even then its up to you whether or not you want to do that. The big reason is just organization. More important than anything is to make sure your textures and render programs are static. You also want to render all of a single type of renderer at the same time to avoid render state changes, possibly even rendering everything of the same type in the same draw pass. 

Looks not too bad to me. Only thoughts are to consolidate your camera to have a setMVPUniform(GLuint uloc, matrix model) or similar function. Likewise make similar classes for Lights, textures, VAOs, etc. Wrap anything you find yourself copypasting into a function. As long as you keep your things public you'll still have low level access where needed. Definitely make a renderer class for each group of objects which share the same program. 

[s 0 0 x] [0 s 0 y] [0 0 1 0] [0 0 0 1] And multiply it * your modelviewprojection However this only works directly for ortho, for perspective, you'll have to w divide manually then apply this matrix. 

It's almost always better to change a state's object instead of changing an object's state. Mostly for readability, as thats how GL is expected to be used. Even if you have multiple attachments in common between FBOs, I would still suggest binding them to each FBO. As for performance, its implementation dependent, but framebufferTexture is likely slower due to it having to compute the mapping for the fragment outputs into the texture, while a complete framebuffer already has this mapping computed and just needs to load it. 

Having done both courses, Bachelors in Information Technology (more like SE) and Masters in Computer Games Technology, I find the difference in curriculum is profound. Just like what Ken had mentioned, SE covered topics that were close to designing software and how the systems worked, while CGT was geared more towards SE for Games. If you're looking for a career in games, CGT would be the way to go as you get to learn (in a concentrated manner) the stuff required for games. It would also give you a better chance at getting a job in the games industry. I say better because it's not necessarily the degree that decides if you get the job or not. But when it comes to game development basics, you'd already be well-versed at how things fit together in game, while a SE student will have to go through the extra learning curve and learn graphics/AI/physics for games on his/her own to prove their worth to game companies. I should warn you about the downside though. As good as the game industry sounds, it doesn't always provide a lucrative career. For those who've settled themselves in a well-paid job in games industry, two words, WELL DONE! You're living the dream! But it's not an easy task. You've to work your mind off to prove your worth before they even consider to keep you in the company. And typically, you'd first end up working in a start-up (or self-found) game company, which doesn't provide you any form of job security and with basic salary. Trying for a big-shot company usually ends up with them telling you that you don't have enough experience. Not only that, if you do manage to get through to a big-shot company, they usually don't pay you as much as what you'd earn as a Software Engineer. With that warning, if it's your dream to be working for a game company, GO FOR IT! But beware of what is to come and be prepared for it. In the end, when all your hardwork pays off, it's the sweetest achievement of your life. If however, you're not sure if you'd like a lifelong career in games industry but still like to dabble on the idea of working with one, I would suggest first taking up an SE Bachelors degree, try to get internship in a games company based on your degree and progress your career from there. Failing to find any, you could go for Masters in CGT and try for a game company after that. If all fails, you'd still be able to get into software companies based on your Bachelors degree and the fact that you've got a Masters degree related to CS (they don't usually care if it's CGT or not) as long as you can prove your worth. 

Double check if you are truly using viewport correctly. You're not rebinding the texture to the FBO. That SHOULDN'T be necessary, however the spec makes no guarantee that glTexImage won't screw up any attached framebuffers, so assume that it can. If that doesn't work then its the way you're sampling from it. For simply allocating a texture with undefined content, use null as the data pointer, much faster. Absolutely never use mipmaps with a framebuffer unless you know for sure you will be minifying it, otherwise you'll use a ton of memory. 

Instead of building the cube in geometry shader from a point, its better to do an instanced render of a simple cube VBO, and forego a geometry shader entirely. The only time you wouldn't want to do that is if each cube has rapidly changing orientation and you need to recalculate its modelview every pass. In which case, have position and orientation as vertex attributes, then build the mvp per point-cube in the vertex shader, then do as Stephane explained in geometry using a corner-table. 

From what I understand you are trying to render a scene many times from many views into a tilemap. To do that I would make each tile a layer of a Texture_2D_Array, and bind to a 3D framebuffer. In the first phase, render your scene instanced with view per instance, then in geometry shader (only place you can set it) set gl_Layer = instance. In the second phase simply render all layers of the array onto the tile atlas. By using a framebuffer you eliminate the need to cull using discard, which would have a severe performance impact. It will also make the code much more readable than doing all of this in vertex stage. 

I know that the image is getting loaded because the logs says so The log says that it managed to add the camera as slave to the viewer 

I know you've already accepted Zhen's answer but I'd like to put another out there just in case it helps anyone else. To reiterate the problem, the OP wants the ability to keep the rendering code separate from the logic and data. My solution is to use a different class all together to render the component, which is separate from the and the logic class. There first needs to be a interface that has a function and the class uses the visitor pattern to retrieve all instances, given the list of s and renders those objects that have a instance. This way, Renderer doesn't need to know of each every object-type out there and it's still the responsibility of each object-type to inform it's via the function. Or alternatively, you could create a class that visits all GameObjects and based on individual condition they can choose to add/not-to-add their renderable to the visitor. Either way, the main gist is that the calls are all outside of the object itself and reside in a class that knows intimate details of the object itself, instead of that being part of . DISCLAIMER: I hand-wrote these classes in the editor so there's a good chance that I've missed something in code, but hopefully, you'll get the idea. To show a (partial) example: interface 

If I were you, I would question myself about 'What do I enjoy doing for a game?'. Do you want to be a graphics programmer or a game programmer? Do you enjoy fixing those odd pixel in the screen that doesn't fit with the rest of the screen or do you enjoy building the game and don't want to be really bothered by the graphics implementation? If you're more into graphics, learn a high-level graphics API first, like Ogre3D, Irrlicht, Horde3D, etc. Once you grasp high-level concepts, take your time and dive into details. If you're looking to game development, learn to use game engines like Unity3D, Unreal3D, etc. They've done the hardwork of getting the engine implementation right and you can concentrate on the building the game itself. 

For my font renderer I store all characters in a texture_array, which eliminates the need for atlas mapping and worrying about uv float precision. For drawing use pointsprites and give each vert only a single attrib for character id, while the vertex shader tracks the spacing and scaling, you could easily adapt it to render multiple lines. Afaik this is the best way to maximally take advantage of gpu for rendering text. 

VBO created and loaded with data VAO is bound Vertex attribs are set, vertexAttribPointer points into the current ARRAY_BUFFER, attribs are part of the state of the VAO Program gets vertex attribs from the current VAO (which may be the default VAO 0 if you never bound one) 

If you're worried about optimization then use vectors more, the optimizer may or may not do that for you. Avoid conditionals, all branches of them will be executed. But the major bottleneck on modern GPUs is from samplers, pow and log are pretty negligible. If possible then just use sRGB internalformat in your framebuffer, most gpus will do the conversion in hardware. 

glBufferData reallocates the VBO in VRAM, use glBufferSubData when updating data. Generally each mesh has its own VBO(s); and own VAO if not batched. There are optimizations where you use a single VBO for all meshes with the same attribs, etc, but don't worry about that. The relation of these things is 

Either recreate the display or Keep your default framebuffer with no multisamples, make a framebuffer with multisample that is blitted to the default fbo, which may be recreated. 

For something with rotation its generally wasd for throttle and roll, shift+wasd or uhjk for strafe orthogonal to forward. Or ws throttle ad strafe qe roll For purely translation, then wasdec or wasdqe. But its ultimately the situation that dictates what is best used. Don't think "I should use the standard", think "Does the standard suit this?"