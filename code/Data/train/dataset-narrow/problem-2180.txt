The satisfiability problem is trivial: all closed formula are satisfiable. This can be demonstrated by constructing a model in the following way based on the structure of the formula. Assume that $s$ is the state which is the current focus of the algorithm. Start from a model with a single state and no transitions. In state $s$: 

It seems to me that the direction you should consider looking in revolves around game semantics for general references and the related semantics for linear logic, such as those based on Conway games. An algebraic account of references in game semantics by Paul-André Melliès and Nicolas Tabareau is probably the best place to start. In this paper linear logic is relaxed to so-called tensor logic to get things to work, so it is not quite the setting you want. But they do rely on Conway games, so there is certainly a connection with models of linear logic. They also do not really exploit linearity as in linear types, but the machinery is there to do so if you want to, I believe. The work of Jim Laird (such as A Game Semantics of Names and Pointers) and Guy McCusker may also contribute to your quest. The recent interesting thesis Game semantics for an object-oriented language by Nicholas Wolverson pushes these ideas further in an OO setting. He considers in detail linear threading, only one operation active at a time, and describes how to add linear classes. Both rely on linear typing. Again, however, the underlying model is not strictly a model of linear logic, but it's close. 

The formal rules which form the essence of the basic tactics are defined in the Coq users guide here or in Chapter 4 of the pdf. A quite instructive paper on implementing tactics and tacticals (essentially tactics that take other tactics as arguments) is: 

A) is true. The above construction can be modified to ensure that if an $a$ edge exists already from a given state. B) is constant time, as all (closed) formula are satisfiable. (Or time proportional to the size of the formula if you need to check that it is well-formed.) 

You might want to look at Barry Jay's Pattern Calculus. This is something beyond the $\lambda$-calculus, but is has the power to do intensional analysis of lambda expressions. 

Semantic subtyping is based on an underlying set theoretic interpretation of types, where subtyping is subset. The original work, I believe, is by Castagna in the context of XML processing language CDuce. Types correspond to sets of XML documents. The ideas have since been reapplied to the $\pi$-calculus and to a calculus objects and classes. 

There is a very deep connection between inductive definitions and impredicativity, but my understanding is that in the context of what you are talking about (im)predicativity is not particularly relevant and the test is purely to guarantee monotonicity, so that fixed point theory can be applied, namely, that the principle of induction is well-defined. (I'm willing to be corrected on this point.) The relationship between impredicativity and inductive definitions is explored in this talk by Coquand. It goes back to some results from the 50s by G. Takeuti that impredicative definitions can be reduced to inductive definitions. The book 

Domain Theory is highly topological in nature, and rather being a one-off application of topology, it is more or less its own subfield of topology. Its application in Denotational Semantics of programming languages, especially functional ones, is certainly one of the most important applications of topology in computer science. Values (including functions) are given semantics in terms of DCPOs (directed-complete partial orders) or some such structure. Recursive domain equations such as $D\cong [D\to D]$ can be solved in this setting, giving semantics to beasts such as the untyped $\lambda$-calculus. The semantics are fundamentally based on the notion of approximation, given by the ordering, and the least fixed point solution of equations, and solutions are generally guaranteed to exist. Stemming from denotational semantics are connections with abstract interpretation, and program analysis and verification. Current research includes providing denotational semantics for concurrency and for quantum languages. Abramsky and Jung give a nice survey of the core ideas: Domain Theory. 

The wikipedia page for ISWIM says that it is a higher-order language, and that ISWIM is syntactic sugar for the $\lambda$-calculus. Although it seems to have no explicit $\lambda$ construct, thereby making it impossible to have anonymous functions, one achieves the same expressive power by combining lexical scoping and first-class functions: Define a new name for a function locally; pass function as value to higher-order function. 

is the core reference 2 decades ago. Such an algorithm will be giving data along with its classifications in different dimensions to learn a decision tree. The algorithms aim for optimal trees, and further possibilities exist for improving the resulting. Recent refinements of the approach include using genetic algorithms: 

Some early(?) work done in this area was by Bart Jacobs (Nijmegen) and Marieke Huisman. Their work is based on the PVS tool and relied on a coalgebraic encoding of classes (if I remember correctly). Look at Marieke's publication page for papers in the year 2000 and her PhD thesis in 2001. Also look at the papers by Bart Jacobs cited in the A Setzer paper you mention. Back in those days, they had something called the LOOP tool, but it seems to have vanished from the internets. There is a workshop series known as FTfJP (Formal Techniques for Java-like Programs) that addresses the formal verification of OO programs. Undoubtedly some of the work uses dependent type theory/higher-order logic. The workshop series has been running for some 14 years. 

The expected amount could vary between 5 and 30. As a junior researcher, you'll be given a few for practice. As you advance through the grades, you'll take on more reviewing as you become a member of PCs. Then as you advance even further, you'll have an army of PhD students to do your reviewing for you. More than 30 in a year is quite onerous. As I said above, I'd say it is appropriate to refuse a refereeing request when either (1) you do not feel that you have sufficient expertise to do a good review, or (2) you have too many reviewing (or other) commitments at the moment. It is also considered good form to suggest a number of possible alternative reviewers, in the case that you refuse. 

From a programming language theory perspective, as opposed to the computability perspective other answers and comments have offered, C++ templates combined with concepts correspond to bounded polymorphism or constrained genericity. Concepts themselves correspond to the constraints or bounds placed on a type. A template is type-level function, parameterised by type that are constrained by a concept to implement a particular interface. When the template is applied to a type satisfying that concept, a new type results. Templates+concepts are analogous to generics in Java, Scala or Eiffel. They differ from templates in earlier C++ because they allow constraints on the type parameters to be specified and checked, whereas C++ templates did not allow that. The benefit is better static checking that the program after applying the template will be well typed. A good reference is Pierce, Benjamin C. (2002). Types and Programming Languages. MIT Press, Chapter 26: Bounded quantification. 

You could encode these into Horn Clauses (= Prolog) and use resolution (= Prolog's implementation technique). More explicitly, your Prolog code file will look like the following: 

Specification languages such as Z have a vast range of data types and operations on them and, at least within that community, these are standardised. This tutorial gives a great overview. Other specification languages exist, such as B and Alloy which may also satisfy your requirements. 

In programming languages research many ideas for new programming language constructs or new type checking mechanisms stem from theory (perhaps informed by experience in practice, perhaps not). Often a paper is written about such mechanisms from a formal/theoretical/conceptual perspective. That's relatively easy to do. Next comes the first hurdle: implementing the new constructs in the context of an existing compiler and experimenting with it, in terms of efficiency or flexibility. This too is relatively easy. But can we then say that the programming construct constitutes an advance to the science of programming? Can we say that it makes writing programs easier? Can we say that it makes the programming language better? The answer is no. A proper empirical evaluation involving scores of experienced programmers over large periods of time would be needed to answer those kinds of questions. This research is hardly ever done. The only judge of the value of a programming language (and its constructs) is the popularity of the language. And for programming language purists, this goes against what our hypotheses tell us. 

A generally high degree of mathematical maturity makes a lot of the formal aspects of (not necessarily theoretical) computer science much easier to understand. So doing a minor in mathematics along with your major in computer science would do more good than harm. 

And now your lemma. The proof relies on to break up , does some manipulation of the result, and puts things back together using . No induction is required. 

Both temporal logic or dynamic logic are two modal logics that can be used to reason about changes in the world over time. Coalgebra is a formal approach to dealing with observations of state-based models changing in time. This approach generalises state machines, labelled transition systems, and so forth, which themselves provide behavioural models of systems. Coalgebraic modal logics are a generalised approach to defining logics for coalgebras, thus providing a systematic way of generating logics for reasoning about systems that change in time. Logics of causality even exist (I'm sure google will turn up even more). 

More details can be found in the Tree Automata book. It seems that you are interested in top-down tree automata, so the answer to your question is no. You will of course have to check whether top-down tree automata are in fact what you are interested in. 

The following question has come up a number of times when testing the security of a system or model. Motivation: Software security flaws often come not from bugs due to valid inputs, but bugs resulting from invalid inputs that are sufficiently close to valid inputs to get past many of the straightforward validity checks. The classic example is of course buffer overflows, where the input is reasonable, except that it is too large. Compilers and other tools can help address these problems by modifying the layout of the stack and heap and by other obfuscation techniques. An alternative is to remove the problems from the source code itself. One technique called fuzzing bombards the program with inputs are close to expected inputs, but are in some places unreasonable (large values for integer or string fields). I would like to understand fuzzing (as one example) from a more formal perspective. Assume that the space of valid inputs is described by constraints $\Phi$. Let $M$ be the set of solutions of such constraints, namely $M=\lbrace m\in\mathcal{M}~|~m\models\Phi\rbrace$, where $\mathcal{M}$ is the space of possible inputs. I'm looking for work describing the following notions: