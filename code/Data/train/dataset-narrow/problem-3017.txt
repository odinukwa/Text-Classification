I want to input the set {A1, B1} for time 1, then {A2, B2} for time 2, etc. A Keras LSTM takes input in the form To enable better generalization, I intend to break these sequences up into subsequences of length as recommended here and here, which means (e.g. if n is 5 and my window of time, k, is 3, then there are 5 - 3 + 1 = 3 sequences total). I have time steps divided into separate windows, so (I think?). Question: what should the values of and be, and how should I shape my input data? 

I have two independent time series with the same length. I want to input them together into a Keras LSTM. That is, suppose I have: 

In more traditional statistical learning methods, such as logistic regression, the coefficients on the pre-selected features can be interpreted as increasing or decreasing the log-odds of success. For example, say we want to use logistic regression with a single predictor variable to predict whether a student will pass the exam. We can estimate the parameters in the model: $log(Y) = \beta_0 + \beta_1$studying_hours where $Y = 1$ if the student passes the exam and $Y = 0$ otherwise, and studying_hours is a variable with values 1, 2, ..., 100 hours. We can say that a one-unit increase (a one hour increase) in the number of hours studying for the exam increases the log-odds of passing the exam by $\beta_1$. It stands to reason that we should be able to make a similar argument when the features are learned instead of pre-selected. But then we would need to understand exactly what the learned feature is. In my example above, instead of pre-selecting to be the feature we want to use to predict whether a student passes the course, we would learn some informative feature instead. I've heard of Hinton diagrams, and of visualizing the feature map for CNNs, but I don't think either of those techniques really works here. A Hinton diagram would just tell me the magnitude and sign of the weights. A feature map would not really be informative. I don't expect to understand what all of the features are, because the whole point is that the algorithm can do a better job of learning features than I can do designing features. But even so, at least some of the features should be human-intepretable, like a 'straight line detector' in a CNN. How can we interpret the features being learned by a multilayer perceptron for logistic regression? Not really sure what to tag here, so please, if there's an edit you would recommend, go ahead! EDIT This is my attempt at starting an answer, but clarification/other opinions would be appreciated. Let's take a case of binary classification, and start from the simplest possible case: one scalar input, one hidden unit with a relu activation, and one output unit. Assume the hidden unit has a bias of 0. We can express the function that describes this network as: $log(Y) = \beta_0 + \beta_1 max\{wx, 0\}$, where 

Please allow me to paraphrase your question to make sure I get your question right. Suppose you have 1 million data entries. Each entry consists of three inputs X1, X2, X3 and one output Y. One of the three inputs, X1, is a noise. You want to find the relation between X1 and Y. So you can remove the impact of X1 on Y. Let's use a simplified example, Y = 2X1 + 3X2 + 4X3 If you update each of the 1 million entries with: Y_new = Y - 2X1 Now you can examine the updated 1 million entries to find the relation between input X2, X3 and Y_new: Y_new = 3X2 + 4X3 The relation between X1 and Y can be computed with different algorithms, such as regression, decision tree. How to measure which algorithm generates the best result, i.e. truly reflects the relation between X1 and Y hence removes the impact of X1 on Y as much as possible? The problem you're solving is an interesting one. I would use a free tool such as Knime to find out which algorithm generates the best result in modeling the relation between X1 and Y. It's faster than coding in Python or Matlab. Details: to put the problem intuitively, suppose the price of a house is determined by location, size of the house, condition of the house. Assume these three factors are independent from each other. You want to remove the impact of "condition of the house" on "price" as much as possible. In essence, you want to find the relation between "condition of the house" and "price" which may not be linear. You can let Knime read the 1 million entries of "condition of the house" and "price", try Decision Tree, SVM, or even Ensemble Learning, and see which algorithm generates the best model of X1 and Y. In other words, try and error :) 

The standard deviation and mean of a categorical variable is not meaningful. It looks like the original data are from a range of [0, 20), and the space has been discretized. Now, instead of ranges, you have ordered categories 1 through 4. The individual numbers within each category have lost their meaning. For more details, see the accepted answer here 

Although the previous answer by @Imran is correct, I feel it necessary to add a caveat: there are applications out there where people do feed a sliding window in to an LSTM. For example, here, for framing forecasting as a supervised learning problem. If your data are not very rich, then you may find that any LSTM at all overfits. There are a lot of parameters in an LSTM (in fact, there are $4(mn + n^2 + n)$ parameters, where $m$ is the input length and $n$ is the output length, according to this post). Since LSTMs do not require fixed size input, they can find the optimal lookback number on their own. However, if you've done a prior autoregressive analysis and decided that, for example, the current time step is most correlated with the 10th previous time step, and not correlated with the 11th or any time steps further in the past, then you could perhaps save yourself some training time by feeding in fixed-length sequences. However, that kind of defeats the purpose of an LSTM. If your data are not rich enough for an LSTM, I would recommend trying something much simpler, like an autoregressive model, and working your way up. EDIT (responding to a comment): Overlapping sequences are used as input, especially when the sequence is very long (although, of course, 'long' is relative). Although LSTMs are better than a vanilla RNN for long sequences, they can still have some trouble remembering time steps from the beginning of a sequence if the sequence is very long. That led to things like the bidirectional LSTM, which reads the sequence forwards and backwards, improving the exposure of the network to the beginning and end of each input sequence. The principle is the same with overlapping sequences, although I would argue that overlapping sequences is more intuitive. 

A random sample is drawn from the database A hierarchical clustering algorithm employing links is applied to the samples This means: Iteratively merge clusters Ci, Cj that maximise the goodness function merge(point1,point2) = total number of crosslinks /expected number of crosslinks Stop merging once there are no more links between clusters or the required number of clusters has been reached. Clusters involving only the sampled points are used to assign the remaining data points on disk to the appropriate clusters 

Using TFIDF (term frequency inverse document frequency) will solve your purpose. Get the TFIDF score for each word in your document and sort the words by their scores by which you can select the important words in your data. 

For categorical data, robust hierarchical clustering algorithm ( ROCK) will work better that employs links and not distances when merging clusters, which improves quality of clusters of categorical data. Boolean and categorical are two types of attributes that are most suited in this algorithm. ROCK is a static model that combines nearest neighbor, relocation, and hierarchical agglomerative methods. In this algorithm, cluster similarity is based on the number of points from different clusters that have neighbors in common. You can use CBA Package in R to perform the ROCK clustering. Algorithm Steps: Data----->Draw Random Sample----->Cluster with Links----->Label Data in DIsk 

For more details refer: $URL$ You can also use association rules for objective 1 by considering node 1 in LHS and consider all events after nodel 1 event in RHS. Hope this helps!!! 

There must be a time lag between LHS and RHS, i.e. time gap between antecedent (if one node has mains failure) and consequent of the rule (next event would be node down) Prediction rule must have relatively stable confidence with respect to the time frame determined by application domain i.e. run association rules on overall 3 months data, you will get some rules with some confidence. Now run association rules by month wise and see the confidence of same rules, if confidence is consistent, you can use those rules in prediction with confidence ÔÅä 

Take a look at this free class Intro to Artificial Intelligence from Udacity. One of the instructors is Norvig. The class is more suitable for beginners than Norvig's book. Even though this class doesn't have programming exercise, it explains concepts so well. Its follow up class Artificial Intelligence for Robotics has programming exercises and does a fantastic job in explaining Partical Filters. 

You may want to consider preprocessing - convert different wording for the same type of talent to same wording. For example, a talent in machine learning is called data scientist at Coursera job site, data engineer at Udacity job site, or data analyst. This preprocessing is similar to stemming in concept. 

When you said "a broader topic", did you mean what algorithms to use to examine the event log with the goal of reducing future calls from the same customer on the same topic? In other words, a customer may call for help for a different topic. If a customer calls in for the same topic repetitively, something can be improved. You may get ideas from a Coursera class Processing Mining since the issue you're solving is similar to the example of a spaghetti process in lecture 6.7: "Spaghetti process describing the diagnosis and treatment of 2765 patients in a Dutch hospital. The process model was constructed based on an event log containing 114,592 events. There are 619 different activities (taking event types into account) executed by 266 different individuals (doctors, nurses, etc.)." By the way, you can click the drop down menu under "Sessions", choose "April 1, 2015 to May 19, 2015" then you can register and view the lectures. On the right of each lecture, there are some icons. The first icon is to download slides for the lecture. You may find reading the slides is faster than listening to the lecture. Suppose a customer called for installation of software release 1.5 then called a day later for running the new features of software release 1.5. Are these two issues logged as two of the 200 call categories? If so, we can use a time period (say one week) to judge whether it is about the same topic. Within a short time period, a customer is likely to work on the same topic, especially with the same key words such as "software release 1.5". We can coach the call center employees on solving possible follow up questions by saying something like "now that we finished installation, let me show you a couple of new features. It'll save you time." This will reduce the number of calls on the same topic from the same customer. 

The general approach in feature selection is to get a score of each feature in the data set and select top features. We can run algorithms like GBM or Random Forest on all the variables simply to get a ranking of variable importance. We can also use œá¬≤ (chi-squared) statistic with cross validation to select a user-specified percentile of features with the highest scoring. But, the disadvantage with these approaches is not detecting correlations between features. We can also use backward elimination: features are tested one by one and the ones that are not statically significant are deleted from data set. In forward selection that starts with out any variable in dataset and then adds variables that are statically significant. Hope this helps. 

There will be so many follow steps (based on results in step 1), for the objectives you mentioned. I am mentioning starting steps to take analysis to next level. For the first objective, you can calculate simple conditional probability for every node for some time window time frame. Here, you will get a view how each node is affecting other nodes. Also, explore Bayesian network on the data. For the second requirement, as recommended by Dan Levin, association rules are good point to start. To simplify process, you can start with two main events (may be mains failure and association path broken) from the available events. Fix the RHS in association rules to the two main events as mentioned. LHS will be events occurred before RHS events (consider some time window). Now run association rules on the data. You will be able to find some precursors for the two events considered. You can find implementation of association rules in Spark in the following paper: R-Apriori: An Efficient Apriori based Algorithm on Spark Link: $URL$ Further, to use association rule for prediction, considering the following points will help