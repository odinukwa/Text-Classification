Goal of the question: To determine the best (or a sufficiently good) approach for finding up to $n$ paths for each of target ($t_i$) within a max-depth of $k$ for a single-source $s$ in a directed graph. In addition $n$ paths from each of the targets to $s$ within a max-depth of $k$. an example function call should be: 

Notation: $k$ - max depth $t$ - the target set $\{t_1,\dots,t_{\lvert t\rvert}\}$ $s$ - the source vertex $n$ - number of paths to find per target 

Specific use case: Suppose you have a fairly large, sparse, but highly connected graph, $\lvert V \rvert\approx50000$, $\lvert E \rvert\approx2000000$. So in the Adjacency Matrix (AM) the sparsity is about 0.0008 and on average each vertex has 40 connections. Then prior to the search, one calculates the extended neighborhood on the source vertex to the scope of the max depth. In other words, construct the sub-graph, where ever vertex can be reached within a path of $k$ ("scope") and only those edges (e.g. only the predecessors of predecessors of the source, and only the successors of the successors of the source). This might sound counter intuitive. If I am searching for paths of max length $k$, why precompute the extended neighborhood (which has all putative paths of length $\leq k$) when I could search for the paths directly. I am not sure about this, but calculating the extended neighborhood is fairly time efficient and I believe the time use to limit the scope (often) reduces the run time of the path finding more than if one were to run the path finding on the large graph. e.g. limiting to a scope of three can result in a sub-graph of $20,000$ vertices and $980,000$ edges. Since many algorithms are quadratic (or $\lvert E \rvert + \lvert V\rvert \log \lvert V\rvert$), I think the reduction may be worth it. (I have no concrete proofs or bounds to demonstrate this). Question What is the best approach to finding $n$ best paths to each target in $t$ from $s$ of max path length $k$? (parallelization allowed) 

This is a rephrasing of Emanuele Viola's answer with the goal to be more understandable. We show that the given problem $P$ is undecidable by reducing the general halting problem $H$ to it. Let $(M, x)$ be any instance of the halting problem, that is we have to decide wether $M(x)\downarrow$ ($M$ halts on $x$). Construct a Turing machine $M^*$ that works as follows: 

Let me mention linear bounded automata (LBA) which can compute a proper subset of the function Turing machines can handle. LBA do model real computers better than Turing machines in the sense that no computation can use an infinite amount of space but there is no (constant) bound on space either. Of course, real computer do not have to have a linear bound. 

It is not allowed to be larger, it has to be larger! If you allow the right side being shorter, you have context sensitive deleting grammars which have the same power as unrestricted grammars, thus leaving the realms of CSL. The space limitation you mention relates to the automaton concept that is equivalent to context sensitive grammars, not the grammars itself. 

In 1936, Konrad Zuse developed what was for all intents and purposes the Z1 the first computer in the modern sense. This fact is little known but has since been acknowledged even by his international competitors, e.g. IBM. While the Z1 was not very reliable, later models (still developed during WWII) actually worked. Shortly after the war, Zuse's company began building (universal) computers for multiple major universities in Europe. Zuse's motivation was not to gain mathematical insight, although he did develop a formal, universal programming language called Plankalk√ºl. He primarily wanted to do away with repeated, mechanical calculations often seen in engineering -- surely a machine could perform such mindless manipulations of symbols! Note how Zuse's early work happened concurrently and, due to different background and the political situation, mostly independently of the better known work in the US. 

Background Definitions (as used here): $\qquad$single-source: for path finding, an algorithm is single-source if it searches from a given node. $\qquad$multi-target: for path finding, an algorithm is multi-target if it searches for at least one path to each of the specified targets $\qquad$multi-path: for path finding, an algorithm is multi-path if it returns at least one path - if one exists - but at most $n$ where $n$ is the number to find. $\qquad$max-depth: for path finding, an algorithm has max-depth if the length of the returned path(s) is less than or equal to $d$ where $d$ is the specified depth. 

With the aforementioned goal in mind, we can consider some interesting questions. For example, is it necessarily always faster to parallelize over the targets (e.g. single-source, multi-path, single-target of max depth) than searching for multi-targets? My intuition hesitantly says yes... however in some cases it might not make sense. For example, in greedy breadth first search (if there are never duplicates of values used in the heuristic), the frontier will always be the same, regardless of target. In such cases, why would one parallelize over the multiple targets, when to find any target, $t_i$, one must first visit all targets that have a "better" path than $t_i$ and continue until it finds all targets "worse" than $t_i$. Many algorithms are focused on the "best-path" problem. In this case, modifying them can be difficult to get the $n$ best paths. In a question on stack overflow Variants of a star the user attempts something similar by constructing multiple variants of . However, the multi-path is also left out. When the $n$-best paths are not considered, but just $n$ paths. Similarly, making all of these adjustments to depth-first-search is fairly simple (below multi-target is left out as I think parallel DFS is faster than multi-taget): Depth first search pseudo code 

Run it with (Using clingo 4.2.1) (the n=7 indicates graphs of exactly 7 vertices) It should return satisfiable if and only if there exists a graph with no TFAS on 7 vertices. 

In this paper $URL$ they claim to have a practically fast persistent union-find data structure for most use-cases, but it's still not polylogarithmic in the worst case (the worst case being, I have two variants which I alternately update between); only in a backtracking search. Are there any implementations of a persistent union find datastructure which guarantee amortized polylogarithmic time complexity per operation regardless of how it's used? 

Is there a better term for "complete k-partite graph" in the case where k is not fixed? If I say "complete k-partite graph", people tend to assume "for some particular k". In other words, what's a term for any graph for whom each connected component in the complement graph is a clique? I asked this before, but it was as part of another question, so it was ignored. 

Here's the (updated) program for generating G.Bach's graph. I added indicators at the end to check that the graph is a well-formed tournament graph: 

I ran a short clingo program which reported no graph without a TFAS, but there was a bug. I fixed it and now it verifies there's no graph without a TFAS for n=8 or less. For n=9, it finds this one: 

Given a minimum biclique edge cover, is it always possible to assign each biclique to a distinct left node (which it contains)? ie one such assignment for this graph (from wikipedia): $URL$ could be (numbering the left vertices from 1 through 5 going down) blue: 1 red: 2 green: 3 black: 4