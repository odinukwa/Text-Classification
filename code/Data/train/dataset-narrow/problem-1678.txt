We have a Dell Poweredge server running ESXi 4.1 U1 that was installed using Dell's customized ESXi ISO. Now we would like to upgrade to 4.1 U2 and upgrade any Dell specific bits that were installed. I downloaded the 4.1 U2 Dell Customized ISO and tried to install over the existing installation but it doesn't look like I can do that non-destructively. I tried upgrading using VMware's Update Manager, and that upgrades the ESXi installation, but I don't think it upgrades the Dell specific bits. 

You're getting that error in the Exchange connectivity testing tool because the Network Solutions SSL certificate isn't trusted by default in Windows Mobile phones. The only ones that are trusted by default are: 

We have a new office that was punched down with Ethernet in every office and every cubicle. The electrician didn't mark the ports on the patch panel or the wall ports, so I don't know where say port 1 on the patch panel goes in the office. I've heard of people using a tone generator to figure out where the cables are but I'm not sure what that entails. I was also thinking of looking up ARP entries on our switches to find out what port is getting what MAC address and then cross referencing it with a list of MAC addresses on all of the PCs in the office. This sounds unreliable though. Is there a quick and easy way to label these ports? 

I know that this question is a little bit old, but this answer might help someone with the same problem. When you add the Script Map in the IIS Manager it creates the handler in the web.config like this: 

I'm trying to install Microsoft Dynamics CRM 2016 with IFD (Internet-Facing Deployment) and ADFS on the same server (Windows Server 2012 R2) ADFS is running and seems to be working fine. I can fetch the metadata on $URL$ I've configured Dynamics CRM IFD to use the following settings: Web Application Server Domain: mydomain.com Organization Web Service Domain: mydomain.com Discovery Web Service Domain: discovery.mydomain.com External domain where Internet-Facing servers are located: auth.mydomain.com The problem starts when trying to setup the Relying Party Trust on ADFS. When using the address $URL$ as the federation metadata address, it is fetching the wrong metadata. It's fetching the ADFS metadata instead of the CRM metadata. The only way to get this working is if I set a different port for the CRM endpoints, for example by appending :444 to the CRM IFD settings. The problem with this configuration is that it would use non-standard HTTPS port 444 for the Dynamics CRM website and also during the Authentication phase using auth.mydomain.com:444 Is there any way to have both ADFS and Dynamics CRM running on HTTPS on port 443? 

What you need to do is to add the attribute allowPathInfo="true" to the handler. The IIS Manager doesn't have this option and you have to edit web.config manually: 

This isn't a direct solution, but would it be possible to use a 3rd party email marketing company to send your emails? This way you don't have to worry about a lot of these issues. ConstantContact.com is one. Eloqua seems to be a popular one for large companies. 

First things first: SPF only specifies what servers will send outgoing emails from your domain. I came up with this record: v=spf1 a mx ip4:1.1.1.1 include:_spf.google.com include:emailpublishers.com ~all Definition: a and mx: allow your domain's @ A record and MX servers to send email. This covers your domain's @ A record and all of your MX records (may be a little redundant in this case but doesn't hurt). ip4:1.1.1.1: the ip address should be auxiliary.com's outgoing email server ip addresses. you may have to add several of these but this allows auxiliary.com to send emails on your behalf include:_spf.google.com: this allows google apps to send email for main.com (this is why the mx tag above may be redundant since main.com's mx records are google apps) include:emailpublishers.com: you'll want to ask emailpublishers.com what SPF to use for this one but I imagine it's similar. If they have one, replace this one with theirs. ~all: softfail any emails from main.com that come from servers not listed in this record. This tells spam filters to use SPF as one of the criteria to flag an email as spam. using a -all is a hardfail, which means anything the SPF record doesn't catch is spam. This can lead to false positives though. (edit) Once you're done, test it out by sending emails to a Gmail account. Gmail logs in the headers whether the email passed a SPF check. It's incredibly useful for testing. (edit 2) The 'a' in the SPF record only allows your domain's @ A record to send mail, not all of your A records. E.g. example.org with the SPF record v=spf1 a ~all would allow example.org to send mail, but not beta.example.org or testing.example.org. Fixed it above. 

This article from Microsoft ( $URL$ ) discusses configuring a proxyserver for the system. For other browsers, such as firefox, you may need to configure the proxy settings independently. 

In summary, step one, the traffic monitoring (Nagios seems to be a standard tool) helps you figure out, in general, what is going on to stop the immediate pain. Steps 2 - 5 help prevent the problem in the future. 

The RPM system creates a database of sorts to track dependencies. YUM interacts with that RPM database and extends it. It could be that either the base database or the yum extension is corrupted. If you are brand new to Linux, it might be a better use of your time to reinstall the latest Fedora version and then configure an appropriate set of yum repositories for installing more software. With luck, the person that gave you the VPS could help you. Trying to repair a package management system gets very annoying very quickly. Personally, I find the Debian/Ubuntu packaging system easier to use, but I disagree with fahadsadah that .deb with apt is inherently better than .rpm with yum. I would suggest using what others around you are using, as you will have a ready source of advice if you run into trouble. 

Having investigated a number of Content Management Systems for several different projects, I must say that it depends. The biggest challenge I experienced was matching the functionality provided by the hosting provider with the goals of the client. Basically, the client wanted a fancy new site but didn't want to pay significantly more for the needed hosted functionality. That said, I have had good success using Drupal. I found it easy to administer, the designers were able to use the modules to deliver the functionality desired, most hosting plans included the PHP and SQL db functionality needed, and the end customer liked the results. Unfortunately, I have also been hit hard by XSS vulnerabilities 

Is there a way to see the update deadline times for updates in WSUS? In the Updates view I can only see if an update has a deadline or not, not the actual time. This is the deadline time you can set for updates to force them to install by that time. 

We monitor our Dell ESXi servers using OpenManage Server Administrator. This will give you hardware info inside ESXi and setup an OpenManage node you can connect to for more hardware and array info. Instructions are here: $URL$ They work for ESXi 4.1. ESXi 5.0 requires a newer version of the OpenManage VIB and Managed Node. 

I've seen this happen with ESXi 4.1 hosts after a patch accidentally wacked the /tmp/scratch folder. You might want to check if that directory still exists on the hosts that exited maintenance mode automatically. If they're missing, you'll want to mkdir to create it. Also, you'll want to check if persistent scratch is setup correctly on each host by following this VMware KB article: VMware KB: Creating a persistent scratch location for ESXi 4.x and 5.x 

We have a Dell Poweredge 2970 with a PERC 6/i RAID controller. We have a one drive RAID 0 array (we wanted to add the drive as a JBOD but the PERC forces you to create an array to access it from the PERC). Can we take the one drive RAID 0 and move it to a new server (one that doesn't have a PERC)? Since there's only one drive in the "array" there's no striping going on...the only issue would seem to be if the PERC has some metadata on the drive that would prevent Windows from reading it. Does anyone have any experience with this scenario? 

My experience has been that the configuration files are in one place, and as long as you backup that one place, restoring the application works fine. For user applications, that one place is usually the user's home directory. For server application, you read the man or info page to find out where the application stores its config, and then you back that up. Usual locations are in /etc or in a sub-directory of /usr. Porting app configurations between different physical, or even virtual, machines, has generally been pretty easy. Install the app, and then overwrite the config file or config directory in the new target. Compared to Windows, my personal experience is that restoring configuration settings is much easier in practice on Unix-style systems. 

I am expanding my rescue usb flash disk. The USB's partition is bootable, and I have installed GRUB to the MBR of the jump drive. It works pretty well - I can boot Freedos and run some utilities,and I can boot PING. I would like to add the Ubuntu 9.10 LiveCD to this rescue usb drive. I have a working jumpdrive, so I can pretty easily copy over the files. I could hunt down the needed entries needed for GRUB, but the Ubuntu LiveUSB uses a fairly complicated syslinux configuration. Besides, I would like to keep as much of the current LiveCD/LiveUSB look-n-feel without porting the syslinux config over to GRUB. At the suggestion of ~quack I tried a entry of: 

The short answer is yes, that's the level of performance you should expect to see from the H200. The long answer: The H200 is the old SAS 6iR with SATA 6Gb/s support. It doesn't have the usual features you'd see on a RAID card (battery backup unit, onboard caching, RAID5/6 support). The cache determines how fast your RAID array is (along w/ the # of spindles and type of drive), so no cache = slow performance. Add the 7200RPM SATA drives (which are slow compared to a 10-15k RPM SAS drive) and that's the level of performance you can expect. 

Our company just switched to a new email domain and wants to keep the old domain around for a year as they notify everyone about the change. The old email domain was on Exchange 2007 and the new email domain is on Google Apps. Everyone is 100% on Google Apps, and the Exchange server just forwards on emails. I would like to replace the Exchange server (it's underpowered and a waste of resources just forwarding emails) with something lightweight, based around Postfix or another email server. It needs to do the following: 

Go to the Member’s Overview page and click on Reverse DNS Configuration Manager (look near the “Network Information” button — there is a link to it). Just make sure a corresponding A record exists for the PTR record (i.e. make sure you have an A record for foo.example.com first). 

After this change you can add the Relying Party Trust using the url $URL$ Another option would be to add it using the url $URL$ With the former, you need to be sure that the change persists when you update Dynamics CRM. The later seems to be better because you don't need to modify the web.config and only if Microsoft doesn't remember to change the handler name and its location. Either way, it's best if you check it after updating Dynamics CRM. Everything is running now on port 443. 

You can try to see what's going on with the SQLAgent (or maybe powershell) process using the Sysinternals Process Monitor: $URL$ With this tool you would be able to see where the "Access denied" is comming from. Another option would be to use a "Operating System (CmdExec)" step instead of powershell and call the powershell script like this: 

This way you can choose the request path of the ISAPI extension (in this example: $URL$ otherwise without this change you will need to call the ISAPI extension with the name of the DLL ($URL$ 

Tried to mess around with the URL reservation but with no success. Tried to add another IP to the network interface again with no success. It seems that ADFS always tries to bind to 0.0.0.0 instead of a specific IP. If the reservation is not there, then ADFS won't bind at all. I've also tried what Thomas suggested, but ADFS always catch the request first. (because of the URL reservation) I've got it working by getting Dynamics CRM to provide its metadata on another URL path. Basically I thought I could do this with URL rewrite rules. I was about to create a new rule when I found this on the Dynamics CRM web.config 

With regards to clonezilla, presumably, the client and the server could reside on the same machine. Install the server, perhaps testing with a separate machine, and then install the client and have it connect to localhost or to an assigned IP of the server. 

Start the capture running, filtering on a windows box. Launch thunderbird. See what comes up. Start a new capture running, filtering on the linux box. Launch thunderbird. See what comes up. Iterate, filtering out stuff that seems obviously not related to the differences between the IMAP connection between the two machines. 

Don't forget your gpg keys (from the same thread) How to install all the desired packages and uninstall all the undesired packages 

When you need to clean a system infected with malicious code: 1) Archive the user data 2) Some some other system, or at least some kind of rescue CD, to scan the data for traces of malicious code. 3) Clean install the system, including MBR on the disk. For the paranoid, re-flash BIOS code on all components. 4) Restore the scanned/cleaned user data to the rebuilt system. In my opinion, do not waste time trying to clean a compromised system in place - with today's malicious code, e.g., root kits, this is impossible. 

Profile the network traffic. Try out some actual monitoring tools: $URL$ You're looking for Top Type of traffic (likely HTTP, but who knows), Top Talkers (should be your servers, but who knows), and potentially Malformed Traffic (large amount of TCP retransmissions, malformed packets, high rates of very small packets. Probably won't see, but who knows) At the same time, work with your management to develop a network resource usage policy. In general, business terms, what business needs does the computer network exist to meet, and what are appropriate uses of the resource. This thing is costing money, so there has to be a business justification for its very existence. Your company has policies for handling the "petty cash" drawer, and I would bet your network infrastructure costs a lot more that. The key thing to focus on is not catching people doing bad things but rather watching for potential malicious activity that is degrading network functionality (i.e., the employees' ability to get their work done). Southern Fried Security Podcast and PaulDotCom Security Weekly cover information about creating appropriate security policies. @John_Rabotnik idea for a proxy server was great. Implement a proxy server for web traffic. Compared to traditional firewalls, proxy servers give you much better visibility into what is going on as well as more granular control over what traffic to allow (for example, real web sites) and what traffic to block (URLs made up of [20 random characters].com) Let people know - the network is having a problem. You are monitoring the network traffic. Give them a mechanism to register network slowdowns, and capture enough meta-data about the report so that in aggregate, you might be able to analyze network performance. Communicate with your coworkers. They want you to do a good job so that they can do a good job. You are on the same team. As a general rule, block everything, and then allow what should be allowed. Your monitoring from step one should let you know what needs to be allowed, as filtered through your network usage/security policy. Your policy should also include a mechanism by which a manager can request new kinds of access be granted.