In Value function methods (or Critic methods) we usually choose one of the following options to select our actions after we have estimated the relevant value function: 

In general, with Policy Gradients (PG) you can have two 'flavors' of them: Stochastic and Deterministic. In your case, you can build a NN that outputs continuous actions or one that approximates the sufficient statistics of a probability distribution that you will sample the actions from. The main references are: Theoretical Proofs and NNs with DPG. There are no rules on which one to use. There are many examples in the papers I cite that DPG outperforms SPG but also the opposite. Sometimes you really want the output to be deterministic. For example, in pricing: If a client comes in a store and you use SGD for assigning prices, the client will encounter every single time different prices (small differences but still differences). In my personal experience I find it a bit hard to stabilize the DPG with NNs and you need also to come up with a good exploration strategy. However, once the network is stabilized you get the continuous action values that could control your vehicle. This is a detailed example of controlling a car with DPG using tensorflow and Keras. For general theoretical exploration about the two frameworks I suggest you this Master Thesis. It provides a concise overview of the two methods (no NN implementations). You can look up also the answer I gave here if you decide to get on board with PGs. 

I think your best approach is to use Imitation Learning. Many techniques in imitation learning use Supervised Learning so you do not need to use emulator. Check DAGGER which is used in continuous action scenarios or the recent AggreVated algorithm (just ignore the theoretical parts of the paper). As a start you can use Supervised Learning just for experimentation and then use the above algorithms. I would suggest though to use even a poor simulator just to have an idea of how your implementations behave after the. Bare in mind that RL tries to solve an optimization problem (maximize an utility/cost function) whereas the Supervised Learning methods try to optimize the difference between model's prediction and ground truth. Just be cautious of the algorithm's behavior before you go live. 

create_model is a function that builds the Neural Network Model. The fitting (last row) gives a long error message: 

Now, I want to apply clustering to this latter set. I guess that I could get better results if I downweight the dummy variable columns (proportionally to their number) because with equal weights the distance based clusters will be distorted. My question is, how could I get the following weight vector from the new data set: 

I have read quite a lot about discretization techinques, measuring WoE and IV etc., but the basic question - when is it worth to use binning and when isn't - is not entirely clear. On the one hand, the result of discretization seems quite "pretty" and more easy to handle for me, but on the other hand it is always emphasized that binning always causes information loss. Could you provide some clues when should one use this technique? 

I have a factor variable in my data frame with values where in the original CSV "NA" was intended to mean simply "None", not missing data. Hence I want replace every value in the given column with "None" factor value. I tried this: 

How could I compute similarity taking semantic distance into account? Shall I use word2vec representation instead of TFIDF? 

I have studied the usual preprocessing methods for Machine Learning but I couldn't cope the following specific problem. I apply the "usual" preparation for modeling (dummy variables, normalization, PCA etc., of course in the necessary cases) to the training data. So far, so good. But when I get the to-be-classified new data to make prediction the model constructed above, it's evident that I must apply these preparatory steps to this new data set as well. And the problem arises here, because if I simply apply the preparatory steps for my new data in turn again, these doesn't take into consideration the characteristics of the training data. So, if I convert the new data factors into dummies, then it takes only the existing factor levels in the new data into account; if I min-max normalize the new data, it will be normalized according its own min-max values, disregarding the values in the training data; and if I use PCA, then the resulting components from the new data will be totally independent of the training data. So essentially my point is that applying the same conversions separately to the train set and the new data set(s) (which could be only one observation as well), then the two resulting transformed sets will have nothing in common, so the prediction will be baseless. I found some traces that in some cases there is some "learning" step in the training phase in these transformations as well and apply this "knowledge" to new data (caret and Sklearn, for instance, with "predict" could transform to the new data with characteristics learned from the training data), but generally speaking this inconsistency remains unmentioned otherwise. What is the correct practice here? 

I will try to answer this question conceptually and not technically so you get a grasp of the mechanisms in RL. 

I assume that you are referring to policy gradient estimates. Adding any kind of function to your policy estimation, which is dependent on the state of the environment, first of all, does not bias your gradient estimator (Proof Here). The basic idea of subtracting a baseline from your action value function (and thus forming the advantage function) is that an unbiased estimator of your policy gradient is still unbiased if a constant is subtracted from that estimator. Then, that constant can be chosen appropriately in order to reduce the variance of the new estimator by optimization. If you have access, you can find a very good explanation in the book Statistical Reinforcement Learning: Modern Machine Learning Approaches in section 7.2.2. Also [2] and section 3 in [3]. As you mention it can be viewed as a control covariate addition [4] which is used to reduce variance in Monte Carlo estimations. A good choice for that function is to use the usual value function ($V(s)$) which reduces the variance of your estimate. Hope it helps! 

Stochastic Policy Gradients (SPG): Output is a probability over actions. For your algorithm, the output would be the parameters of a pre-specified distribution (usually Gausian) as Neil described. Then you sample that distribution in a similar way you sample the Boltzman distribution. Deterministic Policy Gradients (DPG): Output is the value of an action (e.g. speed, height etc). 

The concept of state-action values $Q$ is to denote how good is to be in a particular state and perform a particular action in terms of expected future reward. From what I understand from your question, you are interested in the problem of model uncertainty (uncertainty on the dynamics of the system). In other words, our artificial agent interacts within an unknown environment (transition dynamics $T(s,a,s')$ and reward dynamics $R(s,a)$ or $R(s,a,s')$ are unknown). The framework you should take a look at is Bayesian Model-based RL. I outline an approach so you can have an idea: Modelling Transitions First assume that we have uncertainty on the transitions of the environment $T(s,a,s')$. To tackle this we will assume that our agent maintains a distribution over possible transitions. Without getting into the theoretical math, I will illustrate this by using a simple Dirchlet-Multinomial model: The states are sampled from a Multinomial likelihood $s'\sim Mult(p_{ss'}^{a})$ and we assume a prior over the transitions $p_{ss'}\sim Dir(\alpha)$, where $\alpha$ is set to $1/|\cal{S}|$, where is $\cal{S}$ is the state space. The posterior over transitions will be also a Dirichlet because of the conjugacy of the likelihood and prior distributions. To update such a posterior you need to perform simple algebraic calculations and maintaining the counts of each transition. The Algorithm The agent does two processes: 

while in other cases there is no such an external output but users just leave to the Embedding layer to decide the representation vectors. I don't understand what is the real difference between these approaches regarding the desired outcome? Maybe the internal-only solution is not a semantic representation? What is the point of applying embedding layer to an external matrix of which the rows already have fix length? Moreover, what is the purpose/effect of the parameter of the Embedding layer? Am I correct guessing that this set to True let the Embedding layer fine-tune the imported word2vec weights to take the actual training examples into consideration? Further, how to instruct Embedding layer to properly encode "metacharacters"? Setting the parameter True it can incorporate padding zeroes but what about UNK (unknown), EOS (End of Sentence)? (By the way, I cannot understand what is the point to explicitly sign the end of sentence in a sentence based input...) And finally: how could a model predict the translation of a word which is not represented in the training set? Is it tries to approximate it with the "closest" one in the vocabulary? 

I have started to study ANNs with Tensorflow and Keras. Now I want to find a solution to use ANNs over Hadoop. I have learnt that Spark 2.0 does have a Multilayer Perceptron Classifier, but as far as I can see it is quite "primitive" in comparison with TF and Keras (there are only feedforward types and hidden layer/output layer activation functions are hard-wired), there is no wide variety of optimizers, cost functions, architecture types etc. Is there any competitive alternative for large scale neural networks? Of course, I could use Amazon with very powerful, GPU-driven machines but these are not parallel frameworks either... 

There are lots of questions but I will try to answer in a way that might clear things up for you and also give you some guidance. Please note that the proofs for your questions involve lots of math operations so instead I will provide you with references. Your main reference is the paper from Sutton PG Methods with Function Approximation. I highly recommend you to read the paper a couple of times (or even more!) and do some search in the relevant literature when you will be familiar with the main objectives, notation and math around the general approach of the methods. PG Methods are not easy to get a grasp of them mainly because of their sampling nature, notation and discrete/continuous math involved. PG Methods satisfy (or at least should) the PG theorem (eq. 2 from the paper). An interesting approach would be to substitute the true $Q^\pi (s,a)$ by some approximate function ($f_w$ in the paper, $Q_w$ in your question). Now, we are wondering what conditions should be satisfied by that proposed approximation in order to satisfy the PG Theorem. The first thing you notice is that a natural choice for updating the parameters $w$ is to update them towards the direction that minimizes the mean squared error of the exact $Q^\pi (s,a)$ with the function approximation. In your question this is the $\epsilon$. In such a scenario the exact $Q^\pi (s,a)$ is estimated using unbiased samples such as $r_t$. This is explained in detail in Part 2 of the paper. For the PG theorem to hold (proof consists of the 3 lines before Part 3) the grad of your approximate function should satisfy the compatibility condition. To sum up we started from PG theorem and we found a suitable family of function approximators for our action-value function that the PG theorem holds. In Part 3 you can see an example of a compatible function. From this of course you can use even non-linear approximators such as NNs. A clarification on on/off-policy: The David Silver's slide that you posted here has to do with theoretical guarantees and has nothing to do with an actual RL algorithm. By the way Q-learning algorithm in which you use the $max_{a'}{Q(s',a')}$ is OFF-policy as you you don't actually use for updates the ongoing policy. Hope this helps! 

Is the longer encoding array (word2vec or any other kind) always more precise than the shorter (not regarding the computational cost, only the performance)? 

At human check I see that as a result, all predicted labels are a constant sequence. What are the possible ways of improvement? The training data seems OK (at least as far as any pile of handwriting could be). 

I want to apply Connectionist Temporal Classification for an OCR task where I have a bunch of images with text of variable length and the output is the text itself: 

I have discovered that Amazon has a dedicated Deep Learning AMI with TensorFlow, Keras etc. preinstalled (not to mention other prebuilt custom AMIs). I tried out this with a typical job on several GPU-based instances to see the performances. There are five such in the Ireland region (maybe in other regions exist even more, I don't know, this variance is a bit confusing): 

I face with a special NLP (?) problem. I have a sequence of program development steps with code (actually a Git repository with multiple commits of build sources) and a full list of test cases (which I haven't any more specific closer description about apart from some quite general ones like id, timestamp etc.) of which subset that have been run against in a specific build test runs. Based on this historic data I have to build a machine learning model which predicts/recommends relevant test cases for any future build of the given software. Apart from the extremely large amount of data points (several hundreds of program files with several hundred megabytes of gross length and several thousands of potential test cases) I cannot see any promising way to extract features from the code base for the prediction. My first idea is to trace the changes in respective code files per build runs and the executed test cases (so the independent variables are the IDs of the files with binary values if they are modified or not in the current development stage, and the goal is a multilabel classification with as many labels as the possible test cases are) but it's evident that the source file change is a very poor guess for a necessary test case to involve. Is there any practice to a proper feature extraction for this problem? I'm aware that program code handling with ML/DL does exist but usually in a form of code generation and syntax checking which are not my cup of tea.