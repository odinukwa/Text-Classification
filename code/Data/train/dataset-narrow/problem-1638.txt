This may be a bit off of the beaten path but I've had good luck chaining in iPXE as the actual bootloader. Among other pluses it has a mechanism to load a kernel module along with your initrd. Check out $URL$ for an example. 

Use extended access lists. This will allow you to both insert entries into existing ACL's as well as remove entries without having to remove/re-add the entire list. So, for example: 

Now - finally - with all of this laid out, your question is easier to answer: There's nothing inherent to anycast that makes it more resistant to DDoS. Each of the potentially millions of flows of DDoS traffic will find their way to their nearest instance, likely making it unavailable to any other legitimate clients who are would otherwise be routed to these points. Now, if the vast majority of the hosts on the botnets in use happened to be in, say, Eastern Europe and one of the anycast routes happened to be originated in a nearby PoP (again - "nearby" in terms of routing topology) then this traffic would be sunk to one point while much of the rest of the world continued to resolve to the same route that was also hosted at convenient points on other continents. In this particular case anycast would arguably be one of the best mechanisms to minimize the damage of a DDoS attack. This is highly contingent on how the anycast routes have been distributed and how policy has been configured (see #3 above - not a trivial problem). Clearly this use-case isn't as compelling in the case of a DDoS attack that's truly distributed. If properly engineered, though, the localization of the anycast routes means that the attack load can now be spread across an arbitrary number of geographically dispersed physical hosts. This will tend to dilute the effect of the attack on the target as well as potentially spreading the impact across a bigger chunk of the network. Again - a huge amount is contingent on how things have been engineered and configured. Why is this considered a win over round-robin? Simply because it's possible to deploy an arbitrary number of hosts without the need for separate load-balancers on the individual IP's and there's also no reliance on the timeout value for particular clients deciding to move over to another resolver. One could literally deploy a thousand hosts within a single data center with the same IP and balance the traffic accordingly (nb - obviously massive practical limits based on size of ECMP tables, etc) or deploy a thousand geographically disparate sites each with a thousand hosts. All this could be accomplished without changing a client configuration, without the (admittedly usually clustered) point of failure of a load balancer, etc. In short - when properly engineered it scales as well as the Internet as a whole. 

First one is pure software and second one is kernel accelerated provider accesible as PKCS11 token. Exactly those two on my old T1 Niagara are doing 8.4 sign/s versus 19740.0 sign/s. That's for sure huge difference. Modern x86 CPUs can accelerate AES for example and as far as I know it is used in software kernel provider. Check yourself what's the difference. More important is to have speedy asymmetric ciphers, because they are used during establishing a connection and are more CPU hungry... web applications close connection often. Btw KSSL is in fact just in kernel SSL encrypting proxy... a fact it happens in kernel contribute to speed too. Just to compare... on another machine, ~ same age as T1 noted above, but x86 in VMware is doing for me 42.1 signs/s versus 98.6 signs/s for rsa2048. So more than doubled speed. 

So you have to calculate values for those param to mimic behaviour you get by setting 4 params to Solaris net stack. Btw check in Linux. 

Is there any way to get network configuration of iLOM in Solaris SPARC? I want to get the IP address of iLOM console at least. It's pretty easy on x86 by , but for SPARC I can't find something that do the same. I found some mentions about and , but those are not available for systems with iLOM. 

Zabbix, when compiled with CURL support, can directly monitor web services including complex more steps scenarios. You can setup triggers on HTTP return code, returned data, response time... Documentation here. 

option - when you want to stay with FreeBSD, check FreeNAS to automate complexity you are afraid of. option - NexentaStor, it is Solaris based storage appliance SW with great management web gui. Up to 18TB setup is for free. Again there you can easily manage complex vs. a lot of datasets configuration. 

The whole idea of RFC3927 is that the address should be stateless. As such, DHCP clients are supposed to randomly generate the last two octets of the address and subsequently verify that said address is unique via an ARP. Statically setting a value (i.e. 169.254.1.1) defeats that purpose. Let dhcpcd (or whichever client you're using) do its job and it may address your issue. 

FC and 10GE use different bit encoding mechanisms, which dictates the maximum theoretical throughput for either. FC uses 8b/10b encoding while 10GE uses 64b/66b. What this means is that within the 8 Gbps for FC, 10 bits are sent for each byte of actual data. On the 8.5 Gbps (full underlying line rate of 8G FC) this comes out to 8.5 * 0.8 = 6.8 Gigabits per second. For 10GE this number ends up at 9.7 Gbps - or about 42% faster. There's some nominal amount lost in FCoE for Ethernet headers, of course, but it's a very small amount when compared to a 2.3k frame. That said, the useful bandwidth of the 10GE FCoE can be shared with other network data, although there are environments that dedicate 10GE FCoE to -just- storage traffic. There are a few things to consider when looking at converged fabric, including: 1.) What's the actual amount of data crossing the notional FC link? Very, very few of the SAN's that I've seen (in some very large networks) even have a handful of consistently busy 4G ports, much less 8G. Most of the world would probably operate fine on 2G (..and much does). 2.) There are mechanisms in place with various implementations of DCB to guarantee lossless bandwidth to FCoE traffic. This means that if you set aside 4Gbps for storage traffic that this bandwidth will be available between the CNA and the switch under all circumstances - but in instances where the additional 6 Gbps is not otherwise in use that it will also be made available. By the same token, all 10Gbps is potentially available for normal data if said bandwidth isn't otherwise in use. The specifics of how these allocations is accomplished is going to be somewhat vendor dependent, but the overall behavior should be similar. 3.) Where do you break out the actual FC traffic to connect to the storage target (assuming said target isn't FCoE itself). The design of the intervening sections of your network will vary based on where the FC itself is broken out, requirements for multi-hop, etc. Overall the speed king at the moment is 10G FCoE. This may change with the introduction of 16G FC - and, again, when 40G FCoE shows up. There's often a big win in terms of cabling, manageability, etc for FCoE - one connection to one port on one switch (x2 for redundancy) vs a completely separate infrastructure for traditional FC. FCoE is also generally managed just as normal FC is (same WWN setup, targets, zones, masking, etc). As to IOPS - as mentioned above, this will likely be driven far more by the type of storage in use than the link in question. 

The most likely you forgot to enable forwarding. Add to , then or restart. Also try to add following to OpenVPN config: 

Note that adding interface to bridge, sets promisc flag appropriately. Bridge interface need not to be in promisc mode. I got the same setup running, but on OpenSUSE, TAP interfaces are created during startup and OpenVPN just opens them - no start/stop script in OpenVPN. 

Note that means command has to start in user login shell loading user environment too. In case that login shell is something like for security reasons, there should be a problem. Try to change to and potentially do fine settings in for that command. 

Every chunk of data has fair checksum on ZFS. So ZFS know which drive holds correct data in redundant setup when failure. Running will repair data or spread data to all running drives for RADZ. ZFS employs Reed-Solomon's error correction which is best for bursts of errors. Missing drive is such burst of errors, which R-S can correct. 

Zone virtual interface has some features limited... some states can't be setup, packet filter doesn't work in zone too. If I remember right, zone interface can't send ethernet broadcasts, so then no DHCP. Btw why you doing that bloat about setting up zone interface? What about this? 

I think it is more probably a DNS issue than GSSAPI. likes prompt DNS responses to work promptly during connection phase... cause of logging and access checks. 

May be your old mirror set was a hardware one, not the ZFS. Depends on your HW. Check partitions table with if something is there. ZFS can reconstruct its pools no matter what order are disks placed in. 

Take a look at some of the copy-on-write file systems (btrfs and ZFS, for example) that actually take measures to validate the state of on-disk data. This only makes sense in the context of multi-drive setups, though, as you need to give the file system at least a fighting chance of finding a clean copy of your data. 1TB disks are crazy cheap at this point and certainly wildly cheaper than the potential fallout of the unscheduled crash you're almost certain to hit. Seriously - what you're asking about is analogous to asking to find a way to keep driving on a tire that's bald and has a gigantic bulge on the side. We can't say precisely when it's going to end badly just that it will. 

First, the basics - a socket is the 4-tuple of (srcip, srcport, dstip, dstport). If any of these values change, it's a different socket. When a given host opens a TCP (or UDP, for that matter) socket its source IP is already known, the source port is selected randomly from the ephemeral range (greater than either 1023 or 1024 - forget which) and the destination IP and port are supplied to the stack by the calling process. On the server side, a connection is set up and seen coming from the srcip and srcport given and bound to the dstport and dstip. This entry (again - some 4-tuple) is held in the host's connection table which will then allow incoming packets to be associated with the appropriate connection. TCP handshaking is the process by which the TCP stacks on the respective sides negotiate the parameters for sequence numbers, window sizes and such. By the time this occurs the port numbers have already been determined. Again - if any of the values in these tuples changes after the initial connection then, by definition, the packets are no longer associated with the original socket. There are certain circumstances in which other port numbers may be specified by the application in use (i.e. FTP, RPC) but in all cases this calls for the establishment of a separate socket, not the renumbering of an existing one. In the FTP case this would correspond to the initial connection on port 21 (control) from host -> server and then the subsequent connection on port 20 (data) - which, depending on the mode in use, may be set up in either direction. I can't emphasize enough, though, that this constitutes two separate sockets. Referring back to the OSI stack, this would very much be a layer-5 issue. 

Try to import that certificate to some NSS store. For example to Firefox, which is using NSS. It's other implementation of SSL (in fact the 1st one) and you can see attributes of certificate... of course if you succeed with import. Otherwise you got some wrong certificate. 

Note that x86 can get some accel for SSL from CPU. You can get listing of accelerators by running . Even kernel software provider has some optimizations. Those providers are the same ones running KSSL. To measure the difference run following for example: 

In this case you can play with and in SunOS . But best to block those attempts even before reaching with Solaris . Here you can find pretty often updated list of IPs to block: OpenBL I see attempts very rarely in logs, just using this blacklist. Then you can assemble cron job script to update FW rules or optionally there's formatted file available. 

Use , but also add per every network you want to route through VPN. Btw note that DNS setting on other interfaces will stop work, when that interface will not have route to its DNS servers. This is what happens when drops default gateway from your (W)LAN interface and adds host route to VPN server IP through original GW. Depends on your setup, may be there is no working setup and you'll have to change DNS naming to include some subdomain for internal networks. 

Configure status plugin and install collectd to collect system performance data. It's a very lightweight daemon in means of system resources it needs. There's plugin for nginx monitoring: Plugin:nginx and of course can monitor whole other system performance data. As far as is just collector of performance data (stores it in RRD DBs), a tool for displaying data is required. I'm pretty comfortable with CGP... git version is OK. is a PHP app thus it will eat you CPU just only when you will look at graphs. Example graph: Nginx_connections_and_requests.png Btw Amazon EC was always significantly slower than others and most notably for storage. That could be root of higher load. 

If the on-board NIC supports PXE then netbooting to install would be the easiest way to go, and would allow for quick deployment of additional blades in the future. You don't mention which distribution you're using, but both the Debian and RH-flavored variants have guides available to walk through the process of setting up DHCP, tftp, etc. 

Yes - this is valid, and actually pretty common. The router will treat these interfaces (and associated routes) as being part of a common routing/forwarding table which will be kept separate from any others (unless you manually cross-import). 

There's nothing inherently special about an uplink port. You can use those ports to connect to switches or to individual servers (or NAS appliances). The ports can be configured to support VLAN tagging - or, in your case, can be untagged ports in the same VLAN as the 48 GE ports. Ports 51 and 52 are configurable as either stack ports or uplink (read: normal) ports. The stacking mode is just a proprietary mechanism to make up to 6 of these switches show up as a single device. If you buy more of these switches this might be a handy way of growing your network. If you don't, they're just ports. 

What counts as maintenance staff? The above answers seem to imply that only janitors would fall into this category but staff electricians, HVAC engineers, etc are often part of the maintenance crew in lots of shops. Lots of larger enterprise DC's I've had contact with have actually specifically excluded the majority of IT staff - to include senior systems and network engineers, etc. The idea is that a very specific set of DC facilities/operations people should be sufficient to physically operate the infrastructure without particular non-facilities domain experts being allowed into a space that they're often not qualified to be in anyhow. It's actually only in the smaller facilities that I've seen sysadmins typically involved in standard rack-and-stack / cabling. Some network organizations keep their hands in longer, but even they end up splitting off day-to-day cabling (and even a lot of the layout/design) to dedicated facilities people. I've generally just chalked this up to the greater need for specialization. BTW - Dedicated DC facilities orgs will often have their own specially trained cleaning staff. There -is- need to keep these areas clean over time, albeit through different means than standard office space. 

Try to precompile Ruby files to Java classes for WAR file. It is done during WAR deployment nevertheless, so for why it take a lot of time... especially on SPARC machines. Offload compilation files from server to your development machine. 

I'm running small Ubuntu devel virtual on VMware Fusion. 4GB disk and 512MB RAM is enough for development. Network of VM is in NAT mode, so I can access it even when not on Internet. I also configured AFPd so I can edit files directly mounting share. As far as I'm doing Django only that way setup is as following... Django app running under some user account I created, that user homedir is also a root (loaded on login), I use that user to login to AFP. When new project I just clone template machine and create new user account + . Installing to VMware Ubuntu chooses kernel that holds main virtualisation abilities... thus XEN, KVM, VMware. Deploymnet should then be DevOps way... just copying VM files to cloud and starting it online (maybe conversion of disk file or its growing to production size). 

If you have speedy lines between sites you want to mirror, I can imagine something like you export iSCSI volumes from sites storages and put them mirror and add some local disks for ARC, ZIL, cache to lower read/write peaks running over iSCSI. If your storage is mainly for backups, then it would be OK. Nevertheless SUN once had such product behaving like that on ZFS. 

It will inherit netmask from global zone interface. Of course you can setup more interfaces, or put zone only on 'internal' interface (no public IP) and the let provide .