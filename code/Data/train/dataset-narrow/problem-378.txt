So we're wondering if there's a lower-cost solution that would store as smallint but expose the colunms as ints to readers. Like this: 

But it doesn't say how SQL Server determines what is a "large table" and "small table" for purposes of this optimization. Are these criteria documented anywhere? Is it a simple threshold (e.g. "small table" must be under 10,000 rows), a percentage (e.g. "small table" must be <5% of rows in the "large table"), or some more complicated function? Also, is there a trace flag or query hint that forces use of this optimization for a particular join? Finally, does this optimization have a name that I can use for further Googling? I'm asking because I want this "use the cardinality of the large table" cardinality estimation behavior in a join of master/detail tables, but my "small table" (master) is 1M rows and my "big table" (detail) is 22M rows. So I'm trying to learn more about this optimization to see if I can adjust my queries to force use of it. 

Also, feel free to vent about your experiences with tasks like these. I'm sure I'm not the only one who has been handed down tasks like these. 

I have a table that stores version information in multi-dotted strings, ie '4.1.345.08', '5.0.1.100', etc. I am looking for the quickest way to find out which is larger. Is there a faster, or easier way on the eyes than what I have come up with below? One caveat is that I am 90% sure that we should ONLY find dotted numeric values here, but I can't guarantee that in the future. 

Create a new String table populated with a cleanly joined set of data from the original base tables Index the table. Create a replacement set of top-level views in the stack that include and columns for the and columns. Modify a handful of s that reference these views to avoid type conversions in some join predicates (our largest audit table is 500-2,000M rows and stores an in a column which is used to join against the column ().) Schemabind the views Add a few indexes to the views Rebuild the triggers on the views using set logic instead of cursors 

The above cronjob pretty much checks if mongos is running and if not restarts it, i.e. starts it back up after a crash. And after a crash and restart, the memory naturally drops all the way to 5 to 10GB and stays there permanently, unless we do another heavy aggregation or import. In other words, mongo doesn't really need that memory and according to all the issues on stackoverflow, all the docs and what not, mongo is supposed to let the OS tell it when to free up memory. It's supposed to release memory when something or maybe even mongo itself needs to use it for something else, but it doesn't do that and crashes instead. Thankfully, this isn't a super major problem as our 3 MongoS are behind a load balancer, so if one goes down the other 2 can keep our services online. But it's still an issue I don't want to have to deal with in production, especially as our database and load grows. I've found this - $URL$ and have enabled aggressive decommit, but that did not help. Thanks 

Say for example, if a defect is reported in this spaghetti mess, for example a column may not be accurate in certain specific circumstances, what would be the best practices to start troubleshooting? If debugging this was a zen art, how should I prepare my mind? :) Are there any preferred SQL Profilier filter settings that have been found useful in debugging this? Any good tactics, or is it even possible to set breakpoints in nested procs when using debug mode? Tips or suggestions on providing meaningful debug logging when dealing with nested string builders? 

Here is why s are being used as predicates. The column is formed by concatenating: During testing of these, a simple from the view returns ~309 rows, and takes 900-1400ms to execute. If I dump the strings into another table and slap an index on it, the same select returns in 20-75ms. So, long story short (and I hope you appreciated some of this sillyness) I want to be a good Samaritan and re-design and re-write this for the 99% of clients running this product who do not use any localization at all--end users are expected to use the locale even when English is a 2nd/3rd language. Since this is an unofficial hack, I am thinking of the following: 

Will tempdb I/O from a single query be split across multiple tempdb files? (assuming that tempdb is configured to use multiple files, of course!) For non-tempdb databases, MDSN seems to say that yes, newly-added data will be spread across multiple files in a filegroup: 

Our SQL 2014 server has a blazing-fast tempdb drive (2800 IOPS) but a much slower data drive (500 IOPS). Our application runs a few long-running reporting queries that are I/O-intensive and we'd like to avoid them starving our server for I/O capacity when they run. Ideally we'd be able to limit these queries to 50% of available I/O capacity. Unfortunately SQL Server Resource Pool's IOPS throttling is not percentage-baed nor volume-specific. If I limit to 250 IOPS, then it will unnecessarily slow down performance of queries that make heavy demands on tempdb. Slowing down these long-running queries if the server is busy is OK, but slowing them down by 10x+ if they need lots of tempdb access is not OK. So we're looking for workarounds that will defend other queries from these lower-priority, long-running queries, but without unnecessarily hurting performance of these long-running queries if they happen to use lots of tempdb. It's not practical to change the queries themselves to reduce tempDB usage-- these queries are generated by a custom reporting feature that may sometimes generate really complex query plans that spill results to tempdb. So far the best idea I have is to remove IOPS throttling and instead use the "Importance" of a workload group to defend the rest of the server's I/O capacity from these queries. Is this a good solution to the problem I'm trying to solve? What are the pros and cons of using Importance? Or is there a better way to achieve our goals? 

So I ended up solving this issue myself. Not quite in the way I was hoping to do it, but it does work and the websites are no longer going down or even slowing down noticeably during backups. The fix - I changed the kernel I/O Scheduler from CFQ to Deadline, i.e. made it prioritize smaller, faster I/O tasks. That way any website request is prioritized over the backup process and the websites have no down time anymore. Tested and worked just right. Here's how to do it. We first check what the scheduler is currently set to. 

This may need to be done for more disks than just one, assuming hda isn't your only hard drive. E.g. I also had a hdb disk, so I had to do this 

Solved. I upgraded MongoDB from 3.2.1 to 3.2.3 and it magically started working. I didn't even need to recreate / reconfigure anything. It just started splitting the chunks correctly and works fine. 

When a detail table contains denormalized data, should denormalized columns be included in foreign key relationships between the master table and detail table? Here's more details: We have a master/detail pair of fact tables: an table with about 1M rows and an table with about 20M rows. To improve reporting performance for date-range queries we've partially denormalized by adding to the and creating a covering index on with the other columns INCLUDEd. There's already a foreign key relationship between the column in both tables. But SQL Server is unaware that the in both tables is the same if the is the same. Should I help SQL Server to know about the relationship? If so, how? Finally, will adding denormalized columns to foreign keys improve cardinality estimates when joining the master/detail pages by telling SQL Server that cardinality shouldn't be reduced when filtering both tables by the same a date range? If not, then what's the benefit of maintaining this foreign key relationship that includes the denormalized column? We're running SQL Server 2014 an are soon upgrading to SQL 2017, if that matters to the answer. 

I'm confused as to what exactly the SDO_GEOM.SDO_INTERSECTION returns in Oracle. Documentation is here, but only gives examples of two intersecting polygons. What if I have many lines, some of them are contained fully within the polygon and some intersect the polygon. Assume that I need the geometry of any line fully contained within the polygon (i.e. same as SDO_CONTAINS) AND the geometry of those parts of intersecting lines, which are inside the polygon. Example image below. Assume I only need the geometries of the red lines, but not the green ones. Will SDO_INTERSECTION return all of the red ones or only those red ones, which also go outside of the polygon and have some green bits? 

Someone answered this question for me and it is as I suspected. SDO_INTERSECTION will return any lines fully contained within a polygon, as well as those parts of intersecting lines, which are within the polygon. In other words, it will return the geometry of everything in red in the example image. 

I recently inherited a MESS of a search. It's around 10,000 lines of code in the procs/functions alone. This search is aptly named the "Standard Search." It's a proc that calls about 6 other procs, which are entirely composed of string builders, in which each proc has between 109 and 130 parameters. These procs also call deeply nested functions which generate more strings to be assembled into a final query. Each proc can join up to 10 views, depending on the logged in user, which are abstracted from the table data by between 5 and 12 other views per primary view. So I am left with hundreds of views to comb through. On the plus side, it does have a parameter to PRINT out the final query, (unformatted of course!) It's a hot mess. The string builders also have no formatting, and they don't logically flow. Everything jumps around everywhere. Although I couid do an auto-format, to pretty print the code, that doesn't help with the formatting on the string builders, as that would involve refactoring the content of strings, which is a no-no. 

Does this same fill strategy apply to tempdb? And does this answer depend on the type of query, e.g. parallel vs. non-parallel? Or is the answer different based on the kind of tempdb I/O, e.g. for temp ables I create vs. tempdb usage by the database engine for worktables or spilling? 

When setting MAX_IOPS_PER_VOLUME in a Resource Pool, what exactly does "volume" mean? Specifically, how many "volumes" would be the following cases: 

A multi-billion-row fact table in our database has 10 measures stored as columns. The value ranges for some of these columns won't ever be above the +/-32K range of a . To save I/O, we're investigating whether it's practical to store these columns as instead of . But we're concerned about what problems might crop up from doing this, including: 

What are the pros and cons of this approach? What can go wrong? Is there a better way to reduce storage & I/O without causing problems with overflow and requiring existing readers to be rewritten? 

So I have a collection with 55 million documents or so. I've enabled system profiling to check for slow queries and I do have a few every now and again. Example: 

So our MongoS instances use up A LOT of memory. More than even the shards and sometimes crash because of it. Cluster structure 

Each of these is running on its own dedicated server with 64GB of RAM, except the config servers which are running on smaller servers, but they are not relevant here. Situation Our MongoS instances use up even more memory than that, but only after some intensive workload. Usually after some huge import or aggregation. And the problem is that they never drop it afterwards. If we do not do huge imports, the memory will stay between 6 and 10GB, but during an aggregation it will jump up to 95-99% memory usage and will stay there even after the aggregation is finished resulting in us getting alerts for high memory usage on our monitoring software all the time. But that's not the main problem, the problem is it crashes. Problem Now, I know mongo uses as much memory as it has available and frees it up as necessary, however, it doesn't really free it up correctly. It often crashes, in fact it's bad enough for me to have the following cronjob: