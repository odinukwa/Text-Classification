rather than using a statement (though both will work) Other options for persisting the values include saving them to a table or creating a context, but I think is really the 'right answer' here 

You need to make sure that when you are "storing the data in the database", the client you use is using utf8. Then setting the http header to utf8 completes the round trip. The surprise is that it doesn't matter much what character set is used in the database (although it might well be convenient for that to be utf8 too). 

If is leaving an entry in , then you are hitting a bug - perhaps the one described here As you do not have a support contract, you can't raise a TAR. However the first question Oracle Support would ask you is whether you are running the latest patchset, 11.2.0.3 - I suggest you consider this option first as the root problem is an Oracle bug. Note that "it is a full installation" If that doesn't solve the problem, you'll need to try and work around it because as far as I know there is no alternative to for . The linked article suggested dropping indexes before dropping the MV which would perhaps be a good place to start. 

Everything is now back to normal but one mystery remains: before recovery, the client application logged an ORA-00376 to a logging table when a batch job tried to access a segment in the datafile. However this error did not appear in the alert log, how can this happen? At the time of the ORA-00376 error, the following did appear in the alert log: 

See here for the docs explaining this in more detail, but the simple solution is not to run at all - just . Then your table will probably settle into a steady state where 'holes' in the data are left and can be used by later updates. As for "insert time", I'm surprised at your results. My expectation would be that time would be slower after a - but if all the blocks are in the cache, the overhead of finding free space inside the current block might be higher than adding the new row at the end of the heap even if the number of blocks accessed is higher 

Is there anything there that gives a clue as to why the procedure failed when it got to table 12? Or is there a limit to how long a procedure can run, or a query can run against a linked server? All tables except 12 and 13 have <2,000,000 rows, and 12 and 13 have about 70,000,000 each. Here is the stored procedure: 

The range of a is -9223372036854775808 to +9223372036854775807, which is -2^63 to 2^63-1 — or 2^64 distinct integers. The range of your identifiers is 2^63 distinct integers so they'll fit nicely in a as long as you don't mind an offset: 

You can produce a list of all possible year/month combinations is not null (which it can't be as it is in the PK, but I mention it as there is a way of working around if nulls are permitted) 

Have you considered scanning the texts as they are input and flagging them (possibly with a many-to-many link to another table containing your list of keywords)? 

One way is to nest analytics by using to set a flag on each change of state and then sum them up in a second step (you don't need to specify a windowing clause because "If you omit the windowing_clause entirely, then the default is RANGE BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW.") 

That's really all you need to know about getting the absolute time from anything of type , including . Things only get complicated when you have a field. When you put data like into that field, it will first be converted to a particular timezone (either explicitly with or by converting to the session timezone) and the timezone information is discarded. It no longer refers to an absolute time. This is why you don't usually want to store timestamps as and would normally use — maybe a film gets released at 6pm on a particular date in every timezone, that's the kind of use case. If you only ever work in a single time zone you might get away with (mis)using . Conversion back to is clever enough to cope with DST, and the timestamps are assumed, for conversion purposes, to be in the current time zone. Here's an example for GMT/BST: 

alternatively look up after the (this approach can be helpful in more complicated situations such as if not all records in have a corresponding record in ): 

Does that indicate that clustering succeeded (ie all the rows are now in the order indicated by the index)? What do 'removable', and 'nonremovable' indicate? 

In the example you have given, your best bet is not to have a composite index at all if you are free to change the join order: 

dbfiddle here If you initially load a lot of ordered rows, you may want to use some other algorithm for generating the initial values as you will hit the worst case for space usage (each row will use one more bit for than the row before). You could step through same-length values for a sufficiently large number of bits (eg for 8 values: , , , , , , , ). 

This is possible, with the aid of a helper function which replaces the long content with (any column in my example, but you would modify that for the types you want to suppress): 

and the reason I'd like a single statement solution is to prevent a race condition where another transaction inserts/commits data between the and in the primary transaction. 

You can cut this down quite a bit - the first thing to notice is that you don't need the information in the table at all to get your results: 

A CDBMS is a specialized database for handling queries like "show everything that is related to x". Unless that is what you want (and unless you don't want all the features of a traditional RDBMS like MySQL), you don't want a CDBMS. 

The part of the filter that performs the is . For that to be sensible, it is likely that you need two of the following three statements to be true: 

The rows remain visible. Test (1) shows that the does not prevent any rows from being visible to subsequent queries. Test (2) illustrates the table-lock taken immediately by a normal - the query waits until the finishes and returns a zero count. testbed and long running query: 

This can then be joined to and and filtered for the row for each with the lowest generality (), perhaps something like this: 

Where 96, 116 etc are chosen to match the unit of the value '2000' and the point on the globe you are calculating distances from. How precisely this uses indexes will depend on your RDBMS and the choices its planner makes. In general terms, this is a primitive way of optimising a kind of nearest neighbour search. If your RDBMS supports GiST indexes, like postgres then you should consider using them instead. 

dbfiddle here Notice that the row with =5 is not in the result set — the drops descendants of =4. If the minimum level is global, you need an additional step to strip all results not at the minimum level globally: 

Something must have gone wrong at the O/S level - check this page for some steps to track down the problem 

edit: I've managed to get a simple-ish reproducible test case. To me this looks like a bug due to some sort of race condition. schema: 

I'm not clear is this is a question about modelling or the hierarchical query itself - but assuming the latter. 

In the event of a checksum failure on a replication master, will that corrupt data replicate to slaves or will replication halt. Does it depend on the setting of ? This README has some useful related information but doesn't directly answer the question. 

No. I'd say there are certainly cases when single-field keys are inferior to compound keys, at least for the purpose of foreign keys. That is not to say you should not have a single-field surrogate key as well if you prefer, but I personally prefer the key that is most often used as the target of a foreign key to be called the primary key I'll attempt to illustrate my point in the the following examples, in which: 

It looks like you can using a fqdn rather than an IP address - if that is right then you could make use of a dynamic DNS service such as DynDNS to get a stable DNS name that points to whatever your current home IP address is 

As with any licensing question, I'd suggest you read Oracle's info carefully including the full license and satisfy yourself that what you want to do is OK rather than just taking my word for it. 

How about for a and for a field - eg or ? Normally I'd avoid including the data type in the field name - much better if you can infer what you need to know about the type from the name of any field (a field called is unlikely to be an ) - but being able to tell the difference between a and a is often helpful. 

--edit this is not a good solution after all since on MySQL, is a synonym for , and so allows non-null values than 0 or 1. It is possible that would be a better choice 

Oracle should be able to use the statistics to determine the high and low values for any column if you are keeping them up to date: 

In postgres, can be abbreviated as , and as . I will use the shorter type names for simplicity. Getting the Unix timestamp from a postgres like is simple, as you say, just: 

11.2.0.2 is a minor release so for most purposes it will not be much different to 11.2.0.1 depending on your platform, 11.2.0.2 may not be the latest available (eg 11.2.0.3 on Linux) 

Generally speaking moving application logic into the database will mean it is faster - after all it will be running closer to the data. I believe (but am not 100% sure) that SQL language functions are faster than those using any other languages because they do not require context switching. The downside is that no procedural logic is allowed. PL/pgSQL is the most mature and feature-complete of the built in languages - but for performance, C can be used (though it will only benefit computationally intensive functions) 

This is just intended to demonstrate redo usage of various operations rather than answer the whole question. Results on my 10g instance are not 100% deterministic, but the broad picture remained the same each time I ran through. For the heap tables, I do not know why the generated more redo. testbed: 

Normally when you add a unique constraint, a unique index with the same name is created - but the index and constraint are not the same thing. Have a look at to see if there is an index called and try and figure out if it is used by something else before you drop it! 

Assuming you still have the archivelog files and they were not deleted when they were backed up, you just need to tell RMAN that those files aren't coming back: 

Apex users are stored in Apex tables and you can create groups to restrict access to various parts of an application. 

Yes, that is exactly what you need to do. The export needs a consistent MVCC snapshot from the start of the operation. UNDO_RETENTION is ignored if your UNDO tablespace is fixed size or can't be extended: 

VACUUM rewrites the entire block, efficiently packing the remaining rows and leaving a single contiguous block of free space (though this space isn't zeroed and the physical disk file might contain the remnants of deleted rows which of course are in no way visible to the database user). test schema: 

So any performance gain will be slight. In the following tests I dropped and recreated the table and sequence between each execution: testbed: 

I have a CSV file that can be loaded into a table easily with , but the catch is that the file is stored in the database in a field. Is the only way to use on this data to write it out to a temporary file, or is there some clever trick, perhaps involving ? 

so you cannot find the 'last inserted id' from the sequence unless the is done in the same session (a transaction might roll back but the sequence will not) as pointed out in a_horse's answer, with a column of type will automatically create a sequence and use it to generate the default value for the column, so an normally accesses implicitly: 

If you find that a simple covering index (which will solve all your query performance problems immediately) has an unacceptable impact on INSERT performance, you can introduce an artificial 'clustering' column to the table: 

But, you need to be aware of the modulo and wraparound and frozen xids. There isn't any way of converting this into a "time", but if you capture the value for your tables now, then compare at a later date, you can get a list of tables that have changed 

Because it is just a hint - the CBO is clever enough to ignore it if it means full scanning the clustered index and the table instead of just the table Also the correct hint for a clustered scan is rather than eg 

You already know what order you want your data in and how to do it - in particular, your ordering does not allow ties You have 1 bit field in each row You want to concatenate the bit fields in each 'set' of 3 'adjacent' rows Your version of SQL Server supports windowing functions 

Which tells us nothing about your Oracle Server version unfortunately! The bit below ("Connected to...") is about the server side 

You are too modest - your SQL is well and concisely written given the task you are undertaking. A few pointers: 

--edit To allow archiving without adding complexity or risk, you could choose to keep summary rows in a separate summary table, generated continuously (borrowing from @Andrew and @Garik) For example, if the summaries are monthly: 

If the game area is strictly static, then my preference would be to use the coordinates as the primary key. On postgres, I'd probably do that using a composite type. I don't think there is a direct equivalent in SQL Server - hopefully someone will correct me if I'm wrong. Whether this will simplify the problem or not depends on how exactly you have implemented your searching algorithm - can you post more details on that?