I did not even look at the rest of what you are doing but OpenSSL 1.1.0 simply does not support TLS 1.3 yet. TLS 1.3 is support starting with OpenSSL 1.1.1 only. See Using TLS1.3 With OpenSSL for more information. 

This is a combination of a bad server setup together with missing support for SNI (Server Name Indication) in python 2.7.6. SNI is needed to support multiple certificates on the same IP address. SNI support is in python3 and was added in version 2.7.9 too, but is not in version 2.7.6. If you connect to the server without SNI you get only the certificate, but not the trust chain: 

If you listen on TCP port 443 on a specific IP it will accept new TCP connections on this port even if you only have SSL configured for some of the domains resolving to this IP address. And once the TCP connection is established it will do the SSL handshake and pick the most suitable certificate, that is either the matching certificate if SNI is used (i.e. the client sends the expected hostname in the SSL handshake) or just another certificate if it cannot determine the requested hostname or if no certificate is configured for this name. This means, that you either need to have certificates for all domains accessible on this IP address or accept, that access to domains without proper certificates will cause errors like invalid certificates. This is not specific to nginx but it is because SSL is a layer on top of TCP and information about the requested host are only transmitted within the established TCP connection. 

If you need it, you need it, and that's all there is to it. Admin work is going to be the big scary: sure, you lose time now because some idiot at the co-lo spilled his coffee on a server, but when you bring it in house, and it's your coffee, then the problem is far beyond just calling your hosting provider and demanding that they get their butts in gear. What kind of hardware support are you looking into? It can be very expensive, depending on your needs. Redundant pipe is nice, but the premium is high. We use two sets of bonded T2's and actually had a "moron with a backhoe" incident earlier this year. We stayed up, but it seriously impacted our performance. I'd also add server hardening and monitoring and such. Firewall hardware, patching and patch testing, monitoring...All these things take a lot of time. As an admin, I'd suggest a slow migration from remote to local, to give yourself plenty of time to make sure everything works right (and to back out, if it turns out to be ugly) but as an experienced admin, I know that the likelihood of being allowed to double your costs for a transitional period are very low. Good luck, either way. 

The first line enables listening on port 443 on IPv4. The second line covers IPv6 only. Since you have only a single (IPv4) configuration it is the one which gets used if you connect with IPv4. If you would try to connect with IPv6 instead SNI should show the expected behavior. Instead you might probably use for the default server: 

The point of certificate chain validation is that you have locally trusted (root) certificates and from that you defer trust to certificates send by the peer. So the server should only send the leaf certificate and the intermediate certificates needed to built the trust chain from a local root certificate to the leaf certificate. Which means that you should not send the root certificate but if you don't it gets usually ignored. And you should make sure that you add the certificates in the correct order, that is first the leaf certificate and then the chain certificates in the correct signing order. Some servers or clients might work around a wrong order but you should not count on it. 

Every bit helps, but only a bit. And note that it not only needs the DNSSec records but the forwarding MTA must actually use DNSSec. Most MTA probably do only DNS and don't realize if they get a spoofed MX record. 

To prevent people from being able to see what files are in a directory, with apache, you just need to turn "Indexes" off in the appropriate Directory directive. You can also override the default with an .htaccess file, but for a production server, the default should be no indexes. For file permissions, I'd advocate switching nearly everything to read only, or read and execute if you have to (544 or 554). Leaving write permissions on files that don't need to be written is asking for trouble. 

That should tell machine B that, if traffic comes in for the 192.168.0.0/24, send it out eth1. That should work, and it's a bit easier than setting up a NAT. You'll need to make sure the switch knows to route traffic for 192.168.0.0/24 to 192.168.109.15 (Eth 0 on B). @Ian: Routing isn't necessarily easy, but it's not that hard either. That's what routers do. Assuming your switch is just a switch, with no routing capability, you can't use it to set the route, but you have to have a router somewhere or your network is purely a local LAN with no internet access. Check your machines, and find out where the default gateway is (on the linux boxes you can just type with no arguments. Your default gateway route is the one that looks like: 

You have only configured the use of a certificate for Postfix in the role of the server (i.e. receiving a mail). These are the settings. But the message you refer to is not about receiving mail by your server but about sending mail from your server to another server, i.e. receiving by the other server. In this case usually only the certificate of the receiving mail server gets checked (i.e. the other one) by the sending system (i.e. your Postfix), if TLS is involved at all (your setting considers this optional, i.e. ). Some mail servers are configured to not only provide their own certificate for authentication by the sender (i.e. your Postfix) but also to request a certificate from the sender. This is usually optional, i.e. the TLS connection will be accepted even if the sender does not provide such a certificate to authenticate itself. And this is what caused the message "Client did not present a certificate". If you really want to provide a client certificate you need to explicitly configure at in Postfix with the relevant settings (not ), i.e. etc. See the documentation for details. 

Set up a permissions group, make the computers you want him to be able to admin members of that group, and give him full control over things that are in that group. Pretty straightforward. Active Directory is actually made for that sort of issue. Just create a new group folder, and change the security settings under properties. 

The only thing you really need it for is setting the hostname. Otherwise, you can add some network and gateway settings, but you can just as easily put that stuff in the /etc/sysconfig/network-scripts/ifcfg-ethX files. 

Well first off, JDBC and ODBC are not compatible. JDBC was Java's answer to ODBC, and they fill the same niche, but you can't connect to an ODBC data source using JDBC and vice versa. Now there IS a jdbc-odbc bridge in Java, but that is a compatibility hack, not a native connector (The translation will go JDBC->ODBC->(Target Database) rather than JDBC->(Target Database)) ODBC is simply a standardized interface to access data from different databases. You install the ODBC to (whatever database) driver on your machine, then create an ODBC datasource, and then you can connect to that datasource via odbc rather than having to configure a native driver for the original database. Or if the database is ODBC capable, you can connect to it directly by importing the driver. The benefit is that you can write all your code to be odbc compatible and it'll work with only small modifications regardless of what the real database type is. The negative is a performance hit, and occasional weird behaviour as odbc fails to translate some database specific action in exactly the right way. Support varies. If you're using ODBC to connect to some legacy database, you may have a lot of work to do, but if you're using it to connect to an access database, then it'll work flawlessly. The process that happens on connection is relatively simple. You send your "standardized" query, the JDBC or ODBC driver translates it into the native framework you're trying to access, and then submits the query. If the database returns anything, the process is reversed. If you're having problems, I'd check the driver you're using. If you import a bad driver with java, nothing is going to work right. The first thing you need to do is log the error. If you're using an ODBC datasource, you can just turn logging on in the properties (look to the driver documentation to find out the correct settings; they vary). If you're using JDBC, you're probably going to have to catch and log the SQLException's yourself. 

If all the domains share the same IP address then all you can achieve is to cause a browser error due to an invalid certificate when the user tries to access the site with https. If you don't like this you need different IPs for the domains you want to use with HTTPS and for the HTTP-only domains. 

Redirection is done at the HTTP level, i.e. after the SSL connection was established. Since establishing the SSL connection includes validating the certificate it is not possible to bypass the certificate check for redirects. 

In this case your setup requires Server Name Indication (SNI) and somewhere you have configured another certificate for clients not able to do SNI, like MSIE8 on XP. 

What kind of network timeouts do you have? TCP keep alive will not help if the server is to busy to respond in time. It will only help to detect when the TCP connection is no longer alive because the peer crashed or some packet filter in between closed states because of inactivity of the connection. 

This is probably the issue because for a proper validation of the certificate it will check that the name in the certificate matches the name given to connect. If it would not do the check anybody with some certificate issued to its own site could impersonate your server which is probably not what you want.