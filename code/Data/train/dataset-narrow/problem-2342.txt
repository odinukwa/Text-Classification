One should note that single-tape TMs with time $o(n^2)$ are not as robust and there are quadratic lowerbounds (e.g. for Palindroms) on single-tape TMs whereas a two-tape TM can solve such problems in linear time. As I said above, unless you are committed to single-tape TM model for some reason, even when time is sub-quadratic, there is not a gap to fill, the time hierarchy theorem is as tight as possible. PS: if we are using multiple-tape TMs, i.e. a Turing machine in the class can have fixed but arbitrary number of tapes Fürer's result does not apply. 

Integer Programming. Showing that if there is an integer solution then there is a polynomial size integer solution is quite involved. See 

I am looking for references about the complexity of Boolean formula balancing problem. In particular, 

The complexity zoo doesn't have much about the $\mathsf{SC}$. I am looking for a nice$^\dagger$ problem that is in higher levels of the hierarchy, i.e. a problem in $\mathsf{DTimeSpace}(n^{O(1)},\lg^{O(1)} n)$ but not known to be in $\mathsf{DTimeSpace}(n^{O(1)},\lg^2n)$. As a side question, is there any known reason why finding examples of nice problems in higher levels of hierarchies ($\mathsf{AC}$, $\mathsf{NC}$, $\mathsf{SC}$, $\mathsf{PH}$, etc.) is more difficult than the first levels? $\dagger$ although nice is not a mathematical term I think we intuitively understand what it means, e.g. accepting problem for NTMs is an artificial problem that people are not interested in it aside from it being complete for $\mathsf{NP}$, while the graph coloring problem was interesting before being known to be in/complete for $\mathsf{NP}$ and is still interesting aside from the complexity class it belongs to. 

The other heuristic VSIDS essentially maintains a score for each variable. Every time there is a conflict, all scores are adjusted by multiplying them with a value $\alpha$<1 and adding a constant to those which were "involved" in the conflict. To see what this means think about the sequence $F(v,i)$ which is 1 if variable v was "involved" in the $i$th conflict. Let $0<\alpha<1$ be a fixed constant. The score of variable $v$ at time $n$ is then: $$\sum_{i<n} F(v,i)\alpha^{(n-i)}$$ Intuitively one can say that this tries to emphasize variables which were consistently involved in recent conflicts. You can also think of it as a simplistic but extremely cheap way of predicting which variables will be involved in the next conflict. So VSIDS branches first on those variables. One can claim that the algorithm is essentially a fail-fast algorithm, find conflicts fast. Fast is related to smaller number of variables set, which means blocking large subtrees of the search tree. But this is mostly intuition, afaik no one has formalized it very careful to test it on SAT data sets. Running a SAT solver on one of these data sets is not cheap, let alone comparing it with the optimal decisions (smallest extension of the current assignment to variables which would violate one of the clauses). VSIDS also depends on which variables we bump as each conflict, there are various ways to define when a variable is involved in a conflict. 

I think a cluster of solutions is a maximal set of solutions $T$ s.t. you can reach every $\tau' \in T$ from every other $\tau \in T$ by a sequence of solutions $\{\tau_i\}_{0\leq i\leq n}$ ($\tau = \tau_0$ and $\tau' = \tau_n$) where the hamming distance between each consecutive pair of solutions is bounded, i.e. they are just connected components in the graph where two solutions are adjacent iff the hamming distance between them is less than the bound. See these notes by Dimitris Achlioptas (or papers on statistical physics and random k-SAT). 

It is well-known that DPLL proofs correspond to proofs in resolution. Without CDCL the only resolution proofs we can get are tree resolution proofs which are much weaker than general resolution proofs. There are results that show with CDCL we can get any general resolution proof. However there are caveats, they need many artificial restarts, artificial branching, and/or particular preprocessing, so it is not clear how close these are to what these programs do in practice. See e.g. the following paper for more details: 

Assume that there is polytime algorithm that given $C(\vec{x}) \in F(\vec{x})$ and $\vec{a}$ computed the result of the multi-linearization of $C$ on $\vec{a}$. (w.l.o.g. I will assume that the output $\vec{b}$ will be a vector of $p$-bit binary numbers $b_i$ is $k$ iff the $b_{i,k}$ is one.) Since $P \subseteq P/poly$, there is a polysize boolean circuit that given the encoding of the arithmetic circuit and the values for the variables computes the multi-linearization of the arithmetic circuit on the inputs. Let call this circuit $M$. Let $C$ be an arbitrary arithmetic circuit. Fix the variables of the boolean circuit $M$ which describe the arithmetic circuit, so we have a boolean circuit computing the multi-linearization of $C$ on given inputs. We can turn this circuit into an arithmetic circuit over $F_p$ by noting that $x^{p-1}$ is $1$ for all values but $0$ so first raise all inputs to the power $p-1$. Replace each $f \land g$ gate by multiplication $f.g$, each $f \lor g$ gate by $f+g-f.g$ and each $\lnot f$ gate by $1-f$. By the assumption we made above about the format of the output, we can turn the output from binary to values over $F_p$. Take the output for $b_i$ and combine them to get $\sum_{0 \leq k \leq p-1}{kb_{i,k}}$. We can also convert the input given as values over $F_p$ to binary form since there are polynomials passing through any finite number of points. E.g. if we are working in $\bmod 3$, consider the polynomials $2x(x+1)$ and $2x(x+2)$ which give the first and the second bits of the input $x \in F_3$. Combining these we have an arithmetic circuit over $F_p$ computing the multi-linearization of $C$ with size polynomail in the size of $C$. 

You can use $\mathsf{DLogTime}$ for uniformity of $\mathsf{NC}$ and $\mathsf{NC^2}$. There is no problem and the classes uniform $\mathsf{NC^k}$ remain the same and equal to $\mathsf{ATimeSpace}(O(\lg^k n),O(\lg n))$ (for $k\geq1$). Generally, the only case that we need to be more careful is $\mathsf{NC^1}$ case where one should be careful about what needs to be in decidable in $\mathsf{DLogTime}$. If you use extended connection language description of circuits then everything works even in $\mathsf{NC^1}$ case. For more on uniformity see: Walter L. Ruzzo, "On Uniform Circuit Complexity", Journal of Computer and System Sciences, vol. 22 (1981), pp. 365–383. 

It seems that there is a confusion in the question between oracles resulting in equality vs. oracles resulting in separation. It is easy to find natural oracles resulting in equality. Oracles for separation is a different matter and your question does not contain an example for them. We don't have that many techniques for separating complexity classes and relativized separations are no different. The examples you get are expected to result from those techniques so it is unlikely that such oracles are known. As far as I know, diagonalization is the only method we know of that can separate non-small complexity classes, so no surprise the oracles separating $\mathsf{NP}$ from $\mathsf{P}$ are such sets. 

Interestingly even the threshold phenomenon is more complicated than most people think, Moshe Vardi stated in his talk "Phase transitions and computational complexity" that the median running time of GRASP remains exponential for random 3SAT formulas after the threshold but that the exponent decreases (afaik, it is not clear how fast it decreases). 

It is consistent with current state of knowledge that PSpace=ExpTime. Therefore there are no classes that we know that falls strictly between them. If they are not equal there are infinite number of classes between them by diagonalization (Ladner's theorem). If you are looking for well known classes that we know contain PSpace and are contained in ExpTime you can check complexity zoo. Based on zoology RG is an example. Other than that there doesn't seem to be any class with name that falls between them. 

I prefer teaching the original definition with quantifiers. IMO, humans generally have trouble in understanding formulas and definitions with more than two alternation of quantifiers directly. Introducing new quantifiers can clarify what the definition means. Here, the last two quantifiers just mean "for all sufficiently large n", introducing this kind of quantification can help. The pictures I draw for explaining these concepts match better with the quantifier versions. I think the limit simplification is useful for engineering student who are only interested in computing the growth rate, but won't be as useful for computer science students. In fact, using this simplification can cause more harm than good. This idea is similar to suggestion that we use the rules for computing derivatives (of polynomials, exponentiation, ..., chain rule, ...) in place of the epsilon-delta definition of it, which IMHO is not a good idea. 

Maybe authors don't include these failed attempts and the story of the research in their published papers because of the constraints imposed by editors and PC members. I guess it is very unusual for a journal (and probably even more unusual for a conference) to accept a paper where the main part of it is devoted to failed attempts. But in most cases if you talk with the authors or experts in the area they will explain story and the failed attempts (and many do talk about these in workshops). I have seen several authors explain at lease where the ideas came from in their papers. As an example, Girard explains in his paper that the idea for linear logic came from trying to find a denotational semantics for intuitionistic OR. You can find this kind of information also in monographs and biographies of famous researchers and volumes devoted to them (Halmos's autobiography and more recent "Kreiseliana: About and Around Georg Kreisel" edited by Odifreddi came to mind, there are also volumes and articles dedicated to some complexity theorists). Hopefully more people will do what Ryan have done and systematically explain the process and tell the story. ps: you can think of these as oral tradition of research :) (somewhat similar to Oral Torah which was not allowed to be written down). 

Are there any known natural examples where a similar statement is both true and false for significant fractions of the random oracles (e.g. it is true with probability $1/2$)? 

The classic reference for these kind of results is the survey by Peter van Emde Boas, "Machine Models and Simulations", the first chapter of Handbook of Theoretical Computer Science, Vol. A. For simulations between RAM and Turing machines see Theorems 2.5 and 2.6, pp. 26--27. It also contains pointers to historic references. 

About NP, I asked Steve Cook. The name NP for the class of nondeterministic polynomial-time computable problems was introduced by Richard Karp in his famous 1972 paper. Cook refers to the class of polynomial time nondeterministic Turing machine computable problems in his famous 1971 paper which defines polynomial time reductions and shows that there are complete problems, but without giving a name to the class. Before his paper there was not much interest in problems computable in polynomial time by nondeterministic Turing machines, only after Karp's paper it became clear that so many natural problems are in NP. After Cook's paper some people got interested, particularly two who got interested early on (before Karp's paper came out) were Michael Rabin and Allan Borodin. Karp's 1972 paper surprised people by showing how pervasive NP-completeness is among natural problems. 

I think the answer is positive for $C=L$ and the uniform version of $NC$. Ladner's proof does not use much other than what you stated and the fact that the smaller class is recursively represented and should work with minor modifications but I have not checked the details, take a look at Lance's writeup here. 

[CR96]: An $\mathsf{AC^0}$ function that need super-linear size is the $\frac{1}{4}$-approximate selector. A $\frac{1}{4}$-approximate selector is any function whose value is: 

If I've understood the question correctly, this is just multiplying a fixed 0-1 matrix V with given vectors. You want to optimize these multiplications by doing some processing on the matrix V. You can think of each $(V_i,R)$ as a depth 1 arithmetic circuit. $V R$ will be a very simple arithmetic circuit (depth 1, size $n$ sum of variables for each inner product, so it is a depth 1 circuit of size $n^2$ using only addition). You want to find the minimum size arithmetic circuit which computes the same function. For general arithmetic circuits this is hard, but I am not sure if it is still hard if you restrict it to such a small and simple subclass of circuits. Assuming that we are going to use only $+$, then it can perhaps be solved by a simple greedy or dynamic algorithm. On the other hand, this can also be a NP-hard problem, since it is very close to computing the minimum size circuit for a given CNF of size $n^2$. (If you could take the product of the results of inner products.) 

The key words you should search for are "naming systems", "representation", "notation", "numbering", ... The usual concepts of computability are defined over strings (either unary which is usually stated as $\mathbb{N}$ or binary which is state as computability over $\Sigma^*$ where $\Sigma = \{ 0,1 \} $). (They can be extended to $\Sigma^\omega$ but here I will stick to the countable case.) A model of computation is called universal if it is Turing-complete (i.e. it can compute any Turing computable function). If we want to talk about computability over other sets, then we need a naming system. A naming system for $M$ is a partial function from $\Sigma^*$ onto $M$. If we have two sets $Y$ and $M$, and a naming system for each of them, then we can talk about computability of functions from $Y$ to $M$. One can define a partial order over naming systems of a set $M$. $\gamma \leq \gamma'$ iff some computable function can translate $\gamma$-names of objects in $M$ to $\gamma'$-names of them. Let me give a simple example. Assume that $M$ is the set of Turing machines, and $\gamma$ is one of the common ways of encoding Turing machines as explained in computability/complexity books. Let $\gamma'$ be defined by adding a bit to the names telling if the corresponding TM halts. Then $\gamma'$-names can be translated to $\gamma$-names by a computable function but not vice versa. $\gamma'$-names contain more information. If fact, we we can solve the halting problem for Turing machines if we use $\gamma'$ as our encoding. Obviously we don't want to have non-computable information coded inside the names of objects as above, so we use names which have the least amount of information in computability (assuming that they exist). (In complexity, the complexity of a problem can depend on the naming system that is used for the inputs, and different naming systems can be more suitable for different purposes. There is no least if we change the requirement for translation from being computable to say poly-time computable as is shown by padding arguments.) After defining the computability over other sets using naming systems, one can talk about a universal model of computation over them w.r.t. fixed naming systems. Now, if the names in a naming system contain non-trivial information, then it is not very interesting for a machine to be universal w.r.t. that naming system anymore. For further information: 

A polynomial amount of nondeterministic bits is enough to encode the computation of a nondeterministic polynomial time algorithm. The only thing we need is to check if a given string is an accepting computation which is syntactic task that can be performed by a polynomial-size $\mathsf{AC^0}$ circuit (in fact a polynomial size CNF can do this). Another way to look at this is to consider the Tseitin translation from arbitrary circuits to CNFs which has a polynomial size increase and uses only polynomially many new propositional variables. If you look at the $\mathsf{NP}$-compeleteness proof of CNF-SAT (or 3SAT) you see that the part that checks if a given CNF is satisfied by a given truth assignment can be computed by an $\mathsf{AC^0}$ circuit (IIRC $\mathsf{AC^0_d}$ circuits can be evaluated by an $\mathsf{AC^0_{d+1}}$ circuit). 

Edit: As Kristoffer wrote in his answer we can save a $\lg \lg n$ factor. But can we save a little bit more? Can we replace $O(\frac{\lg n}{\lg \lg n})$ with $o(\frac{\lg n}{\lg \lg n})$? It seems to me that the layered brute-force trick doesn't work for saving even $2 \lg \lg n$ (more generally any function in $\lg \lg n + \omega(1)$). 

If $\bot$ has a proof then every statement has a proof by the $\bot$ rule (there is a restriction of intuitionistic logic that treats $\bot$ simply as an arbitrary atomic proposition called minimal logic). So $\lnot p$ does not meant that $p$ is false, it means that it can never be true. To understand the distinction we should think in the constructive terms not platonic terms. $p$ not true means we have not constructed a proof of $p$. $\lnot p$ means no one can ever construct a proof of $p$ and we have a construction showing that. In constructive mathematics (in the general sense, not in the sense of constructive theories) a proof is not a formal object according to some system of fixed rules. It is a construction. It is a primary notion that need not be further explained in terms of sets or linguistic objects (though there the meaning of logical symbols are closely related to the rules of their manipulation by design of those rules). The rules of proof construction are not limited to what is considered to be acceptable at the moment. We have to consider the possibility that tomorrow someone may come up with a new kind of constructing proofs. The intuitionist negation is constructive because it is shown by constructing a particular kind of proof, it is not shown simply by nonexistence of proofs or objects. To show that $\lnot p$ holds it is not enough that you show there are no proofs for $p$, you have to give a construction that shows that there cannot a proof for $p$ ever.