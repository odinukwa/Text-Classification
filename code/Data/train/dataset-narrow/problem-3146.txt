It would be a waste of information; the gradient is available, so use it and save time. There is reason to believe that the local optima are good; see, for example, Choromanska et al. (notes). Over-optimizing for the training set leads to worse generalization, so sometimes we deliberately don't even try by stopping early. Probably the best free lunch in machine learning. 

It's a neologism for "fitting for pandas"; compare with "pythonic". As stated in the lecture transcript: 

A most popular way of obtaining the approximate nearest neighbors is the locality sensitive hash. And here are some practical results. Then once you have the neighboring keys, it's straightforward to use a key-value store to retrieve the corresponding words. 

Because the smaller learning rate allows the optimizer to escape saddle points, which is what happens at each cliff, instead of overshooting. The validation error oscillated approaching the second saddle point. The noise makes it difficult to state that it increased with statistical significance, but if it did it could be due to overfitting. I do not know of any result that relates the separation between saddle points, so the delay could be arbitrary. At some point you reach the bottom, of course. 

It seems that there is no downloadable data (I clicked Show the complete Project library), but the project was completed in 2012, so you could ask the listed contact, Gary Jahns, explaining your motivation, to be sure: 

The best solution is not to compute the cosine similarity (in order to recommend an item) but to transform your features so that you can use approximate similarity search, for which many fast options exist. In other words, precalculate to expedite queries. (This means you will have found a co-embedding for your users and items.) 

Look at the distribution of the features you want to consider for anomalies (e.g., user attributes) conditioned on the date, so you do not trigger a warning for normal seasonality. An anomaly then is when the current conditional distribution is significantly different from the historical average. For more information look into contextual anomaly detection. Welcome to the site and good luck! 

Welcome to DataScience.SE! If you don't center before you normalize, you either don't take advantage of the full [0,1] range if your input is non-negative. The combination of centering and normalization is called standardization. Sometimes one normalizes by the standard variation, and other times by just the range (max-min). The latter is called feature scaling. The effect is much the same. Normalizing by the range is easier computationally. Normalizing by the standard deviation fixes the sample variance, which is nice from a statistical perspective. When using the standard deviation, the subtraction is usually against the sample mean rather than the minimum. There are several reasons for performing standardization. Sometimes we are interested in relative rather than absolute values. Standardization achieves invariance to these irrelevant differences. By explicitly preprocessing the data to reflect this disinterest, we relieve the model from having to learn it, allowing us to use a simpler one. Another reason is computational; it reduces the condition number -- you can think of this as the skewness or niceness of the loss surface -- making optimization easier and faster. 

Since you say you are a beginner, I suggest using a simple binary classifier: logistic regression. It takes the input vector, and transforms it by taking its dot product with a weight parameter (estimating which is the goal) before passing it through the logistic function: $\hat y(x) = \dfrac{1}{1+\exp(-\left< w, x \right>)}$ and $y=\begin{cases}1, \text{user present} \\ 0, \text{user absent} \end{cases}$. For details read the wikipedia article. For features, I would use the time of day encoded as $\left( \sin (2\pi t/T), \cos (2\pi t/T) \right)$, a hot-encoded categorical variable for the day of the week, a binary variable to indicate weekends, and so on. It's a crude model that leaves a lot of data on table, such as correlations between users, but as a beginner's exercise it is appropriate. 

Add a regularization term for synonymity to the objective function (to be maximized): $\log P(w_i|h) \rightarrow \log P(w_i|h) + \lambda \frac{1}{|S|} \sum_{(i,j) \in S} sim(w_i, w_j)$ where $S$ is the set of synonyms, and $sim$ is your word similarity function (e.g., cosine similarity). 

It's a dimensionality reduction algorithm. Inference is the problem of determining the parameters, or labels, that best fit the model for a given input once the model parameters have been learned, or estimated. 

The animation is by Alec Radford. Momentum helps by "pushing through" the local minimum or saddle point. Of course, it does not push in the optimal direction, which is why it exhibits oscillation, as you can see above, but it eventually recovers. For further study, I suggest reading about preconditioning and the condition number. 

I also tried visualizing the data with the outliers clipped, but did not find it very enlightening, so I did not include it here. 

is the regular squared error, while is the aforementioned asymmetric loss function. If you use you get 

Only Domingos knows for sure, since he invented this taxonomy, but I'd guess it would fall under "connectionists" (which he associates with neural networks), since graphs are all about connections (between random variables). Bayesians would be my second choice. CRFs are not natively Bayesian (you don't use priors or posteriors of the model parameters), but they can be augmented into one. 

It is indeed for computational tractability. You would not lose the all-important sparsity, since that is provided by the second term. The original formulation of SVM, which you can find on Wikipedia actually uses $\lVert w \rVert$: Minimize $\|{\vec {w}}\|$ subject to ${\displaystyle y_{i}({\vec {w}}\cdot {\vec {x}}_{i}-b)\geq 1}-\xi_i$ Obviously, minimizing $\lVert w \rVert$ is the same as $\lVert w \rVert^2$. A quantative difference arises when introducing the constraint using the Lagrangian function, but qualitatively it is the same. 

We see that the truncated decomposition is simply the truncation of the full decomposition. Each row contains the coefficients of the corresponding principal component. 

In general this problem falls under the umbrella of "structured prediction" since you are trying to estimate a number of things that are related by virtue of being embedded in a PSD matrix. Instead of estimating the inverse in one swoop, I'd pick an appropriate algorithm, say inversion by eigendecomposition, estimate the components (eigenvalues and eigenvectors), then piece them together. Here's one paper that shows how: Neural networks based approach for computing eigenvectors and eigenvalues of symmetric matrix. Alternatively, you can investigate approximation algorithms such as the one elaborated in Approximating the inverse of a symmetric positive definite matrix Welcome to DataScience.SE 

If you're not modifying anything you can call it validation. If you are, you can call it preprocessing. 

I believe your output is wrong; the difference between each line is exactly 0.01s. Welcome to the site! 

Sure, you can. Companies are clamoring for data scientists. Be careful though that they all interpret the term differently. Depending on the company you might find yourself asked to do anything from statistics to writing production code. Either one is a full-time job in itself and you have to be prepared for both, so asking for deep specialized knowledge on top of that's not reasonable, in my opinion, and the companies I've talked to stressed the other two areas (esp. the programming). However, I found that it helps to be familiar with the types of problems that you might face. Depending on the sector, that could be anomaly detection, recommendation/personalization, prediction, record linkage, etc. These are things you can learn as examples at the same times as maths and programming. 

You can side step the paucity of training data, and indeed training altogether, by using pre-trained embeddings in numerous languages. After that you can calculate your document embeddings using one of these simple algorithms, which basically amount to running dimensionality reduction on the matrix of stacked word embeddings for each sentence using PCA/SVD: 

Nobody does serious machine learning in Excel; that's not what it's for. Fortunately, you can directly import Excel files into better platforms like python. In particular, there's a great package called , which makes work very pleasant. Here's a demo. 

I don't know what's going on in your code, but you seem to be close: to get multiple labels, simply replace the softmax output layer with a logistic layer (or something else that maps a real number to a probability), then optimize the cross entropy. That way you will have a probability associated with each label such that their sum across labels no longer need to add to unity. 

You omitted the offset terms before the nonlinear transformations (variables b_1 and b_out). This increases the representative power of the neural network. You omitted the softmax transformation at the top layer. This makes the output a probability distributions, so you can calculate the cross-entropy, which is the usual cost function for classification. You used the binary form of the cross-entropy when you should have used the multi-class form. 

LeNet is a family (LeNet-1, LeNet-4, LeNet-5) of convolutional neural network designed by Yann LeCun et al. The name is a play on his name and the French language, where "le" means "the", hence LeNet means "the network". I believe it was originally devised to recognize handwritten numbers on checks (cheques). LeNet is only one early instance of a convolutional neural network; many others exist today. 

If you are not sure what a confusion matrix, see Wikipedia, where the "actual class" refers to the same thing as the "true class" in the Google documentation. 

$\dagger$ M. Opper and D. Haussler, “Generalization performance of Bayes optimal classification algorithm for learning a perceptron,” Phys. Rev. Lett., vol. 66, p. 2677, 1991. * T. Graepel, R. Herbrich, and C. Campbell, “Bayes point machines: Estimating the bayes point in kernel space,” in Proc. IJCAI Workshop Support Vector Machines, 1999, pp. 23–27 

Normally you use regularization. The exception is if you know the data generating process and can model it exactly. Then you merely estimate the model parameters. In general you will not know the process, so you will have to approximate with a flexible enough model. If the model is not flexible enough you will not need to regularize but you won't approximate well anyway. If you use a more flexible model, you will get closer on average (low bias) but you will have more variance, thus the need for increased regularization. In other words, the amount of regularization you need depends on the model. This is related to the bias-variance trade-off. Welcome to DataScience.SE. 

It seems that your 'Result' column is of type and you are trying to set some rows to . Stick to or (docs). 

I think the best thing for a beginner is some practice to let the theory sink in. (If you need book suggestions, do a search.) If you are weak on computer science, contribute to an open source project like scikit-learn. If you are weak on data analysis, compete in a Kaggle competition. Write a paper and/or a few blog posts to prove your skills. Do this now while you are in school. An undergraduate physics degree should be enough to get your foot in the door if you do the above. A degree will rarely get you a job by itself; you still have to pass the job interview, and that's why I recommend getting practice.