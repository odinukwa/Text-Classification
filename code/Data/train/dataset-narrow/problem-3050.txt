It is basically the 1 hidden layer with 2 neurons, but also using the scaling (-1?) and the bias (+5?) of the output layer. $$(-1) \cdot (\max(x - 5, 0) + \max(-x + 5, 0))+5$$ 

When using OPTICS, one wants a reachability-plot as output from which one can read the number of clusters depending on $\varepsilon$. If $\text{dist}$ was used instead of $\text{reachability-dist}$, then some points would have a lower distance in the reachability plot. This could lead to the wrong conclusion that there would be more clusters for some small $\varepsilon$. 

I had this a couple of times. Although I'm currently too lazy to go through your code, I think I can give some general hints which might also help others who have the same symptom but probably different underlying problems. Debugging Neural Networks Fitting one item datasets For every class i the network should be able to predict, try the following: 

combines both, clustering and association rule mining. They could improve ARM by association rule mining. From the abstract: 

Suppose you have a classification task and accuracy is what you care about. Now an old system $s_1$ has an accuracy of $a(s_1) = 0.9$ and a new system $s_2$ has an accuracy of $a(s_2) = 0.99$. This is an absolute improvement of $a(s_2) - a(s_1) = 0.09$ percentage points and a relative improvement of $\frac{a(s_2)-a(s_1)}{a(s_1)} = \frac{0.09}{0.9} = 0.1$. However, when you now try to get a system of $a(s_3) = 0.999$ this seems to be much more difficult, although it is only an absolute improvement of $0.009$ and a relative improvement of $0.00\overline{90}$. So neither the absolute nor the relative difference in accuracy seems to capture this well. Is there a common other way to quantify how much better the system is? 

I would like to know the difference in terms of applications (e.g. which one is credit card fraud detection?) and in terms of used techniques. Example papers which define the task would be welcome. 

Order the columns/rows in such a way, that most errors are along the diagonal. Split the confusion matrix into multiple blocks such that the single blocks can easily printed / viewed - and such that you can remove some of the blocks because there are to few data points. 

I would like to analyze / estimate the influence of the architecture on scale invariance in photos. To clarify: It is possible that a network can classify objects well if they are close, but totally fails when they are far away. Although the "main" object of the photo is still the same. Is there a dataset which is suitable for this task? 

First try a simple model: The input layer and the output layers dimension are defined by your data / your problem definition. Then train a model without any hidden layer. See how good it performs. Is it good enough? If yes, you're done. If no, continue Add a hidden layer of reasonable size or adjust a hidden layers size. Go to step (2). 

For detection, a common way to determine if one object proposal was right is Intersection over Union (IoU, IU). This takes the set $A$ of proposed object pixels and the set of true object pixels $B$ and calculates: $$IoU(A, B) = \frac{A \cap B}{A \cup B}$$ Commonly, IoU > 0.5 means that it was a hit, otherwise it was a fail. For each class, one can calculate the 

Suppose your system equation is $$z = H \cdot x$$ where $z \in \mathbb{R}^{n_m}$ is the observation, $x \in \mathbb{R}^{n_x}$ is the state you're interested in and $H \in \mathbb{R}^{n_m \cdot n_x}$ is a transformation matrix. Then the noise could interact with your system in any way. But most of the time it is logical and practical that the noise is additive, meaning your model is $$z = H \cdot x + r$$ where $r$ is sampled from a random variable of any distribution. 

When implementing mini-batch gradient descent for neural networks, is it important to take random elements in each mini-batch? Or is it enough to shuffle the elements at the beginning of the training once? (I'm also interested in sources which definitely say what they do.) 

It is usually displayed as a directed graph, where each node corresponds to one state $s \in S$ and the transition probabilities are denoted on the edges. Hidden Markov Models are called "hidden", because the current state is hidden. The algorithms have to guess it from the observations and the model itself. They are called "Markov", because for the next state only the current state matters. For HMMs, you give a fixed topology (number of states, possible edges). Then there are 3 possible tasks 

I have recently downloaded the small image net dataset: $URL$ The archives contain a lot of images, but no other files. How do I know which label the images have? 

*: You could probably answer this for other dimensionality reduction algorithms as well, but t-SNE seems to be the most popular one. Please note: I do see the advantage of a reduced dimensionality for compression / easier optimization / faster inference. However, the reduction to 2 dimensions seems to be only for visualization. Hence my question if one can see more in those embeddings than a visually pleasing image of the dataset. 

I am currently preparing for an exam in neural networks. In several protocols from former exams I read that activation functions of neurons (in multilayer perceptrons) have to be monotonic. I understand that activation functions should be differentiable, have a derivative which is not 0 on most points and non-linear. I do not undertand why being monotonic is important / helpful. I know the following activation functions and that they are monotonic: 

Suppose I have a smooth function like $f(x, y) = x^2+y^2$. I have a training set $D \subsetneq \{((x, y), f(x,y)) | (x,y) \in \mathbb{R}^2\}$ and, of course, I don't know $f$ although I can evaluate $f$ wherever I want. Are regression trees capable of finding a smooth model of the function (hence a tiny change in the input should only give a tiny change in the output)? From what I've read in Lecture 10: Regression Trees it seems to me that regression trees basically put the function values into bins: 

Multicollinearity is a problem for linear regression because the results become unstable / depend too much on single elements (source). (Also, the inverse of $X^TX$ doesn't exist so the standard OLS estimator does not exist ... I have no idea how, but sklearn deals with it just fine) Is (perfect) multicollinearity also a problem for neural networks? 

Troll-Answer If you really meant "only a few neurons" then you might want to have a look at Spiking neural networks. Those are incredibly computationally intensive, need a lot of hand-crafting and still get worse performance than normal neural networks for most tasks ... but you only need very little of them. 

The YouTube Faces database (YTF) consists of 3,425 videos of 1,595 different people. Given two videos, the task for YTF is to decide if they contain the same person or not. Having $n$ comparisons, the classifier might get $c \leq n$ right. Then the accuracy would be $\frac{c}{n}$. FaceNet is a CNN which maps an image of a face on a unit sphere of $\mathbb{R}^{128}$. It was evaluated on YTF. How did they decide which person is in the video? (I can imagine several procedures how this could be done, but I couldn't find it in the paper. One example, how it could be done, is by evaluating all images $x_i^{(k)}$ with $i = 1, \dots, \text{length of video }k$ and averaging the results - but I would like to know what they did / how this is usually done.) 

What you describe is called Information extraction and is a big field of NLP (Natural Language Processing). You are looking for temporal expression identification. You can have a look at the Stanford Temporal Tagger: SUTime to get a "live" demo. From what I see here it is a regex-based rule system. To give you an impression how powerful rule-based systems can be: 

I've heard that a multilayer perceptron can approximate any function arbitrarily exact, given enough neurons. I wanted to try it, so I wrote the following code: 

Why does this neural network not work for such simple function approximation? What do I have to change to make the same network topology work for both functions? 

For my bachelors thesis (write-math.com) I wrote my own little toolkit to go through different models / preprocessing steps very fast. Each experiment had one configuration file (see hwr-experiments repository). For example: 

Treat "missing value" as another feature: Imagine you have a feature like "date of graduation". One possible (likely?) reason why this value is missing might be that the person did not graduate. So you could build a model which as a binary feature "graduation date is available" and the actual graduation date as another feature. Predict the missing values: If data is missing because of your lack of knowledge of it (in contrast to the first point), then you might think about trying to predict the missing value. You could also add a feature which encodes the certainty of the predicted value being correct. Skip the feature: If it is missing very often and if it doesn't add much value to your prediction, you might simply want to remove it. 

My question is about how to monitor RL agents in production. To make the question easier to discuss, here is a use case. Please don't focus on difficulties in implementing such an agent, but rather on how to monitor if it is still doing well: 

Every decision boundary that can be found by LDA can be found by linear SVM Every decision boundary that can be found by linear SVM can be found by LDA. Every decision boundary that can be found by LDA can be found by a perceptron Every decision boundary that can be found by linear SVM can be found by a perceptron. Every decision boundary that can be found by a perceptron can be found by LDA Every decision boundary that can be found by a perceptron can be found by an SVM with a linear kernel 

When you have sensors, the values you receive change even if the signal that was recorded didn't change. This is one example of noise. When you have a model of the world, it abstracts from the real relationships by simplifying things which are not too important. To take into account for the simplification, you model the error as noise (e.g. in a Kalman filter). But noise sources can be anything. For example, in a image classification problem, where you have to assign labels (like cat, dog, car, plane) to images, you could model errors of the human labelers as noise. In general, I'd say noise are (undesirable) effects which you can't describe deterministic in a simple enough way, so you describe it with random variables. 

Have a look at The HASYv2 dataset. I tried to do as much of the exploratory work as possible to make sure that others can directly try more interesting thing with the dataset. Image format specific stuff