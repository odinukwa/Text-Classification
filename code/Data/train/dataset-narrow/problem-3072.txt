The standard deviation and mean of a categorical variable is not meaningful. It looks like the original data are from a range of [0, 20), and the space has been discretized. Now, instead of ranges, you have ordered categories 1 through 4. The individual numbers within each category have lost their meaning. For more details, see the accepted answer here 

In more traditional statistical learning methods, such as logistic regression, the coefficients on the pre-selected features can be interpreted as increasing or decreasing the log-odds of success. For example, say we want to use logistic regression with a single predictor variable to predict whether a student will pass the exam. We can estimate the parameters in the model: $log(Y) = \beta_0 + \beta_1$studying_hours where $Y = 1$ if the student passes the exam and $Y = 0$ otherwise, and studying_hours is a variable with values 1, 2, ..., 100 hours. We can say that a one-unit increase (a one hour increase) in the number of hours studying for the exam increases the log-odds of passing the exam by $\beta_1$. It stands to reason that we should be able to make a similar argument when the features are learned instead of pre-selected. But then we would need to understand exactly what the learned feature is. In my example above, instead of pre-selecting to be the feature we want to use to predict whether a student passes the course, we would learn some informative feature instead. I've heard of Hinton diagrams, and of visualizing the feature map for CNNs, but I don't think either of those techniques really works here. A Hinton diagram would just tell me the magnitude and sign of the weights. A feature map would not really be informative. I don't expect to understand what all of the features are, because the whole point is that the algorithm can do a better job of learning features than I can do designing features. But even so, at least some of the features should be human-intepretable, like a 'straight line detector' in a CNN. How can we interpret the features being learned by a multilayer perceptron for logistic regression? Not really sure what to tag here, so please, if there's an edit you would recommend, go ahead! EDIT This is my attempt at starting an answer, but clarification/other opinions would be appreciated. Let's take a case of binary classification, and start from the simplest possible case: one scalar input, one hidden unit with a relu activation, and one output unit. Assume the hidden unit has a bias of 0. We can express the function that describes this network as: $log(Y) = \beta_0 + \beta_1 max\{wx, 0\}$, where 

A very simple approach would be to find some kind of centroid for each cluster (e.g. averaging the distributions of the documents belonging to each cluster respectively) and then calculating the cosine distance of each document within the cluster from the corresponding centroid. The document with the shorter distance will be the closest to the centroid, hence the most "representative". Continuing from the previous example: 

You could use a 3d-scatter plot, where each class would correspond to one axis and the color-intensity of the point would indicate the score value(e.g. for a grayscale colormap, the whiter the closer to 1 the value of the score). Using the above format: 

I agree mostly with what was already said regarding feature engineering and just to provide you with more material this post has a nice analysis regarding different stages in feature engineering, with many links and references to papers and specific challenges. I think it's worth checking out. Also, to some extend you could automate the task of finding "good" compound features, using kernel methods. You could use some of the standard kernel methods implemented in many libraries (e.g. in sklearn) as feature extractors and feed these higher-level features as input to a random forest model that inherently uses feature-importance while training. Then keep only most-informative of these higher-dimension features, as scored by the trained model, as compound features alongside the raw data. 

This will help you in writing scripts in python. For understanding deep learning, you can go to this- $URL$ This tutorial uses Theano for building neural network models. It also explains many terms related to deep learning. And finally (you already know), practice is what you want to learn in this short time. And your biggest positive point is that you have a PhD in Maths, so you can easily understand the mathematics behind deep learning models. 

What's happening is, while passing dataframe, the TfidfVectorizer is only taking the column names and converting them into numeric form. I don't think you need to use tfidf here. As far as I understand, your data is categorical text, so use instead of tfidf. This will convert your categorical data to numeric form which you can use for modelling. 

He also proves that is not a metric by giving an example. Because it does not satisfy the property 3 of metric definition, squared euclidean distance is not a metric. Hope this helps. 

First of all, I looked at the context, it is in 4.3 (Similarity between vectors) in point 4 (Proximity measures) in chapter 2. Is the squared Euclidean distance different from the Euclidean distance? Well, simply stated, yes it is different, the difference being same as the difference between Variance and Standard Deviation. One is a number and another is square root of that number. How is the squared Euclidean distance non-metric? The author says- It is tacitly assumed that the distance function, d(x, y) where x and y are patterns, is a metric..... Now look at the definition of metric, he says a metric is one which satisfies the following properties- 

I want to input the set {A1, B1} for time 1, then {A2, B2} for time 2, etc. A Keras LSTM takes input in the form To enable better generalization, I intend to break these sequences up into subsequences of length as recommended here and here, which means (e.g. if n is 5 and my window of time, k, is 3, then there are 5 - 3 + 1 = 3 sequences total). I have time steps divided into separate windows, so (I think?). Question: what should the values of and be, and how should I shape my input data? 

Great question! tl;dr: The cell state and the hidden state are two different things, but the hidden state is dependent on the cell state and they do indeed have the same size. Longer explanation The difference between the two can be seen from the diagram below (part of the same blog): 

If you have imbalanced classes (for example, if you have 3 classes and 100 examples of class 1 and 1000 examples of class 2 and 5000 examples of class 3), then yes, I would weight the loss function (I would use weighted categorical cross-entropy). If you mean some classes have a higher probability than others, then this is normal and expected behaviour. For example, if you were doing a 10-class classification problem like on MNIST, and you're trying to predict a given image, if the image has some rounded sections then it's much more likely to be a 3 or an 8 than a 1. 

I think the answer also depends on your usecase. If you just want to detect these kind of strings I would focus on heuristic rules that suit my needs, rather than creating a system that learns to recognize the patterns of the strings. However, if your aim is generating similar kind of strings based on the current patterns or finding new strings in a stream of text, you should look for regex generators. There is this list of resources regarding regexes in general and you should focus on the generators for the task at hand. For a similar task in the past, I had used an online, free tool that will generate a regex (if possible) that satisfies as many of your sample strings as possible. So you could give it a try, as a fast solution. Either way, I would first do some data exploration(e.g. are numbers and letters also intermixed or do numbers always come up after or letters/symbols etc.), in order to get a better understanding for the problem at hand. Also, this could lead to simple heuristic rules that could suffice as a crude solution or at least a simple baseline system against which I would "pitch" a machine learning model. 

To my understanding you should be looking for something like a Gaussian Mixture Model - GMM or a Kernel Density Estimation - KDE model to fit to your data. There are many implementations of these models and once you've fitted the GMM or KDE, you can generate new samples stemming from the same distribution or get a probability of whether a new sample comes from the same distribution. In python an example would be like this:(directly taken from here) 

I've seen discussions about the 'overhead' of a GPU, and that for 'small' networks, it may actually be faster to train on a CPU (or network of CPUs) than a GPU. What is meant by 'small'? For example, would a single-layer MLP with 100 hidden units be 'small'? Does our definition of 'small' change for recurrent architectures? Are there any other criteria that should be considered when deciding whether to train on CPU or GPU? EDIT 1: I just found a blog post (possibly outdated? It's from 2014): "...Most network card[s] only work with memory that is registered with the CPU and so the GPU to GPU transfer between two nodes would be like this: GPU 1 to CPU 1 to Network Card 1 to Network Card 2 to CPU 2 to GPU 2. What this means is, if one chooses a slow network card then there might be no speedups over a single computer. Even with fast network cards, if the cluster is large, one does not even get speedups from GPUs when compared to CPUs as the GPUs just work too fast for the network cards to keep up with them. This is the reason why many big companies like Google and Microsoft are using CPU rather than GPU clusters to train their big neural networks. " So at some point, according to this post, it could have been faster to use CPUs. Is this still the case? EDIT 2: Yes, that blog post may very well be outdated because: Now it seems that GPUs within a node are connected via PCIe bus, so communication can happen at about 6GiB/s. (For example: $URL$ about 35 minutes in). The speaker implies that this is faster than going from GPU1 to CPU to GPU2. It would mean the network card is no longer the bottleneck. 

I think what you should be looking for is ordinal regression/classification in order to utilize the ordering of the classes. Example python/matlab implementations, with some extra resources. For fast integration with your system, you could create a new ordinal regression model using the outputs of the already trained network as input to the new ordinal model and the true ordinal value as the output. For another quick/dirty solution you could try simple Mean Absolute Error or Mean Squared Error, according to this study, between the predicted and target classes of your system, as it is right now, instead of the categorical cross-entropy error function. 

If you follow the example, the output for the wanted student (with $student_{ID}=3$) with opinions: {Trump -1, Net Neutrality -1,Vaccination 1, Obamacare -1} will give you two other students with the same opinions and their ids. You can modify the script to fit your needs accordingly. P.S.: Sorry for the messy code, it was written rather hastily. Also, i tried it with Python 2.7. 

There are many ways in which you could create an ensemble from your base models. Some resources to take a look at are the following: Firstly, I would point your attention towards this question, that has a lot of answers and interesting ideas. Regarding implementation: I have used in the past the brew module that has extensive implementations of different stacking-blending techniques etc, while using different combination rules and diversity metrics. It is also compatible with sklearn models. Another interesting project is xcessiv, which I haven't used myself but provides a helpful gui for managing your ensemble's details. Finally, regarding theoretical details, I would suggest you take a look into this survey that focuses on ensembles in regression tasks. 

I too was in your place when I started using Neural networks. There are so many hyper-parameters to choose, and each take on many values. Like Emre said, you need to check the model which is giving best metric score on your data (Cross validation set). The parameter values of that model will be your optimized values. You can also check this link- $URL$ 

The header row is not duplicated, it is a row of the data frame (see index 0 attached with it, The actual columns don't have any index number). That's why you can't remove it using . If you want to remove it after having it in data frame, then 

What is wrong with your implementation is that you are passing a dataframe directly to tfidf vectorizer. If you check your data, it would look like this - 

First of all, you CAN learn them in 2 months time, if you are going to devote everyday for this. As per your background, you don't need much of the basics of ML, you just need Python. There are many answers for the same here- $URL$ The simple roadmap for python here would be- 

Download and install python in your PC Start with a course and book (related to basic python concepts and programming) and start coding. I recommend both course and book because not everything is in a book and not everything is told in the courses. As soon as you start creating smaller working scripts, go to hackerrank and start doing exercises there. This will develop your understanding of data structures and which data structure to use in which type of problem. 

Context / big picture: Two events are independent if they have no influence on each other's outcomes. For example, if event A is "I go get coffee" and event B is "it's raining outside", then events A and B are independent, because I'm a coffee fiend and I don't care whether it's raining - I'm getting that coffee anyway! Logistic regression example: Say you have blood pressure samples from 50 different individuals at a hospital. You want to classify these as high or low blood pressure. In this example, your X (input variable) is the blood pressure from the 50 individuals, and your Y (binary response) is yes/no to the question 'is this blood pressure high?'. Each person's blood pressure is independent (if we assume they are unrelated strangers). If you were to sample the same person's blood pressure twice, like before and after physical exercise, then you would no longer have independent samples. Why: We need independent samples in logistic regression because otherwise the degrees of freedom of the model are not what we expect them to be, and this affects further calculations. For example, in your dataset, you have 50 independent samples. You use 2 to estimate your intercept and slope, which leaves you with 48 degrees of freedom to work with. If you didn't have independent samples, then you would not have 48 degrees of freedom. EDIT: a bit more detail on why independent samples are important. The assumption of independence ensures that each sample contributes the same amount of information to the experiment. With independent samples, this is the case. A blood pressure measurement from one individual is not dependent on the blood pressure of another individual - they're independent. However, if you measure the same person's blood pressure before and after exercise, a different amount of information is added. You can no longer ask the same questions as you could with independent samples. Let's pick a simpler model to describe what's going on. We will follow standard hypothesis testing principles to illustrate the practical difference between independent and dependent samples. I will round to two decimal places for ease of reading. Suppose we have two groups of people: those with blood type A and those with blood type O. (Aside: notice people cannot have both blood types at once). We want to investigate whether those with blood type A have a higher resting heart rate than those with blood type O. We have 12 people with blood type A and 10 people with blood type O. To answer this question, we'll conduct a two-sample hypothesis test. Null hypothesis: The mean resting heart rate of those with blood type A is the same as the mean resting heart rate of those with blood type O Alternate hypothesis: The mean resting heart rate of those with blood type A is different from the mean resting heart rate of those with blood type O. We measure each person's resting heart rate, and we get the following results: Type A: 60, 65, 70, 62, 55, 80, 70, 72, 66, 81, 77, 78 Type O: 61, 64, 70, 72, 59, 62, 66, 78, 69, 75 We then calculate the sample mean and sample standard deviation of each group: mean(Type A) = 69.67; standard deviation(Type A) = 8.35 mean(Type B) = 67.6; standard deviation(type B) = 6.28 We can then calculate a test statistic, which will follow a t-distribution. We may decide to use either Welch's method or a pooled variance test - in this case, I'll show Welch's method, which doesn't require the population variances to be equal (unlike the pooled variance test). The test statistic is calculated via: standard error: sqrt((8.35^2/12) + (6.28^2/12)) = 3.02 test statistic = (69.67 - 67.6)/3.02 = 0.69 The degrees of freedom here are difficult to calculate, but a generally accepted approximation is "the lesser of the degrees of freedom in the two groups", that is min(df_1, df_2). In this case the degrees of freedom of the first group is 12 - 1 = 11 and the degrees of freedom of the second group is 10 - 1 = 9. We subtract 1 here from each sample size, because we have estimated one parameter for each group (the sample mean). If we wanted to continue this example, we could look at a t-distribution on 9 degrees of freedom and decide based on the test statistic whether to reject or not reject the null hypothesis. In this case, we're more interested in the degrees of freedom than the result, so we'll move on. Dependent samples Suppose we have a that same group of people who all have blood type A. Instead of measuring their resting heart rate just once, we’ll measure each Type A person’s resting heart rate twice. This introduces dependence between the samples. Null hypothesis: the mean resting heart rate of people with blood type A is the same as the mean resting heart rate of all blood types Alternative hypothesis: the mean resting heart rate of people with blood type A is different from the mean resting heart rate of all blood types We measure each person's resting heart rate before exercise, and we get the following results: Before exercise: 60, 65, 70, 62, 55, 80, 70, 72, 66, 81, 77, 78 Then we measure their resting heart rate after exercise, and we get the following results (notice that person 1 in the previous list of heart rates is the same as person 1 in this list): After exercise: 90, 85, 70, 72, 76, 88, 89, 92, 96, 93, 84, 85 Now we have two observations for each individual. We can’t ask the same question, unless we want to throw out half of our data! Instead, we’ll ask whether there is a difference in the resting heart rate before and after exercise. We’ll use a paired-difference t-test. To conduct this test, we have to take both observations from each individual into account. We calculate the differences between each person’s before and after measurement, and treat this set of differences as our actual data set: 60-90, 65-85,70-70, 62-72, … Then we can calculate a sample mean, a sample standard deviation, etc. Our degrees of freedom would be 12 -1 = 11. What would have happened if we had assumed that those dependent samples were independent? Notice that when we had dependent samples, our degrees of freedom was 11; when we had independent samples, our degrees of freedom was 9. If we had assumed that the dependent samples were in fact two independent samples, then we could have used the wrong degrees of freedom. ** What if we had had only 10 measurements instead of 12 in the ‘dependent samples’ section? Wouldn’t we have had the same degrees of freedom? ** Yes, but it still wouldn’t have meant the same thing. We would have had: Before exercise: 60, 65, 70, 62, 55, 80, 70, 72, 66, 81, 77, 78 After exercise: 90, 85, 70, 72, 76, 88, 89, 92, 96, 93 And we would have had to ‘throw away’ the last two ‘Before exercises’ measurements and use only the first ten, as the last two had no partner measurement. The meaning has still changed, though. With dependent samples, you cannot ask the same kinds of questions as with independent samples. 

As a fast answer, you can represent each student as a vector with $K$ elements (where $K$ is the number of topics) and values $\{+1, 0, -1\}$, denoting positive/non-existent/negative opinion about this topic. Then, a simple measure of agreement between two students is the element-wise product between two student-vectors. That is the product will be: $similarity = \sum_{i=1}^{K}st_1[i]*st_2[i]$, where $st_1,st_2$ are the student-vectors. Obviously, only the topics where both students have aligned opinions will boost the total [e.g. $1*1=1$ and $(-1)*(-1)=1]$, while misaligned opinions will decrease the sum. If any of the two students haven't expressed an opinion about a topic, then this topic won't matter in the sum. In that sense, you can find the most like-minded students to a specific student, as the ones with the highest $similarity$. If what you really need is a number of agreeing students for each unique student, then a threshold on the $similarity$ score can be set. The value of the threshold can be decided empirically from your data. This is easily implemented and if you are comfortable with coding, I could post a sample script in python. One thing to consider though, is in what format is the bipartite graph (a .csv, a graph file of some kind etc.). EDIT: MINOR EXAMPLE. Fetch example .csv file used from here. 

You can change the scatter plot attributes for marker-size, colormap etc. according to the documentation to match your tastes.