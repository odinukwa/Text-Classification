You can see from the picture, the relation should be $rasterDepth = rayDepth*cos(A)$ So try this - in your function, before you do your view rotation, save your fragment ray's original normalized Z value somewhere, maybe an extra field in your Ray struct. Let's call it . Then in your function the relevant code will be 

The values for each pixel in the Z-buffer are interpolated from the values at the corners of the triangle during rasterization. To make this work, the projection matrix produces depth values that are a function of the reciprocal of the original depth value in camera-space. That is, the $Z'$ value in each transformed vertex is of the form $aZ + b$, where $a$ and $b$ together determine the near and far planes, and the transformed $W'$ value is just the original $Z$. After doing the homogeneous divide by $W'$, the transformed $Z'$ becomes $a + b/Z$. By using the reciprocal of $Z$ in the depth value like this, you can then just interpolate linearly in screen space from the three corners to each pixel to get correct results. Since the last step produces floating point results, and most Z-buffers use unsigned integer values, you'll need to bias, scale and quantize before writing to the actual buffer. I seriously recommend Eric Lengyel's book Mathematics for 3D Game Programming for the nitty-gritty details of all this kind of stuff, he also explains perspective-correct interpolation of vertex attributes and all the other math you'll need to know to write a rasterizer. 

You can clearly see the dark bands around the top right quadrant where one light's terminator intersects the other's area of effect (well, I can anyway, YMMV). I've run an eyedropper over it and confirmed it's a perceptual effect, the pixels do not in fact get darker. 

I've used as part of compute shaders doing surface extraction. I've only ever used it in compute shaders, to store the calculated data in a texture to be consumed by another shader - I know it's possible to do in a fragment shader as well but I haven't come across a use case for it yet. I also haven't used at all, since apparently is often faster (benefits from using the texture cache), and at worst no slower, so I simply bound the same texture to both an image and a texture unit, and used for writes and for reads. The code to get it all working looked something like this (using OpenGL 4.5 and DSA, and assuming holds a compute program that does on channel 0, is a second compute program that samples the generated texture on channel 0, and is a rendering program that samples the texture on channel 0 - of course, if you don't want to hardcode the texture channel in the shader you can use ): 

You can see that the distance along the axis (green line) gets scaled while the perpendicular distance (red line) stays the same. Mathematically, this is the same as rotating until your chosen axis lies along the X-axis, scaling by X, then rotating back again. 

VAOs are essentially 'plumbing' objects that help get data from your buffers into your vertex shader ready for drawing. On the GPU this is handled by the vertex fetch stage, so a VAO is basically just a bundle of vertex fetch state corresponding to a particular model. You can think of it as a bundle of pointers, but with somewhat more complexity than CPU-side pointers. Since OpenGL buffers are just a blob of untyped bytes by default (that's why you get a void* when you map them), the VAO handles type-casting (to vec3 or whatever) when fetching the data. It also deals with the size of the data not necessarily matching the stride between consecutive elements, and makes sure that each data item (position, normal, uv, etc.) is transmitted to the correct 'channel' in the vertex shader input. You can render without a VAO in legacy OpenGL, but this is generally more of a PITA, since you need to bind each vertex attribute separately each time you draw the object. Encapsulating all these bindings in a VAO lets you set them up once, then you have a 'plug and play' object that simply needs to be bound before a draw call, and all your vertex data will be ready to go. 

P1 and P2 in this case represent the actual coordinates of the vertices of the cube in your output mesh. These might be the same as the input coordinates of the function you're using, or they might not be, it's entirely up to you how you output them. A simple way to map them would be to just use integer coords, biased by half the resolution of your sample volume - so a mesh spanning 100^3 cubes would have coordinates from -50 to 50 on each axis. So in the formula you describe, (iso - V1) / (V2 - V1) gives you the amount to interpolate by, and (P2 - P1) is a vector representing a single cube lattice edge along the axis you want. Let's assume we're using unit sized cubes, +Y is up, and +Z is 'into' the screen. In your cube above, you will have crossings on the three edges coming from the left-bottom-front corner. On the X and Z edges you will interpolate by 0.25 : (0 - (-1)) / (3 - (-1)) , and on the Y edge you will interpolate by 0.5. So if the left-bottom-front corner is at the origin, your three mesh vertices will be at <0.25, 0, 0> , <0, 0.5, 0> and <0, 0, 0.25>. Apply the same formula to the right-top-back corner and you get <0.5, 1, 1>, <1, 0.5, 1> and <1, 1, 0.5> The only difference is the left-bottom-front corner values will be P1 and V1 in your formula as it is at the 'negative' end of the edges to be interpolated, and the right-top-back values will be P2 and V2 as it is at the 'positive' end. Hope that all makes sense. 

The overhead on this is pretty miniscule anyway. Modern GPU raster and texture rates are on the order of tens of gigapixels per second. Compared to the cost of running a ray tracer, it's a rounding error.