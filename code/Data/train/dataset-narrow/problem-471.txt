An easier way to understand a deadlock graph is taking that information you have and saving it as a .xdl file. That is the default deadlock graph file extension. Then you can open it with either SSMS or SQL Sentry Explorer. With the graph you will have a quicker understanding on what happened, which process was waiting, which one was blocked, which resource they were waiting for, etc. The using some queries you can grab even more information, find the root cause of the deadlock and try to solve it. Replace the and values on the were clause with yours. On this cases some of the objects are indexes and tables, modify at will for other objects. And finally, most important, do a quick search on your prefered search engine on how to interpret deadlock graphs, grab one of the tutorials and use yours to learn how to do it. 

So, after running out of possible solutions we opened a support case to Microsoft. They asked to run a tool to gather some info while the process was running and afterwards they analyzed it. Here is their answer: 

If you do a quick search on this same site, you will find lot of similar questions. Basically, some process is taking to long and is making your temdb grow until you run out of space. According to Brent Ozar "Tempdb is like a public toilette", everyone (all kind of processes) use it and usually to do filthy things. What you should do is locate what is making your tempdb grow, search that root cause and solve it. If you don't do that, you will still deal with continues issues like this one. You can, as a temporal solution, run a shrink on the tempdb to free some space. Please please avoid having an automated job to do shrinks. You have several options to find the cause, running a SQL Server Profiler trace to track down Tempdb activity. Also you can use sp_whoisactive to log activity on the server while whatever causes the issue is running, so you can check that log afterward and see possible queries or processes that cause it. Kendra Little has a full blog entry on how to do this. For some specific code samples, look here and here. Those are just some samples, as commented on first line, this a common issue, so you should find tons of useful answers here on Stackexchange site. 

returns the hoped for count of 0. The thing I like about this method is that it only needs to read each table once vs. reading each table twice when using EXISTS. Additionally, this should work for any database that supports full outer joins (not just Postgresql). I generally discourage use of the USING clause but here is one situation where I believe it to be the better approach. 

If your problem is file fragmentation then no, there isn't. In Postgres each table gets it's own file, or set of files if it uses TOAST, in the file system. This differs from, say, Oracle (or apparently MS-SQL) where you create pre-sized tablespace files to drop your tables into-- although even there you could have file system fragmentation issues if the tablespace files get extended or the file system is badly fragmented to start with. As to your second question... I have no idea how to would cleanly deal with the file system fragmentation as MS-Windows is the only OS where I've experienced fragmentation issues and I don't run MS-Windows any more than absolutely need be these days. Perhaps placing the database files on their own disk(s) could mitigate that to some extent. 

First thing: look at the logs. You will find warnings, error, fatal, and panic messages. You can find where your logs are in your file. Look for the setting, if it's , you will find your server logs in the directory specified in the setting. If is set to , look at the setting. If it's you need to look at your syslog settings to find where your logs are. If it's you might find something under where is the PID of your running PostgreSQL server. You might find this page of documentation usefull. 

If you can't change your firewall policy, perhaps you could archive your WALs in another machine (with a great backup tool as barman or backrest) and your replicas should be able to retrieve the WALs from that backup machine (but that will make your replication lag bigger). 

The in IPv4 is for small network. You can translate it in for IPv6. Here's the quote from documentation : 

You weren't far from finding it. Here's the documentation you're looking for. Yes you can define a specific locale for chosen columns (from 9.1 version). 

I don't know if my answer is accurate as we don't have much informations in your question, but perhaps you should consider one of this lead: 

Avoid using in the query if at all possible. Since I don't use the construct on a frequent basis the syntax may be a bit off but it should at least get you close. edit How large are these tables? Have you looked at the for the query? Indices? 

No, the schema is the user in oracle. In other databases, where the schema and user are actually different things (and appropriately so), then you could have a schema name that matches the user name but where the user does not have rights to the objects under that schema. 

Once you've learned Postgresql then pick another RDBMS and learn their SQL dialect-- compare and contrast. Rinse and repeat. 

Oracle has a free tool SQL Developer that has can presumably reverse engineer an Oracle database. If the database isn't too large then you could possibly use a tool like DbVisualizer (which is either free or low cost depending on the version). You can only diagram one schema at a time and results aren't directly editible (it's really more of an exploration tool) but you can save the diagram as GML and edit it with a tool such as yEd (which is a free download) (Note that you will need to edit the gml file slightly before bringing it into yEd by replacing all instances of 'customconfiguration "SimpleRectangle"' with 'type "rectangle"'). Note that SQL Developer, DbVisualizer, and yEd are all cross platform tools so you can use them on any system that has Java installed. Update -- I just tried reverse engineering (154 tables) using SQL Developer. It appears to work reasonably well but it isn't going to win any beauty contests... 

There are several ways to use a Dark Theme on SSMS main coding window, either importing a vsettings file, applying Dark Theme that is disabled on a config file or doing it manually. But all those options do not affect Grid Results, Object Explorer and other windows. Those 2 are the main ones I use other than the coding one. I tried the usual Tools>Options>Environment>Fonts and Colors then selected on the combo box Show settings for: the option Grid Results and using White for Item Foreground and Black for Item Background. Saved, restarted SSMS but just the text is white, background is still white. Any idea on what is happening or how to do it? I can't find how to do it for the Object Explorer. 

Well, it was difficult to solve, but with the help of our sysadmin and reading through lot of places, we managed to get it working. It was a mix of things, but the 2 main ones were changing some parameters on SSRS config file: 

Parallelism it not disabled after adding an index, so the reason is some where else. From your text I understand that you ran the query on a restored database on a different server (legacy server you say), is that correct? If yes, then I would check that both instances have same configuration, specially MAXDOP and the Cost Threshold for Parallelism. Check that underlying data is the same on both tests. As Shanky commented, the 2nd query shows a higher number of Estimated number of rows, 132432 against 99489. Also, as recommended on the comments, be sure that all statistics related to the query are up to date, if not, update them to be certain than both queries run with same information to get the same execution plan. 

Since most of the information schema is in the form of views against pg_catalog you can get a big chunk of it using : 

Which is what we would expect-- the $PGDATA/base/83637 directory is gone, there should be nothing to vacuum. Are you sure there isn't something else eating up your disk space? One of your other databases? log files? Something that you could try would be to: 

In psql you can to make psql show you the queries used to generate the output of the commands. I've found these queries to be very useful as a starting point when digging metadata out of databases. 

No, that simply enables the use of SSL. You need to also make the appropriate changes to your pg_hga.conf file. 

My understanding is that when you drop a database then it, and it's files, are gone. Unless you are using tablespaces then each database should have it's data in it's own subdirectory under $PGDATA/base. Using one of my servers for an example (as the postgres user): 

To add to what others have said. Temporary tables can also have primary keys, constraints, indexes, etc. whereas CTEs cannot. On the flip side, you can do some pretty neat tricks with CTEs that would be harder, I think, if done with temporary tables-- such as chaining them to perform deletes, inserts, and selects all in one statement. There are some nice examples of "cool-CTE-tricks" here. 

You allready installed PostgreSQL, you need now to create the cluster. The tool is what you need. You will find documentation here. It might be confusing because when you're under a debian based distros, and perl wrappers do every steps for you. 

I guess you can recreate a new cluster. That will create a fresh bunch of new sys views. And then you move (logically with pg_dumpall) your databases to that new cluster... 

On psql, it's the default, but you can (un)set it with . So I suggest trying at the beginning of the script. From documentation: 

You're under Ubuntu. You can use Debian wrappers. Try (you can change main for your cluster's name. To check if another PostgreSQL version is running, try . If it' won't start with pg_ctlcluster. Look at the logs. For Debian-based, default logging is in . 

Look at the file in your $PGDATA. If someone uses the command to change the settings, you will find the value there. is read after and each setting put in this file overwrite those in ... It's worth giving a look... 

Whenever something goes wrong, you have to look at the logs. You will find warnings, error, fatal, and panic messages. You can find where your logs are in your postgresql.conf file. Look for the setting, if it's on, you will find your server logs in the directory specified in the setting. If is set to off, look at the setting. If it's syslog you need to look at your syslog settings to find where your logs are. If it's stderr you might find something under where is the PID of your running PostgreSQL server. You might find this page of documentation usefull. 

Have you looked at SQLite? It is a widely used, light-weight database which has C/C++ bindings and supports in memory databases. 

No guarantee that the indexes that pop up aren't ever used, but it should provide a list to start looking at. 

Have you looked at the pg_stat_statements extension? It won't give you the role logon time (although setting the log_connections parameter as @francs suggests would deal with that) but it will show you which roles are running queries (and what those queries are). 

In addition to the other comments... If/when you have a database where any given table can be updated by one or more applications or code paths then placing the appropriate constraints in the database means that your applications won't be duplicating the "same" constraint code. This benefits you by simplifying maintenance (reducing the number of places to change if/when there is a data model change) and ensures that the constraints are consistently applied regardless of the application updating the data. 

While you can't "copy" them you can use the database meta-data to dynamically generate the DDL to create them. 

We have a set of spatial queries that are failing and I'm struggling with troubleshooting them. I suspect that we're running into some bug, but I'd like to nail things down a bit better so as to be sure and also so that the resulting bug report is both useful and directed to the correct party (postgresql, postgis, or other). Does anyone have any recommendations for next steps? Given: PostgreSQL v9.1 installed from $URL$ on fully a patched 32-bit install of CentOS 5 

is an value First time I see it and I've no clue on what is being achieved with this. I made some searches, read the definition of CHECKSUM on the tech pages and couple of sources more but can't understand the point of using here. I'm not looking for an answer that explains exactly this case (if possible, even better) but I would be satisfied with an answer explaining the point of this way to use . 

I'm setting up a maintenance plan and on error I added a "Notify operator task" with some text. But I want to be able to add something like: 

When an error log is considered to be "big"? Some of our logs take long to load, close to a million of rows. edit: one has 2 034 546 rows I'm trying to find some tsql to get error log size, but can't find it, it is possible to do it? I see the recommendation to recycle the logs and to configure more than just the default number of 6. Some authors recommend 50, others 10. I guess it depends on the environment, but is there a recommended number? 

For Postgresql on MS Windows, the Postgresql wiki has an "Automated Backup on Windows" article that may be of use. There is also "Instant PostgreSQL Backup and Restore How-to" available from Packt that provides a good overview of the various options available using the tools that come with Postgresql. There is nothing in the booklet that can't be found on the web but they do a nice job of pulling that all together in one document and the price is very reasonable. 

Turns out to have been a bug in the GEOS package-- there is an announcement on the postgis-users list. The link to the bug report is here. 

These queries work on postgres 8.4/CentOS 5 (64 bit). Under 9.1 they also result in a spike in memory usage. I've run vacuum full, analyze, and reindex on the database and have also adjusted the memory parameters in the postgresql.conf (using pgtune)-- makes no difference. 

Which is what we see ($PGDATA/base/83637 being the subdirectory for the new database). Dropping that database should also delete the data files: 

It depends. They all speak different dialects of SQL. Not having any details to work with I would recommend Postgresql though.