Given a list of (small) primes $ (p_0, p_1, \dots, p_{n-1})$, is there an (efficient) algorithm to enumerate, in order, all numbers that can be expressed as $ \prod_{k=0}^{n-1} p_k^{e_k} $, where $e_k \in \mathbb{Z}, e_k \ge 0 $? What about in a certain interval, potentially at an exponential starting point? For example, if we had the set $(2,3,5)$, the first few numbers would be $(2, 3, 2^2, 5, 2 \cdot 3, 2^3, 3^2, 2 \cdot 5, \dots )$. Is there an algorithm to efficiently enumerate all the numbers not expressible as a product of powers of primes from the set? How about in an interval? Note: I just saw the Polymath paper on deterministic prime finding in an interval ( Deterministic methods to find primes ) and thats what inspired this question. I don't know if it's important that the set be a list of primes, but I'll keep it in there just in case. EDIT: I was unclear by what I meant by 'efficient' . Let me try making it more precise: Given a list of $n$ primes $(p_0, p_1, \dots p_{n-1})$ and a bound, $B$, is it possible to find in polynomial time with respect to $lg(B)$ and $n$, the next integer, $x$, such that $x > B$ and is expressible as a product of powers of primes from the list? 

The above mentioned section and especially the quoted passages seem blatantly wrong to me, as if the author doesn't even get the concept of a Turing machine or what the Church-Turing thesis is. And yet, the article is constantly cited as a source by those who argue against the Church-Turing thesis, not just in the Philosophy SE, but even by relatively well known philosophers like Massimo Pigliucci. The main reasons why the article carries so much weight is that the SEP is considered a reputable source in the philosophy community, and the articles there are subject to review, and that the article's author, Jack Copeland, is an established philosopher who has published extensively on Turing and on AI. And yet the way I see it, the article is fundamentally wrong in its presentation of the thesis, reputability of the source and the author not withstanding. My questions: 

(Italics mine). I would dismiss this off the bat as non-serious, given the strong nature of the claims, if it weren't for the fact that this was published in Science, and that related material by some of the authors was published in Nature Physics, in an IEEE journal and in Physics Review E, all of which are reputable peer-reviewed publications that wouldn't let such claims get published without them being serious. So is it true? Can these people solve NP-complete problems in P-time using their model? 

I'm not sure if this is what you're looking for but there's a sizable literature on the 3-SAT phase transition. Monasson, Zecchina, Kirkpatrcik, Selman and Troyansky had a paper in nature that talks about the phase transition of random k-SAT. They used a parameterization of the ratio of clauses to variables. For random 3-SAT, they found numerically that the transition point is around 4.3. Above this point random 3-SAT instances are over constrained and almost surely unsatsifiable and below this point problems are under constrained and satisfiable (with high probability). Mertens, Mezard and Zecchina use cavity method procedures to estimate the phase transition point to a higher degree of accuracy. Far away from the critical point, "dumb" algorithms work well for satisfiable instances (walk sat, etc.). From what I understand, deterministic solver run times grow exponentially at or near the phase transition (see here for more of a discussion?). A close cousin of belief propagation, Braunstein, Mezard and Zecchina have introduced survey propagation that is reported to solve satisfiable 3-SAT instances in millions of variables, even extremely close to the phase transition. Mezard has a lecture here on spin glasses (the theory of which he has used in analysis of random NP-Complete phase transitions) and Maneva has a lecture here on survey propagation. From the other direction, it still looks like our best solvers take exponential amount of time to prove unsatisfiability. See here, here and here for proofs/discussion of the exponential nature of some common methods in proving unsatisfiability (Davis-Putnam procedures and resolution methods). One has to be very careful about claims of 'easiness' or 'hardness' for random NP-Complete problems. Having an NP-Complete problem display a phase transition gives no guarantee as to where the hard problems are or whether there even are any. For example, the Hamiltoniain Cycle problem on Erdos-Renyi random graphs is provably easy even at or near the critical transition point. The Number Partition Problem doesn't seem to have any algorithms that solve it well into the probability 1 or 0 range, let alone near the critical threshold. From what I understand, random 3-SAT problems have algorithms that work well for satisfiable instances nearly at or below the critical threshold (survey propagation, walk sat, etc.) but no efficient algorithms above the critical threshold to prove unsatisfiability. This is just state of the art right now and could of course change in the future. 

In my opinion, this is actually one of the main questions in parameterized algorithms. There is a number of articles that discuss the "art" of problem parameterization, I list a few of them below. In a nutshell, an ideal parameter $k$ for a problem should give you two things: 

In general, searching a neighborhood for a better solution and reporting the whole neighborhood are two different things. There are for example some exponential neighborhoods, that is, neighborhoods whose size grows exponentially in the input length $n$, that can be searched in polynomial time for a better solution, see for example A study of exponential neighborhoods for the Travelling Salesman Problem and for the Quadratic Assignment Problem by Deineko and Woeginger. The PLS framework, however, is not so much concerned with the running time of searching the neighborhood but with the number of iterations that are needed to find a locally optimal solution. 

The problem is fixed-parameter tracable for parameter $k$ with a good running time dependence on $k$. That is, it can be solved in $f(k)\cdot n^{O(1)}$ time for some moderately exponential function $f$. The value of $k$ is small in typical instances of this problem. 

Perhaps I'm missing the motivation for your question but there are many examples of empirical results motivating research, algorithms and other results. MP3 use psychoacoustic to optimize the algorithm for human encoding. Plouffe gives an account of discovering the BBP spigot algorithm for the digits of $\pi$ where he recounts the use of whatever Integer Relation Algorithm Mathematica was using to discover the formula. Along the same line, Bailey and Borwein are big proponents of experimental mathematics. See "The Computer as Crucible: An Introduction to Experimental Mathematics", "Computational Excursions in Number Theory" amongst others. One might argue that this is more experimental Mathematics but I would argue that at this level the discussion the distinction is semantic. Phase transitions of NP-Complete problems are another area where empirical results are heavily used. See Monasson, Zecchina, Kirkpatrick, Selman and Troyansky and Gent and Walsh for starters, though there are many, many more (see here for a brief survey). Though not quite on the level of Theoretical Computer Science or Mathematics, there is a discussion here about how the unix utility grep's average case runtime beats optimized worst case algorithms because it relies on the fact that it's searching human readable text (grep does as bad or worst on files with random characters in them). Even Gauss used experimental evidence to give his hypothesis of the Prime Number Theorem. Data mining (Bellkor's solution to the Netflix Prize to make a better recommendation system) might be argued to be a theory completely based on empirical evidence. Artificial Intelligence (genetic algorithms, neural networks, etc.) relies heavily on experimentation. Cryptography is in a constant push and pull between code makers and code breakers. I've really only named a few and if you relax your definition of empirical, then you could cast an even wider net. My apologies for being so scattered in answering your question but I hope I've given at least a few examples that are helpful.