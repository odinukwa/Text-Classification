With dbvisit you can maintain a standby database, in a very similar way as Data Guard does in the Enterprise Edition. You might want to check this out. The setup is very easy, support is good and the pricing is very reasonable, if not, cheap. I blogged about it a few times, see how to configure dbvisit to manage existing standby database running SE and Cost reduction using Oracle SE and dbvisit standby I hope this helps, Ronald. 

version, OS, tool are helping things to help you solve this. I guess OS is Windows. version 9i. export full. If this is the case: connect to the server, set ORACLE_HOME=/where/is/oracle/home/ set PATH=%ORACLE_HOME%/bin;%PATH% set ORACLE_SID=ORCL -> or your ORACLE_SID name 

It depends on the exact version. From 11gR2 we can use job_queue_processes = 0 to prevent any job from running. Before 11gR2 you could use services for that. To use services make the job classes that you want to use depend from a service that is controlled using the instance parameters and leave it out when starting the service. 

You can calculate the end size before hand. A lot easier is to test with a sample. Just deleting rows from your source table won't solve your space issue. The table uses a certain space and it will remain doing so until you are able to empty complete blocks from the end of the table and can adjust the high water mark. If your source table is partitioned, easiest is to drop the empty partitions. This will free space that can be re-used by your table2. This only works if you commit after emptying a partition, otherwise, the partition will be locked. Fastest will be working by partition and drop them when copied. That is faster than deleting the rows. 

Indexes are not the part of relational math at all. They are part of RDBMS implementations only to speed up data proceeding. Keys in opposite are r/math constraints that are applied on the sets that should conform some restrictions like uniqness or existence. In practice primary key is the table-wide always-existent unique identifier, sometimes implicit and automatic. Primary index can be built over the primary key but that is not necessary. Moreover, table can have more than one key suitable to be primary key at the same time. And that alternatives can become ad hoc. 

Your query haven't appropriate index (only simple index for is used) and thus is forced to fetch 11898928 rows to apply the timestamp restriction. Add the multicolumn index (class, timestamp) and you'll see what happens. Please post the new output to show the difference. 

Reorder fields in the index definition placing more selective fields first. To determine selectivity run the next query: 

Here I suppose your table have some autoincremented column . If no you have to add it and fill with 1..n INTs. If there is no plain dupes then column is not necessary and query can be simplified like that: 

Some of fields are already indexed as primary keys so do not duplicate indices, that can impact RAM consumption as well as performance. First you have to do now - is to launch your query with prefix and show the result. N.B. Always use aliases for joined tables, like third one in your code. Always specify explicitly table you refer to, like or . 

Most likely your database is not open for READ WRITE, as the error states. Question that remains is: why? Documentation about Oracle Database Instance When your database is open READ ONLY and try to perform a log switch, you get ORA-1109 : database not open. 

This works. Be careful, I have seen ppl doing similar things without testing, effectively destroying databases, especially when using patterns and manipulating database objects with the results. If the output is OK, activate the execute immediate. 

For this you use pass through. The client connects to the TT instance and TT passes the query through to the Oracle rdbms, when needed. Why not just add a few more nodes to the grid and also cache the bigger tables? 

I am not sure how smart it is to run multiple copies in parallel. It all depends on what your hardware can handle. If you want to run in parallel, run the copies in the background but be a little careful, when running too much in parallel, it will definately cause a slower throughput than running in a sequential way. For windows it will be slightly different, the loop construct will be different. I hope this helps. 

yes you can, just be sure that the transactions are defined by the uppper layer - the client. A transaction has to be Atomic, so has to succeed (and commit) or has to fail (and rollback). 

In SQL Server there is a separate thread that periodically (default 5 seconds, lower interval if a deadlock has just been detected) checks a list of waits for any cycles. I.e. it identifies the resource a thread is waiting for, then it finds the owner of that resource and recursively finds which resource that thread is in turn waiting for, thereby identifying threads that are waiting for each others resources. If a deadlock is found then a victim is chosen to be killed using this algorithm: 

Sometimes when inserting data in a table with many columns it could be useful to know which columns must be specified if the insert-statement shouldn't fail. I wrote this query to find out which columns are not nullable, identity, computed, timestamp and have no default value. 

The transaction owner/application developer is responsible for minimizing the risks of deadlocks occurring, and to do that they should: 

Identify threads that are not unkillable (e.g. a thread that is rolling back a transaction is unkillable). Find the thread with the lowest deadlock priority. Chose the one that is cheapest to roll back, i.e. the one that has done the least work so far. 

and after that the call from the web server also completed in time just fine. I would suspect that the same plan should be used since I was using the exact same parameters from both connections, but I'm not sure. My question is: Can there be different plans or buffers for different connections? Or could it have been some other side effect caused by me running the above dbcc-commands? 

You have to filter out what you need from the whole product. That can be done in two ways. You can use or . There is significant difference between this approaches. generates the full product and then filter out only those rows that match conditions. create the product that initially contain only pairs of rows that meet requirements. 

Here I suppose that is fetched from the another table and perform the mass update for all the rows at once. If the is bigger than then become negative therefore I've add it to the instead of substraction. 

Primary keys are always unique thus you can't create the pair seller-customer that already exists. Foreign keys with restrictions prevents creation of pair with nonexistent seller/customer/both. 

Probably doesn't listen the TCP sockets at all and console client is connected via filesocket. Try to list all network sockets listened at your host: 

To test whether the query selects only desired rows you can replace by and check the resulting set of rows. 

The other advice is to simplify your calculations as possible. Do not calculate again and again those things you have calculated already, like . My final code now is such simple: 

Accidentally I have the DB with table that contain 50+ million of NMEA rows in it. The table have the complex index (timestamp, lat, lon). Simple query has been launched: 

Make sure to keep transactions as short as possible. E.g. don't show a login form after starting a transaction and wait for user input, instead gather all the info you need and then run the transaction. Use the lowest possible isolation level, e.g. don't set serializable when you just want to temporarily show some values to the user. Please note that setting correct isolation level is a science in itself and out of scope in this answer. If you are the victim of a deadlock, i.e. you get error# 1205, then re-run your transaction transparently to your user. Since the other, competing, transaction has now hopefully acquired the resources it was waiting for and finished, it is unlikey that you will encounter the same deadlock again. 

I'm using Red Gate SQL Compare to create a release script based on differences between SVN and a database. This results in a script containing a bunch of table- and procedure-changes and it works fine. However, one thing puzzles me, it's using transaction isolation level serializable. I know what it does to dml-statements, but I'm not sure what it means for ddl. Can someone enlighten me, perhaps with an example? 

We had an issue in our dev environment where a procedure call timed out from the web server after 30 seconds. I traced the query and ran it manually (same params and all) from SSMS and it executed in about 2 seconds. I then ran 

The hint you specified was almost correct. In the hint you should specify the correct index_name, or the column list. Given the fact that the index_name is system generated, I would specify the column list. 

If you want to switch to a different database just issue . oraenv (if /usr/local/bin is in the PATH) and give it the ORACLE_SID of the database that you want to manage. Most databases will be accessed using a client from an other machine/VM. In that case you also might want to start the listener: 

At this moment, if you want a more up to date replacement for sqlplus, I would say, go for SQLcl. It is part of sqldeveloper and has a lot of nice features you always wanted to have but were afraid to ask for. You can download it for free here sql-developer/downloads (check bottom of page for SQLcl download) It has 

The regular session background processes can normally be killed. The special database processes ... many will automatically be restarted but better is to leave them running. Often those client processes have a wait event, something like 

The number one tuning rule AMM (Add More Memory) is a simple one. It is also one that is very costly and at the end one that is not effective when there are problems in selectivity. Even if a database fits completely in memory, the performance of the application can be bad. In a worst case scenario because of locking and latching during very a-selective SQL executions. Those should be fixed first. One reason is concurrency which is like hitting - and holding - the breaks if every SQL accesses all data in a table every time. Make sure no SQL accesses more rows than needed. That is giving the most effective way to keep performance good. A normal database knows how to handle io and does some form of caching of most used data. If your application has already minimized all possible accesses, and you already use the fastest disk systems, consider using real flash memory arrays. They can crank-up performance an other level. 

Here table should have an multicolumn index (foo, bar) or (bar,foo), or even both. It's depends on certain data stored in your table and some performance measurements should be performed. Even though for different queries different indexes can become optimal and day-wide range could be faster with (foo,bar) while month-wide range could be faster with (bar,foo). 

Another possible reason for that is the turned . Only numerical IP addresses are accepted while can't be resolved into the . Sure the same problem occurs when DNS service is unavailable and is misconfigured. 

means: IF previously assigned value of is equal to the freshly assigned value of THEN ... Third is that rows are ordered in the ASC-DESC order. If any row with the same have assigned, it will be proceeded first and propagate its value to the all consequent rows. If you have assigned two different values to the any two rows with the same by mistake the alphabetically bigger value wiil be propagated across the group. 

The easiest way is to hold reference to itself in the for the master records while their fields contains zero. Then all the group (master and all its slaves) can be extracted by field value: 

where is an IP-address of your host OS. If you have dynamically assigned IPs on your LAN then you have to allow connections from the whole subnet your IPs are belongs to: 

The original log file might be truncated so the space can be reused, but it's size might stay the same. 

Both statements would be evaluated to , thereby we have found out the individual values. Note: In the real world, we would iterate through all values from 1, 2, 4, 8, 16 and so on to the maximum value of an integer (or whatever datatype the parameter was set to). 

The others are combinations of the documented flags, e.g. 24 is 16 and 8. This is a method for simulating optional parameters and is used in e.g. C iirc, the numbers and so on are structured like that because they correspond to binary values that combined in any unique way create a unique number. The function that accepts them then use bitmasking to extract each individual number. E.g. if you send the value 6 to a function, then we know that this is a combination of 4 and 2, to find this out using a bit mask we would do: 

Because when you're creating a full backup you're creating a backup of the data (as it is physically and as it is materialized from the log). Restoring it somewhere else restores it as data, not log.