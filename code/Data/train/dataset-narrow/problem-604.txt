Yes there are significant benefits to data normalization if you are willing to do the work to achieve and maintain it. The two fundamental benefits to normalization are: 

Department - Number, Name Employee - Name, SSN, Address, Salary, Sex, DOB Location - ??? Project - Number, Name Dependent - First Name, Sex, DOB, Relation 

It is important to distinguish between conceptual, logical, and physical levels of design. Conceptual Level Two excellent and complimentary resources are: 

The unique identifier for Manager would be the personnel id. That identifier would migrate to Team as a column and thus refer back to Manager. This will ensure each Team can be managed by one and only one Manager. This constraint would be a Foreign Key constraint. The constraint on Manager, which specifies the personnel id as the key, would be called a Primary Key constraint. 

In looking at the table in the exercise these conditions are met. Once a table is normalized, it can be further normalized as a way to eliminate certain redundancies which occur due to functional dependencies between the columns. By functional dependency we simply mean that the value of one column always determines the value of the second. This is based on the mathematics of algebraic functions. For example, the value 2, when plugged into the algebraic function 2x + 3 will always yield 7. 2NF is defined to mean that each non-key column in the table is fully dependent on the entire key - thus no partial key functional dependencies. The Exercise When inspecting the example table we can easily see is dependent only on and not on . Each time for example we see a value of 295 for we see a value of Miracle Holidays. The repair is exactly as you have done - split and out into their own table where there can be a single unique row for each with the corresponding . Now is fully dependent on the key - . simply stays in the original table and there is no need to redundantly split it out into a third table. A Caution The exercise is asking you to infer the functional dependency of on based solely on inspection of the data. While in this simple example it is obvious, in the real world you cannot simply assume a functional dependency exists based on the existing data. What appears to be a functional dependency might turn out to instead be coincidental. Because of this, in the real world you would always ask the expert in the domain of knowledge the table represents what the functional dependency should be. Getting More Information Normalization is a very complex topic and I have glossed over many important concepts. CJ Date has written an entire book with respect to it called Database Design and Relational Theory: Normal Forms and all that Jazz. While definitive, it is hard to grasp all the formalisms. An excellent reference that presents the formalisms in language more easily understood by common practitioners is Fabian Pascal's Practical Database Foundation Series. Studying both of these references - first Fabian's and then Date's - will give you all the information you need to master normalization as a repair procedure. 

This indicates that each department will be associated to one or more locations, and thus necessitates breaking this out to its own entity type. A second reason to elevate it is that we can think of a location as a place and as such would have its own attributes such as name, address, city, state, zip, and so on. A third reason to elevate it is that the location is referenced by more than one entity type - the department and the project. If you find the element in question is referenced by more than one already identified entity, you should consider it an entity in its own right that is related to the others, and not simply an attribute of the others. There is no science to this however - its purely a judgment call. This is why ER diagramming is at the conceptual level as its subjective to individual perspectives. One person's entity is another person's attribute depending on their perspective into the domain of interest. Regarding the relationships, here is a list of what I highlighted in pink: 

Aside: One thing I will gloss over in the interest of space but that is extremely important is truly understanding all of the candidate keys. In this example we have RESPONDENT_ID as the PK. Are there other data elements that uniquely identify a respondent? Just designating a surrogate key upon which we declare all the other now "non-key" columns functionally dependent does not change the functional dependencies of those non-key columns on some other column or set of columns which also form a candidate key. Another Aside: There is a CURRENT_IND column. That makes the PK of RESPONDENT_ID very suspicious. If this means what it usually means, then the table could potentially have multiple rows per RESPONDENT_ID, many of which are not current, and one of which is. Then the real candidate key is something like RESPONDENT_ID plus CREATE_DATE. This makes the example too complex to analyze though so I will ignore it. But in reality this is a real problem that implies you really have two types of things here - current respondents and former respondents. This gets into temporal concerns which are complicated to address. Let us assume the following functional dependencies for the second table: 

Another benefit of the relational approach is that the performance of the system can be managed to a great extent at the physical level without impacting the logical database schema used by applications to work with the data. With the relational approach you can focus on the correct logical schema first and then in most cases let the DBA implement a correct physical design and infrastructure to best support the read heavy workload. This is not the case in navigational systems where the programmer must specify the access paths directly in programs. 

Its pretty easy to do this but I wouldn't recommend it. To implement a single "document" table for all possible documents and parents, you just abstract the id into a "parent_id" column, and add a "parent_table_name" column. There are a lot of issues with this approach. Some are: 

The short answer here is none. Normalization, more formally called Projection-Join Normalization, is a scientific process in which one can remove redundancies in R-tables due specifically to join dependencies which are not implied by the candidate keys. The join dependencies are exploited by taking projections based on them to create two or more tables from the original table which removes the redundancy. It is important to note that normalization cannot remove all redundancy. Instead, it can remove only redundancies caused by join dependencies not being implied by the primary key. 

The down side of this approach is the time between the execution of drop partition and rebuilding index, those indexes will be unusable that might create performance problems. So my question is, is there any option where I can drop partition and rebuild index "online" in one query? Currently I dont think we have following option. 

I am using Oracle 11g. I have a requirement to drop a partition and rebuild global indexes. The query below does job well but BLOCKS all DML operations on the table until the indexes are rebuilt. 

I have confirmed that OrderID column (Foreign Key column) in the PLAN table has index on it. Tried increasing PCTFREE parameter on the table. 

After 30 seconds a notification needs to be sent out to all clients that a lock has expired and this employee is available again for updates. Now to identify if LOCK_UNTIL duration has reached, application makes a SQL call to database every 2 seconds to see if 10:00:30AM has reached. Performance Issue: This call every 2 seconds is causing lot of overhead on the database and on the application server. I am looking for a better ways where Oracle itself initiates a notification to the server when lock expiration time has reached. Is there any way I can achieve this? Possible solutions: 

Concern:App knows lock expiry time but still checks Answer: A 3rd party CRM application can also add lock in the table. So application is the not the only way to add lock. So application does not always knows what lock expiration times are. Concern:30 seconds locking period Performance issue Answer: This was just an example, the actual locking period is configuration and default is 5 seconds. Concern:Caller needs to go to sleep if entity is locked. Answer: Caller also has a mechanism to request a notification when locks are released. All these requests go to a queue. So it is critical that the caller be notified (in the sequence they requested for a lock) when the lock is expired. So caller going to random sleep may not be an option. 

I am using Postgres 9.5 I have tables with date column. All tables are partitioned based on the date column. Table setup: Example of current partitioned tables are like below 

Current Setup: My application uses Java (Spring) and Oracle 11g and has functionality where logical locks are placed on an object before updates are made in the table. For example there are 2 tables EMPLOYEE and EMPLOYEE_LOCK. When any update is made to employee, an entry is inserted into EMPLOYEE_LOCK table to indicate that for next 30 seconds a particular employee is locked. So EMPLOYEE_LOCK table looks like below (as of 10AM) 

My ordering application uses Oracle 11g Database. This DB has a primary table ORDERS and multiple child tables like ORDER_DETAILS, PLAN etc. ORDERS table is LIST partitioned on STATUS column and all other tables are referenced partitioned with ORDERID as a foreign key. At peak load, when order status is changed and ORDERS table row is moved from one partition to another, Oracle performs row migration for all the child tables referenced partitioned by ORDERS table. Due to many tables that depend on ORDERS table, large number of row movements happen causing a deadlock in one of the child table. My question is, how to resolve a deadlock caused in the ORACLE's internal row migration step? Here is an example setup: ORDERS table: 

So for some reason, ORACLE is not taking Index into consideration while running update query on PLAN table. Am I missing something? 

One solution could be to use DBMS_SCHEDULER package and create a scheduled job. But I could not find anywhere in the documentation, some way for the job to notify application server. It can send an email but that wont help me much. Second option could be to use "Database Change Notification feature" but this is triggered on a DML or DDL change on the DB object which is not happening in my case. 

This way application will not insert submitteddate from the code to support Oracle table structure, instead for postgres its a default value which will get added automatically when a row is inserted in each partitioned table. Therefore calculating single date at the beginning of the transaction would be difficult as this will require changes on Oracle side as well to add date column to each table. Is there any other option that is possible? 

Can some one suggest some approach where Oracle some how notifies application server when lock expiration time is reached? Edit (To answer questions raised by Gil Shabtai) Its probably my bad that I tried leaving some of the points from the discussion which I thought were irrelevant to the question I was asking. Here are the answers to your issues / questions raised 

But If I break the query into 2 parts and rebuilt indexes with ONLINE option separately, DML queries DOES NOT get blocked while indexes are being rebuilt 

My requirement is from the example below, can a new table have only partitions Part_2 and Part3. (Dropping partition Part_1 using WHERE clause) 

But havnt got success yet. How do I handle deadlock for this scenario? ------------------------- UPDATE ------------------------- As per suggestions suggested by Wernfried and Gandolf989, I verified if all my foreign keys have indexes on them by running query given in the Gandolf989 answer. Result was "No Rows Found". So it means, all the indexes seems to be in place. But while analyzing I realized, if I check an explain plan for a simple query like below, I see FULL table scan on the PLAN table even after having an index on ORDERID column. 

There are many more child table where ORDERS is there parent table. Under heavy load, when ORDER status is changed which causes row movement between partition, following deadlock error is printed in the log ORA-00060: deadlock detected while waiting for resource In the Oracle trace log, I see following SQL causing deadlock 

My primary goal (with this question) was to see if Oracle can give me some way to identify this expiration time trigger and initiate an activity rather than Application server initiating one. 

I wanted to know if DBMA_REDEFINITON package allows a WHERE clause to filter contents before migration. I have a partitioned table and wants to copy data and constraint to another table using DBMA_REDEFINITON but while copying contents, I do not want to copy a particular partition from the original table. Is it possible to drop this partition using WHERE clause. The question came from following information given on Oracle Tips site 

Failing Edge Case Since partition for inserting data is decided based on submitteddate which is a current date, there will be a situation where an order comes at 2016-11-30 at 11:59PM and data in ORDER table is inserted in NOV2016 partition but data in ORDER_LINE and PLAN table is inserted on DEC2016 partition as by the time inserts are done, date may change in the system. When I try to drop Nov2016 partition from all tables (child first due to FK constraint), ORDER_LINE and PLAN table drop partition might go through but ORDER table partition drop will fail as orderid from Nov2016 partition would be pointing to the DEC2016 partition data in other 2 tables. How do I make sure that the orders inserted on date change still goes to same partition across all tables? Added info (based on @dezso reply) Dezso's transaction suggestions makes sense. But to make question concise, I left some details. With those details, the suggested solution might differ a bit. Application supports 2 databases, Oracle and Postgres. For Oracle, partitioning has been implemented using Reference partitioning with partitioned ORDER table and child tables are referenced partitioned based on foreign keys. For postgres, since there is no reference partitioning option like Oracle, each table was supposed to be individually partitioned using submitteddate. The plan was not to add submitteddate to each table but to use inheritance like below