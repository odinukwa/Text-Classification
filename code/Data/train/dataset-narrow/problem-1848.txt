31 days doesn't seem unreasonable, but it entirely depends on what's being logged and what the potential usage of the logs would be. The size of the event log depends entirely on how many events have been recorded. You can fix the event log retention at 31 days but you will need to consider: 

Is this for production, business use or just for personal testing and messing around? TFS2010 is a bit of a beast, especially when running SQL on the same box, and you'll need an additional VM runnign concurrently if you're planning to use the build server feature (it should not be run on the TFS server itself). If you have a piece of hardware that you can dedicate to the task, look at running ESXi, Microsoft Hyper-V server, or XenServer, then running the VMs on top of that. They're all free, and installation is straight forwards as long as the hardware is compatible. If you absolutely have to run on top of an existing Windows installation, then VMWare Workstation or Virtual PC will probably suffice. Outside of this, there's a good feature comparison matrix here. 

It is possible to do this, but a far better option is to create some internal test mailboxes, and test against those addresses. Unless you're hard-coding the users email addresses into your application, this method is the least hassle, and a perfectly valid test. 

There were a number of changes to the file sharing system in 2008/Vista which may be causing you issues. 

First off, Joel is correct: For competent IT support you're looking at a higher cost. The bottom end of the IT pro market is saturated with self-taught people working from home. The price is good, but the attention to detail, professionalism and consistency isn't there, and there's the tendency to do things 'my way' rather than adhere to best-practices (or even be aware of what they might be). In other words, shoot for the cheapest and you will usually end up with the cowboys. You should be able to find a local IT support company that'll offer you a fixed-term fixed-price environment support that's specific. For example, a 6-month contract for 25 man-hours of environment support per month, of which 10 hours is remote work doing x/y/z maintenance tasks, 5 hours producing a monthly report, an 10 hours on-site work as required. On-site work beyond this charged at x per hour. Ideally you want a company offering their own helpdesk software (we use Beetil, it's great) and you'll just get logins to the system, not have to run it yourself. Key things to look for from your support provider: 

Not sure if this answers your question because there's not a lot of detail but this is just asking you how you'd like to configure this new domain controller. Do you want it to be a DNS server? Do you want it to be a Global Catalog? I'm guessing the plan is not to have an RODC since you already have one there, but those are recommended if the site does not have a secure computer closet or data center. You only have to have the DNS service on one domain controller technically, but in all honesty with most scenarios I set them up as both a DNS server AND as a Global Catalog. DNS for the local site if there are enough users, and Global Catalog so it can answer queries through 3268/3269 TCP (GC queries). The GC is a read-only port that returns partial data for your entire forest, if you have a multi-domain environment this is especially useful. Old documentation says you should only use it as you need to, but this was based on slow connections in days gone by, these days with reliable network speeds that's usually a non-issue. 

Is the Windows 10 system part of a domain? When you load this module by default, it attempts to look at your service records, find an efficiently located domain controller and map it to the PSDrive "AD:\" so that you can navigate it within powershell and review records. It is possible to disable this: $URL$ 

I believe this will answer your question: $URL$ In short, windows permissions vary based on whether the target is a folder or a file. The tables in this explain it in detail. The information is dated but for the most part this should still be intact. This is a bit more up to date and applies now but a bit more complicated. $URL$ 

Pardon, because I'm not super experienced with various Linux distros, but from the Active Directory side, this sounds like you may have a problem with your RID Master. If the pool is depleted and not being replenished, it may be that the other domain controllers cannot communicate with it for some reason. What happens under the hood is the RID master allocates a pool of (i think) 500 RIDs at a time for new objects, and when that gets to less than half a new request is made and a new block of RIDs is assigned. I would recommend trying a full dcdiag and looking through it for any errors (or asking your AD admin to do so) as a starting measure. 

The question you should be asking here (IMO) is why you're putting time and effort into covering client workstations in a DR scenario. What's on those client machines that's critical, and why isn't it stored on the network? There's likely to be more issues lurking under the surface here. Client workstations are not designed for high availability. There's no redundancy across most of the components. I suspect even if you get doubletake working for the client replication, you're still putting a sticking plaster over a much deeper problem. 

We're not seeing any problems running a standard Sophos Endpoint deployment across 200+ Virtual machines on 5 hosts and dual iSCSI GbE connections to each host. Real-time scanning certainly adds some overhead but it's not a showstopper. You need to do some obvious stuff like excluding file types that are going to cause you hassle (mdf ldf, \WinSXS, \NTDS etc) and disable AV scanning entirely when you're backing up/restoring (use pre/post op commands). 

If you can find a suitable means to query Windows WMI from a non-windows host (and there are some available e.g. here's one for Linux), then you can get the current sessions by querying Win32_LogonSessions. The downside to this method is the WMI service needs to be active on your target. The other (maybe easier) alternative is as you mentioned, use an SSH connection to run the qwinsta command locally, and grab/parse that output. Them thar folks over at stack overflow are all brainy when it comes to the programmin', they'll probably have additional insights. 

One useful principle that I try to apply, and see violated regularly, is that a sysadmin should understand the boundaries of their reponsibility, and take care not to overstep the mark. What I mean by this is that often in sysadmin duties, questions and problems will arise that actually require decisions to be made by other areas of the business, but IT may attempt to address the problem without seeking out those decisions from the business. Some good but by no means exhaustive examples of this are: 

I'm already familiar with a partial solution - MOXA make an IP-connected serial server that can connect to a bunch of modems. This works in exactly the manner we need, but it doesn't go as far as integrating the modems - So we still have a bunch of them sat on rack shelves. Can anyone point me in the right direction on this? Edit: I'm thinking I can probably get closest to what I'm after by stacking one of these with one of these. I was hoping there would be an integrated, single-unit solution but maybe not. 

I would be very surprised if your switch can support this. You might have more luck giving both network interfaces the same MAC address. Saying that, I definitely agree with Holocryptic, here be dragons. 

lookup(program) means automount thinks your map is an executable, does it have the executable bit set? lookup(file) is what you should be seeing See $URL$ 

I then have UNDERSCORETODOT="1" in /etc/sysconfig/autofs and automount will happily pull everything out of NISPLUS. LDAP/RHEL5 (almost works) I tried to do the same in LDAP, the home and workgrp shares work fine but the /opt, not so much: 

(in LDAP or flat files) This would be fine except for when the same library or app works on multiple platforms, for example we don't particularly need a 64bit version of vim and I don't what to have a copy of vim-7.3 for every kernel as the RHEL4 one works on everything from 4u2 to 6u0! man 5 autofs suggests that substitution should work in both the key and locations part of an automount map so I'm not imagining this ( $URL$ ). Anyone got any ideas? Is autofs just broken on RHEL5? 

I'm migrating from NISPLUS to LDAP (openldap) and from RHEL4 to RHEL5. The LDAP server is running on RHEL5u4 as are the clients. On RHEL5 variable expansion in auto.master never works, whether in files or LDAP. NISPLUS/RHEL4 (works) I used to have a whole load of tables in nisplus along the lines of auto_opt_Linux_2-6-18_x86_64 containing shares of programs for that particular arch/os combo. EG. in in NISPLUS: 

Do you want it only to happen at system start up? If not you could maybe do something with automount, along the lines of an executable map which checks to see if the USB device is available and if so makes the dependent locations available for mounting. Something like: auto.magic: 

I'm configuring a Red Hat Linux server which will be sending UDP packets out but never receiving ARP responses. So a static ARP entry is required. The obvious way to do this is. 

You'll need to do a service autofs (re)start. Then when you do ls /magic/films, automount will call your script and mount the film share if /media/usb is available. 

However that won't survive a network restart or a reboot. I could put it in the rc.local but that doesn't survive an ifdown && ifup. The way I've found which seems to work is: Add an entry to /etc/ethers along the lines of: 

By disabling encryption and watching the client in wireshark, I check actually what queries are being run and it is searching for auto.opt.$OSNAME.$OSREL.$ARCH without expanding the variables. I've tried running automount -fd -D OSNAME=Linux and this had no effect. I've also tried putting all of this in flat files (not LDAP or nisplus but this didn't help). What do work are mounts along the lines of: 

If you backup server cannot acquire the data from the network fast enough, you're left with slack space all over your tape. This can add up to a significant loss of space on the tape. Investigate the throughput you're getting and whether there are any bottlenecks you can resolve. Also ensure that you have selected hardware compression on the job. The LTO3 tapes are 400Gb raw but are listed as 800Gb under ideal scenarios with compression. Depending on the data you're putting on the tape, you might get a compression ratio better than or significantly worse than 2:1 If you're struggling to get sufficient throughput from your client servers, you can look at shutting down some services during the backup period. For example, most file servers will back up far faster if the antivirus on-access scanner is disabled during backup. In my environment, use the pre & post commands in backup exec to send 'net stop avservice' and 'net start avservice' before/after a backupjob. You can do likewise to shut down other services depending on the server type. For example, a lotus domino server can be backed up much faster if you stop the LotusDominoServer service... this is often acceptable if a backup runs overnight. Another alternative if you have the resources is to stage your backups to a backup-to-disk volume, then later stage it off to the tapes. 

Regarding the alerting for when an alarm clears, you need to alter the Alarm definition so that the notification is triggered on a change of state 'to green' rather than the default, which triggers when the state changes to anything 'from green'. To do this: 

Assuming your requirements are low enough that none of the above is an issue, the final point to consider (IMO quite carefully) is the reflection it makes on yourself (and if you have one, your department) if you go ahead with this. While IT isn't always client-facing in the usual business sense, it's effectively a service to the rest of the business. How's the business take on running on unsupported configurations for these systems? How's it going to reflect on you professionally? IMO the only time a hokey system like this may be appropriate is in an extremely small business where there's simply no cash and you're trying to bootstrap the whole thing. I'm talking mom-and-pop's flower shop and their kid is keeping their workstations, website and internet connection going. Nowadays, for anything larger than that, you'd be better cobbling together a few cloud-hosted services (dropbox/skydrive, gmail/yahoo) simply because they'll offer small-scale solutions with a solid infrastructure you don't even have to think about. Of course all this is nonsense if you're just playing around with some servers at home. In which case some old laptops sound ideal.