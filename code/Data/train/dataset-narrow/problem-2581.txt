The topic is too broad to benefit from a single answer on this site.. but some better things to do can be covered nonetheless: 

The RodriguesRotation is the general rotation matrix against a w axis by an angle theta. In the picture below, the black arrow is the orientation, the red circle with red orientation arrows is what your circle should look like, and the crimson line is the d distance offset from the object's center. 

The OpenGL most closest equivalent of constant buffers might be the Uniform Buffer Object. More details in the link above. 

Imagine you're flying over the ground (the xy plane) and your ship's orientation is known (the green camDir vector). You want your camDir to coincide with the red vector (from your positions to your opponent's position,i.e. normalize(targetPos - camPos) = targetDir ) To turn in a relevant way for your picture description, you could rotate against the Z axis. To find the amount of rotation, you need to compute the cross product of camDir and targetDir. The cross product tells you the shortest turning orientation because |cross(a,b)| = |a| |b| sin(angle(a,b)) . Note that the angle between the two vectors is less than 180 degrees, in absolute value. Now, you can compute the dot product to find the cosine of the turning angle since dot(a,b) = |a| |b| cos(angle(a,b)). The nice part is that both camDir and targetDir are unit/normalized vectors, so their cross and dot products provide you with the sine and cosine of the angle you want. Now just apply atan2 on the sine and cosine and retrieve the value of your angle (the dark-red arc in the image, for example). The blue circle around the plane is the same as your "radar" circle. That should be about it. So you need the camera direction, just the position of your ship is not enough - you have to "fire" in a direction, right? 

that is, exp usually stands for raised to the power (e is the Euler number, the natural logarithm base, etc.) 

As mentioned, if you find out that you're not limited in a sense and could use UDP, go for that. There are some UDP-based implementations out there, so you don't have to reinvent the wheel or work against years of experience and proven experience. Such implementations worth mentioning are: 

This shader snippet is just a hint on how I would do it, it's not a complete copy-paste-done! solution. 

You can check against the individual triangle edges (segments) using an algorithm that provides you with the distance between two line segments.. it's probably not fast, but it works.. If you google for this algorithm, there are various (pseudocode) implementations to help you get started with your own. See here. 

For example, in case of the integration method, I was thinking of having an abstract Integrator class and derive from it to add specific methods (Euler, Verlet, Midpoint, you name it).: 

Search for "Realistic and Controllable Fire Simulation" on Google. $URL$ is one example For starters, you should set a simpler object, that you create from triangles (not the GLUT ones) and think of ways of animating fire. There are way too many possibilities: particle effects, voxels, shaders. Google for any of these ways of simulating fire.. your question is more about explaining/doing a tutorial on how to implement such an effect, so it can't be answered here. Good luck in selecting your tools ;) 

Now, let's return to the problem at hand: we have a point P somewhere in the universe. Critters in frame Fa see it as a triplet Pa, while those in Fb as a triplet Pb. Pa and Pb, although numerically different describe the same position in space. Why? Well, if you consider the world frame, Fworld to be the universal reference frame, you should try and express Pa's coordinates in that frame, then do the same for Pb. How? Using the transformation matrices: _M_B_to_World_ and _M_A_to_World_. You multiply each Pa and Pb with these transformation matrices and you get: 

Is it possible to achieve something similar to nVidia's rain demo using only shader model 3.0 capabilities? If yes, could you point out a few documents/web resources that are suitable candidates and do not require a heavy programming load (e.g. not more than two hard weeks of programming for one single person)? It would be nice if the answer could also contain a pro/con phrase for the proposed idea (e.g. postprocessing rain shader vs. a particle based effect). 

A recurring problem I get is this one: given two different billboard sets with alpha textures intended to create particle special effects (such as point lights and smoke puffs), rendering them correctly is tedious. The issue arising in this scenario is that there's no way to use depth writing and make certain billboards obey depth information as they appear in front of others that are clearly closer to the camera. I've described the problem on the Ogre forums several times without any suggestions being given (since the application I'm writing uses their engine). What could be done then? sort all individual billboards from different billboard sets to avoid writing the depth and still have nice alpha blended results? If yes, please do point out some resources to start with in the frames of the aforementioned Ogre engine. Any other suggestions are welcome! 

where is between (-180 and 180) degrees or (-pi, pi) rads and the is between (-90,90) degrees or (-pi/2, pi/2) rads. Alternatively, take a look at some astronomical coordinate systems. They provide a fairly more similar approach to what you did to construct your fractal shapes by supplying a so called local direction via an angle (improperly called direction, let's call it heading or something..). This way, you can do some interesting things with L-Systems (even generate some weird random ones via any production rule you set..). 

Here's one quick nice trick implemented in the latest Physics SDK: if you have a ball experiencing multiple collisions, then use impulses to resolve collisions. 

An OpenGL program consists of at least a vertex and a fragment shader that are compiled and linked. Once you want to use a program, you must link it. You can create a program manager/shader manager/material manager (call it however you want, equip it with whatever functionality you need). This program manager keeps a map of all vertex-fragment shader pairs that do appear in the materials your objects use. You should group your objects by program and not recompile-relink the program each time you draw an object. You can use the program manager to check whether a certain program is the active one. That active program will be responsible for the shading of any object that issues render calls. Now, the attributes and uniforms need their own managers. Usually, keeping maps to manage them is a good idea. For uniforms, you could keep two kind of maps: 

Although not 100% accurate, in the sense that it may report intersections even when the objects do not actually collide (but for that a more complex solution like GJK could be easily used), here's the solution I came up with after reading Dave Eberly's original paper on OBB vs moving triangle intersection test. In that paper, the intersection test is performed between a triangle, described by vertices and edges , and an OBB, given through its center, , principal axes, and half extents, . The triangle is assumed to be displaced along a direction . The axis, in this case, is not assumed to be normalized, but has a magnitude equal to the total displacement the triangle is subjected to. The problem is not to find an actual intersection point, but to decide whether the OBB and triangle might overlap. For this, Eberly conceived a series of 13 separating axis tests. In summary, the separating axis test works by starting with a candidate axis, , and checks whether the projections of the boundaries of the OBB and of the swept triangle volume (in this case, an oblique triangular prism) overlap as intervals. The origin of the axis is assumed to be at the center of the OBB. Refer to the picture below for a better understanding 

It's best you read nVidia's pages containing some GPU Gems articles. There's the key formula which I will briefly explain to you in the following pseud-answer: This is where you'll find the complete article, and it's a classic resource by now. I will only assume you want an explanation of that process, done in a simplistic manner (as much as I can). The formula and its meaning: You start with a mesh of vertices, i.e. a set {Pk} of vertices with connectivity. You don't care about the topology that much, so disregard the connectivity (that's something skinning shouldn't deteriorate, it should keep manifolds looking like manifolds, preserve topology, etc.). In that formula, vBindpose gives you the rest position of the character's vertex. This is in object space, i.e. the "world' space of that character's frame. Now, each bone has its own frame, and the way to transform a vector written in bone's bone[i] frame into the object's global frame is by multiplying it with matrixBindpose[i]. (imagine you have to assemble a robot, and its forearm vertices are given to you in its elbow frame of reference.. and the elbow is connected to the humerus/shoulder bone.. that's why you need these transformations, mainly to assemble vertex sets into a mesh) So, what does INV(matrixBindpose[i]) x vBindpose mean then? It means that you get the coordinates of the v vertex in the local frame of that bone[i] bone. Why? Because each bone can memorize its own version of where a vertex it affects is with respect to its own frame of coordinates. That means, each bone can provide you with its own relative view of where a point it affects is, regardless of how other bones see that point. Now, what happens when you multiply with the new bone transformation matrix matrix[i]? Recall from the last point that you now have a local version of the v vertex, i.e. how the bone[i] thinks that vertex looks like with respect to its own frame. By multiplying with matrix[i], you end up with a vector/vertex situated in the global/object coordinate frame. Next, you multiply that vertex with the bone's scalar contribution/weight. What do you have then? You end up with a weighted sum of vectors that lie in the same coordinate frame. That's why you can add them/i.e. take that sum over all bones that influence a vertex. Your code doesn't really reflect that you completely understood the procedure. The nVidia article is focused more on how to do that via shaders, that's why they assume there are at most four bones that can affect one vertex of the initial/bindpose mesh. As you said, if you take your starting pose, the bindpose, then matrix[i] x matrix[i]^-1 is the identity matrix. That's ok because the sum will be equal to _sum(w_i)*vBindpose_. Since vBindpose is the vector written in the global/object frame, it's already assembled and in position. The sum of those weights, by construction is 1 (convex combination resulting from a weight normalization process - typically done by the system after weights are assigned to each vertex). So that formula can be used to check whether a model can be assembled correctly (it was normally done by the MD5 Doom3 animation model loaders if I remember correctly). That's it, in a nutshell.. it means that at each step you need to optimize a bit the computations so that you compute the result of that lil'ol formula there. Happy coding :)