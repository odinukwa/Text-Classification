It seems to me your best bet would be a dynamic mesh, whose material texture is the source texture, and which is drawn onto the target RenderTexture. The mesh's UV coordinates correspond to read position, and vertex coordinates correspond to write position. It'd be one draw call, although you'd need to create the mesh as well. Still, for a simple mesh, creating the mesh would be very quick. Here are some performance tips if you do want to go down this route: If your mesh can be of a fixed size (or you have an upper bound on how big it'll be), you can set it up once and then just update what's already there through . By using a fixed size mesh you avoid having to call Clear(), which can really hurt performance but is only necessary if mesh.triangles changes. For example, if you know you'll always need less than or equal to 1000 quads to do this batched blit, you'd have a mesh of 1000 quads, some of which may be degenerate (all vertices in the same position) depending on how many quads you actually want to draw. There's nothing wrong with a few degenerate quads at this scale -- in fact it'll be better than regularly having to use Clear() (which isn't cheap, apparently) every time the mesh changes. If you're updating the mesh very regularly, you can MarkDynamic. This just tells the GPU it should keep the data somewhere it can quickly and regularly update it. Avoid getting the mesh vertices from the mesh ( actually has to build the array for you, even though it's disguised as a simple assignment). Instead you'll want to store your own copy of the vertices array, uvs array, and whatever else you'll be using. If you're on mobile you can optimise this even more by double-buffering the changes to the mesh, so the CPU doesn't have to wait for the GPU to catch up with the changes you're making to the mesh: 

The biggest optimization you can do is to calculate this on the graphics card -- one way to do that is to use Graphics.Blit and a shader to do the brightness/contrast effect. The result has to be a RenderTexture, but if you need the result in a Texture2D, you can use Texture2D.ReadPixels to copy the data from a RenderTexture to a Texture2D, in about 3 lines of code, apparently: 

The bounding box you described is intersecting the frustum, even if your visualisation isn't making it apparent. At a position of 0, 10, 0, and the camera's position at 0, 3, 10, the difference in position is 0, 7, 10. Further, the difference of position from the camera to the top-most near corner of the 2x2x2 box is actually 0, 8, 9. With a FOV of 60 degrees, the camera has 30 degrees between the horizon and the top plane. Over a horizontal distance of 9 units, the frustum will certainly fall short of the 8 units it needs to rise (atan(8/9) = ~42 degrees). I hope that makes sense. I can't imagine what would bring about the problems in your second picture -- especially the whole first row "intersecting" the frustum, but perhaps it's another issue altogether. 

Simple (no sweeping) GJK to find the minimum distance between object A (the fat ray cast) and object B (the potential collider -- this is after culling candidates through simpler means, of course). Let's call that distance vector D; Calculate the maximum safe distance we can move A along sweep vector V without intersection given D -- that was something like , I think. That is, if we're moving directly towards the target the result is just |D|, but if we're moving tangentially to the D, we get... well... divide by zero. But that's just because we'll never hit it, so you can catch that or a negative denominator before it happens and say, "No collision!" If we haven't covered the whole distance we want to sweep, then a collision is still possible. Move A the distance we just calculated in step 2 along V. Repeat from step 1 over remaining length along V until we know there's no collision, or we have come within a very small range of the target (let's call that the skin thickness). In the latter case, it's the distance covered to get there that your fat ray cast will return. 

Instead of using LookAt, you can have the shader force the object to be rendered facing the camera. Thus, no script needs to be added to the tree, and the extra work the shader does is very little. Here's an example: Billboard Shader Also, since they don't actually move or rotate (as far as the CPU is concerned) and share the same material, you can turn on static batching. I hope this helps! 

Use more samples within your blur range, so that you're not skipping over pixels in the original image; or Downsample the image first, so that, at the resolution you're blurring, you're not skipping pixels. 

Short version: a) 0.5/0.5/0.5 In the metals workflow, the specular colour will be exactly as sampled in the Albedo map. It's possible with PBR to have some metals shinier than others (while of course still obeying conservation of energy), which can only be achieved by allowing dimmer colours in the Albedo Map to result in dimmer specular values. 

Neither of those lists are exhaustive. So while your observations about shadow mapping and stencil shadows are correct, the shortcomings of stencil shadows are show-stoppers for a lot of desirable effects, and shadow mapping has a few advantages of its own. 

HCY could be better for your needs. According to Wikipedia's HSL and HSV page, the problem you're having with HSV -- that the same "V" with different hues doesn't correspond to human perception -- is similar with HSL, but HCY could suit your needs -- the "Y" in HCY is for "luma": 

If target is a transform, you want it to be , (no transform.position). If target is a Vector3, you want the next line to be , (no target.position). I assume it's the first one, because of the wording of the error, but the second one is to show that it could be fixed another way. 

The rigidbody2D's transform has up, right, and forward properties that'll tell you what's up, right, or forward for the object, respectively. In 2D, only two of those will actually be useful to you, and I imagine those would be up and right. So, if you want to apply the force in the object's up direction, do away with and calculate thrust something like this instead: 

There are a variety of formulae for converting colour to black and white. Furthermore, the programmer might mean something else when converting a vector into a scalar (length would be a common example). The shader compiler keeps things as simple as possible, just taking the first value in the vector, in your example. Experiment with different ways to get a black and white that looks good in your situation. Some examples might be: 

Shadow mapping and stencil shadows (shadow volumes) are definitely the big two, as you've mentioned, so I'll stick to comparing the two. And since you've pointed out the most obvious shortcomings of shadow mapping and advantages of stencil shadows, I'll do the opposite. Stencil shadow shortcomings 

In favour of shadow mapping Apart from being without the limitations I listed specifically for stencil shadows, there are a few extra advantages to shadow mapping: 

So, you want the angle of velocity relative to the x-axis? That'd be and for Q3 and Q4, respectively. Though, since the balls are almost aligned along the y-axis in relation to each other, it looks like maybe you're looking for the angle of V3/V4 relative to the surface at the point of impact, even when that doesn't line up with the x-axis. In that case, you need the normal of impact (most collision functions will give you this as some of the output, but it's easy for spheres/circles anyway -- just normalise ). Anyway, you can use that normal and V3 to calculate Q3 like so: . It'll vary a little depending on what libraries you're using, but basically: the dot product of two normalised vectors (that is, vectors with a magnitude of 1) will be the cos of the angle between the vectors. Apply the inverse cos function (acos) and you get the angle between them. For Q4, replace "V3" with "V4". 

Your problem is . I didn't know you could actually set transform.up until now, since it's actually a summary of a more complex state of the object -- its rotation. Setting it tells Unity to orient your object along that axis, but doesn't tell Unity to keep transform.forward as close as possible to what it used to be. You're probably better off using Transform.LookAt, which doesn't sound intuitive at first, but hear me out. It has an optional up parameter. So in BaseObject.FixedUpdate, in order to do this without having to worry about tricky maths myself, I would first figure out a point in front of the character using its current transform.forward. Then I'd use transform.LookAt to look towards that position, passing it TowardOrigin as the up vector. So it'd be something like: