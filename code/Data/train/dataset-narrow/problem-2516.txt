I guess the best answer to your question is to point out that there have been several "barriers" to proving the P vs NP problem which have been identified and which rule out a number of approaches to a proof. The three main barriers identified are relativization, natural proofs and algebrization. All of these are described on the P vs NP Wikipedia page under the heading "Results about the difficulty of proof", here. A more detailed explanation can be found in the papers outlining these barriers referenced on the same Wikipedia page. 

This is certainly not a full answer to your question, but hopefully helps with the first part. There seems to have been quite an amount of interest in using quantum algorithms to identify unknown oracles. One example of this is a recent paper from Floess, Andersson and Hillery (arXiv:1006.1423) which adapts the Bernstein-Vazirani algorithm to identify Boolean functions which depend on only a small subset of the input variables (juntas). They use this approach to determine the oracle function for low-degree polynomials (they explicitly deal with linear, quadratic and cubic cases). 

Pretty much the same as a VCR repair man. Both consider how to get the best performance out of machines which read and write information to extremely long bits of tape. This may be a little more tongue in cheek than what you were after... 

Yes, quantum computation allows the generation of truly random numbers, and the operations necessary are so simple companies like id Quantique are already selling quantum random number generators. It is even possible to generate random numbers in a way that proves to the person generating them that they are random (via a violation of Bell's inequality) but this does need a short seed for the proof to be complete (though the numbers are random anyway). Unfortunately commercial systems are not that sophisticated just yet, and so produce random numbers in a way that is difficult to test. 

I think perhaps you may be over-reaching. As you point out your self, the construction of the computer itself could be made reversible, and so the energy investment in construction won't yield an interesting lower bound. Considering the ancillary register is an interesting idea, but I don't think it is quite as straight forward as you make it sound. In particular, it is not necessary to initialise ever bit or qubit in the ancillary register. We can use a fault-tolerant construction to ensure that the probability of obtaining an incorrect result is bounded. Von Neumann provided such a construction for classical computing using majority gates which has a threshold of $\frac{5}{6}$ for the probability of obtaining the correct output from a gate, and in quantum computing this is a very active research area, where the best error thresholds are on the order of a few percent. This gives a threshold in terms of the polarization of the system (assuming the gates themselves are noiseless). However, if the decoding circuit is noisless, then the classical threshold moves to $\frac{1}{2}$, which seems to indicate that a large noisy ancillary system can be exploited by using the input/output system for decoding, independant of the polarization of the system. In fact, there is a model of computation where the system is composed of a single quantum bit (qubit) together with an ancilla system which is not polarized (i.e. in a uniformly random state, which can be seen as the infinite temperature thermal state). Note that you can prepare such a state at finite temperature. This is known as the one clean qubit model. What's interesting is that this model is far from trivial, being believed to be sufficient to solve some classically intractable problems, while not being as powerful as a universal quantum computer. An example of this is this paper (arXiv:0707.2831) by Peter Shor and Stephen Jordan, showing that estimating Jones polynomials is complete for the model. With this in mind, in general the ancilla system does not seem to need to be initialised to provide a computational advantage, which seems to undermine the key assumption you make. As such, I believe your conjecture is false. 

It seems clear that a number of subfields of theoretical computer science have been significantly impacted by results from theoretical physics. Two examples of this are 

Red circles: Data points from simulation of process averaged over 10k runs. Green: $n \log_2(n+1)$. Blue: $n \log(n+1)$. 

The noise model I am referring to is the one dealt with in quant-ph/0504218, where Aliferis, Gottesman and Preskill prove a lower bound $2.73 \times 10^{-5}$. Note, however, I do not care which type of encoding is used, and it need not be restricted to the code considered in that paper. The highest I'm aware of is $1.94 \times 10^{-4}$ due to Aliferis and Cross (quant-ph/0610063). Has this value been improved upon since then? 

There is a slight issue with the question. There are in fact more than two ways of analyzing an algorithm, and one of the theoretical ways which has been neglected is expected run time, rather than worst case run time. It is really this average case behaviour that is relevant to doing experiments. Here is a very simple example: Imagine that you have an algorithm for an input of size n, which takes time n for each possible input of size n except for one specific input of each length which takes time 2^n. Hear the worst case run time is exponential, but the average case is [(2^n -1)n + (2^n)1]/(2^n) = n - (n-1)/2^n which limits to n. Clearly the two types of analysis give very different answers, but this is to be expected as we are calculating different quantities. By running the experiment a bunch of times, even if we take the longest run time for the sample, we are still only sampling a small portion of the space of possible inputs, and so if hard instances are rare then we are likely to miss them. It is relatively easy to construct such a problem: If the first n/2 bits are all zero, than solve the 3SAT instance encoded with the last n/2 bits. Otherwise reject. As n gets large the problem has roughly the same run time in the worst case as the most efficient algorithm for 3SAT, where as the average run time is guaranteed to be very low. 

The majority vote operation comes up fairly often in fault-tolerance (and no doubt other places), where the function outputs a bit equal to which ever value appears most frequently in the value of the input bits. For simplicity, let's assume that whenever the input contains an equal number of bits in state 0 and state 1, it outputs 0. This can be generalized to dits where there are more than 2 possibilities for each input by returning the value which occurs most frequently in the input, and in the case of a tie, returning the most frequent value which comes first lexicographically. Let's call this function "plurality vote". I am interested in the output of such a function when each input has a fixed probability distribution (and the distribution is the same for each dit in the input). Specifically I care about the following question. 

introduce an indexing on the semiprimes (which in itself I suspect is as hard as factoring them), or by generalising the problem to include non-semiprimes. 

I'm not so familiar with this paper, but I will try to give a rough answer to each of your questions after a cursory skim. 

FLARE produces rather beautiful graphs and visualisations, and in fact I have used it for just this purpose with a small private social network. In particular you may want to look at "layouts" in the demo as it gives an excellent demonstration of transforming between different ways of drawing the graph. 

Apologies in advance if this question is too simple. Basically, what I want to know is if there are any functions $f(x)$ with the following properties: Take $f_n(x)$ to be $f(x)$ when the domain and codomain are restricted to $n$-bit strings. Then 

Now, it is straight forward to calculate the exact answer to the above question as a sum over multinomial distributions. However, for my purposes, this is less than ideal, and a closed for approximation would be better. So my question is: 

This is not a complete answer, but perhaps it goes some way towards answering the question. Since $G$ has infinite order but $C(d)$ is not, then $G$ necessarily contains a non-Clifford group gate. However $G$ has $C(d)$ as a subgroup. But for $d=2$ the Clifford group plus any other gate not in the Clifford group is approximately universal (see e.g. Theorem 1 here). Therefore all such $G$ provide dense cover on $SU(2^n)$. For the case where $d>2$ it seems like it may be possible to prove that you still get dense cover along the following lines (using the notation of the paper linked to in the question): 

P is contained within BQP. This is trivial, since quantum operations are a superset of classical operations. Indeed, BQP contains BPP as well, since you can use Hadamards to produce randomness in the computational basis. Much less trivial is the relationship between NP and BQP. At present, the consensus opinion seems to be that neither entirely contains the other, though clearly both contain P, and certain problems believed to be NP-intermediate like factoring integers, so their intersection is itself non-trivial. There are oracles relative to which BQP is not contained within NP (via recursive Fourier sampling for example), and conversely oracles relative to which NP is not contained within BQP (this is true with probability 1 relative to a random oracle, see quant-ph/9701001). We do know that BQP is contained within PP, but there is strong evidence that it is not contained within the polynomial hierarchy at all (see Scott Aaronson's paper on BQP and the polynomial hierarchy for example). 

My interpretation is that the author is saying that for a Boolean function $f:\{0,1\}\to \{0,1\}$, "a quantum evolution" which implements this function is a unitary operator $U_f$ such that $U_f| x \rangle \otimes | y \rangle = | x \rangle \otimes | y \oplus f(x) \rangle$. (I assume he is only considering unitaries rather than CPTP maps, as this makes more sense in the context of Deutsch's algorithm). Here the second qubit is the output qubit. The reason two qubits are required is that not all boolean functions are reversible, and the ancilla is required in order to satisfy unitarity by making the evolution reversible. I believe this is the situation the author was trying to get across, as this is indeed how the oracle functions in the Deutsch algorithm. Thus, the unitary has not fully been prescribed, but is of the general form $U_f = \sum_{x,y = 0}^1 \phi_{xy} |x\rangle\langle x| \otimes |y \otimes f(x) \rangle\langle y|$ where $\phi_{xy}$ are roots of unitary. This is the most general description of a unitary which implements the required functionality. As Shaun mentioned, the algorithm would also work with an oracle which flipped the outputs, but that would no longer be the oracle for the same boolean function, and the whole point of the algorithm is to show that classical and quantum strategies require different numbers of calls to the same oracle. 

As Peter mentions in the comments, it seems impossible to give an authoritive answer without knowing what exactly you are planning on doing with them. That said, there are at least a few places I can point you which may be of some use. Firstly, Pauli measurements on cluster states can be efficiently simulated on a classical computer. This is a direct result of the Gottesman-Knill theorem (see this paper by Gottesman and this follow-up paper by Gottesman and Aaronson), which Clifford group circuits can be efficiently evaluated via the stabilizer formalism. So it may be that stabilizers are the way you want to go. However, if you want to be a little less general, and restrict yourself to graph states (a general name for cluster states on general graphs) then there are two papers by Hein, Eisert and Briegel and Schlingemann which describe how Pauli measurements performed on a graph state result in states which are locally equivalent to graph states, and provide rules for these transformations. Thus it is quite possible to work with graphs as your data structure, as long as you do not intend to leave the Clifford group. Finally, Ross Duncan and Lucas Dixon have taken a category theoretic approach for automated reasoning about graphs and have produced some nice proof of concept software using this approach (see here). Also, I would point out that Raussendorf, Harrington and Goyal have previously looked at implementing topological computations via measurements on cluster states (they use it to achieve fault-tolerance in cluster states in a very beautiful way), and so you might be interested in their work (see here and here). These papers give an explicit encoding for encoding braids in a cluster state. UPDATE: I notice you have just added the forth point. The Raussendorf-Harrington-Goyal papers I linked to above do provide a very nice way of doing topological quantum computing via cluster states, which allows classical operations on the knots to be done within the Clifford group, and hence the stabilizer and graph transformation approaches I previously mentioned can be used to efficiently simulate these operations.