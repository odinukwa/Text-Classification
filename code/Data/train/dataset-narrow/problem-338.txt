Your design is valid, but you need to carry the management VLAN1080 as well to all switches. As your network isn't that huge, I'd just tag out every single VLAN to every switch (which is how Cisco's VTP works by default by the way) as you'll only see a limited amount of extra broadcast traffic that way. The reason you can't get RSTP to work is that you have enabled almost every single feature of STP on every port. Disable Root Guard, TNC Guard, BPDU Protect and especially BPDU filter on all ports, as these are meant to be used for very specific purposes (read up on what these do before you enable them on select ports). BPDU filter is likely the problem, as it basically tells the switch to not send BPDU (special packets that's used to determine the STP topology) or process the ones it recieves on that port, essentially disabling STP alltogether. It's basically a way to tell the switch to "what I'm going to do looks stupid, but run with it regardless.) You should also tweak the priority of your root switches to make sure that the spanning tree builds it's topology from the root switches instead of from some random switch with the lowest MAC address. Please read through my guide on STP on HP switches, it's meant for the higher end HP Procurve models, but it goes through some core concepts of STP that are useful for you. $URL$ 

You're not giving us a lot of details like routing tables, but one thing I've seen quite often which may cause this is situations where all network traffic is routed via the VPN connection, isolating the machine from the local network. 

The first IP address in a prefix is usually called the network address. In most setups, the first assignable address is the gateway, though any IP address within a prefix could be configured as gateway. So assuming is in a /24 network, would be the network address and the gateway. 

No, it's the other way around. TCP creates a virtual circuit, it is not using one: IP is connectionless. Doing error corection and flow control would be possible in the application using UDP, but what would be the point? If you require those, it would make more sense to use a protocol which already provides it, like TCP. 

Those two addresses are the same, there's not distinction. Changing the netmask does not make it a different IP address, it just changes which range of addresses is considered to be the local network. Also, browsers do nothing with netmasks, they only know IP addresses and DNS names. You can't enter a netmask in a browser. 

The first command sets the "value" for the port to 33W, maximum under the IEEE 802.03at standard The second command forces the port to forget about LLDP and just give the device connected to the port whatever it chooses to consume, instead of limiting it. The third command is probably not necessary, all it should do is to list the port as "critical", aka. tell the switch to shut something else down if it experiences shortage of available PoE power, but I left it in as Cisco Meraki specifically listed it in their fix. The most amusing thing is that the APs doesn't actually draw any more power after this, at least when they are on standby with no connected clients, but they probably operate better at higher loads, and you get rid of that annoying error in the Meraki dashboard. $URL$ 

If you need to electrically disconnect a port via command line (doing electrically the same thing as physically removing the cable from the port) you'll need a Layer 1 switch, which is a very expensive device (since it can connect any port to any other port). If you only need to stop the port communicating, then you might be able to do it with a cheaper managed switch by issuing interface disable/enable commands. Depending on what you're doing, you could also manufacture a device that does it, for example by putting 8 Mosfets between the pins of two outlets, and then trigger those with the I/O of a Raspberry Pi. This won't satisfy the requirements for Ethernet when it comes to shielding and frequency attenuation, but for test purposes it might work. If you're only using FastEthernet, then you can get away with only doing it on 2 pairs of the cable as well. 

Typically, log filtering is done on syslog servers, not on routers, but if you really want to do this on your router, it's possible. A very simple way to achieve it is to set your log level higher than , but that filters out more than just cron messages, so that probably isn't such a good idea. But there's a nice KB article on how to filter syslog output: KB9382, so probably something like this should work: 

For Juniper devices you can use the command, it shows an overview of all interface counters which is automatically updated: 

There's a fair chance this answer won't be applicable in your situation, but I feel I have to say it anyway: You can implement IPv6 and obtain more than enough addresses so every service can run on its own address on the ports you want them to run on. Of course, this requires every visitor to have a working IPv6 connection so it will probably not be a real option at this point in time, but it certainly is one of the best ways to solve NAT problem. 

Assuming the firewall is a Layer 3 firewall (and not in layer 2 mode like some firewalls are) this won't generate a loop. If it will work, that's a totally different question, that depends on your actual configuration. 

I looked over your configuration, and it seems proper on the pfsense side, that's exactly what I'm doing in my own lab with pfsense, and I have several VMs in production with that type of VLAN trunks. The issue seems to be that you're using host networking for your VM. I'd configure a new port group on the dSwitch with the settings VLAN and VLAN ID 152, and assign the NIC to that. 

Yesterday I encountered something new which I can't really understand. A IE2000 switch doesn't want to accept the enable password, it just gives me "Bad passwords" after three tries. I pulled the configuration through SNMP, and encrypted the enable password string. It's the exact password I tried typing in. Also, the password is the same as the vty line password, which on the other hand works... 

The readings you get from the SFPs are in my experience more a "best guess" at the signal level, and are usually only useful for telling you if you have a signal, especially on the TX side. For something close to a real measurement you'd need an optical power meter. For testing purposes, you could wrap a patch cable tightly around a pencil and wrap it with electrical tape. The sharp bend will cause the cable to leak signal, and as such attenuate the signal. (You can test this with a test laser if you have one, the patch cable will start glowing red if you bend it enough.) Some modern patch cables have really tight minimum bend radiuses, so the cheaper the cable the better. I know it sounds like the dirtiest AD hoc trick you've ever heard of, but I know a lot of fiber technicians who use it in the field to fix issues temporarily when they don't have the right attenuation cable handy. 

The entire internet is built on asymmetric routing, so it's very common. Clients are interested in the interface they receive the packet on and the source of the packet, not which router passed it to them on that interface. Asymmetric routing can get problematic however when devices keeping track of state (especially firewalls) and NAT are involved, but as far as I can tell this isn't the case in your example. 

As I said, there's no such thing. You can get an ASN and IPv6 space, but a large part of the internet still only has IPv4 connectivity, so having your services only available on IPv6 will probably not work as well as you'd like it to. You could possibly consider to have a service like CloudFlare or Akamai (or another CDN) to do the IPv4 hosting and connect to your services via IPv6, but I'm not aware if they do support this, and this probably only works for HTTP(S) services. 

Another way to monitor and prevent them is by implementing storm control, which can help you to detect and stop broadcast, multicast and unknown unicast floods. Both Cisco and Juniper support this. On Cisco, you can use: 

That's a feature called L3 Roaming if I remember correctly. Most enterprise Wifi solutions support it, and it works by tunneling traffic from the WAPs (wireless access points) to a central controller, where they are egressed into the network. That way users are always connected to the same L2 network regardless of which AP they are connected to. Enterprises use this feature to simplify management and improve security (firewall rules are for example easier to maintain if the client IPs always are within a certain subnet). How the solution implements it differs quite a lot. Cisco Meraki for example uses a VPN concentrator VM that terminates VPN tunnels from the WAPs and egresses traffic. As Meraki utilizes a cloud-based controller, it doesn't matter if the WAPs are inside your network or outside it. Some other brands use physical controllers that tunnel traffic through your own L3 network and egresses it traffic from WAPs on a dedicated interface. Do note that this is not a plug for Meraki, it's just the solution I'm most familiar with myself. 

About CWDM: CWDM uses optics to merge and split apart (mux and de-mux) several different wavelengths. This way, you can cram 8 signals into the same pair of fiber, and have them enter and exit anywhere along the whole length of a fiber pair by installing add-drop multiplexers. The modules for CWDM has a narrower bandwidth than normal modules to allow for more signals, and they come in 8 different wavelength versions. CWDM saves you a lot when you need a lot of bandwidth and don't want to buy a lot of dark fiber, but it's quite costly when it comes to the muxers and all the different modules you need to stock as spares. 

The level here indicates the percentage of the total interface bandwidth which can be used for broadcast/multicast/unknown unicast traffic. You can then set an action on what happens when this level is exceeded: 

General rule of thumb: never use documentational space. It has a very specific purpose, don't ever use it for anything else or you'll regret it at a later time. If these two servers only need to talk to eachother and don't require any other connectivity for the given IPv6 addresses I'd use ULA space: pick a somewhat random /64 from within and use that between the servers. 

Loadbalancing on F5 can work this way, if you configured it to do so, but without any configuration details it's impossible to tell if that's the case here. In many cases seeing the actual IP address on the backend nodes is preferred, for example when generating visitor statistics or when implementing IP access lists. 

There are already some useful answers here, but one other reason to insert a default route pointing to the discard interface is to prevent traffic to unreachable destinations (for example traffic for networks which dropped out of the default free zone) from being transported throughout the ISP's network. This can happen if routers within the ISP's network do have a default route pointing to other routers, possibly eachother, which would result into routing loops. 

Have you checked that the APN name is configured correctly (normally it's just "internet" but it can be defined arbitrarily (for examples most mobile providers offer a whole APN for enterprise customers, where the APN name is used to differentiate). Additionally, they can be using authentication, or additional paramters. Here's a screenshot from a Smartflex LTE enabled router for reference. 

About Route Cost and Distance Vectors As xpac said, different routing protocols use different methods to find the best route to a specific network. The idea of a routing protocol is that it allows routers to learn from other routers what networks are attached to which router, instead of you typing everything into every router by hand. A route is basically the "way" to a specific network. As you can have multiple ways to take you to the island of Manhattan from New Jersey for example, you can have multiple routes to a network. A router broadcasts to it's neighbours all the networks it has attached locally. The next router takes this info in, and sees if it has got any of the routes already in memory (called the routing table). If it doesn't, it adds it. If it has, it checks if that route is better than the one it already has, and if it is, it adds it higher up than the old one in the routing table. And when it's done, it sends out all the routes it now knows of onwards to it's neighbours (only exception is that it doesn't send back routes to the router it got them from). This way the network as a whole learns of every network connected to every router in that network, and every possible path to take to get from one network to the other. The crux in the matter is how the router determines which way is the "best" one to take, and this is where Route Costs and Distance Vectors come in to play. Every routing protocol takes a different approach to calculating the route cost or metric of a route. Distance vector protocols, such as RIP, looks at how many hops there are in a route, and calculates a metric from that, since fewer hops means faster processing since fewer routers have to forward the packet. This is quite old-school thinking, and dates from a time when routers were slow and bandwidth quite inconsequential. These days routers are quite fast, and the hop-count not that important. Newer protocols have been developed, and some use another approach called link-state. One such, OSPF, factor in a lot of different metrics, and are a lot more complex (OSPF factors in bandwidth and how it learned of a route for example).