I see you were playing around with join hints. Those are very last resort anyone should do when comes to query optimization. keyword forces a physical operation on logical operation. I would recommend running query in PlanExplorer completely free tool (no contact information required) and looking at index analysis that gets information on statistics. 

which made no sense as it is self-join back to the table. After carefully looking at your plan I realized that you are bringing back data from a view and then joining it back to the base tables that are present in the view. It is a known issue that nested views will always produce estimated row count of 1 no matter how much you update statistics. My recommendation is to rewrite query without use of which just hinders performance. Remember: Views are not for performance, they are for simplicity and easy of access of information. 

An easy way to get job to execute only on primary node is to put simple check for the job to verify which node is at the time job is being executed. It can be done with simple sql statement: 

When I look at messages, the command does not return row every loop but shows up as multiple rows at ones. I have added one line and now it shows up 1 line at the time and the the whole process is much faster. 

so why is it when I have added .5 second wait time to each loop iteration it actually runs faster and allows the print message to come out one at the time? 

but it only shows statistics for the whole file and not sure how to split into individual tables. Trying to do this in order to identify tables that should be places in their own files on different disk. 

Not sure if the question title describes perfectly what I am trying to achieve. I am looking for better way of doing what someone else implemented and I am trying to simplify it, secure it and make it easier to manage. Developer has built a website for displaying information to end customer, part of this information is presented internally through SSRS report. Since the site usually resides on DMZ there is no direct connection to SSRS report, therefore developer wrote CLR procedure that calls SSRS server to generate PDF file of SSRS report that he stored in the table temporarily, then just to return that binary to site which in turns displays that report. This process is extremely complicated to debug and maintain as new reports require new CLR procedures which creates problems when trying to distribute CLR to different clients that are using the product. In conclusion, if I want to present SSRS report outside of the network in a secure manner, what is the best route to go about doing that on a webpage? 

I have SQL Jobs that need to only on primary replica on AlwaysOn Availability Group. When jobs were originally created ServerA was primary, at later point ServerB became primary so jobs that were on ServerA failed and jobs had to be manually recreated on ServerB to run properly. What is the way to have jobs run only on Primary replica of AlwaysOn Availability Group? 

I used the following query that I got from EXCHANGE SPILL, but slightly modified it to convert time from UTC to local. 

This questions came from a developer friend who is working with SQL Azure Database. Scenario: There are two Azure DB servers, one in North America, USA region and another one in South East Asia region. One way replication has been setup for a read-only replica to enable a local client to get data faster. The issue is whenever the client has to write data to the database, they need to write to North America, which incurs large latency issue. As a solution, they want to setup bi-directional replication between two servers to keep them in sync and thus allowing a client to read and write from a server located in their region. I'm more of SQL Developer than DBA, thus, I'm not familiar with different options that could be available for bi-direction in SQL Azure DB. I have told him about SQL Broker and how I saw it used for near real-time replication and with some effort it could be set up for bi-directional messages, though not sure if it can be implemented in SQL Azure DB. 

I'm trying to understand what effect database mirroring can have on replication in SQL Server 2008. Here is current environment: (don't ask me why it was done this way or to change it) Server A: SQL Server 2008 (Publisher) Server B: SQL Server 2008 R2 (Distributor) Server C: SQL Server 2008 (Asynchronous Mirror) Server A is setup for SQL Replication and for Database Mirroring. Mirror is going to DR site and replication is used for different purpose at local site. When database mirroring fails (Server C goes off-line or network issue or anything else) and I can see state on Server A (principal, disconnected). At that time replication failed until mirroring was turned off. My understanding was that High performance (asynchronous) database mirroring, should not prevent transaction from being committed on source. The only time transaction should not be committed to the source is when High Safety (synchronous) mode is on. Which would explain that replication not being able to pick up the changes as nothing is being committed on the source. Is there any scenarios in which High-Performance (asynchronous) database mirroring can break database replication? 

I am looking to eliminate an extraneous data file in a filegroup in my DB but don't want to use the dreaded DBCC SHRINKFILE method, I am preferring to rebuild indexes into an alternate FG, perform the shrink with EMPTYFILE followed with removal of the file and then rebuild back into the original filegroup. Is there a method to identify which table/index objects are populating the utilized pages/extents in the target database file? 

I am using a COTS package to perform index rebuilds across my SQL Server portfolio. The package unfortunately doesn't have an option to set index fill factors globally for an individual instance or DB (I can set them when scheduling rebuilds on an index-by-index basis, but with 1,600+ indexes, I'd like to set them in advance in bulk). Outside of an ALTER INDEX command, is there a way I can redefine existing index fill factors? 

I'm attempting to upgrade a SQL 2005 SP3 clustered instance to SP4. During installation, a rollback event took place with the error indicating there was insufficient space on the the disk. The system volume and application binary volume both have adequate space. On examination of the hotfix logfile, the config.msi target folder for the rollback files was the volume used as a mountpoint folder (in this case, M: was the target but it's only 128 MB in size, hosting only folders to be used as mountpoints for other physical disks, like M:\DATA00). Why would the SP install routine use that volume instead of the system volume or SQL binary volume for the rollback file location and can the target path for the rollback file be altered? 

I'm not planning on creating 2016Q3 and 2016Q4 quarters as I don't want to incur the data movement between the filegroups. I've elected to start with 1/1/2017 and create a 2017Q1 filegroup. I execute the following, anticipating that it'll be a quick meta modification. 

I'm testing creating DBs on an Isilon NAS for potential future use to relocate read-only archive DBs. The NAS admin created a file share with my sysadmin domain group and the SQL Service account having Full Control via a common domain group. My SQL Server 2014 instance creates the DB successfully and I can see DB files in the subdir in the context of the service account, but not under my sysadmin context. I read in a related topic that the service account has Full Control and the file owner is the local Administrators group, so I'm assuming it's a missing attribute assigned the DB files that should allow the domain group (including the sysadmins) to see the files in the directory tree. What file system command should be issued by the service account to allow the file to be visible to other members of the domain group? 

I'm restoring to an alternate server in a DR drill and have restored master but can't restart the instance, even with /f and /m switches. The log is indicating tempdb can't be created, probably looking for the paths from the source instance. How can I determine which path it's trying to create tempdb's DB files in? I was able to restore the master backup under an alternate name on a different instance so I can browse the system tables. Is there a spot where I can look for them? I have to go with the assumption that access to the source server is lost so I can only rely on the backups to determine the correct configuration. 

But right now, it's been running for 90 minutes. I'm watching via Spotlight that the new data file is being populated. I spot-check the UTC dates on all the tables in the DB in Prod and confirm nothing is dated after 1/1/2017. I can understand the engine needing to seek/scan indexes to confirm nothing has to move to the FG, but if no records are qualified to move, why all the data movement? 

I'm trying to figure out the peak load I can sustain on new hardware I procured for validating backups and performing DBCC checks. I've been using Crystal Diskmark to get throughput stats which helped me benchmark sequential I/O for the copy/restore tasks. I'm having trouble gauging how much random I/O I can sustain for the DBCC check. I'm thinking about using iometer and sqliosim but want to know config would work best to simulate a DBCC check. The hardware I'm testing consists of one R720 with dual E5-2609s for 8 cores, 32 GB RAM, Windows 2008 R2 Standard, SQL Server 2008 R2 Standard with SP2, and a PowerVault 3620f with 24 15k SAS spindles hooked up to two dual port HBAs on the R720. I've been experimenting with 4, 8 and 12 spindle RAID 0 groups (I can afford to lose the fault tolerance as the DBs have a life expectancy of minutes as part of the testing process). I'm thinking I can run multiple simultaneous DBCC checks with the above hardware without hitting disk contention. I have the option to upgrade the RAM to 64 GB and the O/S to Enterprise but probably can't upgrade the SQL to Enterprise due to licensing costs. Any suggestions on how to determine the max random I/O for DBCC using iometer, sqliosim or another utility would be deeply appreciated.