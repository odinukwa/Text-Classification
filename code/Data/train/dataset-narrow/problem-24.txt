Huh? What AI stuff? There is no AI. Machine learning has nothing whatsoever to do with AI, it's just the "next" technique. It's not like Matrix-style AIs take server administration jobs from us, or like AIs build our OpenShift servers. 

This may sound harsh, but really is not. It could be as simple as keeping your code strictly within a set of VMs, accessed by something like Vmware Horizon or something like that, with cut&paste and file transfer disabled, of course. Be nice to your developers and allow them to transfer stuff into the environment (their favourite editor, for example). The measures so far do not require any physical changes, i.e. your developers still can work remotely or in their usual unsecure office. Obviously, they still can go to extreme measures (screenshots on their client PC...) to get specific parts of the code page-by-page. To go a step further, in increasing severity (although we are going into James Bond territory here...): 

Sure, it's worthwhile to have every commit on every branch tested, by the default, if you have the resources for that. It lets you know exactly which build broke everything. Coupled with very small commits, this is a quite powerful tool. Another useful trigger is to not build every commit, but have Jenkins look at the repository every 15 minutes or so, and build if there is one more more new commits. This may be more realistic when your test suites take longer to run, and if you do not have unlimited amounts of test environments that you can spin up and down regularly. (By the way, if you are completely new to all of this, including "agile" and branching, then maybe it would be good for you to pick up an existing branching strategy, for example Gitflow, so you don't complicate things too much by (re-)inventing too much stuff at the same time.) 

Sure... a virtualbox VM is simply a XML file. You can copy/modify it as such easily. Use to clone the hdd images as necessary. So, I don't think it would be too far fetched to just create a template VM in virtualbox once, and then use a little script to do the rest. This way, you have complete control over everything. As you are using Ansible to provision the fresh VM anyway, you don't lose that much by ditching Vagrant. 

Note: this answer shows how to find branches that have been *updated* a long time ago, not branches that have been *created* (i.e., spliced off of some parent) a long time ago. I believe this is what the OP actually wanted (as opposed to the title he chose for the question), as this would lead to good candidates for deletion. You have to recall that a branch in git is nothing "physical". It is, by definition, and literally, only a simple text file in which only has its file name (which is the branch name) and the hash of the tip/head commit. It has no date information or anything else. So will not help you here. The object type that has dates associated with it are commits, in git. So you have to list commits. This is done with . So a working command would be: 

Frankly, if you go this route, you do not even need Ansible as a CI/CD driver. Ansible does not bring any infrastructure anyways, it just uses an existing ssh connection, so you can just use said ssh connection directly with your own scripts. If the ultimate goal is to avoid any of the established solutions (Jenkins, Gitlab CI, whatever), then nothing really keeps you from rolling your own: 

I have a Dockerfile which builds a ruby environment. Everything works fine and as expected, except that at a certain point, Docker stops using cached intermediate images. If you look at my excerpt, then you notice that up to the part I marked with , Docker finds all cached images. The next COPY step (which, if you look closely, does not copy external files, but from the previously cached "FROM base as builder" image. Why is it not cached? 

Someone said that to each question, there are always three answers. So here's your third answer. Install docker on the dev machine, and build images with it. But: The images should not contain your source code or anything that changes during the usual edit-build-test-run cycle. The images only need to contain the basic stuff that does the build/test/run parts. That is, if you were to develop a Maven application, your image/container would contain the command with all its depdendencies. A second image/container would contain your or whatever you are using as an application server. If you were to develop in ruby on rails, you would have one image/container which contains everything to be able to run , another image/container which contains everything to be able to run or . The images will be recreated seldomly, for example if you want to upgrade the application server, and they will certainly not be 't on each run by the dev. Both images would not contain anything that belongs to your application. No source code, no file, no precompiled assets or application-specific gems for the ruby application. These would be mounted as volume when starting the containers, and point directly to whatever local directory the dev is currently working with. This way, it is guaranteed that your dev uses the exact same base environment as your CI or production. New devs can start building/testing/running instantly and never need to install anything to their host machine. They can use the editor of their choice (if they wish to use something like Eclipse which has lots of magic and wants to compile the stuff itself, then they are free to do so of course - as long as they do at least a cursory check whether everything still works in the docker variant, before pushing). And if you wish to deploy your application as a fully self-contained docker image, finally, then you are free to do so. Your CI server can build those images. It should use the smaller images used by your devs as base image, obviously. This gives you a nice medium ground which should cover everything. 

In short, I see two categories of tests for your infrastructure: 1) does it have everything you need to run your application and 2) does it not have any superfluous stuff. First and foremost, you can treat the test suite of your actual software as a kind of "meta test" for your infrastructure. As long as you create the infrastructure from scratch for each test run, and the test suite runs completely on that infrastructure (i.e., doesn't use outside services) the fact that the whole suite is green means that your codified infrastructure is sufficient as well. Second, especially from the security perspective, you can write tests against your infrastructure. I.e., if one part of your infrastructure is a VM running Linux, you might write a test that does a port scan against that VM, to make sure that there are no unintentional ports open, which may have been installed by an unintended side-effect. Or you could write tests that checks whether any unexpected files were changed after your proper test suite has completed. Or you could check the outputs of your VMs or Docker containers for unexpected processes and such, build white-lists etc., and thus get automatic notification if some 3rd party package changed in an undocumented (or unnoticed way) in some upgrade. These second type of tests are, in a way, similar to what you would do in a classical ops setting anyway, i.e., hardening your servers and checking for intrusions, avoiding full ressource and such. 

Yes, of course. For non-life-threatening risks (say, your common bug which just induces some cost to fix it): take the chance of the problem happening, multiply by cost, and there you go. Easily done by going through your bug ticket system, where you hopefully have recorded the effort gone into fixing. Then there are the life-threatening risks where you do not actually need to factor in the cost, but there is no question that you do your utmost to avoid it. For example, if you are a small company with one product, and you lose your source code, then it is not a question about how much that costs. You can just pack in. You will want to have some sort of backup strategy that is, by all means and purposes, unbeatable (for example: the boss takes a current set of physical backup tapes back home each weekend => and yes, I have worked in companies where that was done (in addition to all the usual other measures, like snapshotting file systems, distributed git repositories, on-site backups, off-site online backups etc.)). Nothing about this is particularly related to DevOps. DevOps just happens to bring a few tools to the table (like CI/CD pipelines, relatively easy HA through read-only redundant containers/pods etc.). 

Learn how to actually run those containers; how works, how to the app together with the DB and the Apache (in case you made a separate container for that - you can skip this obviously if your app is able to serve all stuff including static pages itself). Figure out how to mount/bind , how to debug a container by opening a shell inside it and so on. is nice if you have more than one, and pretty simple to get going with. Now you probably have a working setup. That's great! You have learned the basics, and you immediately feel the great power that comes from containerizing your app. You can deploy it instantly on any server, you can give it to your developer colleagues and go up and running in zero time, and so on. Enjoy that feeling for a while. Make the image smaller. There are a few obvious things you can do (google...), and a few not so obvious ones (thinking is like googling, just better... :) ). If you need help on that, post more questions here. Get Jenkins (feel free to do so with docker, again, if you're adventurous) and make a job to build the image from the Dockerfile and run your test suite directly in the container you built. Jenkins can check your repository for updates automatically, you don't need to bother with hooks at this moment. You're done! This was the CI part. For CD, after the tests are green in Jenkins, simply tag the build and send it on to your deployment server.