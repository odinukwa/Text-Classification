I think you will need to use some looping to do this as you have to define a group of records when the dept changes. I have written this in TSQL so you may need to make some syntax changes but it works. 

You could have separate tables for product type and product catalogue to get to 3NF but that's a decision for you to make depending on the amount of different values those columns could have and the way in which this data will be accessed (OLTP only or heavy reporting used here too?). 

running this will normally map SQL user SPIDs but you will need to make the DB read/write for a while, preferably in single user mode. 

I had to build something like this recently and I eneded up using an AlwaysOn Availability Group to create read-only replicas on the databases that I needed. If Enterprise is not an option then log shipping can do the job but it will put your datawarehouse further behind and your ETL would need to be able to deal with the restores to the log shipped replicas. From these replicas I had a custom ETL process that made heavy use of statements that ran every 5 minutes as a SQL Agent job to pump data new data into the datawarehouse. You could have this run at smaller intervals depending on the amount of data you have to process. If your records are datetime stamped then the only thing that should slow this process down over the months and years would be a big increase in transactions. All ETLs will suffer like this in some way if the transactions increase but there are many improvements that can made to scale-out the deployment. 

Well, the reason you don't see everything is quite easy, the buffer cache gets cleared when new resources are needed. You don't have unlimited memory available, so no unlimited cache to hold the plans. That's why you probably are seeing only a top level of queries... 

If your table is a heap, you will get a lot of key lookups, which are performance wise not good. If you have so much records in the database, you also might wanna take a look at partitioning your data. Spread it over multiple files and over multiple disks/raid configurations will speed up things very much! 

Recently we had an issue with a sql server 2008 r2 HA mirror. I have tried to reproduce this in a lab environment and i came to the conclusion that i am missing something OR something is not possible. I am using 2 sql server 2008 r2 servers. Both the servers have a iscsi storage connection to a storage server. Mirroring is done with a witness server in HS mode. i have followed a blog post by Glenn Berry about how to create a group failover, he told me on twitter it would also be usable in case of failing storage. However, somewhere i can't really get things to work. What am i doing. I have a sql-01 as princiapl and a sql-02 as mirror. 2 databases running in HA mirror mode. I have a failover script running on WMI 7 and 8. When i go to my storage server to disable the storage of sql-01 i would suspect that eventually i get a failover to sql-02. but it doesn't... Anyone can give me some hints on this? Edit : Failover with a shutdown-ed storage for sql-01 looks working, because of a tempdb that is getting corrupt which needs recovery (according to logs). Therefor the instance has to shutdown (also in the logs), so probably that is my reason i can failover from sql-01 to sql-02, by accident and apparently not by design. 

We are using Postgres 9.3 and making use of the JSON column which was new to that version. Version 9.4 added a LOT of utility functions for handling JSON, as well as the new JSONB column. You can see this by comparing $URL$ with $URL$ 9.4 has a function, which returns the type of a JSON object (as a string: 'string', 'number', 'array', 'object', etc.) I would like to know if there is any practical way in 9.3 of retrieving all rows which are scalars (or, equally useful for us, which are not objects). We would like the column of a given table to always be a JSON object. This means that we can do queries like: 

should catch most cases. However, unlike the column, columns store JSON in its original form, not in a canonical form. So this method doesn't exclude the possibility that there will be some weird JSON, for example starting with another space character, which won't be matched. 

The first thing to try is experimenting with sampling instead of performing a full scan every time. If lower samples are causing poor plans or still taking too long you could consider partitioning and if you are on SQL Server 2014 or higher you could use Incremental Statistics. Incremental Statistics allow you to rebuild statistics at the partition level. There is still just one statistics object for the whole table but the statistics for the partition are merged into the histogram. If you're not running on Enterprise edition or you are on a version prior to 2014 you might consider using partitioned views. This way you can partition your data into completely separate tables with their own statistics objects. Partitioning and partitioned views bring their own sets of complications but are usually required for very large tables. You may be suffering from the ascending key problem, see here. If you are then one last option could be to update statistics less often and use the trace flags 2389 and 2390 to improve cardinality estimates in the time between updates. 

The only situation where I would not do it this way, was if 1 to 8 is something which could never ever change. For example, you might be sure that there will only ever by 7 days of the week (but who knows?). In this case I would create a TYPE with this constraint. Then you can use the type in multiple tables. If the days of the week (or whatever) DOES change, you can change the TYPE. 

This is a pain, as we have had strings written to this column some of the time, mainly due to errors. Is there any reasonable way of either filtering these out in a query, or identifying them all so that they can be removed in one go? 

@CoderAbsolute's answer gives you a good design for your tables. Since he or she did not go into detail about why this approach is better, I thought it was worth adding another answer. First of all, design your table structure in accordance with how your data fits together. Don't try to smoosh different types of things into one table, don't add several tables for the same kind of records. Try to 'normalize' - if a table will have the same info repeated many times, then move that info into a different table, and link it from the first table using a foreign key. You should be aware of what first normal form, second normal form and third normal form are. Many real-world databases do not match these standards completely, but being aware of what you should aim for will help you make a much cleaner design. I would say, don't worry about optimization until you've already got a correct design. First of all, you don't yet know how many entries you will have in your tables. Secondly, database engines are designed to make queries as fast as possible, even if you have a lot of entries. Don't second guess the developers of your database software. When you've figured out that you really do have a bottleneck, you should look first at indexing. We think of a database table as being something like an array. In reality, it could be stored as a bunch of 'blobs', one for each row, which might all have different locations on a disk, not necessarily in order. (This is not a super accurate technical description of DB internals. But it should give you a clearer picture of how the pieces fit together.) How does the database find all the blobs? It has a somewhere, a list of 'pointers' which tell it where to find each blob. So typically, finding every single row of a table is an efficient process. It goes through the list and records each one. Now, suppose that you most commonly retrieve all the photos for a given user. This might be a slow process, since it has to go through every single row of the table, and look at the field. In this case, you should add an Index on that field. An index is something like a lookup table, which allows the software to quickly find the location of all the rows with a given . So it doesn't have to check every row in turn. There are different kinds of indexes. Some are optimized for matching a particular value. This is probably the type you want for the column. Some are optimized for finding things greater or smaller than some value. This might make sense for timestamps. (Give me all the photos from the last month.) You can have an index on multiple columns if this is what you regularly query on, or even on some function of one or more columns. Some indexes mean that similar items are stored close to each other on disk, to take advantage of caching to retrieve them more quickly. You should familiarize yourself with the possibilities for your DBMS. Experimentation can also be very valuable here, since the exact speedups depend on your settings and also your hardware configuration. 

SET SINGLE_USER means only one user can connect to the database at a time. If someone or some service (SSIS, SSRS, SQL Agent, etc) connects before you than you will have to wait for them to disconnect 

At a guess, this sounds like the application is processing the data row by row. The fact that the slow processing is intermittent I would say that it's very unlikely to be the network. 

You can have 3 grids all with the same data source and a filter on the year and month. This would be a static filter though so you would be better off having 1 grid that step grouped on year and month. This link shows you everything you need to know in order to do it in one grid. 

I have 4 servers: A, B, C and D. A and B are in a master-master setup. C is a slave to B and D is a slave to C. Changes on A or B get to all nodes other than D. I need them to get to D Changes on C get to D but obviously they don't go to A or B which is fine. After some reading I found that I needed the log-slave-updates option set on C, this way C would log the changes replicated to it and then D could read those and make the same updates. I have set the option to on and D is waiting for it's master to send events but changes from A or B are still not getting to D. 

At my current company where i am hired i always use the minimum and maximum settings for SQL Server. The maximum, if the server is only being used for SQL Server i always set on MAX memory - 1gb for OS. The minimum in our case at 2GB's. But that is proven for our installations. Not every installation and usage are the same, and i still feel that memory is always not doing what you want ;) Might be a known thing in your case, but also look at the swapping of memory. When you only have 2GB on the server and you have the OS and SQL Server running on a "high load", the memory consumption will get too high where the OS will tend to do some swapping. Make sure that your paging file(s) are not on the OS drive, or else you might end up with a totally jammed system (been there before and didn't liked it) 

@wBob got it in one. It's all about those NULLs. The following script creates the Destination table with a modified column and it alters the same column on the TableName_Partition2 to be after creation. This allows the to run. 

When it comes to SQL Server index maintenance you cant beat this maintenance script. $URL$ I wouldn't use the profiler to decide anything like this. If you want to write your own simple maintenance plan you need to look at the index DMVs. 

If you want to remove files pragmatically you could use the extended stored procedure but it is not generally recommended to use this as these types of procedure are scheduled to be removed in a future version of SQL Server although I'm not sure when that will be. If this is something that could possibly be in place for a long time after deployment I would recommend you use a as this functionality is built in. 

If for some row the JSON in the comment column is an object, but which does not have any such key, you simply get back a NULL. However, if there is even one scalar in the table, the query fails with the error 

I would certainly recommend B in most cases. Both A and C leave open lots of room for inconsistency. Enforcing that exactly one of two columns is (for case A) is quite a pain. If you need uniqueness, enforcing that one column is either a unique value, or , and the other one is also either a unique value, or , with exactly one of them being , is a huge pain. C also misses some consistency checks as you recognized. I don't exactly understand what the information is that you are trying to represent. But it usually makes sense to me that if you are trying to make one table have references to one of two different other tables, then the information in the first table actually consists of two types of different thing. It should also be considered that it's much easier and more readable to do a , than to have lots of queries of the form: 

If it's a new deployment then why not upgrade to the latest version of the software? 2014 has many benefits over 2008 and 2012. The install is simple if you have configured windows clusters before. I would recommend windows 2012R2 as clustering is a little easier with features like network teaming and the automatic quorum level settings. One of the hardest things to get right can be the LUNs that you configure on the SAN especially if you're using SAN level snapshotting. 

Powershell is your friend here. When working with cmd commands in Powershell you can use the variable to read the result of the command you executed. The code below passes a command into the Invoke-Expression cmdlet and captures it's output. 

Essentially you have NULL values in salesCategory column. Having NULL values in fact tables is generally bad because of the problem your are facing. The best thing to do is correct your data so that these values are present as this will provide the most accurate results. If you can't remove/correct the NULL values then you are limited to filtering the rows with NULLs but this will skew your aggregates or using the 'UnknownMember' to deal with NULLs. This will mark rows with a NULL Salescategory values as 'Unknown'. This way your aggregates are correct and all the data is visible. If you are using SSAS then you can set 'UnknownMember' in the your dimension -> properties -> KeyColumns -> NullProcessing.