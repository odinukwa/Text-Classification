For first question, there are external commercial solutions which monitor links and optimize your BGP, benefit of them is that they are mostly vendor neutral, as long as you do BGP, they should work. I cannot recommend any, as I've not used such solution personally. I hope someone else will give rundown of what is available in the market and which is recommendable product. Then there is Cisco's PfR (was colled OER before), which will use link which most closely satisfies your performance requirements. It is quite nice, while completely proprietary and not available throughout their routing portfolio. For encryption question, I'm not really sure what to tell, I think IPSEC links we have there to connect some remote factories to customer's L3 MPLS VPN just work, without GFW interfering. It might be useful to include in your post rough pps/bps limit what need to be supported, what equipment you are today using and how much CAPEX you have to spend on the new solution. 

I'm not familiar with Netscaler, so take this with grain of salt. I presume you are using NAT to translate source address of request today, so actual server is not seeing today real request address but some single NAT address translated by Netscaler. I also presume that his toggle would remove that source address NAT, allowing your servers to see real request address. There are some scenarios where one would want to NAT the request address, such as if for redundancy purposes you want to connect your server to two or more SLB devices simultaneously (who would ECMP the VIP, i.e. both SLB are hot:hot, but not connected to each other in anyway). Then when your server replies to a request, it needs to reply via same interface where the original query came in from. On Linux machines you can fix this easily, without relying on natting the source at SLB. On Windows server, you wouldn't really have any way to return the request back to the SLB which sent it, unless you NAT each request. If SLB1 NATs source to 192.0.2.1 and SLB2 NATs source to 192.0.2.2, then it's just matter of setting static route in windows, 192.0.2.1/32 pointing to SLB1 and 192.0.2.2/32 pointing to SLB2. This way you are always returning packet back to the SLB which sent the request. 

Ok 16 is enough, even if we remove 2 hosts, we are left with 14, which satisfies requirement for 12. And 16 is 5th number we found, so it is 5th smallest network. Then we just count backward from smallest possible network, i.e. /32, and we get 

A will PUSH (impose) label 300 and send towards B B will consult FIB for 300, which is Interface D and SWAP 200 D will consult FIB for 200, which is Interface E and POP (or SWAP 0) E will receive frame 

What you want to have is the VoIP traffic is always inside the 'first' 100M, and data can go outside it. So that when far-end policer/shaper needs to drop, they'll just drop the traffic after the 'first' 100M. This is not possible at all. Once the provider policer/shaper starts dropping due to you exceeding 100M for long time, if they don't share your view of QoS policy, they'll drop indiscriminately your voice and data. So config you have right now, is best you can do. 

FEC or 'Forwarding equivalence class' identifies identity for packet which will determine path it will take in the network. For pure IPv4 forwarding, with routing-table 10.0.0.0/24 -> 192.0.2.1 you can think that addresses 10.0.0.0 - 10.0.0.255 share that FEC, as they are treated same way, each of them are going to go to 192.0.2.1. In MPLS typically FEC is the same, some prefix gets specific label, and these are all sharing FEC. However it's completely valid to add more differentiation there, QoS (dscp, cos) could be another differentiator. So 10.0.0.0/24, CoS 1, could get label 42 and 10.0.0.0/24, CoS 0 could get label 100. In IPv4 FEC is determined hop-by-hop, first hop could have 10.0.0.0/24 next-hop could have 10.0.0.0/25 and 10.0.0.128/25, causing traffic that used be in one FEC, to diverge into two FECs. In MPLS FEC is only determined by ingress PE, the traffic will then share same FEC all the way to egress PE. So FEC is way to describe shared path in network for some group of packets, it does not strictly define how that group is defined. 

To extend my comment, these switches don't have any pollable environment sensors. Only WS-C3524-PWR-XL-EN model has pollable fan sensor. They've been completely out of support since 2007 and you'll never get newer software than WC17 which you are running. We are having difficulties finding people who'll accept them free from us, so we usually just trash them. If I'd have XL switches and no budget I'd upgrade them to 3550, which is also EOL, but much superior switch and can be acquired for 50USD/pcs. 3550 runs modern IOS, 12.2(55)SE8 but technically is out of SW maintenance. With 3550 you'd at least get MST, QinQ, L2PT, SSH, proper storm-control, IP routing, QoS, ACLs, SPAN. 

If you see MAC address is 'mac address-table' you have L2 connectivity. So nothing wrong in your test methodology. 

This platform specific limit, so it's just arbitrary limit Cisco came up with at one point in time, probably driven by memory restrictions of device of that era. 

E will allocate either explicit (0) or implicit (default) for 192.0.2.5/32 E will distribute the prefix+label (FEC) to C and D, using LDP C will allocate local label for this, say 100 (could be anything) 

Now you'd get trap from all other interfaces going down, except for 'NoCDP' interface. However I think you actually don't care about interfaces going up/down, you care about CDP neighbors disappearing, for that you'd need to use EEM. EEM can match CDP event and can do plethora of actions based on event, such as syslog. As a starting point, you could use this script 

You're right UDP header is 8 bytes. So I can't really explain your 12+8 scenario. Pseudoheader refers to the header which is considered when calculating checksum, is is combination of IP + UDP + payload, but not all of if, hence pseudo. 20B is normal size for IPV4 (and TCP). So maybe confusion lies there? If you can put your example online, it might help understanding what the author tries to communicate. EDIT the packet you entered is 20+8+13 bytes. If your document claims 20+13, it is missing the UDP header.