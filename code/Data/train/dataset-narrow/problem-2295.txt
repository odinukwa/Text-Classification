Several state of the art approximate nearest neighbor methods in high dimensions are based on reducing the dimension of the space through randomized techniques. The main idea is that you can exploit concentration of measure to greatly reduce the dimension of the space while preserving distances up to tolerance $\epsilon$. In particular, following from the Johnson-Lindenstrauss lemma (upper bound) and a result of Noga Alon (lower bound), there exists a subspace of reduced dimension $$C_1 \frac{\log(N)}{\epsilon^2 \log(1/\epsilon)} \le \text{reduced dimension} \le C_2\frac{\log(N)}{\epsilon^2}$$ such that for any two points $u,v$ in your collection, their projections $\tilde{u}, \tilde{v}$ onto the reduced dimensional space satisfy $$(1-\epsilon)||u-v||^2 \le ||\tilde{u} - \tilde{v}||^2 \le (1+\epsilon)||u-v||^2.$$ Indeed, such subspaces are "typical" in a sense, and you can find such a subspace with high probability by simply projecting your points onto a completely random hyperplane (and in the unlikely event that such a hyperplane is not good enough, just pick another random one and project again). Now you have a nearest neighbor problem on a much much lower dimensional space. A key paper is, 

Concerning technical writing: Check spell your paper, review your notations (even though an index of notation cannot be included in a conference submission in general, doing it early helps for later, longer versions, and for the clarity of the early versions), have someone else than those involved in the writing to read it and check it is understandable. Concerning content: Justify all your choices. When you have several related results, find what they have in common, describe it as a question, and explore alternative answers to this questions that you did not consider before, eventually adding new (minor?) results or justification for discarding alternate answer. When you have a single result, a single answer to one specific problem, describe what are the other possible answers to this problem, in the literature or based on futuristic results. Do not limit your paper to "I did this, and look, it works!". Explain why it works and why or how other choices would work worse or would not work. 

Since the Hilbert Curve iterations converge uniformly, that uniform limit is a continuous path that solves the maze in the analytic sense. It's as if you were able to do the following recursively defined infinite set of moves $P$: $P = APA^{-1}BPB^{-1}CPC^{-1}DPD^{-1}$ Now you might argue that this is not in the spirit of fractal mazes since the Hilbert curve fills the entire square and therefore you could just draw a straight line segment from the start to the finish. This objection is easily overridden though - simply use the hilbert curve diagram embedding directly, as shown here: 

If a solution exists, breadth-first-search should find a solution. However, suppose there is no solution to the maze - then our search program would run forever going deeper and deeper. My question is: given a fractal maze, how can we determine if it has a solution or not? Or alternatively, for a fractal maze of a given size (number of inputs/outputs per copy), is there a bound on the length of the shortest solution? (if there was such a bound, we could exaustively search only that deep) 

For your first question, I think you can show that w.h.p. $X_{\max}-X_{\textrm{sec-max}}$ is $$o\left(\sqrt{\frac{m}{n}\frac{\log^2\log n}{\log n}}\right).$$ Note that this is $o(\sqrt{m/n})$. Compare your random experiment to the following alternative: Let $X_1$ be the maximum load of any of the first $n/2$ buckets. Let $X_2$ be the maximum load of any of the last $n/2$ buckets. On consideration, $|X_1-X_2|$ is an upper bound on $X_{\max}-X_{\mathrm{sec-max}}$. Also, with probability at least one half, $|X_1-X_2| = X_{\max}-X_{\mathrm{sec-max}}$. So, speaking roughly, $X_{\max}-X_{\mathrm{sec-max}}$ is distributed similarly to $|X_1-X_2|$. To study $|X_1-X_2|$, note that with high probability $m/2\pm O(\sqrt m)$ balls are thrown into the first $n/2$ bins, and likewise for the last $n/2$ bins. So $X_1$ and $X_2$ are each distributed essentially like the maximum load when throwing $m' = m/2\pm o(m)$ balls into $n' = n/2$ bins. This distribution is well-studied and, luckily for this argument, is tightly concentrated around its mean. For example, if $m' \gg n\log^3 n$, then with high probability $X_1$ differs from its expectation by at most the quantity displayed at the top of this answer [Thm. 1]. (Note: this upper bound is, I think, loose, given Yuri's answer.) Thus, with high probability $X_1$ and $X_2$ also differ by at most this much, and so $X_{\max}$ and $X_{\mathrm{max-sec}}$ differ by at most this much. Conversely, for a (somewhat weaker) lower bound, if, for any $t$, say, $\Pr[|X_1-X_2| \ge t] \ge 3/4$, then $\Pr[X_{\max}-X_{\textrm{sec-max}} \ge t]$ is at least $$\Pr\big[|X_1-X_2| \ge t ~\wedge~ X_{\max}-X_{\textrm{sec-max}} = |X_1-X_2|\big]$$ which (by the naive union bound) is at least $1 - (1/4) - (1/2) = 1/4.$ I think this should give you (for example) the expectation of $X_{\max}-X_{\textrm{sec-max}}$ within a contant factor. 

It took a few years (five!), but here is a partial answer to the question: $URL$ Optimal Prefix Free Codes With Partial Sorting Jérémy Barbay (Submitted on 29 Jan 2016) We describe an algorithm computing an optimal prefix free code for n unsorted positive weights in time within O(n(1+lgα))⊆O(nlgn), where the alternation α∈[1..n−1] measures the amount of sorting required by the computation. This asymptotical complexity is within a constant factor of the optimal in the algebraic decision tree computational model, in the worst case over all instances of size n and alternation α. Such results refine the state of the art complexity of Θ(nlgn) in the worst case over instances of size n in the same computational model, a landmark in compression and coding since 1952, by the mere combination of van Leeuwen's algorithm to compute optimal prefix free codes from sorted weights (known since 1976), with Deferred Data Structures to partially sort a multiset depending on the queries on it (known since 1988). 

A fractal maze is a maze which contains copies of itself. Eg, the following one by Mark J. P. Wolf from this article: 

This too contains a sequence of uniformly convergent continuous paths going from the start to the finish, by the same argument used to show the uniform convergence of the Hilbert curve. However it is a true "fractal maze" in the sense that it does not fill the whole space. Thus we have a fractal maze that is solvable by the analytic definition, but unsolvable by the graph theoretic definition..!? Anyways, I'm pretty sure my logic is correct, but it seems counterintuitive so if anyone can shed some light on this I would appreciate it. 

Given the spanning tree structure of the solution of the all pair minimum path and the fact that the weights are always decreased, I think that this gives linear time per update. I think you could easily get the same result for directed weighted graphs, at the cost of some extra space and a more complicated structure. 

Are there applications of Voronoi diagrams or Delaunay triangulations where the order in which the points are generated (and given to the algorithm) have some known properties (e.g. concatenation of points belonging to distinct entities, such as when a data set is the union of several pre-existing data sets)? More specifically: 

This implies that (for $k$ not too large) greedy gives an $O(1)$-approximation with probability $1-\delta$, for any constant $\delta>0$. Here is the upper bound, followed by the lower bound. Upper bound Conditioning on the event that there exists a set cover, the greedy set-cover algorithm returns a set cover with expected size $O(p^{-1}\log n)$. Proof of upper bound. Condition on the event that a set cover exists. Assume without loss of generality that $k\ge (12/p)\ln(n)$ (otherwise, since greedy chooses at most $k$ sets, we are done). We prove that there is a fractional set cover $X$ of expected size at most $$\frac{2}{p} + n\exp(-pk/12).$$ By the assumption on $k$ this is $$O(1/p).$$ The upper bound will follow, because (as is well-known) greedy returns a cover of size $O(\log n)$ times the size of any fractional set cover (and so, here, $O(p^{-1}\log n)$) (see here or Chvatal). Define the fractional cover $X$ in two stages. 

You might use only one bit, if $n$ is the only integer you will ever encode, and you need only to remember that you encoded it. The common practical solution is to use 8 bits, if the only integers you will ever encode are all between 1 and 256 (generalize to 16, 32 and 64 bits if you want). If you do not have any hypothesis about the range in which falls the integer you will have to encode, a naive solution is to use $n+1$ bits ($n$ zeros followed by a one) to encode it in unary. This might not look yet as a compression, but it has the opportunistic aspect of compression: the smaller the value of $n$, the smaller the size of its unary encoding. A more serious, general purpose, encoding scheme of integers is the gamma code: encode the value of $\lceil\log_2 n\rceil$ in unary using $\lceil\log_2 n\rceil+1$ bits, followed by $n$ in binary, using $\lceil\log_2 n\rceil-1$ (you do not need the leftmost bit, which is always one, since you already know the value of $\lceil\log_2 n\rceil$). This encoding uses in total $2\lceil\log_2 n\rceil-1$ bits, and is a useful compression of $n$, often use in practice. (Note that in the litterature you will find those results noted $\lg n=\max(1,\lceil\log_2 n\rceil)$ to make notations shorter.) The gamma code is not optimal, in the sense that there are other codes which use less space for arbitrarily many integers, and more for only a finite amount. A very good reading on the topic is "An almost optimal algorithm for unbounded searching" by Jon Louis Bentley and Andrew Chi-Chih Yao from 1976 (I like in particularly their link between the complexity of search algorithms and the size of integer encodings: I find it one of the most simple and beautiful TCS results I know). The bottom line is that $2\lceil\log_2 n\rceil-1$ bits is within a factor of two of the optimal, which most agree is enough in practice given the complexity of better solutions. Yet, taking the "opportunistic" approach to its limit, there is an infinite number of compression schemes taking advantage of various hypotheses. One way to deal with this infinity of opportunistic encodings (i.e. compression scheme) is to require the encoding of the hypothesis itself, and to take into account the size of the encoding of the hypothesis in the total compression size. Formally, this corresponds to encode both the compressed data and the decoder, or more generally to encode a program which, when executed, outputs the uncompressed object: the smallest size of such a program is called the Kolmogorov's complexity $K$. This is a very theoretical construct in the sense that, without a bound on the execution time of the program, $K$ is not computable. An easy workaround around this notion is given by Levin's self-delimiting programs, where you consider only programs with a bounded execution time (for instance, within a constant factor of the length of the original instance, which is a lower bound on the complexity of the algorithm which needs to write each symbol). 

This is not an "answer" to my question, but rather an extended comment that people here might find interesting. I claim that there is a natural "analysis-type" definition of a maze and a solution, and it differs from the computer-science/graph-theoretic definition we've used here. In particular, you can have a fractal maze that has a "solution" under the analysis definition, but would be declared unsolvable by Marizio De Biasi's algorithm and Peter Shor's pushdown automata technique. Definition: A maze $M$ is a compact subset of the plane $M \subset \mathbb{R}^2$ containing a start point and an endpoint $s,e \in M$, respectively. A solution is a continuous function $f:[0,T] \rightarrow M$ such that $f(0)=s$ and $f(T)=e$. Now consider the the Hilbert Curve: 

Am I missing a key difference between those approaches or is it a mere occurrence of weak communication between the database and TCS communities? 

What bounds (lower or upper) are known about the complexity of partitioning a Directly Acyclic Graph (DAG) into paths of respective sizes $n_1,\ldots,n_w$, such that to minimize their entropy $n{\cal H}(n_1,\ldots,n_w)= n\lg n - \sum_{i=1}^w n_i \lg n_i$? Motivation 

Have a look at my (modest) proposal of "fun" problem below. If you work on it, get in touch with me! 

Element distinctness can be solved deterministically in the RAM model in time within $O(n\log\log n)\subset o(n\log n)$ time: Sort in time within $O(n\log\log n)$ your $n$ numbers of $w$ bits using the sorting algorithm described by Han in STOC 2002 ("Deterministic sorting in $O(n\log\log n)$ time and linear space"), then scan in linear time for collisions. As far as I know, that is the best result known to this day.