This is because I think it is more efficient to get X, Y, Zth records in one query execution than in multiple ones. 

After each random post is retrieved, save that post id to a table called "seenPosts" where there is also a column called idOfApp which is used to distinguish users. 

I could store the ID's of already seen posts in an array IN the app and then send this array to the getRandPosts function. The getRandPosts function then uses a "NOT IN" clause. 

I know by using Like with "LIMIT 1, OFFSET x" I can get the Xth record from a returned query result. But what if I want to get X, Y, and Z all at once? Currently I do a for loop and it's like I'm doing this: 

Is it possible to delegate responsibility for specific indexes for a collection to specific shards in a mongoDB database? For instance, say I have a collection called 'books': 

I'd also like to ask about the idea of, for idea number one, creating a temporary table from the array and using JOIN with that temp table vs using the array. I'm avoiding using RAND() because it has been said it doesn't work for large tables. I wonder how large is large? Consider the idea that, at most, 600 posts will have to be excluded for any given user (so an array of 600 ids in the case of the first idea $idsOfPostsAlreadySeen... or 600 records in the seenPosts for a particular person). 

I have an api endpoint that returns X amount of random posts from a table called "posts". This endpoint is used by a mobile app. It retreives random posts by doing a SELECT COUNT(*) on the posts table and returning the amount of posts in the table. It then enters a for loop in which, at the start of each loop, a random number from 0 to the COUNT(*) is generated. A random post is then obtained using the handy OFFSET. This for loop goes until X amount of random posts obtained. pseudocode: 

Now, for each call to a getRandPosts function I want them to always retrieve a unique post that wasn't retrieved before. For this current getRandPosts call AND FOR PASTS CALLS. I've thought of several ways of going about doing this: 

And say I want 3 indexes for this collection: one for the price, one for pages_in_book, and one for author. Also say I have 3 shards. Can I have each of these shards be responsible for updating one index each when a new document is inserted into the 'books' collection? I want to do this because I plan to make a write-heavy application. User actions would initiate the creation of multiple documents at once and each document would need to be added to multiple indices like above (but five or six indices in my real case). I could distribute the writes using a shard key (with luck here, the documents' writes and it's writes to the collection's indices would be distributed evenly to the different shards but I could get unlucky if no good shard keys) or distribute the writes so that each shard is assigned an equal amount of indexes to be responsible for (extremely more likely for there to be a near even distribution of writes this way???) 

I would go for the check constraints, you cannot be positive that records will never get manually entered or adjusted. Data integrity trumps avoiding nulls every single time. 

THe following book talks about using ID as a SQL antipattern and I gree with the author that it is. $URL$ This is a particular problem when you are doing complex reporting and need more than one id as you then have to alias. Using tablename ID also makes it easier to identify the correct FK to join to (as they have the same name) and makes errors from joining to the wrong thing less likely. That said, many databases don't support the USING syntax which makes the problem you brought up not an issue for these databases.Nor do many databases support using a natural join which I would not recommend using in any even whatsoever as they join could change if the table structures change. So suppose you add a field called modifieddate to both tables, you would not want to join on that but the natural join would do so. 

If you follow the very good advice of kgrittn and still have performance issues, you may need to perform the update in batches. You would still perform set-based updates but limit them to the first 1000 (or whatever number works for you, I've used from 500 to 50,000)records that don't match and then keep looping until all are done. 

It is a very poor practice to expect to maintain the PK/FK relationships from the application. In any db that is nontrivial, data has close to a 100% chance of being changed from other sources including ad hoc queries, data imports, etc. It is irresponsible to think the data in the database is protected because the application has protections. Sure you think that you will force all changes through a webservice or some such, but the reality is that no one is going to add a million new customer records, from that company you just bought, one record at a time through a service. Your database has to be designed to account for the fact that people will change it directly at the source or through other applications (some of which may not be able to use your web service). Further, the application interface is usually more likely to be thrown away or redesigned than the data and when that happens, you may lose all or some of the data integrity rules. In designing databases, you have to think about how the data needs to be protected over time not about what is easiest and most convenient for the programmer or the intial application. A data professional sets up PK/FK relationships in the database because that is where the data lives and it is the best place to set it up to protect the data quality. I see data from a lot of different companies in my current position and I see all too many that didn't set up their PK/FK relationships in the database because they have data integrity problems that a correctly designed database would not have. 

I don't know for sure in mysql but I see a bunch of things that casue performance problems in SQL server. First, correlated subquereis are often performance killers as teh run row by agonizing row insted of in sets. Next, do not use a wildcard as the first character in a where clasue as it prevents index usage. Either your table is incorrectly designed that you need to do this (such as when you store a comma delimited list) or you need to start using some type of full text search instead. Frankly there is no reason why users can't put in at least the first charcters of the email or username, so the wildcard is probably not necessary at all. Look at your requirements and be sure you are not goldplating and harming the system inthe process. Next, no production code should ever use select * espcially when there is a join. You have repeated the join field twice by doing this which is wasteful of server reources and network resources. And if you are not actually using all of the other columns that is wasteful too. Further, I know in SQl server there is a performance hit while it looks up the column names and this may be true for myssql. At any rate it is a SQl antipattern just like using implicit joins is a SQl antipattern. It is also dangerous for maintenance as things which are added to the tables should not automactially be added to the query. You could end up showing things in the wrong place (if some person rearranges the columns) or returing data that you do not use or even showing some fields that you don't want the user to see. This is a very poor practice.