PhysX is a C++ API and can thus not directly be integrated with the JavaScript-based WebGL. Depending on your needs, you have the following options: 

Again not true, because the principle doesn't change. The amount may change, except for the special case that the cosine is <= 0 before and after the tilting. The amount does not necessarily grow, except if we define that the cosine equals 1 before, i.e. that the normal points directly towards the light source. This whole paragraph should probably be rewritten to be less ambiguous. Including isotropy could make it more complete. 

Low-pass filtering is a classic tool from signal theory that will effectively remove noise, as you suggested, but will also cancel out desired high-frequency information in the image such as sharp edges. The image will look blurry. Post-filtering Monte Carlo rendering results is an open field of research and many advances have been made over the years, and overview of which can be found in the Denoising Your Monte Carlo Renders: Recent Advances in Image-Space Adaptive Sampling and Reconstruction SIGGRAPH Course here. 

Figure 2: Cosine sampling on the hemisphere Now, the BRDF is not the only thing that can be taken into account. When you have known light sources with strong intensity, it is often desirable to direct more rays towards these light sources, as these directions presumably have a greater impact on the result (i.e. there is more light coming from these directions). The idea to combine various different importance sampling strategies into one is called multiple importance sampling. This technique is not limited to the given example of BRDF + known lights, but can be used with any kind of sensible heuristic you can come up with. Images were generated using Holger Dammertz' Hammersley Points on the Hemisphere. 

When shading a point on an opaque surface, you need to gather incoming light and weight it with the bidirectional reflectance distribution function (BRDF) of the material. The naive approach is to distribute samples equally over the hemisphere and probe all directions equally for incoming light. This is called uniform sampling (Fig. 1). While this works in most cases, it can take a long time to converge towards the correct result. 

You can of course, as you suggested, map (u, v) to (φ, θ). Unfortunately, it does not solve the problem for 5 points: 

Figure 1: Uniform sampling on the hemisphere To speed things up, let's consider the properties of the surface in question. A diffuse surface needs to take light from all directions on the hemisphere into account, where light coming from flat angles has less influence. A perfect mirror only needs a single direction according to the law of reflection. Importance sampling takes this into account and distributes samples where light is expected to have a greater influence, judging from the BRDF. For the diffuse surface, a cosine sampling would be used (Fig. 2), resulting in fewer samples at flat angles that presumably have a lower influence. For the perfect mirror, only a single ray is shot because all other directions have zero influence anyway. 

I've changed Holger Dammertz' code a bit (switched u and v), and you see that the problem still persists. For a higher number of points, it really doesn't seem to make a (visual) difference. The Hammersley point set is a way to very quickly determine points on a sphere that have good, but don't need to have a perfect distribution. In the problematic case of very few points, Hammersley might just not be the tool for the job. Pre-calculating and storing points sounds like a better option to me. 

Ignoring emission and shadowing for simplicity, the rendering equation can be stripped down to: $$L(x, \, \vec \omega) = \int_{\Omega}{f_r(x, \, \vec \omega^\prime, \, \vec \omega) \, (\vec \omega^\prime \cdot \vec n) \, d\vec \omega^\prime}$$ where $f_r$ is the BRDF and $(\vec \omega^\prime \cdot \vec n)$ is the cosine of the angle between light and surface normal. For the Lambertian diffuse BRDF $\frac{\rho}{\pi}$ this is fine - we're multiplying it with the cosine, seeing Lambert's cosine law in action. Now for the specular part of the Phong BRDF, you would commonly calculate a pixel's color like this: $$I_{out} = I_{in} \, k_{specular} \, (\vec w \cdot \vec r)^\alpha$$ where $I_{in}$ is the incident light, $k_{specular}$ is the specular reflectivity of the surface, and $\alpha$ is the Phong exponent. Note the absence of $(\vec \omega^\prime \cdot \vec n)$ in this equation. To use the rendering equation and end up with the same pixel value as the common Phong equation above, we'd need to write: $$f^{Phong}_r(x, \, \vec \omega^\prime, \, \vec \omega) = \frac{1}{(\vec \omega^\prime \cdot \vec n)} \, k_{specular} \, (\vec w \cdot \vec r)^\alpha$$ Note the cosine in the denominator that cancels out the cosine in $L(x, \, \vec \omega)$. In his derivation of the Phong normalization factor, Fabian Giesen states: 

To my knowledge, Software Occlusion Culling (which you already mentioned), is pretty much the only thing a software rasterizer would still be used for. Procworld makes use of a similar technique to display its huge voxel environments. Most other culling methods like frustum culling do work on the CPU, but, to stick with the example, the test against the frustum happens on object level, probably with an axis-aligned bounding box (AABB). This intersection test is way simpler than using a full-blown software rasterizer. In certain cases, software rasterization could be used for mouse picking of objects. In game engines, this is often solved using a physics engine and ray-triangle collision with a simplified mesh. With the CPU being idle while waiting for the GPU in modern interactive 3D applications, one could think that it might be beneficial to use these idle cycles to render on the CPU using a software rasterizer. The problem here, besides the rendering getting horribly complex and convoluted, will often be the bandwidth. Images rendered on the CPU need to be transferred to the GPU before getting displayed, which might cancel out the benefit. 

Here is a side-by-side comparison of a cube, notched sphere and Suzanne for a glossy test material with a slight normal map (rendered in Blender with Cycles, lit with the Dutch Skies 360 - Free 002 HDR): 

Photorealistic rendering has the goal of rendering an image as a real camera would capture it. While this is already an ambitious goal, for certain scenarios you might want to take it further: render an image as the human eye would capture it or even as the human being would perceive it. You could call it visiorealistic or perceptiorealistic rendering, but if anyone could come up with a catchier term (or tell me that there is one already in existence) I'd appreciate that. Here are some examples to make my point clear. When you take a picture with a camera at a low illumination level, you either have a good lens or get a noisy image. For a human observer, scotopic vision kicks in and gives rise to the Purkinje effect (colors are shifted towards blue). This effect depends on the HDR luminance information, which are lost when I display the image on a LDR display. In addition, the human brain may use depth information to 'filter' the perceived image - information which are lost in a final (non-stereo) rendering. Assembling an exhaustive list is probably an elusive goal. Could you suggest some of the effects of the eye and the brain that I would need to consider? 

Spherical Harmonics (SH) are a way to represent low-frequency spherical functions with only a handful of coefficients. They have some nice mathematical properties, e.g. a convolution with a kernel function h(x) (that has circular symmetry) can be calculated as 

This is true, because a Lambertian reflector will never change the way it reflects light. The underlying principle stays the same. Also, Lambertian surfaces are isotropic, so the amount of reflected light won't change either (which is probably what this sentence is aiming for). 

While teapot, spheres and trusty Suzanne are not per se bad test scenes for materials, here are some things to consider. You can apply them to said examples to forge your own opinion. First and foremost, the viewer needs to be able to examine the behavior of the BRDF. Since it is dependent on both the direction of incident light and the direction from surface to viewer, you need to have as many combinations as possible in your test scene. A spheroid nicely covers all possible viewer angles. Notice that all three scenes feature some sharper edges or creases too. Differences in reflective behavior across a smooth surface (like a sphere) may be hard to conceive, so these edges put emphasis on this. Since the observed brightness and color is a combination of the material (i.e. the BRDF) and the lighting environment, it may be hard to tell one from another. The Vray test scene takes care of this. The 100% white 'core' and the 25% black floor help you get an idea of what the lighting in the scene is like. Blender and Vray both feature a grid that helps you estimate the total size of the scene. This helps with evaluating grain or surface texture. 

The shadow is, so to speak, 'correct' and the weird appearance results from the normal map interpolation 'cheat'. Nevertheless, it is visually distracting. The linked article above, written in 2009, suggests removing shadows or self-shadows on objects that show this kind of artifact, which (especially for non-convex objects) is not a satisfying solution. This is a very common problem inherent to ray tracing algorithms. I wonder if there is any better solution to it than subdividing the mesh into flat-shaded high-poly or getting rid of the shadows altogether. 

I don't understand why he speaks about the numerator here, instead of the denominator. Is this a slip in the article, or a misunderstanding on my part? What is the 'modern formulation' of Phong? What does it mean that cancelling out is 'complete nonsense physically'? With the normalization factor, $\frac{\alpha + X}{2 \pi}$ when should you use $X=1$ and when $X=2$? Here is a path traced side-by-side comparison of a reflective sphere, Phong exponent 100, $X=2$ for the normalization. The left image has the cosine cancelled out, while the right image includes it, noticably darkening the borders. 

In many cases, e.g. incident light for a given point on an opaque surface, full spherical information is not needed, since half of the sphere is zero / undefined / unused anyway. Thus, Hemispherical Harmonics (HSH) were born. How does convolution with an arbitrary kernel (with circular symmetry) work for HSH? Can the convolution from SH be extended or is there any paper that goes into details on this? 

The cube does a pretty good job on depicting the normal map. The sphere, as expected, is reasonably good for normals and gloss. Suzanne, arguably, makes both a bit harder to perceive than the spheroid. 

While the previous answer gave some reasons, I feel that it missed the core point of why back-face culling is not a well-suited optimization for ray tracing. Let's take a look at rasterization. For each (potentially) visible triangle, we project it using a matrix multiplication and, if it occupies any pixels, try to paint it while comparing depth values, which need to be interpolated first. By testing the winding order once early on, we can save a lot of operations down the line. Now, in ray tracing, you'd typically trace rays individually.1 That said, when a ray finds a potentially intersecting triangle, the vertex data would need to be loaded from memory (which often is the bottleneck) and only then could backface culling happen. You'll save some cycles for finding the intersection point in case you actually hit a backface, at the cost of a 'useless' test for every single ray hitting a front-face (which, arguably, will be a lot more). 1 In fact, this can be called the main difference: in rasterization, triangles don't know about each other, while in ray tracing, rays don't know about each other. This makes the global illumination problem (i.e. a problem where different surfaces interact with each other) very hard for rasterization, while interactions between different rays would be hard for ray tracing. Good thing we don't observe a lot of those ;-) 

At the base, rendering APIs like OpenGL or Direct3D provide a unified interface to communicate with graphics adapters at a low level. Differences in hardware are hidden behind the abstraction which simplifies development a lot. The low-level APIs provide basic functionality, which gives you total control but leaves you without a lot of desired features. 3D Rendering Frameworks like OGRE or Irrlicht introduce the more understandable concept of 3D objects that can be moved in space (without having to worry about transformation matrices most of the time), introduce light sources and algorithms for shadow casting (e.g. shadow mapping), loading of common mesh formats, and the list goes on. I call this a framework, because you still need to develop the application yourself using actual code, while an engine provides you with an application to develop your software in. This is just my personal distinction and both terms are used differently in a variety of contexts. In a 3D Authoring / Rendering Software like Blender or Autodesk Maya (offline) or 3DExcite DeltaGen (real-time) you can load, display or create meshes using the built-in functionality. These applications target the creation of 3D data, images or movies, and usually you can't create interactive applications easily with it. Be aware that some 3D Authoring have modules specifically targeting this, e.g. Blender and the integrated Blender Game Engine. A 3D Game Engine finally provides you with all the tools to make it easy for you to develop a game. You will usually be provided with a level editor, a material editor and maybe a visible scripting language (like Unreal's Blueprint or earlier Kismet) to implement dynamic behavior and interactivity. You don't have to worry about rendering algorithms, because they are already implemented for you. Your engine of choice may for example feature SSAO and cascaded shadow maps for dynamic objects and a solution to bake global illumination for static geometry. Also, a game engine includes way more than only rendering, e.g. artificial intelligence, sound, loading of assets, exporting your content into a ready-to-install package etc. As you can see, each software package generally puts another layer of abstraction on, ideally making your life simpler. Each has its own purpose and allows you to do different things. Note that said terms are very broad and people have different opinions and sometimes even debates about where one category ends and the next one begins. My advice is to not lose sleep over it.