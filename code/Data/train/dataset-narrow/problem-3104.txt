Christopher is right about the size of the array. To be simplistic about it, if this translates to 1.6B floats, at 16 bytes per float (32-bit version; 64-bit is bigger), then you're trying to create an array of about 26 GB. Even if you have the RAM for that, I'd imagine that it's probably going to overload something else along the way. (Maybe not, but generally speaking, any operations that are that computationally intensive should first raise the question of whether you are doing the right calculation in the first place. And if you do need to do something of that magnitude, you should then try to break it down into more manageable chunks that can be run in parallel or distributed across machines.) But given that you are describing a very, very wide dataset (~40k columns x 180 rows), I wonder whether you really want to take the covariance matrix of the transposed dataset (so 180x180 = 32,400 covariances)? That would be a far more tractable problem, and it's easier to see how it might be useful. In any case, you're probably far better off calculating each pairwise covariance (or at least, the vector of cov(x_i,x_k) for all x_k != x_i) at the point where you'll actually use it, rather than calculating a giant matrix initially then referring back to it later. Memory issues aside, it'll make your life much easier if you start running things in parallel, and will help ensure you don't waste resources on unnecessary calculations. 

The reason that sigmoid functions are being replaced by rectified linear units, is because of the properties of their derivatives. Let's take a quick look at the sigmoid function $\sigma$ which is defined as $\frac{1}{1+e^{-x}}$. The derivative of the sigmoid function is $$\sigma '(x) = \sigma(x)*(1-\sigma(x))$$ The range of the $\sigma$ function is between 0 and 1. The maximum of the $\sigma'$ derivative function is equal to $\frac{1}{4}$. Therefore when we have multiple stacked sigmoid layers, by the backprop derivative rules we get multiple multiplications of $\sigma'$. And as we stack more and more layers the maximum gradient decreases exponentially. This is commonly known as the vanishing gradient problem. The opposite problem is when the gradient is greater than 1, in which case the gradients explode toward infinity (exploding gradient problem). Now let's check out the ReLU activation function which is defined as: $$R(x) = max(0,x)$$ The graph of which looks like 

Is this a reasonable thing to do in Vertica/SQL, or is there a good reason not to bother and just handle it in Python instead? And if the latter, is there a strong case for doing this in pandas rather than sklearn or vice-versa? Thanks! 

create a table of substitute values (e.g., mean/median/mode, either overall or by group) for each incomplete column join the substitute value table with the original table to assign a substitute value for each row and incomplete column use a series of case statements to take the original value if available and the substitute value otherwise 

It's actually even simpler than that, from what you describe---you're just looking for a basic classification tree algorithm (so no need for slightly more complex variants like C4.5 which are optimized for prediction accuracy). The canonical text is: $URL$ This is readily implemented in R: $URL$ and Python: $URL$ 

Idea from Lev Manovich, shown on video $URL$ shows how most effectively plotting picture data to chart is made by showing the images as the dots in the chart. When something interesting behavior is on one point, you can observe the image right away. There was an example about plotting time series data on x-y space and from the form of daily curve there was a anomality in one day. Directly zooming on the pictures on that revealed a fire taken place in the landscape. From average value black and white plots you would not get that kind of immediate observations. 

SGDClassifier has desicion_function which tells the distance to the hyperplane, where the values are compared to. This value could imply too big and too low values. 

I made little search some days ago to get familiar with some Backpropagation related thing and came across to find this pdf. In the beginning the author says that the approach is there taken so that it is most understandable. Directly from Author: 

What you are attempting to do is calculate the distance between two discrete probability density functions. The standard way to calculate this distance is via the Kullback-Leibler Divergence. It is defined as such: $$D(P||Q) = \sum_iP(i)*log(\frac{P(i)}{Q(i)})$$ To be technically correct, this is not a metric in a mathematical sense because it is not symmetric and does not abide by the triangle inequality. But never the less it is a "distance" that is used a lot to compare distributions. From an information theoretical perspective, the KL divergence (given that you're using base 2 for the log) represents the number of bits it would take to encode distribution P as Q. In your case, you would have to normalize each distribution to ensure that the sum is 1, and then calculate distance via the KL divergence. If you want a real metric you can use the Jensen-Shannon Divergence which is defined as: $${{\rm {JSD}}}(P\parallel Q)={\frac {1}{2}}D(P\parallel M)+{\frac {1}{2}}D(Q\parallel M)$$$$ {\displaystyle M={\frac {1}{2}}(P+Q)} M={\frac {1}{2}}(P+Q)$$ Hope this helps. 

Decision stump are decision trees with one step from root to leaves whereas Decision trees can have several steps between root and leaves. Easy example of these two is that a decision stump could be which side of coin faces up when thrown and a decision tree would be that if the coin could is touching the ground already (states are interconnected): 

So, similarly as in convolution every nth pixel is selected for the operation. Edit Selecting points like this: 

As Kyle said on his answer word2vec can be run with the data dump data and you would get a mapping that shows the closest words, that are possible synonym candidates. Same approach is on this Quora post. Here is explained how word2vec makes a vector of probabilities of different words and with cosine similiary (highest cosine distances) you can find the nearest ones = the synonym candidates. A code example is on this Github. There is a KDT tree used and its cosine distance. (KDT = k dimensional tree) Basically a synonym is a word with enough little distance, and you can set some threshold to find all enough near ones or only the nearest. All that in code of course. In the mentioned Quora WordNet was mentioned as a source of synonyms too, but then I came up also with idea of using SE Tag Synonym dump (see here), where we have a superwised source of common synonyms. Those can be used as alternative source of synonyms, or as a database to verify the ones found by the distance method. 

If you look at the derivatives of the function (slopes on the graph), the gradient is either 1 or 0. In this case we do not have the vanishing gradient problem or the exploding problem. And since the general trend in neural networks has been deeper and deeper architectures ReLU became the choice of activation. Hope this helps. 

The order of the files that populate file_list, is the same order X_test appears in, by row. So just match the indices to correlate filename with prediction. X_test[0] ~ prediction[0] ~ file_list[0] 

I re-implemented your set-up in python using keras. I used a hidden layer size of 25, and all my activations were sigmoid's. I got to an accuracy of 99.88%. Try running your algorithm for a greater amount of epochs. Use binary cross entropy as the loss function and try decreasing the learning rate of your gradient descent algorithm. This should help increase your accuracy. My only explanation for the poor performance would be that you are getting stuck at a local minimum, if that is the case different initiations of your weights should fix that problem. 

Like suggested in one answer on this SO question, you could use elastic matching with Levenshtein distance to your task. Levenshtein distance obeys triangle inequality and is therefore a metric distance. Use of elastic matching was suggested for time series data comparison. Levenshtein distance works with characters data. There is an implementation of elastic matching and Levenshtein distance calculation in Python. To put them together you most probably need to build your own implementation. 

In this picture from the question you can observe how it is inaccurate, even compared to SVC with kernel=linear. Further differences may be explained with differences how SMO and SVC with linear kernel are implemented. This post compare the two, and the answer is SMO should be the fastest one, otherwise your dataset maybe is not suitable for it. I would test SVC with linear kernel and see if things change. 

Sample a representative, but small set of your data, which will allow you to compute PCA in memory. But seeing as you have 600,00 observations this will most likely not result in any meaningful results. Use incremental PCA, here is a link: $URL$ 

But the main problem you have is that a number of samples are less than the amount of observations you have. I would recommend a different approach to dimensionality reduction. Autoencoders would be my recommendation to you. Autoencoders can be trained in an iterative fashion, circumventing your memory issue, and can learn more complicated projections than PCA (which is a linear transform). In case you want a linear projection you can have an autoencoder with one hidden layer, and the solution found by the neural network will be equal to the solution found by PCA. Here are a couple of links you will find helpful: 

Boosting can't help if decision tree in my example exactly knows before decision for example the side of turning possibility and for stump always one step after. So, stump can help in finding a statistical pattern in that example but not the underlying external facts affecting the system in certain move, if the conditions vary randomly in time. 

There are alternative solutions for self organizing maps. Best of them I found pymvpa where the example is easy to read and understand. It is also maintained quite activately as you can see from their Github. I tried to run the kohonen 1.1.2 test file, but it did not run after two days of trying. So, let's have a try for the another solution. To run pyMVPA example som.py, you have to do the following (at least):