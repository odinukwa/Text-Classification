I'm mostly a system administrator and I don't directly work with databases other than installing them, setting up accounts, granting privileges, and so on. I realized that if The Boss walked up to me and asked, "What is a relational database?" I probably couldn't give a satisfactory answer... I'd maybe mumble something about data being stored and organized by categories which you can query with a special programing language (i.e., SQL). So could someone give a good "Boss Answer" for what a relational database is? And maybe how its different than just storing data on a file server? Bonus points for clever but accessible analogies and explaining tables, columns, records and fields. I'd define a "Boss Answer" as a quick one (maybe two) paragraph explanation for non-technical folks... mostly your Boss, on those rare occasions they actually ask you what it is you do all day. 

RCS is trivially simple to setup and use. I heartily recommend it for even the smallest of installations. It will help mitigate these problems in the future. 

The only other information I could find on the error was a fellow who posted on TechNet who seems to have had the same issue as me, even down to the issue where he had a previous installation of SCCM that whose WMI namespace was being mistakenly referenced (e.g., instead of ). He ended up nuking the entire WMI namespace as well as the parts of the SMS_ProviderLocation. I'm not sure I want to do that. 

There's no magical way to programatically distinguish between a computer that has been improperly decommissioned (no one removed it from Active Directory when it was off-lined) and a laptop sitting in some executive's desk. If you want to move carefully you can just disable the account and send it off to a specific OU like I do. 

Looking at the historical performance data for the virtual machine indicates that this condition has been going on for at least a year but the frequency has increased since March. 

I then found a few records that I expected to get delete with timstamps from a few years ago and ensured that that the and that time stamp was actually set: 

I'm pretty inexperienced when it comes to setting up power for a network rack. I have a location that has a dedicated 20 amp circuit with a LP-20P socket. I have a APC UPS 3000XL rackmount UPS and a TripLite PDUMH20 PDU. I'm looking at the current configuration where things are configured like this: [Power Source] --> [PDU] --> [UPS] --> [Network Gear]. I can't help but feel this is incorrect. Won't the UPS "obscure" the draw of all the downstream devices? What is the correct power path for UPSs and PDUs? Does the UPS go to the PDU or the other way around? Does it make a difference? 

The vendor's documentation instructed us to add the service account to the Backup Operators and Power User Local Groups - which we did. Reading the Explain tab for each one of the required User Right Assignment policies indicates that the Backup Operators have those Rights by default (TechNet seems to confirm this). Incidentally, there's no mention of Power User being assigned those Rights that I can find so I'm not really sure why that was a requirement. 

The Attempted Solutions: How we have tried to fix it This seems consistent with the information in VMware KB 2038918 and VMware KB 2037408. I tried following the resolution path in VMware KB 2038918 by connecting to vSphere Web Client using the SSO Administrator account () and adjusting the Base DN for groups to be narrower instead of the base of the domain in case we running into timeout issues when doing group enumeration. This did not resolve the issue, however I was able to successfully Test Connection. The web client seems to just crawl though, for example it took over three minutes to open the 'Edit identity source' dialog window. VMware KB 2037408 does not seem to apply in our case as authentication fails whether or not we use Windows session credentials or if I manually supply my Active Directory credentials. I have restarted both the VMware vCenter service and failing resolution of the issue, the entire vCenter server. This did not resolve the issue. 

If it is at all an option find another job. This will not end well. And honestly do you really want to work at a company that treats everyone as replaceable? Request an emergency budget so you can find a consulting company to keep things running and act as a resource while you get up to speed. If this is denied, make sure you have it in writing somewhere for CYA purposes. Read any documentation left behind by the previous IT staff. Perform an inventory of both physical hardware and devices. Figure out what is in warranty and what isn't. 

Check the physical layer and the data link layer first. Nine times out of ten, it's a patch cable that someone ran over repeatably with their office chair (by the way, the port errors will show up in SNMP-reported switch statistics... useful see?). Or the moving company parked their truck in front of one our wireless bridges. Look for bad terminations (use a cable tester), out of spec cable runs, and duplex mismatches, or broadcast loops especially if you don't have physical control over all the switching infrastructure. Layer-7: Look for client or server misconfigurations or configurations that are no longer relevant (Wireshark is your friend here). DNS problems. Are network backups running during the day? Or WSUS updates being applied? Etc. Layer-8: And finally there always seems to be someone watching videos via NetFlix (RMON or SFlow will discover this). 

Configure your clients to not check the trust path of your RADIUS server's certificate (i.e., uncheck the box that says "validate server certificates"). Get your RADIUS server's certificate signed by a "External" CA whose signing certificate is distributed in Trusted Root Certification Authority repository (like Verisign, Comodo, etc.). Setup some kind of captive portal that acts as the supplicant on behalf of your clients. 

What you're really looking for is a load balancing solution that is conversant in Layer-7 protocols, namely HTTP. I think Squid could probably be configured to do this - it at the very least it is a good place to start. EDIT: Doesn't look like Squid will do what you want (it does reverse proxy load balancing for HTTP). I'm not sure why you'd want to Layer-7 aware outbound load balancing anyway - what problem are you actually trying to solve? 

We are in the middle of doing physical inventory of all of our workstations, many of which were deployed in a hurry. Consequently, we do not have all of the information about them documented that we would like. We use a mix of Windows 7 Pro and Windows 7 Enterprise for licensing reasons. We know how many licenses are in use (we have a KMS server in-place), but we don't know which edition of Windows is installed on which specific workstation. So my question is as stands: Is it possible to tell what edition of Windows a domain member is using through AD or some other similar mechanism? It would be really great if this information along with the machine's hostname could be pulled with a script, instead of pointing and clicking through the AD Remote Admin tools. I do have Domain Admin rights if that is required. 

Edit: Ah hah! Google comes through! I was thinking specifically of USENIX SAGE's Job Descriptions and Definitions -- although it turns out they are more specific to job description and less about network description so I'm still interested in hearing about what SF folks use for network description and classification criteria. 

The standard Linux permissions scheme should account for this. Unprivileged users can't modify anything that doesn't belong to them, nor access to folders where important system information is stored. 

Zenmap (the GUI version the famous namp) has a nice topology mapping feature. Although, it will only generate a one time map and will not automatically update or give you threshold alerting. It will however, with five minutes worth of time, give you a quick and dirty topology map. 

However it is preferable to add the name of the module to so it is loaded at boot time. (Notice that if the command command fails to execute your interface won't come up - even if the rest of the configuration is perfectly fine). and (B) how can I re-enable the ip_conntrack_ftp module? From the modules(5) manpage: 

I can coordinate the Content IDs in the PatchDownloader.log back to the entries recorded in the ruleengine.log so, as mentioned previously, I'm pretty sure were looking at the same event generating all of these different errors but if someone knows better please correct me. If I use CMTrace's Error Lookup tool it tells me the following about hr=. 

Daytime and Time and what I would consider "legacy" protocols. My guess is they are included in the default configuration for traditional UNIX-style completeness. They are started by inetd, and unless you need these services (you probably don't if you have to ask) you can disable them by commenting out the relevant lines in your /etc/inetd.conf (see man page). 

You will still be able to do port-based 802.1x authentication but only for the entire hub. As far as the 802.1x authenticator is concerned it is just able to allow or disallow (or assign to different VLANs) that one port that the hub is attached to. Imagine what will happen with a client authenticates this port to a trusted VLAN but then another client authenticates this port to an untrusted VLAN. From the perspective of the authenticator you will not be able to only the port that your hub is attached too (and hence everything that is attached to it). If you require port-based authentication on a switch or need to authenticate a device that doesn't support 802.1x you can rely on MAC Authentication Bypass, which is essentially just whitelisting MAC addresses or port as required. To really take advantage of 802.1x you need a switching infrastructure that fully supports 802.1x (luckily it's pretty common on mid-range enterprise grade switches). 

The Environment: Our Stuff, Hallowed Be Its Breakage Authentication fails for both vSphere Client and vSphere Web Client from my workstation and locally from the vCenter server. Authentication fails for multiple users. I verified that all of the users attempting to authenticate are members of the vCenter Administrators group (via membership in the Local Administrators on the Windows server vCenter is installed on). I can successfully ping and connect the LDAPS port of the domain controllers used as identify sources. The host server does not have any undue resource consumption. We have made no changes to our vSphere install but we do not manage or have any visibility into our directory services (although I cannot imagine what changed there that would break vCenter SSO). We are using vSphere 5.1.0 Build 1063329. I am using Firefox 27 with Adobe Flash 12.0.0.70 with the vSphere Web Client. The host operating system for vCenter is Windows Server 2008 R2 SP1 and MS SQL 2012 SP1. 

We have this working for 802.1x but Novell's client for Windows 7 breaks the 802.1x protocol. So we have to have the certificate based authentication. This doesn't make sense to me. Regardless of whether you are using EAP-PEAP, EAP-TLS or EAP-TTLS your supplicants will need to talk 802.1x with the authenticator. If Novell's client truly breaks 802.1x there's no way you can get this to work regardless of what authentication mechanism you decide to use with EAP (which itself is tunneled via EAPOL) 802.1x will need to supported by your supplicants and authenticators. 

This is probably not the best way to solve this problem but it worked. Here's what I did. 1) I added the following restriction to my Debian-based DHCP server and removed all of the fixed-address entries. This forces any clients in those IP ranges to move somewhere into the .41 - .199 range, otherwise when I turn on the Windows Server clients will receive leases with IPs in the .11 - .40 range that are already present on the network. I then let things sit long enough for any leases in that IP range to expire and have a new one issued. 

You are probably seeing a more restrictive interpretation of (or ?) in Windows Server 2008 R2 which is then calculated against your less restrictive NTFS permissions. I like @Helge Klein could not find any documentation on what constitutes actual atomic permissions of the Share "meta-permission". As you have discovered, this process is complex and prone to errors. Make your life simple and use NTFS permissions exclusively for your access control. Set your Share permissions so they are wide open ( - ) and then set the NTFS permissions to provide you desired access control - something like the following is typical: 

Blocking File and Printer Sharing will break Active Directory Domain Services. If you review the Active Directory and Active Directory Domain Services Port Requirements you'll notice that following ports need to be open. The reason the rules get re-enabled is when you install Roles that require certain services and their ports to be open Windows will automatically enable those rules. 

Can you confirm this via some sort of logging capability on the access point themselves? There's a huge spectrum of wireless hardware and not all it plays nice together or is well built. I have seen a few instances of clients reporting the they are associated but the access point does not record their associations. Changing the wireless card on the client resolved the issue. 

Layer-9 considerations are prompting a migration from Citrix XenServer to Hyper-V as our shop's virtualization platform of choice. This will require me to migrate our existing virtual machines from XenServer to Hyper-V. A hand full of these VMs are running Debian. Unfortunately, Debian does not seem to be on the list of approved/supported guest operating systems. In fact it seems that running Debian as a guest operating system is rather difficult, although apparently not impossible. I have two interrelated questions: