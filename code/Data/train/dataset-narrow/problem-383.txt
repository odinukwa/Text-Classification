This is very importaint key, and make sure you back it up using this guide Now you are going to create a certificate based on which you will create module signing user. Note it is only for singing modules and you cannot use it to test privileges etc. So first things first - Certificate : 

Hope it gives you few startup points, in case your boss is not willing to invest in 3rd party tools, or you dont feel like setting up data collector tool 

Monitoring CPU usage using task manager is not really a reliable source. There are many other(such as core OS activity,device drivers) non-sql processes running in the background that could be adding extra overhead without you even knowing. PerfMon is the tool you should reach for in these cases. Processor/%Privileged Time, Processor/ %User Time, Process (sqlservr.exe)/ %Processor Time Will give you an idea of what is actually happening with your SQL server, without explaining each of these counters, turn on description checkbox and read from there, but it will essentially show you the ratio of SQL Server vs Other processes usage. Even though its easy to spot, it is not so easy to diagnose. There could be other "hidden" issues that are indicating that the processor is the problem. Such as having lot of compilations/recompilations, which are issues related to non-parametrized queries or forced recompilations. You can find these metrics in Perfmon: SQLServer:SQL Statistics/SQL Compilations/sec, SQLServer:SQL Statistics/SQL Re-Compilations/sec. SQLServer:Plan Cache/Cache hit Ratio Indicates memory problem, but excessive page flushing in/out of memory also add extra CPU usage. DMVs can also help you diagnosing the problem. 

Fields to note here that are useful for shrinking are File Size which is the size in bytes, and Status which can be either 0(unused) or 2(used). The amount of % or MBs that could be shrunken is sum of unused VLFs - 1VLF. Be aware that shrinking only happens in VLF amounts, you cannot shrink it in desired MBs such as 5,10,12 etc, unless it fits the summed size of used VLFs. Always make sure to check messages after shrinking occurs. 3) Shrinking does not remove indexes, since shrinking removes only unused(empty) VLFs. If you might have thought, if shrinking database files defragment indexes - yes it does but its off the topic. 

It has nothing to do with SCH-S. Select queries which acquire S and IS locks are compatible with SCH-S, which is called Schema Stability and all it does, it prevents table during query execution from being modified which means SCH-S and SCH-M are not compatible. SCH-S is compatible with all other locks. Your problem however is that you have auto update stats enabled, which are triggered whenever there is certain amount of data changed within a table (20% pre SQL2016 or with 2371 traceflag counts % depending on number of rows) and it does when you execute the query. So what might have happened is that your table was pretty busy and query optimizer decided it should update statistics, so you had to wait. You should disable auto update statistics and update it manually 

Since you are testing it on production,and the tables are busy, there are many reasons why "sometimes" its slow. I cannot give you exact details because i dont know logic and environment in the first procedure, but here are some things to watch for: 

Simply granting VIEW DEFINITION and SELECT permissions on and schema wont give you rights to see the definition of the view. Permissions that you added will provide you information about all objects in sys schema and information_schema. I suppose you want to see the view definition which are created in some other schema, in which case you would have to provide the user with to that particular schema. Such as : 

will show you the most CPU extensive queries which you might want to optimize further. Optimizing these queries you might find missing indexes, outdated statistics, Non sarg-able queries which are real issues that are behind high CPU usage. Its not the only blueprint how to fix CPU problems but i hope it gives you a good start! 

Note that i filtered the view names retrieval query with some silly name, you should apply your own filtering. 

If you were still developing database, you could create views within a single schema, and add that user or group of users/role , permissions to alter that schema alongside permission to create view. Such as : 

.wrk files are just temporary extension names, while log files are being copied. If log is small enough you wont even notice that change, but in your case it is notable. Basically this is way of sql server ensuring that log that has being copied wont be accidentally restored by LSRestore job or by a manual restore. As soon as log has been copied successfully it will change into .trn. I dont know anything about your SQL, hardware and network specifications but having 570 transaction log backups in a span of one hour once in 24h could cause overhead on disk IO. Check the log file size, perhaps adding to the schedule one more LS during 24h span would be beneficial (lower tran log size). 

3) And the last one is IO related with info on all databases: Log reads,writes, read and write stall (how much time sql server had to wait to write or read from a log) and Read/Write ratio on a log and data files. Keep in mind that these are cumulative stats, and they reset only after restart, but you can insert snapshots in a table and keep a track of it. 

Executing this command, will give you no results (since we added that our data equals empty string) however, if you query the table ,table will be empty. Where as in sp_executesql you are explicitly declaring parameters and will be compared as it is So in the example: 

What you`re seeing is the XQuery implementation in SQL Server . Although XQuery uses its own parser and performs its own algebrarization during the query compilation stage, the results are combined and optimized together with the DML portion of the query, then combined into a single execution plan. SQL Server supports five different methods. value , exist , query , and nodes are used to access and transform the data. Last one, modify , uses XML DML to modify the data. The value() method returns a scalar value from the XML instance Lets say you have the xml : ` 

Dynamic quorum basically dynamically adjusts votes depending on available servers. Each time when one of nodes goes down, dynamic quorum will remove the vote from that node. In your scenario you have 2 nodes only and dynamic quorum will automatically remove the vote from your passive node, so the 1st node will have the majority of votes. In planned maintenance scenario when you are shutting down the first node quorum will transfer the vote from first to the second, and remove it from the first node. However in scenario where first node just crashes quorum does not have time to transfer the vote and your second node wont get to vote, which basically will just shut down your cluster. Therefore in scenario with 2 nodes only, it is recommended to have a witness. 

Your second query runs within a second because it does not have to go through each record in (130 million record table) and compare whether it matches the record from a temp table. And there is not much you can do when you are using a temp table with a single record within. One solution would be to save it within a variable and use it in where condition without joining it, but you said temp table will contain more records. Note that ,it does not necessarily mean that more records will increase your execution time. With more rows in temp table, query optimizer will use Hash Join which could possibly give you even better results. However you could optimize your query like this: 

You should try to get a track of long running trans. especially during that time, so you could prevent it from happening again. Check whether you have some kind of bulk inserts, long running jobs that include some DMLs, or simply set the alert. 

If views are already created, you can either add grant alter schema (name of the schema where the views reside) and add permission to user to create a view. But be aware that since you granted a user alter schema, he has all the rights on that schema, including truncating/droping tables etc.. Last option is creating a dynamic query that will grant a user/role to alter each of these views Update 

That happens when you have an open transaction during the database full backup. Whenever you make any DML data pages are hardened to disk, containing the earliest LSN that was part of that transaction.These are called dirty pages (since transaction is not committed), which helps SQL recover after an unexpected shutdown/restart, to restore itself to the point before non committed transaction. While you was doing a full backup you, it ignored the LSN of the transaction that was ongoing, and after it has been committed, you next transaction log backup read the latest committed transaction (which was the one that was active while your full backup was being processed). I managed to demonstrate the same situation on my computer with a simple test: leaving a transaction active, while taking a full backup: 

You should setup a role with permissions you wanted, and assign user to that role,so it would be easier for you to manage that user and permissions in general. "Almost anything" would include alter database, but you have to be aware what the user can do. You could add a user permissions such as : 

Just like the master key, you need to back up this certificate as well. guide Now we are creating a user with certificate which will have all the rights: 

2) This one will show you records that are placed in a log buffer, in what state transaction was, how many records are logged, size in bytes ,and a query that executed it. In a nutshell it displays all inserts/deletes/update from an active transaction that are not committed/rolled back yet 

In order to decrypt the column that is encrypted by symmetric key you would have to create the exact same symmetric key on the new database. Since symmetric keys cannot be backed up, in order to use them on another database you would have to provide them 2 attributes that have to be specified when creating a new symmetric key on a different database to decrypt a column. Those two attributes are KEY_SOURCE and IDENTITY_VALUE After you have specified these attributes you need to encrypt that key, with one of the following: password, certificate, another symmetric key, asymmetric key, or some third party provider. In your case is certificate, therefore you need to create a certificate from certificate(original DB) that you supposedly backed up already. Such as: 

This DECRYPTION BY PASSWORD is password that you specified when you were making a certificate backup. And last step is creating an actual symmetric key, with the same KEY_SOURCE and IDENTITY_VALUE as the one on original DB, with certificate(that you just created) specified encryption. 

As someone who is trying to use the DMVs as much as possible, there are few scripts that can give you a quick heads up 1) This one will give you an info about the full backups and tran log backups, how long it took it, what was the size of the backup,where it is located,expiration date(if it has),logical device (if it exists as well), and server name. Keep in mind its filtered for backups for past 7 days, and it displays all databases on the server, but you can sort it up as u please 

Based on your parameters it will defragment the indexes, either rebuilding or reorganizing depending on @MaxFragmentation. Online or Offline ,depending on edition. Hope it helps 

You need a table between Cities -> Business, and a table between Categories and Business. This way since city can have many business keeping it all in the single table you are breaking the 2nd Normal Form which is suggesting that the groups of data shouldnt be repeated within a table. What that means is that for instance if you have City of New York that has multiple business you will have 2+ records with the same City name but different City ID. Therefore i would keep the table cities with metadata about the City only and add a new table CityBusiness with its own identity id and c_id and b_id OR since cities can have only one business (is it possible for one business to expand and open its office to another city?) you can have a primary key defined as combination of b_id and c_id. The same rule applies for the Categories. A new table BusinessCategory with ca_id and b_id, and a Category table with metadata related to category itself. 

You can use it either way, however there is a one thing you should keep in mind EXEC() function is SQL injection prone Take a look at this: 

To add on my comment, you can create a trigger on a database level and add user to desired role automatically when you create a user. You can add a custom one, but for example sake i added db_datareader. Example : 

It wont yield any results, but it wont execute truncate table, because like i said, it is only treated as a parameters. Other than that, performance vise, there is no difference. Note that you should call procedure with schema name included, and should be aware that dynamic SQL executions will create a new plan individually from stored procedure each time you call it. Update 

Which was NVARCHAR(500) in this case and SQL server does not need to check the underlying structure of referenced functions/tables as long as knows the return type. It will fail to execute tho, if usernames function (in this case) would return something other than NVARCHAR(500), which is expected On the other hand 

Note that specifying database name you are limiting user only on that particular database. To build your own permissions on a user/role take a look at this pdf: SQL Server Permissions Map 

Would give you ID of the first customer from the first order In your particular case value('.') means give me all values from the shredded element( that i will talk about in a moment) Remember in using value function you`re moving through XML. Now to make it more easier ,not to 'move' too much you can use function node which shreds XML into relational data. It returns a row set with rows representing the nodes identified by the path expression. Example: 

Also if you are getting a lot of records from first procedure, consider using temp table instead, table variable is not the best choice. And lastly you can always diagnose procedure execution using Extended Events, and log it to a table/file if it exceeds certain threshold, along with execution plan,and waits so you can compare it with the regular executions. 

To get the most out of it, you would need to create indexes on both tables. Now depending on the query, you would have to decide which indexes to create in the first place. Blog where it just scratches the surface of the indexing can be found here You also need to be able to read execution plans, in order to understand whether your indexes are working properly, and whether you are even using those indexes. Blog about execution plans can be found here To answer your question: it makes no difference which table you initialize first, as query optimizer makes his own plan and order in which tables are joined, unless specified otherwise (with option(force order)), but it will give you no performance boost if you do. 

You should definitely separate it into multiple smaller procedures. Using just one procedure will cause a lot of locking and blocking, and generally degrade performance. As for optimizing: The first statement is update that updates data based on a @Code provided. Unless if its a clustered index created on it, you should create a nonclustered index on a Code column so you could prevent the whole table scan just to find the code. Same logic could be applied for deleting. Having an index on MainObjectID, and generally on any Foreign Key column would provide you decent performance benefits (Whether you delete, or join the table, or even update records in parent table). As you are inserting some data and deleting data based on ID of that insert, you could create a procedure that will do the first part of the code (Update or Insert if condition is met) and then return you the value of that ID(How to create objects with output parameters), which you could store in some int var in your c# app. Once you get a value you could execute the 2nd part of the procedure ,which includes deleting and some value inserting. Still it wont give you a best solution as you are inserting and deleting data in the same transaction - meaning it will most likely escalate into object lock, making you unable ( depending on transaction level) to make other select queries on those tables, but it will overall cut the % time of that update/insert initial statement. In summary keep in mind that smaller and faster transactions are much more effective than larger and complex transactions, for performance and concurrency standpoint.