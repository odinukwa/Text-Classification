Now for each vector in vecs I am getting same probability for each topic. Can anyone point out where I am going wrong? 

I solved this issue. There is a parameter for minimum probability in gensim's LDA which is set to 0.01 by default. So topics with prob. < 0.01 are pruned from output. Once I set min. prob to a very low value the results had all topics and their corresponding probability. 

I would suggest using hyperopt ($URL$ , which uses a kind of Bayesian Optimization for search optimal values of hyperparameters given the objective function. It is more intuitive to use than Spearmint. PS : There is a wrapper of hyperopt speifically for keras, hyperas ($URL$ You can also use it. 

Try different morphologies of the word (ex. morphology of goes will give go and you can get vector of go to initialize goes). Try finding similar word from wordnet. If you are able to find embedding of similar word from wordnet, initialize original word's embedding with this word's embedding. 

Generally for words which are not found in the vocabulary, a zero vector is assigned to them. Some other hacks which I could think of are : 

For your info, Pylearn2 is now deprecated. You can check it's alternatives i.e. platforms that are build on top of theano, which are: lasagne, blocks and keras. 

From some neural net article I read that if you scale up the Neural Net architecture the differnce in different local minimas in the loss surface diminishes. Essentially all local minimas become equivalent. If that is the case then why do different variants of SGD (like Adagrad, ADAM etc.) works better than plain SGD? I believe reason for using these variants of SGD is to solve "bad" local minima problem, but if all local minimas are more or less the same then what is objective of using these variants? 

Voacb. building from this class runs fine. But when I try running the train method, I am getting the following errors: 

and pass it in the model compile phase, the loss drops to a very small value (in range of 1e-8) for some epochs and final acc. is 0. I tried removing the alpha from the custom loss function and only return K.categorical_crossentropy, still same issue. Why is it behaving differently then simple passing the string 'categorical_crossentropy' in the model compile phase? Behind the scenes, it must be calling the same function, I suppose. 

I don't think there is any support for Map-Reduce or Spark for theano but if you want to run it in a distributed setting you can refer to the following technical report on using theano with MPI : $URL$ PS : Authors of the above mentioned report have released the code for the same. You can find it here: $URL$ 

I have a highly unbalanced text classification data. I am trying to over-sample through SMOTE. I have a doubt that applying SMOTE over sequence of word indices will give me valid data points or not (since SMOTE used nearest neighbor like method)? 

I am trying to learn topics distribution for each document in a corpus. I have term-document matrix (sparse matrix of dim: num_terms * no_docs) as input to the LDA model (with num_topics=100) and when I try to infer vectors for each document I am getting uniform distribution over them. This is highly unlikely since documents are of different topics. The relevant code snippet is: 

I have a multi-label classification problem wherein each example can belong to one of the pre-defined classes (or can belong to none of them). I was wondering if I can somehow apply multi-task learning (MTL) to this problem. Essentially, treat each class as a separate class and use a NN based model with common feature extractor layer, and on top of it class specific layer. My doubt is that generally there is a correlation between labels in a multi-label setting. In the MTL model, after feature extraction each class is handled separately without sharing information. Does a MTL kind of model for multi-label makes sense, given this information? 

I am optimizing some loss function using Gradient Descent method. I am trying it with different learning rates, but the objective function's value is converging to same exact point. Does this means that I am getting stuck in a local minima?, because the loss function is non-convex so it is less likely that I would converge to a global minima. 

While working with Twitter datasets, one thing that always confuses me is, How to tokenize the tweets. I have seen different open-source implementations using different schemes for tokenization. They handle URL-mentions, Capitalization, User-mention etc. differently. I usually follow the script accompanying the GloVE code: $URL$ . Are there any std. rules / best practices one should follow while tokenizing tweets? So much variation in different code-bases confuses me sometimes. 

For text processing there are plenty of tools out there like CoreNLP, SpaCy, NLTK, textblob etc. each offering different suites of pre-processing functions and people recommend different tool for different tasks like NLTK for tokenization etc. Which tool would you recommend which performs all these tasks? 

I have training data in form of pair of documents with an associated label - {doc1, doc2, label}. Label is defined as function of pair of documents. Now I want to build a model which can predict the label given two new documents. I want to try different representation of document (instead of common ones say TF-IDF). Can I use vectors (topic distribution) from LDA as features for a classifier? 

I gave some explanations here that may help. The radius r_ij is the radius of the neighborhood of the winner. Only those neurons that are closer than r_ij to the winner are allowed to update. The formulas that you show here say that this radius is not constant but decreases with each iteration. Initially you take a large radius, and then make it smaller and smaller with each iteration. Since you probably initialize your weights randomly, you might want to change many of them in the beginning of training but you might want to disturb less and less weights as they become more trained. That's why you decrease the radius. This formula is just a particular policy of change of the radius with each iteration. In this case it is exponential. You can choose another policy or keep the radius constant or decrease it slightly (not exponentially) which may be reasonable if you use some domain knowledge when initializing the weights. Also, you can keep the learning rate constant if you like. The radius of the neighborhood and the learning rate are hyper-parameters. It is up to you how to choose them. 

The regression line is almost exactly between the population means and is almost vertical because both ordinates of the population means are zero. If I take a couple of thousand points then it will be vertical. The code for this picture: 

The blue line is for bootstrap, and the black line is for SMOTE. The sample means and standard deviations for the minority class are as follows: 

Intuitively, the probability is high where there are training data, and it decreases in the regions between the training data. The model becomes less sure about its predictions far from the training data. The maxima of the prediction probability are not exactly at the training points. This might be because there is no exact correspondence between the underlying classification and regression problems. They are related but they are not the same, and the relationship between them depends on the values of the hyper-parameters, and the learning algorithm. For example, if I change the loss weights, 

In the last picture we are lucky to get a sample that has almost the same mean and standard deviation as the population. Therefore, the separating line is very close to the line in the first picture where the data were balanced. Notice that the standard deviation for SMOTE is always smaller because new data are added between the existing data, not outside of them. You might consider undersampling instead of oversampling. Check this link. The code for the last 3 pictures: 

Now, let's assume that the red class is under-represented (a minority class). I take 500 points for the green class, and 10 points for the red class. Then I oversample the red data by two methods. One is duplicating them 50 times, like bootstrap resampling (I color them red), and another is something like SMOTE (they are magenta). This is not exactly SMOTE. I simply added data that are in the middle between red data points. I was too lazy to calculate nearest neighbors for each observation in this simple example but it illustrates SMOTE nevertheless because in SMOTE, synthetic examples are generated inside the convex hull of existing minority examples which reduces the variance of the data. This is what I get: 

I divide the whole range were the target variable changes (0 to 1) into 10 bins. Each bin is 0.1 wide. The amount of bins can be thought of as a hyper-parameter. The more bins the closer the classification problem to the corresponding regression problem. But too many bins is probably not good. 

Each row of the array contains probabilities of putting a test point to one of three classes. I estimate a regression's analogue of by taking the maximum of these three probabilities. 

Lateral connections exist so that the update of a neuron forces the neighboring neurons also to be updated but to a lesser degree. Think of each neuron that has n inputs as a vector, a point in an n-dimensional space. It's coordinates are the values of the weights on its inputs. When the network receives an input - also an n-dimensional vector, each neuron calculates the distance between its weights and the input vector. The neuron whose weights are the closest to the input is the winner, and is allowed to update its weights. It updates them by moving one step towards the input vector. The size of the step is equal to the learning rate. While it moves, it uses the lateral connections to pull its neighbors (or push some of them away, depending on their distance to the winner and on the form of the function that you use for the lateral connections). As a result, the neighbors also move but they move less than the winner. Only the winner and its neighbors are allowed to update their weights. All other neurons don't move. The neighbors are those who are closer to the winner than a certain radius. This radius is a hyper-parameter. This type of learning allows to chart the space of the input vectors. The result is the weights of the neurons are distributed more or less uniformly in the set of the input vectors. After training, each neuron can be thought of as a representative of some region of the input space. 

Some approaches when there is a small amount of labeled data and a large amount of unlabeled data: Semi-supervised learning $URL$ - mixtures of supervised algorithms on labeled data and unsupervised algorithms on unlabeled data. One of them (label propagation) is even implemented in scikit-learn $URL$ Active learning $URL$ - algorithms that actively choose the data from which they learn, so they can achieve better performance using less labeled data. These two approaches are complementary. Therefore, there are combinations of active + semi-supervised learning algorithms.