Yes it can and you cannot do anything as you have no control over system process in terms you cannot kill it. 

You are correct the last backup time is showing while last copied and restored are much greater and . So I believe somehow the catalog is not getting updated. There can be two reasons: 

You have a odd system. Your database size is 350+ G and you have 32 bit system I would say this is a system which I would never like to have in my environment. Its very difficult to manage 350 G database on 32 bit SQL Server which has VAS limit(by default) of 2 G. You are bound to face memory pressure going ahead. AWE in 32 bit system only allows SQL Server to use memory beyond its VAS capabilities and that too only data and index pages can take advantage of extra memory by AWE. Plan cache, procedure cache and other caches cannot utilize this memory. 

Can you please use Ola Hallengren Maintenance solution for index rebuild please also read the FAQ's I am sure it would help. Online rebuild do produces lots of logs as it is fully logged in full recovery model from SQL Server 2008 onwards. Rebuilding an index will update statistics with the equivalent of a full scan. Do not update statistics on indexes that have just been rebuilt. Paul has some information on the same in this Blog Basically you are rebuilding all indexes whether it is fragmented or not and then updating stats for all the columns whether stats is outdated or not this is sledgehammer approach and IMO this is what causing the resources to be utilized to maximum. If you use Ola script it would only rebuild index which is fragmented and only update stats for column which requires updation this would reduce overall amount of work that needs to be done by SQL Server 

Aaron already pointed out and I am going to emphasize on that, starting from SQL Server 2012 the SQL Server express database engine can consume more than 1G of memory. The BOL information is not updated one it still says memory is which is incorrect. As per this support article 

Second to what Aaron mentioned. You can use DBBCC shrinkfile command for database in logshipping but make sure you dont use TRUNCATEONLY option. If you use this option Logshipping would eventually break. Its better to use NOTRUNCATE option when shrinking Its quite common scenario for log file to grow because of some huge index rebuild or huge delete operation. In such case log file increased because it required space to log information. If you shrink it, again it would grow next weekend when the delete operation or index rebuild job runs. So whats the gain in shrinking ? Instant file initialization is not there for log files( it works little differently for Tempdb log file). When information on log file is being written space is zeroed out first and then information is written this could be a performance bottleneck because whole operation has to wait for information to be written in log file first. More about Instant File Initilization 

Please read about stopat command Make sure you don't forget to test backup validity before restoring just in case. 

PS: It is always recommended to move other applications on different machine (if possible) and let SQL Server run solely on it own system this will help SQL Server run faster and better 

As already pointed SQL Server 2008 R2 SP2 is not fully supported. As a general rule after launch of new service pack within one year previous SP becomes unsupported. Following is difference between mainstarem support and extended support taken from here Think of Mainstream support as “normal”. In other words, mainstream support means Microsoft supports a product with its full offerings including paid incident support, hotfix support, security updates, etc. When a product enters the Extended Support “phase”, the game changes: 1) We still provide security updates at no charge to all customers 2) You can still call CSS or create a case online per the normal support offerings (pay per incident, Premier, etc) But…you cannot obtain a non-security hotfix from Microsoft for no charge. In order to obtain a non-security hotfix, you must purchase an Extended Hotfix Support Agreement. Extended Hotfix Support Agreements are available for Premier customers. Contact your Technical Account Manager for more information. The extended support might involves more cost than normal support and is not available to everyone. If your firm has premier support tie up with MS then you can get extended support eaisly otherwise its costly and bit difficult. This is as per my region MS support/licensing is complex and you should speak to licesning expert to get more information about extended support. MS says that CU should be only applied if user is facing bug which is fixed in that CU. If not Microsoft recommends not to apply it. SP's are more throughly tested and more reliable as compared to CU. So I would also say unless you face issue dont go for CU.If you want help in deploying SP please refer to This Link(its for SS 2012 but process/steps remains same) 

Place all the Sp and CU on one location after extraction and give the location in For example if location is C:\SQLServerFixes the command would be 

The backup strategy is to make sure you have valid backup when needed and using that you can loose as less data as possible, there is hardly a strategy for making backup size less unless you use data compression or backup compression. 

You can enable , both are undocumented so please use it on test environment, and see what interesting message is dumped in SQL Server errorlog. Something Like below would appear 

Full backup does nothing to transaction logs in any recovery model. In full recovery model when you take transaction log backup then it truncates the log. Please note if a long running transaction is still there holding the VLF's or as per Brent's explanation still needs the drawers other transaction cannot re-utilize the drawer or in technical terms it would not be truncated so that it can be re-utilized. It also does not shrinks the transaction log. For shrinking the logs you have to use command 

What if a explicit DML command which is updating numerous rows is running in your database. What is many such commnands are running. You would end up with many SPID's rolling back and eventually will leave you waiting for them to complete. Remember a rollback can take thrice as time as the transaction to complete. So choose steps carefully. Instaed I would like you to first inform all users and application users that you are going to restore the database. So that they would either shutdown application or do not perform any transaction through application. After you have made sure connection is minimal you can put database in single user mode like below. You dont need to specifically put it into multi user mode because after restore finishes it will be in multi user mode. 

Changing recovery model to bulk logged and doing bulk logged operation would make you loose point in time(PIT) recovery so if you are concerned about PIT recovery dont do that. Instead rebuild index through intelligent script which only rebuild fragmented index like one Is Ola Hallengren index rebuild solution please note that if Index is rebuild with full scan for that index stats is already updated with rebuild process. If you do heavy DML please break it into batches so as not to explode log files. 

This basically means there was meta data corruption and checkdb tried finding out page ID and index information but was no able to, so got the value as zero. 

COM objects Extended stored procedure Memory allocated by linked servers (loaded in process ) or other Dll’s loaded in SQL Server process Memory allocated by SQL Server memory manger if the allocation size in greater than 8K and need’s contiguous memory (Multiple_pages_kb). SQLCLR 

There is no direct DMV which will tell you I/O for specific table. I mostly use sys.dm_exec_query_stats DMV which has useful information about query which is taking high Physical and logical reads. Looking at query you can see tables involved if query has multiple table it wont give accurate information but you would know what all tables generate most I/O requests 

Answer is simple you did not restored a full backup first. Differential backup is only valid from latest full backup because it has information about extents that has changed after full backup has been done. So unless you take full backup and restore and then restore a differential backup it will keep on failing with the error you got. 

There is nothing wrong in actually starting from SQL Server 2012 this new feature has been introduced in SQL Server 2012 DBCC MEMORYSTATUS output. Below is output of memorystatus on my system 

Yes very much and believe me its not a hassle it will save you from lot of hassles. As already pointed you are running on unsupported version of SQL Server. Let me tell you personal experience, suppose you face any issue which comes eventually as a MS bug and you plan to ask MS to pay you for your loss you cannot because you are running unsupported version. The legal agrrements are little more complicated and I will not talk about it. Also even if you raise a case with MS for some support related activities MS engineere will straight away say its unsupported version first please apply SP2, YES not even SP1 SP1 is also not supported, then only they will proceed further, after you have applied SP2. There was a critical bug fixed in SQL Server 2012 SP1. 

Uninstall the existing SQL Server and all the components from the add remove program. Backup the registry Delete the following keys using regedit: --HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Microsoft SQL Server --HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\MSSQLServer Go to HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Uninstall and delete all the sub-keys referencing SQL Server. Go to HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services and delete all the keys referencing SQL Server. Rename all the SQL Server folders in the computer. Reboot the machine. 

So at least the query will start running but during runtime its quite likely the intermediate result is spilled to Tempdb making it slow. I strongly suggest you read Understanding Query Memory Grant 

Instead I suggest you to focus on column which is present in . Below code would give you query behind sql_handle 

As I can read your question you got this message previous year and one occurrence now. Keeping its mind in occurrence I would suggest its not that grave issue something running on OS sometimes requests more memory and SQL Server becomes victim. Workaround The workaround would be here to provide SQL Server service account Locked pages in memory privilege. This will avoid paging out of buffer pool but non buffer pool can still be paged out. Solution. 

Yes there is. The header of each page in cache stores details about the last two times it was accessed, and a periodic scan through the cache examines these values. A counter is maintained that is decremented if the page hasn’t been accessed for a while; and when SQL Server needs to free up some cache, the pages with the lowest counter are flushed first. The process of “aging out” pages from cache and maintaining an available amount of free cache pages for subsequent use can be done by any worker thread after scheduling its own I/O or by the lazywriter process. Having said that I would like you to read How It Works: Bob Dorr's SQL Server I/O Presentation Take the following as an example where the table is 1 billion rows. Simplified the process to discuss. 

What is output of this query. It seems like authentication mode is Windows only. . It means authentication mode is windows go to server . Right click on SQL Server select property and then click on security. Change authentication it to mixed mode by selecting radio button and restart SQL Server service again. 

Handy corruption demo script has been created by Paul Randal. These are really good and would help you a lot. Please browse below links 

You can find similar blog related to what you were asking. You can also use undocumented xp_reagread command 

Your question is too wide I am not sure a clear answer can be given but I would try. As such sitting from here I cannot tell you what could break after chaning compatibility level. Only you can find it out by running each and evry procedures,functions, views and triggers you have. Microsoft has documented breaking changes in SQl server 2008 R2 before changing the compatibility level you must go through this article. You must also read Change database compatibility level BOL document to see what could get affected after changing compatibility level. Please read complete article most of the things are documented. Did you ran upgrade advisor before you migrated database to 2008 R2 ? that should as well have pointed out breaking changes. The good thing with compatibility level is it can be immediately changed to previous value if you face issue. So you also have got hit and try method to check out 

There is no absolute logic. To define a good index, in my terms " If an index speeds up your query and makes it faster, it is a good index and you can keep that index". You have sp_blitzindex from Brent Ozar which would point out unused and bad indexes. Also you have Jason Strates index Analysis script which would come handy. I guess both of above will do the job for you. But if you like other approach, below is what I follow sometimes. Regrading and there is no absolute value from which you can determine whether index is aptly utilized or not. But what I actually do is divide user reads by user writes and then multiply it by 100 to get the percentage. NOTE: This would always not be applicable I use it as "tie breaker".