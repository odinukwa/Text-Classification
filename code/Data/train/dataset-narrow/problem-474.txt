I was just about to create a table, insert a bunch of data, create some statistics and do a test. But then I remembered this excellent blog post Erin Stellato wrote over on her blog. She's done that already. Her tests show 20% in action. You can take what she's done her and the scripts provided, test it on your environment and witness the behavior yourself. I've not experienced, seen or read anything that indicates this traceflag, or its behavior, is on by default in SQL Server 2012. I can't point you to official articles or documentations to prove a negative, though. You can see for yourself which traceflags are enabled, however, by running 

It isn't inexpensive. I don't know the price tag but you are talking large capital expense measured in hundreds of thousands of dollars. It is an enterprise scale solution that is optimized for large scale enterprise data warehouses. As such, you may find it tough to get a great answer here other than things like the link I provide below. You said you already did the research, so I won't provide you too many links other than the this one which does a great job explaining PDW. If you have the budget and enterprise scale that requires PDW, you would be much better off talking to Microsoft and asking them for discussions with reference customers. This right now isn't deployed en masse, so you will get individualized attention if you are in the market for a massively parallel data warehousing appliance. 

I see this question is a few months old, but I wanted to tell you about a great update from Microsfot if you've missed the news. On November 16, Microsoft announced the release of SQL Server 2016 SP1. Along with the normal things found in Service Pack releases, they also included CREATE OR REPLACE functionality. And, really important for your question, they've made quite a lot of the features that you can develop for available in editions other than Enterprise. Most critical for you here is the availability of the Always Encrypted feature in SQL Server 2016 Standard and even Express starting in SP1 of SQL Server 2016. You can read about that here on Microsoft's blog post. 

2.) What are some reasons TempDB could be higher? So TempDB is a database and it can have IO stalls like any other database as I just discussed. But what are some reasons TempDB can have higher reads? (not exhaustive, I welcome additions or thoughts in edits, other answers or comments) - 

You should modify the registerallprovidersip setting in conjunction with changing the time to live (TTL) for the records. Without modifying the default TTL, the registration lives too long. Sadly there is still a delay but dramatically reducing the TTL helps. Ideally you should see what you can do to get the client connections to specify multi subnet failover and not have to do this workaround. But with the TTL being lowered, you should be fine, I've done that with several clients and it all works out alright. It still won't instantaneous. You can see more about these settings working in conjunction with each other here. 

I'll often have more data and log drives and sometimes another TempDB drive. Add in multiple instances and you can run out of drive letters quickly. You can certainly get away with putting your instance level files on C:. And I do a lot of health checks for clients that were setup like that - and I never say "oh wow.. we have to fix that now" - Now if their TempDB file(s) are there, too, I'll typically have them change that. Sometimes move their master and MSDB databases as well.. But the world won't end if you don't split these things up. I think the benefit is really just keeping -your- files separate. As a DBA you should have a healthy paranoia around other roles at your company, other applications, other installations, etc. and the more you can isolate yourself from the potential for conflicts, the better you'll be. And it gives you some more options for reinstallation and recovery. So yes separate your binaries from C.. But my advice wouldn't be to go crazy on a separate drive for each instance.. 

Not sure if this should be an answer or a comment. But I've used the script here before for this type of question - $URL$ The last query Jamie uses shows the permission and if it is through a role, the role is specified. If it isn't, there is no role specified. This was written for SQL Server 2005, I've used it on SQL Server 2008 without issues. 

is a batch terminator in SSMS. Every time you have a GO (or a ;) you have told SQL Server that the batch just ended. So you are sending your commands in batches, and the batches are executed after another - not at the same time. SO in this case they are all executed serially. Really anytime you run commands they are executed serially in one connection. The bigger question in my mind is - Why are you doing Shrinkfile? I hope this isn't a production database? This is not a best practice operation to regularly be running at all, as this link describes. Preview: Shrinking your data files shouldn't be needed - unless you just cleared a ton of space you'll never grow back into, are having serious disk capacity issues or are doing something in dev - and even in these situations, there may be alternatives that are better for you. Shrinking means introducing index fragmentation. It means some expensive IO operations and there is a good chance you'll be growing back into the space eventually anyway if your database is still live and in use with inserts and updates. 

I think this may be less an ODBC challenge and more an Access challenge you are facing. The driver is more or less (with differences like when drivers like to introduce server-side cursors and break work up that way, etc) doing what the client tells it to. Access is really well known for doing some interesting things to decent databases and somewhat well designed DBs. Can you at least put your code into Oracle in stored packages/procedures and then just call those from the forms app? Are you 100% married to Access?