3750 does have some internally priority on what it prefers to punt when congested, but it's not configurable. So you should rely on common best practices, that is on all your network edges you should have iACL (infrastructure ACL). In iACL you'd allow UDP highports, ICMP to infrastructure network addresses and drop rest. This way ping and traceroute work, but infrastructure cannot be attacked. iACL should be complemented by policing the allowed traffic to small acceptable rates. This way when external party is attacking addresses on your 3750, it'll be dropped by network border in the edge. iACL usually is 100% static so it's low maintenance, as it'll only include infrastructure addresses (loopback, core links). This will still leave wide open cases where your router is facing customer LAN directly, like when LAN is 192.0.2.0/24 and 3750 has 192.0.2.1 then usually 192.0.2.1 would not be covered by iACL and can be attacked. Solution for those devices is either to invest on device with proper CoPP capabilities or maintain dynamic iACL always adding the router's customer facing address there. If you only face customers via link-networks (/30 or /31) solution is much cleaner, you just omit advertising the link-network and add static /32 route for the CPE side, this way external to this router parties cannot attack the router, as they won't have route. Alternative solution to same issue is to use non-continuous ACL entry in iACL, if your CPE link-network is 198.51.100.0/24 in iACL you could do 'deny ip any 198.51.100.0 0.0.0.254' then all the even addresses would be allowed and odd addresses denied, so if CPE is even and 3750 is odd, all current and future links are protected without updating iACL. 

I feel your own example probably should work, at least I couldn't immediately think of why not, so might be bug. Review CCO Document for the command, namely: 

Now analyze results to determine how often on average the counters are actually being updated. (I can produce script for the analysis if needed) Then comes the part where we'd need math, but I'll suggest one naive solution. If your update interval is 10s, poll box every 5s, i.e. twice as often as it is updated. Then your samples would be t0, t5, t10, t15, t20, t25, t30 Now this would be your raw data, which you wouldn't use, but you'd rather recover actual samples from it like this 

I think tshark can cope with cooked capture today, I rarely if ever use tcpdump anymore. Back when tshark couldn't cope with this (when I was capturing ERSPAN) I wrote script which can pop N bytes out of each frame, quite useful also if you're tunneling over something which is not recognized. 

Only reason this VLAN is even created is because either your VPN-CAM is turned off or you have more than 512 VRFs. If you don't have 512 VRFs, you can free up this internal VLAN without removing VRF by making sure VPN-CAM is activated. This may not be the simplest task as there are various reasons which mean VPN-CAM is disabled, most common reasons are: 

It never really occurred to me, but the way you suggest would make more sense than the way it works. Now it just inherits the server which it should query from 'commands level' and documents are unclear which level, as they may be different, but I guess it would be level 15. 

Think of RED as an arbitrary curve in a coordinate system. Y is probability of drop X is how congested you are (for example how full egress buffers are) Operator could add 5 points there 

use on-band (same routes as production INET) use routing-instance (you'll get routing separation, but use same copy of RPD, but I guess you're using FXP0, which is not supported here) use logical-system, another copy of RPD is ran just for MGMT 

To add to good answers given by @generalnetworkerror and @MikePennington Both pps and bps reported in datasheets are idealized numbers, not only is bps often double counted (the double counting comes from the fact that to cater 10Gbps interface, you'll need 20Gbps of memory bandwidth, so in this context it's fair, but may be confusing to buyer) like Mike explained. But they, especially pps are also idealized to a scenario of vendor's definition of 'typical', the scenario has much less affect in switch-like devices (Cisco catalyst, Juniper ex, Force10, Brocade) as they tend to run in constant-time ASIC type devices for lookup. And it tends to have more effect to router-like devices (Cisco ASR9k, Juniper MX, Alcatel SR) as they tend to run NPU, which is close to normal CPU in design, and it'll take variable time to perform work. This inherent feature is exploited when vendors buy 'verified by 3rd party' tests, like Cisco might pay Miercom to test Cisco+Juniper and Juniper might pay EANTC to test Cisco+Juniper. These EANTC and Miercom engineers are given inside information for both platforms and they use this inside information to show how one platform (of paying customer) out-performs another platform. Because they choose test-cases which target compromises in the idealized scenario chosen by that vendor. Luckily rarely will in switch-like device will pps or bps become an issue to you, it's far more likely you'll be bitten by for example micro-bursting (consequence of small buffers) before even close to platforms bps/pps limits. More typically pps and bps affect you in low-end boxes running COTS CPU's, i.e. software-based boxes, like Cisco ISR, Juniper SRX branch or firewalls. In very generic and rough terms, bps measures memory bandwidth and pps measures lookup performance ('CPU' speed) 

To conserve memory at destination, it was not important to micro-optimize forwarding path in the past. This is quote from RFC4456: 

CEF is Cisco's word for their FIB. When in L3 switch you do 'sh ip cef', none of this information is actually used to push the packets at all, this is just software trie which is used to populate the hardware ASIC. CEF is just term Cisco uses to describe their optimize data storage/retrieval code, it is not specific technology with specific function. In most HW platforms you cannot simply run the box without CEF, as CEF data structure is needed to compile the HW specific information. Some features like MPLS also have dependency on CEF data structure and thus won't work without it. LAN (L2) switching is not abstracted via CEF, so it does not depend on CEF at all. I recommend this book for quite up-to-date information about CEF (it's written after major CEF rewrite around 12.2S) If you constrain CEF definition of IP Trie FIB, then obviously that cannot be used for NAPT, because you cannot pre-determine what is natted and where. But as explained, CEF is not specific technology, it's broader concept, and thus it's debatable if CEF is NAT feature or not, I would err to the side of it being CEF feature: 

In this option VLAN 123 that comes from Router1, gets VLAN 42 on top of it [ 42 123 ], MAC addresses from ALL Router1 VLANs are populated in VLAN 42 mac-address-table. So then MAC lookup is done against VLAN 42 where we only have traffic-manager, once we send the frame out to traffic-manager, we pop VLAN 42 out. Now after traffic manager send it OUT, again in VLAN 123, it gets VLAN 43 on top of it [ 43 123 ], and as previously MAC lookup is done for table 43, where we only have Router2, frame is sent out towards Router2 and VLAN 43 is popped out. 

Yes both LSR[12] could advertise given FEC, say 10.0.0.1/32 with label 10 to each other. Then if IGP says to LSR1 10.0.0.1/32 egress interface is towards LSR2, it'll impose (or swap to) label 10 and send towards LSR2. LSR2 then will find egress interface being something else than towards LSR1 and swap label to what ever that direction has advertised, might be still label 10, or might be something else, does not really matter at all. Labels are completely local today and some RFCs dictate that is how it should be. Personally I'd like IGP labels to be global for simplicity. Because MPLS LSR does not know how labels look from anyone else's POV, we need hacks like tLDP (targeted LDP) when implementing rLFA (remote loop free alternative). We need the tLDP to learn bindings of remote node. With regards label scope, label space today is chassis-wide in every device I've ever seen, but standards fully allow per-interface label spaces. 

Ability to retain label history in-transit Low byte overhead (TTL, TC are redundant in stacked labels) Remove need for transit P 'duck-typing' MPLS payload (breaks ECMP today) Extendable by design (special purpose labels introduce huge byte-cost) Increased label space Co-existence with MPLSv1 

C will program FIB entry, so that label 100 points to interface towards E, and MPLS label operations 'SWAP 0' if explicit null, or 'POP' if implicit null C will program FIB entry, so that prefix 192.0.2.5/32 points to interface towards E, and MPLS label operation 'PUSH 0' if explicit null 

(Original all-BGP answer changed, as-per OP's request of no dynamic routing to ISP) As @Mike suggested, tunnel+dynamic routing, especially in the INET facing is solid solution. But if for some reason you don't want tunnel (no HW support, don't want to lose MTU, dynamic routing not acceptable even internally) your other option is 'IP SLA' tracked routes. I'll explain how it helps in the exact problem you describe, where branch continues to return traffic to INET, while it should use the direct link. In branch you'd have: 

IGP usually is OSPF or ISIS which are link-state based, this gives us all the information of the network, everyone knows network from everyone's point-of-view, which allows for very interesting convergence options and traffic engineering options. BGP is essentially distance-vector, it knows very limited view on the network at whole. BGP handles very well filtering and modifying routing information. Link-state protocol is quite expensive compared to distance-vector, it would be quite problematic to scale it to INET DFZ size. So reason why we have both, is because inside one specific network, we have sufficiently low-complexity to handle it with link-state protocol, which allows us to gain all the advantages of high degree of knowledge of network. But as it does not scale to Internet size, we need some other network to connect these many link-state islands. You could inside your own network carry all prefixes (including customer) in your IGP, but it will negatively impact IGP performance, while all the convergence and TE advantages can be gained by just carrying loopback addresses of core routers. Adding customer prefixes to IGP only hurts your network performance by making IGP unnecessarily complex. 

No it's not possible today, because neither CSCO or JNPR have implemented LDP to distribute labels for IPV6 prefixes. In my opinion people have too sentimental view on this matter, thinking IPV6-only is value in itself, it's not. You should differentiate services you offer from your control-plane used to offer those. There is no particular reason to have dual-stack control-plane, it's just additional complexity. Personally, I'd be happy to run IPv4 + 6PE until in some far future I'll fork-lift or build new network with IPv6 + 4PE. Running MPLS for IPv4 and native IPv6 is probably the silliest thing you could do, as you'll remove all the scaling, TE and convergence benefits you get from MPLS. Segment Routing/SRING which uses IGP for labels instead of LDP will likely happen before LDP for IPv6 prefixes happen. So if you really for some strange reason want to have both control-planes that's your most likely source to get it soon. 

Now if HQ is not reachable via INET, tracking should fail, and INET more-specific route should be invalidated and you should fallback to next best route pointing to the direct link. 

L2VPN is generic term for group of technologies out of which one is VPLS, but could be number of other technologies such as emerging EVPN. For more information read WG charter/mailing lists as L2VPN has lot of active work going on. Also read RFC6624 and RFC4761 

Question seems to be about QoS not VPLS. You are thinking correctly, always when your interfaces actual rate is less than physical rate, you need to configure shaper. Without this sub-rate shaper, QoS configuration cannot work, as you cannot know when to start dropping packets, as the router will think you have 1GE available capacity and will not realize you're congested at 300Mbps. So you'd configure parent shaper of 300Mbps and under this shaper you'd configure your QoS policy. This config should be in physical interface. If you have multiple VLANs, you need another hierarchy level, that is you'll first give physical interface 300Mbps, then distribute this as burstable percentages to different vlans, then apply to each of these vlans their own QoS policy. I would start simple and work up. I.e. 300Mbps shaper in physical and QoS policy in the physical only, treating all VLANs as equal. If you'll specify platform, I can give example of HQoS config. IOS example could be: 

You're right, you wouldn't really see the burstiness easily on SNMP. 1GE can send 1.48Mpps, so it takes very very little time to congest the the 45Mbps, which can handle less than 75kpps. If your ingress is 1GE and egress is 45Mbps, then obviously the congestion point of 45Mbps will need to drop packets. This is normal and expected. If you increase buffers you'll introduce more delay. 1GE takes 0.45ms to send 40 1500B IP frames, which is right now the amount of burst you can handle. However dequeueing them on the 45Mbps already takes 10ms. If you don't have any acute problem, I would probably not do anything about it. But if some traffic is more eligible for dropping than other, then you should replace FIFO with class-based queueing. Say maybe you want to prioritize so that more ftp is dropped and less voip. Then it'll also make more sense to add more buffering on the ftp traffic, as it's not really sensitive to delay. If you want to try your luck with deeper buffers, something like this should suffice: 

dBm is relative change to 1mW of power, i.e. 0dBm is 1mW by definition. And 3dBm change is about half or double of the original signal strength (10^(-3/10) is 0.5mW and 10^(3/10) is 2mW). So in the example above, my -64 dBm is 10^(-64/10) or 0.4nW (yes nanowatt, it's cray cray how these things work) If we want to translate arbitrary mW to dBm we use log10, i.e. 2mW is 10*Math.log10(2), which is 3.