Download the DirectX SDK and fire up the sample code that puts a rainbow colored 3D shape on screen. Try making it draw more shapes. Try making it so that big shapes can shoot little shapes out of them. Try moving the shapes around. Try moving the camera around. You can have a fully playable (but weird) game up and running in an evening that way. Later, you can worry about getting more sophisticated one step at a time. For example, keep the game playable while replacing the rainbow colored 3D shape with a mesh that looks like a person or a bullet or a tank. The key point is to keep the thing playable at all times. That's where the fun is, and it will motivate you to go back again and again to fix "one more detail." It's like the folk story of Stone Soup. 

In commercial game development for TV game consoles, we never store vertex positions on disc or network as floats, because if more than 8 or 10 bits of precision is needed, a model can be subdivided until each piece requires only 8 or 10 bits. So for speed of download or loading from a disc, floats lose out to fixed point. As for speed of rendering, this is an issue only when vertex shading is bottlenecked by vertex buffer read bandwidth, which is actually not likely unless your vertices are really fat beyond just the position being 3x32 bits. 

Trademarks are not copyrights or patents. Trademarks are designed to protect consumers. The idea being that a trademarked M4 carbine, for example, is made by Colt Defense (or someone they have licensed to build them) and not by some other company making cheaper versions. The OEM can stop the cheaper company from calling its product an M4 but can't stop it from making the copies. Stopping manfuacturing can only be done through patents. Recently, there has been a dispute between Ford and the Ferari F1 racing team over the name F150. Although the two vehicles could never be confused, this was done primarily to keep hold of the trademark, not enforcing the trademark is one way of losing ownership of the trademark. There are many other things that appear in video games that have trademarks, are they all licensed? Why should the M4 be special? Do other media pay licenses to use the trademark. For example, if a James Bond novel had a description of someone hold an M4, would that require a license? You are not selling a gun. Trademarks can only be enforced against similar products. A game is not a gun, in the same way it's not a drink, shop, car, etc. In the USA, you can claim the First Amendment (free speech) and fair use. 

and movement is always towards a square with a lower value. When the player places a tower, update each of the eight adjacent squares: for each square, set it's movement value to one more than the lowest adjacent value. If the value changes, repeat the process centred on the updated square. Then, to check that the route to the exit is not blocked, ensure all squares are adjacent to a square of a lower value. When the player removes a tower, set the movement value to one more than the lowest adjacent square and repeat the process above. A simpler approach would be to re-do the flood fill. 

This is almost the simplest "vertex shader" one can conceive: in goes a vertex position from memory, and out comes a new vertex position, half-as-big. The half-as-big vertex is not stored back to memory - it's used immediately for rendering and then thrown away. Though a gross oversimplification that lacks key concepts, this in spirit describes aspects of how movies do graphics. Interactive graphics (games) can't afford to be this simple, because they need to render graphics several orders of magnitude more quickly than a movie. In games, we can't afford to render one point for each pixel on screen, plus extras to cover the gaps. So as a compromise, the gap between each three nearby points is rendered as a triangle, which for various technical reasons prefers to be at least 10 pixels big on the screen. A 3D triangle can be projected onto a 2D screen, then divided into a stack of 1D lines, which can each be divided into a stack of 0D pixels. We can thus divide-and-conquer the problem of rendering a 3D triangle into the simpler problem of rendering a lot of 0D pixels in isolation. A computer can solve simpler problems in less time. A pixel shader is a little program that we run on each pixel so generated from disassembling a triangle. 

it does memory allocations whenever you add a new item. memory allocations are slow. it's full of pointers. pointers are bad. a pointer is a cache miss. a cache miss is bad. 

where D is a direction vector and Dsd' is the required direction at time t. Now, work out the direction of the turret based on current position and maximum velocity and acceleration for a given time t:- 

If you're doing this in your spare time, I would suggest trying to write the game using several different engines. That way, you will get a good feel for what each engine can / can't do. As this is a turn based game, I would definitely start out with something having very simple graphics and probably done as static web pages and then put all your game code on the server, using .Net MVC perhaps. This means that you only have one application to worry about, the server code, and a local DB connection to manage. Once you've got that working, you can then look into doing some client side processing - animations, fancier UI, etc. You can try out the various client systems you mention to see what works best for you. 

The reason for the is because you calculate the angle from end point to the start point. If startX was 5 and endX was 10, the offX is -5. Which is counter-intuitive. Swapping start and end in the calculation of off would remove the need for the -1 in the calculation of p2. But then, why do you need the angle. Knowing a bit of vector maths would be really useful: 

You probably only want to do (4) and (5) for a few frame to simulate friction, e.g. repeat (4) for each preceding (2). 

OpenGL's ambient light is a single color, but ambient light in the real world is usually brighter from above and darker from below. You can approximate this in OpenGL by setting the ambient light to very dark blue, and adding a dark blue directional light pointing downward. That, plus a bright white directional light coming from wherever the sun is, can approximate outdoors light pretty well. What you really should do is write a shader that calculates light per pixel, but getting there from where you are is going to take a little time. 

Procedural texture suffers from the problem that an Art Director can not reliably point at the work of an Artist and say "please make that part a little more X." because the procedural shading system may not support X cheaply or at all. For example, a brick shader may support clean brown brick, but may not support brick that was painted with an advertisement 80 years ago and graffiti 10 years ago. Or it may not support having one purple brick out of 1,000 brown bricks. In exactly that one spot, because that spot appeals to the Art Director's taste. A real texture can support all these things of course, and in this sense a real texture is superior to a procedural texture. The procedural texture exerts artistic control by virtue of its preference for certain use cases over others. A real texture exerts little such control. GPU hardware, however, has a strong preference for proceduralism, because texture memory is so many cycles away from the ALU units that do the shading. 

Ideally then, you would define an interface to your data loading module and then create multiple versions, each tailored to specific needs. On one project I worked on many years back, loading level data off CD was taking far too long (off HD wasn't great either). So I came up with a the following. I had three methods for loading level data:- 

I'd probably go with 1) as there's not much graphically going on, there may be some self-generated code to blit and clip images at the edges. One possible technique that a collegue of mine was working on back then was self-writing sprites, that is, the sprite data wasn't data, it was code. This meant there were no transparency checks and the data read of the blit was effectively free (this was on a 386 where each instruction was read and then decoded so instead of read code->read data->write data it was just read code->write data). It worked amazingly well - we got lots of huge sprites on multiple parallax layers running at 25fps+. But we are talking about Romero here and there's probably a bit of exageration going on about the techniques. 

This works because the sqrt gives the length of the vector from start to end and dividing the vector start->end by the length normalises the vector (makes it length one unit) and then multiplying by 100 gives a vector of length 100. 

As for games like GT5, they are probably not licensing the trademark, and probably couldn't license it as they aren't building cars. They are probably licensing the copyright on the names and logos. 

Computers traditionally represented each pixel on screen as only 24 bits in memory: 8 for red, 8 for green, and 8 for blue. This is almost enough bits that a human wouldn't notice if you added more, and the 8-bit byte is very convenient for microprocessors, so that's what stuck. While 8 bits is almost enough precision to display an image, it is definitely not enough precision to compute an image. At various points while computing an image, at least 32 bits of precision is required. This is why pixel shaders compute colors in 32-bit precision, even when you are rendering to an 8-bit precision image. Otherwise, you couldn't for example divide a value by 1000, and then later multiply it by 1000, because dividing any 8-bit value by 1000 results in zero. There has been a trend in realtime 3D graphics toward keeping all graphics in >8 bit precision until the last possible moment, at which time the >8 bits of red are downsampled to 8 bits, and so forth for green and blue. HDR refers to the act of rendering to images which have higher than 8-bit precision. In contemporary TV videogames, 16-bit precision is the norm, and this may be "enough" in videogames for years to come. 

PVRTC 2BPP encoding, as introduced in this paper divides an image into 8x4-texel blocks, and compresses each block such that only two RGB colors are stored for each thirty-two texel block. None of the thirty-two texels stores a color of its own - each texel stores only information about how to blend between the two RGB colors of its 8x4-texel block. If the source image has an 8x4 texel block with a rainbow of 32 colors in it, PVRTC 2BPP compression will do a very poor job of maintaining its quality, because in PVRTC 2BPP each 8x4 texel block has only two RGB colors to blend with. 

where P is position and V is velocity and the subscript is d for destination (target) and s for source (turret), which gives a direction vector:- 

Why use one method of data storage? In most cases, the game you release to the world does not need the same data storage functionality that the game needs during development. Here's a comparison of some release / development requirements:- 

To generate face normals, use the vector cross product of two edges each triangle and then normalise the result. Make sure you get the directions right, otherwise some normals will point in and some out. Vertex normals would then be an interpolation of all the face normals the vertex is attached to. 

The original Worms probably didn't do anything more complex than bitmap/bitmap intersections - i.e. does the bitmap of the grenade intersect with the collision map of the world. Moving the grenade (or whatever) would follow these steps: 

First off, calculate the vector from turret to target. Then compare this with the turret's current vector. Then use the difference between the two to work out the angular acceleration and angular velocity required to get the turret to turn to point in the right direction within a given time. OK, that seemed simple. However, you should really try to anticipate the target's position since the target is going to move by the time you've turned the turret. To do this:- 

Brute force: just draw the scene Mode-X and panning registers. Draw the bit to be scrolled into view and adjust the panning registers to scroll the scene. You would need to redraw the top display area each frame, but that is less work to do than drawing the main play area. You wouldn't need to redraw the bottom area as there was a register in the hardware that would cause the video DACs to read from address 0 at a given scan line (so the bottom area would be at address 0 in video ram and the top would start after the bottom area)*. 

You seem to have had enough data to produce the handy animation above. Your simulation may need more accuracy than provided by my solution: For each frame of your animation above, record the pixel positions of the centers of each star. Enter these values into two arrays in your program. For a given time t, find the corresponding four consecutive entries in each array and do a bicubic filter on them to produce the position of each star. 

Devoting a texture alpha channel to team color makes DXT textures 2x bigger, so I would rather use chroma key to calculate team color. Designate one color (e.g. green, blue, magenta) as the "reserved color" in your textures. When the engine boots up, make a matrix for each team color that rotates the "reserved color" onto the team color. Just before drawing a team object, set this matrix as a pixel shader constant. In the pixel shader, transform the pixel color by the team color matrix. Lerp the untransformed pixel color with the transformed pixel color, by an amount proportional to the angle of the pixel's chroma to the "reserved color's" chroma. This will cost you some ALU instructions and trig in the pixel shader, but that's a lot cheaper than bloating up your texture cache with textures 2x as big. You also have the ability to have multiple chroma keys (e.g. magenta and cyan get mapped onto a primary and secondary team color) without devoting one texture channel to each color.