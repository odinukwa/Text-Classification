It depends how you define simplest. One of the simplest languages I know are Iota and Jot. A detailed description can be found here. Both are Turing-complete languages that use just two symbols (no variables etc.). Iota programs can be viewed as binary trees with $*$ at nodes and $i$ at leafs. So the whole program is solely determined by the shape of its binary tree. Any such a binary tree forms a valid program. Jot is similar, but slightly different. Programs in Jot are (arbitrary) sequences of 1's and 0's. Any such a binary sequence forms a grammatically valid program. 

(This is because if a term is typable in System F then all its subterms are.) Is there a simple proof the other way around? That is, a proof that typability implies type checking in System F? 

You've basically answered the question yourself. $\lambda K$ is just another name for the standard, untyped lambda calculus. $\lambda I$ is a strict subset of $\lambda K$. $\lambda I$ doesn't allow terms where one abstracts over a variable but doesn't use it. So $$K = \lambda xy.x \in \lambda K$$ but $$ K \not\in \lambda I$$ Thanks to this restriction, $\lambda I$ has some interesting properties, in particular if $M$ has a normal form then so do all its sub-terms. Barendregt, H. P. The Lambda Calculus: Its Syntax and Semantics contains some notes about $\lambda I$, namely: 

While not exactly what you want, esoteric languages Jot, Iota and Zot could be good starting points. They are all Turing complete. In particular Iota language is defined as 

It's not entirely clear what do you mean by a functional programming language without closures. Can you give an example? Functional programming languages are usually based on lambda calculus, whose essential part is that you can have open lambda terms. For example the term for the constant function (the K-combinator) $\lambda x . \lambda y . x$ can be viewed as a function that given $x$ returns a constant function that returns $x$ on any argument - a closure of the open term $\lambda y . x$. However you can use another basis, a combinatory calculus such that it's power is equivalent to the lambda calculus. Then you can take a lambda term and convert it into an equivalent combinator that doesn't use any variables at all, so there are even no closures to talk about - see Completeness of the S-K basis. Which I believe answers yes to your question. This is actually what Haskell compilers do under the hood. Evaluating lambda terms with variables is very inefficient and cumbersome, so they convert the program to a representation without variables and use techniques such as combinator graph reduction. I can recommend two books on the subject which are both available online, the first one more theoretical, the second one focused more on the actual implementation techniques: 

Most resource regarding categorical notions in programming describe monads, but I've never seen a categorical description of monad transformers. How could monad transformers be described in the terms of category theory? In particular, I'd be interested in: 

Suppose we don't know Joe B. Wells's result from 1994 that both typability and type checking are undecidable in System F (AKA $\lambda 2$). In Barendregt's Lambda calculi with types (1992) I found a proof due to Malecki 1989 that type checking implies typability. This is because 

The Wikipedia page for Monad says just that for a monad $(T,\eta,\mu)$ we can define the category of all adjunctions that define the monad: 

The name coroutine suggests that in some sense they should be dual to (sub)routines. Is there a real mathematical duality? I'm hoping for something like "in category theory subroutines are X and coroutines are Y, where X is dual to Y". 

If we have a normalizing system, such as System F, then we can decide $\beta$-equivalence "from outside" by reducing the two given terms and comparing if their normal forms are the same or not. However, can we do it "from inside"? Is there a System-F combinator $E$ such that for two combinators $M$ and $N$ we have $E M N = \mbox{true}$ if $M$ and $N$ have the same normal form, and $E M N = \mbox{false}$ otherwise? Or can this be done at least for some $M$s? To construct a combinator $E_M$ such that $E_M N$ is true iff $N\equiv_\beta M$? If not, why? 

What are the morphisms of adjunctions in $\textbf{Adj}(C,T)$ and what is meant by ... which are the identity on $C$? 

Any monad is also an applicative functor and any applicative functor is a functor. Also, any comonad is a functor. Is there a similar concept between comonads and functors, something like co-applicative functor, and what are its properties? \begin{array}{c} \end{array} \begin{array}{cc} \mbox{Functors} & & \mbox{Functors} \\ \uparrow & & \uparrow \\ \mbox{Applicative functors} & & ??? \\ \uparrow & & \uparrow \\ \mbox{Monads} & & \mbox{Comonads} \\ \end{array} Update: I'd be also interested in possible uses of such a concept. 

To clarify, I'm looking for dictionaries with space requirement as a function of the size of the universe, and not the number of elements in the dictionary. 

This problem was studied in the following paper. Abdol-Hossein Esfahanian, S.Louis Hakimi, On computing a conditional edge-connectivity of a graph, Information Processing Letters, Volume 27, Issue 4, 1988, Pages 195-199, ISSN 0020-0190, $URL$ The idea is identical to Saeed's method, contract pairs of edges and find a min-cut. However, one can be more careful and show $O(m)$ pairs are sufficient. 

Let $y_i=\min\{b_k| i\in J_k\}$. Observe for any feasible solution, $x_i\leq y_i$. Claim: The system of $\max$ equations is feasible iff $x_i=y_i$ for $1\leq i\leq n$ is a solution. Proof. If $x_i=y_i$ is a solution, then the system of $\max$ equations is feasible Consider any solution, and $x_i < y_i$ for some $i$. We can increase $x_i$ to $y_i$ without violate any equation. Assume it violates the $k$th equation, namely $\max\{x_j|j,i\in J_k\}=b_k$, then it implies $y_i = \max\{x_j|j,i\in J_k\} > b_k$, but that's a contradiction because $$ b_k \geq \min\{b_j| i\in J_j\} = y_i = \max\{x_j|j,i\in J_k\} > b_k $$. The algorithm is just compute $y_i$s and check if it satisfies all the equations. This takes linear time. 

Let $d^+_G(x)$ be the in-degree of $x$ in graph $G$. Theorem (Lov√°sz 1973): For a directed graph $G$ and a specified vertex $r$, there exist a subgraph $G'$ with the property that $d^+_{G'}(x) = \lambda(r,x,G') = \lambda(r,x,G)$. The desired sparse graph exists, as we can keep removing edges to reach a minimal graph with the desired connectivity property. 

Let $S(X) = \{\sum_{i\in Y} i | Y\subset X \}$, the set of subset sums of $X$. $S_n(X) = S(X)\cap \{1,\ldots,n\}$. Consider the following variant of subset sum. 

For a graph $G=(V,E)$ with $n$ vertices and $m$ edges, a subgraph of $O(kn)$ edges is an $r$-rooted-$k$-sparsifier if it preserves the local edge connectivity from $r$ to every other vertex up to $k$. Namely, it is a subgraph $G_k$, such that $\lambda(r,x,G_k)\geq \min(\lambda(r,x,G),k)$ for all $x\in V$. Here $\lambda(x,y,G)$ is the maximum number of edge disjoint paths from $x$ to $y$. For undirected graphs, Nagomochi and Ibaraki shows such graph exist and has an algorithm to find a $r$-rooted-$k$-sparsifier in $O(m)$ time. In fact, it finds a subgraph preserves all local edge and vertex connectivity. Are there similar results for directed graphs? Or are there a proof that a $r$-rooted-$k$-sparsifier cannot exist for some directed graph? 

S.A. Curtis, Darts and hoopla board design, Information Processing Letters, Volume 92, Issue 1, 16 October 2004, Pages 53-56, ISSN 0020-0190, $URL$ 

Let $f$ be the running time of calling the oracles, and assume $f=\Omega(m+n)$, then one can find the sets in deterministic $O(f k \log n~\mathrm{polylog}(m))$ time. [1] Now we can reduce the finding witness problem to $1$-reconstruction problem. Here $S_1,\ldots,S_{2n}\subset \{1,\ldots,2n\}$ where $S_i = \{a|a+b = i, a\in A, b\in B\}$. Define the polynomials $\chi_Q(x) = \sum_{i \in Q} x^i$, $I_Q(x) = \sum_{i \in Q} i x^i$ The coefficient for $x^i$ in $\chi_Q\chi_B(x)$ is $|S_i\cap Q|$ and in $I_Q\chi_B(x)$ is $\sum_{s\in S_i\cap Q} s$. Hence the oracles take $O(n\log n)$ time per call. This gives us an $O(n~\mathrm{polylog}(n))$ time deterministic algorithm. [1] Yonatan Aumann, Moshe Lewenstein, Noa Lewenstein, Dekel Tsur: Finding witnesses by peeling. ACM Transactions on Algorithms 7(2): 24 (2011) [2] Noga Alon, Moni Naor: Derandomization, witnesses for Boolean matrix multiplication and construction of perfect hash functions. Algorithmica 16(4-5) (1996) 

There is the common dynamic programming algorithm taught in algorithm classes, which takes $O(n |X|)$ time. It takes $O(n^2)$ time for large input sets. It's not hard to devise and output sensitive version that takes $O(|X||S_n(X)|+n)$ time. It's possible to solve it in $O((n \log n)^\frac{3}{2})$ time by decompose the problem to $\sqrt{n\log n}$ ALL-SUBSET-SUMS with small output size, and we can combine the solutions through FFT. How fast can we solve this problem? There are subset sum algorithms using analytical number theory[1], but they have many technical conditions on the input. It can't be applied directly on ALL-SUBSET-SUMS. Reference: [1] M. Chaimovich , G. Freiman , Z. Galil, Solving dense subset-sum problems by using analytical number theory, Journal of Complexity, v.5 n.3, p.271-282, Sept. 1989 

Given $G$, $C$ and $M$, where $G$ is a graph with maximum degree $3$, $C$ is a hamiltonian cycle of $G$, and $M$ is a matching of $G$. Let $\mathcal{N}$ be the set of all matching of $C$ with size $|M|$. We want to pick a $N\in \mathcal{N}$, such that the number of cycles in $M\cup N$ is maximized. Here we also define edges in $M\cap N$ to be cycles. Is this problem NP-hard? A cool way to visualize the problem is consider $n$ points on a circle and some endpoint disjoint chords between the points. We want to connect adjacent points with disjoint arcs to maximize the number of cycles. 

In the proof of theorem 2 in Improved Approximation Algorithms for Rectangle Tiling and Packing by Berman et al, they proved an upper bound of $\frac{11}{5} \max\{W/p,y\}$, where $W$ is the sum of the weight of all elements, $p$ is the number of rectangles and $y$ is the weight of the largest element. This implies a upper bound of $\frac{11}{5j}$ for your problem.