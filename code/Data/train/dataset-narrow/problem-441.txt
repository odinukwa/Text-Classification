That's the basics of how it works. Remove automatic failover (synchronous), install the updates/patches, reboot (in this scenario). 

The web server can't run under the account or the application context can't run under the account? For example, the web server is sitting in a DMZ? 

See the screenshot above - I think it's working as intended, at least for me... It might not be intuitive, but it works. The end result is the same. If you're not a fan of the GUI giving you grief then there is always PowerShell and T-SQL :) If you feel that this isn't working as intended or you don't like it, I'd ask you to post feedback on Connect. 

Yes and no. Yes the files are sorted in filename order which generally happens to be the same as the restore order, thus it looks like it goes in order of filename. No, because the files may be tested to see if they contain the log records for the next restore, which means their name could be slightly different and still process. Note that the filename sort is case insensitive and that filenames do matter. 

The distribution database is currently not supported for use in AlwaysOn Availability Groups. The only supported method for transparent distributor failover is AlwaysOn Failover Cluster Instances. 

No, there isn't a button to press to tell the REDO to run. It runs. Constantly. All the time. It doesn't run when it's blocked and it runs very slowly when it doesn't have the hardware to run properly (primary may be more powerful than secondary, etc.). If you look at the secondary, there should be an AlwaysOn Health extended events session running. Opening the XEL file in management studio (SSMS) will give you what has been captured. Look for any REDO BLOCKED messages. If there aren't any, run a perfmon on the system to check on IO latency and throughput, memory, and cpu utilization. 

Mirroring. If you can setup mirroring you can setup clustering and use AGs - I'm not sure why the big push to not have to use Windows Clustering. 

Yes, you can... however, it will not be readable as a secondary and if you ever fail over to it, the SQL Server 2012 instance will no longer synchronize and your databases will be upgraded. Unless you're planning on doing an upgrade, I wouldn't mix and match versions - it gets you nothing. 

You're correct on the first part and I don't know what you are considering the "usual" backup schedule. If you have the log backups, you can continue to roll forward through the bad full and covering differentials. You're definitely going to want more than a single recovery sequence. 

Then you have quite a good bit of development work ahead of you. Seriously. Everyone asks this same question, but it is not an easy one to answer with any more resolution than "It failed over at this time." Things such as virtualization make this even harder as the tools and the hypervisor itself may do things that are outside the purview of your sandbox. However, to get you started, places to scrub for data: 

Normally 1 year is pretty standard, 3 months might be expected in a more secure environment where virtual accounts can't be used. 

Remove the node from the AG if it's part of it. Remove the node from any Read Only Routing (ROR) configurations. Disable Always On Availability Groups from the SQL Server Configuration Manager/Powershell if it's enabled. Remove any clustered applications from the Windows Server Failover Cluster Node (WSFC). This includes FCI's acting as part of an AG. You don't have to remove local installs of SQL Server, but you will have to follow the renaming items by dropping and adding the server name. Evict the node from the WSFC. Rename the computer and double check the AD computer object is successfully renamed and replicated to other DCs before continuing. Join the node to the WSFC under the new name. Install any clustered services or applications removed in step 4. Also enable Availability Groups from the SQL Server Configuration Manager/Powershell. Add the node to the AG. Update any ROR configurations. 

The share allows for clients (local and remote) to have a singular shared location to use the streaming windows api for access to filestream data. This works in conjunction with the SQL Server Instance level settings for filestream access of , any other access setting should not work with the streaming API. 

I agree this should be moved to the dba stack exchange, though since I answer there as well I will post it here should it be migrated. The only way to access a mirrored database from the mirror server is through a database snapshot, which will require enterprise edition of SQL Server. It will also be READ ONLY, so this may not work for what you want. The other alternative is to take the backups of the primary (these, I would hope, should be available) and restore them for your use somewhere else up to the time you need. If using already taken backups is not possible, ask for a copy_only backup to be taken on the primary and given to you. 

They could also get an error. I wouldn't assume it'll multiplex perfectly. This is why setting the maximum value too low can have a detrimental effect (also setting it too high!). 

The reason it is most likely (but not 100%, because if this is a VM it could have been over subscribed, paused for a snapshot, paused from migration, etc.) is due to the information later in the cluster log: 

It is and also isn't related to Always On. Always On Availability Groups requires that the database be in the full recovery model - which is why you need log management in the first place. So long as your secondary replicas are staying very close to the primary (i.e. no lagging secondary replicas for hours or days) log management shouldn't be affected very much due to AGs. 

Removing and re-creating is going to be your best bet in the current implementation... though I'd argue the need for a database in a Distributed Availaiblity Group which needs to be refreshed each day. The only other option is to fail the Distributed Availability Group over after removing the database from the global primary so that the forwarder becomes the new global primary. Then you will be able to remove the database from the (now primary) second availability group. That's less appealing of an approach due to the connections being killed, versus the remove + add approach you already have which will keep connections to the current global primary. 

That defeats the purpose of using Always Encrypted if the server that is running SQL Server also has the certificate to decrypt all the data. Please, please, please don't do this! Put it on the client machine or a "tools" or "intermediate loading" machine to get the data in and out. Then remove the key and keep it only on the client machines that are locked down. I'd also keep, in the back of your mind, a solution for when a client machine is lost/stolen/breached (laptop), or a server is compromised. How do you find out, how long do you have to rotate CMKs, do you know how to rotate CMKs, etc. 

When it's not longer needed by recovery (and/or, depending on recovery model) when it's been successfully backed up. See the log architecture link, above. 

Restore the full backup, latest differential, all logs. Restore the full backup, all transaction logs. 

Your table doesn't start at page 0 of file 1, however that space in indeed occupied and used. This page is reserved and called the file header page, each file will have one of these. Additionally there are GAM intervals which happen roughly every 4 GB, PFS that happen every roughly 64 MB, etc. Finally there are the metadata tracking tables, such as sys.sysschobjs, which aren't readily available for consumption yet do need space in the database. 

The reason that this fails outside of the internal Rackspace location is due to the URL Endpoints being set to a value that is not able to be connected through from your local environment. I discuss this process at a high level in this blog post, however to quickly recap the point that needs to be made here: The endpoint url is the address where the connection will be routed in order to connect and run their queries. [roughly speaking] 

So, what happens when you lose a node? What would happen if that server crashed and burned or you lost the data drive? Now you've lost the database backups... 

The domain shouldn't make a difference here, the real difference is whether or not your DNS is setup where it can do the lookups required to get the information (IIRC this would be a reverse lookup zone but don't quote me on that). If you can resolve the DNS name, it will work fine, if you can't it obviously won't... this is more of a question for your DNS/AD admins as they hold all the information to that kingdom. 

The "one size fits all" answer is, the current offerings that SQL Server has won't fit into this. SQL Server doesn't work well in a multi-master zero data loss game. Other database solutions have similar issues where multi-master has the potential (like SQL Server) for data loss. I don't believe you're going to get a single bullet one size fits all solution here from the database layer itself. The only way, with SQL Server, to get what you're asking for it to create the application or middle tier around this philosophy and put the onus on the application/middle tier to make sure all databases are in sync and let the application/middle tier determine how to handle what happens when they aren't (for example, a write fails on one database but 3 the other databases successfully committed - do you proceed?). 

That depends. How any nodes are in the cluster? What are the node weights? What version of Windows is it? 

If there is nothing in the redo queue and the send queue is empty then I wouldn't worry about it as there is literally nothing to be done. This will happen on AGs that have light usage or usage based on specific hours and is then idle the rest of the time. If, however, there IS something in the log send queue and/or redo queue we are extremely interested in the redo rate. We are ALWAYS interested in blocked redo threads as well. 

Yes, it should. We'd need a network trace to really dig down deep (from the app and the server side) and see what's happening if you're not having this behavior. The main point is that even when we use read only routing, the client will always contact the primary first. Period. The only time this wouldn't happen is if the client connection string pointed specifically to a node name instead of the listener name. Assuming that isn't the case (you made no mention if you're using node or listener name or how the test was run [could you add that please?]) it will always connect to the primary first. 

Importance doesn't touch IO, it deal with scheduling. High runs more often than normal which runs more often than low. It doesn't mean it won't eat up your CPU and IO, it just tries to be more fair with scheduling based on your interpretation of the workload (i.e. those marked high are more important). This most likely will not help what you're attempting to solve. 

If you're talking about the option in SSMS to add in the and columns, those are added to the dataset through SSMS and is not part of the query to the server and can't be accessed from the query. It becomes "real" when the result set is returned from the server and SSMS adds the column. The workaround would be to check the in the query, or as you've already stated, pull the results into an intermediate table and process more from there. Using order by would only be able to be accomplished through an intermediate table as each query doesn't know of any other, as SSMS makes a connection per instance behind the scene to do CMS multi-server queries. 

How much CPU (total) does the hosted environment have? (1 CPU = 100%) How much RAM (total)? How many Disks, Total Space, IOPS/Disk, IOPs Total What is my networking bandwidth (Per card and totaled) 

If the individual databases and AGs underlying the distributed ag say they healthy and synchronizing, there is a good chance this is just a hiccup in the DMVs and/or SSMS dashboards. Since there was nothing in the errorlog to suggest the replica didn't connect or was in a disconnected state. Unfortunately since the issue has resolved, it's hard to say exactly what it was... but in the future if this occurs for someone: 

The behavior you're describing is not working as described. The end result is the same but the inner workings are different than what you're thinking. 

Autogrow settings aren't your issue, sparse writes and running out of space on the volume, is. Update: