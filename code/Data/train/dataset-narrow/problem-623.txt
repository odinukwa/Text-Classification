I have some code that is using entity framework to perform a large number of INSERTs in succession. SQL profiler shows a SELECT, then another SELECT, then an INSERT for each one due to it having to read some data first. These occur very quickly so I can see my console logging program scrolling like crazy. Every so often, it will completely stop for a few seconds, then it will resume again. There is no pattern to it. I've had it do all 450,000 inserts without any issues, but sometimes, I find that it pauses for long enough to trip the timeout on the sql command. I'm running it through the profiler and examining the results, and I can see sometimes that the duration is very high for what is a very small insert query that should be instant. Is there a reason why it pauses every so often? Could it be something like garbage collection or something internal? I don't really want to increase the timeout as a finger in your ears approach to solving the issue. And I can't turn the inserts into a BULK operation very easily as they are all distinct calls to a service that logs every time it is called. I am wondering if there's something obvious that may be the culprit. Any thoughts/speculation on this are welcome. 

something like the select above, I want to see all the jobs that have the database names in them and delete them, for example, like in the script below: 

the stored procedure is run from inside the report, as you can see below, and further below a picture with the output of my stored procedure. 

there are similar questions without a conclusion: DatabaseMail process is shutting down What is missing? 

any indication as to where to start troubleshooting this one? I have replication in this server, publications and a distributor server, and I believe this message could be related to that... When I run the following query by Sean Gallardy, I get the results below. 

I have the following update, that creates a table backup in a database called tablebackups with the records that are going to be updated, and then do 2 different updates. 

I know this question might have already been asked, but I am still trying to figure it out how to query the xml data and I really need an example. I have the following XML code that is inside a variable called @x in sql server: 

I see the column equals , I should have noticed this indication before. If it works I will leave this answer as it is, otherwise I will add to it accordingly. 

I was log shipping. I disabled the jobs, and restored the database on the secondary machine and used them for a bit. Now I want to undo this. I don't care about history or syncing, I simply want to turn log shipping back on and carry on my merry way. I enabled the jobs, but 'running' them (right click > start job at step, if that's how you run them) just pops up and says success but nothing happens. I am unsure of how I can start the log shipping going again. I tried deleting the databases from the secondary machine but that didn't help either. Any ideas? Surely it's a simple thing? Edit: Still at a loss. Says the job was successful but nothing happens, no database is created. It was working fine before I took the databases out of recovery on the secondary machine. 

Sorry if this is a noob question. I have transaction log shipping set up for a failover scenario. All the databases are in 'Restoring...' state until I do . At this time I disable log shipping. Once I have resolved the situation and I want to enable log shipping and carry on again. Do I need to run a command to set them back to Restoring or do I leave it and the log shipping will automatically sort it out? 

A1. How large are the databases? If you need logical backup, you can use mysqldump (--single-transaction, --master-data are some options you need to review and use). For speed, you want to go with mydumper/myloader tool. It will also make sure of taking individual dumps. Advantage here is you can grab the database you need to restore without having touch other databases of the instance. If you choose to go with mysqldump, you still can use mysqldumpsplitter to extract the tables/databases of your choice to export from full dump. Alternatively, you might want to use the physical backup using Percona's Xtrabackup which provides hotbackup. Here is a post to setup Xtrabackup using Holland framework. A2. Backup from Active Master? No! If we have replication failure on read-only master than priority is to get it fixed and then take the backup. But try to avoid backsups from Active Master. It's also advised to make sure your backups are stored on a different location other than read-only master! A3. Binary log backups provide point-in-time restore. You can store binary logs to a separate partition and have it backedup/snapshot or scripted to copy files to a separate location. Check out these two articles: 1 & 2 Hope this is helpful!! 

Note that APP2 will read eventually consistent data and it will have to handle writes to DB1 itself. (I do not tend to like master-master replication setup and writes to both ends due to past experiences but it is also an option.) You said no links but below this link includes steps for setting replication, you might want to refer. 

For having a look at what I have got on my table I just run the following query on the same server and database: 

when I run my script to find overlapping indexes below: Finding and Eliminating Duplicate or Overlapping Indexes 

However, it has only happened once, and I could not deal with it there and then. where/how could I search for more information as to what was the primary cause(s) that triggered this alert/message? Edit Actually this question could be something down these lines: what should I query at the time the alert was fired, to add more information to the email the DBAs get? 

Question is: How can I fix this error? Basically get rid of the ghost subscription showing up in the replication monitor and causing it become red flagged? 

Is there a way to script the settings found on the "General" tab, as seen on the picture below? otherwise, where are these permissions stored? how can they be queried? 

When talking about Partitioned Tables and Indexes for tables with less than 100 partitions, no nonaligned indexes: with that I mean: 

In order to see which procedures are using an specific table, () I have a couple of scripts that might help you: this first script shows the procedure and the table(s) the procedure uses: 

--> use --no-create-info to dump only data. (onlydata.sql) --> Have standard table definitions ready and intact for dev server. (definition.sql) --> Refreshing dev => load definition.sql and then onlydata.sql 

but as @greenlitmysql has mentioned you might see performance issues later on as the data grows (and length of column)... 

Tables are InnoDB: If we have InnoDB tables in the picture then things are not as simple. Considering you have Xtrabackup. You will have to import tables one after another. Refer Importing-Exporting-Tables-using-Xtrabackup OR how-to-recover-a-single-innodb-table-from-a-full-backup (For this to work you should have used xtrabackup as backup option.) Splitting mysqldump If you want to extract only one database from full mysqldump then you might want to extract single database from mysqldump and load it (saves you from loading full dump). 

You can check if the return value for the command, if 0 then OK else failure. Put may put it in shell script as follows: 

Notes: - Here HOSTNAME is the hostname/ipaddress of the destination Just to add, if your databases are on same machine, (which is not the case here, but still saying) you can use RENAME operations to move tables: 

So in anycase statement/row, you don't need to be worrying about the data on slave and it should match the master. but still if you want and can fit-the-logic on an event, this might be a possibility on slave at the risk of inconsistency!! As you said this is dw slave, would you consider a separate process to do the task you're looking to do, say a procedure? 

I know there are logins and groups that belong to roles that are have db_reader and db_writer permissions. However, they are not showing here. How can I change my script so that it would show me all the permissions for this table? 

Is there any other way to get this done? On this occasion this table is not big - about 500,000 records on the live system. the delete is part of a SSIS package, it runs daily and deletes about 10-15 records a day. there are problems in the way the data is structured, I just need one AccountCode for each customer but there could be duplicates and if they are not removed, they break the package on a later stage. It was not me who developed the package, and my scope is not to re-design anything. I am just after the best way to get rid of the duplicates, in the quickest possible way, without having to refer to index creation, or anything, just the T-SQL code. 

And looking at the space used by the indexed view and the underlying table: sp_spaceused 'Facts.FactBackOrder' go sp_spaceused 'VfactBackOder' go 

I got a question regarding the best way to put a bad column for an index in an index, so that I can satisfy my query below, and similar others that follow the same pattern. on the query below, all the columns in the select come from table TableBackups.[dbo].[tblBOrderItem]. I only go to the other table - TableBackups.[dbo].[tblBOrder] - to get the orders that are between the @fromDate and @todate AND the order must have a value which in my system here it translates to i.decCatItemPrice > 0