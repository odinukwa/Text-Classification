Your best place would be pgfoundry. But you won't find much. Doesn't look like many plugins are available. 

Even more, the returns zeros in all columns and null for - even if I run a manual vacuum on the table. 

Since you are a programmer, not a DBA, I would recommend the correct programmer's way to do such things. Don't update the database directly - never. Maintain all your schema changes, and base data changes in files which are then pushed with your release. I don't know what your language of choice is, but there are many tools to do this, Ruby has it's own tool - rake - with which you do that. And here are two others: 

If you are referring to the base/pgsql_tmp, then you should be fine. But I don't speak from experience of having done that myself. The only gotcha is that you have to make sure that the location is accessible when your PostgreSQL server starts up. (Ref.: Book PostgreSQL 9.0 High Performance, page 93). The book in question refers to creating a simlink of the base/pgsql_tmp on the OS partition or other less "safe" partition (i.e. non-RAID or the like). 

You could write a procedure with the EXECUTE and build the statement in the procedure by looping over the list of schema which match your query from the information_schema tables. Then you can call your procedure from psql or other passing it your criteria such as 'ceu_shard_test_merge_%'. You could have a parameter to do it or just dry run and instead of execute then it could output the statements or something along those lines. 

I can't find any details as to why fails. If I manually run as the user, that works, but that does not create the config files. I checked everything I could think of, including on-line and can't find a solution. 

Create a credential Connect to the SQL Server that should execute your package. Right-click Security and select "New Credential...". Enter a descriptive name (Credential name), select the Windows account you intend to use (Identity) and enter the password. Click OK. For initial testing, you can use your own Windows account. For production use, I would suggest creating a dedicated AD Service Account (with minimal permissions). Set up a proxy account Expand the SQL Server Agent folder. Open Proxies and right click on "SSIS Package Execution" selecting "New Proxy...". Enter a descriptive proxy name and use the credential you created earlier. Configure the SQL Server Agent step to use the proxy account Open your Agent Job, select the properties of your step executing the SSIS Package. Now you can select your proxy account in the "Run as:" drop-down list. Additional setup: If your package is deployed to the SSIS Catalog you need to grant the Windows login you used for the credential the "SSIS_Admin" role on SSISDB as well. For that, you need to create the account as a regular Windows login in SQL Server (Public) and map the user to SSISDB using the SSIS_Admin role. 

You gave us a long (and very detailed) question. Now you have to deal with a long answer. ;) There are several things I would suggest to change on your server. But lets start with the most pressing issue. One time emergency measures: The fact that the performance was satisfying after deploying the indexes on your system and the slowly degrading perfomance is a very strong hint that you need to start maintaining your statistics and (to a lesser degree) take care of index framentation. As an emergency measure I would suggest an one time manual stats update on all of your databases. You can get the nessecary TSQL by executing this script: 

You should not have two collections, that will give you headache. Have a user collection which has admin and regular users. You can then query users as a whole or look for a given type only. So if you are looking for user id x you don't have two search two collections. As for the question on duplicate ids with 2 collections, it simply depends how the ids are generated. You say your ORM does it... so you will need to check the doc on how it does it. 

Every table should have a primary key (I really can't think of a reason not to have one). So having a paymentID column to your payment table is definitely a standard design. 

Not an elegant solution but after installing the package using (which fails creating the cluster but installs PostgreSQL), I switched to the user and created the database using . Then back to , I created the cluster using the command. This moved the configurations to and set it all up. 

I simply removed the + signs and replaced with ||. On the EXECUTE I removed the format and simply concatenated the strings. 

I have since upgraded to version 9.4 and a whole new server so I can't debug this further. But I believe the problem was with a drive. I found a bad drive which wasn't reported as bad by the machine. 

Time for the backup to run is a reason for doing incremental backup on Progress. My backup is running fast enough that I did not need to use this function and I'm still doing only full backups. It also depends on your requirements. For example, if you have heavy financial transactions and you want to keep a backup every hour or something like that, you would need to do incremental (or real time). But unless you have something forcing you to do incremental, I would do full backup, I find that easier to restore. In terms of performance impact, if you have a fast disk array, I haven't seen much impact of doing a backup even during moderately heavy usage. Obviously it depends also on the size of your system. I'm talking about a 37Gb DB. 

It is provided by Tim Ford in his blogpost on mssqltips.com and he is also explaining why updating statistics matter. Please note that this is an CPU and IO intensive task that should not be done during buisiness hours. If this solves your problem, please do not stop there! Regular Maintenance: Have a look at Ola Hallengren Maintenance Solution and then set up at least this two jobs: 

SQL Server will auto update the statistics if the default is left enabled. The problem with that are the thresholds (less of a problem with your SQL Server 2016). Statistics get updated when a certain amount of rows change (20% in older Versions of SQL Server). If you have large tables this can be a lot of changes before statistics get updated. See more info on thresholds here. Since you are doing CHECKDBs as far as I can tell you can keep doing them like before or you use the maintenance solution for that as well. For more information on index fragmentation and maintenance have a look at: SQL Server Index Fragmentation Overview Stop Worrying About SQL Server Fragmentation Considering you storage subsystem I would suggest no to fixate to much on "external fragmentation" because the data is not stored in order on your SAN anyway. Optimize your settings The sp_Blitz script gives you an excellent list to start. Priority 20: File Configuration - TempDB on C Drive: Talk to your storage admin. Ask them if your C drive is the fastest disk available for your SQL Server. If not, put your tempdb there... period. Then check how many temdb files you have. If the answer is one fix that. If they are not the same size fix that two. Priority 50: Server Info - Instant File Initialization Not Enabled: Follow the link the sp_Blitz script gives you and enable IFI. Priority 50: Reliability - Page Verification Not Optimal: You should set this back to the default (CHECKSUM). Follow the link the sp_Blitz script gives you and follow the instruction. Priority 100: Performance - Fill Factor Changed: Ask yourself why there are so many objects with fill factor set to 70. If you do not have an answer and no application vendor strictly demands it. Set it back to 100%. This basically means SQL Server will leave 30% empty space on these pages. So to get the same amount of data (compared to 100% full pages) your server has to read 30% more pages and they will take 30% more space in memory. The reason it is often done is to prevent index fragmentation. But again, your storage is saving those pages in different chunks anyway. So I would set it back to 100% and take it from there. What to do if everybody is happy: 

Yes, that would be fine if your design goal is that Persons have only 1 Addresses. With this design, each Persons can have 1 Addresses but two or more Persons can have the same Addresses. It's all a matter of your business needs. If the above is what you are trying to get, then yes, it is correct. However, I think it's most common the other way around where the Addresses would have a foreign key to the Persons because a Persons could have more than one Addresses. As for your constraint to check the postal code - well first off you are missing the space and it's lower case. Whether that will work will depend on which database system you are using. I tested it with PostgreSQL and it does not work. I don't think you can really have such a simple constraint to fully validate a Canadian postal code. For example, there are some letters and some numbers which are never used. I'm a bit rusty on my Canadian postal office codes but I seem to recall that number 5 is never used as it's too similar to S, etc. 

Code generators are great! Code generators are evil! 10-15 years ago, I would have said that having a code generator for quickly creating the boiler plate code for database driven applications would have been a great gift to mankind. 5-10 years ago, I would have said code generator sucks, they generate too much duplicate code and that having a data-driven user interface (where the fields on the screen, validation, etc is driven by meta-data in a database instead of coding the screens one by one) would have been a great gift to mankind that supplanted the code generators. Today I would say - write each screen individually. Use existing framework that wire fields and model objects and possibly ORM when doing simple CRUD. But do design each screen to the exact purpose of the screen. Application screens that mirror a RDMS table too much is only good for managing lookup tables. Don't annoy the user with geeky interface that are designed against a computer storage model (RDMS)... make a screen that has only what they need. That may mean that it will save to multiple tables, etc. Who cares. The user isn't a database. So my thought? Don't waste your time making a code generator. 

Please replace 'insert FQDN here' and 'insert NetBIOS name here' with the actual FQDN and NetBIOS name keeping the double quotes. 

To be honest there is no definite answer to your question. If you do the last fallback to N1 you tested it on all available nodes. Within limits, more testing seems to be better to me. But to be honest it might not be absolutely necessary. Leaving it on N2 will get you the added benefit of using the N2 node for primary workload until the next update. It might even be feasible to do the fallback to N1 and then do another failover to N2 to test if N1 is running and then have N2 as primary until the next update. In conclusion: I don't think it is absolutely necessary to the fall back to N1 but if you can afford the time during your maintenance window I would consider just doing it (and maybe even do the failover to N2 afterwards). 

You need to enclose CertStoreLocation, Subject, DnsName and FriendlyName with double quotes. There is no need to specify a location as it will default to "Local Computer/Personal/Certificates" where it needs to be in order to use it by SQL Server. This will generate a valid certificate on Windows Server 2016 that will be usable by SQL Server 2017: 

You could be dealing with a delegation/impersonation problem as DimUser suggested. If your SSIS is just fetching data from one DB server and delivers it to the other the solution is much easier and should be to set up an SQL Server Agent Proxy. If you are executing the SSIS Package from Visual Studio or SSMS by hand the package will try your Windows credentials to log on to the SQL Server. If your account has the correct rights it will succeed. If you set up an SQL Server Agent Job to execute that package the service account running SQL Server Agent executes this package. He does not have your Windows credentials so he will try to use anonymous login resulting in this error message. To enable SQL Server Agent to execute a package using a different account you have to set up three things: 

The simplest - shell script running psql commands :) Otherwise, any high-level language can be used for the sake of experiment to learn database - ruby, python, java, etc. 

I'm trying to install PostgreSQL 9.4.1 on Ubuntu 14.04.2. I'm using the packages from . The package installs but it fails running . If I run manually, I get: 

The answer to your question is that yes, you can, provided that the users will have the necessary rights to update the time on their local machine. In your access front end, you would write some code to get the date/time from the SQL Server as in "select getdate()" and then you could use that to update the local time. But, that's not really the correct solution. The correct solution is to have all your computers and servers keeping their time right against an NTP server. And the other aspect of the correct way of doing that - because even then you don't have full control - is to do the timestamping in the query using getdate() or similar so that it use the SQL server date/time rather than the client. 

To answer your question on those 2 options, neither seem right to me. A) will lock you in and B) is a lot of work. The current schema you describe is not too bad (except for having the information name ("first name", "square foot", etc) as string instead of an ID referenced to a lookup table. However, this seems to me like a good candidate for a NoSQL database ( $URL$ ). While I never worked with such database, what you describe is a typical scenario that this solves. 

My first answer is: why is your database even accessible from outside through the Internet? That network traffic really ought to be blocked by router Internet gateway router or firewall. If you really need to allow some connections from the Internet to your database, then limit it to the valid IP address who should be connecting. At this point that's not really a dba question but a network admin question.