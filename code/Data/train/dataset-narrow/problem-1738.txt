Did you create a Cache Subnet Group in you custom VPC? You need to create a cache subnet in your VPC (inside the ElastiCache Management) first - after that your VPC/Subnet will appear for nodes. 

It is not possible (even for the AWS support afaik) to use Elastic IPs which are not for VPCs with VPC instances. So you're stuck here - the only possible way to do things like this is not to rely on a fixed IP address (you will get the same problem if you try to use ELBs or more than one instance). Your customers should NOT point to an IP address but they should use CNAME records with the given subdomain you provide for them. With that architecture you're able to migrate the whole domain with all subdomains to a new IP address if you need to and with the CNAME records nothing changes on the customer side (as the subdomain they're pointing to has the new IP address). The only solution for you now would be to send out an email to all customers which use the IP in their DNS records to change it to a CNAME and after migrating all customers to CNAME you can switch to the new Elastic IP and change your own DNS records. UPDATE: As pointed out below it is now possible to move an Elastic IP from "classic" to "VPC" - you will find the details here: $URL$ 

Yeah that should work (and is the way you should do it in the cloud). Just copy the whole infrastructure and then shift over the traffic to the new one and control the metrics of both infrastructures. 

In EC2 you should take a look at cloudinit ($URL$ and the user-data ($URL$ With this you're able to provide scripts which will run during the instance launch based on data you can send to the AWS API. But besides that: Why are you doing a RAID and copy over data from the boot volume to the RAID on launch? Without knowing your exact use case but that sounds wrong ;-) Maybe you can elaborate more on this so we're able to actually provide a better solution without the need of doing launch scripts and so. 

Is there a reason why you don't use EBS snapshots? You can use those to save the whole EBS device (incremental) with a simple API call and the snapshots are saved within S3. If you need an old version back just create a volume from this snapshot and connect it to your instance instead of the broken EBS. 

DynamoDB is built for high throughput low latency. So the core feature of this Database is providing answers within single digit milliseconds. As you already figured out you pay per second which means if you don't need nearly the same amount (within the ability of scaling up and down) of reads/writes over a period of multiple hours DynamoDB is expensive as you pay for queries you don't need. 

Security groups in AWS are way more comfortable AND even better from the security point of view (as the traffic you want to block doesn't reach your instance at all). So turn off the iptables and configure the security groups as limited as you can. 

You're fine - if there is a possible data loss AWS wouldn't provide a single click update. As this is only a minor version update it is pretty easy to upgrade without any problems. 

You should be able to see that information in the Billing Management Console with the Usage Reports for the Elastic Compute Cloud service. I can select "SnapshotUsage" for the different regions in there. 

As you mentioned you're running Ansible on a EC2 instance you should actually don't use credentials but roles attached to the EC2 instance: $URL$ The idea is that your instance itself is able to get temporary credentials and can execute the necessary commands which are defined in this role. As you never store any credentials anywhere this is the most secure way to work with the AWS API from an EC2 instance. As Ansible relies on boto this will work out of the box - you just need to create a role which has all the necessary IAM permissions and attach it to your instance you're running Ansible on. After that your dynamic inventory will work without needing any additional credentials. 

Apparently there is a bug in Apache 2.4.17. Disabling the module auto index (which is the cause of the wrong behaviour, will prevent the error. 

I don't know what's wrong... Apache has the right to read the passwd file. In addition if I comment the AuthDigest... lines and uncomment the Order and Allow, apache serves the folder like a charm. Apache responds me with a 403 and doesn't prompt my browser for user/pass .. Any help ? 

Context I have two linux based servers which are remotely located. It has already happened twice: after a system or kernel update, the system wasn't responding/reachable over ssh anymore (configuration errors or disk failure, ...). I had travel to the location to rescue the server. When the boot process fails, the systems lands in the emergency/rescue shell which need to be administrated locally. Question: Is there a way or a feature of the bootloader to monitor the booting process (i.e. a watchdog), if the system is stuck after some time, a timeout triggers a the system reboot with a different image with network capabilities, ssh, ... (stored on a dedicated media, i.e USB key) in order to be administrated over ssh ? Thanks 

There is a useful tool that test the different DNS nameservers available (your ISP, current configuration, DynDNS, Google Public DNS and other one). From my point of view Google DNS are pretty fast but depending on the load GoogleDNS supports my ISP Dns is sometimes faster. NameBench (Linux/Windows/Mac OS X) Output : alt text $URL$ 

Since the last Fusion Passenger update, all my Sinatra applications have stopped working on the following env: Apache (2.4.17), Phusion Passenger (5.0.21). Everything was working as expected before updating the passenger middleware. The problem is, apache directory is trying to proceed to a listing of the public directory when I'm requesting the url ($URL$ of my vhost. The ODD part: If a route defined in the Sinatra controller is requested (i.e: $URL$ passenger is started and the requested page is served as expected. Here is the Virtual Host part: 

Apache is trying to do directory listing (which has been disabled by configuration). Has someone experience the same issue ? How to make apache launch passager and serves the root of the Sinatra app ? 

I've seen you were using an alternative port for ssh so, replace the yyyy be the port the sshd deamon is running. Then restart fail2ban. 

Use the ServerPath like you want (doc in your example) and then use RewriteRule to bind the ServerPath to your new location. Don't forget the $1 (matched part). Don't forget to load mod_rewrite in apache. 

you don't have to worry ! If you MAC address has really changed the only direct issues I can immagine are wireless authentication based on MAC Adresses. If so, go to te admin interface and update mac addresses you allow to join the network. May be you can also have dhcp / fixed ip issues. But if you don't use any of them, it's ok. 

Phusion will address the issue in the realase of Passenger 5.0.22 before Apache 2.5.0 will be released. 

I'm trying to enable DNSSEC on my authoritative dns Bind machine. So far I've done the following Tutorial : 

Does anyone know how to solve this ? Is there any online DNSSEC tool that display more infos about the dnssec status ? 

You can't do this by indicating multiple the MX records for the same (sub)domain. The mail will be delivered to the server(s) defined by your MX record regardin only the (sub)domain, without any look at the user@... Mail routing is done on the (sub)domain part only. Possible solutions : 

In common cPanel you can find the Simple DNS Zone Editor ! It allows you to enter Autoritative records for your domain name. Redirects mysubdomain.mydomain.net to 128.128.12.12 : alt text $URL$ 

I want to share file from my linux box to OSX clients. I don't want to use SMB ! I'm using netatalk 2.1.2 but apparently there is a filname length limitation : I've tried "123456789012345678901234567.avi" (31 characters long), and it works but when the filename is >31 char long. Some programs like Quicktime can't open the file. I've started the share with : 

To reliably send a lot of traffic to a system under test you need to put the client and server as close as possible. The easiest way to do that is to put the client system on the same network as the server (unless you're testing the router or firewall). 

The semantics of your code simply requires the to "be instantiated" (that is, run) before the child directory is created. There is no optionality implied by this - will run if exists, otherwise does not run. 

This sort of thing usually happens because the terminal has been corrupted in some way, often by ting a binary file. Have you tried a ? If it really is the Bash history that is broken that is stored in . 

There's nothing fundamental about a desktop installation of Linux (unlike other operating systems) - you can "convert" one to the other by installing/removing packages and enabling/disabling services. For example, if you want to make sure to avoid any resource-intensive graphical logins it may be enough to remove the package and any Unity packages. You may also want to remove X and graphics drivers. Then check to see if any useless services are still running. 

As others noted at the linked page, this is a bad idea. Passwords are generally much less secure than using public/private key pairs. That said, if the server sent just it isn't using the new configuration yet. Try restarting . 

However, this changes the mode of every file in the directory. Since most of the files (thousands, changing every day) should be handled by another application, how can I tell Puppet to leave all files not in alone? I can't use because that requires that I know in advance the names (or at least globs) of files I don't want to manage. 

Another solution is to ask specifically for files only by using . You can also suppress the printing of the starting points by using . 

tries to give you as specific information as possible (the opposite case would be to always print , which is technically correct but not very useful). ANSI is not a specific encoding, and UTF-8 is a superset of ASCII, so it will report ASCII for both if the bytes contained in the file all are inside the ASCII charset. 

If you're asking how to run more than one statement with a single command, you can either simply separate them by semicolons: 

I'm building an API which will only be accessible to local services by using in the directive. How can I make this return a 404 rather than 403 status code when accessed from a remote address? That way I can hide the fact that there is a service at the requested location. I'd rather preserve the semantics of than use . 

It looks like the package is not installed with APT: "Package hadoop is not installed, so not removed". If you have installed by some other means, such as or , you'll have to look into how that package specifically can be uninstalled. 

That's not running twice, you're just seeing the and the processes, which are separate. Try to see the process tree, explaining how they are related. 

You are passing the input to the loop on standard input (file descriptor 0). Any commands inside the loop which read from standard input will consume that stream, making the content not available for the second time around. This is a common issue with passing standard input to what is effectively multiple commands. The easiest way to fix this is to use a file descriptor that the commands inside the loop are unlikely to use, for example: 

Once you create a host from an AMI you can create a snapshot of the volume (should be done before starting the instance to avoid having irrelevant state on the disk), and create a new AMI from that snapshot. Then you have your own AMI with the same contents. As for which one to choose, there are only two 14.04 images available: EBS (presumably PV) and HVM. Which one is best for you depends on your requirements. 

will be much faster than FTP for synchronising files repeatedly - it simply checks the file size and timestamp on the receiver to find out whether it should overwrite the file, thereby minimising the amount of data transferred for each synchronisation. If there is a lot of files you may want to consider creating a tarball automatically (using for example ) and syncing that, for example using (untested). can be used to make the testing a single command. Something like this should do the trick (you have to use Tab for indentation):