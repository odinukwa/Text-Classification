Based on that, I'd venture to say that the unexpected behavior you are seeing appears to be because you have the parameter order confused -- in your second rotation example, you have asked to create a rotation matrix around the axis which is invalid and produces a malformed matrix that does the unexpected things. Also make sure that 

My friend is working on a side-scrolling shooter in XNA that uses similar effects (here's a video of an early test). He wrote a blog post about the technique that you might find helpful. He's also usually hanging out in the #graphicschat IRC channel on irc.afternet.org (name's Drilian) and could probably help you out. 

The simple solution is to not pre-render the background at all. Draw everything live, in 3D, with a fixed camera angle. Modern GPUs can achieve live results that look better than what we often had to pre-render for back in the day (especially with good art). That said, if you want to reproduce the technique for "authenticity," efficiency, or simply to achieve something even better than what you could do live, it's fairly straightforward in concept: pre-render the scene and save both the color and the depth information. This will allow you to know the depth of every pixel in the scene, which you can reconstruct into a world-space position of every pixel in the scene (also as a pre-computation step, since the camera angle is fixed). This means you can know when the character is in front of or behind that pixel (render characters and dynamic objects live, in 3D, with the same camera transform used to render the scene). You can use this information to slice the scene into 2D layers which can be drawn back-to-front. You'll also want to bake out the collision information for props in the 3D scene, such as (in your example), the payphones. Otherwise the player will phase through them, either instantly (in the case of a layer-based rendering) or "smoothly" (if you only work with the pixel depths). Both cases will look very unusual in practice and you'll want to prevent them. As Krom says, you'll probably also want a simple navigation mesh to define where the player can and cannot walk; this can also be exported from the pre-computed scene baking pipeline. There is, in fact, quite a lot of optimization you can do do with this approach in terms of pre-computing data (the downside, of course, being that iteration on the specific details of the room may require re-exporting through the entire pipeline, which may be costly). 

I think the best approach would be to modify not just the resolution that the game runs at, but also the viewport and projection / field of view used by the game such that you effectively increase the viewable area beyond what was originally possible. This could be a potential concern in games with competitive multiplayer modes. However, I don't recall Sacred having one of those. I can't really tell you how to accomplish this specifically, since it will depend heavily on how Sacred itself was built. In general you are going to want to find where the resolution is set and modify that, and also where the viewport / projection matrices are defined and adjust those. You will likely need to adjust the positioning of various UI elements, and so on. This appears to be the approach taken by the widescreen mod you linked, at least, and possible the D2 one as well (I didn't look at that one in as much detail). However, while the above could be quiet a lot of work it's probably better than your other options, which are: 

Keeping all these in a central pool via a dictionary or set will allow you to know the size consumption of all textures. When you start to reach a limit, you can evict textures based on size or last-recently-used timestamps or whatever else you'd want to track. It's tricky because getting the in-memory consumption of a texture from D3D is not reliable, so you'll be leveraging guesswork most of the time to implement . You can use the on-disk size of the texture as a baseline and add padding as necessary. This method also allows you sufficient abstraction to implement this pooling as one giant "megatexture-like" back-end D3D texture, where you load new textures by rendering them into available regions on the single backing texture and evict them by marking the regions as empty. This does require a good bin-packing algorithm, but it could help with problems related to GPU-memory fragmentation (since you can allocate a fixed-size texture as large as you can get away with and that consumption stays fixed). However, if you find yourself needing to swap textures frequently it could be a performance issue. It is probably worth considering, though. 

Fundamentally, you're going to be doing collision detection with what are essentially physics volumes (simple primitive shapes like spheres and cylinders) to achieve what you want, unless you want to start sacrificing accuracy and doing even simpler ray checks. Consequently, it makes some sense to use a physics API for this. These sort of computations are exactly the kind of things physics APIs tend to do well. Guild Wars 2 used basic physics volumes like you are describing for server-side collision detection between attacks and characters and the like. It is a viable plan; just try to keep your collision volumes as simple as possible for the effect in question and profile when you discover bottlenecks. You may also want to do client-side prediction of the collisions, using physics or simpler checks, to avoid latency and keep your game feeling responsive. 

It's certainly possible (as in the, format supports it), the exact process by which the desired result is achieved will differ from tool to tool. For example, here is a video that purports to illustrate how to make transparent animated GIF images with GIMP. Similarly, this blog post also describes the process. It largely comes down to making sure your image frames have alpha channels in them before you combine and export them into the final format, regardless of which tool you use. 

Far more complete overviews are available at the MSDN and via this MSDN magazine article. Note that the system is not well suited to real-time iterative direction like you are suggesting; you invoke the voice commands by hitting the Start button, and saying something like ", run right" (although the prefix is optional). Further, if the system fails to understand you it may come up with a disambiguation prompt, which will interrupt the action (this may be optional too, I haven't explored it deeply enough). It's probably not a very good mechanism to control the actual gameplay, although you may be able to use it for things like showing high scores, starting a new game, and so on. 

Don't use the task manager to do memory profiling. It doesn't mean what you think it means, especially for C# or other managed applications. ANTS Memory Profiler is a wonderful managed memory profiler that I've used repeatedly. It is not free, but it does have a trial. As Jonathan Dickinson points out in the comments, the CLR itself also exposes several performance counters you can view in perfmon.exe, including several related to .NET memory and garbage collection. There is a very good chance there's nothing actually rooting your bullet objects once they are removed from that list and that, upon a subsequent collection of the appropriate garbage generation, the memory consumed by the objects will be reclaimed. Without seeing more of your code I can't be sure, but I think it's a safe bet. However, you are doing something that is, if nothing else, inefficient and you could improve it: You're removing items from a list, which implies you are allocating them and putting them in the list in the first place -- in other words you are allowing allocation to occur during your game loop, and this means you'll eventually cause garbage collection (and more to the point, possibly cause more frequent collection of higher GC generations). Further, you're removing items from the middle of the list (most of the time), which means every time you remove something, the following elements of the list must be copied. Consider instead partitioning your list into two halves, an "active" and a "disabled" half. You will need an additional integer variable somewhere to track the index of the first disabled bullet in the list. When your game starts, you preallocate some reasonable number of bullets in the list and set the "first disabled" index to 0 (because all bullets in the list are currently disabled). Whenever you need to "create" a new bullet you just set the properties of to whatever values the new bullet should have: 

In Ken Perlin's paper on improved noise, he mentions a very similar problem. The cubic used in the original noise paper creates discontinuities at the integer boundaries due to the properties of its derivatives. In his revised paper, he proposes an interplant of to address those issues. 

Storing the entities themselves in a large tree is also not something I think you should do unless you have a really compelling need for it. It sounds like you are conflating the idea of a scene graph with the idea of your entity system: thus the assumption that every entity has a parent that ultimately leads back to a "root" entity. This is probably a bad idea for the very same reason that (most) scene graph implementations are a bad idea: you end up with a very deep tree, the leafs of which constantly need reshuffling based on new parents or due to lifetime goods (good scene graphs tends not to be particularly deep, and do not try to include "every" object at an extremely granular level). Plus, this design will force you to find physical parents for entities that don't logically need them just because your system says everybody must be parented to the root. A flat list of entities is a more common, and generally better, solution. I would go with that until can demonstrate a very compelling need to organize them otherwise. 

You should serialize a reference to the texture (such as its file name), not the texture itself. That way when you load the serialized game object later, you know which texture to look up and associate with the object. You don't want to serialize the entire texture with the object, because that would be a huge waste of space when objects started to share textures (when serialized, they'd each have a complete copy of the texture bytes). That's why the object itself isn't marked as serializable. It's trying to protect you from yourself. This is related to why it's generally a good idea to separate game logic state from rendering and visual presentation state.