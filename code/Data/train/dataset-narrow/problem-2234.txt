This of course also directly gives the 'intermediate' inequality $\|T_{\rho\sigma}f\|_q \le \|T_\sigma f\|_{1+(q-1)\rho^2}$. The theorem also has a 'two function' version, which can be proven, for example using Holders inequality and then the standard theorem: 

The Hypercontractivity theorem (or Bonami Beckner inequality) is a very useful tool. Unfortunately, it isn't easy to carry over to other spaces than the uniform boolean cube. In Ryan O'Donnel's Analysis of Boolean Functions, he mentions the following theorem which considers the more general problem of general product spaces: 

The two later codes are nearly as bad as possible. Are there any codes for which I shouldn't expect this to be the case? If not, can I at least say something about the distribution of distances being fairly concentrated? 

If $f$ is an elementary symmetric polynomial over a finite field then it can be computed by polynomial-size uniform $TC^0$ circuits. If $f$ is an elementary symmetric polynomial over a characteristic $0$ field, then it can be computed by polynomial-size depth three uniform algebraic circuits (as you already mentioned the Newton polynomial; or by the Lagrange interpolation formula); and so I believe this then translates to polynomial-size uniform Boolean circuits (though perhaps not of constant depth) (but this may depend on the specific field you're working in; for simplicity you might consider the ring of integers; though for the integers I presume $TC^0$ is enough to compute symmetric polynomials in any case.) If $f$ is a symmetric polynomial over a finite field then there is an exponential lower bound on depth three algebraic circuits for $f$ (by Grigoriev and Razborov (2000) [following Grigoriev and Karpinsky 1998]). But, as mentioned in 1 above, this corresponds only to constant-depth Boolean circuit lower bounds (while there are small uniform Boolean circuits in $TC^0$; meaning also that the polynomials are computable in polynomial-time). 

I would like to lower bound the quantity $\Pr[X\ge t, Y\ge t]=\Pr[\min(X,Y)\ge t]$ using the small moments of $X$, $Y$ and $XY$. In particular I am interested in the case where $E[X]=E[Y]=0$, but $E[X Y]>0$. In "The Fourth Moment Method" Berger shows that if $\frac{E[X^4]}{E[X^2]^2}\le b$, then $\Pr[X\ge \sqrt{E[X^2]}/(4\sqrt{t})]\ge1/(4^{4/3}t)$. Alternative formulations of this principle include the Paley–Zygmund inequality. Using the fact that $\min(X,Y)=\frac12(X+Y-|X-Y|)$, we get that that $$ E[\min(X,Y)^2] = E[\min(X^2,Y^2)] = \tfrac12\left(E[X^2+Y^2]-E[|X^2-Y^2|]\right),\\ E[\min(X,Y)^4] = E[\min(X^4,Y^4)] = \tfrac12\left(E[X^4+Y^4]-E[|X^4-Y^4|]\right). $$ Now using $\frac{E[X^2]^{3/2}}{E[X^4]^{1/2}} \le E[|X|] \le \sqrt{E[X^2]}$ we can get a bound: $$ \frac{E[\min(X,Y)^4]}{E[\min(X,Y)^2]^2} = 2\frac{E[X^4+Y^4]-E[|X^4-Y^4|]}{(E[X^2+Y^2]-E[|X^2-Y^2|])^2} \le 2\frac{E[X^4+Y^4]-\sqrt{E[(X^4-Y^4)^2]}}{\left(E[X^2+Y^2]-\frac{E[(X^2-Y^2)^2]^{3/2}}{E[(X^2-Y^2)^4]^{1/2}}\right)^2}. $$ Besides being ugly, this has ended up using the eighth moment of $X$ and $Y$, rather than just the fourth. Is this necessary? Or is there a nicer way taking better advantage of the $\min$ function? Update: We may wonder what the ideal result would be. If we let $\hat{X}=[X,Y]^T$, $\Sigma=\text{Cov}(\hat X)$ and $Z=\hat X ^T\Sigma^{-1}\hat X$, then Markov's (or the multivariate Chebyshev) inequality tells us $\Pr[Z \ge \epsilon] \le E[Z]/\epsilon=2/\epsilon$. If we assume $\text{Var}[X]=\text{Var}[Y]=1$ then $\min(X,Y)\ge t$ implies $Z\ge t^T\Sigma^{-1}t=\frac{2t^2}{1+\text{Cov}(X,Y)}$; and so $\Pr[X,Y\ge t]\le\frac{1+\text{Cov}(X,Y)}{t^2}$. With Paley–Zygmund we then get $\Pr[Z\ge 2t] \ge 4(1-t)^2/E[Z^2]$, which is a fourth moment bound. Of course this is a lower bound on the entire space outside the ellipse, however we might(?) hope that it is also correct up to a constant factor for any convex region at distance t, if $X$ and $Y$ are nice enough? 

If I understood correctly the question, the so-called Buss-Pudlak game provides a simple transformation from a proof system to such a decision tree (see Buss-Pudlak '94 $URL$ The queries are formulas (not just variables). The tree is also completely deterministic. Other decision trees that correspond to different propositional proof systems exist: e.g., Linear Decision Trees correspond to Res(lin) refutations (cf., $URL$ and $URL$ But there are many other examples (cf., Tonian Pitassi's work on CP-like proof systems). 

Yes. If $ 0< \epsilon <1$ is a constant (or $1/\textit{polylog}(n)$), and you are promised that at least $ \epsilon 2^n $ of all possible assignments are satisfying the input 3CNFs, then you can find such an assignment in deterministic polynomial-time. The algorithms is not difficult: Claim: Under the promise stated, there must exist a constant size set $ S $ of variables that hits all clauses in the 3CNF, in the sense that every 3-clause must contain a variable from $ S $. Proof of claim (sketch): Otherwise, there must exist a large enough family of 3-clauses from the 3CNF, in which each variable occurs only once. But this family, when sufficiently large, has already less than $ \epsilon $ fraction of satisfying assignments. QED Thus, you can run over all possible (constant number) of assignments to $ S $. Under every fixed assignment to $ S $, the 3CNF becomes a 2CNF, by the assumption that $ S $ hits the original 3CNF. Now, you can use the known polytime deterministic algorithm for finding a satisfying assignment for 2CNF formulas. Overall, you get a polynomial time upper bound. The algorithm for 2SAT is I think already in S. Cook famous 1971 paper. The algorithm for 3CNFs is from: L. Trevisan A Note on Deterministic Approximate Counting for k-DNF In Proc. of APPROX-RANDOM, Springer-Verlag, page 417-426, 2004 The original paper showing the result for 3CNF is: E. Hirsch, A fast deterministic algorithm for formulas that have many satisfying assignments, Journal of the IGPL, 6(1):59-71, 1998 

Do you know of anything similar for the case of general product spaces, or $p$-biased functions in particular? 

I'm wondering if anyone has studied 'intermediate' inequalities of this two-function version of the inequality? That would mean something like $$\langle T_{\rho\sigma} f,g \rangle \le \langle T_{\sigma} f,g \rangle^q$$ for some value of $q$ depending on $\rho$ and $\sigma$. It doesn't appear to follow easily from any of the individual theorems. One thing we might try is to look at the normalized case, where we write: $\langle T_{\rho\sigma} f,g \rangle = \sum_{S\subseteq[n]}\rho^{|S|}\hat f(S)\hat g(S)$ and divide both sides by $\|f\|\|g\|$. Then we can use the means inequality to show $$ \left(\frac{\langle T_{\rho} f,g \rangle}{\|f\|\|g\|}\right)^{1/\log\rho} = \left(\sum_{S\subseteq[n]}e^{|S|\log\rho}\frac{\hat f(S)\hat g(S)}{\|f\|\|g\|}\right)^{1/\log\rho} \le \left(\sum_{S\subseteq[n]}e^{|S|\log\sigma}\frac{\hat f(S)\hat g(S)}{\|f\|\|g\|}\right)^{1/\log\sigma} = \left(\frac{\langle T_{\sigma} f,g \rangle}{\|f\|\|g\|}\right)^{1/\log\sigma} $$ whenever $\log\rho\le\log\sigma$. However to be valid, this requires the values $p_k = \sum_{|S|=k}\hat f(S)\hat g(S)$ to be non-negative for all $k$, which is certainly not the case for general functions $f,g:\{-1,1\}^n\to\mathbb R$. It also doesn't appear to be very sharp, since it becomes equality when all the fourier mass is on level 1, which the hypercontractive inequalities usually tell us isn't possible for many (smaller) functions. 

The following result by Raz (Elusive Functions and Lower Bounds for Arithmetic Circuits, STOC'08) is aimed at $VP\neq VNP$ (and not directly $P\neq NP$), but it might be close enough for the OP: A polynomial-mapping $f:\mathbb F^n \to \mathbb F^m$ is $(s, r)$-elusive, if for every polynomial-mapping $Γ : \mathbb F^s → \mathbb F^m $ of degree $r$, Image($f$)$\not⊂$ Image($Γ$). For many settings of the parameters $n, m, s, r$, explicit constructions of elusive polynomial-mappings imply strong (up to exponential) lower bounds for general arithmetic circuits. 

For strong enough proof systems the graph representation of a proof in the system seems less consequential, since (as Joshua Grochow already commented), DAG-like and tree-like Frege proofs are polynomially equivalent (see Krajicek's 1995 monograph for a proof of this fact). For weaker proof systems such as resolution, tree-like is exponentially weaker than DAG-like proofs (as Yuval Filmus described above). Beckmann and Buss [1] (following Beckmann [2]) considered restricting the height (equivalently, depth) of the proof-graph of constant-depth Frege proofs and investigated the relationship between DAG-like, tree-size and height of constant depth Frege proofs. (Note the distinction between restricting the depth of the proof-graph and restricting the depth of a circuit appearing in a proof-line). There might also be separations between tree-like and DAG-like Nullstellensatz (and polynomial calculus) proofs, which I currently don't remember. 

For the Laplace distribution, if you use the Bernoulli bound you can write $$Ee^{u\sum_i X_i} = \prod_i \frac1{1-u^2/\lambda_i^2} \le \frac1{1-u^2\sigma^2/2},$$ where $\sigma^2=2\sum_i\lambda_i^{-2}$. Then the classical Chernoff method to gives $$\Pr[\sum_i X_i \ge t\sigma]\le \tfrac{1+\sqrt{1+2t^2}}{2} e^{1-\sqrt{1+2t^2}} \le\cases{ (et/\sqrt2+1) e^{-\sqrt{2}t} \\ e^{-t^2/2 + t^4/8}} .$$ Note that these bounds hold for unrestricted values of $t$ and $\lambda_i$. The bounds on the right show the two possible regimes. For small values of $t$ we get `normal' concentration $e^{-t^2/2}$, while for large values of $t$ we get $\approx e^{-\sqrt{2}t}$, which is also the CDF for a single Laplace distributed variable. The $1-\sqrt{1+2t^2}$ bound allows you to interpolate between the two situations, but I suspect that in nearly all cases one will be firmly in either the large $t$ or the small $t$ camp. For the exponential distribution the same techniques give us $Ee^{u\sum_iX_i}\le\frac{1}{1-u\mu}$ where $\mu=\sum_i 1/\lambda_i$. Hence $$\Pr[(\sum_i X_i)-\mu \ge t\mu]\le (t+1) e^{-t} \le e^{-t^2/2+t^3/3}.$$ So you still get something slightly normal looking, but with $t\mu$ rather than $t\sigma$ as we might have hoped for. I don't know if it is possible to get a bound in terms of the variance. You could try to study $Ee^{u(\sum X_i-\mu)^2}$, but it doesn't seem to be easy to work with. 

The question seems quite open ended. Or perhaps you wish to have a precise characterization of the time-complexity of any possible symmetric polynomial over finite fields? In any case, at least to my knowledge, there are several well-known results about the time-complexity of computing symmetric polynomials: 

It is well known that random $ k $-CNF formulas over $ n $ variables with $ cn $ clauses are unsatisfiable (i.e. they are contradictions) with high probability, for sufficiently large constant $ c $. Thus, random $ k $-CNF formulas (for $ c $ large enough) constitute a natural distribution over unsatisfiable Boolean formulas (or dually, over tautologies, i.e. negations of contradictions). This distribution has been studied extensively. My question is the following: are there any other established distributions over propositional tautologies or contradictions, that can be considered as capturing the "average-case" of tautologies or unsatisfiable formulas? Have these distributions been intensively studied? 

I'm trying to find a low distortion embedding of the trivial metric space into hamming space. It seems this should be doable by using a large set of low dimensional vectors, with approximately equal pairwise distance. My question is if it makes sense to expect error correcting codes to have this property? Usually when designing error correcting codes, we are interested in finding the highest achievable rate, given a minimum distance between points. A similar question is to consider, instead of the minimum distance, the ratio between the maximum and minimum distance, $\rho=max/min$. Given $\rho$, what codes should one consider for maximizing the rate? I tried to count the distances in some of the codes from this list of optimal binary codes: