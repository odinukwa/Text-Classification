During the talk you are not so much interested in, you don't necessarily need all the pre-talk stuff, but always have the paper about the talk open, otherwise you will be lost at some point. Anyway, I think the definite focus would be but not only by listening to talks, also by talking with people, asking questions (to anyone, do not be afraid, usually people are really open to discussion even if they are well-known (might be busier however)). Final advice, be careful, there are many talks so it is easy to lose focus. Some will be much more important for you; it is better not to follow a talk you are not interested in and fully follow one you are interested in than follow half of them (IMHO). 

I imagine that this can be a lot of work to do from scratch, however it may be possible to have an organization specialized in CS to do this for the conference organizer since it does some advertising for them with not a lot of work. Win-win. I know that this will not be used by every one, but I believe that those using it will really benefit from it (personnal experience, I always had a great time CS). 

Read the summary of the proceedings as soon as it is given to you (bring your laptop, sometimes it is on a USB stick) See which talk may interest you, read the abstract+paper if you are interested Then listen to the talks that you are interested in, after having read the paper (keep the paper open during the talk in case you need to catch up) As much as possible, go talk to people, meet new people, especially people working on similar stuff, ask question. 

Encourage CouchSurfing as an accommodation I am not saying to bind people to CouchSurf, however imagine that with the hotel recommendation you add a page where local researchers (or student as I imagine this would be mainly used by students) could post a message saying "I can host so many people, mail me". Only people ready to do it would use it. For example, for students going to the conference it may be a good way to: 

Have you tried to look for what already existed in the field of Scheduling with Communication Cost? If you choose some of the communication cost to be $+\infty$, then it seems to me that it is exactly your problem. A communication cost is defined on an edge between two tasks $T$, $T'$ as: $$ \text{comm}(T,T')=  \left \{  \begin{array}{ll} 0 & \text{if alloc}(T)=\text{alloc}(T')\\ c(T,T') & \text{otherwise.} \end{array} \right. $$ Where the function $c$ is a well-defined cost function (in your case it could be $0$ or $+\infty$, depending on your constraints), and $\text{alloc}(T)$ is the function that states the processor on which $T$ is scheduled. You can read the work of Hanen, and Munier which is more adapted for small communication delays, however since in the general case your communication delays are small, it might be possible to adapt their algorithm. In the worst case you can check on scholar who cites their paper, it may be a good start for bibliographical work. 

Not sure this is your answer (see below) but a bit too long for the comments. I though your problem was something like: $(P|tree;p_i=1|\Sigma T_i)$, where: 

Rabie introduced the model of "Rusted Turing Machines" in his thesis: The Power of Weaknesses:What can be computed with Populations, Protocols and Machines (Chapter 7). The idea is that there is a restriction on the number of time the TM can change its internal state because of decay. Rabie introduced the class $Piv(f(n))$, the class of Turing Machines that change their internal state $O(f(n))$ times. An RTM is then an element of $Piv(1)$. In this model, Rabie 

You could also check the key-words scheduling with security constraints; it seems that your problem could be justified with security reasons (when a computation has been done, you do not want some part of the result to be communicated in case someone were to intercept the communication). So maybe you could check in some security-based conferences (such as SAFECOMP). 

I see here and there mention of the $\lambda_I$-Calculus (in which every variable must be used at least once) and the $\lambda_K$-Calculus (in which a variable can also be unused). Are they equivalent? Why has the latter kinda obscured the former? EDIT By equivalent, I mean they have the same expressive power, namely, being universal or Turing complete. 

Do methods in an OOL contain free variables? If they do, are they statically-scoped? Do method names live in a different name space than variables? 

For both questions, I have kinda guess answers but am not quite sure. However, I would like to know more about the reasons behind and consequences of OOLs' specific answers to these questions. 

I recently read Landin's paper "The Next 700 Programming Languages". But I was a bit confused by ISWIM. In particular, are functions first-class objects in ISWIM? It seems not because every function must occur under some name and there is no $\lambda$-like construct in the language to construct an anonymous function. Landin even explicitly claimed in the first footnote that "a not inappropriate title would have been Church without lambda". Anybody knows the reason behind this choice? Is ISWIM less expressive than a language with $\lambda$? 

As stated in the title, I wonder any relation and difference between CIC and ITT. Could someone explain or point to me some literature that compares these two systems? Thanks. 

I wonder why computer scientist have chosen recursor instead of iterator (or tail recursor if you like) in primitive recursion, given that function defined in terms of iteration behaves more efficiently than that in terms of recursion. EDIT: Let us go further, do you think iteration would be better than recursion to complexity? 

I saw some people uses Krivine's notation for function application when presenting the syntax for the $\lambda$-calculus. For example, the $\lambda$-term $\lambda f . \lambda x . \lambda y . f\ x\ y$ (with the normal convention that function application associates to the left, so it actually means $\lambda f . \lambda x . \lambda y . ((f\ x)\ y)$) is written $\lambda f . \lambda x . \lambda y . (f)\ x\ y$ (with a similar convention that it actually means $\lambda f . \lambda x . \lambda y . ((f)\ x)\ y$). I do not see the point of having another pair of parentheses around the innermost $f$. Why do people use Krivine's notation instead of the usual one? 

I was not a big fan of Object-Oriented Languages (OOL), but recently started to learn a bit more about their pros and cons in a general setting instead of diving into one such language. I have a few questions here: 

I heard that there exist two styles to define an evaluation context: outside-in and inside-out. Can someone give the definitions? Why are they so named (inside-out and outside-in)? What is the difference? Some examples would be appreciated. 

In programming language semantics, it is often heard that people talking about meaning and denotation. They seem not to be the same. What is the difference? Is the former associated with operational semantics while the latter with denotational semantics? Thanks. 

Sometimes I see people put side conditions above the inference line as if they were premises of an inference rule. This feels strange. My understanding (which may be wrong) is that a side condition belongs to the meta-theory, not to the object-theory: an object-proof of a side condition should not be required and may even not be possible. So what is the motivation of doing this? For saving space or for some other deeper reason? $Update$: I am no longer quite sure whether a side condition belongs to the meta-theory. But at least it is outside the object-theory the inference rules describe,that is, the truth of a side condition cannot be derived using the inference rules. $Update^2$: Take as an example the rule for typing variables from the simply-typed $\lambda$-calculus. $(x : T) \in \Gamma$ is a side condition that tests whether the variable $x$ of type $T$ is in the typing context $\Gamma$. In some presentations, the side condition is put aside, leaving the premises empty to indicate that it is an axiom, though conditional: $$\frac{}{\Gamma \vdash x : T} (x : T) \in \Gamma$$ while in others, it is put above as if it were a premise: $$\frac{(x : T) \in \Gamma}{\Gamma \vdash x : T}$$ 

Copied from the comments on the question as per-request. I have taken theory of distributed computing with Michel Raynal and he described a third model, where messages can be dropped randomly. In that model a message can fail silently to be delivered, but that doesn't necessarily mean that the node has failed. It is about link failures rather than node failures "fair lossy channel model", you can read more about it here : Quiescent Uniform Reliable Broadcast as an Introductory Survey to Failure Detector Oracles - Michel Raynal (ftp.irisa.fr/techreports/2000/PI-1356.ps.gz) 

I am not sure if this answers your question but I hope it could be meaningful and leads to some insight. Assume that there is a turing machine $X$ that can simulate every atom in the universe including itself, it then necessarily can simulate itself. Now, reducing that to the halting problem is trivial: Let $X$ take a turing machine $M$ as its input and decides whether it halts or not by simulating the universe (since $M$ is included in the universe), then do the opposite (e.g. $X$ halts if $M$ does not, and loops forever if $M$ halts). Then $X(X)$ demonstrates a contradiction. Essentially this means that the best $X$ can do to decide whether $X$ halts or not is just by running itself (i.e. let the universe work its way), so simulating the universe doesn't give an advantage. The same applies when you want the state of the universe after $t$ time. Since $X$ can not decide if it will halt within $t$ time or not within $t$ time (same argument), then it will let it to the universe to do it. Trying to simulate the universe doing it, can not reduce the time you'll take to decide. And if deciding how the universe will look like in $t$ time takes more than $t$ then the simulation will diverge (as $t$ goes to infinity). This leads to the conclusion that only useful simulator that decides how the universe will look like in $t$ time must take exactly $t$ time, i.e. by letting the universe work. This simulator is then indeed the trivial simulator. 

In fact there is a whole journal that is intended to be funny. The journal of craptology. The topics are usually related to cryptography. There are also some sessions videos (!) One example is the Volume 4 paper of Cryptography in a Hitchhiker's Universe (section 5) is : 

DISCLAIMER: I am not an expert in caches so this might be a naïve idea, and also may be a known idea which I've never heard of before. So excuse me if I fail to cite its reference (if it exists); and please inform me if there is a reference for it to edit the post and add it. (I am suspecting it might have a reference because it is so intuitive). A quick solution after being inspired by Strilanc maybe to just keep a associative map of maximum $c$ entries (where $c$ is some constant) associating an item with the number of times it has has been seen. When the associative map is full and you meet a new item not in the map, flip a coin to add it or not. If you are to add it, then remove an item with probability inversely proportional to how many times it has been seen so far. 

I've looked in the Zoo and it seems it is not true because $PH \subseteq P^{PP}$. Nonetheless I've passed by a paper that appears to have used a positive result. It was in the context that $f : \{0,1\}^* \mapsto \{0,1\}^*$ is a function, computable by probabilistic polynomial time oracle machine $M$ with access to an arbitrary oracle $A$. The authors say and I quote: 

where $G$ is a probabilistic polynomial time oracle machine. They do it again also for $S^{N^A,M^A}$, $S,N$ being other probabilistic polynomial time oracle machine. Update: the paper is "Notions of reducibility between cryptographic primitives" - 2004. It was in the proof of Lemma 1 of the part that "If there exists a fully-black-box reduction from a cryptographic primitive P to cryptographic primitive Q then there exists a relativizing reduction from P to Q as well." The definitions of fully-black-box and relativizing reduction is in the paper. The question now, according to Lance Fortnow's answer that they are not equal, does that mean that this is a gap in the proof ?