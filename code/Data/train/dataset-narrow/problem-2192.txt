A lot depends upon the conference/journal, as each community has developed its own style, so knowing what's expected of the conference/journal certainly will help a little. 

There has been a lot done applying category theory to regular languages and automata. One starting point is the recent papers: 

One day maximum for a conference paper, including reading time. For journal papers, especially long ones, reading the paper carefully might take a whole week. 

Discourage audience laptop usage during presentations. Of course I cannot say for sure which laptop users are listening to the talk and which ones are reading email, but I always find it discouraging (if not rude) as a speaker to be speaking to a wall of heads buried in laptops. Naturally some people are looking up definitions or related work and so forth, to enhance their listening experience. But having I seen plenty of people reading email or facebooking, I really wonder why they are even in the room. 

Regarding 4: There's plenty of work combining formal methods and testing. Googling "formal methods testing" reveals quite a large amount of work. Although there are many different goals of such work, one of the key goals is to generate more effective test suites. This talk gives a nice introduction, based on model checking. Also regarding the issue software security, which has been edited out of the question: formal methods need to work harder to deliver in that realm. Typically one writes a specification for a piece of software and verifies that the specification is satisfied, namely, that when the input satisfies the precondition that the output satisfies the postcondition. To ensure secure software one also needs to check also that the software behaves sensibly for input that does not satisfy the precondition. (This is of course equivalent to setting the precondition to true for all input.) The space of all inputs is typically much larger than just well-formed input, but typically this is one place where even functionally correct software can be violated, simply by violating its assumptions. Regarding point 3: Some work has been done for verifying systems in settings where faults are explicitly modelled, including Faulty Logic: Reasoning About Fault-tolerant Programs. Matthew Meola and David Walker. European Symposium on Programming, 2010. Work on probabilistic model checking and probabilistic verification certainly both also address the issue of faults to some degree. 

Gurevich in formulating the conjecture about a logic that could capture $\mathsf{P}$ requires the logic to be computable in two ways: (1) the set of sentences legally obtainable from the vocabulary $\sigma$ has to be computable, given $\sigma$; and (2) the satisfiability relation needs to be computable from $\sigma$, i.e., ordered pairs consisting of a finite structure $M$ and a sentence $\varphi$ such that all models isomorphic to $M$ satisfy $\varphi$. Also, significantly for comparison with this randomized logic result, the vocabulary $\sigma$ has to be finite. (A vocabulary is a set of constant symbols and relation symbols, for example, equals sign, less-than sign, $R_1,R_2,\ldots$) This is a paraphrase of Definition 1.14 of this paper by Gurevich, which is reference [9] in the quote Kaveh gave. The paper about BPP and randomized logic presents a significantly different framework. It starts with a finite vocabulary $\sigma$, and then considers a probability space of all vocabularies that extend $\sigma$ with some disjoint vocabulary $\rho$. So a formula is satisfiable in the new randomized logic if it is satisfiable in "enough" logics based on extensions of $\sigma$ by different $\rho$. This is my butchering of Definition 1 in the Eickmeyer-Grohe paper linked to by Robin Kothari. In particular, the vocabulary is not finite (well, each vocabulary is, but we have to consider infinitely many distinct vocabularies), the set of sentences of this logic is undecidable, and the notion of satisfiability is different from the one put forth by Gurevich. 

This answers the question asked in the question-title, but not the one asked in the question. A shocking example of jump-in-hardness arises from the question, "How many satisfying assignments does a planar formula have, modulo $n$?" This was widely thought to be #P-hard, and it is for "most" values of $n$, but if $n$ is a Mersenne integer (for example $n=7$, because 7 is of form $2^3-1$), then the answer can be computed in polynomial time. This was first discovered by Valiant, in his groundbreaking Holographic Algorithms paper. 

Criteria: Novelty/originality, expected impact, correctness/validity, extensiveness (how much of the problem is studied? + theorems? + implementation? +experimental results?), quality of presentation. The criteria for journal acceptance are much higher than for conferences. For a conference you give a score based on the previous criteria. Verifying correctness as much as is possible is very important, especially for journal articles. 

Perhaps the point is that all of these models aimed to capture what the notion of computability is. The fact that all of them are equivalent, means that the notion they are trying to capture is robust. So although this does not escape your dilemma, this robustness gives credence to the notion that "a function is computable if there is a Turing machine that computes it". 

I know of work adding temporal modalities to linear logic to produce what has been called temporal linear logic (in contrast to LTL = linear-time temporal logic). This is quite interesting: a formula (without a modality) is interpreted as resources being available now. The next time modality $\bigcirc-$ is interpreted as resources being available in the next time step. The box modality $\Box-$ means that the resources can be consumed at any point in the future, determined by the holder of the resources, whereas $\lozenge-$ means that the resources can be consumed at any point in time determined by the system. Notice the duality between the holder of the resource and the system. 

corresponds to the greatest fixed point of the functor $F(X) = (X+1)^A$. Now assume that there is some relation $R:\subseteq A\times A$. This has no intrinsic meaning, and is just used to formulate the following coinductive predicate. Thinking of $R$ as $\le$ is sufficient. Consider the following coinductive predicate $covers\subseteq$$\times$ (defined corecursively): 

Is the Fast Fourier Transform the algorithmic problem solved most times per day by real computer systems? It has to be close. So I'd nominate the Cooley-Tukey FFT algorithm. 

Edited to add: I don't think this answer is on point, but I will leave it up. Now community wiki so I don't get any more reputation for this. The lower bound is not correct. Intuitively, this is because, to decide membership in $L$, I need to know the description of the DFA, but I may not need to run the DFA on the input if I am a Turing machine. More formally, there is a Kolmogorov Complexity Characterization Theorem for Regular Languages. It states in part that the following are equivalent: 

Suppose we have an orthogonal polygon with holes (all walls are axis-parallel). All vertices can be on integer coordinates, if that helps. Partition the polygon into rectangular rooms. I would like to find the best room to start from, to visit all the rooms (rectangles). There's a limitation on my movement: in any room, I can only leave by two directions, say north and west. (Here best means there would only be one source in the plane dual graph with directed edges showing how to walk from room to room. If more than one source is required, I wish to minimize them.) I have been looking at art gallery problems, and at VLSI papers on building rectilinear floorplans from network flows, and they are all tantalizingly close but far. Can anyone provide suggestions so I can focus my search/proof construction? EDIT to fix problem pointed out by Peter Taylor. I can choose two directions per room. (probably they need to be adjacent, so NE is ok but NS is not.) If I enter one room northward, I am automatically choosing South as one of thst new room's directions. (so only two in or out directions per room) If I choose a direction, and there are multiple rooms adjacent in that direction, I can enter all of them (and all of them then have the reverse direction assigned as one of their two directions), so the naive greedy approach would be to choose the direction that maximizes the number of rooms I can enter at that stage. I hope this is now complete, and understandable. 

Perhaps more interestingly, though I can hardly imagine, are the references in these papers. They point to many varied techniques for simulating zombie and other infections. There is also a lot of work on viruses spreading through social networks (which surprisingly enough is applicable to computer viruses spreading through online social networking sites). Two of many papers available on Arxiv are Affinity Paths and Information Diffusion in Social Networks and Viral Processes by Random Walks on Random Graphs. The book cited by the other reference and a similar one called Networks by Mark Newman have chapters covering the topic too, though without the zombies. 

Naturals and bools and operations on them can be encoded in the pure lambda calculus in a straightforward manner, as so-called Church numerals (and Church bools, I guess). It is not clear how one would encode integers nicely, though it can obviously be done. 

Another mechanical verification of the 4-colour theorem has been done by George Gonthier at Microsoft Research Cambridge. The difference with his proof is that the entire theorem has been stated and mechanically verified using the Coq proof assistant, whereas the other proofs contain only the kernel calculation written in Assembly Language and C, and thus have a risk of being buggy. Gonthier's proof covers both the calculational aspects and the logical ones in just 60,000 lines of Coq. 

Coq's tactic language has the limitation that the proofs written using it hardly resemble proofs one does by hand. Several attempts have been made to enable clearer proofs. These include Isar (for Isabelle/HOL) and Mizar's proof language. Aside: Did you also know that the programming language ML was originally designed to implement tactics for the LCF theorem prover? Many ideas developed for ML, such as type inference, have influenced modern programming languages. 

Yes, there are several papers on this topic, and related ones, in the context of ant colony optimization algorithms for routing on ad hoc or mobile area networks (MANETs). In a MANET, the nodes in the network graph are mobile, and if they move too far away from their neighbors, they fall out of range, hence the communication link is broken (i.e., the edge disappears from the network graph). The routing problem essentially is: how do I keep all the vertices connected, and maintain efficient pathways between nodes, when links can disappear and reappear? Of course, if a node moves out of range from all other nodes, it is the same as if the node were deleted from the network graph. The paper An ant colony optimization routing based on robustness for ad hoc networks with GPSs by Kadono et al. contains a "related work" section you will probably find interesting. In this paper I've linked, there is an assumption of the availability of some GPS information, which probably does not apply to you; I chose it mainly for its discussion of other papers. However, all these approaches assume something about how nodes can be deleted (or suddenly appear), in order to construct an efficient algorithm. You'll have to decide what formal assumptions hold for the problem you are trying to solve. Search phrases like "ant colony optimization MANET" or "ant colony optimization self-stabilization" may turn up other papers of interest to you. 

Stanford University now has a Youtube channel, with free access to HD video of full courses on everything from dynamical systems to quantum entanglement. More conferences and workshops are videotaping their talks. What are videos online that you think everyone should know about? I'll seed this with a few answers to presentations that are mostly expository, but what I'm hoping might happen is that this community wiki could turn into a resource to share excellent presentations of new research, as well as a place to learn (or reinforce) background in an unfamiliar area. 

One could think of a coinductive predicate as an infinite conjunction or disjunction, simply by imagining that it is unrolled completely. Game semantics in this setting would be straightforward: for an infinite conjunction, the Falsifier needs to select which conjunct to falsify in order to win the game; for an infinite disjunction, the Verifier needs to select which disjunct to satisfy in order to win the game. There is, however, a natural order in 'evaluating' the coinductive predicate which is ignored when considering it as an infinite conjunction or disjunction, and I would like this ordering to be captured in the game semantics, namely that the game proceeds by playing each disjunct/conjunct in turn. One additional problem I have is understanding which winning condition to use to capture the infinite nature of the predicate. Or to put it in lay persons terms: how do I know that there is no white cat in this supposed infinite stream of black cats––it could merely be after the point I stop looking? additional example Consider the following data type (again in Haskell): 

An excellent book is Rudiments of $\mu$-calculus. It's not cheap though. You'd be better of learning from survey articles such as 

Journal reviews tend to be more extensive and can include a lot of things for the authors to do to bring their paper into an acceptable state, such as "implement and evaluate these ideas". Your confidence should be based on how confident you feel. Generally this will come with experience, so start out being conservative. It it's out of your field, even though you may understand the paper, being conservative is also a good idea –– it's difficult to assess originality of a paper outside of your field. Some journals say that journal submissions must contain a substantial amount of new material compared to the conference version. 30% is a figure I've heard.