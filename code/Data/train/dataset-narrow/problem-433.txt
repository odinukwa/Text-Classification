If performance is important, Option1 has the clear edge. Every time a query goes across a linked server, performance takes a hit as what you expect to behave like set-based operations become serialised by OLEDB. They go row-by-row. It helps if you follow best practices for linked server queries (OPENROWSET for example) and ensure all remote processing is done on the other side, but the results will still come across serialised. (Look for Conor Cunningham's presentation on Distributed Queries). If all your remote tables are small lookup tables this may not be a serious issue, but if millions of rows have to come across linked servers then performance will suffer. There used to be an extra layer of problems with invisible statistics (for data readers) over linked servers, but as you're running SQL 2014 this will not affect you. If you can afford the Dev cost of eliminating the linked servers, Just Do It! 

Performance advantages of CCIs are not only space-related: batch execution mode is also there to speed things up (in supported operators). Batch sizes can vary from 64 to 900 rows, so it would be reasonable to expect that using smaller datatypes would lead to 'fuller' batches, closer to the 900 max figure. $URL$ Being economical with datatypes is a good habit anyway - why consider bigint if tinyint (or bit) would do the job? 

In the code below, the fails when executed as-is, but removing the in the and replacing it with a static value works fine. Is there a different syntax required when using with an ? 

However, as per the page for sys.syslanguages there are many more listed How should these 2 pages be interpreted together? Once says there are 10 SQL Server languages, and Report Server supports those 10. The other says there are many more, so are these supported by Report Server or not? 

Best practice would be to install and use it from client machines, since every tool running on the server is consuming some resources which will not be available to the Oracle server Installing it on the server -- probably no penalty as pointed out in the comments 

However SSAS itself supports 128 axes I believe. What can I use to run ad hoc queries against SSAS with more than 2 axes? 

I've read about it and want to play around with it for a bit. I think it requires a minimum of 4-5 machines to run, which is feasible with Azure IaaS, but how do I actually deploy it there? 

So you've got 5 'stacked' instances on a single Windows server. You haven't said exactly how many sockets/CPUs are available and how much memory though. I like setting affinity for each instance in such cases, even if I decide to have CPUs overlapping in the struggle to balance the overall CPU load (depends on each instance's load). Any instance with more than 4 CPUs could use an explicit DOP setting in my experience - rarely over '4' in stacked cases like yours. Don't forget to set 'Cost threshold of Parallelism' for each instance to something reasonable (50?) to avoid excessive parallelism - in your case this is even more important. Remember that the memory left "for the OS" should be more now, since you have to account for the footprint of each instance (on top of SSIS etc). Check in SQL Config Mgr if SSAS is also running and adjust its 'max memory' accordingly, by default it goes for 80% of the whole server memory (!) Also maybe worth taking away and 'Lock pages in memory' rights of the SQL Service account(s) so that the OS can breath and do its job better (if it pages, everyone suffers!). Also good practice is to set some reasonable 'min memory' for each instance. I think running sp_blitz and sp_blitz_first on each instance would give you some quick pointers on more pressing issues. You may also want to monitor some windows permon counters like 'available memory' and 'working set' for each of the processes running there in case you find particular times of the day/night when the server is suffering. 

It is possible to set up a method to grant rights to run a job that a user does not have enough authority to run on its own. EDIT: For clarity on the three options presented by explicitly mentioning the SQLAgentOperatorRole as an option and by adding some explanation on the third solution. (1) If the user is allowed to manage the execution of all jobs, then make that user member of SQLAgentOperatorRole. The user will be able to start (as well as stop, enable, and disable) any SQL Agent job on that server. (This solution turned out to satisfy the original asker.) (2) Erland Sommarskog has written a lot on how to grant permissions through stored procedures using counter-signatures. He has a solution at: $URL$ The key point is: "To be able to start a job owned by someone else, you need to be member of the fixed role in . A start is to write a stored procedure that calls for this specific job, sign that procedure with a certificate, and then create a user from the certificate and make that user a member of ." (3) My general resolution was to create a stored procedure in the database allowing a user to start jobs owned by someone else. This requires a table to maintain the configuration of who can run which job. Since the following table is SQL Server Agent Job specific, I would create the table in . But it could be created in some other service database if desired. 

Checkdb creates a snapshot in the background. Snapshots are supported by sparse files (they look large in Windows but are typically almost empty). Could it be that you are looking at this file? 

Yes, but only from SQL2012 onwards, if I remember correctly from Bob Ward's 2013 PASS session (gave me a headache!) 

Long shot perhaps, but worth checking file \Program Files\Microsoft SQL Server\100\DTS\Binn\MsDtsSrvr.ini or the equivalent on your setup. You may have to manually edit it with the instance name. Otherwise SSIS connections might be looking for an msdb of a default SQL instance that doesn't exist. 

The most comprehensive way in my view would be to encrypt/decrypt the database with TDE. This will ensure that each and every page will change in memory and will be flushed to disk. I've tried this with success on 'legacy' dbs that were originally created in SQL2000, after I discovered that several pages didn't have actual checksums on them (0x...200) if you look at the header with dbcc page. If you were to try this, I would recommend testing it on a restored version of the live db, just in case you have undetected corruption that could be caught and stall the encryption process. There are flags to deal with it, but better play it safe. Obviously you'll want to backup the certificate used by the encryption, so you are covered for any eventuality during the time the db is encrypted. If anyone has a better idea for writing checksums on all pages, I'd love to hear it :-) 

From your application layer, stamp each query with a timestamp and batch ID. Instead of executing the queries, store them in a temporary table with the format 

However, it requires me to write the last line in a customized manner for every table I apply it to Instead of the above, is it possible to write something of the sort: 

However, in 2 separate sessions, if I run a long running select with in the 1st and an in the 2nd (or vice versa), whichever query starts first blocks the second query (as per ) Looking at , both queries have a transaction isolation level of read committed (2) As per my understanding, with snapshot isolation on, tempdb usage should increase however blocking should not occur in this situation. Am I missing some configuration steps to achieve this behaviour? 

Depends on who is allowed to access your application. Probably not a good idea to give DB access to users since they'll be able to connect to the database directly which is probably not what you want. If its a whitelist, maintain a list of users and if applicable which roles they hold in the database. Run your web application using a Windows account, and grant access to that identity to the database. Authenticate users using Windows Authentication, dont perform password management on your own. 

It would probably had been slightly better if you had a version of the database before running checkdb with REPAIR_ALLOW_DATA_LOSS. I feel for you - can't be obsessive enough when it comes to backups... 

Unfortunately, as is the case with other counters, the definition of 'reads' is not identical across the board. If a plan has calls to UDFs, statistics IO may under-report them (or hide them completely) while profiler still displays them. Regarding the PLE counter, if the server has more than one (physical) NUMA nodes, it is important to use the ones under 'SQL Server: Buffer Node'. The PLE figure under 'SQL Server: Buffer Mgr' is an average of the Node PLEs, and can be hiding horrors sometimes (I've seen PLE of 400 on one node and 7000 on the other, with SQL supposedly using both nodes). 

Also if you want to restrict number of concurrent connections then Resource Governor comes in handy. Can even tailor it to specific logins only. 

There is enough info for you to estimate roughly how many pages/extents were lost (deallocated by the REPAIR_ALLOW_DATA_LOSS option). What good is that though? Without backups there is no natively-supported way to recover the data. What logs are you referring to? Transaction logs or Errorlogs? TLog entries need to be interpreted (not for the faint-hearted) and then applied to a consistent database file (which you haven't got). Errorlogs are useless for data retrieval anyway. 

Receive side scaling (RSS) improves the system performance when handling network data on multiprocessor systems. This should result in better scaling for receiving data from the network. One place to find details is at: $URL$ From this I would infer that the Receive side scaling primarily benefits the receipt of data from the network. There is a link that you might find interesting for an SAP installation and, although posted in 2012, seems to be using pre-Windows 2012 Operating Systems. Nonetheless, you may benefit from some of the details. One of the charts shows that only 1 processor is consuming the reads, while other processors are idling along doing not so much. See the charts at: $URL$ Therefore, it seems that RSS is primarily affecting how quickly Windows can receive the data and make it available to other processes, such as the SQL Server. This depends on the RSS Profile you choose. For example: 

I do not use the designer for making changes, so I have no particular experience with script problems. I did some years ago find that the designers were prone to errors, so I am not surprised that you have some problems. Nonetheless, you find the tools valuable. From your description, you wind up with two scripts, but they are not identical. So the problem is how to treat the scripts. Or how to avoid any need to deal with the issue. Database backups or database snapshots are a couple of tools that can help. If you are using SQL Server Enterprise or Developer editions, you have the ability to create database snapshots. So this solution depends on your SQL Server edition and you rights on that server and how many people are sharing that database. Scenario using a snapshot database and assuming that you have full rights to do whatever you want to with the design database. First, create a SNAPSHOT database (examples from the SQL Server documenation):