Here are loose lower and upper bounds. Fix $d \le n$ as in the post. Let $k^*$ denote the largest possible value of $k$ meeting the conditions in the post. We show that $k^* = \exp(\Theta(d\log d)$) for $d\ge 3\sqrt n$, but $k^* \le 2$ for $d\le n^{1/3}$. I suspect the latter bound can be strengthened (e.g. shown for larger $d$). Lemma 1. $d!/{n\choose d} \le k^* \le d!$ Proof. The upper bound $k^*\le d!$ holds simply because each chosen permutation induces a distinct permutation on the $d$-subset $\{1,\ldots,d\}$. For the lower bound, say that two permutations of $[n]$ conflict if, for some $d$-subset $T$ of $[n]$, they induce the same relative order on $T$. A given permutation $\sigma$ induces a relative order on $n\choose d$ such $d$-subsets. Each $d$-subset $T$ and relative order on $T$ is induced by $n!/d!$ permutations of $[n]$. It follows that any given permutation $\sigma$ conflicts with at most $c = {n\choose d} \times n!/d!$ other permutations. So, any maximal set of pairwise non-conflicting permutations must have size at least $n! / c = d!/{n\choose d}$. $\diamond$ Corollary 1. If $d\ge 3\sqrt n$, then $k^* = \exp(\Theta(d\log d))$. Proof. By Lemma 1 and calculation from Stirling's approximation, $$k^* \ge d!/{n\choose d} = \frac{d^{2d+1}\exp(d^2/2n - 2d+O(1))}{n^d} = \exp(\Omega(d\log d))$$ for $d\ge 3\sqrt n$. For the upper bound, note $k^*\le d!= \exp(O(d\log d))$. $~~~~\diamond$ Lemma 2. If $2\le d\le n^{1/3}$, then $k^* = 2$. We give two proofs. Proof 1. Consider any $d$ with $2\le d\le n^{1/3}$. To see that $k^*\ge 2$, note that (as Zihan noted in his comment) for $d\ge 2$ every $d$-subset is distinguishable by the identity permutation and its reverse. To finish we show $k^*\le 2$. Fix any three permutations $\sigma_1, \sigma_2, \sigma_3$. Assume WLOG that $\sigma_1$ is the identity permutation: $\sigma_1(k) = k$ for $k\in [n]$. We show that some $d$-subset of $[n]$ is not distinguishable by some pair of the three permutations. If $\sigma_2$ has an increasing subsequence of length $d$, then that subsequence induces the same permutation on its $d$-subset $T$ that $\sigma_1$ does, so $T$ is indistinguishable by $\sigma_1$ and $\sigma_2$, and we are done. Otherwise, by the Erdős–Szekeres Theorem, $\sigma_2$ has a decreasing subsequence of length $n/d \ge d^2$ on some subset, say $T'$, of $[n]$. On $T'$, $\sigma_1$ is increasing and $\sigma_2$ is decreasing. Applying the theorem to $\sigma_3$ restricted to $T'$, $\sigma_3$ has either an increasing subsequence of length $\sqrt{d^2} = d$ on some $d$-subset $T''$ within $T'$, or a decreasing subsequence of that length on some $d$-subset $T''$ within $T'$. In the first case, $\sigma_1$ and $\sigma_3$ don't distinguish $T''$. In the second case, $\sigma_2$ and $\sigma_3$ don't distinguish $T''$.$~~\diamond$ Here's an alternate proof of Lemma 2, by adapting the proof of the Erdős–Szekeres Theorem, for those who are interested in the details. Proof 2. Consider any $d$ with $2\le d\le n^{1/3}$. To see that $k^*\ge 2$, note that (as Zihan noted in his comment) for $d\ge 2$ every $d$-subset is distinguishable by the identity permutation and its reverse. To finish we show $k^*\le 2$. Fix any three permutations $\sigma_1, \sigma_2, \sigma_3$. Assume WLOG that $\sigma_1$ is the identity permutation: $\sigma_1(k) = k$ for $k\in [n]$. Assume for contradiction that every $d$-subset $T$ of $[n]$ is distinguishable by each pair of these three permutations. For each pair $i, j$ with $1\le i < j \le 3$, define $I_{ij}(k)$ to be the maximum size of any subset $T\subseteq [k]$ that contains $k$ such that the order imposed on $T$ by both $\sigma_i$ and $\sigma_j$ is monotonically increasing. Define $D_{ij}(k)$ to be the maximum size of any subset $T\subseteq \{1,\ldots,k\}$ that contains $k$ such that the order imposed on $T$ by both $\sigma_i$ and $\sigma_j$ is monotonically decreasing. Define 3-tuple $$s(k) = \big(I_{12}(k),\,I_{13}(k),\,D_{23}(k)\big),$$ and $S = \{s(k) : k\in[n]\}.$ Claim 1. For every $k,\ell$ with $1\le k<\ell \le n$, we have $s(k) \ne s(\ell)$. Before we prove the claim, note that it implies the lemma as follows. The claim implies that $|S| = n$. Each of the three values in any 3-tuple $s(k)$ is in $[d-1]$ (because, for example, there is a subset of $[n]$ of size $I_{12}(k)$ that is not distinguishable by $\sigma_1$ and $\sigma_2$). So $S\subseteq [d-1]^3$, so $n = |S|\le(d-1)^3$, implying that $d>n^{1/3}$, contradicting that $d\le n^{1/3}$ and proving the lemma. To prove the claim, consider any $k,\ell\in[n]$ with $k<\ell$. Consider the relative order that each of the three permutations induces on the pair $k, \ell$: for each $i\in[3]$, either $\sigma_i(k) < \sigma_i(\ell)$ or $\sigma_i(k) > \sigma_i(\ell)$. Since $\sigma_1$ is the identity, $\sigma_1(k) < \sigma_1(\ell)$. First consider the case that $\sigma_2(k) < \sigma_2(\ell)$. By definition of $I_{12}$, there is a subset $T$ of $[k]$ that contains $k$ such that $|T| = I_{12}(k)$ and $\sigma_1$ and $\sigma_2$ are monotonically increasing on $T$. So $T' = T\cup\{\ell\}$ is a subset of $[\ell]$ that contains $\ell$ such that $|T'| = I_{12}(k)+1$ and $\sigma_1$ and $\sigma_2$ are monotonically increasing on $T'$. It follows that $I_{12}(\ell) \ge |T'| > I_{12}(k)$. It follows that $s(\ell) \ne s(k)$ in this case. For the case when $\sigma_3(k) < \sigma_3(\ell)$, by the same argument $I_{13}(\ell) > I_{13}(k)$, so $s(\ell)\ne s(k)$ in that case. In the remaining case, $\sigma_2(\ell) < \sigma_2(k)$ and $\sigma_3(\ell) < \sigma_3(k)$. By a similar argument $D_{23}(\ell) > D_{23}(k)$. So $s(\ell)\ne s(k)$ in all cases, proving the claim. $\diamond$ EDIT: Comment on the tightness of the proof of Lemma 2. The proof of Lemma 2 shows the existence of a $d$-subset $T$ which is indistinguishable by two of the permutations and that has the following stronger property: the two permutations are both increasing or both decreasing over $T$. Given this, I believe the bound of $\Theta(n^{1/3})$ on $d$ is best possible. That is, I believe that for, say, $d\ge 2n^{1/3}$, choosing appropriate random permutations w.h.p. yields $\exp(\Theta(d\log d))$ permutations such that, for every pair $\sigma$ and $\sigma^*$ of the permutations, there is no $d$-subset $T$ on which $\sigma$ and $\sigma^*$ are both increasing, or both decreasing. (The argument is similar to the proof of Lemma 1 above, with the additional observation that, for a random permutation, w.h.p. there are only about ${n\choose d}/d!$ $d$-subsets on which the permutation is increasing or decreasing.) So, to strengthen the lemma, the proof would have to somehow consider indistinguishable subsets on which the two permutations are not monotonic. 

You can enumerate exactly the decidable languages. I've given this question as a homework problem so I'll just give a hint here: You can modify a TM $M$ to a machine $M'$ such that if $M$ is total (halts on all inputs) then $L(M')=L(M)$ and if $M$ is not total then $L(M')$ is finite. By request I'm burning the homework question and putting in the full proof. I heard the problem from someone else (don't remember who) so this isn't original. Define $M'(x)$ to accept if $M(x)$ accepts and $M(y)$ halts for all $y$ with $|y|\leq |x|$. Note $M'$ has the property mentioned above. Let $M_1,M_2,\ldots$ be a standard enumeration of Turing machines. The enumeration $M'_1,M'_2,\ldots$ is an enumeration of Turing machines such that $D=\{L(M'_1),L(M'_2),\ldots\}$ is exactly the set of decidable languages. DECIDABLE is contained in $D$: If $L$ is decidable then $L=M_i$ for some total $M_i$ so $L(M'_i)=L(M_i)=L$. $D$ is contained in DECIDABLE: If $M_i$ is total then $L(M'_i)=L(M_i)$ is decidable. If $M_i$ is not total then $L(M'_i)$ is finite and thus decidable. 

You can just take the oracle A s.t. NP$^A$=EXP$^A$ since EXP is not in i.o.-subexp. For SAT$^A$ it depends on the encoding, for example if the only valid SAT instances have even length then it is easy to solve SAT on odd-length strings. But if you use a language like $L=\{\phi 01^*\ |\ \phi\in SAT^A\}$ then you should be fine. 

Given a sequence $a_1,\ldots,a_n$, find a $k$ and numbers $1\leq i_1< \dots< i_{2k}\leq n$ that maximizes $(\sum_{j=1}^{k}(a_{i_{2j}}-a_{i_{2j-1}}))-2k$. I can get an $n^3$ algorithm using dynamic programming. Is there a better algorithm? 

Scott Aaronson gives an oracle where $\oplus$P = PEXP which implies the oracle you want. $URL$ (Theorem 12 in the appendix) 

The proof of Ladner's theorem doesn't use any special properties of P and NP and the same proof unchanged will show, assuming EXP<>PSPACE, there is a language L in EXP-PSPACE and not EXP-complete under either P-time or PSPACE-reductions. You need the full Landner look-back trick to keep L in EXP. 

Proof of Claim. Choose $\pi$ by the following process. Choose $\pi(1)$ uniformly from $[n]$, then choose $\pi(2)$ uniformly from $[n] - \{\pi(1)\}$, then choose $\pi(3)$ uniformly from $[n]-\{\pi(1),\pi(2)\}$, etc. Consider any edge $(i,i+1)$ in $S$. Consider the time just after $\pi(i)$ has been chosen, when $\pi(i+1)$ is about to be chosen. Regardless of the first $i$ choices (for $\pi(j)$ for $j\le i$), there are at least $n-i$ choices for $\pi(i+1)$, and at most $2n^{1-\epsilon/2}$ of those choices will give the edge $(i,i+1)$ cost less than $n^{1-\epsilon/2}$ (making it cheap). Thus, conditioned on the first $i$ choices, the probability that the edge is cheap is at most $\frac{2n^{1-\epsilon/2}}{n-i}$. Thus, the probability that all $n/2$ edges in $S$ are cheap is at most $$\prod_{(i,i+1)\in S} \frac{2n^{1-\epsilon/2}}{n-i}.$$ Since $|S|\ge n/2$, there are at least $n/4$ edges in $S$ with $n-i\ge n/4$. Thus, this product is at most $$\big(\frac{2n^{1-\epsilon/2}}{n/4}\big)^{n/4} ~\le~(8n^{-\epsilon/2})^{n/4} ~=~\exp(O(n)-\Omega(\epsilon n \log n)) ~=~\exp(-\Omega(\epsilon n \log n)).$$ QED 

Proof. Fix $n$. Let $G$ be the random graph described above. Since $G$ is bipartite any complete $k$-partite subgraph has to be bipartite. First, I claim that with probability $1-o(1)$, every complete bipartite subgraph of $G$ has at most $4n$ edges. (Here's why. For any pair of subsets $L\subseteq [n]$ and $R\subseteq [n]$ with $|L\times R| \ge 4n$, the probability that the complete bipartite subgraph with edge set $L\times R$ is in $G$ is $2^{-|L\times R|} \le 2^{-4n}$. There are fewer than $2^n\times 2^n = 4^n$ such pairs $L$ and $R$, Thus, by the naive union bound, the probability that any of the corresponding subgraphs is present in $G$ is at most $4^n 2^{-4n} \le 2^{-2n}$.) Also, with probability $1-o(1)$, the graph $G$ has at least $n^2/4$ edges. Thus, with probability $1-o(1)$, every complete bipartite subgraph in $G$ has at most $4n$ edges, and $G$ has at least $n^2/4$ edges. Assume this happens. Now suppose for contradiction that a collection of subgraphs with the desired properties exists. Each of the subgraphs has edge set $L\times R$ for some pair of subsets $L$ and $R$. In $G$, direct all the edges in $L\times R$ from the larger side to the smaller side (to the left if $|L|\le |R|$, and to the right otherwise). Since $|L\times R|\le 4n$, the smaller of $L$ or $R$ must have size at most $2\sqrt n$. Since each vertex is in $O(1)$ of the subgraphs, each vertex now has $O(\sqrt n)$ edges directed out of it. But all edges are directed one way or the other, so the number of edges in $G$ is at most the number of vertices times the maximum out-degree of any vertex, that is, at most $O(n\sqrt n)$. This contradicts the graph having at least $n^2/4$ edges. 

Could one use list decoding of Reed-Solomon codes to show Andreev's POLY function is in P, similar to the way Sivakumar did in his membership comparable paper? Or is the POLY function known to be NP-complete? 

To throw in another view, IP is the generalization if you think about NP as what you can prove to a polynomial-time skeptic. 

Depends on your definition of NPI. If A is incomplete for Turing reductions, the answer is yes since SAT is not in $P^A$. If A is just many-one incomplete then we don't know how to prove it. We have a relativized world with there is a set A in NP such that A is not NP-complete via many-one reductions but SAT can be computed by a single query to A. (Theorem 1.9 in this paper). 

I don't really have references for these results--they aren't hard to prove once you understand Ladner's theorem. 

1a) yes by the inclusion. b) Not sure what you mean by "it". If P=PSPACE then A and B are both in P. 2a) Not necessarily, A could be much easier than B. b) There are relativized worlds where P=FewP<>NP. c) No, RP^UP does not necessarily contain NP. Don't confuse UP with Promise-UP. 

This is $\Pi_2^p$-complete. Let D be the usual reduction from SAT to CLIQUE. Let w be a string in CLIQUE. Consider a $\Pi_2^p$ language $L$ expressed as the set of $x$ such that for all $y$ there is a $z$ with $(x,y,z)$ in $A$ for some $A$ in P (with polynomial-length bounds on the $y$ and $z$). Let $\phi_{x,y}$ be the formula that will be satisfiable if there exists a $z$ such that $(x,y,z)$ is in $A$. Define $C_x(\phi)$ as follows: If $\phi=\phi_{x,y}$ for some $y$ then let $C_x(\phi)=w$ otherwise set $C_x(\phi)=D(\phi)$. Then $x$ is in $L$ if and only if $C_x$ is a reduction from SAT to CLIQUE. 

Upper bound of $O(\log N)$ I only sketch the proof. Fix any sequence of circles. We will argue that as $N\rightarrow \infty$, the total distance traveled by the bug in the first $N$ steps is $O(\log N)$. Assume without loss of generality that the first circle has radius 1. Fix an arbitrarily large $N$. Let $p$ by any point in the intersection of the first $N$ circles. Note that because of the way the bug moves, in each step that the bug moves it gets closer to $p$. First, consider steps where the following ratio is at least $1/\log N$: $$ \frac{\mbox{the reduction in the distance to } p}{\mbox{the distance traveled in the step}}.$$ The total distance traveled in such steps is $O(\log N)$, because the total distance traveled in such steps is $O(\log N)$ times the initial distance to $p$. So we only need to bound the total distance traveled in the other steps --- those in which that ratio is at most $1 / \log N$. First, we argue something slightly weaker: that the total distance traveled in such steps before the circle radius decreases to 1/2 or less is $O(\log N)$. (We show later this is enough to give the bound.) Consider any such step. Let $a$ and $b$, respectively, denote the locations of the bug before and after the step. Let $o$ denote the center of the current circle. Let $b'$ denote the point on the ray $\overrightarrow{pb}$ such that $|pa| = |pb|$: 

Assuming the given problem is feasible, the algorithm returns an $x$ such that $Px\le 1$ and $Cx\ge 1-O(\varepsilon)$. The number of iterations is $O(m\ln(m)/\varepsilon^2)$, because each iteration increases some constraint by $\varepsilon$, and this can happen for each constraint at most $N$ times. The proof of correctness is via the invariant $$\mbox{Lmax}(Px) \le 2\ln(m) + (1+O(\varepsilon)) \mbox{Lmin}(Cx).$$ The invariant implies $$\max Px \le 2\ln(m) + (1+O(\varepsilon)) \min Cx.$$ At termination the left-hand side is $\Omega(\log(m)/\varepsilon)$, proving the performance guarantee. In Step 2.1, the desired $j$ must exist as long as the original problem is feasible. (This is because, for any feasible $x^*$, and any $x$, if we were to choose a random $j'$ from the distribution $x^*/|x^*|$, the expected value of the partial derivative of Lmax$(Px)$ with respect to $x_{j'}$ would be at most $1/|x^*|$ (see the previous proof sketch for Set Cover). Likewise, the expected value of the partial derivative of Lmin$(Cx)$ with respect to $x_{j'}$ would be at least $1/|x^*|$. Thus, there is an $j$ such that the partial derivative of Lmax$(Px)$ with respect to $x_{j'}$ is at most the partial derivative of Lmin$(Cx)$.) Then the invariant is maintained in each iteration because, by the choice of $x_j$ and $\delta$, and the smoothness of Lmin and Lmax, increasing $x_j$ to $x_j+\delta$ increases Lmax$(Px)$ by at most $1+O(\varepsilon)$ times the increase in Lmin$(Cx)$. Learning (following experts / boosting) One reference for understanding this connection is Adaptive game playing using multiplicative weights, by Freund and Schapire. Here is a quick summary to give the technical idea. Consider the following repeated game. In each round $t$: 

This problem came out of my recent blog post, suppose you are given a TSP tour, is it co-NP-complete to determine if it is a minimal one? More precisely is the following problem NP-complete: Instance: Given a complete graph G with edges weighted with positive integers and a simple cycle C that visits all the nodes of G. Question: Is there a simple cycle D that visits all the nodes of G such that the total weight of all the edges of D in G is strictly less than the total weight of all the edges of C in G? 

Under the right derandomization assumptions (see Klivans-van Melkebeek) you get the following: There is a polytime computable $f(\phi)=(\psi_1,\ldots,\psi_k)$ s.t. for all $\phi$, 

It's called Type-2 Complexity Theory. There's a paper by Cook, Impagliazzo and Yamakami that ties it nicely to the theory of generic oracles. 

Syntactic classes give you time hierarchies. Zak's proof of the nondeterministic time hierarchy works for any syntactic class. For semantic classes (like UPTIME($n^3$)?$=$UTPIME($n^2$)) these are open questions. It's easier to create oracles that separate syntactic classes since you only need to diagonalize infinitely often and you don't care what happens on the other input lengths. Conversely it's easier to collapse semantic classes since you can eliminate machines that don't fulfill the promise. 

Even if you have a one-player game there is no computable equilibrium. Consider nature putting probability $1/2^i$ on program $i$. Any computable strategy will achieve some value strictly less than one or you could use it to solve the halting problem. But you can achieve any value less than one by the strategy that for some fixed sufficiently large $t$, simulates nature's program $j$ for $t$ steps and outputs the number of steps it takes program $j$ to halt if $j$ halts in $t$ steps, infinity otherwise. 

I think both of your problems are in P. Here's an algorithm that works in P without any restriction on the $(a_i,b_i)$ pairs. Formulate your problem as the problem of choosing $x\in\{0,1\}^n$ subject to $\sum_i x_i = k$ so as to maximize $$\textstyle f(x) = \pi(x) + \sum_i x_i a_i$$ where $\pi(x)=\prod_i \exp( x_i \beta_i)$ and $\beta_i = \ln b_i$. Relax the problem to allow $x\in[0,1]^n$. For intuition, note that the function $f(x)$ is convex, so it is maximized over $x\in[0,1]^n$ at some $x\in\{0,1\}^n$. (To see the latter, note that any $x\in [0,1]^n$ with $\sum_i x_i = k$ is the weighted average of integer points $x'$ in $\{0,1\}^n$ with $\sum_i x'_i = k$, and $f(x)$ is at most the corresponding weighted average of $f(x')$ over those integer points, so one of those integer points $x'$ has $f(x') \ge f(x)$.) Anyhow, find an optimal integer solution $x$ in polynomial time as follows. The first derivative of $f$ with respect to $x_i$ is $$\textstyle a_i + b_i\, \pi(x),$$ so (given the constraint $\sum_i x_i = k$), there exists $\lambda$ such that for the optimal $x\in[0,1]^n$, $$x_i = \begin{cases} 0 & \text{ if } a_i + b_i \pi(x) < \lambda \\ 1 & \text{ if } a_i + b_i \pi(x) > \lambda \\ ? & \text{ if } a_i + b_i \pi(x) = \lambda. \end{cases}$$ Furthermore, WLOG there are at most two indices $i$ that are undetermined by the above condition (because otherwise three pairs $(a_i,b_i)$ lie on a single line, a condition that can be broken by an insignificant perturbation of the input). Given any $x$ and two such indices $i$ and $j$, consider the neighboring points $x'$ and $x''$ obtained by raising $x_i$ to $1$ and lowering $x_j$ to 0, or vice versa. Since $f$ is convex, one of these points is as good as $x$, that is, $\max\{f(x'),f(x'')\} \ge f(x)$. Finally, to find the best $S$ in polynomial time, determine the $n\choose 2$ breakpoints $\pi$ such that $a_i + b_i\pi = a_j + b_j \pi$ for some $i$ and $j\ne i$. For each such $\pi$, order the indices $i$ by $a_i + b_i \pi$, and take $S$ to contain the first $k$ indices in the order (if there is a tie for the $k$th, there are at most two indices that are tied; try both, giving two sets $S$ for that $\pi$). Take whichever of these at most $2{n\choose 2}$ sets $S$ gives the largest $f(S)$.