Mounting a filesystem at a mount point hides any files that may be in that directory on the parent filesystem. To resolve the issue, move your old files somewhere else temporarily and create a new empty directory to hold the mount point. Then move the files into the new filesystem. 

It's all a garbage. It doesn't make sense to use to add IP addresses. It's used to manage lower layer characteristics of the interface, such as name, MAC address, etc. To set an IP address, you will use instead. 

This does exactly what you think it does: it turns off display for the entire document. If you didn't put that in there, seek help from whoever did. 

Please check the environment variable is defined and is equal to . If it is not, then define it as such and try again. 

Which is really messing with my plan to back up this machine... I have a server which is a KVM hypervisor to several virtual machines. One of these is running Docker. It has its Docker volumes on /dev/vdb, which is set up as an LVM PV, on which Docker uses its direct-lvm driver to store Docker container data. This virtual disk is an LVM LV on the host's local disk. Both host and guest run Fedora 21. The host's view of this volume is (only the relevant volume is shown): 

A 500 error in this context often means your CGI script crashed. Check the various log files for clues, look for syntax errors, etc. 

Ubuntu 12.04 is end of life, of course, but I do want to note that SPICE/QXL of that era was very early code, and on Ubuntu 12.04 in particular wasn't well integrated yet. They were still finding and fixing SPICE/QXL bugs causing slowness well past 2013. These days I don't think this question really applies anymore; SPICE is far faster than VNC for any version out in the last few years. 

You could use the tool which is part of Puppet (but can be used independently, if you don't have Puppet in your environment) to get basic facts about the target system. For instance: 

For people running PHP-based web sites, it's almost always required to track bug fix changes as well as the security fixes that Red Hat might provide. As you know they rarely provide bug fix updates, due to "enterprise" version locking, even for something like PHP where you really need them. On my production web servers I have used the remi repository for many years. It provides current versions of PHP, MySQL, Symfony, and many related packages that need such updates on a regular basis. As of this writing, it provides PHP 5.4.33 and MySQL 5.5.40. (PHP 5.5 is available in remi-php55.) The main difference you will notice is that your PHP bugs start going away... :) 

These days the BEAST attack is generally mitigated through 1/n-1 record splitting, since RC4 is considered too weak to use today. Check your distribution's security advisories for an updated OpenSSL that implements 1/n-1 record splitting, resolving CVE-2011-3389. (Note that Ubuntu seems to already have it.) Of course, using a server capable of TLS 1.2 is the preferred solution. 

Use kexec to reboot your server. This skips the pre-boot procedures entirely, and reboots into a Linux kernel at the end of the Linux shutdown process, rather than resetting the hardware and going to POST. Unfortunately kexec is a bit cumbersome to use, so I wrote a script to make it easier to work with: kexec-reboot will allow you to choose a kernel from your grub boot list, or simply kexec the latest available kernel. 

You cannot install two different numbered versions of the i386 and x86_64 packages without causing a lot of breakage. Just install the same version of both packages. 

CentOS has some instructions on building a custom Live CD. The process uses a kickstart file to determine what is on the Live CD, so you can perform any customizations possible through kickstart. 

I went over the networking configuration with a fine tooth comb, and discovered to my chagrin that the default gateway had a typo in it! 

Based on your filesystem features output, you didn't actually convert your filesystem to ext4. To resolve the issue, convert the filesystem to ext4. Reboot your live CD and run the appropriate commands: 

RFC 2822, which obsoleted it, continued to explicitly allow this particular construction (Section 3.6.2). 

Event 6009 is logged at startup, not at shutdown. It contains only a string identifying the operating system version. It's been that way since NT 4.0 or so. If you're looking for a system initiated shutdown/restart, look for event 1074. The details for this event will tell you what process initiated the restart and what reason was given, and you can check the reason code for further information about why the system shut down or restarted. 

Congrats, you have trashed your system. Now you get to reinstall. Though you might want to figure out how you managed to remove those packages first, and avoid doing it again. You might think I'm not serious and that there's some way to just reinstall the package manager. But if you've done something that destructive to your system, it's likely that much more than that is missing, and you won't be able to easily recover except via reinstalling. 

Per RFC 7208: You can use any mechanism with except for the mechanism. Note that it doesn't make much sense to use unless you are redirecting to records for a domain that is also under your control. If a third party asks you to do this, beat them over the head with a copy of the RFC; they should generally be asking you to instead. 

That bit of configuration looks familiar. I think it probably came from the WordPress wiki. In any event it's way overcomplicated, and mostly unnecessary. I'm going to pull my config from a live WordPress site: 

The user you specified in your configuration, , doesn't exist. Either create the user, or choose a user that does exist. 

This will generalize it for just about any possible filename. Tweak the pattern match if you need to. (Thank you to the Internet for this example.) 

That probably is about the only reason you would use the former construct, these days. The reason you're seeing this is probably that the default of changed in nginx 1.3.4. Prior to that, it defaulted to ; in newer versions it defaults to . This happens to interact with the IPV6_V6ONLY socket option on Linux, and similar options on other operating systems, whose defaults aren't necessarily predictable. Thus the former construct was required pre-1.3.4 to ensure that you were actually listening for connections on both IPv4 and IPv6. The change to the nginx default for ensures that the operating system default for dual stack sockets is irrelevant. Now, nginx either explicitly binds to IPv4, IPv6, or both, never depending on the OS to create a dual stack socket by default. Indeed, my standard nginx configs for pre-1.3.4 have the first configuration, and post-1.3.4 all have the second configuration. Though, since binding a dual stack socket is a Linux-only thing, my current configurations now look more like the first example, but without set, to wit: 

Your yum repositories are out of sync with each other. Your repository is showing files from CentOS 7.1 or 7.2 (I can't be sure which) but your repository is showing files from CentOS 7.3. It might be that you simply have out-of-date metadata on your system, in which case you can fix that by running and then trying to update again. But it is much more likely that someone has messed with the repo, so as to attempt to force it to remain on 7.1 or 7.2. This is not supported, and you should edit the repo file to restore it to its original form. 

I found several references on the Internet that indicate that HostGator installs a custom firewall script on their VPS and dedicated servers. However I wasn't able to find any instructions on how it was installed or how to get rid of it. I would contact HostGator for further information, or consider switching to another provider. 

If this is set to qemu, then qemu will run the machine without any sort of hardware virtualization support. Without this option set, qemu will not be told to enable KVM hardware acceleration. This is fine for, e.g. emulating non-Intel processors, but it is always much slower than using the hardware acceleration provided by kvm. 

Congratulations, you've run into a kernel bug. While Ubuntu is shipping a fixed kernel for 12.10, the fix has never been backported to 12.04 LTS. (And while reading the bug on launchpad, I could not figure out how to mark Precise as affected...) 

Did you wait for the TTL to expire? It appears your TTL on these DNS records is 1 hour, so until an hour has passed, your local DNS server will use the old cached entry. Your web site is working fine, as far as I can tell, if it has something to do with task management and audit control. But I'd probably remove that "blah blah blah"... Speaking of which, you might just have the old site in your browser cache. 

Go pick up a Sempron 140 or something else cheap from a shop and drop it in. If it's the motherboard, you should continue to get these errors; while if it's the CPU the errors should stop. In no circumstances should you try to co-locate this thing as is. You'll just have an unresolved problem that you'll have to actually go retrieve your hardware to fix. And while we're at it, strongly consider not using a cheap desktop-class motherboard for a server that you aren't going to have physical access to. Consider at the very least a SuperMicro server motherboard with IPMI so you have some sort of remote management capability. 

I run all my servers on UTC, and I convert any that come into my control as soon as I can. The one exception so far has been an asterisk server, which I had to leave on local time. Changing it to UTC completely broke asterisk. (It's on 1.6, hopefully this won't be an issue when I get around to upgrading it later this year.) 

Make a second and for the corresponding to . If that one is exceptionally large, though, you might have performance issues, and need to scrap this and do some scripting to obtain the redirect from a database. The s must be in your block, not within a block. If you are hosting multiple sites, the best place to put them, in my opinion, is immediately before the block that they correspond to. 

You want Network Policy Server, which provides a RADIUS server (which you then configure your wireless access points to use) and will require users connecting to Wi-Fi to log in with their AD credentials. This is a standard Windows Server feature you can install from Server Manager or PowerShell. 

Yes, there's a standard: Don't use X-Forwarded-For at all. RFC 7239 defines the Forwarded header, which has rather different semantics from X-Forwarded-For, and new implementations ought to be using it. Unfortunately it suffers from the same problem you have identified with X-Forwarded-For here: it may be defined twice in a request or contain a comma separated list of values. Proxies are also allowed to delete it entirely. And yes, there's a best practice: Use a different header name internally. Remember that X-Forwarded-For and its replacement Forwarded contain untrusted input. It is trivial for a client to put whatever they want in such a header. If you really need to know the public IP address of whatever connected to your server, stick it in a different header. For instance, CloudFlare uses CF-Connecting-IP for this purpose. I've also seen Client-IP and X-Real-IP used in nginx (where you can define anything you want). Whatever name is used, your load balancers should be sending the requester's IP address in some header other than X-Forwarded-For or Forwarded. 

And Apache did, with the directive. You can use this if you wish, but again, don't think that it's going to magically prevent you from getting attacked. 

It says Administrator successfully logged in via Remote Desktop from somewhere in South Korea. If the administrator isn't in South Korea, you've been compromised. 

So, yes, it's probably a real wireless NIC. As mulaz noted, there have been instances where vendors have released more than one card with the same MAC address. This causes all sorts of trouble if both cards end up on the same network. 

It depends entirely on how fast the registrar processes the transfer. An hour or less is typical, though if they want to be obstructionist they can drag it on for weeks. Though your choice of GoDaddy is kind of odd... 

You downloaded the 32-bit version of WebSphere, but you are using 64-bit Ubuntu (and without 32-bit compatibility libraries installed). You can resolve this issue in one of two ways: 

RADIUS uses UDP ports, but you have not opened UDP ports in your endpoint configuration. Once you fix the endpoint configuration to open the correct ports, you should be able to communicate with the FreeRADIUS server. 

Developers should not have root on production; everyone except the developers agrees on this. But the developers can sort of have their cake and eat it too. I am somewhat surprised that nobody explicitly mentioned this: One of my very long time small business clients has a web site with a Drupal installation, several WordPress sites, an SMF forum, and a few other random small web apps. I am the contract sysadmin (and for historical reasons also update/hack WordPress and SMF when needed) and my client has his own contract Drupal developers. The environment is several VMware virtual machines on a public cloud provider. The developers really want to have root access and sort of need it. It's their responsibility to write the nginx rewrite rules to make all their custom Drupal stuff work, for instance. But no way in hell am I giving them root access on the production server, and my client agrees with me on that. So we compromised: They get root access on the test web server (which is generally identical to production except for its IP address and is on the same cloud). Which, like production, has etckeeper so I can see whatever changes they needed to make and any packages they installed. I can then either pull the changes into production or tell them what's wrong with whatever they want to do. And if they really screwed up (they haven't yet, thank gawd) I can easily revert their changes. They have no access at all to the production database server; they don't even have user logins. Only my client and I do. (The web app itself, they deploy directly with git, and if they break it, they get to fix it and explain to my client why they should continue to be his developers. Though my client would CC me on such email so I could either laugh at them or facepalm.)