Model The first is the world or model matrix. This matrix takes the vertices in an individual model (such as a crate) modelled around the origin (0, 0, 0) and transforms them into world vertex coordinates. This might include scaling the object, rotating it around it's origin and finally translating it across to where it's located in the scene. 

These vectors are used to generate what's called a look at matrix. The affect this has is similar to the world matrix in the fact that it is essentially rotating and translating the world around the camera to "position" the camera. A camera that is 10 units along the X axis will translate world coordinates 10 units in the other direction. 

View The second matrix is the view matrix. This takes the world coordinates and transforms them so that they are within the context of the view. The concept of a camera is typically used to generate this matrix. The camera usually contains a position vector, a direction or target vector, and an up vector. This up vector describes the 'spin' of the camera. 

I find the Mega Man series of games has usually been a mix of all 3. Select any stage, and after finishing them, you are given a set of more difficult stages that are linear in progression. Seasoned players have figured out there's sort of a puzzle to the best order of beating the bosses/stages, as the order of weapons you obtain present better options to tackle future stages. If you can work out a challenge based around the order in which the stages are beaten, this could be a fun way to play the game. In other cases, the game has no apparent barriers to let you explore the whole world at first, but the extent to which you can explore it is limited by your character's abilities. This is common in RPGs, but also in Metroid/Metroidvania type of games. The world should be set up in a way in which the player would have to figure out where to go next, but without getting lost. If the process of elimination is made easy for the player, he would know where to go next and incidentally obtain an item that will let him pass through to places he previously couldn't go before. The replayablity of the stages depends on whether or not it is worth the player's time to visit old stages with their newer abilities. You'll have to give him an incentive to go back, either by teasing the player with visible but out of reach areas during the first run, or some storyline component that calls for some backtracking. 

Are either of these still recommended or is there a new feature to query the sampler that I could make use of? Should I be doing it one way over another for performance reasons? Here is my fragment shader: 

I have normal mapping working in my game, but I want to only use normal mapping for some surfaces, and not others. Right now, as far as I can tell, my shader is applying an incorrect normal of (0, 0, 0) in tangent space to my non normal mapped surfaces, since no textures are bound to the sampler. Is there a suitable way of detecting that nothing is bound to a sampler in GLSL and then using a flat normal instead? I've also found old posts around the web recommending the following: 

I have noticed that the tangent vectors that I am calculating are not always facing the correct direction. The tangents on the left and right of the mesh both face the same direction. Here is a screenshot showing this: 

Some things to consider: Do you really need to draw millions of points? Perhaps you could get away with only drawing the points on the surface, in which case you check to see if , and only draw it if it's true. Secondly, depending on how you're generating the points, you could generate them at initialization and sort them once before any rendering is done. Ensure your vertex array is sorted back to front and the frontmost will overwrite the ones back further back. 

I also used that blog link for reference on importing models. I had a similar problem with importing animated models for this reason. Some models came out sideways in the game, they will animate properly but turned sideways. For instance the Dude model would appear correct using the default import setting for the pipeline, but then I made a custom animation in Blender from a mesh I downloaded elsewhere and on export was turned 90 degrees to one side. Applying rotation in Blender's export settings didn't work either because the bones will be misaligned and animations will look all wrong. I managed to solve the problem by making a modification to the content importer and applying the custom rotation during that step. That part of the article with the RotateAll function, I made the parameters editable in the Properties window. The code isn't much, here's what I added to the model prep code in the model processor class: 

Exception handling is a cleaner way to handle unexpected errors from any point in the game. But the beauty of it is, in your codebase, it won't matter how deep you are in the code to handle it. Without exceptions, some kind of error handling routine would be returning a bool or int for some kind of status, then unwinding the nested subroutines till you got to a top-level area (your game class) and exit from there. It will get very cumbersome to write your code to error check from any possible point, wheras using exceptions requires little to no refactoring. Since your question is XNA specific, it will probably be of good use to know how to handle exceptions in an Xbox game, because when a game in there doesn't catch one, you are not left with a clue as to why it happened. This article shows you a clever way to work around that by running your entire game in a try/catch block. It will launch a new "ExceptionGame" if something happens to go wrong, passing the details of the error to display in the ExceptionGame. 

Set UI transformation matrices, set U/V/vertex coordinates for quad, draw. Nothing more to it, really. 

Another solution to transitions and other such things is to provide the destination and source state, along with the state machine, which could be linked to the "engine", whatever that may be. The truth is that most state machines are probably going to need to be tailored to the project at hand. One solution might benefit this or that game, other solutions may hinder it. 

The Matrix. Or rather, lots of matrix math. It's scary stuff for the uninitiated. There's typically three 4x4 matrices involved in turning a bunch of 3D coordinates in space into 2D coordinates on the screen (including the depth into the screen as Z). 3D matrices are a set of 16 floating point values arranged in a 4x4 grid. Algorithms are used to generate the required values and then using matrix-vector multiplication, these numbers transform a 3-dimensional vector (X, Y, Z). 

Projection The final piece of the puzzle is the projection matrix. In a camera-like perspective projection, the matrix transforms the almost screen coordinates to give the illusion of a perspective with a field of view of x degrees. If you think in terms of corners of the screen (for a 640x480 game), this is how projection "aligns" the coordinates along the Z axis within the 2D coordinates of the screen. For orthographic projection, no scaling takes place along the Z axis into the screen. For perspective, the further away the object, the smaller it becomes in relation to the screen's extents. 

To get to your main question, whether or not to influence the vertices before or during the shader, using a shader is preferred. If you know how to write a routine that can take into account the viewport and coordinate system, using a shader is preferred as it allows many sprites to be adjusted in parallel. Also, it ensures the "cosmetic" coordinates stay separate from in-game coordinates in the CPU code so one doesn't influence the other. You can do this either in the vertex OR pixel shader. In the pixel shader it is possible to shift texels to lock them to the pixels on the display. In DirectX9 I needed to do this with HLSL in order get around the quirk that texel corners were located at 0.5, 0.5 relative to the pixel, making crisp textures look semi-blurry. Correcting this changes the physical location of the texels without moving the vertices. In your case, the offset will vary from frame to frame, so pass the true coordinates of the sprite to the shader to calculate the decimal offset. Remember to CLAMP the texture sampling to avoid wrapping in the textures. 

The tangents are correct even over the seams. I am using the following piece of code to compute my tangents: 

I am updating one of my shaders to a version of OpenGL/GLSL that doesn't automatically provide (for educational purposes; I'm not ripping out working code for the sake of it). Therefore, I need to compute my own normal matrix on the CPU and pass it in. I understand that a separate normal matrix is used to avoid issues with non-uniform scaling affecting the direction of normals. However, I've noticed that in my code I never perform any scaling let alone a non-uniform one. So, I'm tempted to use the upper left 3x3 sub-matrix for transforming my normals and calling it a day (perhaps normalising them to allow for uniform scaling). My program assumes that every mesh it loads is already at the correct size, with no scaling required. Will I soon run into something that requires scaling (uniform or otherwise), or is there another reason for using a separate normal matrix that I haven't realised? 

Handle all slope tiles as a rise/run ratio. Each slope tile will have a ratio, and assuming 45 degrees means going up as you go right, 315 degrees will have a ratio of -1. Think back to basic coordinate algebra. Using the player's X position local to the tile, which is the run, solve for pos.Y by multiplying the X position by -1. However since y = 0 is likely at the bottom of the tiles, you'll have to offset the height by adding the height of the tile. So the player is colliding with a downwards sloping tile, the slope ratio is negative and the formula should be 

I would represent the range arc as a spotlight model, since these types of lights stop right when they hit a wall and don't shine through. They have a set lighting angle and attenuation distance which would correspond to the characters' fire range. As you have a deferred lighting system in place already, this shouldn't be too hard to apply. Spotlights are usually represented as a mesh, so you can make it either conical or pyramid-shaped, whatever suits your needs for the game. All you gotta add are the textures. The Cansin Deferred Renderer has a spotlight rendering feature- it would probably be a good idea to see the code behind it. 

I am working on a skill system for use in a game project I'm chipping away at. The project is entirely in C# so I'm using .NET to its advantage. In my preliminary implementation of a Skill Category -> Skill types system I'm using subclasses of ISkill and SkillCategory (base class with some virtuals) and using System.Type based reflection to generate a map of these at runtime (on demand). The great thing about this implementation is that it entirely decouples specific skill types (such as from the SkillLibrary (the container for the generated skill mapping). The goal is to remove the need to explicitly add each skill at some point as they are automatically detected by the library throughout the entire . Only game logic which relies on these skills use concrete types. ISkill 

There's two slight ugliness's with being very based, though. One is that my dictionary can't serialize as because Type isn't supported by (what I'm using at the moment). So what I'm doing in and is translating to and from a string using (i.e. "Game.Skills.Engineering.LaserWeapons"). And the other is potential performance problems, which is something I should profile. This all works, of course, but it feels like the weak point in an otherwise enum-less, strongly typed skill definition system. I don't really want to use enums and a "list" of Skill structures, or move it to an external data definition file because then that requires more maintenance of created and deleted enum values and such. Do you think this is worth it, or do topics such as performance of Type based operations and other considerations make it worth rethinking? 

Move to the scope of the state machine and set the 'blocked' flag to be true by default. When input is received in either state, calculate and check if it's blocked. Therefore, will be in a different location from the player when input is received. 'Blocked' should also be true if (not receiving any input). In both states: 

When you're dealing with vertex buffers, it gets real tricky with doing cross-platform or cross-API code, especially if you want to use the best approach particular to each API. I would look at some open-source cross platform engines such as OGRE to see how they do things at the lowest level- don't try to implement more complex ideas right away, like quadtree heightmap generation or cube mapping. A good first step is to start porting your XNA code to DirectX 9 first. XNA really just hides many things of DX9 behind the scenes, and you will just have to learn how to work with unmanaged code. When you have the basics down of loading and reading resources in DX9, you can begin wrapping it around more general functions. You have one benefit with OpenGL, though. It handles resources in a more managed way (just imagine all of them are in a managed pool). With DX9 you have to handle lost devices to take care of your resources. 

As far as I can tell, the main part of the calculation seems correct compared to things I've found online: 

Generally, normal mapping works great, but there are subtle lighting errors on parts of the mesh after the incorrect tangent space transformation. Hopefully this is all of the information needed to get to the bottom of this. 

Finally, (leaving out a few layers of abstraction), here's an excerpt from my main loop (EDIT: I've made changes based on @Roy T.'s suggestions, as well as the well-known GafferOnGames article): 

I've mapped the XYZ value of the tangent vectors to RGB values in the fragment shader. +Z is towards the camera. As you can see, both sides of the teapot have tangents facing towards the camera instead of only the left as I'd excepted. Also, the UVs for the mesh are correct and increase smoothly across the surface (I used blender's default sphere mapping to generate the UV coordinates): 

The method is just a refactoring of code from the old version of , because I experimented with checking for overlaps in various parts of my code: