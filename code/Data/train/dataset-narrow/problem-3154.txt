To understand how a random forest treats continuous data it is imperative to understand how a random forest works. At the base of the random forest algorithm lays a tree construction. The default in sklearn is to split a tree based on the Gini coefficient (see sklearn documentation). This type of tree algorithm is referred to as CART trees. You can change the to to select ID3 and C4.5 trees. Without going to deep into the maths, the tree algorithm will seek to split the tree based on a cutoff that leads to the lowest Gini coefficient. The random forest algorithm will build a large number of deep trees on your data and average over all the trained trees to give you the final prediction. Depending on your requirements in terms of data size and necessity for parallelization I can highly recommend H2O. It is an open source machine learning software suite with APIs in Python and R. Their random forest implementation is very fast and leads to models with a higher AUC (see this page for a good comparison between different ML libraries). 

This is a typical time series problem. The first step is to make sure your time series is stationary, see here for a good explanation why this is necessary. So we first do, 

Considering ocam's razor I would recommend to use the simplest model first and increase the complexity if the simple models fail: 

We see that this model gives us a much better result. So if you can relate dummy to a point in time you could create a model that has a good fit. According Occam's Razor you should try to keep your models simple. So I would suggest to build a single model for all your time slots instead of three separate models as you suggested in your question. You can turn this back to the original time series with . Hope this helps. 

The problem you are looking at is a common researched problem in the field of reliability engineering. If I understand you example correctly your goal is to gain an understanding in the following: What manufacturing conditions cause certain parts (in your situation hoses) to fail sooner than others? For each variable you mention you will have to establish whether these are constant over the life time of a part or whether they change. Here I measure time as the time that elapses once the part is used by your client in production. In reliability engineering the variables that do not change over time e.g. human operator of the manufacturing process, production batch, etc. are called time independent covariates the ones that change over time, e.g. amount of pressure put on a hose in production (if you have a sensor that reads that kind of information at your client site), are called time dependent covariates. You will have to transform your data such that it becomes censored survival data. This type of data means that every dependent variable is amount of time in operation from production. Since some of your hoses will likely never have a defect they will leave your observation period. Note that you do need to take these points into consideration too. When you combine all these data points you get something called right censored survival data. With the right censored survival data and the covariates together you can start to build a accelerated failure time model. Note that it is recommended to first start with a Kaplan Meier estimate of your survival rates before diving into AFT, but I just want to give you some pointers. A book that I can highly recommend on reliability engineering is: 

Where each letter corresponds to a certain event that occurs at a time. I want to reduce the number of events by aggregating frequently occurring events into a new event. A possible solution data set would look like, 

I have used , and (you forgot that one in your overview ;)). I would agree with Hadley Wickham that is sufficient for most tasks. However, this is only required if you start building advanced functions that operate on objects. Say for example you build a model with one function and you want to create a and function for the object returned by your model building function. For general Data Science purposes I would say that it helps to know the systems but none of them are very good examples of real OOP. For that I would recommend to work in Python, Ruby or Java. All have been build with OOP in mind. In regards to knowing OOP, I think it is vital for someone involved in ML. Not when you are prototyping in R or Python but definitely when you start working on production code. I think this Quora thread gives a good run down on when OOP becomes important in ML. If your focus is more on statistics in R it may be of less importance. 

Without any example of your data or distributions it is a hard question to answer. Without further info I think there are two approaches: 

As far as I know there are no parallel implementations of SVM available directly in R. The benefit of running on RevolutionR is that it will parallelize the matrix computations through the use of the Intel MKL library. Since SVM relies on linear algebra you will achieve probably some performance gain. Another option that you could try and which may lead to better results is using Spark and SparkR which could expose the SVM in MLib that is part of Spark. I have not tried this myself yet but if you are stuck this might help. 

Depending which tree algorithm you want to use you could manually construct the two first levels of the tree. You can just follow the pseudo code explained for example here for the C4.5 tree. Once you have done this you can remove the two features from the data set and create trees for the remaining part of the tree. If you want to create a object you would be required to take some parts of the source and this may be a bit more demanding. Depending on what tree algorithm you use you will just have a binary split at both levels so you will only need to build 4 separate trees and not 264. Note that you may not have the optimal decision tree since after stepping through the first two levels, the country and product type may still be variables that cause a split. But without seeing the data is impossible to tell. Side note, it may be valuable to explain the business that country and product type are not the most sensible variables to have in the top of the decision tree. Sometimes it is better to educate the end users than to force machine learning to do something inaccurate. In my experience end users prefer to have a correct solution than a solution that works because people have a gut feeling that it should be in a certain way. 

You can play with the ARIMA(p, i, q) terms. We should leave $p=1$ since we found that the AR term is significant at lag 1. If we add a MA(1) we find a model that has a versus a for our initial model. So you can create the best model using, 

Welcome to SO! It looks like you have a time series problem. Typically the first step when dealing with time series is to consider the difference. Let us define $f(t)$ as the fuel level at time $t$. You would want to calculate $diff_{\text{fuel}}(t) = f(t) - f(t - 1)$. After this step you will likely see that the spikes that you identify as bad data are outliers. You could detect these by for example looking at all the data below and above your 2.5 percentile or 5 percentile. Typically this requires some careful analysis work to ensure that you do not delete too much. Once you have identified what the best workable percentile is you can could Winsorize your data. Lastly, you would look at the data points in the bottom of your resulting distribution. These will likely be the points you identify as theft. I hope this helps.