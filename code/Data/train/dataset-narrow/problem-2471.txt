Let $X$ be a random variable taking values in $\Sigma^n$ (for some large alphabet $\Sigma$), which has very high entropy - say, $H(X) \ge (n- \delta)\cdot\log|\Sigma|$ for an arbitrarily small constant $\delta$. Let $E \subseteq \rm{Supp}(X)$ be an event in the support of $X$ such that $\Pr[X \in E] \ge 1 - \varepsilon$, where $\varepsilon$ is an arbitrarily small constant. We say that a pair $(i,\sigma)$ is a low probability coordinate of $E$ if $\Pr[X \in E | X_i = \sigma] \le \varepsilon$. We say that a string $x \in \Sigma^n$ contains a low probability coordinate of $E$ if $(i, x_i)$ is a low probability coordinate of $E$ for some $i$. In general, some strings in $E$ may contain low probability coordinates of $E$. The question is can we always find a high probability event $E' \subseteq E$ such that no string in $E'$ contains a low probability coordinate of $E'$ (and not of $E$). Thanks! 

There exist no boolean one way functions, since for a boolean function, you can always guess a preimage of the output, and with high probability, you'll be right. 

Regarding the last eigenvalue: The last eigenvalue $\lambda_n$ measures (roughly) how close is the graph to be bipartite. For example, $\lambda_n = -d$ if and only if the graph is bipartite (this is a fairly easy exercise). You can read more about it in Luca Trevisan's blog: $URL$ $URL$ 

In notes, $URL$ it is mentioned towards end of section $4$ that $URL$ shows that deterministic complexity of $CIS (\mathsf{Clique-IndependentSet})$ problem is $$D(CIS_G)\geq (2-o(1))n$$ with a specific graph $G$. I checked cited paper, it mentions no citation of $CIS_G$. However, it specifies a function for which $$D(f)\geq(2-o(1))n$$ holds in theorem $3$. My question is how is this result connected to $CIS_G$? In other words how does lower bounds $D(f)$ on a function $f$ gives lower bounds on $CIS_G$ and/or vice versa? What is best lower bound known for $D(CIS_G)$? 

Johnson graphs have a dual like construction to Kneser graphs in the sense that in Kneser we encode non-intersecting k-sets by joining vertices that represent the sets while in Johnson graphs we encode intersecting k-1-sets. I have a few questions on Johnson graphs: What is the fractional chromatic number $\chi_f(J(n,2))$ of Johnson graph $J(n,2)$ which is the complement of Kneser graph $K(n,2)$? In general what is $\chi_f(J(n,k))$? It is known that we have a graph homomorphism $G\rightarrow K(n,m)$ if and only if $\chi_f(G)=\frac{m}{n}$ for any graph $G$. Is there a similar relation for Johnson graphs(atleast for $J(n,2)$)? In general do we have $G\leftrightarrow H\iff\chi_f(G)=\chi_f(H)$? (where $\leftrightarrow$ implies homomorphisms bothways?) 

What are some of the best sources (books and papers) to motivate and learn communication complexity on its own and in connection with its relation to computational complexity theory? 

A basic property of vector spaces is that a vector space $V \subseteq \mathbb{F}_2^n$ of dimension $n-d$ can be characterized by $d$ linearly independent linear constraints - that is, there exist $d$ linearly independent vectors $w_1, \ldots, w_d \in \mathbb{F}_2^n$ that are orthogonal to $V$. From a Fourier perspective, this is equivalent to saying that the indicator function $1_V$ of $V$ has $d$ linearly independent non-zero Fourier coefficients. Note that $1_V$ has $2^d$ non-zero Fourier coefficients in total, but only $d$ of them are linearly independent. I am looking for an approximate version of this property of vector spaces. Specifically, I am looking for a statement of the following form: Let $S \subseteq \mathbb{F}_2^n$ be of size $2^{n-d}$. Then, the indicator function $1_S$ has at most $d\cdot\log(1/\varepsilon)$ linearly independent Fourier coefficients whose absolute value is at least $\varepsilon$. This question can be viewed from a "Structure vs. Randomness" perspective - Intuitively, such a claim says that every large set can be decomposed to a sum of a vector space and a small biased set. It is well known that every function $f:\mathbb{F}_2^n \to \mathbb{F}_2$ can be decomposed into a "linear part" of which has $\mathrm{poly}(1/\varepsilon)$ large Fourier coefficients, and a "pseudorandom part" that has small bias. My question asks whether the linear part has only a logarithmic number of linearly independent Fourier coefficients. 

Given $m\in\mathbb N$ is it possible to create in polynomial time an $MILP$ and compute $2^x\bmod m$ in poly time $O((\log (Bm))^c)$ at fixed $c>0$ using that $MILP$ with $O((\log (Bm))^c)$ sized words for any $0<x<B$? Unlike $x!\bmod m$ which is hard to compute both in straightline model and MILP model above we can compute $2^x\bmod m$ in $O(\log x)$ steps in straightline model. 

Integer programming is NP-hard. What is the status of integer programming problem that decides between existence of $\leq1$ solution and $>1$ solutions (note $0$ solutions falls in $\leq1$ category)? Integer programming in fixed parameters is P. What is the status of integer programming problem in fixed parameters that decides between existence of $\leq1$ solution and $>1$ solutions (note $0$ solutions falls in $\leq1$ category)? 

A paper was posted in arxiv $URL$ titled 'Rectangular Kronecker coefficients and plethysms in geometric complexity theory' by Christian Ikenmeyer and Greta Panova with first line in abstract stating 'We prove that in the geometric complexity theory program the vanishing of rectangular Kronecker coefficients cannot be used to prove superpolynomial determinantal complexity lower bounds for the permanent polynomial'. If correct what is the implication of this result to the geometric complexity theory program for separating permanent from determinant? 

Is P=BPP strong enough to derandomize Vazirani-Valiant reduction? If not what other ingredients are necessary to derandomize Vazirani-Valiant reduction? 

Background: In Karp's paper on Probabilistic Recurrence Relations, he develops tail-bounds for random variables satisfying the following recurrence: $$ T(x) = a(x) + T(h(x)) $$ where $T(x)$ is a random-variable that can be thought of as the cost or run-time of some randomized algorithm when run on an instance of size $x$. Here, $a$ is some deterministic "toll" cost involved in processing the instance prior to a recursive call, and $h(x)$ is a real-valued random-variable in the range [0, x], which gives the size of the instance in the recursive call of $T$. Assuming the expectation of $h(x)$ is bounded by some function $m(x)$, Karp then considers the deterministic recurrence relation: $$ u(x) = a(x) + u(m(x)) $$ He then derives various tail-bounds on $T$ in terms of $m$, $a$, and $u$. Question: My question is about one of the steps in the proof of Theorem 3.3 of the paper. The exact statement of the result is not needed to understand the step I'm stuck on. In this theorem, $a$ and $m$ are assumed to be continuous, there is some $d$ such that $a(x) = 0$ on the interval $[0, d]$, $a$ is strictly increasing on $[d, \infty)$, and $m(x)/x$ is assumed to be non-decreasing. Karp then defines $K_r(x) = \mathrm{Prob}[T(x) > r]$. He notes that $K_r(x) = E[K_{r - a(x)}(h(x))]$. His goal is to bound $K_r(x)$ by some quantity. To do so, he starts by defining a sequence of functions $K_r^i$ indexed by natural number $i$ as: $K_r^0(x) = 1$ for $r \leq 0$ and $K_r^0(x) = 0$ for $r > 0$; and inductively, $K_r^{i+1}(x) = E[K_{r-a(x)}^i(h(x))]$. He then says that $K_r(x) \leq \sup_{i} K_r^i(x)$, and proceeds to bound each of the $K_r^i$, thus deriving a bound on the supremum and in turn $K_r$. However, I do not understand why $K_r(x) \leq \sup_{i} K_r^i(x)$. This step is stated without elaboration, so I'm guessing I'm missing something obvious. I see that the $K_r^i$ are defined recursively in terms of the same recurrence about expectation that $K_r$ satisfies, and the supremum of the $K_r^i$ should be a fixed-point of this recurrence, sort of like the construction in Kleene's fixed-point theorem. But that would seem to give a least fixed point, which would justify the opposite inequality: $\sup K_r^i(x) \leq K_r(x)$. I think I see a way to make the argument go through if $\{a(x) | a(x) > 0\}$ is bounded below by some $\epsilon > 0$, but that contradicts the continuity assumption on $a$. Banach's fixed-point theorem would give a unique fixed point, which would do, but I do not see a sense in which this is a contraction. Other references: this result is mentioned in Chapter 4 of Dubhashi and Panconesi's textbook on Concentration of Measure, but the proof is not included. Chaudhuri and Dubhashi's paper "Probabilistic Recurrence Relations Revisited" proves a similar result but the proof technique is different, so it does not help me understand this argument. 

The best deterministic factorization algorithm that is currently known runs in $O(N^{\frac{1}4+\epsilon})$ arithmetic steps. Randomness and quantumness improves upon this. I believe Quadratic/Number field sieve run in randomized subexponential time. What is it that quantumness provides that sieve techniques cannot provide? What exact barrier does Shor's algorithm break that randomness cannot? 

Permanent is random self-reducible. $\mathsf{SAT}$ is not random self-reducible since otherwise the polynomial hierarchy collapses to $\mathsf{\Sigma_3}$. 1) Is $k$-sum random self-reducible? That is if $k$-sum can be computed in time $n^{O(k^b)}$ for some $0<b<1$ on $\frac{1}{2}+\epsilon$ of inputs where $n$ is the number of input integers, then can $k$-sum be computed time $n^{O(k^b)}$ for all inputs of $n$ input integers in a randomized sense? The Patrascu-Williams paper $URL$ talks about deterministic algorithms and does not imply hierarchy collapse but only tells that the ETH fails (somewhat mildly) when we have a deterministic $n^{O(k^b)}$ algorithm. We have a randomized setting here and even if we have such a random self-reducible condition satisfied, we may still not upset the big picture. 2)Likewise can the $\mathsf{NP=coNP}$ problem be random self reducible? That is, if for an $\mathsf{NP}$ complete problem we have short certificates for $\frac{1}{2}+\epsilon$ of the $\mathsf{NO}$ instances, then we have short certificates for all $\mathsf{NO}$ instances in a randomized sense? Update The results in $URL$ seems to indicate the $\mathsf{NP}=\mathsf{coNP}$ issue is not random self-reducible.