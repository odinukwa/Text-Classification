A couple more (superficial) reasons: Auto-update stats will block the query that triggered the update until the new statistics are ready. ...Unless you also enable auto-update stats asynchronously. Then the query that triggered the update won't wait for the new stats, but will potentially run with the old, incorrect stats. I also ran into some strange blocking issues when a database with some relatively large tables (44M rows, 8.5 GB) would start updating stats. We decommissioned that application before I was able to really track down what was going on, though. 

I would probably add a boolean column to the rooms table indicating "virtual rooms" (room "ABC", for example, would have this set). Then add another table called VirtualRoomMembers, with two columns, both of which are foreign keys back to your Rooms table's primary key (assuming here): 

Once that's running, you have to connect to the dedicated-administrator connection (DAC) for the instance. You can use any tool you like, e.g. Management Studio or sqlcmd, but note that you can't connect Object Explorer in Management Studio, since it's running in single-user mode. In either case, specify the prefix for the server name. I think you can do to connect to a local default instance. After that, you can pretty much change what you want, but you'll have to figure out what the actual underlying tables are for certain system catalog views. For example, . To see the view definition, switch to the "mssqlsystemresource" database (not normally visible), and use to see the code for any system views/procedures/functions. 

I eventually managed to track this down to low memory. I was under the mistaken impression that 32-bit Analysis Services could use AWE memory, but alas it does not. I changed the processing strategy from full processing, to incremental processing of only the appropriate partitions. That seems to have reduced the memory requirements, and resolved the issue (hasn't failed in quite some time now). 

If you want the "keep it simple" approach, you can in fact put both types of "items" in the same table, while maintaining proper integrity constraints. Generally speaking, this is what it would look like: 

Usually in my lab setups, I use separate accounts, but place them into a domain group, and grant any permissions to that group. This can be nice if you want to allow for more fine-grained auditing, or verify that you can properly build such an environment, but from a functional standpoint, using one account will typically get the job done. 

Only then are you applying COALESCE, meaning that entire SUM() has to be NULL in order for it to be replaced with zero. COALESCE won't magically dig down into its arguments and replace individual NULL terms. 

There's (a lot) more detail in this article, as well as a nice example using the AdventureWorks database: $URL$ If it's a relatively simple query that's only performing poorly because of a lot of data and (inner) joins, this could be an easy way to improve it. 

I've done this in SQL Server using a modified version of Dice's Coefficient. Basically, you store a precomputed table of q-grams from your lookup data set. This table should have the primary key of the source record, the resulting q-gram, and the number of times that q-gram appears when breaking up the string. Then when you want to do a lookup, break your input string into a similar list of q-grams (temporary table, subquery, etc.), and join those to your lookup table on the q-gram, grouping by the primary key from the lookup row (which, in the lookup table, is actually a foreign key rather than a primary key). For each matched q-gram, calculate the number of matches as the minimum cardinality between the input value and the lookup table row, e.g. the string 'aaaa' could have 'aa' * 3, and 'aaaaa' could have 'aa' * 4, thus you would have 3 matches. Double this number to get 6. Once you've got this doubled number of matches, divide by the total number of q-grams obtained from both the input string, and the rows in the lookup table associated with a given primary key. So 'aaaa' would have 3 q-grams, and 'aaaaa' would have 4. You would calculate the similarity as 6/7. You can then filter this similarity quotient based on your desired threshold. The nice part about this algorithm is that it's fast, since after splitting your input string, it's just a bunch of index lookups. The downside is that it doesn't put any emphasis on ordering of q-grams, so having two words out of order won't affect the match quality (assuming you're not letting q-grams cross word boundaries). Following is some sample code from SQL Server to explain the concept, but it makes use of features not found in MySQL. You'd probably need to rewrite some of this using subqueries rather than common-table expressions. 

Since "operators" are part of SQL Server Agent, I'm guessing it'll use whatever mail profile you've configured that to use. To review/change it, right click SQL Server Agent in Management Studio, and choose Properties. Go to the "Alert System" page, and look for the "Mail profile" drop-down menu near the top. You may also have to check "Enable mail profile", and set "Mail system" to "Database Mail" if you haven't already. 

That will probably be appropriate in most cases. If it's using a different port, or dynamic ports (common with a named instance), then you'll need to determine which port it's currently listening on. Check SQL Server configuration manager to see if it's a specific port, or dynamic ports. If it's using dynamic ports, then as long as you don't have multiple instances on the server, is probably the simplest way to find what it's using. Otherwise, dig through the Windows event log or the SQL Server error log for a message indicating which port is in use by the instance. If SQL Server is using Named Pipes, then I believe if you're able to access shares on the machine, you have adequate network connectivity. This article says you can go further and try connecting to the IPC$ share: $URL$ 

I need to do a little benchmarking of AlwaysOn, both with and without TDE enabled. All the instructions I see cover adding encrypted databases to an availability group, but I see no mention of enabling encryption for a database already in an AG. Can I just restore the server certificate to all nodes, then enable TDE from the primary? I'm hoping I don't need to remove the database from the AG, encrypt it, then reinitialize all nodes, as the database is around 100 GB. We can do that if needed, but it'll certainly slow down our testing a bit. Update I just tried this on some lab VMs. I was able to create a certificate at the primary, restore it on all the secondary nodes, then create a database encryption key at the primary using this certificate. All I had to do after that was at the primary server, and after a minute or two, sys.dm_database_encryption_keys showed encryption_state = 3 at all nodes. So, followup question: Is there any reason to think that I shouldn't use this procedure? SQL Server appears to be happy with the results, but I haven't seen this approach discussed anywhere. When dealing with encryption and high-availability matters, I don't want to leave things to chance. 

You will probably get better performance by using a . This almost always works better in cases where I need to do a conditional join or filter. 

One possible solution: Create a local Windows user, create a Windows login in SQL Server for that user, give it the appropriate rights (sysadmin?), then launch Management Studio (or whichever tool you need) via right click, Run As. Specify the credentials for the local user. You can also launch a program with the "runas /netonly" command to run the program as the currently logged-in user, but authenticate to network services with different credentials. I have no idea how this behaves if the "network" service is on the same machine. 

You can do this pretty easily with (if you're on 2005 or newer). Note that there may be better performing ways of achieving the result, such as using - check execution plans if in doubt. Also, is lazy and inadvisable; I'm just doing it here for illustrative purposes, and because I don't know the real structure of the Heartbeats table. 

Whoops, should have been . Or better yet, name your identity columns descriptively, so it can be . Then if you accidentally use the wrong table alias somewhere, customer_id won't be a column in that table, and you'll get a nice compilation error, rather than empty results and subsequent code squinting. Granted, there are cases where this doesn't help, such as if you need multiple foreign key relationships from one table to another single table, but naming all the primary keys "id" doesn't help any there either. 

I vaguely recall you have to do something like this, since the SQL Server log file is written cyclically. 

It sounds from your description like this Excel sheet will be used as a sort of interactive report. If that's the case, I encourage you to try out Reporting Services. It's included with every edition of SQL Server, ever since 2005. This way, accessing the report only requires connecting to the report server with a web browser, rather than going directly to the database server. You can also do fairly sophisticated reports without having to do all the crazy VBA coding you'd need to with Excel (trust me, I've been there). You can export the reports to Excel files, and it usually does a pretty good job with the formatting. I'd still recommend keeping the report server behind a VPN, however. 

The hashing method is presumably an implementation detail which may or may not change in future releases (as it has at least once already). They're telling you not to do it in order to absolve themselves of breaking your scripts/automation if you try to run them on newer or older releases. It's partially supported purely to allow for migrating logins. At some point in the process, you would have to use the clear-text password to generate the hashed password, so I'm not sure you'd really gain much by supplying pre-hashed passwords. 

Alright, I think I've got a workaround I can live with. First, I created a named set in the cube, called Feb29: 

You'll want to edit the Agent job (or whatever scheduled job is running them) and modify the value it's passing in for the parameter. e.g.: 

You'll probably want to do this with bcp, which is intended for this kind of thing. It would be something like this: 

Differential backups include data that has changed since the last full backup (ignoring any full backups taken with the COPY_ONLY option). If you take a differential backup immediately after a full backup, it will be very small, as little (or no) data will have changed. As time goes on, the differential backups will become larger and larger, until you do another full backup, which resets the differential change map. Note that you must still have the last full backup taken prior to the differential backup, or the differential backup will be useless. 

Assuming you're sending the messages from within a few stored procedures, I'd recommend checking the database name before sending the message, and skipping that step if it's not the production database. It usually looks something like this: 

While I haven't run AlwaysOn in production on 2008 R2, I have experimented with a VM lab using 2008 R2 guests on 2012 Hyper-V. The only real issue I encountered was having to install hotfix KB2494036 to enable changing the node weight property of the cluster nodes. My test environment simulated three quorum-voting nodes in the primary facility, two of which were hosting SQL Server (with Availability Groups), and a fourth "off-site" non-voting node hosting a third replica of the Availability Group. I threw a bunch of tests at it, like ungraceful power downs, fiddling with TDE settings, manually joining nodes to the AG, etc. and I at least wasn't able to make it break down in testing. There may be some edge cases where 2008 R2 clustering falls apart, but I wasn't able to uncover any while playing around. However, if I were building a new production machine from the ground up, I'd probably use 2012 to be on the safe side, and avoid the manual hotfix patching. 

The second one will appear to work because SQL Server replaces multi-byte characters with '?' when casting down to varchar. This small example demonstrates the behavior, as all arguments will be converted to varchar literals: 

The most significant balancing act to consider is whether your need for real-time reporting outweighs the performance hit that both your OLAP and OLTP systems will take from using your OLTP data source directly as fact and dimension tables, and then bouncing ROLAP/HOLAP queries off of them. If the tables are all quite small, and the server isn't under heavy load already, then the penalty is probably negligible. If SSAS queries are going to kick off 500MB+ reads, then that's a problem. And unless you're doing some kind of high-frequency trading, you probably don't need to have your SSAS database that up to date. It seems like SSAS is more useful for big-picture kind of summaries, and if the past few hours of data aren't included yet, it's not going to make a big difference to those running the reports (ask around to be sure, obviously). We load our data warehouses and process the cubes each night, and that's typically plenty. We also have a few simple reports built against OLTP tables for viewing more targeted, up-to-the-minute data (stuff that's really more OLTP than OLAP in nature). 

Easiest option, if you aren't relying on features specific to MyISAM or other storage engines: change your tables to the InnoDB storage engine, and use the option with mysqldump. This tells mysqldump to perform all the work within a single transaction, causing InnoDB to give it a consistent point-in-time snapshot of the data, without blocking other data access (including writes). However, MySQL DDL operations are apparently not transactional, and altering table structures during a backup can cause them to be missing from the backup with this option. $URL$ 

The output has eight columns: the table and column names for the foreign keys (FK_table, FK_column), the names of the foreign-key constraints (FK_name), the referenced PK or unique index table and column names (PK_table, PK_column), the name of the referenced PK or unique index (PK_name), and the update/delete cascade actions (Delete_Action, Update_Action). (Edited to add some more output columns.) 

Backup and restore are the simplest/safest ways to do this. There are a couple of things that you need to watch out for, or you'll get errors like the one you're seeing now. 

Since TDE relies on a certificate stored in master (which is used to encrypt the database encryption key), then this would work only work if you could restore the master database to another server in such a way that the certificate could be decrypted. This is the TDE encryption hierarchy: 

Setting up an Access DB as a linked server gets pretty ugly, but if you absolutely have to do it, you can split that one table into a separate database file, and create a linked table pointing to it from the original Access DB file. Then set up a linked server in SQL Server pointing to the Access database file with that one table. Better option: Build an SSIS package that periodically imports/synchronizes data from your Access database. Best option: Use the upsizing wizard in Access to migrate the data to SQL Server, and continue using Access as the front-end application, but not as the storage engine. 

If your patterns need to get fancier than that (multiple numeric segments, variations in the letters, etc.) then take a look at using sequences and triggers. 

I've got this fairly simple reporting procedure that searches through inventory transaction data (stuff being received, moving around the warehouse, etc). The whole thing is listed below - I've only anonymized the procedure name. It's pulling data from Dynamics GP (explaining the awful table/field names), and an internal data warehouse.