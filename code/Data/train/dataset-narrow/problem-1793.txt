i would install DNS servers on the machines that you are going to use as domain controllers and let the domain controllers auto-populate the srv records so that you have a copy that you can give the bind admins. after that, it sounds as if the zone data may be fairly static and you should be able to tear down the windows dns servers. the way you word your question makes it sound as if there may be other Active Directory based forests on campus. How are they solving their DNS issues? any chance of joining an existing domain / forest? 

if your app on 5555 is web-based : create an "A" record in dns for crm.mydomain.net to point to xxx.yyy.zzz.www on the machine that is xxx.yyy.zzz.www setup a website on port 80. use redirection (from the webserver or from the default doc) to automatically do a html redirect to $URL$ in a defualt doc, it would look like : 

the first part of this batch file deletes the past archive the second part of the batch file renames the current backup from sql server to one that has a date and adds the file name to a file that you will need the next time. then you can do your compression with 7zip. note that i didn't test this but the concept should work. 

After you have created this value, and/or changed its data, you need to restart the machine. You should NOT do this on a production server, it will cause problems. The registry value LogBufferSize overrides the default logging buffer of 64 K. This can be done to reduce memory consumption (but will increase CPU and hard disk use). 

try a traceroute to your ftp server. the network that exists between each hop between your pc and your ftp server allows for a possible sniffing point for someone (or gov't agency) to see your cleartext credentials without your knowledge. 

getting ping to work is the first step. you have a segment connected to 3 computers. you also need to have an interface connected to the corporate lan. in addition, the router that you have connected needs to know where to send packets that are not local. setup a default route to the default gateway on the corporate lan. that will get packets to travel to the corporate lan. to get packets to come back, the router on the corporate lan will need to know how to route packets back to your router. you will to add a route to corporate lan router advertising your 192.168.1.x network (assuming that network doesn't exist somewhere on the network already). 

kind of backwards but here is a doc that shows a full length PCI card can fit into a 345 $URL$ and another doc states that "Full-length PCI cards are 107mm (height) X 312mm (long)." 

in iis 6 (windows 2003) you can try : the following was taken from ($URL$ The IIS 6.0 logs are handled by HTTP.sys. For performance and scalability reasons, HTTP.sys buffers the logging for a while before it writes to disk. By default, the buffer time is one (1) minute, and the buffer size is 64 K. When debugging, and this depends on the log files, it could however be great to not buffer. There is no supported way to do this, but an unsupported way is to create the registry value DisableLogBuffering and set it to 1. 

script the backup to run normally. ignore the exit code of ntbackup. scrub logfile for interesting parts. send status to nagios via send_ncsa. profit. the following from MS technet : Windows 2000 Backup (Ntbackup.exe) does not have a command-line parameter to specify the location to which reports are saved after a backup operation is finished. The backup report is saved in the profiles folder of the user who performed the backup operation. You can view the reports by clicking Report on the Tools menu in Backup. Backup keeps only the last 10 backup reports. The corresponding Backup##.log files are located in the "Documents and Settings\User_Name\Local Settings\Application Data\Microsoft\Windows NT\NTbackup\Data" folder. 

I am trying to install Postfix on an Ubuntu 16.04 server; this is a web server. I need Postfix for send-only for fail2ban and logwatch. All Postfix tutorials show that apt install postfix ends with dialog windows. When I run sudo apt install mailutils all I get is a completed script, no install (that I can tell). How do I configure my install? (there is no /etc/postfix/main.cnf) Sorry for the lack of support info. I do not know enough to know what to give you. Ask, and I will post what I can. 

Our network is a small peer-to-peer / workgroup, no domain controller, no AD. We just added a new file server running Fedora 27. This server has several shares on it. All shares are accessible from all workstations (Mostly windows 7), except one (also Windows 7). This same problem workstation can access the other Samba servers and shares. The new server is visible in 'Network' on the problem workstation. I know I have not given enough info. But, I don't know enough to know what to share. Can anyone give me some direction as to where to look for the problem? Let me know what other info I can provide. 

I found the solution. The problem does seem to have been in the naming of directories. Instead of using the slash delimiter for the space, I had to wrap the directory names in quotes. (I didn't know Linux saw any difference in the 2 methods) In the commands where I was using directory names, this did not work: 

After spending more time than I should have, I removed and reinstalled Postfix. This action resolved the problem. I guess I had a bad initial install. My steps: 

I have a vm running a LAMP stack with Fedora 22. This is a server dedicated to running OwnCloud. The server and OwnCLoud ran fine for several weeks. I had to reboot the server. On reboot mysqld fails to start. Attempting to manually start the service fails also. 

I am trying to set up key-based ssh access to a Fedora 25 box. I have done this on many Ubuntu servers with no issues. I get Here is my set up process: 

I am a software developer trying to help with a network issue (letting you know I don't know what I am doing). We are a very small shop. We have a Samba network with a dozen or so Linux servers, and maybe 2 dozen users. Network discovery stopped working last week. Users cannot browse to network resources. Most users are Windows 7, I have an Ubuntu box and a Fedora box with the same issue. I can connect by manually entering resource locations (by name). Windows users can as well, but have to manually connect by IP address. There have been no intentional changes to the network. We did have a power interruption last week. We have rebooted the router, and the switches. Can anyone give me some guidance? Thanks for any help you can offer. EDIT: We still have this issue. We have one Mac user - Network Discovery works for him. 

I found the problem. The hard drive was running out of space. I suppose there was not enough space for MySQL to write to the log, causing it to fail. Since this is a vm, I added virtual drive space. Posting this in hopes that it helps someone else, and that my hours of time are of some benefit to someone. 

(server) create /home/[user]/.ssh (local) copy key.pub to remote /home/[user]/.ssh/authorized_keys (server) chmod 700 /home/[user]/.ssh (server) chmod 600 /home/[user]/.ssh/authorized_keys (server) chown -R [user]:[user] /home/[user]/.ssh/ 

I am converting a legacy Apache server to Nginx and do not have the luxury of changing URL's or rearranging the filesystem. Is it possible to use nested location{} blocks in the Nginx configuration to tell it to feed the .php files in an aliased directory to fastcgi while serving static content normally? Similar configuration to what fails me: 

GFS is some seriously black voodoo. The amount of work required to get a simple two client cluster working is staggering compared to the alternatives. OCFS2 is a lot simpler to deploy but is very picky when it comes to the kernel module versions involved on all attached servers - and that's just the beginning. Unless you really need the kind of low-level access a cluster filesystem offers, NFS or CIFS is probably all you need. 

That line is asserting that the specified port is an integer. My guess is that the assert fails since wgSphinxSearch_port is now undefined. I don't know why you'd need multiple searchd instances. What prevents you from using a single instance and different indices for each wiki db? 

For obvious reasons, I haven't even started trying to configure BGP throught the tunnel. At this point, I am wondering if it is even proven possible to configure l2l into VPC with a Watchguard firewall? If so, where might I be going wrong? 

If you have to go through scp/ssh, my experiments show that the fastest cipher enabled by default these days is RC4. You specify the cipher via '-c arcfour' in your ssh/scp command: for initial copy: 

Percona's xtrabackup utility has a --throttle option to reduce the IO load of the backup job. The docs say that the value passed is the number of read/write pairs per second. Is 1000/sec an appropriate value on modern hardware? How about 5? I cannot find any meaningful frame of reference for these values. For reference - I am reading from and writing to the same drive array (10k SAS). A 55gb backup job with --throttle=20 ran in roughly an hour with no apparent strain on the system during off-peak hours. But I honestly don't know if this is a high or low value for the throttle. 

I have a branch office behind a Watchguard XTM that needs VPN into an EC2 VPC. I am unfamiliar with Watchguard and am unable to find all of the knobs and dials in the flash admin interface to bring it in line with Amazon's expectations. After much ui frustration, I managed to create configurations for both expected tunnels that meet most of the specified criteria - as I was unable to locate some settings like MTU and ESP options but I have the correct PSK, PFS settings, and am telling it to use SHA1 and AES128 as requested. Attempts to access EC2 private addresses from the office generate a bunch of debug spam as is expected. However, IKE fails with: 

There is no good reason to suffer with half duplex. :) It sounds like the switch is inadequate for your needs. In my (admittedly limited) experience, cheap consumer-grade switches (even from "name" brands) cannot run at the full capacity implied by their number of ports and advertised speeds. Do you have another switch you could try in this one's place? Is it always the same ports that are failing? Always the same clients? It isn't difficult to get a decent 24-port managed switch for $150-500. 

Requests for /foosite/static.jpg are served fine, but nginx appears to garble the path to any .php files when attempting to dispatch them to fastcgi.