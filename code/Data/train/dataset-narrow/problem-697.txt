As the message indicates, your 3GB recovery destination is full; the database is unable to archive any more redo log files. It has reached a point where it needs to switch a full redo log file out, but there are none available for re-use because they can't be archived. I imagine this is why it wouldn't shut down cleanly, and possibly why you wanted to shut down in the first place as you wouldn't have been able to do much with the database in that state. The short-term fix is to increase the size of the recovery area, assuming you have enough disk space, e.g.: 

The connection string you showed at the top of the question is commented out, so you're actually connecting with . That isn't supplying a service name; but your does have , so this should be the equivalent of . (I've tested this with 11gR2 EE and the parentheses around the in the don't seem to matter). But that doesn't quite seem to work as expected. When you changed that and set the user name explicitly: 

You can use the same functions to get your table and view scripts if you prefer - I believe SQL Developer's export uses them under the hood anyway - and to get role definitions and grants. 

... the TNS-12514 went away and was replaced with an ORA-01017, invalid username/password. That means the connection string is now working and your account's password is something other than . With a valid username and password it looks like it should now work. If the substitution works as the comment in the suggests, and contains both the username and password, then making your connection string look like this ought to work: 

It looks like Oracle can't read the file, because it's under another user's home directory. Oracle has to be able to read it, not the Linux user you're running the command as. (Contrary to my earlier misleading comments!) Assuming the two users are in different groups, you may need to make the file world-readable: 

is the same as . So there is no practical difference between the two approaches you showed, if the commands are run consecutively (i.e. you are not doing anything else while it is started but not mounted). However, including in the command clearly shows intent, so I think it's generally better to include it, particularly if you're scripting this as part of a switchover process. 

Just for fun I put in the MV definition to catch the most basic workaround - so trying to insert also fails - but this model is never going to be very robust. I'm not saying this is a good idea, just that it's possible... 

In Oracle, writers don't block readers, and it won't lock the table, just the affected rows. Until you commit, all other sessions will continue to see the data as it was before your delete, and can continue to query. The only blocking you'd see in another writer session is if there is a primary key or unique index on the table and it tries to insert a row which clashes; or it tries to update/delete a row that you deleted. That second session will wait until your first session issues a commit or rollback; the second session will then either get an error (if you committed) or complete the action (if you rolled back). 

The default port used for is 1158. You may have changed the port though, of course. The control commands also display the 'about' URL which includes the port number. For example: 

It uses a range scan because is the first field in the composite index. It's cheaper (according to its stats) to get all matching index records for that location, and it possibly then filters the index results on the date range before retrieving table data. A skip scan would be considered if you were querying on and , or just on . 

So your first clause will match exactly , exactly , and anything in between. Your second clause will match exactly , i.e. any rows where is exactly . 

DazzaL has highlighted the nub of the problem but to give a bit more background, on the assumption you're fairly new to Oracle... The syntax shows that your command is trying to create a table called in the schema . Schema and user are essentially interchangeable in Oracle, so the error saying 'user does not exist' is referring to the schema/user. Unless you've already created that in your database, that won't exist by default. You can either (a) create the user, (b) update the script to replace with a (non-built-in; see below) user that does exist, or (c) update the script to remove the schema reference which will cause objects to be created as the user you're logged in as. But who you are logged in as is something to check - it isn't clear from the question, so apologies if you know this and it isn't relevant to you. It's important not to create your own objects under Oracle's built-in schemas, especially and as you could do a lot of damage, and you should not be logged in to a built-in schema routinely - only for specific activities that can't be done otherwise (like shutdown). The only exceptions to that are maybe the sample and schemas, and even then it'll be better in the long term to use your own account for anything except queries against those. If this is a fresh install and you're logging in as because that's all you had available, please create a new user, assign it the relevant privileges, and do all further work under that. 

At least, I think that's what you're trying to achieve... Unfortunately I can't add a demo as SQL Fiddle doesn't have the partitioning option, but this is tested against 11.2.0.3. Of course, you have to make it use the partitions for the query... if I just do: 

Then you need the grants the user has. Depending on your actual user some of these may not return anything: 

Oracle has an alert log which should tell you when and why it shut down. Depending on how the database was configured, that will be under a directory identified by either the or parameters, and will be called or (in previous versions). If you're running it, you can get information from Enterprise Manager too. Has the server it's running on rebooted as well? If so the listener may be configured to start automatically, but the database not; there's a flag in the file that says whether each instance should start, e.g. when is run, which could be done from an script on boot. You could try changing the flag for your database from to to see if it stops you having to start it manually, at least. If the server hasn't rebooted then the database may have crashed, in which case I'd expect to see errors in the alert log - search backwards, particularly for errors which can be fatal, but also for anything close to the time it crashed. 

Unless you want to get into wrappers, you need to have a password set for your listener. There's a good summary of how to set that up here, if you haven't already done so. Then if you start the listener as normal as , you can shut it down as by going into the interactive mode of the controller, enter command 'set password', and enter the password for the listener: 

This is presumably because truncate is DDL and doesn't do any checks on data in the target table, even to see if it has any rows. It would have to do that DML, and look for any matching rows in all child tables (or at least check there are no rows), etc, which would change the nature of the command and potentially impact performance. And it would have to consider uncommitted transactions against the child tables, which would have expected an error on insert rather than commit if the constraint wasn't defferred. The only way to do this is to disable or drop the constraint, as @ik_zelf shows. 

... and it should shut down. I know this is a different O/S and version but the principle is the same. And the reverse is true; if you start it as , you need to to be able to shut it down as . 

The only real advantage I can see of using the version, apart perhaps from brevity, is that you can change the configuration in the SQL*Net files rather than in the database; and even that only really seems useful if you're cloning databases between machines, or have your listeners and databases running under different accounts (e.g. with a user for RAC/HA). There's more on here. Edit: And this seems quite comprehensive. 

... where is the database service name registered with the listener. You can also create your own in a different directory and set the environment variable to that directory path. But is is the SID and not the service name then you'd need to modify what you'd proposed. For example, create containing: 

However, this is deprecated from 11gR2. The manual mentons using or OEM, but you could also just make switch to the account to start and stop the listener via . Obviously that gives them full DBA access, it isn't restricted to the listener, which is the case for too. I'm not sure why you'd want anyone you didn't trust to administer the database to be able to mess with the listener, or why you need to shut it down and start it up often enough for this to be an issue. 

You need to have different keys for the entries in the two configurations, e.g. and . IPC uses memory and semaphores etc. that may be shared across the machine, so you need to provide a way for the instances to be differentiated. Also make sure you're starting the listeners explicitly, e.g. , and you have the environment set properly for your two accounts, including pointing to the right . 

But you then need to work out how to manage your archives, and set the recovery area size to a sensible value for your backup and retention needs. This may be as simple as scheduling a job to remove obsolete backups, e.g. from the Enterprise Manager console's Availability tab, under Manage Current Backup; but you need to determine the best course for you (and not blindly follow advice from some random guy on the Internet who knows nothing about your requirements, and not that much about EM/RMAN either). 

The account is for adminstering Enterprise Manager, rather then the database; see the predefined user accounts. I would shy away from modifying anything about any of those accounts, even granting an additional role, unless specifically told to by Oracle. The first message you got says that import isn't supported when logged in with the role. That doesn't mean you can't import as , just that you have to log in as with the role rather than as . This assumes you want to do a full import though; for partial (e.g. schema-level) imports you might want to create the user first and perform the import as that user, but I'm not sure that's what you're looking for at the moment. 

I don't see any way to have intervals that are defined in a different calendar than your database-level NLS_CALENDAR. You could get the same effect by partitioning on a numeric representation of the (Persian) month each date falls in, using a virtual column: 

You should also consider using the data pump equivalent, , if you're on 10g or higher, which puts the dump file on the server; and you can create the dump using the API calls rather than with the command line tool if you need to. Getting the dump file off the server might then be an issue - not sure if you're using for a reason, of course. 

Bit late to the party on this one... The database should register with the listener automatically, making the entries redundant, and this seems to be happening with your environment. If the listener is started after the database it can take a while for it to register, and there may be situations where it doesn't do so at all. You can attempt to make it register with an command. The database uses the parameter to identify the listener it should register with. By default that is null, which according to the documentation is equivalent to . If doesn't make the service appear in the output then I'd suspect it's unable to either identify the hostname or resolve it, or it's resolving to a different address than the one the listener is on, or is set to something invalid for the virtual box. You can set the to either match the directly, e.g: 

Not directly answering the question as I don't use Toad, but you can use without a by using the 'easy connect' syntax for the connection string, at least in recent versions: 

If you wanted the log file to be created, it would be need to be writable too (). If it's in a subdirectory of your home directory, you might need to do this to all intermediate levels. But relaxing permissions on your home directory can be a problem, so you might be better off using a directory somewhere else. Or leave the file where put it, and grant permissions on that object to instead. 

So you have one or more Oracle homes (software installations), each with one or more independent databases running from that home. This was the only way to run multiple databases on the same server before 12c's container/pluggable model. The problem with doing that is the database instances are completely independent and have no visibility of each other, so they are all competing for resources (CPU, memory, network) at operating system level, and you have to have enough resources for them all to be running at peak to avoid degraded performance, even if that is unlikely to happen often. Or don't have enough resources and one or more end up starved at some point. You don't want that happening in production, so it's generally considered a bad idea to do this in production environments. This is also touched on in the 12c documentation: 

Clearly I haven't created any indexes yet. If you are looking for a whole month's worth of data, you would only need to query on a single value, and ignore ; but presumably you'd need a mix at least some of the time. 

Neither, it's a single instance (set of processes) with a single database. The definition you quoted refers to an aspect of non-Mulitenant architecture: 

Your parameter is set to , which is defined in your as being on port 1522. But your listener is on port 1522, so the database is unable to register itself. You can change your entry to match the definition. After the database is restarted, or you do , the output will show your service name as registered as well. But your SQL Developer connection isn't getting that far. You can't telnet to the listener port, which means either a firewall is present (which you've assured us is not the case), or you're running a virtual machine you haven't told us about (I'm suspicious about the domain the listener is reporting), or you're connecting to the wrong IP address. Check that on machine A and on both machine A and machine B, and whatever host name you are using for your telnet test and connection, all resolve to the same IP address. You can also use on machine A to check which IP address is listening on port 1522. You may have more than one interface and the listener isn't on the one you expect - perhaps because of a mismatch between a static address in your hosts file and what's assigned by DHCP and/or in DNS, for example. 

It's part of a distributed query. The string after the is a database link. You can see where the database link is pointing by querying the view, or the or equivalents depending on the owner and your privileges. It might refer to a TNS alias, in which case you could get further details from the file. When you query you're (usually) looking at a table in your own schema. When you query you're looking at a table in a remote database. They are different objects so you'd expect different results. Your local table is empty, but the remote one has some data. I say 'usually' because could be a synonym pointing at another schema, or even to a schema on a remote database itself. And could be pointing back to your own database - it would be unusual but I've seen it done as a way of accessing another schema (not something I'd recommend). Without seeing if there is a synonym or where the is going it's hard to be definitive, but these are less common scenarios. In this case though you're clearly hitting two different tables, probably on different databases. (Well, unless there's a VPD and connecting to the same table as a different user gives a different view... but starting to go down the rabbit hole now...) 

As @a_horse_with_no_name says, it's a file created with the original export utility, i.e an command rather than . You'll need to import it with the matching original import utility, using the command.