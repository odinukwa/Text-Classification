If you don't care that the loading screen is not drawn in OpenGL, you can just make a plain View which displays the image corresponding to your loading screen, display that view and then start the view that holds your OpenGL surface. Once that view is loaded, hide the loading screen view. 

Well, I'm assuming that for a given tile, you know the real-axes coordinates for each of its corners. i.e for the given tile (1,0) in your picture, you know its left-most corner is at, let's say, (5,10) on the actual xy axes. If so, you can use Java's Polygon class: create a Polygon instance containing a given tile's xy coordinates for each of its corners, then use the contains(x,y) method giving it as arguments the position of the mouse cursor, and it will tell you if mouse of tile or not: 

My issue is as follows: I've modeled a robot in Blender 2.6. It's a mech-like biped or if you prefer, it kindda looks like a chicken. Since it's symmetrical on the XZ plane, I've decided to mirror some of its parts instead of re-modeling them. Problem is, those mirrored meshes look fine in Blender (faces all show up properly and light falls on them as it should) but in Unity faces and lighting on those very same mirrored meshes is wrong. What also stumps me is the fact that even if I flip normals in Blender, I still get bad results in Unity for those meshes (though now I get different bad results than before). Here's the details: Here's a Blender screen shot of the robot. I've took 2 pictures and slightly rotated the camera around so the geometry in question can be clearly seen: 

First off, make sure you're adding a material on your mesh, then add the texture, set it as "image or movie", and chose "Coordinates: UV": 

Not being something that' I've personally tried with Unity (the whole networking part I mean), I can't give you a 100% sure answer, but I'm imagining that since whatever communication protocols Unity's using, they are (as standard networking protocols like TCP or UDP are) platform agnostic (i.e. if the platform supports TCP over IP, it will be able to connect to any other kind of platform that supports it). Continuing on this, netoworking inside Unity should be platform agnostic as well. This question and answer on the Unity q&a site seems to support this: $URL$ 

Unity's documentation for the "Transparent Bumped Specular" shader/material-type is simply a concatenation of each of the descriptions for its Transparent and Specular Shaders (and also Bumped, but that doesn't apply to the question): Transparent Properties This shader can make mesh geometry partially or fully transparent by reading the alpha channel of the main texture. In the alpha, 0 (black) is completely transparent while 255 (white) is completely opaque. If your main texture does not have an alpha channel, the object will appear completely opaque. (...) Specular Properties (...) Additionally, the alpha channel of the main texture acts as a Specular Map (sometimes called "gloss map"), defining which areas of the object are more reflective than others. Black areas of the alpha will be zero specular reflection, while white areas will be full specular reflection. To me this translates to: 

The short, official answer is no, you can't. Android has it's own specific API's which don't exist for any desktop environment. That being said, there's things you can do: 

Also check out the documentation for the Transform class here: $URL$ it explains pretty well what you can do with it and how it works. 

Well, one advantage of parenting your player to the moving platform versus applying constant force is that (at least with Unity) the former is easier to implement. When you want your player GameObejct to behave as if it's on some moving platform, just set its (the player GameObject's) parent to "platform" and then you're good to go. Obviously in this situation, you will always use the player local transforms to move him around. ==UPDATED as per comment== Ok, don't worry, I've been frustrated by this as well before finally getting it. You need to familiarize yourself with some basic Unity notions: 

I've actually read a good article on this (Deep hierarchies vs composition in game coding), unfortunately I can't find it. At any rate, the guy there advocated composition over inheritance whenever in doubt. I'll try to give you the gist of it as applied to your situation. I honestly think that composition is the way to go (and not inheritance) when it comes to game coding. Usually, the case is you have some basic thing on which you want to put various behaviors in various combinations, rather than make it have one behavior and ALL other behaviors derived from that. I.E. your GameObject base class might support translation and rotation as all game elements should be able to do that. Further more only SOME of your game objects also need collision detection, and more-over some of them might need to emit particles. Now obviously, collision detection ability doesn't automatically imply "ability to emmit particles as well" nor does the other way around make sense. So rather than having CollidingGameObject extends GameObject and then ParticleEmittingGameObject extends CollidingGameObject (in order to be able to also have game objects that support collisions and particle emissions), it's much better to make a base Component class, extend from it a CollisionDetectionComponent (which can work with a GameObejct instance) and a ParticleEmitting component, then have your base GameObject class support multiple components (via composition, like a list of components field), and adding and removing of components. Sure, at the beginning it will be more tedious to code, but once you have all your major components implemented and the component-applying mechanism in place for the GameObejct class, it will be a lot easier to modify/refactor things later, plus your code will be more performant. This is how Unity have coded their engine. So in your case, I'd make two Texture components, one for static and one for animated one, and then add them as needed on the GameObject base class (i.e. some GameObjects will only have 1 Static texture component, some will have 1 Static and 1 animated, and then later on if your fancy strikes you to make a dual turret tank, you can just add 1 Static and 2 Animated and you're good to go). 

Finally, one great newbie tutorial you should watch is Lynda's Unity Essentials, it helped me A LOT in understanding the basics of Unity. And it actually has dedicated chapters on how translations work and on how to make your own 1rst and 3rd person controllers. 

I have a metal drum mesh, and when I bake its normal map, I get the weird colors (see attached pic). I'm talking about the horizontal gradient shift of color. There's no deformation like that on my mesh, how come the colors are like that? I've tried various spaces (camera, world, etc) but it's still bad (though the horizonal gradient changes with each one). Also tangent space returns a blocky blue color all over. Can someone please tell me how to get a good normal map out of my mesh? 

Now in Unity, even though the light comes from the back of the robot, the cog-wheel in question acts as if light was coming from some-where else, its faces which should be in shadow are lit up, and those that should be lit up are dark. Here's some things I've tried and which didn't do anything: 

It looks like the normals are in the wrong direction. This is already very strange, because, while in Blender, the original cog-wheel and its mirrored counter part both had normals facing one way, when importing this in Unity, the original cog-wheel still looks ok (like in Blender) but the mirrored one now has normals inverted. First thing I've tried is to go "ok, so I'll flip normals in Blender for the mirrored cog-wheel and then it'll display ok in Unity and that's that". So I went back to Blender, flipped the normals on that mesh, so now it looks bad in Blender: and then re-exported as fbx with the same settings as before, and re-imported into Unity. Sure enough the cog-wheel now looks ok in Unity, in the sense where the faces show up properly, but if you look closely you'll notice that light and shadows are now wrong: 

OK, I've solved it. Apparently it's normal than Blender does this when you mirror an object. The thing to do, in Blender, before exporting, is to select the object in question, hit CTRL-A, and from the menu that shows up, select (Apply) "scale". To my shame, I do admit that I've now noticed that for objects such as my screwy cog, Blender would issue a warning along the lines of "something's bad with this objects transforms, apply scale to fix" when exporting, but that really didn't explain what was happening (plus, it issued this messages for other objects, which were fine when imported in Unity). 

As an observation to Byte56's answer: (especially) with Android, even if you want to make just a 2D game, it may theoretically seem to suffice using the Canvas draw calls and basic Java API. Doing it like this might seem to have certain benefits to using a specialized engine, especially if you're already very familiar with Java. The problem is, in reality you do want to use OpenGL (ES 2.0 in this case) even for a simple 2d game. First and foremost, it's a question of performance: drawing textures on quads as opposed to sprites drawn via Canvas draw calls will be literally orders of magnitude faster. Furthermore, Android imposes limitations on how many images you can load via Java & Canvas API, limitations which are not present when loading them as OpenGL textures. So in this case, you'll benefit a lot more from using an engine to do all this as opposed to doing it by hand. Just the part where it alleviates all the boiler plate code is a big plus. For example: try making something like an OpenGL "text sprite", using only the basic ASCII characters and one single font, by hand, for OpenGL. Then try using the built in functionality for this comes with AndEngine. You have a ton more options, and the learning curve will take you a lot less time than the by-hand-coding alternative. 

Eyes are hard to do because of their complexity. They have a complex motion and a complex structure. This makes duplicating their aspect and motion hard to do in a "resource-friendly" manner. Also, it might just not be that eyes are so badly done as compared to other stuff as it is more an issue of the fact that eyes are amongst the most studied parts of humans and what's around us. Maybe milk is not properlly done as well, heck, maybe it's worse than with eyes. Milk surface is tricky, there's lots and lots of sub-surface scattering and such. But since people tend to look more (and are more familiar with the aspect of) eyes than milk, it might just make so that eyes rendering is more criticized than milk rendering. Also, at a supperficial glance, eyes seem easy. I mean they're just a shiny ball with some concentric cirles of various colors "painted" on, right :) ? This might lead the junior 3d artist into over-simplifying the task and making just that, a shiny ball with concentric textures that completly lacks, for example the subtle light sparkle that happends in every (living) eye. Finally, eyes are small. Think XCOM Enemy Unknown (the 2012 version). 80% of the time you're in high-above-3rd-person view, you don't really see the eyes. But sometimes the camera zooms and changes angles so that you get a close-up portrait of the character. In such a setup (which is quite often), details which are not often seen from up-close (such as eyes) are intentionally left "un-detailed" so as to speed up development and conserve resources. 

I've also checked the Hierarchy panel to see if I haven't accidentally duplicated stuff which might get to be on top of stuff (such as 100 barrels all in the same position so as to look as if there's just one there), and this is not the case. So my question is: am I misinterpreting the Unity stats window? Is it a bug in the reported triangles count there? Or am I just completely missing how all this works ? 

Well, firstly, are you sure that your bones are correctly connected to your mesh(es)? If you move the bones around in Blender, do they deform the mesh as expected? 2nd: I'm no expert in XNA, but if it doesn't manage to import skinned meshes properly (i.e. take into account bones deformations), what you can do is make your animations in Blender (using bones, with IK or whatever), then bake those deformations into the mesh using the script: select armature (in object mode), open the scripts window and run Bake Constrains script from Animation directory. 

=== UPDATE 2 After further following the advices of Luke B, I've used multiresolution modifier on my low-res mesh, and baked from there. It looks much better than before, the color artifacting are down to a minimum, really, with some corrections in GIMP it's quite usable: 

Since you're talking about JDBC I'm assuming you're using an SQL database. First of all, even though the JDBC specs theoretically state that all JDBC driver implementations should be thread safe, it doesn't really go into much detail, instead it even states that it's ok if said "thread safety" is achieved by serializing all concurrent queries which may lead to a major bog down in performance. If you chose to use JDBC, try using it via a wrapper library that creates a configurable JDBC connection pool. There's tons of those, check those from Apache for example. As far as connecting to SQL databases using drivers created for other languages (like C++) I don't even think there's any spec guaranteeing thread safety at all, so again, unless you find a connection pool library you shouldn't use them. What you can also do is try one of the NoSQL databases. They're made for dealing with large quantities of data and can prove in some cases to be faster than SQL databases. (Not always mind you, but in the context of a MMO this may be the case). I recommend Mongo DB. They have drivers for all major languages (Java and C++ included) and the driver natively supports connection pooling so you don't have to bother with that at all. There's a learning curve, especially for assimilating the Mongo DB equivalent of SQL transactions and such, but it could prove fitting to your need. 

Create a large plane. Make the Cube the child of the plane, either by visually dragging it on to the plane in the inspector, or by adding the following code in the Cube's script's Start function: 

=== UPDATE Solved, thank you very much Luke B. There's still the question of scaling one of the two overlapping versions a bit (the low res one for example) in order to get really good results, but that's how it's supposed to be done. 

Well, I've solved it. I'm not very sure what was happening, but due to the fact that I've created and erased an then re-created the terrain multiple times in the scene (each time adding it to some scripts, etc), at some point something got jumbled up in Unity, probably because of this. Long story short, I created a new Unity project, copied in all my scripts and re created all the game elements and re-attached them to the scripts, and now everything works ok: I only need to scale the Blender object in the Start() method (and it stays scaled), and also it is transformed and shows up right from the start when I play the game. Thanks you to everyone for the help, the hints they gave pointed me in the right direction. 

To answer your 2nd sub-question: A great engine that not only builds on Linux, Windows, OS X and other platforms, but also abstracts away from all the OS-specific build tasks is Unity 3D. It gives you SDK which allows you to build your project for any of those platforms. 

Now, the selected cog-wheel-like piece is the mirrored mesh obtained from mirroring the other cog-wheel on the other (far) side of the robot torso. The back-face culling is turned of here, so it's actually showing the faces as dictated by their normals. As you can see it looks ok, faces are orientated correctly and light falls on it ok (as it does on the original cog-wheel from which it was mirrored). Now if I export this as fbx using the following settings: 

What I suggest is to have a right where the arms and weapon bones all follow the same inverse kinematic target. This ars-and-weapons IK target is in turn parented to the global ik target. For example if you're using Blender use some "Empty" objects for the IK targets. Then in unity just have the arms-and-weapon IK target point towards the mouse when needed. This way your over-all IK target will be controlled by the player controller (and move yoru whose character around) and furthermore, when the mouse cursor moves, the arms-and-weakpon IK target will keep pointing towards them while at the same time also having the transformations for the global IK applied to it (ie if the character is moving forwards and the mouse is somewhere to the right, the character will keep advancing but his arms and weapon will keep facing the cursor). You might also want to add some transform constrains on the transformation of the arms-and-weapon IK target (or the bone it ifluences) such that the player's arms don't gen in non-humanly-possible postures.