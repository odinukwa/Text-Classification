Question: Our solution: write 3 SQL queries search on "c1", "c2", "c3" are followed by conditions above. How can we do that with lowest performance ? 

Format CSV, disable quote (replace multiple spaces to one space then space & to ). is result file. However, please check your input data if it is large because I tested on small data . 

Until now, pgBouncer doesn't support rotate log. Hence, you have to do it by yourself. You can refer to sites below: How to rotate PgBouncer logs in Linux/Windows ? How To Manage Log Files With Logrotate On Ubuntu 12.10 

On the other hand, I could not find the AccessExclusiveLock mode in the pg_locks at the same time. Does anyone know why it happen ? As Tom Lane's message, It can be a lock on Notify queue. 

Description: We have a table on PostgreSQL 9.3 and we migrate it (data & structure) to Oracle 11.0.2.4. Here is our table: 

I am using nethogs command to tracking WAL send/receive between master and slave. Download: nethogs for centos link Install: Tracking: Ref: 18 commands to monitor network bandwidth on Linux server 

You can use Jailer tool, it will find all rows of child tables that reference to master table. Example: I have 4 tables : employee, employee_detail, phone_address, relationship. I want to delete employee which name="JOHN". With Jailer, it will find rows in employee_detail & relationship which FK to "JOHN" (by id). And because of phone_address reference to employee_detail, so Jailer will find rows in phone_address. 

It takes 10 milliseconds to execute the query on the user_info_raw (remote server). But, It takes a lot of time when using theforeign table. When I remove , the query executes very fast. I think that my query on foreign table should send to the remote server for executing, but it's not, I don't know why, may be due to of this excerpt from postgres_fdw document 

But now if I come back on this command with Arrow-Up, position the caret just before the option, add a non-breaking space by hitting Alt+255 on the numeric keypad, and validate this command, it yields the error you mention: 

it does not appear in , but in the current schema ( by default). This is made apparent by the command in psql: 

The fact that UNION deduplicates the tuples takes care of filtering cases when the same word is found among different subqueries of the UNION construct. This query produces only IDs of tasks. They'd need to be joined against and again to get back the columns that need to be displayed or returned. 

Note that is not used with this method. This target query could be generated as a dynamic statement with the following query, which is basically a cross product over the week numbers and (1,2,3) with some bits of SQL around: 

The specific problem of having the trigger consider a group of related changes rather than independant updates could be solved by : 

I'd suspect you have set to OFF on the 9.1 instance (meaning it was changed explicitly, since the default is ON since 9.1), and the opposite in 9.2. Demo: 

Should you want to reduce the number of log files or accelerate their deletion, this is the file to modify. But it would better to identify and suppress the root cause of these huge logs. 

To find the query bound to this statement, look upper in the log for the corresponding entry. If it doesn't exist, you might have to increase the logging level to through , or set to 0. 

Presumably this will lead to the same error message: which is the root cause, the other error from postgres being the consequence of the abrupt end of the data stream. If the file is a copy resulting from a transfer, you want to compare its size against the size of the original, and transfer it again if it's incomplete. If that's the only copy or if the original itself is incomplete, completing the restore will not be possible. 

Here our query and result we want, it means: when deleting id = 1 (parent row), table will automatically set parent_id = null in child rows (first level) . 

PSQL connection is OK. When running ETL, I am sure that client on machine 1 is connected to PG database on machine 2 , I tracked by query below 

Here, as you can see with value it worked (didn't raise error), however others didn't. I'm curious about that. Could someone please explain to me why ? I attach test script 

After testing with 1 million row, here's the best way from @Julien Vavasseur combine with Index on "user_id, grade, grade_date" . We should consider using window function when having a lot of rows. 

I read on Cassandra Calculator, with those configurations, it said that Each node holds 50% of your data. Case: , (* ) OK when I stopped or Question : 1/ Why , (* ) stopped when I stopped 1 in 2 nodes ? 2/ Why "Each node holds 50% of your data" as those configurations? 

Question: After researching , I found 4 solutions. Are there any better solution (not using copy data from files) ? 

Erwin said "You probably don't want to hear this, but the best option to speed up SELECT DISTINCT is to avoid DISTINCT to begin with. In many cases (not all!) it can be avoided with better database-design or better queries" . I think he's right, we should avoid using "distinct, group by, order by" (if any). I met a situation as Sam's case and I think Sam can use partition on event table by month. It'll reduce your data size when you query, but you need a function (pl/pgsql) to execute instead of query above. The function will find appropriate partitions (depend on conditions) to execute query . 

You can then schedule this to run daily with a SQL Agent job or if you have SSRS you can easily schedule and format the report. If you go the SQL Agent route you will need to configure Database Mail and use that as your delivery service for the results of the query. 

We cracked this a while back by replacing the RAID controller on the server. The disks and server configuration were fine but it appears that the RAID controller couldn't deal with the IO. We are now in the good place of reads ~ 2ms and Writes at <= 5ms 

I don't think a cursor is good solution to this as it would have to be running 24 / 7 and you would need to check it was running. A better solution would be to have a SQL Agent job that runs at midnight each night. The Agent job would call a stored procedure that checks the flag, field and field for each employee and then increments the mentioned values for each employee. The stored procedure could send an email after each run to show you how many records were updated. It could also incorporate error handling to alert you if there was a failure for some reason. This way you can be sure the process is running correctly. 

I have used temp tables to build an example of your schema so I can test the query. You can substitute your table names in where I have used the temp tables. 

waits do not necessarily indicate problems with the IO subsytem. These waits are logged when SQL Server is waiting for data page to be read from disk into memory. More information from Paul Randal can be found here as mentioned in the comments. So if there was a problem with the IO subsystem it could cause high levels of waits but they can also be caused by the buffer pool being full so SQL server has to wait for space to be made before the pages are read into memory. Any applications running on the same server as SQL Server could cause these waits by trying to use the same memory that SQL Server wants. Non optimised queries that are not supported with suitable indexes can also cause many waits as the query may perform index scans instead of seeks which will cause more IO and mean that more pages will need to be read from the IO subsystem into memory. Index scans could also be caused by out of date statistics which cause SQL Server to produce non optimal query plans. Obtaining a baseline for your wait stats is a great way to enable you (or your DBA team) to spot wait stats during moments of poor performance that may indicate the cause of the performance drop. Again Paul Randal talks about this here and also gives some explanations of common wait stats. I have these explanations saved in an offline cheat sheet that I always have open for reference. 

An approximation of the size of a row, including the TOAST'ed contents, is easy to get by querying the length of the TEXT representation of the entire row: 

Files below are supposed to be data files that are not meant to be edited. They are not live configuration files. The FHS spec at $URL$ defines /usr as: 

In theory it's possible through connection sharing. Instead of connecting directly to PostgreSQL, would connect to a middleware such as pgBouncer, which has the ability to reuse the same backend connection across its multiple clients. 

The point of is that it must pull all the decompressed data, as opposed to, e.g. . Once the corrupted row(s) are found, can be used to delete them. Note that if VACUUM is working on this table, if might change the of certain rows. Turning off autovacuum or doing all the work inside a transaction should avoid that potential problem. 

The TG_* built-in variables are documented in Trigger Procedures and the and plpgsql constructs in Basic Statements. The query above is absurd by itself (it selects results that go nowhere); the intent is to just show the base syntax on which an actual query could be built. 

The function, as the first choice to format dates, is quite limited with time zones. According to Template Patterns for Date/Time Formatting in the documentation we have and and they'll output a time zone name, not a offset against GMT. You may use to get the offset as a signed number of hours and to get the number of minutes that comes with it. This offset is documented as: 

If the resultset has many wide rows making the sort inefficient, sorting a hashed representation instead might be a better idea: 

To exclude certain tables from your refresh of the integration environment, with a backup from production, you will need to make use of a staging\data cleansing environment. The plan would be: 

Only way it could break that I can see is if you have two movies with the same title and different mID's. 

You could have done this by stacking pivot statements but this can be messy and inefficient. This is a different way of pivoting data called the cross tab method. This method is usually more efficient than the pivot statement and easier to read. 

Alter the name of the file so it ends in .bak or select 'all files' when browsing to the backup file. 

I have 4 servers: A, B, C and D. A and B are in a master-master setup. C is a slave to B and D is a slave to C. Changes on A or B get to all nodes other than D. I need them to get to D Changes on C get to D but obviously they don't go to A or B which is fine. After some reading I found that I needed the log-slave-updates option set on C, this way C would log the changes replicated to it and then D could read those and make the same updates. I have set the option to on and D is waiting for it's master to send events but changes from A or B are still not getting to D. 

If your RTO is ~ 1 minute and your RPO is 1 day of data loss then you could use log shipping. This would not require AD, a Windows Cluster or Availability Groups. If you shipped your logs every 15 minutes the recovery time needed to bring the secondary should be well under a minute (Needs testing but I would be surprised if it wasn't from what you have explained above). The only thing you would need to deal with would be how the application manages the connection to the secondary when the primary is down. This could be handled in the application, a load balancer (with static routes that would need to be altered on failure of the primary) or a clustered name resource (would probably need to be manually failed over). A FCI would mean you wouldn't need to handle and monitor all the log backup/copy/restores of log shipping and your RTO would be lower. This is not possible without AD. Also you can't have Availability Groups without a Windows cluster which also requires AD. 

Here, you can use sequence to increase value. Please noted if the sequence reach to its , you will face this error . For example 

We want to search on "c1, c2, c3" coloumns on dtsc_search_data table with conditions: if seach_value is found on "c1" then return ; if search_value is not found on "c1" then finding on "c2", if search_value is found on "c2" then return ; else return (c3). Example: 

In EDB 9.3 , just one query no.4 can cast. And PG 9.3 , all queries can do. Are there still any ways to cast date in EDB ? EDIT: With EDB : 

About errors above, because of the connection between pgpool and slave server, if I change slave's pg_hba.conf for pgpool host from md5 to trust, it work fine. Two ways to fix: 

From @Craig Ringer, you should re-format the data. Based on your data, on Linux, I used & to format CSV file before importing. For example: suppose that file as below 

Machine 1 (slave) and machine 2 (master) are in a cluster (streaming replication). Sometime, I see "FATAL: terminating walreceiver due to timeout" in slave log. Here is full detailed logs: Slave 

Master ----> Slave : relpica from Master to Slave by asynchronous method (M send WAL, S receive WAL) 2/ Question: How can I monitor (catch) speed of WAL (ex: 1MB/s) is sent from Master to Slave ? 

Firstly, I have no idea about pgAdmin4 Secondly, It would take a lot of time when executing a large SQL file. Please consider using pg_dump (custom format) & pg_restore to enhance your performance. 

2/ QUESTION: What is about "FATAL: terminating walreceiver due to timeout" problem ? How can I fix it ? 

Problem: I tested over 10 times, each of times when data is inserted about 2,000 rows (2,000 records is commited into machine 2) , with insert time about 3 minutes , I got errors: 

Using SSMS the error log can be found in the management folder. I would check the log for more information and run a full DBCC CHECKDB to ensure the database is OK. Paul Randall is an expert in data corruption in SQL Server and he mentions a fix to your problem in this article. I would read this through to get a good understanding of the situation you are in. In brief he says that the situation may be solved by switching the database to SIMPLE recovery mode, back to FULL recovery mode and then performing a FULL backup. The full backup restarts the log chain. I would also inspect the hard drives that the transaction log file is running on as they may have caused the corruption if they are faulty. 

While you could schedule an JET OLEDB type query as a SQL Agent job with something like the code below: 

@wBob got it in one. It's all about those NULLs. The following script creates the Destination table with a modified column and it alters the same column on the TableName_Partition2 to be after creation. This allows the to run. 

I think you will need to use some looping to do this as you have to define a group of records when the dept changes. I have written this in TSQL so you may need to make some syntax changes but it works. 

This looks like a lot of work but I have done it in the past and it is quite simple. You basically create a HTTP endpoint in IIS that your excel clients connect to. The endpoint allows basic authentication while IIS authenticates with the cube on a predefined account. Full instructions here 

If the certificate expires TDE will continue to work so don't panic there but you should still replace it to keep with your security policy. Because the certificate is used to encrypt the Database Encryption Key (DEK) and not the data, it is quite easy to change the certificate. Create or import a new certificate and then use it to encrypt the DEK.