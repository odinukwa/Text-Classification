That's it. If one day AI can learn how to play Starcraft 2, it probably learns in a very different way from what it learns today. 

Competitive self-play without human database is even possible for complicated, partially observed environments. OpenAI is focusing on this direction. According to this article: 

The problem is, at the moment, our state-of-the-art AI models (Deep Learning) don't have the ability to explain their decision or reason about their behavior, which makes learning highly inefficient. In addition, the Reinforcement Learning framework is way too simple and it ignores the complexity of human environment. You might find this answer useful. Geoffrey Hinton, one of the leaders of the field, said in an interview that AI needed to start over. 

That's an important reason for the success of self-play. OpenAI achieved superhuman results for Dota 2 1v1, in August 11th 2017, beat Dendi 2-0 under standard tournament rules. 

In order to train an agent to play a board game, the first important task is to create a Reinforcement Learning environment. 4 essential aspects of an environment is: 

Currently, we're not ready to solve these problems, and if Gartner is right, the hype won't come to peak in 10 years. This is the Gartner hype curve in 2017, and AGI is still in its early phase. 

I now read a book titled "Hands-On Machine Learning with Scikit-Learn and TensorFlow" and on the chapter 11, the author writes the following explanation on batch normalization: 

In many cases an activation function is notated as (e.g. Andrew Ng's Course courses), especially if it doesn't refer to any specific activation function such as sigmoid. However, where does this convention come from? And for what reason did start to be used? 

I'm reading a book titled Python Deep Learning, and in Convolutional layers in deep learning on the chapter 5, the following is written: 

It is not a pre-requisite, and you can learn it easily once you encounter something you are not familiar with. Statistics is pretty old and there are many learning resources on the Web, which you can get to whenever you hit the wall while learning about deep learning. As to which field is pre-requisite, I think it is enough to first learn about Gaussian (normal) distribution, linear regression, and logistic regression. Then when you encounter something you don't understand, it is time to invest your time on statistics. The more requisite fields I believe are calculus and linear algebra. If you haven't learned about them (such as partial derivative, matrix transpose, etc), it is very difficult to start to learn deep learning. Also, the prior exposure to some of machine learning algorithms would make your learning faster. But I'm sure you already got it given that you finished Andrew Ng's course on Coursera. Andrew Ng starts deep learning course on Coursera from August 15th, so you can join it and get a grasp of what is required. The book you linked sounds more like focused on machine learning than on statistics, BTW. 

You could perform unsupervised clustering on the data(k-means), this will give you relationships like weights of the people whose health is not good for a particular number of days. 

Named entity recognition can be seen as a multi class classification problem. A large data set will be required to train a model(preferably bayesian) for recognising different named entities. You can use word embedding (like google word2vec) to for preparing your training set. Also, if you can try IBM Bluemix AlchemyAPI for named entity extraction if you want to get your job done. 

While using RMSLE, you should pay attention to the point that the metric penalises the under predicted values more than the over predicted values, hence driving the the model towards high bias. This might be changing the weights of a model and hence increasing error after second iteration. you can go to the following link 

You can use R or Python for 100 million records with the conventional regression libraries. You will require around 16GB of RAM according to my experience, may be more than that!! A quadcore processor will be fine while running algorithms and during pre-processing steps. It would be better to store the transformed data to an immediate database. 

This answer is based on my opinion and others', it might not be the answer you expect. The problem you want to solve is probably in the field of Artificial General Intelligence. The problems require AGI to solve are informally known as AI-complete. 

Accuracy is probably not a good metric for your problem. For the original dataset, if the model just makes a dummy prediction that all samples belong to the bigger class, the accuracy will be 83% (100/120). But that's usually not what we want to predict in an imbalanced dataset. Let's take a fraud detection problem. The probability that a transaction is a fraud is very small (let's say 0.01%) but the loss of an undetected fraud transaction is enormous (e.x. 1 millions dollars). On the other hand, the cost of manually verifying if a transaction is relatively small. In that case, we would like to detect all possible frauds, even if we have to make a lot of false positive predictions. To tackle an imbalanced dataset, first you have to choose which question you want to answer. Then, what's the good metric for this question. Answer these 2 question first before deciding which technique you should use. Come back to the original question. Why does accuracy reduce when we oversample the smaller class? That's because this technique puts more weight to the small class, makes the model bias to it. The model will now predict the small class with higher accuracy but the overall accuracy will decrease. 

As far as I know, the width and the height should be a figure divisible by 2, in order to use a pooling layer. However, I don't understand why the depth must be a figure divisible by 2. The pooling layer just operates on a 2-dimensional screen based on width and height, and it operates on each filter (depth) separately, right? Why should the depth also be set to a figure divisible by 2? 

The function is from TensorFlow, FYI. The author explains that the γ parameter should not be set on ReLU activation function. However, I don't understand why on ReLU, the next layer's weights can take care of scaling... I understand that the next layer takes input from the ReLU output, which is . Why does it take care of scaling and thus no need to set γ parameter on ReLU? 

I now read a book titled "Hands-on Machine Learning with Scikit-Learn and TensorFlow" and on the chapter 11, it has the following description on the explanation of ELU (Exponential ReLU). 

I wonder whether one epoch using mini-batch gradient descent is slower than one epoch using just batch gradient descent. At least I understand that one iteration of mini-batch gradient descent should be faster than one iteration of batch gradient descent. However, if I understand it correctly, since the mini-batch gradient descent must update the weights by the number of the batch size in one epoch, the training would be slower than the batch gradient descent, which computes and updates the weights only once in one epoch. Is this correct? In that case, is it worth worrying about the loss of the overall training time? 

Firstly, I would like to tell you that domain knowledge plays a crucial role applying machine learning models. Coming to your question 1: ML algorithm will not strictly determine weather to use a feature or not. ML algorithm will only tell you about the relevance of a particular feature present in your feature set. For example, while doing regression, variables associated with higher coefficients are of greater importance than the ones having lower weights,it might be very close to zero as well, which gives you hint to not consider the feature. Similar is the case with the fbeta score of a variable while we deal with trees. User can specify what features are to be considered. User could also make some derivative features which helps a lot while dealing with ML problems Generally this is an iterative process. For the second question : There is no specific mapping between the problem statement and hypothesis. You may develop some of them with experience. You should not(usually) choose such custom functions as they might be fitting your training data perfectly, but could give poor results on test data as there is loss of generality by using a specific function. You could try different algorithms, with making new features, using ensemble methods and other machine learning techniques to improve your score 

In order to converge to the optimum properly, there have been invented different algorithms that use adaptive learning rate, such as AdaGrad, Adam, and RMSProp. On the other hand, there is a learning rate scheduler such as power scheduling and exponential scheduling. However, I don't understand at what kind of situations you should use one over the other. I feel that using adaptive learning rate optimization algorithm such as Adam is simpler and easier to implement than using learning rate scheduler. So how can you use it apart properly, depending on what kind of problems? 

Suppose that I train my image dataset on CNN, but the resolution of the image varies significantly on the dataset. In this case, should I scale the images up to the image that has the maximum resolution, or scale down to the lowest resolution? In that case should I scale up/down the whole images even if the highest likelihood of the whole samples are somewhere in the middle of the distribution of the resolution? Or should I use another technique to deal with the varying resolution problem? 

I just started to learn Convolutional Neural Network, and like to predict a Pokémon type by its apperance (the input is image). However, while many Pokémon has only one type, some Pokémon have two types (no Pokémon has more than 2 types). I'm not sure the proportion, but it is something like 60% for only one type vs 40% for two types, I guess. In this case, how should I classify it? Should I mark all the output probability that has > 33% as predicted types, or is there anything better way? Also, is the convolutional neural network suitable in these cases? 

You can try merging the groups A and D which will provide more significant boundaries to the model. Apart from this you can try to sample the given data to form a new data so as to form a uniform distribution of all the groups. 

I am working on to implement the approach in the paper $URL$ on detecting text in wild using recursive convolution neural net and attention modelling. Being a bit new to deep nets and have developed the following interpretation about the recursive CNN given in the paper: It is given in the paper that while training could be done by " reusing the same convolutional weight matrix multiple times at each layer." Does this means the following thing in the pseudo code below layer1: iterating multiple times over { train convolution_layer(weights) with labels } layer2: iterating multiple times over { train convolution_layer(weights) with labels } Here in the consecutive iteration in a layer we use weights of previous iteration. We use different weight initialisation for different layers here While using the network for test set, we will input the weights we got from the last iterations. Please comment on this interpretation and I would be grateful if anyone could suggest some links and literature for the same. Thank you!