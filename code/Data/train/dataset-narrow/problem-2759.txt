Rigidbody should update velocity each frame, even if you are using MovePosition(), however, if you need to know what the velocity will be before the object is actually moved, then you will have to calculate it. 

You will want to use Texture2D.SetPixel or Texture2D.GetPixels and Texture2D.SetPixels if you want to blit sprites onto your texture. You could render your sprites to a RenderTexture. It may be faster but until Unity5 it was a Pro-only feature (which I don't have) so I haven't experimented with it yet. You can't draw individual pixels on the GPU without a lot of complications though anyway, so if that is a requirement then you will need to use Texture2D. For a 640x480 image, it should be sufficient to just do it on the CPU with Texture2D, even if you update it every frame. 

I would suggest using fbx. It is much more widely supported by other tools and that generally means that even in Unity, more effort will have been spent on making sure that fbx works properly. If you need to import your models to other tools besides Unity for processing etc, you will probably need fbx anyhow. Also, if you decide to use some other modelling program besides blender in the future, then it will be easier to migrate them. There is no real advantage to using blend files, unless you are planning to use blender specific features, which are not going to be available in other programs. PS. The one advantage of using blend files in unity is for speed and convenience during development, but I would still convert them to fbx when they are finalized. 

You will want to store somewhere, so you can reference it outside of . Then you can prepend it to your relative paths to build the full path to your resources. 

The example you have posted should work fine. I would suggest there is something else wrong in your project if that is not working. If your loop repeats too many times, or indefinitely as you have suggested in your question then you might cause Unity to hang, since it will wait for the loop to complete before it updates the next frame. You will need to use a coroutine and put your loop in there instead, if that is the case. $URL$ 

If the client receives a destroy packet without receiving the create packet, then it should know that it is out of sequence. The client won't have a record of the entity. Queue it until the create packet is received. As you said, some packet types are sequence dependent and others are not. The client should know the protocol. Another option is to send the create and destroy as a single event in the same packet if the original create packet hasn't been ack'd yet. This would depend on the protocol you define. You might have an extra bit in your create packet that indicates if the entity has also been destroyed, if there is no other information associated with the destroy event. You might also allow sending multiple events per packet for certain event types like create/destroy or even for all event types though if the data is variable length, then you will need a way to delimit the events in your packets. If the server sends a new (as in different sequence ID) create/destroy packet but meanwhile the client does receive and ack the original create packet, it should know from the entity ID that it's a duplicate, regardless of the sequence ID being different. So it would just ignore the new create event and apply the destroy only. 

1. Scene Manager The scene manager keeps track of the scenes in a game, allowing to switch between them. At it's basic, it provides a centralized place to load and unload the scenes, keeping track of which one is loaded and handle unloading that scene when a new one is loaded. See here for a basic example of a SceneManager. It can get quite a bit more complicated, depending on the engine/game but primarily, its job is to provide a place to manage all the scenes so that each scene doesn't need to be uniquely referenced in the game code. Instead of the game having to deal with 10 or 20 (or 100) different scenes all over the code for example, by using a SceneManager and assigning all the scenes to it the game code can refer to the SceneManager and never need to know about any particular scene, how many scenes there are or if one is added/removed. In this way, to the rest of the code the number of scenes and specific names of the scenes does not need to matter. This is a rather generic description. Some games or engines may use the scene manager a bit differently but the general concept will still apply. Usually the difference will come down to how a scene is defined. In most engines a scene is an entire level. In others a scene might be a subset of a level. In yet others, there may be no concept of a scene and instead they use screens but the same idea would still apply. Really, this is just a software design pattern. In other areas of software development, we might call this a Facade or Mediator design pattern. 

The computer is not aware of what type of object is moving around, whether it is a character walking/running (which you don't want to launch when it hits a ramp) or a car driving (which you would want to launch). Since you said that your character is moving at high velocity, that would explain why it is launching. It is best to think of your character as a vehicle with wheels on the feet to help understand why the physics engine is launching your character when moving at high speed. I haven't done it in Unity but I think the fix would be to restrict to only move in the downward direction if the player is not grounded and not jumping. This should allow your player to fall off cliffs or walk downhill with normal gravity but would prevent it from launching into the air when it hits some type of ramp. As long as the player is grounded, running uphill shouldn't be affected. If the player is explicitly jumping, then don't restrict . If you want to allow for a little bit of bounce, you might try scaling down , rather than limiting it to downward motion or some small positive value. Otherwise if you just limit it, your character will look like it's hitting a ceiling when it launches too fast. 

You need to flip the texture, not the object. Sprite/Plane meshes are one-sided so if you flip the actual object, you are seeing the back of it, which is transparent. Edit: You could also make your mesh two-sided. But Unity doesn't provide a flat, 2-sided rectangle. You will have to make it, either in a modelling program or with code. 

The PS3 controller is not directly compatible with PC. You will need a driver and a program to translate the controls to the format used on a PC. If you search the net, most articles will refer to an app called MotionInJoy but I've read many reports online that it is adware (maybe worse) and it's advised to avoid it. I've not used it but better safe than sorry. I have used pcsx2 to emulate the XBOX controller using my standard 6-axis controller for games that only work with XInput and this app should also work to emulate your PS3 controller (doesn't work in windows) as an XBOX controller (does work on windows). I can confirm that pcsx2 works well. You will need to download the official driver for the XBOX controller and the pcsx2 app, then configure pcsx2 to map your PS3 controls to the XBOX controls. See this forum post for full instructions. PS: Some folks in that thread also mention BetterDS3 as another option instead of pcsx2. I've not tried it but if you are having trouble getting pcsx2 to work, you might want to try that one. It may be a little simpler to get it working, though pcsx2 wasn't very complicated and is more flexible/useful. 

The conditions for your branch will always be true if transform.localEulerAngles.x >= 0 and <= 360. The > 270 part is redundant since if x is less than 270, it is still possible to be >= 0. If x is 0 and lookAxisX is > 0, then adding -lookAxisX will make x < 0, meaning your branch will never be enter again, unless you reset the rotation somehow. Vice versa if x = 359.9 and lookAxisX is < 0. 

3. Where to Store Caches This is very subjective. You might have a ResourceManager to handle these things for your engine. You might have a manager for each type of resource or you might have both, where the ResourceManager coordinates all of the specific managers for each resource type. How much you need to break it down depends on the complexity of your game engine. Stick to the KISS principle and modify your design as you find that things are getting too messy and unmanageable. 

You can also combine both of these approaches to make it even more natural looking. For world boundaries, a simple solution is to use fog so the player simply can't see anything outside of the bounds. 

Don't think in World Units -> Heightmap. Think in World Units -> Model Units -> Heightmap. This is called normalization. You will need to normalize from world units back to your model units before you try to map to the heightmap and vice versa. You can do this with some simple scale-factor, assuming that the vertices in your terrain mesh are uniform distance of 1 unit apart from each other. Lets say you have a terrain mesh that is 100x100 vertices and a height-map that is 100x100 pixels, but you want the terrain to be 257x257 units. Now, when you are painting with the terrain brush, you will find the world coords from your mouse position, then scale that position back down to your model units, so you get a 1:1 mapping with your height-map. In this example, the model scale is 100units but the world scale is 257units, so that is a scale factor of 2.57 Let's say that you click on the position at {183,183,z} in world units. So divide by 2.57 and you get 71.206226... rounded, that would be vertex[71][71] (but don't round yet). You will need to do this for the brush size as well, if the brush size is 3 world units in diameter, then divide by 2.57 and you get a model unit size of 1.167315... for your brush diameter. Knowing this, you can work on the vertices and pixels within a radius of (1.167315/2.0) from {71.206226, 71.206226}. If your brush were square, that would be {70.622568, 70.622568} to {71.789884, 71.789884} Once you determine the bounds of your brush in model units, then you round to integer so you can index the vertices or heightmap. Hope that helps. 

A better way might be to use GL_QUADS. If you just want the outline with no fill, then set GL.wireframe before calling GL.Begin(). The benefit of using a quad would be that you could turn wireframe on or off to show a filled poly if you want. Also, it only needs 4 vertices instead of 8, so it should map to your existing code with no changes. You could also use GL_TRIANGLE_STRIP but if you want to display it in wireframe, it will have a diagonal line through the rectangle. Keep in mind that when using the polygon primitives, the order of the vertices matters. You must specify them in clockwise order in Unity or your shape will not render properly. This doesn't matter if you are using GL_LINES. 

Like most things in software development, the answer is "it depends". Only you will be able to decide which is the right approach because you have intimate knowledge of what you need to accomplish and whether it is worth the extra effort to make it right. Properly designed, you might use both methods. Global anything in code is typically a bad idea. Though it should not and cannot be avoided entirely, anytime you find yourself considering something for being global, think it over carefully because it will implicitly tie things together, which most often leads to greater complications than explicit connections between your code. It's always a good idea to limit the knowledge of one piece of code to another as much as possible. Even though it means more thought and more work up-front to connect things together and will be a bit more complicated to code your classes, it will simplify the use of those classes. It also helps to avoid problems later when you need to make changes to or reuse either part. You want your code to be "plug 'n play" as much as possible. Compromises always have to be made though. It's a trade between get it done fast or get it done well. See Coupling and Cohesion. Always try to aim for low coupling and high cohesion. Sorry this is a round-about answer and I can't explain this in too much detail for you. It is a rather deep topic and will take quite a bit of experience making your own mistakes before you really get it. Just keep these things in mind. To be more direct, if you have to ask then just make it work and don't worry about it too much. It seems like you already have a decent idea of how to make it work, so just go with it. Refactor it later to be better when you find there are issues with your current implementation. What most good engines do is evolve as needed. I don't expect you want to spend several years developing your engine to perfection before you produce your first game. 

You will have to manually add all the frames for your animation. You don't need tweening unless you want to rotate or move the sprite from point A to B and have it automagically interpolate (be)'tween the key frames. For walking, jumping, shooting animations, etc. You have to do it manually. There's no magic for that. Here is a little guide to help you with how to use the timeline: $URL$ 

As you know, if you want to generate the road so that both players see the same thing, then you need to start from a common seed value. If you always start generating the road from the beginning then you can just keep calling your random number generator and the values it produces should be consistent for both players at each step. (Assuming that both players' devices are using the same pseudo random number generator (PRNG), which is not always the case. You may need to provide your own PRNG to ensure that it's consistent for all players). If you want to be able to resume from different points along the track, then you will need to be able to reseed the PRNG from that point and keep it consistent. You would do this by calculating a new seed at each section (whether you are resuming or not) by using the original seed plus some reproducible value such as distance traveled. Eg. (pseudo-code) 

Quad-sphere with chunked LOD is the preferred method if you want to be able to go from space to ground with any level of detailed terrain, either procedural or predefined heightmapping and textures. Icosasphere provides a more uniform mesh and is easy to tessellate but poses problems trying to map textures and heightmaps which you will need to cache and won't be very compact or simple that way. Quad-sphere has pinch points but with enough tessellation you won't see them anyhow. Then you can map textures and implement DLOD effectively as if each region (chunk) is a square grid with little problem. This is simpler to implement compared to an icosasphere and will be more efficient, both in computation and resources. See Sean O'Neil's articles about generating a procedural universe on Gamasutra: - Part 1 Perlin Noise and Fractal Brownian Motion for heightmaps and textures. - Part 2 ROAM Algorithm for procedural mesh with DLOD for planet generation. Suffers from performance problems. Not recommended but good for educational value. - Part 3 Addresses problems with massive scale, optimization and floating point issues. Mainly related to universe scale but also applicable for planets when transitioning from scales of light years to centimetres if you want. - Part 4 Discusses implementation of Quad-sphere with chunked (quad-tree) DLOD for planet generation <-- see this article in particular