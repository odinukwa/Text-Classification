I would link at all/most levels. This denormalized star means that yes, the data is redundant, but it typically makes the reporting and analysis a lot easier. Note that this is very different from OLTP normalization, and you don't typically have to worry about redundant data getting out of sync because in a DW scenario data never changes. New facts get added and dimensions get expired and new ones created. I don't see a Dim_Folder. I would assume that the actual path of the folder would be an attribute of the Dim_Folder. Only the numeric quantity and any degenerate dimensions ($URL$ would be in the fact table. I wouldn't think of the folder path as a degenerate dimension because it keeps coming back in each snapshot (an a folder isn't a transaction). So you could do something like this: 

In addition to JNK's answer, you probably should read this article which discusses aligning table partitions and index partitions. There are many types of scenarios where partitioning scheme does exactly follows the primary key's first column - for instance in a data warehouse scenario where the snapshot date of a fact table is usually the partition column as well as the first column in the primary key. But equally, in OLTP environments where the PK is an IDENTITY or other surrogate key, it makes little sense to use this for the partition, since partitioning on arbitrary numbers is not normally terribly useful. In OLTP systems, you also tend to partition by date the most (probably not in the PK), but potentially also regionally or by some kind of organizational division (maybe in the PK if you aren't using a surrogate). But it's not a requirement. 

You haven't really indicated how this is called. Do you really have a string full of comma separated ints or is that just an artifact of some other aspect of your system? If the outer part of the environment around this requirement is changeable, then typically in SQL Server 2008 and up, I pass a table-valued parameter containing a list of IDs (perhaps from ADO.NET or whatever) and then just use it as a table inside the procedure - i.e. or 

8 million rows? - network speed will determine how long it takes to download the rows, but as to why you are losing connection, you'd have to dig deeper to see if you have a timeout setting? 

I don't think it's a significant amount of data in these tables - about 26000 rows on each side, and I don't think the set of differences is that big of a subset of the two. But I was wondering if there are issues with DB Links which make cursors preferable to regular set-oriented methods? This operation is part of a stored procedure which is run from an Oracle job on a daily basis. 

You haven't really given enough information about the original execution plan, what it changed to or what columns you put in the new NCI. It is possible that the query performance can be improved even further. The cost of the new index is the initial build, the additional processing of entries on INSERT and UPDATE and potentially the fragmentation maintenance. Also, it's possible it may require some statistics maintenance if the data profile is likely to change. It is possible that the index could improve overall system performance by reducing read access to the clustered index, but with a table changing so rapidly, the cost of maintaining the index might be very high. On the other hand, you have to look at the overall requirements. If this query only runs once a month, perhaps it is too high to bear. If it runs 10 times a hours, then running it in a few seconds, with inserts slightly slower might be a small price to pay. A thing to look out for would be index fragmentation if there are a lot of updates because the NCI key is not static. 

No matter what, somewhere there is going to be type selection and some kind of branching going on. If you look at how you would do this in your option 1, it's similar but with either a CASE statement or LEFT JOINs and a COALESCE. As you expand your option 2 with more things being linked, you have to keep adding more LEFT JOINs where things are typically NOT being found (an object that is linked can only have one derived table which is valid). I don't think there is anything fundamentally wrong with your option 2, and you could actually make it look like this proposal with a use of views. In your option 1, I have some difficulty seeing why you opted for the tri-link table. 

All work. Note that in the second case, you cannot add apples and oranges, and so the data is exceptionally easy to be subject to misinterpretation. Also note that conversions cannot be very safe and are susceptible to rounding error, overflows, etc. In addition, there are physical issues like the specific gravity and temperature. Converting 20 gallons of water to pounds would require you to know the density of water. But water's density changes with temperature, so you may need to either know the density contemporaneous to the measurement or the temperature similarly and use a volume correction factor. In the case of the Extended properties, that's only good for documentation - a good column name is better for documentation. The problem with the column implied as being in a fixed unit by name is that you end up putting yourself in a corner when you change measurement units - new client wants oil in barrels and not gallons - and that would be fine since their data is in its own database, but the column name is now misleading. Another option is to store canonical versions in fixed units (i.e. always kilograms and meters) in addition to the varying original measurements. Aggregate operations on the fixed units should be fine (except you wouldn't add temperatures, for instance), but you don't lose the original measurement. 

You can do it with multiple databases, but it will be more difficult to manage multiple schemas (rollouts, upgrades, etc) when there are changes. The single database design is a kind of multi-tenant (now that you have the right term, you should find a lot of material about these designs) and you would need to work on the design of these grouping structures. It's certainly possible to structure search across tenants very much more easily in a single database. In separate databases, you would effectively have to query across databases. This is possible in mysql, but isn't supported within the SQL language to pick up database names out of a table to do your joins - you'd have to generate dynamic SQL. So instead of simply: 

Seems to me that this could be naively refactored (without fooling with any other issues it has like the implicit joins and other garbage) simply as: 

Here is the log from a good run - the same proc, just slightly different data - there seems to be no error running the proc from Windows: 

The reasons you may not want to put extensive processing in your database depend a lot on what your scalability path looks like. It can make sense to put processing in the database instead of shipping a large amount of information out of the database that is simply discarded. But scalability methods can vary from scaling up to scaling out, and even scaling out mechanisms can vary. Ultimately the answer as to what to do with your architecture is going to depend upon your requirements and problem domain and expected future path, none of which you have given in your question. 

Obviously, you might get all the things at one time using your page system. We tended not to link things to the page because they were re-used a lot between pages (and not just in page-fragments or controls), but that's certainly fine. In the case of our Windows-native apps, we used reflection and a mapping file from control to translation tag so that translation did not require re-compiles (in pre-.NET apps we had to tag the controls using the Tag or other special properties). This is probably a little more problematic in PHP or ASP.NET MVC, but possible in ASP.NET where there is a full-blown server-side page model. For testing, you can obviously query to find missing translations very easily. To find places which need to be tagged, translate the entire phrase dictionary using pig-latin or Klingon or something like replace every non-space character with ? - the English should stand out and let you know that some naked plaintext has crept into your HTML. 

I get this error on Linux (Ubuntu and CentOS) when executing a stored proc from Perl/DBI/ODBC/FreeTDS to SQL Azure: 

I would simply use the views if they perform. No need to make copies of data unnecessarily, and many database platforms allow indexed or materialized views and filtered views etc. HOWEVER, MySQL does not support indexed views. So unless the underlying indexes on your table support the various ways you are accessing the data, it might be worthwhile materializing a version of the view yourself. 

Failing intermittently but more frequently. Package just calls a couple stored procs and exports the results to two Excel spreadsheets. Where to go from here: 

Is the primary key also the clustered index? If so, there is no reason to because the unique clustered index will already be used as the bookmark in the nonclustered index. As far as what you are talking about with the OR, that is effectively a second part of the WHERE, but you're just relying on the selectivity of the first to do most of the work (while relying on the INCLUDE to avoid going to the table). But by having * in there, you may have to go to the table anyway. On the other hand, I might not be designing my indexes based on a single query unless this query is very heavily used without taking into account more of the load. And still having a look at the execution plan couldn't hurt. 

You could make the view add a ROW_NUMBER with different ORDER BY for different users and then the outer select (or your reporting tool's sort setting) would always (explicitly) sort by that new ORDERBYTHIS column. i.e. make the view as: 

In addition to the rather odd (and possibly over-restrictive modeling of the phone number components), the "data" concerns me. It basically looks like the EAV pattern restricted to DECIMAL data type, which can have it's place, however there appears to be NO OTHER data modeled at all. When you finally get round to querying these things, it's a lot easier and involves less pivoting if you can just do things like or whatever it is these different types of data are that are attached to each user each week. 

3m rows is not really a lot to pull in daily, that can be just one daily feed in a data warehouse. And with a traditional DW extraction of just the new and changed rows, it wouldn't be a significant load to bring into a datawarehouse on a daily if not hourly basis. And a (dimensional) data warehouse schema is ideal for running a lot of ad hoc queries where data is handled different ways. I would definitely advocate for remodeling your data into a dimensional model and start bringing those disparate data sources into a data warehouse. You're already pulling from different servers and you're already expecting data to only be as of last day, so this is the perfect opportunity to standardize and start your data warehouse. 

For SQL Server, if performance is poor and is declining, the size of the database is really only an indirect cause. The fact is, all databases can perform reasonably when they are small, but relational databases are typically designed to perform well with large amounts of data through indexing. As the size of the data grows, the right choice of indexing minimizes the performance effects. Profiling your application to know which queries are slow and then reviewing the execution plans of the poorest performing queries should be the first step. It's possible that the queries are written poorly or that the database design is poor or that the application/system design is poor. But I would start to see if there are just queries which can simply be a lot faster with better indexes. 

That one X locks the row in the CI (on CREATED column) and then attempts to X lock on the NCI which includes the status column. 

Which of course doesn't work in Oracle (that syntax - apart from the INTO and the optional AS) would be fine in SQL Server $URL$ Is there an alternative to this (which requires me to put the aliasing inside) where the column aliasing is outside: 

Is there a simpler T-SQL construct for "all of these columns to be equal (NULLs ignored) on a row" - effective I want to say: 

It appears that the system does an INSERT into the Order table and then an INSERT into the OrderCharge table. If you want to capture information about the charges, you will need to do it after the INSERT in the OrderCharge table. 

See how the DIM_Folder usage makes the set of dim ids small and then, we're assuming some kind of index on snapshot date and then folder dim id (or vice versa). See how you also now don't need to join on folder at all if you just want the data at a higher level. Since you usually know all this at ETL time, there is a different motivation than in OLTP systems where you want everything to move together when something is changed (leg bone connected to the thigh bone, etc.). In DW scenario, you really don't want anything to move. So, bam! - total Farm usage analysis: 

This would obviously simply be used in your code as or similar Which helped the translator a bit. But .NET String.FOrmat doesn't support commenting within the format string, unfortunately. As you say, you would not want to handle that in your php with locale awareness or meta phrases. So what we had was a master phrase table: phraseid, english, supplemental info and a localized table: phraseid, localeid, translation You've also assumed with INNER JOINS that the localized versions exist - we tended to leave them out until they were translated, so that query of yours would end up returning nothing at first (not even the default) If a translation didn't exist, ours defaulted to English, then fellback to code-provided (in case the database didn't have the ID, and it was also clear from the code what phrase identifier "TXT_LNG_WRNNG_INV_LOW" was actually trying to get) - so the equivalent of this query is what we used: 

It seems like this is a symmetric relationship, however, in usage a symmetric relationship becomes a little problematic, since the table has to be joined both ways (effectively a UNION or OR) and indexed both ways. Sometimes it can be easier to have only asymmetric relationships and enforce that both exist with a trigger or something. 

I don't get this error running the same Perl script on Windows (ODBC/Native) connecting to the same SQL Azure database, and although the proc takes a couple minutes to run, it does complete. Obviously a completely different client/driver environment, but they are both going to the same SQL Azure database and there is no difference in the Perl script itself. Both Perl environments should be similar but obviously the client/drivers will be different. The line which fails is simple - a proc with no parameters which takes data from a staging table and processes it - at the time of failure from Linux call, the environment on SQL Azure is not significantly different than when I test called from Windows (it is completely reproducible when the staging table is loaded with certain data and invoked from Linux): 

"This means that the query engine must take an additional step in order to locate the actual data." Not necessarily - if the index is covering for a given query, no trip has to be made to the data pages. Also, with included columns, additional columns can be added to a non-clustered index to make it covering without altering the key size. So the ultimate answer is - It Depends (on a lot more information than you can really cover in a single question) - you need to understand all the capabilities of the indexes and the execution plan for a given query may diverge from your expectations. A general rule of thumb I have is that a table always has a clustered index (and usually on an identity or sequential GUID), but non-clustered indexes are added for performance. But there are always exceptions - heap tables have a place, wider clustered indexes have a place. Seemingly redundant indexes which are narrower to fit more rows per page have a place. etc. etc. And I wouldn't worry about the limits on the various indexes allowed - that's almost certainly not going to come into play in many real-world examples. 

Yes, you can make more and more database servers and you can also make a larger and larger database server (but the scale up limit can be reached very quickly), but in the scale out scenario are the servers you mention supposed to cooperate in any way or are they completely independent. The CAP theorem doesn't just apply to "database" servers, but since web servers aren't really considered as a "write" part of a distributed data store, they are usually considered to be easily scalable by themselves: $URL$ The fundamental problem for scalability in database is in the consistency part of the CAP theorem - all those ACID guarantees that databases have.