I was messing with glowing lighting when I noticed an odd artifact in my first try. For some reason I can see three defined edges inside of the glowing pattern. I downloaded the image and marked where I see the edges. 

I am running this fragment shader on every pixel on screen. I am trying to make it as efficient as possible. BTW I am running open gl es 2.0. The code segment below is only a sample from the code, there are about 56 different calls to Gaussian() spread across different functions. I am wondering wether it would be valuable to replace the calls to Gaussian() with there appropriate resulting float value. I know a lot of times stuff like this is pre-calculated on compilation of the shader, or calculated only once because the gpu realizes it is the same calculation for all fragments. So would it be worthwhile for me to manually calculate each of these and replace them with their values? 

At least on iOS every frame you are likely to get a completely new texture that you need to draw to due to the OS switching between a couple textures each frame. My question is essentially if the OS feels the need to use multiple textures should we also be using multiple textures for some reason when doing full screen post processing? Is there ever a time where switching between off screen textures can improve performance? 

Although I am not sure it is physically accurate to have those spikes on the edges of the diamond I sure do like the effect that creates. 

So I have a batch draw call I am doing with various squares. I have a 4x4 tile map with the numbers 1-16 going in right/down order. When it gets to the fragment shader their is a varying float "id" that holds the number. In a perfect world if the id was 0, it would sample the top left and display "0", if the id was 4 it would sample the top right and display "4". Thankfully this is mostly working! However the numbers 5, 9, and 13 (which happen to be on the left of the tile map flicker! The values on these squares just change frequently. I have traced it down to being the fault of the sample location. And probably this function here: The goal is to take the id and return the proper row and column to texture map. 

We want to get $pMin$ and $pMax$. $pMin$ should be the lowest value for all the components. $pMax$ should be the highest value for all the components. That would make calculations a lot easier of course. When looking at the graph we can easily find $pMin$ and $pMax$. 

We see three vertices, separated by a white space. Each vertex has two numbers. The first number is the index for the position. The second number is the index for the normal. They are both different. Wavefront does not need to care about caching. It just needs to store the data by using the least amount of memory (if we load everything in to memory. Saving it in plain text is of course not efficient). When looking at your .obj file, we can see that we have 8 positions, but only 6 normals. We have been able to remove 6 floating point numbers in total (2 normals consisting of 3 floats). Wavefront specifies different indices for each vertex attribute. For a vertex, the position could be number 5, but the normal is number 4. Conclusion Wavefront and OpenGL use a different way of storing the data. This means that when parsing your wavefront file, you need to re-index the data to what OpenGL wants. What happened with you, is that OpenGL thought that the indices for the normals were actually indices for positions and that you had twice as many faces. What I would do, is to save all that wavefront data to temporary lists. You create your main vertex buffers. You go through every vertex of every face. You look at the data of that vertex. Then you see if you already have that data combined in your buffers. If so, you use the index of that data, else you add the new data to your buffers and add that index. 

I realized that the problem was that I was sending the instance id to the fragment shader via a varying, somehow that was causing issues even though each vertex should have had the same id. 

Case #3 proving that texture drawing works by drawing to front buffer (I put a texture into a fbo and never drew to the fbo) 

I am trying to recreate a filter that Facebook released back in 2015 when gay marriage was legalized in the US. Unfortunately, that filter no longer works/exists so people are left with only knockoff ones that dont do quite as good as a job. None of them manage to do quite what the actual one did espeically in the red, yellow, and purple stripes. I am trying to figure out what kind of filter they used to achieve this. I have tried a bunch of colors with each of the blend modes available in my photo editing software with no luck. Perhaps it is more complicated than just one math equation? Perhaps they processed the background image before putting on the overlay? I did find in the blog that the algorithm to achieve this was a O(n^2) algorithm but that is about all I know. Any ideas of what could have caused this? Below are some reference photos I used to attempt to find a pattern. Example 1: Mark Zuckerburg (note this isn't exact. I couldn't perfectly recreate the crop they used) 

My goal is to take a point that is inside of a circle with a given radius and put it on the circumference. Recently I have been normalizing the vector between the point and the center of the circle then multiplying that by the radius. However I need (if possible) a less computationally expensive method because the distance formula is expensive. Another thought I had was to use "atan2" to get the angle between the two points and then use sine and cosine multiplied by the radius to get the point on the circumference. Which method do you think would be faster for the computer to process? Can you think of a faster method. Details about the simulation This is an ios application written in swift. Basically there are a bunch of particles moving around randomly. And the user is putting down fingers. Each finger is a circle with a radius that grows as time goes on. The part that is inneficent is that if the dot is ever inside of any of the circles (attached to touchscreen touches) that it goes on the circumference of the circle. 

So you could say that a vector is a direction with scale, and a point is a location. So, if you transform a vector you just rotate and scale it. With a point you also translate it (the rotation and scaling of a point is around the origin, since it iss just a location the point itself cannot be rotated). Most of the times a vector and a point are put into the same container, a vector with 4 components. The only difference is the w component. If the w component is 0, then it is a direction. If it is 1 then the vector is a point. The reason for this can be found in the matrix itself. It makes use of the way you multiply a vector with 4 components with a 4x4 matrix. If you do not know how that works, I would suggest a quick google. Most of the times you use a 4x4 matrix. A normal transformation matrix could look like this: \begin{bmatrix}rot+scale&rot+scale&rot+scale&translation\\rot+scale&rot+scale&rot+scale&translation\\rot+scale&rot+scale&rot+scale&translation\\0&0&0&1\end{bmatrix} (The rotation and scale are put in the 3x3 area you could say, so for just rotation and scale a 3x3 matrix could also be used, but when translation comes in, we need that 4th column.) As you can see, if the last component is 0, then you have a multiplication with 0 and therefore the result is 0 and there is no translation. This makes it easy in computer graphics with polygonal objects. You have the same transformation matrix to transform the positions but also the normals. Because the normals have their w component set to 0 and the positions' w component is 1, the normals are just rotated (and also scaled which can lead to some weird stuff, so most of the times the normal is normalized after. It isn't actually recommended to use the same matrix for positions and rotations because of the weird stuff! Look at @JarkkoL 's comment.) and the positions are translated (and rotated and scaled around the origin). Hope I did not make a mistake :P, and this helped you! 

I am making a 2d game in opengl es 2.0 Inside are tons of rectangles defined by 4 points and one 4 component color. I am using vertex buffer objects, and I have heard that it is efficent to interlace the data. So like traditionally you would do 

Right now I am just taking the oscillating float time and passing it in directly, but eventually I will put it on a function so it fades in, temporarily is extra bright, then goes to the source texture. Edit: I solved my problem, to my surprise the GLSL log function is in base e rather then 10. 

The thing is these capsules are all individual, sometimes every capsule will be elongated, sometimes it will only be a small percent. This shader needs to be as steady fps as possible. Now here are some tips that are useful and might help make things more efficent: 

I have been working on how the GPU does parellel processing, and branching. However I am not yet to the point where I know how to make this shader more efficent. Essentially I dont know enough about how it works. Anyway I have the following shader: 

I have a game with similar unprocessed graphics to his. I have been trying to get a glow effect like this working but it has been near impossible. I am currently using separable convolution gaussian blur methods in addition to downscaling my blur mask. Still, it barely runs in real-time. It is nowhere near as big as a radius as this app pulls off and it certainly isn't as good of quality. Is there some method I am not thinking of? I am open to the possibility (but doubtful) that they are using really large textures with a falloff distance or that lighting is computed from the distance from a fragment to light sources. If you play with it and notice when the FPS drops it it just doesn't seem to have the right FPS to particle amount and HDR on/off quality to it to be that. 

Short answer, set the precision of the image to a higher value. Long answer, When looking at a gamma correction curve, you can see that the lower values get changed much more, this means that the difference between lower values will get greater and that causes this effect. You have a limited amount of values for a color channel and this means that when it rounds the number down to the 8 bit value, it makes some neighboring pixels the same color. You don't notice this in a normal image, but after gamma correcting it this effect comes out, because you lose that bit of change in the color that later on can become a much bigger difference in color. To fix this, you just give the gamma correction more precision to also get that small color change it would've else rounded away. In the actual shader, it has floating point color information and that means that it has more precision and thus not the weird artifact. So when making the frame buffer, you can use for example GL_RGBA16F or GL_RGBA32F. This would increase the precision and remove the artifact. I hope that this can help you fix the problem, good luck and have a great day! 

Generally when wanting the smallest angle between two vectors, the dot product is used. $$ \vec A \cdot{} \vec B = \cos{\angle \alpha} $$ Where $\vec A$ and $\vec B$ are vectors with the length of $1.0$. However, you do not have access to the actual vectors themselves, if I understand your question correctly. You just have access to the angles between the vectors and a common direction. There is a different way you could calculate that angle. You take the absolute value of the difference between the two angles, given that both angles are from a common direction and either both clock-wise or both counter clock-wise. $$ \angle \phi = |\angle \alpha - \angle \beta| $$ Where