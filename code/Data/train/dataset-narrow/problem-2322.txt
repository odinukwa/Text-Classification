What is the story behind the unusual author ordering in these papers? Are there any other examples of major TCS papers in which the order of the authors is not alphabetical? 

Oded Goldreich's In a World of P=BPP is one of the best written papers that I read. This is mostly due to the clarity of the exposition, the conceptual perspective, and the choice to include reflections regarding the meaning of the results in the paper. 

In the adjacency matrix model, there is a lower bound of $\Omega(n)$ on the query complexity of testing whether an $n$ vertex graph consists of two isomorphic copies of some $n/2$-vertex graph (see Introduction to testing graph properties - Goldreich for a survey). Also, there are many lower bounds that depend on $n$ for testers with one-sided error, e.g.: testing $\rho$-Clique,$\rho$-Cut, and $\rho$-Bisection (see Property testing and its connection to learning and approximation - Goldreich, Goldwasser, Ron) Moreover, in the bounded degree graph model, testing 3-Colorability requires $\Omega(n)$ queries, whereas testing 2-Colorability (i.e., Bipartiteness) requires $\Omega(\sqrt n)$ (see Property testing in bounded degree graphs - Goldreich, Ron). 

Are there any known constructions of binary locally testable codes with very low (e.g., independent of the length of the codeword) query complexity and "good" rate (e.g., mapping strings of length $k$ to strings of length $k^{1+c}$, for a small constant $c$) that are also locally decodable (even if the query complexity for decoding is very large (but still sublinear))? 

This problem is NP-hard. Reduction from PARTITION: Given a set of numbers $S=\{x_1,\ldots,x_n\}$, construct the following flow network: $$V = \{s,v,t\}\cup \{x_1,\ldots,x_n\}$$ $$E = \{(s,x_i) | x_i\in S\} \cup \{(x_i,v)|x_i\in S\} \cup \{(v,t)\}$$ $$c((s,x_i))=c((x_i,v))=x_i\ \ \ c((v,t))=\frac{\sum_{i\in[n]}{x_i}}{2}$$ $S$ is partitionable iff there exist "saturate edge or avoid" flow of value $c((v,t))$. 

The algorithm by Orlin you mention is quite old, and max flow algorithms came a long way since; In 1994, King et al. showed that max flow can be solved in $O(n\cdot m)$ whenever $m=\omega(n^{1+\epsilon})$ for some $\epsilon > 0$. This result was lately complemented by Orlin himself, which showed $O(nm)$ time complexity extends for $m=O(n^{\frac{16}{15}})$ (and also gave slight improvement for the $m=O(n)$ case). If you are interested in more practical algorithms you can look at the approximation algorithms for max flow, (see Mądry's paper for example), which is able to produce $O(\log n)$ approximation in near linear time. If you are also interested in single-unit capacities, Mądry recently gave an (exact) $\widetilde O(m^{\frac{10}{7}})$ for maximum cardinality bipartite matching. 

This problem is probably known under some other name, if anyone has seen it before, a reference will be great. Given $n,m,k$ (for $m,k\ll n$), a $(n,m,k)$ separating set is a set of $n$-sized binary vectors $V$ such that for every disjoint $S,S'\subset \{1,\ldots,n\}$, $|S|=m,|S'|=k$ there exists $v\in V: v_{|S}=0, v_{|S'}=1$. That is, for every set of $m$ indices $S$ and a non-overlapping set of $k$ indices $S'$, there should be a vector whose $S$ entries are all zeros and his $S'$ entries are all ones. The goal is to construct a small set $V$ with the above properties. 

An $\mathcal{MA}$ communication complexity protocol is communication complexity protocol that starts with an omniscient prover that sends a proof (that depends on the the specific input of the players, but not on their random bits) to both players. The players then communicate with each other, in order to verify the proof (for more details, see: On Arthur Merlin Games in Communication Complexity, by Hartmut Klauck). The are quite a few lower bounds (e.g., On the power of quantum proof, by Ran Raz and Amir Shplika) of the following form: Suppose we have a communication complexity problem $\mathcal{P}$ with a tight bound of $\Theta(T(n))$ on its communication complexity (for some function $T$). There exists a lower bound that shows that every $\mathcal{MA}$ communication complexity protocol that communicates $c$ bits and uses a proof of size $p$, must satisfy $c \cdot p = \Omega(T(n))$. So one can think of it as a tradeoff between the work that prover has to do, and the work that the verifiers have to do. Moreover, it seems that for every communication complexity problem that I know of (with a tight bound of $\Theta(T(n))$ on its communication complexity), there exists a protocol wherein the prover sends a proof of size $\tilde O(T(n))$, and the verifiers only uses $\tilde O(1)$ bits of communication (cf. the two papers I mentioned above). Thus, in a sense, all of the work has been delegated to the prover (achieving the extreme case of the aforementioned lower bounds). Is there a result that shows that a verifier-"heavy" protocol implies the existence of a prover-"heavy" protocol? Is there a counter example? What about other models (such as $\mathcal{MA}$ decision trees/query complexity) wherein our understanding of the behaviour of $\mathcal{MA}$ protocols is deeper? 

Monadic First Order Logic, also known as the Monadic Class of the Decision Problem, is where all predicates take one argument. It was shown to be decidable by Ackermann, and is NEXPTIME-complete. However, problems like SAT and SMT have fast algorithms for solving them, despite the theoretical bounds. I'm wondering, is there research analogous to SAT/SMT for monadic first order logic? What is the "state of the art" in this case, and are there algorithms which are efficient in practice, despite hitting the theoretical limits in the worst case? 

In general, we look at fixed-points of monotone functions over lattices, i.e. with some partial ordering over your elements. If your lattice is complete (it has a least and greatest element, called a bottom $(\bot)$ and a top $(\top)$), and the function whose fixed-point you're trying to find is monotone, then the Knaster-Tarski Theorem says that a fixed-point always exists. However, finding this fixed-point may be undecidable. For example, in Hoare logic, there is a function whose fixed-point is the weakest-precondition of a loop. But there's no guarantee that this fixed-point can be found, since finding weakest loop invariants is undecidable. The basic algorithm, which might not halt, is as follows: 

I'm trying to solve a particular problem, and I thought I might be able to solve it using automata theory. I'm wondering, what models of automata have containment decidable in polynomial time? i.e. if you have machines $M_1, M_2$ you can test if $L(M_1) \subseteq L(M_2)$ efficiently. The obvious ones that come to mind are DFAs and reversal-bounded counter machines where the number of counters is fixed (see this paper). What other notable classes can be added to this list? The more powerful the automata, the better. For example, DFA's aren't enough to solve my problem, and the counter machines can't do it with a fixed number of counters. (Naturally, if you get too powerful, then containment is either intractible, like for NFA's, or undecidable, for CFG's). 

Let $d,q \in \mathbb{N}$, and let $f:\mathrm{GF}(q) \to \mathrm{GF}(q)$ be a univariate polynomial. In this case, it is possible to test whether $f$ is of degree at most $d$ (or whether $f$ is at least $\epsilon$ from it) using the algorithm of (RS96). The testing algorithm works by simply trying to interpolate the function $f$ on $\Theta(1/\epsilon)$ collections of $d + 2$ uniformly selected points, and checking whether the resulting functions are all polynomial of degree at most $d$. Is there an extension of this result that allows to test whether a bivariate polynomial $g:\mathrm{GF}(q)^2 \to \mathrm{GF}(q)$ is of degree at most $d$ in the first variable, using $O(\epsilon^{-1} \cdot d)$ queries? 

There are many applications of real analysis in theoretical computer science, covering property testing, communication complexity, PAC learning, and many other fields of research. However, I can't think of any result in TCS that relies on complex analysis (outside of quantum computing, where complex numbers are intrinsic in the model). Does anyone has an example of a classical TCS result that uses complex analysis? 

We say that a Boolean function $f: \{0,1\}^n \to \{0,1\}$ is a $k$-junta if $f$ has at most $k$ influencing variables. Let $f: \{0,1\}^n \to \{0,1\}$ be a $2k$-junta. Denote the variables of $f$ by $x_1, x_2, \ldots, x_n$. Fix $$S_1 = \left\{ x_1, x_2, \ldots, x_{\frac{n}{2}} \right\},\quad S_2 = \left\{ x_{\frac{n}{2} + 1}, x_{\frac{n}{2} + 2}, \ldots, x_n \right\}.$$ Clearly, there exists $S \in \{S_1, S_2\}$ such that $S$ contains at least $k$ of the influencing variables of $f$. Now let $\epsilon > 0$, and assume that $f: \{0,1\}^n \to \{0,1\}$ is $\epsilon$-far from every $2k$-junta (i.e., one has to change a fraction of at least $\epsilon$ of the values of $f$ in order to make it a $2k$-junta). Can we make a "robust" version of the statement above? That is, is there a universal constant $c$, and a set $S \in \{S_1, S_2\}$ such that $f$ is $\frac{\epsilon}{c}$-far from every function that contains at most $k$ influencing variables in $S$? Note: In the original formulation of the question, $c$ was fixed as $2$. Neal's example shows that such value of $c$ does not suffice. However, since in property testing we are usually not too concerned with constants, I relaxed the condition a bit. 

In the experts problem, $n$ experts give you binary predictions on a daily basis, and you have to predict whether it's going to rain tomorrow. That is, at day $t$, you know the past predictions of the experts, the actual weather for days $1,2,\ldots t$, and the predictions for tomorrows, and have to predict whether it will rain the next day. In the classic Weighted Majority algorithm, the algorithm makes $O(\log n + m)$ mistakes, where $m$ is the number of mistakes of the best expert. To me, this seems like an extremely weak promise, as it does not allow any benefit from combining predictions of several experts. Assume that each outcome is $\{\pm 1\}$, prediction of expert $i$ on day $t$ is $p_{i,t}$, and the outcome of day $t$ is $o_t$. We can define an ``optimal weighted majority'' adversary as an optimal weight function $w\in\Delta([n])$, such that the decision made by the adversary on day $t$ is defined as $sign(w\cdot p_t)$, i.e. the weighted majority of the predictions, with respect to the vector $w$. Using this notation, the previous adversary (best expert) could only pick unit vectors. We can then define the optimal error for days $1,2,\ldots T$ as: $$E = \frac{1}{2}\min_{w\in\Delta([n])} \sum_{t=1}^T|sign(w\cdot p_t)-o_t|$$ 

I had some hard times trying to formulate the question, so I'll start with some examples: Suppose you are given a Dominating Set instance, $<G,k>$. Now suppose I give you a set of vertices $D$ of size $k$. Deciding whether $D$ is a dominating set of $G$ requires linear time in the size of $G$, and doesn't seem to be possible only by looking at the vertices of $D$. In contrast, if we have a $k-path$ instance $<G,k>$ (asking whether a simple path of length $k$ exists in $G$), and I give you a tuple $P$ of $k$ vertices, you can verify that $P$ is a k-path by reading merely $k$ bits of the adjacency table. 

Tree Automata can be used to model sets of values of a Herbrand Universe, for example, to model possible values in a functional program. Systems of subtype constraints over set expressions have decidable satisfiability, but this is EXPTIME complete, and NEXPTIME complete when projections are allowed in expressions. See here and here. I'm wondering, has the set constraints problem over equality constraints, instead of subset constraints, been studied? Clearly it's not a harder problem than with subsets, but is it easier? Are there simpler solutions than tree Automata for solving such systems? 

However, you can always find a fixed-point using iteration if you have the Ascending Chain Condition, which basically means that there are no infinite increasing chains in your partial order. When you have this, and your function is monotone, the above algorithm will always halt. If you have some other "upper bound" on a solution size, then you can use it to ensure the above algorithm halts. In your case, where you have multiple equations, you just make your lattice contain vectors, and find a fixed-point of a multi-dimensional function. 

As a counter-example to this, consider the Context-Free Equivalence problem: it's undecidable to determine, given two context free languages, whether they accept the same set. If your problem were decidable, we could use it to determine CFL equivalence, since it's always possible to turn a CFL into an always-halting Turing machine. So even for countably infinite inputs, the problem is undecidable. It's also worth mentioning that, for standard Turing Machines, all input domains are countably infinite, since they're sets of finite strings. 

This problem can be viewed as a generalization of summing over sliding windows in streaming data (e.g., see [1] and [2]). 

Any algorithm would need $\Omega(\log n)$ queries. To see this, define $f(k)$ to be the number of queries needed for deciding whether an element $x$ appears at least $a$ times in a sorted array $A$. We assume that $x$ appears in $A[m],A[m+1],\dots,A[M]$, and that $k\triangleq\min\{a, m-1, n-M\}$. Notice that in these notations we are looking to bound $f(\lfloor n/2 \rfloor)$. Claim: $f(k)\ge \log k$. Proof: Consider the first query made by the algorithm. If it was done within radius $k/2$ of the interval (i.e. somewhere in $[m-k/2,\ldots,M+k/2]$), and found the median at the spot, then we still need at least $f(k/2)$ queries to decide the problem. If it was done outside that radius, consider the case where the queried cell did not contain the median. Once again, this leaves us with at least $f(k/2)$ queries to be made. Continue with induction and you get the $\Omega(\log n)$ bound. 

I'm well aware we could probably save some memory by hashing (if $k << \mathcal{U}$), but I'm looking for a deterministic structure. 

This problem is known as Weighted Set Packing and it is NP-complete. In order to see this, assign each customer a set with weight which equals the sum of the item values he asks for. The best known approximation algorithm for this problem gives a $\sqrt |\cal U|$-approximation for the optimal solution. 

$NCM$, the class of non-deterministic reversal-bounded counter machines, has a lot of interesting dependability and closure properties. It's known that, unlike the deterministic version, NCM is not closed under complement. However, I've never seen a proof of this given explicitly. In Ibarra's paper, the non-closure is implied because if NCM were closed under complement, subset would be decidable. However, I've never actually seen an example of a language where $L$ is in $NCM$ but $\overline{L}$ isn't. I'm wondering, can anyone provide such an example, and preferable, a source describing it that I could cite in a paper? 

Context I realize that subtyping often doesn't admit principle types, and that inference in the presence of subtypes is undecidable. I'm working in a context where typechecking should simply fail there is not a single most general solution, and where annotations can be added to aid inference. Suppose that we have a type system where, during typechecking, some types need to be inferred using unification, and may be type variables $\alpha, \beta$ etc. If, at some point, the constraint $\alpha <: T_1 \to T_2$, where $\to$ is the usual function type constructor, then we can decompose this into $\alpha = \alpha_1 \to \alpha_2$, with $T_1 <: \alpha_1$ and $\alpha_2 <: T_2$. The Problem The problem arises if you have constraints $\alpha_2 <: \beta_1 \to \beta_2$, $\beta_2 <: \alpha_1 \to \alpha_2$. In such a case, we get: $\alpha_2 = \alpha_{21} \to \alpha_{22}, \beta_1 <: \alpha_{21}, \alpha_{22} <: \beta_2 $ from the first constraint, and $\beta_2 = \beta_{21} \to \beta_{22}, \alpha_2 <: \beta_{21}, \beta_{22} <: \alpha_2$ from our second problem. If we substitute from our equalities, we then get (among other constraints), $\alpha_{22} <: \beta_{21} \to \beta_{22}$ and $\beta_{22} <: \alpha_{21} \to \alpha_{22}$, which is identical to the form of our original problem. Clearly if we continue solving this way, we will never terminate. When going to write a proof of termination, the problem is that substitution decreases the number of unsolved variables but increases the structural-size of the problems, and solving subtyping decreases the structural size of the problems, but increases the number of unsolved variables, so they don't work in a well-founded ordering. My Solution Attempt With my definition of subtyping, if $\alpha_2 <: \beta_1 \to \beta_2$, $\beta_2 <: \alpha_1 \to \alpha_2$ has no solution. The problem seems to be the "cycle", so if we do a sort of occurs check and fail when cycles are detected (or turn them into equality cycles, which will fail except for $\alpha <: \beta \wedge \beta <: \alpha$). I'm not exactly certain how to formalize a cycle like this, and how such a check would give me something that I can use in a proof of termination. My Question My work is about the specifics of a particular subtyping system, but the problem here seems very general, so I'm wondering if this is a known problem, if there are known solutions to it, and if there is research that either formalizes or disproves my intuition about cycles. What kind of cycles in subtyping constraints do I need to eliminate to avoid this infinite looping? Or is this a deeper problem with no solution? 

This is an upper bound on $N$ for the second formulation: Assume that I pick $N$ uniformly random sets. The probability of a pair of elements not to be covered by a specific set is $$1-\left(\frac{a}{L}\right)^2$$ Therefore the chance that it is not covered by any set is: $$\left(1-\left(\frac{a}{L}\right)^2\right)^N$$ Using the union bound, the chance that some pair will not be covered is at most: $$P={L \choose 2}\left(1-\left(\frac{a}{L}\right)^2\right)^N$$ This means that if $P<1$, there exists some proper covering using $N$ sets. Notice that: $$P={L \choose 2}\left(1-\left(\frac{a}{L}\right)^2\right)^N<L^2e^{-N(\frac{a}{L})^2}$$ Therefore it is enough to demand $L^2e^{-N(\frac{a}{L})^2}<1$, which gives us a bound on $N$: \begin{align} L^2e^{-N(\frac{a}{L})^2}<1\\ -N(\frac{a}{L})^2<-2\ln L\\ N>2\ln L\cdot(\frac{L}{a})^2 \end{align} Which means that there exists a proper covering using $2\ln L\cdot(\frac{L}{a})^2$ sets of size $a$. Now notice that every set covers $a\choose 2$ pair of elements, while $L\choose 2$ pairs exist, so a lower bound for $N$ would be: $$\frac{L\choose 2}{a\choose 2}\approx (\frac{L}{a})^2$$ So the upper bound is tight up to factor $O(\log L)$. (If I had to bet, I'd say the lower bound is loose while the upper is asymptotically tight). This means that if your goal is to compute the value of the optimal $N$ - answering $(\frac{L}{a})^2\sqrt{\log L}$ would give an approximation ratio of $O(\sqrt{\log L})$. 

You are given a $n$-sized binary array. I want to show that no algorithm can do the following (or to be surprised and find out that such algorithms exist after all): 1) Pre-process the input array using unlimited time, but only using $O(n)$ bits. 2) Answer queries in constant time, where query $(x,y)$ asks for the number of set bits between index $x$ and index $y$ in the array. It seems that constant time per query should not allow the algorithm to read enough information to compute the number of set bits.