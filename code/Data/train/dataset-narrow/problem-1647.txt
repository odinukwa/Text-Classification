I am looking at creating backups of our C: drive in case we have a major HD or OS failure... This is a web server, so I want the fastest way of restoring IIS with all the settings and all the program files (SQL Server etc.) and read the built in 'Windows Server 2008 Backup' is what I should be using. So I basically want the C drive backed up so I can restore it to how it was before a failure... 

I run a little 1&1 web server with quite a few sites all powered by SQL Express Db's and have never allowed connections to SQL Express from an external resource (Anything not on the server), as I am paranoid of the databases getting hacked if I allow external access. What is the most secure way for me to allow a local program on my computer here to connect to my SQL Server on my 1&1 server? A step by step guide would be excellent :| 

But I'd like to know what else people do for precaution, for example can you backup the entire IIS and all its settings? As re-creating all the sites and application pools etc... would be a real time consuming pain if the worst were to happen? Any tips / advice greatly appreciated 

And restarted IIS, but nothing changed. It's still blocking Dots in paths. And then I restarted the entire server... And guess what. It's still blocking dots in path? I have no idea what to do next, apart from un-install it? Anyone else this issue? 

I have an EC2 server (Win2008) which has about 10 sites on it, it's completely locked down apart from ports 80 and 443. I can only access port 3389 and 21 from my home and work IP addresses. Just now I received the following from Amazon: 

For all the databases on the server. Its doing this more or less every hour? Is that right? I assumed once the database had started thats it unless I restart the SQL service or shut down the server? Any advice appreciated - Thanks. 

Does anyone know if it's possible to automate the creation of EBS snapshots from the online EC2 Management console (Say every day or every hour) and then tell it how long to keep them (Pruning them)? I don't use the command line tools, so would hope there is an option (That I can't currently see!) to automate snapshots? I know you can do it via third party applications and sites like Ylastic - But was hoping this would be built in by now? 

I have been doing an audit of my SQL Server log, as I 'thought' it was locked down but appeared to be open and we were getting a lot of password attempts on the SA account. After sorting that, I have noticed throughout the day in the ErrorLog I have entries like this 

I have a web server that I run which has quite a few sites on it now all using SQL Db's - Its actually a cloud server from 1&1 (Win2008). I have the following in place in case of the worst: 

We noticed one of the drive lights was not lit at all, and thought this may have failed and be the problem. We replaced the drive with a spare, and tried "F" to repair it again, but we keep just getting the same error as above. In the RAID configuration utility, all drives show as "online" and "optimal". We do have this data on another replicated server, so we're not worried about "recovering" anything, we just want to get the system back online asap. The server has 64 or 32GB memory, can't remember off the top of my head, but either way, with a 14TB RAID, I think it may still not be enough. Thanks EDIT - I checked the memory usage while fsck was running as suggested and after 2 or 3 minutes, it looked like this, using up nearly all of our servers memory: 

All the commands etc work as expected but we need to be able to access the OpenManage web interface, however, it isn't starting up for some reason. 

I previously had cURL 7.22.0 on Ubuntu 12.04 Server.. but I now need to upgrade to cURL 7.30.0. I've done the following to compile this version for Ubuntu: 

I'm trying to import a large 70GB+ database (all ) using . This is a development system on Windows using WAMP server. . I'm getting the following error: 

I usually use MySQL Workbench to do the import but I wanted to use the parameter and I don't think does this. What's wrong with this syntax? I also tried: 

Turns out this was down to the domain being different on the NAS and the server! on the ReadyNAS came back blank, and on the Ubuntu server came back . I changed it with: and updated: to Rebooted with: and it all started working as expected (think I possibly remounted the share as well, but not 100%)! 

I've called Dell twice to try and clarify this basic (or so I thought) question, but I've had two different answers so far. We have a Dell R710, and a Dell R720. Does DRAC come installed by default on either? Dell said it comes build into the motherboard on the R720 but the second time I rang, they said we didn't have it. Can anyone tell me if DRAC comes as standard on the R720? I'm aware we'll need a license as well, I'm just interested in the physical capability on the server. I've never used DRAC before and know next to nothing about it, so apologies if this seems blindingly simple. Thanks 

After doing all that I ran expecting to see the new version installed. cURL had updated to 7.30.0 as expected, but libcurl hadn't: 

However, if I run I get which is correct. Can anyone explain why there's a difference in version numbers? And how to get them all showing the correct 7.30.0? Does anyone have any tutorials / advice / any help at all on the proper way of upgrading everything cURL related to a later version. The topic seems to be incredibly lacking online, not sure why :/ Thanks Edit - Following one of the comments, here's some additional info: gives gives gives gives gives 

Simple. It doesn't have to be complicated technology that is designed for large clusters. I only have a small amount of machines, say 10-20. Data are replicated and stored on all machines. The VPS are not close to each other, so the network connection is slow, therefore all data should be replicated and locally stored on all machines. Decentralized. Machines can be down or removed at anytime due to I forgot to pay timely. So machines shouldn't need to keep connection to a centralized server, but talk to each other. Of couse, when a machine is first setup, it knows a list of machines, but after that they should just talk to each other and maintain a list of available hosts. My desktop doesn't have a fixed external IP so the desktop should just be used to push out data, not to be fetched by those VPS servers. When I publish new data from my desktop, it should just send out roughly one copy of data and after that machine should fetch from each other. (Optional feature but strongly prefered to have) the data directory appears as a vfs and mounted to certain directory. This makes it a lot easier to use. 

One thing I considered was some fuse-based file system backed by git. There are a bunch of git-fs on github or code.google.com but none of them looks mature and reliable. Also, on each machine it still needs to maintain a list of available machines to talk to, which is not automatic and therefore painful. Another thing is to use some sort of P2P file sharing software, but is there any that makes files organized? For example, when I publish the files, I should also be able to specify which directories the files should be placed. This should be done automatically, not I login to each machine and manually move them after P2P downloaded the file. For most distributed files systems, they are designed for clusters where machines are close to each other. For HDFS it requires centralized name node, so I can't use it as well. Any thoughts? Thanks. 

Hi I am running some latency sensitive program on a Linux machine (more specifically, CentOS 6), and I don't want the threads of the process being preempted. So in my plan, the first step is to set cpu affinity of the threads so that threads are running on separate cores, so they don't preempt each other. Then the second step is to make sure other processes in the system not running on these cores. So my question is: is it possible to restrict the whole system running on certain cores, except this process? This should apply to any newly created processes in the future. 

I wonder if someone could recommend me some solution to sharing files between a small amount of linux VPS (virtual private servers). Basically I have a bunch of linux VPS to manage, and I want to share files between them. The typical use cases are to share data files, precompiled libraries or binaries, etc, therefore the write frequency is very low, most of which are just adding new files instead of modifying existing ones. Usually I use my own desktop machine to generate the data and build the binaries, and publish to the machines. The requirements are: