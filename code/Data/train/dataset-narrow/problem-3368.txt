Obviously now findBlobs takes up an even greater proportion of the loop's time but at least we're up to 2fps. So, as suspected, the problem isn't the capture time but the post processing time. The only realistic way to get this faster is run it on a faster machine (e.g. a Pi 2 or a full PC if you ship the frames over a network to it), write a version of findBlobs that utilizes the GPU (this is hard), or find another (faster) algorithm for achieving the result you're looking for (unfortunately I don't know enough about computer vision to point you in a good direction here; it may be worth asking this as a separate question in a more general forum). 

The first thing to understand is what setting actually does. When this is set to 'off' it fixes the camera's analog and digital gains to their current values (which can be queried with and respectively). When the camera is first initialized, the analog gain is typically zero. Unless you give it time to stabilize at a higher value (with a delay of some sort), you'll just be fixing the gain at zero (hence the black frames). So the first thing I'd suggest doing is inserting a delay after initializing the camera but before setting to 'off'. Secondly, you are correct that shutter speed is limited by framerate. In other words, if you have a framerate of 30fps, shutter speed will be clamped at a maximum of 1/30s (or 33333Âµs). You cannot increase shutter speed beyond that without reducing the framerate. Note that if you reduce the framerate you will need to increase any delay introduced to allow exposure mode to stabilize. For example, at 30fps a delay of two seconds will give the auto-exposure algorithm 60 frames worth of data to determine a decent gain. But at 1fps, it'll only have 2 frames (usually not enough to determine a decent gain). The slower the framerate, the longer the delay that needs to be used to allow decent gains. Unfortunately the camera firmware provides no means (that I'm currently aware of), of explicitly specifying the analog and digital gains - the best you can do is let them settle then fix them. I should mention that I think 800 is the maximum ISO you can set manually (the camera will shoot at ISO 1600 under certain circumstances but I vaguely recall exposure mode has to be 'night' for the firmware to allow it). 

Easy enough. We'll use sensor mode 4 which provides full field of view and uses the sensor to perform an initial 2x2 binning. We'll set the output resolution of the camera to 120x90 (this simply means the GPU's resizing block will take the full-frame 2x2 binned sensor data and downsize it down to 120x90). Finally, we'll capture straight into a numpy array but we'll only make it large enough for the Y (luminance) plane of the data; it'll throw an error because the array's not large enough for all the data, but that's okay - we can ignore that and it'll still write the Y data out: 

So, in conclusion, it might be an interesting project but I'm afraid the likelihood of coming up with something operational at the end of it is extremely small. 

The RPi compute module (note: not the regular RPi) has two CSI camera ports, and implements stereoscopic capture. On a regular RPi, however, you'd need to resort to a couple of USB webcams (preferably ones that implement video compression themselves to save the CPU doing any work). Unfortunately, without some means of syncing them externally they'd gradually drift, and I'm not currently aware of any mechanism to synchronize USB webcams. 

This doesn't perform any re-encoding; it simply wraps the raspivid output in an MP4, so it's an extremely fast operation. Why does the Pi's camera produce such a low level stream instead of something like an MP4 file? I don't know for certain but this would be my guess: Remember that the Pi's camera is fundamentally a mobile phone camera. On a mobile phone one would want to combine the video output of the H.264 encoder with some encoded audio stream (something like AAC) and output the result as an MP4 file. In such cases you wouldn't want the camera firmware to produce an MP4 file as you'd just have to decode that to extract the video stream before muxing it with the audio stream. Hence, the camera's encoder firmware produces this relatively low level stream and expects the caller to handle wrapping it up in something more end user friendly like an MP4 file. 

So you could take apart raspivid and try initializing a second preview renderer then sticking it on the video port, or you could have a look at picamera's splitter usage and try sticking one of those on the preview port (then a second preview renderer on its outputs). Resizing a preview renderer to only cover part of the screen is pretty trivial (both raspivid and picamera provide this capability). I'm not sure if either of these will work or if one is better than the other. My understanding is that MMAL is a pretty high level abstraction of what goes on in the firmware so there's probably all sorts of stuff going on under the covers that will make a difference in terms of performance or capability but it's definitely where I'd start. 

Secondly, on the subject of open-source camera firmware (raised in the comments): don't hold your breath. This is extremely unlikely given: 

Compiling raspivid (or any of the raspicam apps) requires a lot more than just . Have a look at the top of that file and you'll see it also uses stuff from , , and , not to mention all the libraries it requires. Honestly, you're best off taking the pain of compiling userland in its entirety once. Then you can modify raspivid (and any of the stuff it depends upon) and recompiling it is just a matter of rebuilding the modified stuff. The one thing I would note is that the default script in the root of the repo is annoying in as much as it defaults to clobbering your existing installation of et al (after building it runs on line 20): 

Note: parameters which return fractions also accept any other numeric type and will automatically convert the value to a fraction. It's even possible to mix picamera and mmalobj in a script (if you're careful). For example, knowing that is actually an object I could've constructed the camera object as in the script above, but this would actually wind up more complicated as we'd then have to deal with the null-sink that automatically constructs on the preview port. 

At this point you should find exists and OpenCV should be able to use the Pi's camera module. Control of the camera module (via the V4L driver) requires the utility. Details for installing this are in the linked forum post. 

The following should work, at least as far as capturing the image and converting it to a format that zbar can use: 

Filtering values The servo has a range from -1 to 1, whilst the pot ranges from 0 to 1 so we probably want to scale the values: 

A better solution that running everything with would be to mount the partition so that all the files and directories on it appear to be owned by the user. This can be accomplished quite easily by tweaking your entry. Just change the item to and reboot. In other words, the line should look like this afterward: