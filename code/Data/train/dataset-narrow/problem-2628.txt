The best way to increase your chances of being employed as a game developer is to get your hands dirty and actually work on the development of some small games. You say that you are "interested in programming games", which makes me believe that you haven't developed any games, yet. I advice you to stop thinking and start doing. However, the first order of business will not be to specialize in any of those fields. Let me elaborate. When I interview you for a job, if you tell me you are interested in programming games and your specialty is game AI, I will immediately ask you which games you worked on. If the answer is "none", I don't care about how many books you read or how you believe you are an expert in game AI. You won't be hired, period. To obtain game development experience in a specific field such as AI, you need to seek out others that are developing games and try to get into their teams. You want specialization, remember? A generalist can develop the whole game, but a specialist needs to work in a team. However, you will again be asked for previous work for someone to trust you with the AI of their game, let's say. Even if you will work free of charge. So, it's a chicken and egg problem right there. Basically, you can't just specialize in anything with no prior game development experience. Here's what you need to do: you need to forget about specializing and start contributing to the development of a game in any way you can. You can develop a simple game using a game engine such as Unity, or ask someone else developing a game and they will tell you what they need done. Then you'll figure it out and that will give you experience. Once you have something to show, not only you will have a better chance of joining a team, but also you'll have a better idea on fields that you can specialize in or that you don't like to work on. Trying to make a decision right now without that experience is like marrying someone without getting to know them. You cannot know if you will hate coding game AI unless you have some relevant experience. 

Few years ago I have been solving the same problem - I wanted to use Half Float extension in OpenGL, so I needed to convert Float32 to Float16. I have found somewhere this code, so I hope it will help you too: 

I can't tell you if it will help you very much, because I believe it depends on company and people working there (and doing interviews). But I can tell you this: I was in similar situation 2 years ago. I was also looking for a job in bigger AAA game company (few month before my graduation, because after graduation a lot of people is looking for a job). And I was on an interview (and passed and was working there for a while). I created 2 small games (in team) and had some other demos - like cloth simulation computed on GPU, some graphics demo using shaders etc. First part of interview was with HR person. Some normal question as you can expect on every other interview. I didn't answered all of them, but maybe half. Second part was interview with my future superior. He was asking me typical programming questions, like what is Singleton, some data structures questions, some puzzles, ... And he also asked me like 3 question about my demos (but it looked like he sees a lot of demos like this every day). And then third part: I sat in front of a computer and had two hours to complete a small game - there was about 20 parts of game and I should complete as many as I could. I didn't completed all of them, but they wanted to test me - how I work under (time) pressure, how I deal with some hard tasks, how optimized code can I write (because this is pretty important in game programming). And I believe that this was the most important part for them. 

Because after first call to Move function, posy is decreased by 8.0 => posy = -8. When you draw your rectangle, you put some coordinates, which also become negative and ywhole polygon is out of window. BTW: And I totally don't understand your Move function: 

Unity animations do not animate changes to mesh details. Animations are only pos-rot-scale of nodes. Unity's skinned mesh renderer uses animations of nodes to deform meshes. How Unity deforms your mesh can be slightly different than Blender's deformation. Unity is trying to do the right thing given the armature animation and the mesh weights for the nodes of the armature. How are you animating the normals in Blender anyway? The armature in Blender isn't supposed to deform normals like the first screenshot you gave. 

Try placing this code in FixedUpdate() instead of Update(). Physics motions happen in FixedUpdate() and Update() is synched with draw calls. The two don't always happen at the same frequency. Most of the time this is the cause of the jittery motion. So, if you are going to affect the motion of a physically-simulated object, you need to do it in FixedUpdate(). Now, why does it happen only when you do the normalization? Probably it's because the extra processing time required by Normalize() causes the Update calls to lag more behind FixedUpdate. The fact that it acts differently in different computers also supports this idea. How much out-of-sync Update and FixedUpdate get depends a lot on the available CPU cycles, which tends to be nondeterministic. 

1) Since speed is a concern, you may want to take a look at approximate nearest neighbor algorithms. I've used ANN in the past and it performed very well for around 12 dimensions. It lets you adjust desired precision so that you can have a trade off between speed and precision and find what works best. 2) Since your visual occlusion is a black-box one (I'm assuming unpredictable moving obstacles), I'm not sure if you have much of a choice other than doing occlusion tests on the points that the NN algorithm returned. 3) I don't believe ANN supports points changing, but I'm not sure since I didn't need that. It seems Cgal and Pastel support dynamic sets, but in terms of insertion/removal of points. Perhaps the papers here would also provide some insight. I don't know if you need this advice, but I found that reusing libraries for such problems almost always is a better idea. There are so many pitfalls one can fall into while implementing the details. Good luck! 

IMHO: If you are programming some real-time application, you should avoid recursion if it's possible. Recursion has to push and pull curent context and it's slowing down your app. It's also more memory complex. Of course if you have just few objects, it's not so dramatic. But when you will add more and more objects in future, it will be slower and slower. You can read something about it in this article on wikipedia. 

I think you should have really good strategy how to get your games to people. There are tons of games in these days and it's really hard to push your games through. Maybe you should think about some publisher. 

It's named Z-fighting. You can google a lot information about it. For example here on wiki. In short: your water and terrain plane are in the same distance from camera (they are coplanar). If you move your camera (direction), float results differ a bit, so sometimes (I mean: some pixels) it chooses water to be drawn, sometimes it chooses terrain. And you can solve this problem easily - just move your water plane a bit above or under the terrain, if they have the same "height" (they are coplanar). 

Maybe I just don't understand your question, but if you want to draw text, you can use function glutBitmapCharacter. 

I unfortunatelly now don't have time to read about that SAP algorithm, but what I have read few months ago (but I don't know, if it's suitable for your problem, it's much more suitable for points - not objects): You can use some special case of kd-Tree - for example octree - which uses half partitioning in each axis (in article I have read, they were using quarter partitioning - each axis is split to 4 peaces). You use dynamic version of it. First you build your structure at the beginning. This means, you split space into kd-Tree. Leaf of tree should have some limited number of objects (for our example let's take 64). Each node knows of course how many objects is in it. When you update positions of objects, you transfer them also in structures nodes. After that, you update whole structure according to these rules: 1) When inner node has less then limited number of points (sum of its leafs is less then 64), then you merge leafs and inner node becomes leaf. 2) When some leaf has more then 64 objects, you split this node. If you want, I can try to find that article - when I will have some time. 

I know exactly what's going on and it's a tricky one:) In time, since one orbits after the other (the update functions do not happen at the same time), their distance increases or decreases little by little. Right when the planet orbits a bit, you want your moon to do exactly the same motion so that their distance does not change. Otherwise you'll make an orbit around a slightly different radius. Your planet, before doing its orbit step, can remember where it was and where it went to, and tell the moon to move in the same direction with the same amount. To show you how to do that, I would need access to your planet code as well. However, it's simple subtractions and additions of transform.position values, you can also figure it out. In the meantime, below is a hacky fix that should remedy the situation if both the planet and the moon are orbiting around Vector3.up. I wasn't sure about how you use the orbit angle, so I changed that a bit. This works for me here: 

It is generally assumed that the mass does not matter and they bounce up to the same height. This is because the coefficient of restitution, which lets you calculate the velocity change after the collision, does not depend on mass. The velocity right after the collision determines the height that the ball will move up to, independent of the mass (just like how mass does not affect free fall duration). So, for games, it's safe to assume that they will end up at the same height. However, in real life, it can be hard to make the two have the same contact properties and balls with different masses may end up bouncing to slightly different heights. This paper can provide further insight. 

1) Yes your observations are correct. 2) The standard global XYZ coordinate system makes sense when you think in terms of a first person shooter, when you are looking through the eyes of a character in the scene with a blank(identity) transformation matrix. Like it would when you draw a coordinate system on a piece of paper, X points to right and Y points upwards. According to the right hand rule (x->thumb, y->index finger, z->middle finger), Z points towards you. 3) It wouldn't be wrong, but it would be a diversion from standards. There are three problems that I can think of at the moment: (a) Let's say one day you want to use a physics library that uses the standard coordinate frame. If you did not follow the standard, now you have to think about the transformation that takes you from your world to the physics world. Can get annoying when you want to fix a bug. (b) When you want to share code with someone, or bring someone over to help with development, they have to get used to your convention. (c) When using standard 3D models, you always have to have a transformation above them to prevent them from looking sideways. Now to add to question 2, it is sooo useful to think of X, Y, and Z as not just three letters, but as right, up and backwards. Every character in the scene has a local coordinate system attached to them, and in their local coordinate frames X is always right, Y is up and Z is backwards. Once you have this, now you can make sense of vectors that you print out, or write your algorithms in a way that makes sense. Let's say you have two characters A and B, and you want to do something if one of them is facing the other. You can simply find B's location in A's coordinate frame (Ta^-1 * p_b), look at the vector you get and see if Z(backwards) is negative and X(right) and Y(up) are small, because that vector tells you how much backwards, right and up B is with respect to A.