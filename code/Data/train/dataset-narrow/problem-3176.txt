I've tried this one, and is very straight forward and easy to follow. Java is very similar to python in some sense so it shouldn't be a problem for you if you have done OO. If you want to get deeper inside natural language understanding and processing I recommend this book. 

A genetic algorithm is an algorithm based on biological evolution, on how nature evolved. It does exactly that, evolve the algorithm so that "it" finds the best solution to the problem at hand. You can use a genetic algorithm to find a solution to a problem which you don't know the answer, you know the answer but want to know a different one, or you are just lazy. The common steps for a genetic algorithm are: 

Look for commonly used methods that are known to be efficient for outlier detection. Summarize their applicability domains (medicine, biology, network security..) and try to link their strong points to your application in order to select some promising methods. Try the selected methods with usual validation processes inherent to machine learning problems. 

This is a basic property of convolutional networks. First layers identify simple features and as you go deeper in the CNN each layer uses the features of the previous layer to build more complex ones. I would say the state-of-the-art article about this is Visualizing and understanding Convolutional Networks by Matthew D. Zeiler and Rob Fergus. You can download it here 

As you have a multiple output problem, you need an implementation of ANN which support minimizing two errors at the same time. This is similar to multi-objective optimization. Python can help you here. See this sknn.mlp documentation for details on use. It will take multiple output, each on its own column in the target matrix. Therefore, your will be of shape , where is the number of samples in your dataset. Just call method with your existing and of the above format. Once the training is done, you get your minimum viable ANN. How to optimize the performance of ANN is a different story though :) 

Then any content-based approach should be fine. I can thing of a good article called TrustWalker using trust between users (you create links between similar users and propagate their tastes in the network). 

Wrong. You can do collaborative filtering with holdings. Just use the numbers/duration of holdings instead of ratings. 

Word2Vec uses the bag-of-word model as input which means that you can use whatever alphabet. Building a bag of word just consists of attributing one feature per word in the entire dictionary of your corpus. You can do this with a cyrillic alphabet as with other alphabet. 

It will randomly initiate weight matrices for input and outside side of your network. If you have just one hidden layer, the number of weight matrix is 2. ANN will pass your input through some kind of activation function, get some output from them and then forward those outputs to output layer for further processing. Look up some tutorial for the actual maths. But this is the intuition. Finally, based on the weights and further processing, ANN will generate some output. They are not within acceptable margin in the first iteration in most cases. Therefore, ANN needs to refine those randomly selected weights. This is where the backpropagation algorithm kicks in. It allows ANN to learn the most suitable weights for the best prediction. This backpropagation algorithm requires a cost function which it tries to minimize. This cost function is usually RMS error function. But this cost function can be anything according to your choice. Cost functions take (mostly) two inputs. One of them is your true location (from training/validation data) and the other is predicted location that ANN got after the processing. As ANN tries to minimize the difference between predicted output and true output, it finds suitable weights for the network. When the error from your cost function reduces to a desired value (or a predefined number of iteration has been passed, or a tolerance level has been reached), ANN stops training and returns you those learned weight matrices. Once you have these final weight matrices, you can apply them on any new data to get the prediction. These weight matrices are your ANN. 

The parameters of a neural network are typically the weights of the connections. In this case, these parameters are learned during the training stage. So, the algorithm itself (and the input data) tunes these parameters. The hyper parameters are typically the learning rate, the batch size or the number of epochs. The are so called "hyper" because they influence how your parameters will be learned. You optimize these hyper parameters as you want (depends on your possibilities): grid search, random search, by hand, using visualisations... The validation stage help you to both know if your parameters have been learned enough and know if your hyper parameters are good. If you want to know more about hyper parameters and parameters in general in machine learning, look for "deep learning versus shallow learning". 

You are assuming improving new samples will always improve the performance. This is not true in many cases. Did you plot a learning curve to see how the performance is improving with respect to increasing number of samples? You probably should try to do this before delving deep into sample correlation. 

Isn't train_test_split expecting both and to be a list of same length? Your X has length of 6 and Y has length of 29. May be try converting that to pandas dataframe (with 29x6 dimension) and try again? Given your data, it looks like you have 6 features. In that case, try to convert your to have 29 rows and 6 columns. Then pass that dataframe to . You can convert your list to dataframe using . 

Use content based approaches if you have data on your items. Use collaborative approaches is you have data on your users. Use both if you have both. I would say content based approaches are general machine learning problems (how do I extract meaningful information from data) whereas collaborative filtering is really a recommender system specific work (how user's behavior can suggest users/items similarity/connections). Well you can. Neural nets are just a kind of algorithm, you surely can use them for content based analysis, and it might be possible to use them to enhance your collaborative algorithm. NN use texts and images as numerical data, so I don't understand your question. 

If you can, take a look at this book. It is not free, but worth a look. Alternatively, take a look at this online book for free, it is a great source and he has his own youtube channel. It's more basic than the other book I recommend, but will help you get started with GAs. To answer the other part of your question, the GA should be used to find a model, but will not act as a model. For example, if you have a neural network you can train it using the backpropagation method, but you could also train it using a GA. The GA will not use anything of the backpropagation mathematics, it could be used to generate weights in its neurons, and evaluate the answer (last layer). Weights will evolve to get closer to the solution. In this scenario, the model will still be the NN, but you used a different algorithm to find the best NN. Hope this helps. 

Looks like you are trying to predict two different things (lat and longitude) with same input. Therefore, this is a problem of predicting multiple output. However, it is easy to understand the basic with only one output. I will describe it that way. Let's say, your target is just one variable , which is your location. After transformation (possibly TF-IDF), you have a feature matrix where one row represents one sample and one column represents one feature. What you need to do now is train an ANN with the input and target . This is the way an ANN will train: