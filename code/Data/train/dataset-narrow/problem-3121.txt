Basically I guess does not support decision trees. I quote from here, This is a big oversimplification, but there are essentially two types of machine learning libraries available today, (CNN,RNN, fully connected nets, linear models) and Everything else (SVM, GBMs, Random Forests, Naive Bayes, K-NN, etc). The reason for this is that deep learning is much more computationally intensive than other more traditional training methods, and therefore requires intense specialization of the library (e.g., using a GPU and distributed capabilities). If you're using Python and are looking for a package with the greatest breadth of algorithms, try scikit-learn. In reality, if you want to use deep learning and more traditional methods you'll need to use more than one library. There is no "complete" package. You can see from here that there are other learning algorithms implemented in which are not deep models. You can take a look at here for tracking algorithms implemented in . 

To me, big data is highly connected to the deep-learning era. The reason is that during past decades, people could make good descriptions and models of data using machine-learning and data-mining but because everyday new data is coming out, social networks increase rapidly and digital gadgets' popularity is increasing among different nations, the demand for processing data and converting them to information and knowledge is increasing. If we want to use previous techniques to gather information from raw data, it will take too much time, if possible, to reach to appropriate results. In big data and deep-learning era, we need more complicated algorithms and more powerful hardware to deal with difficulties. You can also take a look at here and here which have relatively different perspective. Big data is a term that describes the large volume of data – both structured and unstructured – that inundates a business on a day-to-day basis. But it’s not the amount of data that’s important. It’s what organizations do with the data that matters. Big data can be analyzed for insights that lead to better decisions and strategic business moves. 

If you use sigmoid like activation functions, like sigmoid and tanh, after some epochs of training the linear part of each neuron will have values which are so much big or small. This means that the linear part will have an output value which has big amount regardless of its sign. Consequently, the input of sigmoid like functions in each neuron which adds non-linearity will be far from the center of these functions. 

Using machine-learning techniques, nowadays computers can automatically see and understand better than years ago. Tasks like recognition, localization, detection, semantic segmentation and related things have better performance today. This progress has promoted other fields too. Automatic cars, using autonomous driving benefit these advantages for pedestrian detection or car detection. Recommender systems are so popular today. They suggest you exactly what you need based on your actions. On Facebook, you can find your old friends without even trying to. You can see a lot of movies which you may be sure that will like them. During searching in search engines, you can see exactly what you are looking for without even knowing how the learning algorithm is ranking the results to show you the appropriate result. Your email service finds spam and junk mails automatically and delete them for you and you can be confident that you won't loose any important thing. OCR systems transform images of books and make them as text files. Using this, you can search in texts and find what you want without searching a lot of images. Tasks which are difficult for finding algorithms, like walking of robots, are so better than before, using data and machine-learning. There have been designed agents which can defeat human brain in famous games, like chess. Even in AAA computer games, the agents use reinforcement learning to learn how to defeat the human rivals using experience of each specific player based on their behavior. Machine learning is EVERYWHERE. 

I recommend you using and employing its pre-trained models. Because of low number of data-set, you should use transfer learning. There are lots of researches about that like here. Based on the data that you have, you should choose a model which is appropriate for your task and have been trained for tasks which are like yours, and then use it. I guess ResNet and GoogleNet already have been trained on ImageNet data set and are in the . You have to freeze the weights of the convolution and dense layers. You should change the soft-max layer with your own. In this kind of learning, the pre-trained model has already the ability to find features. You are just supposed to let it learn how to classify your data. 

You have imbalanced data set, so you should use score. Also you can use weight for rare classes, so that your cost function will be formed in a way that it cares about rare classes so much and tries to classify them correctly. You also can use confusion matrix for the details, but will suffice. And yes, use instead of precision or recall. You can also take a look at here. 

In regression problems it is customary to normalize the output too, because the scale of output and input features may differ. After getting the result of the model, you have to add the mean to the result and multiply that by the standard deviation, if you have done that during normalizing. 

Data is the most important part of the learning process. Your data have to be representative enough. Deep learning problems are considered those problems which may have better results if their training data increases. If the model should be able to generalize, it is vital to use data from the real distribution. This is just for training. You can use some techniques in order not to overfit the data using your training data-set. It is customary to add small noises to the input to let the model generalize well. Transforming data by translation, rotation and even distorting image inputs are examples of adding noise to the input to avoid overfitting. Although changing the data is dangerous because it may change the distribution of the real population. So, you can do something with your input data to avoid overfitting and let your model to generalize well. 

Finding an appropriate architecture is somehow practical. Those hyper-parameters you are talking may be different. Try to use a base architecture and then train your model. If it does not learn your model try to change the hyper parameters. It is an iterative operation to find a good model. There are a lot of debates, but there is not exactly consensus for making good models in . You can think of if your model does not learn your data, it needs more features to be learned by the . The behavior of the layers are somehow as follows: Max Pooling It is used for adding spacial invariance to the inputs of its layer. It is also used for decreasing the input size. By increasing its size, the two mentioned behavior would increase. Convoluional Layers These layers are used for extracting features to reduce the cost function in order to learn data. If you increase the size of these layers they behavior would be more vast, they would find features of a larger region. Take a look at here and here. 

Definitely they are different. Very deep nets have exploding/vanishing gradient problem. The authors of paper had seen that by stacking many layers of convolution and dense layers, the learning did not increase although they used activation and batch normalization. They used a concept named skip connection which helped the nets to learn whether the input to a typical layer should be preserved or it should be transformed by that layer. Using this concept allowed them to increase the number of layers without hesitating whether they would have vanishing/exploding gradients. The concept of residual nets was originally this. The paper uses this concept for spatial data but recently I've seen people debating using them in temporal cases too __time series data. Recurrent nets are used in temporal domains. Tasks like sequence classification are examples of their usage. In this domain the net should know the information of previous seen data. Well known examples of these nets are LSTMS. Early recurrent nets had vanishing/exploding gradient problem too. but after years s get popular amongst deep-learning practitioners. They defined a concept named gates which could learn when to forget and when to keep the previous data. 

The reason is that you are printing the object of function. Instead call the function. Suppose the name of your function is . You are doing something like 

may help you. If you are doing other tasks I highly recommend you taking a look at this question which is so much popular and has great answers. If I want to quote, I quote the following answer. Network Repository an interactive data repository with over 600+ networks in 20+ collections; from large-scale social networks, web graphs, biological networks, communication and technological networks my help you. 

Generalization techniques are used to make the network generalize well. The customary approaches are: 

Actually for generalizing you have to find a model that does not overfits the training data. For doing so, there are numerous approaches, like L1/L2 regularization which adds noise to the cost function and somehow weights to prohibit the network from having large weights which may lead to overfitting. Take a look at here. Other techniques are drop-out which adds noise to the activations to help the network not to depend on special nodes. These techniques add some noise to different parts of the network which lead to a cost function with a high error value. After finding the error back prop techniques try to set the parameters/weights to go downhill of the cost function. These techniques help the algorithm not to overfit which means the constructed model will be able to generalize, although you have to test it using cross-validation data and test data. Consequently, optimization techniques by themselves always try to reduce the amount of error and they somehow always lead to overfitting because they try to reduce the cost and this is regularization techniques and other variants that have to be used to construct a cost function which its optimum points do not lead to overfitting. This link also may help you. 

Like the update rule for bias terms in dense layers, in convolutional nets the bias gradient is calculated using the sum of derivatives of Z terms: $$ dJ / db = \sum_h \sum_w dZ_{hw} $$ which J is the cost function, w is the width of the activation after convolution and h is the height of the activation after convolution. db is computed by summing dZs. It means you are summing over all the gradients of the conv output (Z) with respect to the cost. Calculating the error of the net depends on the cost function that you have used. Depending on using cross entropy or mean-squared-error or other justified cost functions, you may have different update rule for other parameters of your net. But if you use cross entropy which is common for variants of classification tasks, the above update rule is used for updating bias terms. 

Gradient Descent is an algorithm which is designed to find the optimal points, but these optimal points are not necessarily global. And yes if it happens that it diverges from a local location it may converge to another optimal point but its probability is not too much. The reason is that the step size might be too large that prompts it recede one optimal point and the probability that it oscillates is much more than convergence. About gradient descent there are two main perspectives, machine learning era and deep learning era. During machine learning era it was considered that gradient descent will find the local/global optimum but in deep learning era where the dimension of input features are too much it is shown in practice that the probability that all of the features be located in there optimal value at a single point is not too much and rather seeing to have optimal locations in cost functions, most of the time saddle points are observed. This is one of the reasons that training with lots of data and training epochs cause the deep learning models outperform other algorithms. So if you train your model, it will find a detour or will find its way to go downhill and do not stuck in saddle points, but you have to have appropriate step sizes. For more intuitions I suggest you referring here and here. 

Biased in the context your speaking means that your model overfits the training data and can not generalize well. It means your model performs very well on your training data, but can not do well on cross-validation and test data. It is customary to say that biased learners memorize the training data which is really true. Biased learners don't learn the data, they fit the data. For understanding the other usages of bias take a look at this question. There is something that may worth to mention. You may hear that people say your model has high-bias problem. It just means that your model can not learn the training data whilst the biased learners overfits the training data, means fits the training data. The latter can not generalize well because it has fitted the training data, memorized it, the former can not generalize because it has not learnt even the training data so it has not learnt so much and can not generalize. 

Neural networks can be used for classification and regression tasks. They are also used for transcription and clustering. Each of them can have their own characteristics. For classification tasks you may have different classes. The trained network should flag the output with the greatest value among other outputs which corresponds to the appropriate input. In regression tasks, there are different situations. Suppose that you are given the input feature containing age, gender, height and other related things and you try to find the size of foot and size of length of hands simultaneously. I mean you have numerous inputs and numerous outputs. As an other example, in detection tasks, you are asked the place of the desired object. If you want to do that using neural networks, the output of your system consists of the center of the detected item, the height and the width of the detected item, which is a regression task. The interpretation of outputs for regression tasks is like estimating a real value, this is why linear activation functions are used as the last layers' activation. For classification tasks it is like finding the probability. The probability that your network finds depends on the activation function used in the last layer. If your classes are mutually exclusive, and each input should have just one label, for tasks each image of number refers to just a single number so classes are mutually exclusive, the activation should be and it represents the probability of belonging to each class. If the input may contain different classes, e.g. an image containing both cat and dog, you can use activation which each output represents the chance of existing of each class. You can also take a look at here.