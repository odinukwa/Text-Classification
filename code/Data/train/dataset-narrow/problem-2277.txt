There are several techniques which prove the nonexistence of black-box reductions. They are all inspired by the seminal work of Impagliazzo & Rudich. Let me describe the Impagliazzo-Rudich (IR) technique at a high level. In crypto, it is well-known how to construct a secure secret-key exchange protocol from trapdoor one-way permutations. However, all attempts to construct such protocols from general one-way permutations were failed. So, it was an open problem whether such constructions were possible at all. The hard part of refuting those constructions is as follows: It is believed that both one-way permutations and secure secret-key exchange protocols exist; so how can we refute the latter if we merely assume the former? In IR words: 

In a paper titled "On Deniability in the Common Reference String and Random Oracle Model," Rafael Pass writes: 

Let $f \colon \lbrace 0,1 \rbrace ^ n \to (2^{-n},1]$ be a function. We want to estimate the average of $f$; that is: $\mathbb{E}[f(n)]=2^{-n}\sum_{x\in \lbrace 0,1 \rbrace ^ n}f(x)$. 

Complexity of relations admitting ZK PoK with certain limitations, say limited round complexities (Itoh and Sakurai only consider constant-round ZK PoK). Another example is when the prover is polynomial time: He cannot use the reduction to 3-colorability, as he cannot solve NP-complete relations. All NP-complete problems have a Cook reduction from search to decision. Yet, by Bellare-Goldwasser result cited above, such reductions do not necessarily exist for all NP languages/relations. Other interesting results regarding PoKs which are not necessarily ZK, but whose knowledge complexity is otherwise limited. See Goldreich and Petrank (Comput. complex., 1999). 

Shannon–Hartley theorem describes the capacity of a communication channel in the presence of noise. The noise changes 0s to 1s and vice versa, with some pre-specified probability. If the channel behaved in a deterministic way---That is, if we could model noise in a way that we were able to determine what bits would change---The capacity of the channel would be infinitely large. Very desirable! I like to analogize randomness to friction: It is resisting movement, yet movement is impossible without it. 

EDIT: "Can we factor numbers in poly-time" is another TCS open-problem, yet I think it is younger than the problem mentioned above. Here's two list of TCS open-problems on the web: 

A colleague who works on genetic programming asked me the following question. I first tried to solve it based on a greedy approach, but on a second thought, I found a counterexample to the greedy algorithm. So, I thought it's worth mentioning here. 

Emanuele Viola has published the book "On the Power of Small-Depth Computation" which includes many results on circuit lower bounds. A preliminary version of the book can be found here. 

The computational complexity of computations over real numbers is considered by Blum, Cucker, Shub, and Smale. Here's a partial description of the book: 

We have IP = PSPACE = PPSPACE, where PPSPACE is the "probabilistic polynomial space," as defined by Papadimitriou's Games Against Nature. However, relative to a random oracle O, we have IPO ≠ PSPACEO = PPSPACEO with probability 1. (See The Random Oracle Hypothesis is False.) Another example: let the assumption be false; that is, if A=B the AO = BO for any oracle O and all complexity classes A and B. The contrapositive will say: If AO ≠ BO, then A ≠ B. Specially, since there exists an oracle O such that PO ≠ NPO, then P ≠ N, and a long-standing problem is solved!!! Obviously, this reasoning has no basis, and I believe it can be refuted as such. PS: When we use the "=" sign with respect to sets, we mean that every element of one set is also a member of the other set, and vice versa. However, the sets might have totally different definitions. In the axiomatic set theory, this is called the axiom of extensionality. The reason of "IP = PSPACE but IPO ≠ PSPACEO with probability 1 for a random oracle O" is exactly this: While IP and PSPACE are externally identical, their internal structure is quite different. 

Pricing via Processing -or- Combatting Junk Mail by Dwork and Naor. Moderately Hard Functions: From Complexity to Spam Fighting by Naor. Concurrent Zero-Knowledge: Reducing the Need for Timing Constraints by Dwork and Sahai. Timed Commitments by Boneh and Naor. 

Rabin's signature scheme Rabin's oblivious transfer Goldwasser–Micali semantically-secure cryptosystem Blum-Blum-Shub pseudorandom generator Feige-Fiat-Shamir identification scheme 

The first technique, namely the ability to "monitor" queries to the RO, is very common in all papers referring to the concept of zero-knowledge in the RO model. Now, consider the definition of black-box zero-knowledge (PPT stands for probabilistic, polynomial-time Turing machine): $\exists$ a PPT simulator $S$, such that $\forall$ (possibly cheating) PPT verifier $V^*$, $\forall$ common input $x\in L$, and $\forall$ randomness $r$, the following are indistinguishable: 

I think this can be done for problems with some specified degree of unsolvability. To quote from Wikipedia: "Every Turing degree is countably infinite, that is, it contains exactly $\aleph_0$ sets." Then, I guess, for each problem within the same degree of unsolvability, there is some type of resource (time) bound, which gives an NP-complete language. Remark: Maybe I should have been more conservative when saying "for each problem within the same degree of unsolvability." It might be the case that, the above statement is only true for the class of problems possessing the same degree as, say, HALTING problem. See also: Martin Davis, What Is...Turing Reducibility?, Notices of the AMS, 53(10), pp. 1218--1219, 2006. 

One of the cornerstones of the modern cryptography is the definition of computational indistinguishability: It is used in definition of cryptosystems, pseudorandom generators, zero-knowledge, etc. Below, we will first define this concept, and then investigate some of its properties: Let $U=\{U_n\}$ and $V=\{V_n\}$ be two distribution ensembles, indexed by natural numbers. That is, for every $n \in \mathbb{N}$, $U_n$ and $V_n$ are probability distributions. We call $U$ and $V$ computationally indistinguishable if, for all probabilistic Turing machine $D$ which is polynomial-time in the length of its first input, all positive polynomial $p(\cdot)$, every sufficiently large $n \in \mathbb{N}$, and all advice strings $z \in \{0,1\}^*$: $\left|\Pr_{u \leftarrow U_n}[D(1^n,u,z)=1]-\Pr_{v \leftarrow V_n}[D(1^n,v,z)=1]\right|<\frac{1}{p(n)}.$ The probability is taken over the choice of $u$ and $v$, as well as the internal coin tosses of $D$. An equivalent definition is obtained by replacing the $\forall z \in \{0,1\}^*$ part by a quantifier over all functions $f$, and letting $z=f(1^n)$. Note that $f$ can be an uncomputable function. The question is: 

I like Papadimitriou's book. Specially, it has "Class Review" sections at the end of several chapters, illustrating the relations among complexity classes. The relevant sections are: 

Goldreich argued that in cryptographic settings, where all parties are modeled by probabilistic polynomial-time Turing machines, $f$ must be computable by a polynomial-time machine. Now consider other settings, such as the case of single- or multi-prover interactive proofs. It is proven (Shamir, Babai et al.) that the parties in such systems need not be more powerful than PSPACE and NEXP machines, respectively. 

While they were not succeeded to refute the existence of such black-box reductions, they were able to provide strong evidence that the implication is not provable by standard techniques. To this end, IR came up with the following model: Assume that all parties (including the adversary) have access to a random-permutation oracle (RPO), i.e. a function chosen uniformly from all n-bit to n-bit permutations (where 1n is the security parameter). RPO models the black-box access to some one-way permutation. Now, IR prove that in this model, secure secret-key agreement is possible if and only if P ≠ NP. Since standard techniques cannot resolve the P vs. NP problem, it follows that proving (or disproving) the implication "if one-way permutations exist, then secure secret-key agreement is possible" is as hard as proving (or disproving) P ≠ NP. Other excellent resources on this topic: 

I give a trivial construction which satisfies the requirement. I provide it to merely answer the existence of "reflexive" hash function. Let $G$ be any hash function producing high entropy in the output. Assume that $G$ hashes arbitrary-length binary strings to $k$-bit binary strings, where $k$ is any positive integer. Let $+$ denote the concatenation operator, and let $|x|$ denote the length of the binary string $x$. Define the hash function $H$ on input $x$ as follows: 

I'm gonna assume that, in Step 1, $G_1$ can be generated as needed, and $\langle G_1,G_2 \rangle$ is a hard instance of the Graph Isomorphism problem. (Please interpret the word "hard" naturally; a formal definition is given by Abadi et al. See also the paper by Impaliazzo & Levin.) 

For an ideal hash function, the fastest way to compute a first or second preimage is through a brute-force attack. That is, if there's no structural weaknesses in the design of the hash function, your best bet is to mount a brute-force attack. The knowledge of the message size (n bits) and some parts of it (m bits) will certainly reduce the complexity of finding a preimage under the brute force attack: The worst-case complexity will reduce from 2n to 2n-m. A space-time trade-off over the brute-force attack is by exploiting rainbow tables. I believe the partial knowledge of the preimage may prove useful in mounting this attack too. 

Let $C_n$ be a non-interactive statistically-hiding commitment scheme, able to commit to an $n$-bit string. To commit to $m \in \{0,1\}^n$, the sender picks a random $r$ (of proper length), and sends $C_n(m ; r)$ to the receiver. To decommit, he simply reveals $m$ and $r$. We let $C_n(m)$ denote the random variable obtained by uniformly and independently picking $r$ (of proper length) and computing $C_n(m ; r)$. Finally, let $U_n$, $U_n'$, and $U_n''$ be i.i.d. random variables with uniform distribution over $\{0,1\}^n$. We assume that repeated use of a random variable results in the same sample. For example, $\langle U_n, U_n+1 \rangle$ means a pair, where we first sample $U_n$ to obtain an $n$-bit string $m$, and then let the pair be $\langle m, m+1 \rangle$. I want to prove that the following distribution ensembles are statistically indistinguishable: $$\mathcal {X} = \{\langle U_n, C_n(U_n) \rangle\}_{n \in \mathbb{N}} \enspace,$$ $$\mathcal {Y} = \{\langle U_n', C_n(U_n'') \rangle\}_{n \in \mathbb{N}} \enspace.$$ 

This technique is used in other papers, such as [CGH] and [AH]. I found another technique in the proof of Theorem 6.3 of [IR]. It uses a combination of measure theory and pigeon-hole principle to reverse the order of quantifiers. 

Background and Motivation I did not mention my motivation at the beginning since it requires a great deal of background knowledge. Anyway, for the enthusiasts, I describe it briefly: The need for such estimators arise in the context of "Proofs of Ability," as defined in the following article: Mihir Bellare, Oded Goldreich. Proving Computational Ability, 1992. Unpublished Manuscript. Specifically, at the bottom of page 5, the authors implicitly assumed the existence of such estimators (There's no mention of precision, and the running time is not precisely defined; yet the context clearly defines everything.) My first attempt was to read "A Sample of Samplers---A Computational Perspective on Sampling." It pertains to a very similar problem, yet the error probability defined is additive, while ours is multiplicative. (I didn't fully read the paper, maybe it mentions what I need somewhere.) EDIT (as per Tsuyoshi's request): In fact, the definition of "Proofs of Computational Ability" requires the existence of a "knowledge extractor" whose (expected) running time is $p(n) \over E[f(n)]$. Since we don't know $E[f(n)]$, we want to estimate it; yet this must not change the running time considerably: it should change it up to a polynomial factor. The precision condition tries to capture such requirement. 

No, that's not what you thought! The "K" in K-SAT is not related to the number of variables in the formula; rather, it limits the number of "literals" in each "clause". Let's define the terms: atom = the same thing you called variable; e.g. "x", "y", "z", etc. literal = an atom or its negation; e.g "x" or "$\neg$x". clause = a disjunction of literals; e.g. $(x \vee y \vee \neg z \vee w)$. CNF: A formula is said to be in Conjunctive Normal Form (CNF) if it consists of AND's of several clause. For instance, $(x \vee y) \wedge (y \vee \neg z \vee w)$ is a CNF formula. The followin problem is K-SAT: Given a CNF formula $f$, in which each clause has exactly K literals, decide whether or not $f$ is satisfiable. That is, whether there is a an assignment to the atoms such that $f$ evaluates to TRUE. See also Mike Jason B Punkt's answer in this post. 

background Several years ago, when I was an undergraduate, we were given a homework on amortized analysis. I was unable to solve one of the problems. I had asked it in comp.theory, but no satisfactory result came up. I remember the course TA insisted on something he couldn't prove, and said he forgot the proof, and ... [you know what]. Today, I recalled the problem. I was still eager to know, so here it is... The Question 

You can adapt the work of Okamoto-Chaum-Ohta to this case. Let $P$ be the prover, $V$ the verifier, $E$ the public encryption key + algorithm (common input to $P$ and $V$), and $D$ the secret decryption key + algorithm (private input to $P$). We assume that $E$ can be probabilistic (to satisfy semantic security, it has to be), and therefore it uses some randomness to encrypt a message: $c = E(x ; r)$ Further, assume that $Com$ is a commitment scheme. The protocol is as follows (in the first step, $P$ commits to a random string $u$ using randomness $\rho$):