Anyone have any ideas where I should look next? The results of #2, #3, and #11 in particular are really throwing me for a loop... 

We have a cabinet of dedicated servers with a Brocade FESX648-PREM switch at the top. We own all the hardware and all the MAC addresses are known to us. We also fully manage the servers so we retain administrative access, but our customers have administrative access as well. Currently we use VLANs and subnets to isolate the servers, prevent IP hijacking, etc. However, we want to make more efficient use of our IPv4 allocations. We lose a lot of usable IPs for gateway purposes and we have an abundance of subnets too small for many customer needs. I've considered progressively migrating the servers to a single VLAN and use static ARP to prevent IP hijacking. We'd lose layer 2 isolation, but I'm not sure what sort of abuse that would open us up to. What are the dangers of placing all of our servers on a single VLAN with only static ARP bindings to prevent IP hijacking? In other words, what forms of abuse would be possible with a single VLAN config that would not be possible with every server on its own VLAN? Are there any precautions we can take to prevent the abuse while achieving our goals of more efficient IPv4 usage? 

I use an iSavi IsatHub for Internet connectivity when I'm in the backcountry out of cellular service. This device is a BGAN terminal and it works by establishing a data link with an Inmarsat satellite and then provides a WiFi network to connect your devices. Data usage is very expensive, so it offers a number of safeguards to prevent accidental usage. One of those safeguards is a firewall. I only need SSH connectivity, so I blocked all outgoing ports with the exception of TCP port 22, UDP 53 and TCP 53. All inbound traffic is allowed. This past week I was in the backcountry far from cell service. I fired up the IsatHub and connected my phone to the WiFi network. Much to my surprise, I started receiving text messages to my Verizon phone. More to my surprise, I was able to reply to those text messages and have a back and forth conversation. The satellite terminal has its own SIM card and SMS capabilities, but these messages were received over my Verizon SIM. I've searched the net for any documentation on network ports used for WiFi texting and calling. I found this and it does include TCP53 and UDP53 (which I assume are for DNS lookups), but it also includes UDP500 and UDP4500 which are standard IPSEC VPN ports. Does anyone have an idea of how this was possible? How the heck was I able to send text messages over WiFi with my Verizon phone when I've blocked all outgoing ports except those needed for SSH and DNS? Is it somehow related to accepting all inbound traffic? Could Verizon be re-appropriating port 53 for WiFi texting? Edit: When I returned home, I connected my phone (airplane mode, wifi turned on) to my home WiFi network, sent a test text message and took a capture of the network ports in use. The only ports I saw in use between my phone and Verizon-owned IP addresses were ports UDP500, UDP4500 and TCP443 (500 and 4500 being used for the IPSEC VPN ports I mentioned earlier). 

No other servers in the cabinet are experiencing packet loss. The gateway switch and bad server can ping each other without issue. If I log into another server in the cabinet and attempt to ping the bad server, then I do get the packet loss. The routing table on the bad server is fine -- the default route points to the proper gateway, no other entries exist (except for local IPv4 assignments). Firewalls have been disabled. No VPN setup is in effect (i.e., routing table on the bad server just has the default route). CPU load and network traffic are both very low. Server has been power cycled. Speed and duplex settings are set to auto-neg and are the same on both the switch and server. Forced 100mbit full on both ends, still had the packet loss. There are no port errors (no drops, collisions, FCS etc) recorded on the switch. CPU utilization on the switch is low ($URL$ 

My non-EE-background stab at an answer is: I don't believe fiber was initially looked at as a transmission medium purely for bandwidth reasons, but more for the fact that it [light] can travel much longer distances without amplification/regeneration, and the fact that it's immune to factors that electrical transmission mediums are not immune to, like noise for example. The higher bandwidth rates are a product of engineering done at the PHY level - I'm not sure if you're familiar with WDM technology, but basically it multiplexes light at various wavelengths to increase the total capacity of a single fiber pair - this gives you a higher aggregated bandwidth, but each wavelength still has a max of 10G (disclaimer: I'm not up on optical engineering news so it's possible that you can get higher rates per wavelength). There are certainly specs for (and maybe even small deployments of) 10G over copper, typically over what's called "direct attach" cables - they're twinax copper cables with SFP's on either side. There are also QSFP's which are 40G capable, but these will typically have one QSFP on one end and break out into 4 10G cables on the other end. 

The easiest way to mess someone up on "classful addressing" either on a test, or exam, or whatever, is to use misdirection by way of a subnet mask. Again, remember that the subnet mask does not apply for determining the class of an address. This is easy to forget because as others have said, classless addressing and routing have been around for over two decades now, and the subnet mask and CIDR notation have become ubiquitous in the industry. 

What problem are you trying to solve? Most CDN's work off of Anycast/GeoIP to serve the requested content as close to the source of the request as possible. I'm also not sure what you mean or what you're trying to accomplish by "finding out what users are doing" - CDN's are designed to be transparent to users to provide a better user experience when browsing the web (the biggest use case for a CDN - there are obviously others). I'm having a tough time thinking of what an average user would use a CDN for for reasons that would warrant monitoring of this nature. If you did want to build a lookup table or a database, it shouldn't be that difficult, since you could query whois or an IRR to get IP/routing information (assuming the CDN operator does the Right Thing and puts their info in the IRR - most of the big ones do). I'd start with figuring out which CDN's own which blocks and then cross-referencing which IP belongs to which block, and then you could make a distinction on who owns the CDN IP that your users were hitting. It's possible, but it will get hairy very quickly and may very well be an exercise in futility (thinking if CDN's resell services to other smaller companies and solutions like CloudFront). 

You want to look for anything that has a '2' in the first column. You can then find the offender and grep for the IP to find the routers that it's configured on. This would likely be the simplest way. You're already on the right track with a Perl script to poll the devices via SNMP to get at the IP addresses, but I will say that SNMP code can be a little "hairier" than just simply using or to just log into the devices and dump the running configs to filehandles (at which point you can use the technique above). Assuming you're running Cisco, there's also a fairly comprehensive troubleshooting guide that covers this topic if you're interested. 

TCP/IP Illustrated Volume 1 discusses at a high level how the loopback address works in the context of an Ethernet driver (see Chapter 2 Section 7, and diagram 2.4). The reason why pinging the loopback address is used is because it verifies the correct operation of the TCP/IP "stack" on a system without it needing to be connected to anything else. Here "stack" is used to refer to a software suite, not a conceptual model, such as the TCP/IP model. 

As long as whatever routable address space either network is using on the "External" side is unique, yes, the "Internal" address space may overlap. 

Unless the wifi is intended for administrative access, this would not be a great idea. You will encounter all kinds of interference and increased latency. Your server cabinet is most likely bonded to the ground and essentially amounts to a Faraday cage. Not to mention all the other large metal objects in a datacenter that could cause interference. The datacenter is also very unlikely to allow you to stick antennas outside of your cabinet to improve the signal. Point-to-point wireless bridges such as these work very well and are widely used. However, they are typically deployed on rooftops where unobstructed line of sight can be established and are used for pulling connectivity into a building where fiber is lacking. 

This ended up being a failing switch. A couple days later we started having issues on ports 37-48. The FESX648-PREM is powered by port ASICs which control port regions. Those regions are: 1-12, 13-24, 25-36 and 37-48. One of the failure modes on this box is that a port ASIC can die and cause forwarding problems. The "bad server" above, was the only server we had in use on the 37-48 region. So when we switched the port and re-tested, we had the same result because the failing ASIC affected multiple ports. We replaced the entire switch and that resolved the issue. 

I'm working on testing several FESX448-PREM switches. One of the switches in my test group is known to be bad. It was previously installed as a top of cabinet switch, 42 servers were connected to it, all port lights came on, full duplex, no errors, low CPU, etc but ports 13-24 would not forward traffic. As I understand it, this is due to a bad ASIC that covers port region 13-24. However, I now have this bad switch at my work bench and I cannot replicate the same forwarding issue with port region 13-24. At my work bench, I have port 1 as the uplink and I've been connecting my laptop to ports 2-48 sequentially using a CAT6 cable while running a continuous ping to a public IP. Interestingly, all the ports now work fine -- port region 13-24 no longer has forwarding issues. Does anyone know how this is possible? If there's a bad ASIC for port region 13-24, then I'd expect this problem to occur 100% of the time. I tried a couple other things afterwards. I had the theory that I needed more ports active at once in order to trigger the forwarding issue. So I first took a layer 2 switch and connected it on a bunch of ports with the FESX448. CPU usage immediately went to 100% on the FESX448. I figured something recursive routing was happening with the layer 2 switch. Next, I put the layer 2 switch into boot monitor mode so it wouldn't do any routing. That resolved the 100% CPU issue, but again I'm still unable to replicate the traffic forwarding issues with ports 13-24. Any suggestions on how I can replicate the forwarding issue and effectively test the remaining switches would be much appreciated! 

I have a Brocade FastIron FESX648 and I'm attempting to increase the ARP cache timeout on a port that is connected to an IXP. According to the Brocade docs, I can increase the timeout up to 4 hours with the setting . I've tried applying this setting globally, specifically to the IXP-facing port and variety of other combinations (lower and higher timeout values), but nothing seems to work. Whenever I run I can see that the ARP entries for IXP hosts are continually refreshed and never age higher than 4 minutes. Has anyone faced a similar problem with Brocade FastIron gear and have any config suggestions or possible workarounds? 

I am troubleshooting a bizarre case of packet loss. We have a cabinet of servers with a top of cabinet switch (Brocade FESX648-PREM). That switch runs BGP sessions with our transit providers. We have one server (referred to below as the "bad server") that's experiencing 50% packet loss. The server is running Windows Server 2012 R2 and it's been running for months without issue until this morning. At this point, I suspect something might be wrong with the switch itself, so I'm turning to this community for help with additional troubleshooting rather than ServerFault or SuperUser for server-related troubleshooting. This is what I've checked so far to rule out the cause of the packet loss on the bad server: