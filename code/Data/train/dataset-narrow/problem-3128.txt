The regularization parameter (lambda) serves as a degree of importance that is given to miss-classifications. SVM pose a quadratic optimization problem that looks for maximizing the margin between both classes and minimizing the amount of miss-classifications. However, for non-separable problems, in order to find a solution, the miss-classification constraint must be relaxed, and this is done by setting the mentioned "regularization". So, intuitively, as lambda grows larger they less the wrongly classified examples are allowed (or the highest the price the pay in the loss function). Then when lambda tends to infinite the solution tends to the hard-margin (allow no miss-classification). When lambda tends to 0 (without being 0) the more the miss-classifications are allowed. There is definitely a tradeoff between this two and normally smaller lambdas but not too small use to generalize well. Below three examples for linear SVM classification (binary). 

About the specific question You should not expect a specific behavior (increase and then decrease of accuracy) while you select subset of features, since this will be totally dependent on the problem (and each model) When you calculate the variable importance of features, you are taking into account the contribution of all the the features at the same time. Once you select a subset of features and build a new model, you will get a different representation or modelling of the problem (which does not take into account the other features - informative or not -). Now, you want to select the best number of features. This will also depend from your problem and the characteristics or conditions you need to fulfill. If you actually need to have the fewer features possible while optimizing prediction accuracy, you can select the lowest number of features that achieves the lowest error... and, if you have different cases with very similar errors, then pick a threshold, observe the top cases whose pairwise difference of errors is lower than the threshold, and select one (for example the one with lower number of features - since the errors are marginally the same -). Consider Recursive Feature Elimination The method you are using might not be the most stable approach. You should consider trying something like recursive feature elimination (RFE), a wrapper method where you build the classifier, rank all the features, remove the worst and rebuild the model on the remaining features. Then you repeat the method again. This will tend to be more stable...and you should expect different ranking everytime. Variance is also a critical factor Beyond the actual error (or accuracy) the model is giving you with each subset, you should consider to build each model through a cross-validation procedure and take into account both the mean error of the folds, and the standard deviation of these errors. If the standard deviation is high, then the selected subset of features is not stable, and will tend to vary plenty when testing with unseen data. This is important to evaluate the expected generalization capabilities of the model, and could be helpful for deciding between models (built with different subsets). 

Feature selection does not necessarily improve the predictive quality of the model. Reducing or transforming the features might lead you to loss of information and then a less accurate model. Is an open and complex field of research. However in many cases it becomes quite useful. It will depend on how good and different are your original features for describing the target variable. If you look into bionformatics, you'll see people dealing with thousands or even millions of features, while having only hundred of examples. Here feature selection becomes increasingly relevant. PS: Most commonly I've seen the term "feature selection" used for the creation of compound features, as most examples I've mentioned, while Feature Extraction term is used for actually removing specific features from the dataset without taking into account is relation with the target variable. 

This is the loss function for structured output SVM. The problem you mention is common in object recognition and object classification in images where much more background images are used than images containing the object. A stronger case happens with exemplar SVM's where just a single image of the object is used. 

You can say the first figure where lambda is lower is more "relaxed" than the second figure where data is intended to be fitted more precisely. (Slides from Prof. Oriol Pujol. Universitat de Barcelona) 

When observing other examples, such as the ones presented at sci-kit learn Manifold learning it seems right to assume this, but I'm not sure if is correct statistically speaking. 

This will depend on the type of task you want to perform: Object recognition is a wide task than can be approached in different ways such as: 

I understand from Hinton's paper that T-SNE does a good job in keeping local similarities and a decent job in preserving global structure (clusterization). However I'm not clear if points appearing closer in a 2D t-sne visualization can be assumed as "more-similar" data-points. I'm using data with 25 features. As an example, observing the image below, can I assume that blue datapoints are more similar to green ones, specifically to the biggest green-points cluster?. Or, asking differently, is it ok to assume that blue points are more similar to green one in the closest cluster, than to red ones in the other cluster? (disregarding green points in the red-ish cluster) 

EDIT I have calculated the distances from the original dataset manually (the mean pairwise euclidean distance) and the visualization actually represents a proportional spatial distance regarding the dataset. However, I would like to know if this is fairly acceptable to be expected from the original mathematical formulation of t-sne and not mere coincidence. 

You can go as further or specific you want with this kind of data. I would suggest first to analyze the data thoroughly before deciding which algorithms to apply. Evaluate the mean, quartiles, max and min of each parameter (such as in a box plot), verify if you have missing values and in that case decide a way of dealing with that (removing data, imputating - predicting missing value from data - or average, as common techniques). Also verify if is possible to create a new representative feature from the information you have, or even mix a set of features to reduce (a priori feature selection; just ok if it makes a lot of sense from your knowledge on the context - NBA). For example, a player position can be considerably representative, in the sense of giving context to data (e.g. it will come up that defensive players steal more balls than attacking players but score less). Afterwards I would suggest some simple algorithms such as: 

Each approach (and its combinations) uses different representations for feature vectors extracted from the images. Most typical representations try to be invariant to certain conditions on the images depending on it purpose, such as scale, rotation and illumination changes. Some successful representations are: 

Use a simple bayesian model where the states or nodes are each of your features. Connect the nodes directly when they haven a direct time dependency (for example vote1 -> vote2 -> vote3, but not vote3 -> vote1) and calculate the probability of a final (output) node to be something (for example user type 1, user type 2, etc). You could also use a hidden markov model that naturally models transitions between states. In your case it would output just 1 state (the prediction). I am not very fan of this model for your problem, but it could be nice to try. You will need a lot of data though. You could use Fuzzy Inductive Reasoning (FIR). It is definitely not a simple algorithm, but works quite well for classification having time series or time-related features. In FIR you want to find an optimal mask of a given depth. If you have, say, 5 features per sample, and for instance you want to know which features to select you will learn an optimal mask for your data that address this (and creates fuzzy rules, etc...). Also if you want to relate two or more examples you will define a depth higher than 1 for the mask. This will allow you to find a mask that for instance uses features 1,2 and 4 of sample t, features 2,3 ,4 and 5 of sample t+1 and features 1 and 4 of sample t+2. Moving t from 1 to N number of samples. So, interesting combinations of features from samples that have a time-relation (or some sort of sequence) are created. 

This does not seem a Data Science problem. However there are very nice tools to do exactly that, checkout: logstash, flume and fluentd. Actually if you want to be able to filter in fast and "smart" way checkout Kibana from the guys of ElastichSearch ($URL$ Those tools are enough to solve your problem in a very efficient way. 

Now I would like to determine when a new example is close, far or very far from the prototype (mean). A possible approach is to set two thresholds distance to determine to which class or case corresponds the new example (close, far or very far). How could I determine these thresholds? Possibly using number of standard deviations? What other approaches can be followed to perform all this? Let's assume the distance metric is already selected. 

I'm wondering if e-commerce companies where products are offered by users, such as EBay, are using Object Recognition to ensure that an uploaded image corresponds to an specific type of object (clothing, shoes, glasses, etc) either to classify automatically or more importantly to filter undesired images (such as non related or even illegal types). If so, which algorithms and/or open platforms could be use for doing so? From what I've looked it seems that HOG+Exemplar SVM might be one of the most accurate methods developed so far ($URL$ even having couple of public repo's with Matlab implementations ($URL$ but I'm still wondering if this is being used in industry. 

Feature Selection (FS) methods are focused on specializing the data as much as possible to find accurate models for your problem. Some of the main issues that drive the need for FS are: 

I have a dataset with physiological measures of subjects along time. I would like to create (or select) a mean prototype example in order to be able to identify in new examples how far are they from the mean prototype. A second issue will be to select a threshold to determine what is considered near or far. Each example has 20 numeric features and I have around 300 examples per subject. First ideas (disregarding outliers): 

A common strategy for dealing with imbalance is to penalize harder the missclassifications that select the class with higher frequency. In a binary classification problem you could penalize by dividing 1/n where n is the number of examples of the opposite class. See the following from Prof. Jordi Vitriá 

With the rise of deep networks, surprising results are being obtained feeding the images just as the raw pixels that represent them [6]. However, you will likely need hundreds of thousands of images to proper train a network that can abstract features to further recognize objects. You can actually take a look to the highly successful VGG Convolutional Network implementation at [Find actual links here][2] The previous approaches will typically required that you have each example labeled with target value (cat, dog, etc). In case of deep learning you are being able to learn first representation of images with unsupervised data (not labeled) and the being able to train a classifier on top of the network in a supervised way (with labeled data) You may find most of the references below at: [Google Scholar][3] [1]: [Xin Chen and Shen(2009)] Xiaohua Hu Xin Chen and Xiajiong Shen. Spatial weighting for bag-of-visual-words and its application in content-based image retrieval. Advances in Knowledge Discovery and Data Mining, 2009 [2]: [Viola and Jones(2004)] Paul Viola and Michael J. Jones. Robust realtime face detection. Int. J. Comput. Vision, 57(2):137–154, May 2004. ISSN 0920-5691. [4]: [Jia Deng and Fei-Fei(2014)] Jonathan Krause Michael Bernstein Alex Berg Jia Deng, Olga Russakovsky and Li Fei-Fei. Scalable multi-label annotation. ACM Conference on Human Factors in Computing Systems, 2014. [5]: [Xin Chen and Shen(2009)] Xiaohua Hu Xin Chen and Xiajiong Shen. Spatial weighting for bag-of-visual-words and its application in content-based image retrieval. Advances in Knowledge Discovery and Data Mining, 2009. [7]: [Krizhevsky et al.(2012)Krizhevsky, Sutskever, and Hinton] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In P. Bartlett, F.c.n. Pereira, C.j.c. Burges, L. Bottou, and K.q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1106–1114. 2012. [8]: [Lowe(2004)] David G. Lowe. Distinctive image features from scaleinvariant keypoints. Int. J. Comput. Vision, 60(2):91–110, November 2004. [9]: [van Ginneken(2002)] A.F. Staal J.J. ter Haar Romeny B.M. van Ginneken, B.Frangi. Active shape model segmentation with optimal features. Medical Imaging, IEEE Transactions on. Volume 21, 2002. [10]: [Matthews and Baker(2004)] Iain Matthews and Simon Baker. Active appearance models revisited. International Journal of Computer Vision. Volume 60., 2004.