I want to dedicate some time to learn more about performance and tuning issues. I assign for that a clean DB. I want to know how can I load into it some data and performance-problem queries/DML/DDL? Do you know some scripts that can cause/generate that (purposely or not)? The idea is to learn the basic(and maybe more) of em/awr/addm etc - tuning and performance wise. I dont really know what specific to ask, but I'm guessing index problems, selects issues, wrong way to access the data, etc.. Books are great but I have the knowledge I need some actual work on that. Maybe its not a Q&A at all, but for me any answer (as long as it is an answer to my question) will do. EDIT I dont want just a server load script - because thats has no point, plus thats not what I want. 

Now, in order to be able to I need to copy the logs from the backuped DB to the target archive dir. What I do is coping the last 2 days and it always passed. Some day it will not. So how can I know during backup time(right after it) what files exactly in addition I need to backup..? 

You can see the information how SQL tables are linked. Keep the mouse pointer on the link table. You can see all the information like 

Can I change compatibility mode to 110? What are the implications of changing compatibility mode from 100 to 110? 

Example 2: You can run top 1, top 2, top 3 queries for the below and know the flow. This is going to return 8 times. This query illustration is without column names. 

I have a database created in SQL Server 2008 R2. I have upgraded to SQL Server 2012. I was trying to execute below query to calculate percentile 

I received a MS Access form client. It is in access 2007 format. Initially Navigation Pane was not available. Only form was visible. Then I choose options->Current database->checked Display Navigation Pane check box. It is visible now. But It is suppressed and not able to resize or access it as shown in below screenshot. I checked the code by pressing Alt+F11. I could not find script related to navigation pane. Please advise. 

No Matter how many number of fields available, this code will work fine. I have taken help of cha's UNION ALL Query and developed the code. The results will be stored in a new table created by name "Result". 

Go to the Heidi's tab , subtab , scroll down to the line and change its value to 150. The other way is to launch query like that: 

Normalization is not always necessary because theoretical sets are relatively poorly emulated by RDBMSes. There are always some limitations, overheads, slowdowns and bottlenecks that can be avoided by some denormalizations. Sure for educational purposes it is preferrable to represent each data type by separate table. When your scheme have types and then they have to be declared as two different tables. But in the real production projects sometimes we have to denormalize for sake of performance for example. Unfortunately, each specific case need some specific approach that can't be advised without some analysis. For example table can represent the type "timestamped sensors readouts" like that: 

But those duplicates where none of entries have CategoryID is set are still remains in the table. Let's eliminate them: 

If your base accumulates rapidly incoming inserts, the good idea is to split long-lasting query into the sequence of small queries. That prevents DB from connections shortage when the connections are wait for the table will be unlocked and new connection being refused and data is being lost. Then you'll get the execution queue like instead of 

The setting takes effect immediately without restarting the server. Please refer Configure the remote query timeout Server Configuration Option and sp_configure. 

After several experiments I am here answering my same question. This works as our traditional method. Run queries one by one as below to understand the flow of task. Example1: Query 1: Below query returns . Meaning @a = 

But I am missing few items with my queries. Note: I have 6 million records with 300K GroupID's in my table. Please give an efficient query. 

I need to identify and update co-related records under Co_Related_Item column as depicted below. Table name is CoRelated. Scenario explaination: In the first group i.e. [GroupID] = 1 we can see b=c, c=d and e=d, hence b=c=d=e and need to be flagged 'YES'. They are co-related. But x is not co-related to any item with in the group and hence it is a different item. So we can ignore this record. 

I am here answering my own question. After so many observations and experiments with searching over the internet, I found solution to this issue. You need to run this simple query 

I have a table by name ItemProperties with 15 columns. Columns names are like Feature1, Feature2, Color, Length, Width, Height etc... It has 10K+ rows. I need counts from this table where any 3 columns are filled (Not Null). Can we do this through a query? 

Your query haven't appropriate index (only simple index for is used) and thus is forced to fetch 11898928 rows to apply the timestamp restriction. Add the multicolumn index (class, timestamp) and you'll see what happens. Please post the new output to show the difference. 

You should add an indexes for each table having in the of . Columns used for index should be the same used for s. In case of table you have to add an index for field while corresponding tables and should be indexed on the fields each. The common rule is: When you join two tables with condition, each table should have indexed columns used for . 

Here table should have an multicolumn index (foo, bar) or (bar,foo), or even both. It's depends on certain data stored in your table and some performance measurements should be performed. Even though for different queries different indexes can become optimal and day-wide range could be faster with (foo,bar) while month-wide range could be faster with (bar,foo). 

Dump the database Pump it in the temporary database Empty columns you do not want to transfer Dump the temporary database Transfer it 

is not a commutative operation. When A add B as friend that doesn't means that B do the same for A. That is why the mutuality emerge. You definitely need both rows: