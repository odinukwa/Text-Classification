Use ALTER TABLE SWITCH to switch the data of the table to a range-partitioned table that has only one partition for all the existing data, but for which new inserts will land in new partitions. Eg all row up to today land in one big partition, but new rows will fall into monthly partitions in the future. Then perhaps later you have some time to split the big partition. If you use a RANGE RIGHT partition scheme, then you can split small partitions off of the RIGHT side of the large partition. If you use RANGE LEFT you can split small partitions off of the LEFT side. Or, after you have switched to the new table you can INSERT the data from the old single partition into a new properly-partitioned staging table. Once that insert is done you can truncate the large partition, split the now-empty partition to match the staging table, and then switch the staging table in. Since the data in the old partition is read-only, you can perform the insert over time. Then the truncate, split, and switch-in are all metadata operations. 

The CPUs and RAM are only half the story when it comes to writing. Your ability to Write will be gated by the performance of the disk containing your database log, and by your network connection to the AlwaysOn Secondary (assuming it's synchronous). Consider Memory-Optimized Tables which are designed specifically to scale in write-intensive scenarios on big-memory multi-core machines by 1) minimizing logging, 2) eliminating locking and page latching, and 3) enabling native-compilation of TSQL code. Your ability to Read should be largely unaffected by the disks, unless your frequently-read data is significantly larger than the RAM. 

-Please don't use short, cryptic column names. -There's no reason to wrap individual statements in explicit transactions. You'll want to extract the key value from the table variable in a separate statement. Something like: 

The typical design here is to have a Transaction table and an Account table The Account table has the current balance, as of the last transaction that has "posted". A modern "online" system will post transactions immediately, but older systems would capture transactions online, but run "batch" to post the transactions later. PK on the transaction table would normally be (AccountId,TransactionId), which balances the cost of insert and the cost of pulling the transactions for a single account. I don't think there's a rule controlling whether you also store the running balance on the transaction table. Normalization does not require you to recalculate the account balance or the running balances. Balance is a proper attribute of Account, regardless of whether it can be derived from rows in another table. And you don't necessarily have to perform these calculations as you insert transactions. 

PAGEIOLATCH_SH means waits for reading database pages. When you create a database snapshot, none of the database pages are in the cache, and you will have to fetch them from disk. 

Also must mention that Azure SQL Database is ideal for a small shop with no DBA. And enables you to create readable replicas in the standard tier, and premium tier databases get an automatic readable replica. 

Which will enumerate all the permissible combinations of (BrandId, ChannelId, StaffUserName). Then use that new table in your DAX row filters by setting up bidirectional cross filtering between this table and Brand and this table and Channel. David 

This may have been an existing problem with the database you backed up. Do you need to preserve the existing Service Broker conversations? Messages? Broker Identifier? If you don't need the Service Broker messages, or to preserve any of its conversations or place in a distributed broker topology, you might end all the conversations WITH CLEANUP, or ALTER DATABASE ... SET NEW_BROKER. If you do need to fix this while preserving the other conversations and messages, you would probably need to open a support case, as there are really only a handful of people who can advise you on this. 

This is simple and quick to test. Take that 2TB backup file, copy it to Azure using AzCopy. Provision a SQL Server using the "Free License: SQL Server 2017 Developer on Windows Server 2016" image configured with 12-15 TB of SSD storage. Remote desktop to the SQL VM and restore the backup directly from Blob Storage, or copy it to a local disk and then restore it. Just remember to shut down the VM when you're not testing, and destroy it and the disks when you're done to minimize the charges. 

Both sessions have locks on the second key. The reader has an S and the writer has a U and wants to convert to X. Consider changing the database to use READ_COMMITTED_SNAPSHOT mode, or add an XLOCK hint to the UPDATE so it doesn't read with U locks and then convert to X. 

Replace the tables in the old database with synonyms that point to the location in the new database. This not only minimizes the code change, it gives you a single place to manage the references to objects in other databases. 

It may not be loading the TNSNAMES.ORA file you think it's loading. Instead prefer Easy Connect Naming. Here something like '//aaa.cedd.net/creds' 

A queue is a table. SEND is an insert. RECEIVE is a delete. There is some functionality to process queues from inside SQL Server, but you can always just use external clients. See eg $URL$ for a sample of how to process a queue from an external application. 

(Not an answer, but comments don't allow code.) You don't have an AlwaysOn AG or a mirror, correct? In a user database test your max log flush rate like this: 

If you have an MSDN license you can download and install SQL Server Standard Edition for Dev/Test. Otherwise you can use an Azure Pay-As-You-Go instance for testing. See eg SQL Server 2017 Standard on Windows Server 2016 

Add EmployeeId to columns to both tables. Populate the master EmployeeId with the query you have, and join the transaction details table to the employee master on EmployeeName and lookup the EmployeeId . eg 

You can use SQL Server Merge Replication for this. The SQL Enterprise instance would be the publisher, but Merge Replication can synchronize data both ways. So changes made at the subscriber would be replicated to the publisher. If you configure this to be a Push Subscription the agents will all run at the central instance, and you can schedule them with SQL Agent. 

With ordinary column encryption (not AlwaysEncrypted), the columns are simple varbinary columns, with no special configuration or metadata. So replication of the cyphertext "just works". To decrypt on the subscriber you would need the symmetric key, either by Creating Identical Symmetric Keys on Two Servers, or by restoring a backup of the publisher database containing the symmetric key before initializing the subscription (and restoring the Certificate or Master Key that encrypts the symmetric key).