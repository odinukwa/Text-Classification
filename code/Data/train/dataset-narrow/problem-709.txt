As long as it is run as a single transaction, the value will be re-used on each line, not recalculated. I'm not 100% sure about Postgres or SQL Lite but I'm fairly confident (98%) they will work the same. In the past when I was more confused about this question, I would pre-fetch the date value to a variable and then reuse the variable in the where clause. Now I realize that was pointless unless I have more than one place in a longer set of statements where I would like to use the value. As a test, move the calculated date value to the select clause and execute against a large table that takes more than a few seconds to return. You will notice that the first line will have the same date/time value as the last record. 

If you want to have a computer be assigned to more than one user, you're going to need a junction table between the two to store the relationships. This was not stated in the requirements but it seems like a logical approach given the project scope. 1. Account 2. Computers 3. Account_Computer_Assignments. The third one is your junction table that stores the relationship between the other two. It's primary key would consist of IDAccounts and IDComputers using your naming convention. A foreign key relating back to their parent tables would also be advisable. This structure will also allow for multiple users to link to the same computer. I don't know if that is the intended behavior but it will allow for that with this design. If that is not intended, and a single computer can only be monitored by one and only one user, then a single field on the Computer table for the AccountID is all that is needed. No junction table. 

Every table should have a primary key (I really can't think of a reason not to have one). So having a paymentID column to your payment table is definitely a standard design. 

You could write a procedure with the EXECUTE and build the statement in the procedure by looping over the list of schema which match your query from the information_schema tables. Then you can call your procedure from psql or other passing it your criteria such as 'ceu_shard_test_merge_%'. You could have a parameter to do it or just dry run and instead of execute then it could output the statements or something along those lines. 

Since you are a programmer, not a DBA, I would recommend the correct programmer's way to do such things. Don't update the database directly - never. Maintain all your schema changes, and base data changes in files which are then pushed with your release. I don't know what your language of choice is, but there are many tools to do this, Ruby has it's own tool - rake - with which you do that. And here are two others: 

Code generators are great! Code generators are evil! 10-15 years ago, I would have said that having a code generator for quickly creating the boiler plate code for database driven applications would have been a great gift to mankind. 5-10 years ago, I would have said code generator sucks, they generate too much duplicate code and that having a data-driven user interface (where the fields on the screen, validation, etc is driven by meta-data in a database instead of coding the screens one by one) would have been a great gift to mankind that supplanted the code generators. Today I would say - write each screen individually. Use existing framework that wire fields and model objects and possibly ORM when doing simple CRUD. But do design each screen to the exact purpose of the screen. Application screens that mirror a RDMS table too much is only good for managing lookup tables. Don't annoy the user with geeky interface that are designed against a computer storage model (RDMS)... make a screen that has only what they need. That may mean that it will save to multiple tables, etc. Who cares. The user isn't a database. So my thought? Don't waste your time making a code generator. 

I think given the requirements and that at some point you may want additional information that wasn't listed here, I would go with a fully normalized approach. 3 Tables: Guests, Episodes, Episode_Guests Then depending on if you want to do this for more than one show, another table for Shows(or series). As Paparazzi mentioned, the Guests table should contain sex. The Episode table should contain a date. Also, if you are going to do this for multiple shows, the Episode table should also have a foreign key back to the Shows table. The Episode_Guests table should record every instance of a Guest appearing on an Episode so all it would need is a foreign key relationship back to guests and another for Episode. 

If you want to see all values from Table2 and see all records from table1 where there is a matching email, do this; 

You're question is very broad. But in general the answer is yes. A single instance has to share resources across all databases unless configured specifically not to. By default, SQL uses all available CPU's for all operations. You can configure MAXDOP (max degree of parallelism) on the database so a single query will only have access to that many CPU's. IO is another tricky issue. Generally it's best to seperate your workloads on physical hardware whenever possible. Logs, data and tempdb all on seperate physical storage. In addition, if you are trying to create a seperation of resources for the databases, you can use filegroups to assign the objects or the entire database to a separate storage device. There are many, many other answers but again. The question of 'how can I avoid that?'(in terms of cross database resource allocation) is way too broad for a specific answer. If you are looking at one aspect, say IO, or CPU usage, it's an easier, more specific answer. 

A database server running Postgres 9.4 shows no statistics when running - well only the is non-zero. The is null. I haven't been able to find much as to what should be checked in this case, to find out why statistics aren't reported. and are both set to . Per the manual: 

I have since upgraded to version 9.4 and a whole new server so I can't debug this further. But I believe the problem was with a drive. I found a bad drive which wasn't reported as bad by the machine. 

I'm trying to install PostgreSQL 9.4.1 on Ubuntu 14.04.2. I'm using the packages from . The package installs but it fails running . If I run manually, I get: 

Yes, that would be fine if your design goal is that Persons have only 1 Addresses. With this design, each Persons can have 1 Addresses but two or more Persons can have the same Addresses. It's all a matter of your business needs. If the above is what you are trying to get, then yes, it is correct. However, I think it's most common the other way around where the Addresses would have a foreign key to the Persons because a Persons could have more than one Addresses. As for your constraint to check the postal code - well first off you are missing the space and it's lower case. Whether that will work will depend on which database system you are using. I tested it with PostgreSQL and it does not work. I don't think you can really have such a simple constraint to fully validate a Canadian postal code. For example, there are some letters and some numbers which are never used. I'm a bit rusty on my Canadian postal office codes but I seem to recall that number 5 is never used as it's too similar to S, etc. 

I'm attempting to do a server migration with SSRS 2016 to Power BI (March 2018 release). I have been successful at getting my Data Sources, Data Sets, Folders, Reports, settings and subscriptions to come across. Each of the KPI's that we set up have failed to transfer. Other than this it's essentially the last thing that needs to transfer. I'm using the process outlined here; RS.exe migration process Has anyone else run into the issue using RS.exe that none of the KPI's transfer? Is there a workaround or some step that I have to take to get that to work? I've been looking on Microsofts pages for a bug report but haven't been able to locate anything. I realize that I can script copy the records out of the Catalog table but I was hoping for something a bit more out of the box. Also, to clarify, we are using Power BI report server through our Enterprise license with SA. I have verified that I can create a KPI manually on the new server. 

First off, the sort operation on a simple query like this will probably always be the most costly. That's because it's an operation that has to run after it has all of the data returned. That being said, there are no clustered indexes on the tables. That means they are heaps and they are storing the data totally unsorted on the disk. Without adding a clustered index to those tables where your sort is applied, it's unlikely that any changes to your query will improve performance. The only other thing that might help(might being a very big might), is move the 'WHERE type = 4' part into the corresponding join statement that it belongs to. So instead of WHERE type = 4 it would be added to the JOIN ON for ArchiveConfig (I think that's where type comes from at least) as AND type = 4. The other ON clause for that join seems redundant. Keep in mind, I doubt very much that this change will actually improve performance but it will make your code a bit cleaner. As the others have said, I don't know if it's a good idea to use the (NOLOCK) hint. I doubt it will improve performance in this case and might result in bad data being returned. 

PID 9593 is the most problematic one which other users get blocked by this one. As far as the user is admitting to, he truncated his table, then did inserts in batches of 1,000 committing after each batches. Currently this PID shows the following locks: 

I can't kill this PID (nothing happens when I issue the kill command). I'm not sure what to do to diagnose this further and obviously resolve this. Any input anyone? Running PostgreSQL 8.4 on Ubuntu Linux server. EDIT: As I found other connections in a similar state where the commit was hanging, I looked further and found the following in the server logs: 

Not an elegant solution but after installing the package using (which fails creating the cluster but installs PostgreSQL), I switched to the user and created the database using . Then back to , I created the cluster using the command. This moved the configurations to and set it all up. 

I simply removed the + signs and replaced with ||. On the EXECUTE I removed the format and simply concatenated the strings. 

As Zoltan pointed out, unless you have MANY millions of rows, I don't see a scaling issue. There are also many libraries for scheduling things such as Quartz on Java for example. These will store the recurring schedule as a cron-like expression. Because your example above has a flaw, if the recurrence is every Monday, then it's . So you can store a date, or a recurrence pattern. 

My first answer is: why is your database even accessible from outside through the Internet? That network traffic really ought to be blocked by router Internet gateway router or firewall. If you really need to allow some connections from the Internet to your database, then limit it to the valid IP address who should be connecting. At this point that's not really a dba question but a network admin question. 

This answer will require some dynamic SQL and potentially could make use of a while/do loop. Basically the idea will be to figure our how to define your list of tables and column names. If you really are just looking for a list of tables where a column name exists, you will probably want to make use of the sys.syscolumns table to query the column names. 

Ok. A literal, not a lateral value... Gotcha. A literal value in really any programming language is basically just a value that is defined within the code and does not change programatically. In SQL I might use that by simply defining a varchar variable with a set value. For example, at the start of a command, 

SSIS is going to be your best bet in this case I believe. If you already license SQL Server, it should be included as well. An SSIS package has the ability to access and utilize multiple ODBC or OLEDB sources and destinations including MySQL. It can be run using a SQL Agent job or just using DTEXEC from a Windows scheduled job. Also you can easily build logging, error checking and validation. 

If going down the view approach which would limit disk IO, be more normalized and MAY perform better(depending on the use of the value), it would look something like this; 

Once you have the anchor statement, you can UNION below it for all iterations down the chain. In fact, I'm not going to bother rewriting everything Pinal Dave said since it's so right on point in this case. 

The 'UPDATED' column from this query can be used to do a quick find and replace on whatever database or table name might be found in the view definition. You will want to make sure you do the same for every possible version of the text you are searching. For instance a database called 'db' might be written as [db] or db. Once you have what you want from the query just copy and paste to a query window and execute the whole thing. Please make sure to backup the database prior to running it. Also carefully proof read the output text so you can verify that the replace worked correctly and did not accidentally change more than you intended. BTW, the answer WEI_DBA gave is perfectly acceptable also. It's just a different approach to the same thing. In his example, the GUI will aide in getting all of the object definitions to the query window. Then you would just use Find/Replace(ctrl+h) to replace the database name. To each his own in this case. I don't know that there's an advantage in either approach except maybe that my approach will allow you to limit the query only to certain objects (views in this case) with additional filters added if needed in the where clause. Generate scripts will allow you to filter to only views, stored procs, etc, and then individually select objects but additional filters will be limited to whatever the UI can do.