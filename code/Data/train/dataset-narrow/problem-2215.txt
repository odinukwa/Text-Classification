Yes, it is nearly connected, with 99.91% of individuals belonging to a single large connected component, see this paper. You might also be interested in this one concerning the so-called $N$ degrees of separation. 

A rainbow matching in an edge-colored graph is a matching whose edges have distinct colors. The problem is: given an edge-colored graph $G$ and an integer $k$, does $G$ have a rainbow matching with at least $k$ edges? This is known as rainbow matching problem, and its NP-complete even for properly edge-colored paths. The authors even note that prior to this result, no unweighted graph problem is known to be NP-hard for simple paths to the best of their knowledge. See Le, Van Bang, and Florian Pfender. "Complexity results for rainbow matchings." Theoretical Computer Science (2013), or the arXiv version. 

Feldman et al. [1] give several references to methods for e.g., finding cliques of size $k = \Omega(\sqrt{n})$, including spectral methods, SDPs, combinatorial methods, nuclear norm minimization, and belief propagation. They also say the quasipolynomial-time algorithm is the fastest one known for planted $k$-clique detection for any $k=O(n^{1/2−\delta})$, where $\delta > 0$. 

Colin Cooper and Alan Frieze have a set of results in the context of random digraphs that might be of interest. They study the properties of a simple random walk on the random directed graph $D_{n,p}$ when $np=d \log n, d>1$. They have proved that: 

[1] Guruswami, V., Khanna, S., Rajaraman, R., Shepherd, B., & Yannakakis, M. (2003). Near-optimal hardness results and approximation algorithms for edge-disjoint paths and related problems. Journal of Computer and System Sciences, 67(3), 473-496. 

[1] Gavril, Fǎnicǎ. "The intersection graphs of subtrees in trees are exactly the chordal graphs." Journal of Combinatorial Theory, Series B 16.1 (1974): 47-56. 

Let $u$ denote the size of the union of the sets in $S$, $n\leq 2^u$ denote the size of $S$, $h\geq n$ denote the size of the output, and $S_i$ the set of sets resulting of the intersection of $i$ sets from $S$ (in this sense, $S=S_1$), maintained in lexicographical order. For all $i\in[1..n]$, $|S_i|\leq h$. $S_2$ can be computed in time $un(n-1)/2\leq uh(h-1)/2$: each binary intersection takes at most time $u$, and there are at most $n(n-1)/2$ of them. $S_4, S_8, \ldots$ can be computed in time $uh(h-1)/2$ each in a similar way. Other $S_j$ which are not power of two can be obtained by combining those results, up to $S_n$. The total running time would be within $O(unh^2)$? 

Does it matter in the 3Sum problem if the numbers to be summed belong to the same set or to distinct sets? Let's define 

It took a few years (five!), but here is a partial answer to the question: $URL$ Optimal Prefix Free Codes With Partial Sorting Jérémy Barbay (Submitted on 29 Jan 2016) We describe an algorithm computing an optimal prefix free code for n unsorted positive weights in time within O(n(1+lgα))⊆O(nlgn), where the alternation α∈[1..n−1] measures the amount of sorting required by the computation. This asymptotical complexity is within a constant factor of the optimal in the algebraic decision tree computational model, in the worst case over all instances of size n and alternation α. Such results refine the state of the art complexity of Θ(nlgn) in the worst case over instances of size n in the same computational model, a landmark in compression and coding since 1952, by the mere combination of van Leeuwen's algorithm to compute optimal prefix free codes from sorted weights (known since 1976), with Deferred Data Structures to partially sort a multiset depending on the queries on it (known since 1988). 

[1] Bodlaender, H. L. (1993). On linear time minor tests with depth-first search. Journal of Algorithms, 14(1), 1-23. 

Let $k$ be fixed, and let $G$ be a (connected) graph. If I'm not mistaken, it follows from the work of Bodlaender [1, Theorem 3.11] that if the treewidth of $G$ is roughly at least $2k^3$, then $G$ contains a star $K_{1,k}$ as a minor. Can we make the term $2k^3$ smaller? That is, does say treewidth at least $k$ already imply the existence of a $K_{1,k}$-minor? Is there a proof somewhere? 

In the edge-disjoint paths (EDP) problem, we are given an undirected graph $G$, and a set $\{ (s_i,t_i) \mid 1 \leq i \leq k \}$ of $k$ source-sink pairs. The objective is to maximize the number of pairs connected via edge-disjoint paths. In the bounded length edge disjoint paths (BLEDP) problem, we wish to route the maximum number of source-sink pairs such that each of the paths used is of length at most $L$, for some length bounded $L$ that is part of the input. Guruswami et al. [1] showed that BLEDP can be approximated in polynomial time within a factor of $O(\sqrt{m})$. They also showed this is optimal with a matching hardness result of $m^{1/2-\epsilon}$, for any $\epsilon > 0$. Do we have algorithms achieving better approximation ratios for BLEDP for some restricted graphs? For example, how about (undirected) planar graphs? 

[1] Dragan, Feodor F. Dominating cliques in distance-hereditary graphs. Springer Berlin Heidelberg, 1994. 

Let $X$ be an NP-complete graph problem. Suppose $X$ is solvable in polynomial time on graphs of bounded diameter. In other words, $X$ parameterized by diameter is in XP. (Recall a problem is in XP if it can be solved in $n^{f(k)}$ time). Does this imply solvability in XP time for other interesting parameters? If so, is there maybe even some more or less "standard" list or web of parameters and how they relate documented somewhere? 

A SIGMOD 2014 paper from Microsoft Research states that the "importance of sorting almost sorted data quickly has just emerged over the last decade", and goes on to propose variants of Patience sort and Linear Merge, and measure their performance on synthetic "close to sorted" data. It seems to me that this description matches the theme of "Adaptive Sorting", covering algorithms taking advantage of existing preorder in sequences to be sorted, which has been the topic of various publications (albeit in the community of theoretical computer science rather than databases) from as early as 1979: 

Have a look at the proceedings of the conference "Fun with Algorithms": they should provide you with a good selection of "Fun" problems to work on, and a venue where to submit your results for feedback. Check the publications of people known to consider fun problems and problems for fun. For instance: 

I was assuming that those two problems were equivalent, and I was using the second one as a pedagogical example for some parameterization technique (while there is an algorithm running in $O(n^2)$ for both, the second one can take advantage of variations in the size of the sets to solve the problem much faster, down to $O(n_1 n_3+\sum n_i\lg n_i)$ and $O(n_1 n_2 \lg \frac{n_3}{n_2}+\sum n_i\lg n_i)$). But I realized today that I did not know how to prove this equivalence, and I might even start to think that they are not. 

I am not sure I understood well your question, but if all weights are positive, I think that you can support weights updates in time within $O(n)$ (better than $O(n^3)$ per update, which I understand is what you asked?). Consider 

Have a look at my (modest) proposal of "fun" problem below. If you work on it, get in touch with me!