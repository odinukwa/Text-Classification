If an oracle tells me that $G_1$ and $G_2$ are isomorphic, do I still need to look through all $v!$ permutations of the vertices? I ask because I also think about knot equivalence. As far as I know, it's not known to be, but say detecting the unknot were in $\mathsf{P}$. Actually finding a sequence of Reidemeister moves that untie the knot might still take exponential time... 

although I can't find it in any errata. But more importantly, I believe the comment was to the effect of "finding preimages of hashes can be challenging, and the only obvious way for Merlin to execute the public coin $\mathsf{GNI}$ protocol is to go through each and every permutation $\pi$ of $G_1$ or $G_2$ and calculate $h(\pi(G_i))$ until he finds an $h(\pi(G_i))=y$." But is it the case that all such public-coin protocols put a heavy burden on the prover? For example, the hash $h$ is not required to be difficult to invert; it just needs to be strongly universal, which, as I understand, is not the same as being one-way. So if Arthur were to give Merlin an image $y$ for some "easy-to-invert" hash function $h$, then it might not be so difficult for Merlin to find the preimage. 

The above protocol is not perfect, some kinks I think would need to be worked out. For example, it's not clear how to generate two random graphs $G_0$ and $G_1$ that satisfy good properties of rigidity, for example, nor is it clear how to adjust the difficulty other than by testing for graphs with more or less vertices. However, I think these are probably surmountable. But for a similar protocol on knottedness, replace random permutations on the adjacency matrix of one of the two graphs $G_1$ and $G_2$ with some other random operations on knot diagrams or grid diagrams... or something. I don’t think random Reidemeister moves work, because the space becomes too unwieldy too quickly. [HTY05] proposed an Arthur-Merlin protocol for knottedness, but unfortunately there was an error and they withdrew their claim. [Kup11] showed that, assuming the Generalized Riemann Hypothesis, knottedness is in $\mathsf{NP}$, and mentions that this also puts knottedness in $\mathsf{AM}$, but I’ll be honest I don’t know how to translate this into the above framework; the $\mathsf{AM}$ protocol of [Kup11] I think involves finding a rare prime $p$ modulo which a system of polynomial equations is $0$. The prime $p$ is rare in that $H(p)=0$, and the system of polynomial equations corresponds to a representation of the knot complement group. Of note, see this answer to a similar question on a sister site, which also addresses the utility of such "useful" proofs-of-work. 

I'm having trouble determining whether the authors state the asymptotic randomness used and queries made for this PCP verifier. I see that Lemma C.8 (on page 134) gives an expression for the number of queries used and amount of randomness used. However, I would be grateful if someone could interpret this into a big O notation formula. 

The title of this question does not match the content. As stated in the comments, there are some NP optimization problems for which any polynomial time approximation algorithm implies P = NP. Maybe you are looking for optimization problems that are not approximable within any polynomial factor, or that exhibit a threshold behavior? You may be looking for polyAPX-hard or polyAPX-complete problems. polyAPX is the class of NP optimization problems that have a polynomial time approximation algorithm such that the measure of the approximate solution is within a polynomial factor of the optimal measure. In Theorem 2 of this extended abstract, you can see that the Maximum Independent Set problem is complete for polyAPX under PTAS reductions. For more information on approximation-preserving reductions and approximability results, check out the compendium of NP optimization problems. 

In the 2012 paper On the Concrete-Efficiency Threshold of Probabilistically-Checkable Proofs, the authors state the following (paraphrased from page 11). 

Theorem 2.2 in "Nondeterministic circuits, space complexity and quasigroups", by Wolf, 1994 (a technical report version is available here without fee), proves that NP = NNC, where NNC is the class of languages decidable by an L-uniform family of nondeterministic NC circuits. A nondeterministic NC circuit is an NC circuit with a polynomial number of nondeterministic input bits. The simulation essentially guesses a tableau of an arbitrary NP machine then verifies that each configuration proceeds from the previous one according to the transition function of the machine. My interpretation of this proof is that the NC circuit is in fact an NC1 circuit, since it simply takes the conjunction of a polynomial number of subcircuits, each of which checks that a local "window" in the tableau is valid. Assuming that interpretation is correct, it should follow that NP = NL, since NC1 is in L, so the same argument will apply. What is the error in my interpretation of this result? Why doesn't the strategy "guess a tableau, verify that each configuration follows from the previous" work? 

A difference between a standard graph non-isomorphism protocol and the present protocol is in step 3. That is, conventionally in step 3, Vicky just randomly chooses any old element $\pi\in S_n$ to apply to the graph $G_i$, but in the above protocol in step 3, Vicky walks along the Cayley graph of $S_n$ with generators $\langle\pi_1,\pi_2,\cdots,\pi_r\rangle$ a total of $t$ times. Notice that because each step of the Markov chain is, by definition, an invertible permutation of the configuration space (the configuration space being the set of all adjacency matrices equivalent to $M_i$), such a walk is a doubly-stochastic Markov chain, meaning each column (resp. row) of the transition matrix sums to $1$. Thus, after the selected adjacency matrix $M_i$ is properly mixed, the matrix that Vicky will present to Peggy is uniformly distributed over all matrices isomorphic to $G_i$. Accordingly, if $G_1\cong G_2$, then Peggy would not have better than even chance of deducing $i$, because she will just be given two random matrices isomorphic to $G_i$; hence, the protocol is sound. A key concern is that we need to make sure that the matrices are properly mixed, that is, that the number of steps $t$ along the Cayley graph is large enough to mix the given graphs to the limiting distribution. I believe, although I can't prove it for now, that for most generating sets $\{\pi_1,\pi_2,\cdots,\pi_r\}$ of the symmetric group $S_n$, $t$ is probably $O(\log n)$. But, now to generalize, given any two elements chosen from any discrete configuration space, not just adjacency matrices representing graphs, I think we have a sound zero-knowledge protocol to show that the two elements are not in the same equivalence class, as long as we can construct a fast-mixing doubly-stochastic Markov chain to walk through the configuration space. 

Consider the following variant of a zero-knowledge proof that two graphs, $G_1$ and $G_2$, given by adjacency matrices $M_1$ and $M_2$, respectively, are not isomorphic. Here Peggy the prover wants to show Vicky the verifier that $G_1 \not\cong G_2$, where both $G_1$ and $G_2$ have $n$ vertices. 

References: [GMW85] Oded Goldreich, Silvio Micali, and Avi Wigderson. Proofs that Yield Nothing but their Validity, 1985. [GS86] Shafi Goldwasser, Michael Sipser. Private Coins versus Public Coins in Interactive Proof Systems, 1986. [HTY05] Masao Hara, Seiichi Tani, and Makoto Yamamoto. UNKNOTTING is in $\mathsf{AM} \cap \mathsf{coAM}$, 2005. [Kup11] Greg Kuperberg. Knottedness is in $\mathsf{NP}$, modulo GRH, 2011. 

If $d_1$ is the largest number for which $G_1$ has a subgraph of minimum degree $d_1$, and $d_2$ the largest such number for $G_2$, then the largest such number for $G_1 \oplus G_2$ should be $d_1 + d_2$. In other words, $$\max_{H \subseteq G_1} \delta(H) + \max_{H \subseteq G_2} \delta(H) = \max_{H \subseteq G_1 \oplus G_2} \delta(H),$$ where $\delta(H)$ denotes the minimum degree of the subgraph $H$. $|G_1 \oplus G_2| \in O(|G_1| + |G_2|)$. $G_1 \oplus G_2$ is computable in logarithmic space. 

I have two optimization problems, both of whose inputs are from the set $I$ and whose solutions are from the set $S$, one a minimization with objective function $m_{\min}$ and one a maximization with objective function $m_{\max}$. I am studying what I would like to call the "join" or "direct sum" of these two problems, the problem of maximizing $m_{\max}(x, w) - m_{\min}(x, w)$, though I don't know the correct name for such a problem. In my particular case, each problem on its own has a trivial algorithm that produces optimal solutions, but together, the problem is intractable. Has the structural complexity or approximability of optimization problems of this form been studied in a general way? (The fact that one is a maximization problem and one is a minimization problem is not crucial here, they could both be the same type of optimization.) 

I'm looking for a binary graph operation $\oplus$ that has the following properties for all undirected graphs $G_1$ and $G_2$. 

Like the Boolean Formula Isomorphism problem, the Group Equations Isomorphism problem is $\mathsf{coNP}$-hard and in $\mathsf{\Sigma_2P}$, for any fixed non-abelian group. See The Complexity of Equivalence and Isomorphism of Systems of Equations by Gustav Nordh (2004) for more information. 

What do you call the problem of finding a largest possible subset of strings with smallest possible information content? I'm studying a particular instantiation of this problem in a different setting and would like to know about this more abstract problem. In terms of Kolmogorov complexity, this would be the following decision problem. 

Edit: consider the complexity measure to be one of the resource-bounded Kolmogorov complexity measures (for example, the length of the shortest program that outputs a given string, with an additive penalty of the logarithm of the running time of the program) to make the problem computable, as suggested in the comments. 

In the traditional oracle Turing machine, the oracle is specified as a decision problem. Roughly speaking, one puts a string in the oracle tape, and asks whether it is true or false. I am wondering whether it makes sense to consider a functional oracle. This means, one puts a string $x$ in the oracle tape, and guarantee the returned result $f(x)$ is polynomially bounded in length by $x$, and the oracle is supposed to return $f(x)$. A very natural such an oracle is an FNP oracle, and one can define a class $P^{FNP}$. Any study regarding this? Or the notion is not well defined? Any comments are welcome. 

First of all, I am not sure whether this is a research level question. Please let me know if it is not. The question is about streaming algorithms for the sum of the given data stream. From literature, it is common to consider the questions related to the frequency vector, say, $\vec{m}$, and one has standard algorithms to approximate the p-norm of such a vector. However, a very natural question is, is it possible to approximate the sum of the stream. More formally, suppose we have a frequency vector $\vec{m}$ for the "value vector" $\vec{a}$, i.e., each $a_i$ appears $m_i$ times in the stream, the sum is $\sum a_i\cdot m_i$. (Of course, we assume each $a_i$ is within some interval $I$, as otherwise the question might not be very interesting.) I noticed that Braverman has papers regarding approximate $\sum_i G(m_i)$ for reasonably general function $G$, but here, with different index $i$, $G$ varies. Hence it is not clear to me how to apply. 

Given a complete DFA $A=(Q, \Gamma, \delta, F)$, we can define a collection of functions $f_a$ for each $a\in \Gamma$and with $f_a:Q\rightarrow Q$, $f_a(q)=\delta(q, a)$. We can generalize this notion to a word $w=a_1, \cdots, a_m$ and $f_w=f_{a_1}\circ \cdots \circ f_{a_m}$ where $\circ$ denotes function composition. Furthermore we denote $G=\{f_w\mid w\in \Gamma^*\}$ and $G$ is monoid. [$G$ is usually called transition monoid in the standard textbook, but here I reproduce the definition for clarity.] The question is, given a function $f:Q\rightarrow Q$, can we decide $f\in G$ (ideally in polynomial time), and if this is the case (i.e., there exists a $w$ such that $f=f_w$), whether $w$ is only polynomially long, or can be exponentially long? [I guess that indeed such a word could be exponentially long, but I am looking for a simple example.] 

This is a partial answer; maybe it will inspire someone else to provide a better one. $\newcommand{\EBPP}{\mathsf{EBPP}}$ $\newcommand{\CP}{\mathsf{C}_=\mathsf{P}}$ Your class $\EBPP$ is a special case of $\CP$. I think one way of defining $\CP$ is as follows (see Section 2 of this paper). A language $L$ is in $\CP$ if there is a polynomial time verifier $V$ such that 

I believe the Cartesian product for graphs satisfies the first and the third conditions, but the size of the product graph in this case is multiplicative (specifically $O(|G_1| \cdot |G_2|)$), not additive. I will be performing this operation a polynomial number of times, so a multiplicative increase in size is unacceptable. Edit: changed condition 3 to require a logarithmic space computation instead of a polynomial time computation. Edit: changed condition 1 to require the maximum minimum degree over all subgraphs should be preserved. 

(The completeness probability can essentially be any fixed fraction; I chose $\frac{3}{4}$ so that it matches the probability given in your question.) One way of defining $\EBPP$ is as follows. A language $L$ is in $\EBPP$ if there is a polynomial time verifier $V$ such that 

In Section 3.2 of On Syntactic versus Computational Views on Approximability by Khanna, et al., the authors state that an adaptation of the results from Proof Verification and Hardness of Approximation Problems by Arora, et al. proves that there is a polynomial time computable gap-introducing reduction from SAT to Max 3-SAT, and furthermore, that there is a polynomial time computable function that computes a satisfying assignment for an instance of SAT given an assignment satisfying strictly greater than $1 - \epsilon$ of the clauses of the transformed instance of Max 3-SAT, where $\epsilon$ is the gap. From what I understand, the authors claim that such a function exists because the proof by Arora, et al. utilizes an error-correcting code, so the original message (the assignment satisfying the instance of SAT) can be reconstructed from a corrupted message (the assignment satisfying greater than $1 - \epsilon$ of the clauses of Max 3-SAT). This function for reconstructing a satisfying assignment is critical for proving hardness results in classes of approximable optimization problems under approximation-preserving reductions. Does The PCP Theorem by Gap Amplification by Dinur also imply the existence of such a function?