It is misleading because argument means different things depending on the value of flag , and there is no comment in the code indicating so. If is , then is the input of the sigmoid. If is , is the sigmoid of the input. We can deduce it from its usage in the code. Here the author uses to compute the sigmoid: ; and here the author uses to compute its derivative: . As you can see, in the second case the author supplies as input to , and is the output of the previous invocation to , that is, is the sigmoid of the input. 

According to your description of the data, it is highly probable that training any neural network of reasonable size will overfit the training data. One option is to apply any possible regularization approach; these come to mind: 

I think the closest problem that has been addressed with deep learning is image inpainting, that is, filling a blacked out region in the image: 

In normal RNNs, you can train using either back-propagation through time (BPTT) or teacher forcing. With BPTT the network receives as input for each time step the output of the previous time step. In teacher forcing, the network receives the gold data tokens directly; this induces exposure bias on the trained network. With textual GANs, you normally don't feed the gold data to the generator, because the generator is generating the sequence so there is no gold data to use. In the article you refer, they are taking subsequences from real data, feeding them to the generator and having it only generate the final token. Then the discriminator receives the concatenation of the real sequence prefix and the generated final token. This is what they call "teacher helping". 

For your problem the effort vs benefit tradeoff does not seem to be in deep learning's favour. DL would be an overkill 

I think it depends on what algorithm you are using to optimize the loss. If you are using the normal equations to solve $X^{T}X^{-1}$ exactly you'll face problems because in the case of perfectly correlated columns the inverse does not exist. On the other hand if you are using numerical methods like gradient descent you might actually be OK. 

Assume that $A_{m \times n}$ and $B_{n \times k}$ are to be multiplied to get $C_{m \times k}$. Now if $n$ is too large for a single row $A_{j}$ of $A$ to fit in RAM (and similarly for columns of B) on a single compute node how do we perform the multiplication? 

You need to use an algorithm that does not load all examples in memory at the same time. Use Stochastic Gradient Descent (a good implementation is Vowpal Wabbit). 

Distributed representations (Glove) based on training on a large corpus are directly available from Stanford NLP group. You can use those word embeddings directly in your application (instead of using 1 hot encoded vectors and then training the network to get the embeddings). If your task is not too specialized starting with this set of embeddings will work well in practice. It will save you from training an additional $m \times V$ number of parameters where $V$ is the vocabulary size and $m$ is the dimension of the embedding space you want to project into. 

Let's say that I am an email provider like Gmail. Let's assume that I have two categories of email-address: spammers and non-spammers. When my servers receive a mail I need to quickly check if that email-ID is in the spammers set and if yes I take some action. The problem is that each email-Id could be several bytes ( say 10 bytes each) and I might have 1 billion spammers so I need 10GB of RAM to just store the emails in main memory. Let's say I want to use only 1 GB RAM. To do this I'm now ready to accept an approximate answer. In particular I'm ok with a non-spam email mistakenly flagged as belonging to the spam set but not vice versa. How will I do it? 

In the linked video, a polynomial is fitted to some data. The degree of the polynomial has to be chosen, but it cannot be trained. It is therefore a hyperparameter. In order to choose the degree, several instances of the algorithm, each one trained with a polynomial of different degree, have to be evaluated on a data set that was not used for training. However, if we used the test set for assessing the values of hyperparameters, we would also be coupling the resulting model to the data in the test set, as the chosen hyperparameter (and therefore the whole model) would depend on the test set data. The test set should only be used for the final evaluation in order to know how your algorithm is going to behave on data that has never been seen before by it. This is why there is normally a validation set used to tune the hyperparameters. 

simply recommend the most popular movie in a time window around the current moment (maybe taking into account momentum), do the same but marginalizing over account info, e.g. age, gender, country, language; also, IP location is an option to get such type of info. 

Confidence assessment can be done based on recommendation success rate over the results of the same recommendation to previous users with the same profile characteristics, if such data is available. 

Another option could be to model your problem as a bayesian linear regression where $x_i$ are random variables that follow a Bernoulli distribution with probability $p_i$, that is, $x_i \sim Bernoulli (p_i)$, and $v \sim \mathcal{N}(x^T b, \sigma^2)$. The posterior distribution of $p_i$ would enable you to tell the values of $x_i$ that most likely fit with the observed data. These posterior distributions may be inferred via MCMC or variational inference. The three main frameworks to implement bayesian models in Python are pymc3 (example), edward (example) and pystan (example). The three of them allow straighforward implementation of bayesian linear regression as formulated above. 

The Neural Network language Model (NNLM) by Bengio et.al is a structure extensively used in machine translation, text summarization based on deep learning. What's the computational complexity of this model? Knowing the complexity in terms of number of parameters helps in choosing the size of the training set and in determining what compute infrastructure is required. 

Deep learning approaches have been particularly useful in solving problems in vision, speech and language modeling where feature engineering is tricky and takes a lot of effort. For your application that does not seem to be the case since you have well defined features and only feature interactions etc. are required. Given that deep learning models currently need a lot of computing resources and scientist time in coding stuff up I'd suggest opting for a non-deep learning approach. 

Use logistic regression which allows the weights to be learnt. By using cosine similarity you are forcing the weights to be the same for all features (assuming that you normalize the features first). This is putting unnecessary restrictions on the model. 

The matrix which creates the embedding for each word in the context . This distributed embedding is in $\mathbb{R}^{m}$ space. This matrix is $C :{V \times m}$ The matrix which transforms the concatenated list of word embeddings (active in the current context of size $n-1$) to the hidden layer (of size $h$). This matrix is $H:{(n-1)m\times h}$ The matrix which maps the hidden layer to unmormalized probabilities for each word in the vocabulary. This matrix is $V :{V \times h}$ The matrix connecting the context word embeddings to the output layer. This connection is optional and has dimensions $W:{V \times (n-1)m}$ Note: while training using SGD, for a single example only $n-1$ words in the context are active out of words $V$ in the vocabulary. I have also omitted the parameter vectors for the bias terms $b:V \times 1$ when computing unnormalized outputs and when computing the hidden layer ($d: h \times 1$). 

You don't need word embeddings. Actually, in neural machine translation is frequent not to use them and simply train the embeddings along with the task. Nevertheless, word embeddings work as a data augmentation technique, as you normally use a different (and much larger) dataset to train them, so they can be useful when you don't have much training data. Therefore, the decision of using pre-trained word embeddings or not should be driven by the available data. 

First, a note: don't let the graph structure make you think that this can be addressed with neural networks; this structure only suggests that this is a hierarchical model. Second, let's take into account the following: 

There is something called "sampled softmax" (e.g. tensorflow's implementation), which simply partitions the output space and at each training step only takes into account one of the partitions (see section 3 from this article to learn the math). Sampled softmax is only meant to speed up training for very large output spaces; at inference time you use normal softmax. Also, if your output is of hierarchical nature, you can use hierarchical softmax. See this blog post for a review of different methods to handle very large categorical space. 

The discriminator must classify individual elements as being fake (i.e. created by the generator) or real (i.e. taken from the training dataset). The discriminator generates labels (real/fake) for each element in the batch. The loss functions are computed based on those labels. Elements are fed to the discriminator in batches of the same type (i.e. all elements in the batch are real or all elements in the batch are fake). This is because the fake data batch is directly generated by the generator as part of the same computational graph (i.e. the output of the generator is directly connected to the input of the discriminator). This is so to be able to propagate gradients of the generator parameters through the discriminator. 

Based on what you describe, maybe your problem setting is a multi label setting. E.g: the customer's email might be about an enquiry about two aspects. Say a question about returns policy and price. If that's indeed the case you should try to train multiple binary classifiers like "email about price"and "email not about price" instead of explicitly trading off the categories against each other. If you have $k$ different categories you'll end up with $k$ classifiers. Then while predicting on test data you can take the union of the labels output by the $k$ classifiers as the predicted label. Please see "PT4" in Multi-Label Classification for more details. 

I don't think you can split the combinations and use it for prediction. E.g: I like fish n chips as my first choice but neither fish nor chips might be in my top 10 choices. A better idea is to use what's called "support". Do some preprocessing to find out which of the $(75)^6$ combinations occur at least say 100 times in your dataset. Only use these combinations for training and prediction. You want to identify generalizable patterns in people's choices not events which might have occurred just by chance. 

Use vowpal wabbit for building your regression or classification models. It'll train at the speed at which you can read your data 

Assuming you have an RDD each row of which is of the form , you can do . This is for a basic RDD If you use Spark sqlcontext there are functions to select by column name. 

Several times it does happen that interactions among variables improve the bias of the model. This is especially true when the effect of one independent variable on the target depends on the values of other independent variables. I don't think there's anything mathematically incorrect in doing this. E.g: let's say you are trying to predict revenue as a function of advertising. In this example it's reasonable to assume that the effect on revenue of one extra unit of advertising on Television would depend on the existing level of advertising on Facebook (say). The true data generating function (if you had access to it) might be something like: $$Revenue=Ad_{TV}^{\beta_{1}}Ad_{Print}^{\beta_{1}}...$$ where $Ad_{TV}$ is the # advertisements shown on TV etc. If you use provide the model the opportunity to handle such interaction terms you will be closer to modeling the true data generating function. However adding more feature increases the complexity (capacity) of the model and you might have to use regularization wisely to prevent over-fitting