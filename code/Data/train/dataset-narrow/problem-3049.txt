Obviously you may need to change which columns you drop or use as the training target. The above was for a Kaggle competition, so there was no target data for (it is held back by the organisers). 

Yes. This will give you the most accurate measure of performance in the task you want to use the network in, in order to choose the best generalisation and take it forward to testing. You would only use a simple random split if the end goal of your trained network is to recognise expressions from images of the people in the training set. 

The first variant is the second variant, or more accurately there is only one type of backpropagation, and that works with the gradients of a loss function with respect to parameters of the network. This is not an uncommon point to have questions about though, the main issue that I see causing confusion is when the loss function has been cleverly constructed so that it works with the output layer activation function, and the derivative term is numerically $\hat{y} - y$, which looks the same as taking the linear error directly. People studying the code implementing a network like this can easily come to the conclusion that the initial gradient is in fact an initial error (and whilst these are numerically equal, they are different concepts, and in a generic neural network they don't have to be equal) This situation applies for the following network architectures: 

I have done the opposite of your problem - I have written code to implement shipping costs for e-commerce sites which runs on their sites. Shipping cost rules can be almost completely arbitrary logic. In the general case, you have no good choice but to implement them as logic that is assessed per order. That means your optimiser will have to run that logic for every combination of purchases it considers. Which makes this a planning/combinatorics problem. This may not be so bad - e.g. for purchasing 10 items across 5 stores you have $5^{10}$ combinations which is 9765625. That is possible by brute force. For larger orders, or more choices of store, you may want to look at dynamic solutions to optimise cost, such as simulated annealing, genetic algorithms. Reinforcement learning run per order may also work. 

and then use a linear regression optimiser (sci-kit learn's SGD optimiser linked). With the above data this should quickly tell you $b=1, a,c,d=0$. But what it won't tell you is whether your model is the best possible or somehow "correct". You can scan for more possible formulae by creating more columns - any function of any combination of inputs (if there is more than one) that could be feasible. However, the more columns you add in this way, the more likely it is you will find an incorrect overfit solution that matches all your data using a clever combination of parameters, but which is not a good general predictor. To address this, you will need to add regularisation - a simple L1 or L2 regularisation of the parameters will do (in the link I gave to scikit-learn, the argument can control this), which will penalise large parameters and help you home in on a simple formula if there is one. 

Yes you can use deep learning techniques to process non-image data. However, other model classes are still very competitive with neural networks outside of signal-processing and related tasks. To use deep learning approaches on non-signal/non-sequence data, typically you use a simple feed-forward multi-layer network. No need for convolutional layers or pooling layers. The best architecture other than that needs to be explored with cross-validation, and can be time-consuming to discover as deep NNs take a lot of computation to train. In my experience attempting to use deep(-ish, typically ~ 5 layers) neural networks in Kaggle competitions: 

Theory from item-by-item calculation, converted to matrix math But why is it $W^{[2]T}$ and not some other matrix with the desired dimensions? For that we need to go back to the chain rule. Note that the expression you have written is actually two steps of the chain rule combined - first we derive $da^{[1]}$ from $dz^{[2]}$, then we derive $dz^{[1]}$ from $da^{[1]}$. It is that first step that introduces terms from $W^{[2]}$, so I will just show that. Also, deriving this directly using matrix notation is more complex, so we'll just drop into using the index values, and use the "official" partial derivative notation. To calculate $da^{[1]}$ for a specific $i^{th}$ neuron, you have to sum over the gradients $dz^{[2]}$ for all the neurons it links to: $$ \frac{\partial J}{\partial a_i^{[1]}} = \sum_j \frac{\partial J}{\partial z_j^{[2]}} \frac{\partial z_j^{[2]}}{\partial a_i^{[1]}} = \sum_j \frac{\partial J}{\partial z_j^{[2]}} W_{ij}^{[2]} $$ Doing this for each $i$ value in turn gets your the full $da^{[1]}$ vector. Compare this to how the forward propagation works in the same network using the same notation and indexing: $$z^{[2]}_j = b^{[2]}_j + \sum_i W_{ij}^{[2]} a_i^{[1]}$$ And you can see that the index used to drive the sum is different between forward and backward calculations: 

There is no a priori best approach based on number of output classes. The "best predictions" vs "smallest amount of data" is also a trade-off, where simpler models will perform better than complex ones on small amounts of data, but the more complex models will cope better with larger amounts of data, and will then give you better predictions. At some point you might have enough data so that sampling more will not improve your trained models, but you need to establish that empirically. Most algorithms allow you to explore the trade-off within them by varying hyper-parameters to make the model simpler for smaller data sets and more complex when there is more training data. 

I'm not sure I fully understand this, but it does not seem quite right. One thing that might be confusing you is having your actions as "move to state". Whilst this is quite normal in many deterministic environments, especially board games, most RL formula and tutorials are written using separate state/action pairs, so whilst getting this straight in your mind, best to find another way to represent actions - e.g. move piece from X to Y, or place new piece at (I,J) . . . you can return to "change state from A1 to B2" as the representation later. This is actually more efficient and is called the after-state representation, but most of the literature will show state-action value functions. 

You can use the dataframe's method to access raw data once you have manipulated the columns as you need them. E.g. 

You are confusing "dimensions" with "order of tensor". A softmax with 256 different categories is a 256 dimensional vector, but is also a tensor with order 1 (whilst a matrix is a tensor of order 2). The paper is using the technical terms correctly, so the 256 dimensional vector is just a normal vector with 256 scalar entries. Therefore a 256-dimensional softmax in TensorFlow is typically an output layer that looks something like this: 

The above is rough pseudo code, so typically all the functions above have different names, or are multiple lines that do the same thing that you might not bother to encapsulate into a re-usable method if you are writing a quick script. The part I have shown that splits the features might be built in to the training function, and it is also common that the training process can use the test data to help monitor progress. If the loading and conversion takes a long time, you might do it in a separate script and save the resulting NumPy array in a separate file to load it quicker next time. So the part you are concerned about is how you might build a section of your code from the starting strings. The answer is to use whatever values you can extract from your strings that might be relevant. The string length might be a simple start i.e. - but you can also look into any other measure you can figure out (e.g. number of vowels, which uncommon bi-grams are in the word). Deciding which features to try and testing between them is feature engineering, and this often involves some creativity. The important thing is that the features must all be numeric. For a neural network, you should also try and make them relatively small and/or convert them to have mean 0, standard deviation 1 before going to next stage. When you use the network to make predictions later, you have to repeat most of the pipeline: 

So it essentially uses the parameter as an element-wise multiplication. The parameter update gradient calculation in the project code is feeding in the already-calculated value of $\frac{\partial \mu(s|\theta)}{\partial \theta}$ as a placeholder, and using it for this third param. So the multiplication of the two gradient factors occurs within . 

In terms of the maths, if your loss is $J$, and you know $\frac{\partial J}{\partial z_i}$ for a given neuron $i$ which has bias term $b_i$ . . . $$\frac{\partial J}{\partial b_i} = \frac{\partial J}{\partial z_i} \frac{\partial z_i}{\partial b_i}$$ and $$\frac{\partial z_i}{\partial b_i} = 1$$ because $z_i = (\text{something unaffected by } b_i) + b_i$ 

This looks like almost an ideal use case for a Recurrent Neural Network (RNN) model. I say almost, as your output variable is 2 separate class vectors, that part is slightly fiddly. The main advantage of using a RNN is that the model would be built on the assumption that is of the same type as etc. Whilst with other models you could flatten the time series into a 10 x 20 = 200 long vector, and the output likewise into a 50 long vector, and treat the whole problem as a single step supervised learning, such a model would not include the same assumption. For your problem, the RNN would have 10 inputs (corresponding to the 5 classes of each of 2 variables) and 10 outputs. You would train it by presenting your input sequence one sample at a time in the correct order 1 to 20, then continue running the network 5 more steps, comparing the outputs against expected. When predicting, you do similar, but then just read off the outputs as predictions, one at a time. There are a few different Python libraries that support RNN models, and there are different internal choices (such as using LSTM layer versus GRU) which you might want to explore. I am currently learning the Keras framework, which supports some options for RNNs. Probably the example code which learns to predict an exponentially-decaying sine wave would be a good place to start trying to understand the Keras model - although that's a regression, it should give you some insight into how RNNs can be built for this kind of series-based data. 

For a fully-connected network the precise order of features does not matter initially (i.e. before you start to train), as long as it is consistent for each example. This is independent of whether you have an auto-encoder to train or some other fully-connected network. Processing images with pixels as features does not change this. Some caveats: 

Note that the way you are thinking does turn up again when using function approximators (e.g. linear functions or neural networks) in semi-gradient versus "true gradient" methods, where instead of this being an issue with which Q values to use, it is an issue with calculating gradients due to the TD error, when your TD target is based on the same parameters that you are are taking the gradient for. In semi-gradient methods, this issue is ignored, and the methods still work OK. However, the "true gradient" methods are more theoretically correct. 

To the call to . With this change, you can try with regularisation term set to zero. When I do this, I get: 

In order to learn optimal value of $b$, for one example the gradient you need is $\frac{\partial L}{\partial b}$. We can get that by expanding the loss function: $L = \frac{1}{2}(y-\hat{y})^2$ $L = \frac{1}{2}(y^2 - 2y\hat{y} + \hat{y}^2)$ We could expand further, but typically now we calculate $\frac{\partial L}{\partial \hat{y}}$ and use the chain rule, because that has a simpler, more intuitive-looking result. Terms without $\hat{y}$ are zero: $\frac{\partial L}{\partial \hat{y}} = \hat{y} - y$ We want $\frac{\partial L}{\partial b}$ for gradient descent $\frac{\partial L}{\partial b} = \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial b}$ (Chain rule) $\frac{\partial L}{\partial b} = (\hat{y} - y)(\frac{\partial}{\partial b} a + bx_1x_2)$ Again, terms without $b$ in them are constants: $\frac{\partial L}{\partial b} = (\hat{y} - y)(x_1x_2)$ Note that the $x_1x_2$ term is unchanged from the input. It could be any function $x_n = f(x_1, x_2, x_3 ....)$ 

So when you are told that backpropagation processes an "error signal" or "the error" backwards through the network, just mentally add "the gradient of" to the start of the phrase. Some people will say it knowingly as shorthand, others might be honestly confused. The same applies to deeper layers, although then there is no other source for the confused "this is the error being distributed" other than as shorthand for "this is the [gradient of the] error being distributed". 

That's because in usual training scenarios, you are creating a network that predicts an output value. You can calculate the right corrections for training data, but not when predicting, because the whole point of predicting is to estimate a label that you do not already have. As such, you want to alter your function parameters, and not interim values. 

NB - I have assumed you have fixed the gradient direction here. Classification Loss Function MLPClassifier is running a classifier using logloss, which is more efficient loss function that the mean squared error you are using, if your targets are class probabilities. You can use this too, simply by changing: 

Here is a gist of my solution in Python/Keras. Summary of architecture in the gist: 4 times Conv2d layers with 8 feature maps and 5x5 kernel. Dropout 25%. 4 more times Conv2d layers with 16 feature maps and 5x5 kernel. Dropout 50%. Fully connected layer with 256 outputs. Dropout 50%. Fully connected later with 128 outputs. Dropout 50%. Single output neuron. All layers use ReLU activation, except output which is linear. The example re-sizes images to 50x50 for speed. However, using 100x100 images works fine. I added a couple of convolutional layers to it for that (one each for the 8 features and 16 features section), and get a reasonable loss of ~0.00025 on a validation set. It looks possible to get a lower loss still with more training examples, more epochs and perhaps more feature maps. Many of the predictions were very good, close to the actual value. Interestingly, the worst errors were for short bricks predicting close to a 2:1 or 3:2 ratio with the ground truth, implying that the network had found correct structure but was being confused by the periodic nature of the bricks. I think this is something that could be resolved with more training examples (and perhaps more features in higher layers) and it should be possible to get much lower loss metric on brick height. I think that the corresponding tiny-cnn structure looks like this: 

This is not strictly true. A softmax vector is one possible way to represent a policy, and works for discrete action spaces. The difference between policy gradient and value function approaches here is in how you use the output. For a value function you would find the maximum output, and choose that (perhaps $\epsilon$-greedily), and it should be an estimate of the value of taking that action. For a policy function, you would use the output as probability to choose each action, and you do not know the value of taking that action. 

A minimum operation is differentiable, or at least you can easily express the partial derivatives w.r.t. its inputs: $f = min(x_1, x_2, x_3 ... x_n)$ $ \frac{\partial f}{\partial x_i} = \begin{cases} 1,& \text{if } argmin_i(x_i) = i\\ 0, & \text{otherwise} \end{cases}$ This does not hold strictly when multiple values share the same minimum value. However, that is not a problem in practice for gradient-based optimisers in TensorFlow, which can simply set all tied indices to have partial derivative of 1 (or a fraction $\frac{1}{n_{min}}$), with little impact to the eventual result, because ties for values will happen rarely. Ties may happen frequently enough in a ReLU-based network that the TensorFlow developers have considered a best response for them - I don't know specifically what TensorFlow does for that situation. 

Those are the ones I know, and I expect there are many that I still haven't heard of. It seems that log loss would only work and be numerically stable when the output and targets are in range [0,1]. So it may not make sense to try linear output layer with a logloss objective function. Unless there is a more general logloss function that can cope with values of $y$ that are outside of the range? However, it doesn't seem quite so bad to try sigmoid output with a squared error objective. It should be stable and converge at least. I understand that some of the design behind these pairings is that it makes the formula for $\frac{\delta E}{\delta z}$ - where $E$ is the value of the objective function - easy for back propagation. But it should still be possible to find that derivative using other pairings. Also, there are many other activation functions that are not commonly seen in output layers, but feasibly could be, such as , and where it is not clear what objective function could be applied. Are there any situations when designing the architecture of a neural network, that you would or should use "non-standard" pairings of output activation and objective functions?