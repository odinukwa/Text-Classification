Then you can add permissions to that user as it will exist in that database. NOTE: you may want to go into your master database (or whichever database you were running those commands in before) and remove USER1 and USER2 

Internally within a stored procedure there is no way (that I'm aware of) to determine the calling proc. There are two workarounds that I use 1. This one wont actually be identifiable for a profiler run but can be useful in auditing. When calling any SP call it with (into a value called ), this will mean that it will hold the stored procedure ID (which can get you the schema and name) of whatever stored procedure was first called, by anything (be it a person or automated call (note this can technically be spoofed by someone manually adding in the to the initial call) This is quite a bit of work depending on how many things you want to audit. and generates overheads in your db but will get you exactly what your after just in. 2. The easiest way is each of the different routes into your system has its own login, so you have a login for updating users credentials, a different login for another action. This once again can be a lot of work, but will get you the information you want directly from the profiler, it also will make your system a lot more secure if the logins are stored securely as if one login is compromised then there is only that very specific are is effected someone doesn't have access to your entire system Aditionally depending on how it is being called there is the potential (if you're using php this isnt possible if you have a more complex back end with iss / .net in the background you may be able to set the program_name field for the login, once again this will show up in the profiler and if you have the proc name your calling you can amend that into the program name ( for example) Hope this somewhat helps Ste 

Yes, it is possible and it is covered in documentation and in lot of FAQs like the one at iBase.ru For the example at my development box I have co-installed FB 2.1.5 Win32 SuperServer (at default port 3050/tcp) and FB 2.5.2 Win64 Super-Classic (at a custom 3064/tcp) There might be troubles with FB 1.x as it was using registry, but FB 2.x was made isolated and self-dependent. One option is to download ZIPs and unpack them to different folders. Then you have to run text window of Windows Command Prompt "As Administrator", go into "bin" subfolder and there are all those executables like server itself. There also is "inst_svc" tool. Running it with an option like "-?" would show you brief help. Focus on installing main service, not installing Guardian (only needed on Win98, only shipped for legacy uniformity) and giving non-default "instance name". Then go outside "bin" and open "firebird.Conf" with any text editor like notepad. The documentation is within that file how to set non-default TCP port. That's all. Do the same for your another FB folder and you've done it. Another option is to run two installers. The 1st one would do all the described above things automatically. The second one would unpack files - and ask you to do those configuring operations for the second copy manually. Just do it like described above. 

I found no other relevant log entries in the Error Log or Event Viewer. The closest error that happens in the Event Viewer is: 

I am analyzing the size of some tables in Oracle, and I noticed that the primary key occupies almost the same size of its related table. To be clear, my statistics are the following (approx.): 

I am facing a situation that it is being somewhat hard to address. I need help to understand what is happening. TL;DR: Every time the Transaction Log gets full in SQL Server it needs to shutdown the database to enter in Recovery Mode and rollback the offending transactions? Is this always done by design or this only happens when something bad happens? 

Note, are you wanting those default values in there? would make more sense (in my opinion) to have it as a required field on the execution of the proc so you don't accidentally run it with no values and presume its right 

to generate your static random generator start point the $usersrandomnumber acts as a starting seed see the mysql documentation here: $URL$ it will involve you storing the user's seed in a table somewhere, or cookie /local/server storage but will get you what you're after 

I am currently experiencing a situation where an application is sending several identical queries at once (I found this in SQL Profiler). This application is buggy, it is probably in loop somewhere and the responsible team is working to solve this problem. But the same application also provides other services that are essential to the company, so I can't just disable it. I have to live with it, until the team solves the problem. So far so good. But this SQL Server instance also maintains other applications, whose performance are being penalized due this occurrence, some of them timeout their operation which makes this situation a DoS, in practice. My question is: what can I do, in the DB side, to minimize the impact to the other applications? I don't care if the buggy app gets penalized, but I would not like to penalize the other ones. (I can't just disable it, but I can slow down their queries results, if possible.) 

I'm not sure what your dynamic columns are so I cant 100% promise this will work, but if you get any column errors you should just be able to put ts. infront of it to declare its from that table Ste 

You could try putting the remoteData into a temp table before doing the merge. a lot of your time will be taken up with comparisons of data not the actual retrieval so if you run the 

Reads data as a usual SNAPSHOT transaction, thus db errors would effect it. Some db erors would manifest it in read errors (like if DBA changed column type in a way incompatible with data), but other might result in some data being "invisible" and skipped. Backup file is stripped of fast-access metadata and geta a lot smaller, which is good for archives (example: 1.1GB of raw database -> 380 MB FBK -> ~700 MB after restore). In FB 2.x GBak is known to work considerably slower via TCP/IP than via Firebird Service Manager connection. It is told to be amended in forecoming FB 3. Restore is basically recreating database, so it is slow. But it optimizes the database internal layout and saves some space (no more half-empty pages in the middle of the file). Due to Firebird being very liberal in online (during active operations of users) scheme change (the safe approach was enforced in 2.0 but undone in 2.1 after uproar), the traditional backup might be "unrestorable", so the attempt at restoring a FBK file into a spare free disk is a must. Until you proven you can restore that backup, you may consider you don't have it. 

User 2 can see the entry created by user 1 Now user 1 disconnects or runs a forced rollback their entries to the table are removed so if user 2 re-runs their query the data is no longer there 

to catch up with the backlog from the night before (when most of our deletes take place) I'm wondering if there's any way to change the default settings from 5 seconds and 10 pages to say every second or run over 20 pages, is there any way of doing that or should I continue just spinning up multiple cleanup procs to clear out the data, or if there's any other actions that can assist with this Re-indexing runs on most effected indexs atleast once a week (most are every other day) SQL Server 2012 Enterprise SP3_CU8 (upgrading to CU9 tomorrow) on AlwaysOn High Availabilty Cluster also with replication (distribution on a separate server) 

MYTABLE has four columns, and three of them composes the primary key. So the sizes should be correct. 

This error happened about ~18 minutes before the database start the recovery process, and repeated sometimes during the beginning of the recovery. It is somewhat related with the DBA user, but I really don't know what it is (I had no time to ask for the DBA yet). 

The scenario: One of our heavy used production databases, which runs several ETL jobs, and long running table batches, entered in Recovery Mode and became inaccessible for some time. This happened three times this week (this server is on for ~2 years, and we didn't notice this issue in the past). Looking into the errors logs what happened was clear: the Transaction Log was full, the database needed to rollback the transaction, the rollback failed, the database shutdown, and started in recovery mode. The DBA defends this as normal behavior of SQL Server. That is, according to him, every time the transaction log gets full and a transaction needs to rollback the database will enter in Recovery Mode due to the lack of log space. After the rollback (that can only be done in Recovery Mode according to him), the database will become available again. I found no reference for this info. So I strongly disagree. I would really appreciate if someone convince me that I am wrong. My point: As far of my knowledge, a DBMS is built to manage/run queries. If it lacks space, the query will fail. Simple as it is. And I am not talking about performance of anything else, but availability only. It makes no sense for me to accept that a DBMS needs by design to shutdown itself to rollback any transaction. In my understanding, it does not matter if I am running tons of queries or if the queries are bad designed. The bad queries should fail and life continues. Doesn't it? My guess is that something else is making it fail, and I need to track what is happening. Is my understanding wrong or this is really how SQL Server is designed to work? Supposing I am not wrong, what else can I do to track the source of this issue? 

Your transactionlog backup doesn't clear out the transaction log while the full backup is taking place, this is down to how the backups are organised requiring a full backup to then have the transaction log to work from. When you start a full backup that is the new point in time that the tLog backups will work from, but they cant work until that backup is complete, so during this time the transaction log grows, once the full backup is complete you now have a new point in time and the tLog backups will now be able to actually release the data from the transaction log and it will return to its normal usage levels 

Back-up is very fast (just dumping of changing pages), can use cascading (database -> monthly large snapshots -> daily deltas from last monthly -> hourls delta form last daily). However it does not optimize (by recreating) the database. And it does not detect database errors other than top-level, errors of pages allocation (like "orphane page") OTOH the pages snapshotted are mostly intact, so in case of partially corrupt database they might still have a manually salvageable data. If the corruption would be noticed quickly. In a sense, that amounts to safe and incremental copying of he database file(s), wit hall the cons and pros.