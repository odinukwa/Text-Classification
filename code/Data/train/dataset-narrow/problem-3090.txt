@sebastianspiegel is correct and you should accept their answer. That said, there's something to be added in terms of how you might want to practically go about achieving this. Most NMF libraries give you the option of providing a pre-trained factor/basis matrix, as well as some flag to indicate that you want the algorithm to omit the update step on that matrix. This allows you to take a pre-trained $T$ matrix, and fit a new $H$ matrix to it based on your new data. There are a couple of nice benefits to this: 1) This factorisation should be much, much quicker to run because your $X$ matrix is now a $m \times 1$ matrix, rather than $m \times n$ 2) The features from your previous factorisation should be the same features in the current one, (i.e. if you're factorising movie ratings, and your first factorisation's fifth feature was "scary movies", the feature spat out by this procedure should also be "scary movies") 

This assumes you’re happy with neural networks. If you’re not, this answer probably isn’t of much use to you. Firstly, a little bit about anomaly detection via autoencoders. Apologies if you’re already familiar with this. An autoencoder is a neural network which learns to reproduce its own input when compressed through a “bottleneck” layer. For example, you may want to find a lower-dimensional feature representation of a set of 100 x 100 images. Your neural network architecture has an input layer of 10,000 elements, an output layer of 10,000 elements, and one of its hidden layers will be relatively narrow compared to the input space: say 100 nodes. The objective is to train the network to produce an output as close to the input as possible, while throwing away all but 100 nodes’ worth of activation. You are trying to produce as lossless a compression of the input as possible, so those 100 nodes should be a very information-rich representation of the kind of data you trained it on. “How does this have any bearing on anomaly detection?” I hear you ask. Well, if you train your autoencoder on non-anomalous data, it will learn a non-anomalous lower-dimensional feature representation. This will mean the reconstruction error from pushing something through the autoencoder will be lower for data similar to what it was trained on than it will for other arbitrary data. If it receives input that is substantively different from what it was trained on, the reconstruction error will be higher. So, given a set of novel inputs, those inputs with the highest reconstruction error are the most anomalous, as they are poorly reconstructed from the non-anomalous feature representation. If your data has a temporal structure, and you have plenty of training samples, you might want to consider constructing an autoencoding LSTM. An LSTM is a neural network architecture for encoding and decoding sequentially dependent data, and a full description of how this works is beyond both the scope of this post and my own abilities. There are many magnificent resources available online for getting to grips with this. Here is a relevant paper on using LSTMs for anomaly detection in time series in general. It may be that LSTMs are unnecessary for your purposes if the data isn’t strongly sequentially dependent. 

has you covered. There aren't a lot of great examples of Poisson regression in the statsmodels API, but if you're happy with GLMs, statsmodels has a GLM API which lets you specify any single-parameter distribution, including Poisson. 

Imagine I'm conducting an ongoing poll asking people's favourite animal out of a list of animals, etc. I want to provide an interface that lets people query this poll data to see the relative popularity of each animal by different demographics. For example, querying the general population might reveal the plurality of respondents (36%) prefer penguins, but querying the 18-25 age-bracket might the plurality of respondents in that cohort (41%) prefer cats. It's desirable to preserve the privacy of my respondents' animal preferences as much as possible. However, an attacker may be able to use prior knowledge of a given respondent to deduce their response by asking a specific enough series of queries. I wish to limit an attacker's ability to do this by noisifying the data presented to those querying the data. As such, I want a procedure that pseudorandomly adds or removes a fraction of a percentage point from each category, but preserves their relative ordering. I also wish this procedure to be deterministic over the same set of data (though this can easily be achieved by using a fixed seed in the pseudorandom procedure). Formally, I want $$f : \mathbb R_{>0}^n \rightarrow \mathbb R_{>0}^n;\;\; f(\mathbf{x}) = \mathbf{y}; \;\; |\mathbf{x}| = |\mathbf{y}| = 1$$ where $\mathbf{x}$ is the vector of proportions of each category. One naive way of going about this would be to simply add a pseudorandom Gaussian noise vector to the original vector and then renormalise. This poses at least two problems: 1) the "zero problem": if a cohort has zero people who like cats, how should the noisification procedure treat this? I'm inclined to say it should maintain the value at zero, but I can't think of a principled way of achieving this 2) the variance of the noise should ideally be the same for all elements in the vector, but any obvious procedure for forcing positivity would all typically result in smaller variance of noise for smaller values, so the noisification would end up making large values larger and small values smaller after renormalisation. I feel like this should be a problem people have encountered before, but I can't find it in the literature. 

Consider a matrix, $\mathbf{V}$, where each row corresponds to one of $m$ electoral district and each column corresponds to one of $n$ candidates or political parties. Each element, $V_{ij}$, is the proportion of votes cast in district $i$ for party $j$. Now imagine taking leave of your senses and deciding to carry out a low-rank matrix factorisation of $\mathbf{V}$. It turns out that regular non-negative matrix factorisation minimising generalised KL-Divergence will preserve the row and column sums in the approximation1, so where the rank of your factorisation, $k$, is less than $n$, it's straightforward to find $k$ latent variables for each electoral district that describe contributions to the share of each party's vote in that district. My questions: 1) Are there any obvious reasons I shouldn't be doing this? 2) Is there a sensible interpretation of these $k$ variables? 3) Is there a more canonical method for factorising matrices like this? Or a more canonical method for finding latent variables for data like this? 

Autoencoders are a neural network solution to the problem of dimensionality reduction. The point of dimensionality reduction is to find a lower-dimensional representation of your data. For example, if your data includes people's height, weight, trouser leg measurement and shoe size, we'd expect there to be some underlying size dimension which would capture much of the variance of these variables. If you're familiar with Principal Component Analysis (PCA), this is another example of a dimensionality reduction technique. Autoencoders attempt to capture a lower-dimensional representation of their data by having a hidden "bottleneck" layer which is much smaller than the dimensionality of the data. The idea is to train a neural network which throws away as much of its dimensionality as possible and can still reconstruct the original data. Once you have an autoencoder which performs well, by observing the activations at the bottleneck layer, it's possible to see how an individual example scores in each of the reduced dimensions. This may allow us to begin to make sense of what each of the dimensions represents. One can then use these activations to score new examples on this set of reduced dimensions.