The install is no longer hanging. After opening a call with Microsoft we discovered the problem was trying to open the registry of the other node on the cluster. Additionally we were unable to use admin shares (i.e. \NODE1\C$). Once we enabled the Admin Shares, I was able to perform the installation and get past the point where it was hanging. 

I know there are still some SQL Server 2000 installs that exist out there. With Microsoft no longer supporting these versions, how can I check to see what vulnerabilities remain unpatched? This website lists some of the vulnerabilities but I am unable to determine which have fixes/patches to address the vulnerabilities. $URL$ 

What do I need in order to get around this error and make a successful connection to SSAS? Is there a better/easier way to extract DMV data from SSAS? 

I am investigating a SSRS 2012 Report which used to return data from an Oracle database. However, recently it stopped returning data. The report will sit there and run forever until something times out. Our report catalog database is also on a separate server which does not show any issues in the logs or while monitoring the performance. I do not see any errors in the Event log, SQL Server Logs, Reporting Services logs or the Oracle database logs. If I strip out the query from the report it returns rows in a few minutes. Is there a method for monitoring what is happening with the communication between the report server and the oracle server? How can I troubleshoot this issue if none of the logs are reporting any problems? Looks like I am now getting Timeout errors in the report logs. 

We use a centralized backup location on a network share. Each instance uses the Ola Hallengren scripts to perform nightly backups. I have a team that requires folder permissions to the same location where backups are stored. While the team has permissions to the parent folder it seems they no longer have permissions to any of the sub-folders where the backups are stored. I have narrowed it down to the command which executes the procedure dbo.xp_create_subdir. However, I am unable to figure out why the permissions are overwritten everytime this command is run. How can I ensure the subfolder permissions are not being changed with the Ola Hallengren scrips? 

I have an SSIS (2012) package which was written by one of the developers. I've been tasked to with fixing the error. Each time the package is run I receive the following three messages indicating the Write Output File [339] produced errors. 

The 50 spreadsheets are for the 50 different states. Somehow I need to have the OH.xls filename be dynamic instead of hardcoded so that no matter which state spreadsheet is in the Import folder (C:\import) it will be imported into my database table. My export script is done using a powershell script to dump the results to a csv file. 

How do I handle this situation. This blog post ($URL$ is the only thing I could find for reference and it isn't very clear on the solution. He states adding a second IP address. However, what exactly does this look like in DNS? Do I have one DNS hostname with two IP addresses? 

I have an ETL process that is entirely written using T-SQL scripts. The script grabs an Excel spreadsheet and imports it using openrowset. Once I get the transformed data I copy with headers and paste into a blank spreadsheet. I will have about 50 different spreadsheets that need to go through this process. Without using SSIS (due to budget reasons I can only work with SQL Server Express 2012 or higher), how can I dynamically/automatically grab a spreadsheet in a local folder for import into my database? Then at the end of the process what method should I use to automate the export to an Excel spreadsheet? Here is my import script: 

Ideally you want to create the table first and then use INSERT INTO instead of the SELECT INTO. That way you can control the column data types. 

I am having difficulty verifying my Availability Group Listener configuration. I would like to create an ODBC connection which points to the listener. When I configure the system DSN and try to test the connection at the end, I get timeout errors. However, when I change the server to point to the instance name instead of the listener name, the connection works. How can I test the listener using ODBC? What other method can I use to test the listener? Note: I do not have access to an application to test client connectivity. 

This ended up being a Kerberos problem. We were able to identify through some of the security logs and some network monitoring, that the request was coming in as anonymous. Once we applied the relevant SPNs for MSSQLSVC for the account that was being used to kick off the process, it worked. 

Running the PowerShell script from this technet article is the easiest and most secure in my opinion. $URL$ 

I need to restore a table from a backup. My understanding is the only way to get a single table back into the database is to copy it into a temporary table. Then drop the old table and rename the new one to the old name. My problem comes with this particular step due to the Primary Key column being an Identity column. The table also has multiple foreign key constraints. How can I copy one table from my backup database into the original database and still maintain the Primary key identity column and all foreign constraints? Here is my current scripted attempt: 

It turns out this issue was due to a restore of a database from an incorrect media set. Since the databases were not exact copies, the package was performing differently on the second instance. After performing a backup with INIT and then restoring that database to the second instance, the package now performs the same on both instances. 

In SSRS there are appear to be two separate query timeout options. The Query Timeout option can be found in the Dataset Properties dialog window on the Query tab. The DatabaseQuery Timeout option is found in the rsreportserver.config file. I realize the Query Timeout option is specific to the dataset, and the DatabaseQuery Timeout option is specific to Report Server. What are the reasons for using different settings? Why not just make the Query Setting to always default to the overall report server DatabaseQuery setting? If I have a lower DatabaseQuery setting will that timeout before the Query setting is met? Edit: I also want to know if this is in fact a serverside setting that can be overriden with the report specific setting. I can't seem to find any documentation on how these two settings are related/different. 

According to this Technet article, I need to : Generate a validation key and decryption key by using the autogenerate functionality provided by the .NET Framework. The step does not explain how to generate the validation key and decryption key. 

I've just discovered our prod analysis services is writing to a log that is 1.6 GB in size. I was able to get a copy of the log file saved off to an alternate location and open it in WordPad (Notepad, Word, etc.. said the file was too large). It appears this log has been churning along for almost 4 years without a problem. But now the problem is that it is too big. The msmdsrv.ini file doesn't appear to have a setting for rolling over the log or limiting it's size. What is the best approach to preventing this from happening in the future? How do you properly manage your Analysis Services log file? 

I have a list of addresses which need to be updated in SQL Server. How can I go about adding the Suite # to the addresses which are missing it? 

My understanding is that I should be able to cast the value to a decimal when it's in scientific notation. However, other parcel numbers that have a legitimate 'E' need to remain the same. How can I account for the different 'e' values and still convert the parcel numbers in scientific notation back to their standard parcel number format? I frequently run into the error converting nvarchar to numeric error messages and I'm wondering if it's because of the parcel numbers that have an E but are not in scientific format. For instance I run this query: 

I'm interested in gathering some DMV data from our SSAS cubes and store it on our management box. When I create the linked server for this effort I run into an error establishing the connection. 

Unfortunately we have some parcel numbers that were converted to scientific notation. This means they are in the wrong format for Parcel Numbers and should be converted back to regular decimal or numeric format. I am encountering difficulty converting just those values that have the scientific notation. The column is of nvarchar(255) data type but these values need to be manipulated back into numeric or decimal data type. 

We are currently testing a SQL Server 2012 AlwaysOn HA implementation where we have 2 out of the 3 nodes are synchronous. The third, asynchronous node, does not have a vote in the quorum. There is a scenario in which the first two nodes experience complete failure. When this happens, the third node is up and running but it has it's database in the AG marked as "Recovery Pending". How can I get this database out of this state and make it available? 

If you are looking at alternatives to using Maintenance Plans or are looking for something that is easy to customize, you should really look into the Ola Hallengren maintenance solution. $URL$ 

I'm running into some difficulty getting the AG Listener created for a 3 node cluster where node 3 is on a separate subnet. I'm encountering an error that states 

I would like to setup an automated restore job which takes the latest timestamp .bak file and restores it into the DEV instance. I currently have a job on the DEV box that uses the UNC path to the prod backup location to restore from. However, anytime I want to run this restore job I need to go in and manually change the file so that it's pointing to the latest one. Is there a method, preferably using T-SQL, which will grab the latest .bak file to use in the restore job? If there is a better method for doing this then I'm open to suggestions. I definitely don't want to reinvent the wheel. Below is the current code I'm using to perform the restore. I have it narrowed down to where I only need to change the database name and backup file location. 

I'm trying to determine when a dynamic port is assigned to SQL Server. If the TCP/IP Properties have the TCP Dynamic Ports enabled, does that port get assigned at the time the Server is installed or does it get assigned each time the instance is started. Is it therefore possible that a new port will get assigned each time the instance is restarted? I'm mostly curious about the 2008 through 2014 versions. 

We are in the process of trying to clean up some old accounts on one of our AlwaysOn clusters. This particular account refuses to play nice and allow itself to be deleted. 

The only server role it is a member of is PUBLIC. How can I find out what exactly is preventing me from revoking these permissions so I can drop the user? Thanks. 

We have some Excel spreadsheets with parcel numbers for properties that should be in a standard varchar format similar to: 

EDIT: Some additional info. The user of this application deleted 175 incidents by accident. Unfortunately she didn't tell us until two weeks later and then it wasn't even routed to us until almost a month later. In users have been accessing the application and making changes so a complete database restore is not an option. The user is requesting just the deleted incidents be restored. I looked into the database and noticed an incidents table with the missing data in the current database and the 175 records existing in the backed up database. Hope that provides some better context for why I'm trying to restore just a single table of incidents. It starting to look like additional tables need to be restored. 

I noticed in the last month one of my SQL Server instances is getting a daily SQL Dump saved off to the Log directory. Unfortunately the SQL Dump is getting large and is consuming the majority of disk space. I don't know exactly where to start investigating why I'm getting SQL Dumps. If I run a DBCC CheckDB against my databases it comes back without any errors. Where do I start investigating why I'm getting SQL Dumps into the log directory? What can cause the SQL Dumps? Will reading the SQL Dump help with my investigation? 

I'm still fairly new to PowerShell so I'm unsure of all the troubleshooting steps I can take to solve this problem. I would like to use the below command to determine if the replica is primary or not. I believe I have successfully loaded both the provider and smo. Intellisense appears to recognize the command. If I run the following command 

This error was resolved by adding a second IP to the listener. The IP address for both subnets cannot be an IP address already in use, i.e. the IP address of one of the nodes. A DNS entry is also not needed since the Listener creates the DNS entry as part of the process. 

I have tried granting full control to the folder share to myself, the SQL Agent account, and the SQL Services account. Whether I run the package as myself or as a service account I receive the error messages. I'm not seeing any more details in the SQL Server error logs or the Windows Event logs. 

We recently discovered one of our drives was being filled up by audittrace files. One of our DBAs turned off C2 auditing which should normally stop the files filling up the drive. However, it appears turning off C2 auditing has not had any affect. I've tried looking at the existing traces to determine why the files are still being written. 

I have created a user on my primary node that has access to all of the databases in Availability Group AG1. How do I ensure that same user has the same rights on all the other replicas in AG1? According to this MSDN article ($URL$ I need to determine if they are contained databases first but I'm not entirely sure I have contained databases. The user that I created on the other replica's get's an error when I try to add him to the AG1 databases. The error indicates they are part of an Availability Group and are not accessible. 

One of my clients has some very specific needs to get a spreadsheet run through some ETL processes I have created. I have developed an SSIS .dtsx package which imports an Excel spreadsheet into SQL Server and then performs some transformation processes and then exports the results to another Excel spreadsheet. I am wanting to hand off the tool to the client so they can kick off the ETL process ad-hoc. In order to allow them to run the DTSX package, what are the minimum requirements that are needed to get this running on their system? Can this be done with SQL Express or do they need to have Integration Services running to execute the package?