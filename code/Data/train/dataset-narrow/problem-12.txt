Test Kitchen has a kitchen-ansible provisioner plugin for testing of Ansible code. It isn't as deep as the Chef integration but it does get the job done for most cases. There is also the more recent Molecule project which is a dedicated Ansible testing system. 

Answered on Slack, the likely first issue here is that the output from is not a cert, it's a CSR. But there are some other lingering issues here too. It is recommended to use something like Certstrap to do internal cert generation rather than raw commands as the modern standards are quite fiddly. 

The specifics depend on the exact use case. If the value is only used as a property on another resource, you would use the helper method. If you don't need the output, you would use the resource. In some more complex cases you might use a resource or write your own custom resource. 

Docker is a specific implementation of Linux containers, or if you want to be more precise Docker is a distribution of tools that includes which is an implementation of Linux containers. Other implementations include rkt, LXC, LXD, and (I think) Snappy from Ubuntu. 

For a purely EC2-based environment, the easiest solution is to use AWS IAM roles and S3 bucket policies. Some formulations of this pattern also include KMS for encryption and DynamoDB instead of S3 for storage but I've not found a hugely compelling reason to use either. Check out $URL$ it is specifically aimed at Chef users but the underlying pattern works for anything, you just might have to make your own helper for doing the actual download from S3. 

True continuous integration tools (as opposed to just continuous testing) like Reitveld and Zuul can help, though they are only as good as the tests you write and code reviews you do. 

You can find more details about and friends at $URL$ With cookbooks based around custom resources instead of recipes, things can get more complex but the specifics depend a lot on the how exactly the cookbook you are extending is written. 

In general git allows concurrent operations because one of them will eventually fail. Some specific database updates will have file locks (like updating the index files) but multiple receives can be in flight at once I think. In this case whichever verification completed first would be allowed in, and the second would fail due to having an invalid history (i.e. not being a fast-forward push). In general this kind of operation is not well suited to git's server-side hooks since if something does fail, the person will then have to do some surgery on their branches to get things back in a valid state. These kinds of checks are generally run on the workstation side, though then you run into the "fun" that is distributing git hooks as they can't actually be checked in directly. Another common solution is to have a CI service like Jenkins running verifications (tests) and updating a second branch or a utility tag showing which commits are acceptable. 

There are a lot of specifics but the overall pattern we use is "wrap and extend". The general idea is to make a cookbook that depends on the community cookbook, usually named , and then make recipes in that which call but with either more stuff added before/after or with calls to things like to change resources. Avoid when possible since it leads to brittle code, but it is there if you need it. You can also use wrapper cookbooks to set attributes, subclass or wrap custom resources, and so on. For the specific case of "I need to tweak a template in a community recipe" it would look like this: 

It is my belief that in a cross-functional Agile team following DevOps practices these responsibilities belong to the team as a whole, ultimately being accountable to the Product Owner as the representative of the business. In short, my question is who is... responsible for Environment Management in a team following DevOps practices, i.e., one that does not have an external Environment Manager function available to them. 

Important: Chat and ChatOps is very specific to an organisation, think of Chat as the Fabric for your team's bots to existing within it's the bots that provide the functionality that enables business and technical processes. With the brief lesson in ChatOps Theory over I can talk about the experiences that I have had with ChatOps: DevOps Support Slack and PagerDuty has fantastic integration allowing any newly raised incidents in PagerDuty then posted in one or more Slack Channels with information about the incident and buttons for acting upon the knowledge: 

Specifically in answer to your 3rd question, if you are willing to look outside of the Jenkins Ecosystem there are alternatives out there that might be of value to you. For my clients who use the Microsoft Stack and have fewer than four teams, I have been recommending the use of AppVeyor it is highly tuned for the .NET Stack and integrated very naturally with and Wix. 

Cloud services hosted by Amazon Web Services, Azure, Google and most others publish the Service Level Agreement, or SLA, for the individual services they provide. Architects, Platform Engineers and Developers are then responsible for putting these together to create an architecture that provides the hosting for an application. Taken in isolation, these services usually provide something in the range of three to four nine's of availability: 

Note: this is a Hypermedia API which means the urls could change so I would recommend using the links from the previous response rather than trying to generate them from scratch. The response from the above HTTP request will be something like: 

Google offer a $300 free trial for 12 months in a similar deal to Amazon. In addition they have free usage tiers for many of their core offerings: 

It is worth separating your Continuous Integration "Build" from your Continuous Deployment "Pipeline". For the build portion, CircleCI seems to be your tool of choice, for the latter I recommend using something like AWS CodeDeploy which natively supports Blue-Green Deployments, for completeness you can use CircleCI to orchestrate AWS CodeDeploy: 

The layout of your repository depends in many ways upon the context you are developing the automation in. If, for example, you are building out the infrastructure for a product as part of a product team, then it would make sense to tie the infrastructure to the product - i.e. keep the infrastructure in the same repository as the software source code. If however, you are building out common infrastructure components to stand-up basic infrastructure for DNS, File Storage, Email, etc. then it probably makes more sense to have a single repository. Some of this will be tied to the tool you are using, Terraform for example leads you down the path of having a single repository for all of your environments to support the construct. Reusability You quite rightly called out reusability and code sharing as a problem you are going to encounter sooner rather than later. This is an important principle in Software Development called Don't Repeat Yourse - or DRY for short. Most all DevOps tools allow you to modularize your code in such a way that you don't need to copy/paste code: 

So both have their place. On one hand, putting service definitions in the package helps to keep the number of "moving pieces" to a minimum. On the other, it also requires a more complex package build process to at least some degree (like you would have to build your own packages for things instead of using distro packages). If you have the capability to do it in packages, that seems better since moving pieces in prod will always eventually break. 

Unfortunately there is no one answer here. If you ask each member of your team, they will probably all have different answers. Instead of trying to a root cause analysis on something as generalized as "culture", try looking at return-on-investment instead. A fix to something near a bottleneck will usually have a better RoI than one further from the bottleneck, but if the further one will take 5 minutes and get you 90% of the benefit then who cares, do the needful. 

Application deployment is a hard thing to pin down because it has a lot of sub-problems. Config management systems are excellent at modeling tasks which are convergent and work with "what is the desired state of the system". In the context of app deployment, this is great for things like deploying bits to a machine, managing configuration files, and setting up system services. What it is extremely bad for is things that are inherently procedural, notably database migrations and service restarts. I usually try to put the convergent logic in Chef and let an external procedural tool (usually Fabric in my case) handle the few remaining bits as well as sequencing the actual converges. So, basically, you should use both for the pieces they are best at. 

Talking specifically about the image packaging piece of Docker, not the container runtime there are a few minor bits. The biggest is that a Docker image is more like a chroot, which means you are prevented from accidentally depending on shared system state since every file in use must be explicitly included in the image while a system package might pick up dynamic links you didn't expect or otherwise get more intertwined with other packages. This can come up with complex C dependencies getting loaded without your knowledge, for example OpenSSL. Additionally using deb packages don't de-duplicate shared bits in the say Docker's storage system does. For some this might be a good thing, better I/O performance and fewer moving pieces, but for others it might be a problem. 

The above answer covers part of it but misses one of the important elements: convergent design. I wrote some words a while ago about this in the context of Chef at $URL$ but the short version is that a bash script is a set of instructions, while an Ansible playbook (or Chef recipe, Salt state, etc) is a description of desired state. By documenting the state you want rather than the steps you want to take to get there, you can cope with a lot more starting states. This was the heart of Promise Theory as outlined in CFEngine long ago, and a design which we (the config management tools) have all just been copying since. tl;dr Ansible code says what you want, bash code says how to do a thing. 

"Serverless" mostly just means you've got relatively simple microservices, generally just a little webapp or a single function that is automatically connected to a REST frontend. The same concepts apply as you would use for a more traditional web services: usually some mix of remote syslog and ElasticSearch writers. Networked or remote syslog has been around for a long time and has a fairly robust set of tools around it. You would have to run the central syslog server(s) but the protocol is very simple and there are pure client libraries in every language that you can use for sending logs. One common problem with remote syslog is that it has traditionally been based around UDP. This means that under heavy load, some log messages may be lost. This could be a good thing, helping avoid a cascade overload, but it is something to be aware of. Some newer syslog daemons also support a TCP-based protocol, but client support is less unified so just do your research. More recent but very popular is logging to ElasticSearch. This is mostly useful because of the Kibana dashboard and Logstash tooklit (often called ELK, ElasticSearch+Logstash+Kibana). Amazon even offers a hosted ElasticSearch option, making it somewhat easier to get started. ES uses a relatively simple REST API, so any language with an HTTP client (read: all of them) should be okay with logging to ES but make sure you are careful with blocking network operations in cases of partial system outages (i.e. make sure your app won't get stuck in a logging call that will never succeed and stop servicing user requests). More complex logging topologies are bounded only by your imagination, though these days you'll see a lot of use of the Kafka database/queue/whatever-you-want-to-call-it as a nexus point in very complex log distribution systems. On the "serverless" side, you'll generally want to integrate with these systems directly at the network level, so sending log data directly to syslog or ES from your service/function, rather than writing to local files (though maybe echo to those too for local debugging and development). 

We have a multi-tiered application hosted on Microsoft Azure Virtual Machine Scale Sets composed of: 

I think what you are looking for is an Open Source project that can take inputs from both Amazon CloudWatch and Google StackDriver; there isn't a huge amount out there at the moment, but I will detail what I know. I have made the assumption that you know how to import your application telemetry into the solutions below. Open Source Open source solutions to this problem have been lagging behind as late due to favourable prices of SaaS and built-in solutions. The products below are constantly evolving so if they don't do what you need now they may be in the future. 

As far as I am aware there are no statistical data about the enterprise adoption of each of these frameworks and whether they have adopted DevOps practices in addition to Agile practices, however from experience many organisations appear to gravitate towards Scaled Agile Framework. Kristof Horvath discusses the application of DevOps in these enterprise-grade agile frameworks in his article: Scaling Agile in Large Enterprises: LeSS, DAD or SAFe®?. It is worth noting that adopting an "enterprise-grade" framework is not a requirement in an enterprise, at the end of the day Agile, DevOps and Lean are about adopting the right practices for your organisation, not the most popular ones. 

Having a separate process that pushes the metrics into CloudWatch. Have your Producers, Consumers or Stream Processors push the metrics you need into CloudWatch. 

You only need to compile/test once from the branch; then promote the packages through the Artefact Repository. Your packages are stored forever in an Artefact Repository, for example Sonartype Nexus or JFrog Artifactory, this means you can reproduce an old build as you have the specific packages used for deployment. CodeDeploy will handle the release patterns for you without having to think too much about how to achieve Blue-Green Deployments. 

Again where is the - there could be many of them if one commit resulted in many builds. If the state for the build you care about is then you have your answer and you can immediately return the for the commit. Loop over all of the commits from the first phase, if you run out of commits follow the page that is included in the call to . Complete Flow Diagram At a high level the flow will look like this: 

Because Traffic Manager is a circuit breaker it is capable of detecting an outage in either region and routing traffic to the working region, however there is still a single point of failure in the form of Traffic Manager so the total availability of the "system" cannot be higher than 99.99%. How can the compound availability of the two systems above be calculated and documented for the business, potentially requiring rearchitecting if the business desires a higher service level than the architecture is capable of providing? If you want to annotate the diagrams, I have built them in Lucid Chart and created a multi-use link, bear in mind that anyone can edit this so you might want to create a copy of the pages to annotate. 

To be absolutely clear, these are functions that used to belong to the operations organization and are now owned by the Agile/DevOps organization. There are existing KPIs that drive bad behaviors are: 

There is a fair bit of configuration to plug in to get this setup including setting IAM policies and adding EC2 tags to the instances you want to be party to your cluster. If you were to use AWS Autoscaling Groups then you would add the following to your :