Connection will most likely needs to restarted for the parameter to be applied, but the issue is solved (at least for this user). 

Shared hosting means your web site will be located on the same machine as other customer's. It has repercussions with performances (you'0re sharing them with other apps) and security (a flaw in someone else web server can affect your own). "SSL included" probably refers to having a SSL certificate issued to your host name by your hosting provider and installed in your (virtual) web root. That will secure the communication between your clients and your server. So the two aren't directly linked and being on shared hosting shouldn't have any impact on your decision to purchase your own certificate or use the included one. Personally, I wouldn't use shared hosting for a commercial web site, however. 

I'd be tempted to answer "stop abusing the file system by treating it like a database" but I'm sure it wouldn't help you much ;) First, you need to understand that if your limitation is in the bandwidth available on read, there isn't anything you can do to improve performance using a simple synch command. In such a case, you'll have to split the data when it's written either by changing the way the files are created (which means, as you guessed correctly, asking the devs to change the source program) or by using a product that does does geo-mirroring (like, for instance double-take: check around as I'm sure you'll find alternatives, that's just an example). In similar cases, the main cause of problem isn't typically the file data but rather the meta-data access. Your first strategy will therefore be to divide the load into multiple process that act on (completely) different directories: that should help the file system keep up with providing you with the meta-data you need. Another strategy is to use your backup system for that: replay your last incremental backups on the target to keep the database in sync. Finally, there are more exotics strategies that can be applied in specific cases. For instance, I solved a similar problem on a Windows site by writing a program that loaded the files into the file system every few minutes, thus keeping the FS clean. 

What you're asking isn't very clear. If you mean "can I redirect the client from $URL$ to $URL$ using HTTP redirect or HTML redirect, then the answer is yes. The caveats, of course, is that the user will see your URL in their browser pretty much immediately when they go to the app web site, making it pretty pointless to have created the subdomain in the first place. But your issue is actually a failure to properly define your requirements. Either you didn't define the requirements for your product properly or your customer didn't produce the proper requirement for their project themselves. In both case, you're trying to fix this failure with band-aids and that will NOT result in anything satisfactory. The simplest way to salvage that situation is for your customer to deliver a server to your specifications and use a valid certificate. If they really cannot provide your with a sytem with Apache and PHP (either internally or by renting a vm), then they could explore the possibility of simply adding PHP to their IIS web site (it's trivially easy), get a proper certificate and install it themselves. Another possibility is for then to request a certificate for their sub-domain and for you to configure your server to serve it. Since they already have defined a sub-domain, all you need to do is add an IP address to your server, your customer to link that IP address to their sub-domain in their DNS and for you to configure Apache to serve it over SSL using the certificate for their sub-domain. 

I need to revive an application so it can be safely archived by a third party before being phased out. In order to do so without endangering the whole server, I want to force the user to authenticate before connecting, which means requiring TLS. Unfortunately, the 443 port on that server (xxx.xxx.xxx.120) is taken by another application (a web mail system that uses its own HTTP server) so I added a new public IP to the server (xxx.xxx.xxx.120) and added a binding in IIS for port 443 on this new IP: 

You can always create a new zone in your DNS and enter the DNS entries there but, honestly, it makes a lot more sense to simply connect the DNS servers. Ask them to allow you DNS to forward the request for their domain to their DNS servers: that way, they don't even have to open traffic to your whole network, they only need to allow DNS from your own servers. 

No, Azure does not include an integrated backup system that allows you to rollback to a previous version. For the web site, the data is typically pretty small so it's pretty easy to SFTP or VPN into the web site and take a backup with a scheduled task. For the SQL database, the simplest option is to use the SQL import/export service to dump the content of the database to a storage blob. Since that import/export is not transitionally consistent, you will need to first make a database copy and take a backup of that copy. Other than that, there are a number of vendor who provide SQL azure backup services on premises or to cloud storage. 

Generally speaking, yes: if you have a flaw that leads to remote code execution with root access, you can do it. As a matter of fact, it's possible for a specific flaw not to lead to a remote code execution but still lead to a kernel panic and server reboot. Given the way you've phrased your question, however, I doubt you have the necessary knowledge to perform a postmortem on a system a detect this kind of attack: I would suggest you hire a security professional if you really want the system examined. 

(there are other functionalities, like compression or secure key negotiation, but you don't really care about these). The part that is going to cause you trouble is the second one: authentication. More specifically, the server side of the authentication (in other words, how does the client certify that he is talking to the "right" server). For this, SSL relies of two things: an X509 certificate and the DNS system. It does it through the following steps: 

I wouldn't do it that way: it will be difficult to assert whether your reverse proxy is actually working or whether you're hitting the final interface directly. You'll need to check the content of the data over the wire to be sure. Why not setup a couple of VMs for this instead ? 

This will tell your client that the server 130.255.190.71 is listening to you on port (4*256 + 128)=1152 So: - Try to get the raw message, including the answer from the server (the response to the PASV command) - Check your firewall rules again