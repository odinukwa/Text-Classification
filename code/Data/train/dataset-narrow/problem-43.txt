is a transform from world space to tangent space. The inverse of such a matrix is indeed its transpose, so transforms from tangent space to world space, which is not what you want. 

If you are working with quads, there is a nice trick you can use: for a quad abcd, use and it will handle nicely cases where the quad is in fact a triangle. Iñigo quilez wrote a few short articles on the topic: clever normalization of a mesh, and normal and area of n sided polygons. Normals from partial derivatives Normals can be computed in the fragment shader from the partial derivatives. The math behind is the same, except this time it is done in screen space. This article by Angelo Pesce describes the technique: Normals without normals. 

Nice question! I hadn't seen the game myself but this question got me interested so I watched some footage on Youtube. This one for example gives a good view of it: $URL$ 

Let's reminds ourselves what light is. Radio waves, micro waves, X rays and gamma rays are all electromagnetic radiation and they only differ by their frequency. It just so happens that the human eye is able to detect electromagnetic radiation between ~400nm and ~800nm, which we perceive as light. The 400nm end is perceived as violet and the 800nm end is perceived as red, with the colors of the rainbow in between. A ray of light can be a mix of any of those frequencies, and when light interacts with matter, some frequencies are absorbed while other might not: this is what we perceive as the colors of objects around us. Unlike the ear though, which is able to distinguish between a lot of sound frequencies (we can identify individual notes, voices and instruments when listening to a song), the eye is not able to distinguish every single frequency. It can generally only detect four ranges of frequencies (there are exceptions like daltonism or mutations). This happens in the retina, where there are several kinds of photo-receptors. A first kind, called "rods", detects most frequencies of the visible light, without being able to tell them apart. They are responsible for our perception of brightness. A second kind of photo-receptors, called "cones", exists in three specializations. They detect a narrower range of frequencies, and some of them are more sensitive to the frequencies around red, some to the frequencies around green, and the last ones to the frequencies around blue. Because they detect a range of frequencies, they cannot tell the difference between two frequencies within that range, and they cannot tell the difference between a monochromatic light and a mix of frequencies within that range either. The visual system only has the inputs from those three detectors and reconstruct a perception of color with them. For this reason, the eye cannot tell the difference between a white light made of all the frequencies of the visible light, and the simple mix of only red green and blue lights. Thus, with only three colors, we can reconstruct most colors we can see. By the way, rods are a lot more sensitive than cones, and that's why we don't perceive colors in the night. 

OpenGL error handling is quite tricky, since you have to poll for errors yourself, and it uses the same few error codes for wildly different functions. You have to identify exactly which function yielded the error, then refer to the documentation for the list of reasons this error can happen for that function. To make that task easier, the best way I have found so far is to always surround OpenGL calls with a macro doing an error check. You can find an implementation of a macro like that in the source code of bgfx for example. 

With that texture, instead of painfully integrating, we just fetch the precomputed result, combine it with the rest of the equation (the material diffuse color, etc.) and voilà. Finding the 32x32x6x64x64x6 Since the result looks very blurry, it probably doesn't need to be $64 \times 64$ for storing, and the article assumes $32 \times 32$. But still, each of those $32 \times 32 \times 6$ texels' color is computed separately, by integrating over the entire corresponding hemisphere. That part never disappeared: we just moved it from the rendering stage to a pre-computation stage. So that's still $(32 \times 32 \times 6) \times (64 \times 64 \times 6) / 2$. That's half the figure stated in the article. I suppose the author was either assuming a model that integrates of the entire sphere, or just dropped the $0.5$ factor for the sake of clarity. Finding the 9x64x64x6 We can't help but notice this irradiance diffuse environment map is really blurry: surely there must be a way to store its information using a lot less than $32 \times 32 \times 6$ terms, and hopefully reduce the amount of work as well. That's what spherical harmonics do, by compressing the irradiance in just a few coefficients. I am not familiar enough with the math to engage in an explanation, but suffice to say it is generally considered that for diffuse, order three SH are sufficient. Order three SH involve nine coefficient, and again, for each of these we need to integrate over the entire hemisphere. So that's $(9 \times 64 \times 64 \times 6) / 2$ texels to fetch and process. Here too a $2$ factor is missing from the article. 

Path tracing and reducing noise The balance between light source size, noise and SPP is a classic problem, to which there is no magical solution. From easiest to hardest, you could: 

Display lists were an OpenGL feature that could, in theory, accelerate any part of the API by storing a group of commands for later use. In my understanding, this makes a lot of sense with regards to the current effort to reduce the driver overhead. Display lists were deprecated in version 3.1 though. What is the modern API equivalent and does DirectX expose a similar feature? If there is no such functionality anymore, what is the rationale? 

In the section 6.4 Constant Buffers of the book Practical Rendering & Computation with Direct3D 11 (pages 325, 326) it is mentioned: 

In this context, refers to a very crude approximation of indirect lighting. Direct lighting from a direct source is relatively simple to evaluate and model, even in real-time. But the light that is not absorbed will bounce all over the place and cause indirect lighting. This is why for example a lamp will a lampshade will light a whole room and not only the narrow area underneath. But modeling indirect lighting is difficult and costly. So an approximation is to consider that lighting to be constant and independent from position: that's ambient lighting. In the case of outdoor scene, ambient lighting would represent the blue light coming from the sky dome, as opposed to the orange direct light coming from the Sun. 

Avoid stereo when possible There was some research published recently measuring when users can or cannot tell whether the specular contribution, which is view point dependent, is different between eyes. Perception of Highlight Disparity at a Distance in Consumer Head-Mounted Displays In the future, this could be use to share some computation between the two eye renders. Unfortunately this sounds hard to take advantage of in a typical rasterization pipeline. Some games already avoid stereo when rendering far objects and background. 

Because your monitor is not properly calibrated. On my screen at home the top and bottom parts have the same hue. At my office though, the top part tends to looks a bit yellow compared to the bottom part that looks more red. The difference: my screen at home was from a series that was decently calibrated out of the factory, and on top of that I did calibrate it properly with a color calibration tool. That was quite some years ago and its colors have probably degraded since, but it still makes a difference. A poorly calibrated monitor will not display exactly the color intensity requested for R, G and B, resulting in differences between the color that should be displayed and the one actually displayed. It can even vary depending the area of the screen (many consumer level LCD screens tend to have more light leaking near the edges for example). The image you included is a good example to highlight a calibration issue, because it allows to compare directly the average color at 50% intensity (combining two halves at 0% and 100% intensity) with the color displayed when requesting 50% intensity. Moreover, the gradient allows to see if the hue is constant from 0% to 100% intensity. 

Does IBL necessarily imply at least one matrix vector multiplication per fragment and the optimization I mentioned is an uncommon case, or am I missing something? 

In OpenGL the buffer object functions (, , and probably a few others) have a parameter , described by the documentation as a hint of the intended usage, likely meant to help the implementation yield better performance. 

The best results strongly depend on your use case. They also depend on what effect you want to achieve. Sobel is just an edge detection filter: the edges will depend on the input signal, choosing that input signal is up to you. Here you are using the color image as an input, and the filter rightfully detects faint edges in the blue gradient, while the edges of the cube get interrupted where its color is too close from the background color. Since I assume your program is also responsible for drawing the cube, you have access to other information that you can feed to your Sobel filter. For example depth and normals are good candidates for edge detection. The albedo before lighting could be used to. Test with different inputs and decide which ones to use depending on the results you get. Regarding your question about how to combine the edge information, I suggest you filter a little before using it. Then you can use it to interpolate between the original color and the wanted edge color. For example you could try something like this: