If your using C++11 it has a new Chrono library that allows you to get the system time in milliseconds so you don't have to rely on functions shipped with a 3rd party library. There is also a Boost version if you need to backport it to a non-C++11 system. 

Having said that, many indie devs make stuff using XNA and sell through the Xbox store. Since it's built right into the Xbox they get some good sales and XNA seems like a good platform. 4) Once again I'm not that familiar with DirectX. I've heard John Carmack prefers DirectX and I also saw the dev working on the infinity universe engine say similar things (But they went to DX11 from whatever OGL version they where targeting, Carmack would probably have been using an older OpenGL version). But OpenGL has had massive changes recently, many of which have made coding in it much easier (uniform buffer objects, GLSL uniform routines). Those changes will take a while to roll out. OpenGL doesn't have much OOP. It does however refer to things as 'Objects' (VertexBufferObjects, VertexArrayObjects, TextureObjects, FrameBufferObjects). Those things are fairly easy to wrap in a class for whatever language your using. You don't need to implement the whole API just the parts you want. There is a C++ binding called OGLplus although it's newish and not widely used. From what I can see from Googling DirectX also requires similar OOP frameworks to be produced. If they both need the frameworks then the coding probably doesn't matter to much since you should only be using it in the framework. Finally OpenGL gets some flac for having broken implementation on some system (Such as netbooks with Intel chipsets, even the ones that do work are often only OpenGL 2.x). Or being out of date on many system (The latest version of OSX 10.7.x has 3.2, the previous version 10.6.x was 3.0, then 10.5.x is either 2.0, 1.5 or 1.3 depending on the video card (Minecraft sticks to 1.3 as it's render target for basically this reason), those systems are going to be stuck at that OpenGL version forever unless the users pay for an upgrade and 10.5 was only released in 2007 so you will have to live with it for a while longer). One good thing is you can just target what extensions you want and make your framework support fallbacks (For example you can make a VertexBufferObject class that will use the 1 vertex at a time fixed pipeline functionality if the VBO extension is missing). Also remember OSX doesn't support DirectX at all. 

OpenGL has some 'Object' concepts in it already. For example anything with an id can be through of as an object (There are also things specifically named 'Objects'). Buffers, Textures, Vertex Buffer Objects, Vertex Array Objects, Frame Buffer Objects and so on. With a little work you can wrap classes around them. It also give you a easy way to fall back to old deprecated OpenGL functions if your context doesn't support the extensions. For example a VertexBufferObject could fall back to using glBegin(), glVertex3f(), and so on. There are a few ways you might need to move away from the traditional OpenGL concepts, for example you probably want to store metadata about the buffers in the buffer objects. For example if the buffer stores vertices. What is the format of the vertices (ie position, normals, texcoords and so on). What primitives it uses (GL_TRIANGLES, GL_TRIANGLESTRIP, etc...), size information (how many floats are stored, how many triangles they represent, etc...). Just to make it easy to plug them into the draw arrays commands. I recommend you look at OGLplus. It's C++ bindings for OpenGL. Also glxx, that's only for extension loading though. In addition to wrapping the OpenGL API, you should look at making a slightly higher level one build on top of it. For example a material manager class that is responsible for all your shaders, loading and using them. Also it would be responsible for transferring properties to them. That way you can just call: materials.usePhong(); material.setTexture(sometexture); material.setColor(). This allows fore more flexibility since you can use newer things like shared uniform buffer objects to just have 1 big buffer containing all the properties your shaders use in 1 block but if its not supported you an fall back to uploading to each shader program. You can have 1 big monolithic shader and swap between different shader models using uniform routines if it's supported or you can fall back to using a bunch of different small shaders. You can also look at expending on from the GLSL specs for writing your shader code. For example the #include would be incredibly useful and very easy to implement in your shader loading code (there is also an ARB extension for it). You can also generate your code on the fly based on what extensions are supported, for example use a shared uniform object or fall back to using normal uniforms. Finally you will want a higher level rendering pipeline API that does things like scene graphs, special effects (blur, glow), things that require multiple rendering passes like shadows, lighting and such. And then on top of that a game API that has nothing to do with the graphics API but just deals with objects in a world. 

If your willing to do a bit more work, you could look into refractoring solutions. clang has some interesting stuff happening in that area currently. It should be possible to use the auto keyword, run it through the refactorer which will find all uses of it, resolve them for you and output the code then compile it with whatever you want. But that means less time to work on your game. There is also the potential for some extra features, for example reflection. You could create a class and automatically generate a list of all the properties to inspect at runtime. Could be very useful for scripting, producing game editors and so on. EDIT: Check out clreflect. $URL$ $URL$ $URL$ $URL$ 

It will depend a lot on the artwork and what effect you want to accomplish and how much time you want to spend on it. Do you need the to work with every single object? Or just some wieldable ones (like swords and sticks). If its just some then I would probably just have a separate set of sprites. If you are looking to do rotations on pickup and you are doing pixel art (via a pixmap/bitmap image) then manipulating the original image won't work very well. It will possibly blur (if you have some kind of antialiasing) or it might loose a lot of detail (if there is no antialiasing pixels can disappear or 'tear'). You could try using much higher resolution graphics or even better (but much more complex to program) vector art, so manipulations don't have as many visual issues. A simple 90° bitmap rotation would work without any distortions. It could allow the stick to be flat on the ground and upright when held, 45° wouldn't be too bad either. But some items won't make sense (a cup of water might look weird when left on the side), so you might need to provide a rotation 'hint' with the sprite/object. You will need to track the location of the hand, you might also need to have some bits that are drawn under the item or over it. For example fingers. It might also be reversed when the character is facing the other way. It might be an idea to have some rigging like you would with a 3d mesh. For a simple setup, I would probably make a separate character sprite (or another layer) that just has a couple of pixels in a few color keys. The Red pixel might be the left hand position. Blue for right. You can add other colors for things like the head, torso, legs, knees, feet and so on which would allow things like helmets, armour, pants, shoes and so on. Just scan though the pixels for the 'slots' (there are probably more efficient ways of packing it, but don't make more work for yourself). You might also be able to use different colors to specify if the object should be drawn after to before the character sprite (ie will the hands obscure the object). You could make a fingers a separate 'object' if you need that level of complexity (ie the fingers drawn over the object but the palm of the hand behind it). Otherwise if you just need a few specific items, swords and such rather than every object in existence I would probably just have a animation for 'guy with sword' 'guy with axe' 'guy with stick' and so on. Maybe just have them overlay the generic 'guy' character sprites. But remember an attack animation with a sword will probably be different to one with an axe so setting up some overly complex setup would just get in the way their. Maybe you can offer some colour pallet options so you could have a wooden sword or a steel one with the same sprites just with different colours. 

At the most basic level they use what is refereed to as a 'chunk' or 'world chunk'. Chunks can also be called 'levels', 'maps', 'stages', or 'areas' but the reverse is not always true. In a game like GTA/Noire/Saints the entire city will be made up of several chunks that will load as you traverse the city (maybe there is a graph layout and you get a chunk per suburb, or maybe it's just some kind of grid layout). In that kind of game the term 'level' doesn't make any sense. In AC you will get something similar, however it also has missions that are completely separate from the main game world (at least in AC3), those missions could be considered to be a level but I probably wouldn't refer to the whole of a city as being it's own level (unless it's self contained, the AC series changes from game to game quite a bit, the original might not have had the open game world that the later ones had in which case each city might be considered to be a level). I would consider levels to be self contained discrete chunks and generally provide a one directional linear progression through the game world (although there are exceptions, for example hidden levels). 

For a library I suggest you have a look at Assimp. It's a library for working with various mesh formats so you can just dump it in and get a full mesh importer with little in the way of work. It also allows you to load in a standardized format and output a custom one. The problem is what format you store your meshes in will vary depending on your program and what features you need, what formats you are using internally. You are normally going to need some post-processing for things like converting filenames of images referenced by meshes. How are you going to link custom shaders and so on. There are some standardized formats that you can use but they will tend to be over engineered for most realtime rendering and are more for use in the content creation pipeline. COLLADA is the main example (as the other post mentioned), it's found use from Sony with the PS3. Bullet Physics (and the Physics Abstraction Layer) apparently includes support for Collada. On the down side COLLADA is XML, so it requires parsing and needs to support a huge list of features making even a basic implementation quite a bit of work (for example it supports multiple coordinate systems, so if you want to support custom content you will have to take that into account, plain image textures have multiple levels of linages since the same format needs to support more advanced things like multiple textures and so on). With that said, in the past games often add support for importing the native formats of a 3d modelling program such as 3ds which wouldn't be too different so the overhead isn't that great. Although might be to much for mobile phones and some game console systems. idGames such as the Doom 3 mesh format (MD5) is simply a plain text ASCII file. There are multiple files, one for geometry and another for animation. You could also have a separate one for physics as that will generally use lower polygon geometry. Personally I would recommend using a binary format as your final mesh format. You can make a format that can be loaded into memory as basically a single block of memory and just requires some updating of pointers. The only potential problem is big endian vs little endian but just standardize on one and convert on the other. Rather than using triangles it might be better to use an indexed vertex array of triangle strips for storage as it should save a bit of storage space (and ram when loaded into memory) but it does require some conversion to the format. 

Load the basic game world properties. Global scripts. Load the player's position. Load the gridcell/BSP node of that position. Load just the bounding boxes of the objects in that cell. Work out what would actually be rendered (tests on the bounding boxes, maybe occlusion rendering tests) Load the meshes of the stuff that is rendered. Load the materials the textures of the meshes. Preform more tests to see if more regions are visible and need to be loaded (ie query a render a whole grid cell as a giant cube). Once everything visible is loaded start rendering. Load the rest of stuff in the cells and Load more stuff as required. Load nearby regions and do some basic prediction on the players movements to choose which regions need to be loaded first. Keep going until memory fills. 

In the case of something simple like Super Meatboy you could try and log all the input and replay it. Just make sure it's a high enough resolution. If you want to do anything advanced it's a good idea to look at abandoning basic data types. For example you can create your own 'property' class. This class can do things such as log changes in it's value over time (such as in this case). Other features are Network replication. A unique identifier (useful for network replication). Serialization (also useful for network replication and saving). Mark areas of memory as being changed since last game load (allows for almost instant saving/loading). You could also do things like reflection (ie give the property a name, get a list at runtime of all the properties). Script binding and so on. 

glRotatef should apply to whatever matrix is currently selected by glMatrixMode. Just execute it after glMatrixMode. It's worth nothing that glMatrixMode is a deprecated command. The new way of doing things involves using shaders, binding a matrix to them and multiplying them together with the vertex coordinates. 

You can also look at using different level of detail meshes (LODs) based on the distance from the camera (or generating them at real time using geometry/tessellation sharers. 

It looks like your problem is that your not resetting your GL_PROJECTION matrix after you draw the zoomed in board. Then next render loop its still using the zoomed in projection matrix. When it draws the next board it will set the projection matrix to the same thing again. Either set it to the default unzoomed matrix at the beginning of the loop or after you draw the board. Otherwise you can use glPushMatrix and glPopMatrix to store and revert to the previous state. EDIT: Also be aware that glMatrixMode, glLoadIdentity, glOrthof, glTranslatef, glPushMatrix, glPopMatrix are all fixed pipeline functions that are deprecated/removed in modern OpenGL versions and have been replaced by uploading matrices to shaders. Might be fine for learning (then again it might be best to learn the proper way from the start). EDIT2: