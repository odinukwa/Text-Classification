Community wiki answer: I think the MSDN is fairly clear. The R Installer is on the SQL Server 2016 install media; it's the last item on the 'Installation' tab called 'New R Server (Standalone) installation'. Alternately you could follow the instructions for Installing R Components without Internet Access. Practice on a VM if you want to confirm. 

If you use a RAID partition, you will get fault tolerance in case of drive failure. If you instead stripe your database across N drives you are N times more likely to suffer a fault causing you downtime. If you are paying for Enterprise Edition, why on earth are you trying to save money on drives? Spend a few thousand dollars and build a decent RAID. Or use software RAID. Every OS has it built-in. It'd certainly be more real-world tested than most hardware RAID software. 

Community wiki answer originally left as a comment by the question author: I believe I have solved this problem but I don't know why. In short, instead of using SSMS on my PC, I remoted into the server logging in with the account associated with the new credential. I removed the cred, created it again (this time SQL Server didn't error at the ) and then I attempted a test execution of one of the jobs. It worked?! Apparently this changed something and helped SQL Server resolve the account. 

There are more details, including a bash script to perform the actions listed above in the linked post. 

Community wiki answer: Modify the job step to record job step output, and configure the stored proc to some text at the beginning of the proc code. You should be able to tell if the proc is being called by looking at the job history with that in place. Seems like you have some troubleshooting to do. It's also possible the proc is returning an access denied error as a result of not being able to send the mail to the SMTP server. 

...each might get a good enough plan to make it worth while. If each query applies the predicate early in the plan, you wouldn't have to join so many rows that are ultimately filtered out. 

Community wiki answer: The benefit here is that while you are loading your data in the staging tables (maybe it takes 8-10 hours), your destination tables are still accessible to your reporting queries and they are logically consistent (for example, contain data up to yesterday). As soon as you finish your data loading, it takes one second or less to switch in your staging table. Once completed, your reporting table has logically consistent data up to today. So the advantage is not only availability, it is consistency. If your ETL takes a long time, and you load your tables one by one, the overall state of your database during the loading process is not consistent; you have some tables with data up to yesterday and other tables up to today. Concerning performance: load the data using minimally logged operations like , and add nonclustered indexes only after the load is completed, and just before the switch to match the structure of destination tables. So there is also a performance gain. See The Data Loading Performance Guide for more details and techniques. 

Community wiki answer: You could use the following script, which has been used to find bad data. It might be not fast, but it has worked for me on several occasions when I needed it: 

Community wiki answer: Your condition only eliminates about 50% of the rows in the table. In that case the Seq Scan will be faster than the Index Scan. See for example this Stack Overflow answer, where user a_horse_with_no_name says: 

Foreign key validation has to occur under (locking) read committed for correctness. See Snapshot isolation: A threat for integrity? by Hugo Kornelis for details. The deadlock graph shows two concurrent executions of causing the deadlock. Your triggers are missing a join condition between and . It seems likely that this is an oversight and your trigger is accidentally updating all rows in so deadlocks are extremely likely to occur if there is more than one concurrent trigger execution. 

Community wiki answer: The long and the short of it is: Most workloads for SQL Server are OLTP which benefits from higher clock speeds because it is a serial operation. Unless you're specifically designing for a massively parallel system, clock speeds will always win out. Edge cases exist but that's the 95% of the time answer. The fact that it ends up costing less is also a nice thing. 

I would only recommend #2 if you don't really need/want the users managing their own subscriptions. If the users are going to need the ability to generate their own subs then #1 is your only real option. 

Community wiki answer: These will show up in the ring buffer with extra information, see Connectivity troubleshooting in SQL Server 2008 with the Connectivity Ring Buffer by the Microsoft SQL Server Protocols team. 

The inner join to drops rows (in particular, , but also the "outside" boundary, so you end up with only returned in my example). Change this to a . This is SQL Server 2008 R2 (see question tags) so doesn't work. 

The MySQL documentation at 14.21.3Â Troubleshooting InnoDB Data Dictionary Operations has the official guidance. Aleksandr's alternative suggestion is: 

Created a view in DB A, joining all tables in it. Granted access to the user on that view, and NOT to any of its tables. User was successfully able to query the view and not the tables. Created a view in DB B, joining tables in this DB together with the view in DB A. Granted access to the user on this second view, and also NOT to any table. User was successfully able to query this final view and see data. 

Community wiki answer: You could create a on that table to capture the row(s) and the date deleted and store that information in a history table. Then the base table and the history table (perhaps using a view) to get your results. You may need additional logic in triggers if the "same row" (identified by some key) could be inserted and deleted many times, and you want to count all those actions as just one change. The question does not specify whether that would be a concern or not. 

I suggest loosening your resolve for a fully declarative solution. There may not be a declarative way to solve your problem, and there are many cases where dynamic SQL is a much better solution in terms of performance - even if it lacks readability. Dynamic SQL gives the optimizer the opportunity to use different plans optimized for different parameters instead of having to come up with a single plan that solves all permutations well. The latter is quite difficult to achieve. Your problem is called relational-division. See questions in that tag and: 

This wait is associated with a sort operator that runs out of memory and has to spill to tempdb. It is not immediately obvious why rebuilding an index would involve sorting, perhaps there are some details missing from the question. 

There may be additional considerations in the AWS RDS world. Note that is deprecated in favour of . The documentation says: 

Community wiki answer: The problem was the architecture. We had 32 bits; we have now migrated our system to another server with SQL Server 2014 Enterprise 64-bit and all our problems are over - MelgoV (question author). 

Triggers or a job that runs every 5 minutes could handle making sure that all options for a product are represented for each project. However, forcing an update is a matter for an application, as is forcing someone to set the default values in the first place. If you create a record for the default but leave the value NULL (assuming that's not a valid option). you can use that in the app to signal that values need to have a default set. 

Answer originally added to the question by its author: As per Jason's answer, I issued the following update instead: 

You mentioned you added to the PK, but it was not partitioned. You also need to specify the partition scheme when you recreate the PK index so that it is aligned. Also, note there are restrictions on tables with foreign key relationships. See Transferring Data Efficiently by Using Partition Switching. 

Community wiki answer: You could use a computed column for . No need to be actually stored. Would something like work for you? 

Research Tally Table and Numbers Table for better, more efficient, ways to do this. Here's a link about tally/numbers tables, and here's a link about creating a calendar table. 

Community wiki answer: This is normal and expected behavior for a cluster. It is the responsibility of the application to handle the disconnect gracefully. Any in-flight transactions will be lost, since only committed transactions are replicated between servers. 

Or you could try . One problem is that table variables don't have statistics and that causes the optimizer grief - using can frequently solve this. Another typical work-around is to materialize the table variable into a local temporary table (which does support statistics) and use that instead. An example with query plan (nested loop join changing to a hash join) can be found in Improving SQL Server performance when using table variables by Matteo Lorini. 

Community wiki answer: Postgres will perform a step called view-resolution before executing the queries that contain a view name instead of a table name. This step will copy the select statement definition of the view into your executed select statement. If you have an index on your ID column in the source table, this index will be used. Otherwise if ID is a stand-in for multiple columns from multiple source tables, you can create a materialized view with an appropriate index on it. If ID is a single column in the source table it is unlikely that a materialized view will yield a speed-up of your query, because the first and second queries are structurally different. In the first case Postgres makes 1000 (if your example actually had that many different ID values) single queries and concatenates all results in a single result cursor. In the second query Postgres is able to see the big picture and can better choose a strategy for selecting from an index (e.g. a range scan). 

Within a given transaction, completed queries would be considered as if they were committed. prevents you from seeing rows written/updated by other transactions before they finish, not by the transaction you're in. From the Microsoft documentation Transaction Isolation Levels: 

Community wiki answer originally based on a question comment by RDFozz. Please edit it to improve it if you can. 

Indexes are good to reduce many rows to a few rows based on a condition in your query. Unless you have a very unequal distribution of values, a "not equals" condition will most probably not be considered for an index lookup. So, it all depends on how selective this index would be. Unless it eliminates much more than 50% of rows (usually 90%+ is desirable) it's probably no better than a sequential scan. See Use The Index, Luke 

Answer originally left as an edit to the question by the author: Solution Used After presenting different options to System Admin in charge of the project, decision was made to go with BitLocker drive Encryption. I stopped the SQL Instance during the encryption process and had no problem restarting it after encryption was complete. 

Hopefully, this last schema update will run quickly once all values are non-null and the default is in place. 

Most of the Stack Overflow solutions would work as they are, or with minor modifications for SQL Server. Note how the more complicated queries, with multiple joins or subqueries (that would need dynamically produced code for the arbitrary cases) are more efficient than the query. 

Community wiki answer generated from comments left by the question author: I was able to create a script to parse the JSON objects without using JSON or APEX JSON packages. 

I think it's odd a view is able to query tables in its DB that the user doesn't have direct access but is unable to do it in tables from other DB. At least it worked. 

Once that's complete, I plan to run the final schema adjustment to make that new bit column non-null: 

Community wiki answer: You can create an index on a column, it just won't be as efficient as if you had used a more appropriate data type. But searching on the index will be better than a full table scan. It would probably be better to create a new table rather than changing the existing column type. You could do that with 

Community wiki answer: A generic reason that affects those new to is forgetting the semicolon at the end of the command, so that it doesn't get executed. Otherwise your command looks good and should do what you want. 

Community wiki answer: Use a domain account with access rights on the share, then this should work. will not be able to reach it. If using a domain account is not possible for some reason, you can try granting the machine account permission to the share. The machine account follows the following format: . The is needed to denote it's a computer and not a user.