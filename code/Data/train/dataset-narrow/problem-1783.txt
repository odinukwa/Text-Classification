Folder Redirection in Group Policy is the answer to your prayers and is designed for exactly this - however, you should use full UNC paths (\\SERVER01\HomeFolders$) rather than mapped drivers. Here is the place to start reading: $URL$ 

There is a built in "Event Log Readers" group on the local machine. Add your users to that. Of course, if you want to use an AD Group, then just add the AD Group to it. You could also use Group Policy Preferences to add users and groups to this group on your domain computers. 

So, you end up with HO-DC01.ourcompany.com, along with EC-DC01.east.ourcompany.com and WC-DC01.west.ourcompany.com With the added benefit that everything still makes perfect logical sense (And works!) even when not referencing using a FQDN. 

Moving offline and booting on a different architecture should work fine. You don't say what operating system you have, but Windows should simply detect and install drivers for the new processor. With that said, there's no guarantee. The problem is that the CPU is paravirtualised, rather than fully virtualised, meaning the virtual machine does have some exposure to the real instruction set. Ultimately, the only thing to do is try. For machines that don't work, you will need to do a P2V style process using XenConvert. Live migration, on the other hand, is a different topic as it's incredibly picky about processor similarity even in the same processor family, let alone manufacturer! 

If you perform a from the command prompt then you'll see a list of routes and their metrics. 0.0.0.0 denotes the Default Route, so if traffic is going through both NICs then I'd say you either: 1) Have a default gateway set on both NIC's - this is bad, so remove the DFGW from the second. 2) The other traffic which is going through NIC2 is on the same subnet as the NIC's IP. Windows would never route traffic on a different subnet through a NIC which is not on that subnet, or that doesn't have a gateway set (Or a static route) I'm not understanding your position well enough to offer you specific advice. Could you update your question with the IP configuration of both NIC's and what IP's need to go where. 

In my opinion, the article isn't wrong as such, but it is badly written and unnecessarily focused on failures due to power. All the issues he alludes to are problems of management, documentation and design. You should avoid using dasiychained switches where possible, but there's nothing inherently wrong with it in certain cases. Running a cable all the way back may not be cost effective, (Remember, Ethernet has distance limits, and fiber will be even more expensive especially if your infrastructure isn't geared up for it) and the access layer of a network will nearly always have single points of failure. If the network is documented, then these should take minutes to diagnose. I also don't really see his point about 'fried' switches, either. Whether the switch is daisychained or not - if it's going to go up in smoke, it's going to go up in smoke. Finally, it's pretty rare that I see access layer switches on UPS anyway. Some people do it, and it's nice to have, but in the case of a power outage then the devices connected to it will probably not be on UPS anyway. Ultimately, it seems that his real gripe is with networks that grow organically, with no forethought and without consultation. I agree with this completely, but it's that there is a hidden switch behind a cupboard in the first place that's the problem, not how it's connected to the rest of the network. Just to reiterate - I'm not saying daisy-chaining is a good idea, but I do find that article very flawed in its reasoning. Finally, your core question still makes no real sense to me. 

You've answered this yourself with your CUPS server accepting everything as PostScript. Each printer has different features, capabilities and support which is why different printers require different drivers. Remember, Postscript and PCL aren't mandatory. It's easily circumvented when discussing text and a simple B&W laser printer, but take it to the extreme. You have a 50 page booklet in MS Word and you want to print to a big complicated multi function printer. Firstly, where would you configure Duplexing, which tray to get the paper from? These options come from the print driver - so is the print server expected to interpret the options and display it to the client somehow? Secondly, when you click print, what exactly is MS Word meant to do with this document? Send it as a raw document - imagine the processing overhead? Or maybe MS could develop a custom universal driver - entirely possible, but it's unlikely to support complex features nor have any guaranteed success. 

My second thought was to find a relatively unique form name, such as and search the whole registry. Surprisingly, the entry crops in quite a number of places. My suggestion, therefore, would be to build a blank 32 bit machine (32 bit to avoid confusion with shadow keys) and go to town on some of those keys. If you get it working it should be easy to transpose to wherever you need to do this. However, it has to be said, that given how much Microsoft clearly don't want you to do this, you are opening yourself up to support and compatibility issues. 

Without meaning to be obtuse, but you've absolutely answered your own question. To my knowledge, there are no real underlying issues doing this - it'd probably be better with a SAN so that you get a proper LUN and therefore drive letter presented to the server, but this should work okay. The key issue will simply be throughput and if you've tested it and you're happy, then that's great. There is, however an article here that implies you may be better off using full UNC paths rather than mapped drives. 

The volume licensed editions of Microsoft Office, at least, allow you to create an MSP to do unattended installs. You can then roll this out using the software delivery mechanism of your choice - Group Policy, SCCM, Altiris etc. 

You give no context, but an image will generally be a complete bit for bit copy of the entire hard drive of a machine, whether virtual or physical. They are normally transportable and are standalone ready to be imported or imaged onto another machine. A snapshot is a virtualisation technology that relies on delta discs to work efficiently. They are intended for short term use and are useless without all other virtual machine files. There are plenty of resources out there for learning more. 

Without a doubt, your host should not be running AD DS. Definitely create a virtual Domain Controller. 

Lots of things - it sounds like your understanding of Roaming Profiles is fairly lacking. The user registry (Which is really NTUSER.DAT) is only one half of the story. I'd suggest browsing to your roaming profiles location (The one on the server, rather than the workstation) and you'll all of the other stuff such as Application Data and maybe the Desktop etc etc. All of the things you see there are uploaded at log off, and downloaded at log on. 

The fact that it's virtualised is almost immaterial. You configure the IP address as per best practice for any server on your network. What would you do if you were installing a physical server? If it's set it statically (Almost certainly the right answer) then do this. If it's using reserverations in DHCP (Which is also acceptable) then do that. With the latter, again, it's the same as a physical server. Yes, you'd need a list of MAC addresses and, yes, those would need to stay the same. With XenServer you can either stick to the generated MAC or define your own - though I'd recommend against this unless you have some specific need. How you're doing it sounds fine to me, assuming provisioning servers isn't a daily task. If you're trying to completely automate server deployment in a "cloud" style manner, then that's something a little different. There are products out there to help you do this, but nothing really built into vanilla XenServer. 

The correct way is to have two different Web Interface sites set up. One for external use and one for internal use. The even better way is to put the CSG in between two firewalls, or in your DMZ and then have another Web Interface server behind your firewall for internal use. Alternatively, can you not configure two different connection types? One Direct, one Gateway Direct? 

Personally, if this is a centralisation project, I would use one domain and use OUs to separate things. This saves having to have full redundancy for each child domain, makes roaming and movement easier and cuts down massively on configuration and complexity. There are almost no downsides if you configure sites and services properly. 

is the GUID of a particular GPO on your Domain Controller(s). You need to find this GPO and temporarily disable it. This will help you narrow down whether you simply have an issue with that GPO or further issues. You may be able to find out what GPO it represents using this Powershell command: 

On a very fundamental level - if your hardware is as static as you say then all you need is a Windows 7 image (On that hardware, of course) which is configured as you require and is then prepared using Sysprep. You can then use something like RunOnce commands to do things on first boot. In my opinion, that's actually going to be the most challenging part of the project. If you're new to Windows deployment then you need to do some real development and learning work - especially around Sysprep and automated deployments. Beyond that, any of your Linux imaging tools that can read NTFS should work fine. For example, I use CloneZilla on a regular basis for imaging Windows gear. (Sorry, I'm not sure which actual tool within CloneZilla I use). Here are some starting points: Sysprep - $URL$ Windows 7 AIK - $URL$ 

As @jscott kindly points out below - a great alternative to custom ADM's is to use Group Policy Preference Registry settings. Ticking the "Remove this item when it is no longer applied" solves the tattooing issue. They're not so portable or distributable, but they're highly transparent and easy to use. Plus, if you copy a GPP (Or a group of them, whatever) using the GUI, you can past the results to a text file. It's all XML, and you can simply paste them again in future by copying the text and pasting in the GUI. 

As an aside, depending on your infrastructure there's nothing to stop you from simply backing up your machines as though they were physical servers. I.e., you could just use Symantec Backup Exec, DPM or whatever else. I'm not saying this is necessarily preferred or anything, but it may be the most supportable option for you. 

There are two fundamental things you need to do: NB: Do these in order, and test the first section by using your static IP from outside of your home. Router If your server connects to the internet through a router, then you need to either add your server to the DMZ (This will direct all traffic to your IP address to the server. This is dangerous!) or forward the specific ports that you need to it. To do this, you'll need to log onto the router itself and find "Port Forwarding" DNS You need to configure www.mydomain.com to point to your static IP. Do this by going to your registra (The place you purchased it from), and figure out how to add an "A Name Record". This record will contain your static IP. This may take a few hours to take affect, so be patient. Test it first by going to the command prompt and pinging wwww.mydomain.com and looking to see if it's returning your static IP. 

Just for completeness, there are 3rd party products that do allow roaming between operating system versions. Three examples off the top of my head: $URL$ $URL$ $URL$ 

is ultimately only ever going to be relative. However - you need to be aware that failover / HA will only protect against host failures - not Operating System / Application level failures. It's key that you take into account the different failure modes and the different ways to protect. For example, HA will also require shared storage - this in itself could become a single point of failure for your server. You need to weigh up the likelyhood of such failures along with how detrimental downtime will be. 

There is a limit of 10 NIC's enforced in vSphere: $URL$ XenServer supports 7: $URL$ Hyper-V 2008 R2 support 12 (But caveated): $URL$ 

No, you need to enable "Read" as well. And possible "Read & Execute", too, depending on the contents. 

You could use psexec from Sysinternals to remotely run the command "quser". This will list all logged in users, along with whether they're at the console or on RDP. That said, I don't think it should automatically take other sessions and I'm pretty certain you can have up to 2 as standard. 

Within each of the latter 3, each 'level' can have multiple GPO's and their order is decided by the system administrator. This is called the "link order" and the lowest number is processed last, which means that policy has the final say. OU policies are applied starting at the "root", and then downwards, if that makes sense. Here is some good reading on the subject: $URL$ With regards as to what to actually do with the individual GPO, well that kind of depends on the policy itself, but generally, they have the following three options: