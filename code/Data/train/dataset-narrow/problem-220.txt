If that doesn't do it, sometimes disabling CDP on the interface can help by forcing it into using the IEEE class negotiation for 802.3at rather than CDP. Often not as power efficient, but may resolve the issue. 

There are a couple of options. If you are doing DHCP snooping, you should be able to find the IP address in the binding database with the following command: 

I believe the section you quoted from ISO/IEC 7498-1:1994(E) may have been modified from a previous version of the standard to adjust the standard to the reality of UDP. Unfortunately, I can't find the copy I used to have of ISO/IEC 7498:1984, or its two amendments/corrections, to confirm this (not willing to go through all the floppy disks I still have to find this). This would explain the widespread appearance of this conflicting information; especially as many people often refer to the derivative works of others rather than going directly to the source themselves. This same effect has also sometimes lead to inaccuracies in some information being propagated. However, since these are simply conceptual models, this is not an actual problem. Consider for a moment the TCP/IP model. This model does not have a "definitive" source for the model, but is generally considered to be based off RFC 1122. This has resulted in a situation where some people consider the TCP/IP model to have four layers and others to view it as five. Additionally, people have given the different layers different names that they believe it to be more representative of the nature of the layer. None of these are wrong, but neither can anyone claim that their representation of the model is the correct one. Wikipedia provides a number of these on this table. Yet the TCP/IP model is still relevant and useful despite all these discrepancies. Although I personally believe this is a large part of why network professionals often gravitate toward the OSI model (which is defined by a standard). 

An ad hoc network is simply a network where two STAs connect directly to communicate. If you have three STAs in close proximity, each STA will make separate connections to each of the other two STAs. This continues to be true as you have more STAs; five STAs in close proximity would each need to manage four connections (one to each other STA), 10 STAs means 9 connections per STA, and so on. Communication in an ad hoc network only occurs directly between two connected STAs. Unless someone runs another process on the ad hoc STAs, there is no means of forwarding or passing along data intended for another destination than the connected STA. 

As I currently do not have the full standards available to me, the best answer I have seen on this topic is from the forums at the BICSI website. Based on this post by an employee of Fluke Networks (manufacturer of a number of popular cabling test units), there appears to be no minimum length in the standard. However, there are both an implied minimum length and recommendations for minimum lengths when using minimally compliant components for Cat5e/6 and Cat6A. Below is the applicable quote: 

The answer is that it can be either broadcast or unicast - and in some cases both unicast and broadcast before it reaches the client when an is used. A client doesn't actually have an IP address until the DISCOVER-OFFER-REQUEST-ACK exchange is completed. It is possible (although very unlikely) to have a situation arise where the server will resond to the REQUEST with a NAK. However, since unicast is generally preferred over broadcast, many clients will accept a unicast reply that matches their L2 address. 

So your Little-endian address hex representation of the bits is 12-34-56-78-9A-BC, but the very same address in a Big-endian network would be 48-2C-6A-1E-59-3D (if I did all that right...been a while since I had to convert Little-endian addressing to Big-endian). The exact same address, just a different way to store/transmit the bits. Heading off the beaten path, so feel free to stop reading... This lead to some really interesting interactions with TCP/IP. Since Ethernet (Little-endian) came before Token Ring (Big-endian), ARP was designed around the Little-endian addressing. When you bridged traffic between Ethernet and Token Ring, the bridge has to support IP (in order to recognize ARP packets) and re-order the bits in the ARP address fields appropriately. This was a less than ideal solution as it forced L3 functionality on L2, but it did resolved the problem. When FDDI (Big-endian) came along even later, the TCP/IP designers wanted to avoid this problem and designed ARP on FDDI to use Little-endian for the address fields. This avoided having to modify both the headers and the payload of ARP packets when bridging Ethernet to FDDI. However, it also meant a bridge connecting Token Ring to FDDI would need to re-order the bits in the address fields. It was just much rarer to bridge Token Ring to FDDI than it was to bridge Ethernet to FDDI. Other protocols such as AppleTalk were also similarly affected, but generally weren't provided a solution as they were not as prevalent as TCP/IP. So, if you were to bridge Ethernet to Token Ring, this would break AppleTalk. 

You are correct, they leave the IP information so that you don't accidentally lose connectivity when working on an AP remotely. As you noted, the button (which requires you to be local) will default the configuration including the IP. You can also do this by entering the following command: 

With the arrival of 802.11n and with 802.11ac almost here, this can be a viable option. However the expectation of cost savings may not be accurate as you may have additional initial costs (for instance hiring a wireless consultant to make sure things go right) or even in ongoing costs (do you need new staff?). You will carefully have to consider your path forward and how this impacts your users and IT staff. That being said, there are a number of things to keep in mind when considering not deploying a traditional 802.3 Ethernet network (the larger the wireless network and number of client devices, the more these are true): 

1000Base-T strictly speaking does not need auto-MIDX. However, when the standard was written, inter-operation with older technologies was taken into account. So to keep things simple, when communicating with a 100base or 10base device, auto-MIDX is there so crossover cables are no longer necessary. Edit: I forgot to mention, while auto-MIDX is in the standard, it is an optional feature, so it may not be present in all hardware. 

It can be tough to troubleshoot when you don't know where the problem lies and you have three devices from three vendors before you seem to have any visibility (the SRX100, the 1400, and the IP670). I can't speak to the specific devices, but you can never go wrong by tracing the packets. Since the ProCurve 1400 is an unmanaged device, you would need to use a network tap. You would want to capture the traffic in the following locations (based on your statement that the 3500yl is not receiving the discover packet): 

Yes and you really should have done so before making the change. When other parties are involved, it is almost always best to reach out to them before making any changes. Reverse the situation, what if the other person were to make such a change without informing you? Go a bit further and say this disrupted operation on your network in some fashion. Wouldn't you prefer that you had been informed before hand so you wouldn't have to spend time and effort troubleshooting an issue created by someone else? 

The advantage of capturing based on MAC address is that you are guaranteed to get all traffic sent or destined to the host. If you only capture based on IP address, you will not get any non-IP based traffic such as ARP. This would also include any IPv6 traffic if you are capturing based on an IPv4 address. If you are using an IPv6 address for your filter, you wouldn't get any IPv4 traffic and capturing based on IPv6 can be a bit more complicated as there is often a link-local address, a global address and multiple IPv6 multicast addresses that would apply to the device. 

This can introduce a number of problems, like additional attenuation or cross talk. Splicing is to be avoided whenever possible, but I have seen this work in a pinch although I would never recommend it. They key is to use a cable certification tester (not just a continuity tester) to make sure it still passes your required standard (Cat5/5E/6) after splicing. I would also be concerned about the manner of the splice. Twisting and electrical tape are not a reliable splicing method. Even if it is working properly initially, there is too much chance of problems to occur over time as the cables move slightly (temp changes, building vibrations, large vehicles driving by, etc). The very first type of problem you can experience would be the inability to connect over the cable. Since it appears you can do so, then the next problem would be frame loss, often from corrupted frames that will get dropped when they fail a consistency check. If you have real equipment on both sides of this link, you should be able to view the interface statistics and see how many frames are being dropped as well as the reasons. 

Based on the characteristics of the graph provided, this appears to be an MS Excel generated line chart. The question is the nature of the data used to build the chart and how the data was imported. Do you even have assurance that the data represented is accurate? What time zone do the times represent? Each data point on this chart represents a day and is connected by a line. Without any other frame of reference, there is no way to understand what these data points are representing. Here are some possibilities: 

Your problem is that you are trying to capture CSMA/CD behavior in a full duplex environment. If you want to capture this behavior, you need to connect your devices to a hub, not a switch. 

That is the point of DHCP relays, so yes. DHCP relay takes the broadcasted DHCP discovery and converts it into a unicast packet destined to the DHCP server. 

Mike has posted a great answer, but let me provide some examples to illustrate the problems inherent in flooding all traffic. Imagine you have a switch with 24 1000baseT ports (i.e. capable or running at 1000 Mbps - while not really achievable in the real world, for simplicity let's say we can). Ports 01-20 have computer/workstations connected to them, ports 21-23 are connected to servers, and port 24 is connected to the "internet gateway." 

With this many APs deployed and as many users as you expect (and the other non-802.11 devices they bring with such as bluetooth), what is the noise floor in 2.4GHz? It may be that your noise floor is in the -70 to -80 range, which means that "usable" signal outside the room is nominal. You mentioned RF paint (or mesh), behind the APs. Another option to consider is using it on all walls and deploying two APs per room (for redundancy and extra capacity). You may also need to use it on ceilings of rooms below. However, test any product before using it widely as some do not work as advertised. You mention that your transmit power is set to the lowest setting (2dBm) and your antenna is 6dBi. Since you can't turn down the transmit power anymore, you could consider a lower gain antennas or use attenuators to further decrease the signal strength. You will want to make sure that you are able to produce enough signal for a good connection (and high data rates) for a range of devices that will likely be found in the room. Test. Test. Test. High density deployments are a challenge, and the challenges change all the time as technology and the types of devices users bring into the environment are always changing. Unfortunately, you can't just set it in place and walk away. Realize that there will be problems and issues. This is just a fact of life in an environment where you don't have control over devices that will be brought onto your wireless network. Keep a list of device details that have been reported as having issues (type, make, wireless chipset, driver version, OS, what steps resolved the issue, etc) as this will allow you to start finding trends and general solutions so you can provide information (i.e. "try these steps" or "avoid this hardware") to your support staff and/or end users. Subscribe to mailing lists and utilize resources such as forums and industry blogs regularly to keep aware of problems that might be the result of OS updates or other problems. Some examples: Educause wireless-lan mailing list (probably applies since you are an education organization of some sort), Aruba Airheads forums, Ubiquiti forums, Cisco Wireless-Mobility support forums, Ruckus Room (while a number of these are vendor sites, there can be very good information on them and many wireless problems will apply to other vendor environments as well). 

When you connect any of the PCs (PC1, PC2, or PC3), the MAC address of the PC is learned by the switch and is inserted into the MAC address table. When the PC learns its IP address from a DHCP server or when there is a static IP binding configured on the switch, their is an entry added to the IP source binding table (containing the PC's MAC address, the assigned IP address, lease time if applicable, VLAN, and interface). L2 traffic is allowed and L3 traffic is allowed if the source IP address matches the information in the IP source binding table. Finally, let's look at an example utilizing both features working in conjunction. Again we will use the same three PCs and have the following configured on the switch interface: 

In 802.11 wireless (which I assume is your case), typically broadcast/multicast frames (as well as many management frames) are transmitted at the lowest base/basic/required (term varies by vendor) data rate. This is separate from the supported data rates. Typically, for best range and maximum compatibility, this defaults to the 1Mbps data rate, although in the past several years, some vendors have been increasing this default. Some vendors also now include a multicast-to-unicast conversion keeping track of multicast clients with some form of multicast snooping, however I know of no similar means for broadcast (a wireless device can't know all the clients out there that it may need to reach). If you are talking about an ad-hoc mesh network, then often you do not have control over this in the driver settings. 

If you actually stopped to think this through, clearly this can happen. All you need to consider is all the consumer/CPE devices that default their management interfaces to an IP address something like 192.168.1.1. Generally speaking, in a single local network (i.e. flat network or VLAN) a single IP address can only be assigned to one interface/device at any given time. The reason why is that other devices need to learn a correlation between a MAC address and an IP address. This is what the ARP process provides. However like many general rules, there are exceptions and in this case these are normally mechanisms used to provide some sort of load balancing and/or redundancy. Let's say you have two devices that both want to provide the same service on the local network. They could be programmed in a fashion where one responds to ARP for queries where the source MAC address is odd, and the other for even (or hashes or some other mechanism to group sources). This will allow the load to be distributed between the two. If your two devices can then also maintain some sort of "heartbeat" and have the ability to "take over" the other device's MAC address in case of a failure, then you now also have a means of providing redundancy. The presences of these capabilities and exactly how they work would be dependent on the features available in the device.