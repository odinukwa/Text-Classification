All of these commands are accessible for all users (all are views) but the views are restricted to just your current spid unless you have the permission 'view server state' which can be granted on a database by database level. A user is granted the basic permissions to view their own connection as it is information about yourself (Note that this will always report back that you are running a select command as that's what you're doing at the time) If you have the 'view server state' permission then you can run a query such as: 

Thoughts I know the above is not possible strictly like that and it would need to be done on a table by table basis of the data to move across. but not sure where to start or how to do it. Theoretically(in my head) this would be done with splitting the clustered index on a filter across the filegroups (and the files just deal with it), there's options on a table properties for storage for filegroup or partition scheme, however on a clustered index this seems to always be greyed out so I'm not sure if I'm just clutching at straws there. NOTE: it is potentially possible to upgrade from SQL 2008R2 to 2012 if that is absolutely necessary(but not desired) EDIT: The query for this would ideally be based on data in another table, so DataToOldSAN = where userid in (select id from users where active =0) Thanks Ste 

(And you can do other values in there aswell) There will be many other things you can try, but as mentioned initially the problem is when a query is ran, the SELECT part almost right at the end so you can't use that anywhere else in the query unforutnatly EDIT: Select doesn't come dead last in a query, full execution order for reference is: FROM-ON(JOIN)-OUTER-WHERE- GROUP BY-CUBE/ROLLUP-HAVING-SELECT-DISTINCT-ORDER BY-TOP 

I'm running a high transactional database (~175k transactions / Minute on average, almost 9M records per hour added and removed) Up till recently this hasn't been too much of an issue as we've been at ~7.5M records added and removed but with the latest influxes of data the ghost cleanup doesn't appear to be able to keep up with cleaning up the unused space on tables / indexes. A few days ago we reached 53 GB of 'Unused Space' across 16 tables (mostly 2 of them) so as a result started looking into the ghost cleanup process to find it runs once every 5 seconds and runs over 10 pages. My current solution is that early morning I am running three threads of the following command: 

One of our SSRS reports loads images from external sources and one of these sources has been locked down to tls 1.2 (which is fair and we're expecting more of our clients to be doing this in the future) The tls 1.2 update has been applied to the server holding the ReportServer database and the two Reporting Services endpoints held on seperate machines. All three machines have been tested locally and can all load the secured image in their browsers but when a report is ran using SSRS it fails I've set up a report for testing which has no database connection, it is simply a non secured image link and a tls 1.2 secured image link, The error I get is: This leaves the image as in the actual report So I feel like I've missed something in the installation of the tls 1.2 patch Anyone have any ideas what might be going on? 

Its not nice or clean and requires two lines per potential database but it'll get the job done (don't run it in dynamic SQL or you'll still end up with the same issue of the main thread not being changed) Note though that using USE commands is prohibited in procedures / functions 

There's two main forms of failover you can use with SQL post 2012, The Windows Failover Cluster (WFC) which has been around for a while and the Always on High Availability Group(AO) WFC uses a single database which can move between multiple machines this method requires the database to be installed on shared drives (such as a SAN) so that when the cluster moves the node the other machine has direct access to the information that makes up that database AO uses two completely seperate instances of SQL Server, both instances run at all times, (depending on how close they are you can set them up to be synchronous or a-synchronous, (if they're on the same network synchronous is better if they're separated globally a-sync is a better choice), due to there being two databases you do not require any shared storage, the two machines work completely independently of each other, we currently have ours set up with a quorum witness which is actually on a shared drive but is not technically required (it is advisable to have a witness however) Worth Noting about using AO, if you use the secondary node for any data access / backups (which you can as it can be accessable through readonly) you will require the extra licences to access it, if it sits there purely as a redundancy for when your primary node fails you do not require any extra licences as only one database will be accessible at any one time, with WFC since there is only actually one instance you only require one set of licences for that instance as it moves, you do have the disadvantage of only one disk failure will cause problems but using a RAID 10 on those disks will limit this from becoming a problem Ste 

All of these can be used in conjunction with each other to help paint a better picture of what is actually connecting to an instance or will give you these metrics apart from the which I find is the most reliable source of information to use in conjunction with the field, if its on a location you can investigate and is an application that runs for long enough for you to check. 

The dynamicSQL is not actually executed specifically in-line with the rest of your code its a separate entity (even though it is ran as if it is in-line If you run the code: in palce of your current set you'll notice the results that come back indicate that you have moved the active database, but you are still running under the same connection. If you want to change the in-line database selection the best way is to do something like this: 

If you have named instances and don't want to declare your ports then you need to ensure that SQL Server Browser is running By default this runs through port UDP 1434 

You've created the login for user1 and user2 to the master database not the database you want. rearrange to: 

Firstly as mentioned the Ola scripts are safe and very thoroughly tested across a lot of people. Your fill factor should only really ever be set to 100% (0) if the index will only ever add onto the end and delete from the beginning (aka a bigint identity field is fine) If you have data that will insert into the middle of data sets so in order of a person then when you're adding to that index you will want to insert into the middle of your dataset. if the fill factor is 100 then there is no space and it will create / add to a new record completely out of place fragmenting the index. There's no 100% definitive answer to what is the right fill factor to use, and it can change for different indexes on the same tables. if something is constantly fragmenting then lowering the fill factor can help alleviate that. NOTE reducing fill factor while can reduce fragmentation, WILL increase disk space usage. if you have an index that is FF100 and uses 100GB and you change it to FF50 then it will now be using 200GB of disk space for the same amount of data. 

Most server backups can effectively take the place of a 'Simple' backup solution where you only want a FULL backup of your database to be done, The database backups in 'FULL' mode will allow you do recover the database right up to the point of failure (if you do a tail log backup) Which means you can end up losing next to nothing, or if there was a problem introduced into the Database you can do a point in time restore and restore the database to just before the issue occurred (say someone decided to truncate a few tables at 13:27 you can restore the database and then run the transaction log up to 13:26 and you still have all of the data there.) We currently run both, the Server backup gets the machine back up and running, but the SQL server backups are used to get the database back up and running to the most recent point possible 

Depending what the colum is for your userID (i've declared it as TeamMemberUserID which you are using later) Simply Join the timesheet table to the User Registration table on the TeamMemberUserID and select the user.name field End result looks something like: 

The only people who should ever have access to run queries direct on a server is authorised people who actually know what they're doing. That being said if this is something that absolutely needs to be done here's my advice. Firstly create a new schema and fill it with views for any information they're allowed to access. The user will have connect and select access to that schema and deny to everything else. the user also wants to have a custom timeout set on it (easiest to set on the webpage) which is something short. 10 seconds maximum, if they're running a query which lasts longer than that then they're doing something too complicated or malicious. Also on the input I'd validate the input severely and harshly. no -- or /* allowed, words like 'cursor', 'create', '#' instantly get the entire query rejected. If there are some standard things which are more complicated, have a set of predefined queries (stored procedures with validated input for parameters(aka userid)) which they can select from a drop down somewhere which can run for slightly longer.