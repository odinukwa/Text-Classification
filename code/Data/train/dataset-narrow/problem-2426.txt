The short answer is: they don't. Whenever you introduce coercions, type classes, or other mechanisms for ad-hoc polymorphism into a language, the main design issue you face is coherence. Basically, you need to ensure that typeclass resolution is deterministic, so that a well-typed program has a single interpretation. For example, if you could give multiple instances for the same type in the same scope, you could potentially write ambiguous programs like this: 

Adding dependent types does not change the consistency strength of the theory. Simple dependent types have the same consistency strength as the simply-typed lambda, and the calculus of constructions has the same consistency strength as System F$_\omega$. In fact, pure Martin-Löf type theory ($\lambda{P}$ in the lambda-cube) cannot prove that 0=1 implies false. To do this, you need at least one universe -- and universes gigantically increase the strength of the theory. The calculus of inductive constructions (roughly speaking, $\lambda{P}\omega$ plus inductive types plus countably many universes) is equal in consistency strength to ZFC with countably many inaccessible cardinals. I do not know what the strength of the impredicative calculus of constructions is, if you add inductive types and large eliminations. 

Typically, you use binary parametricity to prove program equivalences. It's unnatural to do this with a unary model, since it only talks about one program at a time. Normally, you use a unary model if all you are interested in is a unary property. For example, see our recent draft, Superficially Substructural Types, in which we prove a type soundness result using a unary model. Since soundness talks about the behavior of one program (if $e : A$ then it either diverges or reduces to a value $v : A$), a unary model is sufficient. If we wanted to prove program equivalences in addition, we would need a binary model. EDIT: I just realized that if you look at our paper, it just looks like a plain old logical relations/realizability model. I should say a little bit more about what makes it (and other models) parametric. Basically, a model is parametric when you can prove the identity extension lemma for it: that is, for any type expression, if all of the free type variables are bound to identity relations, then the type expression is the identity relation. We don't explicitly prove it as a lemma (I don't know why, but you rarely need to when doing operational models), but this property is essential for our language's soundness. The definition of "relation" and "identity relation" in parametricity is actually a bit up for grabs, and this freedom is actually essential if you want to support fancy types like higher kinds or dependent types, or wish to work with fancier semantic structures. The most accessible account of this I know is in Bob Atkey's draft paper Relational Parametricity for Higher Kinds. If you have a good appetite for category theory, this was first formulated in an abstract way by Rosolini in his paper Reflexive Graphs and Parametric Polymorphism. It has since been developed further by Dunphy and Reddy in their paper Parametric Limits, and also by Birkedal, Møgelberg, and Petersen in Domain-theoretical Models of Parametric Polymorphism. 

I saw "a" CPS transformation, since as I mentioned earlier, there are many negative translations which let you prove this theorem, and each one corresponds to a different CPS trasnformation. In operational terms, each transformation corresponds to a different evaluation order. So there is no unique computational interpretation of classical logic, since you've got choices to make and the difference choices have very different operational characteristics. 

Plotkin used logical relations in his unpublished but nevertheless widely circulated and influential 1973 paper "Lambda Definability and Logical Relations". I have a copy of this note on my webpage. I used to think that this is where the name came from, but when I asked Rick Statman about this, he told me that Mike Gordon coined the term logical relation to describe Tait's method, and that he and Gordon Plotkin picked it up from him. I think this is how it entered programming language jargon, though you could make sure by asking Plotkin. 

This isn't a "nice" property, because whether it's true or false depends upon the encoding. See David et al's Asymptotically almost all $\lambda$-terms are strongly normalizing, which proves what it says in the title. However, this paper also shows that the opposite holds for SKI-combinators (into which lambda-terms can be compositionally embedded). In the lambda calculus, a reduction is the equivalent of a step of a Turing machine, and strong normalization is the property that every reduction sequence eventually reaches a normal form -- ie, no further reductions are possible. (Since a given lambda-term may have many valid reductions, strong normalization is a bit like saying a given nondeterministic Turing machine always halts.) So the fact that asymptotically almost all $\lambda$-terms are strongly normalizing means that with probability approaching 1, reducing a large lambda terms will always reach a normal form. However, lambda-terms can be translated in a meaning-preserving way into a combinatory calculus such as the SKI combinators (and vice-versa), and in combinator calculi asymptotically all terms loop. 

There was a nice paper at POPL last year, EigenCFA: Accelerating flow analysis with GPUs, which represented lambda-terms as matrices and then used GPUs to rapidly perform dataflow analysis on them. The paper didn't point this out explicitly, but what they were basically doing was exploiting the categorical structure of vector spaces to represent trees. That is, in ordinary set theory, a tree (of some fixed height) is a nested disjoint union of cartesian products. However, vector spaces also have direct products and sums, so you can represent a tree as an element of a suitable vector space, as well. Moreover, direct products and direct sums coincide for vector spaces -- i.e., they have the same representation. This opens the door to parallel implementations: since the physical representations are the same, a lot of branching and pointer-chasing can be eliminated. It also explains why dataflow analysis is cubic-time: it's computing eigenvectors! 

The subtraction $a - b$, when it exists, is the solution $s$ to the equation $s + b = a$. Truncated subtraction is the supremum of the set of solutions to the equation $s + b \leq a$. That is, when $b > a$, no solutions $s + b = a$ exist, and the supremum of an empty set of natural numbers is 0. Otherwise the supremum will just be the subtraction. So saturating subtraction is left adjoint to addition (i.e., $a - b \leq s \iff a \leq s + b$), and is hence a natural definition. Predecessor is just subtracting 1, so it should be saturating too. 

As a positive answer to your final question, normalization proofs of polymorphic lambda calculi such as the calculus of constructions require at least higher-order arithmetic, and stronger systems (such as the calculus of inductive constructions) are equiconsistent with ZFC plus countably many inaccessibles. As a negative answer to your final question, Ben-David and Halevi have shown that if $P \not= NP$ is independent of $PA_1$, Peano arithmetic extended with axioms for all universal arithmetic truths, then there is an almost polynomial algorithm $DTIME(n^{\log^{*}(n)})$ for SAT. Furthermore, there are presently no known ways to generate sentences which are independent of $PA$ but not $PA_1$. More philosophically, do not make the mistake of equating consistency strength with the strength of an abstraction. The correct way to organize a subject may involve apparently wild set-theoretic principles, even though they may not be strictly necessary in terms of consistency strength. For example, strong collection principles are very useful for stating uniformity properties -- e.g., category theorists end up wanting weak large cardinal axioms to manipulate things like category of all groups as if they were objects. The most famous example is algebraic geometry, whose development makes extensive use of Grothendieck universes, but all of whose applications (such as Fermat's Last Theorem) apparently lie within third-order arithmetic. As a much more trivial example, note that the generic identity and composition operations are not functions, since they are indexed over the whole universe of sets. On the other hand, sometimes the relationship between consistency strength and abstractness goes in the opposite direction. Consider the relationship between measures and motivic measures. Measures are defined on families of subsets ($\sigma$-algebras) over a set $X$, whereas motivic measures are defined directly on formulas interpreted in $X$. So even though motivic measure generalizes measure, the set-theoretic complexity goes down, since one use of powerset goes away. EDIT: Logical system A has greater consistency strength than system B, if the consistency of A implies the consistency of B. For example, ZFC has greater consistency strength than Peano arithmetic, since you can prove the consistency of PA in ZFC. A and B have the same consistency strength if they are equiconsistent. As an example, Peano arithmetic is consistent if and only if Heyting (constructive) arithmetic is. IMO, one of the most amazing facts about logic is that consistency strength boils down to the question "what is the fastest-growing function you can prove total in this logic?" As a result, the consistency of many classes of logics can be linearly ordered! If you have an ordinal notation capable of describing the fastest growing functions your two logics can show total, then you know by trichotomy that either one can prove the consistency of the other, or they are equiconsistent. But this astonishing fact is also why consistency strength is not the right tool for talking about mathematical abstractions. It is an invariant of a system including coding tricks, and a good abstraction lets you express an idea without tricks. However, we do not know enough about logic to express this idea formally.