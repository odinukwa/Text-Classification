Your algorithm is closest IMO to Monte Carlo Control, which is a standard RL approach. One of the big advantages of Q Learning is that it will learn an optimal policy even whilst exploring - this is known as off-policy learning, whilst your algorithm is on-policy, i.e. it learns about the values of how it is currently behaving. This is why you have to reduce the exploration rate over time - and that can be a problem because the exploration rate schedule is a hyper-parameter of your learning algorithm that may need careful tuning. 

Your problem is that neural networks work poorly when the input is not scaled to a simple range. A usual choice is to scale and offset each column so that it has mean 0 and standard deviation 1. In your case, and vary from 0 to 49999 and roughly 10 to 50009. This range for inputs will causes lots of numeric issues. With a balanced dataset as you have, 51% accuracy is basically just guessing (within experimental error), so the network has learned nothing. Try again with scaling - e.g. 

is technically correct, but not useful as an answer to your colleague's query. For classification, the terms confidence and probability are almost interchangeable, so you cannot really use one word to explain the other. If someone asked why one of the numbers was larger than another in your model, you would not answer "that is because x is greater than y" - essentially this is what you have done here. So your statement is not wrong as such, but neither does it explain the result. 

By transforming both your signal and kernel tensors into frequency space, a convolution becomes a single element-wise multiplication, with no shifting or repeating. So you can convert your data and kernel into frequencies using FFT, multiply them once then convert back with an inverse FFT. There are some fiddly details about aligning your data first, and correcting for gain caused by the conversion. If you have a good FFT library, this can be very efficient, but there is overhead cost for running the Fourier transform and its inverse, so your convolution needs to be relatively large before it is worth looking at FFT. I have explored this a while ago in a Ruby gem called convolver. You can see some of the code for an FFT-based convolution here and the project includes unit tests that prove that direct convolution gets same numerical results as FFT-based convolution. There is also code that attempts to estimate when it would be more efficient to calculate convolutions directly by repeated multiplications or use FFT-based solution (that is rough and ready guesswork though, and implementation-dependent). 

Note that the above analysis applies only to certain episode-based approaches. It depends critically on what you count as an episode, and whether the agent is able to take an action which ends the episode. 

The area of computer science this most closely relates to is natural language processing. There is a large body of work and ongoing research in this area. You have a few barriers to achieving interest in your ideas from people studying the subject: 

Yes, this is a key feature of multiclass logloss, it rewards/penalises probabilities of correct classes only. The value is independent of how the remaining probability is split between incorrect classes. You will often see this equation averaged over all examples as a cost function. It is not always strictly adhered to in descriptions, but usually a loss function is lower level and describes how a single instance or component determines an error value, whilst a cost function is higher level and describes how a complete system is evaluated for optimisation. A cost function based on multiclass log loss for data set of size $N$ might look like this: $$J = - \frac{1}{N}(\sum_{i=1}^{N} \mathbf{y_i} \cdot log(\mathbf{\hat{y}_i}))$$ Many implementations will require your ground truth values to be one-hot encoded (with a single true class), because that allows for some extra optimisation. However, in principle the cross entropy loss can be calculated - and optimised - when this is not the case. 

This is not true. Some options used in CNN architecture, such as max pooling, or strided convolutions, can add a moderate amount of translation invariance. However this will not cover larger translations - anything that is a significant percentage of the image width/height. CNNs also support translation equivariance, which is where image textures and motifs that appear repeatedly but in different locations (lines, corners, curves) are learned efficiently and appear in the feature maps. This is probably closer to the USP of CNNs, that they learn the "representation language" of a signal, such as photograph, where it is consistent across the dimensions that are being convolved. The cause of your problem is therefore very likely that your training set does not include enough images with the more central translation, or enough variations with the separate images. To cover large translations you could look at one or more of: 

The loss functions are only simple convex functions with respect to the weight parameters (and specific data) when there is a single layer. More exactly, they can proven to be always convex with respect to the weights in the simple models (linear or logistic regression), but not with respect to weights of deeper networks. You can prove that there must be more than one minimum in a network with 2 or more layers - and thus the loss function cannot be convex - by considering swapping the weights around when you have found a minimum value. Unlike with a single layer network, it is possible to swap the weights around that feed into the hidden layer whilst maintaining the same output. For example, you can swap the weights between input and hidden layer so that values of neuron output 1 and neuron output 2 are reversed. Then you can also swap the weights feeding out of those neurons to the output so that the network still outputs the same value. The network would have a different set of weights, but generate the same outputs, and so this new permutation of weights is also at a minimum for the loss function. It is the "same" network, but the weight matrices are different. It is clear that there must be very many fully equivalent solutions all at the true minimum. Here's a worked example. If you have a network with 2 inputs, 2 neurons in the hidden layer, and a single output, and you found that the following weight matrices were a minimum: $W^{(1)} = \begin{bmatrix} -1.5 & 2.0 \\ 1.7 & 0.4 \end{bmatrix}$ $W^{(2)} = \begin{bmatrix} 2.3 & 0.8 \end{bmatrix}$ Then the following matrices provide the same solution (the network outputs the same values for all inputs): $W^{(1)} = \begin{bmatrix} 1.7 & 0.4 \\ -1.5 & 2.0 \end{bmatrix}$ $W^{(2)} = \begin{bmatrix} 0.8 & 2.3 \end{bmatrix}$ As we said the first set of 6 parameters was a solution/minimum, then the second set of 6 parameters must also be a solution (because it outputs the same). The loss function therefore has 2 minima with respect to the weights. In general for a MLP with one hidden layer containing $n$ neurons, there are $n!$ permutations of weights that produce identical outputs. That means that there are at least $n!$ minima. Although this does not prove that there are worse local minima, it definitely shows that the loss surface must be much more complex than a simple convex function. 

There is unlikely to be any useful pattern analysis for this problem. I cannot prove it, but I think it highly likely that the raindrops are being generated using a pseudo-random process. Even if they are not, you have been given more than enough information in what your controller can "sense" in this simulation, so that predicting future raindrops is not necessary. Instead, this is a problem of optimal control and planning. In machine learning, often reinforcement learning can be used to solve problems like this - potentially Q-Learning would work here, but it would be quite hard to implement it so that it learned quickly due to the large number of states. You would need to use a function approximator like a neural network, and although that can be made to work nicely here, it does not seem necessary. I don't think you need any Data Science technique here, this is closer to more traditional AI planning. However, there are strong links between planning and learning models, so what I am going to suggest is also something you might see in game-playing systems that use a combination of machine learning (reinforcement learning) and planning. A quick analysis shows that you have a large number of visible states ($2^{42}$) which would take a long time to produce optimal rules for using the simplest reinforcement learning algorithms - although it is feasible. However, you also have perfect knowledge of the dynamics and a fully deterministic system within what you can see. In addition, the branching factor of your action decisions is not high, just 3 (and sometimes 2) per step - so for instance looking 8 steps ahead will involve checking under 10,000 scenarios. That seems possible to do on every time step, which immediately suggests a simple search-based planning algorithm - similar to Monte Carlo Tree Search, except in your case you can just brute-force all combinations up to an arbitrary horizon. As long as the horizon is far enough ahead, you can achieve optimal results this way, because it only takes 3 steps to fully traverse the different positions the collector can be in.