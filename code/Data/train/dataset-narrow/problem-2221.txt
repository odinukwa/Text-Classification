Move right until you find a ')' or the end of the input; if the latter, the expression is fully evaluated, and we are done. Mark the ')', go back to the left to find the matching '(', mark it as well. Create a copy of the prefix before the marked '(' behind the input. Find the operator between the marked parentheses, branch to a Turing machine performing the corresponding operation, depositing the result behind the copied prefix. Copy the suffix after the marked ')' after the result from 4. Go back, delete the original input, place the head on the first copied symbol, and go to 1. 

The definition you give is essentially the one you need. A probabilistic function from $X$ to $Y$ assigns to each $x\in X$ a subdistribution of elements of $Y$ (rather than a single $y\in Y$). Such a subdistribution is a function $\delta:Y\to[0,1]$ such that $\sum_{y\in Y}\delta(y)\leq 1$. In other words, a probabilistic function $X\to Y$ is a function $f:X\to(Y\to [0,1])$, i.e. $f:X\times Y\to [0,1]$, such that for all $x\in X,$ $\sum_{y\in Y}f(x,y)\leq 1$. In your case, both domain and codomain are $X^M$; a probabilistic function from $X^M$ to $X^M$ then is a function $f:X^M\times X^M\to [0,1]$ such that for all $x\in X^M$, $\sum_{y\in X^M} f(x,y)\leq 1$. The intended meaning of this is that the function still returns a single point when given some input, but this point is chosen randomly according to the relevant distribution. For example, if $M=2$, and $f(1,2) = \{(1,1)\mapsto 0.5,(2,2)\mapsto 0.5\}$ (all others being $0$), then when you call $f(1,2)$, you will get $(1,1)$ or $(2,2)$ each with probability $0.5$. 

Let me expand on that because it seems to be approaching what I'm looking for. Suppose an error-prone machine is one such that, with a fixed probability $p$, an attempt to change a value on any working tape fails. If it writes the existing value back that always works. Halting with an output means halting strictly more than half the time with the same output, and halting within a certain time means always halting within that time. The latter definition means that if we write each bit by sitting in a loop until it is correct, then the time becomes unbounded; this lets us simulate an ordinary Turing machine, but not with any time translation. But we can simulate an ordinary machine on the error-prone machine up to $t$ steps by writing every bit $b(t)$ times, making its probability of being incorrect $p^{b(t)}$. There is a $b(t) \in O(\log{(t)})$ sufficient to operate perfectly for $t$ steps more than half the time, and $t \times b(t)$ is only loglinear, but for a time translation we need a universal machine and in that situation we're not given $t$ in advance. So what I'm curious about now is how tightly we can constrain the time translation factor and still be correct half the time, and whether we can use something like this to characterize essentially linear time translations vs. quasilinear ones. I believe I can demonstrate that this model has a time translation in $O(t^k)$ for every $k>1$. Dedicate a working tape to keeping track of the number of times our universal machine is to repeat each emulated step. It contains a string of ones delimited by blanks at the ends. The outer loop of the universal machine walks the list of ones from left to right, attempting to repeat the same emulation step in between movements. When it reaches the end, it attempts to write another one, and then resets itself by walking back to the left end, finally advancing to the next emulation step. This seems like more than enough repeats to be half-correct, and we're effectively taking $b(n) \approx (1-p) \times n$ resulting in an essentially quadratic time translation. We can generalize this by adding another counter tape to control the extension of the first one, bringing the time translation down to $O(t^{\frac{3}{2}+o(1)})$, and repeating this gives us any superlinear polynomial bound. Is it possible to bring this down to quasi- or essentially linear? 

Without any further constraints, this expression will in general be unbounded, so the maximum won't exist. Let $V$ be $\{v_1,\ldots,v_n\}$ with $n\ge 2$. Pick $i\neq j$ such that $v_i,v_j$ are not both isolated. The submatrix of the Laplacian for $v_i,v_j$ has the form $\begin{pmatrix}d_i & -a_{ij} \\ -a_{ij} & d_j\end{pmatrix}$ with $a_{ij}\in\{0,1\}$ and $d_i,d_j\ge a_{ij}$, and $d_i+d_j>0$. For any $\varepsilon>0$, define a function $f_\varepsilon:V\to\mathbb{R}$ by $f_\varepsilon(v_i) = \sqrt{n}-\varepsilon, f_\varepsilon(v_j) = \varepsilon - \sqrt{n}$, and $f_\varepsilon(v) = 0$ for all other $v\in V$. With this, your expression becomes $(d_i + d_j + 2a_{ij})\frac{n-2\varepsilon\sqrt{n}+\varepsilon^2}{2\varepsilon\sqrt{n}-\varepsilon^2}$. $(d_i+d_j-2a_{ij}) \ge 1$ by the above, and for $\varepsilon\to 0$, the numerator and denominator of the fraction converge to $n$ and $0$ (from above); therefore, the expression goes to $+\infty$. 

If I understand the requirements correctly, the answer is no. I will restate (what I understand to be) the problem: We are dealing with graphs $(V,E,\delta)$ (undirected, though directed edges won't change the answer), equipped with a distance $\delta$ which generates a ranking $Rank_u$ for each node $u$. This means that there are no $u,v,w$ with $v\neq w$ and $d(u,v)=d(u,w)$; for any node $u$, $Rank_u$ is then the unique function $V\to\{1,...,|V|\}$ such that $Rank_u(v)<Rank_u(w)$ iff $d(u,v)<d(u,w)$. I will concentrate on the case $a=b=1$, i.e. consider the sums $s_1 := \sum_{v\neq u}\frac{1}{Rank_u(v)Rank_v(u)}$ and $s_2 := \sum_{v\neq u}\frac{1}{Rank_u(v)^2}$. The question is whether $s_2$ can be used to approximate $s_1$. Note that for a graph with $n$ nodes, $s_2$ will always be equal to $n\sum_{i=2}^n\frac{1}{i^2}$, which is in $O(n)$, since $\sum_{i=2}^\infty\frac{1}{i^2}$ converges. It is enough to find a family $(V_n,E_n,\delta_n)$ of graphs with $|V_n|=n$ for which $s_1$ grows more quickly. Consider this example: $(V_n,E_n)$ is the complete graph $K_n$ on $n$ nodes. The metric $\delta_n$ is given by $\delta_n(v_i,v_j) = 2^i+2^j$ for $i\neq j$ (and $0$ for $i=j$). The resulting ranking functions are 

The Hennie-Stearns theorem says that $k$-tape Turing machines with $k \ge 2$ are intertranslatable with loglinear blowup ($O(t \times \log{(t)}$). This would define an equivalence class of models, except that loglinear blowup is not closed under composition. So if we really want to abstract away from Turing machines, we'll have to consider at least quasilinear ($t \times \log{(t)}^{O(1)}$) blowup, which is transitively closed. However, we also have a transitive closure under essentially linear blowup ($t^{1+o(1)}$). Both classes preserve the exponent of polynomial-time algorithms and the base of exponential-time algorithms. My overall question is, what are the qualitative differences between these two classes and which is the more natural one to study (in terms of physical or other intuition)? And more specifically, what is an example of a model that is translatable to a multi-tape machine in essentially linear time but not quasilinear time, or in the other direction? I know we can construct one artificially by changing the definition of a time step, but I'm hoping for something that yields more insight. One example I came up with that illustrates the difference is an algorithm that counts all $n$-bit strings satisfying some polynomial-time test. This runs in $2^n \times n^{O(1)}$ time, which is preserved by a quasilinear blowup, but after an essentially linear time translation that becomes $2^{n} \times 2^{o(n)}$, meaning that it takes more than a polynomial to test each bit string on average, even though in the same model we can still test a single string in polynomial time. I'm imagining this explained by a machine that experiences random errors and is only expected to be right more than half the time; in order to usually guess the right count, it will have to be virtually certain about the presence or absence of each of the $2^n$ strings, that's why it spends more time on each string on average than the polynomial-time test that only has to be right $51\%$ of the time. [EDIT: I think there are too many errors in the below for it to be useful. I'm still trying to develop the same idea, a machine that has too much trouble keeping track of a count to be translatable in quasilinear time.] 

Since you say that $T_w:=\{\delta(q,w):q\in Q\}$ should have at most one element, I'll assume that you use the version of DFA where $\delta$ can be partial. Then this is a counterexample: $X=\{a,b\}, Q=\{0,1,2,3,4\},\delta(q,a)=q+1$ for $q<4$, and $\delta(1,b)=2,\delta(2,b)=3,\delta(4,b)=0$. $F$ and $q_0$ obviously don't matter for this question. The automaton is $6$-local, but not $5$-local, since $T_{abaab} = \{0,3\}$. Edit: this counterexample does not work, I'll keep it so that the comments make sense. The following does, though. Take $X=\{a,b\}, Q=\{0,1,2,3\}$, with transitions $0\to 1(a),1\to 2(a), 2\to 3(a), 2\to 0(b), 3\to 2(b)$. This automaton is $5$-local, but not $4$-local: for $aaba$, we get the paths $0\to 1\to 2\to 0\to 1$ and $1\to 2\to 3\to 2\to 3$, i.e. $T_{aaba}=\{1,3\}$. 

A simple class of examples can be found by considering singleton languages $\{w\}$. These are measurable (Let $C_n(w)$ be the set of words agreeing with $w$ up to the $n$-th letter, then $\{w\}$ is the intersection of all $C_n(w)$). However, unless $w$ is eventually periodic, the language is not $\omega$-regular. For a concrete example, consider the word $w=ababbabbba\dots$. If $w$ is accepted by a nondeterministic BÃ¼chi automaton $A$ with $k$ states, then while parsing the $n$-th block of $b$s, with $n>k$, $A$ needs to visit some state twice, so we can construct another word accepted by $A$ by pumping this block suitably. Therefore no automaton recognizes $\{w\}$.