However, when the application is running with live data over a longer period, we find that this no longer matches. 

You should migrate your mail using the utility from Dovecot. This will preserve the UIDs and even POP3 UIDLs if necessary. Run using the option, to 'reverse backup' from the remote IMAP server to the local Dovecot server. You need to have a special configuration file created, something like this: 

Example: A similar method can be used to protect a location using a different method, such as or . If you use , you can return the email address in the same query, from where it will be loaded into the environment. You can then use in your Apache config to push it into the HTTP headers to be picked up by the definition. See here for the documentation. 

Your Ironport is failing the set up an encrypted TLS session for mail transfer. Possibly, either your Destination Controls or HAT Policy is mandating encryption, or the remote end is mandating it. You need to have a Certificate installed and configured in order for incoming SSL/TLS to work. This may be self-signed, but if it is then a remote side cannot have a policy of verification. If you have a Destination Control mandating verified encryption to a remote domain, and their certificate is expired or has an invalid chain, then you will not be able to set up TLS. Similarly, remote sites mandating validated certificate will not be able to connect to you if you have an expired or self-signed certificate. If you have 'encryption preferred', as most domains so then it should fall back to an unencrypted link and so your email will still flow. 

dsync is 'idempotent' and will synchronise from whatever state you are currently in; there is no queue of pending changes as there is with something like MySQL replication. This means that, when server B comes back up, the next time a dsync is triggered, you will end up with both servers back in sync. No manual intervention is required, and they will get back into sync even if all the mails filesystem on B was wiped (though it might take a bit of time to achieve this). You would probably want to have users normally access only via server A, and in the event of A failing and your proxy redirecting them to B, remove A from the proxy pool until your dsync has completed and A is once again in sync. 

If done right, all you need to do is update the master DNS server, and have the rest as slaves, which will either poll the master or be informed by the master of a pending update. Once you have automatic updates set up, you can have as many slaves as meet your availability needs and then they'll keep up to date, provided the master itself is reachable. 

I'm not 100% clear on what your goal is, but there's a Unix reverse proxy program called pound (likely available in the Ubuntu package repo) that will allow you to redirect HTTP requests to different back-end servers based on matching expressions. So you would have pound running on your front-end Ubuntu server on port 80. On the back-end, you'd have Apache+Ubuntu running on some other port (8080, for example) and the Windows server(s) running on some port(s). You then tell pound which back-end server/port to forward the request to for each domain and/or URL regular expression you need to sort the requests by. I'm unsure why you wish to avoid a proxy in the mix, as this is exactly what you do need. I don't believe you can accomplish what you need without one. As far as such proxies go, pound is pretty small and fast. If it's caching you wish to avoid, due to the use of dynamic content, pound does not do any caching. 

I prefer either little shell scripts or shell aliases. In the finest UNIX tradition, I name them as short as possible to minimize typing. For example, I have an alias "ns1" which is the 1-line SSH command needed to login into the (obviously) first name server I maintain. Usually the command name is a short mnemonic for the machine name, and the un-prefixed version of that name defaults to ssh. So "ns1" will ssh me in, and (were it a Windows box) the alias "rns1" might fire off a remote desktop client command to that machine. If I had so many that I couldn't keep track of them all, I'd use the shell script method, and keep a description line on a comment in each one, with a common tag for all scripts. Then I'd write a small script that would print out the name of each file in my custom script directory followed by whatever I grepped from that comment line. Running that one script would then document to the screen each command and what it did. 

Note that the sender domain has no effect on how the message is delivered, and is only referred to if there is a bounce. Assuming the initial MTA is the one that handles the sender's mail domain (though it does not have to be), then the difference between your two cases is whether or not step 1 above handles the delivery or not. 

This assumes your DS to be names of course, and is not as efficient as doing a when you already have the required RRA. 

Now, the filesystem reports a usage of approx 110GB, but the zramfs device reports 165GB. At the same time, the zramfs memory is exhausted, and the filesystem becomes read-only. The zram figures confirm that we are getting a 2.2 : 1 compression ratio between orig_data_size and compr_data_size; however, why does the filesystem show much more free space than the zram device? Even if this is space already allocated for re-use by the filesystem, shouldn't it be reused rather than allocating new space? The data consists of a large number of small files which are added and removed at irregular intervals. 

Since you are using MRTG with Routers2, there is a generic cfgmaker host template available at $URL$ which will automatically generate MRTG configurations for many things, including the storage OIDs. These take advantage of the Routers2 additional features to give you combination graphs. It should work with any SNMP-capable host. You can use it with standard MRTG cfgmaker like this: 

Other people have reported a similar issue with FCGID processes hanging and being unkillable in other systems, such as Wordpress and Sympa. A suggested fix was to add the option 

In your example, you should be using the namevar, . For example, if you define a resource like this: 

If you place another gateway device between your Ironport and the Internet, then your only option is to disable SenderBase and any other IP-based authorisation in your Incoming Policy definitions in the HAT. You cannot tell the Ironport to obtain the previous-hop IP address from any other method than the incoing TCP connection itself (for obvious reasons - otherwise it would be far too easy to forge). One option would be to reverse the order of the devices -- IE, put the Ironport between the Internet and the Proofpoint box, and set the Ironport to have a fixed SMTP route to send all incoming email via the ProofPoint. Otherwise, you lose out on the Ironport's Senderbase rules, which are (IMHO) one of the primary benefits of the Ironport. 

There is a maintained Windows port of Squid. You can get pretty fancy with Squid, as far as ACLs go. I'd give that a try first. 

You should check out asciidoc. I've made a few short things with it, and the output it pretty sharp (and customizable, of course). The plain text is very readable by design, and you can have it output docbook, HTML and PDF easily. Using any number of other converters you can transform it to other formats, too, such as CHM. Very versatile package, though, being a UNIX-centric package, I don't know how the Windows support is. 

Sampling a random CD-ROM drive in the old junk box shows that it's rated at 29W. Even if it ran all the time, it wouldn't amount to much. I'm guessing that the power draw while idle would be negligible. EDIT: Worst case scenario example for my $0.08/kWh, my CD drive running at its fully-rated power draw of 29W would amount to $20.32/year or about $1.69/month. 

Examining DNS records for all related site hostnames will give you a hint a the topology of the site. You may see multiple IP addresses (which don't necessarily mean multiple physical machine, but often will) and the same or different network addresses, which may hint at how they distribute the load for redundancy or speed reasons. Examining the HTTP headers of a site's various services will give you a possible idea of the front-end. Are they using a reverse proxy, such as nginx or Varnish, or are you hitting the web servers directly? Are requests for PHP pages coming from a different server (apache) than those for static HTML and image files (nginx,lighttpd , etc.)? Examining SMTP headers from mails sent from a site will give you more hints. Traceroutes and pings will yield a little more info. Of course, much info gathered will be speculation and guessing on your part, because a well configured site will not give out too much info about its internal architecture. What you'd be doing is, in essence, much what a penetration tester would do for certain info. Just make sure to not cross the line and disrupt the site. 

This may sound like sort of a bad hack, but on several past occasions when copying huge directory structures, I used WinRAR to archive (with compression) to a file and then extract it to the destination drive. Not the most graceful solution, I know, but it worked in a pinch. These days, I'd probably try to use rsync or tar from the cygwin utilities. 

Sympa authentication is configured by the file. This can contain one or more stanzas defining alternative authentication methods, such as the internal database, LDAP, cas or generic_sso. Sysmpa identifies users by their email address. The first two (internal and LDAP) take the user email address and password, and authenticate directly. CAS authentication uses a CAS service. Generic_sso authentication uses the Web server's own authentication to return a userID, and then obtains the user email address either from metadata or via an LDAP lookup. One example would be using Shibboleth (via mod_shib) and pulling the email address from the Shibboleth metadata. However, any web server authentication may be used, so you can easily use mod_mysql or similar to authenticate against an external user database. In order to get the email address, you can either use an assosciated LDAP lookup, have your web server authentication module return metadata (as an HTTP header), or ensure that the authenticated userID is the same as the email address. In short; use generic_sso, and then configure the necessary authentication in your web server, making sure to return the email address in the metadata if you cannot map user to email via an LDAP lookup. The (admittedly poor) documentation on this is here : Sympa authentication Example: This stanza uses to authnticate via Shibboleth; if the metadata is returned then it will be used, otherwise an LDAP lookup will be performed to obtain the email address. In order for the authentication to work, the location is configured in the web server to be protected by Shibboleth using . 

Running NTP in a virtualised environment, you'll be luck to achieve 20ms accuracy (that's what we've done using VMware). The virtualised clock skew is bad, particularly in a virtualised environment with resource contention. It depends on how accurate you need to be. If you only care about to the second (EG for web servers) you'll likely be fine, as long as you dont have resource contention. If you want millisecond accuracy (such as a busy database, log server, research project) then forget virtualised time servers. NTP servers should always be on physical hosts. You should have at least 3 of them peering in a pool (so that one rogue server gets voted down by the pool); and if possible, get their time from GPS or other local tier-0 source rather than over the Internet. 

Set up a basic wiki and pgp/gpg-encrypt the document/documents with such info. Set the client(s) up with the FireGPG firefox plugin. You can even mix inline encrypted sections into a plain-text wiki page and it'll take care of decrypting it for you. Just make sure you encrypt documents to yourself (in case the client loses their key), in addition to whoever needs access. 

I think most any option will be "manual stuff". Under UNIX, it's pretty standard to unmount/remount a device before each benchmark run, often with a "newfs" thrown in for good measure. I don't know if you can use command-line tools under Windows to unmount/mount devices, but if automation is your goal, then it would be worth looking for such utilities. 

I'd brute force this one: run tripwire on the entire device for a baseline, then run a check some time later and the offending directory will stick out like a sore thumb. 

Sounds like billable busywork to me. Aside from the fact that many consumer appliances use the 192.168.x.x address space (which can be exploited, like anything else), I don't feel that really changes the security landscape of a corporate network. Things inside are locked down, or they aren't. Keep your machines/devices on current software/firmware, follow best practices for network security, and you'll be in good shape. 

I don't believe that there is an official bacula mechanism for doing what you want. This is what I did this for a customer with a small office (5 PCs, a server, and 2 servers on the internet). First, I ran the nightly backup jobs, which backed up to the server's disk. Next, I ran a script that would restore to another location on disk (we have plenty of space) and then tar and feather to tape. The reason I did this, as opposed to sweimann's good suggestion of simply backing up the backup files to tape, was that I wanted maximum portability of the final tapes. You can pop a tar'ed tape into any machine (even windows, w/ the right software) and restore the files. The online backups are the true primary, and they go back for six months. The tapes, which were always limited to the most recent full restore, are mainly for CYA disaster recovery in case the office burned down (they were meant to be taken off site). No, it doesn't scale terribly well, and it takes much longer than a direct backup, but it works for some scenarios.