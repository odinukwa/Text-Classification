This is clearly a bug in AMD's driver, though it's a very understandable one. What is probably happening is that the driver does not correctly detect that there is a dependency between the framebuffer operation and the use of the texture. It's easy for an API to tell that you've bound a texture that was the subject of an outstanding render target command, since the API sees exactly what texture you're going to use. It can simply keep track of the texture objects recently used in an FBO, and if it sees you render with one of them bound, it issues whatever dependencies are needed to ensure that the operation is complete before the rendering operation. It's much harder when you're not binding the texture; you're just passing an arbitrary number to the shader. The API has no idea if that number represents a render target. To verify that this is the problem, insert a between the clear command and the draw command that uses the texture. Odds are good that it will go away (or at least be partially mitigated). A call might also fix it. If you're getting the texture handle after attaching the texture to the FBO, you might also want to get the handle first. Just in case that shakes something loose. Note that you shouldn't have to do any of these; the specification requires that things work exactly as you expect. What AMDs driver needs to do is to realize that you're using bindless texturing and emit the appropriate dependency operations when you change framebuffer render targets. That is, it has to assume that any or any calls that modify the FBO that's currently bound will result in any subsequent rendering command reading from previously written textures. Therefore, it must issue a synchronization at the first rendering command that involves a bindless texture operation (if the texture used by the FBO is made resident). Either that, or it somehow has to figure out which arbitrary integers represent which textures. This is very difficult, since you can pass those handles in a variety of ways. UBOs, for example; you don't want implementations combing through images. 

No, they're not, and you do the article a great disservice by suggesting that it says such a thing. Yes, Half-floats are 16-bit floats. But they are floating point numbers, as defined in IEEE-754. Which means that they can express a relatively large range of values. Normalized integers can only directly express values in the range [0, 1] (for unsigned) and [-1, 1] (for signed). Floating-point numbers can extend beyond this range. 16-bit floats can express numbers approximately on the range [-6.55*104, 6.55*104], and it can handle decimal values in-between them. 

This sets the vertex shader input to come from attribute location 2. Before 3.30 (or without ARB_explicit_attrib_location), you would have to either set this up explicitly with before linking or query the program for the attribute index with . If you don't explicitly provide an attribute location, GLSL will assign a location arbitrarily (ie: in an implementation-defined manner). Setting it in the shader is almost always the better option. In any case, fragment shader outputs work almost exactly the same way. Fragment shaders can write to multiple output colors, which themselves get mapped to multiple buffers in the framebuffer. Therefore, you need to indicate which output goes to which fragment output color. This process begins with the fragment output location value. It's set very similarly to vertex shader input locations: 

No, it doesn't. A divisor of 0 means that the texture coordinate work exactly as if you hadn't called for that attribute at all. 0 is not a special case; it's the default. That means that the texture coordinate will be updated at the same rate as the position. So it will get the same value for each instance, exactly like it does for the positions. There is no way to use instancing to get an attribute to always increase. It either is a per-instance attribute (changing more-or-less based on ) or it is a per-vertex attribute (changing based on ). To get the per-instance behavior, you use a divisor 1 or greater. To get the per-vertex behavior, you use a divisor of 0. 

This is not an integer image format. It's an unsigned, normalized, fixed-point image format. For those, you use floating-point image/sampler variables. So for a 3D texture, you would use , not . If you want a true signed integer format, you would do this: 

You fell victim to one of the classic blunders. Never use a in a UBO or SSBO. While s take up 12 bytes, they always have the alignment requirements of a . Yes, even under layout, they always have alignment. So if you have an array of s in a U/SSBO, what you really have is an array of s where one of the elements doesn't get used. 

Notice something missing from that list? It mentions "shader buffer variables", and "shader image variables". But atomic counter variables are not mentioned. And atomic counter variables are a different kind of thing from either of those. Therefore, that entire section (which is what explains the behavior of ) does not apply to atomic counters. Well, except where it specifically says otherwise, but that's only where it defines that helper FS invocations don't have side-effects. Note that even this explicitly calls out atomic counters as being something different from regular buffer or image operations: "stores, atomics, and atomic counter updates". To lend greater credence to this view, the ARB_atomic_counter_buffer_object extension does not mention barriers at all. It doesn't require ARB_shader_image_load_store as a companion extension either, which is what defines . Indeed, it has no interactions whatsoever with image load/store, while image load/store does have specified interactions with atomic counters. Given the weight of the evidence in the specifications, I would have to say that the Wiki is right. In the interest of full disclosure, I did write that Wiki article. Though I also did all this research before I wrote it. Basically, what the API seems to be saying is that buffer updates from atomic counters are treated like buffer updates from feedback operations. The implementation is required to track when you attempt to use that buffer for a read operation, then issue any synchronization needed to make sure that you can read the value. Or if you write to the value in the buffer (like clearing it to a value), then the implementation must automatically synchronize this. The likely reason the specification permits this for atomic counters but not for image load/store is that the range of data for atomic counters is fixed. SSBOs and image load/store through buffer textures can write to arbitrary parts of the bound range of buffer data. By contrast, atomic counters write to 4 bytes times the number of atomic counter variables used by the shader. Note however that atomic counters are affected by the in-shader function. That is, atomic counters are subject to the requirements of, as the Wiki calls it, "Internal Visibility". Granted, that's not very useful most of the time, as the primary use of atomic counters is for when multiple invocations are all updating the same one. 

The second parameter to is not the number of bytes in the data. OpenGL can figure that out from the part of the command (a 4x4 matrix of s). The second parameter is the number of matrices you're sending to that uniform, in case it is an array of s. So that value ought to be 1. You almost certainly got a error from calling it with . 

The stride of zero will be a real stride of zero. This also has the benefit of looking a lot more familiar to D3D users than the API. 

Usually, #2 happens when the GPU is on-die with the CPU. Which would also mean that this would be device-local memory. Since you've said that it isn't device-local, odds are good that it is simply not cached. It may use write-combining (which improves the efficiency of sequential writes), but CPU writes otherwise are delivered as they are. Given that the memory does not seem to include "cached" in its description, I'd say #1 is the most likely. Note that just because you don't need to explicitly flush memory does not change the fact that you still need a dependency between the CPU operation writing that data and the GPU command(s) that read it. This would generally be some from of event with a memory dependency. 

You can program your own resolution by rendering with a multisample texture to a non-multisampled texture. In the FS, you read each fragment's worth of pixels, doing whatever operation you would like to combine them. But that's probably not going to be as fast as letting the hardware do the multisample resolve itself. 

You render to your own depth buffer entirely. Just copy/render the image data out to the default framebuffer when its time to display it. 

There isn't a DSA way to bind buffers to the feedback buffer bindings. But that's because transform feedbacks are objects, and there is a DSA way to attach buffers to transform feedback objects: 

Now granted, image load/store doesn't exactly make read/modify/write passes easy, due to incoherent memory access rules. Of course, image load/store cannot handle (easily): 

Given such information, you can devise an allocation strategy for data uploading. Open-world games use streaming of fixed-sized blocks of data. Each sector of the game contains resources that take up X number of bytes, and therefore, artists have X number of bytes of storage to work with. It's their job to fit all of the stuff needed for that region of the game into that much space. There will also be stream blocks for larger collections of regions, or other data that gets shared between all game regions. For other games, some of them load everything at level-load time. Some do hard-loads in the middle of "levels" (see many Source-engine games). Others stream things more dynamically, but even there, artists are restricted to using resources that total up to X number of bytes. On consoles, such sizes can be known a priori, but with Vulkan/Metal/D3D12, these have to be queried for a particular piece of hardware. So in the latter case, you have to be more reactive to what's available, but all of the information is available to the user. For other APIs/platforms (OpenGL/D3D pre-12), things are rather more complicated. Consoles and the lower-level APIs give you direct access to memory allocations, which gives you more flexibility. You can use a piece of memory for vertex data at one point, then put an image into it later on. Memory is divorced from its current use, so it's much easier for an application to choose how to recycle memory. For GL/D3D-pre-12, things are difficult. A buffer object is a buffer object, and the memory currently associated with it cannot later be used as a texture. Well, it can, but you cannot explicitly ask that to be done. The most you can do is deallocate the buffer, allocate the image, and hope that the buffer's memory was recycled. For cases where you're doing dynamic loading in fixed-sized blocks, you basically have to give your artists more limitations. Instead of limiting the artists by memory, you have to spell out the limitations by object So you'd say that a stream block can have "X numbers of textures of Y formats and Z size, and W bytes of vertex data". Artists can't sacrifice 4 textures to allow one texture to be 4 times the size, or sacrifice a texture to get more vertex data, or sacrifice some vertex data to get more texture space. That is what you have to do if you don't trust the implementation's memory allocator to effectively recycle resources. For hard-load cases, it's generally OK to just delete all of the objects and expect the implementation to recycle effectively. But even here, it's a good idea to make sure that you don't try to allocate all of the available storage; leave the implementation a healthy percentage for fragmentation and so forth. Of course, these kinds of things are a big part of the reason why IHVs have application-specific optimizations: they know up-front which resources were allocated for which purpose, and the driver makers can design allocation schemes that work with these assumptions. Which is why Vulkan et. al exist: so that application developers don't have to rely on IHVs to work around problems in the API. 

While Debug Output is good, and manual usage is adequate, it's often better to employ a more dedicated tool for finding exactly where OpenGL errors came from. RenderDoc is probably the most up-to-date tool for this process, but there are quite a few in various states of functionality. These tools can also give you a detailed log of every OpenGL call you've made. 

First, you "attach" images to framebuffers. You "bind" objects to the context; you "attach" objects to other objects. Second, no, you cannot attach images of the default framebuffer to anything.