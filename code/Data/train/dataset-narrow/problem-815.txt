A quoted field with a line-break at the end, should extend to the next quote on the following lines. The quoted line-breaks should be treated literally. 

The UNION condition is also very buried in there, and it has too much indentation ;-) Finally, white-space is cheap in SQL, I really don't like the space-less expressions, spacing them is easy . Right, that's the nit-picky stuff done. Talking about the UNION, the statement could be rewritten as: 

I am particularly interested to know whether the rolling average can be computed in a more SQL Server friendly way. There does not appear to be a performance problem, but the CTE and self join seem.... ugly. Any other observations? 

At this point, you have an array of counts... where the index is the sum, and the value in the array is the number of times that sum was encountered. For example, for 2 balls of maximum 6, the counts are: 

If you look, you'll see the regex version is Slower than the exception version... it runs in 1.95ms but the exception version runs in 1.67ms Exceptions But, there's a catch. In these tests, the stack trace for the exceptions is really small... and the "cost" of an exception depends on the depth of the trace, so let's increase the stack depths for the regex and exception code. Well add a recursive function to simulate a deeper stack: 

Algorithm The algorithm works well enough. You have captured the essence of the problem in your code, and reading your code it is apparent what it does, and why. This is a good thing. It is not very often that the intent of the code is so easy to discern. The draw-back of your algorithm is that it is not going to be able to support the more esoteric operations like parenthesis e.g. .... That sort of support will require a preprocessing step. Keeping the equation as a string is convenient for some things, but i would have preferred to see the process broken down in to components, for example, a parse step could break the input down to . at the same time, it could be trimming the white-space. Then, you just need to scan the and find 3 items separated by the highest precedence operator, and swap all three with the resulting value. Code Style You should read through the Code Style Guidelines for Java. This code, for example, has a number of faults: 

Then, about your code, it is really confusing why you split the data in to chunks based on the number of cores you have (actually, twice as many), and then you loop through the chunks, and for each loop, bizarrely, you split each chunk in half again, and process each half in a separate routine. If you have 4 real cores, you split the data 8 ways, and each of those 8 ways spins off 2 routines - making the whole thing a 16-way split on a 4-core machine. That's not technically, horrible but if that's what you want, just do that with instead of .... It's all just.... odd. Now, you have a that's created outside the function (I assume). This is a problem because if your code is called multiple times concurrently, then you will have two different result slices being locked by one lock. The amount of locking you do is really concerning too.... why lock for every record? I would accumulate each goroutine's results in a local slice, then, when the routine is complete, I would add them all to the (locked) result slice. All told, I would probably make life simple with: 

You declare convenience variables which really are unnecessary.... You are working with the data 'backwards'.... you build up the byte, then set the value in to the output, instead of just shifting the bits in to the output. 

THe most striking issue I can see in your code is that you're severely limited in the size of files you can compute the hash on. This line of code is particularly jarring: 

By having a permanent head and tail, you can avoid almost all the conditional exection in your add/remove methods. 

Right, that makes it generic, there are still a bunch of problems... but, it can be easy to use with: 

Executors have their own Runnable that they use to create the thread. When you have your runnable, you pass it to the Executor, and the Executor runs it on one of the Executor's threads. So, you can create an ExecutorService with something like (using Java 8): 

I like the concept, and the code. Given that your primary concern is about performance, I recommend three things: 

I am not particularly fussed with the name . This is a generic class ... if it was directly linked to Partners, then it would be different. You could probably do something to indicate the underlying data infrastructure, like which allows you to understand that the tool manipulates Lists to show different views of the data, and, in this case, two tools are available for modifying the View, sorting, and filtering. Other things I think you should consider changing though, are: 

Your query is well structured, and consistent, but there is one issue you have failed to incorporate.... not all bad questions have a user. When questions are migrated, or there's a user deleted, the question may have no link back to the Users table. This requires an Outer Join to Users. Additionally, I have found that CTE expressions are fantastic for aliasing column names. For example, both Posts and Users tables have an ID column, and you need to have and in different places. Both also have a . When these aliases become long, and complicated, it bogs down the readability of the query. Oh, about the readability, the case statement is dead code. Don't use code that serves no purpose, or, alternatively, "debug code" should be removed before deployment. If you cannot trust yourself to have the right conditions on the query, then you cannot trust yourself to have the right debug code either. So, if you alias these values in a CTE, and use the CTE to make the Outer join neat, you can reduce your query to: 

Note the use of the . The trick here is to do the boundary-condition logic right, requiring the size-1 and the guard-conditions to check for a size-1 list... 

Concurrency Strategy In general, using is complicated, partially because the meaning of changed in Java 1.4, and also because it is hard to spot. I recommend against using it at all. Instead, you should use a more visible concurrency mechanism like synchronization, classes, , or So, while I recommend against , what's a real problem is using multiple different locking schemes in the same code, and you use both and . The use of synchronization as your concurrency strategy should be fine all on its own in this case. General 

I'm not convinced that the mechanism of using a stack is a good one. Why did you choose that data structure? In Go, a slice would be a much more appropriate structure, and also probably a lot more memory efficient. What you are looking for is a 2-element queue, not a 2-member stack. The need for you to and then push it back again, is an indication that the stack is the wrong structure. Consider a much simpler method, that does essentially the same logic as your method, but .... simpler. 

This code is begging for an EnumMap as the data store, rather than the individual variables. Creating an Enum with values like: 

That whole thing begs the question as to why you need it at all..... Instead of running, for example: 

In essence, you now have a Texture pool, with a clear, and always-safe open/close pairing, and reference counting. Hope that helps. 

You should have better handling for what you do with the buttons as well, but these three changes will go a long way to making your code more readable. 

That would be a method that prompts the user for a car type, and returns the user's selection. The code inside that is what you have written above. There are a few things you should consider though: 

For consistency, the Collate class should be hard-coded to work with Journal, or your actual OutReview and other classes should be generic too. Naming 

This way, at the end of the loop, you have the index to the first pair of indexes for values with the largest diff. You can then loop through that list using and pull the actual values back from the array. This is not actually very different fdrom the way you have done it, but it is a bit neater. Nice job. 

Next up, is that computers have different precision in their nano clocks. A nano-second is far faster than the actual computer's time interrupt, so, it is common for computers to "step" significantly as it updates. The steps can be as much as 190 nanoseconds is where i have seen it.... ... what that means, is that the results for all your seed values here may be the same: 

then the numbers will wrap neatly. Now, if you take that example, and extend it to start at a certain day of the week, you can do: 

For a beginner I would consider this code to be pretty good. The code structure is clean, and has nothing "superfluous" - all the code is necessary, and the use of standard library functions to do the work is great. The only real criticism I have is in the variable names. , , and are crummy names. With a simple variable rename, your code is significantly better: 

Now there is no need to ever compile that again. The second time the pattern is used, it is used to replace the parameter named placeholders with the placeholder. How can this be improved? By doing the replacements during the initial matcher loop. Consider the following code: 

A class, that takes a function, and an initial state. The function is used to convert the current state, and a new value, to a new state. Consider an initial seed state for the partial solutions... this would be an empty solution: 

You have immediately 'un-nested' a large portion of your code, which makes readability better. When you write code to match a specification, like you have here, it is valuable to include the specification you used inside your code, which, you have also done... but, do that before the code, not after it. If you have an if-else block, and the if-side (the true side) always does a , then there is no need for the . This can simplify the code further.... Performing these changes, we get to: 

This is a generic class that has this neat ability to collate generically typed data based on matching Rank and Predicate instances.... On it's own, it is nice, clean, and general purpose. But, you put them together, and you have a bit of a mess: 

Your script is doing a number of unnecessary file copies and scans. I tried to streamline the process a chunk, and came up with the following to replace the line-stripping. Your code does: 

Your question comes in multiple parts: General code review, adding algorithms, and then performance relative to standard General Review Let's focus on this method, it shows essentially all the general issues I see: 

Sometimes this is not avoidable... but, in this case, it is unnecessary. By splitting the logic in to two methods, it becomes harder to 'grok' the recursion. Additionally, your previous questions where you have had similar sorts of work, you have always 'defaulted' to using collections instead of using simpler mechanisms like arrays of primitives. Arrays are much faster, and smaller, and tend to lead to better structured code. This is because the array is essentially a stack, and popping the stack is as easy as changing it's size.... you do not have to do as much manipulation of the stack head. Finally, this problem is one which should have a more general solution. You have hard-coded this solution to only work with 1-step and 2-step 'strides'. The problem is actually simpler if you make the stride options a general thing.... Consider this alternative code... it: 

EDIT You ask why ? It is a 'co-incidence' --- (NOTE: A really, really mysterious one, needs more investigation): 

DISREGARD BELOW HERE Right, you'll have to work with me on this one.... (it takes multiple 'heads' to work around 'threads'....)... First, let me restate the problem I think you are trying to solve: 

Overall I feel the concept is unnecessary in a disciplined environment, but I agree that asserting a different protocol for applications may be useful. Note that there are a couple of drawbacks to your approach: 

The Integer.compare static method will not need to create the Integer objects from the ints. I like the continue and break statements in Java, and use them regularly, but, your use of continue here is not necessary (and you do not use a 'braced' 1-liner here....!!!): 

Note that I use the in there to show off the fact that I know it exists as a language feature (since Java 7 at least). Next, I don't see any Java-8 features in there. For an interview I would expect you to "wow" me... but there's nothing that's exciting technically in your code. For example, an easy-win would be the creation of the input array: 

Note, that you synchronize on two different Objects at different times too... you sync on the as well as an instance of the Client (). Further, your concurrency is broken.... you are doing network activity in the sentMessage() class, and it calls: 

To be consistent with the other selects, as much as these are short statements, you should maintain consistency: 

Having stewed over this code for a few days, I have decided there are three things I did not like that are significant. 

With the above code, there is no need to actually return anything from the method at all, as, other than when the is null to start with, the method will never be called with a null . That means the code can become: 

Foot Notes Note that your code has horrible performance on List instances that are not random-access... Consider someone who feeds your code a ... it will perform badly because each time you get or set with an index, it has to scan the data for the value. is an interface available on collections that support fast and easy index-based access. The Java implementation solves sorting non-RandomAccess Lists by dumping all the data in to a different RandomAccess list, sorting that, and then copying it back again, something like: 

Again, though, note that the total time was 30ms, and the actual transformation time was only 1.5ms. In your situation, you are not reading, or writing the file to disk, but you are extracting and re-applying full arrays of pixel values, and, in my estimates, that is about 75% or more of the time. Or, put another way, and relating it back to your code, here's your method, and I will apply estimates of timing for that: 

Using the private lock prevents other people from hanging your application by using your class as their own monitor. Imagine if your class is called , and someone does: 

The value in will be positive, 0, or negative if the value is larger, the same as, or less than the value. Now, if we have done the same with the and values (stored in ), we can use some math-tricks to help: 

Those poor knights.... what did they do to deserve that? But, there is a mathematical treat that allows this problem to be solved in O(1) time, and space. What does that mean? Well, it means that solving the problem for 1 knight takes just as long (and just as much memory) as solving it for 10, or 1000, or 1000000000000000 knights. How is this possible? There's a pattern that emerges when you study this problem. If there is 1 knight, then knight 1 survives. If there's 2, then knight 1 survives, and, if we map this out for say the first 32 knights, we get: 

That will set a buffer that can read all, or big chunks of the file. If you do it that way, you can also remove the code path for files that are smaller than the buffer, they become irrelevant. Byte[] method: This method is horrible overkill. Since you have to return all the bytes anyway, you may as well just allocate a single large buffer for the file size, populate it, and return it. 

If your data is correctly indexed with the ID column from each table as the primary key, then I don't see this query slowing down much as the data grows. You are worrying prematurely, I think.... but, the purpose of the query is complicated... it is almost as if you are tying to find a random lead to prompt people to look in to 'next'.... where that lead has not been handled in some way yet. I think you may have better performance with a cursor.... (in a stored procedure?). Still, even if you have to do the large checks of all the 'handled' leads, you may find it faster to do just the one 'big' subselect instead of multiple smaller ones. you will need to test this on your system to get an idea of the performance.