The Knot Equivalence Problem. Given two knots drawn in the plane, are they topologically the same? This problem is known to be decidable, and there do not seem to be any computational complexity obstructions to its being in P. The best upper bound currently known on its time complexity seems to be a tower of $2$s of height $c^n$, where $c = 10^{10^{6}}$, and $n$ is the number of crossings in the knot diagrams. This comes from a bound by Coward and Lackenby on the number of Reidemeister moves needed to take one knot to an equivalent one. See Lackenby's more recent paper for some more recent related results and for the explicit form of the bound I give above (page 16). 

Positions where $t \leq s-2$ containing an odd number of stones. Positions where $t \geq s$ containing an even number of stones. 

If it's NP-complete, then wouldn't you have proved that no set of (uniformly) polynomial-time computable invariants of 3-manifolds distinguishes 3-spheres from other 3-manifolds. I would be very surprised if this is known. 

First, you should write up your result so it's comprehensible as you can make it, and send it to a journal. If your write-up looks like it is a real research paper, your paper should be sent out for review and possible publication. How to choose a journal? Look for other papers which are on roughly the same subject matter and around the same quality as yours, and see where they got published. Don't publish in a fourth-tier journal; if you do, it will never be read; these seem to exist only for the purpose of increasing the number of researcher's publications, and are not subscribed to by many academic libraries. To make sure you're not choosing one of these, check the online databases of a few good academic libraries, and make sure most of them subscribe to the journal you have picked. 

It has been a while since I read von Neumann's paper, but I don't recall having the same impression that Scott Aaronson had. It seemed to me that von Neumann was addressing an argument that human brains must be qualitatively different from computers, because while human brains' individual components, neurons, are unreliable, human brains on a whole are much more reliable than electronic computers are. While Claude Shannon does not actually make this argument explicitly in his 1953 paper "Computers and Automata" (it's quite possible other people did), he certainly brings up the question of reliability and brains. I have extracted a few sentences below: 

Suppose you have two arbitrarily powerful participants who don't trust each other. They have access to bit commitment (e.g., sealed envelopes containing data that one player can hand to the other but that can't be opened until the first player gives the second a key). Can you use this to build an oblivious transfer protocol. Is this true even if the players agree to open all the envelopes at the end to detect cheating (e.g., after the poker hand is played, everybody agrees to reveal their cards)? I assume that you can't get oblivious transfer out of bit commitment, because oblivious transfer is cryptographically universal, and I can't find any references that say bit commitment is, but is there a proof somewhere that you can't do it? Finally, has anybody looked at the problem if the players are quantum? 

bin $1$ can be capped only before you've capped bin $2$, bin $1$ can be capped only after you've capped bin $2$, bin $1$ can be capped any time, bin $1$ can never be capped. 

There are a number of game-theoretic characterizations of complexity classes. The most famous may be 

The amplitude $\alpha(n) = \Theta(n)$ for expander graphs. A random 3-regular graph is asymptotically almost surely an expander graph (see Wikipedia), so the expectation of the amplitude will be $\Theta(n)$, since the probability that it's not an expander graph goes to $0$ as $n$ goes to $\infty$. For an expander graph with parameter $\beta$, for any set of $s$ vertices with $s \leq n/2$, there are $\beta s$ neighbors of the set. Now, let the number of vertices on level $j$ be $\ell_j$, with $\ell_0=1$. We then have from the expansion property that as long as $j$ is not too large (i.e., we haven't included half the vertices yet) $$ \ell_j \geq \beta\, \sum_{i=0}^{j-1} \ell_{i}  $$ Now, look for the level $\ell_j$ which contains vertex $\frac{n}{3}$. That is, so $\sum_{i=0}^{j-1} \ell_i < n/3$ and $\sum_{i=0}^{j} \ell_i \geq n/3$. If this level is large, i.e., $\ell_j \geq n/6$, we are done. Otherwise, the next level has size $$ \ell_{j+1} \geq \beta \, \sum_{i=0}^{j} \ell_{i} \geq \beta \frac{n}{3}, $$ and we are done. While this proof looks at the number of vertices in a level rather than the number of edges (which the OP asked about), there are always at least as many edges added in step $i$ as vertices in level $i$, since each vertex must be reached by some edge. 

As far as I can tell, this question really comprises two distinct questions, the first of which appears in the title and the second of which is given after the edit. (1) Do many-one reductions and Turing reductions define the same set of NP-complete problems (i.e. problems that are both in NP and which SAT can be reduced to)? Whether NPC under Turing reductions is the same as NPC under many-one reductions was still an open problem seven years ago, and I don't believe it has been closed since. See this survey from the June 2003 ACM SIGACT News for details. (2) What is the class of problems which SAT has a Turing reduction to, and vice versa? This is the class of NP-hard problems (under Turing reductions) which are in PNP. For more information on this, see Noam's answer. 

The lower bound is $\Omega(\log n)$ paths with $O(\log n)$ branching nodes, if you have at least $\Omega(\log n)$ branching nodes in the tree. This can be achieved: use a tree which has one long path (length $n$) all of whose nodes are branching nodes, with no other branching nodes in the tree. Here is a sketch of the lower bound. First, compactify the tree by contracting any interior node that isn't a branching node. If the original size of the tree was $< n^c$, the new tree must still be $< n^c$, since you've only reduced the number of nodes. Now, the depth of a leaf is the number of branching nodes on the original path to that leaf, and we have a complete binary tree (every node has either degree 2 or 0). If there are no leaves of depth $\Omega(\log n)$, then the number of paths is one more than the number of branching nodes, which is $\Omega(\log n)$, so we can assume that at least one leaf has depth $\Omega(\log n)$. Next, recall Kraft's inequality. If the depth of a leaf in a complete binary tree is $d(v)$, then $\Sigma_{v \mathrm{\ leaf}} 2^{-d(v)} = 1$. Now, we have fewer than $n^c$ leaves. We want to show that we have a lot of them at depth $O(\log n)$. Suppose we eliminate from consideration the ones that are depth at least $\log_2(n^{c+1}) = (c+1) \log_2 n$. This removes at most weight $1/n$ from the sum in Kraft's inequality, so for those leaves $v$ at depth at most $d(v)\leq (c+1) \log_2 n$, we have $\sum_{v\mathrm{\ low \ depth \ leaf}} 2^{-d(v)} > 1-\frac{1}{n}$. We also have $\sum_{v\mathrm{\ low\ depth\ leaf}} 2^{-d(v)} < 1$ (since at least one leaf has depth too large to be included in this sum). It's fairly easy to show that to get a sum of numbers $2^{-k}$ strictly between $1$ and $1-\frac{1}{n}$, we need at least $\log_2 n$ of them. This shows that there are $\Omega(\log n)$ paths with $O(\log n)$ branching nodes. 

Most NP-complete problems are NP-complete under LOGSPACE reductions; that is, you can take an arbitrary problem in NP, and using a LOGSPACE algorithm, reduce it to your problem. Any NP-complete problem under LOGSPACE reductions will also be P-complete under LOGSPACE reduction, even if P=NP, as you can use the same procedure to reduce a P-complete problem to your problem with a LOGSPACE reduction (since P $\in$ NP). Some problems are only NP-complete under P reductions. In this case, it is not clear whether or not they are P-complete. I don't know of any NP-complete problems which are not also known to be P-complete, but I haven't tried searching to see whether anybody has investigated this. 

There are pretty much no complexity-theoretic consequences of Factoring being in P. This means that there are no good justifications for factoring being hard, other than that nobody has been able to crack it so far. Polynomial-time factoring would make it possible to take square roots over $Z_n$ (and also over a much more general class of rings as well), and give polynomial-time algorithms for a number of other number-theoretic problems for which the bottleneck in the algorithm is currently factoring. As for practical consequences, banking transactions are probably not that much of a problem -- as soon as it was known that factoring was in P, the banks would switch to some other system, probably causing only a brief period of delays while this was being implemented. Decoding past banking transactions would probably not cause serious problems for the banks. A much more serious problem is that all the communication which was previously protected by RSA would now be in danger of being read. 

We will never be able to prove this statement, because we can never be able to know for sure whether we have the exact laws of physics, or just a very good approximation to them. Even if we had a satisfactory theory of everything which we could use to make good predictions about every experimentally measurable physical system, there would be no way to tell whether it was correct or a very good approximation. Having said that, we are still quite far from even coming close to a proof of this statement. For example, we can't even prove that the Standard Model is simulable by a quantum computer. Jordan, Lee, and Preskill have two papers showing how to use a quantum computer to simulate a quantum field theory which is much simpler than the Standard Model. This turns out to be harder than it might at first appear. 

so maybe I would label it as a "working hypothesis" rather than a "physical law". Let me finally note that mathematicians also use such working hypotheses. There are a large number of mathematics papers proving theorems whose statements run "Assuming the Riemann hypothesis is true, then ...". 

The problem is definitely in random double exponential time, and likely in exponential space. The first result was in my original post below, and the second in my update. ORIGINAL POST: Can't you get a good approximation by simulation, if you're willing to spend time exponential in $k$ and $d$? The input length is logarithmic in $k$ and $d$. So clearly the problem is in random double exponential time. Since nobody knows how to compute these values efficiently in practice, it seems clear this is not known to be in random exponential time. I would be very surprised if any other complexity results were known about this problem. ADDED UPDATE: Actually, I think the problem is very likely in EXPSPACE. Let's fix the dimension (to make things easier, and because I don't understand the subtleties of percolation in varying dimension well at all) so the input is just $k$. Also, let's say $k$ is given in unary so that I can drop the exponentials and talk about PSPACE. I propose the following algorithm. First, we must make the assumption that there is a class of pseudorandom functions $F_\alpha({\mathbf{x}})$ which tell you whether the bond at coordinates $\mathbf{x}$ is present, where $\alpha$ is the seed for the pseudorandom function, and for which the bonds given by $F$ behave like random bonds with respect to percolation. Now, suppose that we have a fixed value of the pseudorandom function seed $\alpha$. Consider the following two-player game, which two players A and B play, after being given a bond probability $p$ and a seed $\alpha$ for $F$. Player 1 gives two sites ${\bf a}$ and ${\bf b}$ within distance $2^{k\nu}$ of the origin, but which are still $\theta(2^{k \nu})$ distance apart, where $\nu$ is chosen so that if the percolation probability $p$ is within $2^{-k}$ of the critical percolation probability $p_c$, then with high probability there will be a cluster of diameter $2^{k\nu}$ near the origin ($\nu$ is called a critical exponent, and I believe its value is known with mathematical proof). Player 1 claims that there is a path of length $d$ connecting these sites with bonds in $F_\alpha$, and also gives the site that is the midpoint of this path of length $d$. Player 2 then claims that either the first part or the second part of this path is not connected. Player 1 responds by giving the point he claims is the midpoint of this allegedly disconnected section of the path. The two players continue in this way for $\log d$ steps, until they reach a segment of path consisting of a single bond, whose presence or absence is easily verified. This is a two-player game whose result tells whether the critical percolation probability is within $2^{-k}$ of $p_c$, and by the result that alternating polynomial time is in PSPACE, the outcome of this game can be computed in PSPACE. A PSPACE machine could then compute this outcome for all seeds $\alpha$ to find which player wins with high probability: this will tell him whether $p$ is larger or smaller or approximately equal to $p_c-2^{-k} $. It can then do a binary search on $p$ to find $p_c$. CHALLENGE: Find a PSPACE (or EXPSPACE if $k$ is not given in unary) algorithm without using the assumption that there are good pseudorandom functions for percolation. 

Suppose to the contrary that $$ P(x^2 \geq n_i) \geq C/n_i^2 $$ for some infinite sequence $n_1$, $n_2$, $\ldots$, $n_i$, $\ldots$, with $P(x^2 > n_i) > 2P(x^2 > n_{i+1})$. Then we have $$ Var(x) = \int_{x=0}^\infty x^2 d \mu.$$ But now, let's set $n_0 = 0$ and break this integral up into $$ Var(x) = \sum_{j=0}^\infty \int_{n_j}^{n_{j+1}}x^2 d \mu.$$ We have$$ \int_{n_j}^{n_{j+1}} x^2 d\mu \geq n_j^2 \int_{n_j}^{n_{j+1}} d\mu = n_j^2(P(x\geq n_j)-P(x \geq n_{j+1})) \geq C/2.$$ This shows that the integral diverges, contradicting the variance being 1. 

Do there exist $k$ rows of length $n(n-1)$ so that no number appears twice in any column, and for each pair of rows all ordered pairs given by the columns are distinct? Do there exist $k$ rows of length $n^2$ so that for each pair of rows, all ordered pairs given by the columns are distinct? 

I think you should put this on the ArXiv. I would really recommend against putting papers which are less than a minimum publishable unit on the ArXiv if you are planning to do more work on them and incorporating them into a more complete conference or journal paper later (since I think your ArXiv papers should more or less mirror your conference and journal publications -- otherwise things get very confusing), but if you're not planning on doing anything else along these lines, I would recommend going ahead and posting it. I don't really see a downside. 

Here is an upper bound for question 2. Theorem: After $TM$ steps, you can have at most $T \log M$ positive counters. Notation: we assign each step a number, ending with the $0$th step, and starting with the $(-TM+1)$th step. Observation: in the optimal strategy, if you increment a counter, you do not let it return to $0$. Proof: Let us assign a value of $1$ to each time you increment a counter in the last $T$ steps; $\frac{1}{2}$ to each time you increment a counter in the previous $T$ steps; $\frac{1}{3}$ to each time you increment a counter in the $T$ steps before that, etc. That is, if you increment a counter between step $-kT+1$ and $-(k-1)T$, it gets a value of $\frac{1}{k}$. We need a lemma that we will prove later. Lemma: for a counter to be positive, it must have had a total value of at least $1$ assigned to it. With the lemma, the proof is easy: the total value we have to assign is $T \sum_{i=1}^M \frac{1}{i} \approx T \log M$, and each positive counter requires us to have assigned total value of at least $1$ to it. Proof of Lemma: Look at the first time you made that counter positive. If that was between $-kT$ and $-(k-1)T$, then it has been decremented at least $k-1$ times, and so you need to have incremented it $k$ times to have it end up positive. But each of these increments assigns it a value of at least $\frac{1}{k}$. And here is an upper bound for question 1. Let's use the same overall proof strategy as before. Again, we assign a value of $\frac{1}{k}$ when we increment a counter on a step between $-kT+1$ and $-(k-1)T$. We also assign a value of $\frac{1}{M}$ when we increment a counter on a step between $-2MT$ and $-MT$, and a value of $0$ if we increment a counter before that. By the same reasoning as before, any counter that was $0$ at step $-2MT$ must be assigned a value of at least $1$. But now, if a counter was positive at step $-2MT$, it still must have been assigned a total value of at least $1$, since it must have been incremented $M$ times to end up positive. Now, the total value we have to assign is $T\log M + T$, so we can have at most $T (\log M + 1)$ positive counters.