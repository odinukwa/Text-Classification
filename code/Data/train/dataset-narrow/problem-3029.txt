You want to see which variables best describe your output $Y$. First step plot your features against your output to see how they are distributed. 

In your problem description you want to extract all the rows after the truck starts moving. You state that you want all the rows after 06:51:04 where the speed has reached 6. However, the speed reached 6, twice before that. Without stating a more robust decision boundary this would not be an easy to solve problem. However, if I assume you want to detect the first presence of a speed greater than or equal to 5 then it is simple. You can play with this threshold speed value. I would first find the DateTimeStamp for the first non-zero element in the recordlist and then get all rows that are greater than that. 

I will add my 2 cents at the end of this answer. However, this is how it can be done using a neural network. Firstly, yes, you should expect to need more data to train even a simple neural network because their are more parameters that need tuning. Think of them like little faucets that you need to tune in order to get the right output volume based on an input. If you have millions of these faucets you an imagine that this is an arduous process. You will need some of the following imports 

Then you can split your data based on the output and see the output distributions separability. This will give you information regarding the importance of each feature for building a classifier. 

Now if we get a new data point $x_{new} = [2, 8]$ we will label this as being a woman. So the entire decision is based on the numbers $1, -2, 1$ from our linear equation. We need to tune these values using the training data. We usually call these trainable parameters the weights $w$ associated with the features $x$ and we also add a bias $b$. In general the linear separator in 2D is $0 = w_1x_1 + w_2x_2 + b$. Our predicted label is $\hat{y} = w_1x_1 + w_2x_2 + b$. If $\hat{y} > 0$ then woman, else man. In $n$ dimensions this can be written as a matrix multiplication as $\hat{y} = w^Tx + b$ Obviously, a linear separator is not sufficient for most classification tasks. Things are not always linearly separable. So we use more complex models. Neural networks In neural networks each node is associated with a function much like the linear separator. However, we will use the sigmoid function $\sigma(w^Tx) = \frac{1}{1 + exp^{-(w^Tx + b)}}$. The weights here have the same effect. They will modulate the input values $x$ such that we are able to learn some classification or regression. 

Generating text as an image is extremely difficult and I have never seen a GAN applied in the image space to generate pages of text. The reason this is so hard is because of the way in which text is perceived by humans and the way a GAN works. Humans read arbitrary symbols which are sequenced from left to right along the same line and combined into rows. Moreover, these symbols are combined into groups which represent words. This is extremely complex. The symbols must be intelligible, the words must be real ones as invented by humans. Lastly, the combination of words into sentences need to be logical and follow guidelines of human language. And even FURTHER the sequence of sentences must be coherent to transmit a message. A GAN operating in image space will try to learn the distribution of the training set in a pixel-wise manner as that is your inputs. The distribution of the pixels will not effectively be able to group characters together in a logical manner, and the words will not be real, and the sentences will all be nonsense. You will most likely end up with blurry lines of random looking symbols, kind of like a zebra print. Another problem is the amount of data you have. Even if this problem was possible with a GAN you would need tens of thousands of instances to train a GAN effectively. 

This is not correct. Every connection between neurons has its own weight. In a fully connected network each neuron will be associated with many different weights. If there are inputs (i.e. neurons in the previous layer) to a layer with neurons in a fully connected network, that layer will have weights, not counting any bias term. You should be able to see this clearly in this diagram of a fully connected network from CS231n. Every edge you see represents a different trainable weight: 

It should be possible to install Python without admin privileges. This has been discussed on Stackoverflow here. The easiest method appears to be installing Anaconda. And according to this answer you should also be able to use WSL without admin rights, which will get you bash. For handling relational data you can use sqlite, which is built-in to Python. For example: 

For multiclass classification where you want to assign one class from multiple possibilities you can use : 

Unfortunately none of these three variables can go directly into linear regression. looks like a numerical variable, but it is actually categorical. For example is probably orthogonal to , and should not be interpreted as twice as significant. The correct way to handle this is to create a boolean dummy indicator variable for each possible site code. You can also use this method for some of your other variables which also appear to be categorical. and will not work well in a linear regression because their relationship is highly non-linear. For example two points can have the same latitude/longitude but be very far apart. One typical method is to convert pairs into predefined zones, and treat the zone as a categorical variable. The final type of variable you have is a . You could convert this directly to a categorical variable, but it might be better to take only the month to reduce the number of categories and generalize seasonal effects better. 

You can replace with and use . This will work if you don't already have rows with entries that you want to keep. 

If I understand correctly, we are interested in soft multilabel classification, where a single text can have multiple correct genres. According to your comment, we don't have any training data, just a list of keywords associated with each genre. We can try computing the similarity between each document and each keyword list: 

What definition of BLEU is the Google Brain paper using? I could not find a separate definition in the paper itself. 

Since our cross-validation scheme was resistant to over-fitting, the training and test accuracy are close on the practice set, and our practice training accuracy is close to our test training accuracy. Now we can safely assume that accuracy on the unseen test set for the real data is about worse than training accuracy. However, imagine you a single validation set or no validation at all on the practice data. Now your model is more likely to overfit and you might see results like this: 

These are in comparison to a simpler model like a 1D conv net, for example. The first three items are because LSTMs have more parameters. 

Is there a standard way to do this? I am most interested in neural networks but also open to advice relevant to any other type of model. Note that I also have other categorical and numeric features such as and that also need to be fed in to my model. 

Backpropagation This is a method used to compute the contribution of each parameter on the error term. We then use gradient descent to update these parameters such that the next pass through should result in a lower loss rate. Picking the right loss function is essential for this process. For classification tasks, as is the case with a GAN, we typically choose binary cross entropy as defined by $L = - ylog(\hat{y}) - (1-y)log(1-\hat{y})$ and over $N$ instances as is typically the case for stochastic gradient descent the loss function is $L(w) = - \frac{1}{N} \sum_{n = 1}^N \big[ y_n log(\hat{y}_n) + (1-y_n)log(1-\hat{y}_n) \big] $ where $y$ is the true label and $\hat{y}$ is the predicted label. Backpropagation in deep neural networks Take a look at this answer here which describes the process of using backpropagation and gradient descent to train a single neuron perceptron, and then a multi-layered network. The only difference is we are using the binary entropy loss function here which has a different derivative with respect to $\hat{y}$. This becomes $\frac{\partial L}{\partial \hat{y}} = - \frac{1}{N} \sum_{n = 1}^N \Big[ \frac{y}{\hat{y}} - \frac{1-y}{1-\hat{y}} \Big]$ You will then backpropagate this loss through the network by using the chain rule with the activation functions you selected at each layer. Backpropagation in a CNN Please refer to this answer for extensive details and a derivation of backpropagation for a CNN. The process is very similar to that for a deep neural network. However, the CNN uses the cross-correlation function at each layer, so you need to backpropagate the loss function through the derivative of this function. That question asks about the inner working of a CNN with two outputs very much like our discriminator. Finally, in order to train the generator, just imagine the same process with more convolutional layers ahead of it. When we have the associated gradient $\nabla L(w)$ for each parameters, only apply the gradient descent algorithm to the parameters that are a part of the generator model. I am sure that through these derivations you will see that the number of intermediate nodes between layers does not affect the backpropagation algorithm. It remains the same process. You should also be convinced that for a non-human the intermediate nodes that are the generated image from the GAN, are no different from any intermediate nodes in the model. We simply train these intermediate nodes in such a way that we can perceive some meaning, such as generating instances of the MNIST dataset. 

Then the resulting operation is a element-wise multiplication and addition of the terms as shown below. Very much like the wikipedia page shows, this kernel (orange matrix) $g$ is shifted across the entire function (green matrix) $f$. 

To determine the top $K$ classes for a tree you will need a model which can do this (most can). If you use K-NN (different K) then you can pick the $K$ closest neighbourhoods. With Random Forests or Naive Bayes you can pick the $K$ classes with the highest probabilities. 

Check out: Learning Minimum Volume Sets $URL$ Anomaly Detection with Score functions based on Nearest Neighbor Graphs $URL$ New statistic in P-value estimation for anomaly detection $URL$ 

How many instances do you have? $\# instances > 100* \#features$? Then you are all set to use a deep learning technique such as neural networks, 1D convolutional neural networks, stacked autoencodders, etc... Less than that!!!! The you should stick with shallow methods. Check out kernel support vector machines, random forests, k-nearest neighbors etc.. 

The simulation starting at (4) can be run once or many times saving only the best result. At the end you should arrive at close to or exactly the best possible seating arrangement. 

Compute mean and variance on entire data set (train + test) and use these to standardize each set. Compute mean and variance on train set and use these to standardize each set. 

The CNN might perform better since your data is inherently spatial. However you have to decide what to do if two or more points overlap. The simplest solution is to pick one randomly, which might be OK depending on your specific task. With a Recurrent Neural Network: 

As Elias Strehle mentioned in the comments, this does not sound like a problem for machine learning. You will be able to solve this much faster by identifying a few simple patterns that indicate which type of log you are looking at. I can already see that lines start with date string followed by a few tokens which look different in each log. You can define regex patterns that will only match a certain type of log, for example: will match your Linux-syslog will match your MySQL-error log will match your Event log will match your FTP log And will match your MSSQL Server log. 

Your approach of adding negative examples to your data set, replacing the 10-output softmax final layer with an 11-output softmax, and retraining is certainly valid. You need these additional examples because the network can only learn to classify categories that it has seen before. Retraining the final layer only is a good first attempt because it is the cheapest, but if you don't get good results you can try fine tuning one or more additional layers at the end of the network. All these techniques fall under the topic of Transfer Learning. Regarding the second part of your question about how many negative examples to include - this is known as Class Imbalance Problem in Machine Learning. Whether your model will always predict the negative class if you include many negative examples depends on the data and the model, but it is certainly a possibility. The safest approach would be to undersample your negative examples and include only 7000 to match the frequency of other classes in the training set. This should be a good first try, but if it doesn't work you should consider a couple of questions: 

Here we have created a model with high variance by overfitting. Training and test accuracy are far apart on the practice data, and training accuracy on the practice data does not match well with training accuracy on the real data. It is not as easy to estimate test accuracy now, because we can't really say if we have overfit by the same amount on both datasets and the second set is harder, or if we have overfit the second set by less. In the first case we might predict test accuracy, and in the second case we might predict . 

Contrary to what others are suggesting, trying to extract data on square footage and number of rooms from apartment ads is not a problem for machine learning - especially not when you don't have any training data. You will spend 10 times as long trying to build a machine learning system for this task than you would manually extracting the data yourself. If you really want an automated system, consider building some simple regex rules based on ads you have seen. There are only so many ways people describe these features, and you should be able to get most of them when they are present in the ad. You won't get 100% accuracy, but you're not going to do any better another way, in my opinion. You can start with some simple rules, and improve them as you look at more and more ads. In Python, for example: