I would like to propose implementation D I'm using in the implementation template so that it is easy to implement for example case insensitive by just providing an appropriate type trait. The namespace hides the implementation, although possible to abuse, I find it unlikely to happen by accident. As the template is explicitly instantiated for both const and non const types any accidental modification of the non-const parameter will cause a compile error when instantiating the const version. 

It doesn't handle negative h. You're basically just changing the lower bits, the high bits are mostly unaffected. A better (and faster) way is to simply multiply by a sizeable prime number: 

you will dereference the iterator + 1 which is beyond the end of the range. Also note that typically one expects the first iterator to be inclusive and the second to be exclusive (as is returned from and on containers). So when used as expected your code will go 2 elements outside of the range. This results in undefined behaviour. Also here: 

Execution can begin immediately and the total delay is 1 div as the divs can run in parallel. This is assuming the compiler doesn't do this behind your back already (I'm not sure if it is allowed to by the IEEE rules) . It also depends on if there is enough instructions to fill the execution units while calculating the reciprocal. Any way it's worth trying. Apart from that I can't think of anything that would make the function as it is any faster. If you really want speed you need to use SIMD instructions in addition to using many thread. But that requires you to restructure your data from "array of structs" to "structs of arrays". Edit: I've had an opportunity to take a closer look and there are some things that you can do to improve performance. This calculates at two occasions: 

Now note that \$6k+5 = 6\left(k+1\right)-1\$ so after checking the divisors we would miss at the start (2 and 3) set and try division with and then increase by six and repeat. This means that you're skipping 2/3rds of all divisors, i.e. ~300% faster. Your exit condition for the loop is useless As the input is guaranteed larger than zero, will always be true in your loop condition. In fact you can reduce branching even further like this: 

Without profiling data, I can only guess... so here goes: At a quick glance the biggest inefficiency I can see is that you allocate and de-allocate the vector capacity for in each iteration. This takes some time, just move the vector outside of the loop and use clear() at the head of the loop. Like this: 

If you evaluate 300 000 nodes, we can estimate a upper bound of the chance to not get a collision anywhere in the table by: 

First of all nice, consistent naming. I approve. What I'm not too fond of is macros, I find the code hard to read because of the macros; however I appreciate that this kind of traits-system is hard (or impossible) to do without macros. The comments do help with understanding the code. You seem to be missing s for , and . You might want to consider adding the common types from as well ( for example). One thing that I wonder about the code is that how will you handle a type that evolves? Say that fields change name, get added or removed how would a parser handle this using this traits class? For example if a field changes name (or type). Other than that I can't see anything wrong with the code. Good work. 

This basically prevents use of with polymorphic classes. But the compiler will happily accept such use and produce code that crashes at best or silently produces erroneous results/leaks memory at worst. You may remember all the ins and outs of using this class every time but your next colleague may not even think about it and just go: 

Don't use virtual if you don't need it doesn't need a virtual destructor as far as I can tell and I can't imagine why you would inherit from this. So simply remove . Wrapper classes should match types exactly You're not matching the types properly for the constructor of ( should be see here). Move assignment/construction As you have already initialized in the in-class declaration, you do not need in your move constructor. Your move assignment operator is missing a return statement. That said, as you are not supposed to instantiate CWindowWrap directly you could simply the move assignment operator and constructor. They won't be used. Same comments goes for the other wrapper. 

Java GUIs: Swing & AWT In standard Java there are two ways to make GUIs: Swing and AWT. To be short, Swing is platform-independent and AWT is platform-dependent. Typically you pick either AWT or Swing and stick to it. I believe Swing is the more popular choice today as it is easier to work with. You can tell Swing code from AWT code by the fact that Swing component classes are prefixed with a . For example is AWT and is the Swing equivalent. The Swing components inherit from the AWT components so a is a . Windows and Frames Now the difference between and (and and respectively) is that a is a plain window without any decorations; no borders, no title bar, no window management buttons. A is a with all these decorations. And I put emphasis on is a in the above as actually inherits . So the inheritance tree looks like this: 

This will make sure you get the correct rounding and remove one division per channel. Also when running the code un-optimized (while debugging) this should perform slightly better due to elimination of some subexpressions. 

Firstly, we cannot rely on arguments that the "compiler isn't smart enough to do that" because that may change tomorrow. We need to base our assessment of the code on what the compiler is and isn't allowed to do. Given two input strings and a yes/no return if the string match, any program performing the function can be transformed into the trivial short-circuited for-loop. Provided that there isn't anything prohibiting the compiler from doing the transform. Will writing the sum of xor differences to a volatile prevent the transform? Maybe, it depends on the execution model and guarantees of volatile in Java. In C++ under the as-if rule all writes and reads to/from volatile memory regions must occur in the same order as-if the program was executed according to the wording in the standard (somewhat simplified but that's the gist of it). So for C++ this would work; However I'm unsure if Java has the same guarantee. For C++ this limitation is natural because it must be able to interface with device drivers and bus addresses where writes and reads cannot be re-ordered or omitted. But Java does none of the kind and as such I wouldn't be terribly surprised if volatile has a more lax meaning in Java. What about ? Well it is really just volatile reads from in disguise to get the seed, the rest is deterministic which the compiler that compiled the JVM isn't allowed to reorder or remove, but what the JIT is allowed to do in this context is beyond me. It is conceivable that the JIT can deduce that the read from will not affect the program in an observable way and it will remove the code all together. If the volatile keyword doesn't have the same meaning in Java as in C++ I believe that doing this kind of processing in Java in a reliable and guaranteed way is difficult. I would consider a hand written loop in assembler in a JNI library, this is guaranteed by your C/C++/assembler to not be fudged in any way. I'm sorry this isn't an authoritative yes/no answer on correctness, but I hope that it is of some help any way. 

This would have been \$\mathcal{O}(n\cdot \log (n))\$ for a binary tree implementation or \$\mathcal{O}(n)\$ for a hash-based implementation. Sorting and ditching order If you sort the array first (as you suggested), you can then iterate over the array and copy the first element and then any remaining elements that differ from the previous element. This algorithm will have \$\mathcal{O}(n\cdot \log (n))\$ runtime. The sorting dominates the run-time here. For small lists, the code by Janos is adequate. If you want performance for large lists you really do need the sort. This solution will not preserve order of elements but will have faster run time for larger lists: 

has even worse entropy as it is only using the lower bits due to the modulo operator. Using modulo skews the distribution As if low entropy wasn't enough, using modulo in this way also skews the distribution of numbers. To see this, imagine and you do then the possible outcomes are: 

Naming The name is discouraged in Java. You should use (without the ). The names and are terrible. Members The field isn't used outside of the main method and you can simply make it local. Leaking resources You really should use Try-with-resources to auto-close the readers. 

Algorithm Let all the walls be defined by \$w_k\$ for the low position of wall \$k\$ and \$W_k\$ for the high position of wall \$k\$. To check if the point \$G_i\$ is in cover the point should be projected onto the Y-axis along the line through \$G_i\$ and \$E\$. Then see if it is inside of any of the walls. We take the line between the points using the straight line equation and solve for the coefficients: \$y = kx+m\$ inserting coordinates for \$G_i\$ and \$E\$ yields two equations: $$E_y = kE_x+m$$ $$G_{iy} = kG_{ix}+m$$ Subtract the lower from the upper: $$E_y - G_{iy} = (E_x - G_{ix})k \Leftrightarrow k = \frac{E_y - G_{iy}}{E_x - G_{ix}}$$ inserting \$k\$ into the first equation yields: $$E_y = \frac{E_y - G_{iy}}{E_x - G_{ix}}E_x+m \Leftrightarrow m = E_y - \frac{E_y - G_{iy}}{E_x - G_{ix}}E_x$$ or put on a common divisor: $$m = \frac{(E_x - G_{ix})Ey - (E_y - G_{iy})Ex}{E_x - G_{ix}} = \frac{ G_{iy}Ex- G_{ix}Ey}{E_x - G_{ix}}$$ Recall the straight line equation: \$y = kx+m\$ we want the projection onto \$x=0\$ so \$y=m\$ for \$G_i\$ projected onto the Y-axis. I.e. the point is safe if for any \$k\$ the following holds: $$w_k < m < W_k$$ At this point we could calculate the above \$m\$ using floating point numbers and convert all the wall positions to floating point too. But we introduce errors, errors that might give the wrong answer if the point is exactly on a wall (which should be unsafe). Let \$a = E_x - G_{ix}\$ and \$b=G_{iy}Ex- G_{ix}Ey\$ so that we get: $$w_k < \frac{b}{a} < W_k$$ To get rid of the division we will multiply through by \$a\$ but need to take care to flip the inequalities if \$a<0\$, so we get: $$\begin{align}w_ka < b < W_ka \qquad& for\quad a \ge 0\\w_ka > b > W_ka \qquad& for\quad a < 0\end{align}$$ But recall that in the problem description it is said that \$E_x < 0\$ and \$G_{ix} > 0\$ which means that \$a < 0\$ always. Thus we can simply use: $$w_ka > b > W_ka$$ Now we can implement the solution using only integer arithmetic. Implementing This is some pseudo c++ code for implementation (I have a fewer so I make no guarantees on correctness here). 

and change the visibility in , it's a private nested class so you're not breaking encapsulation any way. This will put less strain on the GC. Also, if there is any reason to believe that is progressing even roughly linearly through the indices, a pre-fetch running in the background would speed things up. Edit: So with the stats from the comments, the lines executed 99% of the time is: Not thread safe: 

Your design with a trivial factory that calls for each invocation will achieve the first benefit above, but not the second. To obtain the second benefit you would need to create non-trivial factory and deleter which to me seems like unnecessary work. For this reason I believe that your overall design is poor as you can get both benefits for no additional hassle for the user with a better design. Contiguous memory object pool To obtain all the performance gains we need to use a contiguous memory region for our objects. I'm going to show an untested example (without the blocking part in your implementation) of how you could do it: 

Locality of reference You are using linked lists. Linked lists have notoriously bad locality of reference and this will severely impact your CPU cache hit/miss ratio and is likely a part of your performance problem. The best thing you can do is to use a float buffer directly and build the data in it, I'm assuming the is basically just an array of s. This will have the best locality of reference you can get. The fact that you are allocating a new for each vertex is likely causing significant stress on the GC. Another good reason to use directly. Use fixed sizes With some math you can figure out how large your index list needs to be, this means here too you can use an directly and save lots of work of adding to a list. Avoid unnecessary branches This here: 

In the case where there are a few duplicates, your algorithm has \$\mathcal{O}(n^2)\$ runtime. If Javascript would have a widely available Set Which it unfortunately does not yet have then one could do a really fast implementation like this: 

which matches the formula for sample variance. But I don't know how your application, you need to think about it and figure out if you need the population or sample variance. Gosh look at what you did? You made me do maths lol :) 

I'm in a bit of a hurry so I'll only touch on the big high level stuff that I noticed. Abuse of SFINAE I think that the way you are designing two different use cases into one template class and switching between them is an abuse of SFINAE. The most obvious indication of this is the fact that you define two aliases and to act as if they were two separate templates. I understand that this is a way to avoid code duplication but I do not approve of it. The added clutter and the extra overhead of a mutex and atomic variables etc on the single threaded queue is also less than ideal. What I would do instead is to make a shared, hidden base class which holds the shared functionality. Something like this perhaps: 

Zero is twice as likely as any other outcome. This is clearly not uniform! Even for larger RAND_MAX and range values the problem persists. There are some special cases where it works but in general it doesn't. So how should you do it? Unfortunately though you see the approach with modulo shown everywhere on the internet because it's easier to understand. Or because people don't know better. Whatever the reason using modulo is not the way to limit the range of a random number generator. There are a few different ways: Use all the bits - Floating point rescale As rand() returns a some what uniform random number in the range [0,RAND_MAX] and we want all numbers to be used; So we can re-scale this range to [0,1[ and multiply it by our desired range and quantize it. This requires a bit of care so that we don't introduce additional bias around the edges of the range: 

I'm not sure that linear time is possible for the problem as you have described it. It is, see accepted answer. I have a way to make it faster though: Go through the array once and find the largest value in \$A\$, label it \$A_p\$ . Make sure you keep track of the largest and smallest \$p\$ if there are duplicate values. Then let $$G^* = 2A_p+ p_{max} - p_{min}$$. This is our greedy guess at the maximum sum that we will use as a starting point. Next we want to find a lower bound on \$j\$ to limit our search space for any given \$i\$: $$A_i+A_j+j-i > G^* \Leftrightarrow j > G^* +i-A_i-A_j$$ Unfortunately we can't have \$A_j\$ in the calculation of the bound of \$j\$ as that wouldn't be possible to compute. But note that if we replace \$A_j\$ with \$A_p\$ which is at least as large, then we will only lower the bound on \$j\$ and we will not miss any solutions. At this point we can start scanning of \$A\$ and update \$G^*\$ as we go to have higher and higher lower bounds on \$j\$ as we go. Technically this is still \$O(n^2)\$ but with a smaller constant as you are able progressively take larger and larger skips. Pseudocode: 

To create a binding, a kind of numerical expression that can be evaluated. And whenever you need the scale factor simply call: . The result is cached and automatically updated as soon as either the canvas or the sliders change. So the above snippet will eliminate the method. Going with the Model-View-Controller idea, your "model", the would have: that you would bind to the value of the sliders of your UI ("view") in your "controller". And they would automatically be kept in sync. Avoid allocating new objects in your game loop. I see that you frequently allocate new elements in your game code. While memory allocation is cheap in Java, frequently allocating objects and throwing them away like you're doing will cause a significant strain on the GC and can cause your game to appear "jittery". I would advice to allocate the position objects once and re-use them where it makes sense. For example here: