1) No implication is known in either direction. We know that P=NP implies P=PH. But we don't know if BQP and QMA are in PH, so maybe P could equal NP yet BQP and QMA still wouldn't collapse. (On the other hand, note that QMA⊆PP⊆P#P, so certainly P=P#P would imply BQP=QMA.) To show that BQP=QMA implies P=NP seems even more hopeless in the present state of knowledge. 2) Absolutely, all three barriers apply with full force to BQP vs. QMA (and even to the "easier" problem of proving P≠PSPACE). First, relative to a PSPACE oracle (or even the low-degree extension of a PSPACE oracle), we have P = NP = BQP = QMA = PSPACE, so certainly nonrelativizing and non-algebrizing techniques will be needed to separate any of these classes. Second, to get a natural proofs barrier for putting stuff outside BQP, all you need is a pseudorandom function family that's computable in BQP, which is a formally weaker requirement than a pseudorandom function family computable in P. Addendum: Let me say something about a "metaquestion" which you didn't ask but hinted at, of why people still focus on P vs. NP even though we believe Nature is quantum. Personally, I've always seen P vs. NP as nothing more than the "flagship" for a whole bunch of barrier questions in complexity theory (P vs. PSPACE, P vs. BQP, NP vs. coNP, NP vs. BQP, the existence of one-way functions, etc), none of which we know how to answer, and all of which are related in the sense that any breakthrough with one would very likely lead to breakthroughs with the others (even where we don't have formal implications between the questions, which in many cases we do). P vs. NP isn't inherently more fundamental than any of the others -- but if we have to pick one question to serve as the poster child for complexity, then it's a fine choice. 

Adiabatic QC is typically "based on qubits" just as much as circuit-based QC is -- I don't know where you got the idea that it isn't! (Though one could also use qutrits or other building blocks, in either the circuit or the adiabatic models.) As Mateus pointed out, the justly-famous result of Aharonov et al. says that "adiabatic QC is equivalent to standard QC." But that result needs to be interpreted with a bit of care. It holds if the final state of the adiabatic computation can be arbitrary -- so that, in particular, the final state can encode the entire history of a circuit-based quantum computation. However, if the final state needs to be a classical computational basis state---as it typically is in the adiabatic optimization algorithm (the "original" example of adiabatic QC)---then adiabatic QC can certainly be simulated in the circuit model, but the reverse is not known and is far from clear. So with the latter assumption, it's possible that adiabatic optimization really does give rise to a new complexity class intermediate between BPP and BQP. 

This is something I've often wondered about as well! If by "results in computability theory," you mean results that are invariant with respect to the choice of machine model (Turing machines, RAM machines, etc.), then I don't know a single example of such a result, and I definitely would've remembered if I'd seen one. The closest I can suggest to an answer is: I think there are many interesting questions in computability theory that might depend on the machine model. For example: is the Busy Beaver function, with its usual definition in terms of Turing machines, infinitely often odd? Is the value of BB(20) independent of ZFC? Whatever the answers to these questions are, they could surely be different for relativized analogues of the BB function. 

I'm interested in the following problem. We're given as input a "target permutation" $\sigma\in S_n$, as well as an ordered list of indices $i_1,\ldots,i_m\in [n-1]$. Then, starting with the list $L=(1,2,\ldots,n)$ (i.e., the identity permutation), at each time step $t\in [m]$ we swap the $i_t^{th}$ element in $L$ with the $(i_t+1)^{st}$ element, with independent probability $1/2$. Let $p$ be the probability that $\sigma$ is produced as output. I'd like to know (any of) the following: 

András, as you probably know, there are so many examples of what you're talking about that it's almost impossible to know where to start! However, I think this question can actually be a good one, if people give examples from their own experience where the proof of a widely-believed conjecture in their subarea led to new insights. When I was an undergrad, the first real TCS problem I tackled was this: what's the fastest quantum algorithm to evaluate an OR of √n ANDs of √n Boolean variables each? It was painfully obvious to me and everyone else I talked to that the best you could do would be to apply Grover's algorithm recursively, both to the OR and to the ANDs. This gave an O(√n log(n)) upper bound. (Actually you can shave off the log factor, but let's ignore that for now.) To my enormous frustration, though, I was unable to prove any lower bound better than the trivial Ω(n1/4). "Going physicist" and "handwaving the answer" never looked more appealing! :-D But then, a few months later, Andris Ambainis came out with his quantum adversary method, whose main application at first was a Ω(√n) lower bound for the OR-of-ANDs. To prove this result, Andris imagined feeding a quantum algorithm a superposition of different inputs; he then studied how the entanglement between the inputs and the algorithm increased with each query the algorithm made. He showed how this approach let you lower-bound quantum query complexity even for "messy," non-symmetric problems, by using only very general combinatorial properties of the function f that the quantum algorithm was trying to compute. Far from just confirming that the quantum query complexity of one annoying problem was what everyone expected it to be, these techniques turned out to represent one of the biggest advances in quantum computing theory since Shor's and Grover's algorithms. They've since been used to prove dozens of other quantum lower bounds, and were even repurposed to obtain new classical lower bounds. Of course, this is "just another day in the wonderful world of math and TCS." Even if everyone "already knows" X is true, proving X very often requires inventing new techniques that then get applied far beyond X, and in particular to problems for which the right answer was much less obvious a priori. 

For all i,j, if the edge (i,j) exists and has weight wij, then set Mij:=exp(wij). (This turns the sums into products.) For all i,j, if the edge (i,j) doesn't exist, then set Mij:=0. Pad M to make sure that there are two or more permutations π such that Π Mi,π(i) = 0. (This rules out spurious solutions that don't correspond to any perfect matching in G.) 

I'm not aware of any direct consequence of $NP\subset P/poly$ for $BQP$. Of course it might lessen the interest in quantum computing, since it would mean that we could do something far more impressive with just good old classical nonuniformity than we could with quantum mechanics. And it would imply $NP\subset BQP/poly$. :-) OK, it would also weaken the arguments that BosonSampling, and other quantum sampling tasks of that kind, are classically hard. For those arguments involve showing that if the sampling tasks can be done in classical polynomial time, then $PH$ collapses to the second or third level (along with several other strange things, like $P^{\#P}=BPP^{NP}$). But if $NP\subset P/poly$, then $PH$ collapses anyway. 

Consider the following model: an n-bit string r=r1...rn is chosen uniformly at random. Next, each index i∈{1,...,n} is put into a set A with independent probability 1/2. Finally, an adversary is allowed, for each i∈A separately, to flip ri if it wants to. My question is this: can the resulting string (call it r') be used by an RP or BPP algorithm as its only source of randomness? Assume that the adversary knows in advance the entire BPP algorithm, the string r, and the set A, and that it has unlimited computation time. Also assume (obviously) that the BPP algorithm knows neither the adversary's flip decisions nor A. I'm well-aware that there's a long line of work on precisely this sort of question, from Umesh Vazirani's work on semi-random sources (a different but related model), to more recent work on extractors, mergers, and condensers. So my question is simply whether any of that work yields the thing I want! The literature on weak random sources is so large, with so many subtly-different models, that someone who knows that literature can probably save me a lot of time. Thanks in advance! 

Huck, as Lance and Robin pointed out, we do have oracles relative to which PH is not in PP. But that doesn't answer your question, which was what the situation is in the "real" (unrelativized) world! The short answer is that (as with so much else in complexity theory) we don't know. But the longer answer is that there are very good reasons to conjecture that indeed PH ⊆ PP. First, Toda's Theorem implies PH ⊆ BP.PP, where BP.PP is the complexity class that "is to PP as BPP is to P" (in other words, PP where you can use a randomization to decide which MAJORITY computation you want to perform). Second, under plausible derandomization hypotheses (similar to the ones that are known to imply P=BPP, by Nisan-Wigderson, Impagliazzo-Wigderson, etc.), we would have PP = BP.PP. Addendum, to address your other questions: (1) I'd say that we don't have a compelling intuition either way on the question of whether PP = PPP. We know, from the results of Beigel-Reingold-Spielman and Fortnow-Reingold, that PP is closed under nonadaptive (truth-table) reductions. In other words, a P machine that can make parallel queries to a PP oracle is no more powerful than PP itself. But the fact that these results completely break down for adaptive (non-parallel) queries to the PP oracle suggests that maybe the latter are really more powerful. (2) Likewise, NPPP and coNPPP might be still more powerful than PPP. And PPPP might be more powerful still, and so on. The sequence P, PP, PPP, PPPP, PPP^PP, etc. is called the counting hierarchy, and just as people conjecture that PH is infinite, so too one can conjecture (though maybe with less confidence!) that CH is infinite. This is closely related to the belief that, in constant-depth threshold circuits (i.e., neural networks), adding more layers of threshold gates gives you more computational power. 

Thanks for your question! There are two answers, depending on whether you're interested in the hardness results for exact or approximate BosonSampling. In the exact case, we prove that given any n-by-n complex matrix A, you can construct an optical experiment that produces a particular output with probability proportional to |Per(A)|2. This, in turn, implies that no classical polynomial-time algorithm can sample from exactly the same distribution as the optical experiment (given a description of the experiment as input), unless P#P = BPPNP. In fact we can strengthen that, to give a single distribution Dn (depending only on the input length n) that can be sampled using an optical experiment of poly(n) size, but that can't be sampled classically in poly(n) time unless P#P = BPPNP. In the approximate case, the situation is more complicated. Our main result says that, if there's a classical polynomial-time algorithm that simulates the optical experiment even approximately (in the sense of sampling from a probability distribution over outputs that's 1/poly(n)-close in variation distance), then in BPPNP, you can approximate |Per(A)|2, with high probability over an n-by-n matrix A of i.i.d. Gaussians with mean 0 and variance 1. We conjecture that the above problem is #P-hard (at the very least, not in BPPNP), and pages 57-82 of our paper are all about the evidence for that conjecture. Of course, maybe our conjecture is false, and one can actually give a poly-time algorithm to approximate the permanents of i.i.d. Gaussian matrices. That would be a phenomenal result! However, the whole point of 85% of the work we did was to base everything on a hardness conjecture that was as clean, simple, and "quantum-free" as possible. In other words, instead of the assumption "approximating the permanents of some weird, special matrices that happen to arise in our experiment is #P-hard," we show that it suffices to make the assumption "approximating the permanents of i.i.d. Gaussian matrices is #P-hard." 

The Matrix Mortality Problem for 2x2 matrices. I.e., given a finite list of 2x2 integer matrices M1,...,Mk, can the Mi's be multiplied in any order (with arbitrarily many repetitions) to produce the all-0 matrix? (The 3x3 case is known to be undecidable. The 1x1 case, of course, is decidable.) 

I posted some quantum complexity theory project ideas at $URL$ (But beware, most of these are problems that have been open for years! My suggestion for an undergraduate project would be to break off a chunk of one of the problems.) 

Just one thing to add to John's answer: Under a plausible derandomization hypothesis, AM = NP. In that case, certainly we would have AM ⊆ QMA. 

In short, I'm not hoping for a "classification theorem" -- just for a list that usefully reflects what we currently know about efficient algorithms. And that's why what interests me most are the techniques for putting things in P or BPP that have broad applicability but that don't fit into the above list -- or other ideas for improving my crude first attempt to make good on my boast to the physicist. 

Normally, the way people prove that a complexity theorem relativizes is using the following two-step procedure: 

The very terms "on the P-side" and "on the NP-side," and of course the question title, encourage us to imagine a "cozy neighborhood" surrounding P and another "cozy neighborhood" surrounding the NP-hard problems. However, I'd like to argue that these two neighborhoods are not so "cozy" at all! As a first observation, there are problems "on the P-side" that seem "morally" much closer to NP-hard than to P. One example, anticipated by Gil of course, is the general problem of inverting one-way functions (depending on exactly what type of reductions are allowed; see Bogdanov-Trevisan or Akavia et al.). Conversely, there are also problems "on the NP-side" that seem "arbitrarily far" from being NP-hard. One silly example is a random language L, with probability 1 over L! For if such an L is in P, then 0=1 and math is inconsistent, and therefore PH collapses also. ;-D (Note that a random language L is also "on the P-side," with probability 1 over L. For almost all such L's have the property that if they're NP-hard, then NP⊆BPP and PH collapses. And this gives a proof, much simpler than the appeal to Ladner's Theorem, that there exist languages on both "sides." Indeed, it shows that of the uncountable infinity of languages, "almost all" of them -- in fact, 100% -- are on both sides!) This sounds like juvenile game-playing, but there's a serious lesson I'd like to draw from it. I'd argue that, even though QUANTUM SAMPLING is formally "on the NP-side," that problem is barely any closer to being "morally NP-hard" than the random language L was. Arkhipov and I (and independently, Bremner-Jozsa-Shepherd) showed that, if QUANTUM SAMPLING is in P (or rather, in SampBPP, the class of polynomially-solvable sampling problems), then P#P = BPPNP, and therefore the polynomial hierarchy collapses. Yet if you're a BPP machine, an oracle for BosonSampling would, as far as we know, bring you no closer to solving NP-complete problems than a random oracle would. Only if you already have the ability to solve NP-complete problems -- say, if you're a BPPNP machine -- do you "notice" that the BosonSampling oracle boosts your capabilities even further, to #P. But the property of boosting NP up to #P seems different than, and maybe even "orthogonal to," the property of being NP-hard on one's own. Incidentally, a wonderful open problem suggested by Gil's question is whether BosonSampling is also "on the P-side." That is, can we show that if NP reduces to BosonSampling then PH collapses? While I might be missing something obvious, at first glance I have no clue how to prove such a thing, any more than I know how to prove the stronger implication that if NP ⊆ BQP then PH collapses.