Sys.dm_tran_locks  will list all current locks which are granted or pending. You can join it on session_id with  sys.dm_exec_sessions  will list all current sessions which includes client host name and login name. You can even use the GUI - activity monitor to watch for activities going on on the server instance. (You can even profile using PROFILER and then use the T-SQL to monitor your server instance). 

First of all, Welcome to stackexchange and thanks for your 1st Post. To answer your questions : if this process of reorganize the Full Text Catalog is done online or offline? The reorganize process is Online, but is slower than Rebuild. When the catalog reorganize is occurring the users will still be able to query the full text data? Yes, but there might be blocking. Also you need to take account of below from BOL : Depending on the amount of indexed data, a master merge may take some time to complete. Master merging a large amount of data can create a long running transaction, delaying truncation of the transaction log during checkpoint. In this case, the transaction log might grow significantly under the full recovery model. Also, I would suggest you take a look at the fragmentation of the full text Indexes using : I normally go for REORGANIZING if you have between 30-50 fragments per table 

Rebuilding an index will update stats on the columns associated with that index. There can be a lot of variables which might be different between 2 servers (2014 vs 2016) 

The above will not allow connections to secondary. So your secondary will be TRUE standby and will be used when you do a failover. 

As you can see above that there are essentially 3 main phases of restore - data copy, undo and redo phase along with other sub-phases where in it opens and loads the backup set as well. 

I have written an extensive list of things you should check when you migrate from 2000 to higher versions 

Converting my comment as answer : You have to use undocumented fn_dblog Begin Time, SUSER_SNAME([Transaction SID]) or fn_dump_dblog -- when reading from T-log backups. Be careful and try it on a NON PROD system as every time fn_dump_dblog is called, it creates a new hidden SQLOS scheduler and up to three threads, which will not go away (and will not be reused) until a server restart. Also, its not that straight forward to be able to read T-log and come on a conclusion - If you have not done it or are new to it. You are better off trying a 3rd party tool (like Apex SQL Log) if it is that critical for you to find out who did the mess. 

This is a perfect case of implementing Logshipping. You can even delay applying transaction logs and have the secondary database in standby mode which can be used for reporting queries. Alternatively, if you are on sql server 2012 Enterprise and up, then you can look into implementing AlwaysON Availablity groups. 

There are many ways to do it. a. Backup from 2000 instance and restore it on 2008R2 instance. This will require your application downtime until you restore the backup successfully to 2008R2 instance. Also, you have to kill all the connections to the 2000 instance as you dont want to loose the data. b. Set up logshipping from 2000 to 2008R2. When failing over, you just have to take a tail logbackup and restore at the secondary (2008R2) instance and then bring the database online. Option b will be requiring less downtime. EDIT : You can have Log-shipping between SQL 2000 and SQL 2005/2008R2. BUT you cannot keep secondary in STANDBY mode. Some references on Implementing Log shipping on SQL 2000. Note that this applies for setting up log-shipping from 2000 to 2005/2008/2008R2 as well. Implementing Log Shipping Migrating a SQL Server 2000 Log Shipping Configuration to SQL Server 2008 Custom Log Shipping Logshipping using Powershell 

It's Query Data Store related and you can safely ignore it. Query store is a new feature in SQL 2016. Refer to $URL$ Edit: Below confirms that I was correct - You can safely ignore/filter out this wait type. Paul Randal has commented in his Wait statistics, or please tell me where it hurts 

You should check breaking changes. you can use upgrade advisor or dbatools - Test-DbaMigrationConstraint 

I would suggest you to use POWERSHELL as it is very flexible and the code can be resuable and is best supported on windows & SQL Server 2008 R2 and up. Note: there are tons of scripts that are available on internet, so no need to reinvent the wheel ! Scripts can be found : 

An index is an on-disk structure associated with a table or view that speeds retrieval of rows from the table or view. An index contains keys built from one or more columns in the table or view. These keys are stored in a structure (B-tree) that enables SQL Server to find the row or rows associated with the key values quickly and efficiently. 

Depending on how much RAM is on your server, below is a good starting point (given you have SSAS ONLY running on the server) : click to enlarge: 

This has is_dynamic = 0 in the . This means that when you run , the VALUE will be set to , but it wont take affect until you restart sql server. You should check value in use for or run_value of the output. 

First of all, apologies for such a long answer, as I feel that still there is a lot of confusion when people talk about terms like collation, sort order, code page, etc. From BOL : 

Suspend data movement of the T-DB2 database. I hoped this would free up network bandwidth for the priority database, E-DB1. No effect. Rebooted the secondary node (LDPRDENTDB1). No effect. 

Not in terms of sql server's performance. Better for maintenance and when you want to find specific error - the size of the log file will affect any app or even opening the error log. To recycle error log - use . You can even schedule this using a sql agent job on a weekly or biweekly (depending on the activity of your server). If you have Logshipping, to avoid getting your log bloat, I use 

Note: Enabling , you dont need to set the compatibility level of the database to a lower level. From KB2801413 : 

As per BOL, an account with database owner dbo privileges is required. More info here. So, the user who created it, or a member of the db_owner role can open diagrams. 

Converting my comment to answer (read the comments in the script) : if you are using the same backup file name () every time you backup, its better to use . This way the previous file gets overwritten. 

Since you mentioned about Availability Groups, this means that you are using SQL Server Enterprise edition (and possibly SQL Server 2012 or 2014). You can use database snapshots, but in our case there were lot of dependencies on different components and so using database snapshot was ruled out. Below will require a bit of more planning and testing. 

The difference between and are not that major meaning that is a compatibility view and is depreciated (for backward compatibility). So, just a replacement for for newer versions of SQL Server. So its a good idea for transitioning from compatibility view to DMV's (as you mentioned that the script won't be used for any SQL 2000 servers). DMV's can be run for any databases if you have required permissions e.g. VIEW SERVER STATE permission to run Reference : QUERYING PERFORMANCE COUNTERS IN SQL SERVER for an excellent script to make sense out of the performance counters. 

@Lu4 .. I voted to close this question as "Tip of Iceberg" but using query hint you will be able to run it under 1 sec. This query can be refactored and can use , but it will be a consulting gig and not as an answer in a Q&A site. Your query as is will run for 13+ mins on my server with 4 CPU and 16GB RAM. I changed your query to use and it ran under 1 sec 

Note: Above will give you an estimated time taken by BCP and the size of the BCP file. So above point will answer you 1st and 2nd points. Now to get the time it took to apply the snapshot to the subscriber, you have to run from the publisher server specifying the SUBSCRIBER_SERVER_NAME 

In order to see the additional logged information you need to use this stored procedure sp_help_jobsteplog or you could query the table directly. More info here 

@Eric213 already answered your question. To add more to it : -- find out what traceflags are enabled on the instance 

Your best bet is to use Transactional Replication. T-Rep is meant to be suited in your scenario as you want to replicate a table from server A to server B. No need to run a store procedure every min (unless you are doing something specific with these SP's which you have not highlighted in your question). Note: Only when you generate an initial snapshot in transactional replication, there will be locks on the table. So better to set up during maintenance window if your table is very large. Also, in T-Rep changes are tracked asynchronously and applied asynchronously to the subscriber (destination) server. It has Wizards to allow you to configure T-Rep correctly with no coding efforts. Service Broker on other hand will track data changes synchronously and apply to the destination server asynchronously. It will require more coding and depending on the amount of data to be transferred, it will be slow. I have dealt with Service Broker and trust me ... its very hard to debug .. just like a black box vs Replication will show you errors on GUI and is easy to troubleshoot as well. 

I would say, use bcp out the file, compress it using zip or 7zip and then send using powershell or ssis. 

Discontinued Integration Services Functionality in SQL Server 2012 The options to configure a data viewer to display data in a histogram, scatter plot, or column chart have been discontinued in this release. Data Viewer : Previous versions of SSIS included four types of data viewer—the Grid, Histogram, Scatterplot, and Column Chart data viewers. Because the Grid data viewer is the most commonly used, the SSIS team removed the other types of data viewers from SSIS 2012.