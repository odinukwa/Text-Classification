Spark only recently implemented CountVectorizer, which will take the labels (as strings) and encode them as your 100-dimensional vector (assuming all 100 labels show up somewhere in your dataset). Once you have those vectors, it should be a simple step to threshold them to make them 0/1 instead of a frequency. 

This can be thought of as an active learning problem. You want feedback from the learning process to influence what kind of input examples you next train on. Active learning is still a difficult problem in most settings. As @cohoz points out, hill climbing is an intuitive option in this case, but only if you always want to increase or decrease your measure of frustration. If instead the goal is to explore frustration as a function of as much of the music parameter feature space as possible, you will want to read about active learning and uncertainty. 

It is possible that most of the variance in the dataset exists between input images (or between input and output images). In that case, the most informative principal components serve to separate input examples or to separate the input from the output. If only the less informative PCs describe the variance b/w outputs, it will be harder to distinguish between outputs than it was in the original feature space. That said, PCA in images is rarely all that helpful. In this example I can see it being appealing, but it would probably much more meaningful and memory-friendly to learn a different lower dimensional feature representation. You could try a simple autoencoder, SIFT/SURF features, or haar-like features. 

A word-level metric would prefer the first result, since it contains both alcohol and potatoes.. A URL-level metric would likely label the second result as more relevant, which would allow a model to form associations beyond simple word-matching (i.e. it could learn that "alcohol" and "vodka" are associated). 

Scipy's entropy function will calculate KL divergence if feed two vectors p and q, each representing a probability distribution. If the two vectors aren't pdfs, it will normalize then first. Mutual information is related to, but not the same as KL Divergence. "This weighted mutual information is a form of weighted KL-Divergence, which is known to take negative values for some inputs, and there are examples where the weighted mutual information also takes negative values" 

If you initialized the weights randomly and predicted immediately, you should probably expect accuracy. But you are training first. The data may be random, but it is still possible to overfit it. With a sample that small and wide, you could imagine some degree of separability of the classes by chance. In that scenario, your network can learn weights that are better than random (far from perfect in only one iteration). The effect would likely be lessened by using much more data or holding out/using new random data to test accuracy. 

is the number of training batches to do before you stop. is the number of training batches you've seen. Each iteration, you decide whether or not your validation is lower than your previous best. The improvement is only stored if the score is lower than . It appears that is a multiplier. Every time you have a new lowest score, you up the total number or training batches to , but not below the current value of . is just the number of batches between times you check the validation score. 

The cost function controls the algorithm completely - the new regularization from weight decay is likely responsible for the jump in loss. If you only added a small amount of regularization and the loss ballooned, you were likely very overfit. The solution is cross validation. The only way to know the optimal amount if regularization is to see what does the best on held-out data. 

First point is that convolutional neural networks would be incredibly expensive to train on images that large. The minimum size you could allow would be . It sounds like you want to achieve two tasks: OCR then document classification. Given that you have predominantly text in these documents, you would almost certainly be better served by using a more a traditional method for OCR. If you want or need to use NNs for the OCR piece, you will almost certainly need to use a sliding window. For inspiration on how you might do that, you could look to Recurrent Neural Networks that process images in sequences Once you have the text there are again much simpler methods to deal with text than NNs, but they can achieve great results. To deal with different document lengths, you can use an RNN architecture like LSTM. Or you can use paragraph vectors, which do most of the heavy lifting for you and give you an N-dimensional representation of your text. 

Been a while since I used SAS but I think you could use a data step where you make an ID var on which you can aggregate, then vars for each "cat1-4." You could then use proc transpose or do a proc SQL sum() with a "groupby" statement on the ID variable. So first step is to get to: 

As a first step you should try logistic regression on your tf-idf vectors. It is simple to implement and would provide a good baseline for comparison. You can find an implementation in whatever language you're using. You could also try some kind of (perhaps supervised) topic modeling to create a better feature space, but that would be more involved. 

I would like to use the shared information here (since all of these are physical properties) in a multitask neural net setup. For examples in both S and T, I have a target vector as long as X union Y. But for members exclusive to S or T, I am missing values. In the examples, I have 

There are two main functions they undo. The pooling layers in the convolutional neural network downsample the image by (usually) taking the maximum value within the receptive field. Each image region is downsampled to a single value. What this implementation does is store which unit had the maximum activation in each of these pooling steps. Then, at each "unpooling" layer in the deconvolutional network, they upsample back to a image region, only propagating the activation to the location that produced the original max-pooled value. Thus, "the output of an unpooling layer is an enlarged, yet sparse activation map." Convolutional layers learn a filter for each image region that maps from a region of size to a single value, where is the receptive field size. The point of the deconvolutional layer is to learn the opposite filter. This filter is a set of weights that projects an input into a space of size , where s is the size of the next convolutional layer. These filters are learned in the same way that as regular convolutional layers are. As the mirror image of a deep CNN, the low-level features of the network are really high-level, class-specific features. Each layer in the network then localizes them, enhancing class-specific features while minimizing noise. 

A simple approach might use document vectors or LDA representing the text of each section (possibly chunking longer sections) with a simple classifier trained on those vectors to predict section. These can be found in gensim or sklearn. A harder to implement but likely more powerful technique would be to train a recurrent neural network on the text of each section (again likely breaking it into smaller chunks) so that the representation your model learns is learned specifically to predict Abstract/Indices/etc ... 

Trigram models can be more powerful for document retrieval than unigram models, but if you want to handle spelling errors, they will not be of much help. You need some form of fuzzy matching for that. For example the string, "I like dosg too" would fool a unigram model because "dosg" is likely "dogs" misspelled, and it will encode it as . But you have the same problem in a trigram model. It will encode , . Which is not really better, as it will still not match any trigrams with the word "dogs" in it. 

The model likely won't have much predictive power if the input is a single day. No weather patterns longer than one day can be captured that way. Instead you should aggregate the days together. You can come up with different features that describe your larger, aggregated unit of time (months, year). For example mean precipitation is a very simple one. Binning the data and using counts within those bins would also work. More advanced options would roll the time all the way up to a full year and learn a feature set at that level. 

You can still use sklearn.linear_model.LinearRegression. Simply make the output a matrix with as many columns as you have dependent variables. If you want something non-linear, you can try different basis functions, use polynomial features, or use a different method for regression (like a NN). 

The distinction between ordinal and categorical does matter. If in truth the difference between white and red was drastically different from red and black, your (10,20,30) ordinal model would not have performed well. One hot encoding can learn the relationship between the ordinal values more finely, but throws out the information that the variables are related. Similarly, with insufficient data it is more likely to overfit. Ordinal variables lessen those problems but at the cost of forcing you to define the interval. There are a number of methods for defining the values of your ordinal variables, like rologit. 

Your R,G, and B pixel values can be broken into 3 separate channels (and in most cases this is done for you). These channels are treated no differently than feature maps in higher levels of the network. Convolution extends naturally to more than 2 dimensions. Imagine the greyscale, single-channel example. Say you have N feature maps to learn in the first layer. Then the output of this layer (and therefore the input to the second layer) will be comprised of N channels, each of which is the result of convolving a feature map with each window in your image. Having 3 channels in your first layer is no different. This tutorial does a nice job on convolution in general. $URL$ 

A word in your vocabulary A set of syllables in your vocabulary A combined set of letters and syllables in your vocabulary 

If you did a simple linear regression for each of the 5 outputs, your r-squared would be a good measure of the "aggregate correlation." If you wanted to compare to the other results, it would be helpful to do the same for each of your single features as well. 

As a simple measure, you could simply take the point biserial correlation between duration and rating for a given user. If they aren't correlated (positively or negatively), this variable is likely not important to that user. Another way to determine the impact of a variable is by training another model excluding that variable and determining how that affects the quality of the predictions. 

I don't know of a well documented python option for collaborative filtering, but one option is to use Spark's MLib. There is a wrapper that allows you to run it in python (pyspark) and it is straightforward to get it running on your local machine. 

There are a number of implementations of Word2Vec, but most assume the basic unit to be 'sentences' - though they don't care what those sentences look like. If you are using something like you will need each sentence in its own list, and each sentence will be a list of tokens. If you are using another package you may be reading from the disk in which case you likely need all documents concatenated with each newline representing a new sentence. The fundamental consideration in deciding on the size of a 'sentence' (whether it's the entire note, all a patient's notes, or a single sentence) is what surrounding words should be used to train the model. The W2V model will consider all words within a certain distance of the target word when tuning its vector representation (regardless of if it's or ), but will not look beyond a 'sentence' boundary. So if your fundamental unit is a document not a sentence, associations will bleed across sentences (this may or may not be what you want). As an aside, consider the nature of the documents first. Patient notes can be messy and full of automatically generated text, so it can be important to strip out or replace certain strings. Similarly, they often use whitespace and newlines rather than punctuation. You might want to consider treating existing whitespace as the end of a sentence and you might want to use a parser to replace dates, names, etc with default tokens to increase generalizability. 

Benchmarking is the process of comparing your result to existing methods. You may compare to published results using another paper, for example. If there is no other obvious methodology against which you can benchmark, you might compare to the best naive solution (guessing the mean, guessing the majority class, etc) or a very simple model (a simple regression, K Nearest Neighbors). If the field is well studied, you should probably benchmark against the current published state of the art (and possibly against human performance when relevant).