The problem is that it is not capable of providing me ALL of the and their respective if it exists, OR their if it didn't exist. It is perfect at providing any one of them, but not at all. Basically, it's to enforce that if a language does not have a translation for a specific key, then the default is to use which is of translation. (Ideally, it would not even do that, but instead load the translation for , which I can do myself if pointed in the right direction for the rest of the query.) I've spent a LOT of time on this, and I know if I were to just write it in C# (like I usually do) it would be done by now. I want to do this in SQL, and I'm having trouble getting the output I like. The only caveat, is I want to limit the number of actual queries applied. All columns are indexed and such as I like them for now, and without real stress-testing I cannot index them further. Edit: Another note, I'm trying to keep the database as normalized as possible, so I don't want to duplicate things if I can avoid it. Example Data Source dbo.SupportCategories (Entirety): 

Is there a best practice method to handle localized strings via a view? Which alternatives exist for using a as a stub? (I can write a specific for each schema owner and hard-code the language instead of relying on a variety of stubs.) Can these views be simply made deterministic by fully qualifying the nested s and then schemabinding the view stacks? 

Also, feel free to vent about your experiences with tasks like these. I'm sure I'm not the only one who has been handed down tasks like these. 

Here is why s are being used as predicates. The column is formed by concatenating: During testing of these, a simple from the view returns ~309 rows, and takes 900-1400ms to execute. If I dump the strings into another table and slap an index on it, the same select returns in 20-75ms. So, long story short (and I hope you appreciated some of this sillyness) I want to be a good Samaritan and re-design and re-write this for the 99% of clients running this product who do not use any localization at all--end users are expected to use the locale even when English is a 2nd/3rd language. Since this is an unofficial hack, I am thinking of the following: 

Remember that there's a disk space issue involved as well, as you stated that your table contains a large number of rows. The clustered index will not change the amount of disk space your table consumes either way, but adding a non-clustered index will allocate extra space. If you all the columns from your table, like I did in the example, the non-clustered index will roughly take up as much space as the rest of the table does. 

Solution Store dates as dates, store numbers as numbers. It's really that simple. Speak to your software people about this. After all, they type all their variables in C#, why so lax in the database? Short-term You could replace with if you're on SQL Server 2012 or newer. This returns a value instead of an error whenever the input won't convert. On older SQL Server versions, there's no easy way to do this - you could try a lot of conditions inside the , making sure that invalid dates are passed to as values. 

Create a new String table populated with a cleanly joined set of data from the original base tables Index the table. Create a replacement set of top-level views in the stack that include and columns for the and columns. Modify a handful of s that reference these views to avoid type conversions in some join predicates (our largest audit table is 500-2,000M rows and stores an in a column which is used to join against the column ().) Schemabind the views Add a few indexes to the views Rebuild the triggers on the views using set logic instead of cursors 

Say for example, if a defect is reported in this spaghetti mess, for example a column may not be accurate in certain specific circumstances, what would be the best practices to start troubleshooting? If debugging this was a zen art, how should I prepare my mind? :) Are there any preferred SQL Profilier filter settings that have been found useful in debugging this? Any good tactics, or is it even possible to set breakpoints in nested procs when using debug mode? Tips or suggestions on providing meaningful debug logging when dealing with nested string builders? 

I'm working on the assumption that the table in question is a fact table, not a dimension table with a huge composite key: Just to fix the performance issue in the short term, I would add all of these key columns as the table's clustered index, which means you won't have to a lot of measures and stuff, like the suggested index does. Also, make the index unique if the data allows for this. As for the column order of a clustered index on a fact table, it depends on how you're accessing them. If you're only using a cube to read large chunks of data, I would probably prioritize priority by making the index chronological, i.e. putting the date column first - that way, new rows get added to the end of the index (in the best of worlds). If you're running user T-SQL queries on the fact table, I would try to arrange the index columns in an order that gives you Index Seeks or Range Scans as much as possible: first, columns that are filtered on single dimension keys (think "year", "type", "unit" or "department"-type dimensions), then those columns that are filtered on multiple dimension members, ranges, or used for sorting. There are, of course, other schools on how to build indexes - this is not a "single correct answer". Edit: More on clustered vs non-clustered indexes: I'm guessing that you already have an existing clustered index, and that's why SQL Server suggests a non-clustered index. However, non-clustered indexes have to be explicitly defined with columns. Clustered indexes define the actual storage/sort order of the table, and as such, they will implicitly include all columns in the table (I won't go into LOB columns like varchar(max) and xml). The clustered index is normally the "catch-all index" that takes care of queries that are not suitable for an existing non-clustered index, which makes it even the more important (in my opinion) that it's well-designed and not, for instance, just on an column. Plus, a non-clustered index will take up more drive space, so a non-clustered index that covers all of the table's columns will in effect take up as much space as the table itself. A clustered index is the table. 

Nested views are non-deterministic, so we cannot index them Each view references multiple s to build the strings Each UDF contains nested s to get the ISO codes for localized languages Views in the stack are using additional string builders returned from s as predicates Each view stack is treated as a table, meaning that there are / / triggers on each to write to the underlying tables These triggers on the views use that stored procedures which reference more of these string building s. 

I recently inherited a MESS of a search. It's around 10,000 lines of code in the procs/functions alone. This search is aptly named the "Standard Search." It's a proc that calls about 6 other procs, which are entirely composed of string builders, in which each proc has between 109 and 130 parameters. These procs also call deeply nested functions which generate more strings to be assembled into a final query. Each proc can join up to 10 views, depending on the logged in user, which are abstracted from the table data by between 5 and 12 other views per primary view. So I am left with hundreds of views to comb through. On the plus side, it does have a parameter to PRINT out the final query, (unformatted of course!) It's a hot mess. The string builders also have no formatting, and they don't logically flow. Everything jumps around everywhere. Although I couid do an auto-format, to pretty print the code, that doesn't help with the formatting on the string builders, as that would involve refactoring the content of strings, which is a no-no. 

To my knowledge, there's no practical way of running CLR code (in the database) on a different machine than the SQL Server itself. Given your requirements, I would recommend an alternative similar to the Windows service solution. Polling a table regularly may not be very good from a performance perspective. Rather, could you use CLR code in your trigger to just append some output data to a file, and let a Windows service on a different machine intermittently poll that text file? That way, the Windows service could reside on another server, run asynchronously, and use a network share. As a bonus, the Windows service won't place locks on the table that has the trigger. 

Your intermittent errors probably stem from the explicit conversions that you're performing, combined with the filters. For any given query, SQL Server will create an execution plan, which you can think of as a roadmap of operations in a given order. The execution plan for any query can often vary in a number of ways, and SQL Server tries to choose the plan that will be the most efficient for every particular scenario. In this choice, lots of factors will influence how the plan is generated. Once created, the plan is cached, so it can be re-used. If the (verbatim) query isn't re-used any time soon, the data that it depends on is changed significantly or the server is restarted, the cache is cleared and the query plan will be regenerated the next time you run the query. This may very well result in a completely different plan. Now, for your query, consider the following example table: 

This seems pretty rotten to me, but I only have a few years experience with TSQL. It gets better, too! It appears the developer who decided that this was a great idea, did all this so that the few hundred strings that are stored can have a translation based on a string returned from a that is schema-specific. Here's one of the views in the stack, but they are all equally bad: 

While profiling a database I came across a view that is referencing some non-deterministic functions that get accessed 1000-2500 times per minute for each connection in this application's pool. A simple from the view yields the following execution plan: 

I have a client agent application that stores a replica of user metadata for the entire organization in a or earlier database. It gets accessed fairly often based on user activity so low latency is appreciated by everyone. Normally this is all well and fine, but some organizations have to store multiple orders of magnitude more metadata, which would total around 10-300 million rows, instead of 1 million or less that a typical agent uses. The Jet DB runs on each client workstation, and must operate locally on aging laptops without consistent network access. Full user data syncs get pushed to it every 24 hours or the next time it can reach the servers upstream at corp, and metadata stored while offline gets replicated back to corp whenever convenient. I have a copy of the schema that works for the server side agent which should have not changed since it was first implemented back in the early 2000's, should have very similar data access patterns, and runs on SQL Server 2008 R2 and up. The server agent does a substantial amount of reads and writes in a RBAR fashion, so I am going to assume the same access patterns are used in both. If I profile the activity for a few hours on the server agent to generate some queries to replay activity to benchmark, how can I run it against a Jet database? Are there testing or development utilities that can create a new Jet database and populate it with a schema and data? I am wondering if there would be any benefit in pushing for the development team to upgrade the client database engine to one of those new SQL Server engines that get loaded up only when the application code needs it. Not sure what the best options are out there, or if there is either a useful benefit either in performance or memory consumption. The client agent code runs in an old version of Java, .Net, and C++, but I have not been able to determine which language the data access layer is written in.