I use data modeling tools because they allow me to document important information about the database other than what "fits" in a database. Meta data like privacy/security/sensitivity concerns, stewardship, governance, etc. That may go beyond what some need in documenting a database, but those things are important to the business and helping them manage their data. Formal tools also help me in managing data that is stored in more than one database/instance/server. This has never been more true than in our packaged application world. 

If you have already created these entities/tables, there is a sample macro that comes with ER/Studio that does just this. You'll want to run this on a test model first as you may want to customize it to get the exact results you want. The sample macro is named ADD BASE ATTRIBUTES TO PERSON ENTITY. You, of course, will want it to work on all or selected entities. You could customize the macro to add these attributes based on the table being selected. If you want this to happen for new entities, you can create an entity/table that contains these common attributes/columns, then use that as a template. I have created a macro to create tables with all the properties I want to be common. The advantage to doing this is that macros can be tied to a hotkeys, making the creation of the new table easy. If you don't have the resources to mess with a macro, you could do the workaround of creating a new set of Domains (reusable attributes) in their own folder. Then you can at least quickly grab all 4 or so and drag them on to the tables. Still manual, but only once per table. So the answer you are looking for is dependent upon what the status of your model is currently. 

return the result in . Change the @HoursWorked data type in your scalar value function to instead of . 

Run Query 1, and run query 2. DMV shows query 1 incur exclusive lock, but query 2 returns details with 'Original' without query 1 commit the transaction. It appears that READ_COMMITTED row versioning is in place. Adding on query 1 and query 2, and run query 1 or query 2 returns error - Snapshot isolation transaction failed accessing database 'TEST' because snapshot isolation is not allowed in this database. Use ALTER DATABASE to allow snapshot isolation. Third test, rollback previous transaction. Set READ_COMMITTED_SNAPSHOT OFF and ALLOW_SNAPSHOT_ISOLATION ON. 

After some tests, in order for the DMF to work, it appears that the service account needs to have at least READ access to the root volume of where the database files are located at, in addition to the SQL Server login security VIEW SERVER STATE permission. The READ permission does not have to be granted explicitly to the service account. It could be granted through other user/groups, 1) SQL Server service account 2) Everyone 3) Users 4) NT Authority\Authenticated Users The idea is SQL Server service account needs at least READ permission to the root volume. I have listed the test and details here. 

Return (since I have changed the server name) exec completed successfully. However, there was error when running the log shipping backup job, 

I did some reading, and opening the master key was only part of what I had to do. I had to completely configure the new server for SSIS. I found the following blog post helpful, $URL$ The following microsoft documentation was also a good second source of information, as a check against the blog posting. 

Well, it looks like when the first sql server install ran, it also configured the file server services, which were still trying to hold the disk and the network name. Odd thing is, the disks where all showing 0 dependencies. This probably could have been fixed if we opened a case with microsoft, maybe cleaned up some registry entries, etc, but the guys from out windows team wanted to just rebuild from bare metal, as it would be quicker and easier. I was on board with that, so that is the solution I am implementing 

I currently have a impdp job running for a fairly large schema (2TB) and ran into an error. The source database has one column that is larger then the destination database. This is causing ORA-02374 and ORA-12899 errors. This has caused the import job to come to a crawl on this one table. I am trying to get past the errors, and the best solutions I can come up with are to either fix the table definition, or tell the import job to skip the current table. I can not currently fix the table definition because the data pump job has the table locked. Is there a way to pause the job, make the column modification, and then resume the import? Alternatively is there a way to tell the job to skip the current table, and move on? This would allow to to come back once the job is finished, fix the table definition, and then re-import just the one table. ETA: This is one of the last few tables in the import, I would rather not kill the whole thing and start over. 

I have no issues with the same code on other SQL Server 2012 or SQL Server 2008 R2 instances. Does anyone know the reason behind and has fixed this problem? 

Version - SQL Server 2012 SP1 CU3. I have been using sys.dm_os_volume_stats for a while and it was working. Recently I have not been able to return any result from this DMF on one SQL Server 2012 instance. e.g. 

From this article, I know to that slash (/) is used instead of a hyphen (-) when a net start is used with startup option, 

The script show the log file was initially set at 1MB, change the size to 10MB, and shrink it back down to 1MB. Data file size remains at its initial size, 5MB. 

performs best to be unique, narrow, static and ever-increasing by itself. So in this case, the inclusion of DeletedDate actually result the clustered key becomes non-unique non-static (presuming the DeletedDate value could be changed). The in the non-clustered index is useful to cover the query without having to perform a key lookup to the table. However, as the INCLUDE columns only stored in the index leaf level, it does not help in searching for the values of the query predicate (in this case, NULL) A of DeletedDate allows a more effective search on the range of records that satisfy the predicate (NULL). A filtered non-clustered index could further narrow down the subset (only NULL records) and provide a better performance as well as storage for the non-clustered index. With your description showing that the query returns all columns from the table with a single predicate, You could create a single filtered non-clustered index key for DeletedDate with . Test it and examine the execution plan. 

There are few things to consider other than normalization. For instance, you have a column for AGE. Are you going to update that every year? How will you know when to do that? The same goes for years of experience. There are some columns that will probably have multiple values for each applicant: School, Course, etc. You may also want to check your optionality on those relationships. Right now an applicant must have a related exam, but an exam does not have to have an applicant associated with it. I'm guessing that's backwards to how things work in real life. You have similar issues with all the other relationships. It helps if you read out the relationships after you create them. 

I prefer to use data lineage features of data modelling tools such as ER/Studio, ERwin or PowerDesigner to track the transformations required for migration from one data store to another. I get all the benefits of RE and compares to the source and target, plus none of the tradeoffs mentioned above. It really makes a difference. 

It's difficult to give an answer without knowing what the columns are. If the table is properly designed (normalized properly), then having many columns isn't a bad thing. You can use views as suggested, or you can just select the columns that you need to see based on the reason you are doing a query. My guess, though, is that if we saw the columns, we'd find some cases of normalization issues since you already naturally want to see only part of them. That's just a guess, though, not a design rule or anything like that. 

I have noticed differences in executions between data pump imports lately, and I am trying to determine what is causing the differences. I am using impdp to move data between databases, and remap the schemas. It seems like I have seen two different things happen when I run the import in parallel, and I am not sure what is causing the differences. Sometimes I see the job running and it is importing a bunch of tables side by side, other times the import process is only importing a few tables at a time, but using parallel inserts for each table. Using Parallel inserts on just a few tables at a time seems to cut my overall run time substantially, but I can't figure out why it only runs like that sometimes and not others. 

I have a powershell script that I am trying to add in to a sql agent job, but the job step is failing with the following error 

I am hoping someone can shed some light on this. When running an import into an oracle database, I sometimes see different behavior based on the parallel option. Some times, I will see multiple data pump workers all running insert commands, with (parallel 1) query hint in them. Other times I have seen a single, or just a few data pump workers, running insert commands with (parallel X) [where x is more then 1] table hint in the queries. I have seen this when running imports that essentially identical. The imports are using different dump files, but where created from the same nightly job, just done on different days. I am using the following options SCHEMAS=XXXXXXXX parallel=32 cluster=y DIRECTORY= DUMPFILE=XXXXXXX_%U.dpdmp CONTENT=DATA_ONLY TABLE_EXISTS_ACTION=APPEND DATA_OPTIONS=SKIP_CONSTRAINT_ERRORS LOGFILE=XXXXXXXXXX.log