You have different delimiters but I'm sure you have a small set of them. In your sample data the two delimiters are "space" and "comma-space". You can amend one of the linked solutions to work with multiple matching conditions. This will deliver unpivoted values. The data will then look something like 

Indexes help to find data, sure, so you may choose not to index small tables. They also have a role in data quality. For example, to ensure a value occurs in at most one row in a table, a unique index could be used. For these roles the index must be defined no matter the number of rows in the table. 

It sounds like your architecture has gone past partitioning and is sharded. If you have system software or an application framework in place which can handle sharding that is where this requirement should be implemented. In order to get the correct sort sequence you will have to have the results from all involved shards in the same place at the same time. There is no way to get some results from one shard and then get more results from another shard and be able to guarantee any sort sequence the user may choose. One way (as BriteSponge suggested) would be to define a view which spans all the shards. Let the DBMS deal with the complexities of marshalling the various shards' results sets. This would be my preferred solution if it can be managed. It separates the application from the DB implementation details. If you have to write this yourself there is some hope in the fact you have paging in place. This limits the maximum number or rows you will show in the UI and, therefore, the maximum number of rows that have to be returned from each shard. From the date range you can determine which shards are required. To each shard you submit a query of the form 

for every table in the database. If I ever had to do it again I'd install a new instance from scratch with the proper collation, then migrate the tables, then the data, then the constraints. Hopefully you have good source control in place. 

My experience was with IBM IMS. The root held file offset pointers to locate its children, which in turn held pointers to their children, and so forth. The nearest equivalent in the relational world is Row Identifiers (RIDs) which are sometimes used to link the leaves of BTree indexes to the table row. The analogy with Windows file system is not a bad one, in terms of the nesting and how access to lower items is through higher ones. The idea of a table doesn't really work, however. It may be better to think of it like a JSON document in a NoSQL store that has been shreaded by the DBMS and then internally linked through file pointers. 

You never ever need the id column in any table. It is a surrogate key which replaces the natural key for pragmatic reasons. Often this is because a narrow, monotonic key gives better performance, or because join clauses become cumbersome with multiple natural key columns. Some shops have an id on every table as a standard. Consistentcy is good, so that would suggest having the id in this table, too. If this table is the parent of a 1:m relationship it can help performance to have the parent keys directly in the child table without having to follow surrogate keys. It does take room on disk. If you have a gazillion rows that may be an important performance consideration. This would suggest omitting the id. Broadly, in my experience it is best to omit the id from intersection tables, and add it later if the model changes. 

If your attributes are more-or-less fixed, and you don't mind doing a code release when the attribute list changes, then unrolling them with JOINs and a COUNT would do it, too. 

Looking at Relationship 1 as an example. The primary key of this table is ID_PhoneNumber and ID_Company. The CompanyName, however depends only on ID_Company alone. The table is not in second normal form. For the design to be normalised it would have to be: 

The query optimiser should be considered, too. Its role is to take your declarative SQL and translate it into procedural steps. To find the most efficient combination of procedural steps it will examine combinations of index usage, sorts, caching intermediate results sets and all sorts of other things, too. The number of permutations can get exceedingly large even with what look like quite simple queries. Much of the calculation done to find the best plan is driven by the distribution of data within the tables. These distributions are sampled and stored as statistics objects. If these are wrong, they lead the optimiser to make poor choices. Poor choices early in the plan lead to even poorer choices later on in a snowball effect. It's not unknown for a medium sized query returning modest amounts of data to take minutes to run. Correct indexing and good statistics then reduces this to milliseconds. 

Say you're running at the secondary and need to perform a restore. How do you get the backup files from the primary which is (most likely) down/ stolen/ on fire? Much better to have backups separated from the thing backed up. The flip side is that to restore a DB that independent storage must be available. Is that dependency in the corporate DR plan? Has anyone told the server and storage teams? If finances allow having a backup on clustered storage and a copy elsewhere is great. 

Exposition The rules you give are all binary rules. They relate one entity type to another. If you have a rule which mentions three entity type then a ternary table would be appropriate, but you have not. For example the intersection entity type "TargetMarket" would be ternary - Red Bull (company) targets energy drink (product) to software (sector). I'm inferring from the many-to-many between company and product that the products are generic. For example "chocolate" and "spreadsheet". They cannot be "Toblerone" and "MS Excel" as they are trademarked and can be produced by only one company (ignoring licencing agreements). If all companies stoped producing chocolate (God forbid!) I imagine you would still like to record that "chocolate" was in the sector "food". With a ternary table this would not be possible. If all companies ceased producing chocolate (i.e. deleted corresponding rows from the ternary table) the chocolate <-> food association would disappear. Similarly a newly-formed company could not be recorded in this system until it was producing products. Should it ever cease producing products (e.g. become a shell company, go into administration) it would have to be removed from the ternary table and hence from the system entirely. As I mentioned in the parent question, the full answer depends on the meaning of the relationships between the entity types, as embedded in their names, and both binary and ternary tables may be required. Say a company produces 4 products, each of which is categorised in 3 sectors. That would produce 12 possible sector-product-company combinations. If your system requires to capture that only, say, 9 of these possibilities are valid or exist in fact, or some other well-named constraint then the ternary table would be the right way to do this. But this is a rule which is not mentioned in your set above. I would suggest you examine the constraints between a company's sector and those of the products it produces. There may be redundancies there which should be removed for the model. 

To be effective there must be an index on the reversed text. This is an overhead at write-time and will use additional storage. Waiting until read-time would require a scan of the data, which rather defeats the purpose. See examples here amongst others. 

The GROUP BY clause allows for grouping sets, rollup and cubes. See here. Of these it sounds like CUBE would be the most useful to you. You could run the CUBE query during quiet hours and write the results to a table with NULLs for the skipped columns. Indexed liberally with filtered indexes read performance would likely still be adequate. 

creates a new execution context which ceases to exist when the executed statements return. The USE only has effect within the . Try this 

For the Users, Groups and userGroupLink tables you have the relationships correct. I would suggest you change to just to make it a little bit clearer, but that's a minor point of preference. Since userGroupLink has no attributes of its own nor is it the parent in any relationships you could remove the ID column from this table without any loss of meaning or utility. Indeed you will get a small performance boost by storing more rows per page. To find all the groups of which a user is a member you would join the tables together: 

I'm sure you can add in the appropriate actions for when the monitored job has failed. You may also like to add in a timeout so this SP doesn't run indefinitely. This may be one of the exceedingly rare occasions where it is appropriate to use the prefix. I'll leave that to your discretion. Now the T-SQL to run your existing jobs becomes 

Using the name "has" for every relationsip is a problem. For example, the link between Company and Address - is that the mailing address, delivery address, registered headquarters etc. By using such a weak word you obscure these differences which may cause ambiguity and data inconsistencies. 

Are you familiar with the syntax? It's a useful trick for deconstructing situations like this because it creates a table on the fly with just the right data types for the given list. You can break up your calculation into its constituent steps, applying SQL Servers' precedence rules as you go, to see how the definition changes. Here's how your first example would look: 

DATEDIFF calculates how old each invoice is, according to your chosen reference date. The CASE decides which bucket it should be in, and returns zero for each row which is not in this bucket. This assigns each invoice's value into exactly one of the buckets. The output from the inner query is one row per row in the base table, with key values and one column per bucket. The SUM in the outer query tallies all rows according to the grouping conditions, but we've substituted zero for non-matching rows so effectively filtered them out. This assumes you want a column per bucket. If you need a row per bucket the same technique applies. Instead of returning order.balance from the CASE, return a bucket name. Wrap the whole query in and you will have the desired outcome. 

Using avoids having to jump through hoops to find a join condition for an clause. Next you need something to count. This is where the statement comes in. The case returns an integer value per pair of first names, which is what gets counted. (If I'm reading your question correctly you want where the LastNames match so that's the comparison I have. Hopefully it's obvious how to modify this if I'm wrong.) 

In logical modelling one tends to record the natural keys to entity types. These will often require the "parent" and hence weak entity types are documented. Consequently they can show the scope of re-use of particular values. Take the cannonical Customer-Order-Order Line-Product example. Although the Order will have Customer Number as a foreign key it is usual for Order Numbers to be globally unique so Order is a strong entity. Often each Order's line numbers will start again at 1. By making Order Line weak we document this possibility more transparently. In physical design we may choose to implement "weak" tables to reduce the number of joins in some queries. Continuing the example, if we make Customer Number part of the key to Order and also to Order Line then a query to count how many widgets the Jones Trading Company has ordered need only touch one table. Needless to say this involves costs in other areas and should be weighed carefully before implementation. 

Once you select questions insert them to this table. Each time you show a student's quiz check this table before creating a list. 

Response to comment: My reading of your question is that contains structured text in which links are embedded. Your application shreds from time to time to find and use these links. This is expensive and as a performance optimisation you would like to store the links separately in . This is where the update anomalies can creep in. If is updated so that has a different set of links, but is not also updated then there is inconsistent data in the database. It works the other way, too - a row can be deleted from without changing and, again, the data is inconsistent. This is why the application has to be diligently coded and tested. There is another rare-but-possible case to consider, too. Changing the data will require two statements - and . RDBM systems use pre-emptive multitasking. This means that at any time the RDBMS may choose to halt one workload and run another instead. So it could, for example, halt the updating stream after the first update statement and before the second. If there is another workload which is trying to read or at that point it may run, and get inconsistent values, if the reading transaction's isolation level allows it to do so. As I said, rare but possible. If these problems are acceptably unlikely in your application and the implications of it happening are not severe enough to worry about then denormalisation can serve performance advantages. If the risk of data inconsistency is too great then you'll have to find other solutions.