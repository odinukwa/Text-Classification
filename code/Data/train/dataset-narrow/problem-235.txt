There are quite a few different autonegotiation standards. The earlier standards (pre 802.3ab in 1999) were less than reliable, so you would have some cases where negotiation was done incorrectly between a host port and a switch port when both were doing negotiation. The negotiation standards work by sending pulses, called FLP pulses, across the link. These pulses are 16 bits in most (all?) standards. The important bits are 0 through 12, 13, and 16. 0 through 12 defines the standards used and capabilities of the link. 13 defines a link failure and 16 defines an acknowledgement. Most NICs won't send or pay attention to an FLP if autonegotiation is disabled in the NIC software. It is possible the port will come up in half duplex, depending on the switch (many early-generation 100Mbits switches were this way). On Gigabit capable ports, the port should default to full duplex, but this depends on how the manufacturer implemented the standard. However, you didn't state whether or not your host was connected to a port capable of 100 Mbps operation. Not all Gigabit ports are (e.g., in a 1Gb/10Gb switch). If the port is connected to a switch which does not advertise the correct modes supported by your card, the port will not connect. Here's an excellent reprint from PowerSolutions that describes the process in detail -- and this is a great presentation on autonegotiation from the IOL at University of New Hampshire which is an exceptional treatment of the subject. 

Then you would run a routing protocol on each of these interfaces, almost exactly like an individual router would have multiple distinct interfaces to different routers. The provider can provide you with a list of acceptable VLAN tags or can allow you to run any tags, depending on how their service is provisioned. If instead what you want is to bridge large broadcast layer 2 domains between multiple sites and have servers/hosts/clients at multiple locations be on the same VLANs so they can communicate to each other "locally", there are many additional concerns. This can create real problems without a thorough understanding of your environment, since providers doing QinQ often fail to pass some important protocols that do not operate well in a QinQ environment. 

PAgP silent permits the channel to be formed, but PAgP packets can still be interpreted by the device that is set to silent. Imagine the case with some devices that do support PAgP but may not enable it on a "secondary" LAG, but you still want this LAG to be online in the event of a failure so you can switch over to it. You could force the channel on, which would have the same function in terms of getting the channel up but may have other side effects. In networks with spanning tree configured, PAgP silent will also prevent spanning tree from reporting the port to be down. Since LACP/PAGP frames are normally received only over one link in the bundle, if they are not sent or received on a port configured in a bundle and set to "on" then the port will be marked as down. See these two links. It is important to understand that there are a lot of other features such as loop guard and UDLD that may cause your ports to be suspended. 

Yes, the VPN module is a linecard that's plugged into the router and can be replaced. A 'DEAD' hardware state simply means the router has lost communication with the ISM. This does not actually mean the hardware is dead, as a variety of issues in the field have caused this for me: Cards that somehow get unseated, incompatibilities with IOS and the card's firmware, etc. The first issue usually has different evidence, though. You can try to reload the router, but the best bet would be to engage Cisco's TAC to identify whether it is a backplane problem on the router or a problem with the module itself. The fact that the command still returns results about the card is an indicator that the card itself may be OK and has just crashed, but the HW state 'DEAD' doesn't mean the hardware is gone forever (though it could indicate a hardware problem such as memory corruption, electrical failure, etc). Cisco bug CSCtz51773 may be relevant here, find a release that has a fix for this bug and try it out to see if it works for you. 

The normal place you'd find the release notes for is under the platform itself rather than the code release, though it depends and isn't consistent. Some release notes are organized by software rather than platform. ED means early deployment. The release you're mentioning is probably a bugfix release for Cisco 3700 switches. That information is here: $URL$ Typically, you'll look at the main release for the caveats, then follow the train down the bugfix releases. Usually, a bug scrub is also helpful, e.g. searching the features you're deploying using the Cisco bugsearch tools to determine whether any of them are things you might be concerned about. Your Cisco or VAR SEs can also assist in choosing the right release for your needs. 

You didn't specify a manufacturer, I will assume Cisco switches though most other vendors should behave similarly. If the channel group's mode is active, the interfaces will not forward traffic, since the switch will actively be trying to form a channel and if the channel negotiation fails, the port channel will be "down". If the channel group's mode is passive or analog to passive, the interfaces will forward traffic normally and will listen for LACP/PaGP negotiations. This will bring the interfaces up and listen for traffic on the interfaces. If the switch sees these LACP packets from the host and a negotiation commences, the channel will be negotiated and packets will be forwarded over the port channel interface and not the individual interfaces. From the server's perspective, at the IP layer, if the destination host is in the subnet defined by the network configuration on the interface, the server will attempt to ARP this address. If somehow both interfaces were connected to the same subnet (most OS will show a warning or disallow this behavior), they may ARP out both interfaces. So once the ARP is received on a given interface, the server will know to send the packets out this interface, and the switch will also identify which IP and MAC are tied to each individual interface (but the forwarding behavior will be controlled by the below). If you are purely talking about Ethernet packets, and not IP, the server will forward the frames using whatever interface is specified. If I am not mistaken, in Linux the interface MUST be specified; in Windows it will probably use the interface with the highest (top) priority in the network interface bindings. This behavior varies OS to OS. From the switch's perspective, the switch will flood frames with a MAC address out all interfaces until it learns which port a given MAC address is on. It will learn this port by listening for a frame with the source MAC address. So if 0111.2222.3333 is sending a frame to 0111.2222.3334, the switch will flood the frame out all ports in that VLAN 0111.2222.3333 (Fa0/1) -> 0111.2222.3334 will flood to all ports Until it sees a reply 0111.2222.3334 (Fa0/2) -> 0111.2222.3333 (Fa0/1) Then it will commence forwarding all traffic to the specific port that issued these frames. There are a number of edge cases here that might bring more confusion such as the potential spanning tree interaction, but this covers the basics. 

You didn't really provide enough information to answer this directly (routing protocols, traffic patterns, vendor choices, etc), but this is the way I would evaluate the problem. If you are working with a routing protocol that may reach "all" the peers on a single VLAN, you may get unusual behavior you didn't expect depending on how your IP addressing is handled. For instance, in a network with default settings, the "highest" IP will be elected in the case of OSPF. This means that, for instance, if you number your networks with .1 being the "core" and .254 being the most distant edge, .254 will be the designated router. So you'll have to carefully consider the choice of routing protocol and the configuration of that protocol. In a case where you may want to split the routing into two areas in the case of OSPF, you'll probably want multiple interfaces. In the case where the latency is highly variable between sites (e.g. a QinQ service provided through a single "billing" carrier but potentially multiple delivery carriers, fairly common in large metro markets), there may be unusual effects on routing protocol timers as well. This may cause problems in protocols such as EIGRP, which relies heavily on hello timers and acknowledgments of updates. This is going to lead to poor scaling, in the sense that you'll be sending a lot of hellos that every router will have to process. Further, without specific configuration (EIGRP stub routing) each site will also end up acknowledging every other site's routing information, and more importantly, when a site is down HQ will ask every other site for that subnet. In a situation where you have a flapping site, this will also cause issues. Some vendors may have varying support for features designed to mitigate these design issues, so the vendor choice is a factor. BGP would solve these problems but it is a higher learning burden for most network engineers not experienced with it. There are ways to solve these problems with OSPF and EIGRP as well, but it requires a sensitive awareness of conditions and requirements. For 20 sites, I don't think this would be a significant amount of processing or concern. In the case where sites need to communicate to each other frequently it may make more sense. But if the sites mostly need to communicate with the core and very rarely to other sites, I would consider single VLANs used as "transit VLANs" from the core to each site. This means you only have to provision the VLANs once on each location, and once at the core. Doing is quite a big provisioning exercise and will not scale well and will be enormously complex to operate as the number of sites grows without a good network operations team that understands templating and programming. For a hybrid design with most traffic going through the core and an occasional "side VLAN" for direct communication between two sites, there may be some value to you. But you'll have to carefully consider the routing interactions that might result. It is best to consult with your provider to understand the details behind how the network is built and provisioned on their end (do they own all the resources? Are there "central POPs" that might remain up where other pieces of the network can be down? etc) in order to better understand your choices. It's a common trope in the industry that routing designs scale better, but this is mostly a response to very flat very large switched data center networks which have a wide failure and broadcast domain. In the design options you have proposed, I would want to understand the requirements better before making a specific recommendation, but I think that I would lean to individual interfaces from the corporate site to the remote site as "transit" VLANs with each site maintaining its own VLAN database and treating each interface at the core independently (hub and spoke). An example interface configuration for this option on Cisco gear would be: R1 (Core)