Use Algorithm 1 from this paper. The rest of paper is more about the analysis of algorithm and theoretical background which is very nice but not necessary for your purpose. Implementation of the algorithm in Python or R is pretty straightforward. Hope it helps :) 

The question is a bit fuzzy so if i did not get the point please comment me. You have a binary classification problem and you want to solve it using NB. Well, then you go through Bayes formula: $P(class|data)=P(data|class)P(class)$ This is the representation of each class GIVEN a sample point i.e. this representation varies according to feature values. The simple percentage of each class does not need NB. If the probability of predictions for one class on validation set is low it means that they were not from that class! For more detailed answer please say how did you set up your train/test split and what were these small numbers? If some numbers are strange then you may need to attach your code as well. Update 

After these points let's have a look at your question. What are time-series? If time-series are pretty non-stationary or simply speaking is the dynamic behind variation is complicated enough, then there is not a one-to-one map for time-series characteristics and time points (imagine a simple ECG signal. It's pretty simple but if you explore research community you will find super sophisticated methods for feature extraction on ECGs. I'm pretty confident finding a time point at which ECGs differ is almost impossible.). You may extract features from your time series or embed it into some n-dimensional manifolds and look at it. In best case you may find some time-related features which describe your time-series and you may find some time-related criteria at which time-series differ (however I'd say it's not likely to find them). Assuming time-series are pretty well-behaved (!!) with a simple dynamic (should be super simple). Then a solution might be to define a distance function of time-series which outputs the pair-wise distances of all time-series as a single score. Then the maximum of this function returns the time-point at which these time-series are pretty distinguishable. If you provide more information on your data I might be able to give a more detailed precise answer. Good Luck! 

You can simply convert a MxN matrix (your image) into a MN dimensional vector. For instance if you have 10 images each of which 10x10 pixels, you can convert them to 10 vectors each of them containing 100 element (you can do it row-wise or column-wise, does not matter). Then putting all these vectors on each other constructs a nxd matrix where n is the number of images and d is the number of pixels in each image. So far we just prepared data for analysis. Now let's go to the question: 

Do you really mean textual attribute or just Categorical attribute? e.g. if an attribute has three values $a$,$b$ and $c$ it does not mean that you need to work with text but just categories. Here I assume you have really textual attribute e.g. 

First of all please ignore my previous answer. I did not understand you properly. The difficulty of the your problem goes back to the distribution of normal points in train and test. The distribution of new normal points are so different than what your model can learn from only green points (training set) so I assume any model will have a relatively poor performance on it. Anyways, having a look at scikit-learn documentation is good if you did not. About Clustering as you asked I have an idea. An active are in Graph Analysis and Network Science is Community Detection (which is simply the clustering of graph nodes). The idea is that, assume all your points in this figure are nodes of a graph and based on their Euclidean distance in PC space, you put an edge between them (either binary or weighted). Then you can use Modularity-based algorithms to find dense subgraphs. I assume if you apply it to your data it will work. Also a similar idea is Spectral Clustering which internally uses the idea of graph partitioning. According to the density of clusters, I would also propose to use density-based clustering approaches. They might be able to distinguish between those overlapping clusters "a little bit" (e.g. this paper). This paper has an approach to find the peaks of densities. Sounds compatible with your data. Please note that those overlapping clusters are anyways difficult to distinguish. My apologies for the first misunderstanding and hope it helps. Good Luck! 

Depends on how you define "hubs". In Network Science, a hub is simply a node with high degree i.e. those node who contribute to the power-law nature of the degree distribution the most. But you can also find other definitions for instance according to the information flow in the network where hubs are defined as those node that are critical in the process information flow (also called central nodes). My Suggestions 

Regarding clustering algorithms you can find enough if you have a quick look at SKlearn. Hope it helped. Good Luck :) 

Let me start with an important point; what other criteria than classification success can U use to identify my cluster separation performance?: 

Classification as an indicator for clustering performance has an internal paradox. If you have the classification the clustering question does not apply anymore. These two concept are coming form two totally different philosophies so I would say be careful if you have already understood the concepts (what you say may make sense in semi-supervised learning which is not in your tags so I assume there is a misunderstanding). Clustering has no performance evaluation! this is the problem I see most of data scientists struggling with. In practice you may define a good criterion (but honestly, what is good?!!) and deliver your solution, but in research schema there is no evaluation for clustering as the question itself is not well-defined i.e. you never have label to be sure who is who so you need to define closeness of points from which the problem starts; how close is called closeness?!! Be careful about Curse of Dimensionality while using k-means for time-series clustering (I'm not sure how you do it). 

Pretty interesting question! First of all have a look at my edit as your question was a bit unclear according to the standard terminology. you have a set of Time-Series and you want to detect the outliers (abnormalities). 

K-means Your data has $7$ dimensions so k-means is worth to try. See the PCA of your data and check if any cluster is visible there as K-means will have a tough time if clusters are not Gaussian. the setup is: 

Using Community detection you can build a recommendation system. The most commonly used algorithm in this field is Blondel Algorithm which u have probably seen in SNAP. Blondel is almost the fastest Community Detection algorithm among widely accepted ones and its result is pretty acceptable (at least according to modularity score). As a side comment, you may also want to have a look at NetworkX and igraph libraries for more graph algorithms and visualizations. Hope it helps, good luck. 

Very interesting question! First Approach: PCA + K-means Your data will be explained very well on the second principle component. If you apply PCA on your data the first PC captures the data along the lines in which you completely lose the differentiation but the second PC is prependicular to the first one so your data will be projected in a way that points correspondig to each line are placed closer to each other. As you know the number of lines (number of clusters) a priori then you simply apply k-means and that's it! See image in the link to have an idea how the second pc vector would be. Second approach: GMMs Gaussian Mixture Models are fitted to clusters in a data using maximum likelihood estimation (you use Expectation Maximization algorithm for that). Your clusters along second PC are pretty gaussian so you will get a good soft-clustering if you fit a mixture of $n$ (number of lines again) Gaussian kernels to them. Variant: $a$s Are Not Equal In this case your lines cross each other as slopes are different. Your image does not show that but I include it here anyways. In this case you fit a linear regression to each line and keep the coefficients of the line. The you have a $2D$ data in which each line is described by just a slope and an intercept. Then the prependicular distance between each point and all lines tells you which line is closer so that is the cluster. (The distance can also simply be the residual of that point from regression line. You just need a distance metric to determine the closest line) If you need implementation as well, please drop a comment here so I can update answer with Python code. Good luck :) 

I didn't get what you meant by "other LDA Methods". To the best of my knowledge, Fisher method is just one. Sklearn has the implementation and you can find it here. Is it good or bad? When it comes to dimensionality reduction you don't know as dimensionality reduction methods are usually unsupervised. So if PCA works better? No one knows. At least in the context of two-class classification, Fisher method is still brilliant. Hope it helped! 

According to graph theory, wouldn't this make Person B also very influential? No. You need to think about how to set up your graph first and then go further. In this case your graph is not simple but both weighted and directed thus handling a request of B by A neither reduces A's influence nor improves B's. Because the weights of directed edges are determining the influence. Is there a better way than just counting number of interactions? Community Detection or what computer scientists call Graph Clustering. If you do not have to limit yourself to hierarchies, you can extract communities who have the most interaction and share suggestions inside the community. 

The classifier could be a simple neural network (a MLP should work here) or a simple SVM. The answer above can not be evaluated without more information about your data and a more specific description of the question (e.g. What are your images exactly? if you can post one of them here I can probably help more) 

Your question is not clear in a way there are two different Graph Clustering problems. One is having a dataset of different graphs and you would like to cluster similar graphs (in this case each object is a graph), and the other when you have a graph (e.g. a social network) and you would like to group similar nodes inside that graph (here each object is a node). The first problem needs Graph Embedding which is to transfer graphs into a n-dimensional manifold and from there on, you have a classical clustering problem. For this case have a look at what Horst Bunke has done. A more simplified approach would be feature extraction from networks and apply classical clustering methods. These features are statistical and topological network measures such as Clustering Coefficient, Assortativity Index, Average Shortest Path Length, Density, Diameter, Average Centrality, etc. This approach is more about the characteristics of real-world networks e.g. biological networks or social networks as it takes into account those characteristics which determine the real-world phenomena. The second problem is usually called Community Detection in literature. There are several method for doing that based on Modularity Score, Information Theory, Topological Structure or Spectral Graph Theory.