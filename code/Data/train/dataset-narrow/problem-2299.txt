where $Init_{\mathcal{X}}$ denotes the set of initial states; $Next_{\mathcal{X}}$ specifies the next-step transition. $Init_{\mathcal{PC}}$ specifies the initial values of the variables in $var_{\mathcal{PC}}$ involved in $\mathcal{P}$ and $\mathcal{C}$. The primed variable ($pc'$ here) is used to represent the modified version of its unprimed counterpart ($pc$ here). The two-phase handshake protocol can be described as follows, where $p$ and $c$ are initially equal. 

In the paper "THE COMPLEXITY OF SATISFIABILITY PROBLEMS" by Thomas J. Schaefer, the author has mentioned that 

The short paper "Computer Science and State Machines" by Leslie Lamport seems quite strange to me. On the one hand, I am surprised to see that an important hardware protocol called "two-phase handshake" can be derived from a trivial program, simply by mathematical substitution. On the other hand, I think that this example is (and should be) chosen deliberately. What I doubt about is its generality. If this method (i.e., describing state machines mathematically and deriving a protocol from its specification by mathematical substitution) is so fresh that researchers have not developed a general theory, I would like to see more examples. My question is straightforward: 

$\hat{w}$ and $w$ are two consecutive write operations in process $i$ and $\hat{w}$ is the former one. According to the rule of in Fig.3, we have $ts(\hat{w}) \le ts(w)$. (Actually, we have $ts(\hat{w}) < ts(w)$.) 

You may be aware of PRISM, a probabilistic model checker, and PRISM Case Studies which documents (besides others) case studies on the correctness and performance of various randomised distributed algorithms taken from the literature. It is quite common that one distributed computing problem has several variants (with different assumptions), and each variant has several randomized algorithms. That is, there are quite a few randomized distributed algorithms in the literature. One way of identifying "new" protocols to check is to pick up an article from the PRISM Case Studies, make a list of randomized algorithms for variants of the problem it is focused on, and choose one algorithm to work with. 

In the linearizability proof, the $m$ high-level writes will be put in an order, for example, according to their pids. However, if we consider the low-level operations in such order, the second writer cannot obtain $\langle 0, 1, 0, \ldots, 0 \rangle$. Instead, it should obtain $\langle 1, 1, 0, \ldots, 0 \rangle$, because it is put after the first writer's high-level write operation. 

As I understand, the notion of stability of approximation is relative to a specific problem $Q$ (e.g., TSP above), a specific approximation algorithm for some subproblem of $Q_0$ (corresponding to $L_0$ above), and a specific distance measure. Given that, how can we construct a spectrum of the hardness of of problem $Q$ according to their polynomial-time approximability? Specifically, how to show that "$\epsilon_r(n)$ is the best achievable relative error for the language $L_r$" using the concept of stability of approximation? Concrete examples are appreciated. 

Particularly, is it possible for a read of an actual register/memory unit in a multiprocessor computer to return an arbitrary value when it is concurrent with a write? What about going to a lower level such as circuits/flip-flop? In what sense, does a safe register exist? 

In the paper "On Interprocess Communication", the author Leslie Lamport have developed a formalism for interprocess communication via shared registers based on lower-level, non-atomic operations and have proposed the definitions of safe, regular, and atomic registers. Also Lamport's description for the "interprocess" publication. For safe registers, a read that overlaps a write operation is allowed to return any one of the possible values of the register. My problem is 

The derivation of the "two-phase handshake" protocol from a trivial program: The trivial program mentioned above is just to alternately perform the $\mathcal{P}$ and $\mathcal{C}$ operations: 

For classic Paxos, the proof (Page 8) is split into three cases: $k < j < i$, $j = k$, and $j < k$, where $k$ is largest round number in which some acceptor has reported to the coordinator by phase $1b$ message. I failed to understand the argument for the third case: 

This is not a complete answer, but it may be helpful. A piece of Mathematica code finds a counterexample of your greedy strategy: Consider the list of integers $\{51,54,55,70,98\}$. Your algorithm gives $\{98,51,70,54,55\}$ (Have I misunderstood your idea?) and the sum is 83. The program produces four permutations with the sum 125: $\{54, 98, 51, 70, 55\}, \{54, 70, 51, 98, 55\}, \{55, 98, 51, 70, 54\}, \{55, 70, 51, 98, 54\}$. 

Now, I want to study the complexity issue of this decision problem. Specifically, is this decision problem NP-hard? 

The scheduling problem (arising from distributed computing) is defined as a decision problem as follows: Instance: 

According to wiki, mathematical analysis includes the theories of differentiation, integration, measure, limits, infinite series, and analytic functions. It is OK to focus on real analysis (wiki) that deals with the real numbers and real-valued functions of a real variable. "Algorithmic" means studying something from the perspectives of computability theory and complexity theory. 

Recently I found an interesting algorithm book entitled 'Explaining Algorithms Using Metaphors' (Google books) by Michal Forišek and Monika Steinová. "Good" metaphors help people understand and even visualize the abstract concepts and ideas behind algorithms. For example, 

References: ([Bernstein et al@TODS'1983]) Multiversion Concurrency Control — Theory and Algorithms ([Cahill et al@SIGMOD'08]) Serializable Isolation for Snapshot Databases 

The arXiv paper "Non-Monotonic Snapshot Isolation" [1] proves several impossibility theorems demonstrating that SI (Snapshot Isolation) and GPR (Genuine Partial Replication) are incompatible. To this end, it first decomposes SI into four properties: 

Googling of "algorithmic mathematical analysis" leads me to "mathematical analysis of algorithms" or "applications of analysis to algorithms", which is not what I mean. 

However, if we know that $H$ is $\textrm{SI}$ in advance, we can decide efficiently whether it is 1-$\textrm{SR}$. The key here is that the version order $\ll$ has been determined by $\textrm{SI}$. See [Cahill et al@SIGMOD'2008]. Furthermore, I guess that, for an $\textrm{SI}$ history, the above two different definitions of serializability coincide. More precisely, 

To perform a read operation (), a reader reads from all the underlying $k$-atomic single-writer registers, obtaining $m$ vector timestamps. Two elimination rules are then applied to choose some value (along with its timestamp) to return. The case $k=3, m=2$ and my problem: Consider the following possible execution of this multi-writer construction when taking $k=3, m=2$. As shown in the figure, the dashed arrows denote which vector timestamps (value is omitted) are obtained when the writer performs operations on the underlying $k$-atomic single-writer registers. Two writers interleave in a "highly concurrent" way. Concurrent writes are put in the same row. In particular, the write of $\langle 4,3 \rangle$ by Writer 1 is concurrent with the writes of $\langle 1,4 \rangle, \langle 1,5 \rangle, \langle 1,6 \rangle$ by Writer 2. According to the multi-writer construction, the operation by Writer 1 first reads from the two underlying $k$-atomic single-writer registers, obtaining two vector timestamps $\langle 4,3 \rangle$ and $\langle 1,6 \rangle$. what_is_the_staleness_(3,2) $URL$