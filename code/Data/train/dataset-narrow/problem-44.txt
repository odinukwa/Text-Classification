Drawing back-to-front is not optional with blending (unless you use an order-independent blending technique), so getting the visual effect you actually want has to trump performance. But drawing front-to-back is purely a performance thing. Specifically, it makes it possible for early-Z to save you fragment processing time, as earlier objects are in front of later ones. So it's a GPU performance improvement. Batching is a performance improvement for the CPU, in that you're spending less CPU time doing state change stuff. So you need to decide which is more important: CPU performance or GPU performance. With deferred rendering, saving FS computation time also means saving memory bandwidth. But the general construction of your scene, what you're actually drawing, can help inform which is more important. Oftentimes what happens is that you render static terrain first, perhaps using your visibility system to order what you render coarsely front-to-back. Then you render your dynamic objects. Because dynamic objects usually require a lot of state changes, you won't necessarily draw all of them in a single batch (though there are techniques you can use to hide even texture changes). In any case, the point is that there is a tradeoff. And the one you pick depends on exactly what you're rendering. A top-down game like StarCraft or DotA would never bother with front-to-back sorting; you just render the terrain after the characters. A tight-quarters FPS by contrast would probably really need to employ such techniques, particularly for terrain. 

The space after clip space is normalized device coordinate space, which is obtained by dividing by . If W is zero... oops. Oh sure, you could put a conditional test to see if W is zero or very close to zero. Or you can just do the clipping math in clip space and never have to worry about it. No post-clipping vertex can have a W of zero, because the clip box for each vertex is based on being in the closed range (-W, W). And if W is 0, then the closed range is an empty set, and thus the vertex is not on that range. Also, negative values of W are outside of the clipping space. The closed range (-W, W) is inverted where W is negative, but the meaning of the range itself is not. Consider a W of -1; the range becomes (1, -1). There are no numbers that are simultaneously greater than 1 and less than -1. Therefore that space is empty. And yet, division by a negative W can still land NDC points in the [-1, 1] NDC-space range, where they will not be clipped if you didn't clip them beforehand. 

is for when you want to make writes from one rendering command visible to reads from a subsequent rendering command. That is, you can do all the atomics you like during a rendering command. But without the barrier, there's no guarantee that any of those atomic actions will be visible to later commands. Yes, they will still be atomic, but this only ensures that they will each execute in some order and each operation will fully execute before the next. What they won't ensure is that operations which OpenGL defines has happening "later" will execute before earlier operations. Sometimes, you actually don't care about the order of the operations; being atomic may be enough. And sometimes, you do care. Consider the case of a "linked-list" style blending buffer. Each pixel in the image is an integer that represents one of the items in the list. Each FS invocation will use an atomic increment to get a unique identifier, then swap that identifier with the current value in the image. It writes its pixel data to the entry in an SSBO, along with the identifier it swapped out of the image. When doing that atomic increment on the unique identifier, you don't care if FS invocations from two different rendering commands execute out of order. You simply need the operation to be atomic. Similarly, the swap doesn't have to be in order; it just needs to be atomic. As long as they all execute, and the atomic operations do their jobs, then they will be fine. Now, when it comes time to do blending based on this data, that is when you need to be able to read all of the data that was previously written. And that rendering operation requires . Of course, you're not necessarily using atomic operations to read these values. 

Note two changes. First, the internal format is , which specifies a 32-bit-per-channel signed integer format. The second is the use of for the pixel transfer format. Even though you're not actually performing a pixel transfer (since you're passing NULL), you still must use an appropriate format and type. And the formats must be used if you're transferring to/from a texture with an integer format. When you do this, you must of course use the and types in your shaders. 

You can map buffers for reading; invalidation is negatively useful for that. Also, you can map a buffer and only overwrite part of it. Invalidation is negatively useful for that too. has to be able to work outside of the narrow use case of mapping for overwriting an entire range. 

Emphasis added. So, if you have an array of , the array stride shall be the same as the base alignment of a . Also, each array will be padded at the end to the base alignment of a . Yes, a takes up the same room as a of the same size. Welcome to the wonderful world of layout. If you want to have a real array of floats in a UBO... well, it's best to avoid wanting that. But if you have absolutely no other choice, then I suggest making it an array of . Obviously, the size of this array will be the size you actually want, divided by 4 and rounded up: 

You might find this paper useful—its authors describe a way to make reasonably plausible caustic effects just by modulating their shadows with the surface normal and refractive index. It won’t get you the more complex/interesting hot-spot shapes that you’d get with photon mapping, but it could work well depending on your scene. 

As the documentation describes it, it looks like each group of three parameters is a vector in 3D space. The first three are the position of the “eye”, the next three are the position it’s looking at, and the last three are the “up vector”, i.e. the direction of an arrow pointing out of the top (…sort of—more detail below) of the eye. The positions aren’t related to the size of your display window—the view of your 3D geometry from the camera at that point will fill the display. In other words, if you have a box centered at (0, 0, 0) that’s 1×1×1 units in size, then your camera should be positioned a few units away—say (2, 3, 4)—and looking roughly at the center of the box, i.e. (0, 0, 0). If the box is 100×100×100 instead, then the eye position should be on the same order of magnitude in distance, e.g. (200, 300, 400). The behavior of the up vector is a little more complicated to explain, but leaving it at (0,1,0) is fine for most purposes—it’s mostly there to help you avoid gimbal lock. 

There’s a good writeup of the idea, with some visual examples, by Íñigo Quílez here—I’m not sure whether he invented the technique or was just one of the first to write about it: 

It doesn’t matter that it’s a point light; the 1/π normalization factor is what’s causing your surface to be 1/π as bright as you’d like it to be. You can either up your light intensity so the maximum brightness is π or just get rid of the normalization factor. There’s a good article with some notes about this and how it interacts with more complex lighting techniques here. 

…which is a 4×4 square, centered on (0,0), with the points specified in clockwise order (which won’t draw correctly with a triangle strip, as you’re seeing in the above screenshot). It looks like you’re expecting values outside of the [-1, 1] range to not be drawn at all, but all that’s doing is drawing triangles that are larger than the viewport. 

You can get fancy with individual weighting on the blur maps if you want to adjust the look, but an equally weighted mix (and yes, it should be additive) will work too. I’m not sure about whether you need to gamma-correct the blurred image before adding it to the source one, but you should definitely get it into linear space before doing the blur; this article has some good illustrations as to why. 

Different, yeah? Note the way the edge loops have changed from being roughly equidistant on the left box to a more complicated, pinched-and-stretched arrangement on the right. Now let’s turn off the wireframe and see how it’s getting lit. 

For 3D modeling, the usual reason to prefer quads is that subdivision surface algorithms work better with them—if your mesh is getting subdivided, triangles can cause problems in the curvature of the resulting surface. For an example, take a look at these two boxes: 

The easiest fix would be to do your light calculations in the fragment shader rather than per-vertex—you’ll avoid the discontinuity you’re currently seeing and your lighting will appear more natural (while still fitting the “low-poly” aesthetic). If that’s not an option, then you’re probably stuck with passing in each quad’s center along with the normals you’re already supplying. 

See the weird pinching in the highlight on the right box? That’s caused by the messy subdivision. This particular one is a pretty benign case, but you can get way more messed-up-looking results with more complex meshes with higher subdivision levels (like the ones you’d usually use in film). 

All of this still applies when making game assets, if you’re planning to subdivide them, but the key difference is that the subdivision happens ahead of time—while you’re still in quad-land—and then the final subdivided result gets turned into triangles because that’s what the graphics hardware speaks (because, as mentioned in the comments above, it makes the math easier). 

There’s also the assortment of techniques in the GPU Gems article PaulHK linked to. One thing I’d add is about the light-falloff-texture approach mentioned at the beginning of it, which Valve used for a couple of interesting effects in Source games in addition to fake SSS—see their documentation for some information about that. 

Short answer: yes. Longer answer: yes, because the vectors you’re using are meant to represent directions, not directions-and-distances. Think of it in terms of light: it doesn’t matter how far a photon’s traveled, whether it’s from the sun or from a lamp on your desk—once it arrives at a surface, it’s going to get reflected in exactly the same way. From a mathematical standpoint, you’re making lighting calculations based on the angles between the incoming light, the view point, and the surface normal. The dot product is defined as the length of the two input vectors multiplied by each other and by the cosine of the angle between them. To get just the angle—which you need for the lighting model—you need to factor out the length of the two vectors, i.e. normalize them. 

The documentation on the corresponding Unreal feature is here; their description of the technique is a little vaguer, but still helpful. 

The main advantage of this approach is its high visual quality—most other techniques don’t handle varying penumbras well, and it doesn’t suffer as much from the aliasing problems of shadow maps. The main disadvantage is needing a distance-field representation of your scene geometry to trace rays through; unless you built it that way in the first place (there’s a fascinating talk about how the team behind “Dreams” did exactly that), you’ll need a way to construct the SDFs out of your mesh geometry, which is an expensive and sometimes error-prone operation. UE4 handles the SDF generation by building them offline beforehand and then stamping them into a volume around the camera at runtime (more information on that here), but it’s not a cheap process so UE’s approach is only viable on high-end hardware for now. That said, if you do already have your scene geometry in SDF form, it’s quite easy to implement—see this Shadertoy for an example.