I'm wondering if anyone knows of a formalization (even limited) of any part of finite model theory in any of the major proof assistants. (I'm most familiar with Coq, but Isabelle, Agda, etc. would acceptable.) Especially of interest would be any of the results in descriptive complexity. 

That definition implies that there is no required unique bottom in a predomain, i.e. each down set (reverse chain) could have a distinct least element, but that there is no necessary least element for the whole structure. You could form a domain by appending a bottom under all other least elements, or by identifying all least elements (i.e. say they are all the same element) if that is semantically viable in your structure. 

If you are having trouble with the concept of least fixed point, I would recommend spending some time getting a background in more general order theory. Davey and Priestley, Introduction to Lattices and Order is a good intro. To see why the transitive closure is the least fixed point, imagine building up the closure from an empty set, applying the logical formula one step at a time. The least fixed point arrives when you can't add any new edges using the formula. The requirement that the formula be positive ensures that the process is monotonic, i.e. that it grows at each step. If you had a negative subformula, you could have the case where on some steps the set of edges would decrease, and this could lead to a non-terminating oscillation up and down, rather than a convergence to the LFP. 

I was going through Les Valiant's seminal paper and I had a tough time with Proposition 4.3 on page 10 of the paper. I cannot see why is it the case that if there is a generator with certain values for $valG$ with a basis $\{(a_1,b_1) \ldots (a_r,b_r)\}$, then there exists some generator with same $valG$ values for any basis $\{(xa_1,yb_1) \ldots (xa_r,yb_r)\}$ ($1^{st} kind$) or $\{(xb_1,ya_1) \ldots (xb_r,ya_r) \}$ ($2^{nd} kind$) for any $x,y \in F$. Valiant points out the reason in preceding paragraph - namely the $1^{st}$ kind of transformation can be achieved by appending to every input or output node an edge of weight $1$. The $2^{nd}$ kind of transformation, Valiant says, can be achieved by appending to input or output nodes chains of length $2$ weighted by $x$ and $y$ respectively. I have not been really able to understand these statements. Maybe they are already clear, but still I cannot really see why the above construct helps achieve any realizable $valG$ values with one basis with the new basis which is one of the above types. Please help illuminate them to me. On a different note, are there some tensor free surveys for hologaphic algorithms available online. Most of them use tensors which, sadly, scare me :-( Best -Akash 

Let me offer the simple, intuitive way that I think about this. If you restrict yourself to closed lambda expressions, you have an equivalent of the combinatory logic. In fact with just a few simple closed lambda expressions you can generate all the others. Closed lambda expressions give you the equivalent of implications where any conclusion/output you reach is either something you put in as an input, or something that you built by combining your inputs (in the general case, possibly recursively). This means that you can't pull a result "out of thin air" the way you can with non-constructive logics / mathematics. The only tricky bit left is how you handle negation / non-termination, which is a whole area by itself, but hopefully I've already given you the simple, but deep, correspondence between the three that you are asking for. 

I would recommend investigating the field of Finite Model Theory and more particularly its sub-field Descriptive Complexity. It can be used to model such sorts of problems. 

Though it many not be obviously directly related, one thing that comes to mind is the concept of "blame" by Wadler et al.. This give you a theoretical basis to think about mixing together different typing regimes into a coherent whole. In essence, blame allows you to mix together languages with weaker type guarantees with languages that have stronger type guarantees without losing all the benefits of the strong guarantees. The idea is that the parts of the system with weaker guarantees will get the "blame" if certain things go wrong, localizing runtime type errors. Hopefully you can see how that might be useful for FFI and bindings that apply to languages with varying type systems. Edit: See Sam TH's answer for a fuller intellectual history of the concept of "blame". 

How about the fact that computing permanent is #P-Complete but computing determinant - a way weirder operation happens to be in the class NC? This seems rather strange - it did not have to be that way (or maybe it did ;-) ) 

Recently, I started (independent) learning of the theory of metric embeddings from the Fall 2003 course offered at CMU . I had a very basic question from the very first lecture of this course which I would like to get more intuition about. On page, $5$, the notes say that this technique can be used in a straightforward way to give an $\alpha$ approximation algorithms for problems like TSP if (say) the following hold. (i) The metric embeds into a tree. (ii) The embedding has distortion at most $\alpha$ What I am not sure about is whether the solution generated by using the embedding is even valid - because for all I know, it could be that the TSP solution on the tree uses only from among those edges which were contracted. To be more precise, I feel more comfortable accepting that if we have got a mapping $f$ from the original space $(X,d)$ to the tree metric $(V, d')$ which expands all the pairwise distances, then I can use the TSP solution on this tree as an approximate solution to the TSP problem on the original metric with approximation factor same as the expansion of the mapping $f$. I am not sure about how approximation factor can be the same as (or even related to) the distortion of the mapping $f$. Thanks -Akash 

I think this a misanalysis of the "co" prefix in this case. "Coroutine" is "co" in the sense of "co-worker"; something that works together with another. The term precedes by a long way the gross overuse for programming concepts of the prefix "co" in the Category Theoretic sense of a dual of another concept. (Yes, there is editorial content there. ;-) ) 

I don't know of any work that pursues this line, but a few moments thought about it led me to this hypothesis: wouldn't the "root" of the exponential type just be the codomain, and the "logarithm" of the exponential just the domain? 

I find that the most "natural" way to get an intuition of complexity classes is to revert to Turing's starting point and try to solve the problem "manually". Start with a sorting task. From a jumble of, say, five words have the class order them alphabetically. This should be easy. Then double the number of words, and repeat the exercise. It will be obvious that, though the second problem is harder, it isn't that much harder. Next try a traveling salesman task. Start with a grid of say three cities with distances between them. The class will probably be able to solve this in short order. Now double the number of cities to six, and continue with the exercise until everyone's head is spinning. An experience like this is very likely to leave a lasting visceral impression that a purely technical introduction may not. 

This should really have been a comment, but for the lack of space I am posting this as an answer. Thanks for the answers and comments everyone. Recently, I came across another survey by Robin Thomas. You can find it here $URL$ Other than this, I would also add one statement about the tiling connection (which was pointed out to me by Prof Dana Randall). If you take the dual lattice, then 2x1 domino tiles are just edges. Therefore, a perfect tiling is precisely a perfect matching in the dual. Then, the theory of Pfaffians can be used to count perfect matchings in planar graphs. This means that you can just primarily focus on counting perfect matchings in the graph - the rest just follows trivially. 

Recently while working on a problem, I had to go through some of the literature on nested dissection. I happen to have one (maybe two?) questions related to the same. First, I will define a few relevant terms. These terms come up when we study the process of Gaussian elimination graph theoretically. Say, we have got a matrix $M$. With this matrix, we associate a directed graph $G_M = (V,E)$ where we have $V = \{v_1, v_2, \ldots, v_n\}$ where $v_i$ corresponds to row i and variable i and $(v_i,v_j) \in E$ iff $M_{ij} \neq 0$ and $i \neq j$. The elimination process may create some new non-zero elements in locations in $M$ which contained zeroes to begin with; the edges corresponding to these elements are called fill-in. In graph-theoretic terms, removing a vertex $v$ calls for addition of the following set $S_v$ of edges. $S_v = \{(u,w) | (u,v) \in E, (v,w) \in E, (u,w) \not\in E\}$ In order to make the elimination process efficient, we can target minimizing fill-in. By a result of Yannakakis, we know that fill-in minimization is a NP-Complete problem. It is easy to see that the value of fill-in depends on the ordering of vertices which leads to definition of a related parameter. An elimination ordering is a bijection $\mathbf{\alpha \colon \{1,2,\ldots, n\} \to V}$ and $\mathbf{G_{\alpha} = (V,E, \alpha)}$ is an ordered graph. Basically, this represents the order in which the vertices will be picked for deletion in the corresponding directed graph representation. Corresponding to different orderings, we get different values of fill-ins. The ordering which minimizes the fill-in size is called the minimum elimination ordering. And again we (of course) have that computing the minimum elimination ordering is NP-Complete. My question 

This started as a comment under Andrej Bauer's answer, but it got too big. I think an obvious definition of ambiguity from a Finite Model Theory point of view would be: $ambiguous(\phi) \implies \exists M_1,M_2 | M_1 \vDash \phi \wedge M_2 \vDash \phi \wedge M_1 \vDash \psi \wedge M_2 \nvDash \psi$ In words, there exist distinct models of your grammar encoded as a formula $\phi$ that can be distinguished by some formula $\psi$, perhaps a sub-formula of $\phi$. You can connect this to Andrej's response about proofs through Descriptive Complexity. The combination of the existence of an encoding of a particular model plus its acceptance by an appropriate TM as a model of a given formula IS a proof that the axioms and inferences (and hence an equivalent grammar) encoded in that formula are consistent. To make this fully compatible with Andrej's answer, you would have to say that the model is "generated" by the formula acting as a filter on the space of all possible finite models (or something like that), with the encoding and action of filtering on the input model as the "proof". The distinct proofs then witness the ambiguity. This may not be a popular sentiment, but I tend to think of finite model theory and proof theory as the same thing seen from different angles. ;-) 

You could have a look here EDIT I should have added that these are the lectures by Amitabha Bagchi at IIT Delhi 

I am not sure if the following question falls within the scope of this site; if it does not, I will request the moderators to take appropriate action I have been going through Jin-Yi Cai's expository paper on Holographic algorithms. I had a question. Immediately after introducing Grassman-Plucker Identities for Pfaffians and the definitions for matchgates, recognizers and generators, Cai introduces signature tensor for matchgates. My question arises from the following statement. He states that a generator $\Gamma$ with $m$ output nodes is assigned a contravariant tensor $\mathbf{G} \in V_0^m$ of type $\binom {m} {0}$. I looked up contravariant tensors on wikipedia but did not find any reference to the type of contravariant tensors which Cai talks about. Maybe I am not searching properly, but I was unable to find a proper reference which defines what I am looking for. I would be glad if anyone could help me with this. Thanks -Akash 

I was and still am surprised by Euclid's algorithm. To me, it is a testament to power of human thinking - that people could conceive of such an algorithm so early (around 300 BC if I trust my memory). Fast forwarding, there is mind numbing literature on the subject. I think Scott Aaronson's list should be helpful in this regard - though, as Aaronson himself says its not complete (and not entirely theoretical) 

I will go out on a limb here and say that, if you are willing to squint a bit, proofs and terminating programs can be identified. Any terminating program is a proof that you can take its input and produce its output. This is a very basic kind of proof of implication. Of course, to make this implication carry information more meaningful than stating the obvious, you need to be able to show that the program works for any and all instances of input drawn form some class with logical meaning. (And so too for the output.) From the other direction, any proof with finite inference steps is a symbolic program manipulating objects in some logical system. (If we don't worry too much about what the logical symbols and rules mean computationally.) Types and propositions can be made to work similarly. Any type T can be assigned the proposition $\exists x : x \vdash T$ with obvious truth conditions. Any proposition can be turned into the type of it's proofs. This is pretty simplistic, but I think it does suggest the robustness of the idea. (Even if some people are bound not to like it. ;-) )