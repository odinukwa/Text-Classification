If I want to use my own Ray Tracer in a program like 3DS Max to render geometry, how could I do this? Can someone explain to me what I need to do and where I can learn to use an SDK to make this happen? Is this worth it to render test scenes to compare and test my ray tracer? 

So when the ray hits the object from the inside do we use a BRDF to calculate the surface properties and then that ray leaves the glass block with the refraction origin and direction? 

1) I understand that if there is no total internal reflection, first we must find the refraction direction ( is the object's index of refraction): 

Where can I learn about particle and fluid simulation? Is there a good book available? I want to couple volume rendering with fluid/particle simulation to be able to finally render smoke and ocean with foam etc. Is there a chapter in PBRT on it? 

After reading a few articles online I am can confidently say I am clueless on how Anti-Aliasing works when Ray Tracing. All I understand is that A Single Pixel/Ray is split into 4 sub-pixels and 4 rays rather than 1. Could somebody please explain how this is done (preferably with code)? 

Can somebody explain how to implement volume rendering and scattering? I've been told that this is the easiest and most effective method to render things like smoke, milk and fog. Scratchapixel has a great article about it here but the lesson on implementation is unavailable. I understand what Ray Marching is because of this answer from a while back but similiar to my previous problem I have no idea how to implement it. To make it clear and narrow this question down so it is easier to answer here are my main problems: 

Smith's shadow masking function $G1$ gives the fraction of microfacets with normal $w_h$ that are visible from direction $w$. $G(w_o, w_i)$ $=$ $G_1(w_o)G_1(w_i)$ In the literature, the Smith masking function is often expressed as a fraction involving a function $\Lambda(w_o)$. This function is expressed as an integral over the slopes of the microsurface and its form is derived with a raytracing formulation of the masking probability. $G_1(w_o,w_m)$ $=$ $1\over1+\Lambda(w_o)$ For the Trowbridge-Reitz distribution it is quite simple: $\Lambda(w)$ $=$ $-1+ \sqrt{1+\alpha^2tan^2\theta}\over 2$ How is this $\Lambda(w)$ function derived? 

I commonly hear the terms 'Ray-Marching' & 'Volume Rendering' being used together when it comes to rendering fog or any other volumetric object. 

I am currently following a tutorial at Scratchapixel.com on refraction Here is the refract function: 

How are fluid simulations handled in Computer Graphics? For a novice, there are hardly any novice friendly tutorials explaining how particles or fluids can be simulated. Things like smoke, crown splashes and fog are they animated by artists as opposed to physically based simulations? If there are any worthwhile resources that you could recommend please do! :) 

What are some of the challenges of Offline Rendering? I know one problem that is faced is performance and rendering times, are there any more issues? Path Traced photos seem to have reached the point where it is hard to distinguish a CGI image in comparison to a real photo so being able to render a photo-realistic image doesn't really seem like a challenge anymore. Is offline rendering just about refining current techniques to find an improvement in render times now and not so much about photo realism? 

What are Spherical Harmonics & Light Probes? How useful are they in computer graphics? What exactly do they do? I've heard the word spherical harmonics & Light Probes everywhere, from siggraph presentations to blog posts. Recently Matt Pettineo posted a 6 part blog series on them but I still don't understand what they are. Is it another way to improve ambient lighting? 

In my ray tracer, I render my images out to a .PPM file and then view it in photoshop. To make things easier and faster I want my ray tracer to open up another window and show the image being rendered each ray at a time like how modern renderers like Mental Ray or V-Ray do it. How is this done? What is the most common method? I tried writing a viewer in C# that uses a to load the image which is then constantly refreshed but that doesn't work since the image cannot be shared. After asking on reddit most people suggested writing the image to an OpenGL quad as a texture but I don't want to do this as it complicates things. I've taken a look at at the function which will set the pixel's color at a specified coordinate and color, is this the way to do it? It is really great to watch your image be rendered, it is also helpful because if your image is rendering wrong you can stop rather than waiting for the entire image to rendered and then noticing there is a problem. (the image below is 3DS Max with Mental Ray which shows image being rendered each Ray at a time, I want to achieve this) 

Currently I'm 16 and in high school and I love graphics programming and I'm seriously considering it as a career path. I was wondering what a day as a graphics programmer looks like for offline Rendering (path tracing). Since were close to hitting photorealism now it seems like the only main challenge for the next decade is performance. I've been digging through the recent SIGGRAPH papers for the Future directions for rendering but haven't found much. So what kinda tasks can graphics programmers do these days? Working on performance can't be the only thing. Also, is a lot of the work research based as opposed to traditional programming? 

I have implemented a sky rendering model in my ray tracer which saves the color of the sky to an image. I was wondering how I can render the sky dome and texture map the image on to it. Do I render the sphere at the center of the environment and add the image to the inside of the sphere? How would I add a texture to the inside of a sphere as opposed to the outside in code? Do I just reverse the normals? 

Is Ray Marching the method used to render these volumes such as Fog, Smoke & Fire (this is regarding path tracing)? Since we can use signed distance fields to easily manipulate the shape of the volume? At the approximated intersection point is a BSDF evaluated (BRDF + BTDF) or is it just a BRDF? 

I have a few doubts and questions regarding volume rendering, I was hoping someone here could help me understand. 

In my CS class in high school I'm doing a presentation on Computer Graphics and I wanted to talk about Ray Tracing. I was wondering what would be the easiest way to do this without confusing my audience? What are some of your personal favorite analogies and explanations that have worked effectively to people who have never touched the field? Please don't mistake this as a 'do my homework' type question. I'm just curious to see what has worked and what hasn't. 

Is there a material table that lists all the specific values to obtain a certain type of material (e.g: copper or asphalt) available? I found this table in the paper: A Reflectance Model for Computer Graphics. However, it doesn't really seem to have all the materials I would like to use. 

Pick a random point s from the unit radius sphere that is tangent to the hitpoint, and send a ray from the hitpoint p to the random point s. That sphere has center (p+N) 

Recently I wanted to improve the lighting in my game engine, naturally I looked towards Ambient Occlusion. I was considering SSAO but are there any better techniques? Maybe some that are Physically Based? Are there any other techniques that replace SSAO or HBAO+ entirely? I am aware of VXAO but is that basically it? 

In appendix A of the paper Microfacet Models for Refraction through Rough Surfaces there is a derivation provided for $\Lambda(w)$ but the mathematics is very confusing. Could somebody help me by explaining it? 

I have begun learning how to create a Ray Tracer and 1 thing I am confused about is how the pixel color from a Ray Tracer is stored into an image. Do we use for a Bitmap? Do we use a third party library like libpng? Most tutorials don't really explain this well online, so if anyone can explain what the most common method for this is that would be great. 

Right now I am reading Peter Shirley's Ray Tracing in One Weekend book, I want to be able to use the provided source code but I am not sure what platform or IDE it was written on, because of this it does not compile in Visual Studio. Can somebody help me? Whenever I try compiling the code it asks me to 'attach to process'. How can I get this working? In a few reviews I have read online it says that I'll need to do some tweaking to get it working but I honestly don't know how. Would it mean rewriting the entire ray tracer as a Win32 Project? 

2) We then compute the refraction ray origin (which is the point where the ray will leave the object?) 

How is glint rendering done with ray tracing in the photo below? Is it just a simple normal map? I found a paper online that describes the exact same method used in the photo above but I have no idea what's going on, especially with all the jargon being thrown around. Here's the link: $URL$ 

How is measured BRDF data recorded? Like the MERL BRDF Database. What kind of equipment is used and what are the key values that need to be recorded? I know one place this data was used was in the Disney PBR paper. 

Recently a question that came to my mind was that How are Physically Based Rendering Techniques researched and developed? Do people just use current equations by physicists or do they create their own? Hopefully I've explained my question correctly. 

A number of analytic formulae have been proposed for BRDFs. Many Formulae have been derived from physical principles. A number of models such as the Torrance-Sparrow model have been derived from the distribution of microfacet orientations. One may postulate a certain microgeometry for the surface, and simulate the resulting BRDF by using a software raytracer. Once the scattering of light on a surface is understood how is a reflectance function derived? Can anyone recommend any resources on this topic? 

After reading about the new Disney Diffuse BRDF, I have been wondering if this is currently the best Diffuse BRDF at the moment? How much more of a difference would this make compared to using a Lambertian BRDF? I have heard of other BRDFs such as Hanrahan-Krueger and Oren-Nayar, are these also better? 

Recently I have developed and interest in Ray Tracing and like most computer graphics tutorials online there is nothing regarding implementation for a beginner like myself. So far I have only done real time rendering (DirectX), however I understand the basics of Ray Tracing but I want to buy a book before proceeding any further. I've taken a look at Physically Based Rendering: From Theory to Implementation (I want to learn Ray Tracing with a Physically Based Approach so I thought this would be great) but I don't think it's good for a beginner since there's not much emphasis on the code. When I was learning DirectX 11 everyone recommended Frank Luna's Introduction to 3D Game Programming, which was great since it covered all the basics and also a few advanced topics like SSAO with an approach a beginner could understand. Is there something like this for Ray Tracing? 

I have been following the scratchapixel ray tracing tutorials online which have been great at explaining the theory behind everything & how to implement it. However, I reached the point where I want to implement Area Lights but I am clueless on how to do it. The tutorial on the website hasn't been uploaded yet (all though there is a placeholder link with a bunch of others). There's not much online regarding implementation so this has lead me here. Could someone please explain how I can implement Area Lights with Ray Tracing as opposed to Point Lights? 

At the approximated intersection point for the volume what BRDF/BSDF do we use (if we do use one, I might have a wrong understanding of the topic)? Are there measured BRDFs & BTDFs that is the common standard? 

Derive an analytic formula using physical principles Use simulation given an assumed or measured model of the surface microgeometry Measure the BRDF based on empirical observation 

How can I make an object give the effect that it is giving out light when it isn't? I basically want to make an object glow, for example Neon Lights. Also Area Lights in my engine work properly but to increase how realistic it looks I wanted to give the effect of a haze or glow rather than a flat white rectangle/disk. This photo basically shows what I want to do perfectly. 

(Grand Moff Tarkin in Star Wars: Rogue One was entirely created through CGI since the actual actor had died) 

After reading a wikipedia article about Global Illumination It mentions a variety of techniques like Ray Tracing, Path Tracing and Ambient Occlusion. One technique it mentions is Radiosity which I don't know much about. After a quick google search I found this blog post which discusses what it is, a basic overview & implementation details. However I'm still a little confused. So from what I gather it is a way to precompute diffuse lighting on to a lightmap to give the illusion of light bounces? Could someone explain it to me over here without all the jargon? Is this technique used alongside techniques like Image Based Lighting & Ambient Occlusion or is it intended to be an entire Global Illumination technique by itself? 

So far I understand how a glass block (or any other transparent object which undergoes refraction) bends the light towards the normal when entering the object and then away from the normal when exiting but I am confused as to how refraction works or is computed when there is another object in the medium as shown in the image below. 

Can someone provide some pseudo code or code of a basic Volume Rendering and Ray Marching loop with Absorption & Scattering coefficients, Phase Function and a Density function? How is Ray Marching used alongside Path Tracing? Is this the best way to render sub surface scattering? 

What are the recommended ways to render caustics? I am aware bidirectional path tracing and photon mapping can do this. Could someone give a brief overview of photon mapping and a good resource of where to learn about implementation? 

Can somebody where the actual definition for the function is? Because in the first part of the the return is . Will it execute the function again except go through the ? Regarding what the function actually does here is what I have gathered from the book based on my current understanding: 

I am currently reading through Peter Shirley's Ray Tracing in One Weekend. In the beginning chapters where the author introduces diffuse surfaces we a presented with this function: 

How can I a find the Light Direction/Ray from a Point on a surface to an Area Light? Is there a universal technique to do this for all types of lights (circle, rectangle, sphere & tube)? Although this is for area lighting in a ray tracer, is there another way to implement area lighting through an emissive value that I can apply to any object or surface (I have seen this mentioned online)? As opposed to doing it like this. For example for a point light I know it can be done like this: