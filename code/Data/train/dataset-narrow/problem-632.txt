To determine whether it is an temp space or not, the is not suitable, since it is 6 for all of the tablespaces. However the container type seems to be different for temp spaces. is 0 for temporary tablespaces and 6 for all other tablespaces. When I use the parameter, I get another piece of information. 

AFAIK, you cannot force a plan to stay in cache. However, a query can be thrown out of the cache for several reasons. Read a blog about execution plans. It states some reasons why execution plans get invalidated: 

but in case of sp_spaceused (without any parameters) - can not insert output, since it produces 2 result sets. I know 2016 has nice parameter @oneresultset = 1, but is there any way you can insert 2 or more output result sets into #temp in SQL 2014 or lower ? Is there any tricks ? 

I have a production SQL server and a linked server (Azure SQL Database) I join two tables to do update 

I have a table on Azure SQL Database that I would like to have replicated/mirrored to our on-premise SQL Server So the on-prem SQL Server would have a copy of table from Azure that is always up-to-date, available for read-only queries Is there a technology for this ? The reason I need this is because I need to join this Azure table to some tables on-premise (on 300K + rows) in a query, and linked server is not working for me very well, despite all the tricks and workarounds I have tried Regards, 

I am trying to move data from a csv file into a SQL server database. Some of my values are in the scientific notation. I figured out on how to get most of them converted but for one value I get the Arithmetic overflow error. The value that is causing the error is . If I change the part before the E by removing the 1 so it reads the import works fine. All the other values I need to import work fine. I use a format file to import the data. Below the line for the column that is giving me grief: 

it doesn't require any transactionality or a different isolation level, because the insert-if-not-exists statement is already an atomic operation; it executes at most 2 SQL statements (insert-select) instead of 3 (select-insert-select), making the calling code simpler; since we're not using INSERT IGNORE, if the INSERT statement throws an exception it will be for a legit reason - e.g. auto_increment has reached the max value of column - and thus we don't have to worry about needlessly logging duplicate key exceptions or ignoring actual database problems. 

I am writing a script to restore databases from backup. The script should need as little input as possible to fulfill this task (in an efficient way). The restore command takes the parallelism parameter. The idea is to set this parameter to the number of tablespaces that are not temporary tablespaces. I found the db2ckbkp command which will not only verifies the backup file, but also outputs lots of (useful) information. Currently I was planning to run it with -t to get the tablespace information. I just have troubles to interpret the information printed. following the output that is printed for one of the tablespaces. 

Tried to exclude MyDB from availability group, set compatibility level to 130, and include back to AG But since database in NORECOVERY mode, it does not allow me to set compatibility level If I restore database with recovery, I can set compatibility level to 130, but can't return database in norecovery mode, and can't add back to AG What do you suggest on above ? I need to keep primary SQL server replica as 2012 for now, and I need to make async replica (SQL server 2016) to be available for read-only access 

The format in the database is decimal(18,9). Any suggestions on how to avoid this error without manually changing values in the source file? To put it into perspective. The CSV file contains more than 2.2 million rows with 154 columns each. Which results in a CSV file size of more than 2GB. Currently I am working with a test file. When the final go live comes. I need to switch over fast. Which means I can not analyze and edit the file for several days. Update I played around with the values a little bit. 

I need to get the resulting PK ID, regardless of whether the row was inserted or already exists. However I don't want to burn through auto-increment values, so I'm not going to use the kludge. I'd rather just execute a second SQL statement to SELECT the id value, because I'm only inserting a row every ~5 minutes into a table with ~100k rows, so query performance is not a major concern. The reason why I'm not using the VARCHAR column as the column is that this lookup table has a FK reference from another much bigger (500M rows, 20 cols) table. I'm using this lookup table pattern to store reference data - e.g. failure codes ("eligibility.failure.not_enough_foobar_data") or time zones ("America/Los_Angeles") - so that the much larger main table can use a short FK id to reference that value (e.g. or ), instead of repeating the long string. 

Another pitfall could be that the query changes. This can happen if the where clause changes (e.g. you filter by date) and you don't use bind Thinking about, you haven't stated that you checked whether the execution plan is still in cache or not. You should query the cache to figure that out. However, since you say your query runs only once a day, the plan might just be expired. The blog actually mentions the formula on how sql server determines when to expire a plan: 

replica-01 and 02 are in on-premise data center replica-03 is on Azure VM (03 is synchronizing, no problems here) When I try to use replica-03 as reporting read-only server, running "use MyDB" statement shows the following 

The update of 100 rows takes about 18 seconds! 1000 or more rows take much more time I made sure below is true: 

In SQL Server, is there an ability to limit allowed single query duration for certain logins (users) ? I would like to limit some logins so they won't be able to run queries more than 30 seconds. For a second set of logins, I would allow max query execution time of 60 seconds, and so on Regards, 

Rows are inserted, but never updated or deleted. The table is small (thousands of rows at most), so query performance is not a concern. I have 2 different processes (running identical code) which insert rows into this table. Each process has an in-memory write-through cache of the -> value. Whenever a new entry is put into the cache that doesn't already exist, it's synchronously inserted into the database table, then added to the cache. How do I handle the scenario where 2 (or more) processes try to insert the same value at the same time? Since the table has a unique key constraint on , my thought is to have both processes try to insert the new row and get the new PK value. One of them will succeed, and can add the new entry to its cache. The other one will fail, but then it can fall back to querying for the of the row that the other process inserted. I call this the "write-read" approach. Is there a better way of handling this contention between the two processes? Would a "read-write" approach be better, potentially with a different transaction isolation level ("read uncommitted" e.g.)? The one potential downside I see with the "write-read" approach is that it involves catching a duplicate key exception, and using that as an indicator to query for the existing row. This is perhaps a misuse of an exception, because this scenario is expected to occasionally happen, and is not really exceptional. The problem with the "read-write" approach is that I'm not sure there's a good way to have process 2 "see" the row that process 1 added, until it finishes its transaction. So this might need to be a "read-write-read", in which case I'm better off just starting with the INSERT like in the "write-read" approach.