Let CNF-SAT be the problem of determining whether a given CNF formula is satisfiable (no restrictions on the width of clauses). 

To my knowledge, no one has figured out how to exploit the "counting solutions" property of #SAT in any lower bound on deterministic algorithms, so unfortunately the best known lower bounds for #SAT are basically the same as that for SAT. However, there has been a little progress. Note that the decision version of #SAT is called "Majority-SAT": given a formula, do at least $1/2$ of the possible assignments satisfy it? "Majority-SAT" is $PP$-complete, and given an algorithm for Majority-SAT, one can solve #SAT with $O(n)$ calls to the algorithm. The closest that people have gotten to new lower bounds for #SAT (that are not known to hold for SAT) is with lower bounds for "Majority-of-Majority-SAT": given a propositional formula over two sets of variables X and Y, for at least $1/2$ of the possible assignments to $X$, is it true that at least $1/2$ of the assignments to $Y$ make the formula satisfiable? This problem is in the "second level" of the counting hierarchy (the class $PP^{PP}$). Quantum time-space lower bounds (and more) are known for this class. The survey at $URL$ gives an overview of results in this direction. 

If I would like to know something about intersection models, the first reference I would check is the "Topics in Intersection Graph Theory" by McKee and McMorris. Theorem 1.5 answers your (combinatorial) question. 

This question is about subset problems (the solution is a subset of the instance, so trivially enumerable in $2^n \cdot n^c$ time), and the parameter is the solution size, so-called the standard parameterization. The answer to the question in the title is obviously yes: the clique problem on sparse graph ($m = O(n)$) remains W[1]-hard but can be trivially solved in $2^{o(n)}$ time. The trick here is that the solution can never be $\omega(\sqrt n)$. So to make the question nontrivial, we have the requirement that 

On interval graphs, minimal vertex separators are well understood: they are cliques, there are no more than $n$ ones. However, when we turn to the minimal edge cut, my search found no even one single paper, which surprised me. To make it more precise, the edge cuts are defined as follows. In an interval graph $G=(V,E)$, a pair of vertices $u$ and $v$ is called a dominating pair if there is a path $P$ between them such that all vertices of $V$ are adjacent to some vertex in $P$. A partition $(X,Y)$ of $V$ is called an edge cut if $u\in X$ and $v\in Y$. A slight different way to define edge cuts is through the clique path decomposition. It is known that a graph is an interval graph if and only if there is a linear ordering of its $c$ maximal cliques in a way that each vertex appears in a consecutive subset of them. Then an edge cut is defined as a partition $(X,Y)$ where $K_1\setminus K_2\in X$ and $K_c\setminus K_{c-1}\in Y$. These two definitions are not exactly the same, and may have different minimum cut sizes. But I believe they are related. Also note that it's uninteresting to study the number of minimal edge cuts, as a clique is an interval graph where each partition decides a different minimal edge cut. EDIT. My question: what's the properties of the minimum edge cuts? 

One should be careful about having a head that large. If your tape head can read the entire input, then technically speaking all inputs can be decided in one step. This is because "making a head so large you can read the input" is formally equivalent to making the tape alphabet so large that it contains all input strings. But if this is the case then your entire function can be encoded in the transition table of the Turing machine, so every input can be decided in one step. It makes perfect sense to have a superconstant number of symbols in the TM, but when you do that you are no longer talking about "uniform" computation (fixed-size programs) but rather "non-uniform" computation, where the size of the program can grow with the input length. In complexity theory, non-uniform computation is typically studied in the language of circuit complexity: having a polynomial-size Boolean circuit family for solving a problem is equivalent to having a "polynomial-size transition table" for solving a problem with a polynomial-time TM, and the general feeling is that circuits are more natural to reason about. But it's possible that the TM perspective could be useful too. Allowing a specific number of states or specific structure probably will not change the definitions of the standard complexity classes. This is because there are universal Turing machines which have a small number of states and/or simple structures in the transition table, which can simulate arbitrary Turing machines with polynomial time overhead. (Sorry, don't have references at the moment.) Restricting yourself to these machines does not change the overall difficulty, up to polynomials in the running time. 

I have no idea about your problem. But I happen to know a great(est) collection of papers pertaining to Graph Matching algorithms WITH PDFS. Applause for Seth Pettie! 

Indeed, vertices in a sub-path induces an interval subgraph. So you can use standard interval models (the endpoints of the interval for a vertex $v$ are the indices of leftmost and rightmost bags that contain $v$) to construct the tight example easily. The graph given above have intervals: [1,1], [1,2], [1,3], [2,4], [3,4],[4,4]. Likewise, you can construct for a subpath of length n: the basic idea is that each interval either starts from 1 or ends at n. 

A short answer is $URL$ A more comprehensive reference might be the classic "Matching Theory" by Lovasz and Plummer, which is the best on this topic. 

Let $H$ be a maximum induced interval subgraph of a graph $G=(V,E)$. If $n=|V|$， then what is the smallest number of $V(H)$? The number is at most $3n/4$： consider a set of disjoint $4$-holes. Can it be smaller? 

I believe the title of this paper is self-explanatory and answers your question: On product covering in 3-tier supply chain models: Natural complete problems for W[3] and W[4] 

There are arbitrarily long sub-path whose induced subgraph contains no three independent vertices. See the graph below for length 4: 

Here are a few additional references. More can be found by looking at the papers that cite these. Duris and Galil (1984) give a language in $P$ which requires $T^2 S \geq \Omega(n^3)$ on one-tape Turing machines with any constant number of read-write heads. Karchmer (1986) showed that the same lower bound holds for the element distinctness problem. Babai, Nisan, and Szegedy (1989) give a very natural language (generalized inner product) that is solvable in $O(n)$ time and $O(1)$ space on a $k+1$-head one-tape Turing machine, that requires $T S \geq \Omega(n^2)$ on any $k$-head one-tape Turing machine. Ajtai (1999) shows time-space tradeoffs for deterministic random access machines computing element distinctness. In particular if $S \leq o(n)$, then $T \geq \omega(n)$. Subsequent work by Beame, Saks, Sun, and Vee (2000) proves time-space tradeoffs for randomized computations. Santhanam (2001) showed that $TS \geq \Omega(n^2)$ holds for multitape Turing machines solving SAT, building on Cobham's analogous lower bound for PALINDROMES. 

and the relevant result from the paper is that for every $\varepsilon > 0$, there is an $O(n^{2+\varepsilon})$ time algorithm that, given any $n \times n$ 0-1 matrix $A$, the following operations are supported: -- For any vector $v$ with only $t$ nonzeroes, $A v$ can be computed in $O(n(t/k + n/\ell)/\log n)$ time, where $\ell$, $k$ are free parameters satisfying ${\ell \choose k} \leq n^{\varepsilon}$. (One nice setting is $\ell=\log^c n$ and $k=\varepsilon(\log n)/\log \log n$, so that the running time is about $nt/\log n + n^2/\log^c n$ for any desired constant $c$. -- Row and column updates to $A$ can be computed in $O(n^{1+\varepsilon})$ time. We used this data structure to give faster theoretical algorithms for APSP in sparse unweighted graphs. 

The best option might be the FCRC, which combines lots of famous and interesting theoretical conferences and others. I didn't see the policy for supporting undergraduate students, but I believe they would waive your registration fee should you write to ask (maybe they need volunteers?). Especially, as you mentioned POPL, PLDI suits a little more for undergraduate students. 

The second definition uses the hitting set formulation, which is equivalent to the set cover problem. To see that, you may reverse the roles of sets and elements. You can find more information on the wikipedia page. 

Chordal graphs can be defined as intersection graph of subtrees of any tree. So the answer to your decision question is trivially YES. On the construction side, for each subtree $T_v$ of bags (it's convenient and conventional to call the nodes of the tree as bags), you'll have a unique new vertex $v$, which is put into all bags of the $T_v$. 

According to Brooks'_theorem, a cubic graph (3-regular graph) containing no $K_4$ can be properly colored by three colors. (Such a color can actually be found in linear time, which is not our primary concern.) The questions are: 

Informally speaking, the W[1]-hardness characterizes the hardness of instances with extremely small solutions, while the subexponentional solvability concerns with the solution being almost half of the instance size. So it seems perfectly fine that such a problem exists (?). 

To me, the "canonical" proof of the Karp-Lipton theorem (that $NP \subseteq P/poly \Longrightarrow \Pi_2 P = \Sigma_2 P$) has this flavor. But here it is not the actual theorem statement in which quantifiers get reversed, but rather the "quantifiers" get reversed within the model of alternating computation, using the assumption that $NP$ has small circuits. You want to simulate a computation of the form $(\forall y)(\exists z)R(x,y,z)$ where $R$ is a polynomial-time predicate. You can do this by guessing a small circuit $C$ for (say) satisfiability, modifying $C$ so that it checks itself and produces a satisfying assignment when its input is satisfiable. Then for all $y$, create a SAT instance $S(x,y)$ that's equivalent to $(\exists z)R(x,y,z)$ and solve it. So you've produced an equivalent computation of the form $(\exists C)(\forall y)[S(x,y)$ is satisfiable according to $C]$. 

If what you are studying worked out, it definitely would not be trivial. It would imply that 3SAT has (non-uniform) circuits of size $n^{O(\log n)}$. Then, every language in $NP$ (and the polynomial time hierarchy) would have quasi-polynomial (i.e., $n^{O(\log^c n)}$) size circuits. Even if it took $2^{2^n}$ preprocessing time to produce a data structure of size only $2^{O(\log^2 n)}$ which could then correctly answer arbitrary 3SAT queries of size $n$ in $2^{O(\log^2 n)}$ randomized time with high probability, 3SAT would have quasi-polynomial size circuits, using the known translation of randomized algorithms to circuits. This would not improve the known algorithm time bounds because of the preprocessing, but it would still be extremely interesting as a non-uniform result. What do you mean by "to within an error that can be adjusted to an arbitrary amount"? Is the algorithm randomized?