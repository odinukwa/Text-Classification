If you are only needing data that's changed since last backup, and your tables does have modified date as one of the columns (consider to add if you can to ease the process?) you can: 

So, make sure that the tables which are not "visible" have proper permissions for mysql to access it. Also, when you say you moved "database" do you mean all-databases-on-server or single-database? If your database have innodb tables you cannot move them like this! 

(Consider backing up binary-logs if you want point in time restores.) For backups you can use traditional mysqldump or mydumper/loader. If your data size is large, it'd be better to go /w physical-backups, follow settingup xtrabackup for mysql with Holland framework. 

Because the flush to disk operation only occurs approximately once per second, you can lose up to a second of transactions in an operating system crash or a power outage. Well I just copied all of the above from documentation ;) Anyways, now about your points. 

How about setting a slave replicating only tables of choice? Did you know about mydumper/loader? That can take faster dumps than mysqldump. Are you backing up your data daily? If yes then why don't you setup an automatic nigtly restore process that restores your dev with latest backup? You may also choose to play the binary logs from production and discard other tables: 

Oprn. is "slow" because you do ensure the data durability & consistency. Oprn. is "Fast" because you did less work (disk operations). Even for the "lies" mysqld goes an extra step. 

You can setup second mysql instance with replication. If you need to "copy" only during certain time period, you may choose to cron a script which will start and stop replication according to the time. Again you can only choose to replicate-do-db certain databases as well. 

Ofcourse a physical files restore will be quicker than loading a dump. So, if you have full physical backup / snapshot of mysql data-directory - you can go fast! (Below answer assumes that you have physical backup available.) Tables are MyISAM: 

If you have binary logging enabled, you can analyse your binlogs to identify types and number of statements executed. 

Yes, if your mysqldump is backing up all databases! If not, for future make sure you use with mysqldump. Alert but this will take your system to the state what it was earlier... If you know what your java programs are connecting using (user-name and host) then you can choose to fix the permissions for that!! 

--> You might consider setting up a special slave on which you make your changes and then do a full datadump. --> You can prepare SQL commands to run after dump load finishes as follows: 

When you cannot identify a primary key for a table you need to use surrogate key; auto_increment columns are most common surrogate keys which database internally provides and hence you should use them then. You may alternatively have sequences or programmatically handle uniqueness... 

You should rather monitor your slaves for replication lag or if it's actually catching-up / connected with master or not! You may simply write a shell script to look at io_thread and sql_thread values and alert if they're NO. There are already many scripts available for reference. You can use Percona Monitoring Tools's pmp-check-mysql-replication-delay / pmp-check-mysql-replication-running. You may google for setup instructions to setup with Nagios or setup with Zabbix or this. Btw, 7 days worth binary logs (with backups) are sufficient but again depends on policies! If you are to avoid disconnects due to transient network issues, depending on which MySQL you're on you may want to set combination of master-retry-count / slave-net-timeout / master-connect-retry. (but it's better to fix network right?) 

Now, as you talk of replace in all tables, that's something not possible in single query. Though you can create a routine to do so! For the sample you can refer a procedure which is finding a string in all tables of all databases. 

MySQL Server master-master replication is possible for max 2 nodes. You might want to use Galera Cluster (Percona Xtradb Cluster or Mariadb Galera cluster) Or Oracle's NDB Cluster to achieve HA as a better solution than master-master. 

Nice! Now the case is you have lost your datadir on master and some how you have the master log position!! Care to tell us how? Anyways... In such case we will have to reconstruct the master from slave. But because your slave is behind master (not caughtup with binary logs) we need to fix that first. Backup your slave before working The slave co-ordinates are mysql-bin.000040 and some-log-pos... This means that when your datadir was getting "lost" slave was still lagging behind. Now, if your slave has fetched all the required binlogs (relay logs) from master before master went down then just start sql thread and let it catchup. On Slave: 

If you need to loop over the output of one query to another you can use cursors. For eg, in this stored routine to find in all tables databases we have looped over: 

(Seems like I'm able to post detailed answer here today... let me know if this has helped you) Here is a test output for you... 

You do not want writes to be replicated from A to B then A better be slave of B, master. again, to complicate things you can use master-master with replication filters but let's not get there if you do-not wish to. That said, writing on slave is sort of risky business where you're going to "accidentally" write on databases being replicated from master and that's not good! As long as you're making sure you're only writing to slave-only databases it should be fine. You should choose to create dedicated users for such activity. You might also want to wonder: - What about backups of master data? - What about backups of slave data? just saying... but yea to answer your question go with master-slave. 

So here I'd suggest to implement triggers on the tables you require the logging. It'd create additional load to the system but if that's the requirement... 

Server is responsible to purge their own file. Guess what happens if your master has two slaves and one of it catches up faster deleting logs required for second?!! So yes expire_logs_days is the way to go. Set that value wisely and let master do its business. That said why do you want to purge logs? You have command but I'd still let purging upto master unless disk is critical. If you want it, then you can script out the manual purging logic: Check all slaves have caught up to latest Purge a logs before the exec_master_log_file. In my opinion this is absolutely unnecessary unless there some odd requirement that u just can't think about... 

You can use pt-table-sync with --verbose to generate the differential sql and verify the data from there. Whichever version you feel is correct you may choose to keep. I'm not aware of any tool which will merge, as such what will be the merge condition upon conflicts? Writing on both nodes in master-master can cause bad bad data! I'd try to avoid that. 

As you already mentioned documentation that you cannot use table level filtering on replication. You might like to try another approach instead. Replicate whole database and change all tables except those you need to use to engine. 

The behaviour is correct and it is incorrectly observed as per update from the questioner. This got verified by further debugging the dumpload adding a debugging select syntax after the erroneous SQL in the dump. 

"My current mysql backup strategy is replication." Replication cannot be a backup strategy but you can setup backups on slave! I guess you're talking about HA "if master fails then use slave"... Anyways... MySQL 5.7 comes with Multi-source replication. You can have single slave machine replicating from two different masters. You may write your own script to loop around CHANGE-MASTER-TO switching masters and replicate. Idea for you: