My old answer: I'd still look some more at a secure multiparty protocol for computing the sum $S = \sum_j b_j$. The only way that this falls short of your scheme is it reveals one additional bit: it reveals whether $S<N/2$ or not. Does that one bit of information matter in your setting? You say the sum $S$ is sensitive in your application. I hope you are aware that revealing the value $R$ reveals $S$ up to two possibilities (i.e., given $R$, we can compute a value $Q$ such that $S \in \{Q,N-Q\}$). If you absolutely must conceal this information, here is a different approach that you might be able to make work. For each pair of parties $i,j$, securely compute $E(c_{i,j})$ where $c_{i,j} = b_i \oplus b_j$, $\oplus$ is the xor operation, and $E$ is some sort of additively-homomorphic encryption scheme. Then you might be able to compute $\sum_{i<j} c_{i,j}=R$ from this. There are some details to work out and the threat model may not be what you were hoping for, but it's possible you might be able to make something like this work. 

A trivial observation is that if $|S(x)| \le 2$ for all $x$, then this problem is solvable in polynomial time, by reduction to 2SAT. Here's how. Introduce a variable $v_{x,i}$ for each vertex $x$ and each $i$ such that $i \in S(x)$. For each pair $x,y$ of vertices, if there is a path from $x$ to $y$, we get some constraints: if $i\in S(x)$, $j\in S(y)$, and $i>j$, then we get the constraint $\neg v_{x,i} \lor \neg v_{y,j}$. Bijectivity gives us another set of constraints: for each pair $x,y$ of vertices with $x\ne y$, if $i \in S(x)$ and $i \in S(y)$, we add $\neg v_{x,i} \lor \neg v_{y,i}$. Finally, the requirement that each vertex must be assigned a label gives us yet another set of constraints: for each $x$, if $S(x)=\{i,j\}$, we get the constraint $v_{x,i} \lor v_{x,j}$. (Note that only the last set of constraints exploit the promise that $|S(x)|\le 2$ for each $x$.) I realize this observation won't help you in your particular situation. Sorry about that. 

Do methods in an OOL contain free variables? If they do, are they statically-scoped? Do method names live in a different name space than variables? 

I saw some people uses Krivine's notation for function application when presenting the syntax for the $\lambda$-calculus. For example, the $\lambda$-term $\lambda f . \lambda x . \lambda y . f\ x\ y$ (with the normal convention that function application associates to the left, so it actually means $\lambda f . \lambda x . \lambda y . ((f\ x)\ y)$) is written $\lambda f . \lambda x . \lambda y . (f)\ x\ y$ (with a similar convention that it actually means $\lambda f . \lambda x . \lambda y . ((f)\ x)\ y$). I do not see the point of having another pair of parentheses around the innermost $f$. Why do people use Krivine's notation instead of the usual one? 

In a Cartesian Closed Category (CCC), there exist the so-called exponential objects, written $B^A$. When a CCC is considered as a model of the simply-typed $\lambda$-calculus, an exponential object like $B^A$ characterizes the function space from type $A$ to type $B$. An exponential object is introduced by an arrow called $curry : (A \times B \rightarrow C) \rightarrow (A \rightarrow C^B)$ and eliminated by an arrow called $apply : C^B \times B \rightarrow C$ (which unfortunately called $eval$ in most texts on category theory). My questions here is: is there any difference between the exponential object $C^B$, and the arrow $B \rightarrow C$? 

I wonder why computer scientist have chosen recursor instead of iterator (or tail recursor if you like) in primitive recursion, given that function defined in terms of iteration behaves more efficiently than that in terms of recursion. EDIT: Let us go further, do you think iteration would be better than recursion to complexity? 

Of course, one pragmatic approach might be to try converting your problem to a SAT instance and applying an off-the-shelf SAT solver. If you are working over the integers ($\mathbb{Z}$), you might also want to try formulating it as an integer linear programming (ILP) problem and then apply an off-the-shelf ILP solver. Depending upon the size and complexity of your circuits, this might work reasonably well in practice -- though there is no guarantee that it will always run efficiently. 

One straightforward approach is to first compute a $d \times k-d$ matrix $M$ such that $Mx=0$ iff $x \in K$. Then you can determine which points from $A$ lie on $K$ by computing $Mx$ for each $x \in A$. The running time for that will be $O(d^3 + k(d-k)n)$ or so, i.e., $O(d^3 + d^2 n)$. Since the input has size $dn$, any solution has to take $\Omega(dn)$ time. Thus this solution is at most a $d$ times factor slower than the best possible. You mention in the comments that you are interested in the case $k,d \ll n$. This might suggest counting the running time as a function of $n$, i.e., focus primarily on the dependence on $n$, and largely ignore the dependence on $k,d$. As a function of $n$ (treating $k,d$ as constants), the solution above runs in $O(n)$ time. 

Check for success: If the resulting sets $A,B$ each have size 6, test whether they are a valid solution to the problem. If they are, stop. If not, continue with the loop over candidate values of $z$. 

It has plenty of theoretical/foundational content, and is in the vicinity of the sort of thing you mention. 

I hope that this was not already obvious. Your question did not make it entirely clear how much you already know about the general approach and what specifically you were looking for in an answer. 

In programming language semantics, it is often heard that people talking about meaning and denotation. They seem not to be the same. What is the difference? Is the former associated with operational semantics while the latter with denotational semantics? Thanks. 

I heard that there exist two styles to define an evaluation context: outside-in and inside-out. Can someone give the definitions? Why are they so named (inside-out and outside-in)? What is the difference? Some examples would be appreciated. 

Sometimes I see people put side conditions above the inference line as if they were premises of an inference rule. This feels strange. My understanding (which may be wrong) is that a side condition belongs to the meta-theory, not to the object-theory: an object-proof of a side condition should not be required and may even not be possible. So what is the motivation of doing this? For saving space or for some other deeper reason? $Update$: I am no longer quite sure whether a side condition belongs to the meta-theory. But at least it is outside the object-theory the inference rules describe,that is, the truth of a side condition cannot be derived using the inference rules. $Update^2$: Take as an example the rule for typing variables from the simply-typed $\lambda$-calculus. $(x : T) \in \Gamma$ is a side condition that tests whether the variable $x$ of type $T$ is in the typing context $\Gamma$. In some presentations, the side condition is put aside, leaving the premises empty to indicate that it is an axiom, though conditional: $$\frac{}{\Gamma \vdash x : T} (x : T) \in \Gamma$$ while in others, it is put above as if it were a premise: $$\frac{(x : T) \in \Gamma}{\Gamma \vdash x : T}$$ 

I was not a big fan of Object-Oriented Languages (OOL), but recently started to learn a bit more about their pros and cons in a general setting instead of diving into one such language. I have a few questions here: 

I have a multiset $S$ of $n$-bit strings. Let $1_S(s)$ denote the number of times that string $s$ appears in $S$, i.e., the multiplicity of $s$ in $S$. I want to find a partition of $\{1,2,\dots,n\}$ such that the projections onto these index sets all have large multiplicities, but let me explain what I mean by that. If $I$ is a subset of $\{1,2,\dots,n\}$ (i.e., a set of indices), then let $\rho_I(s)$ denote the projection of string $s$ onto the indices in $I$. In other words, if $I=\{i_1,i_2,\dots,i_k\}$, then $\rho_I(s)$ is the $k$-bit string $s_{i_1} \, s_{i_2} \, \cdots \, s_{i_k}$. Also, let $\rho_I(S)$ denote the multiset $\{\rho_I(s) : s \in S\}$. For instance, if $S=\{01,01,11\}$ and $I=\{2\}$, then $\rho_I(S)=\{1,1,1\}$. We'll say that an index set $I$ is good if every element $t$ of $T=\rho_I(S)$ has multiplicity at least $m$ in $T$. In other words, every string in $\rho_I(S)$ has to appear at least $m$ times in $\rho_I(S)$. Here $m$ is some threshold fixed in advance, e.g., $m=100$. Suppose we partition the set $\{1,2,\dots,n\}$ into three disjoint sets: $I_1 \cup I_2 \cup I_3 = \{1,2,\dots,n\}$, where $I_1,I_2,I_3$ are mutually disjoint. We'll say that this partition is good if $I_1$ is good, $I_2$ is good, and $I_3$ is good (in other words, projecting onto each index set yields a multiset where all elements have multiplicity at least $m$). I want to find an algorithm to compute whether there exists any good partition (and if yes, to output an example of a good partition), given $S$ and $m$. Can anyone suggest a good algorithm for this? Or, if this is intractable, I would be satisfied with good heuristics or approximation algorithms. Or, is it easier if I want to partition $\{1,\dots,n\}$ into two sets $I_1,I_2$? Does anyone know of any algorithmic techniques that might be relevant, or any connection to problems that have been studied in the literature? This arises from a practical application I have, where (intuitively) the requirement that all multiplicities of the reduced set be at least $m$ corresponds to a sort of privacy/anonymity/plausible deniability property, and larger values of $m$ correspond to greater anonymity. Sample parameters for my problem might be something like: $S$ is a multiset with thousands or tens of thousands of strings (counting multiplicity) and hundreds or thousands of unique strings (not counting multiplicity); $m$ is maybe 100 or so; $n$ is maybe 20-200 or so. References to research papers would be fine. I hope this kind of question is suitable for this site; if not, let me know and I'd be happy to flag it to have it migrated over to CS.StackExchange. 

As stated in the title, I wonder any relation and difference between CIC and ITT. Could someone explain or point to me some literature that compares these two systems? Thanks. 

I see here and there mention of the $\lambda_I$-Calculus (in which every variable must be used at least once) and the $\lambda_K$-Calculus (in which a variable can also be unused). Are they equivalent? Why has the latter kinda obscured the former? EDIT By equivalent, I mean they have the same expressive power, namely, being universal or Turing complete. 

For both questions, I have kinda guess answers but am not quite sure. However, I would like to know more about the reasons behind and consequences of OOLs' specific answers to these questions. 

In Wadler's Recursive Types for Free! [1], he demonstrated two types, $\forall X . (F(X) \rightarrow X) \rightarrow X$ and $\exists X . (X \rightarrow F(X)) \times X$, and claimed they are dual. In particular, he pointed out that the type $\exists X . X \rightarrow (X \rightarrow F(X))$ is not the dual of the former. It seems the duality in question here is different from De Morgan duality in logic. I wonder how the duality of types is defined, specfically for the three types mentioned, why the second is dual of the first while the third is not. Thanks. [1] $URL$ 

I recently read Landin's paper "The Next 700 Programming Languages". But I was a bit confused by ISWIM. In particular, are functions first-class objects in ISWIM? It seems not because every function must occur under some name and there is no $\lambda$-like construct in the language to construct an anonymous function. Landin even explicitly claimed in the first footnote that "a not inappropriate title would have been Church without lambda". Anybody knows the reason behind this choice? Is ISWIM less expressive than a language with $\lambda$? 

Here's one possible approach. For a fixed value $b$, it's easy to build a Bloom filter that stores a set $S$ of values and lets us answer queries of the form "is there any $s \in S$ such that $s \in [x,x+2^b-1]$?", where $x$ is a parameter of the query and is constrained be a multiple of $2^b$ (in other words, the ranges are $2^b$-aligned and are of length $2^b$). (How do you do that? Store $\{\lfloor s/2^b \rfloor : s \in S\}$ in a regular Bloom filter.) Now, for a value $x$, you can decompose the range $[x-k,x+k]$ into a union of $O(\lg k)$ ranges of the above form. So, our succinct data structure will be composed of multiple Bloom filters: one Bloom filter for each non-negative integer $b$. We store a copy of $S$ in each of those Bloom filters. To answer the query "is there any $s \in S$ such that $s \in [x-k,x+k]$?", we decompose $[x-k,x+k]$ into $O(\lg k)$ ranges that are $2^b$-aligned, and then we make the appropriate queries to the component Bloom filters. Since we increase the number of queries by a factor of $O(\lg k)$, the false positive rate doesn't increase too much. In fact, I think you can show that you will only need to make at most 2 queries to each component Bloom filter (i.e., the constant hidden by the big-O notation is at most 2), so I expect this should work pretty well in practice. Note that this data structure actually allows you to generalize even further: $k$ does not need to be fixed in advance; it can instead be specified as part of the query. I would expect this data structure to be a little bit more compact than a binary search tree, but not a lot.