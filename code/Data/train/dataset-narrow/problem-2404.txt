To the best of my knowledge, there is no such study; furthermore, without some nontrivial advances in the technology of sum-of-squares (SOS) problems, it is not currently clear what the immediate benefit of such a study would be. (I'll focus on the SOS connection since that, as far as I know, is the best way to solve these general quartic problems.) This statement should be taken in a positive light: I believe there is a lot of research depth surrounding these problems. I'll substantiate my claim in a few ways, hopefully in ways people find useful.. Firstly, for the most basic problems of the type you discuss, the SVD connection gives a much better solver than the SOS black box; in particular, the latter constructs an SDP with $\binom{n+2}{2}$ terms, where $n$ is the total number of variables in the source optimization problem (for instance, the total number of elements in all unknown matrices; to see where I got these numbers, see lecture 10 from Pablo Parrilo's 2006 course: $URL$ ). This is an SDP you would never want to solve (running time depends on $n$ as $n^{6}$ using an interior point solver?), especially when compared with the ridiculous speed of an SVD solver (using consistent notation, SVD will be something like $\mathcal O(n^{1.5})$; you can de-fudge my computations by tracking the number of columns, rows, and target rank, but it's a disaster no matter how you rectify my negligence). Along this vein, if you designed a specialized algorithm to solve SOS problems where the max degree within any polynomial is two: this would be amazing, and then the sort of survey you seek would have lots of value. Secondly, since the basic formulation of these problems is out of the window, one may wonder if certain variants of these problems are well handled by SOS solvers. As an important example, consider the NMF (non-negative matrix factorization) problem, where the matrix unknowns you are optimizing over (in your above formulation) must now have non-negative entries. Unfortunately, if you take the standard SDP used to solve these problems (see for instance Pablo Parrilo's notes from above), there's no way to introduce those constraints. (And since some formulations of the resulting problems are NP-hard, you would now be building an approximation scheme; i.e., this can get nasty.) Furthermore, there is recent work that exploited the polynomial structure of this problem to build solvers with some guarantees: see $URL$ by Arora, Ge, Kannan, and Moitra. They build a few algorithms, however when they solve an "exact" NMF problem (where there is an exact factorization, i.e., one giving objective value 0), they do not use a SOS solver: they use a solver checking feasibility of "semi-algebraic sets", a much more difficult optimization problem which allows the kinds of constraints which NMF raises, but now with exponential running time. Anyway, to summarize and give some further perspective; since SOS is afaik the only solver for the quartic problems you speak of (i.e., I don't think there is a specialized quartic solver), I've discussed how these solvers have better alternatives for the kinds of quartic problems people care about. To effectively use SOS tools here, you would either have to build some amazing solver for the quartic case (inner polynomials of degree at most 2), or you would have to find some way to add constraints to these problems. Otherwise, the connection to SOS problems, while fascinating, does not give you much. You also mention that you are surprised that the literature you have found does not make this connection. I think that is mostly due to the newness of practical SOS solvers (even though abstract consideration of SOS problems goes back very far), and what I said above. In fact, when I first found SOS solvers, it was via Parrilo's notes and papers, and I similarly wondered, "why isn't he talking about PCA-type problems"? Then I checked the above facts and frowned a lot. I think it's also a bad sign that Parrilo himself has not, as far as I can tell/skim, discussed these problems outside the reference you mention in his thesis (meanwhile, he has papers on various extensions, and I have a lot of respect for his work in these field: he must have thought about these specific quartic problems many times.. okay I just checked and found a related negative result by him and some colleagues, which has some connections in its introduction: $URL$ ). 

(Edit notes: I reorganized this after freaking out at its length.) Literature on coordinate descent can be a little hard to track down. Here are some reasons for this. 

Before giving some detail, I'll briefly say: regarding (1), kernel trick is basically the worst thing for privacy, and regarding (2), you have replaced each vector with a vector of kernel products (via the linear kernel), and this is non-equivalent to replacing inner products with kernel products inside the algorithm. Some background on (1) for those who asked. Suppose you want to learn a linear predictor; among those consistent with the data (i.e. correct on all examples), which do you choose? The SVM solution is to pick the one which maximizes the minimum distance to any example (maximizes the 'margin'). In the case that no consistent predictor exists, small penalization is assigned to margin violations (learning the best linear predictor, in the presence of noise, is NP-hard). now say your data is a set of records for some group of people (for instance, each "point" is a vector of medical parameters for some person). You want to release a predictor (say, a website where someone can put in some info and see if they have some disease or not), but don't want anyone to be able to reconstruct properties of the people in your data set. Here's an example; let's say I know someone is the only person in some remote part of saskatchewan, and I want to see if they were used to learn this predictor. And say, one of the attributes in the web form is location. For privacy ignorant algorithms, I can see how sensitive the algorithm is to queries with this personal data, and infer things about the person in question (for instance, if saying I'm from remote-part-of-saskatchewan but varying all other parameters always says I have the disease (and the answer is sensitive to location), that means that guy has the disease). Said another way, if I know some unique characteristics of a person, I can see how the algorithm reacts to them in order to determine other properties of that person. Anyway, here's why the kernel trick is the worst thing for this. The kernel trick just means you store your predictor as $$ p(x) = \sum_{i=1}^m a_ik(v_i, x), $$ where $\{v_i\}_1^m$ are your support vectors. This means the predictor for any kernelized linear classifier (not just SVM) will explicitly store a set of records! Therefore, if you manage to plug in a support vector for $x$, you can expect it to strongly affect the value of the predictor. Going back to the remote-part-of-saskatchewan example, suppose the kernel is gaussian, and you fix all other parameters, and see the effect of the saskatchewan input. Since distances are all huge in high dimension, that means, if the guy is in the dataset, the predictor value will be strongly affected by matching the remote-part-of-saskatchewan fields or not. Thinking geometrically, each support vector plops a gaussian in the space (color them white or black based on their label), and if this guy is really the only person in that remote part of saskatchewan (and if he was a support vector), that means his gaussian will basically entirely control the value of the predictor. EDIT. I should say that research in privacy preserving learning is all about taking privacy-ignorant algorithms and making them output privacy-secure predictors. There are definitely strategies to make kernelized algorithms more privacy preserving--for instance, don't let any region of the space be "covered" by just one support vector (some sort of smoothness condition, i.e. how much the predictor is affected by removing or adding in this example); this in turn usually means.. have TONS of support vectors. Another strategy is to inject perturbations in places, but notice this is delicate--if you just perturb this saskatchewan record, it is still basically the only gaussian in that region of space, so you've done nothing. Anyway all I am saying is that kernelized algorithms are a difficult starting point for privacy preservation. (and the "have TONS of support vectors parts" starts to mitigate the benefits of the kernel trick..) About (2).. let $A$ contain all points as columns; a linear predictor can be written as $$ p(x) = x^\top Av $$ where $v$ is a vector of weights (in the linear case, you can "compress" this and just keep weights $w=Av$ around). (to be explicit, you also check the predicate $p(x)\geq 0$.) In your case, you have replaced $A$ with $AA^\top$, and also you must map any input vector $x$ to $Ax$, thus your predictor is $$ p'(Ax) = x^\top A^\top AA^\top v'. $$ For general matrices $A$, it is possible that some choice $v$ does not have a choice $v'$ which makes these equivalent...... thus, this is not the same as simply "kernelizing" the algorithm. That said, it is reasonable to replace your data with kernel products in this way, but that is a feature selection issue; i've attempted to discuss the math part...........