I have working omnidirectional shadow maps for point lights. Rendering a shadow map for one point light consists of rendering the scene to 6 separate rendertargets, then send those render target textures (shaderresourceviews) to a pixel shader which will shadow the scene by projective texturing. Switching rendertargets 6 times for each point light however is quite costly, and so does sampling from those in the pixel shader later. My question is how would I access a face of the cubemap to render to? Now I am doing Id3d11devicecontext::omsetrendertargets(1,NULL,&depthtarget) . My depthtarget is a texture2d which is bound to a depthstencil view and a shaderresource which is set up to be a cubemap. Can I even set an individual face to render to or is the process completely different? 

Sure, you have to set it to a shader which you draw your model with. If it is a hand-written shader then you have to set it tothe shader: 

I use the following method in a 3D scene for grass and vegetation animation, but is easily transformed to 2D geometry. The method is as simple as offsetting vertices of wind-affected geometry by some sine fuction. (vertex shader code follows) 

You probably would want to do it like this: Have a VBO (+index buffer) for each mesh in the scene (a mesh is just the collection of vertices and indices). Have your objects separately hold a reference to their meshes and store their transformation. From there you can have multiple objects referencing the same mesh with different transformations which is memory efficient and you can sort your drawing to bind each mesh's VBO once while drawing. A bit more advanced technique would be geometry instancing which could be done as the next step. This way you won't even have to do a draw call for each object separately. 

I couldn't find a way to do this, I write the distance instead in SV_DEPTH so I can compare the values. It requires a few additional constants so it is not as nice as I wanted it to be, so I am still interested in case anyone could solve this an other way. 

You need to set the blendstate to alpha blending when rendering your objects. You can do it like this: 

Use Luna for C++ object binding to Lua. It is essentially a single header file containing helper functions to bind a C++ class to Lua. The previous version also contains instructions as to how to get it working. The usage is very simple, but you have to write boilerplate code, a wrapper class for each of your classes you wish to bind. 

The term you are looking for is multitexturing. For this, you have to send every texture you want to use to the pixel shader, and there decide from some per-vertex weight factor how much each texture will add its value to the final color. For example, if you want to use 4 textures, you add a float4 value to your vertices (I'd call this weight). Then in pixel shader sample your 4 textures into values. The final color then would be: 

I have a particle system. So far it worked like this: I have a dynamic vertex buffer for a system, which is created with a size that can hold for example 100 000 particles. I map/unmap this and write the new data into it every frame. But what if the particle count gets bigger than the buffer can hold? I thought of recreating the vertex buffer with the double of its previous capacity (then map/unmap into it). Is this the right direction for this or should I solve it in a different way? A short example: 

Here time is a shader constant which is an ever increasing number (system time for example). The pos variable is the vertex position of which I add every component to time when calculating sin for a nice variety, so it seems like wind travels nicely across the screen. 0.1f is just a magic number. I just multiplied by it because it seemed nice after some tweaking (it makes the waves seem bigger). wind_direction and wind_strength are the wind parameters, a three component vector and a float. If you want to use this with 2D sprites then you just need to forget the z component of the vectors (making them only two-component vectors) and it will work the same. If you can't use vertex shaders for some reason it should also be applicable on the cpu, but you need to store the default vertex positions (or corners) for your sprite, but draw your sprites with your modified vertices (corners). I hope I could help, also please note that this is not physically correct wind effect, but only something that is easy to compute and nice to look at. You can also check out this effect if you would be interested in this youtube video. The grass field and the tree leaves are both animated by this function. 

You also mention that you tried to combine normals somehow and got a result where it seems as if it were a plain. It should be the correct behaviour, you can add more immersion to your scene by shadowing it and doing ambient occlusion. 

These two have the same effect which is discarding the pixel if its alpha is below 0.1. I am not sure what exactly is referencealpha in the renderstate but I am guessing it is an unnormalized alpha value, so 200 would be 200.0/255.0 in pixel shader. You can turn off depth write by: 

Then in your entry point function you will have to multiply your vertices' positions with your world matrix: 

I never used the stencil buffer for anything until now, but I want to change this. I have an idea of how it should work: the gpu discards or keeps rasterized pixels before the pixel shader based on the stencil buffer value on the given position and some stencil operation. 

Ok it was a really stupid amateur mistake. I filled my btVerts array of btVector3 references whereas I should have done it for btScalars for every one of the three vertex position coordinates. The function even expects that, and I din't see it because copy-pasted from my mesh loading, where I send btVector3s as vertices in the btTriangleIndexVertexArray method (but I can specify the striding there, whereas in btSoftBodyHelpers::CreateFromTriMesh I cannot). 

First of all, you don't do your render in gamma space to get the same results as in linear, but to have "gamma correct" final image. The topic is a bit complicated, so see the link in the end of the answer if you want to learn about it in-depth. The process of applying gamma correction to your scene follows: (I've seen this in a presentation from a graphics programmer at Naugty Dog, the creators of Uncharted.) You have to sample your diffuse color in shader (be it texture or material color), then take the of it then apply your lighting calculations. After all your lighting is calculated and applied to the color, you have to take the power of the inverse of your gamma eg. 1.0f/2.2f for your final color. So in the end you have the following process (taken from the presentation): 

You should set the weighting while generating your heightmap, and the weights should normally add up to 1. The logic, which you generate those should be trivial for you, but if you are having problems, then I recommend Riemers' tutorials which is XNA, but it can be easily implemented. You can even look at his multitexturing algorythm based on terrain heights. 

The problem is that you don't sort your polygons here thus can not use alpha blending properly. The common ways to resolve this are: