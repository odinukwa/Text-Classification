You can see that the last two images look sharper, but also have the characteristic color fringing. Here's the downsampling pattern used for the last image: 

The most commonly suggested method seems to be Mueller calculus, which boils down to tracking the Stokes parameters of a light ray to represent the polarization of light transmitted along that ray. A ray might be unpolarized—Stokes parameters of (1, 0, 0, 0)—or it may be circularly or linearly polarized in various directions, which is a property of light in the aggregate. At the surface light is scattered according to the polarization and the Stokes vector is propagated by multiplying it by the Mueller matrix of the surface. Here's a writeup by Toshiya Hachisuka about ray tracing while tracking light polarization. It seems like a good introduction, and there are several references that seem promising. The article argues for direct tracking of the polarization state of the ray: instead of an aggregate representation, individually tracking the direction and frequency of the two harmonic oscillations of a given light ray. This may have the disadvantage that you need more samples to reproduce the polarization effects accurately, but it may be able to reproduce more effects (in the article, thin-film interference). 

You could try some form of color quantization algorithm, which generally extract the most dominant N colors. The one I've seen referenced most is modified median cut quantization [.pdf], which is based on median cut quantization [.doc]. The benefit to this kind of algorithm is that instead of simply averaging every color in the image, it extracts and discards other highly prominent colors instead of allowing them to pollute the average. The concept is that color space (RGB space) is partitioned into 3D axis-aligned rectangular regions (the paper calls them vboxes) and iteratively subdivided by splitting vboxes, attempting to leave half of the pixels on each side of the split. The result is color clusters that should correspond to color clusters in the image. The largest color has a strong likelihood of being "perceptually similar" to the image. There's a JavaScript implementation and demo of this algorithm called Color Thief. 

When you use in your source code and in your shader, you're telling OpenGL that you want to represent the texture coordinates as 0 to width of texture and 0 to height of texture rather than using normalized (0-1) coordinates in both directions, which you would get with and . A shift of 0.2 might be hard to notice on a large texture, as it's less than 1 texel. If you want to move it by 20% of the width, you need to pass in the texture width and height to the shader and calculate it (or just pass in what 20% of the width is). 

I haven't done a lot with optical flow, but I would guess that as you figure out the path of motion of various points on an object, you would be able to classify them into one of the above categories. If all the points in an object move in tandem, then you probably have rotation and/or translation only (rigid transforms), but if they don't then you may have an affine or perspective transform between the points. 

When drawing in OpenGL and related APIs you usually have one or more models. In your case, this would be the cylinder and the icosahedron. The models are generally generated at some default size which may or may not suit your needs for a particular task. You move them, scale them, and rotate them by setting up a model matrix which describes how you want to transform them. So how would this work in your case? I recommend having 2 functions. One which draws a unit cylinder and another which draws a unit icosahedron. Say you have a data structure for your coordinates like this (I'm assuming a C-like language): 

Here's a chromaticity diagram that includes a projection of the sRGB color space: a triangle whose vertices are red (1,0,0), green (0,1,0), and blue (0,0,1). 

Yes, occlusion culling is still worth it. At minimum, a draw call that you skipped due to culling is a draw call that doesn't have to run the vertex shader. Triangle count goes up as quickly as GPUs start supporting more triangles, because why not? With unified architectures, vertex shaders use the exact same hardware that pixel shaders do, so every vertex that you skip because of culling is more compute time for the stuff that you can see. Not to mention all the other stuff you're skipping (CPU draw call processing, and throwing the tris far enough through the pipeline that the rasterizer realizes that it doesn't need to shade them). There was a great presentation from two Ubisoft studios at SIGGRAPH 2015 about GPU-driven rendering pipelines. On the surface it's about some of the things you mentioned: batching and instancing, reducing draw call counts. But one of the major advantages they get out of a GPU-driven pipeline is incredibly fine-grained occlusion culling: better culling than you would normally see at the draw-call level. It's all in the service of asymptotically approaching the goal: processing only what you can see, which means that what you can see looks better. (Also: consider console, mobile, VR, and desktop-without-the-latest-and-greatest-GPU-that-money-can-buy. Even if all your tris disappear into the gaping maw of your top-of-the-line GPU, you may not be the primary target.) 

In a path tracer, when tracing a ray to a light with a shape other than a point ("area light" is the usual terminology), you generally select a random point on the surface of the light. When enough samples are taken, this results in both softer specular highlights and softer shadow penumbras: light will be reflected off a surface from a random distribution of incoming directions, and rays cast towards an area light with an obstruction in the way may or may not intersect the obstruction depending on which point on the light was sampled. In real time, approximations to area lights are used. This Unreal Engine 4 white paper describes some of the considerations. One possibility is the "representative point" method, which per-pixel selects a point in the light's volume (brightest or nearest) and uses that as though it was a point light. Another option is modifying the roughness that is fed into the specular calculation based on a cone that approximates the area light (in the paper as "Specular D Modification"). 

The best way to do proper outlines, is unfortunately, also the hardest. It involves calculating an offset curve, usually from the medial axis or straight skeletons, which are non-trivial to calculate. Once you have those, you can calculate the distance of any point from the input object and decide whether to draw the outline or bolding, or not. A simpler way to do it, which doesn't look quite as nice, is to use a MinMax filter. This is a sliding 2D box filter that calculates either the minimum or maximum value within the box at each input pixel. If you have black text on a white background, you can use the min filter to generate an outline. Use the max if it's white text on a black background. Other shapes, such as a circular area will produce different results that may be better for some fonts. In most cases, you'll end up either rounding off or squaring off corners as the size of the filter increases. 

The way I would approach this is to get the outline curves of the font and then break each one down into some fixed number of steps (like, say, 100), and make a series of very small straight lines. So if you've got some bezier curve, you loop from 0 to 1 in 100 steps and plug it into the equation: 

Note that there are 11 buckets because the range includes both 0 and 1. If you want only 10 buckets and want to put any pixels that have a value of 1 in them into the top bucket, you could do: 

I believe that is the output range. In 8-bit it would be 255. Let's say was 25, was 132. You want to scale that range to 0-255. The part of the equation in brackets: will give you a percentage - a value between 0 and 1. If is 25, you'd get 0%. If is 89, you'd get ~60%. If it's 132, you'd get 100% So once you have the percentage, you then need to multiply that by the output range, . That converts from a percentage to whatever range you need it in, and ensures that the largest value in the input is mapped to the largest output value, and the smallest input is mapped to the smallest output value. 

Substituting the definition of $D$ into (4) gives: $\int_{\Omega'} D(\omega_m) d\omega_m = \int_{\Omega'} \int_{\mathcal{M}} \delta_{\omega}(\omega_m(p_m)) d p_m d\omega_m$ A fundamental property of the Dirac delta function is that it integrates to 1 over its domain. Unfortunately, we are integrating over $\Omega'$, not $\Omega$. Here I will stop pretending that I am actually able to use formal mathematical terms, and I will just note that the integrand has non-zero values only where both $d p_m \in \mathcal{M'}$ and $d\omega_m \in \Omega'$, which is what (3) says. As long as at least one of our integration domains is restricted to the set we care about, we will get the same result. So I'll just move the prime around a tad: $\int_{\Omega'} D(\omega_m) d\omega_m = \int_{\Omega} \int_{\mathcal{M}'} \delta_{\omega}(\omega_m(p_m)) d p_m d\omega_m$ Now we can apply the Dirac delta identity, since we're integrating over the whole domain: $\int_{\Omega'} D(\omega_m) d\omega_m = \int_{\mathcal{M}'} d p_m$ Ta-da! 

Most raytracers do both! Have you ever seen a ray-triangle intersection test where the triangle is transformed so that one vertex is at the origin and the triangle is flat along one of the axes? That would simplify the test, but storing the inverse transformation to apply it to the ray takes more space than the triangle itself. So you could say that almost all ray-triangle tests do apply the forward transformation to the object. And yet most of the time you are rendering a mesh of many triangles, and "most raytracers" don't transform every vertex in the mesh by the object's transform matrix, they inverse transform the ray into object space. There isn't necessarily a good reason for this—it can definitely be more performant in some cases to preprocess the mesh and put every vertex in world space. But then say you're doing animation studio-scale path tracing. Now your scene geometry might not all fit in RAM at once. It's no longer a performance win to pre-transform all your vertices/control points, because (a) you need one copy of the object per instance, and (b) you need to transform it each time you read it into RAM. When data bound, the cost of two matrix multiplies per ray is insignificant anyway. Floating point precision is a potential reason as well: when you have an object a long way from the origin, you get more error from floating point positions, and if you pre-transform the object you apply that error in different directions to each vertex, whereas if you inverse transform the ray you have one fixed error amount from the imprecise position of the ray, but your vertices have less error relative to each other. Although I suspect the real answer to "why do most raytracers inverse transform the ray" is that most raytracers with public source code implement a wide range of geometric primitives, either for educational purposes or as a proof-of-concept, and it's easier to implement one cheap ray inverse transform than N shape forward transforms. (Plus, as has been mentioned, some intersection algorithms are much simpler if the object is in its canonical space.) 

I notice that you've enabled blending and depth testing and are drawing with a non-opaque alpha channel. What's going to happen is that if you don't draw from back to front (relative to the camera), then any objects you draw that are farther away won't get drawn, so you won't see them through the nearer objects. If you turn off depth testing does it look correct? To handle this correctly, you can also use a technique known as depth peeling. EDIT I see how you calculate your view matrix, but I don't see any code calling it, so it's hard to say whether your camera is pointing in the right direction or not for this to work as expected. One other thing I notice is that your projection matrix sets the near plane to 0.1, but you're translating your geometry in increments of -0.003. If your camera is at the origin (i.e. your view matrix has a translation of <0,0,0>), then you won't see any layers until you get to the 34th layer. Also, with such small increments between the layers, you may end up with Z-fighting, as well. You might try hard-coding your loop to display just a single layer and make sure each layer is drawn correctly when drawn alone. Once you've verified that, you can figure out why drawing more than 1 layer doesn't work as expected. 

Cube maps - 6 images that show what the camera would see if it were looking forward, backward, left, right, up and down from the same position. Equirectangular - the 2nd image you show above is this type of photo. It must be exactly twice as wide as it is tall. The rows and columns of pixels then correspond to the latitude and longitude of the sphere you map it onto. Dual hemispherical - this is when you take a camera (real or virtual) and make an image with a 180° field of view. There are 2 - one for the front and one for the back. 

One way to think of it is that graphics cards often implement non-power-of-2 textures simply by padding them until they are a power of 2 in each direction. This makes most things "just work": tiling and hardware filtering, for example. The only thing that needs to change is the conversion from texture coordinates to image coordinates. If implemented like that, it's obvious how to do mipmapping: nothing changes. Even if you have a GPU that supports non-power-of-2 textures without padding, the mipmap levels would end up with "padding". e.g. a 3x3 texture would have a 2x2 texture as lod 1. 

This is the method available in the control panel for selecting what layout ClearType uses. Additionally, it seems that iOS and the Windows modern UI style de-emphasize subpixel antialiasing heavily, due to the prevalence of animations and screen rotations. As a result I expect the OS vendors to not spend a lot of effort trying to figure out the subpixel layout of every screen. 

Yes, lookup textures are still used. For example, pre-integrated BRDFs (for ambient lighting, say), or arbitrarily complicated curves baked down to a 1D texture, or a 3D lookup texture for color grading, or a noise texture instead of a PRNG in the shader. ALU is generally cheaper than a texture sample, true, but you still have a limited amount of ALU per frame. GPUs are good at latency hiding and small lookup textures are likely to be in the cache. If your function is complicated enough, it may still be worth using a lookup texture. 

Forward rendering is the process of computing a radiance value for a surface fragment directly from input geometry and lighting information. Deferred rendering splits that process into two steps: first producing a screen-space buffer containing material properties (a geometry buffer, or G-buffer) built by rasterizing the input geometry, and second producing a radiance value for each pixel by combining the G-buffer with lighting information. Deferred rendering is often presented as an optimization of forward rendering. One explanation is that lighting is fairly expensive and if you have any overdraw then you are lighting pixels that will never be seen on screen, whereas if you store material properties into a G-buffer and light afterwards, you are only lighting a pixel that will actually appear on-screen. Is this actually an advantage of deferred, given that you can also do a depth pre-pass and then do a forward rendering pass with depth test set to or or the equivalent? Deferred rendering also has the potential to schedule better on the GPU. Splitting one large warp/wavefront into a smaller geometry wavefront and then smaller lighting wavefronts later improves occupancy (more wavefronts in flight simultaneously). But you also end up with a lot more bandwidth use (writing out a large number of channels to the G-buffer, then reading them back in during lighting). Obviously the specifics here depend a lot on your GPU, but what are the general principles? Are there other practical performance considerations when deciding between forward and deferred rendering? (Assume that we can use variations of each technique if necessary: i.e. we can compare tiled forward to tiled deferred as well.)