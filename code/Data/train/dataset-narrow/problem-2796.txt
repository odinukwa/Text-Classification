$URL$ Edit: Sorry for the french reference but i haven't find any date on the english wikipedia. The translation is: 

The smallest increment you can do on a float in C# and have a significant value to be different from 0 is float.Epsilon. From MSDN Single.Epsilon 

EDIT I've made a huge mistake in my previous explanation. GetButtonDown() is fired only during one frame when the button is pressed as well for GetButtonUp() when the button is released. So to know if the button is hold or not, you have to check state changes when GetButtonDown() or GetButtonUp() are true. For example, you have a variable "buttonDown" wich is a bool(true, the button is down / false, the button is up). When GetButtonDown() return true, you can assume that the button is hold by user. So you assign true to "buttonDown" and you only change the value of "buttonDown" when GetButtonUp() return true. 

If you don't have a WP7 and only a desktop computer the best solution is to use the WP7 Emulator. You can find more informations here: $URL$ If you have installed the latest version of XNA, the WP7 emulator is installed at the same time. If you don't have it, you can get it from here: $URL$ You can also read this post on AppHub, someone else had had the same problem as you: $URL$ 

Input.GetButtonDown is fired during the frame when user pressed the button. The function return true only one time when user pressed it. From the Unity Script Reference: Input.GetButtonDown() 

_signPlane is an array of Vector3 using the following formula and applied for each plane once per frame when the frustum is updated: 

Now I have three models to draw, Model1, Model2 and Model3, each one use a different technique from the previous list. If I add those three models in the render queue I will end up with the following render queue : -> Set state A -> Draw Model1(Pass1) -> Draw Model2(Pass1) -> Draw Model3(Pass1) -> Set state C -> Draw Model1(Pass2) We can see that the Model1 will be drawn two times (two passes for the Technique A) but the draw call are separated by others draw call. I don't know how to resolve this. What will be a good solution to this problem? How can I design my render queue to reduce state changes as much as possible between draw calls. EDIT: After searching a little more, I have found this helpful article : $URL$ It seems to be an elegant way to sort object by depth and materials. And in the material id, I can had a pass id to sort same pass together that will allow to reduce state changes. Currently my renderer draw objects directly in the backbuffer without lights nor shadows, everything is flat. So I will just loop over my render queue and call draw methods. But in a future I will probably implement deferred rendering. I don't know a lot about it but I know that I have to render my objects in different render target to gather differents informations (Z-buffer, Normal buffer, G-Buffer, ...). I assume that for z-buffer and normal buffer I will use the same shader on every objects no matter which material they use. But for the G-buffer I will use the materials of each object so that means different shaders with different number of pass. If I use the solution in the previous link, if I have an object with a material that needs to do two or more passes, my render queue will have at least two times the object (one for each pass). But for Z-buffer or normal buffer isn't it useless to have the same object multiple times? What I see is that I will need to have at least two different render queue : one for G-buffer in which an object can be multiple times depending on the number of pass the material require and another render queue for Z-buffer or other buffer in which each objects are only one time. Am I right? 

They may have been referring to the actual act of updating a resource and not to the actual function call. In general, UpdateSubResource should be used for default resources that are not subject to frequent updates (i.e.: not every frame. In this case, it is more likely that the buffer could be to be copied to a temporary buffer accessible from the command buffer (due to race conditions for example). It will also allow you the update sub resources (in textures, for example). Map/Unmap should be used when a resource is going to be updated very frequently (i.e. every frame), such as some constant buffers. The most common case is when you are overwriting the whole buffer with WriteDiscard. There's an nVidia presentation where they reccommend this practice. 

In code, by setting the GameObject's `renderer.material.color' to a different one Through the GUI by selecting the GameObject and in the inspector choosing the desired color in the main color field of the assigned material component 

HCI researcher here. I am not aware of any "standard" values that work for all scenarios. If you search on scholar.google.com for "user reaction time" or "target selection time" you should be able to find more. Although these works might investigate different setups than yours. For example target selection with multitouch input or eye-trackers (the "dwell time" is a known interaction technique in gaze-based systems). However, this is the typical thing that you could investigate in a small qualitative user study. Try different conditions (ie 100ms, 200ms, 500ms, 1000ms) in alternating order and have users rate which condition they liked the most on a scale of 1 to 5 or 1 to 7. Users should repeat each condition a number of times, say 5. You might not be able to publish these results :D but if you can spot a trend, then you'll have your answer. If finding at least 10 users is something that is feasible for you. 

An easy way to do this if your tiles are objects is to set a bool for whether a tile has been visited, which you trigger when the player coords match a "visitable" tile's. Then at render time you can go through your tileList and make any bulk actions to all tiles which have been visited or not. 

GoActionHandler would be a of ActionHandler type, and I would put in ActionHandler or the general messages/actions/reactions, and then in GoActionHandler I'd elaborate on the possible "Go" actions. But, one thing at a time... The code above does not compile, as ActionHandler is not a "type". Considering that for now I am just focusing on having a (scalable!) program that reacts differently based on the word entered, could someone help me by showing me real code? I've found online people making examples in pseudocode, but I'm to big of a noob to then know how to execute what they are talking about, even though I understand the logic. 

I have a little 2D game made with Allegro, and I want to introduce strings during gameplay. For example, I've just made it so a key is needed to open a door, so I'd need some tutorial text to appear when the player collides with the door that tells them they need the key first. This is just a learning experience for me, really - the key is pretty visible but I would like to learn how to handle this sort of messages especially for tutorial purposes in a bigger, better game. Right now, during gameplay, I have running (which handles player movement, collisions etc) and , which obviously runs after it. To introduce these messages, what I have successfully done so far is to add a global to which I messages whenever appropriate. My then goes like so: 

Please ignore for the moment the crude division by 100, I wanted to get the rotation right before optimising that part. That code causes the behaviour in the first picture. I tried to rotate the UnitX axis used in the second quaternion by doing and then using in place of in . I have looked all over the web but so far none of the solutions seemed to work for me. What am I doing wrong? EDIT every time the mouse moves, I am updating lastPointerPosition with the current one (after updating the rotation matrix); when the mouse pointer is released, totalYaw/totalPitch are restored to 0. 

You can check the property of the Device object. If it returns then it means that the hardware supports SM5. 

I am implementing the same functionalities. Here's how I have architectured my engine: Entities are not just renderable objects. In my ECS cameras and lights are also entities. 

To render just the edges of an arbitrary polygon you could use a Solid Wireframe technique. It uses barycentric coordinates to determine which edges to draw. For example you might have a triangle whose barycentric coordinates are (for each vertex) . Put it simply, when these values are interpolated the further any of the values are close to zero, then the further you are to an edge. You can use this to shade the edges differently than the inside of a the polygon. I have used this technique to draw the sphere and hexagon tiles you see in this video. As the others have suggested, you can add a glow/bloom effect to improve the overall effect. 

In the second picture, instead of dragging, I am releasing the mouse button and performing a second drag movement. So the local Y-axis is not rotated as in the first picture. This is the code I am using: 

I've just tried to draw it normally and it appears at the center of the scene. After that, i don't know what to do to make it appears at the bottom left of the screen regardless of where the camera looks, as if that was part of the UI. 3DSMax has the same thing at the bottom left of each frame. 

But it doesn't work at all and I don't find what's going wrong. Objects in my scene seems to appear and disappear in a strange way, sometimes appears when I'm not watching them or disappears when frustum faces them (I look at draw calls to determine if the isVisible test returns true or false). Here is my code: The Axis-aligned bounding box 

I'm creating an application which allows the user to manipulate 3D models. I would like to draw 3 axis representing the world coordinate system in 3D in order to give the user an idea where he is. These axis are represented by a 3D model. Now I want these axis to be always shown at the bottom left of the screen. I know how to draw them but not how to project them at specific coordinates on the screen. How can i achieve this? EDIT: I use XNA. The axes have been created with 3DSMax, each axes is a cylinder surmounted by a cone. When I load the 3DSMax file, I get an instance of the Model class. 

But I don't know if I can deferred pass rendering from a same technique instead of using them one after the other as in the previous example. Here is an example of what will probably happen with the solution I try to build: 

It seems that the UDK only support iOS for the moment and so there is no way to make it works on Android: $URL$ As you say Dungeon Defenders was built with Unreal Engine and not with the UDK. According to Epic Game some problems such as performance and app size issues on Android prevent them to release an Android version of the UDK. 

Is it possible to create a Direct2D PathGeometry object from a XAML string (or from any other vector format) or do I have to create such a parser myself? 

Is this a problem in the coordinates or a mipmapping issue? Edit: the GeoSphere is generated using SharpDX's Toolkit code. My SamplerState is set to Clamp. Further, I am using a normal mapping shader. Could it be that the tangent of the duplicated vertices needs to be altered? I am using a Vector4 element for it, where w is the handedness. If so, how should it be modified? Edit2: this is the code used to duplicate the vertices along the seam 

Texture Coordinates are usually expressed in the range between . Each (textured) vertex will have these coordinates. These coordinates are mapped to texels in the actual texture. is the top left corner, the bottom-right corner. When the coordinates are in a range that is multiple of 1, the texture will repeat itself. For example, for a pixel halfway between having and v1 having , the rasterizer will interpolate the texture coordinates value, resulting in . The texture lookup would then sample the texel in middle of the texture. 

However it seems that over time the sphere tends to become distorted and then shrink back to its original size. Is this due to the simplicity of the integration or am I doing some simple mistake? 

That error indicates that XNA expect your custom vertex type to implement as shown here. Otherwise you need to use the overload that accepts an explicit object. 

I am implementing a mouse rotation which works by accumulating the X-Y delta between frames to yaw and pitch rotation angles. The problem is that I wish rotations to be independent of each other. But as you can see from the picture: 

I then call this in my after I've drawn everything else, to ensure that the message appears on top. Now, again, this does work - the message displays for 2 seconds at render time once it's added to the . However, this solution doesn't really seem particularly neat to me, since as far as I know there are standardised ways of dealing with these sort of events - I just don't know what they are! So my question is - is my current method efficient enough? Can it be improved, in Allegro (or generally, in logic!) and if so, how? Thank you in advance! EDIT: I've modified the code since posting the question, so it should now behave like expected - showing a message for 3 seconds. It does so, but when I have two strings in the vector at the same time they overlap - I thought the timer would work as "pause" between each iteration but I guess not... If someone could point me in the right direction for that too, it'd be much appreciated :) 

Apologies if this has been asked before, or if it is a bit too simple for this forum. I'm new to C++ and as one of my first projects I have decided to try a textual adventure. Step 1: the text parser. Note that I am aware that there are libraries out there that will make this for you, but I want to code it from scratch to learn what is going on :) So! I've gotten to the point where I add the user's input to a , so that I can handle each word individually. However, I cannot for the life of me figure out a "handler" that does not hard-code everything. I want to create a through which to associate the , for example "Go", to the relative action handler. Something like this: 

I am refactoring some parts of the game engine I am working on. This engine is made in C# with XNA. The part I have trouble with is shader/material and the render queue. In the new version I will give ability to indicate differents render states per pass within a technique (blend state, rasterizer state, ...). In my engine, all renderables have a material and each material is associated to one technique (thus to one or more passes). Later, when the game will be running, I will add renderable objects to the render queue and then sort it based on the pass used by each material. What I want to achieve is to reduce states changes between draw calls by first grouping same passes together and then passes with nearly identical states to be as closed as possible to minimize changes. But this solution seems to be problematic when it comes to use technique with more than one pass with completely different states. Passes within a same technique can be completely separated in the render queue and not rendered one after the other. If I say that it's because I have always see multi-passes technique render in a loop and all passes rendered immediately like that : 

I'm trying to implement an axis-aligned bounding box with center/half-size instead of min/max. And I have some problems when it comes to create a method to detect if the aabb is visible or not. I try to do the same as on this website at "Method 5": $URL$ I'm trying to implement this formula: 

When you try to call the Effect property of your ModelMeshPart you get an Effect instance and Effect is the base class for all effect. So you can't cast an Effect instance to a BasicEffect because BasicEffect inherits from Effect. Furthermore you don't need to cast it to a BasicEffect. Your line: