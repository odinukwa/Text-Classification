As an introductory reference to the notion of a complexity operator (and demonstrating some applications of the idea), the best I have found so far is 

The quantum Cheshire Cat experiments appear to require postselection even to exhibit. Of course, postselection is itself a computational resource (and an extremely powerful one!) for "bounded" error quantum computation, and one whose power is characterised. To wit: the class PostBQP of decision problems decideable with bounded error using poly-size quantum circuits, conditioning on some single bit having the value 1 (provided that this occurs with non-zero probability) is precisely the class PP. (Recall also that PP contains NP as a subset.) Perhaps one might ask whether quantum Cheshire cats might somehow represent a useful idiom in the design of PP algorithms for difficult problems. Alternatively, perhaps quantum Cheshire cats correspond to some intriguing feature of quantum mechanics that can be described without resorting to (anything equivalent to) postselection, and which has implications for quantum computation. However, it isn't obvious at the moment that either of these are the case. 

For any gate set $\mathcal G$ which contains the constant gate $G(x,y) = 1$, we may simply build a circuit $C$ using that gate alone, in which case $C$ accepts any $x$. OR and NAND. For any gate set $\mathcal G$ which contains $\OR$: if all other gates $G \in \mathcal G$ satisfy $G(x,y) \implies \OR(x,y)$, then there is no advantage to choosing any other gate but $\OR$ in building the circuit $C$. A circuit of only $\OR$ gates accepts any string except $x \in 0^\ast$. Otherwise, there exists a gate $G \in \mathcal G$ such that $\{G, \OR\}$ is tautologous. So any instance of OPSAT with $\OR \in \mathcal G$ is easy; and similar remarks apply for $\NAND \in \mathcal G$. Implication-like gates. Consider the gate $G(x,y) = \neg x \ou y$, which only outputs zero if $(x,y) = (1,0)$. For what follows, a similar analysis will apply for the gate $G'(x,y) = x \ou \neg y$. Consider any string $x \in \{0,1\}^n$. If $x$ ends in $0$, decompose $x$ into substrings of the form $w_j = 1^\ast 0$; on each such $w_j$, we recursively apply $G$ from the right to the left, which yields output $0$ for each $w_j$. (For a substring of length 1, we use the trivial circuit, i.e. leave that input alone.) Similarly, if $x$ ends in $1$, decompose $x$ into substrings of the form $w_j =0^\ast 1$, and recursively apply $G$ from left to right on each $w_j$, which yields the output $1$ for each $w_j$. Thus we may reduce the problem to building circuits which are satisfied either by $0^m$ or $1^m$, where $m$ is the number of substrings $1^\ast0$ or $0^\ast 1$. For $m \geqslant 2$, we may accept either using $G$ gates by recursively applying $G$ from left to right. This just leaves the case $m = 1$, for which the problematic case are inputs $x \in 1^\ast 0$. For $x= 1^\ast 0$, any circuit consisting only of $G$ gates will only yield shorter strings of the form $1^\ast 0$, ultimately yielding the single-bit string $0$: so that no circuit of $G$ gates can be satisfied by this input. If there is also a gate $H \in \mathcal G$ for which $H(1,0) = 1$, then $\{G,H\}$ is tautologous; or, if there is a gate $H \in \mathcal G$ for which $H(1,1) = 0$, we may reduce strings of the form $11^\ast 0$ to strings of the form $(1^\ast 0)^\ast$, by applying $H$ to the first two bits of $x$. Otherwise, no circuit can be constructed which accepts $x \in 1^\ast 0$. Thus, for any gate-set $\mathcal G$ which contains an implication-like gate, OPSAT is easy. Negations of projections. Consider the gates $\neg \pi_1(x,y) = \neg x$ and $\neg \pi_2(x,y) = \neg y$. We consider $\neg \pi_1$, the analysis with $\neg \pi_2$ being similar. On its own, $\neg \pi_1$ can accept any string in $0(0|1)^{n-1}$ for $n \geqslant 2$ by reducing the final $n-1$ bits to a single bit, and then applying $\neg \pi_1$; and it can similarly accept $1(0|1)^{n-1}$ for $n \geqslant 3$ by reducing the final $n-2$ bits to a single bit, and then applying the circuit $\neg \pi_1(\neg \pi_1(x_1, x_2),x_3)$. The only inputs that $\neg \pi_1$ circuits cannot accept are then $10$ or $11$; determining whether any supplemental gate accepts these is trivial. Thus, OPSAT is easy for the negations of projections. PARITY and EQUALITY. Consider the gate $\PARITY(x,y) = (x \ou \neg y) \et (\neg x \ou y)$. The gate set $\mathcal G = \{\PARITY\}$ obviously can only be satisfied precisely by strings $x \in \{0,1\}^n$ with an odd number of 1s; we consider the benefit of adding any other gate. 

For a stack operation $(s,t) \in \Gamma^{\{0,1\}}\times\Gamma^{\{0,1\}}$, there are the three cases $(s,t)=(a,\epsilon)$, $(s,t)=(\epsilon,a)$, and $(s,t)=(a,b)$. The stack operation $(a,\epsilon)$ gets reversed to $(\epsilon,a)$ as follows 

To understand the hole in the correctness proof, András Salamon comment about Weisfeiler-Lehman was very helpful, as were the explanations from 

Here $\epsilon$ is the empty string, $\Gamma^{\{0,2\}}=\{\epsilon\}\cup\Gamma\cup(\Gamma\times\Gamma)$ and $\Gamma^{\{0,1\}}=\{\epsilon\}\cup\Gamma$. This notation is used because it is similar to $\Gamma^*$, which is used in many definitions for pushdown automata. Diagramed verification of reversal for (non)advancing input and stack operations An advancing input operation with $b\in\Sigma\subset\Sigma\cup\{\epsilon\}$ gets reversed as follows 

This question is not research level, even so showing the equivalence of closure under kleene-star to the well known open problem L=NL was a nice challenge. Obviously $S\cap T$ and $S.T$ are in DLOGSPACE, if $S$ and $T$ are in DLOGSPACE. 

Yes, there is a known relativization barrier. It's given by $A:=TQBF$, because Emil Jeřábek (see comments) is right: the statement that $TQBF$ is $PSPACE$-complete under logspace many-one reductions is well known. 

No, the conjectures rather go to the opposite site, namely that GI is in P. Since GI is in NP, it won't be possilbe to refute this type of conjecture anytime soon. 

Reversibility for Turing machines A machine with more than one stack is equivalent to a Turing machine, and stack operations can easily be reversed. The motivation at the beginning also suggests that reversal (of a Turing machine) should not be difficult. A Turing machine with a typical instruction set is not so great for reversal, because the symbol under the head can influence whether the tape will move left or right. But if the instruction set is modified appropriately (without reducing the computational power of the machine), then reversal is nearly trivial again. A reversal can also be constructed without modifying the instruction set, but it is not canonical and a bit ugly. It might seem that the existence of a reversal is just as difficult to decide as many other question pertaining to Turing machines, but a reversal is a local construction and the difficult questions often have a global flavor, so pessimism would probably be unjustified here. The urge to switch to equivalent instruction sets (easier to reverse) shows that these questions are less obvious than they first appear. A more subtle switch happened in this post before, when total functions and stochastic matrices were replaced by partial functions and substochastic matrices. This switch is not strictly necessary, but the reversal is ugly otherwise. The switch to the substochastic matrices was actually the point where it became obvious that reversibility is not so trivial after all, and that one should write down details (as done above) instead of taking just a high level perspective (as presented in the motivation at the beginning). The questions raised by Niel de Beaudrap also contributed to the realization that the high level perspective is slightly shaky. Conclusion Non-deterministic machines allow a finite number of deterministic transitions at each step. For probabilistic machines, these transitions additionally have a probability. This post conveys a different perspective on non-determinism and randomness. Ignoring global acceptance conditions, it focuses on local reversibility (as a local symmetry) instead. Because randomness preserves some local symmetries which are not preserved by determinism, this perspective reveals non-trivial differences between non-deterministic and probabilistic machines. 

I imagine that some of the choices above may be arbitrarily fixed without losing any generality. What I am interested in is a reference for the correspondence, or at least a description of how to modify the description above to obtain the standard one. 

We may therefore suppose that the cost of performing arithmetic on the amplitudes does not compound with the number of amplitudes being transformed in parallel, and that a rational linear combination (with coefficients $a_j$) of rational numbers $x_j$ may be performed effectively at a cost depending only on the complexity of specifying the coefficients $a_j$ themselves. 

I think that a complexity class for decision problems, taking quantum states as input is likely to have a fragile definition. For promise problems, either the definition will be sensitive to numerical choices, or it will essentially solve classical decision/promise problems encoded in some efficiently-decodable basis of quantum states. Tsuyoshi's answer describes what I would consider the correct generalization of function problems. If what you want is a generalization of decision problems, you could specialize to families of channels $\Phi_n: \mathrm L(\mathcal H_2^{\otimes n}) \to \mathrm L(\mathcal H_2)$ from n-qubit states to single qubit states. Of course, a quantum circuit is a perfectly good channel; if we are going to speak of performing specific channels which are comp­u­ta­tion­ally bounded, we may as well just speak of uniform quantum circuit families (or for that matter, any uniform way of implementing a CPTP map). For good measure, the circuit should end with a standard basis measurement, if we want to retain the semantics of deciding something with bounded probability. Suppose we define a "quantum language" $\mathcal L$ to consist of subsets of the density operators on n qubits, ranging over all n. In defining some quantum-input, bounded error quantum polytime class QBQP, we cannot hope to perform a crisp division between states in the language and states outside of the language, and at the same time bound the probability of acceptance for NO instances away from those of YES instances as we do for BQP and BPP; any state $\rho'$ which is very close to a state $\rho \in \mathcal L$ (either having high overlap or low trace-distance) is going to yield an acceptance probability very close to that of $\rho$, whether or not $\rho'$ is in the $\mathcal L$. So it seems to me that a class QBQP would only contain "quantum promise problems", in which the input is promised to belong to a class of YES instances or of NO instances which do not exhaust the space of all possible states. (A quantum-language decision class with unbounded error, a sort of QPQP class, may not be restricted to a promise-problem formulation to be meaningful.) In addition to this, it seems likely — because the no-cloning theorem prevents you from making copies of your input state — that you can't amplify the success probability. So any definition of QBQP which involved some constant success probability bounded away from 1/2 would probably depend critically on just what that probability was. To avoid this, as with the function classes FBPP and FBQP, we would in practise probably define QBQP so that for an input state in $\mathcal L$ in QBQP, any state in $\mathcal L$ yields the outcome 1 (from the final measurement) with probability 1 − o(1), that is a probability which is closer to certainty as the input size grows — and similarly, the probability of rejection of any state that the decision routine is able to reject should also converge to zero. The quantum-promise problems that a QBQP circuit (for inputs of size n) would be able to distinguish would then be 

A naive idea for proving ALogTime != PH: The Boolean formula value problem is complete for ALogTime under deterministic log time reductions. Hence if ALogTime = PH, then PH = coNP = ALogTime, and hence the Boolean formula value problem would be complete under deterministic log time reductions for coNP. Hence there would be a deterministic log time reduction from the tautology problem to the Boolean formula value problem. The deterministic log time reductions should be harmless, they cannot contribute much to the solution of the tautology problem. They are just a nice formalisation what it means that a reduction can only work very locally. Hence the remaining task is to understand why the tautology problem cannot be turned into a Boolean formula value problem by very local reductions. I still don't see how to do that, but at least the remaining task is very clear, so that I have at least a chance to understand why it is hard (or not). 

Have there been any attempts to show that Kolmogorov randomness would be sufficient for RP? Would the probability used in the statement "If the correct answer is YES, then it (the probabilistic Turing machine) returns YES with probability ..." be always well defined in that case? Or would there only be upper and lower bounds for that probability? Or would there only always be some probabilistic Turing machine, for which the probabilities would be well defined (or at least the lower bound which should be bigger than 1/2)? The class RP here is relatively arbitrary, and one could also ask this question for weaker notions of (pseudo-)randomness than Kolmogorov randomness. But Kolmogorov randomness seems to be a good starting point. 

The manufacturers of the computing hardware themselves are their own customers, and use supercomputing to approximatively solve a number of well-know and more obscure NP-hard tasks. One of the oldest and best-known is place-and-route, a short overview of electronic design automation reveals many more NP-hard tasks. Often the employed algorithms are true NP algorithms, whose success (or approximation quality) depend crucially on additional hints directly and indirectly provided. A typical indirect hint is the hierarchical structure used for compression in the input file formats. A direct hint might be given by restricting the range of some parameter which is optimized, but the need and possibility for giving hints can go far beyond that. Rather than dismissing those hint as cheating and semi-automation, it can make sense to explicitly acknowledge the need for those hints. Nested Words and Visibly Pushdown Languages used for Program Analysis are an example of an explicit theoretical framework showing which hints are needed for shifting some problems to more tractable complexity classes. 

Thus, OPSAT is easy for any $\mathcal G$ containing $\PARITY$.A similar analysis applies for the $\EQUAL$ gate as for the $\PARITY$ gate: because $\EQUAL(x,y) = \neg \PARITY(x,y) = \neg \PARITY(\neg x, \neg y)$, circuits of $\EQUAL$ gates essentially count the parity of the number of $0$s in the input. We may then reduce the analysis for $\EQUAL$ to that of $\PARITY$ by exchanging $0$ and $1$. Projection gates. The gates $\pi_1(x,y) = x$ and $\pi_2(x,y) = y$, taken on their own, can only build circuits which accept strings starting or ending in $1$, respectively. Consider the effect of augmenting the gate $\pi_1$ with any other gate (a similar analysis holds for $\pi_2$): 

Scott Aaronson was often fond of pointing out (and probably still is fond of pointing out, assuming he hasn't gotten tired of doing so) that physical processes do not always find the global minimum of an energy landscape. In particular, if you were to formulate an instance of an NP-complete optmization problem as an energy-minimisation problem for a physical system, there is no reason — either theoretical or empirical — to believe that such a physical system will "relax" after some time to a solution of the problem (i.e.  an energy configuration which is a global minimum). It will more likely relax to a local minumum: one for which slightly different configurations require more energy, but where a substantially different configuration may have less energy. So, while proving NP ⊆ BQP would be a triumph of the first order — for all complexity theorists, not just for quantum computation theorists — it would suggest that there is a whole new theory of "physical" models of computation waiting to be discovered. Why? Well, models of computation can be construed as models of physics (albeit highly specialized ones): namely, what computational resources are physically reasonable. One of the 'slogans' of quantum computation is that † — so unless you can simulate quantum mechanics on a classical computer, what you can physically compute efficiently is almost certainly more powerful than P. And yet, we have evidence that it is less powerful than NP; so it would have to be less powerful than BQP as well, if it so happened that NP ⊆ BQP. So, a proof of NP ⊆ BQP would present us with a trilemma: either 

Here $P(Q)$ is the power set of $Q$ and $ssM(Q)$ is the space of substochatic matrices on $Q$. A right substochastic matrix is a nonnegative real matrix, with each row summing to at most 1. There are many different reasonable acceptance conditions The transitions are only one part of a machine, initial and final states, possible output and acceptance conditions are also important. However, there are only very few non-eqivalent acceptance conditions for deterministic machines, a number of reasonable acceptance conditions for non-deterministic machines (NP, coNP, #P, ...), and many possible acceptance conditions for probabilistic machines. Hence this answer focuses primarily on the transitions. Reversibility is non-trivial for probabilistic machines A partial function is reversible iff it is injective. A relation is always reversible in a certain sense, by taking the opposite relation (i.e. reversing the direction of the arrows). For a substochastic matrix, taking the transposed matrix is analogous to taking the opposite relation. In general, the transposed matrix is not a substochastic matrix. If it is, then the matrix is said to be doubly substochastic. In general $P P^T P\neq P$, even for a doubly substochastic matrix $P$, so one can wonder whether this is a reasonable notion of reversibility at all. It is reasonable, because the probability to reach state $B$ from state $A$ in $k$ forward steps is identical to the probability to reach state $A$ from state $B$ in $k$ backward steps. Each path from A to B has the same probability forward and backward. If suitable acceptance conditions (and other boundary conditions) are selected, then doubly substochastic matrices are an appropriate notion of reversibility for probabilistic machines. Reversibility is tricky even for non-deterministic machines Just like in general $P P^T P\neq P$, in general $R\circ R^{op}\circ R \neq R$ for a binary relation $R$. If $R$ describes a partial function, then $R\circ R^{op}\circ R = R$ and $R^{op}\circ R\circ R^{op} = R^{op}$. Even if relations $P$ and $Q$ should be strictly reversible in this sense, this doesn't imply that $P\circ Q$ will be strictly reversible too. So let's ignore strict reversibility now (even so it feels interesting), and focus on reversal by taking the opposite relation. A similar explanation like for the probabilistic case shows that this reversal works fine if suitable acceptance conditions are used. These considerations also make sense for pushdown automata This post suggests that one motivation for non-determinism is to remove that asymmetry between forward steps and backward steps. Is this symmetry of non-determinism limited to finite automata? Here are corresponding symmetric definitions for pushdown automata