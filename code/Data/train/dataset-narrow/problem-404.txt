I can never get it to do an index seek on the DispatchLink table. It always does a full index scan. That is fine with a few records, but when you have 50000 in that table it scans 50000 records in the index according to the query plan. It is because there are 'ands' and 'ors' in the join clause, but I can't get my head around why SQL can't do a couple of index seeks instead, one for the left side of the 'or', and one for the right side of the 'or'. I would like an explanation for this, not a suggestion to make the query faster unless that can be done without adjusting the query. The reason is that I am using the above query as a merge replication join filter, so I cannot just add in another type of query unfortunately. UPDATE: For instance these are the types of indexes I have been adding, 

When I browse to the address $URL$ it works fine when I specify the same login credentials as I did for the subscription. If we have a think about some of the possible causes, proxy server, we're not using that. URL is fine (as far as I can tell). Login credentials are OK as far as I can tell too. The other thing I noticed is that I can see the https traffic in fiddler when I put the ?diag address in my browser, but when I start the merge agent I don't see the traffic in fiddler. UPDATE: I used wireshark to look at the traffic going between my subscription PC, and the IIS server. When I run the subscription agent I get this traffic, 

So in this case if I wrote out a select statement to apply my filtering for petermc it would look like this, 

I have a SQL express database and was wondering what options I have for securing it? More specifically it is a replicated database using merge replication. I want to stop a user with a replicated copy of the database opening the database, modifying it and running queries against it. UPDATE: Just wondering though, what happens if I have a database which is secured, but I then take it offline, copy the database, and attach it to a different database server. Does the security go with it, and can I not break into it like that? 

So the first question there. If a person syncs every day, and they are pulling back about the same amount of changes specifically why would we have to regenerate the snapshot? I'm not sure why a sync such as this would become slower? Is it a case of knowing when it can clean up metadata? The user with the oldest snapshot has just regenerated a new snapshot, so it means we can clean up the metadata up to the next oldest snapshot? This leads to the second question. If I have a user who I have given a merge replication solution to as a demo. It turns out that they never use the system and have only synced it once at the start to test it out. They may have even removed it from their computer. If their shapshot job has been turned off and they never sync does it mean we get stuck with a whole bunch of metadata that replication cannot clean up? Does replication at some point conclude that that person is not using the system and block them out? The reason I ask this is I am using anonymous subscribers. When we have normal subscribers and we delete the subscription the server is connected to directly and the subscription is removed from the publication. This does not happen for web sync. 

The point of the DispatchLink table is to link two Dispatch records together. By the way I am using a composite primary key on my dispatch table because of legacy, so I cannot change that without a lot of pain. Also the link table may not be the correct way to do it? But again legacy. So my question, if I run this query 

UserRegion Table, (think of this as a security table, which region is a particular user allowed to see) 

And that part of my filter is fixed, I cannot edit [dbo].[berm] and replace it with a view. So what I think you are suggesting is that I can change the above to, 

I set up merge replication web sync using a pull subscription. When I try to start the merge agent at the client side I get this error, 

The last item there RST is hilighted as red, and does not appear when I browse to the ?diag page. Perhaps there is a problem with the certificate I have. I created a self signed security certificate and installed it on my PC. When I browse to the ?diag page I don't get a security certificate error. 

I'm not convinced about the publication article limitation of 256. If we look here, $URL$ It is talking about a particular query that causes issues on SQL 2005 when you have more than 256 articles. First I ran the stored procedure sp_MSmakesystableviews. It then generated a view called MSmerge_cont4F93AE6035D14E46ADA56D87F66E8962_90. If I take the underlying query in the view it has more than 256 tables. I run that query against a SQL 2005 database and I get the error, 

If I take that same query and run it on SQL 2012 it works fine. My belief is that the MS documentation is outdated, and unless I find other issues with running merge replication with more than 256 articles I would have to take it that this limitation does not exist anymore from SQL2008 upward. 

There doesn't seem to be a good answer to this question. When you have a secondary database for log shipping it remains in the 'Standby / Read-only' mode, and you cannot do much about that. At some stage I will have to set up a second secondary database, leave it running for a week and then break the log shipping to test that it has worked. Or else I will just have to break the current log shipping and recreate it. Looking at the files in the Filestream proves that data is being log shipped correctly. 

I am using identity columns in my merge replication solution to give people an id they can tell us over the phone (i.e. to support staff). A rowguid column would be too hard for this. I have noticed that int identity columns are replicated no trouble, and each client subscription has a range of identity values it can use. i.e. select * from MSmerge_identity_range 

I am using merge replication on SQL 2012. I have a very large database with a large number of tables. The subscribers are using netbooks. I have a filtering scheme that controls which tables end up being empty at the subscriber. We are saying this for instance 'the user bob is not interested in the street light table so he will receive that table, but with no data in it'. That filtering is working fine. What I am interested in is whether you can go further and not even sent the streetlight table to that user. The reason I ask is running the scripts to create the tables and add the replication triggers is quite slow on the netbook when you are talking about a large number of tables. Here is the breakdown of my sync to the netbook, 

So the only other option mentioned is a view, but I cannot get the view to work either. The first problem is that if I replicate a view it just gets replicated as a view and the underlying table needs to be there. But Simon I read your other post and that is not what you are saying. I think what you are saying is that I should be able to select a table in my replication publication, and then filter it based on a view like this, 

What is occurring here is not what it appeared to be. The stored procedure to write into my top level tables were using transactions. It was the transactions that were causing heaps of extra locks. It worked fine once those transactions were removed. 

So are you telling me that every user of my database has to have sysadmin access if they are going to be doing deletes? This seems wrong. Is there a way to turn this tracing off or an alternative way to remove this requirement? 

I am using merge replication with SQL 2012. I am trying to rename a published table, and rename a published field. These operations are not permitted in merge replication. However I can think of an action plan if these things are needed. For instance renaming a field would include, 

It becomes apparent that these records do not represent actual changes to the underlying records, unless I am missing something? Why are those records there? Is it because of partitioning, or column level tracking? I am using column level tracking, but am wondering whether I need to revisit this option? 

Thanks RThomas it looks all good. I found a table which had records added previously using BCP. After BCP had been run the 'sp_addtabletocontents' stored procedure had been used. I joined that table to the MSmerge_current_partition_mappings table, and found that some items in my table didn't have a corresponding MSmerge_current_partition_mappings record. It seemed fishy, but the reason was very simple. The records in the table corresponded to a partition that no longer existed. I added the partition back in and closing the publications properties dialog took a while (it would have been adding the new MSmerge_current_partition_mappings records). After that was done I could see that all records in my table had a corresponding MSmerge_current_partition_mappings record. It all checks out. 

I am using merge replication with SQL 2012. I have a number of articles in my publication and will have to come up with quite an elaborate set of filters. If I add a filter on my article it can contain a sub query, but I noticed it doesn't pick up on changes properly. For instance Table1, and Table2. Filter on Table1 is 

I am using merge replication with SQL 2012. I have a table, and it would be useful to propogate that table to the subscribers, but just the table structure, not the actual data. The data in the table is populated per session, i.e. when you log into the software data is added into the table, and then deleted when you exit the software. What this means is that the data in the table shouldn't be replicated from publication to subscriber, or from subscriber to publication. But it would be good for the table structure to be replicated so that we don't have to run a script manually at the subscribers. 

I am using merge replication in SQL 2012. When I bulk insert rows into a table, and run this stored procedure to add references into the merge tracking table, sp_addtabletocontents Is that missing out on precomputing the partitions using the precomputed partition optimisations? 

It pivots on the DRC, but not on DISP as well. The result is like this (0, and 1 is the DRC value). I understand why that is, but how do I pivot on the DISP as well? 

I am in the process of defining filters on a merge replication publication for our database. The problem I am coming up against is that merge replication has these rules, I don't like to call them limitations because I can understand the purpose. 1) When creating an article filter you cannot include a subquery, or at least you shouldn't. If you do then it will appear to work the first time you sync, but if something changes in the subquery table the filter will not be re-evaluated. 2) When using a join filter you can only join two tables together. The problem I am coming across is that the relationships in our database are more complicated than that. For instance here is one of our relationships, 

I am designing a system which will have one SQL server 2012 central database. Each client will user merge replication to take an offline copy of their data to edit on the road. If I add a publication I can add filters to that publication. The thing I can't work out though is that is only one set of filters. For each client I have they will have a different set of filters to filter data that relates to them only. This is quite important from a security point of view. How is this achieved with only one publication? 

I am using merge replication in SQL 2012 with web sync. When I create my subscription I am getting a timeout. The error message is not clear about cause of the timeout, but what I do know is if I increase the QueryTimeout parameter on the merge agent from default of 300 to 1200 it works. How can I find the SQL that is timing out? I have used SQL profiler, but it isn't clear which query is timing out. In SQL profiler I selected the option 'Attention' under 'Errors and Warnings', but didn't see anything about a timeout. 

I mean how much of an impact will this have on performance? The key lookup has an operator cost of 0.295969 (99%), but what does that really mean? How do you know that you need the second index there, and at what point does it become the case that you are trying to add too many indexes and it is not worth it? It seems to me that some queries can include index scans, key lookups, and still seem to perform very fast. 

So I am doing two things there. First filtering based on region to reduce a very large table into a smaller more manageable subset. The second is specifying whether the subscriber is interested in the StreetLight table at all. If they are not then the subscriber should have an empty StreetLight table. This part is important because the publication will have a large number of tables in it, so it doesn't make sense to include stuff that the subscriber is not going to use. Our biggest database has millions of records in some tables, and there will be a moderate amount of updates of these records too. We must get this filtering correct. The option to not filter those tables is not feasible. 

The contactNo field is an INT NOT NULL in both source and destination, so I cannot see why translating the value using case then casting it to INT is not working? My BCP import looks like this (minus the server, user, password etc), 

In merge replication if I run sp_removedbreplication will that remove rowguids or not? Will it take into account the preserve_rowguidcol values in the sysmergearticles table? 

I am using merge replication with SQL 2012. I am writing a custom business logic handler, and would like to know if there is a way to get a session id? For instance if the custom business logic handler catches an insert into a specific table, and then the custom business logic handler catches an insert into another table, how can you tell if they are part of the same session? Also by session_id I am referring to something like the session_id which is in the MSMerge_sessions table. 

I am using SQL 2012 with merge replication. Our setup is quite complicated and thus it seems running the publication with precompiled partitions is not going to work. The merge agent is slow to do the first sync because of calling the sp_mssetupbelongs stored procedure. I am wondering if it is viable to run the publication without precompiled partitions? The first sync is faster without precompiled partitions, but subsequent syncs are a bit slower than before. Just a few changes will result in a sync of about 5 minutes. I also tried an option called 'Optimize Partitions' and would like to know what this does? Deleting a record at the publication end did not propagate to the subscribers. The subsequent syncs were faster though. Why do deletes not propagate, and is there a workaround for this? 

So it orders the items based on removing the leading underscore characters. It can't just blindly remove the underscore characters though because the underscores might be in the middle of the items. For instance this does not work,