[Update] In practice, the challenges associated with each of these three things are different: it doesn't necessarily make sense to talk about any of them as special cases of the others. However, as Jake points out below, there are (slightly restricted) libraries of functional reactive programming that are implemented on top of self-adjusting computation primitives. Similarly, it might be possible to effectively implement something like partial evaluation on top of self-adjusting computation, since SAC makes the very few a priori assumptions about its input data. 

The keywords you want to look into are "AC Unification," "ACU Unification," and "Associative-commutative unification." People have been looking into this for a long time (see this 1987 paper, for example). 

Your observation was used in by the Sewell et al. in the paper "Ott: Effective Tool Support for the Working Semanticist" (PDF), in which they observed that, if all you really and truly ever want to do is formalize the operational semantics of core functional languages, then you don't, in fact, need alpha-conversion in the formal definition of the programming language. The main issue is that you have to catch yourself before you do something that causes this to stop working. The most obvious example is the one Gilles mentions, reducing under binders; the paper discusses some others. 

The simplest answer is given by the fact that typed lambda calculi correspond to logics (simply typed lambda calculus -> predicate logic; system f -> second-order logic) and consistent logics cannot prove their own consistency. So let's say that you have natural numbers (or a Church encoding of natural numbers) in your typed lambda calculus. It's possible to do a GÃ¶del numbering that assigns every term in System F to a unique natural number. Then, there is a function $f$ that takes any natural number (that corresponds to a well-typed term in System F) to another natural number (that corresponds to the normal form of that well-typed System F term) and does something else for any natural number that doesn't correspond to a well-typed term in System F (say, it returns zero). The function $f$ is computable, so it can be computed by the untyped lambda calculus but not the typed lambda calculus (because the latter would amount to a proof of the consistency of second-order logic in second-order logic, which would imply that second-order logic is inconsistent). Caveat 1: If second-order logic is inconsistent, it might be possible to write $f$ in System F... and/or it might not be possible to write $f$ in the untyped lambda calculus - you could write something, but it might not always terminate, which is a criteria for "computable." Caveat 2: Sometimes by "simply typed lambda calculus" people mean "simply typed lambda calculus with a fixed-point operator or recursive functions." This would be more-or-less PCF, which can compute any computable function, just like the untyped lambda calculus. 

Other resources could be found referenced in Kaustuv Chaudhuri's thesis "The Focused Inverse Method for Linear Logic", and you might be interested in Roy Dyckhoff's "Contraction-Free Sequent Calculi", which is about contraction but not about linear logic. There are opportunities for efficient proof search in linear logic, but I don't think current work indicates that it's easier than proof search in non-substructural logic. The problem is that if you want to prove $C \vdash(A \otimes B)$ in linear logic, you have an extra question that you don't have in normal proof search: is $C$ used to prove $A$ or is $C$ used to prove $B$? In practice, this "resource nondeterminism" is a big problem in performing proof search in linear logic. Per the comments, Lincoln et al's 1990 "Decision problems for propositional linear logic" is a good reference if you want to get technical about words like "easier." 

While the obvious answer is that the fundamental complexity can't change, there may be better or worse algorithms for parsing the strings you're actually going to encounter. However, it seems like the issue is less the relative frequency of individual grammar productions (the A's, B's, and C's in the question) and more an issue of the unused, dead end parses that one binarization versus another may produce. With a bit of searching I found Better Binarization for the CKY Parsing (Song, Ding, and Lin, EMNLP 2008), which seems to definitively conclude that you can pick a "better" or "worse" binarization relative to the strings you actually expect to have to parse. Their name for the "dead end parses" that one would hope to minimize in practice seems to be incomplete constituents, and there is a good example on the first page. 

So, this is an instance of the fact that $(A \supset B) \wedge (A \supset C)$ is equivalent to $A \supset (B \wedge C)$. A different type isomorphism, namely that $(B \supset A) \wedge (C \supset A)$ is equivalent to $(B \vee C) \supset A$, looks more like what you wrote down, but that's because in logic programming notation we write B :- A when we mean $A \supset B$. (Note: $\supset$ is "implies," $\wedge$ is "and," and $\vee$ is "or.") What you're trying to do is related to the binarization transformation, which is discussed in McAllester's "On the complexity analysis of static analyses." There may be a better name for the transformation, but if so, it wasn't known the authors of the paper "A succinct solver for ALFP" (Alternating Least Fixedpoint formulas are a generalization of Horn clauses for bottom-up logic programs such as yours) - in Example 1 on page 4 they discuss a similar transformation and just call it "exploiting the possibility of sharing of pre-conditions." 

C's not a terribly good example, because it provides access to many low-level details that, I believe, really assume that the computer is based on binary with 8-bit words. As a thought exercise, you could add n-ary (what's-the-generalization-of-trit)wise operations into the C language as it exists. As Burdges suggests in his answer, the programmer could reasonably expect that a tritwise operation would be more efficient on the tritwise architecture, and the bitwise operation would be more efficient on the bitwise architecture. But what do you do about sizeof()? To deal with that, you'd probably need an incompatible "ternary C" that has a different sizeof (reporting the size in "trites" or whatever). This wouldn't be terribly difficult, I imagine (at least relative to the difficulty of writing a C compiler, which is hard). Lisp is a better example, since it abstracts out irrelevant low-level details like the number of bytes in an object's memory representation. I imagine a tertiary Lisp compiler would look nearly the same as a binary Lisp compiler, save for the code emitted when you do bitwise operations. 

If you're looking for a neat, functional reference to type-inference, I'm a bit partial to Gundry, McBride, and McKinna's 2010 "Type Inference in Context", though this may not be a good guide to any actual existing implementations. I think part of the answer is that, beyond the value restriction, there really isn't that much difficulty adapting Hindley-Milner type inference to imperative languages: if you define as syntactic sugar for and define as syntactic sugar for , where is a regular recursive function 

There was a reading course at Carnegie Mellon a few years ago, Languages and Logics for Security, which tried to survey some of the literature in authentication, authorization, information flow, protocol calculi, protection, and trust management; the course web page has slides for the papers we discussed as well as a further list of references for each topic. Information flow in particular might be something worth taking a look at relative to the topics you listed. The curriculum for Anupam Datta's course Foundations of Security and Privacy is also relevant. 

We know what the second example does: it creates two new ref cells containing , then puts in the first one (an ), then puts in the second one (a ). But think about the first example in terms of how we would represent in System F (the polymorphic lambda-calculus). In such a setting, would be a value of type "$\forall \alpha. \mbox{ref}(\mbox{option}(\alpha))$", so that means that, as a term, the value of must be a (type) lambda: "$\Lambda \alpha. \mbox{ref}[\alpha](\mbox{NONE})$". This would suggest that one "good" behavior of the first example is to behave exactly the same way the second example behaves - instantiate the type-level lambda two different times. The first time we instantiate with , which will cause to evaluate to a reference cell holding and then . The second time we instantiate with , which will case to evaluate to a (different!) reference cell holding and then . This behavior is "correct" (type-safe), but it's definitely not what a programmer would expect, and this is why we have the value restriction in ML, to avoid programmers dealing with this unexpected sort of behavior. 

then everything will work fine, including type inference. As for the value restriction being a special technique, I like the following story; I'm pretty sure I picked it up from Karl Crary. Consider the following code, which the value restriction will prevent you from writing in ML: 

One way of looking at database query languages is through the lens of deductive databases, where queries are represented as logic programs. In this setting, the most relevant work related to your question is McAllester's On the complexity analysis of static analyses, which observed that you can reason about the running time of a query by reasoning about the number of "prefix firings" in the rules of your program. What a "prefix firing" is isn't terribly complicated, but I'll refer you to the paper for that. In the functional programming world, this sort of thing is called a cost semantics: it doesn't mean you can only implement efficient queries (programs), but it means that you can reason about the asymptotic complexity of your declarative program in a reasonable way. Some later work on implementations of McAllester's ideas include From datalog rules to efficient programs with time and space guarantees (Liu and Stoller) and Dedalus: Datalog in Time and Space (Alvaro, Marczak, Conway, Hellerstein, Maier, and Sears). I admit I haven't yet read the latter of those two papers, however. 

I don't think it's correct to call them synonyms; there is some overlap in terms of research and implementation. I'm not familiar at all with Jay's work, and I'm only somewhat familiar with term rewriting systems, so I may be missing something too. Pattern matching in general deals with the following problem: you have some structure (a tree or a list or a multiset) and you want to check whether the structure matches a pattern (or one of a number of patterns). This question is certainly relevant to term rewriting, because in term rewriting systems the fact that a term matches a pattern means that the term can be rewritten to a different term, but it's not synonymous term rewriting. (There may be a formulation of pattern matching as rewriting: "Given a term, can you rewrite it to match pattern?" but I've never seen this.) Pattern matching in a functional programming language has a logical interpretation in terms of focusing (see Krishnaswami's "Focusing on Pattern Matching", for example). Term rewriting systems, on the other hand, often do matching modulo some equational properties, which isn't present in most functional programming languages (you can't match against a multiset in ML or Haskell). There's no fundamental reason why matching modulo equational properties shouldn't be present in functional languages, however. 

The notion of "next" and "last" - prior to the idea of "expansion" and "reduction" - are essentially lost when we read a big-step operational semantics as an inductive definition, which is too bad because the way we usually prove things about big-step operational semantics is by reading it as an inductive definition! You can prove a "full expansion" property (big step preservation) that $e \Downarrow v$ and $e \in R$ imply $v \in R$, and then observe that the proof of this property actually proves a small-step expansion property. This is because computational structure of the proof actually recovers the small-step evaluation process that Prolog/Twelf would use to execute the big-step semantics as a logic program! But for appropriately tricky semantics (especially non-deterministic ones), I've noticed it's frequently the case that you can prove a big-step property when it doesn't hold for the small-step evaluation process that Prolog/Twelf proof proof search would imply. In my view, the best option - if we want to use big-step specifications but recover the information that was lost when we treated the big-step semantics as inductive definition - to characterize the small-step evaluation process that Prolog/Twelf would use. (This ends up basically or exactly being an abstract machine semantics.) This may not be satisfying depending on what it is you're trying to do, of course. There are a couple of versions of this: see Hannan and Miller or Ager, for instance. (There's also a bit about this in my thesis, some work I did with Ian Zerny, but I'll link to that when there's a thing to link to.) You could also carefully characterize a set of partial derivations of $e_1 \Downarrow v_1, \ldots, e_n \Downarrow v_n \vdash e \Downarrow v$ that corresponded to partial executions, but when I've contemplated this the details have seemed fraught enough that the abstract machine interpretation seems like a safer bet.