Another thing to try would be to make a symlink to a location outside the version control system, typically another filesystem. As a side effect you'd gain protection against builds filling up the jenkins' filesystem :) 

There is an improvement issue open for, I think, exactly what you're looking for: Bamboo should be able to group projects: 

The breakage is always theoretically possible because the pre-commit verification performed by the developer is done in isolation and thus can't take into account other in-flight changes being verified in parallel. Such changes can interfere with each-other and cause regressions without actually have collissions detectable at the SCM-level. A simple example of such interfering changes: Let's assume the code in the latest version of a project branch includes a certain function, defined in one file and invoked in a couple of other files. Two developers working in parallel on that project are preparing to make some changes to the code. Developer A reworks that function removing or adding a mandatory function argument and, of course, updates all invocations of the function in all the associated files to match the updated definition. Developer B decides to add an invocation of said function in a file which didn't contain any such invocation before and is thus not touched by developer A's changes. Of course developer B is filling the argument list to match the function's definition visible in the latest label. Which is the old definition, as developer A's changes aren't yet committed. Both developers correctly perform the pre-commit verifications with a pass result and proceed to commit their code changes. Since the two changesets do not touch the same files no final merge happens, which typically would be an indication of potential problems, warranting a closer look and maybe a re-execution of the pre-commit verification. Nothing whatsoever can give even a subtle hint that something may go wrong. Yet the end result is catastrophic - the build is broken as the function call added by developer B doesn't match the function definition updated by developer A. 

The embedded software world often uses build-time flags, in the app code itself (/ statements, for example) and/or in build tools configuration files ('s, for example). Build flags can be used, in similar manner, not only for features, but also for all kinds of code refactoring, migrations, debug support, etc). They allow committing in the integration branch partial or unverified changes without breaking the build or causing regressions in the features/projects already working in the branch. Excellent for handling point fixes alongside large/risky/slow progress changes (which would otherwise require a long-lived branch) in a continuous integration manner. But in addition to verifying the already existing branch code for regressions, it's also possible to perform progress/stability verifications of the new code. For this the build-time flags need to be toggled. One way of toggling the flags would be to use, in a separate verification pipeline of the CI system of the same branch (if it has support for such functionality), a patchfile toggling the flag - to be applied to a separate workspace prior to the build. A different set of artifacts would be built in this workspace and then verified. Alternatively a long-lived feature branch can be pulled from the main integration branch, but the only change in this feature branch would be the toggled flag. Due to this tiny change the feature branch can be automatically sync'd extremely fast - practically shadowing very closely the main integration branch. A separate CI execution on this branch would not need a preliminary patchfile anymore. It'd be trivial to carry around such feature branch even for an extended period of time. It may also be possible to create, in the main integration branch, new build artifacts which would really be just clones of the existing build artifacts but with the flags toggled. This way neither the preliminary patchfile nor the feature branch would be necessary to verify the new code, right in the main branch. 

As Michael mentioned, offer a standard solution based on release versions/numbers, with a reasonably long lifespan for your industry (maybe interleaved with one or more shorter lifespan intermediate versions, if it makes sense for your typical customers). Give your customers the option to embark on this standard release track, maybe with a decent migration deadline in place. If they insist on a full-custom branch support strategy just charge them accordingly, to properly cover all your extra costs for offering such full-custom support - it only makes business sense. Some customers will migrate to reduce their costs (which will help you reduce the number of custom branches), some won't. Variable support billing, growing progressively with the age of the release versions from which the custom branches originate can also be an incentive for customers to migrate to newer branches faster, helping with faster closing of the older custom branches. This can help reduce the number of custom branches per customer - if you have customers that simultaneouly run multiple versions of your software. Make sure you don't fall into the trap of doing full branch merges from/to any of the release branches (both standard and custom), all changes to them should be either individually developed or cherry-picked merged fixes. Because each of these branches will gradually diverge from each-other, the number of hotfixes requiring customisation/individual development will grow exponentially (plain cherry-pick merging will fail). You need to take the development cost for these into account. With no (significant) branch merges in the picture you can (and should, I cannot stress its importance enough) build fully automated CI/CD pipelines for these branches, accompanied with a good hotfix tracking/management system in place, making the hotfix delivery just routine (or almost). 

IMHO that depends on what the role of the tester was before such transformation. BTW, I believe my answer applies to the DevOps transformation in general, not only to the paradigm. If the role was a drone job - mindlessly executing tests - it's bound to go away, automation will eat such jobs. If the role included writing the test plans and specifications they could continue to do that or even morph into writing the corresponding automation scripts, side-by-side with the developers. Or standalone - in some environments it is mandatory to use a completely separate methodology/infrastructure/personnel for testing than the one(s) used for development. In many organisations testers are software engineers, just as developers. If the role was executing intentionally unreliable/surprise tests, specifically requiring human traits (emotion, subjectivity, inspiration, reflexes, intuition, etc) to complement automated testing - that also won't change. 

A CI/CD system is, after all, just another software product/system. Regular upgrades and at least some changes in the plugin installations could be treated as changes of the software product itself. I see no founded reason for which such software product can't also use as part of its own development process a CI/CD solution. Maybe even itself - the most recent production version, for example - if it's flexible enough (I'm still investigating such possibility for the CI system I'm working on). Now, a CI/CD system deployed/actively used in a certain software development project is effectively a software product deployed in a specific environment. Changes to the configurations of the CI/CD system aren't actually software changes, they're changes of the environment in which the software system is deployed. These are trickier to address in a CI/CD pipeline. Some even impossible. You'd have to replicate the environment as close as possible. But it's unlikely you'll be able to do that at 100% - just like no dev/QA/etc environment is exactly as the production one. Some environment changes could be tested in a CI/CD pipeline. For example changes to a test script: both the old and the new versions can be executed as parallel pipeline jobs with the same set of artifacts for a period of time, long enough to determine the test quality and stability before making the new version the official one and retiring the old one. Finally, don't forget that most CI systems are reactive: they detect breakages, but human action is required to identify the culprit and/or make repairs before the development process can be resumed. Applying such CI system to an actively used CI/CD system one would allow the show-stopper problems you mention. Only a non-blocking CI system can completely prevent them (disclaimer: I'm the founder of the company owning the referenced page and offering such system). 

Note that is just following a PEP8 convention, one can always follow that statement with . Python doesn't really have constants, see How do I create a constant in Python? I'd use lowercase (i.e. following the Python variable naming convention), to prevent confusion: 

Since time series datastores are indexed by timestamps they can be very useful in a CI/CD environment, depending on what kind of information they store, for things like: 

Personally I consider the of the software to a target just an intermediary step of a deployment - installation/activation of that software being necessary to complete that deployment. To me the (as in ) stops when the software to be deployed is created and made available for deployment (i.e. for distribution, installation and activation) So, to answer your 1st question, no, I wouldn't consider distribution and installation as reflecting the manual step differentiating continous delivery from continous deployment. Yes, in some (hopefully rare) cases that manual step is just the final human decision for deploying into production, reflecting the cultural mistrust in process automation and the mental comfort of having a human double-checking and signing off the deployment decision (thus assuming responsability for it) even if that decision is purely made based on an algorithm that can be automated (like counting pass/fail testing results). But in general it simply reflects the fact that the decision for performing the deployment in production is not simply the result of an automated algorithm. Here would be some example such cases: 

Side note: personally I dislike using a minimum version: you never know beforehand how the language will evolve and if a certain future version will still be compatible with your code. For example many 2.7-compatible scripts can't actually work with python 3.X, setting 2.7 as the minimum version won't cut it. I prefer to explicitly list as allowed the (existing) versions that I can test and confirm to be compatible. Any future version can be added later on, after such confirmation is obtained. If you agree with the above then the plugin can apparently perform such version checks (it doesn't looks like it supports a minimum version): 

So one (crude) option would be to parse that page to get the price matching your particular instance type and region. I don't know if the equivalent info is available via the AWS Price List API: 

Some processes can, through their behaviour and not through their actual memory footprint, contribute to the overall system memory starvation and eventual demise. So it might also be of interest to look at the OOM Killer related information, which already takes into account some trending information: 

If the project is for a startup's minimum viable product or some other sort of sanity-check/proof of concept effort - hackathon before DevOps is often recommended - basically in the "fail-fast/fail cheap" lean startup strategy. Some might recommend hackathon before DevOps in other situations as well, for time-to-market reasons. But if DevOps doesn't followup fast enough that initial market bridgehead can easily collapse. Possible reasons for that include: 

You'll effectively use the CI build artifacts from that main branch across all environments. Well, as long they build and pass the respective QA verifications, obviously. Note: there are no feature branches in this strategy, at least not long-lived ones - in order to eliminate/minimize the integration delays and costs. Practically you can eliminate branch merges from your development. 

A more generic approach would be use a separate virtualized environment for each project, see virtualization. That could be either: 

IMHO it is at least a nice-to-have if not critical. But if critical it's not for the reason stated in assertion #2. The ability to exactly replicate the containerized build from the CI can be critical to reproduce and debug some issues found in the CI verifications, then develop and test fixes for them. Such ability can also be useful to optionally run parts of or the entire CI verification prior to committing the code for potentially high-risk changes. And, in general, docker on the dev workstations can contribute overall to reducing the differences between the development and QA/staging/production environments. But running CI-class verifications prior to commit can also be done on shared resources or on resources temporarily borrowed from the CI system, not necessarily on the developer's workstation, which in some cases can even be prohibitive - some CI builds can require much more resources than those available on the typical developer workstations. But when having such ability it can be very tempting to actually require developers to perform such verifications in an attempt to completely eliminate regressions in the real CI executions, which is, I believe, implied in assertion #2. But that would IMHO be sub-optimal - such verifications are still done in isolation and thus cannot completely eliminate CI failures (see How can successfully pre-verified changes cause regressions which should have been caught?), while being expensive and chewing developer time for stuff that can be entirely automated. If eliminating regressions in the real CI executions is desired then the precommit verifications have to be coordinated, ideally centralized, automated and immediately followed by automated commits. 

Ideally you'd be looking for the GCE instance creation events, but some/all of those may be too old for the logs retention policy. Then maybe search for VM instance start or other relevant events. You could also check and maybe cross-reference the audit logs info with that from Usage Reports or exported billing info. If this is a regular, on-going activity you may want to setup an automated log exporting and processing pipeline. 

The key aspect in a Dev(Sec)Ops-oriented organisation is the inter-field communication and collaboration. From this perspective, the QA role can include activities such as: 

Nope, credit goes to this SO answer, still valid today: How to upload files directly to Amazon S3 from a remote server?. But if you have a server in the cloud, ideally in AWS, it'll probably go faster to execute the copy operations on that server - higher bandwidth and lower latency to/from such server when compared to your computer. 

JMeter supports headless operation, in fact it's even the recommended mode for best load testing results. From Non-GUI Mode (Command Line mode): 

Note: this answer comes from my background of building custom solutions, it's not a config-only one, which, if available, would obviously be preferable. But it's maybe something to consider otherwise. You could teach the script executing the unit test job to persist a per-branch execution state. When the script is launched it would need to obtain the branch name from its invocation parameters. It would then obtain the persisted state for the branch. If the state is either missing or it would set it to , execute the unit test and then set the state to (or delete it). If the state is it means another unit test is already running and you have the corresponding job ID. The script can then (gracefully?) terminate the already running job by its ID and take it's place, updating the state to . But IMHO terminating a job in progress (which may be very close to completion) is not ideal - it leaves room for constantly killing jobs and launching news ones only to be, at their own turn, killed by subsequent jobs - all without actually completing any of them. Instead I'd modify the logic to: