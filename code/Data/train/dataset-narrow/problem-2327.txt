The fixed point operator you give is not well-formed according to the grammar of types in the $\kappa$-calculus. Note that the grammar of types does not contain the function space $\tau \Rightarrow \tau'$ -- this is because the $\kappa$-calculus is a language of first-order functions. An expression has a type $\tau_1 \to \tau_2$, to indicate that it is an expression of type $\tau_2$, whose free variables are typed by $\tau_1$. The proper typing for a fixed point operator would be roughly something like: $$ \frac{\Gamma, x:1 \to \tau \vdash e : 1 \to \tau} {\Gamma \vdash \mu x:1 \to \tau. e : 1 \to \tau} $$ Once you've fixed this, you still can't define a factorial. The plain $\kappa$-calculus does not have sums or natural numbers as a base type. As a result, you can't write branching programs, and so you cannot define interesting recursive functions. If you added natural numbers and their iterator, you could define a factorial function. Hasegawa actually gives factorial as an example in his paper on the $\kappa$-calculus, in a calculus augmented with a basic natural number type. However, you might wonder why you are able to define factorial in the pure lambda calculus, even though it has no apparent control structures. The reason is that the interaction of fixed points and recursion gives you a "universal type" (i.e., a type $V$ such that $V$ is isomorphic to $V \to V$), and this lets you encode any datatype as a subset (more accurately, a retract) of it. In more syntactic terms, you can write the Y combinator (i.e., Curry's paradox) in any language with (a) higher-order functions, (b) recursion, and (c) the ability to use each variable more than once in a program. Since the basic $\kappa$-calculus lacks (a), adding even an unrestricted fixed point operator does not make it Turing-complete! 

Pataraia's fixed point theorem gives a constructive proof of the fact that if you have a monotone function $f$ on a DCPO, then it has a least fixed point. I've frequently used this fixed point theorem to construct logical relations for proving properties about dependent type theories. The idea is that you can view a syntactic type system as a pair $(U, \phi)$ of a partial equivalence relation (or PER) $U$, representing the codes of types, and an interpretation function $\phi : U \to \mathrm{PER}$, sending each type code to its interpretation. Semantic type systems can be equipped with a domain structure, by saying that $(U', \phi') \sqsubseteq (U, \phi)$ just when $U' \subseteq U$, and for every type $A \in |U'|$, we have that $\phi(A) = \phi'(A)$. The empty type system is the obvious bottom element, and joins exist in the obvious way. Then a logical relation can be constructed as a fixed point of a monotone map with respect to this ordering. However, this semantic definition also bears a strong relationship to inductive-recursive definitions, where the set $U$ and the interpretation $\phi$ are defined simultaneously. (Unsurprisingly, since generalizing universe constructions was the prime motivation for the invention of induction-recursion.) Has anyone looked at the relation between the two? It's not obvious to me, since the proof of Pataraia's theorem, while constructive, is rather impredicative (it seems to rely essentially upon the powerset operation). So: 

Does anyone know where I can look to find out what the generally categorical semantics of S5 is? For S4, the answer is well-known: we want a Cartesian closed category with a product-preserving comonad to model $\Box$ and a monad strong with respect to the comonad to model $\Diamond$. In Awodey's book on category theory, on page 235 he remarks in passing that this setup becomes S5 when the monad is left adjoint to the comonad. However, he doesn't give a reference. 

Has anyone formalized the relationship between shift-reduce parsing techniques and delimited continuations? When constructing a bottom-up parser (eg, LR parsers), we take a grammar and then represent parse states as sets of items: augmented productions of the form $A \to \alpha \bullet \beta$, where $\alpha$ and $\beta$ are sequences of terminals and nonterminals. The marker $\bullet$ represents how far the parser has gotten into the string, with $\alpha$ representing what has been seen so far, and $\beta$ representing a prediction of what yet may be parsed. A shift action in a transition of the LR parse automaton matches a prefix of the stack against $\alpha$, and replace it with $A$. Such a deep manipulation of the stack resembles the effect of a control operator, but this is just a qualititative observation. Has anyone studied the connection between shift-reduce parsing and delimited control operators such as shift/reset? 

Type theories tend towards predicativity mainly socio-technical reasons. First, the informal concept of impredicativity can be formalized in (at least) two different ways. First, we say that a type theory like System F is impredicative because type quantification can range over all types (including the type the quantifier belongs to). So we can define generic identity and composition operators: $$ \begin{array}{lclll} \mathit{id} & : & \forall a.\; a \to a & = & \Lambda a.\;\lambda x.\;x\\ \mathit{compose} & : & \forall a, b, c.\; (a \to b) \to (b \to c) \to (a \to c) & = & \Lambda a,b,c.\lambda f, g. \lambda x. g(f\;x) \end{array} $$ However, note that in standard (eg, ZFC) set theory, these operations are not definable as objects. There is no such thing as "the identity function" in set theory, because a function is a relation between a domain set and a codomain set, and if a single function could be the identity function, then you could use it to construct a set of all sets. (This is basically how John Reynolds showed that System-F style polymorphism had no set-theoretic models.) However, set theory is impredicative in another way, via the powerset axiom. Power sets are impredicative because you can say things like "let $X$ be the intersection of all subsets of $S$ with property $P$" and then proceed to prove that $X$ itself has property $P$. As a result, $X$ has been defined "impredicatively", in terms of a set of which it is a member. This notion of impredicativity is incompatible with F-style quantification; see Andy Pitts's paper Non-trivial Power Types Can't Be Subtypes of Polymorphic Types. So F-style impredicativity is incompatible with a naive view of types as sets. If you are using type theory as a proof assistant, it's nice to be able to port standard math easily to your tool, and so most people implementing such systems simply remove impredicativity. This way everything has both a set-theoretic and type-theoretic reading, and you can interpret types in whatever fashion is most convenient for you. 

Yes, it is. Here's how you do it: You can compile basically any program you like to circuits. See for instance the work of Dan Ghica and his collaborators on the Geometry of Synthesis, which shows how to compile programs into circuits. 

I just realized that there is a nice solution to your problem, if you are willing to consider linear logic as your ambient logic instead of intuitionistic or classical logic. As is well-known, linear logic with the exponential modality $!A$ is not decidable. Furthermore, the exponential is a comonad featuring the duplication axiom $!A \multimap !!A$, which is evidently an axiom of nesting depth 2. (I got this far right away, and then I got stuck -- which is why this answer is so late.) However, I just realized that in implicit complexity, people modify the exponential $!A$ of linear logic to more precisely control the space and time usage of cut-elimination. Critically, all systems for doing so eliminate the duplication axiom! As a result, you can choose a system for which normalization likely goes past PSPACE (e.g., Elementary Affine Logic is as strong as elementary bounded Turing machines), and then the axiomatization of that will be unlikely to be in PSPACE, since that would imply that you could find cut-free proofs quickly. Link: Ugo dal Lago and Simone Martini, Phase Semantics and Decidability of Elementary Affine Logic 

It's obvious that concatenation is constant time, and that it's $O(n)$ to turn this into a cons-list. If you think about the usual representation of closures, you can see that this is basically the same pointer representation as the usual representation of datatypes. (Alternatively, you can view this type as a defunctionalized difference-list.) 

Basically, this says that if $(\sigma, T)$ is a solution to the type inference problem, then it will be a solution of the generated constraints $C$. However, the inference rules in Figure 22-1 sometimes will create some new contraint variables (for example, rule CT-App), all of which are recorded in $\mathcal{X}$. If the domain of $\sigma$ overlapped with the new variables constraint generation introduced, then you're in trouble, because now we have the possibility of inconsistencies between $\sigma$ and the constraints in $C$. So, instead Pierce proves that: 

It looks to me that you can encode the word problem for groups within the theory of categories in the following way. Pick an object $X$, and then for each generator of the group introduce two morphisms $x,x' : X \to X$, and assume the equalities $x \circ x' = 1_X$ and $x' \circ x = 1_X$. Then you can define the unit to be the identity map, the composition to be the group multiplication, and the negation of a string $x\circ y\circ z$ to be the reverse primed string $z' \circ y' \circ x'$. Hence this problem is undecidable. However, the word problem is solvable for many specific groups, so if you have more details about the problem this may help. In particular, one idea from the theory of groups which might help you a lot is that absolute presentations of finitely generated groups are solvable -- the inequations can prune the search space enough to make the theory decidable. EDIT: One additional thought I had is that adding irrelations might still be a useful tool for you, even if the concrete models you're interested validate the equations. This is because in categorical situations you often only want "nice" equations, for some value of nice, and you can use the inequations to rule out solutions that are too evil for you. Your decision procedure might still be incomplete, but you might get a more natural characterization of the solutions it can find than "we search possible proof-trees to a depth of 7". Good luck; that functor thing you're doing looks pretty cool! 

$R$ (and $P$ and $Q$) are not talking about specific heaps --- they are properties of heaps (since subsets and predicates are equivalent). The best way to understand what's going is by looking at the definition of what it means for a Hoare triple to hold: 

The reason I ask is that I want to construct a model of System F in which the types additionally carry metric structure (among other things). It's rather useful that in constructive set theory, we can cook up a family of sets $U$, such that $U$ is closed under products, exponentials, and $U$-indexed families, which makes it easy to give a model of System F. It would be very nice if I could cook up a similar family of constructive ultrametric spaces. But since adding choice to constructive set theory makes it classical, obviously I need to be more careful about fixed point theorems, and probably other stuff too. 

As you perhaps already know, a common metric on words is the Cantor metric, which is defined as: $$ d(l, k) = \left\{ \begin{array}{ll} 0 & \mbox{if } l = k \\ 2^{-n} & \mbox{where } n = \min\{i\in\mathbb{N} \;|\;l_i \not=k_i\} \end{array} \right. $$ Roughly speaking, if a string is a sequence of events, the distance between two strings is $2^{-n}$, where $n$ is the first time they differ. This can be lifted to a metric on (nonempty) languages by using the Hausdorff metric. (If you allow infinite strings, you also have to ensure that the languages are Cauchy-complete.) This metric shows up a lot in verification. The first reference to this that I know is Alpern and Schneider 1985, Defining Liveness. (Sorry for the absence of a link, but I couldn't find an online copy.) Jean-Eric Pin has written a survey article, Profinite Methods in Automata Theory, in which he surveys some more general metrics, and also draws some connections to Stone duality. 

I would suggest looking at these survey papers and tracking down the citations in the bibliography, and digging up subsequent papers which cited these. 

you are interested in any logic in which the principle of monotony fails, and you want a categorical semantics in the sense of categorical proof theory (rather than, say, a hyperdoctrine semantics), then 

Yes, the bounds depend on the assumption that function composition takes constant time. Basically, if you have a join list: 

In terms of number computability (i.e., computing functions from $\mathbb{N} \to \mathbb{N}$), all known models of computation are equivalent. However, it's still true that Turing machines are fairly painful for modelling properties like interactivity. The reason is a little bit subtle, and has to do with the kinds of questions that we want to ask about interactive computations. The usual first pass at modelling interaction with TMs is with oracle tapes. Intuitively, you can think of the string printed on the oracle tape as being a "prediction" of the Turing machine's I/O interaction with the environment. However, consider the sorts of questions we want to ask about interactive programs: for example, we might want to know that a computer program will not output your financial data unless it receives your username and password as input, and furthermore that programs do not leak information about passwords. Talking about this kind of constraint is very painful with oracle strings, since it reflects a temporal, epistemic constraint on the trace of the interaction, and the definition of oracle tapes ask you to supply the whole string up front. I suspect getting this right is doable, and essentially amounts (1) to considering oracle strings not as a set, but as a topological space whose open sets encode the modal logic of time and knowledge that you want to model, and (2) ensuring that the theorems you prove are all continuous with respect to this topology, viewing predicates as continuous functions from oracle strings to truth values viewed as the Sierpinski space. I should emphasize that this is a guess, based on the analogy with domain theory. You'd need to work out the details (and probably submit them to LICS or something) to be sure. As a result, people prefer to model interaction using things like the Dolev-Yao model, where you explicitly model the interaction between computers and the environment, so that you can explicitly characterize what the attacker knows. This makes it a lot easier to formulate appropriate modal logics for reasoning about security, since the state of the system plus the state of the environment are represented explicitly. 

Can such machines be built in practice? Yes. By "machine", Schmidhuber just means "computer program". Are they at least feasible in our Universe? Not in their current form -- the algorithms are too inefficient. 

Imperative programs are typically interpreted in domains, which are a special kind of topological space. The axioms of domains are not usually given in algebraic form, since the operations on domains include taking limits of chains, which is a non-finitary operation. But most applications of domains only need limits of $\omega$-chains, so it may be possible to describe them categorically with a generalization of the concept of algebraic theory, in terms of "locally presentable categories". However, this is a bit of category theory which is really a step beyond my knowledge, and I can't really tell you for sure either way. Specifications about imperative programs are typically given via Hoare logic. Michael B. Smyth wrote an (IMO) very readable article on the topological semantics of specifications in 1983. Michael B. Smyth: Power Domains and Predicate Transformers: A Topological View. ICALP 1983: 662-675 He also wrote the chapter "Topology", for the Handbook of Logic in Computer Science, Vol I. I have not read this chapter myself, but it is very widely cited. 

For languages with higher-order functions and strictly positive datatypes, having a fold is equivalent in expressive power to permitting structural recursion. However, it is not equivalent in terms of efficiency. If you just have fold, then implementing the predecessor function on the Peano natural numbers is O(n), whereas if you have a case form, it is O(1).