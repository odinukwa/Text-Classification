Please note the available physical memory is very low. There was almost no memory in buffer pool Regarding clerk which is consuming more memory 

SQL Server: Memory Manager-- Target Server Memory (KB) SQL Server: Memory Manager--Total Server Memory (KB) SQL Server: Memory Manager- Free Memory (KB) SQL Server: Memory Manager--Database Cache Memory (KB) SQLServer:Buffer Manager--Free Pages SQLServer:Buffer Manager--Free List Stall/sec SQLServer:Buffer Manager--Page Life expectancy 

Click on permission and then on page that occurs make sure you account is there if it is not there add you account and click on check box full control and then click on apply. It would take some time and then click ok. 

Shrinking will not improve performance de-fragmenting the indexes should increase the performance, if that is really the reson behind the slowness. Shrinking is going to cause massive logical fragmentation so always bear in mind to avoid it. ONLY shrink when you have freed up large amount of space in SQL Server and you need that space at all cost. For all other reasons avoid shrinking PS: Before going to shrinking at all I would first suggest you to find the cause of problem and Analyzing SQL Server Performance should get you started. 

From above definition please note that memory allocations are tracked by clerks. But for practical purposes you can safely assume sys.dm_os_memory_clerks is place to look for memory breakup as used by various components.Every component that allocates a significant amount of memory must create its own memory clerk and allocate all its memory by using the clerk interfaces. Frequently, components create their corresponding clerks at the time SQL Server is started. Please note clerks only track memory allocated to various components they never do allocation of SQL Server memory. 

If you are running concurrently the one started first will block other and the other will finish after the fist one allows it to start. But at the end both will finish If you are not running concurrently, both will just finish fine. Unless there is some other issue. In both cases the integrity of backup is NOT compromised. 

The advantage with using domain account is that they are more secured and when you configure availability groups using SSMS GUI the endpoints are created by the SSMS GUI and also granted the connect permission(. As you can note from above that when using built in account you need to create endpoints using certificates manually and grant connect permission. You also have option of running SQL Server services with different domain account, if you do so you just need to make sure both the logins are created on remote machine in master database. More over before changing I would suggest you to read Troubleshooting Always ON Configuration( see the accounts section) 

Whenever you run patch SQL Server writes in Log what all instances and features it has found on the current system and as per log you only installed 

No it does not contains backup of ldf file. It might contain some information not the complete backup. A full or differential backup contains enough log to be able to recover the database to a transactionally-consistent view of the database at the time the data-reading portion of the backup finished (or as far back as the oldest log record that transactional replication has not yet processed â€“ to ensure that replication works properly after a restore). 

First thing is after changing recovery model from full to simple you loose point in time recovery. Is this database production database(which i feel it is)? if so you are not doing correct thing. If database fails at this point in time you would loose point in time recovery and would loose data are you ok with it. If you switch from the full or bulk-logged recovery model to the simple recovery model, you break the backup log chain. Therefore, its strongly recommend that you back up the log immediately before switching, which allows you to recover the database up to that point. After switching, you need to take periodic data backups to protect your data and to truncate the inactive portion of the transaction lo 

This can be a bug make sure SQL Server 2008 R2 is patched to SP3 There was old logshipping which was configured and was not removed correctly and completely. 

You are correct for the first part but why would full backup or log backup change "Last Modified" time of mdf file. Its actually reading from the data file its not making any changes. Normally in daily operation SQL Server reads quite often from data file but for every physical read it is not going to update the time stamp or modifiy the date that would be unnecessary. Moreover the way SQL Server access the data and log file are not that straight it uses different mechanism so I guess its useless worrying about modified date. 

The first and second point will only make sure query/batch runs with 4199 trace flag while third point makes sure any query which runs sees this trace flag enabled. NOTE: Enabling trace flag requires sysadmin permission so if you ask your developer to use it in query for instance he might not be able to assuming developers have limited access. I would suggest you to use or to see if queries are performing up to mark. 

Connect to the Database Engine of either mirroring partner. From the Standard bar, click New Query. Issue the following Transact-SQL statement: 

I strongly suggest to read Working with Multiple instance of SQL Server. If it is not interrupting at all STOP the SQL Server services which is not required its more safer approach. Microsoft has history of behaving incorrectly when instance is removed from server having multiple instances present. I have seen it multiple times IMHO If you have multiple instances its more safer to use command prompt to remove SQL Server instance as compared to GUI. Below is the dummy script. You have to open CMD as administrator and move to folder where Setp.exe file is present and then run below script. 

This can cause a bit of more time being spent by optimizer in preparing new better plan and I/O bringing pages into memory but this would just be the first time. But there would not be much performance degradation. In many cases the new plan would execute query much faster and this would outweigh the time taken by optimizer to build new plan and I/O to get pages into memory. Since you are interested about plan cache I suggest you read Plan Cache and Reuse. As per the document among other following two can also cause plan to recompile 

is a hash value which identifies SQL text of the batch/query being submitted to the server. Text can be NULL for encrypted objects so this can cause to be NULL 

You need to first update your script to point to new location. Then you need to make sure SQL Server service account has read write permission on the new NAS where you would be taking backup. You also need to make sure that SQL Server can see the NAS otherwise if there is no connectivity you would not be able to backup there. A simple test can be done, create a test database on SQL Server and try taking backup using TSQL command, you were using to backup on previous NAS, if you succeed well and good if not resolve the error. 

Although there are many answers posted I would like to post answer which is not technical but would surely tell you whether database is used or not. There is no PERFECT and ABSOLUTE way IMHO. What you can do is 

Yes database compression affects I/O and as far as my experience goes it decreases the I/O and is actually benefitial if you look at I/O consumption. I will tell you how. 

If you remove from primary replica means you are completely removing it from AOAG. You have to remove it from secondary replica as sated above. 

of SQL Server. Fast recovery which is feature could bring database online after second phase(redo) of recovery Amount of transaction log SQL Server has to rollforward and rollback to bring database online in consistent state. Because database would go through crash recovery this would always be the one influencing the time most. If database takes lot of time to recover due to large amount of uncommitted transactions gettign rolled back it would definitely increase your time. The hardware strength and network capabilities supporting the cluster. Some of this is documented here. 

Generally on stable system these 2 values are equal. Free Pages counter is removed from SQL Server 2012. And also its value does not holds importance as the values for BCHR,PLE,Target server memory and Total Server memory 

This is not totally correct when a transaction starts in SQL Server it reserves space in transaction log in case the transaction has to rollback. From Transaction Log architecture BOL doc 

Rohit, If you would have searched a bit on web you would have found the answer that Buffer cache hit ratio should be as high as possible and 98.2 % value is quite good value please make a habit to search it will be helpful to you. Bufffer cache hit ratio definition could be eaisly obtained from perfmon. A simple definition is the percentage of sql server pages requested and retrieved from the buffer cache without reading from disk. If SQL Server would not have to read data from disk it will save I/O which is costly and perhaps on busy system it would reduce query time and increase its processing speed. Reading from memory is quite fast as compared to reading from disk. Below article will surely interest you $URL$