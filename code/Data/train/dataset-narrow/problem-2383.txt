Let me try to reformulate the question so that it makes a bit more sense. The main issue is how to choose the representation of the input and output, as well as the notion of approximation, so that exponentiation stands a fighting chance of being computable in polynomial time. One way was mentioned by Kaveh in the comments: restrict the domain to a fixed finite interval. While this works, it is unnecessarily restrictive; in particular, there is no good way how to transform an algebraic number into one that is bounded so that their exponentials have anything to do with each other. A more flexible approach is to represent the input as a fixed point number, and the output as a floating point number. To be specific, a fixed point representation is a string $$\pm a_ra_{r-1}\dots a_0.a_{-1}\dots a_{-s}$$ denoting $\pm\sum_ja_j2^j$, where $a_j\in\{0,1\}$, and a floating point representation is $$\pm2^e\times a_0.a_{-1}\dots a_{-s}$$ with a similar interpretation, where $e$ is a binary integer, and $a_0=1$. (As an exception, we also allow $0$ to represent itself.) An approximation of a real $x$ to $m$ bits of absolute accuracy is a real $x'$ such that $|x-x'|<2^{-m}$, and an approximation to $m$ bits of relative accuracy is $x'$ such that $|1-x'/x|<2^{-m}$. We will use absolute accuracy for fixed point approximations, and relative accuracy for floating point approximations. It follows that in both cases, we may as well assume $s\le m$ (up to loss of one bit of accuracy). Fixed point and floating point representations of dyadic Gaussian rationals, and approximations of complex numbers, are defined similarly, using a pair of reals. Everywhere below, $n$ denotes the total size of the input. Now, let (real or complex) exponentiation be the following problem: the input is a fixed point representation of a number $x$ and a unary natural number $m$, and the output is a floating point approximation of $e^x$ to $m$ bits of relative accuracy. Dually, logarithm takes as input a floating point number $x$ and unary $m$, and the output is a fixed point approximation of $\log x$ (say, the principal branch in the complex case) to $m$ bits of absolute accuracy. As long as we stick to run-time bounds satisfying some mild regularity conditions, and ignore constant multiplicative factors, we have: 

This is actually provable in ZFC: if $\phi$ does, indeed, have a ZFC-proof of length $\le s$, then we can just take it, and derive (3) by ignoring the premise; if not, then for each string $|w|\le s$, we can “by inspection” produce a ZFC-proof that ‘$w$ is not a ZFC-proof of $\phi$’, and concatenating all these together, we obtain a proof of (3) (specifically, a proof of negation of its premise). This brute-force proof will have length exponential in $s$, i.e., in our case, $2^{O(|\mp|^2)}$. There are in fact proofs of (3) of length polynomial in $s$ and $|\phi|$: this was proved by Pudlák [1] using efficient partial truth predicates. The improvements in [2] are only stated for consistency statements, but I believe they apply to more general reflection principles as well. Thus, there are ZFC-proofs of (1) of length $O(|\mp|^2)$. However, this is still not enough to get a contradiction, as it seems any reasonable proof of (3) needs length more than $s$ (though the lower bounds in [1,2] are not quite that strong). Thus, ZFC-proofs of (1) will need length more than $|\mp|^2$. This cannot be fixed by raising the bounds in $\mc$ or by padding $\mp$, as these will just raise the length parameter $s$ in the same way. Thus, there is no paradox. So, to summarize: ZFC proves that $\mc(\mp)$ outputs NO and that $\mp(x)$ halts in constant time, but the proofs are a little too long. It is also worth mentioning that nothing in these arguments depends on ZFC specifically. The same reasoning will work for many other theories, for example Peano arithmetic. (Basically, we need a sequential theory axiomatized by finitely many axioms or nice schemata, see Pudlák’s papers for details.) [1] Pavel Pudlák, On the length of proofs of finitistic consistency statements in first order theories, in: Logic Colloquium ’84 (Paris, Wilkie, Wilmers, eds.), Studies in Logic and the Foundations of Mathematics vol. 120, 1986, pp. 165–196, doi: 10.1016/S0049-237X(08)70462-2. [2] Pavel Pudlák, Improved bounds to the length of proofs of finitistic consistency statements, in: Logic and Combinatorics (Simpson, ed.), Contemporary Mathematics vol. 65, American Mathematical Society, 1987, pp. 309–331, doi: 10.1090/conm/065. 

Sure. If $C_1\ni x_i$ and $C_2\ni\neg x_i$ are in $F\restriction I$ (in particular, $x_i$ is unset by $I$), pick clauses $D_1,D_2$ in $F$ which restrict to $C_1$ and $C_2$, respectively. Then $x_i\in D_1$ and $\neg x_i\in D_2$, hence their resolvent $(D_1\let\bez\smallsetminus\bez\{x_i\})\cup(D_2\bez\{\neg x_i\})$ is subsumed by some $D\in F$. If $D$ contains a literal made true under $I$, then so does $D_1$ or $D_2$, contradicting their choice. Thus, $D\restriction I$ is in $F\restriction I$, and it subsumes $(C_1\bez\{x_i\})\cup(C_2\cup\{\neg x_i\})$. 

The basic sum-of-squares proof system, introduced under the name of Positivstellensatz refutations by Grigoriev and Vorobjov, is a “static” proof system for showing that a set of polynomial equations and inequations $$S=\{f_1=0,\dots,f_k=0,h_1\ge0,\dots,h_m\ge0\},$$ where $f_1,\dots,f_k,h_1,\dots,h_m\in\mathbb R[x_1,\dots,x_n]$, has no common solution in $\mathbb R^n$: a refutation of $S$ is given by polynomials $g_i$ and $e_{I,j}$ such that $$\tag{$*$}-1=\sum_{i=1}^kg_if_i+\sum_{I\subseteq\{1,\dots,m\}}\sum_je_{I,j}^2\prod_{i\in I}h_i.$$ (One could work with any real-closed field in place of $\mathbb R$.) Stengle’s Positivstellensatz guarantees that $S$ has a refutation if and only if it has no solution. The main complexity measure here is the degree of the refutation, which is the maximum of total degrees of the polynomials that appear under the sum signs in $(*)$, that is, $g_if_i$ and $e_{I,j}^2\prod_{i\in I}h_i$. As usual with algebraic proof systems, one can also consider it as a refutation system for unsatisfiable Boolean formulas $\phi$ by including in $S$ the axioms $x_i^2-x_i$ for each variable $x_i$, and a translation of $\phi$ by polynomial inequalities. More on the history and development of SOS systems can be found in $URL$ . 

Niel de Beaudrap answered the question as such, but let me mention for the record that the complexity of Presburger arithmetic is known more precisely than the Fischer&Rabin bound: full Presburger arithmetic is complete for alternating doubly exponential time with a linear number of alternations (due to Berman), and the $\Sigma_{i+1}$-fragment is complete for $\Sigma_i^{EXP}$ (Haase). 

For example, FO+TC has the Löwenheim–Skolem property (as it is a fragment of $L_{\omega_1,\omega}$), hence it can be neither complete nor compact, and a fortiori the same applies to all extensions of FO+TC such as fixed-point logics. (This is of course easy to show directly.) 

Concerning 3, I believe $\mathrm{PP\subseteq\oplus P^{C_=P}}$, as there are at least $a$ numbers $x<2^n$ satisfying $P(x)$ if and only if the number of $y<2^n$ such that $P(y)\land|\{x\le y:P(x)\}|=a$ is odd. (Note that there is always at most one such $y$. That is, the argument actually shows $\mathrm{PP\subseteq UP^{C_=P}}$. (In fact, it even shows $\mathrm{UP^{PP}=UP^{C_=P}=UP^{C_=P[1]}}$.)) 

(I understand the description of the problem so that the input numbers are bounded by a constant, so I will not track dependence on the bound.) The problem is solvable in linear time and logarithmic space using sums of logarithms. In more detail, the algorithm is as follows: 

The NP-completeness of the original problem was proved by Manders and Adleman [1] using a reduction from 3-SAT. Their reduction is parsimonious. Thus, (taking into account that the number of prime factors is upper bounded by the length of the input $n$, while in the M–A reduction, it is at least $n^\epsilon$) your problem is complete for promise-FewP. Note that by Valiant–Vazirani, already promise-UP is NP-hard under randomized polynomial-time reductions, hence the same holds for promise-FewP. Thus, the problem is essentially as difficult as NP. EDIT: The answer above assumes that in the question, the unclear phrase “the number of occurrences of $a$” means the number of residues $x<L$ such that $x^2\equiv a$. The OP indicates in a comment below that they rather intended it to mean the total number of residues mod $b$ that square to $a$. In the latter case, the problem is solvable in promise-ZPP: using the factorization of $b$, just compute all possible square roots of $a$ modulo $b$ by the usual algorithm (Tonelli–Shanks + Hensel’s lifting + Chinese remainder theorem). Reference: [1] Kenneth L. Manders and Leonard M. Adleman, NP-complete decision problems for binary quadratics, Journal of Computer and System Sciences 16 (1978), no. 2, pp. 168–184. 

Due to popular demand, I’m converting my comment to an answer. A simple padding argument shows that for every constant $\epsilon>0$, there exist EXP-complete problems in $\mathrm{DTIME}(2^{n^\epsilon})$. Indeed, fix an arbitrary EXP-complete problem $L$, and assume that it is computable in time $2^{n^c}$. Let $d>c/\epsilon$, and consider the problem $$L'=\left\{0^m\#w:w\in L,m\ge|w|^d\right\}.$$ On the one hand, $L$ is polynomial-time${}^\dagger$ reducible to $L'$ via the function $w\mapsto0^{|w|^d}\#w$, thus $L'$ is EXP-hard. On the other hand, $L'$ is computable in time $2^{n^\epsilon}$: given an input of size $n$, we first check (in polynomial time) that it is of the form $0^m\#w$ for $m\ge n'^d$, where $n'=|w|$. Then we check if $w\in L$, which takes time $2^{n'^c}\le2^{m^{c/d}}\le2^{m^\epsilon}\le2^{n^\epsilon}$. 

Proof: In view of the preceding discussion, it suffices to show that if $\cC$ is a permutation clone, and $f\in\sym(A^n)\smallsetminus\cC$, there is an invariant $w\colon A^k\to M$ of $\cC$ such that $f\nparallel w$, and one can take $w$ to be a master weight function if $\cC$ is a master clone. Put $k=|A|^n$, and let $F$ be the free monoid generated by $A^k$ (i.e., finite words over alphabet $A^k$). We define a relation $\sim$ on $F$ by $$x_1\cdots x_m\sim y_1\cdots y_m\iff\hbox{$\exists g\in\cC\cap\sym(A^m)\,\forall j=1,\dots,k\,g(x^j_1,\dots,x^j_m)=(y^j_1,\dots,y^j_m).$}$$ (Words of unequal length are never related by $\sim$.) Since each $\cC\cap\sym(A^m)$ is a group, $\sim$ is an equivalence relation (in fact, its restriction to words of length $m$ is just the orbit equivalence relation of $\cC\cap\sym(A^m)$ acting in the obvious way on $A^{mk}$). Moreover, $\sim$ is a monoid congruence: if $g\in\cC\cap\sym(A^m)$ and $g'\in\sym(A^{m'})$ witness that $x_1\cdots x_m\sim y_1\cdots y_m$ and $x'_1\cdots x'_{m'}\sim y'_1\cdots y'_{m'}$, respectively, then $g\times g'\in\cC\cap\sym(A^{m+m'})$ witnesses $x_1\cdots x_mx'_1\cdots x'_{m'}\sim y_1\cdots y_my'_1\cdots y'_{m'}$. Thus, we can form the quotient monoid $M=F/{\sim}$. The swap permutation witnesses that $xy\sim yx$ for each $x,y\in A^k$; that is, the generators of $M$ commute, hence $M$ is commutative. Define a weight function $w\colon A^k\to M$ as the natural inclusion of $A^k$ in $F$ composed with the quotient map. It is easy to see that $\cC\subseteq\pol(w)$: indeed, if $g\in\cC\cap\sym(A^m)$, and $y^1=f(x^1),\dots,y^k=f(x^k)$, then $$\sum_{i=1}^mw(x_i)=x_1\cdots x_m/{\sim}=y_1\cdots y_m/{\sim}=\sum_{i=1}^mw(y_i)$$ by the definition of $\sim$ (using the notation as in the definition of $\pres$). On the other hand, assume $f\pres w$. Let $\{a^j:j=1,\dots,k\}$ be an enumeration of $A^n$, $b^j=f(a^j)$, and let $a_i,b_i\in A^k$ for $i=1,\dots,n$ be again as in the definition of $\pres$. Then $$a_1\cdots a_n/{\sim}=\sum_{i=1}^nw(a_i)=\sum_{i=1}^nw(b_i)=b_1\cdots b_n/{\sim},$$ hence by the definition of $\sim$, there exists $g\in\cC\cap\sym(A^n)$ such that $g(a^j)=b^j=f(a^j)$ for each $j$. However, since the $a^j$ exhaust $A^n$, this means $g=f$, i.e., $f\in\cC$, a contradiction. This completes the proof for permutation clones. Even if $\cC$ is a master clone, $w$ needn’t be a master weight function, in fact, the diagonal elements are not even necessarily cancellative in $M$, hence we need to fix it. For each $c\in A$, let $c^*=(c,\dots,c)\in A^k$, and define a new equivalence relation $\approx$ on $F$ by $$x_1\cdots x_m\approx y_1\cdots y_m\iff\exists c_1,\dots,c_r\in A\,x_1\cdots x_mc_1^*\cdots c_r^*\sim y_1\cdots y_mc_1^*\cdots c_r^*.$$ Using the fact that elements of $A^k$ commute modulo $\sim$, it is easy to show that $\approx$ is again a congruence, hence we can form the monoid $M'=F/{\approx}$, and a weight function $w'\colon A^k\to M'$. Since $\approx$ extends $\sim$, $M'$ is commutative, and a quotient of $M$; in particular, $\cC\subseteq\pol(w')$. On the other hand, if $f\pres w'$, then the same argument as above together with the definition of $\approx$ would give a $g\in\cC\cap\sym(A^{n+r})$, and $c_1,\dots,c_r\in A$ such that $$g(x,c_1,\dots,c_r)=(f(x),c_1,\dots,c_r)$$ for all $x\in A^n$, thus $f\in\cC$ as $\cC$ is a master clone, a contradiction. The definition of $\approx$ ensures that $$xc^*\approx yc^*\implies x\approx y$$ for all $x,y\in F$, and $c\in A$. It follows that the elements $c^*/{\approx}=w'(c^*)$ are cancellative in $M'$. It is an easy well-known fact that any commutative monoid can be embedded in another one where all cancellative elements become invertible. The composition of such an embedding with $w'$ is then a master weight function $w''$, and $\pol(w')=\pol(w'')$, hence $w''\in\minv^*(\cC)\setminus\minv^*(f)$. QED 

We aim to characterize permutation clones and master clones by certain invariants. Let me first motivate the latter by a few examples on $A=\{0,1\}$: 

Note that the pivot row in particular gets added to itself, which zeroes it out (we won’t need it any more). Since each iteration kills at least one column, the loop will iterate at most $n$ times. Now, the Horn system (H) uses variables $$y_{t,i,j,a}\qquad(0\le t\le n;1\le i\le n+1;1\le j\le m;a\in\F)$$ with the intended meaning “after $t$ iterations, the $(i,j)$th entry of $A$ has value $a$”. The system consists of the “initial” clauses $$y_{0,i,j,a_i^j}\tag1$$ for $1\le i\le n+1$ and $1\le j\le m$; the negative “rejection” clauses $$\ET_{i=1}^ny_{t,i,j,0}\to\neg y_{t,n+1,j,1}\tag2$$ for $0\le t\le n$ and $1\le j\le m$; and the “next iteration” clauses $$\ET_{i=1}^{p-1}\ET_{j=1}^my_{t,i,j,0}\land\ET_{j=1}^{q-1}y_{t,p,j,0}\land y_{t,p,q,1}\land y_{t,k,l,a}\land y_{t,p,l,b}\land y_{t,k,q,c}\to y_{t+1,k,l,a+bc}\tag3$$ for $0\le t<n$, $1\le p\le n$, $1\le q\le m$, $1\le k\le n+1$, $1\le l\le m$, and $a,b,c\in\F$. It should be clear from the discussion that (H) is equisatisfiable with (L). 

The question is not entirely clear to me. However, concerning the example that is spelled out more precisely: if a language is recognizable by a poly-time machine with a SAT oracle which must accept whenever the oracle answers “yes”, it is in fact in NP. First, observe that regardless of the oracle answers, we can simulate in polynomial time the only possible run of the algorithm by pretending all answers are “no”. (In particular, we can compute the list of all oracle queries beforehand.) Then, the original algorithm accepts iff one of the oracle answers in the simulated run is actually “yes”, or the simulated algorithm accepts in the end. This is an NP property. More generally, if we use an arbitrary oracle $A$ in the above scenario in place of SAT, a language can be computed in the indicated way iff it is poly-time dtt-reducible to $A$. 

The question seems to be predicated on a misunderstanding: the statements “relative to a random oracle $A$, $\mathrm{P}^A=\mathrm{BPP}^A$” and “$\mathrm{Almost\text-P}=\mathrm{BPP}$” are not meant to be rephrasings of each other. The complexity zoo refers to a paper of Bennett and Gill, which proves the former statement (and many other things) in detail, but it also separately claims the second statement (though they do not use the $\mathrm{Almost\text-P}$ notation, and do not really give a proof). The definition of $\mathrm{Almost\text-P}$ in the zoo is correct. The proof of $\mathrm{Almost\text-P\subseteq BPP}$ goes as follows. Let $L\in\mathrm{Almost\text-P}$. Since there are only countably many Turing machines, there exists an oracle polynomial-time Turing machine $M$ such that $M^A$ computes $L$ with positive probability $\epsilon>0$. Using the Lebesgue density theorem, there exists a finite oracle prefix $A_0$ such that relative to random oracles $A$ that extend $A_0$, $M^A$ computes $L$ with probability $\ge3/4$. We can hardwire $A_0$ into the Turing machine, and simulate access to the rest of the oracle by random coin flips to obtain a randomized poly-time machine for $L$ with probability of success $\ge3/4$. Thus, $L\in\mathrm{BPP}$. 

Consider an equation $$\tag{E}a_1x_1+\dots+a_kx_k=0.$$ Let $s=\sum_ia_i$ be the sum of its coefficients. If $s=0$, the equation (E) has solutions in every nonempty set $S$, hence you are out of luck. If $s\ne0$, let $p$ be the smallest prime not dividing $s$. Note that $p=O(\log |s|)$. Then (E) has no solution such that $x_i\equiv1\pmod p$ for all $i=1,\dots,k$, hence the set $$S=\{1,p+1,\dots,(n-1)p+1\}$$ works, with maximum $(n-1)p+1$. For a fixed equation (E), this is $O(n)$, and the set can be computed as easily as it gets. 

Let us start with the $\oplus L$-complete problem of counting mod $2$ the number of paths of length $n$ from vertex $s$ to vertex $t$ in a directed graph $G=(V,E)$. We apply a couple of logspace reductions as follows. Let $G'=(V',E')$ be the graph such that $V'=V\times\{0,\dots,n\}$ and $E'=\{((u,i),(v,i+1):i<n,(u,v)\in E\}\cup\{(w,w):w\in V'\}$ (i.e., we take $n+1$ copies of $G$’s vertices, make edges go from the $i$th copy to the $(i+1)$th copy according to $G$’s edges, and add all self-loops). Then the original problem is equivalent to counting paths of length $n$ from $s'=(s,0)$ to $t'=(t,n)$ in $G'$. Moreover, $G'$ is acyclic, and we can explicitly define an enumeration $V'=\{w_k:k\le m\}$ such that all edges in $G'$ apart from the self-loops go from $w_k$ to $w_l$ for some $k<l$. Without loss of generality, $w_0=s'$ and $w_m=t'$. Let $M$ be the adjacency matrix of $G'$ wrt the given enumeration. Then $M$ is an upper triangular integer matrix with $1$ on the diagonal, and the number of paths of length $n$ from $s'$ to $t'$ equals the top right element of $M^n$. It is easy to see that $$M=\prod_{j=m}^1\prod_{i=0}^{j-1}E_{i,j}(M_{i,j}),$$ where $E_{i,j}(a)$ is the elementary matrix whose only nondiagonal entry is $a$ in row $i$ and column $j$. In this way, we reduced the original problem to computing the top right element of a product of elementary matrices. In the $\oplus L$ case, the computation is modulo $2$, i.e., we consider the matrices over $\mathbb F_2$. (In this case, the elementary matrices can be only $E_{i,j}(0)=I$, which we can ignore, and $E_{i,j}(1)$, which can be simulated by a single CNOT gate, as mentioned in the question.) If we consider them as integer matrices, we get a $\#L$-complete problem, and if we consider them modulo $k$, we get a $\mathrm{Mod}_kL$-complete problem.