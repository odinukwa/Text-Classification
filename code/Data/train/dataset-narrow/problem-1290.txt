I'm making a fair few assumptions about the internal format that may not be justified, but this works on the demo data I tried: 

Note that I say this as someone who has never touched this kind of stuff; mathematicians are far ahead of any na√Øve implementations and if you want to take this seriously you'd probably want to look at the literature first. 

or, better, just used the format specifier. The says there should be 0 fractional digits and the displays the number as a percentage. 

I'm looking at since it's the simpler of the two. You start with , which is a great candidate for a function: 

If you want to avoid , a faster method would just split and join. This is faster simply because and are fast: 

by making count as . Then any subsection with more heads than tails will have a positive total. This simplifies the whole code to 

takes arguments , and . These arguments could be more explanatory: doesn't sort anything and it doesn't take an array, and and don't say much. It's canonical to use and as indices, so that would be more appropriate - although longer names wouldn't hurt either. The argument would be better as or similar. is -like, so put it first. 

is used over because the buffering used in breaks the code for repeated indices. The mean is taken with a simple division of the number of repeated values that map to the same index: 

Try to keep everything in functions. Local variables aren't just nicer to work with; they're also marginally faster. Split model generation into a new function. Consider . I change this under alexwlchan's suggestions of hoisting out to 

I don't entirely understand your docstrings, so I may have mangled them a little. The final and assignment itself will eventually be moved into another function. Next, 

Change to just . Try to use and replace / with booleans. I'll initially use and discard the second parameter to keep the transition small. Then consider that your structure is 

We check the and that for every in the words which must not be in . If this matches, we return , otherwise we go to the next iteration of the loop. This can be simplified: 

I don't know what you mean by . If you have encountered difficulty with something, a good comment will explain what is problematic - not just that it is. Hopefully you'll find the new one more capable: 

and removing the call to , but it should already be fast enough. Then you should focus on cleaning up the code. Spacing is important as are appropriate variable names ( and not-single-letter names). The trick is to make comments redundant: 

One also only need the first square root of factors, since larger numbers are paired with the smaller numbers anyway: 

I hadn't realized that is only used with as the funciton; maybe you should move it into to make calling more convenient. You should split your s up: 

Nice, that seems to have made it fast enough, but more important the code is more readable. We can improve speed further by avoiding , using directly to get buffering. 

over the implicit one; logically you are checking for a sentinel and that's the canonical way to express it. Using would imply that you expect a could be falsy. doesn't need to be a property; just set it during initialization. Personally, though, writing the one time you use it seems more natural. The same goes for ; you aren't buying much per unit of work. is a property but entails a nontrivial amount of work. It's rarely a good idea to hide work like that under attribute access; I suggest making it a fully-fledged method. For some reason it also feels to me that it should be a standalone function. I'm not sure why. This gives 

You should structure your code into functions, not have everything in the top-level scope. You should wrap long lines, even if they are only strings. Don't do 

The last two constraints again tell us nothing we didn't already know. Which constraints haven't we entirely used? 

This actually removes an instruction from the total, but it seems to be pretty much random that before the compiler was going and now it does . Nonetheless, the faster early-out is a good thing, and you should see a small win proportional to the frequency of large integers in the program. The rest of the logic is a little unfortunate, and as you said it's not easy to simplify. However, you can always use a lookup table! 

The name should also be improved to, say, . Same with (I chose ). Now in , remove dead comments like 

With PyPy3 this takes me ~1.5s to run. Most of the time seems to be in , so we can speed that up by letting it short-circuit against . This gives: 

This is because the new method allows better case analysis - we've forgotten to check that such operations are actually valid! 

The calls are not part of this; only the looping and the slicing. This gives a cost of $$ 1(n)^2 + 2(n-1)^2 + 3(n-2)^2 + ... + (n-1)(2)^2 + n(1)^21 $$ or $$ \sum_{k=1}^n k (n-k+1)^2 = \frac{1}{12} n (n+1) (n^2+3 n+2) $$ which is \$\mathcal{O}(n^4)\$. So caching does improve this a lot from \$\mathcal{O}(2^n n!)\$... but not nearly enough. Note that we could prove that it's valid to ignore failed calls by moving the check into the caller and seeing that the cost is the same. Using range objects like Python 3's would give costs of \$\mathcal{O}(n)\$ each, not \$\mathcal{O}(n^2)\$, since slicing would be \$\mathcal{O}(1)\$. However, the check prevents this from mattering. You can see this by moving the check into the caller. If you changed this to using an \$\mathcal{O}(1)\$ hash, this would work and reduce the cost to \$\mathcal{O}(n^3)\$ overall. Math elided for brevity. 

Don't print values; that's an unwanted side-effect of the function. It makes it unusable from other pieces of code that don't want the array to be printed. You should use a list for with an automatic resize; pre-counting is overhead that isn't needed nor useful. Since this can actually be done during traversal of , I don't think it's worth creating at all. You have far too many local variables, and this trims a lot of it down. It also brings space complexity down to \$\mathcal{O}(1)\$. You check 

\$\newcommand{\sub}[1]{\text{sub}_{#1}}\$ This is more of an alternative algorithm than a review on your code (at least for now), but it seems like both could be helpful. A good algorithm results from solving a slightly more informative version of the problem, which is simpler to decompose: 

One big advantage (other than simplicity) is, whereas is for s, and are for s. should be extracted into a function, as others mentioned. However, this code is imperfect: 

Right off the bat, I see you're using arbitrary-width types to represent fixed-width values; though this isn't wrong per-se, it is certainly needlessly confusing. If you use something like , comment it with why you did so. As far as I can tell from looking at the assembly, this never produces different output than a straightforward, standard , so I'm strongly tempted to use the later. Being performance conscious doesn't even mean you get to drop operator spacing, so do more of that! We see that your is late; doing an early test as 

This takes 3.3 seconds for \$n = 10^9\$ for me, where the old version took 11.2. All of the changes are below: 

I don't see how to speed it up further, but this should be a 2x improvement when staying on CPython and a 10x improvement if moving to PyPy (an extra 5x from the better interpreter). 

(not the recommended method for distibution). This is fast, taking ~0.075s. It doesn't support PyPy. Note that if is determined at runtime, it takes closer to ~0.27s, which means PyPy is effectively close to optimal. Just for fun, here's the absolute fastest I could get this to go: 

but I'm still going to pick you up on it ;). should really have a separation of concerns; the logic that needs abstraction is going from the input to the boolean. As such, you should probably have and let the caller call . In changing this, I noticed that you have 

Sorry, Sieve of Sundaram, you're not that great after all. :P And, heck, that Python 2 version is way too fast. 

Every number is a composite of primes $$ n = \prod \text{prime-factors}(n) $$ A factor of \$n\$ is a product of some non-strict subset of \$\text{prime-factors}(n)\$. Technically speaking it's a sub-multiset, but I'll gloss over that technicality in this answer, here and onwards. A proper divisor of \$n\$ (you incorrectly use the term "factor") is a product of some strict subset of \$\text{prime-factors}(n)\$. Consider the proper divisors of \$2n\$. $$ \text{proper-divisor}(2n) = \text{factors}(n) \cup (2 \cdot \text{proper-divisor}(n)) $$ We know this because the \$\text{prime-factors}\$ of \$2n\$ are just the \$\text{prime-factors}\$ of \$n\$ plus a factor of \$2\$, so the strict subsets of \$\text{prime-factors}(2n)\$ can be separated along those that do include the additional \$2\$ and those that do not. Ergo we can consider the statement $$ 2i \in \text{proper-divisor}(2x) \implies 2i \in \text{proper-divisor}(2y) $$ to be equivalent to $$ (2i \in \text{factors}(x) \lor i \in \text{proper-divisor}(x)) \implies (2i \in \text{factors}(y) \lor i \in \text{proper-divisor}(y)) $$ \$2i \in \text{factors}(x) \implies i \in \text{proper-divisor}(x)\$, so this is just $$ i \in \text{proper-divisor}(x) \implies i \in \text{proper-divisor}(y) $$ Consider now the proper divisors of some composites $$ x = \prod_{p \in \text{PRIMES}} p^{k_p} \\ y = \prod_{p \in \text{PRIMES}} p^{k'_p} $$ There are two cases to consider. 

Let's quickly work on that docstring. First of all, note that it's in the wrong place and not in the right voice. 

This is a carry-on from alexwlchan's answer, since he fixed some bugs. As far as I understand, this is wrong: 

does nothing, so should be removed. It's also impossible for it to happen. Now I tidied up the next loop: 

Talking about the tests, I'm going to ignore the framework and focus on the meat of the problem. I would try and get automatic test generation down first. You really want two things. First, for small values do an exhaustive check: 

This swaps an \$\mathcal{O}(n^2)\$ operation for an \$\mathcal{O}(n)\$ one. You only check for is-empty, so this can be swapped with: 

It's worth noting that these aren't much faster than bog-standard untyped attributes, but as long as you're aware they are a good datatype. This is actually why I expect PyPy will give better speed advantages. Instead of just write . In you have , but only add to it once. Giving this a type is pointless, especially because your return casts it to a Python type. A similar but lesser concern exists for . Instead of do . is meant for if you use the return value. You write: 

Anyhow, I'm finding this takes 30-50% less time than the original. There are still more optimizations you can do. This is a more efficient which avoids shifts by holding the moving element. This can be further improved by using to avoid the indexing. 

I (arbitrarily) set (instead of ). This meant that was a valid value, so it didn't make sense in my opinion to use it as a sentinel. I would have chosen instead of if PyPy didn't have a special optimization for lists of only integers, meaning it's faster if I use an integer. If you don't want to use PyPy or don't mind PyPy being a little slower, I suggest using . 

so it is \$\mathcal{O}(\text{zeroCount})\$. This is worst-case \$\mathcal{O}(n)\$ if all of the elements are 0. 

It is possible to avoid this by doing more intelligent work (such as maksing values), but since we search the whole tree this doesn't actually help much. currently looks like 

You can make , the only remaining \$\mathcal{O}(n)\$ aspect, faster by replacing with a list of cumulative counts and doing 

this makes it obvious that something is amiss. Do you really mean and not ? What about ? Should this not be ? What about . Do you mean ? You can also simplify the further by noting that the loops correspond to the function over iterators: 

But this still isn't great as it takes \$\mathcal{O}(n^2)\$ time - we're iterating over every value, and for each of those over every triplet. Rather try the other way around: 

instead. This will automatically do walking of the dependency chain, too, so is technically more efficient algorithmically. 

There are some other minor style points: , and are terrible names, and and are poor. Most variables are temporary, so tells you nothing in a lot of space. could refer to solving any problem. is silly - obviously it's "your" list and you don't even care if it's a list - you just want a sequence. There are other improvements, too. Instead of giving a bounded iterator, produce an unbounded iterator and call . If took a reversed iterator directly, it could be 

This solves your second problem by having a function "extract" the control flow, and use a sentinel as the return to signal failure to match. I used as the "not found" return value because it's significantly faster than the cleaner alternative of returning on PyPy. Times: 

The first thing that irks me with the code is the use of . This is evil incarnate, bring equivalent to . You should use instead. Your can be generated with a list comprehension. I suggest you do so. You should move things out of the global scope, even if you just put it in a function. You get free speed improvements out of it and it's easier to refactor. Your loop is very straightforward, which is good, but there isn't a real need for the second ; we could just calculate the start and ends: 

Your isn't needed - one can interleave reading and writing. Letting take its argument by value, this is just: 

's docstring needs indenting too. Personally I'd use for a circle; it looks much rounded and easier to distinguish. Your comments for 

We can avoid a third of this by not including the highlighted terms. Then, continuing from rolfl's improvement, we get 

Since you seem to by using this for caseless comparisons, note that might be more appropriate - it depends on whether you're using Python 3 (or Python 2 with ) and what alphabets you expect. 

So, your C. Many of the points from the other reviews hold for C, too, but C doesn't easily allow the cool abstractions we were talking about. So here are some lighter abstractions that actually work quite nicely. In some ways I'll be taking this further than with Rust, and in other ways less far. The main challenge is again in . The use of really isn't very nice; traditionally one woiuld try to avoid this with functions. So let's aim for a similar functionality to the I used with Rust: 

and of doing the division before checking instead of hardcoding a coefficient of +288. If this means we get the same amount of code doing more general stuff with better error checking, great! 

In this case just remove the first comment, because is already known to mean deduplicate. And remove the second since it's implied by the fact you've written the code. 

So the next thing to tackle is or , depending on which interpreter you care about most. Going with , I currently have: 

Note that you put the end marker in the edge with the previous character, not on its own. This adds more complexity to the design over the other options, and doesn't allow containing an empty string. So let's reconsider the code 

This is different to Shepmaster's version because you explicitly check for (only check once, though, as there's no guarantee that it will continue returning ). This means the length is exact, wheras Shepmaster's will accept any number of extra ignored arguments. I suppose the next thing to do is delegate to some proper routine and handle resulting errors: 

Rather than pushing performance further here, I would suggest rewriting to a more optimal algorithm like a Sieve of Eratosthenes.