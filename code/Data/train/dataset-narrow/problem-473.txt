A Bit more complex solution would be to create a message in a queue using Advanced Messaging with the Buffered Messaging feature instead of inserting into the table. You could then either have a job that periodically dequeues a bunch of messages and inserts them in bulk, or have a callback on the queue that calls the package to insert the data. This solution would likely have the best performance, but would also be the most sensitive to server failures with the messages stored in memory for a short time. 

As Paparazzi said for MSSQL, just shutdown the database and then decompress the files and startup again. If your architecture allows, you could offline a tablespace at a time and then just decompress the datafiles for that tablespace. Eventually though you will need do do the system tablespace and will need to shutdown the whole system. 

The package code could use the UserRoleID sequence to populate the Id in the Users table and the Id in the Roles table as necessary. The Permissions table could then have elements assigned to roles that are in turn assigned to users and/or have elements assigned directly to users. 

No, you do not have a name collision. Your DB_DOMAIN, DB_NAME, and INSTANCE_NAME can all be identical as long as the databases are on different hosts (as you have indicated). However, as others have stated using the same DB_NAME is a bad idea for anything other than perhaps a recovery operation. A policy enforcing distinct passwords will be broken. Connections will be confused. Most things you can do to ensure that changes are not inadvertently made on production are probably worth the hassle. Here is the relevant documentation: DB_DOMAIN 

You could execute each script using a separate call to SQLPlus and have them run in the background. If you just need to remove errant parameters you could search for them in the scripts and not run those scripts that contain un-supplied parameters. 

When building a new standby database, is it possible to have it start shipping archive logs from the primary without having the datafiles on the standby yet? 

This option uses the select from dual to drive the query and the select from foo only to decide which dates to keep. The same query could also be written as this: 

You will need to run through these steps again when a column in your new table changes, but you won't need to write any new code. An alternative would be do run the SQL dynamically in step 2 rather than have it create a new package, but for performance reasons I like having it generate static SQL. Doing it this way may be overkill. Have you considered just copying the query you have for one table 100 times and then modifying them to have the appropriate table and column names? 

If instead you want feedback when this occurs rather than just ignoring the duplicate, you can nest the INSERT into a BEGIN...EXCEPTION...END block that catches the constraint violation and handles it by doing a DBMS_OUTPUT. 

First we lock the new ProcedureLock table and then update it. If either of these fail we know the procedure is running and therefore we should not run it again. 

PowerShell is installed. Scripting is enabled ( run as administrator). The script is in c:\oracle\WalletCreator. Wasp.dll from Windows Automation Snapin for PowerShell is located in the script folder. 

In case someone comes here looking for a way to find the size of a long field, below is a way to do it. I will remove this answer if the question is separated. Sample data... 

I recommend you read the whole article. Note in particular the warning to sort by something unique so that the results are consistent between pages (given static data): 

As Mihai said, use a LEFT JOIN on feat_attrib_type_4 and feat_attrib_type_5. Either old style (as you have) or ANSI joins style would work. For the former add (+) after every reference to a column in the where clause from those two aliases. For the latter, convert the statement to ANSI join syntax and add LEFT before the word JOIN for those two tables. 

I have setup a Credential/Procedure/Program/File Watcher/Job to monitor a folder for new files, but it is not working. I am trying to figure out what I can check to troubleshoot why this isn't working. After scaling back the code to a bare minimum, here are somethings I have already done. 

Get a book such as the SQL Certified Expert Exam Guide. Whether you take the test or not the books are designed to teach all aspects of SQL, particularly the more advanced usage. 

No, it is not possible to have this single backup command do the level 0 to a different location than the level 1. It is possible to change the script to determine which files don't have level zero backups and do a separate backup just for those datafiles before running the existing command to the incremental location. 

You create the link between the tables with a UserVehicles table containing foreign keys to the Vehicles and Users tables. The HardwareId would never be stored more than once. The VehicleNumber would be stored multiple times only when necessary to show the relationship. No, each vehicles data should be in the same table. 

You were getting close. WIDTH_BUCKET does not allow a subset of the ranges to be specified, you must specify the start and end of the whole range and how many buckets you want that range divided into. In the case of 1-100, 101-200, 201-300, 301-400, & 401-500 your start and end are 1 and 500 and this should be divided into five buckets. This can be done as follows: 

In my 11.2.0.2 Standard Edition RAC environment the above query returns no results, yet the following variations all return results. 

Getters and Setters implies retrieving the data for one row and changing the data for one row. It sounds like you are planning to call a get procedure for each row you want in the file level table and then for each of those rows loop through getting the corresponding data from the batch level table etc. My suggestion is that you write one SQL statement that selects all the data you need from the file level table joining in the data you need from the batch level table. I don't know anything about BI Publisher, so based on your comment, none of this may apply. Perhaps someone familiar with BIP can say whether these parameters could be bound into the query to allow the query to determine what data should be returned. A PL/SQL method may be required for this application, but it needn't be more than a wrapper for your SQL. 

I don't think it matters, but you also don't need to have the duplicate command in a run block. If you haven't already, you may also need to set the db_file_name_convert and log_file_name_convert parameters in the spfile. Documentation: $URL$ 

Adding Date/Time/Timestamp and such at the end is particularly useful when the abscence of the addition would conflict with another column. For example, a table may need both a Status and a StatusTime. 

Interesting question. As far as I can tell, this is not possible, though it seems like it should be. Unless someone else can show how it can be done, a workaround would be to nest the code in a block: 

Yes. A refresh changes the table storing the data that is materialized from the materialized view query. The indexes for these tables are updated just as indexes for "normal" tables would be updated when they change. 

A method we found to be viable was to convert production to use the HSM and then clone the partition containing the key for production to development. This needs to be repeated with each clone of the database and then a post clone step can modify the key for the lower environment using normal key change processes. 

Here is an example of the API structure to replace a trigger. By calling the AddEntry procedure in the package, the data is inserted into both tables. 

You will need to modify this code to use your primary keys, table names and database links, but the nice thing is that you don't need to list all the column names for every table. 

One of the key differences between these platforms is that a database in MySQL is a schema in Oracle. Since a schema in Oracle is also a user, the command you are looking for is . I highly recommend that you read the Oracle Concepts Guide before continuing. You may also find this documentation on MySQL>>Oracle migrations using SQL Developer useful. Here is the part relevant to your situation: 

When this doesn't return the results they are expecting, they realize that it does not include NULLs and would need to write this: 

The page includes much more information on the issue including examples. The "Segment Space and the High Water Mark" section from the Concepts Guide might also be useful. 

Here is an analytic solution using the tables miracle173 built in SQL Fiddle. The analytics alleviate the need to query the Ratings table two three times. 

Unfortunately this requires the swapping the existence of both the public and private database links before and after any deployment that creates materialized views using links. The temporary creation of a public database link with embedded credentials is also a security risk and is in fact a problem we are trying to solve. 

Yes, if you want to prevent a level 1 from being created without a corresponding level 0 being available, then you need to do a crosscheck in the script so RMAN knows that the level 0 is not available. Prior to 10g (or with compatibility < 10.0) Oracle would do a level 0 when a level 1 was done without a level 0. Since you are on 11g, I would expect your compatibility to be >=10.0, so RMAN "copies all blocks that have been changed since the file was created". This behavior seems to have changed between versions and there is some confusion as to what the current behavior is, but your findings are consistent with the documentation. 

Oracle support has been no help. After about a week of the jobs sometimes running they stopped running completely. Every single job got an initialization error every time it ran and every time it was retried. We are in the process of re-creating the repository and migrating to dbms_scheduler jobs. 

If you want each field on a separate line, that is a simple change. If you really do want to have the gathering of the field names be dynamic, then you will need some dynamic sql inside the loop. This would certainly be doable, but not a normal requirement. 

If even an does not remove the session after a bit, then the only thing you are left with is to kill it on the OS level. In Linux you can get the command to kill the process as follows: 

As a_horse indicated, is sufficient. The and multiple conversions just slow things down without accomplishing anything. Even as parts of queries, you should eliminate this needless processing. As Phil indicated, the displaying of dates is controlled by the NLS_DATE_FORMAT parameter and can be altered to display dates as you wish. This does change how the queries run (unless there are missing format specifications or SQL injection is being done). Change the display format like this: 

Depending on your database platform, another option is to delete the tables in databases 2 and 3 and replace them with materialized views of the table in database 1. Pros 

Using the where clause that works and wrapping each field in a to_char returns the following interesting results: 

We are testing a particular HSM to use replace the Oracle wallet and user as a key-store for column encryption. We are using redundant HSMs so that things will continue to run even with a complete loss of one HSM. This works fine when connectivity is lost to one HSM at a time. The problem we found is that when there is a complete loss of connectivity to both devices, a restoration of connectivity is not sufficient to continue to allow decryption of encrypted columns. The only way we found to restore connectivity in this scenario is to restart the instance. The question is, which of these do our findings represent? 

If you need the timing for one execution you could set timing on in an SQL*Plus session, turn on tracing with TIMED_STATISTICS, or add a comment to the SQL so that there is only one execution in v$sql. 

The line requires c.X_ID to be equal to the constant value or for there to be no record from the C table. Of course since it is left joined it won't limit the records from the A table, only limit the records from the C table that get joined. Here is a demonstration: Setup: 

For various reasons these statements need to be run from SQL*Plus. The blank lines are easy to resolve with a simple... 

Justin Cave is correct.+1 If you would like to learn more about roles you can get a good overview from the Concepts Guide. The Security Guide has more in depth information including limitations such as roles not being enabled in Definer rights methods. 

I was able to figure out from the trace file that there was line in the sqlnet.ora file that it didn't like. I narrowed it down to the WALLET_LOCATION line and finally recognized that there was a missing parenthesis at the end of the line. 

Some additional information to go along with the other answers that relates to your question (although not to your specific issue). When you can't build a fast refresh-able materialized view and sometimes even when you can but the same values are being updated in the same rows many times, you need a different technique to keep the replication current while limiting the undo and redo volume. One possible solution is to do a merge statement from your source table to your replicated table. Since the merge is base on the current values without regard to how many times they have changed, a compromise can be made between how up to date the data is and how much redo/undo is generated to maintain the relevancy of the data. For example, if a particular value in a particular row changes an average of three times an hour, a full refresh run once an hour will generate redo/undo for that row even if there are no changes to it. A fast refresh will maintain all three changes and apply them sequentially, but a merge would only apply the latest change and only if the data changed, yet still be as up to date as the other solutions. As a caveat, it may take longer to run, so you will have to test it to ensure it meets required refresh window.