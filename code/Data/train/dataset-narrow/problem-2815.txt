I'd solve this by breaking down an achievement into constitute parts and then randomly selecting values from pools for each part. Based on your examples, you have an achievement grammar that looks like 

The out-of-memory exception likely occurs because the vertex buffer must be backed by system memory in order to handle device loss in the underlying D3D9 implementation of XNA (it's probably in the pool). Transferring the data to the GPU and filling a vertex buffer with the data are the same thing (a vertex buffer is the only place in XNA's API you can store this data). You're going to have to chunk the model file up into multiple vertex buffers when you load the data off the disk. The best way to do this is probably not to the naive thing and chunk it up sequentially, but rather to partition the model using some kind of spatial partitioning scheme (octree, bsp, what have you) so that you can completely avoid issuing draw calls for chunks of the model that are outside the view frustum. This approach will: 

That should look a little more familiar. If we call the whole second half of the equation D, we end up with or in general -- so D, is in fact (-APx - BPy - CPz) where P is the point that defines the plane. This is useful for ray intersection because it will be zero for any (x,y,z) on the plane -- and the point that the ray passes through the plane will be on the plane. That's the computation that the page you linked to is demonstrating. For any point (x,y,z) that isn't on the plane, the evaluation of (Ax + By + Cz + D) will be nonzero. 

is an EffectParameterCollection, which is indexable by string and integer. Indexing it by string returns an EffectParameter for the named parameter, which can be used to assign values to that parameter, as illustrated in the link documentation. In short, the answer to your first question is "yes." 

You can apply to the program right now. You don't need existing hardware. There is no application fee for the program and, if accepted, you get access to two development kits. Whether or not you will be accepted is another matter, and not something we can answer. In the early days of the program, bias will likely be shown to more established developers (in the sense that they already have games that show well to bring to the system), but that does not preclude your acceptance. 

The artists probably won't do this manually quite like you're thinking, but you're on the right track (or at least a viable track). You can generally degrade mesh and texture quality programmatically pretty easily since you probably already have the relevant data. For geometry, you'll probably have lower level-of-detail version of a mesh you use as part of your LOD system when an object is far enough away. These may be manually generated by an artist or may be automatically generated via tools in whatever modelling software you're using (or both). For textures, you'll probably already have lower-detail MIP levels for each texture for similar reasons. You can thus pretty easily hook up a system where you have a few different "texture detail" quality options in your settings, with "highest" using all the available textures and with each subsequently lower setting reducing which MIP level of the texture you load and use for the top quality version. For example, if you normally generate and load five mip levels at "highest" quality, you might only load the bottom four at "high", the bottom three and "low" and the bottom two at "lowest." The same goes for mesh geometry, which you can put on an independent control or just tie together into one simple slider. You can make similar adjustments to other graphics features; for example if you have shadows in your game, you can reduce the size of the shadow map render targets by some factor for every lower value of the "shadow quality" slider, or simply turn them off. Of course it's possible to use these sliders to swap in entirely different collections of art tailored for different performance criteria, but building all of that may be a lot more work and time than can afford, and the cost/benefit over simply degrading through automatically-generated LOD and MIP options may not be there (with the exception maybe of your main character or a handful of other high-visibility items). 

AWS is not as obvious a solution as it seems. It can be expensive, in some cases more so than using in-house data centers (for a publisher the cost of these may be amortized across multiple products, for example). Additionally, there could be concern about storing sensitive binaries and logs on hardware outside of one's control. Finally, actual computation power is not always the reason for segregating the "server" population in MMOs. 

This blog post may address your issue to a degree (it's fairly old but I don't think things have changed that much). If you are only targetting the PC, you don't need to use the networking functionality provided by XNA (which will tend to want Live profiles) -- you can use the standard networking functionality built in C# itself and roll your own networking layer entirely, or use a third party one. Whether or not that is "better" depends on a lot of factors, but if Live accounts are a deal-breaker for you, using something third-party is probably better in that regard. 

You may be interested in this paper on hardware-determined feature edges. In particular, the paper discusses techniques for texture parameterization of feature edges, which can be used to apply dotted/dashed stipple pattern textures to your lines. It is probably possible to do this without that kind of texture parameterization (but still using texture coordinates to transport information about a particular fragment's position along the line segment in the pixel shader) and division to decide which bucket of size N the current fragment falls in to, and if that is an even bucket you color the fragment otherwise make it transparent. You'd have to use the rate of change of the texture coordinates to determine whether the line's slope was such that the line was longer than it is taller (or the other way around) and use that to determine which axis to quantize into the buckets). 

Then, your keyboard callback (and all other static callbacks) can unpack the from user pointer storage and forward to it: 

There are pros and cons to both. Depending on what information you need each command implementation to have available, it may make more or less sense to take one approach over the other. Note that in some cases you may need to expose "debug only" APIs out of a subsystem to implement these commands. Languages with preprocessors, like C or C++, can help you ensure that relevant code is only available in debug builds if you like. In other cases, the second of the above two options often lets you expose debug commands that otherwise access private APIs of the subsystem since the subsystem itself is doing the registration. 

Set the object's property. Note the, per the source as of this writing, that property only exists for Windows projects using DirectX as a back-end. 

I think that your constraints of "don't letterbox" and "support the same viewable area in both orientations" are irreconcilable. I think you need to relax one of those constraints somewhat, and then your question devolves into the already-asked-and-answered question of handling multiple resolutions (see the "Related Questions" sidebar for a collection of links). That said, I think how you relax those constraints is an interesting topic on its own. Obviously you can't relax "support both orientations" much. That's a binary issue: you either support them or you don't. But the letterboxing and viewable area issues are more flexible. Since your game will be relatively unique in supporting both orientations (most games require one or the other), you should further emphasis this point by incorporating some unique gameplay aspects that leverage the decision of which orientation to use. Some things to consider: 

You think XNA has a lot of surface formats? Boy, you ain't seen nothing yet. Joking aside, I suspect the reason has to do with a combination of legacy/backwards compatibility and flexibility. Obviously, once a surface format is introduced it needs to be supported forward for some time, or the entire API takes a backwards compatibility hit. So there are probably a few surface formats that float about just due to that reason alone. But the bigger reason is the flexibility that is afforded to developers by having many surface format options to choose from -- this allows developers to do things like use as few bits as possible for a particular channel, or as few channels as possible for scenarios where optimizing space is an important consideration. It allows for the possibility of expanding to very large bit-widths when broad ranges of color data are going to be required (for example, for HDR, as you mentioned). Some formats exist for relatively specialized (but very common, possibly performance critical graphics operations) tasks -- the format, for example, is commonly used in bump-map related operations (per the documentation). Some of the strange looking formats can be useful for non-graphics operations, too, like general GPU computation where you may only need two channels of input data (and thus, why waste space on the third when that video memory could be put to better use -- such as allowing more of the two-channel data you're trying to process?) Having a variety of formats also allows the developer to communicate to the GPU how the data will be arranged and used (this is why the DXGI format list is so big, because it not only includes bit-width variations, but some type information as well, such as whether the values will be signed or unsigned), which in turn can allow the GPU to optimize its buffer layouts for maximum pipeline throughput. Finally, some of the formats are restricted in where and when they can be used, which necessitates a list that may seem sprawling at first, but is actually relatively compact for any given scenario. Relatively few formats can be used for a depth buffer, for example. 

It seems very odd that you see the performance of your game fall to 1 FPS. It's likely there is more to that than just the texture binding, but that is not really relevant to the issue, because it is still true that binding a new texture for every draw call is expensive and will have some negative impact on performance, which you are looking to alleviate. Unfortunately, the technique of using texture atlases is designed to help combat exactly the problem you are seeing (a sprite sheet is just a specialized texture atlas where every sub-image is stored in a cell in a regular grid). State changes on the GPU are slow and should be down as infrequently as possible. This includes changes of texture and shader state. In your case, you're changing the texture every time you render a new object, which is just about the worst-case scenario you can have without being intentional about degrading performance. Minimizing state changes improves performance, and your middle ground solution (to draw everything using the same texture at once) does eliminate a lot of state changes. It's a valid approach, and one that is commonly used. Sprite sheets or texture atlases allow you to further minimize your texture state changes, and are consequently superior if you can use them. So unfortunately, the answer to your question is you can't improve this scenario(*) beyond what you have described as your middle ground solution or using the sprite sheets you explicitly don't want to use. (*) I am speaking only of the texture-state-change-related performance, obviously. I still suspect that something else is wrong in your code, but that's neither here nor there. 

No, Ogre itself doesn't seem to have an oriented bounding box class (according to the API documentation). An OBB is a mathematic concept though, and a properly-designed interface to one isn't tied to a particular rendering engine anyhow. Thus you could build your own, possibly referring to implementations you can find around the Internet, or utilize one of those implementations directly (presuming the license permits it). 

Direct2D bitmaps are hardware device dependent resources and not generally easily accessible from the CPU side of the system. They don't even have lock or map methods. This makes it difficult to get at the pixels of a bitmap; it's not really designed to be used that way. If you only have the D2D bitmap object available, you can render it to a -backed render target, and from there you can get the 's device context and with that you can create a compatible bitmap and blit from the render target DC to your new bitmap's DC. This is obviously exceedingly clunky. However, in your link you indicate that you are passing through the WIC bitmap source object to load your pixel data. If you cache that object for your level editor, you can more easily obtain the pixels, since has method you can use to dump the bitmap data into a raw byte array (although the method may be better if you want to avoid a lot of copies). From there you can create an or from the raw bytes in the traditional fashion. (If your bitmap ended up backed by a D3D texture or DXGI surface instead, you could also use the pixel access methods of those interfaces.) 

The APIs available on Windows and the 360 are similar, but they are not the same. It's not really possible to provide more details without running afoul of NDAs, really. However, you could look at the difference between how XNA operates on Windows and on the 360 for some vague, rough ideas of about the class of differences you may expect to see, at least in terms of how behavior and functionality can differ even though the API remains the same. 

Presumably you have the center point of the ring (), and the radius of the inner and outer segments of the valid portion of the ring ( and , respectively) Then, given any touch point , you simply need to perform two point-in-circle tests. The touch is valid (within the ring) if it is both outside the inner circle and inside the outer circle. A point is within a circle of radius with a center at if it satisfies the inequality: 

The short answer is no. The longer answer is that you can (and should) specify the hardware requirements of your application via your application's manifest. You should read the section on MSDN explaining this as well as the available sensor capabilities you can expose. Essentially, you add the appropriate keys from that page to the element of your manifest file: 

Then use to submit the draw command to the card. Note that you'll likely need to adjust the coordinates of the vertices depending on your current projection matrix (et cetera); the provided coordinates will fill the screen when the transformation pipeline is configured with a series of identity matrices, but is only really suitable for initially bootstrapping rendering. 

It's not clear from your question whether different player type enum values correspond to actual different player classes or just instances with different data (which is probably the approach I'd vote for, but I don't know anything else about the nature of your game). However, whichever it is, the above approach will work fine. If construction of your players becomes complicated or you have a lot of player types or permutations, the above code can present a maintainability problem as it scales. To handle that, you can build your implementation towards an abstract factory pattern, which can afford you some interesting options for data-driving your player creation, or at least keeping the maintenance overhead down. 

Now, whether or not you should do this is a different question. Certainly, if you game and its render needs are simple enough and you prefer this style for some reason, you can create a game that way. But as the complexity of a project scales, you can run into some issues: 

Use the smallest type that makes sense. 3-components vectors are smaller, but may involve "overhead" when converted to 4-component vectors for some operations. 4-component vectors, on the other hand, are larger and thus have some space "overhead." In most cases "overhead" in either case is negligible. However, the additional storage taken up by a 4-component vector is not negotiable. It's always there; the additional conversion required of a 3-component vector can be deferred or amortized in a lot of cases, and doesn't happen constantly (or if it does, then you've found a scenario where you should be using the larger type). Plus, it is far more likely to be optimized into a complete non-issue. So you should evaluate the context in which you are going to use the types, and pick the type that will achieve the most optimal storage/runtime overhead for the majority of the use cases, and which also represents the data in the most accurate fashion for the use cases involved. If you don't need four components, don't store four components. Wait until that is a demonstrated problem before worrying about it further.