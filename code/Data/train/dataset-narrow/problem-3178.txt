When you build a classification or regression model, you typically split the data into a train data set and a test data set. The test data is a randomly selected subset of the overall data. Once you are done with the training, you discard the test data, and apply the model you built to new unknown data. But in the case of time series, this wouldn't work: You can't just randomly choose any subset of the data for your test data set, since the data is ordered. The test data set has to be composed of later observations of the data, while the train data set is made up of earlier observations of the data. Say for example that you have data for 11 months of the year, and you want to forecast the values for December. You would train your model with the data from January through September, and then test it on the data from October and November. But after you've successfully trained your model, what do you do with the test data? On one hand, it doesn't make sense to forecast the values for December using a model built with Jan-Sep data. Such a model would miss any important trends that occurred in Oct and Nov. On the other if you bring the data from Oct and Nov back into the model, the parameters of the model will change, and so we're no longer sure that we are going to get the same accuracy that we got when we trained the model with just the Jan-Sep data. There seems to be a dilemma here. So how is this problem approached when using machine learning models, especially non parametric models like Neural Networks for time series forecasting? Do they incorporate the test data into the model or do they discard it? 

I think your weight function should depend on $d$ as well. The contribution from a single dimension is just the sum of incoming weights. Even for simple graphs this does not seem to have a standard name, although it is quite commonly encountered. In the context of random walks it is sometimes called simply degree, in the context of empirical networks, I found node strength. I think this is a simple and natural statistic to look at if you wish to model something as a multidimensional network. However, being a linear sum of rather trivial functionals over simple graphs, I doubt it is of much fundamental interest. 

I assume you are looking for a similarity measure between items. A quick and simple one is item-item cosine similarity. An item (product) can be represented by a vector $x$ with $x_i = 1$ if it was in the $i$th purchase, otherwise $x_i=0$. The similarity between two products is then $$ \frac {<x,y>} {\lVert x \rVert \lVert y \rVert} = \frac{x_1y_1 + \dots + x_n y_n} {\sqrt{\sum x_i^2 \sum y_i^2} } = \frac {\mbox{number of times 1 & 2 sold together}} {\sqrt{\mbox{number of times 1 sold} \times \mbox{ number of times 2 sold} }} $$ Matrix decomposition techniques can probably give better results, but they might require a bit more knowledge of linear algebra or finding some ready-made products. 

I just started working with PySpark this week, and the instance I have access two has Pandas installed. But what use is having Pandas on Spark? Isn't the whole purpose of running scripts on PySpark to overcome the limitations of packages like Pandas? Does Pandas performance improve if it run on Spark? Is it compatible with Spark's RDD ? 

Some useful free real world time series data for testing and benchmarking are: The sunspot time series. Hyndman's data library. And I don't know the technical details, but you can download all sorts of financial times series (trading and stock indexes) from Yahoo and Google. 

I've invested lot of time trying to understand the theoretical aspects of Deep Learning and Neural Networks - but I'm now questioning whether it is worth it or not, given that I am someone who works mainly on the applied/business side of things. By advanced theoretical knowledge, I mean things: Like understanding the details of how the Kernel trick works in SVM, the "no free lunch" theorem and its relationship to machine learning, the details of how neural networks are universal approximators, the VC dimension of a classifier, etc... By applied Data Scientist, I mean someone who solves business problems using existing algorithms as opposed to someone who develops new ones. So my question: Are there situations in an applied Data Scientist's life where such theoretical knowledge is useful? Or is this type of knowledge useful only for people who work on developing new algorithms? 

I would rather look at mixture models. Or, if there is additional noise besides the lines, use computer vision algorithms, for example, some variation of the Hough transform. 

Nothing too surprising here. As you sample more and more words, the sample mean is a better and better estimator of the population mean. This is called the law of large numbers. 

I know Facets $URL$ - which should be nice for a preliminary in-browser preview (I have not used it much). 

You can't retrospectively do it just with counts of unique visitors per day. If you represent the unique users on each day by sets $A_1, A_2, \dots, A_n$, the union can be as small as $|A_1|$, if all sets are equal, or as large as $|A_1| + \dots + |A_n|$ if all sets a pairwise disjoint. If you could estimate average number of days r per month a user visited the site (over the users that visited at least once), then there were exactly $(|A_1| + \dots + |A_n|)/r$ unique visitors that month. A nice proper way to do this, starting with new online data, is to use Hyperloglog or other memory-efficient approximate counting algorithms, as Marmite Bomber suggests. 

I'm training a Neural Network in Keras (with Tensorflow backend) on the IRIS data set. After fiddling around with the number of layers and neurons a bit, I got to a point where the accuracy on the test set is 0.99 - OK, Great! Success!!! Then I retrained the model and re-evaluated - and ended up with accuracies between 0.7 and 0.97 - I never got back to that original 0.99, and this despite using the exact same configuration and training parameters every time. How do we avoid this problem, especially if we want to run out NNet in production ? 

Consider the following example: You have a rare disease whose occurrence seems to depend on a certain number of variables. You build a model which tries to predict patients most likely to be effected by the disease which is partially successful, that is, it predicts likely onset of the disease with some accuracy, but less than desired. Ideally, you update your model as more historical data on patients with this disease comes in, and the accuracy starts to improve. Eventually you start notifying high risk patients and provide them with steps to counter this disease. Because of this, more and more patients who would have been classified as high risk are actually not catching the disease and therefore decreasing the accuracy of your model. In a sense the model was a 'victim' of its own success. Are there any strategies for dealing with such prediction scenarios: Where a model designed to predict an undesirable outcome looses accuracy due to it successfully averting the outcome in real world cases? 

As a workaround, you could minimize another function that includes both the objective and the constraints, then check if sol.fun is (numerically) equal to zero. 

A counterexample in dimension 1: take three points: 1, 3 and 4 on the x axis. When you sample two points at random, the expected sizes of the bucket each point falls in are 4/3, 2 and 5/3 respectively. But maybe your hypothesis is "approximately true" for data from some good enough distribution (for example, uniformly random points on a sphere). 

For in-memory computation wouldn't it be simpler to start with Python or R and one of their machine learning libraries? If the data does not fit into RAM, you can load only a part of it (for example, load only p fraction of vectors by loading each vector with probability p at random). Once you analyse your data, and understand which methods work best, you can scale up your system and rewrite your code in one of these distributed computation tools (or perhaps you find that you don't need this at all). 

You might want to try a Bayesian classifier instead of SVM or Decision Trees. Your problem is a generalization of the spam filter problem, i.e. classifying emails into 'Spam' (class 1) or 'Not Spam' (class 2). Most Spam filters use Bayesian classification instead of the more common supervised learning method. 

Data Analysis and splitting the data into test and train sets. Choosing a model. Training the model, and testing the model against the test set. If the accuracy of the model is not acceptable, start over with a new model. 

Step (2) can be automated to some extent using a grid search, but are there ways of automating the whole process? I'm thinking in particular of large scale data applications (for example when dealing with retail data, or customer analytics on a site like Netflix, ) where there are millions of instances of similar but distinct machine learning problems that each need to be trained and validated separately. In such a situation it is impossible for a team of analysts or data scientists to perform the above steps and some sort of automated model development framework must be used. What are the frameworks that allow for this?