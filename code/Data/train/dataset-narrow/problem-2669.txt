Imagine I have a skeleton — that is a set of bodies held together through various constraints and joints — and I want to flip it. Bodies cannot be flipped in Box2D, so how can I fake that? Here's an example: I have a humanoid skeleton, made out of different Box2D bodies: the head, torso, upper arm, lower leg, etc. These Box2D bodies are held together by joints. Some of the joints have angle constraints, like the ones between the upper and lower arm, which do not allow the 'elbow' to twist unnaturally. When I turn my character the other way around, I should flip the skeleton and its joints should be flipped as well. How can I achieve that in Box2D? 

I'd like to know if someone has found a way to build a component-based entity system in their game(s) without using IDs. I find that IDs tend to do away with one of the major (possible) advantages of OOP: not having to be aware of the type of a certain object. 

When positioning manually One piece advice, though, if you decide to position your widgets manually: don't use absolute pixel coordinates. If you your screen is 400px wide and you want a widget's X coordinate to be 200px, set it to , instead. This will allow your widgets to be position correctly, even if the screen size changes and it takes no extra effort on your part. 

You mention doing frustum culling on individual blocks — try throwing that out. Most rendering chunks should be either entirely visible or entirely invisible. Minecraft only rebuilds a display list/vertex buffer (I don't know which it uses) when a block is modified in a given chunk, and so do I. If you're modifying the display list whenever the view changes, you're not getting the benefit of display lists. Also, you appear to be using world-height chunks. Note that Minecraft uses cubical 16×16×16 chunks for its display lists, unlike for load and save. If you do that, there's even less reason to frustum cull individual chunks. (Note: I have not examined the code of Minecraft. All of this information is either hearsay or my own conclusions from observing Minecraft's rendering as I play.) 

If that seems hard, then you should change how you do it so that it's easy. It's not clear from your question what makes the case problematic, but I suspect that it's that you're trying to load chunks based on how the player/camera is moving. This is a bad idea, as it's much simpler to do something which works independently of movement. First, if you don't have it already, you should have a data structure which stores loaded chunks keyed on their chunk coordinates. Example of chunk coordinates: The chunk containing the "middle" of your world is at coordinates . Then the chunk immediately to the right of that one is , and the one below that one is , and so on. Then you can use those coordinates as keys in a hash-table or similar to keep references to the chunks you've loaded. Every frame, do the following: 

Is there any editor out there that would allow me to define complex entities, with joins connecting their multiple bodies, instead of regular single body entities? For example, an editor that would allow me to 'define' a car as having a main body with two circles as wheels, connected through joints. Clarification: I realize I haven't been clear enough about what I need. I'd like to make my engine data-driven, so all entities (and therefore their Box2D bodies) should be defined externally, not in code. I'm looking for a program like Code 'N' Web's PhysicsEditor, except that one only handles single body entities, no joints or anything like that. Like PhysicsEditor, the program should be configurable so that I can save the data in whatever format I want to. Does anyone know of any such software? 

An event system will do for a lot of cases, but that is more suited for propagating information that changes or is generated 'rarely' (like the death of the player). For something like entity health and position — which need to be known every frame — an event system isn't well suited. 

I have a very general question: In games, what use does the programming concept of a window have? Or, in other words, why do some game dev libraries offer interfaces through which to create multiple windows? — Why would you need more than one windows in a game? Are multiple windows used as different views/states of the game? (I.e. in-game, main menu, pause menu, etc.) 

If you do this completely, you will have a list of connected regions of the map (a partition of it into connected components). If your map is static, you only need to do this once. Otherwise, it's still not hard: If a node becomes unwalkable, rerun just on the component containing it and you may find you have two components. If a node becomes walkable, then if if it has immediate neighbors from two different components, combine them. 

Unless you explicitly set up non-power-of-two (often imprecisely called "rectangle") textures using an extension, each dimension of your texture must be a power of two. To draw an image with a different size, add margins so that it has power-of-two sizes, then adjust your texture coordinates (‛glTexCoord‛) to crop off the margins. 

If your game is real-time: You should make sure that as soon as a movement is completed, if a key is currently pressed, the character will immediately start moving in that direction. This ensures that the game does not require the player to have exact keypress timing to move efficiently, which in my experience is very frustrating. There are two possible ways to do this: 

(Similar problems can happen when trying to collide against meshes, since meshes specify surface rather than volume.) You should also consider whether you really need more than one box to get the gameplay effects you're looking for. This is all largely speculation since you haven't specified what your collision shape is in detail. 

I need some advice on how to design the Entity module in my game, how to apply the MVC pattern and generally how the Entity should interact with its controller and its representation. First some details about the game: it's a 2D action-platformer, it's data-driven (or rather, I'd like it to be data-driven — it isn't yet) and the Entities should be scriptable (I'll be using Python). I'm still at the very beginning of the development and I have some general ideas about how the Entity module should work, but I get stuck on the details. Here's roughly what I have up until now: 

I'm in a bit of a dilemma regarding how certain engine components — like camera and UI — know who to follow, whose health and other attributes to represent on the screen. How do you architect a system where does communication between those components and the entities take place? I could have a separate entity that represents the player, but that seems a bit 'hard-coded'. What if I want to pan the camera? What if the player starts controlling another entity? In other words, how do I abstract away the data sources for components like the camera and UI, such that they don't care what entity they represent? 

I'd like to know how others have handled the issue of storing the entity's position. (Or maybe it's not an issue and I just make it too complicated.) I'm undecided on whether to store the position of each entity in the world map file or in the entity's script file. From what I figured, both approaches have their own good and bad points: If you store the entity's position in the map: 

filter is GL_NEAREST you are using a incomplete mipmap pyramid with 1 level your Uvs are not normalized vertex components (but you have to normalize them in the shader) 

The point is that over optimizing for performance is likely to miss the most importan optimization point: human. As side point, if component A is coupled with component B, there exist the chance that the coupling can be moved into a component C (again, this not always happens, but try to look for that) 

Since there's no accepted answer I add some info, I wanted just to add things not already said by Sean in his answer. TexelFetch treat the texture as a Image, so you can access exactly the content of pixels. You usually do that when you need exactly that content, wich is in few but usefull occasions: 

(this assumes you are using standard UV so coordinates between 0 and 1.. if not just replace 1.0f with your MAXUV values 

Dose not increment UV coordinates unconditionally, but reset them after sometime. UV coordinates (even if you use integer coordinates) are converted into floating point inside GPU registers and so are subject to typical loss of precision of floating points. Instead of doing 

This not claim to be a complete answer, but trust me I were in you, I tried to make a rendering engine with similiar choices etc. Decoupling is possible until you start incurring the cost of integrating togheter the decoupled components. If at logical level, there's coupling you just can't get rid of it in the code unless you can think a way to decouple it at high level. Many real software systems exists and are hell to change because are highly coupled, but there's no one assuring to you a less coupled solution exists. Maybe you can refactor here and there but if you have a goal that requires a certain solution, fighting to change that solution is likely to change the problem (I'm not saying your particular problem does not have a solution). Also this is the kind of questions that is unlikely to receive an answer because everyone has his super secret rendering engine best of all and don't want to share details about underlying architecture. I'm of the opinion that something nice could born only when some people start sharing ideas and do some group brainstorming about it. Groups may be eons better than individuals. Also some of the solutions you discarded because are "bad" are in reality not so bad, I tried them and are widely used by notable frameworks and rendering engines. 

My question: Can I configure PhysicsEditor so that it saves shapes in this format automatically? If not, is there some software that allows that kind of configuration? 

What good free and widely used tools are there for editing 2D skeletal animations? Preferably, one that allows me to write custom animation exporters. One pretty good indie tool that I know of Demina, but it's not ideal. It doesn't allow you to export the data as you want (although, it is open source, so you can change that) and I find it clunky in how you edit individual joints. What other tools would you recommend? 

How do you usually solve collisions between entities and the ground? Sending collision events hardly seems appropriate since almost everything touches the ground at almost all points in time. Calling collision handling functions doesn't sound any better. How is this normally achieved? Is handling of slopes significantly harder? 

I'm writing a component-based entity system and one of the components is the entity's state, which dictates how it reacts to game events. In case anyone has experience with implementing states, how granular should they be? To give you an idea about how granular they are in my case, I have a state and a state (as opposed to simply a state) and I fear that they might be too granular. What do you think? 

If you have only a basic knowledge of C++, I don't think that's going to be enough to (easily) develop something like a platformer. Python is an easy to learn language so it's not a bad choice at all. If you pick C++ I recommend you use SFML for graphics and sound, and either Box2D or Chipmunk for physics, if you want such a thing. If you do pick Python have a look at Pygame (graphics) and Pymunk (physics). But developing a platformer isn't exactly easy; you might want to start with something easier, like an adventure game (where you only click around the screen and you don't have to worry about complex character interactions). 

The difference between these two cases is whether a key which is pressed and released before movement becomes possible will still cause another movement. I recommend trying both to see which feels better. If your game is turn-based: I recommend, for turn-based games, that you take your second approach: use all the keypresses you got, in sequence. Furthermore, if you have keypresses buffered like this, speed up your animations. This allows the player to quickly move through parts of the game they're replaying without it getting tedious. If it is infeasible to speed up the game when there is buffered input, then you should either ignore input or buffer exactly one keypress (first or last doesn't matter much). 

Your option #5 — generating shadow textures — has a low per-frame cost compared to performing blurs, is readily customizable, does not introduce a lot of complexity in other areas of the rendering process. I recommend trying it out and seeing whether the memory/startup cost is too much before investigating the other options. Since you are using the depth buffer, you will need to draw the shadows after the opaque sprites and use a subtractive blend function, to avoid drawing-order dependencies. 

I've confirmed by eyeballing a graph that this gives reasonable numeric results. However, it is not obvious that this will improve performance at all, since we are doing more high-level JavaScript operations. I have run benchmarks on the browsers I have handy and found that takes 50% to 80% of the time taken by (Chrome, Firefox, and Safari on macOS, as of April 2018). Here is my complete test setup: