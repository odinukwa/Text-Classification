Deep learning is generally not well suited for domains that have only very small training datasets. This includes fields like medicine where, for example, patient data can only be obtained through expensive and time consuming clinical trials. Clinical trials will often contain only several hundreds records. By design, deep learning models have many parameters (e.g., millions), and like with any machine learning algorithm, you're likely to see overfitting when the number of parameters greatly exceeds the number of training records. 

If the old variables and the new variables are highly correlated then you could do a more advanced form of imputation and make a model for each new input that predicts the new input given the old inputs. This model would probably be pretty effective good at predicting the new inputs because, as you said, there is a strong correlation among the inputs. Then you would split up all of your data across the years so that you have an equal proportion of old records and new records in your training, validation, and test sets. 

There is no technique that will eliminate the risk of overfitting entirely. The methods you've listed are all just different ways of fitting a linear model. A linear model will have a global minimum, and that minimum shouldn't change regardless of the flavor of gradient descent that you're using (unless you're using regularization), so all of the methods you've listed would overfit (or underfit) equally. Moving from linear models to more complex models, like deep learning, you're even more at risk of seeing overfitting. I've had plenty of convoluted neural networks that badly overfit, even though convolution is supposed to reduce the chance of overfitting substantially by sharing weights. In summary, there is no silver bullet for overfitting, regardless of model family or optimization technique. 

How would someone go about using deep learning to classify sign language gestures? For example, suppose I had video files of many different gestures. For any given gesture, I might have many videos of it, and each video would be comprised of many frames. When trying to classify MNIST digits in images the dimensions of the input are comparatively simple: height, width, and RGB channels. How would gestures (multiple frames over time) be accounted for? Would time be a fourth dimension? What should the neural net's architecture look like so as not to overfit? Should I use something instead of a convoluted neural network? Edit: I know that there are probably clever ways to hand-code predictors for sign language, but I'm more interested in how to architect the neural net and take advantage of the time component of the data (where there is value in the transition of video frames over time). Classifying gestures is a simplification of the actual problem I'm trying to solve, so I'm looking for an approach that would be generalizable to other types of problems where it might be necessary to look at many frames of the video to predict the target variable. 

Low decision threshold: As is often the case in class imbalance problems (e.g., fraud detection, marketing response, bankruptcy, etc) a low probability could still indicate the minority class if the decision threshold is also low. In your case, if the model might have automatically decided to pick a threshold of 0.05, then 0.1 would be flagged as C1. Cost function: This is what you mentioned -- if your model has a high cost associated with missing true positives then 0.1 could be classified as C1. 

You can treat the missing feature as the target variable of a sub-problem and create a classifier (e.g., a linear model, SVM, etc) for it. 

This is not the answer you'll want to hear, but I would say, "no." Priming is not a good idea. A model is only as good as its inputs. If you're making up the inputs yourself, then your model isn't learning real patterns and you might as well just hard code a set of rules that represent how you would have created the primed data. A fancier way might be to make a bayesian model where you create priors based on your assumptions. The bayesian model would then evolve as real data becomes available. 

Unfortunately you're not going to be able to do much without at least 200-300 records. You're going to be limited to simple (i.e. mostly linear) models until your dataset expands to at least 1,000. Anything less than 1,000 will require very thorough cross validation, and if you're not careful you'll be at risk of building a model that easily overfits. @EricLecoutre makes a great point that you should use Amazon's Mechanical Turk. It usually costs just a penny or two per record and could save you a lot of time. 

Inspired by Google's recent AlphaGo project, I've decided that as a fun personal challenge I'd like to use deep learning and convoluted neural networks to build an algorithm that can beat an ordinary chess program when the difficulty is set to expert. I'm awful at chess and generally can't beat a computer beyond easy/medium, so making something that's a lot smarter than I am (at chess, at least) sounds fascinating. Question: What should the target variable look like for predicting the next best chess move? I've been building predictive models for a long time, but I can't wrap my head around this question. Some things that have me confused: 

I've been using tree-based enesembling methods such as random forests and gradient boosting for several years now, and I have to say that I've never seen that behavior. Some packages measure variable importance solely based on trees' final splits rather than candidate/surrogate splits, so if you have two important inputs that are correlated, but one is consistently a tiny bit better than the other, the less important input might never get selected as a final split and thus look as important as some of the less valuable inputs. However, this phenomenon is independent of the number of trees/ensembling, so I don't think it fully explains the behavior you're seeing. 

PCA simply finds more compact ways of representing correlated data. PCA does not explicitly compact the data in order to better explain the target variable. In some cases, most of your inputs might be correlated with each other but have minimal relevance to your target variable. That's probably what is happening in your case. Consider a toy example. Lets say I want to predict stock prices. Say I'm given four predictors: 

The two languages have pretty similar benefits since Scala can call Java libraries. So Java machine learning packages like Weka ($URL$ can in theory be easily used with Scala. There are minor pros and cons to each, however: 

There isn't a well established way of estimating the number of data points that you'll need. It's much more an art than a science. As you gain more experience, you'll learn some common sense lessons (in hindsight) along the way. For instance, you should never have more parameters than data points; if you're building random forests, you should not have more trees than data points. If you're doing deep learning, you should never have more neurons than data points (these are extreme examples). As a rule of thumb, try to avoid having 10 times more features than you have data points. 300 data points is very small, and you so you should probably limit yourself to linear models. As Hobbes mentioned, cross validation and/or holdout sets are the proper way to judge the readiness of your model irrespective of the datapoint-to-feature ratio. If the error on the training set is 10% better than the error on the test/validation set, you're probably overfitting and your model is not fit for production. The 10% threshold is very much a rule of thumb. Also worth mentioning, once you feel comfortable that your model is fit for production, you should never deploy it and then just walk away. If your model has value, you should do ongoing post production monitoring. In the real world, your model's performance will inevitably degrade over time (regardless of how strong/accurate it was at the time of the model build) as the environment that your model was trained for evolves. This is certainly true for finance and insurance industries. Production monitoring can also be used to pull the plug if your model performs far worse than you anticipated. If you have a very low opinion of the potential stability of your model, deploy it silently (don't allow the predictions to go to downstream systems) and measure how your model would have done had you hypothetically deployed it. A last piece of advice: when you do predictions on new documents you'll nearly always come across words that you have not accounted for in your training set. You should find a way to build this awareness into your model. For example, you could choose to use only keywords as predictors or you could create indicator variables for new words. Either way, you should not expect the vocabulary size to remain the same. 

Parquet and Avro both support data types (strings, integers, floats, etc). These are the primary file types used for "big data" projects, although your data doesn't have to be big. Apache Spark is able to read both with ease. Having said that, I'm a big fan of keeping data in a SQL database (e.g., MySQL or Postgres) because that is what they are built for. If you can't re-use the database you're pulling from, could you make your own database locally or on a separate server? I would try using a relational database until your data exceeds 50 GB (an arbitrarily "somewhat large" size), and then I would use Avro or Parquet. 

It's much easier to identify projects where a neural network won't work than to identify projects where it will work. For example, if you have fewer than 10,000 records (highly subjective rule of thumb), then a neural net will probably overfit and not work. If a human struggles with the labels, then a neural network will probably struggle too. In my subjective opinion, your problem sounds like it will be difficult for a neural network (or any model) to learn without a substantial amount of records. You would probably need several hundred thousand at a minimum, not including data augmentations like flipping images, etc, or something on the scale of an ImageNet dataset (i.e., millions). I say that because even as a human being I don't know what those collar types are that you described. On a dataset in the low tens of thousands you could probably train a convnet to identify shirts vs pants, but I highly doubt it could accurately detect collar types. 

Can I conclude from all of this that the squared error of an autoencoder prediction is just the average across all of the record's dimensions? 

I recently came across Matt Zeiler's deconvolution (reversing convolution) paper . How is deconvolution able to reverse the rectified scalar output? From what I understand it sounds analogous to reversing the output of a linear / logistic regression model. How is it possible to go from a scalar to a vector? 

I've recently started using OpenCV's python implementation, and I found some good OpenCV tutorials on this website: $URL$ that I really liked. OpenCV allows you to do Haar Cascades for fast facial recognition (by default it doesn't use a convoluted neural network but an optimized implementation of Ada Boosting that evaluates frames in stages for faster processing). OpenCV converts each frame into a multidimensional numpy tensor/matrix that you can then feed into into your ML algorithm (e.g., in TensorFlow or some other library), although I think most people just use the built-in OpenCV face classifiers. In any case, I believe OpenCV can process up to 70 frames per second, so it should be fast enough for you. The original paper that invented Haar Cascades: $URL$ The OpenCV documentation that further explains Haar Cascades: $URL$ 

I had the same problem when I used TensorFlow to build a self driving car. The training error for my neural nets bounced around forever and never converged on a minimum. As a sanity check I couldn't even intentionally get my models to overfit, so I knew something was definitely wrong. What worked for me was scaling my inputs. My inputs were pixel color channels between 0 and 255, so I divided all values by 255. From that point onward, my model training (and validation) error hit a minimum as expected and stopped bouncing around. I was surprised how big of a difference it made. I can't guarantee it will work for your case, but it's definitely worth trying, since it's easy to implement. 

If all you're trying to do is identify the most frequent/recurring buyers, then the answer should be as simple as putting the data in a relational database (or if it's a lot of data, then Hadoop for Spark) and doing a simple SQL query to get the top ranked customers: 

Yes, they both measure the exactness of y and y_hat and yes they're usually correlated. Sometimes the loss function might not be accuracy but you're still interested in measuring the accuracy even though you're not optimizing it directly. Google's TensorFlow MNIST example minimizes/optimizes cross entropy loss but displays accuracy to the user when reporting results, and this is completely fine. Sometimes you don't want to optimize accuracy directly. For example, if you have serious class imbalance, your model will maximize accuracy by simply always picking the most common class, but this would not be a useful model. In this case entropy / log-loss would be a better loss function to optimize.