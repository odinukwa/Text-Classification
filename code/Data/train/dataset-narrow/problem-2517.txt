The latter two problems are not known to be efficiently computable even with oracle access to #P. However, they are computable within the so-called "counting hierarchy". For some more natural problems classified within this class, see e.g. this recent paper. Counting Nash equilibria is apparently #P-hard, see here. Also, even problems where the search problem is easy can be #P hard to count, e.g. counting perfect matchings. 

This doesn't answer the question, but it might be helpful. Mossel and Umans have made a detailed study of the complexity of approximating VC-dimension, when the set system is succinctly presented: On the Complexity of Approximating the VC Dimension $URL$ 

A new paper by Demaine, Demaine, Eisenstat, Lubiw, and Winslow makes partial progress on this question---it gives a polynomial-time algorithm for optimally solving $n \times O(1) \times O(1)$ cubes, and shows $\mathsf{NP}$-hardness for optimally solving what you might call "partially-colored" cubes. It also shows that the $n \times n \times n$ cube's configuration space has diameter $\Theta(n^2/\log n)$. Sweet! One possible next question that their work seems to suggest: is there a fixed family of partially-colored $n \times n \times n$ cubes, one for each value of $n$, such that optimally solving from a given configuration is $\mathsf{NP}$-hard? 

Great question! Short answer: no implication like $$ \mathsf{P} = \mathsf{BQP} \Rightarrow \mathsf{IP} = \mathsf{AM} $$ is known; but that doesn't mean it's not worth trying to prove... I would say, though, that finding such an implication seems unlikely. I think the message of much quantum complexity theory is that, while quantum computers are not an all-purpose panacea for solving hard problems, they can be much more powerful than classical computers in certain specific circumstances. For example, in query complexity, quantum algorithms can efficiently solve certain problems classical ones provably cannot, when the input is promised to obey some nice global structure. E.g., Shor's algorithm is based on an algorithm to quickly find the unknown period of a function promised to be periodic. On the other hand, quantum query algorithms are not too much stronger than classical ones for solving problems in which there is no special structure assumed on the input. (See Buhrman and de Wolf's survey on query complexity for this last point.) Similarly, I think the results $\mathsf{QIP}(3) = \mathsf{QIP} = \mathsf{IP}$ tell us, not that interaction is unexpectedly weak (even if $\mathsf{P} = \mathsf{BQP}$), but that quantum computation is unexpectedly strong, specifically in the context of interaction with computationally-unbounded provers. 

The hypothesis that the Polynomial Hierarchy does not collapse has been one of the most fertile paths to discovery in complexity theory. Many of these results can be phrased as saying that specific algorithmic tasks are "on the P-side" or "on the NP-side". $PH$ non-collapse seems to have many more consequences than the weaker hypothesis $P \neq NP$, and it would be impossible to corral them all into a single brief post. Let me just give three examples that give a small sense of the variety of this work. 1) Suppose we are given as input a circuit with "oracle gates," that makes two oracle queries during any computation. We want to know whether it accepts when run on a $SAT$ oracle. Of course, this is $NP$-hard. But suppose we would be content to reduce the circuit to an equivalent one that makes just a single query to $SAT$. Is even this task hard? We don't know if solving it would imply $P = NP$. However, Kadin in '88 showed that doing so would collapse the Poly Hierarchy. For a survey and improved result, see this paper of Fortnow, Pavan, and Sengupta: $URL$ 2) Suppose we are given a $SAT$ instance $\psi$ of size $m$, with only $n \ll m$ Boolean variables. Can we efficiently "shrink" $\psi$ to a satisfiability-equivalent formula of size bounded by some fixed polynomial in $n$ (independent of $m$)? This would be a valuable preprocessing step before running an exponential-time algorithm. Such a reduction would also be a strong expression of the idea that the hardness of $SAT$ derives primarily from the dimension of an instance's solution-search space. In answer to a question of Bodlaender, Downey, Fellows, and Hermelin, it was shown by Fortnow and Santhanam that such a compression reduction is unlikely, because it would collapse the Poly Hierarchy: $URL$ Their result applied to randomized reductions allowing one-sided error. I proved a corresponding result for two-sided error in $URL$ (Each of these papers actually gives stronger and more specific information than the results quoted above.) 3) Sometimes we want to prove a problem is hard using the $PH$ non-collapse assumption, but no such result seems forthcoming. It's sometimes possible to use oracles to explain why. For example, we'd love to be able to show that the class $PPAD$, which expresses the hardness of finding fixed points of continuous functions (and Nash equilibria of games) is hard assuming $PH$ is infinite. But Buhrman et al. showed that there is an oracle $A$ which $PPAD^A$ (and all other problems in the class $TFNP^A$) are easy, yet $PH^A$ is infinite: $URL$ Some people dispute the importance of oracle results, but I think there is no question that they can save complexity theorists from pursuing fruitless lines of inquiry. This is definitely one such case. (Basically all known proofs of form "$X$ in $P$ $\Rightarrow$ $PH$ collapses" are relativizing, AFAIK.) It points to the limitations of the $PH$ non-collapse assumption and the importance of exploring additional hypotheses. 

First, let me comment on the specific case of the Valiant-Vazirani reduction; this will, I hope, help clarify the general situation. The Valiant-Vazirani reduction can be viewed/defined in several ways. This reduction is "trying" to map a satisfiable Boolean formula $F$ to a uniquely-satisfiable $F'$, and an unsatisfiable $F$ to an unsatisfiable $F'$. All output formulas are always obtained by further restricting $F$, so unsatisfiability is always preserved. The reduction can be defined either as outputting a single $F'$, or as outputting a list of $F'_1, \ldots, F'_t$. In the latter case, "success" in the case $F \in SAT$ is defined as having at least one uniquely satisfiable $F'_i$ in the list. Call these two variants the "singleton reduction" and "list-reduction" respectively (this is not standard terminology). The first point it's important to note is that the success probability in the singleton reduction is quite small, namely $\Theta(1/n)$ where $n$ is the number of variables. The difficulties in improving this success probability are explored in the paper "Is Valiant-Vazirani's Isolation Probability Improvable?" by Dell et al. $URL$ In the list-reduction, the success probability can be made large, $1 - 2^{-n}$ say, with a poly$(n)$-sized list. (One can simply repeat the singleton reduction many times, for example.) Now, it is not at all evident or intuitive that we should be able to directly derandomize a reduction that only has success probability $1/n$. Indeed, none of the hardness-vs-randomness results give hypotheses under which we can do so in this case. It is much more plausible that the list-reduction can be derandomized (with a somewhat larger list). Note though that this would not imply $NP = UP$: our output list of formulas may have many uniquely-satisfiable formulas, and perhaps some with many satisfying assignments, and it seems hopeless to try to define a uniquely-accepting computation over such a list. Even if we could somehow give a list-reduction in which a satisfiable $F$ always induced a list $F'_1, \ldots, F'_t$ where most of the $F'_j$'s are uniquely satisfiable, there is no clear way to turn that into a deterministic singleton reduction for isolation. The real underlying difficulty is that we don't know of any "approximate-majority operation for uniquely-satisfiable formulas", that is, a reduction $R(F'_1, \ldots, F'_t)$ whose output is uniquely satisfiable if most $F'_j$'s are uniquely satisfiable, and unsatisfiable if most $F'_j$'s are unsatisfiable. This also seems like a general phenomenon: reductions output more complex objects than decision algorithms, and the properties of these objects are harder to check, so it's harder to combine many of these objects into a single object that inherits some property of the majority. For the Valiant-Vazirani case, it does not even seem likely under plausible derandomization assumptions that we'd be able to obtain $NP = FewP$, that is, to deterministically reduce satisfiable formulas to satisfiable formulas with $\leq$ poly$(n)$ solutions. Intuitively this stems from the fact that the isolating procedure has no idea of even the rough size of the solution set of the formula $F$ it is given.