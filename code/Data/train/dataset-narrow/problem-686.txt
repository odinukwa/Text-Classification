It looks like your database experienced an incomplete recovery. This is why it complains about open resetlogs. After an incomplete recovery you need resetlogs. To be able to get this database up and running again, you need to know what caused the current situation and some skills to get it out of there. If the data is not important at all, just trash the database and create a new one. The actions needed for this depend a little on the platform where you are on. the documentation looks like a good starting point. 

If you want to run in parallel you have to design for it. Things to consider is using partitions so they can be scanned in parallel. Don't forget that parallelism in general will try to use full scans on segments. Partition pruning can help reducing the size of your scans. If you really are using pq for this sort of small queries, make sure you have enough PQ slaves ready for use. Otherwise, the starting of new slaves is added as overhead to your query. Not that bad for dss type queries that run for hours but deadly for short running queries. Also think about using queuing your PQ queries to guarantee that when a query starts, it has all the slaves it needs. This can be done in Oracle Resource Manager. 

I have added all available actions for the login event (SELECT * FROM sys.dm_xe_objects WHERE package_guid = '655FD93F-3364-40D5-B2BA-330F7FFB6491' AND object_type = 'action' ORDER BY name) but none appear to give the network protocol. It may be of course that Login is not the correct event to give this information, but I can't see a connection event (or similar) within the XE DMV. To confirm, I want an extended event session to expose the net_transport information that is returned by dm_exec_connections: - 

An alternative would be to use virtualisation and maintain separate instances on the same physical host. The benefit in doing this is each instance maintains its own dedicated allocation of CPU and memory that can be scaled up or down as necessary. If you merge all instances as you describe, then all databases will be competing for the same CPU and memory. In addition, a shared plan cache will now mean plans from one database can age out plans from another. If you maintain separate instances in VM, then you can retain the same linked servers and not have to make any database or software changes. Performance monitoring is also far easier with multiple instances. It is simple to see which VM are contributing to peaks in resource usage. In order to use maximum virtualisation described, each core of the physical host must be licensed for SQL Server enterprise edition. Valid software assurance is also a condition of this licensing model. 

Since no platform or version is mentioned, I assume Unix/Linux and 11gR2. First of all, check Running ASMCMD in Noninteractive Mode. This explains that we can use ASMCMD in a script so whatever we can dream of, we can script. In this case in order to copy files we can use 

The fiddling with controlfile is only needed when you want to rename the database, what not is what you want. In you case ASM does not change anything for you. 

IF you just want to be able to restore a few tables and the volume is not to high, why not just make copies of them? You do need to set up good auditing and security around all your tables to prevent data loss, in the event that users did manual updates on tables that are going to be restored because of a process failure or something similar. It could be that despite that failure, the user modifications are still needed and valid. Best would be to prevent any user changes in the period where processes run that could cause the need for a restore. 

The Cardinality Estimation logic was updated for SQL Server 2014 and could potentially be a reason. You would have to test the same queries with the old cardinality estimator and compare performance metrics. You could do this by lowering the compatibility level to <120. I would perform all this testing on a test server and not in production. $URL$ $URL$ 

Take a look at tSQLt ($URL$ which is a great free tool to create unit tests in SQL Server. You can fake tables (by executing tSQLt.FakeTable) within your tests and then create mock data as you describe. Tests are themselves stored procedures and any changes made as part of your testing suite are rolled back post test. You run your tests by executing the tSQLt.RunAll stored procedure and it gives you visual feedback summary of your test results. To get started, navigate to $URL$ and download. Running Example.sql will create an example database called [tSQLt_Example], create the testing framework and some example tests to see what is possible. Also available is the Redgate paid product SQL Test ($URL$ This is a GUI wrapper around tSQLt but is not required to use tSQLt. 

You can calculate the end size before hand. A lot easier is to test with a sample. Just deleting rows from your source table won't solve your space issue. The table uses a certain space and it will remain doing so until you are able to empty complete blocks from the end of the table and can adjust the high water mark. If your source table is partitioned, easiest is to drop the empty partitions. This will free space that can be re-used by your table2. This only works if you commit after emptying a partition, otherwise, the partition will be locked. Fastest will be working by partition and drop them when copied. That is faster than deleting the rows. 

No, there is no simple query for this, if you did not setup auditing for this. You could use auditing to track ddl that changes tables. You could create a procedure that tracks the # of rows affected by dml operations by reading dba_tab_modifications, before the table is analyzed, and store that in an own table to keep a historical view on it. Not for the # of dml. If you have ASH available you could also find out the # of dml on a table and record those statistics in own tables. ASH tends to grow a lot so there is a purging policy on it. So, the simple answer is no but as is the case with many things in Oracle database, with a little creative thinking, it can be made available. 

I realize that granting root access from % is not a great idea, but at this point, I'm trying to figure out why I can't connect to the MySQL server, and when I solve that, I will restrict access. 

I am trying to connect to an AWS RDS instance from an AWS EC2 instance. I ssh into the AWS EC2 instance, and then issue the following command: 

In my /etc/mysql/my.cnf, I made sure that bind-address is set to the real IP address of the server. I can connect to the server from my laptop using sftp and ssh Yet, it refuses to connect me when I try to connect using mysqlbench error: can't connect to MySQL server on 'xxxxxxx' 10061 I can also access the web server fine from a browser, it displays the default ubuntu page. What am I missing? Update: I was able to connect when I did: bind-addess=0.0.0.0 This obviously creates a security risk, as you can connect from any IP address, but there was an issue with the way Amazon EC2 assigns internal and external IP addresses, so I have to figure out the correct IP address to bind to. 

Note that the filter on cal.[name] is not sargable. This means that even if that column is indexed, the index will not be used for a seek due to the leading "%" in the predicate. 

You can create a SQL Server Extended Events Session. Monitor the sql_batch_completed and sql_statement_completed events, add a filter for your stored procedure name and include client_app_name in your global fields to return. You can also collect a host of other information that may be relevant for your needs such as nt_username. 

Maybe log file contention or latency as you are performing 1,000 separate inserts. Far better to perform one set-based insert of 1,000 rows. You can achieve this by creating a numbers table and performing the insert via: - 

You can also edit this for your needs to only do an individual database. The logging you require can be achieved by specifiying the LogToTable parameter. As a final point. Remember you don't have to update statistics if you are rebuilding your indexes. Rebuiliding your indexes during your weekly maintenance window might be your best option. Again you can use the Ola Hallengren solution for this.