We have a huge suite of reports currently running on Oracle BI Server, on a Windows Server 2008R2 platform. There is a requirement to upgrade the Server and the OBI version, so my thoughts are to do a side-by-side upgrade. What I'm missing is some guidance on how to migrate the reports, and also whether it's possible to go from version 10 to 12, or would I have to do an in place upgrade on version 10 (to 11) first? I have no experience with Oracle BI server, but have been presented with this issue, so any thoughts or advice is more than welcome. 

This happens because of the protection level set in the package properties. When you create a connection manager and test it, it doesn't save the username and password for runtime in the way you think it might. To fix, set the pacakage protection level to EncryptSensitiveWithPassword or EncryptSensitiveWithUserKey (recommended for dev projects) and this should work. At runtime, the connection credentials are encrypted, so if your protection level is something like DontSaveSensitive, you'll run into issues like this. EncryptSensitiveWithUserKey or EncryptSensitiveWithPassword will decrypt the credentials at runtime. To change the package protection level, follow the instructions in this link 

Unfortunately PLE alone isn't going to tell you much. What's your baseline PLE? Does it dip then steadily rise again, or does it stay low for an extended amount of time? Are you getting any performance hits? My advice would be to build up a bigger picture of the issue by looking at other memory related performance measures. Page Life Expectancy is just one of a number of measures that relate to memory pressure. It needs to correlated with other memory counters, such as Buffer Cache Hit Ratio, Stolen Memory, Lazy Writes per Second, Memory Grants, CPU utilisation - to name but a few off the top of my head. Once you get an overall picture of a wider array of memory related counters you will be able to build up a picture of where the actual issue might lie - and branch off into different investigations. All you have with the Low PLE alert is exactly that, an alert that PLE has dipped below your set threshold in SCOM (What is your threshold by the way?) The below query will take a ten second sample and return some useful memory pressure counter, but is by no means extensive. 

Yeah you can do by using It's going to be fiddly to do what you want... simply because you'll need to unpick the stored procedure and re-engineer it, so to speak. I have sort of done the same thing myself for our in-house SQL inventory app that the DBA's use. The following produces a single result set which brings together the two result sets in and also adds in some info from It uses dynamic SQL to get the data for all databases in the instance. It might not be exactly what you want, but it might do something for you. Note that sometimes we have to explicitly COLLATE the joins so let me know if you get COLLATION errors and I'll try to help somehow. The script: 

I recently inherited a SQL Cluster (2008R2) which for the most part behaves itself impeccably. The windows cluster is made up of two nodes running Active/Passive, Node1 and Node2 are dedicated blades in two different data centers. There are 3 SQL instances all running on Node1. Quorum is established by a File Share Witness and we have a heartbeat between the two nodes. The other day someone switched off the file share witness by mistake, and the windows server failed over from Node1 to Node2. Or should I say, in Failover Cluster Manager, Node2 was now specified as the active node by Windows. However, the SQL Cluster didn't do anything. All the instances stayed up and hosted on Node1. I would have expected them to move Nodes, but no. There was no adverse affect on the databases at all. Once power was resumed to the File Share Witness I brought it online again and the Windows Cluster failed back to Node1. Our Windows Technicians are looking into why the cluster failed over, and I'm left scratching my head with the SQL bit. All I can think of is that the heartbeat kept the SQL instances on Node1 and losing the witness wasn't important. I'm still learning the small details of Windows Clustering, being much more used to Log Shippping and Mirroring when it comes to HA solutions, so any insight into why the SQL Instances didn't failover would be appreciated. 

This comes from this MSDN link found almost instantly after a quick web search. Bear in mind there are a few obvious and not-so-obvious functional reasons why you would not want to failover from Enterprise to Standard as some features are not supported in both. The idea of a clustered environment should be that you maintain integrity of the database environment upon failover, so why compromise that in any way, whether talking about support or features? 

As Remus Rusanu says you do not need rights to run a trace, you need permissions. I don't know anything about your company, but as a DBA in a large public organisation I have much experience of users asking for rights because they want to run a trace to 'figure out what a query is doing...' When asked that question, I don't flatly say no, I explain why it isn't a good idea to run client side traces and to put SQL Profiler in the hands of the users. Sure one of the reasons is long traces can have a performance impact on production systems, which is of course a worry, but there's also the setting up of the profile and the interpreting of the output - you don't want any help with that? The fact that you may have never used Profiler before or understand its complexities and consequences would fill me with worry. I always engage in a bit of dialogue about why server side traces or extended events is potentially better. I ask why they're investigating what the query is doing - maybe I, or one of my team, can help without a trace. It works both ways though, I am wondering if you have fully explained what you want to do to your DBA's or IT Management Team. I think sometimes when people go guns-blazing asking for SysAdmin rights without effectively engaging in a bit of dialogue you end up with closed doors and brick walls (bureaucracy as you call it) rather than collaboration, co-operation and learning experiences. You may have done this of course, and your IT team may just be stubborn - but this is just my two cents. Plus, if I found out that third-party tools were being used in isolation without the authority to do so I would put that user in breach of our acceptable use policy and report it - so please be careful if you're going down that road. Doesn't matter what company you work for - one team, remember? 

I've got a database which has an 850GB MDF file. Over the course of 20 months a logging mechanism in the application created a variety of huge tables. The maintenance script which was only supposed to retain a weeks data at any one time wasn't executing properly (it is now) so we had to manually clean things up. We actually cleared about 600GB worth of data. I want to free up some of this space as the drive it is on is near limit. The data file will never grow this large again so it's just wasted space. I was planning to use SHRINK-FILE to deal with this. I have tested on a clone of the database it takes around 3 hours. There doesn't seem to be any performance degradation on the database. Yeah, the indexes are heavily fragmented afterwards but I can sort those. My question is this: The application needs to remain 'up' throughout this process as it's critical. I understand that the SHRINKFILE operation is a fully online one, or am I wrong? The database has a large amount of write activity occurring pretty much every second. Will the increased I/O caused by the SHRINKFILE operation affect these writes? Turning this on its head, will the continuous writes affect the SHRINKFILE operation, i.e. make it slower to complete? Finally, is there a better way to do this? 

Setting the database to single user mode will close/rollback any existing connections except the current session Set it back to multi user afterwards to return it to normal. 

If I run a rebuild of sys databases via command prompt using the install media, am I able to specify an alternative drive for the files? My understanding is on a rebuild, the pre-existing default instance location is used, which in this scenario wouldn't be accessible. Assuming I can get a copy of the mdf and ldf files (by restoring a backup to another instance, for example) is there a way to start SQL Server by pointing it to a different location for the MASTER database? Like a service parameter? 

As far as I can see from checking gpedit, Lock Pages in Memory is not enabled on the server. Each instance has the following memory settings: Min: 1024 Max: 7168 I'm getting performance issues with Instance 2. Slow queries etc. Instance 1 is fine. To do a basic check for memory usage I'm running: 

Likely your database (the one your are attempting to restore/replace) currently has open connections. Try this logic: 

Here's an example which does what you want and will get you started with using PIVOT. Sorry it is a bit rushed, but hopefully it will get you started and show you how it can be done fairly easily. There will be limitations and I would fully advise researching and playing with the pivot functions because they are really powerful. 

Honestly, the biggest headache I've had with this is with application compatibility and agreeing downtime. The process was simple for me and my nodes... 

I've never tried this but I'm reliably informed it is 100% not possible. It is certainly not supported by Microsoft, for good reason, so why would you do it? 

To answer your question, in FULL recovery mode, growth of the log file is absolutely normal, and of course it will grow to either the limited size or the size of the disk unless it is maintained. The rate of growth will be determined by the type of / frequency of / load of transactions hitting your database. The rate of growth, will also determined by your auto-growth settings, i.e. grow by fixed value or percentage etc. Logs are typically maintained by performing a log backup at suitable intervals. This truncates the data within the log file itself allowing the space to be re-used for new transactions. What is important to note is that the problems you might be facing due to a large log file (i.e. where the size has gotten out of hand due to lack of suitable maintenance) is not necessarily remedied by a transaction log backup alone. You may need to shrink the file to free up disk space, for example. 

I'm in the process of writing up a proposal for implementing TDE on some critical tier databases. The actual process of configuring the TDE hierarchy and enabling encryption is easy, I think. However I want to make some recommendations about backups and I'm a bit confused. The last contractor here left some notes saying only certificates (i.e. the master database Certificate) needed to backed up with a .pvk private key. That seems to be corroborated across a lot of different tutorials online. Do we need to backup the Master key, and the Database Encryption key too? Is it safer to keep all these various bits in different locations? The context of the backups is around changing the documented recovery plans for these database for the guys and girls in the systems team, i.e. What to backup and monitor, how often, how to restore and in what order etc. 

I would be interested to hear what you have tried and what issues you have faced. You say you have SSMS 2012 installed, but it's not clear if you have a database engine in which to host a database and associated tables? If you do not have a database engine installed, you can install the SQL Express engine locally. Download it here. You are going to need to import the data from Excel into a table within an existing SQL Server database. To do this, you can use SSIS, if you have access to it, and create a simple ETL package involving a Source (Excel) and OLEDB Destination (SQL Server). You will need to create an existing table in your database, with appropriate fields and data-types, in which to 'dump' the data. Or, You can import the data through Management Studio directly, but you will need an existing database. Right click the database, click , and then follow the steps, selecting Excel Source and SQL Destination. 

I have a weird problem I am struggling to troubleshoot. I have a development server with 18GB RAM and two SQL Server 2012 SP3 instances with @@version output: 

At the moment I have around 125 production instances, each with a script-based maintenance plan running as an agent job. The tasks run are Index Reorg/Rebuild, Stats updates and Checkdb. Backups are looked after by Netbackup so they dont form part of the plans but for a couple of exceptions. I moved all the instances last year to script-based maintenance plans from plans created with the SSMS wizard (hate those) and they're efficient and effective so overall I'm pleased. I'm wondering whether it's feasible to take things a little further. I've recently been working on a powershell script that, when pointed at an instance, iterates through the databases on that instance and performs those three tasks on demand. My question is whether anyone can see any downside by doing this for all instances, I.e. Having a single powershell script on our DBA server that iterates through a list of instances on a windows schedule and executes the maintenance tasks. Any errors would be handled / written out to logs etc. The main benefit of this in my eyes that we won't be deploying mp jobs to new instances and configuring schedules. We will just be adding the name of any new instance to the instances the script must iterate through. I'd welcome your thoughts. 

We are forced to perform some essential maintenance on one of our production environments. Instructions from the vendor state the recovery model of the database should be set to SIMPLE before the work is undertaken. I'm OK with that and the business has approved the change, but I'm wrestling with the best way forward in terms of disaster recovery planning. I want to ensure that the database can be restored up to the point of changing the recovery model, if needs be. We take a full database backup at midnight every night, and log backups every hour between 4am and 9pm (operational hours). This work will commence at 9am, just after a scheduled log backup, but of course I'm prepared for slippage into that hour based on various factors. I want to ensure we have a log backup in place to cover, lets say, 9.00am to 9.20am. EDIT: Users are being locked out of the system at 9.00am or thereabouts, however I can't rely on the last log backup alone, as there may be transactions slipping past 9.00am that need to be restored. It depends when Management push the button... What's the best course of action? Should I just take an ad-hoc Transaction Log backup which I can, if needs be, apply after the latest Full Backup and sequence of Log Backups? Or should I be looking at taking a Tail Log Backup, despite the fact I may never use it? 

Is there a reason you don't want to failover? That would be the most common way. To do a rolling update and away you go. If you have a witness, disable it first. When you patch a mirror in this way the mirror will continue to function, to a fashion, and you'll have no application downtime. When a mirror partner is patched it will leave the mirroring session, but then rejoin it when it is complete - then re-sync. You can do it the way you described but you're going to have downtime when you patch the prod server and I'm guessing the patch when applied to Prod will trigger failover if that's how your mirror is configured. 

My understanding is this is only possible by 'collecting' the data from something like between restarts of SQL Server of course. MS Systems Centre Ops Manager has mechanisms to collect this data for you, but that's outside the scope of SQL Server itself. You could maybe look at "turning on" SQL Server Management Data Warehouse which could act as the collection mechanism for this type of data. Personally I have a daily collection of index usage which I use to drive a custom I/O stats dashboard that is part of a 'SQL Health' web app. I simply run an ETL routine in SSIS to pick up the data from and create a running commentary of usage, I/O approximations, unused indexes etc. The bottom line is, there is no built in view of monthly usage, you have to take what's there and create it. 

I understand why of course, and I never do, mainly because I never need to. Now though, I have a database with an .mdf file that is 800GB. It was a system designed to collect data over a certain amount of time, with no retention period. A retention period of 90 days was recently placed on this data, and as such the developer has cleared about 3 years worth of data from the tables. Data and Indexes now total roughly 70GB, so now I have an .mdf file which is grossly over-sized. I want to shrink it to reclaim some of that valuable disk space. I'm planning to perform a shrink prior to rebuilding indexes and updating statistics (i.e the weekly maint. plan) I'm not breaking any DBA laws here am I? I assume this is an acceptable scenario in which to perform a SHRINK as it is a true one off? Thanks