You can't measure these statistics by sniffing traffic because most of packets would normally pass thru OSI layer 7 and response could be delayed due to application responding slowly and many other factors. As Ron mentioned you can use IP SLA and draw graphs for example in Cacti or nProbe, NTOP could be as well used to collect jitter statistics. Netflow would be good source of information. For very simple measurments you can use tools like mtr. 

Of course it does, it route packets from A to B. Usually home routers only have a default route, but this doesn't mean that they don't route packets. Their routing table is just smaller with one default route and few directly connected interfaces. 

Full process in details is described in RFC 2581 3.2 The fast retransmit enhancement works as follows: if a TCP sender receives a specified number of acknowledgements which is usually set to three duplicate acknowledgements with the same acknowledge number (that is, a total of four acknowledgements with the same acknowledgement number), the sender can be reasonably confident that the segment with the next higher sequence number was dropped, and will not arrive out of order. The sender will then retransmit the packet that was presumed dropped before waiting for its timeout. 

You need tower to increase coverage within area. Antenna located on tall tower would have much better coverage. Each object in line of sight between antenna and client would affect signal in few possible ways, listed below: 

If destination hosts is sending echo replay response and is not recived by sender. There could be two common issues: 

Back all the way up to the premise of the question. A collision, as it were, can only happen in a half duplex situation when "the wire" is common to both receive and transmit, and both sides try to transmit at once making the output from the wire unusable. So, even between two PCs attached to the same switch that communicate with each other, the two ports are not a collision domain as long as both hosts are linked at full duplex. If two hosts do happen to be both half duplex, then there is a collision domain between the switch and the host, which will be arbitrated by the efficacy of the carrier sense logic of each. 

I would approach it this way: fetch sysuptime, then calculate the boot date/time and the predicted next wrap date/time. Write both to the log. Calculate the next poll time and if it is after the predicted next wrap date/time by a little margin, write a 'wrap expected' bit to the log. Next time you fetch, look at the 'wrap expected' bit and the predicted next wrap time from the log, and if the bit is 1 and if the predicted wrap time and current boot times are pretty close (extremely close if your server and router are both using NTP) then you know it has wrapped and not rebooted. If not, you know it rebooted. If the wrap expected bit wasn't set, simply go back to the main script logic and calculate the new boot time and predicted wrap time, make your wrap prediction bit, and write it all to the log. You are trying to guard against a reboot false positive by not anticipating the uptime wrap, AND a reboot true negative by assuming the counter wrapped when it actually rebooted. To do both you need pretty careful timing, and even then it just reduces the probability (down past 1 in 1,000,000), but it doesn't eliminate them. If you want to go full tilt, you can do something like adding a second detection layer on top. For example look at the UDP traffic counter: since you are polling via snmp you will be constantly incrementing it a tiny bit. Since there probably arent many other SNMP polls taking place, it will not likely wrap very often (if at all compared to reboots for other reasons) so if you looked at sysuptime going down AND udp traffic count going down you can increase the confidence that you caught a reboot. 

Is is possible to setup site to site ipsec tunnel on two ASA with certificate authentication without available certificate authority for both ASA. As per guide from: Cisco site Certificate authority is required. Anyone could help with some materials, guides etc? Business need is to eliminate PSK. 

Option 2 could be done with split DNS. Over here you can download ready to use geolocation databases for BIND. Option 3 you can build your VPN solution based on anycast. 

IP address 10.150.174.20/17 is part of 10.150.128.0/17 network. First valid host: 10.150.128.1 Last valid host: 10.150.255.254 Broadcast: 10.150.255.255 

You can configure an interface to convert the dynamic MAC addresses to sticky secure MAC addresses and to add them to the running configuration by enabling sticky learning. To enable sticky learning, enter the switchport port-security mac-address sticky command. When you enter this command, the interface converts all the dynamic secure MAC addresses, including those that were dynamically learned before sticky learning was enabled, to sticky secure MAC addresses. The sticky secure MAC addresses do not automatically become part of the configuration file, which is the startup configuration used each time the switch restarts. If you save the sticky secure MAC addresses in the configuration file, when the switch restarts, the interface does not need to relearn these addresses. If you do not save the configuration, they are lost. So they wont be ageing from port security, mac address only ageing from CAM and ARP. Port security and ARP table are two different concepts. 

No such device exists, as to my knowledge. To be honest auto negotiation should work fine. It was standardised as part of fast Ethernet but is backward compatible with 10base-T. If auto negotiation doesn't work why not setup port where device is plugged to 10Mbps. Otherwise you can use 10/100 switch between both devices. 

The secondary limitation in how a network switch performs (primary being the link rate between the switch and the device) is the switch fabric speed (usually expressed in gigabits) and packet forwarding rate (in millions of packets/sec). If all devices are connected at 100mbps not many modern switches would still have a bottleneck (they usually anticipate something close to the total of all ports at highest speed.) To directly answer your question: the switch will not constrain the traffic or act in a derated manner, the switch fabric speed operates independently of the link rates of any of the attached devices. 

Going to thrown a partial answer out there. The packet trace shows that from when the ack is sent by your PC to your FPGA, about 1ms elapses until the FPGA starts sending more traffic. The first example shows the FPGA putting lots more bytes "in flight" (sent before an ack) which means the ACK delay penalty was only seen twice. The second example shows an ack after every 2 packets, and then a 1ms delay. This adds up quickly so that at the end of the 32kb transmission, the 10 extra acks slow it down by about 10ms. Why the FPGA is waiting for acks more frequently is unclear, but if it's Linux based there are ways of controlling the window size, scaling, and acking in the kernel that you should probably try in order to have more deterministic behavior (the defaults often try to automatically adjust to network conditions by watching things like delays, buffer capacity, dropped packets, etc). If you suspect the PC is at fault for some reason you can control the behavior in windows via specific registry keys. One other thing to look at is the use/effect of Nagle's algorithm by the host/client. 

Switch uses Layer 2 Spanning Tree Protocol to ensures a loop free topology for any bridged Ethernet network. So if yours IOT device will forward bridge protocol data unit (BPDU) accross, switch will maintain loop free topology. BPDU are transsmited accross local network to detect loops in network topologies. If BPDU won't be forwarded you can possibly have a loop. But effect of STP could be something like on this picture. So two IOT boards would be able only to talk with DHCP server thru another IOT board. 

IMHO I assume that lidgren is working on Layer 7 (application layer). Which would increase CPU overhead and latency. Packets are still handled as normal by lower layers supported by operating system, as Lidgren don't change how UDP is handled by operating system. So to process additional checks on higher layer it would take time and use additional resources. Not really sure but maybe lidgren is adding additional headers in data which are handled by application and to process this you need CPU and is where latency could increase. 

I would suggest to define security-zones and policies. Configure security zone on all interfaces and default policy for that zone which would permit ping. 

01:30:4d:ff:ff:ff seems to be a broadcast destined for ESI PBX. Routers doesn't forward broadcast packets between diffrent broadcast domains. My guess is that both phone and PBX system needs to be in same broadcast domain. Not sure about pfsense but in cisco world you would setup ip helper address. other option is OVA. 

You can use source and destination filter in wireshark: ip.src==192.168.0.1 and ip.dst==192.168.0.2 if it's http you can add filter: tcp.port == 80 or tcp.port == 443 for ssl. When you find your stream you can click "Follow TCP Stream". You can compare time of failed download and apply time filter to narrow yours search. Unfortunately there is no way to filter failed download unless you know reason for fail or some additional information. 

You certainly can ask 10 people and get 10 different answers. Mainframe salesmen aside, you will probably find a majority who will favor something between balanced and fully distributed. A defense of distributed processing The key advantage that moving the processing away from the core creates is a more peer-oriented topology where outages do not affect entire services. If you put your firewalls at the edge of your network, and you have 5 peering points, then you have (ideally) 5 redundant points so losing one of them does not affect more than a small portion of your services, and ideally they are laid out so that a failure in one cause the others to automatically take over the traffic. In addition this scales gracefully because if you need to expand from 5 to 6 peering points, you will likely only need to invest in one small piece of equipment, vs outgrowing a single core device and having to scrap it in favor of one that offers a little more capacity. When dealing with intelligent devices, scale capability is often all-or-nothing. 

Some more sophisticated DHCP solutions will use ping or other activity metrics to gauge actual lease usage, instead of always reserving the address for the entire lease (counting on poorly behaved clients, like you pose). This would allow them to verify and maintain leases still in use and reclaim leases that were issued to a malicious actor. Most (especially basic ones) will simply get to the end of the reservation block and stop answering requests until the leases expire. So, yes a DOS based on lease exhaustion is quite possible on most networks. But, other forms of DOS are more mischievous (such as sucking up all uplink bandwidth, or attacking actual hosts) so most attackers don't bother causing problems for DHCP, and therefore mitigation countermeasures are rarely ever considered. 

If NAT-T is enabled and client is behind NAT, then NAT-T is used no NAT exists, then Native IPsec (ESP) is used So not gonna affect your current tunnels. 

For ping you need to allow ICMP Type: 0, 8 thru ASA Depend of technology used for VPN it could be 500/UDP, 4500/UDP, 5000/UDP, ESP[50], 443/TCP, 1723/GRE[47] If inside host if based on RFC1918 and need to be accesible from internet than you need add NAT statment as well. 

You will encounter overruns. Which indicate number of times receiver hardware wasn't able to deliver received data into a hardware buffer. Usually hardware buffer is is shared with other ports for example ports 1-8 share same buffer and ASIC. In some switches models if ASIC buffer is overload than data is queued to general buffer but this is causing what we calling punting and slows down whole switch as packets need to be processed by CPU. packets which would not fit into hardware or general buffer will be dropped. But in your scenario wouldn't really be a issue. Issue is when you have more traffic than backplane could handle. 

There is need for output and input rules. A stateless firewall filter, also known as an access control list (ACL), does not statefully inspect traffic. Instead, it evaluates packet contents statically and does not keep track of the state of network connections. In contrast, a stateful firewall filter uses connection state information derived from other applications and past communications in the data flow to make dynamic control decisions. 

Ideally you want to setup directory like Radius to authenticate against. Please see guide hope that help: guide Although you can have a local users as per this guide: guide