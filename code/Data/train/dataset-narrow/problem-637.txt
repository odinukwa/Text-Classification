The general rule is you do not replicate from a newer version into an older versions. Replicating into a newer version is the exact upgrade path you should do. If you only have one slave then you would just promote that to master. You'll want to be sure to retain the output of show master status before promotion so you can reconnect the old master as a slave. If you have multiple slaves have only the new master replicating from the old master and then have the other slaves replicating from the to be new mater prior to promotion. 

I can imagine a way to manage the general logging for harvesting the logins could be accomplished with a script. The process is: 

You could setup an NFS mount to your target server and just point the general log path to that mount. 

I think the bigger problem here is your default database context was database1. Thats's why your slave tried to execute the update on database2 since it was specified in database2.table format. Basically it's not safe to user db.table syntax with wildcards or you find yourself in the situation you did. If you're wanting to use the wildcard do or ignores it's generally safer to always specify your default db using "use" and execute the query in that context. 

innodb foreign key. It's just short hand naming convention. You could call it asdfqwerty and the name would still work. 

I was trying to run through the docs for manually working with the MONyog internal sqlite dbs for purposes of configuration automation. At the point trying to open the data file I get an error 

It sounds like the table is on the cusp of a cutover for the query optimizer between the estimated total rows and estimated rows being looked up. The use/ignore index syntax exists for situations like this. The pitfall to this approach is when the table dynamics change in the future such that it's really not the best choice. 

You certainly could do this but if revision control is what your after for documents use something like subversion or git. In general storing documents that do not depend on transactions and you are not storing sensitive information that requires immediate atomicy and consitency like financial transactions then a nosql solution may fit your needs and be appropriate. To the best of my knowledge you'd have to be rolling your own in the revision diffs if that's something you're really looking for whether from a nosql or traditional sql solution. Regardless, if you're working on something trivial and want to play around with newer technologies that you're unfamiliar with that's great, go for it. At the same time, if this is for a commercial client and you aren't familiar with mongo, couch, etc it's probabaly better to stick with what you know and leave the sandbox for play time until you're ready. 

clause. Something to keep in mind with all indexes is the query optimizer determines the where clause would result in examining a significant portion of the total record space it will do a full table scan anyway. 

Your own profile showed the bulk of the execution time was "Sending data", which means transferring records from the server to the client. Selecting a single integer id column is going to be small, requiring less bandwith than reading all the columns of a the row, which is what 'select *' is short hand for. This can be especially noticeable if any of your columns are large text or blobs. 

Given your description it's impossible to give any meaningful value to memory or storage requirements. You say "16 million per year" but what is that? 16 millions rows/tables? With no schema example it's still impossible to estimate. For tables you do have you can run 

You might want to read up on using LVM snapshots to backup you myisam tables and minimize replication down time to a couple minutes: $URL$ If you're able to migrate to innodb you could user xtrabackup and take live backups with next to no noticeable interference. Edit Personally we were using LVM snapshots for a ~220 GB instance spanning 3 databases. The overhead of the LVM snapshot and load on the disks caused replication to fall behind as much as 8,000 seconds nightly. There wasn't any locking but the poor little disks just couldn't keep up with the heavy replication load and the backup. We had to switch to fusion io. That's certainly nice -- but very costly. 

I wasn't clear where you want the transformed id_estado inserted, so I chose "state" as a destionation. You can modify accordingly: 

If you go the route I suggested in your previous post I'm going to say < 10 minutes. But again, soooo much that isn't given will affect the wall time. Is this a dedicated MySQL server or is other stuff going on? Specifically things that will be affecting the disk i/o? How fast are the disks? Are they SSD? Is it write through or write back cache? In short there is no "explain" for operations like this that will give you a time value. Don't forget to disable/enable keys during the modifications. Report back and let us know the results. 

I ran into apparmor headaches getting multiple instances going on ubuntu myself. Your configs look legit but I'm not super familiar w/ apparmor to spot a subtle problem if one lied there. In my case I was just running multiple copies of the same binaries w/ different cnfs Your initial error cited 

I've been having this issue where replication was breaking on colliding primary keys. The annoying thing was these were auto increment generated. I think my issues been tracked down to $URL$ for a couple reasons 

Finally, if you're worried about the ordering to the limiting effectively creates the correct range restrictions for you then this concern is completely eliminated to begin with. 

Copy your db.tgz over to your new server and uncompress it on your new servers datadir while it is shutdown 

If you're running MyISAM tables then at least make sure your key_buffer_size can accommodate the index_length (or size of your .MYI files). 

I have a binary log that mysqlbinlog chokes on with the error in the title. The file itself has much more activity after the cited position. Doing some basic confirmation it's not all garbage by running it through the strings command shows theres legit traffic until the end of the file when it got rotated. I've seen a similar post about using hexdump to get past an error related to event too large, but in my case mysqlbinlog chokes to continue to get further information. I'm not familiar enough with the binary format to look for what might be a position of a next event it would recognize. It gives a starting position it can't get past so I have a script running to basically mysqlbinlog --start-position=X incrementing X by one until it returns with a 0 exit code but that looks like it's going to take a month to completely get through everything at this rate. I tested the POC of this idea on "good parts" by starting it at weird offsets and it returned correctly at the next one it found w/o error. I'm running percona 5.6.20 for this instance. I realize this report might be lacking in information needed to answer the question so I'm happy to edit with comment requests as needed. 

If I'm reading your query correctly your asking for anything NOT in domain_setting EXCEPT for those with is_keyword_checked = 0. You should be be able to just get rid of that first not in check. Further subqueries are not always as efficient as you might think. You could rewrite the query to be 

I'm trying to get multiple instances of mysql running on a dev box for some testing. This guy is running ubuntu so the setup's a little different than our production setups. I'd like to find the easiest way to basically sudo service start and have it use a different .cnf init file (other than the distro default location of /etc/mysql/my.cnf Any ideas? 

For IP ranges MySQL only allows you to use % as a "anything" wild card or an underscore for a single digit. Alternatively you could setup reverse DNS to have desired IPs resolved to something like mysql-%.mydomain.com 

The table you see is just a pair of table schema (db name) and table name you wish to exclude for the "everything but these" list. The authorized users tables came in a second iteration to allow certain users access to otherwise default restricted tables. Some other columns there include a ticket column which is a reference to our internal ticketing system for audit trail purposes, and a couple of date columns for create and expire. I then have a user maintence script which runs a query like 

Google attempts aren't very useful as I'm mostly running across pastes of this message for different contexts. Basically what I'm wanting to know is: Is this just simply the wording/error code Mysql uses when truncating a sequence of multibyte characters, or is there something more telling I should be gleening from this message? I was initially thinking it meant the byte sequence was getting split in such away it was resulting in malformed characters. Attempts to try and make it do this didn't work (e.g. mysql seemed good about recognizing proper byte boundaries for a character encoding). Edit: After relooking over it does look to be the char splitting I initially dismissed. I had an off by one brain fart looking at it initially. 

I'll bet it's 50. The queries in your OP are likely showing a query time of this b/c of the time out setting. They're probably sitting there not really doing anything waiting for some other query to finish until they're effectively killed. If they're hitting that limit it should be easy to just run show processlist; to see what's going on. You'll likely see many threads in a Locked state (i.e. doing nothing but waiting) where there's one thread "doing something". Are your truncated "insert" statement ultimately doing some kind of expensive subselect from other tables? Is your application explicitly locking tables and not letting go? Edit: Queries hitting a lock wait timeout does not mean they are holding on to a lock for that limit but rather they got tired of waiting for someone else to give it up and let them have a go. 

The query times are wall time from start to finish. The lock times are times spent actually locking affected tables. If you run 

You say you have bin logging enabled. This leads me to believe you have replication running. To minimize down time you could promote your slave to master. The finer details on how to do this really depends on your setup. Maybe it will involve updating application configurations, maybe it will involve changing the DNS to your "master server". Prior to the promotion ensure you slave is configured they way you would like, e.g. not writing binlogs to the data directory. Make sure all the grants present on the master are on the slave. Make sure the slave is not running in read only mode when you are ready to bounce. Prior to promotion you'll want to ensure both the master and slave are in read only mode. Run show master status; on the slave to see what log file and position you'll use for change master to after you reconfigure and restart the old master to run as a slave. Once the old slave is promoted to the new master you can take your time to move the old binlogs to a new file system and reconfigure to keep new logs out of the data dir. 

I have a table that's showing some differences on a master and slave according to pt-table-checksum (PTC). I've found some verifying differences beyond that. Certain tables in the mix have composite primary keys which, while work well enough for PTC do not so much with pt-table-sync (PTS) to try and find the differences. PTS does not seem to honor the newer options of PTC to restrict the depth of composite key searches. End result is the a 5.5Mrow table is left spinning for hours w/ PTS. I'm somewhat convinced there's an infinite loop bug in PTS as the output has just started spitting out the a correction for the same exact record (and nothing else at this point) over and over. So, really this post is more about alternatives than trying to get this tool to run right. The most straight forward alternative that comes to mind is flush table with read lock on the master; select * into outfile on master and slave; unlock and diff. However this table is fairly active and I cannot afford it being locked out on the master that long. I was hoping to do something like start a transaction in repeatable read isolation level and select out from the master there. However I can't find a way to have the slave be stopped specifically at the point in the transaction history as on the master. Things like show master status continue to update after starting a transaction so I can't simply start slave until then. There's no guarantees about atomicity to "quickly" do "show master status; begin;" The only other solution of a non blocking method is to write a custom script that might be able to more efficiently do a PTS type thing but with application specific domain knowledge about how the columns of the table in question are generated. While it might come to that I was hoping to find a more generalized solution for the future when prebuild wheels like PTS turn out not as well rounded as I'd like. 

I think the biggest gotcha would be around innodb being transactional. You'll want to know if the MySQL libraries being used by your applications auto_commit by default or not. [Python|mysql-python.sourceforge.net/FAQ.html#my-data-disappeared-or-won-t-go-away], for example, does not auto commit. This means if an application was inserting a row right before closing it's connection that insert would now be rolled back after you alter to innodb. The python script for example would need to be sure to call connection.commit(); Another point of difference could be around around multi row inserts or updates. Consider a single multi row inser insert into tbl values (...row1...), (...row2...), (...rowN....); Consider what happens if there is some kind of error such as a unique key collision on row3. With MyISAM the first two rows would have been written, under innodb all rows being written would be rolled back leaving nothing written in the even of such an error. With innodb you will enter the world of deadlocks. These aren't inherently bad unless they're occuring with such frequency to prevent any work from being done. However your applications will need to be coded in a such a way they anticipate deadlocks and handle them appropriately (which most likely means just retry). Consider memory/storage limitations. Innodb is a lot more resource intensive than MyISAM. If you have enough RAM to keep your buffer pools large enough to accommodate all your tables then you're golden. Look for tables that have large primary keys. Innodb's clustered indexing means each secondary index holds another copy of the corresponding row's PK. If you have 2 secondary indexes that means each rows PK is stored 3 times (PK + each index). If the pk spans across several column and large datatypes (char(N) for example) you can see how the index requirements can quickly explode under innodb.