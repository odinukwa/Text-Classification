I have a table with a clustered index and 2 non clustered index. Now, in my view I am selecting everything from my table 

I have seen column store index and also tested it. I have no doubt admiring that it is lightning fast. But every tutorial and blogs I have considered, says about its limitation "table will become read only" and I won't be able to insert/update/delete data in a table. And at the same time it is recommended to use them in data warehousing projects where data is in huge amount and updated periodically. On based of that I have come up with the solution to 

Question 5: Am I going in wrong direction? Is there is something which I am missing that should be done to rectify this issue? Edit Why I believe this is related to Log file size? If there is an IO issue then read write ability of SQL will decrease, this will result in long running transaction hence cause large log file size. 

I have many SQL Server jobs on the server. I need to execute a SP whenever any job execution is done. One hard way is to create a step on every job which calls that SP. Is there any simple way to do that? Does, SQL Server execute any SP or anything after job completion? 

I've a table with 80 columns and that is a base table for most of the application. Daily load inserts almost 8,000 records and update upto 2,000 records. This table is now having more than 5 million records. Unfortunately, I can't change the architecture and for next few months I have to continue with this. Now, as I said there are multiple application connected to the table which are fetching data from it and almost all of them using different columns for their purpose. Example of few of those queries are: 

Is it possible to set up an SSRS e-mail subscription, with an attachment, that is encrypted? POST ANSWER-ACCEPT UPDATE: Does the SMTPUseSSL switch in the RSReportServer.config file help me at all? ONE MORE UPDATE: No, the SMTPUseSSL switch does not get me there. I have validated the answer below here: 

I need to make a decision about deployment and security of some SSRS reports and would like some advice on what the most appropriate method is. We have 40+ sites that have access to information for only their site. Each site has access to a number of reports, say Report A, Report B, etc. For each site, the report name is Report A - Site 1, Report B - Site 1, etc. For each report (Report A), there is a Master report (Report A - Master). The Master report is the report itself - dataset and formatting - and includes a hidden parameter for site name. Each sites' report links to this Master report and passes in the site name. Thus if the report changes, the change is made in one place. Please note that limiting the reports to site-specific information on the database is not possible. Our data source uses a service account and all users access to the information is handled in the Web Portal. Now it is time to grant users access to these reports. Currently it is set up that all of the reports and the master reports are in a single folder. Access is controlled on each report individually and users must have access to the Master report as well as their site reports. I would strongly prefer to set up a folder for each site and control access that way. Obviously that would make it easier as site-specific reports are added, and it's superusers who will be controlling access. My concern is this will make deployment of reports incredibly difficult. Because of the linked mechanism, all 40+ versions (plus Master) are in the same BIDS project - and it seems like a nightmare to deploy to 40 different locations for every change. I would like to know if anyone else has had a similar challenge and found a good way to solve it. I have played around with linked reports, but then the site report can't "find" the Master report, even if a linked copy is placed in the same folder. EDIT: To clarify, and use the exact terminology, the Master report is a sub-report of each site report. 

You don't have a choice of character set on XE so you cannot change it to suit the database you are trying to import. Would it be practical to migrate the source database before export? The import should work, but character set conversion might mean some text columns with non-ascii characters won't look the same after the import. And rows can be rejected if they are too long in the new character set. In your case, you are converting to UTF8, which will mean it is possible for a single byte character to grow during conversion to 2 (or more in theory). You may need to increase the column size before export or adjust the target schema and import the data in a separate step. See here for other possible data truncation problems 

Simply speaking, there is no direct analogy for MySQL 'databases' or a 'cluster' on Oracle: the closest match is a 'schema' but that is still very different. This is apparently going to change in 12c with the introduction of pluggable databases: 

No, you can specify the 'params' (the parts of the clause) in any order and the query optimizer will handle it. The optimizer will do the filtering in the order that it estimates is most efficient, but note that this is more complex than just choosing which order to filter: filtering might be done before or after joining for example. You can't exactly prove this, but you can demonstrate it is true for a particular query by experimenting and seeing if the plan changes. It may even be true that there are edge cases where the order does matter, but my advice would be to ignore the possibility and assume it never happens as otherwise you will expend a lot of effort trying different permutations. Much better to focus on the kind of tuning which you know can pay dividends (eg correct indexing). 

As you can see that one column is being used in where clause and some query it is in select clause. I have created indexes and now I realized that I need more but that doesn't seems to be feasible to me as I will end up in having so many indexes. 

I have to change the initial size and autogrowth of my tempdb on production. Along, with that I have to add new files to tempdb. What is the right way to do it? As it is production and I don't want to take any risk. Can I directly go to temdb properties and can change the settings/T-SQL or do I have to consider other things also? 

What statistics are required to theoretically prove that X query work better than Y query. As per my knowledge I need to look for: 

I am facing issues related to constantly growing log file due to which I am getting error. When I checked SQL Log I found below messages (error log is filled of these messages almost 90%) 

Now, assuming that I have limited space (i.e. is 180 GB + 20 GB) which I believe is good enough for a database in SIMPLE RECOVERY MODE. How possibly can I identify this issue and do a rectification before it occurs? Replication: I have tried to replicate this scenario via creating new sample database with below setting 

I am using the PIVOT function in Oracle and am curious if I can replace the null values with zeroes? I know I can wrap the entire query in another SELECT and then use COALESCE on the values, but I am curious if there is a shortcut. 

It is a bit kludgey, but here's an idea sketch... instead of merging the cells, could you fake it by placing an A in the left cell aligned to the right and a D in the right cell aligned to the left? You could then use expressions to control value, alignment, color, and borders of the two separate cells. Let me know if you need more details. I think this would give you the visual you want. Good luck! 

I am looking for the most efficient way to solve this situation: We are an Oracle shop. I need to load data from one table to another. My source table, which contains 30 million rows, has a person identifier that is supposed to be unique, but it is not enforced so there are cases where it is not - about .1% of the time. There is logic I can use to resolve the non-unique cases. I want my target table to "enforce" the uniqueness of the identifier - so I have defined this identifier as my primary key. (Note that the data that is loaded isn't a straight source table -> target table; there are a few look-up tables that are used too) My goal: load this table as quickly as possible. It's a lot of rows, so efficiency is key. I have thought of a number of ways to address this, but I'm not sure what would be the most efficient. My ideas include: 

I'd say No, though of course this is not so much an answer as a principle. The main result is probably a false sense of extra security. The question you need to ask is "what am I trying to protect myself from?". In general on a database that is exposed to the public internet, you would not be opening a port to the database at all - the world would only see your application layer. On databases that only need to be accessed by users on your LAN (and VPN), often you do need direct access to the database, and this is probably the case you have in mind? Security on the LAN is a balance of convenience and protection, and I think you have to choose whether you either: 

dbfiddle here If there are multiple columns and you want them all to be default, you can still use a CTE: 

I will shortly be migrating a database from SQL Server 2000 (part of SBS 2003) to SQL Server 2008 R2 Express Edition The database is small, and there are only a few hundred short transactions per day - I'd like to keep everything as simple as possible from a recovery perspective whilst minimising the amount of lost data in the event of a failure Can I just run a full backup every hour using Windows Scheduler? I already have a solution for long term archival of database backups which these could just plug in to What can I do to back up the 'logins' - I understand they are not saved as part of a full database backup. I want to have everything necessary to be able to perform bare metal recovery if we have to Anything else I need to think about? 

(I fear what I wish to accomplish shall not be easy, but a lot of my googling around about it yielded rather dated results, so here it goes...) I have a Postgres 8.4.x database with over 100 tables. I need to recreate these tables in SQL Server 2008. I'm not concerned about the data, just the structure. Are there any slick tools, shortcuts or suggestions to accomplish this? Heck, I'll even take the find-and-replace values to run on Postgres CREATE TABLE scripts! Thanks! 

I have an SSRS report that can I access from the Web Portal but once I try to access it directly through the URL, I get the following error: . I looked at the logs on the Report Server and the error there says . But, of course, it is in the database - I can access the report from the Web Portal. Any ideas? UPDATE The same link now works, two days later. 

Set up a new dataset to pull back the language value (let's say language_dataset into column language_value). Create a parameter, let's say called language. Set it to internal and set its default value -> Get values from a query and use the dataset and value you just set up. In the Report properties, Language -> expression and set the expression to . 

Our SQL Sever 2008 application database is replicated from Server A to Server B (push replication). We use the copy, let's call it database_b, on Server B to create reports and run other queries so that our reports do not interfere with the application. Currently we leverage inefficient views to combine data across multiple tables in database_b so that report writing is simplified for our report writers (who have basic SQL skills). 99.9% of the database activity is INSERTS, so we are exploring a way to replace the inefficient views with tables we can optimize. Here is a simplified example: There is an table and a (lookup) table. Every time a new appointment is scheduled, a row is added to the table. Every time this INSERT happens, I want to take that and insert it and its corresponding location name (join on from both tables) into a reporting table. I have accomplished this with a trigger on the appointment table in database_b on Server B. My question is - are there any particular considerations given that database_b is a replicated copy? Do I need to worry about a failing trigger mucking up the entire (push) replication process? Anything else I am missing? Unfortunately it's difficult to test this in our development environment, so I don't have the opportunity for a lot of trial and error. 

And the same is true for . I don't know of any simple way round this, and I'm not sure I agree that it is the most sensible behaviour, I think a better choice for would be to set the nullable columns to null if there are both. 

Aside from the fact that I think you are giving yourself a false sense of security, you can do what you want by reserving the upper half of the identifier for the 'real' sequence and filling the lower half with random bits: 

The fourth mode of is . This is like pulling the power cord - the instance stops now without any cleanup. You usually want to bring the database up again afterwards and shut down cleanly immediately afterwards as in your example. The concepts guide says: 

dbfiddle here Note though (thanks @Erwin), that performance is going to be very substantially worse than the built-in aggregates. If this matters you will have to consider writing the helper functions in C, which is much more of an undertaking. 

For SQL Server I'm not sure if the situation is the same but someone else will say if it isn't... As for "why", I'd say the should be rare, usually only if something has gone wrong, and of course is likely to be much more common - so it makes sense to optimise for 

I have set up a predefined replication alert (as outlined here) in SQL Server 2008 and the alert for Replication Warning: Transactional replication latency does not appear to work. This is triggered by error 14161. I found a number of posts around the web that indicated this was a bug, but the posts were so old, I'm not sure that it is still the case. Is this still a broken feature? If so, can anyone suggest a work-around? EDIT/ADDITIONAL INFO: I see there are a number of scripts that have been highlighted in similar questions. To refine my question, I'd like to confirm this is a bug and I am looking for a work around that is rather out-of-the-box... that is, just another way to write the SQL Server Agent alert to get it to work... 

I want to know if it is possible to create a group "internal" to SSRS without having to add the group to the ReportServer box itself. I do not have permissions to the box to create groups and/or add users to existing groups. I know that this is a way (the only way?) to have groups that can be assigned permissions in the SSRS interface. I do have all the permissions I need within the SSRS interface. I have the System Administrator System Role on the ReportServer instance, as well as full sysadmin rights to the ReportServer database. I am trying to simplify our security structure and it would be peachy if I could create a group, assign users to that group, and then permissions to the group. But all I seem to be able to do is assign permissions to the groups set up on the box. Any ideas? EDIT: Active Directory groups are not an option for me. :( 

From what I can see your stored procedure just loops through the rows - it does not 'save the data' anywhere and you have commented out the : 

A space efficient index to cluster on (this is coming in 9.4 with GIN compression, or better still in 9.5 with the new BRIN index type) A 'vacuum-like' process that would scan that index to detect which blocks need to be deleted/reinserted (this would ideally be able to reinsert the rows into fresh blocks so auto-vacuum can be left at default) 

You'll need to substitute your ordering for my s - and probably do something with the pattern to make it match with your pattern table :) 

Rather than justify the syntax I think it is better to point out that there are no good general rules-of-thumb when dealing with s and the syntax used to handle them. For example: 

It needs to be calculated based on how quickly you are generating UNDO (with DML), and how long the export takes â€” you'll need to find a size that works for you or try and do the export when less UNDO is being generated, or speed up the export. If you have the space available, increasing the size of your UNDO is the easy solution. 

The clustered index does not contain a pointer to each row of the table - instead it contains pointers to clusters (groups of rows with the same ). It could in theory be useful for a query like (no idea if it actually would though), but you still need to go to the underlying table to answer .