The best way to install XE and other products is to install XE first, then any other Oracle version afterwards. I think you can fix this by 

It is certainly possible that joining complex large views can cause additional overhead but the best way is to test. Create queries on the base tables that yield the same results as using the complex views. Then run a series of queries against the base tables and compare the same series against the complex views. I've seen one database structure where all application access was through views and speed of response issues started appearing with the reporting tool as the size of the database grew. As a_horse_with_no_name points out there are built in tools to help you analyze what is going on under the hood. With large applications it is hard to point to one thing and definitively say "this caused the problem". Test in development, change one thing, test again and repeat. This approach keeps cautious managers happier than just saying "the problem is this". 

Oracle can connect to MySql with an ODBC connector. This is described in some detail here. Also see the official Oracle documentation here. This will work for Oracle 10 and 11g to MySQL 4.1, 5.0, 5.1 or 6.0. There are limitations both in performance and what you can do. My understanding is 

first define a window with . This window could have a 24 hour duration and repeat every day so that it is always open. modify your job submission so that they run under this window (parameter of the procedure). when you need to enter maintenance and/or release resource, use . When your maintenance is done, use . 

The type is a nested table of , so you need to build a set of objects to get the list. The following query works on 11.2: 

You can create the database link by connecting directly to the remote database. As suggested in the askTom discussion, you can also use or to create a distinct remote transaction that can initiate the DDL statement. 

The jobs submitted while the window is closed should be queued and run later when the window is opened. 

I don't think Oracle keeps track of past closed queries. However, you can find out what cursors a session has opened with . Since many applications cache the cursors for later reuse (this is automatic in PL/SQL: a cursor won't be completely discarded unless you reach the maximum number of open cursors), in many cases all past queries will be in this view: 

You need to have SQL*Net installed in order to connect PL/SQL Dev to Oracle. SQL*Net is installed by default with most Oracle DB products (Oracle client for example). Since PL/SQL dev is a Windows app, you can see what Oracle products you have installed by looking into the registry (HKEY_LOCAL_MACHINE\SOFTWARE\Oracle). Also in PL/SQL dev settings (Tools/Preferences/Connection) you will have a list of all Oracle Homes (which allows you to specify which one you want to use if you have multiple homes). If you already have an Oracle Home, the file is by default in the directory . You can set the registry key if you want to specify another directory. 

When this is called from a DBMS_JOB from one package it inserts Greenwich mean time. When it is called from a DBMS_JOB from another package that uses a db_link to another database it correctly inserts the local time. I think this is caused by the client's timezone being used but adding this to the job did not resolve it: 

Is it possible to create a trigger that fires when a view is created on a user schema? I want to create a meta data table which will hold all the views and a listing containing what they are supposed to do. In this system (Oracle 9.2.0.8) there is only one user who has the create view privilege so that may make it easier. The sequence of events should be something like view is created trigger fires and inserts the name of the view in the table of meta data table I could write up some package to create views using dynamic sql but that seems a bit of overkill 

I had the same issue with a migration from 9i to 11g. I decided that I did not want to take a chance that Oracle's character set conversion would do something I did not expect. Managers don't care about character sets, they just want to know that the data was migrated without any problems. I kept the WE8MSWIN1252 character set and there were no problems. However, if you had to have a UTF8 character set, and did not want to take any chances this is how I would do it. Requirements: a virtual machine in a test environment, to make the install and testing painless 

The most likely cause of a mutating table error is the misuse of triggers. Here is a typical example: 

This is an interesting question: When does Oracle really delete data physically ? The unit of data in Oracle is a block. Let's see what happens when we delete a row. Here's an example with a simple table on 11gR2 (see "How to dump Oracle Data Block?"): 

Yes, most analytic functions can be rewritten with semi-join. In your case it would probably not be very efficient: 

One way to force data to actually be overwritten would be to update it to a meaningless value before deleting the row. This wouldn't work with indexes since updates are translated to delete+insert in a b*tree index. 

In the first case the database will interpret PROCESSED as a variable, and its value, even if constant won't be learned until execution time. An index on will be used only if status has a strong selectivity for all values (ie there are many different values). Since you have only 3 values, Oracle makes a FULL SCAN since from its point of view any of the 3 values could be used. In the second case, an index will be used if you have statistics on this column that show that the value 0 is very selective (ie there are few rows processed). Oracle knows at compile time that this value won't change thus an index scan will always be effective. So, if you're sure that there are few rows processed, you will have to help the optimizer, for example with an hint like . Or you could use comments in your SQL: . Also remember that FULL SCAN are not evil. 

I assume you are using the 64 bit version of Oracle to install. I assume you have Windows 7 x64 - Professional, Enterprise, or Ultimate edition In the case of unusual errors it would not be impossible for something silly like the language pack in use on Windows to be the cause. Try starting with a new Windows install that has not been customized or has had any other Oracle products installed or removed Check the install logs. With all the versions and folder name changes I am not 100% sure where but you should look in the c:/Windows/oracle or in the install folder if located on hard disk 

This is for Oracle Grid Control 11g OEM. The listener is considered to be a target for monitoring. Go to the Hosts tab, select a server and click on the Targets tab. This will normally list all the database instances, the listeners and the OEM agent. The status is indicated by the Availability column. You can configure OEM to send you an email if the listener is down under the custom alerts configuration. 

One way to do this is to ensure that all tables of properties, or metadata, have a date_created and date_last_modified date. Then you can filter where the date created is in a time period and export the changes as inserts or updates. What I do is harder. All changes to development properties are saved as a script of inserts/updates and I use a project/bug tracker to keep track of all the collection of changes. I refresh development weekly so you see right away if you forgot something like a sequence or a grant. A typical change could involve data inserts, package updates, grants, sequences and triggers so you have to keep it organized. Edit: Luke asks how do you keep changes as scripts? I do two things: 

Only one base table is updated All other tables are key-preserved: each of them must have at most one row for each row of the base table. 

You should never rely on implicit conversions because the behaviour is context dependent. Your interval filter is wrong: two intervals and will intersect if and only if: 

Range partitioning involves a bit more maintenance because you have to create the partitions yourself. However, once the partitions are created, range and interval work similarly. 

There seems to be something wrong here: the 0 cost on the index full scan is suspicious and if I had to guess I would say that you're missing something: probably the stats on the index. This in turn leads the optimizer to believe that it can run the FULL INDEX SCAN "for free" and goes on with a suboptimal plan. This could also be a rounding error problem, since there is very little data (1k tiny rows, probably fits in a single block!). So either there is some stats missing, or too little data to be meaningful. Interestingly, if we run your test with a large sample (say 1M rows), the optimizer is happy to go with an index scan. If we insert some data instead and do a standard stats analyze, we find a more logical plan (11.2.0.3): 

We use Lucene.Net in our ASP.NET application. It indexes the database fields using the Query Parser methods and I believe it will do what you want. My only proviso is that with the number of records you want to search I would think a dedicated server or two would be required to index the data. And it is quite possible that at the end of the day when you have spent some time configuring Lucene and the hardware and writing the connectors you could have just made a data warehouse and got something similar. 

Try doing a test case. Make a dummy table and insert 100,000 records using your sequence from the database. I'm betting you will have no problems. Next try inserting the same thing from your application. Could this be caused by other issues such as an Oracle client mismatch? Another solution that would fix the issue but not problem is to add a trigger on the table. Before Insert on table on Dallas.X IF :the_id is null THEN SELECT x_seq.nextval INTO :the_id FROM dual; END IF; 

Try adding (SERVICE=Dedicated) to your TNS entry on the client computer that you are using and then run your query again. There could be other causes but this is the most common: you are trying to access data using a shared connection when a dedicated connection is required. Edit: Are there any other log or trace file entries when this error happens? 

Here the row in sale will be replaced (updated) by its first split component and the additional components will be inserted. 

Most likely using a cluster for this query won't be beneficial. A cluster in Oracle allows data from multiple tables to be stored physically close when they share a common key (here I suppose). This allows some query to perform better but a cluster will intrinsically consume more space than standard heap tables because for each key there will be some unused space. Insert-only heap tables on the other hand are one of the most efficient way to store data space-wise, since the rows fill all blocks nicely up to the HWM. In your case since you don't have a filter so all rows will be read, producing a FULL SCAN of the data. Because the rows are stored in a more compact manner in heap tables, the cost will be less than the cost for the cluster. The cluster, however, should have an edge when you look for a specific key, but this will also depend on the distribution of the data (number of rows per key), and on the length of the rows. You could build an example where the heap tables with regular B-Tree indexes will outperform a cluster for single-key queries. In conclusion, clustering tables in Oracle will help for some queries, but will also be hurtful to others, it has restrictions and drawbacks, it is not a silver bullet for optimal performance. Heap tables are the default for a good reason: they have good performance for most queries. 

I have to continue maintaining an Oracle 8i database that receives information (about 20 Inserts and Updates a day) from a 9i database. I am upgrading the 9i database to 11g which does not support database links to 8i. The plan is to link from Oracle 11g to Oracle 10 XE and then to the 8i and hope that one day I won't have to do this. I have already found articles noting that installing XE after any other version is not recommended. Has anyone installed XE and another Oracle version side by side? What order was used and what problems were encountered? Edit: @George3 some forum posts indicate that you can use JDBC drivers to connect 11g and 8i. Please add some detail and post it as answer. 

In order to do RMAN backups you need the SYSDBA or SYSOPER roles. In order to grant those you need to be able log on as a user with those roles: ie the SYS role. There are variations where you use OS authentication or if your OS user is a member of the ORA_DBA group. Easiest is if you can do this: 

Log in as sys, create an account for yourself and grant that account just the privileges you require. Give yourself a reasonable password. "Password" is not a good password. Here are the top ten most used passwords and even if it is your intention that only you log on as DBA these are still not good passwords: 

(additional restrictions on updating views apply) In your example you update table only. Oracle has to make sure that for a single row of this table, only one row of the other can be found. This seems to be the case since you have a PK on . Therefore you should be able to update the join. See for example this SQLFiddle with a similar setup. In your case you should either: 

In Oracle, DDL on remote database is not permitted. One likely reason is that a distributed transaction commit can not be initiated at the remote site (you can't ) and since DDL statements include a commit they are not permitted. You would get an with other DDL statements: 

If your table is updated concurrently, a bitmap index with a unique value will be a point of contention and shouldn't be used. 

You have the basics right. There is only one type of commit (no normal, fast...). from the concepts doc: 

I agree the Oracle docs can be a bit bland, however they are in general very complete. Once you learn how to locate the relevant piece of information you're looking for, they are often the best resource you can find online. In your case I would suggest you take a look at the PL/SQL Packages and Types Reference book, where you will find the complete documentation of all standard packages. The chapter, contains a collection of examples, in particular how to set up your directories to enable access. Once you have setup your directory object, you can create a file in PL/SQL with something like the following :