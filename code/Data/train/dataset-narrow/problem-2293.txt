It's not enough for a self study, but the notes from this DIMACS workshop might help: $URL$ Then there is Arora-Barak: $URL$ (there is a draft online, and, as it is a draft, it has some typos). 

Q2: I think usual theoretical definitions don't really distinguish between algorithms and programs, but "algorithm" as commonly used is more like a class of programs. For me an algorithm is sort of like a program with some subroutines left not fully specified (i.e. their desired behavior is defined but not their implementation). For example, the Gaussian elimination algorithm doesn't really specify how integer multiplication is to be performed. I am sorry if this is naive. I don't do PL research. 

The class of functions computable by formulas of polynomial size is equivalent to the (nonuniform) class $\mathsf{NC}^1$ of functions computable by (bounded fanin/fanout) circuits of logarithmic depth. Proving the two implications is a nice exercise. In one direction, you can recursively "untangle" each level of a circuit by creating copies of gates. This increases the size by a constant factor for each of the $O(\log n)$ levels of the circuit. The other direction is proven by balancing the tree of the formula. Also, $\mathsf{NC}^1$ is equivalent, by Barrington's theorem, to width 5 polynomial size branching programs. We know that the uniform version of $\mathsf{NC}^1$ is contained in $\mathsf{L} \subseteq \mathsf{P}$ ($\mathsf{L}$ stands for deterministic logspace). It is not known whether any of these containments is proper AFAIK. Also it is not known whether the non-uniform $\mathsf{NC}^1$ is a proper subset of $\mathsf{P/\text{poly}}$. 

There are examples of $n\times n$ real matrices of rank at most $3$ and non-negative rank at least $\sqrt{2n}$. So the non-negative rank cannot be bounded by any function of the rank in general. The construction I am aware of goes through extension complexity. An explanation follows. The extension complexity $xc(K)$ of a convex set $K$ in $\mathbb{R}^d$ is the smallest possible number of facets of a (higher-dimensional) polytope that projects to $K$. (Equivalently the smallest possible number of constraints in a linear program for $K$). A famous theorem of Yannakakis shows that if there exist $A$, $b$ and $v_1, \ldots, v_n$ such that $K = \{x: Ax \leq b\} = \mathrm{conv}(v_1, \ldots, v_n)$, then $$ xc(K) = \mathrm{rank}^+(S), $$ where $S$ is the slack matrix, defined by $S = b1^T - A^TV$ for $V = (v_1, \ldots, v_n)$, i.e. the matrix whose columns are the $v_i$. A result by Fiorini, Rothvoss, and Tiwari shows that there exist two-dimensional $n$-gons with extension complexity at least $\sqrt{2n}$. Since any $n$-gon $P$ in $\mathbb{R}^2$ can be written as $P = \{x: Ax \leq b\} = \mathrm{conv}(v_1, \ldots, v_n)$ for $A$ an $n\times 2$ matrix, $b$ an $n\times 1$ vector and $V = (v_1, \ldots, v_n)$ a $2\times n$ matrix, the slack matrix $S = b1^T - A^TV$ of $P$ is the sum of a rank $1$ and a rank $2$ matrix, and therefore is of rank at most $3$. On the other hand by Yannakakis's and Fiorini et al.'s results $\mathrm{rank}^+(S) = xc(P) \geq \sqrt{2n}$. 

Universal algebra is an important tool in studying the complexity of constraint satisfaction problems. For example, the Dichotomy Conjecture states that, roughly speaking, a constraint satisfaction problem over a finite domain is either NP-complete or polynomial-time solvable. Note that by Ladner's theorem there are problems in NP which are not in P and not NP-complete, unless P = NP, so the conjecture says that CSPs are special in having a dichotomy that the larger complexity classes do not have. It also would provide some explanation why most problems we encounter in practice can be classified to be either NP-complete or in P. Dichotomies were proven for several special cases, e.g. binary domain CSPs (Schaefer) and ternary domain CSPs (Bulatov), and homomorphisms into undirected graphs (Hell and Nesetril). But the general case is fairly open. One of the major lines of attack is through universal algebra. Very roughly (and I am definitely not an expert in this!) one defines a polymorphism of CSP to be a function on the domain of the CSP which leaves all satisfied constraints satisfied if it is applied to each variable. The set of polymorphisms of a CSP in some sense captures its complexity. For example if a CSP A admits all polymorphisms of a CSP B, then A is polynomial time reducible to B. The set of polymorphisms forms an algebra, whose structure seems helpful in desining algorithms/showing reductions. For example if the polymorphism algebra of a CSP is idempotent and admits the unary type, then the CSP is NP-complete. Idempotence is a simplifying assumption that can be made more or less without loss of generality. Showing that a CSP whose algebra is idempotent and does not admit the unary type can be solved in polynomial time will prove the Dichotomy Conjecture. See the survey by Bulatov: $URL$ 

While continuity in a strong sense is probably too much to ask for, locality sensitive hashing (LSH) achieves a very similar goal: given a set of items $x_1, \ldots, x_n$ which belong to some metric space with metric $d$, an LSH family of hash functions (parametrized by $D, c, p, q$) maps $x_i$ and $x_j$ into the same bucket with probability $\geq p$ if $d(x_i, x_j) \leq D$ and maps $x_i$ and $x_j$ into different buckets with probablity $\geq q$ if $d(x_i, x_j) > cD$. Of course there is tension in the requirements, so one looks for tradeoffs. Look at the wiki page and Alex Andoni's webpage, especially the CACM survey. For context, LSH is a tool to solve the approximate nearest neighbor problem in high dimensional metric spaces. There exist solutions for various metrics, like hamming distance and $\ell_p$ norms, see the CACM survey. The approximate nearest neighbor problem is an important problem in computational geometry, and the high-dimensional version comes up in a variety of applications, for example in computer vision and information retrieval. The way LSH is used to solve approximate near neighbor search is a fairly natural two-level hashing scheme. As you note, there is tension between the "continuity" property and bounding the number of collisions. For that reason, usually the gap between $p$ and $q$ (as I defined them above) is pretty small. To overcome this we construct a new hash function by choosing several functions from an LSH family and taking their concatenation. The resulting universe is too big, so it is reduced using traditional hashing. Then we build several hash tables using these hash functions. A query is answered by hashing the query point and inspecting the other points in the same bucket(s) to determine if they are close enough (this is the linear search part). Parameters can be chosen so that a query is answered in sublinear time. 

where the notation $L_n$ means the slice $L_n = L \cap \{0,1\}^n$. To do this in exponential time with a $\Sigma_2^\mathsf{P}$ oracle, you can use binary search over subsets of $\{0,1\}^n$ (think of them as $2^n$ bit integers) to find the first such set which has circuit complexity $> 2^{n/2}$. You just keep the current guess of $L_n$, and use the oracle to test if there exists a $L'_n \prec_{\text{lex}} L_n$ of circuit complexity at least $2^{n/2}$. Since this gives a machine in $\mathsf{EXP}^{\Sigma^\mathsf{P}_2}$ which writes down the whole slice $L_n$, clearly we can also decide membership in $L_n$, and, therefore, in $L$. This is very much as in Kannan's argument, but scaled up and streamlined to use the exponential time. Then you should be able to use a scaled-up version of the Karp-Lipton theorem to show that if $\mathsf{NEXP} \subseteq \mathsf{P/poly}$, then $\mathsf{EXP}^{\Sigma^\mathsf{P}_2} \subseteq \mathsf{NEXP}^{\mathsf{NP}}$, and you can carry out the case analysis in Kannan's proof. 

This answer expands on Chandra's comment, and on my follow up comment. The problem is indeed solvable in polynomial time. More general versions of it are also solvable in polynomial time: $\Theta$ could be given by a separation oracle, rather than explicitly, and it is also possible to solve an appropriately formulated version for a polyhedron. Observe first that we have an efficient separation oracle for $P = f(\Theta) = C\Theta + d$. Indeed, deciding $y \in P$ amounts to solving the feasibility problem $$ y = Cx\\ Ax \le b $$ over the variables $x$. This problem can be solved with the usual techniques, e.g. the ellipsoid method. By the equivalence of separation and optimization, we can also solve arbitrary linear optimization problems over $P$. Let us first assume $y \in P$: otherwise we are done. Then, by Caratheodory's theorem, $y$ can be written as a convex combination of at most $m+1$ vertices of $P$. Moreover, such a convex combination is computable in polynomial time using a separation/optimization oracle for $P$: this is proved, for example, in Corollary 14.1g of Schrijver's Theory of Linear and Integer Programming. I.e., given an efficient separation oracle for $P$, we can construct a procedure that takes $y$ and returns vertices $v_1, \ldots, v_k$ of $P$, $k \le m+1$, and coefficients, $\lambda_1, \ldots, \lambda_k \in (0,1]$ such that $y = \sum_{i = 1}^k{\lambda_i v_i}$. Then, $y$ is a vertex of $P$ if and only if the procedure returns $v_1 = y$ and $\lambda_1 = 1$. It is clear that if this happens, $y$ is a vertex. Conversely, if this does not happen, then $y$ can be written as a convex combination of other points in $P$, which means that $y$ is not extremal, and, therefore, not a vertex. Computing $v_1, \ldots, v_k$ and $\lambda_1, \ldots, \lambda_k$ using a separation/optimization oracle is not too hard. First you compute a vertex $u_1$ of $P$: figuring out how to do this by optimizing over $P$ is a nice exercise. If $u_1 = y$, we are done. Otherwise, let $\ell$ be the half-line starting at $u_1$ and going through $y$, and let $z$ be the point where $\ell$ intersects the boundary of $P$. Then $z$ lies in a proper face of $P$, and we can recurse on that face to express $z$ as a convex combination of the vertices of the face. We finish by expressing $y$ as a convex combination of $z$ and $u_1$. See Schrijver's book for the details. 

Another relaxation of $\ell_1$ dimension reduction is to require that $S$ lies in a $c$-dimensional subspace of $\mathbb{R}^d$ and make $k$ depend on $c$. Talagrand proved that given a $c$-dimensional subspace $V$ of $\ell_1^d$ (he even proves it for $L_1$), there exists a map $f:\ell_1^d \rightarrow \ell_1^k$ for $k = O(\epsilon^{-2}c\log c)$ such that for all $x, y \in V$, $(1-\epsilon)\|f(x) - f(y)\|_1 \leq \|x - y\|_1 \leq (1+\epsilon)\|f(x) - f(y)\|_1$. His embedding is a simple randomized procedure, but it proceeds in steps and each step succeeds with constant probability; after each step you need to check that the step indeed has been successful and repeat if it hasn't. So Talagrand's embedding lacks a crucial feature of JLT: the fact that $f$ can be picked from a distribution that is independent of $S$. Very recently, Woodruff and Sohler have proved a result analogous to Talagrand's, but with the added feature that $f$, just like in JLT, is a linear mapping picked from a distribution independent of $S$: you need to pick a $k \times d$ matrix where each entry is an iid Cauchy random variable. This is in the spirit of Indyk's stable projections: Cauchy is 1-stable. 

Another natural topological problem, similar in spirit to Peter Shor's answer, is embeddability of 2-dimensional abstract simplicial complexes in $\mathbb{R}^3$. In general it's natural to ask when can we effectively/efficiently decide that an abstract $k$-dimensional simplicial complex can be embedded in $\mathbb{R}^d$. For $k=1$ and $d=2$ this is the graph planarity problem and has a linear-time algorithm. For $k=2$ and $d=2$ there is also a linear time algorithm. The $k=2$, $d=3$ case was open until last year, when it was shown to be decidable by Matousek, Sedgwick, Tancer, and Wagner. They say that their algorithm has a primitive recursive time bound, but larger than a tower of exponentials. On the other hand they speculate that it might be possible to put the problem in NP, but going beyond that would be challenging. However, there doesn't seem to be any strong evidence that a polytime algorithm is impossible. The latter paper has many references for further reading. 

Let me give some details for the Cauchy matrix construction, which is simple. Let $x = (x_1, \ldots, x_n)$, $y = (y_1, \ldots, y_m)$ be sequences of pairwise distinct numbers. The corresponding Cauchy matrix is $$ C = \begin{pmatrix} \frac{1}{x_1 - y_1} & \frac{1}{x_1 - y_2} & \ldots & \frac{1}{x_1 - y_m}\\ \frac{1}{x_2 - y_1} & \frac{1}{x_2 - y_2} & \ldots & \frac{1}{x_2 - y_m}\\ \vdots & \vdots & \ddots & \vdots\\ \frac{1}{x_n - y_1} & \frac{1}{x_n - y_2} & \ldots & \frac{1}{x_n - y_m}\\ \end{pmatrix} $$ Then any square submatrix of $C$ is an invertible square Cauchy matrix. In particular, the $n\times n$ submatrix $C_J$ with columns indexed by $J = \{j_1, \ldots, j_n\}$, $j_1 \leq \ldots \leq j_n$, has determinant $$ \det C_J = \frac{\prod_{i = 2}^n\prod_{k = 1}^{i-1}{(x_i - x_k)(y_{j_i} - y_{j_k})}}{\prod_{i = 1}^n\prod_{k = 1}^n{(x_i - y_{j_k})}}. $$ Because $x$ and $y$ are sequences of distinct numbers, the determinant is nonzero, and $C_J$ is invertible. If you prefer something even more concrete, you can take the Hilbert matrix: the Cauchy matrix with $x = (1, \ldots, n)$ and $y = (0, -1, -2, \ldots, -m)$. With $n = 3$ you get the points $(1, 1/2, 1/3),\ (1/2, 1/3, 1/4),\ (1/3, 1/4, 1/5),\ (1/4, 1/5, 1/6)$ and so on. 

Does this count: Chazelle's semi-group lower bounds for fundamental range-searching problems (in the offline setting). All lower bounds are almost optimal (up to log terms when the lower bounds is polynomial and log log terms when the lower bound is polylogarithmic). 

You are asking for the near neighbor problem for the Jaccard index. If you are ok with some approximation and randomization, I think there exist subquadratic algorithms based on Locality Sensitive Hashing (LSH). LSH schemes based on MinWise independent hashing are known for the Jaccard index: see the LSH survey by Andoni and Indyk and references therein, plus the LSH webpage (which has some code, although I am not sure if there is any for Jaccard specifically). Also see the original MinWise hashing paper. 

For the benefit of the others, the minimum code distance problem for codes over $\mathbb{F}_2$ is: given a matrix $H$ ($m$ by $n$, with elements from $\mathbb{F}_2$) and a positive integer $w$, does there exist a vector $x$ such that $Hx = 0$ and the hamming weight (number of 1's) of $x$ is at most $w$. Since both max ind set and the minimum distance of a linear code are NP complete, there are many-one reductions in either direction. However, it doesn't seem that any of the reductions in the literature that I have seen are directly from max independent set. Actually it seems that the known reductions are from other problems related to coding. Alexander Vardy proved the NP-hardness of computing the min distance by reducing from the max-likelihood decoding problem: given a matrix $H$ over $\mathbb{F}_2$, a vector $s$ and a positive integer $w$, does there exist a vector $x$ such that $Hx = s$ and the weight of $x$ is at most $w$. This problem was shown to be NP-hard by Berlekamp and others by reduction from 3-dimensional matching (which is one of the Karp problems and can be reduced to from 3SAT). If you want to reduce max independent set to 3SAT and piece together all these reductions, I guess you can get what you want... There is a line of work that looks at hardness of approximating the min distance problem. However, reductions there are also from other coding problems. For example, Dumer and others show a hardness of approximation result for the min distance problem by reducing from the nearest codeword problem (given a message find the nearest codeword in hamming distance). A hardness of approximation result for the latter problem was proved by Arora and others, as far as I can tell using two methods: reducing from set cover, or reducing from label cover.