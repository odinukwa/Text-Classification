Each of these options will install various components depending on the installation type selected. A list of items installed with each installation type can be found on Oracle's support site in the following document: What Are The Different Oracle Client Components Installed With Different Installation Type In 12.1.0.x (Doc ID 1610994.1) (Oracle Support) 

Delete the backup file on my disk (or rename it), just because it is the one in-between. Open up the Restore GUI in SSMS and add .. 

Looking at the above questions consider using a different cleanup time (e.g. ) to keep additional hours worth of Transaction Log backups on your database server's disks. Benefits 

sys.dm_os_memory_clerks But instead of looking at individual caches, you might want to consider querying the view: sys.dm_os_memory_clerks 

I would go and look at the setup.log files for the Service Pack 3 Upgrade. They should be found here: 

My initial answer referenced deprecated features instead of discontinued features. This has been corrected Service Pack Levels (incl. RTM) are not the same as compatibility level settings. I'll explain: Features in different SQL Server versions SQL Server provides a rich set of features for different versions of SQL Server. These features can be removed from one version to another. Your application could be using individual system stored procedures, system functions or some other features that are no longer available. Here an example: 

Click on the Protocols for NAMED_INSTANCE_n branch and in the right pane right-click on the TCP/IP Protocol Name entry. Open up the properties. In the Protocol register verify the following: a. Enabled = Yes b. Listen All = No Switch to the IP Addresses register You should find sections in the form of IPn for each of your IP addresses 10.10.1.1 and 10.10.1.2. This is the tricky bit: You are already in an instance and want to configure only one IP address for this instance. In the IPn section corresponding to the IP address for this instance check the following: a. Active = Yes b. Enabled = Yes c. IP Address = 10.1.0.n d. TCP Dynamic Ports = leave empty e. TCP Port = 1433 Scroll down to the IPAll section Verify the following: a. TCP Dynamic Ports = leave empty b. TCP Port = leave empty In all other IPn sections ensure the following: a. Enabled = Yes b. Active = No (unless you want to communicate with the instance via the IP configuration settings in the section). 

Summary So while sys.dm_os_buffer_descriptors does provide an overview of the data cahce, you will have to query other sys views to determine the actual current memory usage. Your SQL Server instance may at one time have occupied 53 GB of RAM, but it is now only currently consuming 39 GB of RAM (in data cache) and more GB in plan cache and memory objects, which may sum up to 53 GB RAM, but which may also sum up to less because of free pages in RAM, that may not yet have been released by SQL Server. 

No, when considering the FULL and DIFF backups. No, when considering the TLOG backups, but it will free up the VLFs inside the TLog file, if the database engine has some time to spare. 

The name is the name of the share you provide when initially configuring FILESTREAM in the SQL Server Configuration Manager. But what is it for? So far I read through all the available FILESTREAM documentation starting at: 

If a user is reported as orphaned you can relink the SQL Server Login with the Database User by issuing the following command: 

The DMVs reference some system base tables, which can be accessed (but shouldn't): Reference: System Base Tables The policy you are looking for is built in to the code of SQL Server and is set per default for each new account. When creating a SQL Server Login you can decide to turn off the defaults: 

Alternate Solution (process node empty) After further investigation of some blocked_process_reports the following alternate explanation can be made. The Extended Events are capturing blocked_process_reports which are unrelated to any other processes at the time. Ergo: They must be blocked for a different reason I would suggest you capture a time frame of wait types from the sys.dm_os_wait_stats view on your SQL Server and correlate the numbers with the blocked_process_reports happening during your measurements. Paul Randall has a good script: Send me your wait stats and get my advice and 30 days of free Pluralsight in return The scripts captures the current counters, waits for 23hours (can be modified), recaptures the current counters again and compares them to give you the top 95% of wait types. You could try this out for say 1 hour and have the XEL file handy. You might find a wait type (e.g. LCK_M_SH, …) that is telling you that your storage is slow in writing. Or that you have some other overhead (e.g. CX_PACKET_WAITS, ….). Something is slowing down your Updates. You can then see if the sys.dm_os_wait_stats relate to the blocked_process_reports with the empty nodes. There are cases when a blocked SPID is being blocked by the same SPID: The blocked column in the sysprocesses table is populated for latch waits after you install SQL Server 2000 SP4 

There are various possibilities and I will try to answer them according to the numbered questions. Valid connection strings for the SSMS login box are: 1st instance 

SQL Server Configuration Manager When you run the SQL Server Configuration Manager and modify settings the configuration manager will add and remove permissions on files and directories based on the configuration changes made. This is why Microsoft states, that configuration changes should always be made with the SQL Server Configuration Manager. 

Deleting the Filegroup If you still have a file associated with one of your filegroups, then the complete command to delete the filegroup's logical file and the filegroup itself would be: 

SQL Server Management Studio Startup When Microsoft's SQL Server Management Studio (SSMS) starts it tries to connect the Certificate Revocation List (CRL) of Microsoft: $URL$ The underlying .NET components of SSMS are trying to contact the Certificate Revocation List and SSMS is unable to do so. This slows down the overall loading procedure. (15 seconds per certificate apparently) 

Full-Text Catalog Indexing takes time. The official Microsoft Document states (emphasis by me) in the section Population based on change tracking: 

You could always combine the two queries into one query and output the required information in one go: 

You can not force the switching unless you shut down your database. If Oracle thinks it still needs the old UNDO TS then so be it. Your system will already be writing to the new UNDO TS so there is no need to force anything unless you have special requirements. 

(emphasis mine) Reference: Enterprise Manager Advanced Configuration (Oracle Docs 10g) In your case you would run the following commands: 

Owner? You should have reset everything the way MySQL prefers it, unless you modified the owner of the sub-directories and files, which should normally belong to the linux user and equally to the group. In the case that the files and directories no longer belong to then you might have to reset the owner and group using: 

Open up the SQL Server Configuration Manager with Navigate through the SQL Server Network Configuration branch and you should find two sub-branches for ... a. Protocols for NAMED_INSTANCE_1 b. Protocols for NAMED_INSTANCE_2 

Double-Checking Filegroups in Database Verify that the filegroup does not have any files left attached by issuing the following command: 

The order is date descending You can see the last backup at the top, the previous (in-between) backup at , the previous backup at , and so on. To restore the database to the current point-in-time I would have to use the first backup from (because I deleted the in-between backup) together with all the backups. Let's give that a try. 

This can help ensure that you can restore to the RPO requested by your company's Business Continuity (BC) team and Information Technology Service Continuity (ITSC) team. You will also have to somehow guarantee that the database backups (FULL, DIFF and TLOG) are stored in a location that will not be affected by a down-time/data-loss in your current location (e.g. separate data centre, off-site data centre). When storing data off-site, ensure that you can still guarantee the RTO if you have to calculate in some additional time for data copy operations or restore over slower network connections. References 

If I weren't using Simpana for the backup, who would (have to) delete the *.mmmmmmmm.backup files in the WAL Archive directory? The command? Why is there still a full WAL file in the Archive Log directory, when it should have been deleted like all the other WAL files in the Archive Log directory? Why isn't there a file for the still existing WAL file in the archive log directory? 

This script was handed over to me by Ramesh Meyyappan at a course on "SQL Server Performance Tuning and Optimisation" at Microsoft SA, Wallisellen, Switzerland. 

Part I of II (had to split my answer) There are one or two things to think about here. In my opinion you may have taken a shortcut in the thought process. I'll explain while I'm making this assumption... You might want to have a quick look at my accepted answer here which was posted in reference to the question Need suggestion on Back up Strategy [closed] to give you some ideas about RTO and RPO. Why? Because setting might be a bad idea... First to answer your question. Is it a bug in Ola's script? - No. Ola designed his script to check for the last (Full Backup) and then to delete the transaction logs backups () which have occurred before that backup and only if the is lower than when the FULL Backup occurred. 

Fixing Permissions To alter the directory and file permissions you would have to set off two commands like this: Directory Permissions 

If you are on the SQL Server where the database resides, then you can follow these steps to connect to a database in PowerShell First you have to load the SQL Server PowerShell Module. This is achieved with the following simple command: 

Ola Hallengren's solution is based on beset practice and years of experience. There are some quirks though. Take a look.... Before you start Before you run the initial script, check the first few parameters and the database used: 

assuming you are using a CMD / DOS Prompt Depending on the path in which your mysql.exe resides, you might have to incorporate using double quotes as follows: 

Solution Nr. 2 As pointed out by @RamakantDadhichi, changing the filename to be unique by adding the date and time can help prevent multiple backups in one file. 

...and the feature is also documented in the Administrators Guide in the section 5.10.5 Cancelling a SQL Statement in a Session as follows: 

You can configure your (company's) firewall to send a timeout faster to your client for requests accessing Microsoft's CRL. 

_2105058535 being the object_id of his procedure_ My musings Now if you take the information retrieved with Mark's analysis and compare that to the original statement using the code, it is probably safe to assume the following: is similar/related to because the data retrieved contains the source code for the dummy Procedure containing the string . 

Instance | IP | Port | Alias (CNAME) Each instance is related to an IP address and each IP address has an individual Alias (CNAME) so that the SQL Server can always listen on port 1433. This simplifies the firewall configuration as the rules only have to be added for the default SQL Server port. Hmmm. 

Now if you know that these jobs belong to Ola's solution, then you can leave them as is. Otherwise if you would prefer to mark them, then you might consider prefixing a string inside the before you execute the script. Do a regex search for the string with a suitable text-editor and add a prefix to the job's name e.g. 

You can configure IE to no longer "Check publisher's certificate revocation" in the advanced settings. (See above mentioned blogs 1 and 2 for details) 

Have you compared the permissions of the files on master with the files on slave? Have you compared the owner of the files / directories on master with owner of the files / directories on the slave? Have you compared the contents of the directories on master with the directory contents of the files on slave? (Missing files) Are the mounted filesystems on the slave still Read-Only? 

If you would adapt that to your 2 Million row table, then the statistics would update roughly after on a SQL Server 2014 or older version. It's slightly different for SQL 2016 and newer: 

Reference: Identify SQL Server Database Growth Rates (mssqltips.com) Based on the information provided with these scripts you could then set the Growth Setttings of your database to be near the (Microsoft Technet Script) value or near the value (Mssqltips.com Script). If you interpolate into the future, you could possibly set the Max Size of the database files (*mdf) to be a multiple of the returned values (12 month worth?) plus the current DB size. 

You would create a template for your standard profiling and use this in the command-line startup Start SQL Profiler via command-line referencing your template Query data stored in table 

You might find that your old UNDO tablespace is still being used or in the state. You could also try switching the UNDO tablespaces back and forth with: 

Deadlocks can occur because two processes are accessing the same tables, but each from the other side, while additional locking occurs. Looking at your code I wouldn't think the two updates are the root cause of the deadlocks, but more victims of other statements running in the background. If you can stop all other activity on the database and run only the two update procedures simultaneously, do you still have deadlocks? Deadlocking is explained here: Detecting and Ending Deadlocks You can monitor deadlocking by following the information in the article: How to monitor deadlock using extended events in SQL Server 2008 and later A quick win can be achieved by turning on two trace flags to capture the deadlocks in the errorlog file of SQL Server. If you have the deadlock information you will be able to pinpoint the actual root cause. On a side note: Are you actually having deadlocks where one statement is rolled back? Or are you having a blocking chain where multiple spids are waiting for one spid to finish? (You should be able to see deadlocks in the SQL Server errorlog at all times). Deadlocks != Blocking Chains