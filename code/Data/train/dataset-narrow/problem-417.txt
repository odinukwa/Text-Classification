(That would be in Oracle 10gR2) In short, I want to know how much (the total, over and below the HWM) space does Oracle have to keep inserting things. If my query is wrong, can someone provide one that does that? pd. I also have other messy queries summing the segment sizes of objects, but then again, I do not know how all this wraps together with fragmented space and HWM. An explanation here would be highly appreciated. 

The good one. All I did here was taking the result of (seven records), and added an to the end of the big query. Same as described: 

The partition containing my Oracle 11g installation began to fill up with trace files and when I went looking for what are these I've found this disturbing post about how to disable the trace file generation. The two related questions I have are: 

Mind that I am not asking "how to rewrite it more efficiently", but rather how do I find witht the explain plan what the most costly operation there. Meanwhile I am reading about it, but I'd appreciate some help. 

Inserting 10 rows each 2 minutes will result in (24*60/2)*10 rows per day (7200) -> that is not a large value to worry about. Also, it's great that you think on the "future - 10 years", but don't lose time with premature optimization. If your only concern it's about this table where you're inserting data every 2 mins, there's no point in creating additional databases (one per year), so let's stick with this table (Table_A). Now, because this table will slowly increase in time, and your want your queries to run fast, you have plenty of options: 

This is a sample matrix. The query should find all the green cells, with value = F (as is TO BE FOUND :) ) ... 

Why would you want to use the DB to check for files ? Just write a shell script (if using Linux) or bat script (Windows) to check for the files, and use cron or Windows task scheduler to run that script. You can also create the INSERT query in the script (on Linux + MySQL that should be easy enough), and every time the files are checked, a new row should be inserted in your Frequency table. Don't try to use the DB for something that can be easily done at OS level. 

Can I simply delete these trace files from my FS w/o consequence ? (I know, if they apeared one should probably look at them.. but won't happen and the database has no production use whatsoever. Going to have to happen another time.) What should be done to prevent it from appearing again? 

The Question is: I have an index containing the columns , and (that , which it even uses on the first part). Since there is no further joins, why would it be doing the FTS? 

I am running a query in some big tables, and although it runs fine even tough is a lot of data, I'd like to understand what part of it weighs on the execution. Unfortunately I am not too good with explain plans so I call for help. Here is some data about these tables: 

I'd use a to generate a list of components a certain customer has and compare it to another list generated the same way somewhere else to detect inconsistencies. The problem is that sometimes a customer can have too many components and exceed the maximum output of . I was wondering if there could be a way to generate a hash from a group of ordered rows, in a way I could use to make the same validation. Sort of: 

In your first query, both valid values + null values are returned from the database, but because you use the "limit 50", and because NULLs are displayed last, you don't see the rows containing NULL Price. The correct for for the first query should be: 

From docs: ($URL$ The Oracle Flashback Database feature, which provides an convenient alternative to point-in-time recovery, generates flashback logs, which are also considered transient files and must be stored in the flash recovery area. However, unlike other transient files, flashback logs cannot be backed up to other media. They are automatically deleted as space is needed for other files in the flash recovery area. When Files are Eligible for Deletion from the Flash Recovery Area There are relatively simple rules governing when files become eligible for deleteion from the flash recovery area: 

* I'm not an ORACLE Dev, but the function should look something like the one above... Once you create the function, you will create a functional index that will use the function to look for the rows you need: 

The query causing me the issue is too big, but the piece that seems to be the core is quite simple, I will try to go with that: The query has an structure like: 

And that has a cardinality of a few billions, bytes read around 12gb and explodes my temp. However, if I execute the , which yields (always) 7 records, take these records and change the query to: 

(I mean, values hardcoded) Cost, cardinality, and bytes read drop dramatically and the query executes in a few minutes. Now, I have tried to isolate the "small query" in a clause, tried to use an join instead a sub query and nothing... the result is always the same. Why is it this way and how could I possibly prevent it from happening? Maybe worth mentioning that in both cases (fast and slow) the costly part of the query is a FTS on one of the big tables used in the join. Also, I am using Oracle 11gR2 [EDIT] These are the explain plans of the two example executions The bad one. Notice I didn't use , but rather a simple join adding to the clause. 

Your questions is a little foggy You don't mention how the archiving process works You're basically asking about a feature on the User Interface side ... not on the DB side. 

Also, I believe that the application that's using your database already knows what those values mean. So what is the real use case for your constants ? If you really want to go with your approach, without changing the existing column type, you can create a function that will return the STRING value for each integer code: 

The reason for which you first query didn't use the index was because the result of isnull(Price) returned an un-indexed value. 

FRA will be purged automatically when free space is needed Put the archived logs in FRA as well Use RMAN to make the backups and use RMAN commands (REPORT OBSOLETE / DELETE OBSOLETE) to manage the purging of backup pieces + archived logs from FRA 

you can partition the table by certain criteria. Because your using the table for a "weather station", and your data is time series values, your best option would be to partition by [station_id] then by [created] Create a Table_A_Archive, where you can move data that would be to old to keep in Table_A. How long do you intend to keep data in Table_A ? would it make sense to delete old rows that become obsolete for you application