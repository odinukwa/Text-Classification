Once the first model has scrolled entirely off the screen you can stop drawing it, so at most you are only rendering the model twice. If the model is particularly huge, you may want to break it into smaller chunks simply to avoid all the extra geometry processing to determine that large portions of the model are offscreen. 

If you are using a third-party library to do you window management and view creation, that library should provide you a mechanism to deal with key events. If it doesn't, look and see if it offers a way to provide a custom , and if it doesn't do either of those things, you should find a better library. SFML is a popular alternative. 

I would say, as somebody who has worked on MMOs, that this approach seems a little half-baked (perhaps that's just because you're omitting portions of it?), but more importantly takes far, far too granular approach to the distribution of tasks across a network of machines. The overhead of the intercommunication protocol would impact your performance significantly, and debugging the thing would be quite a nightmare. A far better approach would be to distribute work at a higher level, choosing a level of granularity that maintains as much local coherency as possible so all your processing for "nearby" actions, players, events, et cetera occurs on one server (on Guild Wars 2, we distributed work at the map / region level, for example -- although note that this is not how the "sharding" in GW2 worked, that was a done differently and for a different reason). 

Don't guess -- profile. Determine where your bottleneck is (provably) before you go jumping at random ideas about what might be slow. You may not be bound by the performance of the GPU at all, there may in fact be some other bit of code that runs on the CPU in your game that's killing your performance. Your best bet is going to be actually sitting down and analyzing the performance of your program with various tools to determine if you are CPU or GPU bound, and then looking into profiling that. If you end end CPU bound, look for regular C# code profilers to examine your application. If you are GPU bound, consider something like this. That said, a typical technique for improving render performance is to reduce draw calls, but you have limited capability to do this with XNA's model objects. As far as I know, you'd have to re-export the model such that there was only one large mesh part that contained the entire thing, and this might actually make the model much larger (if each mesh part's vertex buffer had different attributes) and is complicated by the fact that you'd have to use the same texture and shader for the entire thing, which could also result in much larger textures due to the necessity of having to pack multiple images into one texture object. Furthermore, if the object is really big you could actually reduce your performance this way, because you couldn't apply any kind of culling to the model to entirely remove draws for parts that are completely off-screen or occluded. It's not always a simple solution, which is why analysis tools and metrics are so important. 

This allows you to treat all objects in your game that act like humanoid characters -- which have a position in the world, probably have health and speed stats, et cetera -- in a uniform fashion, but doesn't force you to treat other objects (such as bullets, which probably don't need health) that way. Even though a bullet will have a position, it doesn't need to be a and you don't need to cram it into that inheritance hierarchy at all. The interfaces a type implements should be based on how that type may be used, not how it actually functions. It's okay to have two different classes that each have a function to get a position that isn't the same function. Sometimes you need to use that data in two different ways (consider the circle-ellipse problem, which leverages a common-but-flawed example used to academically demonstrate inheritance). And if you ever need to use that data in the same way, that's when you can consider what to do based on your actual, practical needs at the time. The principle at work here is that you should evolve your interfaces by making the least functional thing that could work. Expose only the data and types you need now, expose only the relationships you need now... name things accordingly. This allows you to control the growth of dependency on the interface more easily, which is more manageable in the long run. 

You probably refer to your shader file in code via the relative path (or similar, adjusted for your actual directory structure). Visual Studio uses the project directory as the working directory by default for C++ projects when launching your process. In the case of the above structure, that is the path and consequently your relative path reference to the shader is . This path exists, so everything runs normally when launched from the IDE. But if you double-click the compiled from your working directory is and consequently your relative path reference becomes . That path doesn't exist, and until it does, your game won't run properly. The solution is to ensure that the shader and other assets get copied to the output directory of the executable (maintaining their directory structure, if applicable). One way to do this is via custom build steps in your project, for example by launching a batch file to copy the relevant files or using a direct invocation of something like robocopy. Visual Studio has a fairly robust set of macros that can be used to make construction of this build step easier, so you could do something like: 

Both of these options may very well be impractical due to the level of control Unity may give you over the lower-level OS objects -- window handles, primarily, and hooks for periodic updates. Even if you got either of the above options to work, you'd be sacrificing portability (if that matters). Based on some of the questions floating around Unity Answers (this one and this one, for example) it looks like video playback is not in an ideal state and may not get there any time soon. Finally, you may want to browse some of the resources linked in this external Unity resources question, which might lead you to something useful for your purposes. 

The problem is basically spelled out for you in the error message: Direct3D does not support the use of the format in a typed UAV. MSDN has a section on supported formats for typed UAVs. is not on the list. 

You make up most of this information, and the rest of it is supplied to you when you signed up for your relevant store publishing account. It's all documented in the manual: 

Specifically in the case of , you can see that it can fail in a variety of ways (as of the writing of this answer): 

For pixels that are part of the triangle, pixels on the top edge and the left edge are filled. Pixels on the bottom and right edge are not. A top edge is completely horizontal, otherwise it does not exist. You may want also peruse the documentation on mapping pixel coordinates to texel coordinates in D3D9 and, should you ever consider moving to D3D10+, take a look at the modern rasterization rules. It's also worth noting that you will see variation in this behavior from hardware to hardware (although usually that variation is minimal). It could be that that explains your discrepancies, and it also means it will be extremely difficult for you to achieve a 100% match to a hardware-based rasterizer from D3D; consider using the reference device when doing your testing. 

What you're seeing is a signed zero, which is a possibility in certain systems of number representation, including the IEEE standard used for C++ floating point representation, even though it isn't really a thing in "ordinary" mathematics. The link contains some notes on behavior and handling of the occurrence, although beyond that it's not entirely clear to me what you're asking, specifically -- it's not a bug with though. In fact, the wiki article on the function notes that the relevant Intel assembly instruction (), which you're likely generating here, will produce a negative zero when first parameter in negative zero but the second is positive zero. Your explicit negation of your vector's Y component could easily produce such a scenario. 

Assuming you have the data set you want to plot already, and the map you want to plot it to, you can start by creating an intensity map: a grid of floating point values that is proportional in bounds to the final map (if the maps are small enough, 1:1 is probably fine). Initialize the entire array to 0. Then you walk through each point you want to plot, map it into the coordinate space of the intensity map (which should be a simple scale operation, normally), and then plot a "blip" in the intensity map centered at that position. A basic "blip" might just consist of increasing the intensity at the blip point plus some radius by a small amount. More complex implementations could read the existing intensity, and use a bigger falloff radius the more intense the blip point already is. You can experiment with the blip plotter to find an implementation you like the look of. Once you have an intensity map you can use the intensity at each individual point as a 1D look-up into a color gradient, which will allow you to achieve the desired visual impact (this is how you can get the multi-colored results that are most commonly seen). You should do this color lookup as you transfer the intensity map to your final plot (rescaling, obviously, as necessary to account for size differences in the intensity map versus the final image). This should be enough for a basic implementation, but there is room for optimization. For example, the intensity map will not be normalized, so you may need to renormalize it (probably slow) or keep track of the maximum intensity as you plot each blip, so that you can perform the renormalization of an individual intensity at the same time you are doing the recolorization. Additionally, it's possible that the distribution of your values is such that it is not memory-efficient to be storing the entire coordinate space of the map, and you may want to use an alternative solution that doesn't involve preallocating a large chunk of memory that will be mostly-empty. If you have enough data beforehand to query the minimum and maximum intensities you expect to see in the data set you can avoid having to renormalize at all -- basically if you have some map between (X,Y,Z) to the number of "hits" of the plotted data that occurred at that point -- that's something you can build in to the system that collects the data which will help you optimize the mapping portion. Since the intensity map is just, essentially, a grayscale image a really easy way to prototype this kind of system to use a bitmap for the intensity map and your drawing API of choice (for example, in C#) to plot partially-transparent circles to produce an intensity map. It doesn't look the best, but its functional. 

Very little. MonoGame doesn't provide any explicit thread-safely guarantees about that I can see, and as such you should assume it is not thread-safe in any way. For the most part, if you access the device from a single thread at a time (using your own synchronization primitives) you'll be "safe-ish" (although you may run into problems where running a different platforms gives you wildly different results due to different graphics APIs), but there are a handful of threading-related bugs in MonoGame's code (for example, while glancing through the code for this question I saw several instances where events are raised without caching local copies, which creates a potential threading bug). You're better off adopting a model where you can do all your work in your background threads using your own data, and then setting a flag or putting the results of that work into a queue that the render thread consumes to do the actual graphics device work necessary. 

Anti-aliasing is one of those things GPU vendors like to add explicit controls for in their drivers and control panels. Typically the options in these control panels can be used to override an application's requests to D3D for things like anti-aliasing support, and so on, and typically only work for retail runtimes. It is consequently possible that the control panel for your GPU drivers has a setting (which is active, either by default or because you toggled it willingly or not) to disable AA. 

Yes, it can. A simple way of doing so would be to just set the particle size to "Random Between Two Constants" as described here, choosing two constants that work for you, and using the texture sheet animation module to refer into a texture where each row represents the animation of a particle (and setting the "Random Row" checkbox). You could also just use an animated sprite, as you noted. It would give you more fine-grained control at the cost of less random / dynamic behavior (and could require more up-front work to author and tweak the animation). It's up to you. 

Branching should be sufficiently fast on modern cards that you don't need to worry about it, and you can always profile it if you think there's a bottleneck. Branching will also make for somewhat more readable code, since your intent is much clearer, and the code you're currently using does imply the possibility of a "25% mix" between the two, for example. Your current example also causes the texture to be sampled, even if it would be the "default" value, which you could avoid with an actual branch, potentially. But there's really nothing horrible with your current solution, and if you have a need to support cards/versions that are old enough that branching isn't very fast, it's probably better. 

Configure your servers to capture and save crash dumps (if you run on Windows, MiniDumpWriteDump and SEH is a good place to start) so that you can diagnose hard crashes offline. You'll probably want to make sure your release builds generate and archive a PDB or other symbol- and debug-help related data internally as well as provide a mechanism to allow you to correlate the crashed build ID and the proper PDB (et cetera) for in-house examination. You may want to familiarize yourself with debuggers other than Visual Studio -- later versions of VS are much better at just "doing the right thing" with a minidump but it's also worth looking at WinDbg, which is a standalone debugger that's more expressive (but much harder to learn to use). It's saved me a lot of trouble on occasion. For code in C#, for example, minidumps plus the SOS extension for WinDbg make for a very useful postmortem debugging environment for managed code. Additionally, the process that collects the crash dumps should probably report metrics back to you somehow so you can monitor how frequently you are getting live crashes. As for handling non-crash bugs, build logging and metrics gathering tools. This is harder to do after the fact, but possible (it's much easier if you integrate the construction of these metrics into the construction of the software itself, but the ship has probably sailed on that option by now). You want to allow for the logging of just about anything. In an ideal world, you want to support the ability to add logging after the fact without having to reboot the entire (player-facing) server. Whether or not this is possible depends largely on how your server infrastructure handles failover -- if you can bring a new build of the server executable (with new logging) up and transparently migrate users over there before beginning your analysis, that'd be best. Of course, that's not a small task to accomplish either. But at the very least you'll want your logging to be toggle-able -- you don't want to log everything, everywhere, for every user, all the time. You should gate the ability to enable the log to a server-side check of a user's rights (so only developer or GM users can toggle it) and only communicate that log data through a private, back-end channel that your developers can access from the privileged side of the network. On top of the logging you can build your own custom live debugging UI, if you log in a structured enough form. This is essentially a debugger you can connect (again, only through the internal side of the network) that parses the log data (when enabled) and displays it in a more human-readable form, but does not halt execution of the server process itself. That way you disrupt other players minimally. Obviously you can tailor the specificity of this to your needs and your actual problem. I'd recommend that approach actually -- build a log/debug view that's very specific to the actual problem you are having and generalize it after the fact. That way you focus on the real issue first and don't get lost trying to build an overly complex generalized system with features you may not yet need. 

That means is not marked with the attribute, which is a requirement (just because Unity says a type is serializable doesn't mean it's serializable with all serialization engines; in this case, it probably means that the type is serializable with Unity's engine, not the one that is part of the .NET BCL). You can either use Unity's serialization mechanism, or you can try your hand at implementing an object for the Unity game object (this is likely to be tricky, and more effort than just using Unity's serialization).