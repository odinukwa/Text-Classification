If you already have the names in R, you can iterate over the vector of names, and use the function. Here's the help page for it. would be another option, if you prefer that approach. An example of this would be 

I tested this out on rubular, and it works on your example. It should generalize to others as well. To walk you through the regular expression, and match the beginning and end of lines, respectively, while (.*?) uses non-greedy wildcard matching to capture, first, everything up to the newline character, then everything up to the next end of line. 

Blast. It looks as though ARIMAX is currently a work in progress for the cloudera/spark-timeseries community. Although some work has been done on the model development side, it doesn't appear to have been merged in. 

Separate out $10\%$ of your data for hold-out evaluation. Divide the remaining $90\%$ into $5\%-10\%$ parameter optimization, and the remainder for final parameter eval. Compare the feature distributions of the parameter optimization and final parameter eval data sets, if they're not comparable, redraw the samples (within the context of the $90\%$ development sample) When you have a good subsample, run your parameter optimization experiments on the small parameter optimization data set. With your final parameter settings, evaluate with cross-validation on the the $90\%$ combined parameter optimization and final parameter eval data sets. Perform final model analysis by training on the $90\%$ data set and classifying on the $10\%$ hold-out evaluation set. 

Interesting problem. You certainly could use time series methods for this, but you should consider whether you think there is face-validity to song plays following a temporal pattern. From your plot above, I don't see any structure in the variation, but perhaps there is something that an algorithm can pick up. A good starting point would be R's function for autoregressive integrated moving average (ARIMA). The ARIMA method is more fully described elsewhere (e.g., here), but an interesting extension to this approach might be using exogenous predictors in your model. For example, does the presence or absence of rain affect the likelihood of some songs being played? My guess would be yes (I love listening to post-rock bands on rainy days), but perhaps that's not true of everyone. This extension to ARIMA is referred to as ARIMAX in the literature. A really nice breakdown of how to handle covariates in your type of analysis can be found on Rob Hyndman's blog. A second thought is that, with sufficient computational resources, there's an interesting hidden markov model (HMM) problem here. You could potentially model the transition probabilities from one song to another. Presumably there's a high likelihood of transitioning from track 1 to track 2 of a given record, but sometimes people might switch to a different record after listening to a favorite track on an album. The extent to which this is true for different tracks could be a part of this analysis, and you could attempt to model the hidden underlying states that motivate a listener to stay within a record, or to move to a new one. 

I've done something like you're describing using mongoDB--I think you'll best use your time using some sort of NoSQL approach, rather than creating a specialized one-off solution. If you're using Python, I've had excellent experiences using PyMongo to handle reads and writes from within my code. I would strongly caution you against adopting your approach #1. This could break very easily in the future, and there are databases designed to handle your exact problem! 

I'd recommend spending more time thinking about feature selection and representation for your SVM than worrying about the number of dimensions in your model. Generally speaking, SVM tends to be very robust to uninformative features (e.g., see Joachims, 1997, or Joachims, 1999 for a nice overview). In my experience, SVM doesn't often benefit as much from spending time on feature selection as do other algorithms, such as Naïve Bayes. The best gains I've seen with SVM tend to come from trying to encode your own expert knowledge about the classification domain in a way that is computationally accessible. Say for example that you're classifying publications on whether they contain information on protein-protein interactions. Something that is lost in the bag of words and tfidf vectorization approaches is the concept of proximity—two protein-related words occurring close to each other in a document are more likely to be found in documents dealing with protein-protein interaction. This can sometimes be achieved using $n$-gram modeling, but there are better alternatives that you'll only be able to use if you think about the characteristics of the types of documents you're trying to identify. If you still want to try doing feature selection, I'd recommend $\chi^{2}$ (chi-squared) feature selection. To do this, you rank your features with respect to the objective \begin{equation} \chi^{2}(\textbf{D},t,c) = \sum_{e_{t}\in{0,1}}\sum_{e_{c}\in{0,1}}\frac{(N_{e_{t}e_{c}}-E_{e_{t}e_{c}})^{2}}{E_{e_{t}}e_{c}}, \end{equation} where $N$ is the observed frequency of a term in $\textbf{D}$, $E$ is its expected frequency, and $t$ and $c$ denote term and class, respectively. You can easily compute this in sklearn, unless you want the educational experience of coding it yourself $\ddot\smile$ 

When I run with the stackexchange URL, suitably modified code for the different URL, would give a vector of data. The reading I've done about google stopping screen scrapers, suggests that they only stop people who abuse it and usually will do it with a Captcha or similar. Any thoughts on a solution? 

Just to throw an extra option in, Microsoft Azure's Stream Analytics. As far as a comparison of the different technologies and their relative performance, those stats seem difficult to get, but there is a useful comparison of a few technologies in this article, though doesn't mention Azure Stream Analytics. I think the difficulty is that the architectures are so different, that is makes simple metrics hard to produce. What is needed is some more robust measurement tools that can be applied to give a few metrics for various operations. I would think that Gartner would have something to say, but the nearest I could find was their BI and Analytics quadrant, which only obliquely address stream analytics. 

This will allow you to be able to do the query of pages with the maximum number of steps, or pages most frequently linked to, etc. 

Java I'd have to disagree with the other posters on the java question. There are certain noSQL databases (like hadoop) that one needs to write mapreduce jobs in java. Now you can use HIVE to achieve much the same result. Python The python / R debate continues. Both are extensible languages, so potentially both could have the same ability to process. I only know R and my python knowledge is quite superficial. Speaking as a small business owner, you want to not have too many tools in your business otherwise there will be a general lack of depth in them, and difficulty supporting them. I think it will come down to depth of tool knowledge in the team. If the team is focused on python, then hiring another python data scientist is going to make sense as they can engage with the existing code base and historic experiment code. 

I would set up a sensor as idle, purposefully to determine the decay in the signal. Then try an ARIMA or similar model on the data collected to find out what an approximation of the decay function is. Further to this you should determine if the model still fits if they decay is put into a "stop/start" mode. Afterwards you can apply a corrective function to remove or take into account the decay on your other data. 

I'm not sure if this is the type of analysis you are after, but you mention that the visual side is restricted in STATA. A colleague wrote a blog that utilised neo4j to read web data into a graph database, and d3js to display the data graphically. I realise you don't have web data as such, but your data can be stored in a graph database, but I guess when I was asking about what types of analysis you were planning on doing, I was asking were you needing a qualitative or quantitative direction. But it seems like you are still in the process of working that out. The nice thing with neo4j is that you can pull the data into R and do any sort of analytics you want on it. 

It sounds a little like homework, your bio suggests otherwise, but geographic data is very interesting, so I thought I'd give it a bit of a comment! If you have location tracking in Google turned on, which the data can be downloaded, you can determine many things. You will need to enrich your data with extra GPS data sets of locations of schools, shops, particular buildings, places of worship, Some for examples of what can be determined ... 

Good luck! If you have any other specific questions, feel free to ask me in the comments, and I'll do my best to help! 

There are many approaches you could take here. Assuming data overlap in such a way that the classes are still distinguishable from a human expert—human performance is generally regarded as the performance ceiling in supervised classification tasks—you can approach this from the classifier side of the feature side. First, I would consider the feature representation you're using for your task. A classifier is only as good as the information you give it, and there are many ways you can adjust how you're representing your input data. For example, if you're working in text classification, using a unigram representation will typically lead to different performance as compared with a bigram representation. Similarly, there may be non-linear transformations you can apply to continuous-valued data that a classifier may not pick up on its own. You should consider the domain in which your problem exists and create a set of feature generation and feature selection cross-validation experiments using a hold-out training data set. Next I would consider what alterations you can make on the classifier end. If your data are highly skewed (i.e., one class is much more prevalent that another), it may not make sense to use an algorithm like Naïve Bayes, at least not with out some sampling approach. If your problem isn't high-dimensional, I recommend generating a graphic that will let you explore the spatial breakdown by class. There are certain overlap patterns that linear classifiers like Support Vector Machines will not be able to classify without using a kernel function, such as a Radial Basis Function. 

It's perfectly fine to have different features in your training and testing partitions. If the two groups are randomly selected from the same population, one would expect, if there are many possible features, as is frequently the case in text classification, that differences would be observed. To handle this, the standard practice in the field is to train your model on the training partition, and then evaluate on the testing partition with any new features in that grouping being discarded. If you encounter the edge case of a test observation with no features after this procedure, it is common practice to use a heuristic to classify it, bypassing the model entirely (e.g., an observation with no features is classified as neutral). 

Welcome to the site, Martin! That's a pretty broad question, so you're probably going to get a variety of answers. Here's my take. 

Seasonal decomposition doesn't make sense in this situation. You're sampling frequency needs to be greater than 1 for this to work! I know this changes your model, but just for the sake of example: 

Cloudera recently added the spark-time series library to github. According to the user docs, it definitely can fit autoregressive integrated moving average (ARIMA) models, but I see no mention of ARIMAX, which takes into account explanatory variables. Does anyone with more experience with this library know whether ARIMAX is possible in the library's current implementation? Mathematically, I understand the difference between ARIMA and ARIMAX, and I know extending it myself wouldn't be terribly difficult, but I'm right now looking for an off-the-shelf solution in spark. Can anyone recommend an alternative spark implementation? 

There are several ways to do this, but, assuming the lists are the same length and the sentences you wish to compare are in the same indices int their respective lists, you can handle this with a simple list comprehension: 

I don't know about that specific implementation, but we use the mllib k-means here at my work, to some degree of success. It is distributed and runs on Spark! 

This is a really interesting problem! Like most really interesting problems, you're unlikely to find an out-of-box solution for this, but I think the field of graph/subgraph similarity has some promise here. I'll go into more detail, but, at a high level, I think you can view players' paths during a play as a collection of five traversals through x,y space, with a vertex being any x,y point you have available (presumably there's some level of time granularity here), and an edge describing a player's movement from one point in x,y space to the next over time. It should be possible to cluster your data using a similarity metric (e.g., see Koutra et al., 2011 for a nice overview). Then, using your own domain expertise, you should be able to identify whether the clusters you've derived have some real-world meaning in basketball.