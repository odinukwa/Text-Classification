This will result in interruption of service. Update the init file to contain the commands necessary to re-create the root user. The procedure is documented here: $URL$ 

I freelance and am not full time consulting. Were I full time, I would probably consider Basecamp as others have recommended. I use a combination of spreadsheets and standard documentation procedures, which I often apply to my full time job as well. For tracking time at client sites, I have been using the EternityLite iPhone application. I usually prefer to avoid vendor lock in and use more standard approaches where appropriate. Ta-da list is a free version of Basecamp that is substantially simpler and Remember the Milk is quite popular in personal circles. I have also heard good things about Action Method. Of course, there's argument for Microsoft Project when it comes to very large projects as well. 

Check permissions, make sure ownership and permissions are correct on . This is typically . If this is not the issue, produce output of your log file. Try the following to correct permissions: 

It looks like the /dev/sdb hard drive is suffering a hardware failure. Run to check for additional kernel errors. Run the manufacture's hard drive diagnostic software against the disk to determine actual failure. 

You're attempting to upgrade the package provided by the distribution. Often, distributions will not change the version but backport security fixes, which is done to maintain a more stable environment. As major revision changes can affect the operation of software, they typically will only be done between major revisions of the distribution. If you want to run the latest PHP, you'll likely have to create your own package, locate a third party package, or compile from source making your own build standard. Based on the output provided, it's reasonable to assume you're running the latest version of PHP available for the current distribution you're running. It looks like the CentOS wiki has a walkthrough for updating to 5.2 using the development repo. 

No. The name "hardware virtualization" specifically indicates that the feature is based in hardware. If the CPU does not have the instruction set, you cannot enable it otherwise. Intel's testing the market with "software enabled" upgrades but I believe it to be cores and cache, not instruction set. 

I'd say the solution proposed is over engineered. There's a variety of technologies that would be suitable for your situation. I use LVS for load balancing and SQUID for caching, particularly in regards to Jboss. For static content, it's generally better to serve from Apache. You can still use heartbeat or pacemaker for redundancy with these technologies. The main reason I use SQUID is for rewrites but a lot of the content I deal with is dynamic. Caching is a bonus. Most of my Java applications have virtually no static content, so I often skip the mod_jk part. Point being, your requirements can drastically simplify even my proposed solution. One possible example: NAT to SQUID (ha cluster -> SQUID transparently proxies to LVS VIP -> LVS VIP to Apache cluster -> mod_jk to Jboss 

You're going to have a hard time finding a distribution dedicated exclusively to e-Mail, as there's not much point when you can simply install the software on top of any distribution. QMAIL Toaster is a software collection meeting your specifications. 

The error log is located in the data directory, which appears to be in your case. Verify in your cnf file. 

Yes, it is possible. Realistically, you need a new snapshot of DB2 to build DB3. At that point, if you have a good position, you can either use than for DB4 or take another snapshot of DB3. Don't forget to set unique for each server. High Performance MySQL is a great book for referencing more advanced MySQL administration. 

I would recommend writing a script to run with the flag to backup schema. If you want to show the schema, you can run . 

LVS has different schedulers, which specify exactly how connections are handled. It's also very lightweight, I run it on small commodity hardware handling the traffic for a high volume Web site. 

Nagios has host and service dependency configuration files. I've linked the documentation below. You can cross-link the checks and each will only alert if the other is down. Nagios Dependencies 

It is better to dedicate role to purpose, as it reduces complexity and security risk. You would likely benefit from a minimum of two load balancers, two database servers, and two Web servers. Be aware of additional points of failure, as high availability will not stop there. Your network will likely be the next obvious single point of failure. It also enables standard builds, scaling, and transferring roles with less complication. Nevertheless, these features are not unique to separating server roles. 

I am assuming Apache, as your Web server is not identified. Your Web server would have to be configured to only be bound to the local interface, which could potentially affect any other applications running. Another solution would be to run an additional instance. You can also use an ACL in Apache. If your server is not on a segregated network, you will not be able to fully eliminate the risk. Apache Documentation 

You're specifying the name of the server, literally. Pick the one that you want your server to be, it's up to you. It will be the default name for any services you run on the server unless you specify otherwise. For example, this is often going to be the default host to remotely connect to the server to such as with SSH. Nevertheless, many other protocols will be specified to utilize hostnames outside of your server's hostname. For example, rarely will Apache's primary role be to serve files on the server's default hostname. With Debian, is read by the init script and will reflect any changes upon reboot. To change dynamically without reboot, you can also run the command . 

increases the amount of file descriptors that mysqld requires. This is a system limitation, which can also be limited in MySQL with the configuration option. To see the current system wide limitation in Linux you can run . This can also be limited on a per-user basis using . If file descriptors are not the issue and you configured the setting correctly, MySQL could be using a different my.cnf. Look at the processlist, where the could specify the cnf. Also, if unspecified the is often where the my.cnf can live. You could also search the filesystem for additional my.cnf files. 

Oddly enough, this may not be documented in MySQL's documentation. I have been using this configuration option for a long time and verified its performance again after not being able to find official reference material. 

You could categorize your data according to importance and work with the business to create an appropriate SLA. Typical things to focus on are time to restore and how stale the data gets. Weight is given between full and incremental, typically weekly and daily accordingly. If your backup SLA is developed properly, the technical options should be obvious. LTO-4 can store 800GB. While not always ideal, plenty of people use tape. You are going to find you encounter logistical issues regardless. Often, compromises are made between importance of the data and cost. The more data, the more cost. Hard drives can be appropriate but due to them being mechanical I wouldn't recommend using them for rotation. For off-site storage, the primary choices are between copying over a network to a physically separate location or a portable medium that's less prone to failure. (Tape) Edit 

Not on a technical level. There may be a business advantage depending upon the incentives offered by the registrar's hosting. 

You cannot do a direct data copy between MySQL 4.1 and 5.1. This is generally a bad idea regardless and the upgrade procedure should be followed within the MySQL documentation. From the upgrade documentation: 

Yes. It's stored in ".config" in the top level of the source directory. Additionally, if using distribution kernel, some distributions such as RedHat store it in /boot/config-$(uname -r). (kernel version) [1] Finally, if compiled in the kernel you're running, it's available in /proc/config.gz. I forget what version introduced this option. [1] These options: 

The is adding it to the chain named . Unless that chain exists, I would not use it. You can see your existing rules with: 

If you can avoid cascading switches, I'd strongly encourage it. The downsides are easily exampled with two people on the same cascaded switch copying a large file to a fileserver. Having a single user under normal usage being able to cause usage issues for multiple other users is obviously bad. Implications are even worse with servers. 

It's well advised to set the sticky bit on directories like that. Then, only the owner of the directory and owner of the file may remove content from the directory. 

Ultra Monkey was essentially packaged ipvs. It hasn't been updated in a while. I suspect partially because ipvs was merged into the mainline kernel. You can still use ipvs, ipvsadm, ldirectord and all that fun stuff for Layer 4 switching in Linux. LVS is my preferred solution but there are definitely alternative architectures depenending on your needs. These include HAProxy, Nginx, and Pound among others. Nginx is quite popular as well for highly available Web architectures. 

DNS by design does not enable having an authoritative copy of all zones, as it utilizes a hierarchical naming system. The root servers are authoritative for identifying the server responsible for the Top Level Domain (TLD) in question. For example, resolving will first query a root server to identify the authoritative nameserver for . The nameserver will identify the authoritative nameserver for , which will then return the record for . You cannot download a copy of all zones. However, you can run a local caching nameserver. The caching nameserver will provide a local copy of all records resolved, which expire using the Time To Live (TTL) specified for the record. Please keep in mind that my explanation is a simplistic description of the DNS protocol, which can be explored in detail by reading definitions in the Request For Comments. While NXDOMAIN hijacking can be avoided by running a local cache, keep in mind that all DNS resolution traffic will still be transmitted via your Internet connection unencrypted. Your ISP could potentially monitor that traffic and still see the communication. The contracts you have with your ISP as well as your local laws are going to be your definitive means for establishing how your communications are treated. Your ISP's contracts will include the Terms of Service, Privacy Policies and any additional contracts that you may have with your ISP. Using encrypted protocols is one of the best methods for insuring your data against eavesdropping during transit. However, even that has no guarantee of anonymity. There are additional protocols out there such as Tor and Freenet, which attempt to introduce anonymity to the Internet, as it was never designed to be truly anonymous. 

Run a similar version of the earlier script on the source server using cron, where it archives and encrypts all the files to a directory. At an appropriate interval following that, have a cron on the target server rsync over SSH that location. You could scp but rsync has many advantages. Otherwise, I'd scrap your requirements and run an on-demand hybrid solution from the target using SSH pipes. 

What you are describing is the Sender Policy Framework (SPF). You can use this tool to walk you through configuring SPF records. You will want to identify all servers that are authorized to send mail on behalf of your domain. 

You are going to have a difficult time with this, as Wordpress has redirects all over the place with many of them being in .htaccess files. If you specify the "Wordpress URL" in the settings as well as any reference to the URL throughout the configuration as https, it will probably work. I doubt it is your ngix configuration at this point but it would be easy enough to test with a different VirtualHost not running Wordpress. 

The previous list is far from comprehensive. Depending on the type of IT shop, there are varieties of differing roles within that. Roles can involve responsibilities ranging from architect to support. Some of these roles are not available in all shops and some roles are very different between shops. External IT services can be many things. Consulting services and contract or staff augmentation services often overlap, which are often contact to hire. 1099 and corp. to corp. consulting are very different, which are often contract based and better resemble freelance consulting. What do you want to do? I do not want to do many of these things, as they are entirely outside of my career focus and not things I enjoy. One consulting firm can may large interesting projects using the technologies you enjoy, where another may churn out support contracts as the primary focus. Most fall somewhere in between. My favorite type of internal IT shop is where one can have substantial involvement in the direction of technology, which seems to be more common in technology-focused companies. These shops often involve higher-level architecture as well. The contrast would be an internal IT shop focused on providing support to the intranet and internal end-users, which can have a substantially smaller budget and less responsibility as opposed to engineer roles. Ultimately, IT is a big space. If you feel that you are stuck in a support role and not interested in business or management, chances are you can find a highly technical role that does not involve support. These choices are not necessarily distinct between consulting and in-house IT departments. 

mod_env is used to set variables that can be passed to CGI and SSI pages. What you want to do does not necessarily have any native support that I am aware of. One solution would be to use the third party module called mod_macro, which would allow a more dynamically generated httpd.conf but is not exactly what you are seeking. If you have mod_perl installed, you can embed perl code within the httpd.conf using and , which would allow a more dynamic httpd.conf. mod_perl executes it on startup. If you are not already running mod_perl, the overhead introduced would not be worthwhile. A third option would be to dynamically generate your configuration using a set of scripts, which would be outside of Apache all together. Perl would be a very useful language for this particular application. 

It seems you already have a file sharing solution in place, which could be replicated. All modern operating systems include permissions management with groups. This is a simple task that any entry level IT person should be able to complete without detailed assistance. Edit 1 If you don't have any authentication or access control in place, how can you realistically expect to prevent them from modifying the files in the first place? Obscuring the fact that they can modify the files does not change the situation. If you simply want to segregate the storage locations, you can create another place for them to view the files. Edit 2 Ultimately, I agree entirely with what Kyle recommended regarding providing a high level view to your IT person for his technical recommendation. He's paid to provide technical solutions within the needs of the business. Without knowing what technologies you currently use it's difficult to provide a recommendation that will be a good fit your your environment. Generally speaking, sharing files between internal users on a LAN is often best accomplished with CIFS/SMB. (The protocol used by SAMBA and Windows file sharing.) This can be implemented easily between UNIXes and Windows, while providing a less intrusive user experience. I would create a generic "Design" (or whatever department) share, which would be readonly to the entire company and write-only to design. Then there would be the design share only to be used by the design department, which would be read write. Exact roles and access would be defined by how the business operates. Additional controls would have to be implemented for data that has to be treated more securely and this is not an end-all solution but should fit your goals.