Your Statement I is not true. You can have two non-isomorphic graphs G1, G2 with T(G1), T(G2) isomorphic. Basically, the problem is that there is no reason why isomorphisms between T(G) graphs should map vertex-cycles into vertex-cycles. 

Transform G into a weighted flow network N: connect the source s to all nodes in A by arcs of capacity n/2 and cost 0, connect all nodes in B to the sink t by arcs of capacity n/2 and cost 0, and connect each A-node u to each B-node v by an arc of capacity 1, with cost -1 if (u,v) is in G and 1 otherwise. The maximum flow in N is n^2/2. Its cost can be written as -|E|+d, for some d >= 0. This d is the minimum edit cost of transforming G into a balanced n/2-regular bipartite graph with partition A,B. 

What you seem to be looking for is called the minimum common string partition, and it has been well studied. In particular, it's known to be NP-hard. There is also a closely related concept of edit distance with block moves, which would capture the extension you mention, where substitutions are allowed. 

Perhaps I don't understand the question, but the way I interpret it, Colin's answer seems correct. It is enough to find the minimum possible k for which such partition exists, say k_0. (This is a partition with maximum number of K_21's.) There is a partition for any k between k_0 and 3n/2, because splitting K_21 into two K_11's increases the sum of cardinalities by 1. The K_21's in your partition induce a matching in the line graph of G, so this k_0 corresponds to the maximum matching in the line graph, and this matching can be found in polynomial time. 

Backpropagation is computationally expensive. Has any research been done on a partially greedy implementation of it? Intuition: at the beginning of training, big rough learning steps can be taken, so do we really need to tune each individual weight? If we were to plot time series of every parameter as its value changes over training time, might we get correlations between certain parameters, particularly at the beginning of training? For example, since the gradient propagates backwards, we might get correlations between the parameters of neurons behind one another. Such correlations could suggest that it's a waste of time to compute partial derivatives for all parameters, i.e we can use the partial derivative for a neuron in front to approximate the partial derivatives for the neurons behind it? Has anyone seen any articles exploring anything like this? 

Unless I'm mistaken, deep neural networks are good for learning functions that are nonlinear in the input. In such cases, the input set is linearly inseparable, so the optimisation problem that results from the approximation problem is not convex, so it cannot be globally optimised with local optimization. Support Vector Machines (try to) get around this by choosing features such that the projection of the input space into the feature space is linearly separable, and we have a convex optimisation problem once again. I don't suppose that a deep neural network always learns features that make the input set linearly separable in feature space. So what's the point in it learning features? 

Your statement is a little ambiguous: first you write that "...such that no edge is incident between the nodes in $R$", but the next paragraph implies that there are also no edges between vertices in $A$. I'll also assume that the stars are disjoint, and that you count all edges (including those initially present in the stars). Let's also assume there are at least two stars, and at least one of them has degree $\ge 2$. In that case, you cannot beat the $2N-4$ bound ($N$ = number of all vertices). Consider a slightly different scenario: start with any set of $N$ vertices, some red some black, at least two of each kind. At each step add arbitrarily an edge between a red and a black vertex, as long as it does not create intersections or duplicate edges. I claim that when you get stuck, all cycles have length $4$. Your scenario is a special case of this process where you start by first creating stars and then later adding the remaining edges. If all cycles have length $4$, the $2N-4$ bound follows. More generally, it shows that no matter what bipartite graph you start from, you can always complete it to a quadrilaterated (a word I made it up) graph. Now, let's show the claim. In this process, all paths will have alternating black and red vertices and each cycle will have length at least $4$. If the graph is not connected, you can connect any red vertex on the outer face of one component with a black vertex on the other face of another component. So we can assume the graph is already connected. Suppose you have a face $F$ of length $6$ or more. $F$ must have at least three black vertices (some possibly equal). If some vertex $x$ is repeated on $F$, take two clockwise consecutive appearances of $x$, say $x-a-...-x-b...$. $F$ must contain a black vertex $z\neq x$, so, depending on the location of $z$, we could connect either $a$ or $b$ to $z$ inside $F$ without duplicating edges. If no vertex is repeated, pick a clockwise section $x-a-y-b-z$ of $F$, where $x,y,z$ are black and $a,b$ are red. If $x$ is connected to $b$ then $a$ cannot be connected to $z$ (by planarity), so we can add one of the edges $(x,b)$, $(a,z)$ inside $F$. 

I'm reading CLRS's (Cormen et.a al) Introduction to Algorithm, and arrived at the maximum flow section. It shows that Edmonds-Karp algorithm runs in $O(E^2V)$ time by showing that: 1) If we let $\delta_f(s, u)$ be the shortest distance from source $s$ to vertex $u$ in the residual graph after flow $f$ is picked, then CLRS shows that $\delta_f(s, u)$ does not decrease as $f$ gets augmented, for a fixed $u$. 2) Let $c_f(e)$ be the capacity of edge $e$, and $c_f(p)$ the capacity of a path $p$ (the minimum of all $c_f$ on the edges in the path $p$) in the residual graph after flow $f$. If we say that an edge $(u, v)$ in a residual network $G_f$ is critical on an augmenting path $p$ if $c_f(p) = c_f(u, v)$, then CLRS shows that each edge can't become critical more than $|V|/2$ times, and therefore, there can be at most $|E||V|$ augmenting flows, if we sum up this bound over all of the potential $2|E|$ edges in residual graphs. In particular, CLRS's proof of the 2nd point goes as follows: If $(u, v)$ becomes critical at $f$, then $\delta_f(s, v) = \delta_f(s, u) + 1$, because we have picked the shortest path from $s$ to $t$. The next time it becomes critical, there must have been some flow $f'$ that ran through $(v, u)$. Then $\delta_{f'}(s, u) = \delta_{f'}(s, v) + 1$. This must mean $\delta_{f'}(s, u) \ge \delta_f(s, u) + 2$. Hence every time $(u, v)$ becomes critical after the 1st, $\delta_f(s, u)$ must have increased by 2. As this minimal distance can't exceed $|V|$, each edge can become critical at most $|V|/2$ times. Therefore, there can at most be $O(EV)$ augmenting paths and hence the number of augmentations. So my comment now comes in as a possible improvement over this proof. Why can't we say the following: If we augment the edge $(u, v)$ $k$ times,then $\delta_f(s, u)$ increases at least by $2(k-1)$. Now instead of looking at edges, we look at vertices and their degrees. We just need to guarantee the edges going out from any particular vertex $v$ become critical infrequently enough so that $\delta(v)$ doesn't exceed $|V|$ (from now on for convenience I will shorten $\delta_f(s, v)$ to $\delta(v)$). I claim that for any vertex $v$, $\sum_{(v,u)\in E \text{ or } (u, v)\in E} \text{number of times $(v, u)$ becomes critical}$ is at most $indegree(v)+outdegree(v)+|V|/2$. (we are looking at "reverse" edges here too because they may appear in the residual graph) Indeed, the first time any edge becomes critical is "free": it doesn't bump up $\delta(v)$. But for every time after that $\delta(v)$ goes up by 2. So we can get every edge going out from $v$ to become critical once (this is the $indegree(v)+outdegree(v)$ term) and then all other times are bounded by $|V|/2$ for the same reason CLRS writes. Now if we sum up this bound for all vertices, we get $$\sum_v indegree(v)+outdegree(v)+|V|/2 = 2|E| + |V|^2/2 = O(V^2)$$ This bound is better than the $O(EV)$ bound given by CLRS. Combined with the $O(E+V)=O(E)$ bound for BFS, we find Edmonds-Karp is actually $O(EV^2)$ instead of $O(E^2V)$. I'm suspicious that I'm missing something, since every where I looked, the bound given for Edmonds-Karp is $O(E^2V)$. So please nitpick away and see where I reasoned incorrectly. 

Dijkstra's algo (for finding single-source shortest path) assumes that once a vertex has been chosen for expansion (aka exploration), its shortest path has been found. This can only be true if there are no negative-weight cycles reachable from the source. Therefore, I was thinking that in order to detect a negative-weight cycle, we could run Dijkstra's algorithm a 2nd time, but instead of initialising the vertex distances to -infinity, we use the ones found by the 1st iteration. If at any point during this 2nd iteration, there is a distance update, the assumption above is violated, therefore there is a negative weight cycle. There must be a mistake in my reasoning, because if this works, it would have same time complexity as Dijkstra's algo, which would be better than the Bellman-Ford algorithm, which is the textbook algorithm for solving the single-source shortest-path in potential presence of negative weight cycles. Is my mistake that a 2nd iteration may lead to no distance updates even in the presence of a negative weight cycle? If so, could someone provide such an example? 

This 2013 paper integrates deep learning models with structured hierarchical Bayesian models, for the net to learn novel concepts from very few training examples. It shows encouraging results. Another approach is to import a general-purpose top-performing neural net, and replace the top fully connected layers with initialised ones (including an output layer that corresponds to the number of classes in your specific task). OverFeat is an example, for image classification - it was trained on ImageNet.