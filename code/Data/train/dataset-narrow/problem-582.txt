Well the problem is that your CASE statement is ambigious. If the value is @p or @p1 both case when expressions will be true. In that case just the first one hit (returns 1) is going to be evaluated. There are two alternative ways I could think of: 

I admit that this solution is not pretty and presumably it won't work if another dynamic sql comes around with another resultset...however it works for me now. Martin 

However now I put it like (where 1 is id of order type) So this is bad. Of course I can write custom validation, like grab type from database, compare it's name to photo etc. Plus, everywhere where I need to check orders type, I need to do a join. What I thought is: use , etc as a PK. This way: 

instead of retrieving from dictionary table and comparing it to So all the time I start a new project I ask myself the same question. For example, right now I have roles. I mean not "roles-permissions" system, where we can add a lot of roles with different permissions. I just have 2 big roles, with different logic. So I can make a separate table with 2 values, or varchar and compare string values. I'm talking about MySQL. Sorry for silly question, not an expert in DBA. I suspect that using dictionary table is also better because it'd be faster to search statuses by int, instead of strings, but not sure. 

So nothing of this really worked. Could you think of an alternative how to keep the trigger doing its job but also have the data collector work properly? Resources Here's the original sourcecode of the database level trigger: 

The problem I have got an issue setting up Management Data Warehouse on SQL Server 2017 CU 5. The job "collection_set_1_noncached_collect_and_upload" keeps failing. It is related to the "Disk Usage" collection set. Error Messages are the following (I highlighted the part which is most relevant IMHO): 

On the other hand I'm not a DBA expert and I'm not sure if it's a good practice. I have quite a lot of order characteristics, all of them are in lookup tables. For some lookup tables I can't use this method, as they have up to 10 values with long names (like ) I can't use enums (as possible values can change + I have some associated information for every value). So is this a good practice, or should I stick to id's? I don't care about DB size for now. 

As you described we'd like to return X first, then possibly Y (if no X present) and at last Z and only one row per student if the applicable subject code. XYZ is easy because it is alphabetical but let's introduce some additional complexity guessing that there could be also an A which we would like to return if there is no X. Here is a SQL Fiddle with my example I would use the following query to achieve this: 

However, depending on the order type, server needs to receive additional information. If the type is server needs number of photos. If the type is server needs number of minutes. I use Laravel for backend development and I found it really inconvenient to work with . In general situation I would write validation rule as: 

Use varchar for status column Use enum for status column Use separate dictionary table, which has id and name, and in orders table keep status_id as a foreign key. 

Then I join it to my table and add a rownumber based on the priority: So here's the output of my subselect (or derived query) named "res": 

By the way: Having a time as datatype varchar(10) sounds a little bit odd. Datetime would seem to be more fitting. Have a look here for advice. 

as fas as I understand you would like to control the order of execution if x is not present search for y and so on. Therefore a simple IN clause or the case expressions probably won't work if you've got entries for x as well as for y and z....by result returning multiple rows. The best way I can think of would be to introduce a priority factor within a cte and use this to return only the relevant row. Here is an example. For simplicity I use a table with the following structure 

Type has four values (it is possible that they will change a bit) For each type there is associated information, such as budget factor (used for internal business logic) 

Imagine next situation. I have an order entity which has type. For now type has such values as "photo", "video", "panorama", "supervideo" :) 

I remember I already found an article illustrating the best approach, but I lost the link and couldn't find it :( So, very common situation. Imagine we have orders table, and an order has a status. Three choices: 

Eventually I have found a solution to my problem. As on a StackOverflow question described your can hack you way around the problem using . So I added these two lines to my code (within the IF clause and before doing other things) and got things to work: 

First of all thank you guys for helping me to get on track with your comments. I have now worked through an example and have a better understanding what's going on. The problem arises with moving LOB-Data (such as VARCHAR(MAX), XML and so on) to another filegroup. When you rebuild a clustered index on another filegroup the LOB-Data stays at it's former place (set by the command in the CREATE TABLE statement). One classic way to move the LOB-Data is to create a second table with the same structure in the new filegroup, copy data over, drop the old table and rename the new one. However this brings in all sorts of possible issues like lost data, invalidated data (because of missing check constraints) and error handling is quite tough. I have done this for one table in the past but IMHO it doesn't scale well and consider the nightmare of having to transfer 100 tables and you got errors for table 15, 33, 88 and 99 to fix. Therefore I use a well-known trick regarding partitioning: As described by Kimberly Tripp LOB-Data does move to the new filegroup when you put partitioning on it. As I do not plan to use partitioning in the long run but just as a helper for moving that LOBs, the partition scheme is quite dull (throwing all partitions into one filegroup): I don't even care, which partition the data is on as I just want to get them moved. Actually this technique and the implementation is not invented by myself...I use a formidable script by Mark White. My mistake was to not fully understand what this script does and what the implications are....which I have now: For LOB-Data it is necessary to rebuild (or recreate) the table (mostly the clustered index) twice: first with putting partitioning on it and second with removing the partitioning. Whether you use or not this results in having to provide the space of the original data TWICE: if your original table has 100MB, you need to provide 200MB for the operation to succeed. At the beginning I was quite puzzled, ending up with my new data files which had a lot of free space after the operation was finished. Now I accepted that I can't cheat around avoiding the free space. However I could avoid the necessity to shrink files afterwards. Therefore my solution is to do the first rebuild on a temporary filegroup and the second rebuild (removing partioning) on the destination filegroup. The temporary filegroup can be removed afterwards (if hopefully I don't hit the error message "The filegroup cannot be removed" (have a look at my question here) anymore. Thanks for reading and your help Martin Here is a repro script for my problem which includes the solution I have come up for it: 

I can change number of types and their additional information like budget factor I can directly compare the value of type in orders table. Like 

So I made a lookup table with following columns: id, name, title, budget_factor Therefore, orders table has type_id which is foreign key to id of types table. Everything's good. However, when I started developing I came across next case. 

Which is the best approach? I know that enum is evil, because it's hard to modify database if we need to add a new status. Probably dictionary table is the best: we can see all possible values and we can easily add a new one. However, when using varchar, it's way easier to work with it, like: 

The procedure "AUDIT_TO_OTHER_FG" is a database level trigger. Its purpose is to put audit tables (with history data) into another filegroup. Our Java Application running on top of the database is using Hibernate and doesn't bother specifying filegroups. However all of these audit tables follow a certain naming convention. Thus the trigger fires at a CREATE_TABLE event, rolls back the table creation and creates the table again on a different filegroup. Maybe this is not the most elegant version to put the tables onto a different than the default filegroup...however it has worked fine in the past and has never been a problem until now. I had Management Data Warehouse data collectors set up before for that environment as it was running on SQL Server 2008. There haven't been any problem regarding these triggers in that version. Recently we moved to SQL Server 2017 and now I am experiencing these issues. I dropped the trigger temporarily and the data collector worked fine. So somehow it appears to be interfering with the actions of the data collector and the problem is the dynamic SQL used. However I do not get why this causes a problem as the data collector seems not to create any table in my user databases and the trigger doesn't fire while the data collector is run. Workarounds tried I have read a bit into "WITH RESULT SETS" and tried to change my trigger as follows: