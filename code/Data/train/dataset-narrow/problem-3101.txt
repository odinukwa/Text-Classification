should not be considered as black boxes at all. Zeiler et all in this amazing paper have discussed the development of better models is reduced to trial and error if you don't have understanding of what is done inside these nets. This paper tries to visualize the feature maps in . Capability to Handle Different Transformations to Generalize use layers not only to reduce the number of parameters but also to have the capability to be insensitive to the exact position of each feature. Also the use of them enables the layers to learn different features which means first layers learn simple low level features like edges or arcs, and deeper layers learn more complicated features like eyes or eyebrows. e.g. tries to investigate whether a special feature exists in a special region or not. The idea of layers is so useful but it is just capable to handle transition among other transformations. Although filters in different layers try to find different patterns, e.g. a rotated face is learned using different layers than a usual face, by there own do not have any layer to handle other transformations. To illustrate this suppose that you want to learn simple faces without any rotation with a minimal net. In this case your model may do that perfectly. suppose that you are asked to learn all kind of faces with arbitrary face rotation. In this case your model has to be much more bigger than the previous learned net. The reason is that there have to be filters to learn these rotations in the input. Unfortunately these are not all transformations. Your input may also be distorted too. These cases made Max Jaderberg et all angry. They composed this paper to deal with these problems in order to settle down our anger as theirs. Convolutional Neural Networks Do Work Finally after referring to these points, they work because they try to find patterns in the input data. They stack them to make abstract concepts by there convolution layers. They try to find out whether the input data has each of these concepts or not in there dense layers to figure out which class the input data belongs to. I add some links which are helpful: 

Based on here, use in order to split your data into train and test. Train your model on train-split and use the method to see the performance on the test data. As an example take a look at the following code which splits the data to two separate groups. 

These are for dealing with over-fitting problem. First of all you have to learn the training data to solve high bias problem. The latter is more common in usual tasks. They are just fine for imbalanced data set but consider the point that first you have to deal with high bias problem, learning the data, then deal with high variance problem, avoiding over-fitting. 

The table that you are referring to is doing OR operation. whenever you have just a neuron in your net you are able to have one line to separate your data. but for xor data you have to have two line separators. it is common for solving this problem to have two neurons in the first layers which both do OR operation and in the second layer to have one neuron to do and operation. if you stack these two layers, you will get the result. I suggest you to see the shape of this to see the nonlinear decision boundary. Also the second table is doing AND operation. 

We use cost function to have the amount of error for a specified set of weights. We should find the weights which minimize the cost function. The used approach for minimizing the cost function are based on gradients. Means that you should move toward directions which minimize the error. To do so, the cost function have to have derivatives. Absolute function does not have derivatives in some places. Quadratic functions like square, has derivative. Although there are other reasons that we have become to this squared function, the reason it's not absolute was what I referred to. 

It depends on your task and the amount of data you have. If you have so much data but you can not find similar tasks to have appropriate architecture you should stack convolution and dense layers yourself. But if you have appropriate amount of data and there exist good architectures then you have to decide what you want and how is your situation. Suppose that you want to have recognition task, there are so many architectures that are applied to data-set. You can use transfer learning but there is a point here. Suppose that you want to fine tune . This is a very large network and is capable for recognizing about a thousand distinct classes. If you have a recognition task with 5 classes and you have an agent that should be online, this is not logical to have such a big network. You may have similar performance by stacking a few layers and get better time complexity. If you don't have so much data, freezing the layers and applying transfer learning to the last layer maybe a typical solution. 

Take a look at professor Andrew Ng's course about deep learning. Its homework is written in and . It is a very good point to begin. 

It is the result of taking the multivariable derivative, gradient. Yes the goal of them is to find the local minima, but they have a different approach. Gradient descent tries to go downhill whilst normal equation tries to find the location by finding where the derivative is zero for all features. I didn't figure out the last question to help you address it. 

It tries to find whether the predicted values are the same as the real ones. You can run y_pred_cls to see the probability of each class for your desired input. 

The purpose of writers of net was to make a network which was just deep enough to perform well on data-set. have lots of hyper parameters which setting them is not based on well behaved math stuff. I mean there is no proof to show which hyper parameter is better than the others. They are found based on experience. They are gained practically. Writers of this paper tried to show that if you use same hyper parameters for convolution layers and just make the network deep and deeper, it was much deeper than , you will gain good performance without caring about different set of hyper parameters. For understanding what do, there is already an answer here which may help you, it contains the interpretation of different layers in . They have reached to this setting of convolution layers and architecture by experience and there idea was too use just a deep net without complicated hyper parameters.