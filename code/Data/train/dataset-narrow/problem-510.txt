I have a bunch of *.frm, *.myd and *.myi files, which are a MySQL database store. I need to extract the data kept in these files, but I do not have a MySQL database engine at my disposal. Is there a tool or library, which can do this? Please, do not suggest installing the database engine (be it MySQL or MariaDB or other MySQL forks). My question is specific about extracting the data without the full blown database engine. In essence, I am looking for an embedded database engine, which can be distributed with my code and which demands no installation. I am not looking into doing complex queries, I just need to fetch all the data from one or two particular tables. Thanks. EDIT I am aware of $URL$ but I do not understand how to download it. What is the licensing for commercial usage? Should I download and install the full blown database to get hold on the libmysqld files (headers, lib + dll)? Even though I do not need the full database? In short, it creates more questions than answers. I am still looking for an advice. 

I know how to filter the list of tables - using the filter icon. However, this icon is disabled when the Databases node is selected. Still, I am wondering whether it is possible. 

Stupid, right? We also came to think so. So, I had to write a tool, which given an old export file converts it to some database independent binary format. The problem is how to unit test it? At the very least, I need 4 databases with identical data, which brings me to my question: 

Note, that I do not need a super duper tool, which knows to do all the pairwise conversions. Until now I used $URL$ to convert SQLite to Sql Server CE, but besides having had to fix several bugs in their code, there is a deeper issue with their conversion - all the integral types become 64 bits long after the conversion, which is not good for me. Thanks. 

All it does is fire a simple SQL query to a local Sql Server 2012 instance using async I/O API. Notice the Max Pool Size is set to 1000. This function is then called by the following code: 

There is also an export feature, which allows to export some data and here is the problem - it actually exports the database itself: 

And I am calling this code with various combinations of and to explore the behavior of the async database IO. Please, find below example outputs: 

At first I am starting 100 async IOs, each taking 16 seconds. And it works great, so I increase the count to 200, 300, ..., 700 - boom! A failure, which I have never encountered before. Now, I know the formal cause - the default is exactly 15 seconds. Indeed, increasing the connection timeout (in the connection string) or firing 700 IOs for 15 seconds, instead of 16 - works. But something bad happens - the overall time jumps by a factor of two. It is as if the database server refuses to accept that many concurrent IOs (the exact figure is between 600 and 700), but the client side does not know about it and attempts to open all of them. Anyway, through error and trial (using binary search) I have found that limit to be 648. My question is - where does this number come from? How to change it? The max user connections is 32768, but that is not the case in reality. Is this because this is a Development license of an otherwise Enterprise edition of Sql Server 2012? 

Materialized Views - A Different Approach to Sorting Joined Tables You alluded to Materialized Views with your question referring to using triggers. MySQL has no built in functionality to create a Materialized View but you do have the tools needed. By using triggers to spread the load you can maintain the Materialized View up to the moment. The Materialized View is actually a table which is populated through procedural code to build or rebuild the Materialized View and maintained by triggers to keep the data up-to-date. Since you are building a table which will have an index, then the Materialized View when queried can use the fastest sort method: Use index-based access method that produces ordered output Since uses triggers to maintain a Materialized View, you will also need a process, script, or stored procedure to build the initial Materialized View. But that is obviously too heavy a process to run after each update to the base tables where you manage the data. That is where the triggers come into play to keep the data up-to-date as changes are made. This way each , , and will propagate their changes, using your triggers, to the Materialized View. The FROMDUAL organization at $URL$ has sample code for maintaining a Materialized View. So, rather than write my own samples I will point you to their samples: $URL$ Example 1: Building a Materialized View 

The side-effect of creating a Custom Connection is that you will need to reestablish the Default Connection: 

Whenever you create or drop a you are changing the definition of your database landscape. So, yes I would consider that risky unless you have tight controls on when those steps can run. If a connection resets the it is changing that synonym for the Server and database not for the connection. This means that a set of "complex work" running in another process could wind up switching from your remote data (for example) to the local data without any warning. That would leave a mess to clean up. View this like you would view dropping and recreating tables in a running system. It may often work without anyone knowing, but it can indeed cause you problems. Of course, if another connection tries to define a that already exists it will raise an error such as: "There is already an object named 'SY_SUBJECTS' in the database." This will save you from switching context (but you must deal with the error) until after the synonym is dropped. (There is no function, which would apparently be much more dangerous.) Therefore if you try to change the without dropping it first, it will fail. If a code path runs it will succeed once it can acquire the needed locks. From MSDN: "References to synonyms are not schema-bound; therefore, you can drop a synonym at any time." So, the will not stop a running transaction, but will wait for the transaction to finish. 

Then you can query the Excel file using the 4-part Linked Server syntax (the DB name and schema part are shorthanded here with ...): SELECT * FROM [YourLinkedServerName]...[Sheet1$] 

The ClusterName in the registry is okay and product discovery runs successfully. Not seeing any errors in Event Viewer. Here is the stack dump from Detail.txt 

Having generalized security across all servers / applications would be a bad approach. With your suggestion, once you let John Doe from Customer Service have read access the Customer database (which he needs in order to accomplish his job), you effectively give him read access to job applicants in an HR database. The HR database would contain Personally Identifiable Information (PII). Since John Doe is not apart of HR, he shouldn't have the permissions to access this data. Allowing him to read out of the HR database would cause compliance issues and could result in major fines when those auditors bust through the door! John Doe would also be able to read salaries out of the Payroll database, and ohh boy, once that info goes public you're bound to have some tension in the office; employees will question why the new guy is being paid more than them, etc. Now that I've beat that horse, here is an article I ran across recently discussing how to setup AD groups with specific levels of access to specific databases. This goes into a little bit more detail than you may need, such as creating scripts to create the AD groups, but should be a helpful start. $URL$ 

You'll need to use a more recent driver since the excel file was created in Excel 2016. Install the following driver on the machine running the SQL Server Database Engine: Microsoft Access Database Engine 2016 Redistributable (64-bit) $URL$ After the driver is installed, open up SSMS and navigate to: Linked Servers -> Providers -> right-click Microsoft.ACE.OLEDB.16.0: Enable "Allow In-Process" From here, you can use the Microsoft.ACE.OLEDB.16.0 in your OPENROWSET command. I prefer to setup linked servers to these files, as follows: 

What sticks out to me here is the failure of grabbing the resource group "Available Storage" using WMI; so I thought I would play around with writing some powershell to collect resource groups using get-wmiobject to see if I could track down any possible errors there. When I run the following snippet 

I have a 2 node failover cluster that I'm trying to patch. The initial patch level of the nodes was SQL Server 2016 SP1-CU1 (OS: Windows Server 2016 Standard) and I'm trying to upgrade to SQL Server 2016 SP1-CU3. I started by patching the inactive node (Node 2) and I was able to install CU3 without any issues. After performing a failover to Node 2 and determining that the patch was successful, I tried patching the newly inactive node (Node 1). The installer fails a few seconds after hitting "Update". Here are the Summary results: 

You have received good advice for keeping the index as narrow as possible, both for space use and for controlling the amount of I/O. But you have maneuvering room in your decisions. If you create an index on and is the clustered index, then your index on will include the clustered index as well. Depending on the distribution of your it could be that the index could be better used alone in some contexts. This could be when you are searching some data set where the SchoolID is the important join to some data in another table. This other table will likely have its own clustered index. Disclaimer: As long as we are only talking about 2 small columns, 1 of which is the cluster key, the impact will be pretty much zero (0). Thinking Ahead: But do now think about the future and how you want to be designing more complex indexes to support a more complex system. If you assume that you will eventually be creating a larger database, with many tables, and many indexes, you should begin thinking about these issues now, so that you are developing a strategy that will stand up well for the future. For What it is Worth: I have seem code that tried to include pretty much everything in the index. (I am sure that you are not planning to do that.) But the big fat index that was generated was, let us say, not optimal. 

To get your DATE formatted in the way you want it, you have to insert the '/' delimiters then use the STYLE 3 in converting from the string to the DATE. (I am sure that there are other workarounds and conversion styles that would work as well.) Likewise when displaying the DATE as you desire, you need to use STYLE 3 

Either of these models look reasonable. The intersection table is not strictly needed since you can get the information through either model. The question is: How do you intend to use the NoticeMasterFile information? Perhaps this change would support some future plans that you have. Of course, the problem is that looking at a data model does not really reveal the thinking behind the model. Nonetheless, separating the blob seems like a very good idea to me. If nothing else it protects you when someone runs on the table. As an aside and for what it is worth, if you use sp_tableoption to set the blobs will leave a 16-byte key that references the blob pages. The blob will still be considered part of the table. The separation would allow you to query the other columns of without the processing overhead of the blob data. (This is the default setting.) Questions: How many documents do you expect to accumulate in your database? How large is the average size? Et cetera? If your document load will be relatively light, then this should be fine. But remember that every document is included in the backups that you take. If you think it likely that you will eventually move into the Terabyte size range, you should do some further planning for how to store the documents. Perhaps: