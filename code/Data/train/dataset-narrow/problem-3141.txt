From the above you can clearly see that when continuous action space is involved, PG offers much more plausible solution. For more information I suggest you the Master thesis PG Methods: SGD-DPG of Riashat. He worked closely with one of the co-authors of Silver's DPG paper and the thesis is very well structured with MATLAB code available. Policy Gradients methods are much harder to understand because of the math involved (and many times the different notation used). However I would suggest you to start from Jan Peters and then get back to Silver's slides (my opinion :) ). 

You could use the same reward function that Openai's Inverted Pendulum is using: $costs=-(\Delta_{2\pi}\theta)^2 - 0.1(\dot{\theta})^2 - 0.001u^2$ where $(\Delta_{2\pi}\theta)$ is the difference between current and desired angular position performed using modulo $2\pi$. The variable $u$ denotes the torque (the action of your RL agent). The optimal is to be as close to zero costs as it gets. The idea here is that you have a control problem in which you can come up with a quadratic 'energy' or cost function that tells you the cost of performing an action at EVERY single time step. In this paper (p.33 section 5.2) you can find a detailed description. I have tested RL algorithms in this objective function and I did not encounter any problems for convergence in both MATLAB and Python. If you still have problems let us know what kind of RL approach you implemented and how you encoded the location of the pendulum. Hope it helps! 

First of all the weights update is derived using gradient descent. So the proper form of update is the first one you have. This is derived using math and satisfies that the updates are towards minimizing the Mean Square Error between true value and approximated one. For the true value we use a biased sample of the true value at the next time step plus the reward obtained at the current timestep: $r_t + \gamma \hat{v}_{t+1}$ which is your TD target (what you try to approximate). So the tricky part in RL with FA is that you try to approximate something which is also an approximation of a true quantity. I am not very sure what do you mean by making the value of γVt+1 closer to Vt−rt. For linear approximation, as you stated at the beginning of your question, the update needs to have the following form in order to reduce the MSE of the true value function: $\Delta w=\eta(v_\pi-\hat{v}_w)\phi(s)$. As I mentioned above because in RL we don't have knowledge of the true value, but only reward signals, we substitute the target (true value) with the target form I described in the previous paragraph. Your first proposed update, first of all, eliminates the discount factor which ensures that in an infinite horizon scenario the sum of rewards converges into a real value and not to infinity. Even if you consider episodic tasks the sign change makes your weights updating into a direction that doesn't follow the gradient of the MSE. In your second update again you use a an arbitrary rule for updating and not derived through gradient descent. By the way cutting in half the learning rate does not help in any sense as again you can define a new learning rate. I refer you to these documents in which you can clarify how the update rule is defined and why it has that particular form and what are the optimality criteria and the role of the discount factor in RL: RL with FA, 2.3 Optimality Criteria and Discounting Hope this helps! 

Nothing in the components they used is novel. All approaches have been explored. Checking their references you will notice many researchers doing similar work. The novelty was the pipeline they followed and the combination of model-free and model-based Reinforcement Learning approaches. I will try to give you a non technical different perspective on what they captured. Model-free approaches usually attempt to approximate functions such as Value functions (representing how good it is to be in a particular state - board configuration - in terms of future reward) or parametrized policy functions (probabilities of selecting an action given a state. Briefly, your model gains some kind of 'intuition' on which moves are relative good - something similar to the intuition professional Go players have, when they declare that they do a move because it 'feels' good. This is very important at the early stage of the game when planning is inefficient to use. Model-based approaches attempt to simulate every single possible trajectory of the game in the form of a decision tree. Thus they are useful for planning (before you actually make a move in the game you check and evaluate all possible contingencies and then you decide which move to take from your current position). The MCTS is such an algorithm, creates a decision tree over possible future courses of the game from the current board position, and evaluates these heuristics according to some criteria. The best algorithms in Go so far were based in this algorithm (and is considered as a RL algorithm). So in terms of novelty, with few words: combination of planning and intuition, which means combination of MCTS algorithm with function approximators for evaluation of the simulated game trajectories. In this case they used very deep convolutional neural nets for the 'intuition' part. In addition to this, the whole model is data-driven as it was first trained on human expert moves (this could be useful in applications in many other domains apart from gaming). If you examine every single component, there is nothing novel...but the whole process to combine effectively all these elements and achieve Mastery in that complex domain is something novel. Hope it helps! 

In general we prefer to normalize the returns for stability purposes. If you work out the backpropagation equations you will see that the return affects the gradients. Thus, we would like to keep its values in a specific convenient range. We don't follow this practice for theoretical guarantees but for practical reasons. The same goes with clipping $Q$ value functions in Q-learning combined with NNs. Of course, there are some drawbacks with these approaches but in general the algorithm behaves better as the backpropagation does not lead your network weights to extreme values. Please take a look at this excellent post by Andrej Karpathy (I attach the part related to your question as a blockquote) which gives additional insights: 

Eventually, and if you have chosen an appropriate distribution for your domain the agent will adapt to the unknown environment. Of course richer priors with non conjugacy will lead to MCMC sampling methods. I refer you to this paper to get an overview of the problem: Model-based Bayesian Exploration and the quite advanced: Bayes-Adaptive MDPs for further research and exploration. 

The Policy Gradient theorem states that the gradient of the expected reward is equal to the expectation of the log probability of the current policy multiplied by the reward. Notice that to compute the integral of the expectation we can use Monte Carlo method. For this you would need sampled trajectories. Have you looked at a derivation? The whole theorem comes from mathematical derivations. Check in that page the Assumptions and the Likelihood Ratios (REINFORCE) sections. Hope it helps! 

I think that the field has moved on from that paper. There is a trend to use the data from the expert to either "precondition" the agent or extracting a policy directly from the data. You can search for imitation learning or behavioral cloning. Some of these algorithms: Generative Adversarial Learning, DAGGER and Deep Q-learning from Demonstrations . AlphaGo used Supervised Learning as well to get to a good policy before getting trained in a RL setting. So instead of trying to recover a complicated reward function from the data you can use the above methods to get a good policy or initialize the parameters of your agent to more promising directions. Hope this helps! 

Although the previous answers cover a lot to get you started in Reinforcement Learning (RL) field, I give you here an illustrative simple example to understand the concept and also what is the relationship between Supervised Learning (SL) and Unsupervised Learning (UL). Imagine that you have a robot and you want to teach it to drive a car. Every let's say image of the road that the robot receives is going to be an input. One option that you have, in order to teach the robot, is that you can instruct it EVERY time that it receives the image of the road how much to steer the wheel. This is SL as you will have for every input state of the road a mapping to the proper angle of rotating the wheel. The main point here is that you know what is the optimal thing for your robot to do and you teach it by examples. In a RL setting, you just let the robot try whatever it wants and you give it a reward/punishment regarding the action(s) it takes. The magnitude of the reward/punishment might be dependent on e.g. damage to the car, staying long time on the same lane etc. the reward/punishment might be given delayed and not at every single action that the robot takes. In the first example (SL) the robot tries to minimize the error between your recommendation and its choices. In the second example the robot tries to maximize its reward by finding on its own what is the best to do. The SL approach at its best will lead you to a robot that "mimimcs" what you taught it. In the RL approach at its best, the robot will have a behavior that will be optimal in terms of driving the car and also might be better than yours. In other words it will create its own strategy. To sum up,in SL you have a teacher that tells you at every single timestep exactly whats the correct response. In RL you try and find it on your own and the teacher gives you a reward/punishment. In UL you dont have any external feedback. So RL falls between SL and RL. I simplified lots of the terms just to conceptualize the learning techniques with the example. Hope it helps! 

Have you looked at the Rainbow RL? It combines all improvements of RL. Apart from structural changes other improvements come from Reward Shaping, Curiosity-driven Learning, RL with Unsupervised Auxiliary tasks and RL with external memory (e.g. Neural Episodic Control, Model-free episodic Control). It's pitty that you leave out from your question Policy Gradients, they are lots of fun :) Happy reading! 

There are many papers out there that deal with neural networks and RL. This blog will give a very good insight on a Policy Gradient Network: Deep RL with PG Now for your question. You really need to be familiar on how we train a neural network. A simple one for classification. If you check the derivations and how the weights are getting updated things will be very clear to you on how you can implement the above. I will describe it to you as simple as possible so you get the link. A Neural Network in a very broad sense consists of nested functions. The function that contains all the others is the one at your output layer. In case of Stochastic Policy Gradients this is your Boltzmann function. So your output layer will take all the previous layer's outputs and will pass them through the Boltzmann function. In the case of NN the parametrization comes from all previous layers. The blog link I sent you describes a very nice and simple example of a vanilla Policy Gradient with NN (REINFORCE algorithm). By using the code (plus your understanding on feedforward networks) you will see that the gradients are multiplied by the reward. It is a very good exercise! For Actor-Critic, you need in general a network performing PG (stochastic or deterministic) -- you Actor -- and a network that will give you the reward signal (like the simple case in the blog. However, for various reasons, instead of the actual reward we use another network that estimates the reward by performing Q-learning as in Deep-Q learning (minimizing square error between estimated reward and true reward). Hope this helps!