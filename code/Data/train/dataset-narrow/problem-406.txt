All that you need is to add an extra column where the total mileage should be written each time you get a new lat-lon point. That approach has a lot of advantages, most valuable of which are: 

If you want to get the latest records from for each then this can't be performed by queries only in an easy way (at least within MySQL dialect). The simplest way is to create the copy of the table but with UNIQUE index on the and syntax. Each record written into the should be written into too. IODKU ensure that old data will be overwritten by new ones and when you you'll get all and only latest log lines for each . 

Here two bottom tables contains nothing but IDs of corresponding hotels, merchants and addresses. When you want to know what adresses are associated with some hotel your query is: 

By default server listen only on the for security reason. When that line is commented, server become listening on the all available interfaces because default value is . Now you can connect to the from the host OS by IP-address of the VM. To restrict guest access to the single database/schema you have to perform the next query: 

Stored routines can't help with non-indexed tables. Without indexes rows are searched by brute force. Create the complex index for (cat_no, period_year): 

That approach keep data untouched but require some modification of the SQL code. There is no general rule, you have to decide what way is more suitable for your needs. 

There are three tricks here. First is that (UDVs) are persistent through the rows proceeding. Once assigned they will store the value until reassigned. Second is that in context of UDVs there is difference between comparison and assignment that have different precedence. Here 

You have to remove all the duplicates prior to create the or (that is also in fact) index. In your case that can be performed via joining the table with itself: 

It's very probable that next sequence would be way more fast and painless comparatively to the software upgrading: 

Create folder on the biggest partition you have. Add the line to the section of the mysql.conf. restart mysql perform REPAIR for all the tables in your working base 

Also there is UUID_SHORT() function that returns 64-bit INT instead of string, that is way faster for indexing and search. $URL$ 

Here I've replaced all references to the by that contains the values of calculated in the first statement. Just to make it clear: only the last calculated value of the UDV is passed across the statements and then it stay unchanged. Therefore you have the same value in the last column of the resulting table. And even now I'm not sure the result would be correct because of complexity of the whole query. 

all the databases export all accounts with permissions deinstall install import databases and accounts 

stands for one-dimensional distance while is standing for two-dimensional one. Let's the data is the timestamp. It is one-dimensional and you can define part of data "in range" or . If your data represents the plane of points x:y then you can define part of them as "points closer than radius Z to the given point" or . 

To find a distance between arbitrary two points you have to substract beginning odometer value from the endind. That easy. All the calculation-related load will be distributed over the time. You can verify distances, provided by the trackers. Most of them are buggy and can send garbage data that drive to the complete reports unadequacy. 

In your case all tables need the fields and to be indexed. N.B. 0.0066 sec/query isn't slow. This is the usual value for the fast/optimized queries. You have to profile your query to detect the most time consumpting phase: 

Here is used instead of . Plain s generate full cartesian products of the whole sets, that can drive to the huge disk-based temporary tables. Then this huge product table is filtered by . in opposite create the cartesian product of subsets, containing only rows that satisfy conditions. Instead of "multiply and then filter" here "filter and then multiply" strategy is used. 

MySQL is absolutely excessive for that purpose. There is another very useful software that intended exactly for storing various stats - RRDtool RRDtool is not a RDBMS, it hasn't indexes, queries and so on. There is only the circular table (only one per DB) of predefined capacity that overwritten from the head row by row when filled to the end. RRDtool store not exact readings but rather averaged values with some granularity. For each table you can define different granularities and incoming data will be processed automatically. Then you can refer to the data in terms of timestamps [begin-end] and RRDtool automatically returns you the dataset with appropriate granularity. RRDtool have various interfaces that returns data as CSV, XML and even pretty cool plots. If you want to store loads, RRDtool is the tool of choice. 

Here I suppose that , and columns contains the unique values therefore proposed indexes have that specific orders of columns. But for other cases the different indexes like can be more efficient. The actual order of columns in the index depend on the size of table(s), type of columns and the ratio value. Also for best efficiency you have to avoid literal values and replace them by IDs from the corresponding reference tables, if possible. 

It is amazing but your query returns the wrong result. of the returned row isn't associated with the row containing the minimal . But you can get the desired result such way: 

You should distinguish the from the . Single physical machine aka can run number of that are DBMS engines. You can run few MySQLs, few PostgreSQLs, few MongoDBs and few Redises on the same machine at the same time. But you can run single dedicated engine. Each DBMS engine can contain number of databases - hundreds databases tor hundreds websites for example. Each database aka scheme can contain lot of tables, stored routines, events, views for some project. Saying literally you can store all your data for pile of projects not only within the single database but even within the single table. But in practice it is way more useful to separate projects from each other at least by separate databases. But when some project exhaust the host's resources you have to separate projects by hosts. Certain strategy depends on the projects' resource consumption and number of indirect factors. 

Sure you have to remember that functional restrictions like in the , or clauses cannot be speeded up by indexes and can be VERY slow. I hope that query wouldn't be runned on the regular basis. 

Operator returns 1 if operand is 0, 0 if operand is non-zero, and if operand is . Parenthsis added because adding have higher precedence over negation. 

It's look like your reporting query lock for a while the tables needed by general queries. There is no general recipe for such cases and you have to refactor your reporting to avoid locking. Sometimes you can achieve this with splitting one big query into the series of smaller ones. Sometimes the good approach is to copy some tables for exclusive use by report generator. Sometimes it is something else. All depends on the your DB scheme. 

Updates performed just for software actuality are inappropriate in the production. is well-known as unsecure software with lot of breaches so its location should be at least password-protected and restricted to the secure subnet(s). 

Do not make any column you want to manipulate explicitly. That can confuse an engine and cause serious problems. If no column you have used for primary key are you can do anything you want with them via triggers. Sure generated values will be rejected if they violate the mandatory uniqness of the primary key. 

filled with millions of rows. When you want to get the last stored readout for each sensor that can take a while. But if you have the identical table where field have UNIQUE index and all s have clause, filled simultaneously with the main table then you can simply from that table. Normalization suggests that no different tables allowed that store the entitites of the same type. But in the real world this isn't a dogma. 

you have to order index fields in range from producing the smallest result set to the broader condition. If your table contains 10 million records for an hour, then 5 min part contain about 1/12 or 800k records. If you have 1000 unique keys f.e. then you have about 1/1000 or 100k records with the same unique key. Therefore index should be defined with UniqueKey first then Timestamp: