You can't specify this, but it doesn't matter I believe it's not possible. However, I think that there is relatively little inefficiency in having to do it as two separate actions. In order to create a clustered index the table would have to be sorted anyway, and a copy of the unsorted table would have to be created somewhere in before the sort operation. A point to note about temp tables is that the intermediate join results are persisted in exactly the same data structures in as temporary tables. Therefore temp tables are no less efficient in terms of processing or I/O, particulaly if they are created with minimally logged statements. Given that most table creation by tends to happen in tempdb anyway the actual I/O from a followed by a clustered index creation isn't going to be radically different from an operation that creates the temp table with a clustered index anyway. Ergo, I dare say MS decided (fairly sensibly) that they don't need to fix it as it ain't broke. More on Temp Tables A corollary to this is that most arguments against temp table usage based on performance are factually incorrect. As the system uses to store intermediate join results it is using the same data structures as a temp table would. One feature I really would like to see, though It would be nice to be able to use minimally logged operations to create partitions on the appropriate file group and then swap them into a partitioned table (think pupulating monthly snapshots on a snapshot fact table). There are a couple of workarounds. You might be able to use the technique described here but it's a bit yuck - for example it wouldn't support concurrent operations on the same table in different file groups. Another way to do it is to create the table in the default filegroup and, create a clustered index on the desired filegroup and then drop the clustered index. 

I'm not aware of any open-source tool that would scale to 'hundreds of enterprise databases.' The closest things that come to mind are: 

EDIT: See my answer - the problem was due to lack of write permission on the lock file and an ambiguous error message returned by the driver. 

Best to avoid entity-attribute-value structures if possible as they complicate the model and are unnecessarily fiddly to work with. It sounds like the fields are going to be the same or substatnailly the same across all the houses, so you're probably just better adding them onto the property record. The exception being if you really have a genuine 1:M relationship. An example might be if you wanted to record (say) dimensions and room type (living room, bedroom, bathroom etc.) by room for a variable number of rooms. 

No. I don't believe this is possible. The staging table must have a constraint on it that matches a specific partition and will only work with one partition at a time. The closest you could do to this is to write something that iterated over the partition ranges and swapped the table to a temp table and then into another partitioned table. 

If you have the following database schema with Date, Product and Reseller dimensions and a price fact table: 

On the whole, I would rate VS2010 about a B-. It was clumsy on a relatively simple database project with two fact tables and around 15 dimensions. The largest data model I ever did was for a court case management system, which had around 560 tables. I would not recommend VS2010 for a project that size (I did that on Oracle Designer). Essentially it's trying to be clever and the paradigm doesn't really work all that well. You're better off with a best-of-breed modelling tool like PowerDesigner or just using create table scripts by hand. SSMS is simple and reliable but manual. You have pretty much infinite control over how you want to manage the model. Combine it with a schema manager such as Redgate SQL Compare and maybe a decent modelling tool such as PowerDesigner and you have a much better package than VS2010. Summary I'm not sure I could quote any killer features or benefits apart from (perhaps) integration with VS solutions containing other projects. If you already have VS2010 premium or ultimate then you get a somewhat flawed database development tool thrown in with your .Net tool chain. It will integrate DB projects into your VS solution, so you can use it to make sproc deployment scripts at least. However, VS has no modelling tool to speak of, so PowerDesigner or even Erwin is better on that count. Redgate's schema management is much better and SQL Compare Pro is quite cheap (about Â£400 IIRC). IMHO SSMS works much better for T-SQL development but you can certainly do it with VS2010. VS2010 premium isn't much cheaper than a best-of-breed database modelling tool and VS2010 ultimate is at least as expensive. At the expense of tight integration with your VS project you could probably do better with third party tools. An alternative I guess one shouldn't slag off VS2010 too much without suggesting at least one alternative and outlining its pros and cons. For the purposes of this I'll assume a large project. Although I mainly do A/P work these days I have been involved in a 100+ staff year project where I did the data model (and some development work) and a couple of others in the 10 staff-year range, where I mainly worked as an analyst or a developer. Mostly I work on data warehouse systems these days but the larger projects were mainly applications. Based on my experiences with various tools, here are some suggestions for an alternative tool chain: 

The columns are each individual item, sorted by the total Line Value over all months, i.e. from all-time best seller to all-time lowest seller, with items having no sales at all excluded. The rows are the months for all time where data exists. You can use a set definition to specify any date range you want. Note that this assumes the existence of a dimension called [Date]. This query will show the items in order of total sales across the columns and the sales for each month by item down the rows. I'm not 100% sure if this is what you want; feel free to clairfy your question if I'm not on the right track. 

It's almost a matter of semantics. A lot of hot air gets released in discussions about this but I'm not really convinced that there is any real philosophical depth to a distinction between the two. At some level you can view ETL as transforming data in a client-side tool before finally loading it, with ELT implying that the data is transferred to some sort of staging area with relatively little change to the format. 'Transformation' takes place afterwards. These are very fluffy definitions and could be applied to a wide variety of technical architectures, and there are many possible designs that either term could be used to describe. I'm very strongly in favour of an architecture where all the transformation and business logic can be built into a more or less homogeneous code base, and I've done a lot of systems where the transformation logic was quite complex. This tended to just use the ETL tool to land the data and then all of the transformation was done in stored procedures. Arguably this could be described as ETL or ELT with the difference merely being one of semantics. Some tools are very database centric, however (Oracle Data Integrator, for example, is often referred to as an ELT tool). If you subscribe to this view, then 'Extract' and 'Load' are happening before the data is transformed as they are being landed into a staging area and then crunched by SQL or PL/SQL code (which may be generated by the tool or hand written). Several people I've talked to seem to regard the principal merit of ODI as that it's not OWB. If you use a client-side tool such as Informatica Powercentre or MS SQL Server Integration Services then the tool can do extensive transformation to the data client-side. Some ETL tools, such as Ascential Datastage and Ab Initio are designed to do a lot of work with flat files and in-memory data structures for speed. In this sort of architecture the transformation has already been done before it's loaded. Perhaps this type of architecture could be definitely classified as 'ETL', although I've seen many tool-centric projects where all the real work is done by a bunch of stored procedure code. There are advantages to various tools and architectural approaches, but one can't make a blanket statement about the merits of 'ETL' vs. 'ELT' approaches because the terms are so broad that the difference is almost meaningless. Some tools and architectures may have specific advantages - for example, Ab Initio's heavy use of flat files gives it a significant performance advantage on large data volumes. In practice, making the distinction between 'ETL' and 'ELT' is pretty meaningless without going into a much deeper discussion of the system requirements, platform and technical architecture.