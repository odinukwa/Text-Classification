You can reduce the log generated by temporarily switching to BULK_LOGGED while your index maintenance runs, but you lose the ability to do point in time recovery for that period of time. Apparently it also does not reduce your log backup. --More info on log usage from DBCC SQLPERF (logspace) and SELECT LogRecordsGenerated = COUNT(*) FROM sys.fn_dblog(NULL, NULL) WHERE AllocUnitName = 'dbo.Logging.IX_LOGGING' to show actual log sizes, note the index is only 648KB This is the log after populating it with data: Database Name Log Size (MB) Log Space Used (%) Status IndexLogging 14.99219 70.89956 0 LogRecordsGenerated 10903 Here is the log after a log backup: Database Name Log Size (MB) Log Space Used (%) Status IndexLogging 14.99219 5.100313 0 LogRecordsGenerated 0 Log after log backup and then Online rebuild: Database Name Log Size (MB) Log Space Used (%) Status IndexLogging 14.99219 22.642 0 LogRecordsGenerated 11160 Log after log backup and then offline rebuild: Database Name Log Size (MB) Log Space Used (%) Status IndexLogging 14.99219 10.79338 0 LogRecordsGenerated 140 

Maybe I am missing something, but you realize that start off with 8 columns, do an insert then alter the table and add a 9th column and then do another insert. Of course the size is going to be different when the first insert was into 8 columns and the second was into 9 columns. 

It sounds like what you are looking for is an Always On Availability Group in Synchronous Commit mode with automatic failover. The transactions sync from the primary to the secondary and in the event of a system failure, will pass ownership of the database to the secondary. Your application would need to be aware of both the primary and secondary and which one is active. This feature does make use of the cluster service but you don't have to have an actual cluster with shared storage (see this post from Brent Ozar for more details about hot to work around that). This is the more expensive option, since it requires Windows Enterprise. You can implement mirroring in high safety mode which does essentially the same thing on any edition of windows, even a desktop if you want, since it doesn't require clustering features. You won't get all the extra benefits, but if you just want to have a synchronous hot standby of your database then it will do that. The caveat here is that mirroring is deprecated, so if you implement this now, you will eventually have to migrate to availability groups in the future. 

Short answer: No, you can't, and not because it's a system type. You can't anchor a freestanding type to any table's column data type. %TYPE is a PL/SQL construct. CREATE [OR REPLACE] TYPE is SQL. You can't use %TYPE in SQL. It somewhat makes sense that you can't. If you use MYTABLE.MYCOLUMN%TYPE in PL/SQL, you have anchored that PL/SQL type to the table, and should the type of MYCOLUMN change PL/SQL can invalidate your code, then recompile it. It's much less clear what Oracle would have to do if your example worked. Imagine what would happen if you were storing objects of SQLID_T in a table, and the definition of SQL_ID in V_$SQL changed. Would Oracle need to change the definition of the stored objects? What if it couldn't (e.g, SQL_ID was changing from a VARCHAR2 to a NUMBER)? Should that prevent the definition of V_$SQL from changing? Or should the table that used that type become invalid in some way ... you can't SELECT from it anymore? Oracle tries to prevent this kind of thing from happening (from Oracle's Object-Relational Developer's Guide): 

This sounds like a job for Change Data Capture (CDC), which allows you to (among other possibilities) ship your archivelogs from the OLTP database to the reporting one, mine them for the changes, then query the changes out, ignoring any you don't want (e.g., changes of type 'D' for DELETE), and using whatever process you might devise apply those changes to your reporting tables. I have no idea how well CDC would do with a ruleset encompassing 4700 source tables from another database. I've never used it for more than about 50 tables myself. FYI, there are licensing-related limits on CDC. The full feature set is only available on Enterprise Edition. 

I've seen it mentioned a few times that you can configure an event session to take an action when an event fires. Does anyone have any information on how to do this? I know that you can use the service broker to take an action on an event, but I don't consider that to be part of an event session. 

Is it acceptable to have downtime on this database? This was probably either restored from a replicated database or it was possibly a subscriber that was improperly removed, though that is unlikely. You could try doing a backup from express and restoring to a standard or higher edition then setting up replication again and removing it. Then you can backup from standard and restore to express. As long as you don't enable any features on the database while at the higher edition, there shouldn't be a problem downgrading. You can test this out in advance of an actual outage to ensure it will remove the status and script it all out to minimize downtime. If you don't have another server you can use, grab the evaluation copy and install on your local machine, a VM, the original machine if it is acceptable, or anywhere you can find. You have limited options with express as you have observed. 

Do you know what pages are getting the latches? DBCC heavily uses tempdb as @ShawnMelton was alluding. Have you considered that tempdb contention could be the source of your pagelatches? You should be able to investigate that with sp_whoisactive if you aren't already. How many data files do you have for tempdb? It sounds like you've only got one, try bumping it up to 4 and increase in intervals of 4 as needed, though 8 is often enough for most systems and many suggest starting with that number of data files. You'll also want to ensure the files are equally sized. You can also look at this post which has lots of info and links to reduce the impact of dbcc including a post from Paul Randall on running DBCC on a VLDB which might be helpful. 

I don't understand why it fails as a SQL Agent job yet succeeds 'interactively'. Is there something about that INSERT statement which needs special attention? I would appreciate some help to allow me to resolve this problem. Thanks. 

and then it worked, which deals with my issue. It is notable that I have the same security setup against my own login and that still doesn't work. Odd... but that isn't really a problem for me now - I don't know why it that doesn't work though. Note that I'm aware that giving the login db_owner is a bit of a sledgehammer approach, but now that it works I'm able to make the security a bit more fine-grained. Thanks! 

I have now sorted out the disk space issue and have re-run the full backup successfully, but I remain concerned about the log item above which references F:\sql\log because the machine does not have an F: drive at all, just a C: and an M: drive. It does appear that the 'Could not generate mail report' issue is related but again this error is a bit odd; I am receiving emails when the various jobs complete and I was alerted to the disk space error via an email, so I wonder why an error is logged if the email was sent successfully? I've dug through the SQL Server, SQL Agent, Database Mail and the maintenance plan configurations but I can find nothing which is set to F:\sql\log and so I have no idea why the maintenance plan is trying to write to that path, or why it is logging an error when email is sent successfully. Can anyone help me to work out how to resolve this? Thanks for any assistance.