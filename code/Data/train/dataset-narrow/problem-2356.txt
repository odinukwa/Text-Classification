First: Student's heuristics. At your level, what can you prove? Equivalence with TM or PDA should work by simulation. Equivalence to LBA might be slightly harder. Anything in between is certainly harder. So start with the easy inclusions. You can see very quickly that languages like $a^nb^nc^n$ that are not context-free can be accepted by a 2-PDA. So you are certainly stronger than PDA. Next in line is TM. Of course, every 2-PDA can be simulated by a 2-TM which can in turn be simulated by a TM. Turns out that the other direction works out, too. Take a TM and simulate it by a 2-PDA. You can keep the state graph but have to translate reading/writing/moving to reading$^2$/writing$^2$. What can simulate the TM's head? Also, take care what happens if the TM accesses new fields. Can you figure it out now? 

In addition to what others say, I like the package todonotes for LaTeX that allows to have colorful reminders of what remains to do in the text. 

In "Purely Functional Data Structures" (1998), Chris Okasaki proposes tries of binary trees using type aggregation (10.3.2). I don't know wether this is immediately helps; the solution given there might not be implementable directly. 

I would then explain that while PCS people would see to a fast implementation or good integration in complex systems, TCS people wonder about what is possible and proving things that provide safe, reusable knowledge/techniques for PCS to use. You can also use people's frustration about computers ("It does not do what I want!"). You can point out that (T)CS deals with how to express things in a way computers can understand and process efficiently (referring to syntax, semantics, datastructures, algorithms). 

If given one object (e.g. trace in case of LTL), you consider only one future for every point in time, in CTL you have a plethora of them. In particular, gives a unique action in LTL but (potentially) a whole set in CTL. 

I like Concrete Mathematics by Knuth. It gives a good overview/basic knowledge of many important tools. If you like generating functions (see generationfunctionology by Wilf) as a tool, complex analysis comes in handy, too. 

It is common to implement lists using arrays for obvious reasons (random access). If you have an implementation that extends the array by a constant number of fields $k$ every time it is full, you can achieve constant time for figuring out the number of elements without having additional information stored. Execute a linear search for $A[A.length - k - 1]$ to $A[A.length - 1]$. By data structure invariants you know that the list's end has to be in this interval; hence you can simply calculate the total number of elements. This only works if you also decrease your array, of course, or consider szenarios where only adding occurs. Since you have to consider at worst $k$ elements, you are in $\mathcal{O}(k) = \mathcal{O}(1)$. This does obviously only work if you can identified unused indices (e.g. by null references). Of course an expected runtime of $\frac{k}{2}$ is worse than $1$ (for $k>2$). Disclaimer: I am sure most people would opt for the additional variable when implementing lists. I just wanted to point out an additional possibility that is equivalent in terms of rough worst-case bounds. 

Now, you can consider the bit strings of your elements wrt either partition, i.e. $w_1 = n_1(1) \cdot \dots \cdot n_1(n)$ and $w_2 = n_2(1) \cdot \dots \cdot n_2(n)$ (with $n_j(i) = n_j(S), i \in S \in P_j$). Then, the desired quantity is $d_H(w_1, w_2)$, i.e. the Hamming distance between the bit strings. 

Instead of laboriously finding, justifying and analysing a specific model, you might want to use what real life data you have (if you have any). That means defining a generic probabilistic model and training its parameters given your data (e.g. by maximum likelihood estimation). For example, you can describe an SCFG for trees (e.g. $S \rightarrow p_1 : (S)S \mid p_2 : \varepsilon$) and assign probabilities ($p_1, p_2$) based on relative occurrences in your real life data set, which provably yields an MLE. You can even train probabilities using the inside-outside algorithm. As a bonus, you even have a concise description for your model which can be used in a variety of analyses. Obviously, the specific grammar can (and should!) use domain knowledge. Consider e.g. different grammars used for RNA secondary structure prediction in Dowell, Eddy (2004) for a taste. Find some details on this technique in Weinberg, Nebel (2010). I do not know how (well) it can be applied to general graphs, though. If you need more power you can move to stuff like multidimensional (S)CFG (e.g. Seki, Kato (2008)) or length-/position-dependent SCFG (Weinberg, Nebel (2010)). 

Get someone to set you up a repository and give you URL, username and password Go to where you want to have your lokal copy In a shell (command line?), type: (check-out). Now a new folder with the repository's contents appears. 

This answer does not directly answer the question since it does not use It offers another solution for the problem of finding collision free keys based on object content, if not a particularly efficient one for complex types resp. large object structures (if objects are mutable; if they are immutable, you only have a small overhead for every object creation and that's it). Assume we have a type which can take arbitrarily large integers and an injective function that encodes arbitrary tuples of values to one using some generalized Cantor scheme (exists and is computable). Find injective mappings to for all primitive types. That should be easy. For sake of simplicity we call all such mappings by the same name . Now we can define for arbitrary (structural) types by recursing to its members, i.e. 

Well, ask your professor why he did it. I can only guess. They are not as interesting as Turing complete models and PDA because they are in the void of uselessness* they share, of course, with their language equivalent: not as powerful as possible, but already very much intractable. Another reason might be that not as much is known (guessing here) about them, but that might come down to a chicken-egg-problem. It is unclear weather $NLBA = DLBA$, so that might pose problems for didactics. Also, typical proofs (e.g. accepted language, model equivalences) are much harder than for other models. (*) deliberate exaggeration 

Note that I want to exclude chains of reductions that end up at HP in the end. Having such an independent proof might open up new ways for students and laymen to understand the underlying issues. Failing that, are there reasons for that lack? Do we need the kind of self-applicability we employ when proving HP not to be computable? PS: I was unsure wether or not this question should go here or rather onto math.SE. As the proper place might depend on the level of answers, I went with the specialist community. 

I attended a talk by Sebastian Lindner in early April this year. He works for Springer Materials working on normalising citation data (can't find a reference, sadly). This is still work in progress, but we will hopefully see some significant improvement. From what I remember of the talk, authors can help a lot by adhering to some standards, wherever they take them from. 

I will answer for the general setting. Assume a directed graph $G=(V, E)$ with costs as assumed in the question, that is the costs for entering a node depend on which edge it is left on and vice versa. Let $v \in V$ arbitrarily; the following construction can be performed iteratively for all nodes in any order. Let $e_1, \dots, e_k \in (V \times \{v\}) \cap E$ (incoming edges) and $f_1, \dots, f_l \in (\{v\} \times V)\cap E$ (outgoing edges). Now create for all pairs of $e_i =: (u,v)$ and $f_j =: (v,x)$ a node $v_{ij}$ and edges $(u, v_{ij})$ as well as $(v_{ij}, x)$. Assign the costs as per the described model: costs for edge $(u, v_{ij})$ are the costs for entering $v$ from $u$ when leaving in direction of $x$; costs for edge $(v_{ij}, x)$ are the costs for leaving $v$ to $x$ when having come from $u$. Add all thusly created $v_{ij}$ and their incident edges to $G$ and remove $v$ and its incident edges from it. Iterate for all nodes. The resulting graph has $\mathcal{O}(|E|\cdot d^2)$ many edges ad $\mathcal{O}(|V| \cdot d^2)$ many nodes, $d$ the maximum node degree in $G$. It can be reduced by merging (or not splitting) those combinations that have the same costs. The original problem is equivalent to finding a path with exactly one from each $\{v_{ij} \mid v \in V \}$.