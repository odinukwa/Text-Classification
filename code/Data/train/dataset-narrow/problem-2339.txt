If the only randomness you can obtain is sampling from a set of polynomial size, you are not going to be able to get two random permutations, because the probability of any particular pair of random permutations is $$ \frac{2}{n! (n! - 1)} $$ and $n!-1$ doesn't necessarily factor into polynomial-size numbers. 

You can be a quite successful theoretical computer scientist without programming. For a few people, programming is quite difficult, and if you are one of them you shouldn't despair and switch fields. However, for most math and computer science graduate students, learning to program is not particularly difficult, and is a skill which is very useful. You should learn a programming language, and if you enjoy it, you should try to get enough practice to become reasonably proficient at it. Then, when the point comes (and it will) that it will be useful in your research to write a program, you will be able to do it. If you don't learn to program now, it is quite likely that when you eventually need to write a program, you won't have time to learn, and so you may not actually write it, and end up being less effective in your research. While getting a grad student or an undergrad to do this for you isn't too hard, there are a lot of times when it's much easier and less time-consuming to do it yourself rather than explain the problem to them. What language should you learn? I'd recommend an object-oriented language, since these are the ones that are currently in most use, and I suspect this will be more true in the future. Maybe Python or Java—they're both object-oriented languages, and while they're used less in practice than C++, my impression is that they're both much, much easier to learn. (Caveat: I don't know C++, despite having worked at Bell Labs, so maybe I'm wrong about this.) 

There seems to be a typo; I assume you mean to find $u \in \{0,1\}^n$ which is not the sum of $(\log n)^{O(1)}$ vectors among $v_1,\dots, v_m$ (not $n$). It's not clear to me if any constant in $(\log n)^{O(1)}$ works for you. If you can settle for sums of less than $\log m$ vectors maybe there's something to be done. But If you want this quantity to be $(\log m)^{1+\delta}$, then I think it is quite hard (I have been working on this problem for a long time). Still you may be interested to know that this is an instance of the Remote Point Problem of Alon, Panigrahy and Yekhanin ("Deterministic Approximation Algorithms for the Nearest Codeword Problem ") for certain parameters. Let $m > n$ and $v_1,\dots,v_m$ be the columns of the parity check matrix of a linear code in $\{0,1\}^m$ of dimension $d = m - n$ (if this matrix didn't have full rank, the problem would be trivial). Then your problem is equivalent to finding $u \in \{0,1\}^n$ that is $(\log n)^{O(1)}$-far from the code. This setting of parameters, where the dimension is very close to m, is not studied in the paper. However, they can only achive remoteness $\log m$ up to dimension $d = cm$ for some constant $c$. In fact, I don't think we know of any polynomial-sized certificate that allows us to prove that some vector is more than $\omega(\log m)$-far from a space of dimension $\Omega(m)$, let alone find it. Another connection is with learning parities in the mistake-bound model. If one can efficiently learn $(\log n)^{O(1)}$-parities (defined on ${0,1}^m$) with mistake bound strictly less than $n$, then one can set arbitrary values to the first $n - 1$ bits of $u$ and ``force a mistake'' on the last bit by setting it to the opposite value to that predicted by the learner. This seems much stronger though. The problem is also related to separating EXP from certain reductions to sparse sets. 

If you work for an industrial lab, it can be much, much easier to get a paper approved for release than to get code approved for release (even if the paper contains all the information needed to rewrite the code). Blame bureaucracy. 

Look at the graph that is formed by the comparisons. Each element corresponds to a vertex. If an element is a vertex of degree $d$, then the probability that it will be the lowest in at least one of these comparisons is the probability that it is not larger than all of its neighbors. This probability is $d/(d+1)$. Now, expectation is linear. This means that the expected number of losers is $$ \sum_{v \in G} \frac{d_v}{d_v+1}.$$ To illustrate, with your example there is one vertex of degree 2 and two of degree 1, so the expected number of losers is $\frac{2}{3} + \frac{1}{2} + \frac{1}{2} = \frac{5}{3}.\ $ It is easy to prove that this quantity is maximized when the degrees of the vertices are all as close to equal as possible, so the vertex degrees all are either $d$ or $d+1$ for some $d$. 

And indeed, it's not the algorithm from my original factoring paper. It uses the phase estimation procedure from Kitaev's factoring algorithm, and a variant on that, discovered by Griffiths and Niu (not by Parker and Plenio, as I said in a previous edit of this answer) that lets the algorithm output the estimate of the phase one bit at a time. 

The answer is $C_{n-1} n!$ . That is, the $(n-1)$st Catalan number times $n$ factorial. There are $C_{n-1}$ ways of making a complete binary tree with $n$ leaves, and there are $n!$ ways of assigning these leaves to the symbols to get a Huffman code. This sequence goes 2, 12, 120, 1680, 30240, and is listed in the Online Encyclopedia of Integer Sequences as Quadruple Factorial Numbers, with a simpler formula than I give above. The above argument shows that the quadruple factorial numbers give the number of ways of assigning $n$ symbols to codewords in a complete binary prefix code. It's not hard to show that you can assign probabilities to the symbols to make this encoding optimal (so it satisfies the definition of a Huffman code). It's not clear that a given Huffman coding algorithm will generate all of these, however. 

The boolean satisfiability problem is in $\mathcal{NPC}$. But if you only get Horn clauses, it is in $\mathcal{P}$. I've already heard similar statements. Do you know a more general statement when problems in $\mathcal{NPC}$ get to problems in $\mathcal{P}$ because of restrictions? If something like that does not exist, do you know literature where such restrictions are described? 

Find a number $x \in \mathbb{N}: 0 < x < |V|$ $S := \{v \in V | dist(v) > x\}$ and $V \setminus S = \{v \in V | dist(v) < x\}$ There is no edge $(u,v)$ with $u \in S, v \in V \setminus S$ in the residual network. $(S, V \setminus S)$ is a min-cut. 

So the given algorithm can find at least one min-cut quite fast after push-relabel was executed. As $x$ can have more values (the labels of some nodes might be the same and they can go up to $2|V|-1$ as far as I know), you can also find more than one min-cut. My question: Do I find all min-cuts this way? 

$dist(u) \geq dist(v) + 2$ as we defined $S$ like this: $\begin{align} & dist(u) > x \land x > dist(v) \\ \Rightarrow & dist(u) - 1 \geq x \land x \geq dist(v) + 1\\ \Rightarrow & dist(u) \geq dist(v) + 2 \end{align}$) For every edge in the residual network you can say: $dist(u) \leq dist(v) +1$ 

$\Rightarrow 2 \leq 1 \Rightarrow $ Error $\Rightarrow$ there is no edge between $S$ and $V \setminus S$ in the residual network As there is no free capacity in the residual network between the sets $S$ and $V \setminus S$, the value of the min cut is the max flow. According to max-flow min-cut theorem $(S, V \setminus S)$ is a min-cut. 

Intuitively, the theorem says that a line is not a finite union of points, a plane is not a finite union of lines, etc. The simplest proof is to observe, for example, that a finite union of lines has zero area, whereas a plane does not. More concretely, observe that it is enough to prove the claim for manifolds on $\mathbb{R}^n$ by passing to their closures. Consider an affine manifold $M\subseteq \mathbb{Q}^n$ given by the set of solutions to the linear system $A x = b$; its closure will be precisely the set of solutions to the same system over $\mathbb{R}^n$, hence this step does not affect the dimension of the manifolds involved. Also, the closure of a finite union equals the union of the closures. Now note that the $d$-dimensional Lebesgue measure of a manifold of dimension $\le d - 1$ is null. Therefore the $d$-dimensional Lebesgue measure of a finite union of such manifolds is still zero. But the $d$-dimensional measure of an $d$-dimensional manifold is infinite, hence non-zero. As for your second question, I'm not quite sure what you mean. But if the base field $\mathbb{F}$ is finite, then any $d$-dimensional affine manifold over $\mathbb{F}^n$ contains $|\mathbb{F}|^d$ points. So by a similar counting argument, you need at least $|\mathbb{F}|^d/|\mathbb{F}|^{d-1}=|\mathbb{F}|$ affine spaces of dimension $\le d - 1$ to cover an affine space of dimension $d$. 

It's not "obtained", but rather the bound the authors want on $\mathrm{Prob}[|u_1|\ge s]$. The Chernoff inequality says how large $s$ needs to be in order to guarantee the desired upper bound. As they assume $d \le n$, it suffices for $s$ to satisfy $s^2 \cdot d/2≥\ln(20n^2)$, which leads to $s=c\cdot d^{-1/2} \sqrt{\log n}$ for some appropriately chosen constant c. 

How do you decide what the "wrong way" is? Take the first wrong-way swap gate, and interchange the two wires going out of it (including all their associated gates) so that it's correct. This doesn't change the fundamental circuit. It may introduce more wrong-way swap gates, but they're all later in the circuit. Now, you can keep doing this until you've gotten rid of all the wrong-way swap gates. At this point, you may have permuted the order of the output wires, but you will have a circuit which always outputs a fixed permutation of the sorted order. But since if you input the numbers in sorted order, the right-way swap gates never change the order, this fixed permutation must be sorted order. So for any sorting network, there's an equivalent one with the same number of gates and the same depth which has no wrong-way gates. 

Both of these trees have $43/17$ for the expected length of a codeword, which is optimal. It is possible to show that the length of the longest codeword is no more than a factor of $\log_\phi(2)\approx 1.44$ larger than $-\log_2 p$, where $\phi$ is the golden ratio; the worst case example is the large-depth one above, using Fibonacci numbers. So for your example the length is at most $28$. And because there are 1024 codewords, the length of the longest one has to be at least $\log_2 1024 = 10$. So the correct answer is the length of the longest codeword is between $10$ and $28$. But unless the OP changed the wording of the question, these bounds don't seem to be what the problem was asking for. 

Are there any known natural examples of optimization problems for which it is much easier to produce an optimal solution than to evaluate the quality of a given candidate solution? For the sake of concreteness, we may consider polynomial-time solvable optimization problems of the form: "given x, minimize $f(x, y)$", where $f:\{0,1\}^*\times\{0,1\}^* \to \mathbb{N}$ is, say, #P-hard. Such problems clearly exist (for instance, we could have $f(x, 0) = 0$ for all $x$ even if $f$ is uncomputable), but I am looking for ``natural'' problems exhibiting this phenomenon. 

The paper "Random low-degree polynomials are hard to approximate" by Ben-Eliezer, Hod, and Lovett answers your question. They show strong bounds on the correlation of random polynomials of degree $d$ with polynomials of degree at most $d-1$, by analyzing the bias of random polynomials. See their Lemma 2: the bias of a random degree-$d$ polynomial (up to some $d$ that is linear in $n$) is at most $2^{-\Omega(n / d)}$, except with probability $2^{-\Omega\Big(\binom{n}{\le d}\Big)}$. 

The answer to both questions is yes. The matrix $A B - C$ gives rise to a linear application over your field $\mathbb F$, which is a multivariate polynomial of total degree $d=1$. By the Schwartz-Zippel lemma, when $x$ is drawn from $S^n$ (where $S \subseteq {\mathbb F}$), the probability that $(A B - C) x = 0$ is at most $d/|S| = 1/|S|$, provided that $A B \neq C$. As you observe, this can also be proved directly using the standard argument for Freivald's algorithm. 

Mergesort satisfies all three requirements (when merging is performed in place). See Pardo, L.T., "Stable sorting and merging with optimal space and time bounds", SIAM J. Comput. 6 (1977), 351-372.