To test if your folder is correctly shared and accessible from outside you czn try with a net use command : 

You can add to your service an event-handler which will execute a script each time your service comes to non-ok state. Your script will be able to store for example the datas from a top command and a netstat or others tools () which shows the usage of your system. 

Moreover, you can test locally your command if you write directly in this file with the following syntax : 

Otherwise, you can install NRPE on the remote server and execute commands with their results will be send to your icinga2 server. NRPE can be run on windows or linux. I think these two solutions are more simple than install icinga2 as a satellite on all your servers to be supervised. 

You can use log2timeline to export and parse the logs of your windows envronment. With that, you will be able to search and edit some timelines of interesting events. By this link you will find others tools to work on yours logs files. 

If you want to have an idea of what happening on your server, you can use the Performance Analyser of Windows 2012. You can launch a record during 24h of all counters and with that, you will be able to see where come from your performance problem. It could be the processsors, the memory, the disks or one process. Be carefull, the report files generated by the analyser performances can be very big, think to split them during the analyse. 

By default the routing is not activated. To activate routing on a linux you must add this line: net.ipv4.ip_forward = 1 In this file: /etc/sysctl.conf If you want to test you can activate the routing with this command: sysctl -w net.ipv4.ip_forward=1 

You could see some commands which are already created in nsclient.ini in the form of alias. You could look at the help of each command to specify which options you want to see in your supervision. Finally, you can create your own script and add it in the nsclient.ini 

In somes cases, from a windows client in a domain, you must specify the machine in your login if the machine is not in the same domain. Ex: Like that, you force the authentication with the credentials stored in your debian box. 

You can use directly the command snmpget instead check_snmp because check_snmp will use snmpget (or other tool of netsnmp tools) and works on the result stored in a variable. I paste here the skeleton of a script I use for that : 

You can use to log the executed process on your server. You can add a new collector and select all the counters for your process. When your process with weird activity starts it will be logged and you will be able to see the time when it starts and ends and which is his parent process. Moreover, you will be able to see all infos for this process. Be carefull, to select in yours counters the option to save the process which will be created after your perfmon. 

You can start a scheduled task when a event from windows event log appears. In Hyper-V-worker\admin you have a log which describe the start or the restauration of a VM (see event log ID: 18596) You can select "On event" in your scheduled task for launching the task and select the event log which corresponds with your need. 

You can put your command in a script with an output on the console. Bacula will write this output in the log file of your job. Like you, i have a job which save dumps of databases. For that, I have a script which do the dump before the job and a script which delete the dump after the job. Example of my script which do the dump before the job (it's in powershell but the idea is the same): 

So you can export this key, edit it and apply yours modifications. When you are ready, you can plan a scheluded task which will run a last robocopy and import yours new keys the sunday. Like this, the next monday, you will have yours files up to date with corrects share and NTFS permissions. 

The thresholds can be passed by arguments or can be fixed directly in the script on your client. Nagios doesnt store any of these values. Nagios reads these values from the perfdatas sent in the results of the command. If the thresolds are passed by arguments you can modify them in the concerned service in your nagios conf. 

Until your initial replication will not finished, you won't be able to have a operational DFS. You can use before the initiale replication to accelerate the process. To verify the state of your replication, you can look at the backlogs, it's the files wich are waiting to be replicated, normally, you would not have more than 100 files in staying state : 

The problem is that most of the new "dumb" terminals like Wyse thin client terminals are actually quite expensive. It can be hard to persuade management to buy machines with reduced functionality compared to a desktop PC of almost equivalent price. 

We are thinking about purchasing 4 x EqualLogic PS6510X SANs (the Sumo boxes). Each has 48 x 600GB 10k SAS drives. They will be stacked to form a logical pool of storage (all in the same location). I understand that when you create a RAID group its done on a "per box" basis. So one box could be Raid 50, another Raid 10 etc. My question is, should I make one box a "performance" box ie Raid 10, and the other boxes "standard" ie Raid50? How do people configure their EQL arrays in the real world? 

In a single domain environment without Exchange you don't need a GC, even with multiple sites. GC has got nothing to do with locating domain controllers in this scenario. 

How big is your intranet? As an alternative you could try the free MS Search Server Express which we use on our intranet (2000 employees). There are some docs on the web comparing the two. 

For example, say I have a full backup done using robocopy. Can I then use rsync to replicate just the changes? Or will rsync do another full copy? I don't want that to happen because it's over a slow WAN link. 

What Windows backup software do you use? Most have Linux agents (Backup Exec, Arcserve etc). If you are using NTBackup then there is no Linux integration. We have a similar situation. We just have a cron job that runs on the Linux box and copies files to a Windows server using Samba. From there the Windows backup software backs it up. Yes it's not very "Windows-ey". For that you really need a Linux agent which will have the smarts and integrate into the main backup console. 

Do you have a "delete first ask questions later" policy for music and movie files? Or are you more lenient? Just wondering if there's a difference between how large and small organizations deal with the issue. 

If you have I'm interested in the user experience and admin experience. Did users prefer Outlook over Notes? Did the sysadmins prefer Exchange over Domino? If so why? 

My client has a Mac at home and wants to remote control (run apps) on a Windows XP PC at the office. What is the best software to enable this? Any Mac <-> Windows weirdness to be aware of? 

Yes stsadm is fine, it will backup the database contents and site. You should also do a SQL backup of the backend db just in case. We have a scheduled task that runs stsadm daily and dumps the backup to a folder. From there our backup software slurps it up and puts it on tape. EDIT - it appears my advice above is only good for small (<15GB) sites according to Technet. For bigger sites they recommend not using stsadm. This is news to me too, so I'd better read the link! 

Easiest method is to run newsid.exe from Sysinternals on each distributed VM. It changes the SID (and optionally renames the PC) so conflicts don't occur. It's best to create the VM while the guest is an a Workgroup. Then distribute the VM, run newsid.exe and then add the guest to the domain. Alternatively you can use Sysprep to prepare the VM, it achieves the same thing and is more of an automated process. 

You can try enabling the Traverse Folder/Execute File permission (need to go to Advanced display in the file security properties). At the same time disable read permission. I haven't confirmed this, but give it a go and let us know if it works. 

Backups are one thing, but long term archival is another. For example, you might be required to store emails for 7 years, or keep all project data indefinitely. I used to save archives to tape, but then I've had tapes get destroyed (drives rip the tape out). So...write to 2 tapes I hear you say. Is that what others do? Have 2 (or more) tapes of the same data for redundancy? But then the other issue is that tapes cannot usually be read by different backup software vendors. Eg if you go from Arcserve -> Backup Exec -> Commvault over 10 years you would need to keep all 3 systems so that you could restore old data. Likewise for hardware. Old tapes might not be barcoded. Might not be compatible with the new library etc etc. So do you keep old tape hardware AND old software just in case you might need to restore a 10 year-old file? Or...when you move to a new backup system do you migrate all archived data to the new system and re-archive it onto new tapes? That could be a huge job. Any thoughts? 

In bacula, there is the option to declare the client to put the saved files : $URL$ By default, if you have only the option declared in your job, it will define the source and the destination of the restore job. If you want to plan a restoration, you can add the option in your job declaration. 

Maybe, your FTP traffic is NATted and not your SFTP traffic ? Can you verify on your router if the ports of your server are correctly natted to outside ? Can you join logs of the fail of the SFTP connction ? 

If you can mount and open shared folders but you can't see any files, it's certainly permissions problems. Did you do modifcations on that ? 

1. Testing NRPE connection You can test your connection between your windows and Nagios via NRPE like this : 

The certificate of your server is not trusted by a AC which is recognized by your browser. You can add the AC certificate which has emitted the certificate of your kibana in your browser like this, it depends of your browser. 

On your VM, you could have a text file which contains the declaration of yours hosts. This file is in the configuration directory of nagios, or you can do a search with (host declaration) When you will find this file, you will be able to delete all the text in the brackets 

After, don't forget to restart your service. And test with to be sure your new port is correctly opened. 

When you receive a SNMP trap, the daemon snmptrapd will forward this trap to an external program (typicaly your script). snmptrapd will add some args (IP address, hostname, notification, OID) which will be sent in the standard input of your script. An example of script which receive the trap, this script is from $URL$ 

You should install wireshark on your server and use tshark to save the traffic in pcap files with a rotating rule. For example : you will capture during 1 hour and create some pcap files with a size of 1 Mo 

You can test your check_snmp_int.pl directly in a console and see if you have datas after the pipe. Moreover, you must activate the perfdata in icinga.conf and declare your broker. 

You could use the VMSnapshot applets. Example for retrieve the snappshots of a VM on HYPERV server : 

the users GPO are applied during the opening of a session and the computer GPO are applied during boot. Moreover, The GPO are periodicaly updated by your system. If you want to follow the delay during these updates, you can look at yours events logs here : In this log you will read the next update for yours computer's and user's GPO with the log . The log will show you if the download of the GPO is or not succeded. And the will show you the applicables GPO like a . For the script which execute a gpupdate, it's not necessary. The computer updates his GPO during boot. So you will execute ypurs GPO two times. 

In your you specify : With that, will send a ping to each entry in . Moreover, you will ping all your domain for each member of it with the . So if you have a big domain it can explain your problem of quota. You can trie with : 

If you have a mount, this part is ok. If you don't have a share, it's not necessary to test with psexec, it will fail. Moreover, be sure your ports are well forwarded, and yours firewalls are correctly configured for the port 135. 

I've never used graphite but from the tutorials on the web, it seems pnp works with graphite. pnp4nagios will wait icinga sends the perfdatas from the results of the checks which are in the right part of the pipe in the response. It's for that, you must activate the perfdata in the conf of icinga and explain to icinga2 which broker it will use for treating the perfdatas. When pnp4nagios receive the datas, it write them in files stored by default in . You will have a directory per host, and in it, you will two files for each service a xml and a rrd file. 

You can specify the port number directly in the command but you will must create one command per tested port. Or you can create a custom variable to store the port number , with your example: 

If you want to deactivate the notifications to all yours deveices. You can modify directly yours templates with . And for yours hosts with notifications you put le propriety in his configuration or create a specific template. 

To be sure of the synchronisation of your 2 servers and because you have under windows2012, you can use DFS for the sharing and DFS-R for the replication of your two servers. You could follow those simples steps for that : 1/ install and activate DFS replication (DFS-R) between the servers. When you are sure the initial replication is finished, all your files will be synchronised, and your user can be on any server, yours files will be the same. 2/ with DFS, you can create sharing which point to the two servers, with a priority on the new R:. 3/ You can migrate the users gradually. The old users point again to the ancient server. And the migrated users point to the DFS. 4/ When you finish to migrate all yours users, you can stop the replication, and delete the ancient server from your DFS. good luck ! Edit from my comment : If you want to save your share permissions and apply then to your new drive, you can work with the registry. Your share permissions are stored in the registry in this key :