So under this model Emma has fully funded the cost of the service, and Alex has "withdrawn" $50 from his account (though he only gets to keep 40 of it). Thus, Alex ends up with a -$10 account balance. Alex's account will become solvent again when he does more work, or pays his balance to the company. Is this a good idea, even though the reality is just that Emma handed Alex $50? 

I'm developing a new double-entry accounting system for my company, which has Customers and Workers. Customers pay the company for services rendered, and in turn the company pays Workers for providing these services (sort of an Uber model). The idea is to model an Account for each Customer, Worker, and the company itself. Each transaction therefore, is either a transfer of funds between two of these accounts, or a transfer of money in/out of the system as a whole. The latter case would be, for example, when a Customer makes a payment or when a Worker receives a paycheck. For example, consider the following accounts: 

Transaction 3 is essentially "Emma funding her account", while transaction 4 is "Alex withdrawing from his account". Any account's current balance can be derived by adding all of the amounts where their account appears in the , and subtracting all of the amounts where their account appears in the . Our "accounts receivable" would simply be the sum of all negative balances, and "accounts payable" would be the sum of all positive balances. This seems like a good approach, but it doesn't really reflect the actual flow of money. When Emma pays us from her credit card, from the banks' perspective there is no "Emma's internal account" - the funds are simply transferred directly from Emma's bank account to the Company's bank account. Is it a problem for us to model "Emma's internal account" anyway? There is also the possibility that Emma will pay $50 to Alex directly when services are rendered, and then Alex needs to pay the Company their share ($10). Under my proposed system I would model this as follows: 

I am designing a database which needs to store some fairly complex relationships among many different types of entities. In the simplest example, let us say I have three types of entities - "Student", "Tutor", and "Course", each with their own tables (, , ). First of all, I need to represent simple many-to-many relationships between each possible pair of entity types: 

This is nice because it explicitly lays out the full relationship in a single row. However, I now need to maintain this table in addition to my simple relationship tables. Option 2 - Meta-relationships Table : 

Negative account balances are allowed, but transaction amounts must always be positive. So at this point in time Emma has an account balance of -50, Alex has an account balance of 40, and the company has an account balance of 10. For the system as a whole to become solvent, Emma must deposit (make a payment) and Alex must withdraw (be given a paycheck). As these occur, we add additional transactions: 

Online MSDN article "Execute Statements Against Multiple Servers Simultaneously (SQL Server Management Studio)" (SQL Server 2012) states: 

Isn't it self-contradictory paragraph ("until" vs. "retained")? Then, if "the isolation level from the last SET TRANSACTION ISOLATION LEVEL statement is retained" after closing the connection and returning it to a pool, how it should be understood: 

I've tried to set Mater and Target Servers on various cobinations of SQL Servers 2012 R2 on Window Server 2008 R2 and Windows 7 machines but always getting one or 2 of the following errors during setting either target or master server: * 

Though, I removed any CMS (Central Management Server) instance and still getting the same results over multiple SQL Servers on 2 differing machines 

In SQL Server 2012 Enterprise Ed., which index types do not support online index rebuilds? and why? and which activity-operations can prevent online index rebuilds? 

What is the possible use of owner of an SQL Server Agent job, I wonder? Removed, and left it blank and the job still succeeds 

On Windows 7 Prof I have installed SQL Server 2012 R2 with, as I am pretty well remember, client tools. Then, I tried to install Visual Studio 2013 Ultimate but failed. Then I've installed VS2012 Prof (30-day trial). Now, on launching SQL Server 2012 (i.e. VS2010 Shell)) Data Tools --> New Project I am getting: 

while executing a rebuild from mentioned article and observe that it is being delayed until index rebuild finished What is blocking and how to avoid it? Update: Microsoft SQL Server 2012 (SP1) - 11.0.3128.0 (X64) Enterprise Evaluation Edition (64-bit) on Windows NT 6.1 (Build 7601: Service Pack 1) 

The enlist operation failed (reason: SQLServerAgent Error: Unable to connect to MSX 'Target-Server-Name'.) (Microsoft SQL Server, Error: 22026) For help, click: $URL$ 

I am having a maintenance plan configured with Windows Authentication being run by an SQL Server Agent job with owner 

I have a weekly and nightly SQL Server Agent jobs executing a weekly and nightly maintenance plans, resp. These weekly (on Sunday) and nightly (on other 6 nights of the week) maintenance plans differ only in the 1st task from total consecutive six steps. That is, in nightly/daily maintenance plan the 1st step is "Reorganize Indexes on all User databases" and in weekly/Sunday maintenance plan the first step is "Rebuild Indexes on All User Databases". Is it possible and how to run just one (instead of two) maintenance plan, all 7 nights of week, with condition that on 6 days the 1st step is one (Reorganize Indexes) and on the rest 1 day of week the 1st step is another (Rebuild Indexes) ? 

After installing Download SSDT for Visual Studio 2010 then Download SSDT for Visual Studio 2012 from Download SQL Server Data Tools - October 2013, everething seems started to work but I am eager to understand whether VS2012 setup damages SQL Server 2012 (i.e. VS2010 Shell) Data Tools for everybody or just me? 

Your key_buffer, innodb_buffer_pool and innodb_log_buffer together already exhausts the available memory in a a t2 instance. If you also consider the per thread allocations (>3M) * 70 (max connections) the possible usage is growing by another 210M minimum. You have 270MB InnoDb data. If you don't expect it to grow significantly over time almost 600MB buffer pool is overkill. Make it somewhere around 300MB or if you expect it to grow then adjust with care. The 256MB innodb_log_buffer is completely superfluous. It's in the same order of magnitude as your data. You're never going to write that much that needs to be buffered. Increase your innodb_log_file_size rather if you are maintaining a write heavy operation. Key_cache_size is also way overprovisioned. You don't need more than what the size of your myisam indexes. Drop your per thread buffers and improve as it is necessary. Without seeing the usage hard to say exact numbers but I would start with something like this: 

OOM is usually a result of bad my.cnf config and actually your my.cnf overcommits memory big time. Run mysqltuner.pl to get a sense of what parameters needs tweaking. Many of your configuration values are way high and some of them doesn't even make sense. For example: 1) binlog_cache_size 3Gb? default value is 32768 bytes(!) According to dev.mysql.org: 

The error itself is happening because there is a create view (or procedure) statement and the slave doesn't have the user who created it. In mysqldump you will have this format: 

MySQL is completely capable of serving as your full text search engine. InnoDB FTS indexes are reasonably good. With the size you described it should all fit in memory but of course that depends on the other tenants on the shared hosting. If it's not available (you mentioned shared hosting) you can implement your own pretty easily. Full text search is just basically inverted indexing. This way you can also have any custom trickery over it you want in your application which might not be possible with InnoDB FTS. You can see a basic implementation of this and performance + overhead comparison here: $URL$ 

If you want to directly pipe it to mail you can A) use the flag. The help is a bit confusing by saying: 

You didn't attach the table schemas so I just assume this would work. You might need to fully qualify the field names (using tablename.fieldname). Also feeds_processing_data could benefit from a composite index covering the involved fields. 

Workload Assuming InnoDB engine you want primary key to be a continuously incrementing id so INSERT won't be random causing too much page split and fragment the tablespace. Benefits of : 

There is an alternative method of splitting where your main table contains only the small, filtering and sorting columns (including these columns too) and "data" is stored separately. It's always best to try which works best for your dataset and query patterns. I did a benchmark about a year ago comparing one big table with two splitting strategy that can help you get started: $URL$ 

that is, update (but not read) operations require space for row versioning should have IMO failed due to lack of space. And why do read ops fail if they do not require version storage in tempdb ? 

incur limitations on the whole server, the instance of server or specific database created as contained one? Apparently, it is only to contained database but, then, why it should be enabled server-wide first? and what does it incur (for example, if I enable it on server but did not enable it on any particular database)? 

Seems like I am failing to find in online MSDN the illustration of the sense (or points) of an SQL Server Central Management Server. Can you provide me with some? 

Invalid class (System.Management) 3) Ensure the agent startup account for 'Master-Server-Name' has rights to login as a target server (if Success here, then 4) is error or both 3) and 4) are errors) 

ADDITIONAL INFORMATION: Failed to retrieve data for this request. (Microsoft.SqlServer.Management.Sdk.Sfc) For help, click: $URL$ 

Whom does Windows Authentication of a maintenance plan authenticate, for whom and for what - why is it needed and/or how can it be used? To which user I should give permissions for accessing backup (being stipulated in maintenance plan) dir location? And how does the owner of an SQL Server Agent job enter into all this considering that an owner has SQL Server authentication? 

The rebuild of indexes takes appr. the same 55 min. with both "Do not rebuild indexes and "Rebuild indexes offline" "For index types that do not support index rebuilds" with "Keep index online while reindexing" checked 

Fig.1.1. On launching SQL Server 2012 (i.e. VS2010 Shell)) Data Tools --> New Project and on pressing OK button 

In SQL Server 2012 R2 Enterprise, whenever I check Management\Data Collection, right-click --> Configure Management Data Warehouse , I see whatever users-accounts-logins except the one I am logged in I cannot understand what's the point in "Map Logins and Users" in Management\DataCollectiion of SQL Server? 

4) Enlist 'Target-Server-Name' into 'Master-Server-Name' (if Success here, then error in 3) or both in 3) and 4)) 

Also, a book on SQL Server 2012 I am reading tells me that in such situation I should see (to choose) "SQL Server Database project" template (which is absent in my case) When I launch VS2012 --> choose New Project , I see SQL Server Database project template 

In all cases I was using a domainUser login with sysadmin role for setting SQL Server Agent service and owner of an SQL Server Agent job (or, in the latter, tried also ), Windows Authentication to in used by them maintenance plans. The used domainUser is part of (local) Administrators group in each of the Windows (machines). The SQL Server Management Studio is run . How can I troubleshoot these errors? What should I check and look for? Specifically, I do not quite understand: 

It actually doesn't accept NULL values it considers it as empty string. That's because you have your server in non-strict mode. That controls how MySQL handles invalid or missing values in inserts and updates. You can read more about modes here: $URL$ 

You still need to wrap this into and make sure your mail is sent as proper html otherwise it will be only be creepier. As it was described by Baron Schwartz: $URL$ 

To keep all the state information with the current row to avoid conditions you can so something like: 

2) Populate your data and keep it up to date This can be done from your application, cronjob or trigger in mysql. Whichever you feel safe with. 3) Change your query to something like this 

Create a slave with xtrabackup (set master_info_repository to table if you have MySQL 5.6.2+) Have it replicated and let it catch up Stop the replication: (optional) Make a backup of master info file if you have MySQL older than 5.6.2 Make a note of the position with Make the mysqldump Destroy the database and all the files in and remove all the relay logs Restore the database with mysql_install_db and execute the dump (make sure you have file_per_table settings) (only on <5.6.2) If necessary restore replication (if it was in table than the dump contains the information so no need) 

Properly sized and configured servers with the right indexes can still serve queries from tables in the TB scale in the milliseconds range. If you do point queries (single row lookups with primary key or fully covering secondary keys) then you will find that the significant time is spent on parsing the query string and authenticate/authorise the user not the actual data fetching. 

Now, matching (same structure) is actually not mandatory if you only want to drop it but you need to have a valid frm file. If you do this you should be able to drop the table afterwards: 

That really depends on the size of your system and the level of automation you're using. What you can do with log_bin and log_slave_updates: 1. Quick failover It's not hard job to find the most up-to-date slave promote and determine the position on the other slaves and point them to the new promoted master. There are tools to help your there. Since MySQL 5.6 and the different flavours of it provides different implementation of GTID which makes it even less painful. Without binlogs being enabled a restart is required which (again depending on your setup) can take significant time (hours even). 2. Incremental backups and point in time recovery You can take a base backup of your slave with mysqldump, percona xtrabackup or any tool you prefer and make sure you don't run out of binlogs in case of a recovery you can just replay those logs. 3. Analytical capabilities In case of statement based replication you can process those binlogs and gather useful statistics about which tables are the most written to, updated etc. +1 Scalability Once you reach that point you will already have the tools and configs for it but it worth to be mentioned: you can move your slave around easily, change the topology whenever you feel so.