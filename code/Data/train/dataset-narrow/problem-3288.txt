Have a look at the example script in the "recording to a network stream" recipe in the picamera docs. The script at the end of that section listens on a network port and only starts recording video when someone connects. It shouldn't be terribly difficult to modify that recipe so that 1) it's more than a one-shot affair (in the recipe it streams for 60 seconds then quits), and 2) so that it only initializes the camera object after connection (the camera draws quite a bit of power even when its not recording, simply because things like auto-exposure and AWB are constantly running). 

I'm afraid this is most likely impractical. My (crude and thoroughly incomplete) understanding is that the Pi's camera module (which uses an OmniVision OV5647 sensor) is very closely tied to the camera firmware on the GPU. The camera's own ISP is largely ignored and the GPU itself performs the majority of post-processing (de-mosaic, AWB, AGC, etc). In this forum post jbeale (who knows a good deal more about these things than I do) goes into some of the complexities one would face in trying to make an interface-compatible camera: 

This is typical for a PiNoIR camera (or any camera with the IR filter removed): black appears as purple because although it's black to your eyes, the camera's still seeing a fair amount of red (from near IR, invisible to humans), and a little of blue (from near UV, which IR filters aka hot mirrors also filter). Here's a nice graph of CCD sensitivity, and hot mirror response which should explain the phenomenon far better than I ever could in text: 

Threads The alternative is to use threads to really do two things simultaneously. Conceptually this is a bit harder (you need to think about two control flows rather than a single linear one) but I think it's actually a lot simpler. I'll switch to GPIO Zero for this example just to keep things really easy: 

You can get to stream without saving by telling it to write to then redirecting that to something like netcat. There's some brief instructions on how to do this in the middle of the recipe "Recording to a Network Stream" in the picamera docs (just search for "raspivid" and you'll find it - it's a couple of lines). The pistreaming project mentioned in Shak7's answer was my attempt at providing a streaming solution that would work more or less universally with web-browsers by not relying on them supporting HTML5 video features (it uses Dominic Szablewski's JavaScript MPEG1 decoder to avoid such dependencies). Because of this, it isn't very good quality and you'll find streaming H.264 directly from raspivid or picamera much better (although obviously you need something capable of playing the stream at the other end, like VLC). 

However, you can't run more than a single process (simultaneously) that calls snapPhoto in this case, so it's a bit pointless to split off into a separate process. Generally, the cases involving picamera and multiprocessing use one process to control the camera and capture images, and then multiple other processes to perform computations on those images. Hence, the camera object doesn't wind up getting passed around, only the image data (which is absolutely fine). 

The script you've posted is starting an H.264 recording (which will be running at about 24fps) and then goes on to perform simultaneous JPEG captures and load the raw data into a numpy array (without decoding, so it's not exactly image data at this point). The JPEG captures aren't going to run at anywhere near 24fps partly because Python will be quite busy writing out H.264 data to the SD card in a background thread. However, even removing the background recording, I strongly doubt you'll be able to capture and process frames in OpenCV at 24fps (in Python, or for that matter in any language). Remember that the processing is happening on the CPU, and Pi's CPU is tiny compared to its GPU (where all the camera capture and encoding occurs). So, forget 24fps processing in OpenCV: it's not going to happen. However, you can manage a more realistic 1-2fps or so by avoiding the JPEG decoding and just giving OpenCV unencoded BGR data straight from the camera. The following script demonstrates capturing in this way and having OpenCV convert the resulting data to HSV before printing some average values (rather basic, but this is just for demo purposes): 

I've never seen this done myself, but there was some discussion a while back on the RPi camera forum about either trying to use the MMAL video splitter component with the camera's preview output, or attaching a renderer to both the video and preview ports. Quick high level overview of both raspivid and picamera: 

AFAIK, when the camera's not on (i.e. when the LED isn't lit, assuming it hasn't been disabled) it takes no power at all (in fact you can swap out cameras while the Pi is on as long as the camera isn't running at the time). How fast the camera can startup depends on several factors: assuming you want a decent exposure and white balance you need to wait a few frames after startup to give the AGC and AWB algorithms time to calculate appropriate values. If you start recording video or capturing frames immediately after camera startup you tend to get over/under exposed images with weird color balances. How long do you need to wait? How long is a frame? Depends on the mode you're running the camera in. For example, recently people have been experimenting with 6 second exposures (for night shots), but that requires running at extremely low framerates (1/6fps), so still captures actually wind up taking about 15 seconds because of the frames required for AWB. If you know the white balance gains you want to use in advance you might be able to shave some time off by setting them manually, and likewise for the exposure time the shutter-speed can be set manually. Basically camera startup itself is extremely quick (try hacking apart to stop immediately after it creates the camera component and configures it - it should only take a few tens of milliseconds), but the time you need to wait after that to actually get a decent capture will depend on several things including the conditions you're shooting in (might be a fraction of a second for daylight stuff, or 10+ seconds for long night exposures - incidentally this is why most of the examples in the picamera docs have a 2-second delay after initialization - I've found it's a delay which gives reasonable results in most circumstances - though it's probably overkill for many!) One other thing to bear in mind if you're running headless is that you can disable the HDMI port () to save a few mA - nothing like as much as the camera sucks down - but it's something. 

In other words, you're passing instead of and instead of . Python's truth test considers any non-empty string to be , ergo you're always setting vflip on. You need to either pass a "proper" bool to or alter to handle the conversion itself, e.g.: 

In my tests it usually gets close to a decent solution in 10 or so steps, and then wobbles around a couple of values. There's almost certainly betters ways of doing this (starting with more sensible values, varying one at a time, using YUV captures instead, decreasing the increments as the values converge, terminating when acceptably close, etc.) but this should be enough to demonstrate the principle. 

Only one process can access the camera module at a given time, so you couldn't have, say, feeding data to ffmpeg and simultaneously use to capture images. That said, the underlying firmware does support the concept of a "splitter" component which can be used to accomplish something similar (e.g. by connecting one port of the splitter to a video encoder, and another to an image encoder). An example of using this from python is given in this recipe. 

With this script I get about 2.5fps on my overclocked Pi. A little faster than 1-2fps, but then it's not really doing much! If you really need 24fps processing in OpenCV your only realistic option is to pipe image or video data from the Pi over a network to a more powerful machine and do the processing there. 

No need to constantly assigned the attribute (that'll actually cause a lot of work behind the scenes). Simply assign it once, then execute to leave the script waiting on termination (i.e. ): 

You'll want to add an section which describes where MediaTomb should look for media files. I tend to add this just before (it needs to be within the element). For example, assuming all your media is under (which might be an external USB hard drive) this segment of the config file should wind up looking like the following: 

Specify the framerate as the argument, and the frame size as the argument (this is necessary as the raw video passing down the pipe has absolutely no metadata, not even picture size, so the target avconv instance needs to be told). The resulting file should play back happily at 60fps in VLC. One final thing to bear in mind is that raspivid is a demo application. It is intended to introduce people to the C API (MMAL) for controlling the camera; it is not intended to be a complete solution for camera in and of itself. 

You initialize the camera with the PiCamera() line. This activates the sensor, sets the initial resolution to the display's resolution, and the framerate to 30fps. Immediately after this point the camera's gains will be 0 (which would result in black frames), but the AGC will start adjusting them upwards according to the scene. You set resolution to 640x480 - this causes the sensor to reset, which sets the gains back to their default (0) and AGC once again starts adjusting them. You wait 30 seconds - this will give the AGC plenty of time to adjust the gains to reasonable values. You set to off which disables the AGC. From this point on the camera's gains will be fixed. You set framerate to 30. Firstly this is unnecessary (it's already 30). But a side effect of this is that setting the framerate (or the resolution) resets the sensor, and once again the gains are back to 0. Unfortunately, the AGC is now disabled so they won't float back up and you'll just get black frames. The rest of the script continues from here... 

The splitter has 4 ports and the resizer runs in the GPU, so you can have up to four simultaneous recordings at different resolutions going to different outputs, including subprocesses via pipes. However, the GPU only has so much "ooomph" - you can max it out without using all 4 ports depending on resolution and other factors. This recipe from picamera's examples shows the basic theory used above. 

I can't say I know anything about Spotify so I'll leave that part for someone else. For streaming audio to an Internet radio you'll need a DLNA server (most "Internet capable" radios that I'm aware of act as DLNA clients). A couple of decent choices are MediaTomb and miniDLNA; these are both packaged for Raspbian so installing them is trivial with apt. So, to install a DLNA server (I'll use MediaTomb here as it's what I'm most familiar with): 

Your problem is that you're wanting to look at input from two different things simultaneously, and one of your inputs (the call to to read from the keyboard, or more likely the barcode reader acting as a keyboard) is a blocking function meaning it'll halt execution of your program until it gets a response. So, walking through your code: 

In other words, if you're not sure check the API docs (I know they're long and boring, but generally a search of the attribute or method name will have the API reference at the top of the results). 

The picamera library permits unencoded video recording and image capture by specifying a format like or . See the start_recording method for details. That said, I'd be very surprised if you could record such a stream at 1080p to the SD card. For example, let's assume you've got a class 10 card (i.e. one capable of writing 10MB/s). At 1080p (1920x1080 at 30fps) with RGB (3 bytes per pixel) format you'll need to transfer: 1920 * 1080 * 3Bpp * 30fps = 177MB/s Even if you go with YUV which cuts pixel size down to 1.5 bytes per pixel you'll still need 89MB/s. Given those numbers it's unlikely you'd even be able to pipe frames over the LAN. The only way to get it close to 10MB/s is to cut down the resolution or reduce the framerate. In other words, you'd be much better off experimenting with an H.264 stream first to see if it's good enough (I'm always wary of claims that things won't be good enough without actual experience to back them up). Switching from an H.264 stream to an unencoded stream is trivial (at least on the picamera side), although you'd then need to tackle the question of bandwidth... 

You could easily extend this with another button to save and load recordings (the pickle or json modules in the standard library should be fine for that). A note on PWM implementations One final thing I should note: the standard pin driver that GPIO Zero uses is . This uses software PWM which means servos will jitter a bit. If you want much smoother servo control I'd recommend using the pigpio backend instead. This is pretty simple as of the latest version of Raspbian and GPIO Zero. Firstly, make sure the pigpio daemon is started, then tell GPIO Zero to use the PiGPIOPin factory (case sensitive, sorry! This'll probably change to just "pigpio", case insensitive, in the next version for simplicity) before starting your script: 

At this point I'll also shamelessly plug our GPIO Zero library by providing the equivalent code using its RGBLED class: 

So, given that the camera is always capturing frames even when we're not capturing images or recording videos, what actually happens when we elect to capture an image? We tell the firmware to activate capture, and the firmware waits for the next frame to complete before passing it back to us (actually, if you're capturing images from the still port instead of the video port there's a lot more that goes on including mode switches, but you're concerned with the video port so let's ignore that). Consider what this means for synchronization (your particular use case). The camera isn't "ready" to capture a frame at any particular point. It's already capturing a frame and when you ask for one it'll hand you the next complete one that becomes available. In order to synchronize the cameras' frames all the cameras would have to be initialized at exactly the same time, and then their internal clocks would have to run precisely in sync (the cameras have their own internal clock; they don't rely on the Pi's clock). Sadly, I don't think this really is a realistic prospect. If I recall correctly, the Pi compute module (which has 2 camera ports on-board and supports 2 camera modules simultaneously) uses some special calls in the firmware to get the 2 modules to use a single clock signal (I have no idea how this works at the hardware level but I assume it's using something specific to the compute module); I can't imagine how you'd do something similar across 4 Pis. Update: I should add that it is possible to do rough synchronization with some reasonable networking knowledge (e.g. UDP broadcast packets). In other words, it's possible to get all Pi's on a network to trigger a capture within a millisecond of each other (assuming a decent low latency network like Ethernet), but as described above that still won't guarantee that all the cameras will actually capture a frame at the same time; there'll be up to a frame's worth of lag (plus network latency) between the start times of the resulting captures. If that level of synchronization is enough for people, they may want to check out the compoundpi project which is another project I wrote on top of picamera for just this purpose.