First of all I would say restart is not a problem of solution and the better way is to found the offending process and why it's consuming high memory. Like mentioned above linux already have OOM mechanism to find the offending process and to kill it to release memory pressure Other way to find it out using Kdump,configure this parameter vm.panic_on_oom = 1(/etc/sysctl.conf),this will generate vmcore when system is going out of memory.You can find more info about it here $URL$ Also limits.conf has lot of limitation,better Solution is to use cgroups to restrict the memory utilization per process So in /etc/cgconfig.conf you can define the control group like this(here I am restricting my app to use only 256MB of memory) 

ifconfig command is depricated in rhel 7 and replaced with ip command.If you still need that command please install 

I am not sure if I understand your question correctly but first I would like to find out actual memory usage along with actual shared memory.Please use the below mentioned python script 

and you are absolutely correct 20:39:57.038706 open("/usr/local/hadoop/libexec/../conf/hadoop-env.sh", O_RDONLY) = 3 <0.000253> When I moved this file then its referring to file inside /bin 20:42:51.024234 open("/usr/local/hadoop/bin/../conf/hadoop-env.sh", O_RDONLY) = 3 <0.000332> Then I configured the latest version of hadoop 

I know you are looing for something installable via package manager but if you are looking for some simple script the below one do a trick for you 

I go through some docs where it's mentioned that we can run sql query select based on _id but my question is when we created this river only then this unique id is created and that is created on the Elasticsearch side so as per my understanding mysql has no knowledge about this.Please let me know if I am missing something So if I am writing sql satement like this 

As mentioned in above reply..checking updated package is the first debugging step as based on the log trace we can see kernel is doing it job i.e out of memory function is called when all the system memory is occupied including(RAM+Swap) and system will not resume normal operation till memory is freed. Other option you have is to configure kdump along with vm.panic_on_oom = 1(/etc/sysctl.conf),this will generate vmcore.You can find more info about it here $URL$ 

This is just a general overview and not covering everything.As long as nr_requests remains the queue_Depth,I/O will pass quickly.The issue starting arising when these requests exceeding the queue depth and the I/O start helding in scheduler layer. Looking at your graphs I would highly suggest 1: check the disk having high peaks 2: Try to change the value of nr_requests and queue_depth to see if it helps 3: Change the scheduler in your test environment(as your data here doesn't contain merge request(read/write)..so I cant comment) 

I think you can use audit for specific file/directory or you can write custom rule based on your requirement 

Same way you can look inside /proc/net/{tcp,udp} and look for tx_queue & rx_queue Same way you can use 

Now coming back to your question Is it safe to remove those? These packages are only for debugging purpose and they could cause performance degradation..So yes they are safe to remove and only need to install during debugging. 

pd (value obtained by c - value obtained by a)/60 --> This will give you how long vmtoolsd is in D state Also from OS side some changes you can made is to lower down the values of vm.dirty_ratio and dirty_background_ratio as this will flush the dirty pages quickly. But based on my experience I would say best approach is to disable vmtoolsd as its has known history of freezing filesystem and raise case with VMware in parallel to know there opinion. 

AFAIK I understand devices in dev mapper are created early in the boot process and /dev/dm-n are for internal use only Even in the output of fdisk I see those 2 devices 

I am new to AWS this might sound like a weird question.My question is when I try to create a new EBS volume and try to attach it with running EC2 instance,The connectivity between two(EC2 instance --> EBS volumes) is over Public IP or that connectivity is using Private IP.I understand that EBS provides just a block level storage but I want to understand how EC2 instance is going to access that storage. 

$URL$ Just to add,you cant kill D state process,D' state will not respond to signals in the case of processes in uninterruptible sleep, any signals sent while the task is blocked will be delivered to the process as soon as it returns to either the 'R' or the 'S' state (normally once IO or other kernel activity is complete). Expert from wiki 

Completely agree with Grant,Swap utilization is not always a performance issue but if Database is swapping you will see a performance issue but it all depend on the situation.Next time when your system is swapping can you please run this script to find out which process is swapping 

So as far I see most of the script is moved out from bin in the latest version but still I dont have answer to your question 

Also there are some mysql tuner script but please test it in test env before applying in production $URL$ Also now MySQL support hugepages which are not swappable and you will see performance improvement as the size of TLB reduce and your cache hit will increase $URL$ 

So if you check the last line of output it will show command executed is vim and with uid=0 which is root If you want to make these changes persistent across reboot,inside /etc/audit/audit.rules add the entry like this 

All command line utilities were in 'hadoop install dir'/bin directory. All hadoop commands are invoked by the bin/hadoop script 

and the message are logged in /var/log/mcelog For more info you can refer to there main website $URL$ 

So here in my system you can see 725376(pages)*4096=2971140096bytes/1024*1024=708 megabyte So this 708 megabyte of memory is used by application for sending and receiving data as well as utilized by my loopback interface.If at any stage this value reached no further socket can be made until this memory is released from the application which are holding socket open which you can determine using netstat -antulp. 

This shows lot of apache process in D state.The way linux calculate load average is the sum of running process(R)+ Process in uninterruptible state(D) You can easily find out these process using top command or you can use 

With kernel-debug* you means both kernel-debug as well kernel-debuginfo as there is a difference between these two package 

and then in /etc/cgrules.conf I can define that your app utilzation(in your case django can't exceed beyond 256)