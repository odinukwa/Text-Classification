I'd say we have no good reason to think BQP is in P/poly. We do have reasons to think that BQP is not in P/poly, but they're more-or-less identical to our reasons to think that BQP≠BPP. E.g., if BQP⊂P/poly then Factoring is in P/poly, which is enough to break lots of cryptography according to standard security definitions. Also, as you correctly point out, there's no quantum analogue of Adleman's trick---indeed, there's no way to "pull the quantumness out of a quantum algorithm," analogous to how one can pull the randomness out of a randomized algorithm. So I don't think anyone has a guess for what the P/poly advice for simulating a quantum computer should even consist of (any more than they have a guess, say, in the case of NP vs. P/poly). A final note: my work with Alex Arkhipov (and the independent work of Bremner-Jozsa-Shepherd), can easily be adapted to show that if QUANTUM-SAMPLING is in P/poly (OK, in "BPP-SAMPLING/poly"), then P#P⊂BPPNP/poly, and hence the polynomial hierarchy collapses---in this case, I think, to the fourth level. At present, though, we don't know how to adapt this sort of result from sampling problems to decision problems. 

In his celebrated paper "Conjugate Coding" (written around 1970), Stephen Wiesner proposed a scheme for quantum money that is unconditionally impossible to counterfeit, assuming that the issuing bank has access to a giant table of random numbers, and that banknotes can be brought back to the bank for verification. In Wiesner's scheme, each banknote consists of a classical "serial number" $s$, together with a quantum money state $|\psi_s\rangle$ consisting of $n$ unentangled qubits, each one either $$|0\rangle,\ |1\rangle,\ |+\rangle=(|0\rangle+|1\rangle)/\sqrt{2},\ \text{or}\ |-\rangle=(|0\rangle-|1\rangle)/\sqrt{2}.$$ The bank remembers a classical description of $|\psi_s\rangle$ for every $s$. And therefore, when $|\psi_s\rangle$ is brought back to the bank for verification, the bank can measure each qubit of $|\psi_s\rangle$ in the correct basis (either $\{|0\rangle,|1\rangle\}$ or ${|+\rangle,|-\rangle}$), and check that it gets the correct outcomes. On the other hand, because of the uncertainty relation (or alternatively, the No-Cloning Theorem), it's "intuitively obvious" that, if a counterfeiter who doesn't know the correct bases tries to copy $|\psi_s\rangle$, then the probability that both of the counterfeiter's output states pass the bank's verification test can be at most $c^n$, for some constant $c<1$. Furthermore, this should be true regardless of what strategy the counterfeiter uses, consistent with quantum mechanics (e.g., even if the counterfeiter uses fancy entangled measurements on $|\psi_s\rangle$). However, while writing a paper about other quantum money schemes, my coauthor and I realized that we'd never seen a rigorous proof of the above claim anywhere, or an explicit upper bound on $c$: neither in Wiesner's original paper nor in any later one. So, has such a proof (with an upper bound on $c$) been published? If not, then can one derive such a proof in a more-or-less straightforward way from (say) approximate versions of the No-Cloning Theorem, or results about the security of the BB84 quantum key distribution scheme? Update: In light of the discussion with Joe Fitzsimons below, I should clarify that I'm looking for more than just a reduction from the security of BB84. Rather, I'm looking for an explicit upper bound on the probability of successful counterfeiting (i.e., on $c$)---and ideally, also some understanding of what the optimal counterfeiting strategy looks like. I.e., does the optimal strategy simply measure each qubit of $|\psi_s\rangle$ independently, say in the basis $$\{ \cos(\pi/8)|0\rangle+\sin(\pi/8)|1\rangle, \sin(\pi/8)|0\rangle-\cos(\pi/8)|1\rangle \}?$$ Or is there an entangled counterfeiting strategy that does better? Update 2: Right now, the best counterfeiting strategies that I know are (a) the strategy above, and (b) the strategy that simply measures each qubit in the $\{|0\rangle,|1\rangle\}$ basis and "hopes for the best." Interestingly, both of these strategies turn out to achieve a success probability of (5/8)n. So, my conjecture of the moment is that (5/8)n might be the right answer. In any case, the fact that 5/8 is a lower bound on c rules out any security argument for Wiesner's scheme that's "too" simple (for example, any argument to the effect that there's nothing nontrivial that a counterfeiter can do, and therefore the right answer is c=1/2). Update 3: Nope, the right answer is (3/4)n! See the discussion thread below Abel Molina's answer. 

Here's my favorite analogy. Suppose I spent a decade publishing books and papers arguing that, contrary to theoretical computer science's dogma, the Church-Turing Thesis fails to capture all of computation, because Turing machines can't toast bread. Therefore, you need my revolutionary new model, the Toaster-Enhanced Turing Machine (TETM), which allows bread as a possible input and includes toasting it as a primitive operation. You might say: sure, I have a "point", but it's a totally uninteresting one. No one ever claimed that a Turing machine could handle every possible interaction with the external world, without first hooking it up to suitable peripherals. If you want a TM to toast bread, you need to connect it to a toaster; then the TM can easily handle the toaster's internal logic (unless this particular toaster requires solving the halting problem or something like that to determine how brown the bread should be!). In exactly the same way, if you want a TM to handle interactive communication, then you need to hook it up to suitable communication devices, as Neel discussed in his answer. In neither case are we saying anything that wouldn't have been obvious to Turing himself. So, I'd say the reason why there's been no "followup" to Wegner and Goldin's diatribes is that TCS has known how to model interactivity whenever needed, and has happily done so, since the very beginning of the field. Update (8/30): A related point is as follows. Does it ever give the critics pause that, here inside the Elite Church-Turing Ivory Tower (the ECTIT), the major research themes for the past two decades have included interactive proofs, multiparty cryptographic protocols, codes for interactive communication, asynchronous protocols for routing, consensus, rumor-spreading, leader-election, etc., and the price of anarchy in economic networks? If putting Turing's notion of computation at the center of the field makes it so hard to discuss interaction, how is it that so few of us have noticed? Another Update: To the people who keep banging the drum about higher-level formalisms being vastly more intuitive than TMs, and no one thinking in terms of TMs as a practical matter, let me ask an extremely simple question. What is it that lets all those high-level languages exist in the first place, that ensures they can always be compiled down to machine code? Could it be ... err ... THE CHURCH-TURING THESIS, the very same one you've been ragging on? To clarify, the Church-Turing Thesis is not the claim that "TURING MACHINEZ RULE!!" Rather, it's the claim that any reasonable programming language will be equivalent in expressive power to Turing machines -- and as a consequence, that you might as well think in terms of the higher-level languages if it's more convenient to do so. This, of course, was a radical new insight 60-75 years ago. Final Update: I've created a blog post for further discussion of this answer. 

Yes, it's really as simple as that. To make it rigorous, you should rewrite the whole proof adding superscripts of "A" all over the place. In practice, though, if people notice this issue at all, they'll usually just add a remark like "this result is easily seen to relativize." If people seem cavalier about this, it's because they've learned, from experience, that only certain techniques (such as arithmetization) can possibly cause a proof not to relativize. So if your proof doesn't use those techniques, then it relativizes. (A close analogy: suppose you prove a theorem about real numbers, but your proof never uses anything about the reals other than the fact that they're a field. Then it suffices to note that fact, to show that an analogous theorem must hold for complex numbers, p-adics, etc. There's no need to redo the proof.) The one situation where more discussion is necessary, is where it's not even obvious what it means to relativize your theorem. (E.g., what's the oracle access mechanism?) As Kaveh pointed out above, there's no well-defined mathematical operation of "relativizing" a complexity theorem, just like there's no well-defined mathematical operation of "complexifying" a theorem about real numbers. Notice that, in the latter case, it's not enough to replace every occurrence of R by C: you probably also need to replace x2 by |x|2 (in some places, not others!), and make other changes that are "obvious" to a mathematician but hard to list formally. Likewise, in complexity theory, it's usually obvious what it means to "relativize" a theorem (i.e., who should get access to A, and what does it mean for them to access it?), but in some cases it can be quite subtle. See here for more about this issue. Turning your question on its head, one could ask: Is there any example of a relativizing complexity theorem, for which it's significantly harder to prove that the theorem relativizes than that the theorem is true? Interestingly, I can't come up with a single indisputable example (though maybe someone else can)! Here's the best I can do: 

Your question is vague in several ways. By a "non-negative vector," do you mean a quantum pure state, all of whose amplitudes are nonnegative real numbers? (Or rather, a state that's equivalent to such a state under multiplication by a complex scalar, which of course is physically unobservable?) By a "quantum oracle," do you mean a unitary transformation that outputs ACCEPT when fed a non-negative state, and REJECT when fed any other state? With probability close to 1? If so, then it's immediate that no such oracle exists. Indeed, the probability that the oracle accepts is a continuous (quadratic) function of the amplitudes, but the amplitudes can be arbitrarily close to 0 while still being either positive or negative. So at the least, you'd need some lower bound on the absolute values of the amplitudes -- together, perhaps, with the assumption that the amplitudes are all real (or if complex, that their angles with each other are either 0 or else sufficiently large). If you impose these conditions, then the "non-negativity oracle" does exist for quantum states of dimension 2: there it's called the "Hadamard gate," or the "Deutsch-Jozsa algorithm"! On the other hand, because of quantum-mechanical linearity, one can prove that the non-negativity oracle won't exist for states of dimension 3 or greater -- or will exist but only with very large probability of error (approaching 1). 

The empty set The set generated by the NOT gate The set generated by NOTNOT (i.e., NOT gates applied to any 2 of the bits) The set generated by CNOT (i.e., the Controlled-NOT gate) The set generated by CNOTNOT (i.e., flip the 2nd and 3rd bits iff the 1st bit is 1) The set generated by CNOTNOT and NOT The set generated by the Fredkin (i.e., Controlled-SWAP) gate The set generated by Fredkin and CNOTNOT The set generated by Fredkin, CNOTNOT, and NOT The set of all transformations 

We'd like to identify any remaining families, and then prove that the classification is complete---but before we spend much time on it, we'd like to know whether anyone has done it before. 

Consider what I like to call the CONSISTENT GUESSING problem. Given as input a description of a Turing machine $M$: 

John, while your kind comments are appreciated, I confess that I don't understand how your question relates to the simple point I was making in the quoted remark. All I was saying was that we do know various separations between complexity classes, like P≠EXP, MAEXP⊄P/poly, NEXP⊄ACC, etc. So, if you believe that a particular separation, like P≠NP, is "too deep to be either proved or disproved in ZF set theory" (or whatever), then it seems to me that the burden falls on you to explain why you think that separation has to be independent of ZF, while other separations turned out not to be. For this argument to have force, I see no necessity for the other separations to have the particular form you specified. But to address your question anyway: well, the obvious challenge in answering is to find any complexity class A for which we can prove that A is strictly contained in NA, where NA is "the natural non-deterministic extension of A"! (Indeed, as Robin points out above, finding such an A is equivalent to answering your question as you've stated it.) And the only examples of such A's that I can think of are things like TIME(f(n)) (it was proved in the 1970s that TIME(f(n))≠NTIME(f(n)) for f(n)≤n log*n, since NTIME(f(n)) can simulate time slightly greater than f(n)). (An earlier version of this post claimed that was known for all f(n). Thanks to Ryan Williams for the correction!) So setting A=TIME(n) and B=NTIME(n) would indeed answer your question in the affirmative. A more "natural" example will probably need to await a breakthrough in complexity theory. [Endnote: I wish to clarify that I didn't give things portentous names like "The Shtetl Optimized this or that"---those were Sidles's elaborations!] 

Emanuele: Unfortunately, we don't know of any black-box problem capturing BQP as simple as the one you mentioned capturing BPP. Intuitively, this is because it's hard to talk about BQP without bringing in unitarity in one form or another. The ability to sum both positive and negative numbers is what makes BQP more powerful than BPP, but then unitarity is what makes BQP less powerful than #P! :-) Having said that, besides the Dawson et al. paper that Martin Schwarz linked to, you should definitely check out this and this by Janzing and Wocjan, which give "surprisingly classical-looking" promise problems that capture BQP. Also, let S ⊆ {0,1}n, and consider a Boolean function f:S→{0,1}. Then I have a conjecture from years ago which says that Q(f), the bounded-error quantum query complexity of f, is polynomially related to the minimum degree of a real polynomial p:Rn→R such that (i) p(x)∈[0,1] for all x∈{0,1}n, and (ii) |p(x)-f(x)| ≤ ε for all x∈S. If this conjecture holds, then an "approximate counting problem capturing BQP" would simply be to approximate the value of a polylog(n)-degree polynomial p:Rn→R, at a specified point on the Boolean cube, given that p is bounded everywhere on the Boolean cube. This might be about as close as one could get to an answer to your question. 

Addendum: In answer to a further question of the OP, I don't know of any techniques whatsoever for showing that, if you could prove a certain complexity conjecture (which you haven't yet), then your proof would necessarily relativize. Yes, as long as you restrict your "search for a proof" to relativizing techniques only, you can be sure that, if you ever succeed in finding a proof, then your proof will necessarily relativize. And in practice, that's often what people do (e.g., because they have certain ideas about what a proof would look like, and those ideas relativize). But I don't know of any way to guarantee, a priori, that by broadening your search to include non-relativizing techniques, you couldn't find a proof that had eluded you before. 

I found the above highly misleading: QC is not a "parallel model" in any conventional sense. In quantum mechanics, there's no direct communication between the "parallel processes"---only interference of amplitudes---but it's also easy to generate an exponential number of "parallel processes." (Indeed, one could think of every physical system in the universe as doing so as we speak!) In any case, whatever you think about the interpretation of quantum mechanics (or even its truth or falsehood), it's clear that it requires a separate discussion! Now, on to your (interesting) questions! No, I don't know of any convincing counterexample to the ECT other than quantum computing. In other words, if quantum mechanics had been false (in a way that still kept the universe more "digital" than "analog" at the Planck scale---see below), then the ECT as I understand it still wouldn't be "provable" (since it would still depend on empirical facts about what's efficiently computable in the physical world), but it would be a good working hypothesis. Randomization probably doesn't challenge the ECT as it's conventionally understood, because of the strong evidence today that P=BPP. (Though note that, if you're interested in settings other than language decision problems---for example, relational problems, decision trees, or communication complexity---then randomization provably can make a huge difference. And those settings are perfectly reasonable ones to talk about; they're just not the ones people typically have in mind when they discuss the ECT.) The other class of "counterexamples" to the ECT that's often brought up involves analog or "hyper" computing. My own view is that, on our best current understanding of physics, analog computing and hypercomputing cannot scale, and the reason why they can't, ironically, is quantum mechanics! In particular, while we don't yet have a quantum theory of gravity, what's known today suggests that there are fundamental obstacles to running more than about 1043 computation steps per second, or resolving distances smaller than about 10-33 cm. Finally, if you want to assume out of discussion anything that might be a plausible or interesting challenge to the ECT, and only allow serial, discrete, deterministic computation, then I agree with Dershowitz and Falkovich that the ECT holds! :-) But even there, it's hard to imagine a "formal proof" increasing my confidence in that statement -- the real issue, again, is just what we take words like "serial", "discrete", and "deterministic" to mean. As for your last question: