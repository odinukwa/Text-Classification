Caveat: I am not a Java programmer. I am an ActionScript programmer. The two languages are somewhat related, and basic design principles should carry across. At first glance, I don't see anything that would scare me away from hiring you if it were my decision. I would say that most of this code is better/cleaner than code I have seen from Computer Science graduates who have been programming 10 or more years. So, what follows are just some observations that I have after 15 years or so of developing in AS + a few other languages. 

Keep in mind, these are the type of suggestions I would offer to a very experienced developer, if I thought he/she was open to hearing it. I suspect that for most jobs, especially entry-level jobs, these points would be completely irrelevant. Note that if you take my suggestions that would have you deviating from the Slick2D example code, you might wind up shooting yourself in the foot with developers who feel it's better to stick to established norms for a variety of reasons, some of them good. I think your code shows solid competence, and I wouldn't hesitate to hire you if all the other parts of the interview and hiring process lined up. Good luck! 

as you are creating as a copy of ; any change to changes as well. Also, you can iterate through your unique values in instead of checking if each element is in . Sorry I could not be of more help. Let me know if anything is unclear. 

When performing a chi-squared test, one takes the square of the differences of the expected counts per bin and observed counts per bin, and divides these per-bin differences by the expected counts per bin, as seen in the formula below. 

Since the sum changes with each iteration through your dictionary values of , I don't know of a way to pre-compute the right-side sums before-hand. This would be a little easier if you could use external modules. That said, you might see a slight speed-up if you use more comprehensions. For example, you can get via . Also, use (not ) to check zero equality and use (not ) to check . Also, you are iterating through all possible combinations. In the case of using and , you know that and . So you know that your lower bound is cut-off at 76. Those are 76 permutations that you do not need to use. You can include a statement at the lower bound since you are incrementing downward (), but I would instead use this lower bound as the starting point and iterate by incrementing upward as you may find some other way to restrict the sums in the upper limit. Lastly, the use of globals and non-descriptive variable names makes it hard to edit the code. For example, instead of , instead of , etc. Also, you can pass into . I personally prefer iterating over lists instead of dictionaries since you can use zip. I don't have a full solution for your problem, but it may help as a start. 

Note that isn't strictly required (you're not assigning to it), but it's useful making it explicit as documentation. 

Grid.center - you already know what it is on initialisation, so create a normal variable for it and remove the @property version. This avoids calculating it several hundred thousand times. 

Beware of optimising things, trying to get tiny improvements. By running your program with varying optimisation settings, I got the following ratios (memcpy/memset): 

It reads very much as a Basic program, but it's a reasonable first(ish) python program. Steps to improve it: Turn your existing global code into a "main" function, and call it via 

Since we're all sharing methods, here's mine. Not as elegant as some, but may be considered simpler. 

Enumerate over the array/columns when displaying as you do in other places. This will stop you doing an list index that's repeated: 

I think you can use these to prove anything you like. If your program is running too slowly, your attention probably should be directed elsewhere. 

Because I was in the mood, I adapted your program to how I'd do it. It's a bit over-engineered and it's not 'there' yet, but should give you several ideas. 

I think the big thing here is to beware of external data. Assume that it could be garbage. For instance: 

However, I've heard that it is good practice to modify the bins such that the observed counts are above a threshold (typically 5, sometimes less) as a large number of bin counts below such a threshold can result in a bad fit (assuming minimized chi-squared). If the observed bin count is less than this threshold, then the bin is merged with the next bin. Assuming a distribution (such as a Gaussian) with a central peak, the next bin would be the next-right bin (i to i+1 bin) when the bins are left of the central peak while the next bin would be the next-left bin (i to i-1 bin) when the bins are to the right of the central peak. I've created an algorithm that I believe works and covers all-edge cases (assuming a single central peak). I was wondering how it could be improved in terms of speed/efficiency. I also feel like I am duplicating code using similar approaches in two while-loops; can this be averted? 

This can be rewritten using a helper function (or alternatively a for-loop), which may look something like this: 

Since I could not get to compute a derivative successfully, I wrote a script to compute it manually. Running the script below will output a plot of two functions and over the interval . 

may be better split up into two functions, one to create it and the other to read the contents from the source, even if the function is called from - and you need to be aware of the possibility that the allocation may fail. On the other hand, you may wish to consider just allocating a single block of size rows*cols of memory, and indexing it via rather than allocating several smaller blocks. For 'neatness', you should consider using instead of , just to give a (possibly) sensible initial value. could do with some layout changes, such that each row is on a separate line, and the corresponding cells line up under each other, something like (but would depend on the type of data expected). 

Which is just another version of ony's answer. Slightly simpler (maybe), but either version will need keeping in step. 

One of the points that should raise a query is the amount of repeated code - you have a duplicate for every if. This suggests creating a function to handle it instead. Personally, I would end up with something like the following: 

Disclaimer: I'm not too familiar with C#. The modified form has expressions that are a little too complex for me, but either would be ok. My only comment is that the initial comment line is screaming for some notice: 

Can the speed and/or accuracy of this algorithm be improved? Is it proper to use centered differences at interior points and one-sided differences at the boundaries? 

One alternative method I have yet to explore is using set intersection/unions to find the same indices, though I'm not sure if that would necessarily improve performance. I posted a similar example some time ago, though I later realized the code had bugs and could have been improved upon as an example. 

I have written a script that I believe works and covers all edge-cases. I am curious about ways to improve upon speed. While the given example below covers a multi-dimensional array of , my actual use case will be (where depends upon the number of data parameters being searched). Given individual arrays of data points, the goal is to combine them into a multi-dimensional array and find the columns in which all conditions are satisfied. If the same column of each row satisfies a given condition, the index that corresponds to that column is output; otherwise, an error is raised. I have included a small named because it has many other functions relevant in my main code, though I've only included the parts relevant to the goal in this question. 

and probably should be initialised to NULL as well. doesn't call for any previous loaded dlls. If Load should be called once only per , that should be detected. A more informative exception could be thrown. "...during attempted insertion of plugin xyz", for example. Beware of function name decoration, if that could be an issue. 

As an extra bonus, it seems likely that the loops now in will be common in your code, so that it's now usable elsewhere. 

Personally, option 2. In option 1, by breaking out of the loop, you have to look down until after the end to see what happens, just to see a return. In option 2, it's obvious what you want and when you get it. Although, I think the condition is a little complex. There are, of course, other options: 

I'd add another vote for "don't do this", you (or another) will suffer having to reread/debug it. I'd try something along the lines of: 

There's nothing intrinsically time-consuming here code-wise - I think the best approach is to make sure that your database tables are set up correctly with the required indices. Perhaps create a view on the db to push some work onto that rather than select in code? I am a little puzzled however by your middle_consumption function. The inner loop contents do something like: