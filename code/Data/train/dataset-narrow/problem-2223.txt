Did you try searching Google scholar for "induced cycle basis"? There is not much, but the following reference seems to be relevant. It characterizes the graphs for which the set of all induced cycles forms a cycle basis, which I think is more restrictive than your question (you are allowing a proper subset of these cycles in your basis). McKee, Terry A. (2000), Induced cycle structure and outerplanarity, Discrete Math. 223 (1–3): 387–392. In any case I believe the following approach will work for your problem. Separately, within each biconnected component, do the following steps: 

Let's try a similar counting argument to the one in the earlier version of my answer, more carefully. Given an input 0-1 matrix with q nonzeros, define a "solution" to be a permutation of the rows, a permutation of the columns, and the connected 0-1 matrix that one gets as output after performing the permutations. The important observation here is that there are at most $n^2 2^{5q}$ different output matrices possible: once we make one of $n^2$ choices for the position of one of the nonzeros, we can encode the rest of the output matrix in $5q$ bits by doing a preorder traversal of a spanning tree of the nonzeros and recording for each edge in the tree whether it goes to a leaf, whether it is the last edge from its parent, and what its direction is. So the number of solutions is in total at most $n^2 2^{5q} (n!)^2$. Now each solution works only for a single input, because we can reverse the permutations to get the input back from the output matrix. The number of inputs that have exactly $c$ nonzeros per row is $\binom{n}{c}^n$, and for $c$ constant this can be rewritten $\exp(cn\log n-O(n))$. But for $q=cn$ the number of solutions is $\exp(2n\log n+O(cn))$. For $c>2$, the inputs outnumber the solutions, so there is an unsolvable input. 

The edge sets whose deletion leaves a cluster graph can easily be described as a formula with one free variable (the edge set) in monadic second-order graph logic. Therefore, by an optimization version of Courcelle's theorem, the minimum cardinality set can be found in linear time for graphs of bounded treewidth, which obviously include the 2-trees. See e.g. Arnborg, Lagergren, and Seese, "Easy problems for tree-decomposable graphs", J. Algorithms 1991. 

I'm not sure about this specific method for achieving $O(n^2)$ time, but two different methods for performing Kruskal in $O(n^2)$ time are given in my paper "Fast hierarchical clustering and other applications of dynamic closest pairs" (SODA 1998, arXiv:cs.DS/9912014, and J. Experimental Algorithms 2000): 

The textbook proof requires less creative insight, but the uncommon proof introduces a technique that's very useful in other algorithm analysis e.g. for randomized incremental algorithms in computational geometry. 

It's the base rather than the exponent, but there's an $O(\varphi^k n^2)$ FPT time bound in "An Efficient Fixed Parameter Tractable Algorithm for 1-Sided Crossing Minimization", Vida Dujmovic, Sue Whitesides, Algorithmica 40:15–31, 2004. Also, it's a lower bound rather than an upper bound, but: "An $n^{1.618}$ lower bound on the time to simulate one queue or two pushdown stores by one tape", Paul M.B. Vitányi, Inf. Proc. Lett. 21:147–152, 1985. Finally, the one I was trying to find when I ran across those other two: the ham sandwich tree, a now-obsolete data structure in computational geometry for triangular range queries, has query time $O(n^{\log_2\varphi})\approx O(n^{0.695})$. So the golden ratio is properly in the exponent, but with a log rather than as itself. The data structure is a hierarchical partition of the plane into convex cells, with the overall structure of a binary tree, where each cell and its sibling in the tree are partitioned with a ham sandwich cut. The query time is determined by the recurrence $Q(n)=Q(\frac{n}{2})+Q(\frac{n}{4})+O(\log n)$, which has the above solution. It's described (with a more boring name) by "Halfplanar range search in linear space and $O(n^{0.695})$ query time", Herbert Edelsbrunner, Emo Welzl, Inf. Proc. Lett. 23:289–293, 1986. 

There is an inapproximability result for coloring bounded degree graphs in Khot's FOCS'01 paper, "Improved Inapproximability Results for MaxClique, Chromatic Number and Approximate Graph Coloring" — it's probably weaker than you want, but at least it's in the right direction. He proves that, for a parameter $k$ (assumed to be constant), and for $k$-chromatic graphs with degree $2^{k^{O(\log k)}}$, it is NP-hard to find colorings that use $\exp((\log k)^2/25)$ colors. So in terms of the degree $d$, it is hard to color within an $O(\log d)$ factor, but the same inapproximability ratio is also a superpolynomial function of the chromatic number. 

I think that by any reasonable standard an n × n × n three-dimensional grid graph would have to be considered sparse, and that rules out most candidate definitions involving surface embeddings or minors. (Sublinear treewidth would still be possible, though.) My current favorite sparsity measure is degeneracy. The degeneracy of a graph is the minimum, over all linear orderings of the vertices of the graph, of the maximum outdegree in the directed acyclic orientation of the graph formed by orienting each edge from earlier to later vertices in the ordering. Equivalently, it's the maximum, over all subgraphs, of the minimum degree in the subgraph. So for instance planar graphs have degeneracy five because any subgraph of a planar graph has a vertex of degree at most five. Degeneracy is easy to calculate in linear time, and the linear ordering that comes from the definition is useful in algorithms. Degeneracy is within a constant factor of some other standard measures including arboricity, thickness, and the maximum average degree of any subgraph, but those are I think harder to use. 

No. The illustration shows a graph (the graph of a cube with one corner truncated) and a valid removal sequence such that the vertices left at the point when the minimum degree equals $|S|-1$ (the two red vertices) do not form a maximal clique. 

Incidentally, if $T$ has no degree two vertices, then $G(T)$ is called a Halin graph; with or without the degree two restriction, it has bounded treewidth, so an alternative method for finding an optimal coloring in linear time is to use dynamic programming. However, this method does not tell you ahead of time which trees require only three colors. 

Think about your question in reverse: suppose you have a dynamic data structure for some problem — does that imply that you can solve it statically, faster by a log? Why should it? And in fact it is not true. Consider range counting in one-dimensional intervals, in a comparison model of computation. That is, the data is a set $S$ of numbers, the query is an interval $[\ell,r]$, and the answer to a query is the size of $S\cap[\ell,r]$. Statically, you can solve it in $O(\log n)$ query time by storing $S$ as a sorted array and using binary search, in preprocessing time $P(n)=\Theta(n\log n)$. Dynamically, you can still solve it in $O(\log n)$ query time by using balanced binary search trees, with insertion time $O(\log n)=O(P(n)/n)$. 

It is known that it is NP-complete to test whether a Hamiltonian cycle exists in a 3-regular graph, even if it is planar (Garey, Johnson, and Tarjan, SIAM J. Comput. 1976) or bipartite (Akiyama, Nishizeki, and Saito, J. Inform. Proc. 1980) or to test whether a Hamiltonian cycle exists in a 4-regular graph, even when it is the graph formed by an arrangement of Jordan curves (Iwamoto and Toussaint, IPL 1994). For which other k is it known to be NP-complete to test Hamiltonicity of k-regular graphs? The particular case I am interested in is 6-regular graphs, with the additional condition that the graph have an odd number of vertices. If this case could be shown to be NP-complete (or polynomial) it would have impact in a graph drawing problem described in $URL$ . The "odd number of vertices" condition is because what I really want to know is, for 6-regular graphs, whether the graph contains either a Hamiltonian cycle or a bipartite 2-factor; but having an odd number of vertices eliminates the possibility of a bipartite 2-factor leaving only the possibility of a Hamiltonian cycle. 

What you're looking for is almost the same a robust central tendency: a way of reducing a cloud of data points to a single point, such that if many of the data points are close to some "ground truth" but the rest of them are arbitrarily far away, then your output will also be close to the ground truth. The "breakdown point" of such a method is the fraction of arbitrarily-bad outliers it can tolerate. The difference is that in your case you want to replace "close to" by "within the convex hull of". One way to capture this is with the notion of Tukey depth. A point has Tukey depth $p$ (with respect to a given set of $n$ data points) if every halfspace containing the given point also contains at least $pn$ data points. If there is a good convex subspace that you want to be inside, then a point with Tukey depth $p$ will be inside it as long as there are at least $(1-p)n$ of the data points inside it. So the breakdown point of this method is the largest value of $p$ that you can attain. Unfortunately this breakdown point is $1/(d+1)$, not close to 1/2, both for Tukey depth and for your problem. Here's why: if your data are clustered near the $d+1$ vertices of a simplex, then as long as fewer than $1/(d+1)$ fraction of them are outliers (but you don't know which ones) then any point in the simplex is safe to pick as it will always be within the convex hull of the non-outliers. But if more than $1/(d+1)$ of the points can be outliers, there is nowhere that is safe to pick: whichever point in the simplex you choose, the outliers could be all of the points from the nearest simplex vertex, and you'd be outside the hull of the non-outliers. If you're willing to tolerate a worse breakdown point, more like $O(1/d^2)$, there's a randomized method for finding a deep point that's polynomial in both $n$ and $d$: see my paper Approximating center points with iterated Radon points, K. Clarkson, D. Eppstein, G.L. Miller, C. Sturtivant, and S.-H. Teng, 9th ACM Symp. Comp. Geom., San Diego, 1993, pp. 91–98, Int. J. Comp. Geom. & Appl. 6 (3): 357–377, 1996, $URL$