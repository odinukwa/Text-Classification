Note also that I've taken and out of the joins and put them in a where clause. Don't include non-key columns in your joins, it's bad form. Note too that these columns are not indexed, so you will be potentially be causing a table scan with these. I suspect that even if they were indexed they wouldn't be selective enough and you might end up with a table scan anyway. 

HLGEM and BillThor both make excellent points. I would add that in addition to thinking about the stability of the key attributes and the efficiency of the key fields for index storage, there is one other factor to consider. There is a trade-off that could impact performance when you are looking at your primary key fields. Depending on how you define your key and how fast you add data, you might end up with a hot spot that slows you down. For example, if you use an auto-increment integer surrogate key, very high transaction rates can result in contention for the active page of data. This could limit the rate at which new data can be inserted. On the other hand, if you use a natural key that has widely distributed values, then you need to make sure that you use a fill factor that leaves enough space for inserts. If you have a fill factor of 100% then it effectively turns your whole table into a hot spot since the DBMS will have to move a bunch of rows to make room for an insert. 

You can use for your PK on the table if you like. That is probably the way that I would do it. You have a choice, however, to consider making the PK of the combination of and . This will be just as good, possibly slightly better, if you will have no other tables directly referencing the table. The potential advantage has to be considered in light of how you plan to manage the space for this table and how fast events will accumulate. Using as a clustering index could create a hotspot. On the other hand, using the compound key could create long delays in your inserts if you don't manage your page fills properly. You've mentioned that the model is simplified, but you haven't said in what ways. One thing that I would say is that the model as presented gives very little information about events. You know who did what (and when) - but the what is limited to a static description string. Other event logging systems I have seen (and built) include places to record specific details of the object of the event. If I were you I would ask myself questions like: 

Real world financial transactions do not follow simple assumptions such as "the client will make one payment per month". Make a table that records money coming in and money going out along with the details of that transaction, such as where the money came from (which client), where the money went to (which loan), and importantly when this happened (the date). You might want to consider adding a transaction type to provide extra information, with values like "loan", "payment", "adjustment" etc. 

Queries to Answer Resource Allocation Questions Using allocation tables with date ranges (instead of one record per day) makes answering some kinds of questions a little more complex. Still, it isn't hard to answer the same questions and the time you save maintaining daily rows will more than compensate for a little extra complexity in some of your queries. For example: What does the resource allocation look like today? 

You were more or less on the right track with (b), but you want to make sure that latitude and longitude are handled independently. What you need are indexes that cover your query parameters. This is what you are looking for: 

What you are proposing is a good solution for your requirement of M:N products to categories and hierarchical categories. To avoid exposing yourself to numerous updates: You need to do two things to ensure that you don't have a lot of updates in your intersection table. First, you need to be sure that your categories have a stable, persistent primary key. Second, you need to link food items to leaf categories. Don't join to , , and - just join it to and . Your nested sets take care of all of the secondary (and higher level) associations. 

When drawing ER diagrams, I have used the following graphical convention: Label the relationship lines with the foreign key column name(s), like so: 

Note that the way this works is you have to impose a single composer on an anthology by making the composer part of the anthology's primary key. You do the same thing with composition. Then, when you create an intersection between composition and anthology, you have the composer ID twice. You can then use a check constraint to declaratively enforce that compositions and anthologies have not only a single composer, but the same one. NOTE: I'm not saying you should do this, I'm just saying you could do this. YMMV etc. 

There are possible disadvantages too, which may or may not matter to you. Chief among these is having to have another database server. A lot of the advantages could be meaningless if you are using the same server to host the production and/or data warehouse databases. 

Don't worry about having too many transactions in your table. Relational Database Management Systems like MySQL are designed to handle amounts of data that would overwhelm other formats, like a spreadsheet for example. You can easily store millions of records in your table and MySQL will find you the ones you want quickly, especially if you define indexes on the fields that will be part of your clauses in your queries. For example, if you want to do monthly reporting, make sure you have an index on . 

Generally... A a rule of thumb: don't pre-optimize for performance. I think a lot of developers assume that joins are inefficient and they don't trust DBMS to do what it's built to do. Start with a properly normalized design. Make sure your indexes and queries are optimized for your particular balance of reads and writes. If and when you start to find that performance can't keep up with the best hardware you can afford, then start to think about denormalizing. If you denormalize early you are just setting yourself up for maintenance headaches down the road. More Specifically... Looking at your suggested table layouts, I would suggest that you're trying to make do too much. Anything which could appear in (e.g. ) almost certainly doesn't belong in . I suggest you adjust your thinking a little bit. I get the impression you are thinking very much about what topics and posts are going to look like on the page. This may be leading you to thinking of topics as a small super-set of posts, whereas they are probably rather more like subject headings. The fact that you plan on displaying the first post under each subject heading along with the heading isn't a good reason to co-mingle posts and headings. I think you may want to rethink some of your cumulative total columns too. Thinks like up and down votes may need to be tracked in their own tables. You may need to do this to keep people from up or down voting repeatedly and to allow people to rescind their votes. Similarly, you may want to know all editors, not just the last editor. 

The motivation for piling every kind of address into a single table is usually a misinterpretation and misapplication of the notion of code reuse. People can make the mistake of assuming that because you have two entities with some common set of attributes, that those attributes belong in their own table. Sometimes entities have similar or identical columns coincidentally. One wouldn't create a table for every or or in your database, at least I hope one wouldn't be tempted to do this. Some people mistakenly refer to all instances of removing columns out to their own table as normalisation. Normalisation involves removing columns to their own tables, but not every instance of removing columns in this way is actually normalisation. Normalisation prescribes very specific reasons for removing columns from a table. If none of these reasons are applicable then you aren't normalising you're just making things complicated. You have two correct ways of thinking about this: Either your addresses all belong in one pile because you have an entity super-type that incorporates all of the common features of several entity subtypes, including addresses, - or - your addresses belong in separate piles (tables) according to each kind of thing that has an address and you write your procedural code against an IAddress interface which is implemented for each address table. If you actually have an entity super-type, say which has subtypes like , , and so forth, then having an table that is a child of is a legitimate approach. It may even be a valuable approach if there is significant overlap between your customers, vendors, employees (or whatever you're tracking) because you can change addresses once instead of in multiple locations when a legal entity moves. On the other hand, if you don't have such a super-type, then you are going to face the problems pointed out by Richard Tallent. If you keep your addresses in different tables according to what type of entity owns the address, then you can still achieve code reuse, assuming the language you are using supports interfaces. As an aside: tvCa pointed out in a comment that addresses may be stored as columns rather than as rows in a separate table. That will depend largely on how many addresses you need for each entity. If you are tracking two addresses (physical, mailing) or if you are storing address history then go with an address table. If you only store one address per addressee, then a separate table is likely overkill.