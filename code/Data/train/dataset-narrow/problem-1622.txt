I think that it's often a matter of aesthetics. Personally, I would segregate things by virtual host: $URL$ $URL$ etc.. It does give you options for more easily relocating things to other hosts in the future, but you can always accomplish that after the fact by using redirects in Apache. Virtual hosts also let you partition off directory structures more tightly, which adds a little more security to a web server (which may not be as big a concern for you in an intranet setting). Vhosts are also easier to restrict access to during testing than a simple sub directory under the main site's directory. Again, this can be done the other way using simple .htaccess rules, but I prefer to have things logically (if not physically) separated as often as possible. 

You may want to take a look at Bacula. Run the server on the Windows machine, and run the client piece on the Linux server. Not the most simple package to set up, but it's a full-featured backup system, and it will do incremental backups, compression, and even encryption. You can tune it to the amount of space you have and then have it expire older file sets. 

There are many variations on how to accomplish this. I prefer to have one application dedicated to deciding on which server handles what kind of content, while the back end servers simply serve the files they've been requested. To that end, I employ the Varnish reverse proxy on the front end, listening on port 80. Behind that, I have Apache (port 8880) and nginx (port 8881), both configured for the same domains and pointing to the same directory structure. In my Varnish config file, I have something like this: 

I know that nginx can perform a lot of functions, but why not delegate each piece of the architecture to software that does one piece of it really well? Consider some, or all, of these pieces: pound or haproxy for the load balancing, varnish or squid for the reverse caching proxy, and having nginx and apache on the back end for static and dynamic content (respectively). That said, I'm not exactly sure what your question is. You've told nginx to pass all requests (I assume by "pass" you mean not caching them) to an apache back-end. Without caching, the benefit would be distributing the load across multiple apache servers on the back end. If you only have one back-end apache server, then you'll only get the benefit by caching the content, not just passing requests straight through. More details about your setup, and what you want to do, would help. 

Of course, there's a little more to it, but you get the idea. Since you already have Apache and nginx installed, you may want to browse this link, which describes a very simliar situation to yours, but uses nginx as the front end for static content and then passes on requests to Apache. If you want to go really simple, you could simply use a reverse caching proxy (such as Varnish or nginx) in front of Apache. What it will do is cache requests to quickly serve them out to clients, while at the same time relieving the web server itself from serving identical requests. This, by its very nature, will give the same effect as what you're asking for. Since static pages and images rarely change, they will be almost always cached by the front end, whereas dynamic pages will be detected as such and always passed on to the back end. 

I had a recent problem similar to this. After upgrading one of the packages on my 7.2 system, the gd-driven captcha on my phpBB2 installation stopped working. I re-built all of the php ports and it fixed itself. I know that's a bit vauge, but sometimes things will break over months of incremental upgrades due to dependencies getting out of whack. 

It's possible, depending on the print server, to create a custom filter to do this. This could be done in a standard UNIX lpd setup. The filter could scan the submitted file, look for hints that it's a PowerPoint document, and then pipe it through the likes of enscript or mpage and then pass it onto the printer. This, however, this is a nasty kludge that I'd never want to take on. I agree with others, in that per-page accounting an billing is the only reasonable solution to this problem. If people bear the true costs of printing, then they'll think twice before submitting such jobs. If you search "print server" and "by document type" you find some interesting products. For example, this product will police the printing of color documents by document type. So I assume that there are obscure off-the-shelf solutions, or you could contract such a vendor to provide the solution you desire. 

Utilize the "logger" command to let admins send one-liners to syslog. Since the user name is included, it makes for easy scripted reporting. If you're logging to a central host, you get these changes logged centrally as well. Simply appended all changes to /etc/motd, not only documenting the changes, but also displaying them to everyone when they log on. 

I use SSH keys, use the same one for all servers, and maintain a good password on my keyfile. Saves a lot of aggravation. For devices where this wouldn't work, I'd use a password that had a hard-to-guess core, then use the devices dns name, IP, or other common characteristic (such as OS or brand), to make the password unique for the device. This worked especially well for groups of similar devices. Just keep the pattern/mnemonic secret, along with the core, and you have a difficult, unique password that was easy to remember. 

A very lightweight solution is to utilize the services of $URL$ If you want total control, and have a spare machine to use as a server, the combination of Squid and SquidGuard is a pretty versatile solution. 

Not 100% certain about the first two questions, but there are lots of options for #3. I personally use CCleaner to scrub the cruft from the family PC automatically once a day. If you run it by hand, it should give you list of files found, depending on the function you're using. For #1 you could also try running "dir /s" while standing in that "Temporary Internet Files" from the command prompt. For #2 it could be from any software that does automatic updates (such as Adobe Acrobat viewer, Sun's Java, and MS's own Security Essentials and Automatic Updates). This is just a guess, as I'm not 100% certain where these apps store their downloads. 

How many cores in the box, and what is the actual load? What is the actual rate you're getting messages sent out? Like most, my first thought is disk, so check that. However, network utilization might be the cause, as may be high interrupt load (bad card?), so check those. I've found that even for a modest mail server, having a fast caching DNS server (I'm partial to "unbound") on the same box helps to alleviate latency and network load.