Encrypt it. It's really that simple. Since you're trying to discourage casual editing (rather than a dedicated hacker), the encryption algorithm could be fairly simple. There's no need for PGP or something. You could use ROT13. Or develop a substitution cypher of your own. 

It is there for the sake of completeness and consistency with the other APIs. There are vector forms of glVertex, glColor, glTexCoord, etc. glEdgeFlag is part of that group of functions; therefore, there is a vector version of glEdgeFlag. 

You can store your files as plain text in some arbitrary format, JSON, Lua script, XML, an arbitrary binary format, etc. And none of that will require that your in-memory representation of that data take on any particular form. It is the job of your level loading code to convert the file storage paradigm into your in-memory representation. For example, you say, "I see less redundancy of data using a database to store the tile data." If you want less redundancy, that's something your level loading code should look to. That's something your in-memory representation should be able to handle. It is not something your file format needs to be concerned with. And here's why: those files have to come from somewhere. Either you are going to be writing these by hand, or you are going to be using some kind of tool that creates and edits level data. If you're hand-writing them, then the most important thing you need is a format that is easy to read and modify. Data redundancy is not something your format even needs to consider, because you will be spending most of your time editing the files. Do you want to actually have to manually use whatever mechanisms you come up with to provide data redundancy? Wouldn't it be a better use of your time to just have your level loader handle it? If you have a tool that creates them, then the actual format only matters (at best) for readability's sake. You can put anything in these files. If you want to omit redundant data in the file, just design a format that can do so, and make your tool use that ability correctly. Your tilemap format can include RLE (run-length encoding) compression, duplication compression, what ever compression techniques you want, because it's your tilemap format. Problem solved. Relational Databases (RDB) exist to solve the problem of performing complex searches over large datasets that contain many fields of data. The only kind of search you ever do in a tile map is "get tile at position X, Y". Any array can handle that. Using a relational database to store and maintain tilemap data would be extreme overkill, and not really worth anything. Even if you build some compression into your in-memory representation, you'll still be able to beat RDBs easily in terms of performance and memory footprint. Yes, you will have to actually implement that compression. But if the in-memory data size is such a concern that you would consider an RDB, then you probably want to actually implement specific kinds of compression. You'll be faster than an RDB and lower in memory at the same time. 

Not all textures use texture coordinates that come from the mesh data. For example, with projective texturing, you transform the world into the space of the texture. Well, a lot of that world falls outside the [0, 1] range of the texture. Without some sort of clamping mode in place, you're going to get problems. 

You can't compute the "Z" of this plane because the plane has no Z. If the sphere is not directly in front of the camera, the plane will be tilted relative to the camera. So there isn't a single Z coordinate that will work. If you were using desktop OpenGL, I would suggest that you compute the plane equation and employ user-defined clip-planes. But that's not available in ES 2.0. So the only answer that's left is to compute the dot-product between the view direction and the normal at that point. So your vertex shader is going to have to calculate the normal (in camera-space), pass it to the fragment shader, and let the fragment shader do the dot product. If the normal is negative, discard the fragment. Computing the normal is a bit tricky. See, it's easy to compute it in model space, assuming your planet data set is centered at the origin in model space. You simply normalize the position of the vertex, since the vertex position in model space is a point on the sphere. The problem is transforming that into camera space. If you can guarantee that does not have any non-uniform scaling applied, then you can simply use that. But otherwise, you need to use the inverse-transpose of the model-view matrix. So you'd need to compute that and pass it to your vertex shader. 

You can even make a simple stub generator function (it would need to be more complex to find things that aren't direct members of the registrar): 

Quake 1 used BSPs, and it ran just fine on GPUs that are inferior in every way compared to anything that implements OpenGL ES 2.0. Also, what is not "decent occlusion culling" about BSPs or portal systems? You seem to have some misinformed ideas about what are good ideas and bad ones. 

More or less no. Only on MacOSX is the difference between 3.1+core and pre-3.0 versions really apparently. The compatibility profile is implemented by all drivers for Linux and Windows, so you can assume that the core profile from these drivers is really just adding checks to prevent you from calling compatibility functions. Under Mac OSX 10.7, GL 3.2 core is available, but not the compatibility profile. That doesn't necessarily mean anything for performance techniques on one versus the other. But it does mean that if there are differences, that is the platform you will see them on. 

Well, what is a shadow map? A shadow map is a texture who's texels answer a simple question: at what distance from the light, along the direction represented by the texel, is the light occluded? Texture coordinates are generated using various projective texturing means, depending on the particular shadow mapping algorithm. Projective texturing is simply a way of transforming an object into the space of the texture (and yes, I know that sounds backwards. But that's how it works). Shadow mapping algorithms use several different kinds of transforms. But ultimately, these are just transformations from one space into another. When rendering the shadow map, you take the vertices of your geometry and transform them though a standard rendering pipeline. But the camera and projection matrices are designed for your light position and direction, not for the view position and orientation. When doing forward rendering with a shadow map, you render the object as normal, transforming the vertices into the view camera space and through the viewing projection matrix. However, you also transform the vertices through your light camera and projection matrices, passing them as per-vertex data to the fragment shader. It uses them via projective texturing to access the shadow texture. Here's the important point. The projective texture access is designed such that the location it accesses on the texture represents the direction between that point on the surface (the point that you're rendering in the fragment shader) and the light. Therefore, it fetches the texel that represents the depth at which occlusion happens for the fragment being rendered. But there's nothing special about this pipeline. You don't have to transform the vertex positions into the shadow texture and pass those to the fragment shader. You could pass the world-space vertex positions to the fragment shader, and then have the fragment shader transform them into the projective space of the shadow texture. Granted, you'd be throwing away lots of performance, since you'll come up with the exact same texture coordinates. But it is mathematically viable. Indeed, you could pass the view camera-space vertex positions to the fragment shader. It could then transform them to world, then into light camera-space, then into the projective shadow texture space. You could put all of that transformation into one matrix (depending on your shadow projection algorithm). Again, this gives you exactly what you had before, so when forward rendering, there's no reason to do it. But in deferred rendering, you already have the view camera-space vertex positions. You have to, otherwise you can't do lighting. You either wasted a lot of memory and bandwidth by writing them to a buffer, or you were smart and recomputed them using the depth buffer and various math (which I won't go into here, but is covered online). Either way, you have view camera-space positions. And, as stated above, we can apply a matrix to transform them from view camera-space into shadow projective texture space. So... do that. Then access your shadow map. Problem solved. 

In terms of utility as a measurement for how good hardware is? Absolutely nothing Yes, it means thousands of triangles per second. But it means nothing in terms of how good hardware is. It's about as useful a performance metric as CPU clock-speeds: at best an order-of-magnitude approximation. These metrics are usually taken under ideal, benchmarking conditions, not real-world situations. In general, achieving these numbers in real applications will be impossible outside of the most ideal conditions. And what constitutes "ideal conditions" will change from hardware to hardware. 

Given the definitions you've outlined here, "global effect" means "context state" and "local effect" means "object state". Broadly speaking, no. OpenGL does not have a consistent naming scheme for what functions change object state and what functions change global state. This is primarily due to the fact that many of the functions that change object state were once only changing global state. Yes, even . Texture objects only came into existence in GL 1.1. The reason OpenGL requires you to bind objects to modify them is precisely for this: so that you can't tell the difference. This way, if they later decide to make some state "object state", you would use the same APIs to set that data, rather than having separate function calls for the "old way" and the "new way". This is better nowadays in core profile OpenGL, as most functions that modify object state are named in accord with the objects that they modify. All of the functions modify texture object state ( was removed from OpenGL in 3.1). All of the functions modify buffer object state. All of the functions modify framebuffer object state. But even here, there are exceptions. modifies program object state, but since it doesn't take the program as a parameter, it omits the word "Program" from the name (unlike or the more recent ). Futhermore, older calls like and modify framebuffer object state, despite not using the word "framebuffer". Again, that's for backwards compatibility reasons; the functions were all new functionality, but setting the read and draw buffers was old stuff, and thus had to use the old APIs. So generally speaking, no, there is no way to be certain just from looking at a function's name to know whether it modifies context state or object state. 

It has a CMake-based build system. This makes it much easier than most build systems for working on multiple platforms. This seems like a minor thing, but it's pretty important overall. It makes building the library much simpler. SFML is still in significant flux. 2.0 is a pretty substantial change. Whereas Allegro has already been through a significant change with version 5, and it's now all about stability. It covers modularity in a different way from SFML. With SFML, you ask for specific components, but you get all of them. If you ask for the graphics component, you get all of the graphics component. With Allegro 5, you always get the graphics component. But if you don't want to use Allegro's bitmap loaders, you don't have to; you can use your own. It has very good documentation. SFML uses Doxygen-style documentation, but even that is pretty Spartan. Whereas Allegro 5 is more comprehensive, with functions grouped together based on specific systems that they cover. It's not perfect, but it is better overall. They're smart enough to not try to do threading. With C++11/Boost.Threads either widely available or just around the corner, using another thread wrapper can be more than just problematic: it can be dangerous to getting your code working properly. 

That's unbinding your buffer object before the call. You need to have a buffer bound in order for that to be a reasonable call. It's the combination of having a buffer bound to and calling that sets up an association between a buffer object and a vertex attribute. Once you've called , then you can unbind the buffer. But not until then. Your other problem is this your element buffer setup code. 

There is a reason why and both take an array of strings. It's so that you can effectively insert code before your actual shader text or otherwise implement your own "include" system. The issue here is that you now need to break your shaders down into pieces: 

You probably couldn't implement 2nd edition. But WotC was kind enough to release 3rd and 3.5 edition under what they called the "Open Gaming License", which is what basically allows RPGs like Pathfinder to exist. Basically, you can probably use the 3.5 edition rules, but without any of the setting details. So no Beholders, etc, nothing that is part of D&D setting-wise. Now that doesn't guarantee legal cover, but Pathfinder is still around. If they're basically able to implement 3.5 (with modifications), you probably can too. Though the OGL may not cover videogames. In any case, if you're worried about it, it really doesn't take much to just make the system sufficiently different. Start with it as a base, lose some ability scores, make a couple of new ones, change some feats, invent a few tables. It's not that hard. Plus, you'll likely make a better game if you're not hidebound to some edition of table-top D&D. 

OpenGL itself is platform-netrual. In order to maintain that neutrality however, OpenGL expunged anything platform specific. Like creating a window. Or managing a window. Or creating OpenGL itself. Yes, the API does not explain how to create it. Only how to use it once created. The creating of an OpenGL context is something that is done using platform-specific APIs. On Windows, this means using the Win32 API to create and manage a window. On Linux, this requires using X11 or something similar. And so forth. Now, there are many, many platform-neutral wrappers around these APIs that allow you to create and manage an OpenGL window without touching platform-specific code directly. But that platform-specific code still exists; it's just hidden from sight.