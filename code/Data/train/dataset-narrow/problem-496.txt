And it creates there a schema named with a handful of tables prefixed by It's probably good enough to connect to the database and interpret the result of: 

Per comments, is set to for that role so it needs to be increased a bit to allow for several simultaneous connections. Choose a more reasonable value, say (or for unlimited) and issue as superuser: 

If done from a non-interactive program, dedicated functions can be used to send the data. They're provided by the PostgreSQL driver that the program uses. 

There's a misunderstanding here, because does not estimate, it runs the query for real and reports the actual time taken by each steps, as opposed to without that just reports the estimates without running the query. Consider this line from your EXPLAIN ANALYZE output: 

In order to not rely at all on , you might transform the INSERT queries in systematic way following this pattern: 

This is a close approximation to the number of bytes that will be retrieved client-side when executing: 

It's a case-sensitivity issue. including the quotes is a quoted identifier, which implies that it retains its case (as opposed to an unquoted identifier which is implicitly converted to lower case). Quoted identifiers allows other columns of the same table named for instance or or other variants that differ only by case (not that it would be a good idea). The drawback is that every subsequent SQL instruction refering to that column must use too, with the double quotes and the exact same case. In your UPDATE, the double quotes are lacking, which is why it errors out saying that does not exist (note how it differs from ). In practice, an identifier should be either always double-quoted, or never. A mix and match is not allowed. When an identifier is not quoted, it can be written in camel case if you fancy that style: the SQL parser will ignore the case. See Identifiers and keywords in PostgreSQL documentation for more. 

Crosstab queries with a large or unknown (dynamic) number of columns have the issue that you need to enumerate these columns in the query. There are some ways to avoid that: 

The JSON representation of in a trigger is just , so that part is straightforward. But writing into files on the server is a different story. It's only allowed to superusers because it gives the ability to corrupt or destroy all the instance's data. I don't think there's a builtin function to write on the filesystem, but the contrib module adminpack provides one: 

The quotes around fields and inside fields are part of the CSV format, and they're required here, because, according the CSV spec: 

It shows that the developers are aware of the behavior, even if the reason why it's like that originally is not specified, and its effect on were probably not intended. C-style comments don't have this problem. They're multi-line and nestable, so they're handled quite differently by the lexer. Comments inside functions are also no affected, since the function body as a whole is an opaque string for psql. 

This double connection depends on whether a password has to be submitted by the client (according to ) and, if yes, whether the option is passed to . In fact avoiding that second connection attempt is the only raison d'Ãªtre of this option. According to psql manpage: 

Still, concurrent transactions should be blocked by the program to work on the same "boxes", otherwise a trigger-based check could let incoherencies pass. This is because the trigger can't see the potential changes of other sessions that would not have yet committed when it runs. 

TLS between pgbouncer and server is not enabled through the connect string, but with , which is disabled by default. It should be set to at least , and also some of the other parameters might be needed to, depending on the TLS configuration at the other end. See $URL$ for all the details. 

where contains the EUR/USD exchange rate at the time of the last update. The idea is to increase the interval to take into account the maximum variation of every currency. The increase factors for both ends of the interval are constant between rates updates, so they could be pre-computed. Since the rates change only slightly over short periods of time, the above query is likely to give a close approximation of the final result. To get the final result, let's filter out the products for which the prices have slipped out of the bounds due to the changes in rates since the last update of : 

Identifiers are limited to 63 bytes by default, which doesn't qualify as "extremely long". Quote from the manual: 

If the other server is the master, it's normal because having different passwords in the replicated database is not possible. PostgreSQL replicates everything from the master, including user accounts and their passwords. There's no way to divert that temporarily. If someone needs to be superuser on the secondary without knowing this password, a different authentication method should be used on this server, such as Certificate Authentication, or external methods (ldap, radius...) . As a configuration file, the can be different on the slave. 

If you're refering to "New Database..." dialog box in pgadmin, the collation list does not seem to be fed from , but from the values of plus the hard-coded and . For example, I see 54 entries in my , all of them with , but in the pgadmin list I see only , , . At this point all my databases have and I doubt it's a coincidence. As a test if I run in a psql session, and restart pgadmin3, now the list has in addition to the rest. That name is not even present in , the closest entry being . As for the namespace of the collation being in or , it seems irrelevant. If we use with in the search patch, then the name of the collation is related to the schema, but so what? 

It's not possible currently (and probably not documented well enough, or not clear enough in the error message: when it says invalid locale name, it means from the set of locales provided by ). This is discussed in this thread on the developers mailing list: Can ICU be used for a database's default sort order? 

Or set it right at connection time with which would normally support it in the conninfo string (see Connection Strings in the manual) In your trigger, use to retrieve this information, instead of which doesn't do what you want, as already answered. There should also be a check in the trigger that this value is what you expect before storing it, since the trigger may be fired in any session, not just the ones initiated by PHP. 

I think that extensions don't do this by default in general to avoid having the number of operators growing quadratically with the number of types. See the installed in your extension directory for the operators and implicit casts defined with . 

The maximum number of client connections () is factored into the amount of shared memory that is pre-allocated at server start. In Managing Kernel Resources, the doc says that its size in bytes is: But memory is not the main factor in deciding the maximum number of connections, it's the raw power and number of cores of the machine, and wether a connection pooler is going to be associated to the server. The point is mostly to avoid too many concurrent active queries provoking the server to halt to a crawl. 

Consider implementing the check in a trigger, not a rule. The trigger is the perfect tool for the job in your case. On the other hand, it is a known fact that rules are touchy and much harder to master. 

is an error message from old versions of the PostgreSQL ODBC driver. You should go here: $URL$ to get the most recent version and update your system. Also there are in fact two driver flavors that get installed: and . Presumably you need if you decided to switch to . 

It seems you're looking for the function, knowing that it will return 0 if the byte is not contained in the string: 

It could be normal if there are newlines in certain text fields. Newlines are allowed when the value in the field is enclosed by double quotes. And obviously that makes the number of lines in the file greater than the number of records. Example : 

Ubuntu or Debian can run multiple instances of PostgreSQL and provide a specific way to autostart/stop/start each cluster. There should be a file named inside (or more generally /etc/postgresql/<version>/<clustername>) with these self-explanatory contents: 

To know what means for a schema, we must refer to GRANT in the doc, (in PG 9.2 there are no less than 14 forms of GRANT statements that apply to different things...). It appears that for a schema it means and . On the other hand, will grant and and , but in this context relates to schemas, not permanent tables. Regarding this error: , it happens when trying to create an object without schema qualification (as in ) while lacking the permission to create it in any schema of the . 

Be aware that there are two conflicting conventions for the sign, east of Greenwich or west of Greenwich. You want to check that the sign is what you're expecting, otherwise invert it. Another word of caution related to your edit: 

the documentation suggests that auto vacuum should kick in and be cleaning up these dead rows, yet clearly that was not happening. To know if autovacuum processes a table, look at . You seem to believe that autovacuum doesn't run because a manual shrinks the table and autovacuum doesn't. But that's normal, as autovacuum is not supposed to shrink tables, it only flags the space occupied by dead rows as reusable. 1) how an index gets bloat, and 2) how to prevent an index from getting bloat. Index bloat happens as soon as writes happen, it cannot be prevented. What needs to be checked is if the bloat grows indefinitely or stays stable and how bad it is. The postgres wiki provides a query for that: $URL$ 

Have a look at repmgr. It's a free software product that is meant to manage failovers and switchovers with PostgreSQL built-in replication. 

Besides , is workable in this context, it's just a matter of returning a scalar rather than a set. Your function slightly fixed to work would be: 

The formats accepted by include localized patterns that make it not immutable. Example of different results with the same input: 

The restore must copy from the archive directory (which typically is outside and entirely under the DBA's control) into (as expanded by ). I'm assuming that is your archive directory since I don't see what else it could be. Personally I'd place it outside of by principle but that's not mandatory. 

Also, the binary contents are sliced into tiny chunks of 2000 bytes in . There is one row per chunk, so when importing large contents (large by today's standards), the number of rows in this table tend to grow quickly. Although that does not imply a hard limit, users should be aware of that for performance reasons. 

When and are used together, the argument to is not the name of the database to create, it's the name of an existing database to connect to run the statement, because it's impossible to create a database if you're not already connect to another database. This is documented as: 

it means: this function name, with an empty list of arguments, does not exist. The presence of parentheses around nothing is relevant, because in postgresql, functions always go with their argument types: is not the same function than , or or . 

The idea behind the large object API is to mimic a file-like API. The OID is like the path of the file, and the file descriptor obtained by or is the equivalent of the POSIX and system calls for files. JDBC provides and the both libpq (in C) and the server have built-in functions. So yes it makes sense to replace LO contents (keeping the same OID) by a truncate followed by if libpq, or if server-side code, or if using the dedicated JDBC class, as you could do with a file on a filesystem. It would work as well to create a new LO with a new OID and then unlink the old one, it's just less elegant and it consumes a new OID for no good reason. 

As to what is logged, it can be configured with log_line_prefix but it doesn't go as far as logging tables and columns names. To log which columns are affected by UPDATEs you'd need to do that in triggers in every table, as in tablelog 

It mostly matters if you have specific sort requirements. In this other thread of the mailing-list: What users can do with custom ICU collations in Postgres 10 some concrete examples are given of ICU collations behaving specially compared to what libc can do. When not having specific requirements, I think most applications won't care whether they use an ICU collation or a libc collation, except for portability. One of the big points of is to get exactly the same ordering across all operating systems for a given locale, which is really not the case of . I guess that the pre-created would be fine for general English, and for West-european languages, I'm not sure why any collation would be better than . As a side-note, it's not a good idea to use any other collation than when you don't really care about the sort rules, because as a collation will outperform any language-aware collation. And is portable across operating systems. 

You could abuse the setting to pass the HTTP client IP from PHP to Postgres. In PHP, once connected: 

Use psql if WIN1252 is unusable with PgAdmin. Once these characters are removed, you'll be able to switch later to UTF8 client encoding. 

Now compare and with diff or cmp. I suspect you'll find that they are not identical. In which case it demonstrates that the index replica can't be used, since its build rules on the master differ from the scanning rules on the slave. 

With PostgreSQL 10 or better, variables are also expanded in backtick-executed strings, so we may also call an external evaluator through a shell: 

That's not sufficient. I think what is required to mark these rows as dead is that, when these transactions were started, there was no other transaction that had touched these rows (doing an UPDATE or DELETE on them). Updating or deleting a row will keep the previous version of the row physically where it was, and set its field to the TXID of the current transaction. From the point of view of other transactions, this old version of the row is still visible if it is part of their snapshot. Each snapshot has an and to which the and of the row versions can be compared. The point is that VACUUM must compare row versions against the combined visibility of all live snapshots, as opposed to simply checking if a row change is definitely committed. The latter is necessary but not sufficient to recycle the space used by the old version. For example, here's a sequence of events such that VACUUM can't clean up dead rows even though the transaction that modified them has finished: 

The nice thing with the shell is that it has no problem concatenating with ,whereas psql will not accept or (as far as I know) offer a simple alternative. 

If such languages are not available in your postgres environment, a non-efficient version can be made in pl/pgsql. It's non efficient because it has to create a temporary large object and copy the entire data into it before exporting it as a file, and then purge the large object. pl/pgsql version (inefficient) 

The extract from shows that for each output column of the result, executes two queries against the catalog, one to obtain the formatted name of the type from its , another to obtain a potential base type. In your extract, if I'm counting right, the total number of such queries appears to be . From the ping time of , let's assume that the minimum time for a round-trip to the server is . So the minimum time taken by this set of queries would be , or On the other hand, the command-line client doesn't do any of these queries, so it can be expected to have finished at least seconds before pgadmin. I'm not aware of any option in pgadmin to disable that, you may want to ask the developers or check if you have the newest version, but a question similar to yours was asked last year on the mailing-list, and apparently got no response: Long query results rendering (in pgadmin-support archives). It's a disappointing implementation detail in pgAdmin. It could get all results through a single query, with the list of types OIDs passed in an construct or an array. It could also cache these results and not even repeat queries on OIDs that it has already seen. 

To create a cursor in a plpgsql function that may be used outside of its "parent" transaction, it's just a matter of syntax. You want the SQL implementation of a cursor, not the plpgsql variant. For this, must be used. As an example, here's the skeleton of a function similar to yours, but using SQL-level cursors that outlive the transaction: