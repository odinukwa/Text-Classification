It seems that it is easy to build $V$ randomly (which would yield $|V|=O(2^{m+k}(m+k)\log n)$ by choosing uniformly distributed i.i.d. vectors). The $2$ at the base of the exponent can be improved if $m\neq k$ by setting each bit with probability $k/(m+k)$. Is this an optimal construction? 

(More of an extended comment - it's not readable as a comment). Are you sure a closed form for it exist? This is solvable by the following recurrence formula: $$P[m,k]=P[m-1,k-1]\cdot P_k + P[m-1,k]\cdot(1-P_{k+1})$$ Where $$P_k = \begin{cases} \frac{(N+1)^{k-2} (N + k )}{N^k} &\mbox{if } k \leq N \\ \ \ \ \ \ \ \ \ \ 0 & \mbox{else}\end{cases} $$ and $$ P[0,0]=1, \forall k\neq 0: P[0,k]=0$$ The answer you're looking for (the probability of $t$ marked elements) is $P[s,t]$. 

You have many $FPA$ (fixed parameter approximation) algorithms for which a sublinear parameter translates into subexponential time in the length of the input. For example, approximating the number of simple paths of length $k$, for some $k=n^c$ (where $c<1$), gives you a running time of: $O((2e)^{n^c}\cdot 2^{polylog(n)})$. 

(In fact, we only "identify" $a$ if $id=a$ at the end, and we can show that in such cases, $\mathbb E(C) \le n$, so we are not concerned with overestimation here). 

This problem is NP-complete. Here's a simple reduction from MAXCUT: Given an input graph $G$, we create a new graph $G'=G\cup K_{n^2}$ (i.w. add a separated clique over $n^2$ vertices). We can now set the weight to be $-1$ for the original graph edges, and $1$ for the edges of the clique. Claim: If $G$ has a max cut $(S,\bar S = V\setminus S)$ such that $|E\cap (S\times \bar S)| = k$, then $G'$ has a minimal "special cut-set" whose value is $|n^2-k-1|$. Proof: Since the special cut set's value is required to be positive, $S\cap K_{n^2}$ is neither empty or the entire clique. Since the contribution from the negatively weighted edges of $G$ less than $n^2-1$, we can conclude that $|S\cap K_{n^2}|\in \{1,n-1\}$, and it's contribution to the special cut value is $n^2-1$. Now the set intersection with $G$ has to be the max cut of $G$, as all of the weights are negative, which contributes another $k$ to the special cut value. 

This problem can be viewed as a generalization of summing over sliding windows in streaming data (e.g., see [1] and [2]). 

$f(x)$ has a root iff $h(x)$ has a root in [-1,1] (scaling, i.e. $h(x)=f(\alpha \cdot x)$). $h(x)$ has a root in [-1,1] iff $g(x)$ has a root in [0,1] (simply define $g(x)=h(\frac{x+1}{2})$). 

You are given a $n$-sized binary array. I want to show that no algorithm can do the following (or to be surprised and find out that such algorithms exist after all): 1) Pre-process the input array using unlimited time, but only using $O(n)$ bits. 2) Answer queries in constant time, where query $(x,y)$ asks for the number of set bits between index $x$ and index $y$ in the array. It seems that constant time per query should not allow the algorithm to read enough information to compute the number of set bits. 

Given David's bound it's unlikely you can do better worst case, but there are better output sensitive algorithms. Specifically, if $m$ in the number of medians in the result, we can solve the problem in time $O(n \log m + m \log n)$. To do this, replace the balanced binary tree with a balanced binary tree consisting of only those elements that were medians in the past, plus two Fibonacci heaps in between each pair of previous medians (one for each direction), plus counts so that we can locate which Fibonacci heap contains a particular element in the order. Don't bother ever deleting elements. When we insert a new element, we can update our data structure in $O(\log m)$ time. If the new counts indicate that the median is in one of the Fibonacci heaps, it takes an additional $O(\log n)$ to pull the new median out. This $O(\log n)$ charge occurs only once per median. If there was a clean way to delete elements without damaging the nice Fibonacci heap complexity, we'd get down to $O(n \log m + m \log k)$, but I'm not sure if this is possible. 

Is there an $O(n+m)$ algorithm to check whether a size $n$ regular expression matches a size $m$ string, assuming a fixed size alphabet if that matters? The standard NFA algorithm is $O(nm)$ worst case. Groz et al. achieve linear time for a variety of regular expression classes, but not all. Are there any better results? Groz, B., Maneth, S., & Staworko, S. (2012, May). Deterministic regular expressions in linear time. 

The prover P randomizes its coloring, turns it into a (salted) Merkle tree, and sends the root to the verifier V. V picks a random edge $e$ and sends it to P. P sends the Merkle tree paths from the root to each endpoint of $e$ to V. 

Say we want to understand a game tree search algorithm in a theoretical context. Thus, we want a parameterized family of problem instances, separate from actual games such as a chess, so that algorithms can be theoretically tested on small instances and then scaled up. The simplest example would be a random tree (or random DAG) with random values at leaves chosen i.i.d. from some distribution. Unfortunately, this model seems too simple to capture many interesting properties of game trees. In particular, in a domain such as chess or go the values at leaves are both (1) highly correlated to nearby leaves and (2) highly correlated to simpler heuristics available at interior nodes. A model that includes these correlations/heuristics would, for example, allow one to monitor the increasing strength of an agent as the depth of tree search increases. Question: Are there papers which construct this kind of theoretical model of game trees with correlations between leaf values, or between leaf and heuristics at interior nodes? 

In their famous paper from 2004, Flum and Grohe have developed the $\#W$ hierarchy, the counting equivalent of the $W$ hierarchy for parameterized problem. Among their results in the paper, they have shown that counting the number of $k$-paths is $\#W[1]$-complete, and conjectured that counting the number of matchings of size $k$ in a bipartite graph is hard as well. I'm unaware of any results addressing this conjecture, but was wondering about even a weaker result: 

This problem is NP-hard. Reduction from PARTITION: Given a set of numbers $S=\{x_1,\ldots,x_n\}$, construct the following flow network: $$V = \{s,v,t\}\cup \{x_1,\ldots,x_n\}$$ $$E = \{(s,x_i) | x_i\in S\} \cup \{(x_i,v)|x_i\in S\} \cup \{(v,t)\}$$ $$c((s,x_i))=c((x_i,v))=x_i\ \ \ c((v,t))=\frac{\sum_{i\in[n]}{x_i}}{2}$$ $S$ is partitionable iff there exist "saturate edge or avoid" flow of value $c((v,t))$. 

First, notice that the first objective is a minimization problem, who's solution is a vector $x$, while the second is merely a number. The objective $\min_{x} E\left( \parallel Ax-b \parallel_2^2 \right)$ asks for the vector $x$ which best explains the data. If $A$ is stochastic, it still looks for the best $x$ which, on average, is the best one. The lower objective asks for "What's the expected error of the best $x$, in hindsight ", i.e. after we fix $A$, take the minimum over all $x$'s, and ask what's the error. (more formally, for different $A$ values there are different "best" $x$s, so the each time take the best one for computing the error). Since we are usually interested in finding the feature vector $x$ which explains the data, and not the expected error of the best $x$ after $A$ is fixed, the first stochastic objective makes more sense. The only reason I can think of as why would someone be interested in the second is for competitive-analysis like proofs (i.e. what's the gap between the best $x$ prior to knowing $A$ and the best one afterwards). 

In a related work, Kalai et al. discusses the symmetric version of this problem, where both companies have the same power of attracting candidates. In this setting, the simple (symmetric) equilibrium is that you hire a secretary iff the chance of she being better than the remaining candidates is at least 50%. 

Related result: Recent result by Bläser and Curticapean shows that weighted counting of $k$-matching in bipartite graphs is $\#W[1]$-complete. 

The algorithm by Orlin you mention is quite old, and max flow algorithms came a long way since; In 1994, King et al. showed that max flow can be solved in $O(n\cdot m)$ whenever $m=\omega(n^{1+\epsilon})$ for some $\epsilon > 0$. This result was lately complemented by Orlin himself, which showed $O(nm)$ time complexity extends for $m=O(n^{\frac{16}{15}})$ (and also gave slight improvement for the $m=O(n)$ case). If you are interested in more practical algorithms you can look at the approximation algorithms for max flow, (see Mądry's paper for example), which is able to produce $O(\log n)$ approximation in near linear time. If you are also interested in single-unit capacities, Mądry recently gave an (exact) $\widetilde O(m^{\frac{10}{7}})$ for maximum cardinality bipartite matching. 

If randomness is in bounds, one rough idea would be to generate a bunch of "random monotonic signature" functions and use them to approximate the subset relation (a la Bloom filters). Unfortunately, I don't know how to make this into a practical algorithm, but here are some estimates that don't immediately prove the idea impossible. This is very far from a useful solution, but I'll write it out in case it helps. Assume for simplicity that the sets are all nearly the same size, say $|S| = s \pm O(1)$, and that $s = o(u)$. We can assume $1 \ll s$, otherwise we're done. Define $$\begin{aligned} q &= [s/2] \\ p &= \left[\frac{u \choose q}{s \choose q}\right] \end{aligned}$$ Note that $p \gg 1$. Here is the wildly impractical part. Randomly choose $p$ subsets $A_1, \ldots, A_p \subset U$ with replacement, each of size $q$, and define a function $f : 2^U \to \{0,1\}$ by $f(S) = 1$ iff $A_i \subset S$ for some $i$. With $S$ fixed and $A_i,f$ varying randomly, we have $$\begin{aligned} \Pr(f(S) = 0) &= \Pr(\forall i. A_i \not\subset S) \\ &= \Pr(A_1 \not\subset S)^p \\ &= \left(1 - {s \choose q}/{u \choose q}\right)^p \\ &= e^{-\Theta(1)} \end{aligned} $$ Since $f(S)$ is monotonic, $S \subset T$ implies $f(S) \le f(T)$. If $T \not\subset S$, fix some $t \in T-S$. The probability that $f$ detects $T \not\subset S$ is $$\begin{aligned} \Pr(f(S) = 0 < 1 = f(T)) &= \Pr(f(S) = 0) \Pr(f(T) = 1 | f(S) = 0) \\ &= e^{-\Theta(1)} \Pr(\exists i. A_i \subset T, A_i \cap T-S \ne 0 | f(S) = 0) \\ &= e^{-\Theta(1)} \Pr(\exists i. t \in A_i \subset T | f(S) = 0) \\ &\le e^{-\Theta(1)} \Pr(\exists i. t \in A_i \subset T) \\ &\approx e^{-\Theta(1)} p \Pr(t \in A_1 \subset T) \\ &\le e^{-\Theta(1)} p {s \choose q-1} / {u \choose q} \\ &\approx e^{-\Theta(1)} p \frac{q}{s-q} {s \choose q} / {u \choose q} \\ &= e^{-\Theta(1)} \end{aligned}$$ Some of those steps are pretty tenuous, but I don't have time to improve them tonight. In any case, if they all hold, then at least it's not clearly impossible to randomly generate signature functions that have reasonable likelihood of distinguishing subsets from nonsubsets. A logarithmic number of such functions would then distinguish all pairs correctly. If generating a signature function $f$ and computing $f(S)$ could be reduced to $\tilde{O}(n+u)$ time, the result would be an overall $\tilde{O}(n^2+u^2)$ algorithm. Even if the above calculations are correct, I have no idea how to generate monotonic signature functions with the desired features quickly. It's also likely that this technique doesn't extend to significantly different set sizes. 

Unfortunately, this algorithm is $\Omega(n^2)$ even if no unnecessary vertices are generated. If the $S_i$ form a convex polygon inside $T$, the BSP tree is degenerate regardless of the order in which segments are inserted. 

In the classic setting, an automaton for a language $L$ is required to accept all words in $L$ and reject/get stuck on every word in $\Sigma^*\setminus L$. All of the related concepts are then defined with respect to $\Sigma^*$ as the possible inputs (e.g. $L$'s regularity/CF/state complexity/ etc.). But we can think of a model where the possible input is some set $I\subseteq \Sigma^*$, and consider $L$'s attributes with respect to $I$. 

I'm looking for examples of hard optimization problems, for which we have an optimal approximation (not that this is not the same as $PTAS$, as we require a completely tight approximation, and not $1+\epsilon$-multiplicative approximation), in a sense defined as follows: Formally, let $L\subseteq\Sigma^*\times \mathbb N$ be the decision version of some minimization (or maximization, flipping the definitions) problem, i.e. $$\forall n\leq m:(w,n)\in L\implies (w,m)\in L$$ Which examples of such known languages $L$ is $NP$-hard, but if we are allowed of a minimal relaxation, it is poly time solvable. By minimal relaxation I mean that there exists a TM such that given an input $(w,n)$: 

These data structures use $n(1+o(1))$ bits and answer such queries in $O(1)$ time (we can get improved memory bounds if some bound $m<n$ is known on the set size). 

Consider the problem that receives two trees $T_1$, $T_2$, and asks to find a minimum size tree $T$ such that there exists a subtree of $T_1$ which is isomorphic to $T$, but there is no such isomorphic subtree in $T_2$. 

In general, assume you have some graph problem that includes a parameter$L\subset \mathcal{G}\times \mathbb{N}$, whose witness is a set of $k$ vertices/edges ($k$ is the parameter of the problem). Now we can define the set of problems which are "easy" to verify as: $EasyVer=\{L\subset \mathcal{G}\times \mathbb{N}| $ a witness $w$ of $L$'s instance can be verified in $poly(k)$ time$\}$, i.e. independent of $|V|,|E|$. It seems, for example, that $EasyVer \not\subset FPT$ and $FPT \not\subset EasyVer$, as 

A Non deterministic XOR automaton (NXA) fits your question. A NXA $M$ is essentially an NFA, but a word $w\in \Sigma^*$ is said to be in $L(M)$ if it is accepted by an odd number of paths (Xor relation) instead of being accepted if there exists an accepting path for it (Or relation). NXAs are used for creating small representations of regular languages as well as some parametrized algorithms. In a result from 2009, Vuillemin and Gama gave an efficient, $O(|Q|^3$) algorithm for NXA minimization, which can be used to answer whether $L(M_1)\subseteq L(M_2)$. 

In addition to the Heavy Hitters problem you've mentioned (which has quite a few algorithms: batch-decrement, space-saving, etc.), I'd consider presenting the following: 

Notice that the problem is NP-hard by a reduction from 3-coloring of planar graphs - Given a planar instance $G$ which we want to check for 3-colorability, create a vertex $v'$ for each vertex $v$, connect $v'$ with $v$, and force all of the new vertices to be colored in the 4th color.