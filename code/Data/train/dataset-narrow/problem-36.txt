One way to speed things up if you're doing this on a mobile GPU is to avoid indirect texture look-ups. This is where you calculate the texture coordinates then use the results of that calculation in the call to sample the texture. So it might be natural to write a 3-tap blur fragment shader doing something like this: 

You can put the translation first, and then the rotation, but have the translation happen in the direction of the cow's head. To do that, you need a vector from the center of the cow to its head. Then you need the length of the translation. You can then do the translation in that direction. Like this: 

In my opinion, yes, it does. I've heard it claimed that in reality, the ratio of the tallest mountain on Earth to the Earth's diameter is smaller than the variations in height of an apple's skin to its diameter. So I can't say that it would be realistic, but it certainly looks better than not doing it. At the least it gives you some shading between high and low spots, even with a simple directional light. 

Gravity is a form of acceleration. So what I've done in the past is continually add it into the velocity. On each frame, for each particle, I'd do something like this: 

You can sample the texture in your vertex shader and set the color appropriately. You don't say what system you're using, but for something like an OpenGL vertex shader, it would look something like this (this is a little old school, so it may need updating for modern GL) : 

D3D12 has 4 separate kinds of command lists: direct, bundle, compute, and copy. Vulkan has similar concepts, but they happen in a different way. Command buffers are allocated from command pools. And command pools are directly associated with a queue family. And command buffers allocated from a pool can only be submitted to the queue family for which the pool was built. Queue families specify the kinds of operations that the queue can take: graphics, compute, or memory copying operations. D3D12's command queues have a similar concept, but D3D12's command list API has you to specify the list type. Vulkan's gets this information from the queue family the pool is meant for. The D3D12 "bundle" command list type seems similar on the surface to Vulkan secondary command buffers. However, they are quite different. The principle difference is this: bundles inherit all state from their executing direct command list, except for the bound PSO itself. This includes resource descriptor bindings. Vulkan secondary command buffers inherit no state from their primary command buffer execution environment, except for secondary CBs that execute in a subpass of a renderpass instance (and queries). And those only inherit the current subpass state (and queries). This means you do different things with them, compared to D3D bundles. Bundles are sometimes used to modify descriptor tables and render stuff, on the assumption that the direct command list they're executed within will have set those tables up. So bundles are kind of like light-weight OpenGL display lists, only without all of the bad things those do. So the intent with bundles is that you build them once and keep them around. They're supposed to be small things. Vulkan secondary CBs are essential for threaded building of commands intended for a single render pass instance. This is because a render pass instance can only be created within a primary CB, so for optimal use of threads, there needs to be a way to create commands meant to execute in the same subpass in different threads. That's one of the main use-cases of secondary CBs. So the intent is that you'll probably build secondary CBs each frame (though you can reuse them if you want). So in the end, bundles and secondary CBs are intended to solve separate problems. Bundles are generally dependent on the executing environment, while secondary CBs are more stand-alone. At the same time, Vulkan secondary CBs can do something bundles cannot: they can execute on compute/copy-only queues. Since Vulkan makes a distinction between the primary/secondary level of the command buffer and the queues where that CB can be sent, it is possible in Vulkan to have secondary command buffers that execute on compute or copy-only queues. Direct3D 12 can't do that with bundles. The function can only be called on a direct command list. So a copy-only command list cannot execute bundles. Granted, because Vulkan doesn't inherit anything between secondary CBs except for subpass state, and compute/copy operations don't use render passes, there isn't much to be gained from putting such commands in a secondary CB rather than a primary one. 

I've done something similar. The way I did it was by using a height map image to change the positions of vertices in a grid in a vertex shader. I generated a 2D grid of vertices that were evenly spaced on the CPU. I uploaded the vertices and a texture and in the vertex shader, I sampled the texture and used the resulting brightness as the height. So I'd keep the x and y positions of each vertex and change the z based on the image. Now there are other ways you could utilize this to get something more like in the video. You could either have a 2D grid of single vertices, or even a grid of small 3D models. Change the z position based on the color of a height map. But additionally, you could upload a small noise texture and vary the x and y positions by a small amount based on the noise, which might vary per frame. I would see if you could vary the x and y by a large enough amount to see, but small enough that no point ever crossed over a neighbor point. That would get you some more movement and randomness. 

Line up the image of the scene in front of the corresponding geometry For each triangle in your geometry, project the 3 corners onto the image The coordinates where the projection hits your image are the texture coordinates you need for that triangle 

What this does is makes the context of current. It then enables the 2D texture unit and binds to it. Next it makes the context of current. If you were to check the enabled state of it would be false (unless you had previously enabled it on the context). Likewise, if you check which texture is bound to the 2D texture unit, you would likely find no texture was bound, or some texture other than was bound. Basically all drawing happens in the current context. If you make a different context current, then all drawing will now happen in that context. It is common to have a single context per thread in a multi-threaded OpenGL application. That way each thread is drawing into its own context and doesn't mess up the state of contexts on other threads. 

The difference is how well a mapping preserves locality and how easy it is to encode/decode the keys. The paper "Linear Clustering of Objects with Multiple Attributes" by H V Jagadish says: "Through algebraic analysis, and through computer simulation, we showed that under most circumstances, the Hilbert mapping performed as well as or better than the best of alternative mappings suggested in the literature". On the other hand, z-order is a bit simpler to use, for example compare the various methods listed in Bit Twiddling Hacks for z-order and Wikipedia for Hilbert-order. As for the applications, I think the main advantage in using space filling curves is that they map points from higher dimensional space to space of lower dimension. For example, they make it possible to window query for points using traditional B-tree database index. Again, on the other hand, the disadvantage is that one needs to know the bounds of the input in advance as it is difficult to "resize" the mapping later. PS: "Z-curve" is the same as "Morton code". PPS: Additional mappings include Peano curve and for applications see also Geohash. 

Radiosity does not account for specular reflections (i.e. it only handles diffuse reflections). Whitted's ray-tracing only considers glossy or diffuse reflection, possibly mirror-reflected. And finally, Kajiya's path-tracing is the most general one [2], handling any number of diffuse, glossy and specular reflections. So I think it depends on what you means by "ray-tracing": the technique developed by Whitted or any kind of "tracing rays"... Side-note: Heckbert [1] (or Shirley?) devised a classification of light scattering events which took place as the light traveled from the luminaire to the eye. In general it has the following form: 

One solution would be to use repeating texture wrapping, and then use a texture transformation matrix. So let's say you normally set the texture coordinates of your to be 0-1 in and . Instead, ensure that your and are set to or and set the texture coordinates to be the tile number to tile number + 1. So the first one would still be 0 to 1. The second one would be 1 to 2, the third would be 2 to 3, etc. Next, your vertex shader would have a uniform called or something like that. The texture matrix you pass in could have scaling in it (and probably other transformations, too). You just multiply the texture coordinates for the polygons by the texture matrix in the vertex shader, and it should apply appropriately. 

The usual way of doing this is to work in a color space that has luminance as one of the color components. For example the Y'CbCr color space has a luminance channel (you can think of this as similar to brightness which is a poorly defined term) and 2 chrominance or color channels. There are other color spaces that separate colors in this way, too, such as HSV, HLS, Lab, etc. There are conversions to go between RGB and these various color spaces. The article linked above shows some of the conversions for going between Y'CbCr and sRGB, for example. You don't say what type of graphics library you'll be using, so it's hard to give any more detail than what I've given you here. But I've done this in OpenGL, Core Graphics, and others. It shouldn't be too difficult. Edited to add: I should point out that HSV defines value or "brightness" not as luminance but as the average of red, green, and blue, and as such does not track very will with our intuitive sense of brightness. HLS also suffers from some issues (discontinuities, bad but popular reference implementations) and is generally not recommended by the color scientists that I know. 

How to draw a NURBS curve? Compared with Bezier curve, I just evaluate the Bernstein polynomial, multiply it with control point positions and that's it. Looking at the "General form of a NURBS curve" paragraph of NURBS Wikipedia page I have a hard time seeing a polynomial in it. Maybe this "basis function" is a polynomial in the end? Is there an efficient way of constructing the basis function and evaluating it? 

How to reliably find out whether two planar Bezier curves intersect? By "reliably" I mean the test will answer "yes" only when the curves intersect, and "no" only when they don't intersect. I don't need to know what parameters the intersection was found at. I also would like to use floating-point numbers in the implementation. I found several answers on StackOverflow which use the curves' bounding-boxes for the test: this is not what I'm after as such test may report intersection even if the curves don't intersect. The closest thing I found so far is the "bounding wedge" by Sederberg and Meyers but it "only" distinguishes between at-most-one and two-or-more intersection, whereas I want to know if there is at-most-zero and one-or-more intersections. 

Could anyone please explain to me how drawing works with respect to scrolling? Suppose there is a window, which has an area where one can draw to (a "canvas"). Are there two copies of this canvas? One for CPU and one for GPU? I mean, I know one could copy from main memory to graphic card memory very quickly with OpenGL (PBO) so one way to draw is first draw with software in main memory and the "blit" it to GPU, right? Now, I would assume that when the window scrolls there is some GPU functionality which would rapidly copy over the non-dirty parts in the appropriate positions without leaving GPU-land and then ask the CPU for redrawing the dirty/missing parts. But now I have two un-synced copies of the canvas! So how does it work? Do I do the scrolling in main memory in software and upload the whole canvas to GPU as the first time? Or perhaps I scroll the canvas in GPU and "download" then download it to CPU-land and "repair" it there? Or something else entirely? Perhaps somehow I don't have to care that the copies are unsynchronized? 

I don't know if this is exactly what you're looking for. I work tangentially in the film and TV industry. I don't work for a studio, but I work on the software that studios use for their productions. It's software that any user could buy if they wanted to, but it has been used in feature films, television shows, commercials, music videos, and more. About half of my time is spent programming and debugging. This is the fun stuff. It involves implementing rendering and filters of various types, sometimes based on papers I've read from SIGGRAPH or cool web sites or book. I don't do very much user interface work, though there is some. I work with OpenGL and Metal mostly. I need to know image processing algorithms in addition to general computer science algorithms, like graph processing and traversing, for example. This includes dealing with source control, continuous integration, stepping through code to find and fix bugs, talking to QA about problems, etc. The other half of my time is spent doing non-programming things. This is a variety of things like attending conferences to learn (things like the Game Developers Conference or SIGGRAPH), conventions to promote the company and/or product (things like NAB, IBC, etc.), answering tech support emails and writing sample code (we have an API so 3rd parties can extend our product), attending meetings (scrum, management updates, company-wide meetings, etc.), talking with marketing about how our new features work, meeting with customers to understand how they use the product, explaining to our documentation people how the product works so they can document it, etc. This seems like a pretty similar breakdown to other programming jobs in other industries from talking to my friends who work in other areas. 

You can map buffers for reading; invalidation is negatively useful for that. Also, you can map a buffer and only overwrite part of it. Invalidation is negatively useful for that too. has to be able to work outside of the narrow use case of mapping for overwriting an entire range. 

The vertex format for each attribute. The offset within the buffer for each attribute. The number of vertices you send in the command. 

Drawing back-to-front is not optional with blending (unless you use an order-independent blending technique), so getting the visual effect you actually want has to trump performance. But drawing front-to-back is purely a performance thing. Specifically, it makes it possible for early-Z to save you fragment processing time, as earlier objects are in front of later ones. So it's a GPU performance improvement. Batching is a performance improvement for the CPU, in that you're spending less CPU time doing state change stuff. So you need to decide which is more important: CPU performance or GPU performance. With deferred rendering, saving FS computation time also means saving memory bandwidth. But the general construction of your scene, what you're actually drawing, can help inform which is more important. Oftentimes what happens is that you render static terrain first, perhaps using your visibility system to order what you render coarsely front-to-back. Then you render your dynamic objects. Because dynamic objects usually require a lot of state changes, you won't necessarily draw all of them in a single batch (though there are techniques you can use to hide even texture changes). In any case, the point is that there is a tradeoff. And the one you pick depends on exactly what you're rendering. A top-down game like StarCraft or DotA would never bother with front-to-back sorting; you just render the terrain after the characters. A tight-quarters FPS by contrast would probably really need to employ such techniques, particularly for terrain. 

[1] Paul S. Heckbert. Adaptive radiosity textures for bidirectional ray tracing. SIGGRAPH Computer Graphics, Volume 24, Number 4, August 1990 [2] The Siggraph 2001 course "State of the Art in Monte Carlo Ray Tracing for Realistic Image Synthesis" says the following: "Distributed ray tracing and path tracing includes multiple bounces involving non-specular scattering such as . However, even these methods ignore paths of the form ; that is, multiple specular bounces from the light source as in a caustic." [3] Eric Veach. Robust Monte Carlo Methods for Light Transport Simulation. Ph.D. dissertation, Stanford University, December 1997 

The most common way I saw is to have photons of several different wavelengths. One then renders with each wavelength and blends the results into the final image. "Existing work": Psychopath Renderer and The Secret Life of Photons. 

I would like to rotate a raster image by an arbitrary angle. I don't really care for speed: the rotation should be of highest quality possible. Could someone please suggest a suitable algorithm? I'm familiar with the rotation by three shears but I'm not sure if the shears would not cause too much aliasing/blurring. 

And just to add: it is called "conflation" artifact and this is what AntiGrain Geometry used the compound shapes rasterizer for, see: flash_rasterizer.png $URL$ Also, this is what NV Path Rendering claims to improve on: An Introduction to NV_path_rendering (p. 67) or NV_path_rendering FAQ (#29). 

Splitting Bezier curve into two parts at some parameter is easy thanks to De Casteljau's algorithm. Is there a similar algorithm for NURBS curves? How to split a NURBS curve?