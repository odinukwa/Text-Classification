Are any of these data storage options designed for moderately big data i.e. data over a million observations? 

I have a data set which has continuous independent variables and a continuous dependent variable. To predict the dependent variable using the independent variables, I've run an ensemble of regression models and tried to compare them against each other. Here are the results for reference: 

I have a dataset that has continuous independent variables and a continuous dependent variable. The data has linear relationships, so a Linear Regression Model works perfectly well. However, I want to, for experimentation, model the data using a Decision Forest Regression and compare it against the Linear Regression output. I am using Azure Machine Learning Studio for this purpose. I get 30 decision trees constructed. Since we're dealing with continuous independent variables, each tree node/leaf is defined in terms of or . It looks like this: 

I'm working on detecting duplicate text documents using a classifier. I am looking for training data - a corpus of text documents and corresponding metadata which lists out pairs of duplicate documents from the corpus. Where can I find such data? 

I can interpret what the for each of those models means. However, I can't understand what the means. Especially, why is it Infinity for Linear Regression and Boosted Decision Tree, and a finite value for a Decision Forest Regression? Edit: Data Description: The data that went into these three models is all continuous independent variables and a continuous dependent variable. There are a total of 542 observations and 26 variables. These 542 variables are split 70 - 30 to get training and testing datasets. Therefore, the training dataset has 379 observations and 26 variables; the testing dataset has 163 observations and 26 variables. No missing data. 

Edit 2 Possible Explanation - (click here): Apparently, Linear Regression and Boosted Trees in Azure ML don't calculate the Negative Log-Likelihood metric - and that could be the reason that NLL is infinity or undefined in both cases. 

I've worked with SAS tables before, but they were never larger than a couple hundred thousand observations. 

I am trying to draw a process flow (like a template) to be followed while on text analysis projects. So far, I've come up with this. Text Analytics Steps 

I have a dataset that contains 20 predictor variables (both categorical and numeric) and one target variable (2 classes - Good and Bad). But, there are only 23 observations in the dataset. While I wait to receive significantly more observations, what tests / models can I perform on the available dataset to understand the variance between the good and bad cases, and to understand the variance within the cases classified as 'good'? Ideally, for the data to make sense, I would want the variance within the good cases to be low, and the variance between the good and bad cases to be high. Would multivariate analysis of variance (MANOVA) work in this case? 

If you have the purchase data for all these customers, this could be one way to approach the problem: You could either cluster the customers into natural groups based on their most recent purchases or find association rules (the likelihood of the customer purchasing in a particular category based on their most recent purchases). I'm not sure how well this approach is going to work for you, but it worked for me in my scenario of propensity scoring. So, if it makes sense, try to relate your data to mine. I had a list of customers and their purchase behaviors. From those purchase behaviors I deduced association rules to determine which customer is likely to purchase in what product category (based on their previous purchases- associated with the previous purchases of the group). You can also include recency and frequency of the product purchases into the model to decide whether or not to recommend a particular product/offer to the customer for a specific time. Based on the confidence, support and lift metrics of those rules for each customer, I mapped them to highly likely to purchase in this category with confidence being their propensity score. Let me know if this works with your data. 

As referenced in @E_net4 's comment, 1x1 convolutions are equivalent to fully connected layers that allow for feature expansion or contraction based on the number of filters used. Looking at the architecture diagram in the picture it looks like the 1x1 convolutions are in fact reducing the feature space by cutting the number of filters in half. I.e. looking at the the 3rd block in the image, the input is 56x56x256 but the first convolution is a 1x1x128 so assuming stride 1 the output would then be 56x56x128. 

Assuming the output size of your model is an integer (i.e. $\in\mathbb Z ^{[0,100]}$) a vector $\in \mathbb R^{100}$ should definitely be the output to your ANN. This is the most common approach to creating output classification for neural networks. The values of this output vector could be all over the place depending on how your weights are initialized/trained so in order to interpret the output as a probability of belonging to one of the given 100 classes (and sum to 1 as you say) you need to feed the output through a soft max function. You can then take the output of this to get your size class prediction and train using the cross entropy. If your output is continuous the easiest approach is just to have one output size, train on MSE, and get your answer by rounding between 0 and 100. 

Your problem is that you are not specifying the axis that you want to convert your tensor into a one hot tensor vector with so it's defaulting to looking at all elements at once, making logic_b of shape (2, 4, 8) when really what you want it to be is of shape (8, 2, 4). See below: 

A random forest will work, however standard regression will also work with categorical variables as predictors. You will have to "one-hot" encode your categorical predictors into 6 "dummy" variables (classes-1 = 7-1 = 6). The first dummy variable will encode 0/1 for whether or not the observation is class A, second dummy variable as 0/1 for class B, etc. You only need 6 dummy variables because if all of them are 0 for a given observation, that means the observation is in group 7 (G). In some languages, such as R, the regression command will automatically do this one-hot conversion for you. For python, the pandas package can do this for you with . 

You're dataset seems fairly small for a recommendation system so I am not sure how these approaches with scale but you are looking to solve a collaborative filtering problem which is essentially a sparse matrix completion. I popular and effective method for doing this is softImpute. For further resources here is a survey paper on Top-N Recommender Systems via Matrix Completion. 

This methodology seems a bit strange and potentially overkill for the problem. I would try having your input just be a 5 dimensional vector that is the difference between racer 1 and racer 2 and have the output just be the result for racer 1 (or what every racer's features are being subtracted from the other racer's features.). From here you can just use a few fully connected layers (which is essentially what you are doing be having kernel's of size 5 anyway. I would also try some more traditional machine learning algorithms with this type of 1D input. EDIT: since signs are important for the difference between your two racer features, do not use a relu as your activation function for this approach. 

PCA (unsupervised): this creates "new" linear combinations of your data where each proceding component explains as much variance in the data as possible. So the first 7 components (out of 27) should be able to explain a good percentage of the variation in your data. You can then plug these seven components into your logistic regression equation. The disadvantage here is that because the components are combinations of your original variables you lose some interpretability with your regression model. It should however produce very good accuracy. This same technique applied to other dimension reduction methods such as Another common method in regression is forward stepwise where you start with one variable and add on another each step, which is either kept or dropped based on some criteria (usually a BIC or AIC score). Backwards stepwise regression is the same thing but you start with all variables and remove one each time again based on some criteria. Based on a brief search it doesn't seem that python has a stepwise regression but they do a similar feature elimination algorithm described in this Data Science post. Lasso Regression uses an $L_{1}$ penalization norm that shrinks the coefficients of features effectively eliminating some of them.You can include this $L_1$ norm into your logistic regression model. It seems sklearn's LogisticRegression allows you do assign the penalization you want in order to achieve this. Note: Lasso will not explicitly set variable coefficients to zero, but will shrink them allowing you to select the 7 largest coefficients. 

Word of warning from a former airline Revenue Management analyst: you might be barking up the wrong tree with this approach. Apologies for the wall of text that follows, but this data is a lot more complex and noisy than might appear at first glance, so wanted to provide a short description of how it's generated; forewarned is forearmed. Airline fares have two components to them: all the actual fares (complete with fare rules and what have you) that an airline has available for a certain route, most of which are published the Airline Tariff Publishing Company (a few special-use ones are not, but those are the exception rather than the rule) and the actual inventory management performed by the airline on a day-to-day basis. Fares can be submitted to ATPCO four times a day, at set intervals, and when airlines do so, it will usually consist of a mixture of additions, deletions, and modifications of existing fares. When an airline initiates a pricing action (assuming their competitors aren't trying to make their own moves here), they usually have to wait until the next update to see if their competitors follow/respond. The converse goes when a competitor initiates a pricing action, as the airline has to wait until the next update before they can respond. Now, this is all well and good with respect to fares, but the problem is that, because this is all getting published in ATPCO, fares are the next best thing to public information... all your competitors get to see what you've got in your arsenal, so attempts to obfuscate are not unheard of, such as publishing fares that will never actually be assigned any inventory, listing all the fares as day-of-departure, etc. In many ways, the secret sauce comes down to the actual inventory allocation, i.e. how many seats on each flight will you be willing to sell for a given fare, and this information is not publicly available. You can get some glimpses by scraping web info, but the potential combinations of departure time/date and fare rules are quite numerous and may quickly escalate beyond your ability to easily keep track of. Typically an airline will only be willing to sell a handful of seats for a very low fare and the people who snag those have to book quite far in advance lest the fare rules lock them out, or other travelers simply beat them to the punch. The airline will be willing to sell a few more seats for a higher fare, and so on and so forth. They will be quite happy to sell all of the seats for the highest fare they've got published, but this is not usually feasible. What you're seeing with fares getting higher the closer you get to the day of departure is simply the natural process of having the cheap seats get booked farther out, while the remaining inventory gradually gets more expensive. Of course, there are some caveats here. The RM process is actively managed and human intervention is quite common as the RM team generally strives to meet its revenue goals and maximize revenue on each flight. As such, flights that fill up quickly may be "tightened up" by closing out low fares. Flights that are booking slowly may be "loosened up" by allocating more seats to lower fares. There is a constant interplay and competition between airlines in this area, but you are not very likely to capture the actual dynamics just from scraping fares. Don't get me wrong, we had such tools at our disposal, and, despite their limitations, they were quite valuable, but they were just one data source that fed into the decision-making process. You'd need access to the hundreds, if not thousands of operational decisions made by RM teams on a daily basis, as well as state-of-the-world information as they see it at the time. If you cannot find an airline partner to work with in order to get this data, you might need to consider alternate data sources. I'd recommend looking into getting access to O&D fare data from the Official Airline Guide (or one of their competitors) and try to use that for your analysis. It's sample-based (about 10% of all tickets sold) and aggregated at a higher level than would be ideal so careful route selection is imperative (I'd recommend something with plenty of airlines, flying non-stop multiple times a day, with large aircraft), but you may be able to get a better picture of what was actually sold (average fare) and how much of it was sold (load factor), vs. merely what is available for sale at a given point in time. Using that information you might be in better position to at least explore the outcomes of the airlines' pricing strategy, and make your inferences from there.