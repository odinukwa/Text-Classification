What if we have a prior that says that feeding a cat can only help its chances of making it to the top, and not hurt its chances? Well, that's an example of the first of two options I suggest in the last paragraph. However, it's still not informative enough to let us decide which cats to feed. The problem is that we need to tell whether feeding the cats your friend likes improves their odds more than it improves the odds of feeding the cats your friend doesn't like. We just don't have any information on that. The chances could be 70% vs 69% for cats he likes and 90% vs 20% for cats he doesn't (i.e., cats he like have a 70% chance if fed or a 69% chance if not fed, etc.); or it could be 70% vs 0% for cats he likes and 21% vs 20% for cats he doesn't. In the former case, you should feed the cats he doesn't like, as they see a 70 percentage point improvement from feeding, while the ones he likes see only a 1 p.p. improvement. In the latter case, you should feed the cats he does like, because they see a 70 p.p. improvement, while the ones he dislikes see only a 1 p.p. improvement. You just can't distinguish between these two possibilities from the dataset you have. So your prior would need to be considerably more detailed than that, before you can make any headway. If you'd like to learn more about the subject and under what circumstances we can draw conclusions about causality despite the lack of a controlled experiment, you might start by reading Judea Pearl's work. 

It depends on the meaning of the classes, and whether they have any meaningful order. If they are ordinal or a scale, then there is a meaningful ordering, and it can potentially be reasonable to order them and then assign labels $1, 2, \dots, n$ in order to the classes. This is what you call "LabelEncode". In some cases, some form of regression might also be appropriate and worth trying. If they are categorical or nominal, with no meaningful ordering, then LabelEncode makes no sense and will be no better than a one-hot encoding -- and potentially worse. So, the default is one-hot encoding. If they are categorical but have some hierarchy, there are smarter methods. You can build a tree that represents the hierarchy, then have a multi-hot encoding. If there are $n$ classes, you will have a tree with $n$ leaves and (say) $m$ nodes in total (where $m > n$). Then you can encode the class as a $m$-vector, whose $i$th entry is 1 if the class is a descendant of node $i$ in the tree, or 0 otherwise. If the classes are categorical but have various attributes, you can try an encoding that has one element for each possible attribute. For instance, if there are $k$ binary attributes, you can have a $k$-vector with a 0 or 1 in the $i$th entry according to whether the corresponding class has the $i$th attribute or not. You can generalize from there for more general situations. See $URL$ and $URL$ for more about different types of measurements. 

It sounds like you are looking for active learning. In active learning, the classifier learns which samples would be most useful to have labelled by a human. There are many techniques for active learning, and many ways to adapt an existing (standard) learning algorithm to the active learning setting. The particular approach you mentioned is called "uncertainty sampling", and can be applied to any standard classifier that outputs confidence/certainty scores. There are other selection methods as well, which may perform better in some settings. You can also apply unsupervised methods to cluster the samples, then label one or a few samples from each cluster. 

You could, but it won't be very effective. Image hashing is aimed at detecting two instances of almost the same image. So, if your training set contains an image of a dog, and the test set contains an almost-identical image, then using image hashing you could use that to learn the label of the test set image. But in practice that doesn't provide much generalization power. In practice, we want to take hundreds of different images of dogs, and use that to learn to recognize new images of docs, so that if in the test set we are given a totally new image of a dog, we can still classify it as a dog. Image hashing won't help with that. In other words, image hashing isn't designed for this sort of thing and will work poorly. 

Since apparently each feature is encoding something about two different categories, I would suggest that you should replace that with two features. Your two features would be $(x,y)$ where $x$ is 0 or 1 according to whether it exists in the first category, and $y$ is 0 or 1 according to whether it exists in the second category. In other words, instead of -1, 0, and 1, you would use $(1,0)$, $(0,0)$, and $(0,1)$, respectively. I think that is closer to the true data and might give better results. It might also make your results easier to interpret. Then you can try both distance metrics and see which you find more helpful -- but it might not make a large difference. 

You can obtain one negative example by taking one of the 3000 customer records and pairing it with any user record that is known not to match. In this way, you can obtain $3000$ positives and $3000 \times 5999$ negatives. You could then train a boolean classifier on this entire training set. This might work better than using one-class classification on just the positives. Even better might be to use techniques for learning to rank. If $c$ is a customer record that is known to match a user record $u$, and $u'$ is any other user record (which $c$ doesn't match), then you want your classifier to rank the pair $(c,u)$ higher than $(c,u')$. In this way you can obtain $3000 \times 5999$ such ranking-pairs, and try to train a classifier to learn to rank, then use that to find the best match for each of the 1000 customer records. 

P.S. Even if you could hide the outputs and reveal only the ultimate classification (i.e., hide the continuous probability values from the softmax and just reveal the highest-probability class), that's probably still not enough. Revealing the class is still enough for an adversary to label a bunch of instances, create a training set, and then train their own network. 

Side note: My understanding is that the Adam optimizer generally is more effective than plain SGD, though I don't see any reason to expect that to be the issue here. 

Define $Z_i = 1[Y_i>0]$, i.e., $Z_i = 1$ if $Y_i > 0$, else $Z_i = -1$. Now the data set $(X_i,Z_i)$ defines a two-class boolean classification problem. The solution to that boolean classification problem with highest accuracy solves your original problem (minimizes your loss function). So, you are basically asking how to train a neural network to solve a two-class boolean classification problem. A reasonable approach is to put a softmax layer at the output of the neural network and train it using the cross-entropy loss, as usual. During test time, to pick a class, you pick whichever of the two classes has the higher likelihood output from the softmax stage. In other words, at training time, you look at the actual outputs (continuous-valued) from the softmax layer and use the cross-entropy; at test time, you compare the two outputs and pick whichever is larger to choose the classification (and apart from that ignore their exact values). The shortcoming of this approach is that it doesn't take into account the value of the $Y_i$'s, only their sign. To improve this, use weights on the samples. We're again going to train a boolean classifier on the training set $(X_i,Z_i)$, but this time we'll weight each sample in the training set differently. Put the weight $Y_i$ on the sample $(X_i,Z_i)$ in your training set. In other words, when training the boolean classifier, the loss function will be a weighted sum of the loss for each sample: $$\text{Loss}(\theta) = \sum_i Y_i \cdot \ell(f_\theta(X_i),Z_i)$$ where $\theta$ are the model parameters, $f_\theta(X_i)$ is the output of the classifier on input $X_i$, and $\ell(\cdot,\cdot)$ is the cross-entropy loss. Notice that errors in samples where $Y_i$ is large are penalized more, and errors in samples where $Y_i$ is small are penalized less; this is appropriate, as that's exactly what will happen when you use the classifier $f_\theta$ to compute $L(f_\theta)$. This should improve on the approach you're taking.