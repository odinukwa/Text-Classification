To answer the question of will you get the same results running an analysis such as Brent's sp_Blitz scripts in a non-production environment the answer will lie in what information you are trying to gather and what that information depends on. Take the two examples below as a means of showing the difference in results (or no difference) based on the type information sought. 1) Suppose I am interested in analyzing the indexes in the database to determine if I have duplicate indexes. To do this I would look at the database metadata which would be part of the database structure itself. In this case the results should be the same regardless of environment because the data that is being queried to draw the conclusion is part of the physical database structure. Brent's sp_IndexBlitz does this as one of the steps in its analysis, among others. 2) Suppose I am interested in analyzing an index to find out if the index is utilized. To do this I would examine the DMV (sys.dm_db_index_usage_stats) to determine if the index in question has any scans, seeks, lookups or updates. Using a combination of this data I could then determine if this index is making inserts run slower or is a benefit to select performance in a way that justifies its overhead. In this case though the data will be different in results between production and the non-production environment unless the exact same workload and configuration is running in both environments. Brent's sp_IndexBlitz also performs this same check and will provide the usage details based on the settings specified. To further clarify on the "why would data be different" which seems to be a sticking point here lets dig in to what DMVs are. A DMV is at high level just an abstraction layer that provides a view of the instrumentation that SQL Server has running. With this being said as mentioned by Tony when SQL Server restarts the data that is within these DMVs is not persisted. For this reason when you perform a backup and restore it is functionally equivalent to a server restart for the purposes of the discussion around DMVs. Once the database has been decoupled from the original host of this instrumentation data the data would be lost unless it was persisted elsewhere. This is mentioned in Microsoft's documentation as shown below: 

From a reliability standpoint of deadlocks per second the perfmon counter will be more accurate. The system_health monitor is based on ring buffers which can overflow over time or miss events if there is enough activity within a brief period in order to facilitate reliability. There is a file that backs this which you can look at which will give you more history but if you are experiencing as many deadlocks as you are indicating there is a distinct possibility that you may not have full history within these. This would explain your discrepancy between the two measurements, you aren't doing anything wrong from what I can see. The files will be located in your SQL Server install directory under %Program Files%\Microsoft SQL Server\MSSQL11.[Instance Name\MSSQLSERVER]\MSSQL\Log\system_health_*.xel. 

Then you would not get wasted space in tablespace and also even in case your data grows fast and catches up with autoextend operation - there will be only small delay in writes because 10GB will not take a long time to format. And Oracle will autoextend tablespace proactively. It will not wait for the last blocks to fill up. 

2nd step is to drop all the active connections and unfinished transactions so that node is free from user queries. And all the applications continued to work on the other SQL node. 

I do that now and then to create test environment. Just for such more complex directory structure I prefer . With it you can resume copying if you run into some disk space or network issues. 

To import data into a different schema you need to grant role to the user. Oracle MOS Doc ID 351598.1. Or you can use SYS user: 

First insert/select goes from dba_objects then it is faster to insert/select from filler table itself. Now you just have to check datafile using RMAN: 

Usual rules apply. Code changes should not be required but something will break if code is more complex. Also there will be some SQL plans regressions. Official documentation about deprecated features is there: Deprecated and Desupported Features for Oracle Database 12c Also you can check Metalink note 567171.1 about parameter. This parameter is used to enable or disable certain bugfixes which affect query behaviour. In 11.2.0.4 the view contains 854 rows which means that there are so many potential quirks while upgrading to the newer version of Oracle Database. I am sure that 12c contains much more rows. 

Maybe db_cache_size, shared_pool_size, sga_target or other memory related parameters are set to non zero? Remember that when using AMM those parameters specify minimum memory allocated for particular pool. So if sga_target is 6GB you will not be allowed to set memory_target to 4GB. Also sum of internal variables __sga_target, __db_cache_size, etc. may be more than your specified value of 4GB. If you see those symptoms you can cleanup pfile bounce Oracle with pfile and recreate spfile. In the same step you can also set to zero. 

Also there are a bit different instructions in $URL$ but I believe things a bit changed now when flash MOS page is retired. 

You will need some more space for row overhead and PK storage. More information you can find in MySQL Documentation. For the most detailed information you can use ndb_size.pl utility. 

With any more complex data it is almost impossible to restore data in MySQL Cluster in one step. Usually one needs two steps: 

If that looks too simple - keep in mind that you will have to drop and later recreate foreign keys if you have any. :) 

Now you can unconfigure and remove old machines from old cluster. Of course Oracle version on new machines has to be the same as on old machines. Or it is possible to do upgrade right away on new machines. You should test procedure of course. There are quite a few possible problems on the way. The idea is that Oracle DB does not store anything in "cluster". All the data is in datafiles, controlfiles, redo logs and spfile. Which is stored on ASM can can be mounted on another server. 

However, depending on the order type, server needs to receive additional information. If the type is server needs number of photos. If the type is server needs number of minutes. I use Laravel for backend development and I found it really inconvenient to work with . In general situation I would write validation rule as: 

Type has four values (it is possible that they will change a bit) For each type there is associated information, such as budget factor (used for internal business logic) 

So I made a lookup table with following columns: id, name, title, budget_factor Therefore, orders table has type_id which is foreign key to id of types table. Everything's good. However, when I started developing I came across next case. 

Imagine next situation. I have an order entity which has type. For now type has such values as "photo", "video", "panorama", "supervideo" :) 

Use varchar for status column Use enum for status column Use separate dictionary table, which has id and name, and in orders table keep status_id as a foreign key. 

I remember I already found an article illustrating the best approach, but I lost the link and couldn't find it :( So, very common situation. Imagine we have orders table, and an order has a status. Three choices: 

On the other hand I'm not a DBA expert and I'm not sure if it's a good practice. I have quite a lot of order characteristics, all of them are in lookup tables. For some lookup tables I can't use this method, as they have up to 10 values with long names (like ) I can't use enums (as possible values can change + I have some associated information for every value). So is this a good practice, or should I stick to id's? I don't care about DB size for now. 

instead of retrieving from dictionary table and comparing it to So all the time I start a new project I ask myself the same question. For example, right now I have roles. I mean not "roles-permissions" system, where we can add a lot of roles with different permissions. I just have 2 big roles, with different logic. So I can make a separate table with 2 values, or varchar and compare string values. I'm talking about MySQL. Sorry for silly question, not an expert in DBA. I suspect that using dictionary table is also better because it'd be faster to search statuses by int, instead of strings, but not sure. 

I can change number of types and their additional information like budget factor I can directly compare the value of type in orders table. Like 

Which is the best approach? I know that enum is evil, because it's hard to modify database if we need to add a new status. Probably dictionary table is the best: we can see all possible values and we can easily add a new one. However, when using varchar, it's way easier to work with it, like: 

However now I put it like (where 1 is id of order type) So this is bad. Of course I can write custom validation, like grab type from database, compare it's name to photo etc. Plus, everywhere where I need to check orders type, I need to do a join. What I thought is: use , etc as a PK. This way: