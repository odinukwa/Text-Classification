Thanks for an interesting question. My review starts with your main interest -- speeding up what you've already got. It then goes to discuss a number of other topics you may be interested in. Speedup 

Inspired by Calpratt's observation that this is really a question about testing matrix symmetry, I wanted to post a version. 

Since your sentence is a Python string (not a NumPy character array), you can generate the data for your histogram quite easily by using the data type that is available in the module. It's designed for exactly applications like this. Doing so will let you avoid the complications of bin edges vs. bin centers that stem from using entirely. 

That way, users would not always have to supply arguments for and if they didn't feel like it. Notice how I had to re-order the parameter to achieve this effect, because arguments with default values must all be listed after arguments with no default (and you won't likely ever want a default value for ). 

Why do you need pandas at all? Are you using it just to store HDF5 files? If so, I'd recommend looking at h5py instead. You may have to learn a bit more about how HDF5 file formats work, but I think it would be more efficient than Pandas. In particular, in an application like this where the game board is naturally a matrix, Pandas' model that rows and columns need to have labels seems like annoying overhead instead of a feature. You could even store the iterations of the game as a single datacube instead of as separate HDF5 datasets. Probably defining your own chunking will improve HDF5 performance at the largest sizes, but it depends on how you want to view the data. Do you want to view the entire board for a single iteration easily? Or do you want to view a small region of the board across multiple iterations easily? Unless you have particular views in mind, a good starting place would be to just use 's autochunking, i.e., don't define your own. This is minor, but why are you using any interpolation at all in ? I'd recommend . 

By adding your code into its own function, I was able to compare timing between the version and your version: 

I'm not a veteran of pandas but I think it's safe to say that building a data frame row-by-row in pandas is not very efficient. How did I refactor your code to run in Python 2 and ran on it? Essentially I just wrapped everything you wrote into a dummy function called and called the line profiler on that function. For Python 2: 

Assignments to Pandas dataframes always work better when doing entire columns at once, instead of filling a single row or a small number of rows at a time. In the code below I first completely define a NumPy array of cluster numbers and only after it is completely defined do I pass it into the Pandas DataFrame. Even aside from the speed issue, your loop where you assign cluster numbers is very confusing because of a misuse of : you have but the idiom is always . You have the position of the index and the value switched. In your particular case this works and is a nice trick but you should document how this works in your code if anyone besides you is ever meant to read it. I changed variable names to be PEP8 compliant and to be easier to read. I also added some comments, docstrings, and formatting. Particular changes of note were defining and as variables, wrapping the code you wrote into a function (so that I could line profile it), making a variable defined in all-caps (which in Python suggests it is a constant), and defining the radius in the call to to be in terms of rather than just be another magic number hard-coded into the code. I think most of these changes improve your code's readability and make it more in line with style guidelines for Python. Minor point: coercing to boolean with works on SciPy sparse CSR matrices, and so doing the coercion before converting to a non-sparse NumPy array with should be slightly faster and use less memory than doing things the other way around -- no time is wasted on converting zeroes. 

To figure out what's slow in your code, you've got to profile it. Python make this easy with the package. I used this tool on your code and found that the was responsible for 88% of the run time. After thinking about it, this didn't surprise me. The way you build is very inefficient. Keep in mind the output of is a new array. So every loop iteration requires instantiating a new NumPy array and copying all the existing data into it. A better way is to pre-allocate the array with something like this: 

I think this is a fine restriction, but I'd recommend carving up your code into at least two functions: one to find primes by searching over integers, and another to find factors by searching over primes. 

Is the fifth element in this array really supposed to be "3"? Right now the code functions because of the mutability of . I'm not 100% sure it could be done cleanly, but if possible I'd suggest re-writing and to return instead of returing . That way the initialization of could happen in those functions too, which feels more natural to me. You are using your (mysteriously named) variable in a way that looks like a , so you might consider using this built-in datatype of Python. Now, to issues of performance. In Python, line-profiler is an easy to use package for assessing the performance of your code. I use this package in a Jupyter notebook like this: 

Since the function and the function are related by , i.e. , then iff you are OK with the weird approximation for , you should be able to work out a similar approximation for . You might be interested in the module, which provides a generalized capability to compute symbolic derivatives of most NumPy code. 

This converts the data frame to "long" format, with one record per row. So if the original data frame had six columns and rows, the new one will have rows, and three columns, one named , one named and our extra column not included in the call. 

I renamed the function to eliminate the word from its name, which was misleading: you are multiplying the Earth's radius, so the result is not distances on a unit sphere but rather on an Earth-sized sphere. I added docstrings to explain the function. Rather than have the "magic number" in the bottom of the code, I moved it to the top, defined it as a variable with an all-caps name. This make it more apparent that the function relies on this value and is consistent with recommended Python PEP8 style best practices. The units (km) in particular may not be apparent to users of the code. It is obviously inefficient to compute every distance twice, as my version currently does. However, your version also double-counted, which would be difficult to avoid without storing a copy of the keys in your dictionary in a list or tuple, so you could them and then make the second loop go only from the index of the outer loop on up. For easy array-based trigonometry, I switched the calls to and to and . After that the only import I needed from was , but as Jaime pointed out that is available from numpy, so I just used that version and eliminated all imports of anything from . 

Minor other note: using in my original code was a mistake. Although it results in a better (lower memory, more precise) representation of the connectivity matrix, eventually this matrix will be multiplied by a vector, , which requires that convert all the elements to anyway. It's (slightly) more efficient to instantiate the matrix with a for this reason. 

You could make the code forward-compatible with Python 3 by adding a at the top and using instead of . For consistency with most other regular Python functions, you might want to or something similar instead of when the search does not succeed. I'm only slightly joking when I say that Pythonic users might interpret a as meaning that the desired value was found at the end of the array! You don't use PEP8 style recommendations. and would be preferred names relative to and . 

I did actually time this version vs. the old version using the small test case in the OP, and there was an improvement, but it wasn't very dramatic. My untested guess is that my approach will be much more efficient as the problem size increases, if the density of links between knots is large. The caveat is that much larger amounts of memory will likely be required. (If instead there are a tremendous number of knots but relatively few links, Jaime's sparse approach might well be faster.) 

I would personally find a name like or something to be far more illustrative and informative than . Also, the function itself could do with a slightly more informative name, such as or . The way you are generating feels very unnatural to me. Since you have already, you can just use numpy to define a without any for loops, like this: 

The output of this operation is at the end of my answer. It shows that you are right, and the slowest part of the operation is indeed the nested loops in . Dictionaries, unlike lists, can't be mutated using slice notation. The only reason you need the nested loop is because you can't change whole swaths of the dictionary at once. With lists, you can. Thus, if you agree that the possible bug I described above is in fact a bug, you can switch to storing the output values in a list instead of a dictionary, and get rid of the nested loop. The speedup is very small for short input arrays but grows with array size. On my machine it led to a ~2-10× improvement at input arrays of 10,000 elements. I'll put the output of my line profiling below too. Using numpy for applications like this makes sense because python doesn't have a mutable fixed-type data structure. Interesting, using numpy only to represent the variable in your original code, while leaving in place all other loops, speeds up the execution another 2× or so for lists of 10,000 elements. 

Why is not a parameter in the function? Having it as a parameter will make it much easier to change, and more importantly, it will make future users of the code (including yourself six months/weeks/days from now) realize that in fact this function depends on a parameter to compute its result. If you do , then will be a parameter with a default value of 10, so you don't have to pass it in if you don't want to. 

Watch out for places where you are inadvertently causing interconversion of NumPy data types and native Python data types. For example, should be I thought the part you did was clever, but to my eyes at least it was a bit easier to grok using as a criterion and using instead of , so that is what I did in my example above. I did a few timing tests with my revised function and it was at least ~20-30x faster than your . Using instead of was responsible for most of the speedup. Using instead of gave an additional 2× or so. Here's an example of some code from my IPython notebook, along with some output from the line profiler. 

Counting neighbors in a game board is very easy to do via convolution with the proper kernel. Below, I used SciPy's function, not the much faster , because the latter only works on floats and so rounding back to integers / bools would be required. Nonetheless, I suspect for large game boards, the FFT method and rounding may still be faster. I doubt that Cython would speed up NumPy very much, but I would love to be proven wrong! (A Cython guide for NumPy users says Typical Python numerical programs would tend to gain very little as most time is spent in lower-level C that is used in a high-level fashion.) Anyway, to my eye at least this function is a bit easier to read and interpret. 

The bottom-line results: your code took about 25.6 seconds (on my machine) to run, but simply filling the Pandas dataframe by column after parsing was done instead of row-by-row during parsing took 1.2 seconds. This simple modification led to speedup of more than 20×! You could probably get even faster by pre-allocating Numpy structured arrays for each column, and then using those to fill a dataframe by column. In addition to the timing issue, there are a number of other aspects of your code that you may want to consider revising: 

are likely to be slow. And the function in R is unvectorized as also a bottleneck. Possible solutions Vectorization of assignments I'm a big fan of the tidyverse system of packages. For this case, the , , and system of packages allows a very nice functional implementation of your problem. However, they won't automatically vectorize functions like . Vectorization of Googling around, it appears there are a few vectorized implementations of . One I found easy to use is from the genefilter package, . And although it isn't as much of a bottleneck, I also used a vectorized computation of by using the and functions from . Putting it together Below I implement some of what your original code does. I didn't worry about the particular flavor of t-test that you were using (i.e. ), and I didn't implement computing the correlation between p.values. 

In addition to changing things to be compatible with , there were several other stylistic and efficiency notes: 

For large datasets, avoid converting between native Python and NumPy objects as much as possible. Look at the and functions. They may help you go from saved files of your data to NumPy arrays without having to make any Python lists at all. But suppose you do have a Python list. You shouldn't have to convert all of the data to a NumPy array, and then later convert each users's data to an array separately. I would try something like this, assuming that doesn't work for you: