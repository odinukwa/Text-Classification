Beware the OS authentication is done by client machine, not by the database server. I think that within reasonable pessimism to Oracle tools you could expect that it boils down to this: database server receives a TCP connection from whichever IP address that might pass the network path, and that connection just claims "I promise I've done OS authentication and, trust me, this connection is made on behalf of OPS$JohnSmithCEO." Neither the database client or database server is given OS password for additional OS authentication! And you can't even trust that this connection came from a valid Oracle software. It can be man-in-the-middle in reality. It's worse than telnet. Whatever "Windows-specific" checks are done by Oracle, they seem contrived and you can't really trust they are complete and secure. 

Jay, this isn't officially documented, so I'm speaking only from my own experience. In RMAN, the command is synonymous with . Also the command is synonymous with . I'm not sure about ; it might be also a synonym of , although I've never tested the latter. In your scenario, the former would work as expected. In particular, neither or require all the datafiles to come from "a single backup of database" (from a single run of ). Actually, RMAN doesn't even have a concept of "a single backup of database". 

I currently have a impdp job running for a fairly large schema (2TB) and ran into an error. The source database has one column that is larger then the destination database. This is causing ORA-02374 and ORA-12899 errors. This has caused the import job to come to a crawl on this one table. I am trying to get past the errors, and the best solutions I can come up with are to either fix the table definition, or tell the import job to skip the current table. I can not currently fix the table definition because the data pump job has the table locked. Is there a way to pause the job, make the column modification, and then resume the import? Alternatively is there a way to tell the job to skip the current table, and move on? This would allow to to come back once the job is finished, fix the table definition, and then re-import just the one table. ETA: This is one of the last few tables in the import, I would rather not kill the whole thing and start over. 

You don't want your test database to retain the same internal id number as the production database. Hence it would be best to use RMAN's command DUPLICATE, because it sets a different id (and also a new database name). This command is specifically designed to do what you require. 

The script creates certain objects, and it will put them in the current schema. Oracle wishes that these objects be created on SYSTEM schema, hence the requirement. There are two Oracle schemas, SYS and SYSTEM, because they have two purposes. In short, SYS truly is the "special" user in respect to database internals, while SYSTEM works normally. The SYS is the owner of the database, the only user with access to X$ tables, the only user which acts as SYSDBA (internally), that bypasses logon triggers, and that, for some obscure reason, cannot obtain read-consistent view of data. As a schema it holds crucial Oracle stuff, mainly the data dictionary. The SYSTEM is fairly normal schema with DBA privilege (database administrator, not the same as SYSDBA), only that it is built-in and contains some additional (but also quite important) Oracle stuff. You shouldn't put your own stuff in this schema, if not instructed so by Oracle. 

I have a powershell script that I am trying to add in to a sql agent job, but the job step is failing with the following error 

I did some reading, and opening the master key was only part of what I had to do. I had to completely configure the new server for SSIS. I found the following blog post helpful, $URL$ The following microsoft documentation was also a good second source of information, as a check against the blog posting. 

I am hoping someone can shed some light on this. When running an import into an oracle database, I sometimes see different behavior based on the parallel option. Some times, I will see multiple data pump workers all running insert commands, with (parallel 1) query hint in them. Other times I have seen a single, or just a few data pump workers, running insert commands with (parallel X) [where x is more then 1] table hint in the queries. I have seen this when running imports that essentially identical. The imports are using different dump files, but where created from the same nightly job, just done on different days. I am using the following options SCHEMAS=XXXXXXXX parallel=32 cluster=y DIRECTORY= DUMPFILE=XXXXXXX_%U.dpdmp CONTENT=DATA_ONLY TABLE_EXISTS_ACTION=APPEND DATA_OPTIONS=SKIP_CONSTRAINT_ERRORS LOGFILE=XXXXXXXXXX.log 

Don't use bare because without SET UNTIL it doesn't work as you'd expect in some scenarios. I wouldn't say any of these two solutions are inherently safe, both have as many pitfalls as any other Oracle's advanced features, so you need to do some research upfront. 

Verify here you have had only a single control file and not two or three of them. If you have had more than one control file, then this instruction is not for you. 

The green bug you're seeing (look closely) means the package have been compiled with DEBUG option. It is also seen on bare procedures and functions. It is a coincidence they don't expand - maybe a bug (duh) in SQLdeveloper :) On mine, these expand without problem. 

In case you specify each block is read once and written two times to backupset copies. The two backupset copies are supposed to be bit-to-bit identical. Both copies have the same backupset key (BS_key) in RMAN. You cannot mix tape and disk copies - either both copies go to or both to . 

The "device" in RMAN is a misnomer, it should be really called "storage". The "sbt" (synonym of "sbt_tape") is a misnomer again, as it has NOTHING to do with any tapes, it should be simply called "non-rman". This is just an empty placeholder, to be filled with any "plugin"; the plugin is called by Oracle either the "Media Manager library" or SBT_LIBRARY. This plugin allows rman to store and retrieve files through it, so rman only tells that it needs a file handle "xyz" (file is identified by a string handle) and doesn't need to know how the file is delivered, from tape or anything. The plugin is normally a part of an independent backup software, such as IBM TSM or Symantec NetBackup or many others. Oracle provides a simple emulator SBT_LIBRARY=oracle.disksbt for testing. Since you didn't fill that placeholder with any "Media Manager library", you receive an error message as expected. 

I have noticed differences in executions between data pump imports lately, and I am trying to determine what is causing the differences. I am using impdp to move data between databases, and remap the schemas. It seems like I have seen two different things happen when I run the import in parallel, and I am not sure what is causing the differences. Sometimes I see the job running and it is importing a bunch of tables side by side, other times the import process is only importing a few tables at a time, but using parallel inserts for each table. Using Parallel inserts on just a few tables at a time seems to cut my overall run time substantially, but I can't figure out why it only runs like that sometimes and not others. 

if you are running the deletes in batches, do you have an index on column that you are using to chunk up the process? IF there is no index and the query is resorting to full table scans to try and find the data that should be deleted, you could be adding a lot of time to the process. Unfortunately backing up the good data then dropping/truncating the table is probably your best bet. You could always rename the existing table and create a new table to do this rather then trying to extract just the data you need to keep. This way if you found your initial load of data was lacking, you still have everything to go back to for a second look. 

XE does not support Streams, as officially documented. SE and SE1 support Streams but without redo capture. EE and PE (Personal Edition) fully support Streams, and also support Logical Standby. 

What is the most costly thing for you? Prioritize. Usually the most costly thing is confusion of users. Ordinary users very rarely use a TNS name. They usually use the data through a dedicated application, and know just the name of application, like CRM. Power users connect to TNS names (think SQLdeveloper), but they also connect to the ordinary applications. Why would you want application CRM use a TNS name of ORION? Power users see TNS name and all they care is TNS name; not SID, not hostname. They talk to other users in terms of TNS name (get that report from FINANCEDB John!). The state of minimal user confusion is when TNS name is the same as the name of application using it. Either exactly the same of with "db" appended. So TNS name is like CRM or CRMDB. I prefer the latter. The next costly thing is changing anything inside tnsnames.ora. You don't know where it is distributed. You put the file in one's client machine or an application server, and voila three months later it magically appeared on dozen others. Can you prevent it? (No, you cannot.) So in practice this means once you give any user a TNS name, its definition is set in stone. So, don't put SID there, put a SERVICE_NAME. Don't put IP address there, put a hostname. And those two would stick to TNS name "forever", so there is no reason for them to be different than TNS name (Occam razor). 

I am trying to track down a cause for a difference in oracle impdp processing, and I am not finding anything, so I am wondering if anyone here can explain the cause of the differences. At times I have seen times where using the parallel=x parameter in oracle datapump will cause either multiple tables to be inserted at once, up to the value of 'x', or other times will use parallel threads to import a single table. I have not been able to track down what might be causing the difference in performance, and I am wondering if anyone has an explanation, or even just a direction to point me to. It would be helpful to determine why it runs in a given matter. For instance right now I am monitoring a single table, data_only import that was started with parallel=8 in the command, but the import job is only using a single thread to do the import, as shown by the following hint the insert query /+ PARALLEL("XXXXX",1)+/. If the process would use the maximum 8 threads specified to should run much faster. 

No. If the datafile has been occupied by a partition of a table, you need to ALTER TABLE EXCHANGE PARTITION with some empty dummy table. Then drop the exchanged table. Then rebuild the invalidated indexes. This works if you don't have foreign keys to the table. This is quite a troublesome operation. Then you are free to drop the corrupted datafile. 

Because for some strange reason, although regexp_like looks like a function on a first glance, it is not a function in reality, but is a condition (an integral part of SQL syntax). But, more confusingly, it is a function if used in PL/SQL: 

This is a full database import which also includes tablespace definitions. Find out from the original Solaris database what are all the target tablespace names and sizes. Pre-create the tablespaces manually using the datafile paths of your choice () before performing the import. You will see which you can ignore. 

In some versions of Oracle, you cannot put space between the beginning-of-line and the WALLET_LOCATION keyword, and you must put space between the beginning-of-line and the definition of a wallet. Your snippet indicates that you failed at one of these things. I think they removed this silly requirement starting from some Oracle version, but better safe than sorry. Good: 

Well, it looks like when the first sql server install ran, it also configured the file server services, which were still trying to hold the disk and the network name. Odd thing is, the disks where all showing 0 dependencies. This probably could have been fixed if we opened a case with microsoft, maybe cleaned up some registry entries, etc, but the guys from out windows team wanted to just rebuild from bare metal, as it would be quicker and easier. I was on board with that, so that is the solution I am implementing 

I am trying to setup a new sql server 2008 cluster, on windows 2012R2, and the installer is failing on the cluster shaed disk availability check. I have verified that there are 5 disks assigned to "Available Storage" when viewed in the fail over cluster manager. Some background, this is my second attempt to install sql server on this cluster. The first time, the cluster object was unable to create the new computer object during the cluster installation. This caused the installer to fail do to lack of permissions. I have since resolved this, and have run "Remove node from cluster" to uninstall sql server from the node. I am now trying again to run the installer. About the environment, OS: windows 2012R2 SQL Version/Edition: 2008/Enterprise I am running the installer from the current cluster host(node1), and all storage is owned by node1. This includes the quorum, as well as the 5 disks assigned to the available storage group. Both cluster nodes are up and available, and accessible either through the node names, or through the windows cluster name. There are no cluster validation warnings that I know of, but I have asked the windows admin to rerun the validation tool to confirm that, that is still the case. SQL 2008 is required by the front end application (I pushed for at least 2012, but was told it was a no go) 

The next thing in terms of cost is an OS (machine) and its IP address. You cannot afford a separate system for every TNS name. So crmdb.mydomain.local is not the only name for the IP address; the same IP address would have more names, like financedb.mydomain.local. Your OS admin would decide how to do this best, and how to determine the main hostname of the OS. They have the same problem with many other systems - multiple names referring to one OS - so they should have a solution at hand. The only people who are confused now are DBAs and OS admins, they see multiple hostnames leading to the same IP address. But users don't care about that and are not confused by that. (By the way this approach is coherent with SCAN. ) The next thing in terms of cost is either one of the two: an Oracle instance or "administrative cost of separating schemas out of instance". The tradeoff is for you to decide. 

In Oracle starts a PL/SQL block. In other words, after you ought to provide a text of a program that is written in a procedural language that is somewhat different than SQL (although it bears some similarities). Your sqlplus had read everything but it was not yet parsing or executing anything, it was waiting for an and a line containing only a slash like this: