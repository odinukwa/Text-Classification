Let me answer this from the point of data in general. I understand you want the answer for longitudinal data but the reason I don't want to specifically answer that is because pretty much all these frameworks treat data in similar ways. What you should be looking for? 1. How proficient are you in coding? If you code well enough and you can tackle most problems by coding the I suggest Python + Pandas. The reason is simple since Python is a programming language you can use the language outside just the normal ecosystem of running your task. So if you want to integrate other components such as maybe a web server, store in a database etc. Python will be most useful. where as if you are not comfortable with coding then I suggest R is the way to go since it is much more easy to learn but the disadvantage comes when you are trying to do things other than just perform your daily data analysis. It might be limiting 2. In terms of efficiency Now pretty much all these tools work pretty fast and the differences are hardly noticeable. Usually, you would have a better edge with R in just the way computations are performed. It is a bit more optimized and it utilizes CPU cores to perform a tad bit faster computation than python does. However these days with the heavy intensive RAM etc it is not really that big of a difference. Also, most libraries for heavy matrix calculations are present in both these toolkits. So not really much difference. In my opinion I would go for Python just because of the diverse uses it has. Also has a great supporting environment for libraries etc. 

Well PCA, as suggested above by @CarltonBanks, does help you remove features with the least correlation and use mash the features together such that they have the highest correlation. To answer your question, how to visualize higher dimensions using PCA 

The answer to the first problem, you have to check the accuracy first before making choosing any new features if the rest of your features give a good enough accuracy then there is no need to choose new features. The second problem, to predict those values of the features you are using now if they are discrete in nature try classification algorithms whichever fits the model best, else try something along the lines of regression if the input values are continuous. And then use these predicted values along with your existing model and check how the accuracy varies. 

I need to generate some synthetic data using Kernel density estimation from a sample of smaller data. I need to use the Gaussian kernel to generate the data because I think it best suits the type of data distribution I have. But I need only positive samples, negative samples give a domain error. How can I generate positive samples using Kernel Density Estimation in the Sci-kit learn Library in Python? Documentation for the function used: $URL$ 

I have used decision Trees and Random forests both give a score of 0.74 and 0.79 respectively. I have also tried SVM and Naive Bayes just for seeing the score they gave 0.37 and 0.33 respectively. I was using 75% to train and the rest to test. Which is the best classification algorithm for this data? Multiclass Classification 

It's as you said there is a chance of bias in the dataset. To avoid this you'll have to conform to an algorithm to generate data. Right off the bat, the solution I would propose is parametric methods. Find the statistical distribution of data and depending on that distribution fill in data accordingly. If you want to know a bit more about parametric methods you can look at my answer here. What does it mean for the training data to be generated by a probability distribution over datasets For some machine learning methods look at this answer I posted here. What are the best way to handle missing values There is never one single correct way to do what you're asking, but there are good ways and it depends on the nuances of your data. 

You could try a classification approach like random forests for categorical data. For example in your brand name assumption, Take your existing data and treat the brand name as the response variable. So when you give the values of the other features in the entry where the brand name is missing it should try to predict what the brand name is. The reason I am suggesting random forests is that it uses an ensemble approach. Please explore the working of Random Forests if you don't know the working behind it. 

They effectively try to achieve the same result but the methodology used by each technique varies a little. RFE removes least significant features over iterations. So basically it first removes a few features which are not important and then fits and removes again and fits. It repeats this iteration until it reaches a suitable number of features. SelectFromModel is a little less robust as it just removes less important features based on a threshold given as a parameter. There is no iteration involved. 

If you know the nature of data you require as suggested by @Emre for manual cleaning you can use Apache Spark or Pandas. Utilizing the data frame aspects of both Spark and Pandas will help you preprocess data with a much easier convenience. Because data frames account for any data type it's easy to access and clean specifically the segments you require. If the corrupt data is of repetitive nature you can use several Map reduce techniques to clean it fast as well. If it's the case of big data. 

I have a waveform which looks like this. The peaks are indicated by the points. I want a type of curve fit which best connects these peaks as smoothly as possible. I have tried polynomial fit but that does not yield a smooth curve. It looks something like this. 

Transform the feature matrix with the number of components of your data set to 2 or 3 This ensures you can represent your dataset in 2 or 3 dimensions. To simply see your answer just plot this transformed matrix into a 2d or 3d plot respectively. This helps you visualize a higher dimensionality data as a 2d or 3d entity so while using regression or some predictive modeling technique you can assess the trend of data. 

Both words can be used interchangeably. However in the interest of a more accurate answer to help you out, you can say statistical methods include the application of mathematical models and fits which aid in the best estimation of the trend for a set of variables in a dataset. These include Regression, Clustering etc. Predictive analytics deals with the "forecast" aspect of statistical methods. An easy example would be that of regression as you mentioned in your question, deals with a function where x is evaluated to find y. To conclude we can say that predictive analytics is a subset of statistical methods. 

Your question is not clear, but I'm assuming you're trying to say one of the features you used for SVM has values containing zeros, and how does this affect SVM in terms of making a decision. Think about it in a Euclidean hyperplane(three dimensions for better visualization), if the input contains all zeros where will you plot a point related to that feature on the hyperplane. The value of that point will be zero, so effectively you're not using that feature at all. 

In my experience, there is no one correct answer to this question. It depends mostly on your problem itself. I'll base my answer on most general cases though. For most Numerical data, a parametric method is the best way to go. If you have some sample data which you can use as a baseline. Try using a parametric method such as Normal distribution, Exponential Distribution or so depending on your model. For categorical data, you can try various oversampling techniques or undersampling techniques. Methods such as SMOTE, ADASYN etc can be used to fill in some missing values. However, these methods are just general cases. But as aforementioned it depends more on your model and the type of data you are working with. It could be that maybe you are using scientific data and you don't need to generate synthetic data so based on how much data you have available you can discard the missing rows. All such nuances come into play before making the most optimal decision for your data model. 

Here the green line is polynomial fit. As you can see it isn't as uniform as I want it to be. Also, poly fit fails to reach the max peak amplitude for some waveforms. So overall it is not what I am looking for. Is there a better fit that can account for these points better?