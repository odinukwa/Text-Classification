The solution was to change the processing in Visual Studio (VS) from Default to Full. The issue is described in detail here: $URL$ (see Cathy Dumas's reply under "Posted by Microsoft on 8/25/2011 at 2:02 PM") 

(we ensure that Excel is connected to the deployed Tabular Model and not the user's workspace copy in Visual Studio) 

Log in with the user that has the User DSN entries that you with to convert to System DSN Open the Registry Editor and navigate to HKEY_LOCAL_CURRENT\SOFTWARE\Microsoft\ODBC\ Right-click ODBC.INI and select export to save it as a file on the desktop (or anywhere else you fancy) Open the .reg file with a text editor such as Notepad Replace the text HKEY_CURRENT_USER with HKEY_LOCAL_MACHINE. Save your changes Double click the .reg file and proceed to import it into the registry. 

I am having a real hard time trying to convert a TSQL statement (which be a store procedure) into a Dynamic SQL. Basically I must be able to pass the schema name as a parameter. Here is my code: 

I am trying to create a contained user for a database in SQL Azure that only as read-only access. What I am doing is: 

I have a date dimension table in which I need to add a new column in which I define the iteration of the day of the week within the month (2 for the second Mon/Tue/Wed/Thu/Fri/Sat/Sun etc). Is it possible to do this be making calculations solely on the date column of the table, which is of type 'date'? 

This only seems to happen with ONE table (all other tables are fine). So we tried recreating the table, but the same problem persists. Any idea of what might be causing this issue? 

At this point I am just trying to select the rows based on the @backDays variable, but I cannot concatenate the @query string with the variable. Not sure of what I am doing wrong here. I am fairly new to dynamic queries, though. 

I have a table in my db that stored both successful and failed log-in attempts. I am creating a stored procedure that allow us to delete records which are older than X days old. So far so good, but I want to raise it a notch (or two) and allow us to specify whether to delete records on whether the [Success] column is true, false or both. I am having some issues with the concatenation of the script that needs to be executed, though. Here is what I did so far: 

Here is the answer. Even though the job was configured to run via a PROXY Account, the SQL Server Agent is still responsible for the job. I had a look and the SQL Server agent was configured to run under the Local System Account on that server. So what I did is to put the agent to run under the superuser admin account and it worked as expected. Now in this case the fact that the job no longer needs a proxy since the Server Agent itself is running under the ultimate account. However I appreciate that this is not the right way moving forward (even though this isn't my server and I hope I never get to touch it again!) I will be advising the customer to reconfigure SQL so every service runs under a dedicated domain account (i.e. created solely for this purpose), which is the way it should be! Now what I would love to understand is why the job would run as long as the proxy account used for scheduling the job was logged into the SQL server! 

The exec @distproc section is roughly at Line 537 of sp_MSrepl_helpsubscription. It's building the proc call and should be something like [SeverName].distribution.dbo.sp_MSenumdistributionagentproperties From the error, it appears that @distproc variable is not getting set properly, or at least getting set to an empty string. Why? That's hard to know without more information or being able to test on your system. Some unusual setup with the distributor, perhaps? But this will hopefully point you in the right direction so you can walk through it. (Run that code on the Database that is being published). 

The old-school workaround for this was to have your SQL Trace log to a table, then once the trace started, create an AFTER INSERT trigger on that table. The trigger would then write out the Execution plan to a different table. 

Perhaps that backup was taken with a different certificate than the one you've backed up? You can check to see which Certificate was used with this query: 

However, since you are on SQL2014, you should really be using Extended Events. There are several ways to do this with XE that are all superior. Here's an example of exactly what you are trying to accomplish. 

Short answer is yes, that is the default behavior. SQL is a data access language that will run statments in batches, you'd have to use other features (Service Broker for example) to run statements asynchronously. One of the main concepts of the language is the idea of Transactions. There are several articles out there which explain this concept in detail with straightforward examples. For Example. I'd highly recommend practicing this because understanding how to use Transactions becomes especially important when one statement relies on the successful execution of the previous statement. 

Whenever you want to see what SSMS is running, you can always fire up a Profiler/Extended Events trace and filter on your login. Doing this from MSX master and viewing a Multi-Server's job history gets you this query: 

The trick to Resource Governor is thinking about what you want to protect rather than what you want to throttle. You set minimum/reserve resources for your good users which protects them from the bad users. My advice is to start conservative and slowly turn the knobs. You'll likely just need one Resource Group for your "Good Users". You would set their minimum CPU and/or Memory to, say 10%. Everything else, including your Bad Users, will go to the Default pool. If Bad Users were all running CPU heavy queries, and the Good User's query came along, it would throttle the Bad so the Good would be guaranteed to at least have 10% of the CPU. You may decide that a 10% minimum isn't enough and you need to increase it. Just be measured in your approach. I personally have never found the need to touch the Maximums, though I suppose there are use-cases. Also, the Resource Governor DMVs provide some great information. You can create your "Good Group" and just leave it at the defaults to see the stats. They are quite interesting on their own. Another thing to be aware of is to make your Classifer function very simple. Remember, this function will get called on every connection so SQL will know which Group to route the connection, so don't put a lot of crazy logic in there. Typically you'll just do simple CASE compare based on Application Name or Host Name or Login Name. for example: 

Now I don't know what configuration I could make to make one or the other (or both) faster but it seems like the VARCHAR FK see faster in queries for data (sometimes a lot faster). I guess I have to choice whether that speed improvement is worth the extra data/index size. 

I am build a web application (project management system) and I have been wondering about this when it come to performance. I have an Issues table an inside it there are 12 foreign keys linking to various other tables. of those, 8 of them I would need to join to get the title field from the other tables in order for the record to make any sense in a web application but then means doing 8 joins which seems really excessive especially since I am only pulling in 1 field for each of those joins. Now I have also been told to use a auto incrementing primary key (unless sharding is a concerns in which case I should use a GUID) for permanence reasons but how bad is it to use a varchar (max length 32) performance wise? I mean most of these table are probably not going to have at many records (most of them should be under 20). Also if I use the title as the primary key, I won't have to do joins 95% of the time so for 95% of the sql, I would even occur any performance hit (I think). The only downside I can think of is that I have is I will have higher disk space usage (but down a day is that really a big deal). The reason I am use lookup tables for a lot of this stuff instead of enums is because I need all of these values to be configurable by the end user through the application itself. What are the downsides of using a varchar as the primary key for a table not excepted to have many records? UPDATE - Some Tests So I decided to do some basic tests on this stuff. I have 100000 records and these are the base queries: Base VARCHAR FK Query 

What are some of the advantages and disadvantages to these different methods of storing record history? Are there other methods that I have not thought of? 

One Table/One Field - Basically there is one table that store the history of all the table that need history storage. All change are recorded in one field as a text data type. Table Per Table/One Field - Same as above except the each table has its own history table (ie. Projects/ProjectsHistory, Issues/IssuesHistory, etc...). Table Per Table/Field Per Field - This is like the above in the each table has it own histroy table but also the history table has pretty much the same definition as the regular table with an additional of additional history related fields (updateDatetime, updateUserId, etc...). 

Everything I am talking about relate to relational database, specific MySQL. I have a number of tables in a database and for a moderate number of them, I am going to want to store a history of the records values when it changes. I have seen this done in a couple of different ways: 

Have Types and Statuses tables for each table that needs them. Have one Types and Statuses table that every table that needs them uses