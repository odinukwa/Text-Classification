The 1994 game Ecstatica and its 1997 sequel rendered ellipsoid segments instead of triangles, but I don't think anyone else has ever used this exact technique. Direct raytracing of BÃ©zier patches was a popular research topic in production graphics a few years ago, but the industry has moved onto polygon mesh subdivision. Subdivision gives much more predictable results for artists, and polygon meshes are much easier to deal with inside the renderer. 

Your comparison with 0.0000001f suggests that you've also been having trouble with self-intersections (that is, with the ray intersecting the object being shaded). In a one-hit tracer, you can't fix that problem by simply discarding the intersection, because you still don't know whether the shadow ray would have intersected another object further along. To avoid self-intersections, you need to push the start point of the ray along its direction by epsilon: 

Yes, of course. The duration it has to run for doesn't make much difference. Your Android smartphone's main GUI is OpenGL, and phones typically have an uptime longer than four weeks. Air Traffic Control centres might use OpenGL SC (the subset of GL for safety-critical applications) continuously for months or years. If you do have free choice of software, OpenGL is probably not the thing you're concerned about. Artists often use Processing for art projects, because it's simple and lets you get to drawing stuff on the screen with minimal 'boilerplate' code. It's very nice for drawing maths-as-art: particle systems, L-systems, strange attractors, &c. For a 3D scene that's supposed to look like the real world, Unity 3D might be a good choice: it's a game engine that handles all the use of OpenGL or DirectX for you, so you can create the scene you want to draw without much programming. If you have more recommendation- or opinion-based questions about what might be useful to look at, maybe drop into our chat room and be ready to tell us what kind of art project you're making. 

Your condition makes me suspicious. You should include the diffuse and specular shading if the intersection test didn't hit an object; that is, if . So, your code should look as follows: 

It's very different between the Khronos standards (including Vulkan) and DirectX. In DirectX, Microsoft implements the API, but they publish to GPU vendors a HAL API. There's actually two HALs: one that runs in kernel-mode, to communicate with the card directly; and one that runs in user-space, to do other tasks (like manage memory, set up data structures, and compile shaders). In Vulkan, Khronos only publishes the specification and a test suite. It's up to each GPU vendor to ship an implementation of the library. The application's interface to the driver is a normal C API, like any other library. You'll get confused if you try to think of an "API part" communicating with a "graphics card driver": the driver is the API implementation. This is why it's very hard for third-parties to make open-source OpenGL (or Vulkan) implementations: you need to know all the details of the GPU hardware to do it. These graphics APIs are defined by industry groups consisting of the same people who design the hardware (at a high level), and deep understanding of the GPU architecture is what you need to design an efficient graphics API. 

BTW, the Red Book is not a great introduction to graphics in general. It's a technical standard, not a tutorial, so the explanation of concepts is just enough to be unambiguous - it's not trying to teach the material. It's also about a scanline rendering library in particular, so there's a lot of important concepts it doesn't describe at all, such as what BRDFs are important, how they relate to real-world surface properties, what an image really is, and many higher-level techniques. And finally, even in the area it's about - how to use a GPU to accelerate rendering - it's hideously out of date. The GL architecture is based on GPUs from 20 years ago. Although lots of features have been added to keep up with changes to hardware, the fundamental abstractions are not a good way to understand current GPU architectures. If you want to learn about the theory of computer graphics, start with a proper textbook such as Physically Based Rendering or Computer Graphics: Principles and Practice, or an online course. If you want to learn how to make a 3D application, start with a tutorial about the kind of application you're interested in (e.g. if it's a game, try a Unity tutorial). 

Yes. For years, the main family of denoise techniques worked in two parts. First, a line detection algorithm would detect the true edges in the image. Then, the output in this would set weights in a filter kernel to perform some kind of guided blur, so that the blur would be strongest (i.e. remove more frequencies) where there are no edges, and weakest (i.e. allow high frequencies) where there were edges. This would remove more noise from gradients while preserving edges. Nowadays, deep learning has provided a new family of techniques. Using the same technique that's producing the noise (i.e. path tracing if you want to denoise path-tracer renders), produce lots of image pairs where one has high noise (few samples) and one has low noise (high sample rate). Feed these pairs into a convolutional network, to train it to produce the low-noise images from the high-noise images. Then feed your high-noise image into the trained network, and enjoy the low-noise output. Variations on this technique include having the network produce a filter kernel for each image patch, instead of learning the output image directly; or classifying an image patch according to its structure (what edges are present), so similar patches can be handled similarly. Some networks can even take the raw samples as input and directly produce a denoised image; others can guide your adaptive sampling algorithm. 

In this code, the "screen resolution" means the number of pixels in the output image (which happens to be a window in the windowing system), while the "window dimension" means the corresponding size in the 3D scene. Computing tells you the ratio between one unit of 3D space in your scene and one pixel of output image. You need to do this in order to know how far apart in world-space to cast your primary rays. These uses of "screen" and "window" aren't standard; they're specific to this code. No wonder you're confused. 

It's a geometric assumption like the other two. Consider a flat macrosurface. Its projected area in any direction $v$ is just $v\dot\ \hat N$ times its area (where $\hat N$ is the surface normal). In particular, the case where you're looking at it along the normal is simplest: the projected area is equal to the area of the surface. Now split the macrosurface into microfacets. The total area of the microfacets is at least as much (assumption 2), but each 'kink' in the surface bends the normals of the separate microfacets away from the original normal. Whatever the shape of the microfacets, the sum of their projected areas doesn't change. In the case where you're looking along the normal, it's easy to see that the total projected area is the same: the surface would have to get larger or smaller for it to change. For any direction, the microfacet has to cover a portion of the original projected area of the surface. Changing the orientation of the microfacet while still filling that portion doesn't change its projected area. There's one tricky case, which is where the microfacets overhang each other. In this case, the total area is greater, because some area is covered by more than one microfacet. But in this case, at least one of the microfacets has to end up pointing away from the view direction, back into the surface. In this case, the dot product is negative, so this cancels out the area covered by more than one microfacet. This is why the text is careful to single out that it's the signed projected area. There's one more tricky case, which is where the microfacets extend past the silhouette of the object. This might happen when you're looking from very glancing angles, or where overhanging facets overhang outside the perimeter of the surface. In this case, the projected area of the microfacets will be greater, violating the third assumption. We don't generally consider this case. Intuitively, it matches up with the fact that techniques like bump-mapping don't change the shape of the silhouette of the object. 

Yes, in the simple case, primary rays conform to the frustum. If you're doing depth-of-field optically, then the rays don't quite conform to the frustum, because you need to vary the ray origins slightly as well as the directions. How exactly the variation works depends on how closely you're simulating the lens system and aperture. You can picture it as sampling from several frustums, one for each pinhole camera you're averaging. I've never done full Monte Carlo for ray sampling: generating the rays according to a uniform distribution is bad at distributing them usefully around the image. Popular options include: 

On a PenTile display, the mapping from pixels in the framebuffer to individual subpixels is not trivial. Each pixel in the framebuffer may only have RG and BG subpixels, in some interesting geometric arrangement. I'm trying to do some subpixel antialiasing of content that will be displayed on a PenTile display, so to get best results I'd like to understand how I can set the values of each pixel in the framebuffer to control the subpixels individually. (N.B. I know the subpixel layout of the display I'm targeting.) 

No, but this is the most obvious technique. A good denoiser isn't just a filter that runs on the image, but actually performs the reconstruction; i.e. it's a function from random samples to an image, not a function from an image to an image. 

I'm not sure I've correctly understood the question, but here goes. You're trying to sample directions uniformly, so you've got $p(\omega)$, which is the probability of getting a particular direction. But what is a direction? You actually need your probability distribution to produce numbers in some representation, and the easiest representation to deal with is lat-long (i.e. two angles). So the thing you actually need to sample from is the probability distribution of pairs of angles. This is what $p(\theta, \phi)$ is: the joint probability of two variables. $p(\omega)$ and $p(\theta, \phi)$ mean the same thing geometrically, but the former gives you an abstract direction you can't sample from directly, while the latter more usefully gives you two numbers that represent a direction. The reason for your third bullet point is to do with the point you've made about how it isn't just a single direction. These aren't really functions: they're distributions. A direction is infinitesimal, so you can't have a probability of just one direction. What you actually need to do is integrate it over the directions you're interested in. $$ \int p(\omega)d\omega = \int_0^{2\pi}\int_0^\pi p(\theta, \phi)d\theta d\phi = 1 $$ Whichever representation you use, the integral over the hemisphere has to be 1, because it's a probability distribution. 

I hope you're up for learning lots of linear algebra, then. This is a great idea for a hobby project to learn from, but don't go into it thinking you can get by without learning any maths. 

The main part of it is simply Pythagoras's Theorem. The square root gives the length of the central line segment (which is the hypotenuse of a triangle formed by the change in x and change in y). The ratio between the hypotenuse and the change in x is the same as the ratio between the line width and the line width in y (they are similar triangles). Dividing by two is because $w_y$ is the half-width, not the full width. The -1 doesn't appear to make sense in a continuous context, so I assume it's to make sure the number of pixels is rounded down as part of the division. This prevents the line from jumping in thickness when it crosses a pixel boundary. 

is the projection of the point onto the plane of the torus, so gives the distance from that projected point to the inside of the torus. (I've labelled it $r$ on the diagram.) is the distance from the point to the plane of the torus, so the length of the vector ($r$, ) is the distance from the point to the inside of the torus. (Computing the length of a two-element vector is a handy way to use Pythagoras' theorem to find the hypotenuse of the triangle.) Cone Like the torus, the cone code uses the fact that the shape is symmetric about an axis: the z axis in this case. The cone is defined by a unit vector that's normal to the surface of the cone, and the apex of the cone is at the origin. Similar to the above, gets the distance from the point to the z axis, so that we can work in the plane where the cone is a triangle. is the projection of into this plane. The dot product projects this vector onto the direction : how far the point is along this direction tells us how far it is from the surface of the triangle.