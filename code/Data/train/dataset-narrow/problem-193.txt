Ignoring any form of PoE, standard Ethernet does draw a certain amount of power for each port that is connected (in an up state) whether that port is transmitting or not. IIRC, you can count on about 100-300mW of power per port with link. If you want to reduce the power consumption, you need to remove the connection or disable the port. However in many devices, these ports will still consume at least some power even then. Alternatively, newer devices are supporting 802.3az Energy Efficient Ethernet and/or "Green Ethernet" (non-standardized but reduces power usage in potentially several ways) can reduce the power usage of active Ethernet links by using one or more of several techniques like letting the transmitter "sleep" when there is no data to send, reducing power when shorter runs are detected or simply through more efficient electronic components. 

Your wireless vendor may provide a setting to "disable 802.11b clients" or similar. Generally this will implement the next method automatically for you and may tweak some other settings as well. Remove the following data rates (these are the 802.11b data rates); this is generally a WLAN or ESS based setting: 

No, it does not. The 2148 FEX has 48 1000Base-T host interfaces. You can confirm this by checking the interface capabilities with the following command (this taken from a 2248 which does support 100Base-TX): 

I haven't used a Fortigate FW for load balancing, so I will answer some of the questions more generally. First, as to your problem, the load balancer is doing exactly as it is supposed to do and I think your servers may not be configured correctly to respond to a request on their IP address. If you were to test this behind the load balancing, you could set the domain name in a local client's hosts file, behind the firewall with the server and access it both with domain name and internal IP. You will probably get the same result you are seeing now. My guess is that you have virtual hosting turned on (to support multiple domains on a single server) and the "default" is not serving the same pages as your domain. You are getting a web page back from the server in both cases. If you need help with configuring your web server, you may want to try on ServerFault. Second, to get into a bit more detail. A load balancer typically operates at L7 for at least HTTP and HTTPS clusters. This means that they don't just look at the IP address and forward it, nor do they "redirect" the page. When they receive a request, they actually parse the request and forward it to a server after processing the request. There are many things that they can do at this point, such as rewriting the headers in both directions, potentially adding cookies (for persistence) into the data going back to the client, terminating SSL sessions, matching based on the URL, etc. I recommend you spend some time fully reading the vendor documents to get a better understanding of how load balancing works (with Fortigate you can read both theirs and Coyote Point - another load balancing company Fortigate acquired). Understanding what it is doing will help you in cases like this and will allow you to unlock capabilities that you didn't realize existed. 

Ethernet originally utilized CSMA/CD (carrier sense multiple access with collision detection). Note that "collision" is part of the name. CSMA/CD is required in a hubbed environment due to the nature of how a hub operates. Hubs operate at the physical layer by repeating a signal received on one port out all the other ports on the device. If a hub receives a signal on two different ports at the same time, the resulting "merged" signal that gets repeated is meaningless. Ultimately all this means is that collisions on a hub are to be entirely expected. However in your case, the number of collisions is very high, over 30% of your traffic. So what can create a situation like you are facing? Here is a non-exhaustive list: 

You can download the 802.11-2012 standard from the IEEE web page. On page 429 you can find the the information contained in the probe request body, none of which contain a time stamp. However, you will note that the final element is defined as "Vendor Specific." It would be possible to add a time stamp in this this element. This would typically require that you modify the code of the driver on the client device. Any device not running your customized driver would not be sending this time stamp. If you want the access point to recognize this and act on it, then you would need to modify the access point as well. If it would suffice for your needs, you would be able to view the contents of this element if you were doing something like a packet capture as probe requests are not encrypted. 

The third method allows you to keep the 1/2/5.5/11 Mbps data rates enabled in case you have non-802.11b devices that require them. This may not be common, but I know that most Nintendo Wii gaming consoles require 1 and/or 2 Mbps to be supported to connect (although after they connect, they will operate at 802.11g speeds). I have heard third hand that there is also a smartphone that has the same problem, but I do not know make/model or experienced it myself. 

There could be many causes for this, but your suspicion about the AP not being built to broadcast multiple WLANs is not one of them. I would start by looking at this list: 

Generally speaking, routers utilize the IP protocol (i.e. IP addresses) to make decisions while directing traffic and do not care about the transport protocol. The IP protocol would map to the internet layer. The TCP or UDP protocols would operate at the transport layer. Typically a device that would direct traffic based on transport layer protocols (or application layer) would include proxies, load balancers, or shaping devices depending on its exact function. Devices that operate at the link layer (most often Ethernet today) would include switches or bridges. 

No, nothing in the Ethernet standard specifies that frames must arrive in order or duplicate free. However the Ethernet standard is written in a way to provide those characteristics to meet the expected behaviors of the architectural model. 

If it helps, think of the difference as pressing a paper towel to a three inch gash on your arm versus getting stitches at the emergency room. One is an immediate solution that mitigates the problem at hand, the other is a better, longer term solution. WEP was broken (badly) and needed to be fixed. However standards bodies sometimes move much slower than solutions are needed. So WPA/TKIP was provided as a better solution than WEP that could be implemented purely in code/software/drivers. Basically, TKIP is WEP with a few extra features, provided to the community largely by a major network vendor they had previously developed as proprietary enhancements. While it provided fixes to many of the problems with WEP, it is still based on WEP. WPA was derived from a draft of the 802.11i amendment from the IEEE. There were some changes from the draft to the finalized version of 802.11i, which is why the 802.11i standard version is called WPA2. Ultimately the differences are relatively trivial in the grand scheme of things. For encryption, the IEEE selected AES for 802.11i. This was a much stronger encryption than WEP however many existing wireless devices were not capable of handling the demands of AES, often requiring new wireless adapters to be installed/used. Because of this limitation to AES, it was allowed to run either WPA/WPA2 with TKIP instead of AES until the IEEE finalized 802.11n. 802.11n no longer allowed support for TKIP, so an access point was supposed to disable the HT (high throughput) data rates if it were used and operate like an 802.11a/g device. Since that time, only WPA/AES or WPA2/AES are supported officially by the 802.11 standards.