No, the directory can't be omitted even if the server has been properly shut down before copying the data directory. If you try, you should see that the target server will fail to start, complaining that it can't find a checkpoint record. 

Possibly the option to csvsql may solve that problem simply. Otherwise has a clause that can pre-process the data. It receives the lines it on its standard input and must output the filtered result on its standard output. $URL$ 

The lexical analyzer removes the dash-dash-style comments from the user input as if they were non-significant whitespaces. Even without using , the command displaying the query buffer shows that they don't even make it into that buffer. Is it a bug? The source code () contains that comment on processing the token: 

This last command will block. Let it run until all the results are accumulated, then it can be terminated with ^C or kill. In SQL, the fifo has to be fed with: 

If multiple passes on the query must be run by independant connections (such as if there are multiple invocations by a web server), another possibility is to not use the function, and do the pivot operation in the main query itself. This is not difficult because there are only 3 static columns. For instance: 

First let's note that at the time of this answer, PostgreSQL 10 in still in beta stage. Some issues about the ICU integration and how it's documented are still under discussion, and there might be changes before a GA release. 

Note that not using explicit BEGIN/COMMIT does not change the race condition: the BEGIN/COMMIT pairs are just implicit around each SQL statement. To have this parallel sequence work seamlessly, would need to be to what is to , but that's not the case. To deal with conflicting or identical DDL that might run in parallel, probably you should use an explicit lock at the beginning of the sequence to put it as a whole in a critical section. Advisory locks might be used for that. Or you need to implement a "retry on failure" strategy with savepoints around individual statements. 

will not issue statements for databases like or that have been created by at cluster initialization time. However it will dump the contents of these databases if you created any objects in it (by default they're empty). So when restoring the result of into the same instance, it's likely that such errors will occur if these databases happen to have contents. 

If that solves the problem but in a way you find inelegant, you might reconsider how you're using schemas and : make sure that all your custom types are accessible no matter what. 

the two decimal constants are implicitly casted to as the type, because float(n) is equivalent to real when 1<=n<=24, as explained in the documentation on datatypes: 

comes before , because the accented E compares against O as if it was non-accented. It's a cultural rule. This differs from what happens with a locale: 

When doing a TCP connection with the first rule doesn't match. The second rule matches and it triggers the demand for a password, but as none was set this can only fail (empty passwords are not allowed either). The solution is to remove from psql invocation, for the first rule to be taken instead. As it's on a Unix system, it will attempt a connection through the Unix domain socket. Once logged to psql as the postgres user, you may set a password with the command or See also $URL$ In the section Using pgAdmin III GUI , they suggest to change the first rule from auth to , after having set a password. Personally I don't quite get why. I'd rather leave that rule alone and run pgAdmin from my own Unix account, choosing localhost TCP connections. 

does not match for the same reason than the previous line. If you want to "trust" everyone locally through the Unix domain socket, you may change the first line to: 

(assuming that is automatically set by its DEFAULT clause). Or the same as two statements in a transaction, using a different form of update: 

The SELECT permission allows for opening the object in read-only mode, and for UPDATE, the doc says: 

It stores the contents of large objects as small bytea pages (~2000 bytes)in the table, with as the ID or the large object, and as the unique key for that table. The generator of new values for can be compared to a integer sequence, except that after wrapping around at , it can avoid conflicts with existing values. As users, we don't have to care about this, it's managed by the internals. 

No it's not possible in functions, independently of their language. The structure of a function result set is always static, just like the result set of any given query. In fact it's related, as this structure (column names and types) must be incorporated into the SQL query that calls the function. Remember that a client application must be able to prepare a query and obtain a description of its results before executing it. Also a prepared query can be repeateadly executed with different parameters, and cannot return a different structure across executions. It would be impossible to guarantee that property if a function could independantly and dynamically decide what structure it wants to return. I'm not sure whether the documentation makes that point explicitly somewhere (of course it tends to describe what can be done, not what is impossible) but here are some bits in the protocol flow documentation that support this answer: 

Q: Does PostgreSQL uses a hash function for checking equality of integer arrays or does it perform a brute-force algorithm comparing one-by-one the elements of the array? Not according to Array Functions and Operators in the doc: 

it's taken as an unfinished SQL sentence for the interpreter, so it's just waiting for the rest. That's why nothing happens as you wrote. This command is out of context. What you want is: 

No buffer space available corresponds to a network error where the client is not enable to allocate the resources to initiate a connection. Generally, this means that there are other programs holding too many connection handles (sockets). According to a post on IBM developerWorks: 

On Windows, it's likely that the needed locale is already installed, it's just named differently than on Unix. As an example, the following database creation appears to works fine for me with PG9.1 on my Windows XP, with its default locale and no additional language pack installed that I'd recall. 

is an aggregate function that evaluates to true if at least one value is true, so that should lead to the same results as the multiple in the question, except as one single subquery. To push these columns into an otherwise unrelated query, you may add the above subquery as a CTE in a WITH clause, and just put it in the FROM list. For instance, modifying your original query: 

A possible explanation would be that the requested columns is very big and the system fails to find a contiguous piece of about 512Mb of RAM to generate its textual representation. A similar problem is explained in this message from pgsql-general. Excerpt: 

Note that has be run to initialize an empty data directory before creating databases or users. That would be the equivalent of if you prefer to look at this from the MySQL analogy angle. 

If this view was pg_dumped and pg_restored, the column would be created as with again the warning mentioned: 

DDL queries, including CREATE statements, cannot be prepared. Moreover, even in DML queries that can be prepared, parameters cannot be used for identifiers. They're allowed at places in the query where a literal would be allowed. As an alternative, you can safely inject column names into a query with the dedicated function that PL/Python provides for dynamic SQL: : $URL$ 

Normally we'd expect that when postgres was restarted, the crash recovery process would have removed files related to a rollback'ed index from the data directory. Let's assume that it didn't work, or at least that it has to be checked manually. The list of files that should be in the datadir can be established with a query like this: 

PostgreSQL command-line utilities (and more generally all programs that rely on the library) automatically use the environment variables and when they're defined. So if you do in the shell: 

On the other hand, files under are the live configuration files for a PostgreSQL cluster named with version X.Y, such as created by the command . Each cluster (=each postgres instance) has its own that is generated with it, as the other writable configuration and data files of that cluster. Personally, on my Ubuntu 14.04, I don't find a under /usr. There is however a template file at , but note the suffix. That file belongs to the package, as revealed by : 

The main indicator is the exit status of the command. If it's non-zero, then something went wrong otherwise it worked all along. This is the implicit contract between any command and the shell, and breaching it would be a bug. And it's also testable when the sub-command could't be launched at all. Here's a basic skeleton in Unix shell that tests the exit code and acts accordingly: 

It requires at least PostgreSQL 8.4, but previous versions ought to be retired nowadays. An even more modern and cleaner version (quote the table's name if necessary): 

but based on the surrounding code, let's guess it's . That's the reason why gets a permission denied trying to read it: it is the future creator of the table, , that should execute this statement. (to be sure, note that you should also mention to what database you're connected when running the mentioned commands). As stated by the doc (emphasis mine) at $URL$ : 

Note: I've also changed to , the specifier dedicated to identifiers. If the schema identifier required quoting, it would happen automatically, whereas with the query would fail. 

A output with the (custom) is not a SQL script, it's a compressed archive that only is able to handle. You may either pass the dump file to and pipe its output to (often done that way and close to what you tried): 

You'd need also to set to OFF to mute the warning message and get a final result that plausibly matches your question: 

Concerning the need to be superuser, any solution would have this requirement anyway, since it is a feature by design that a normal user has zero access to the filesystem. A DBA (superuser) can grant access to an otherwise-forbidden specific functionality through a proxy function defined with access rights. For instance: 

At least the chunks will be written only twice (once in buffer storage and once in final durable storage), and only once in the journal, as the temporary table won't be WAL-logged. 

generally doesn't need much memory when playing large SQL files, since it doesn't buffer the whole file, only one query at a time, or it uses a stream. The main situation when it may run out of memory is not when importing, but when SELECT'ing a large resultset, especially on 32 bits systems. This situation is generally solved by setting . On import, the dump would need to contain unusually large rows to cause client-side memory issues. The maximum size of a column is 1Gb, so it's theorically possible that it would cause trouble on import, if the database has that kind of contents. If the system doesn't have enough RAM, maybe you could just add swap space. 

For current PostgreSQL version (up to 9.5), queries are received by a backend in a buffer, which is limited to , defined as: 

This shows that the same locale orders differently the two strings and (even though they do not even contain any character outside the US-ASCII set...) Here's a test that I suggest you try on your own dataset: On the master: 

Although populates with a list of pre-defined collations, a complete list would be subject to combinatorial explosion, as ICU collation names are formed by assembling tags, as listed here: $URL$ Currently the doc does not mention explicitly that you can do: 

I think that with this log from iptables, you're just noticing the mentioned "wasted connection attempt". Note the time elapsed between the two connections: and . Presumably this is the time it took you to enter the password: about seconds. If you try the same test with it should ask the password before attempting the connection and make only one connection. 

Indeed an empty password is equivalent to no password so it's not going to be accepted by the server when says that a password is required. Still, there are various ways to avoid inputting the password: