You can speed up a join using an index (whether or not that index is unique). A primary key constraint enforces uniquness of an index and tells the DBMS what a foreign key points at. The fact that a primary key constraint often results in a unique index doesn't mean that one thing is equivalent to (or can stand in for) the other. Constraints are declarative. They make a statement about how your schema is structured. On the other hand, a select query is procedural - especially if you are going to use query hints to tell the DBMS which indexes to use in joining two tables. The DBMS uses a constraint definition for different purposes than it uses an index, even if for practical reasons the constraint definition and the index are linked. 

Since you are using states, provinces, and boroughs in your design, and not prefectures, for example, I'm assuming that you are working in a North American context. If that's true, then you have well established postal authorities (USPS, CPC) with very well regulated postal data and readily available address data quality tools. Even if you are working outside of US/Canada, there are probably data quality tools that will do what you need. With validation and standardization of your address data, you can make sure that you are able to meet your first goal. Using ZIP+4 in the US and Postal Code in many other countries, you can get everything you need for your second goal. A lot of people are really tempted to break addresses down into granular fields. This is a reaction to how bad address data typically is when all you have is "address_line_1, address_line_2,...". However, breaking out lousy, unvalidated city names into its own field only mean you've got a smaller pile of garbage instead of a larger pile. The only way to solve this is to use an address data quality tool to validate and standardize your addresses. If you attempt to normalize your address data you end up with a big pile of many-to-many associations. This is because addresses in real life don't fit into the neat hierarchies that you would see in a textbook. Unless you have some really specialized need for addresses, just keep your tables simple (a few address lines, with maybe the postal code broken out) and get a good address data quality tool to scrub the data on the way in. 

You may have noticed that there is a difference between (1) and (2)/(3) when it comes to the nature of the extraction that you need to do. If you need to do both month/year over year and month range extractions then none of these options are perfect. If that is the case, I'd suggest you consider using a combination of (1) and either (2) or (3) - I'd pick (3) myself, since I'd value ease of display/use over storage space. If you do use a combination, make one or the other a computed column(s) and index it for efficient retrieval. 

You could also define a view which outer joins to in order to show all of the columns of both tables together. 

One option would be to replace your n identical SQL Server databases with writable views against your data warehouse. In this way, you would keep all of your data in one place, but you could use views to filter the data such that each subset is viewable as a distinct entity. Since you really only have one copy of the data, it won't matter whether the insert/update/delete is done to the filtered view or to the unfiltered table. Both sides will see it at the same time and there will never be a synchronization problem. Note that in order for a view to be writable, certain conditions must be enforced. See here. 

Accounting is very complex and is mission critical for any business. The most central piece of an accounting system is the general ledger. Building your own G/L is pretty hard to justify. You should buy a well-supported desktop G/L, like QuickBooks. There are some good, cost-effective choices available. The other functions in your business, even ones that are related to accounting (outside of G/L) can be specialized. Focus on custom-building these areas, if you find that a general purpose package really doesn't handle your needs. I have built customized subledgers for areas like cost accounting and profitability reporting for businesses with highly specialized needs, but they still used off-the-shelf G/L software. You won't necessarily end up with a perfectly suited, fully integrated custom system, but you don't have time to build such a system and still operate your practice. 

You might consider adding a partitioning attribute to the table that would indicate which educational system is being applied. This may be helpful for some of your application logic that might need to apply different processing depending on which system is in use for a given . 

Your professor is wrong. The cardinality in almost all versions of crows foot notation is read such that the cardinality next to the entity is the cardinality of that entity. You can also see by the fact that common sense tells you that whoever drew this ERD did not intend what your professor is reading. I will say that I have (very rarely) seen people read cardinality the way your professor is doing it. I think of it as "looking down the relationship" at the other side. The fact is that this approach doesn't make sense for readability in my opinion. It's also just not the way the vast majority of people do it. You're better off to stick with the conventional wisdom on this. If you can't convince your professor, then go with the flow until the end of the semester, but then forget this inappropriate convention and do it the way almost everyone else does from then on. As an aside/pro-tip: when you see many/optional cardinality, I'd suggest you get in the habit of reading it "zero or more", not "zero or many". I think "zero or many" kind of implies "not 1", which isn't the nature of such cardinality. 

EDIT: It's important to get the terminology clear. When you say "bin" you mean a box without a lid that sits on a shelf. In a lot of inventory situations, a "bin" is a space on a rack where you could put a skid or one or more shelves. As long as we're clear, you want a hierarchy of SHELVES each of which has many BINS each of which can have many BIN_CONTENTS each of which is an intersection between BIN and ITEM and which has a quantity. Don't store quantity on your ITEM table, store it on the intersection table, that way you can have multiple bins of the same item, or even many different kinds of items, each in their own distinct bins or even multiple different items mixed within a bin, all on the same shelf. Consider the following ERD: 

The General Advice: When you are starting off learning how to model databases, one of the most important rules of thumb is: Every tangible thing that matters to your system is probably an entity type. This is a really good place to start with any logical database design. If you spend some time up front thinking about what kind of things matter to your system, then you're going to come up with a solid foundation on which to build your system. The things your organization cares about will change much less frequently than the business processes and rules your organization uses to deal with those things. That is why a solid data model is so important. Another important rule of thumb is: Normalize your data model by default and only denormalize when you have a (really) good reason to. This is especially true for a transactional system. Reporting systems and data warehouses are a different story. The Specific Answers: Cardinality: If you think about it, it is easily the case that a car could have never been serviced (by your shop). Therefore a minimum cardinality of zero is very plausible. On the other hand, by the time the vehicle matters to your system it may well be because it has had its first service - so a minimum cardinality of one is also plausible. You need to think about what the business rule is for your organization and model accordingly. I would think, for example, that a car dealership would have lots of cars in its system that haven't been serviced by the dealership yet, whereas a muffler shop wouldn't care about cars it hasn't serviced. Service Items: You asked: 

Also, some taxes are calculated on the base amount and others are compounded (applied to other taxes) as well. These are some of the kinds of rules that apply in the jurisdiction where I live. You might have others as well. I would suggest not limiting yourself to two applicable taxes per item. Instead, I would create an intersection table (many-to-many) that indicates which taxes are applied to each item. This could be driven by a similar intersection table between PRODUCT and TAX to indicate which taxes are usually applicable to each product, if that is an important distinction in your case. While adding another table may not seem like a simplification, it will actually make your code simpler and easier to maintain because you are normalizing your applicable tax data. The design you have illustrated isn't even in 1NF, which is almost always a recipe for trouble. 

I would recommend against updating the "updated_at" value for a post when someone comments on that post. Instead, keep the time that each record is inserted or edited on the record itself (main post, comment or reply on a post). Updating the parent record when a new child is inserted hides information and makes some types of questions difficult to answer. You are better off keeping a separate activity table that records transactions relating to posts and comments being added or updated. This way you can join the content to the activity transaction and sort by the most recent (i.e. max) transaction date (group by item ID). EDIT: @Luccas - The issue is the design of your activity table. First, you seem to have left off any reference to time in your sample data, but your query references so I assume that column should be there. Also, I'm not sure I understand what you mean by - I may not understand what you intend there. From what I can tell, what your table is missing is a reference to the parent post which must be populated every time. Activities which are posts should refer to themselves. Activities which are comments should refer to the posts that the comment applies to. Then to get most recent posts you select from your posts list, filtering for type ="post" and joined to your activities list (for all types). You need to group by the columns you're using from the post and use the join to select max(created_at) from the unfiltered activities. You then sort the whole thing by this date/time descending. This way you select only posts and sort them by the latest time for each post.