For any group $G$, the $G$-test problem is clearly in NP. My question is: Is there a group $G$ such that the $G$-test problem is NP-hard? A few remarks about equivalent problem statements: 

For each $k \in \mathbb{N}$, I will call $\mathcal{T}_k$ the class of the graphs of treewidth at most $k$. I'm interested in the problem $\text{Hom}(\mathcal{T}_k, \mathcal{T}_k)$, which I see as a parameterized problem (by the treewidth bound $k$). My question is: what is the complexity of this parameterized problem? Is it known to be FPT? or is it W[1]-hard? Here are some things that I found about the $\text{Hom}$ problem, but which do not help me answer the question. (I write $-$ for the class of all graphs.) 

The homomorphism problem $\text{Hom}(\mathcal{G}, \mathcal{H})$ for two classes $\mathcal{G}$ and $\mathcal{H}$ of graphs is defined as follows: 

Note also that this line of thinking is very old: J. von Neumann wrote about this in 1956 [8]. Maybe even more impressive C.-A. Petri invented Petri-nets in August 1939 at the age of 13 for the purpose of describing chemical processes. 

What's relevant here is the base case (1). Translated into prime factorisation, it says that you need to know something about the factorisations of $x+1$ and $y+1$ from the factorisations of $x$ and $y$. However, as far as I'm aware the factorisations of $x$ and $x+1$ are essentially unrelated. 

H. Barendregt, Introduction to generalised type systems. H. Barendregt, Lambda Calculi with Types. B. C. Pierce, Types and Programming Languages. 

You can turn every programming language $L$ into a homoiconic language $L_{hom}$ by adding a suitable representation of programs, such as ASTs (abstract syntax trees) or quasi-quoted programs, together with some operations that mediate between $L$ and the representation of $L$-syntax (e.g. evaluation or splicing instructions). The details of this depend on your precise rendering of homoiconicity (e.g. compile-time as in Template Haskell vs run-time as in the MetaML family of languages or Javascript). One extreme example is the empty language $L^{\emptyset}$ that has no programs. Clearly its homoiconic variant $L^{\emptyset}_{hom}$ isn't Turing complete. For more interesting examples, you can take any of the usual type theories such as the simply typed $\lambda$-calculus, System F or the Calculus of Constructions. Of course one would have to prove that the homoiconified version of such type-theories doesn't allow us to encode recursion somehow. I doubt that this has been done. Cody's comment below suggests that this might not be possible. 

I've implemented a completely functional DSL, and now I'd like to reason about it. It would be helpful to be able to compare it to existing languages. The type system is parametric polymorphic with higher order functions but no dependent types. The only recursion allowed is structural recursion in the form of a fold over an inductively defined type (including natural numbers). The way I see it, the language shares its turing-incompleteness and form of recursion with Coq, but shares parametric polymorphism with Haskell. Is there a more exact comparison to be made? In particular, I was wondering if System F can be pared down to only support structural recursion, and what that would look like. Thanks! 

I am working on an application using Quantifier-Free Bit-Vector/Array satisfiability which may or may not require mucking around with the internals of an SMT solver, and would like to understand what's going on behind the scenes of existing methods (beyond merely reading their API's). What papers should anyone interested in working with SMT solvers, particular in QF_ABV, read? I'd be particularly interested in 1) why and how current SAT solvers are able to work so well on an NP-complete problem and 2) the specifics of how bit-vector and array theories are implemented and any tricks that speed them up. 

Mutation Testing Repository: Mutation Testing Theory. R. A. DeMillo, R. J. Lipton, F. G. Sayward, Hints on Test Data Selection: Help for the Practicing Programmer. R. G. Hamlet, Testing Programs with the Aid of a Compiler. S. Berghofer, T. Nipkow, Random testing in Isabelle/HOL.. 

W. Taha. Multi-Stage Programming: Its Theory and Applications. W. Taha and M. F. Nielsen. Environment classifiers. T. Sheard and S. Peyton Jones. Template meta-programming for Haskell. L. Tratt. Compile-time meta-programming in a dynamically typed OO language. M. Berger, L. Tratt, Program Logics for Homogeneous Meta-Programming. R. Davies, F. Pfenning, A modal analysis of staged computation. R. Davies, A Temporal-Logic Approach to Binding-Time Analysis. T. Tsukada, A. Igarashi. A logical foundation for environment classifiers. 

Let me explain this in more detail. Turing machines can obviously model all existing interactive models of computing in the following sense: Choose some encoding of the relevant syntax as binary strings, write a TM that takes as input two encoded interactive programs P, Q (in a chosen model of interactive computation) and returns true exactly when there is a one-step reduction from P to Q in the relevant term rewriting system (if your calculus has a ternary transition relation, proceed mutatis mutandis). So you got a TM that does a step-by-step simulation of computation in the interactive calculus. Clearly pi-calculus, ambient calculus, CCS, CSP, Petri-nets, timed pi-calculus and any other interactive model of computation that has been studied can be expressed in this sense. This is what people mean when they say interaction does not go beyond TMs. If you can come up with an interactive formalism that is physically implementable but not expressible in this sense, please apply for your Turing award. N. Krishnaswami refers to a second approach to modelling interactivity using oracle tapes. This approach is different from the interpretation of the reduction/transition relation above, because the notion of TM is changed: we move from plain TMs to TMs with oracle tapes. This approach is popular in complexity theory and cryptography, mostly because it enables researchers in these fields to transfer their tools and results from the sequential to the concurrent world. The problem with both approaches is that the genuinly concurrency theoretic issues are obscured. Concurrency theory seeks to understand interaction as a phenomenon sui generis. Both approaches via TMs simply replace a convenient formalism for expressing an interactive programming language with a less convenient formalism. In neither approach genuinely concurrency theoretic issues, i.e. communication and its supporting infrastructure have a direct representation. They are there, visible to the trained eye, but encoded, hidden in the impenetrable fog of encoding complexity. So both approaches are bad at mathematisation of the key concerns of interactive computation. Take for example what might be the best idea in the theory of programming languages in the last half century, Milner et al's axiomatisation of scope extrusion (which is a key step in a general theory of compositionality): $$P|(\nu x)Q \ \equiv\ (\nu x)(P|Q) \quad\text{provided}\ x \notin fv(P)$$ How beautifully simple this idea is when expressed in a tailor-made language language like the pi-calculus. Doing this using the encoding of pi-calculus into TMs would probably fill 20 pages. In other words, the invention of explicit formalisms for interaction has made the following contribution to computer science: the direct axiomatisation of the key primitives for communication (e.g. input and output operators) and the supporting mechanisms (e.g. new name generation, parallel composition etc). This axiomatisation has grown into a veritable research tradition with its own conferences, schools, terminology. A similar situation obtains in mathematics: most concepts could be written down using the language of set theory (or topos theory), but we mostly prefer higher level concepts like groups, rings, topological spaces and so on. 

Apologies if this is not a well-thought-out question, but I am interested in formalizing a problem which is ultimately described by a SMT formula in the theory of quantified arrays and linear arithmetic, and I realized that, while I understand what SMT formulas are useful for in a practical sense, I do not have a good sense for how to fit what I know about foundations of logic/computing/type theory/category theory with the idea of SMT theories and formulas. What can be said formally about the general theory about statements which can in some way be turned into SAT instances? Edit: I understand that SAT solvers work with propositional logic and SMT solvers work with fragments of first-order logic, but how exactly do we understand what a "fragment of logic" is and how it chops up the whole universe of first-order logic? Obviously we have different predicates and functions for linear arithmetic vs the theory of data types, but how at a formal level can we come up with a mathematical universe in which certain statements can be combined and others can't? 

You can see sequential computation as a form of well-behaved, nice concurrent computation. This niceness comes out when comparing the processes of reasoning about, an implementing sequential vs concurrent computation. But this insight is much older and forms the essence of the Actor model. See for example Hewitt's 1976 "Viewing Control Structures as Patterns of Passing Messages". There are various ways of enforcing sequential computation in process calculus. A simple one is only to allow processes that have at most one active output, i.e. an output that is not under an input prefix. This can be enforced by typing see e.g. "Strong Normalisation in the $\pi$-Calculus". Now coming to your question: does interaction on $x$ in $x(v).P \ |\ \overline{x}y.Q \rightarrow P[y/v] \ |\ Q$ represent sequential behaviour as such? No. The reason is that other processes could be running in parallel with $x(v).P \ |\ \overline{x}y.Q$. All we are seeing here is an exchange of messages, a handshake, where the receiver has to wait until the sender's message arrives. 

We further require that there are no duplicate elements in the infix tree, i.e., for every node $x$ of the infix tree, each leaf of $P$ appears at most once in the labels of $x$ and its descendants. A node $x$ in an infix tree codes a set $S(x)$ of leaves of $P$, namely, those which appear in the node's label and its descendants (and as we just assumed it is always duplicate-free). The idea of the infix tree is that, by keeping around some explicit elements, we can both use them when enumerating to make progress when visiting the nodes, and use them when unioning two sets of descendants to have sufficiently many elements to annotate the newly created nodes in the infix tree. Our indexing data structure will map each node $n$ of the polytree to an infix tree node $N(n)$ capturing exactly its reachable leaves, i.e., $S(N(n))$ is the set of leaves that are reachable from $n$ in $P$. There are two claims: 

The logics mentioned are what is called observationally complete, meaning that the operational and the logical semantics coincide. Arthur Charguéraud used this completenss for his work on verifying functional programs Hoare-style in Coq. 

You are probably thinking of Gower's work with Ganesalingam, based on the latter's MSc dissertation (1). Gowers blogged about this in (2) and other places, and they've written a paper on the subject (3). There is other work in that direction, for example from the interactive proof assistant community. The most well-known example here might be the Isar language (4). This is quite an active area of research, see e.g. (5). I know that this is also pursued by more linguistics oriented researchers, but I don't have references handy. 

Communicating and Mobile Systems: The $\pi$ Calculus, by R. Milner. The $\pi$-Calculus: A Theory of Mobile Processes, by D. Sangiorgi and D. Walker. A Calculus of Mobile Processes, Part 1 and Part 2, by R. Milner, J. Parrow and D. Walker. The Polyadic pi-Calculus: A Tutorial, by R. Milner. Communication and Concurrency, by R. Milner. A Calculus for Cryptographic Protocols The Spi Calculus, by M. Abadi and A. Gordon. Mobile values, new names, and secure communication, by M. Abadi and C. Fournet. Applied pi calculus, by M. Ryan and B. Smyth. 

In a recent paper, we propose such a scheme. The scheme is illustated in a specific crowdsourcing application setting, but the idea is fairly simple. We just see the order constraint $\mu(u) \leq \mu(v)$ of each DAG edge $(u, v)$ as a linear inequality constraint, we consider the admissible convex polytope of assignments to these linear inequalities, and we choose the center of mass of this polytope as our interpolation result. The paper explains the scheme in more detail, includes an algorithm, computational hardness results, an approximation scheme that derives from existing work, a tractable case when the DAG is a tree, and a discussion of alternate schemes. 

Consider a non-empty language $L$ of binary strings of length $n$. I can describe $L$ with a Boolean circuit $C$ with $n$ inputs and one output such that $C(w)$ is true iff $w \in L$: this is well-known. However, I want to represent $L$ with a Boolean circuit $C'$ with $n$ outputs and a certain number of inputs, say $m$, such that the set of the output values of $C'$ for each of the $2^m$ possible inputs is exactly $L$. 

According to ncatlab's page on category theory and haskell, "we can identify a subset of Haskell called Hask that is often used to identify concepts used in basic category theory. One considers Haskell types as objects of a category whose morphisms are extensionally identified Haskell functions." So types are objects, and functions are morphisms. How, then, does a value, such as the list [1,2,3] or the boolean "true", fit into a category-theoretic definition of Haskell? (I realize that lists, as monads, are presumably different in any category theoretical representation than booleans, which could be described as a coproduct, but I don't understand how the actual values in either case are related to the definition of Hask). 

In particular, I am thinking of a function which involves conditionals changing the recursive behavior and multiple F-algebras. 

Is this capable of being written just using the operator? The reason I'm asking is because I want a functional language that is fairly expressive but not overly expressive, and it seems like one way to do that might be to not allow explicit recursion but make a primitive function. 

It turns out that it is possible to do what I want with linear preprocessing and constant delay, by an argument suggested by my friend Louis. Here is my summary of his idea. Edit: The scheme below, in a much more concise and understandable form, appears as Theorem C.1 in our recent preprint. I'm leaving the messy sketch below in case it's useful, but you should probably look at the preprint instead (the theorem and proof are independent from the surrounding material). The proof is generalized to multitrees and also refines the scheme slightly to show a logarithmic bound on memory usage. 

I will introduce my problem with an example. Say you are designing an exam, which consists of a certain set of $n$ independent questions (that the candidates can get either right or wrong). You want to decide on a score to give to each of the questions, with the rule being that candidates with total score above a certain threshold will pass, and the others will fail. In fact, you are very thorough about this, and you have envisioned all the possible $2^n$ results, and decided for each of them whether a candidate with this performance should pass or fail. So you have a Boolean function $f : \{0, 1\}^n \to \{0, 1\}$ that indicates whether the candidate should pass or fail depending on their exact answers. Of course this function should be monotone: when getting a set of questions right makes you pass, getting any superset right must make you pass as well. Can you decide on scores (positive real numbers) to give to the questions, and on a threshold, so that your function $f$ is exactly captured by the rule "a candidate passes if the sum of scores for the correct questions is above the threshold"? (Of course the threshold can be taken to be 1 without loss of generality, up to multiplying the scores by a constant.) Formally: Is there a characterization of the monotone Boolean functions $f: \{0, 1\}^n \to \{0, 1\}$ for which there exist $w_1, \ldots, w_n \in \mathbb{R}_+$ such that for all $v \in \{0, 1\}^n$, we have $f(v) = 1$ iff $\sum_i w_i v_i \geq 1$? It is not so hard to see that not all functions can be thus represented. For instance the function $(x_1 \wedge x_2) \vee (x_3 \wedge x_4)$ cannot: as $(1, 1, 0, 0)$ is accepted we must have $w_1 + w_2 \geq 1$, so one of $w_1, w_2$ must be $\geq 1/2$, and likewise for $w_3, w_4$. Now, if it is, e.g., $w_1$ and $w_3$, we have a contradiction because $w_1 + w_3 \geq 1$ but $(1, 0, 1, 0)$ is rejected; the other cases are analogous. This looks to me like a very natural problem, so my main question is to know under which name this has been studied. Asking for a "characterization" is vague, of course; my question is to know whether the class of functions that can be represented in this way has a name, what is known about the complexity of testing whether an input function belongs to it (given as a formula, or as a circuit), etc. Of course one can think of many variations on this theme. For instance, on real exams, questions are not independent, but there is a DAG on questions indicating the dependence, and candidates can only answer a question if all prerequisites have been answered. The condition on the monotone functions could then be restricted to valuations in $\{0, 1\}^n$ that satisfy the dependencies, and the question would be to determine whether an input function can be thus captured given an input DAG on the variables. One could also think of variants where the scores are $k$-tuples for fixed $k$ (summed pointwise, and compared pointwise to a threshold vector), which can capture more functions than $k = 1$. Alternatively you could want to capture more expressive functions which are not Boolean but go to a totally ordered domain, with different thresholds that should indicate your position in the domain. Last, I'm not sure about what would happen if you allowed negative scores (so you could drop the monotone restriction about the functions). (Note: What made me wonder about this is the Google Code Jam selection round, where candidates are selected if they reach a certain score threshold, and the scores of problems are presumably carefully designed to reflect what sets of problems are deemed sufficient to get selected. Code Jam has a dependency structure on the questions, with some "large input" questions that cannot be solved unless you have solved the "small input" one first.)