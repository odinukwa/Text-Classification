There is nothing wrong with nullable columns, even a lot of them, if that is what your data domain calls for. On the other hand, the fact that you say these columns can be grouped logically implies to me that something else might be going on. If they can be grouped logically because different sets of columns apply to different sets of rows, then you might have an entity-subtype situation. Conversely, if columns can be grouped because they apply at different times, then you may have a normalization issue. For example, if your columns are something like "January Sales", "February Sales", etc. these should be rows in a child table. While there is nothing innately wrong with nullable columns, neither is there anything wrong with joining. It's what RDBMS does for a living. UPDATE: Given additional information about the logical groups of columns: There are two kinds of sub-typing that can be represented in a database using 1:1 relationships. If the logical groups were mutually exclusive, then the parent entity could have what is known as a partitioning attribute that tells you which one of the subtypes is applicable. However, without a partitioning attribute, it is possible to have zero, one or even multiple subtypes being applicable at the same time. The same fundamental question applies then to what you do with this situation. A good way to resolve it would be to look at the logical groups of columns. Are the columns in logical group A the same as in logical group B - or are the totally different? If they are different they might be best modeled in the single table with nullable fields. If they are the same, then this might be a clue that they should be multiple child rows instead. Another clue to look at is whether it makes sense that a logical group of columns could take on a life of its own and start attracting relationships from other tables. If logical group B might sometime soon find itself with multiple child records from another table, then it might be a sign that it makes sense to promote that group to its own subtype style table. One last thing to consider is physical implementation. If a logical subgroup is very sparsely populated, you might be able to make a case for segregating these columns into another table to optimize physical storage. This step shouldn't be done proactively. This kind of optimization should be done when performance testing proves it is necessary. If none of these things are true, then you are probably best off leaving the nullable columns in the original table. 

Your table is an intersection between and . That is a perfectly legitimate design. Intersection tables don't have to be pure intersections. They can have other attributes in their own right (such as your or ). 

Your first design is the way to go. Keep everything about a process on a single record. Your challenge isn't one of database design, it's one of restartability/disaster recovery. Since your processes are long-running, you have a higher risk of the data getting out of sync with the processes that are being tracked in the data. For example, you lose power causing a process to crash. The solution is that you need to change your processes so that when they start they confirm what the data says. Does the database say that process X is still unfinished? Is process X still running? If not, your data is out of sync and you need to mark the record for process X as failed. You might want to consider adding a couple of columns to your first table design, for example: 

The table is optional. You could use it if you have a standard list of fields that can be added to a template. If you don't want to lock down this list or reuse fields then you can skip this table and just use by itself. 

Track attributes against a product (always) and optionally against a retailer. If the attibute value is used by all retailers then the retailer foreign key would be NULL. When you join attributes to a product include an OR clause in either your join or where (depending on how you do your joining) so that you select out attribute values where the retailer FK is as given or where the retailer FK IS NULL. 

Then you need to decompose this relation to get it to third normal form (3NF). This is because D is not determined by the key of . To bring this to 3NF you need to have two relations: and . Now each non-key attribute is fully dependent on the key of its relation (and nothing else). 

Use a variation of number 2 - a location table with a involuted (self-referencing)relationship, but add something to make the hierarchy management easier. For example, use nested sets/visitation numbers (see here) or an adjacency list. The advantages of this approach are that: 

Grade level represents the year/form/grade at which the subject is being presented. The material actually being taught is the subject. When certain material is taught to students at a given grade level, that is a course. When a particular instructor teaches a course to a given group of students in a certain place at a designated time (or set of times) that is a class. Whether you choose to use natural or surrogate keys is a separate discussion. The fundamental structure of your data model will probably end up looking like this ERD. 

I take it that you are OK with these assumptions, since you have noted an index on in your question. Then your user's global ranking for their scores would be something like this: 

The best approach for this type of situation is to use logical sub-typing. Keep columns that are common in a central table which also contains a partitioning attribute. Then keep sub-type specific columns in sub-type tables which have identifying 1:1 relationships with the common table. In your case, this would look something like this: 

If your various types of individuals and organizations (employees, clients, suppliers) each have multiple phone numbers of various types, then you want to keep the phone numbers in a child table. This child table should be an intersection between the individual/organization and a phone type table. Since you have different allowable phone types for each type of legal entity, you can create a table for each of these lists of allowable phone types (your Case 1). The only issue with this is that you may have duplication of data, for example having a "mobile" record in multiple allowable types tables. There is a way to have distinct lists of allowable phone types while at the same time rationalizing these lists for management purposes. Consider this ERD: 

This lets you keep the initial origin and the final destination in your trip table and it lets you add zero or more waypoints along the way without storing any redundant data (i.e. the schema is in third normal form). 

You need to learn the difference between repetition and redundancy. Sometimes a field value is repeated coincidentally. This kind of repetition cannot be normalized out. Normalization is about studying the functional dependencies between key and non-key elements. It is not about putting everything that might occur twice into a table with a surrogate key. For example, you say that a phone number may be used by multiple companies and have modeled phone number as an independent table. This would prevent update anomalies if when one company changed its number all of the other companies using that number changed theirs at the same time and in the same way. Does any of that actually make business sense? It doesn't to me. Also, normalizing geography into a hierarchy is a questionable proposition. However, you haven't even normalized into a hierarchy. Is region ID not determined by country ID? If so, your LOCATION table is not 3NF. Similarly, are there postcodes that bridge states? I don't know the Australian system well enough, but I know that in Canada, the US and the UK this doesn't happen. You've designed everything like its a star schema and company is the fact. Facts in star schemas are usually transactions, not static entities. Transactions don't tend to change their properties, because they usually represent something that actually happened and one doesn't generally go back in time to change history. Static entities live for a long time in your data (maybe forever) and change their properties fairly often. Star schema is not a good, efficient model for this type of data. What you should do is run queries on the frequency distributions of each column and on combinations of columns that you think may be keys combined with columns that you know aren't keys. This will help you test your assumptions about which columns are truly able to act as keys. Then you should temper those findings with some common sense around what could potentially happen to your data in terms of changes to column values. Once you know your actual functional dependencies found in your data, then follow the relatively mechanical process of moving your relation to 3NF. 

What you are describing is a data warehouse. The live, normalized, read-write system is OLTP (online transaction processing) and the denormalized read-only snapshot is a data warehouse. The structure of the data warehouse could be a Star Schema, especially if it's highly denormalized. Data warehouses often have summarization in addition to denormalization. There can be many copies of the same data summarized over various dimensions and/or timeframes. The disadvantages of this technique are that the snapshot is generally not 100% up to date and you have to be very careful about how your snapshot is taken or you could actually introduce discrepancies other than timeliness into your data warehouse. Another possible issue is that you may have difficulty doing some kinds of reporting out of a data warehouse because of the choices you made when rolling up details into you summary tables. Also, if your data warehouse has multiple summaries over different timeframes, for example, you have to be careful to keep these consistent with one-another. The timeliness issue in particular is one you have to be careful about. I've seen users make a change to their online system and then get angry that it didn't show up right away in a report that is run against the data warehouse. Users tend not to know or care about the vagaries of reporting systems. 

There are very few scenarios in which you would probably benefit from your second design: One is if there is a sub-typing scenario where only certain a certain sub-type had the relationship (and the super-type doesn't) and the sub-type has other additional fields, besides the foreign key. Another might be where the existence and value of the foreign key needs to be controlled for security reasons at a different level of granularity than the rest of the entity containing the foreign key. Most DBMSs can handle security per table. Most don't handle security per column. 

From what I understand of your spreadsheet and your description, this is the normalized logical ERD for your data: