You can see sshd (through PAM) tries to authenicate me against the Domain Controller, fails and then attempts against the local user accounts (where I promptly mistype my password). EDIT: I just saw your comment. Take a look at Fail2Ban which will probably do what you want. No need to reinvent the wheel and all that. 

Running the WQL against the ProductName property seems to be a good way to go. If I run in against the namespace I get the following: 

It sounds like you've inherited quite the mess and unfortunately it is such a broad task to clean something up like this it is hard to give specific advice beyond this: 

I am working on a Freeradius backed 802.1.x authentication infrastructure for our wireless clients. I am using a rather generic Freeradius configuration with EAP-PEAP. Our clients are predominantly Windows XP SP3 machines but a few Windows 7 32 and 64 bit laptops also exist. Our domain is at the Windows Server 2003 functional level. 802.1x authentication is working with manually configured test clients. I want to create a GPO that autoconfigures our clients by 1) deploying the self-signed CA certificate to them as a Trusted Root Certificate, and 2) sets up our ESSID as a preferred network with the appropriate 802.1x configuration. I am having no difficultly deploying the self-signed CA certificate to clients using a GPO. However, I cannot figure out how to configure the certificate as a Trusted Root Certificate in the GPO. This is from the GPO settings found under : 

I think you have some misconceptions about IPv4 exhaustion. Just because there are not more IPv4 addresses to be allocated does not mean there are no IPv4 addresses available. ISPs (generally Tier-1 providers) and many older established companies still have lots of IPv4 space left they can assign to customers. What it does mean is that RIRs have no more addressing space left to assign. This means ISPs and companies will have to make do with what they already have. The United States and Europe have managed to snarf up the lions share of the IPv4 space so the shortage is going to be felt much sooner and harder in Southeast Asia. On top of this, there is always NAT which lets your stretch your existing IPv4 space even further. When there are no more IPv4 address to be allocated (already happened!) it is not like IPv4 immediately stops working. The existing IPv4 customers will still be able to use their existing IPv4 addresses and traverse the existing IPv4 internet. It's not like a magical switch gets thrown and IPv4 becomes broken. This is whole point of running dual stack. 

To further complicate things I'm seeing multiple explanations of the error. WMI Troubleshooting Tips says that it is a failure to load a WMI provider: 

This is a tricky problem. The best solution would be to replace the LAN2 Router with something that you actually have a degree of management control over. If you can't disable NAT there are probably other places this router falls short. However, my hunch is that you cannot replace this router with something more appropriate for political reasons (otherwise you already would've done it...). My first suggestion would be to put a Squid proxy in somewhere and use that log to per user internet access. The best place to put it would be on LAN1, but then all of web traffic from clients on LAN2 will appear to originating from the LAN2 Router courtesy of NAT. You could place it downstream of the LAN2 router, but then you'll have to port fowrard through its NAT so the clients from LAN1 can reach it. Doing this is kind of ugly. Another idea would be to rely on something like Netflow, SFlow or RMON if your switching infrastrucutre on LAN2 (and LAN1) supports it. Just forward the appropriate ports through your LAN2 router and place your flow collector on LAN1. Unfortunately, flow analysis is largely Layer-7 unaware so while you will be able to measure traffic, usage and counter statistics you won't get the HTTP-centric details that a web proxy would provide. Still it might be better than nothing. 

You will want to make these settings mandatory (otherwise a knowledgeable user can just un-set them). See the Gnome Deployment Guide for more information. 

If you're interested in using SCCM's Configuration Baselines (or just regular old Group Policy) to check for compliance and/or force remediation of IE ESC here are relevant Registry Keys: 

If my reading of MSDN is correct MSIs register the ProductCode in . A reasonable assumption is that the Application Detection checks that location, again you could confirm this with ProcMon. 

OK. It sounds like you are working with a pretty small environment. You might want to reconsider whether or not SCCM is an appropriately size toolset. Take a look at my answer to Is SCCM overkill for medium-sized organizations? and give it it some thought. You might be happier with Windows InTune or a smaller, less complex, less featureful endpoint management system. 

Yep. I understand that, but please consider some of the technical limitations and dangers you might find yourself in down the road. A datacenter license of Windows Server looks like mighty affordable if SCCM has exploded your site's only domain controller. 

In more detail we have a 48 port HP2610 (Switch A) and a 24 port HP2610 (Switch B). Switch B is "downstream" of Switch A by virtue of a DSL connection to one of Switch A ports. The dhcp server is connected to Switch A. The relevant bits are as follows: Switch A 

WAN2: So as you can see from the Rrdtool graphs while WAN1 is hovering around 2.5-3Mbps during peak usage, WAN2 seems to always stay around 1-2Mbps range. The and values have been set the maximum value (65535) for WAN2 and set to 10 for WAN1. The manual has this to say about these configurations options: 

I'm much more conversant with HP ProCurves but I think Cisco works the same way in this respect. Each VLAN has its own IP address that you can use as a gateway to that subnet. You either need to publish a route to those subnets to your clients or to re-enable on your switch so that your switch will route between the directly-connected interfaces. See: Configure InterVLAN Communication 

Absolutely. Publically facing services should be isolated in a DMZ. Period. Anything that the big bad Internet can reach should be separated from your internal network and its services. This drastically limits the scope and damage a security breach will cause. This is good application of The Least Privilege Principle and Functional Separation.. 

This is really a question about password re-use. Ostensibly, any password re-use at all is considered a Bad Thing(TM). The mechanism only works because the password is a secret known only to you... and the more frequently you use the secret the less likely it is to remain a secret. However in the real world we have to re-use passwords, either by literally re-using the same password in multiple accounts or by using one master password to secure a list of unique passwords. The answer to this question really depends on your environment and threat model. As @boris quiroz pointed out, what kind of server is? Who needs access to it? In what other places would the password likely be re-used (i.e., is just for SSH or is also for the nuclear missile control panel? It'd probably be fine to re-use the password for the former, but not for that latter). What do you stand to lose if the password is stolen (i.e., how many eggs are in that single basket)? And what's the price of the data hidden away in your database with the password-less accounts compared that? Then document this process and the reasons underlying your decision. If there's ever an issue in the future, you can justify your choices. There's no hard and fast rules here. You just need to think about the threat model (what am I protecting and from whom), and decide whether the benefits outweigh the costs. Security is always a compromise, so make sure you buy as much of it with what you give up (say in administrative overhead for example) as you can. Personally, I would be inclined to use the passwords on both the SSH and MySQL accounts, preferably different ones, but even a re-use here would be better than nothing in my opinion. (Unless you have lots of user accounts, which is an entirely different problem). 

I know you have not been able to reproduce the problem if you remove your router from your network path but have you verified that this problem exists with other clients? I would ensure that you can reproduce this problem on more than one client (try your ping test on another machine) before you continue with the assumption that the router is underlying problem (although it likely is). DD-WRT has never really impressed me as a very stable platform. It's hard to tell whether that's a result of the COTS-nature of the hardware it gets run on, DD-WRT itself or a combination of the two. Regardless a quick google and a stroll through the forums finds lots of "connection dropped" issues. Packets are often dropped because there is not enough memory to keep the state of all TCP connections that are made by the clients. This is a common problem for COTS "routers" regardless of firmware (DD-WRT has a wiki about a related issue here). Try accessing your router via ssh (or telnet) and looking through for anything from ip_conntrack (the kernel module that is used to keep track of connection state). You'll likely find something like this: . I see you have already adjusted the setting to the maximum of 4096 but try verifying that using the command line (). If your WRT150N has a decent amount of memory (e.g., 32MB or greater) you can manually set to a number larger that 4096 (see here). How many clients do you have behind your WRT150N? Are you using an P2P protocols? It has unfortunately been my experience that Linksys hardware and DD-WRT kind of suck especially in situations where there is any real kind of network traffic. It might be time to graduate to more robust solution. 

In general, the answer is no the user does not need to be logged in, the only exception being that if in the Application-Program model the Deployment Type you are using in this instance requires a user presence (Deployment Type - User Experience - Logon Requirement). I am not very familiar with the Package-Program model but I believe it does not have the ability to make this distinction. All that is required for software deployment is that the SCCM client can successfully pull the machine policy, download the content from the Distribution Point to the local ccmcache via BITS, have an actionable service window (either a Maintenance Window or an expired deadline) and away you go. The Management Point and Distribution Point need to be reachable by the client on HTTP and HTTPS. For Software Updates (i.e., SUP/WSUS) you need HTTP/HTTPS or 8530/8531 if you are using the alternative ports. See here for details. You also need to have your Boundary Groups and Distribution Point assigned appropriated. Naturally you will also need fresh DNS records and a network path from the client to the MP and DP in question. Generally if you the laptop comes in and sits on the corporate network long enough to "normalize" you should be fine without any additional configuration of SCCM infrastructure. 

SCCM is a pull-based technology. I know what you meant but I find that with my tier-1 guys, every opportunity I get to emphasize that SCCM is not a push-based technology helps them understand it quicker. 

In my opinion, only Bitlocker, but only if you already have the licenses. TrueCrypt is an excellent product in my experience. The other thing to mention about Bitlocker, is you still can't get away from the password issue... I believe the official line from Microsoft is that they do NOT recommend storing the password in TPM as it is vulnerable to a cold boot attack. From TechNet: "The TPM-only authentication mode is easiest to deploy, manage, and use. It might also be more appropriate for computers that are unattended or must restart while unattended. However, the TPM-only mode offers the least amount of data protection. If parts of your organization have data that is considered highly sensitive on mobile computers, consider deploying BitLocker with multifactor authentication on those computers." Additionally, the enterprise addition allows you use AD to store "recovery keys" (presumably copies of the keyfiles required for encryption. This is a nice integrated Windows version of TrueCrypt's Recovery Disk Functionality. 

Consider using Tripwire. Tripwire will allow you to monitor files for changes by recording their checksums and then encrypting them. It's not integrated in with the system as well as mtree is on BSD, so it requires more configuration and upkeep but it will get the job done. 

In my opinion moving the port that an application runs on does not increase security at all - simply for the reason that the same application is running (with the same strengths and weaknesses) just on a different port. If your application has a weakness moving the port that it listens on to a different port doesn't address the weakness. Worse it actively encourages you to NOT address the weakness because now it is not being hammered on constantly by automated scanning. It hides the real problem which is the problem that should actually be solved. Some examples: 

You're thinking about this the wrong way around. RDP should not be accessible from the Internet. Ever. But... you can specify the number of failed logon attempts before that user account is locked out using local security policy (Security Settings - Account Policies - Account Lockout Policies). However if I recall correctly Account Lockout Policies are not applied to the local administrator account. 

Is this 100 simultaneous clients total or 100 simultaneous clients per access point? Either way accomplishing this on your budget with any semblance of reliability will be incredibly difficult if not impossible, although the former is within the realm of possibility, just not your budget unfortunately. The reason you don't find any data on how many simultaneous clients consumer-grade access points can support is that consumer-grade access point barely support a dozen clients. They are just not designed for this type of use case. Cisco's recommendation is 24 clients per access point and that is with "enterprise-grade" Aironet equipment (reference). There is no product/s in the market that I'm aware of that can accomplish your goals within your budget. Anyone who tells you otherwise is probably lying. Robust industry grade 802.11 networking is very difficult to do right and thus correspondingly expensive. I would avoid DD-WRT (especially DD-wRT! UGH!) and consumer grade access points if at all possible. You'll be better served by putting your existing budget towards hiring a specialty contractor to temporarily setup and rent you a wireless infrastructure or investing in proper equipement. 

A bit inflammatory but his point is well taken. In theory virtualization is supposed to provide complete isolation between the virtual machines and their host. In practice there are occasional security vulnerabilities that allow advanced attackers to circumnavigate these protections and gain access to other virtual machines or even worse their host (see An Empirical Study into the Security Exposure to Hosts of Hostile Virtualized Environments). As Ryan Ries mentions these kinds of vulnerabilities are pretty rare (which doesn't mean they aren't there) and often not disclosed by vendors but they do exist. If you are concerned about the potential for these kinds of attacks (and I think to some degree you should be) I recommend that you do not mix security zones on a single virtual host or virtual host cluster. For example - you would have a dedicated two host virtual host cluster for DMZ virtual machines, a dedicated cluster for middleware and a dedicated cluster for protected assets. This way in the event that a vulnerability is exploited in such a way that allows an attacker to subvert other virtual machines or worse the hypervisor itself your security model is still intact. 

File / Print Services I can see a number of solutions for the file and print services - I believe that we can just extend the Extranet as a VLAN onto our virtulization platform and then we can eliminate the rackmount server and associated equipment. Unfortunately this does not cover the DR/BC services - I am looking at things like Azure File Storage, an Azure-based Virtual Machine that we use as a DFS target or even OneDrive for Business. I just cannot figure out how to glue these technologies together to address our requirements. Ideally we could just use some kind "cloud" service for file access but I am concerned with internet usage (restriction #1) and the lack of ability of having a local copy on-network in case of a service outage. I feel like there is "have my cake and eat it too" solution here but I just do not see it. Visibility and Management I would, love, love, love to have these computers joined to our Active Directory domain but I cannot see a way to do that considering restriction #2. I have started looking at Active Directory in Azure but admittedly I do not really understand it and it seems just limited to Single Sign-On services. What I really want is a way to get GPOs to those machines and have a central authentication store. I am further limited in that our Active Directory domain is managed by another group so any proposal to "extend it" would be politically and bureaucratically difficult but not impossible. Our AD team is working on an organization-wide Office 365 tenancy which will implement a DirSync of some kind but I cannot see what that would buy me other than OneDrive for Business (which could address file services but not configuration management). I am currently working on implementing Internet-based Configuration Management which if I can manage the bandwidth issues (restriction #1), I will get some visibility with Hardware Inventory, Windows Updates and 3rd Party Application deployments. Configuration Items are a pretty hacky way to replace Group Policy but I suppose, push comes to shove that would work.