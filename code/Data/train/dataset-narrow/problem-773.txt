You can build the "Export from SQL to Excel" part also via the SQL Server Import and Export Wizard in BIDS. Check this video for a demo. For the Send Mail Task, check this BOL article. If you want to automate and schedule it, you can use a SQL Agent scheduled job with an SSIS job step. A video demo is available here. If you automate it on a server, make sure Microsoft Access Database Engine 2010 Redistributable is installed (you do not need Excel there, just the "drivers"). 

The main point has already been stated in answer by mendosi: the Columnstore Object Pool is located outside the Buffer Pool. See the blog he linked. The question is a bit unclear. Do you want the Buffer Pool to have the same memory available when you start using columnstore indexes? If yes, then: 

As Jon mentioned, you can do it using system stored procedures. Ref: Glenn Berry wrote this for 2008 and this is to add extended property on a table. Also MSSQL Tips has a good example as well. You can adapt his script for columns as well. 

If you want to use SQL Azure database, then you have to go about either using any one of the approach as below : 

Instead of rewriting your own solution, my suggestion is to use SQL Server Maintenance Solution - SQL Server Backup Edit: Below script will help you once you deploy Ola's script and create required objects: 

I refer to the advice of Paul Randal from his article "Why you should not shrink your data files" (read the whole article to get a clear picture what happens with index fragmentation): 

Unless you have some kind of history table & trigger in place, to retain old values at every change, or you made a copy of the table before you ran the update, you will need to use the last backup that was taken before that update. Restore it (as some temp database) and extract the data. You do have a backup available, right? BTW, next time you're doing an update, I suggest you set it up like this: 

From my experience, its worth to invest in Redgate's Schema and data compare tools as they have commandline options and they integrate well with Powershell as well. 

So if you loose server C, the only option to get distribution running on server D is to do a from a good backup. You can use this script to restore your distribution database (with some changes as per your environment). I would go for a clean install of replication ! Make sure you script out your replication topology whenever your do any changes. You should have handy scripts of both drop and create, so in a disaster situation, you have scripts that will help easily create replication. Also, since you are using always-ON with T-Rep, I would suggest you to enable TF 1448. 

The where condition is not applied at this part, as you can see from the whole query you posted. Looking at the predicates and outputs of the nested loop and the clustered index scan, I suggest you try to create a nonclustered index on . 

On the other hand, if you're fine with Buffer Pool getting less memory and/or you can't increase memory on the machine - decrease the MAX memory setting on the instance, since you'll need memory for the Columnstore Object Pool. In any case, I recommend monitoring memory usage and tuning machine memory and/or MAX memory setting accordingly. As a starting point, some DMVs: 

Instead of reinventing the wheel, I would highly recommend you to take a look at Ola's SQL Server Integrity Check solution Efficiently running DBCC CHECKDB : You just need to be creative when you are tight in maintenance window having huge databases or high number of databases to run CHECKDB on. After attending SQLSkills training, what I have implemented in my environment is : 

Agree with Jon. Instead of going through the pain of setting up Maintenance Plans, I highly recommend to use Ola Hallengren's SQL Server Maintenance Solution This solution is flexible (can be adjusted as per your needs) and is supported on SQL Server 2005, SQL Server 2008, SQL Server 2008 R2, and SQL Server 2012 as well as it is widely used in SQL Server community. We have a much more complex environment and we use backup as well as maintenance solutions from Ola's site. 

It specifically mentions SQL 2012 at some point, but I guess the same would work for 2008 R2 aswell: 

The query you posted is not valid for creating a view; running for this query will result in an error. Are you using a clause? A view, being a table expression (a set), can't have the order defined, since that would be against the principles of a relational model (there is no order for rows in a relational table - a set is an unordered collection of tuples). Same goes for other table expressions - derived tables, CTEs etc. From BOL article about the clause: 

Unfortunately you cannot do this with Extended events unless the data from XE can be converted into the format that DTA uses or can be stored in the tables generated by DTA. Not recommended though but Trace is is still available, so can be used if you want. You can even use DMV's for finding missing indexes, etc. Also worth mentioning Aaron's post : Don't just blindly create those "missing" indexes!, since you are using DTA for analysis. 

All system and user databases should be check using CHECKDB. The only exception will be tempdb as you cannot generate a snapshot of tempdb and checkdb behind the hood generates a DB Snapshot. The tricky part is for tempdb (When Tempdb is corrupted) as we ALL know that it gets recreated when a SQL Server restart is done, but Tempdb files are not deleted when you restart SQL Server and are also not 0 (zero) initialized so the corruption can persist. Tempdb corruption (which is rare .. and most likely will be due to a bug in SQL Server) is described here. 

Long story short: Use the clause in the outer query that references the view. Do not use it in a view. Even using it with (or on SQL Server 2012, the equivalent) does not guarantee presentation order, it just means you'll get the top 100% of the rows, in any order. 

First, are you sure the data growth won't happen again? If there's a realistic chance it will and the empty space doesn't hurt you, leave it, do NOT shrink the database. However, if you're positive you want to reduce the data file size, then you should be aware of the pitfalls of data file shrinking: 

Script out the database SCHEMA_ONLY and recreate an empty database on the destination server. Below are the screenshots : 

If you are running SQL Server 2008 and up, then you can use Performance Data Collector on SQL Server. This will get you all the metrics (and even more) that you need. Below is from my TEST server : 

When in doubt, run process monitor. Also ProcDump is useful for getting dump of particular process. Very useful tools from sysinternals. 

You might have selected "restore with NORECOVERY" as below. You have to select "RESTORE WITH RECOVERY" to bring the database out of restoring state. 

I would not say that its a bad idea, but its always prefer not to touch the CPU affinity on sql server unless you know for sure that the problem is due to excessive context switching (and this too might be not the actual problem, but just a symptom). 

I can show you an example for SQL Server, but it should you help for your RDBMS aswell. I hope I understood the question. Suppose you want to get TOP users based on the number of same skills as the current user. Some sample data: 

Run the first to see exactly which rows will be updated. If needed, adjust the clause to get the rows you want to target. Only then mark the sentence from the till the end and execute it. Much safer then running a "blind" update, as you did, forgetting the clause (I assume that was the problem). Even safer - do it on some test database first :).