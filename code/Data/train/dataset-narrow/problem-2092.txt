There is a blog entry on how to put the entire root filesystem on NFS, and boot the Raspberry from there: $URL$ The generic documentation on how to boot your root file system via NFS is here: $URL$ Will give it a try and write a quick summary if successful. Edit: Nothing much to summarize. Works like a charm. 

Windows gives you a single implementation of a single desktop on top of a single implementation of a single API / framework, all done by Microsoft. On Unix systems, you get an API / framework (X11 / X Window System) for which exist multiple implementations (Xorg, Xfree86), on top of which you get various "higher level" API / frameworks (GTK+, Qt, ...) because raw X11 is so primitive, on top of which you get various desktops (Gnome, KDE, ...), all done by different people. Moreover, the X11 system has been designed from the ground up with remote GUIs in mind - i.e., a lokal machine displaying the GUI of a remotely running application - which introduces the concepts of a "X Server" and "X Client". Then there is a nomenclature that "feels" the wrong way around for newcomers: Your local machine is running the "X Server" providing the "display a GUI" service, while the remote machine is the "X Client" making use of the services on your machine to display the GUI. Well, that's the quick overview; once you got that sorted out, understanding any articles / forum posts on the subject should become much easier. Edit: To answer the two first comments of the OP. Yes, "X11" is merely a protocol, and Xorg / XFree86 are two implementations. At its basic level, X11 is only about drawing lines and dots, which is not terribly useful if you want to do a GUI. On top of the X11 protocol, people implemented many things, and it is pretty difficult to do a 1:1 comparison with Windows because Microsoft never bothered to really keep things separate. Also I am not a GUI-type developer, i.e. my actual experience with either system is minimal. At the bottom, a "window manager" provides a window (handling borders, close / minimize / maximize buttons, resizing etc.), and offers the "real estate" within the window to the widget toolset. There are many window managers, some mimicking other systems (Windows, MacOS, AmigaOS, whatever), and they are mostly interchangeable transparent to the remaining system. The "widget toolset" offers you buttons, sliders, text fields etc. on which to build your GUI. This is what you (as an application developer) actually get to "see", API wise, and what decides most of the "look & feel" of your application. A "desktop" builds a number of applications on top of a certain widget toolset / window manager combination, in order to provide a consistent look & feel. You don't have to bother with these unless you actually want to develop the desktop itself. The desktop "Gnome" uses the widget toolset "GTK+" on top of the window manager "Metacity". The desktop "KDE" uses the widget toolset "Qt" on top of the window manager "KWin". Note that especially those two, GTK+ and Qt, have evolved far beyond simple "widget toolsets" into "application development frameworks". If you want to develop GUI apps for Linux, effectively you have to pick which one of those two you want to use. There are more choices, if you want a more "lightweight" app (not needing the big library dependencies), but today most systems have GTK+ and Qt libs already installed anyway. It's perfectly possible to use Qt apps on a Gnome desktop or GTK+ apps on a KDE desktop (it wasn't always like that), so you have to worry little about compatibility. Given a choice between two apps of comparable functionality, people will usually prefer the app using the "native" widgets of their desktop of choice, but I wouldn't worry about that. Other, more important bullet points in the choice of "widget toolset": Licensing terms, support for your language of choice, cross-platform compatibility. 

I would need to print the contents of a file (e.g., ), but wrap output at a given column width (let's say 80 cols), independently of terminal width. Is there something in the "standard" Unix arsenal that would give me this functionality? 

That should be it. (Unless the dump format changed between 1.6 and 1.8, which I don't know about; you might want to test this with a small repo beforehand.) 

The most productive solution would be to not have multiple instances of running in seperate consoles, but to familiarize yourself with the various possibilities of having multiple files open in a single session. I've shirked that issue for a long time myself, but once I got the hang of it, I found it to be a distinctive boost to my productivity (and terminal efficiency, too). Check , , and . Edit: More to the point of your original request of closing a session after inactivity: An external tool would still have to signal to properly save / close the file, so changes get saved / the file removed. So, if there indeed is such a solution, it should probably come in the form of a plugin. But I know of no such plugin, and couldn't find one ad-hoc. 

Even better, run the command -- this gets rid of Vagrant reporting that a provisioning command returned with non-zero exit status. 

This massively depends on the games you will install on C:\ and the applications you will install under Linux. Moreover, Windows tends to accumulate additional fluff over times (downloaded patches, service packs etc.). Linux does this, too, but usually to a lesser extend. A partition that is "just big enough" today can become awkwardly cramped over time. The general rule of thumb is: Make the partitions bigger than you need 'em. Resizing partitions on an already cramped system is a major pain, and risky too. Better have enough space for the operating systems right from the start. Because, if you run out of space for data, you can just plug in a second hard drive. Edit: The most useful dual-boot setup I have found so far is to have the "data" partition formatted as ext3fs, mounted as /home/username under Linux, and using ext2fsd under Windows. I found that to be more transparent and comfortable than working with data on a NTFS partition under Linux. Then again, I'm primarily a Linux user, so your mileage may vary. 

This line is run as user . Consequently, it does not set up , but . The solution is to run the line as user : 

...but there still is no . I don't quite understand why , which does the trick (of setting up ) works fine when I am logged in to the virtual machine (via ), but fails when executed during provisioning as displayed above. How can I initialize during Vagrant provisioning stage (without actually accessing a real repo)? 

Let's say I have been working in a given Vim session for some time, and come to a point where I really would like to have everything I am looking at in GVim instead. The actual use-case is a terminal Vim session with a large number of windows... I'd like to click-activate them instead of going through Ctrl-W each time, something that can be done in GVim but not Vim (not mine, at least). Is there a way to take the buffers / windows / tabs currently open in Vim, and open them in GVim instead? 

I have multiple systems in my home network that are running Linux Mint. I also have a NAS server hooked to the home network, which already serves the various user's home directories (so no matter which machine you use, you always have "your" home directory available). I also have a Raspberry Pi, which I successfully made to boot its root filesystem from the NAS instead of the SD card. That latter experience made me wonder, if it would be possible to have the multiple Linux Mint machines share a common root installation on the network? I.e., install Linux to a NAS server share, then net-booting that installation from my various machines. I've done this for one machine already (the Pi), and am wondering if such an installation could be shared, so I would have to administrate / update only that one installation. I see several problems, from over to potential concurrency issues, and wonder if this can be done, if it has be done before, issues I should be aware of (like, which system directories should not be shared but reside on local mounts), and if there might even be some how-to / caveats articles available on the web. Things I found myself were mostly concerned with single system netboots, cluster systems, or enterprise Windows setups, which is why I am asking here. 

...and the command does get executed (with error, since of course there is no working directory to call on)... 

-i does the replacing inline (i.e. infile == outfile). Without -i the result of the replacing would be printed to stdout. 

A proxy is specified by setting the , and environment variables, either locally (e.g. in ) or globally (e.g. in ). These settings are honored by virtually all net-software packages (like apt-get, wget, curl etc.): 

Having set up a SuSE machine for a couple of co-developers, I found that SuSE does user permissions differently (all users being group "users", umask 022) than the RedHad / Gentoo way I am used to (all users having their own group, umask 002). Thus, I went along and deleted all users, and re-created them each having their own group, and setting umask 002 globally. However, after I did that, SSH pubkey authentication no longer works. It did work before, and the directory does have mode 700, and the file does have mode 600, but SSH still insists on asking for a password, which it didn't do before the rework. What did I miss? 

...would result in those warnings, as "/home/..." is converted into "home/...". Unpacking the tarball... 

The next step, of course, would be badgering the maintainers of to include whizbang support in their next release. ;-) 

This way, you get to honor the global setting for http_proxy, instead of duplicating the setting for in some arcane apt-specific config file. 

Your termcap / terminfo settings, and the configuration of your terminal program, must be consistent, or you get exactly the behaviour you describe. Sadly, I'm not as proficient with these settings as I would like to be, and can't really pinpoint what you should change where. (Have to leave room for others to earn upvotes, huh? ;-) ) 

Works with spaces in filenames, and does need neither (which recurses by default, which might not be what you want) nor (output of is alphabetical by default). 

While I generally prefer the command line for my development, I find the overview provided by this dialog to be very helpful when tracking down an offending commit. I tried to find a standalone Linux tool that provides a similar dialog, but came up empty. (RapidSVN comes close as it gives me the overview, but -- unless I am mistaken -- does not allow to access a file's diff from that dialog.) Suggestions? 

How about displaying the properties of the corresponding GUI menu entry (usually via right-click / Properties)? That will tell you how the executable is named exactly, and where it resides... 

Not quite what you asked for, but very similar in effect (i.e., you "pay" storage only for files that actually have changed): Using rsync, creating hard links for unchanged files. The big advantage is that each "snapshot" is a full-fledged backup in its own right, i.e. on recovery you only have to restore that one snapshot (instead of recovering a base and its increments). There is good documentation on that approach available at www.mikerubel.org/computers/rsync_snapshots/ 

shows you interfaces. I.e., you might see an entry for each wired (physical) connection, and one for your wireless. What does not show you is connections. You might be serving hundreds of file transfers to as many connected systems via ethernet, and you would still only see that one ethernet card in . 

There are basically three ways to identify a network interface: Vendor ID, MAC address, and IP address. Only the IP address has any kind of semantic meaning in the context of a network route. "Broadcom NetXtreme Gigabit Ethernet" (Vendor ID) or 00-1C-C5-33-21-52 (MAC address) wouldn't be any more helpful, would they? 

Found the solution in a German FAQ on the similar IB-NAS4220... Apparently, client and box negotiate a block size of 32 kB... on which the NAS, strangely enough, chokes. (Even much larger blocks usually don't pose a problem with NFS.) Setting smaller read block sizes in the mount options () seems to remove the problem. 

Post Scriptum: Coming back several years later, I've picked up some GUI programming experience of my own, and realize one thing is missing in the above explanation if you're looking for a "which way to go" advice: wxWidgets. This is a framework that builds on top of whatever you're using natively, and allows transparently portable GUI development, without sacrificing performance or having any licensind strings attached. C++ API. It's the path I've chosen for my GUI needs, and I felt it should be mentioned for completeness. 

( is necessary to get Firefox to cooperate. is a limitation of the NAS. Paths and IPs are tripple-checked OK.) This works. However, I experience rather frequent freezing-up of the clients, especially when surfing the web (firefox) after startup / logging in. The application freezes up for about 10-20 seconds, then everything returns to normal. Apparently this gets better some time after starting to work, but it is annoying as hell (and keeping me from continuing to set up the remaining machines like this, as it would be a show-stopper for my wife and kids). Note that this happens even if only one machine is accessing the /home/ directory, so it isn't about concurrent access going wrong. (Although there is a media server mounting the same share, but that one is idle 99% of the time.) I don't really know enough about NFS and the things going on behind the scenes to know what to look for, and where. Can anyone give me a hint? Is this due to bad NFS server performance? A caching issue? How can I find out? Can this be mitigated by setting certain NFS options? 

Interactively, I would select option 2 (ignore dependencies), as the installed version of libopenssl is 0.9.8j, and the difference does not matter for my purposes. (No, really, it doesn't.) The problem is that I would like to run the installation from within a script (Vagrant provider script), so... How do I tell zypper to ignore a dependency, non-interactively? 

I'm in the process of beautifying some source code using vim. {1} I've created a couple of vim macros that do the individual steps: removing trailing whilespace, removing empty lines after , removing empty lines before , that kind of stuff. Now, I want to create a macro that executes all the individual macros in sequence. However, once the first recursive macro terminates (because it can no longer find any matches -> matching error), my "wrapper" macro terminates, too. Is there a way to make a vim macro continue after a submacro generated an error? {1} I know about automatic reformatters. I might even use them on my current problem. I merely mentioned source reformatting for the example's sake. Do not post any answers about this source beautifier or that. The question is not about code reformatting per se, but about vim macros. Example: 

I have a small home network running, with a NAS box (ICY BOX IB-NAS5220) exporting for my various Linux Mint machines. on the NAS: 

I am using Cygwin / CygwinPorts. Actually, I'm running RoxTerm instead of gnome-terminal, but the latter is also possible. This solution is somewhat heavyweight, but since I not only need the terminal emulation, but also the bash / perl / vim / gcc / make environment, X Window server et al., it perfectly fit my bill. And it gives you "the real thing", i.e. gnome-terminal, not something "almost like it".