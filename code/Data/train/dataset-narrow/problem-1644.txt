We have 2 separate VMWare environments, one is the main environment which has hundreds of virtual machines across lots of sites. The other is a much smaller one installed on one server, just for archiving old systems. What I would like to do is take a snapshot of the current state of one of our live VMs, and use that to copy across to the other VMWare environment and create a new machine there, using that as the archive of that system. Is this going to be possible/easy? 

And it showed the correct certificates (QuoVadis) I also tried it using the IP address of the server and again it returned the correct certificates. So then I tried using the external IP address and that is where it was returning the old certificates (TERENA), so that is where the problem lies. So I know what the issue is, but I'm not sure how to fix that. There is nothing about SSL in my httpd.conf file, but there are virtual hosts and NameVirtualHosts for http on the internal IP address. All the SSL configuration is in the ssl.conf file, where the virtualhost uses the domain name instead of either IP address: 

I'm following exercise 1 of this Microsoft official Azure lab under the heading . The VM machine in the lab is . When I run the the following command of step 6 it prompts me for an input shown below. Not sure what input I need to put there. Note: I've installed Azure PowerShell 5.1 from here: Step 6 of above linked tutorial: 

I'm running Windows Updates on my that are taking too much time. How can I have auto-stop after updates are installed (instead of me keep checking if the updates are done)? 

I'm following this tutorial to install Azure PowerShell. And, on step 2 of the tutorial I'm getting the following error when I run the following command specified in the tutorial: 

And I even commented out the new.mydomain.com virtualhost, but whichever domain you go to in the browser it goes to the same documentroot. 

I've tried adding in a new virtualhost to serve the old domain name and test pointing it to a different documentroot, e.g. 

After restarting httpd and visiting the website it is still using the old certificate which expires in a few weeks. (I also tested by running the domain through $URL$ I confirmed that it is defintely using that ssl.conf file, by removing it and then the server failed to load. And then confirmed that it was defintely loading those files, by changing to an incorrect path and the server again failed to load. I ran the following from the terminal: 

I am using nginx as web server and php-fpm, in port 127.0.0.1:9000. This is my conf in /etc/php-fpm.d/www.conf: 

I want to put nginx in front of Varnish, which at the same time will go back to nginx to serve some PHP (Drupal). But I am only getting blank pages (from Varnish) with 200 response, but length 0. Only the first access after restarting varnish works, but then blank pages all the time. This is the configuration for Nginx: 

So it depends on the tools available. For example CF doesn't give ou any access to logs, it just shows some page visits counter, but not the details by ip user etc. The best way might be Google Analytics like solution. Add some javascript in your backend that fires requests to teh server with the proper information. There are though several complications with this approach: -It's hard to get the IP from user via javascript, even the Ip of the server where the javascript is being executed. It can be easily modified by other users. -If you are using cache it means that you worr about performance, sending a request at each page view is adding lots of overload to your server. So you might use a different server to treat those. If anybody else knows of a very smart system for that please let me know. 

On my Azure VM (Windows Server 2016) I get usual notice that I initially ignored thinking the Azure will automatically install these updates. But updates are not installed and I keep getting the above message. When I open the message it shows the following message box. Do I have to install these updates or I should still wait for Azure to do the job? This official Azure article is not clear to me. 

I'm following an official Microsoft tutorial on Azure with free trial where they ask us to create a of size on a region closer to you. But when I select any of the following regions, I see no size as shown in the image below: 

Could anyone advise me what I need to change in the configuration so that accessing through the external IP address loads the new certificates? Thanks. 

RedHat 6.2 Apache 2.2.15 I've installed a new SSL certificate on my apache server and updated the /etc/httpd/conf.d/ssl.conf file to include the new details: 

I have 2 old servers, which I am not using any more, which have been replaced by 1 new system, and I want anyone who tries to go to the old domain names, to be redirected to the new one. So, for example, anyone who tries to go to a.mydomain.com or b.mydomain.com should be redirected to new.mydomain.com What was done was that a member of the network team simply changed the IP address associated with a.mydomain.com and b.mydomain.com names to point to the new server, where new.mydomain.com is being served. This works fine if you go to the top level, e.g. if I go to $URL$ it will redirect to $URL$ perfectly fine. However, if I try to go to a full file URL of the old name, e.g. $URL$ instead of redirecting to $URL$ it redirects to: $URL$ (Missing the slash "/" ) and therefore obviously fails, because that's not a valid url. They said it was a server configuration thing, so I've tried messing around with the virtualhost on the new server. I've tried using the old domains as ServerAliases, i've also tried actually just setting them up as separate virtualhosts to point to the same document root, but I am still experiencing the same problem each time. Does anyone know what might be causing this? And if it's something I need to change on the server, or if it's something the network team need to change in the DNS? (I am using CentOS and Apache) Thanks. Edit: This is what the virtualhost on the new server looked like originally: 

So it looks like the user was properly set. But it's not working. Now what I really don't understand is that if I execute this: 

I have an issue with php-fpm. It is actually the php7 version. I have drupal and it will complain that some directory is not writtable. Only if I start php-fpm as a service like that: 

Visiting my webpage only returns empty html but 200 response. No PHP error or anything. Access logs on Nginx are just access, no errors. If I access directly from Varnish (either I access via port mywbpage.com:6081) or if I set it to port 80 it works. If I set the php executing backend to Apache+php-fpm (instead of Nginx: nginx->varnish->apache) it also works properly (I have the same issue though if the php executer is HHVM but that might be a different problem). EDIT: sorry that was wrong, if I use apache it works only if I uncheck drupal's default cache (cache pages for unauthenticated users). With nginx this doesn't matter, it never works regardless of this checkbox. Any of you know something that can guide me? 

I'm working on exercise 4 of this Azure Official Lab. But the instructions for creating an seem outdated there since in the new portal I cannot find the information they are asking for in their walk-through. For example, step 3 says . But I don't see this info in the new portal. Question: What would be the alternate steps in the new portal for the above tutorial so I can complete it in the new portal? 

According to the first note on this Microsoft Azure article: Now, I'm using the free Azure trial account for Azure databases and would like to test the scenario of connecting to a Azure SQL Server database on an Azure VM. Which should I be using for this test? 

What change should I do? (here, kind of argues that the way it is set, should have no permission issues) (I tried to change the context for the directory "vanilla drupal" to look the same as html (I assum that if I had put the dir inside html I wouldn't have any problem), by changing the user. But I just got even more errors (forbidden {execmem}). 

Which is the command described in the service (/usr/lib/systemd/system/php-fpm.service) - Then I have no permission issues. The output of ps aux | grep php-fpm is the same, with nginx being the user. I am executing everything as root. How is this possible? Edit This has become a SElinux question. I have this in the audit logs: