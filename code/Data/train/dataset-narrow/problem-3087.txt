In order to build the ROC curve and AUC (Area under curve) you have to have a binary classifier which provides you at classification time, the distribution (or at least a score), not the classification label. To give you an example, suppose you have a binary classification model, with classes c1 and c2. For a given instance, your classifier would have to return a score for c1 and another for c2. If this score is a probability-like (preferrable), than something like p(c1), p(c2) would work. In plain English is translated like "I (the model) classify this instance as c1 with probability p(c1), and as c2 with probability p(c2)=1-p(c1)". This applies for all type of classifiers, not only for decision trees. Having these scores you can than compute ROC or AUC by varying a threshold on p(c1) values, from the smallest to the greatest value. Now, if you have an implementation of a decision tree and you want to change that implementation to return scores instead of labels you have to compute those values. The most used way for decision trees is to use the proportion of classes from the leaf nodes. So, for example you have built a decision tree and when you predict the class for an instance you arrive at a leaf node which have (stored from the learning phase) 10 instances of class c1 and 15 instances of class c2, you can use the ratios as the scores. So, in our example, you would return p(c1) = 10 / (10+15) = 0.4 probability of class c1 and p(c2) = 15/(10+15)=0.6 probability of being class c2. For further reading on the ROC curves, the best and inspiring source of information I found to be the Tom Fawcett paper called An Introduction to ROC Analysis, it's solid gold on this topic. 

The InterQuartileRangeFilter from weka library uses an IQR formula to designate some values as outliers/extreme values. Any value outside this range $[Q_1 - k(Q_3-Q_1), Q_3 + k(Q_3-Q_1)]$ is considered some sort of an outlier, where $k$ is some constant, and $ IQR = Q_3 - Q_1$. By default weka uses $k=3$ to define something as outlier, and $k=3*2$ to define something as extreme value (extreme outlier). The formula guarantees that at least 50% value are considered non-outliers. Having a single variable (univariate sample of values), it's practically impossible to reproduce your result. Note however that this filter can be applied to a data frame. When applied like this, it will consider as an outlier any instance of the data frame which has at least one value of the instance considered as outlier for that variable. Now, supposing that you have a data frame with 2 variables, totally uncorrelated (independent). Considering again that only 10% of the values from each variable are considered outliers, due to independence, one can expect that $(1-0.9)^2$ values will not be outliers. If you have $p$ variables like that in your data frame, you might expect to have only $(1-0.9)^k$ normal values, and is not very hard to arrive in that situation. There are two things which you will have to consider. One is to increase the factors for outliers if in general too many values are considered outliers (ideally you would like to take a look at each variable graphically and if possible to get some idea about the distribution beneath). The second one is to check if you have many values which are totally independent. The second hint does not solve your problem but might give you a reason why it happens. 

You can't calculate the conditional distribution because that distribution is defined over the entire population and assuming a probabilistic model which you do not know. In real life you have only a sample of the population. More often than not, in real life you do not have the structure of the model also. You do not know if it is a linear model, a polynomial or another type. The probabilistic approach is to assume a model, consider some assumptions for that model and fit the model's parameters using data. That is not the Bayes classifier, it's only an approximation. The trick is 'all models are wrong, some models are useful'. There are many approaches to learn this approximation. You can estimate the joint probability (generative classifiers), you can estimate directly the conditional probability (discriminative classifiers) or you can use a different approach (like SVM, for example) and twist it somehow to approximate the conditional probability. But all of them are approximations. The real one, THE Bayes classifier will remain unknown. [Later edit] Looking at the Wikipedia page for Bayes classifier it states that: "The Bayes classifier is a useful benchmark in statistical classification.". My opinion is that this statement is confusing, if not dead wrong. Probably this is related with the wrong usage of expression "Bayes classifier" to designate the Naive Bayes classifier. If one search on google for "Bayes classifier", he will see a lot of materials related with Naive Bayes. Personally I do not agree with the statement from wikipedia. Bayes classifier is that state of nature that we do not know but we aim to approximate as well as possible. 

I have heard of genetic algorithms, but I have never seen practical examples and I've never got a systematic introduction to them. I am now looking for a textbook which introduces genetic algorithms in detail and gives practical examples how they are used, what their strengths are compared to other solution methods and what their weaknesses are. Is there any standard textbook for this? 

How people use it: Because they do not model the problem in a way which allows humans to directly say what happens for any given input. Personal thoughts I don't think this notion of a "black box model" makes much sense. For example, think of weather forecasting. You cannot expect any human to say which weather will be predicted if he is only given the data. Yet most people would not say that physical weather models are black box models. So where is the difference? Is it only the fact that one model was generated using data and the other one was generated using insights into physics? When people speak of black box models they usually say it as if it is a bad thing. But humans are black box models, too. The critical difference I see here is that the class of errors humans make is easier to predict for humans. Hence it is a training problem (adverserial examples on the NN side) and an education problem (teaching humans how NNs work). How the term 'black-box model' should be used: An approach which makes more sense to me is to call the problem a "black box problem", similar to what user144410 (+1) writes. Hence any model which only treats the problem as a black box - hence something you can put input in and get output out - is a black box model. Models which have insights (not only assume!) about the problem are not black-box models. The insight part is tricky. Every model makes restrictions on the possible function which it can model (yes, I know about the universal approximation problem. As long as you use a fixed-size NN it doesn't apply). I would say something is an insight into the problem if you know something about the relationship of input and output without poking the problem (without looking at data). What follows from this: 

I've posted this link on Reddit and got a lot of feedback. Some have posted their answers here, others didn't. This answer should sum the reddit post up. (I made it community wiki, so that I don't get points for it) 

So before PCA, the classes were perfectly linearly separable, but after PCA they are not separable at all. I'm not saying this happens in your case, but it could be. 

It seems to me that t-SNE and other dimensionality reduction algorithms which reduce the dimensionality to two dimensions are mainly used to get an impression of the dataset. If done well, they look nice (e.g. like this), but I wonder if this is better than just showing random images / grouping them by class on a grid. I would like to get an answer to the following aspects: 

Let's say we have a neural network with one input neuron and one output neuron. The training data $(x, f(x))$ is generated by a process $$f(x) = ax + \mathcal{N}(b, c)$$ with $a, b, c \in \mathbb{R}^+$, e.g. something like 

Well, pretty much that. You usually don't have enough memory. Lets say we are talking about image classification. ImageNet is a wildly popular dataset. For quite a while, VGG-16D was one of the most popular mod.els. It needs calculcate 15 245 800 floats (in the feature maps) for one 224x224 image. This means about 61MB per image. This is just a rough lower bound on how much memory you need during training for each image. ImageNet contains several thousand (I think about 1.2 million?) images. While you might have that much main memory, you certainly do not have that much GPU memory. I've seen GPU speeding up things to about 21x. So you definitely want to use the GPU. Also: The time for one mini-batch is much lower. So the question is: Would you rather do n update steps with mini-batch per hour on a GPU or m update steps with batch without GPU, where n >> m. 

I suggest , go for Anomaly detection: Anomaly Detection is done assuming our data has a probability distribution(gaussian). We can plot data to see if thats the case, if not we can make it gaussian using log transforms. Gaussian distribution specifies the regions and probabilities of our data lying in those regions. For example : replace original feature x -> Log(x) or feature x -> (x)^4/3 etc.. Also regarding the threshold value which decides outliers you can play with it and see that with Higher threshold you will be rejecting more entries and this might be required where doctors are trying to isolate cancer patients amongst many normal ones without taking any risk/chances. Again outliers here doesn't mean cancer patient but definitely worth a medical test. And you can set it to lower value if you are getting too many normal data flagged as outliers. We have skewed data sets since we have more examples of one kind than the other. For example when we get air craft engine data we might just have data for few bad ones and mostly for good ones.Use of cross validation data is suggested.F1-Score is a pretty good metric to evaluate the performance of the algorithm. To get a proper hold on this topic , I also suggest go through anomaly detection course videos by Andrew NG in machine learning on Coursera. Free course and very nicely made. 

As far as practical applications are concerned : Here are a few: 1) Aircraft Engine Anomaly Detection: Input Features can be heat generated by engine, vibrational intensity, fuel consumed etc etc. Here outliers can be sent for testing again and further decisions can be made. 2) Fraud Detection : Features can be features of users activities on a website. We can model probabilities from the data. Identify unusual behavior by checking probability less than certain fixed threshold. 3) Monitoring Computers in a data center : features can be memory use, no of disk accesses, CPU load, network traffic etc. Abnormal behavior here can help predict future breakdowns. Anomaly Detection is done assuming our data has a probability distribution(gaussian). We can plot data to see if thats the case, if not we can make it gaussian using log transforms. Gaussian distribution specifies the regions and probabilities of our data lying in those regions. For example : replace original feature x -> Log(x) or feature x -> (x)^4/3 etc.. Also regarding the threshold value which decides outliers you can play with it and see that with Higher threshold you will be rejecting more entries and this might be required where doctors are trying to isolate cancer patients amongst many normal ones without taking any risk/chances. Again outliers here doesn't mean cancer patient but definitely worth a medical test. And you can set it to lower value if you are getting too many normal data flagged as outliers. We have skewed data sets since we have more examples of one kind than the other. For example when we get air craft engine data we might just have data for few bad ones and mostly for good ones.Use of cross validation data is suggested.F1-Score is a pretty good metric to evaluate the performance of the algorithm. 

The well known techniques for the same are: Density-based techniques (k-nearest neighbor),Support vector Machines,Cluster Analysis Anomaly detection. People also use Fuzzy logic based techniques. And for python I suggest use libraries like scikit-learn and TensorFlow , they have superb implementations of usable machine learning algorithms.Heres a video link from a googler,giving general overview for the same :$URL$ 

Please check this website: $URL$ its a collection of data portals from around the world. Might find here what you are looking for. 

Without code you could have made to many mistakes to answer your question directly. However, I suggest two things: 

I would like to see if I can reproduce some of the image net results. However, I could not find the data (the list of URLs) used for training / testing in the ILSVRC 2012 (or later) classification challenges. I only found $URL$ Where is the data used for the ImageNet ILSVRC 2012 (or later) classification challenge? 

VC-Dimension 3: No, that doesn't work. Imagine the classes and being ordered like . Your classifier can't deal with that. Hence it has a VC-Dimension of 2. Proof Obviously, the points $x_1, x_2, x_3 \in \mathbb{R}$ can only be distinguished if they have different values. Without loss of generality, we can assume that $x_1 < x_2 < x_3$. Hence the classifier has to be able to classify 

Please add a reference / quote to your answer. My thoughts I think Lasagne does (3) (see code). This might be the simplest to implement. However, closer to the original idea might be (1). Seems to be similar for Caffe (see code). For tensorflow, the user has to decide (code - I'm not sure what happens when is passed). How it should be (2) and (3) don't make much sense as it would cause the network to add invariance to spacial positions, which is probably not desired. Hence (1) is the only variant which makes sense. But I'm not sure what happens if you use the default implementation. 

Pearsons correlation coefficient is defined as $$\rho_{X,Y} =\frac{\operatorname{Cov}(X,Y)}{\sqrt{\operatorname{Var}(X)\operatorname{Var}(Y)}}=\frac{\sigma_{X,Y}^2}{\sigma_{X}\sigma_{Y}}$$ It measures the linear relationship of two random variables $X$ and $Y$, but a correlation coefficient of 0 does not mean that there is no relationship. This is visualized pretty well in the following Wikipedia image: 

Dropping complete feature maps (hence a kernel) Dropping one element of a kernel (replacing an element of a kernel by 0) Dropping one element of a feature map 

After having read a lot more papers and having talked to many people about machine learning topics, this is how I would define the words: A class as an abstract concept which exists. Each class has properties and can have a lot of different labels. For example, the class cat has the properties "feet" (with the value 4), the property "Genus" with the value "Felis". There are many way members of the class can look like. Also many labels: cat, Katze, Felis silvestris, , . A label is just a sticker you put on the concept. A name. We need a word to be able to talk about the concept. I use labeling for the manual process of defining which parts of the dataset belong to which class. And I use classification for the process of the automatic classifier deciding which part of the data belongs to which class. So typically, labeling is done by a human and proceeds classification which is done by the machine. 

Take this with a grain of salt, but I think this is simply not true. You can evaluate it with the code I just wrote: 

What is the perceptron good for? It's good for learning how multilayer perceptrons (MLPs) work. I'm not sure if there is any real use case where you would want to apply a perceptron. Decision stumps (if-statements) are extremely simple, just like perceptrons. If such a simple model is enough, you could probably also just set it by hand. (Please let me know in the comments if you know an example where a single perceptron / decision stump with the associated learning method provides value compared to an ad-hoc decision method)