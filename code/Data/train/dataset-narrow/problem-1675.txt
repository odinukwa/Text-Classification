Obviously, make sure you have permissions to execute and that your includes the directory where it is installed. (You could obviously pipe to and leave the alone, but if things get nontrivial, you want to avoid littering the with hard-coded paths to every binary. Like in the shell, Procmail's default lists a number of standard system-wide locations like etc.) The flag causes Procmail to regard this as a secondary destination, so every message will also be written to the default inbox (or maybe a in the user's home directory, these days). Once you are confident your doesn't lose mail (test for disk full, out of memory, etc) you can take out the and maybe eventually run it straight out of the Postfix and get rid of Procmail and the Unix user account in favor of a virtual user. Technically, Postfix is your MTA and MSA; it knows how to use Procmail (or Maildrop, or a bunch of other alternatives) as the MDA. In order to have an MX record, you need an MTA on the host which the MX points to. A correctly configured Postfix just needs to be told which domain name(s) it should accept inbound traffic for (basically and , but should othewise be ready to serve as the primary MX If you are unfamiliar with a lot of this territory, you should probably avoid running your own mail server at all. E.g. Amazon SES allows you to easily run a script of yours on every incoming message without having to worry about the quite significant administrative burden and tall learning curve of stable, secure email service. 

This message repeats every five minutes. I suspect this is the IP address of an earlier build of the file server since I can see it's a linode.com address if I run "whois" on it. But what's more interesting is that if I to a "grep -Rn 45.79.65.48 /etc" I see this address in the /etc/mtab file. I see now that this is the previous file server's IP address since I neglected to unmount the file server's directory before I destroyed and rebuilt the file server. I did "sudo umount -l /var/www/mysite.com" to unmount it. I then did "sudo mount -a" on the web server and now I can see that the file server directory is mounted onto the web server. However, if I re-run "sudo rpcinfo -u fs02 mountd" command on the web server, I still get the "Connection refused" message. I don't see how I can get that message if I'm now seeing the cross-mounted directory. I've been up all night working on this so maybe I'm tired and missing something. 

If you've installed digital certificates on your servers, are there any potential problems when replacing one digital certificate and its key with another? Are there any hidden ramifications? I'm building a website that will hold sensitive data. I have a web server running Nginx, a fileserver that I cross-mount via NFS that serves up static files, and a PostgreSQL database server. Ideally, I'd like to purchase a wildcard certificate with Business/Organization Validation but they're pretty expensive. What I'm thinking of doing instead is purchasing a cert with Domain Validation to start and then figure out how to get SSL running on all three servers. Once I understand that process and start getting customers, I would then replace the DV cert with a BOV cert. What problems, if I, will I encounter if I take this approach? 

It's hard to see any other solution than changing the on your side to an identifier which does not get filtered on their side. Based on the log excerpt, I'm guessing the organization you are corresponding with is Debian, and that it's completely out of the question that they would add a special case just for you in ther system. Not being able to send bug reports from when you could just as well use, say, seems like a minor inconvenience. You could configure an alias on your side to route replies back to your preferred address, and perhaps even correspondingly rewrite outgoing email from this address to this particular destination, though that seems like a lot of work for very marginal gains. 

It's deceptively simple, but that's really all it takes. A command in backticks will receive the current message as its standard input. (If is a static file, you need to add locking, but that's a tangential topic.) If this is all your Procmail rules do, you might want to think about whether Procmail is at all required. The delivery part is still useful and robust, so I recommend you keep it for that, but since Procmail is good at doing precisely what you are trying to do with PHP, perhaps you want to think about your approach. If your database doesn't change very frequently, you might want to consider using a script to generate your from the database instead. Then the Procmail rules themselves can be static (ideally not require any external process) but you will need to regenerate them whenever the database is updated. 

When I enter my website's non-SSL URL "cms00.example.com" into my browser, it won't redirect to $URL$ If I enter the HTTP address, I can see the site and if I enter the HTTPS address, I can see the site. I just can't get the redirect from http to https to work. I've read numerous articles on how to do this and tried all the suggestions but my configuration still isn't working. I'm running Apache 2.4.10 on Debian 8, and this is my first time working with Apache. I've run these two commands and verifed that the rewrite and ssl modules have been loaded: 

On the one hand, I know Supervisor should only be used on foreground process, which is not the case with this script. On the other hand, I've seen other questions here which seem to imply that you should be able to run a shell script from Supervisor. Thanks for your help. 

Should one always use "www" as the prefix to one's official domain name? I will be launching an e-commerce website shortly and my attorney needs to know the official domain name of my site for copyright purposes. Are there any reasons why I should designate it as "www.example.com" as opposed to "example.com"? I've always used "example.com" during development because it was easier to type and I'd have Nginx rewrite all requests for "www.example.com" to "example.com". Should I perhaps say "www.example.com" is the official domain name and reconfigure Nginx to rewrite all requests to "example.com" to "www.example.com"? Thanks! 

I was investigating a bunch of spam domains, and none of them have an SOA record. I have never seen this before -- I didn't even know it was possible. How can DNS work without an SOA record? If it isn't strictly necessary, what are the implications of omitting it? I don't particularly want to add links to spam sites but an example domain is "vance miller kitchens uk dot co dot uk". 

The Awk script locates the first MIME boundary, then the first empty line after that, then inserts the snippet, and sets a couple of state variables to prevent further processing. Unfortunately, the script is slightly brittle; it will fail if one of the extracted values contains unpaired double quotes, and process paired double quotes incorrectly. Furthermore, this will only work correctly if the first body part within the multipart is a text part; it will fail similarly to your current case if you have nested multiparts (top-level message multipart/related containing a multipart/alternative structure, for example). It can be extended to cover more cases -- a simple tweak would be to skip up to the first instead --, but at some point, it will make more sense to do the MIME manipulation in a properly MIME-aware tool (a simple Python script, for example). 

The special operator causes Procmail to collect the matching string into the variable , and we then use that instead of the external you were running. This is hardly a crucial change, but should be more efficient, as well as easier to read and debug. (The trailing wildcard is superfluous in the first regex; Procmail is satisfied if a match is found anywhere in a line.) You can test from the command line (but this of course requires that your recipe file doesn't override ): 

Is this the right approach to take to change my worker log permissions and recreate the pid directories before the celery workers are restarted? If this is the right approach, why isn't it working? If it's not, what's the right approach? If I were using an init.d celeryd daemon script instead of Supervisor, there's a CELERY_CREATE_DIRS setting that will automatically create pid and log directories that are owned by the user/group. Is there a way to replicate this setting when using supervisord? 

I have an Nginx/Gunicorn/Django web server and PostgreSQL database server that I only want to access using SSL. I've purchased, installed, and configured a certificate on my web server from a certificate authority and so now my users can only access my website via HTTPS and it's working fine. Now I'd like to implement secure two-way communications between my web server and database server over SSL. Since the only machine talking to my database server will be my web server, will it be OK from a security standpoint to generate my own private key and certificate using the openssl command ("self-signing") or should I get a free ones from somewhere like letsencrypt.org? 

I have a Django/Nginx/Gunicorn web server ("web02") and an Nginx file server ("fs02") that is used to store user images. When a user uploads images through the web site, they are saved to the file server via a directory that is cross-mounted from the file server via NFS. I build my servers using an Ansible playbook that provisions each server and then configures the file server first and the web server second. When I initially build my servers, NFS works perfectly. However, if I rebuild and reconfigure just the file server (for example, if it crashes and I need to rebuild and restore it), NFS doesn't work. In that situation my web server is unable to see the exported directory on the file server. I have confirmed this two ways: 

In case the link goes bad, reinventing the same wheel again should not be a significant challenge; you basically need to identify the MIME part which contains an encrypted payload, decode it (it's probably encoded, unless it uses 's own "ASCII armor"), and pass it to . The existence of an encrypted payload is probably a good trigger, but perhaps the wrapper should simply pass through anything which doesn't contain an encrypted payload, and you would feed everything to the wrapper. Tangentially, there is nothing which defines in Procmail or in your rules. You can do something like this: 

This is ideal for incoming email, but the simple and straightforward way to also get your outbound communications logged is simply to Bcc: the account address on outgoing messages. On the other hand, if you use a web interface for the interactions, you could do the writing from the web scripts, then generate an email and send it, and use Procmail (or something similar) only for the incoming part of the conversation. This would simplify the system somewhat, at the expense of sacrificing the possibility of using the system simply by writing emails, as would come quite naturally from this sort of system. The above Perl script is simple enough -- and Procmail is versatile enough -- that you could do all of this from your , but for actual database processing, you will need a small external script in any event. 

I've solved the problem. Here's what's going on. /etc/apache2/apache2.conf includes a call to any config files that have symlinks in /etc/apache2/sites-enabled. Since there was a symlink in that directory pointing to /etc/apache2/sites-available/000-default.conf, that latter config file was being loaded and it was over-riding the blocks and directives in my vhosts.conf file. Once I deleted that symlink, my vhosts.conf settings were able to take effect. The lesson for me was that any file that has a symlink in sites-enabled will be enabled. 

I have a Django 1.6.2 application that uses Celery 3.1.7 for asynchronous tasks. I'm starting my Celery workers using Supervisor. So far everything is working well except when I reboot my Debian 7.8 server. When that happens, my Celery workers won't restart because when the server reboots, it changes ownership of the celery log files from my "celery" user to "root". Also, the system deletes my /run/celery directory where I write my pid files. If I make these changes by hand and restart Celery, all my workers start normally. Since these changes need to occur before starting the workers, I thought the solution would be to write a shell script which, due to its higher priority, gets executed from my supervisor.conf script before the celery worker commands (See below). However, this setup script won't run. My supervisor log just says,