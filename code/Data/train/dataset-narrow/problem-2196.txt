The GMD decoder is an algorithm for decoding concatenated codes up to half their minimal distance. The standard presentation of this algorithm usually proceeds in two steps: First, one shows a randomized decoding algorithm, and proves that this algorithm works in expectation, rather than with high probability. Then, one uses the latter proof to show how to derandomize the latter algorithm to get an algorithm that always works. In all the presentations that I could find, it is never shown that the randomized algorithm works with high probability. However, it is not hard to see that this is indeed the case. I need to use this fact, and I wonder whether it was proved somewhere and I can give a reference, or whether I need to prove it myself. So in short, my question is: Is there a reference for a paper that proves that the randomized decoder works with high probability? 

Strictly speaking, the "Boolean hypercube" is not the same as the set $\{0,1\}^n$. The Boolean hypercube is the graph whose vertices are the set $\{0,1\}^n$, and whose edges are defined as follows: two strings are adjacent if and only if they differ on exactly one bit. I guess that usually, people refer to $\{0,1\}^n$ as the Boolean hypercube when they really want to think of it as a graph and not as a set of strings. 

It is an old open problem whether a direct-sum theorem holds for deterministic communication complexity, that is, whether solving $t$ independent instances of a problem is $t$ times harder than solving a single instance. [FKNN95] showed the following results: 

A paper of Klawe, Paul, Pippenger, and Yannakakis gives an hierarchy theorem for constant depth monotone formulas: $URL$ Specifically, for every $k$ it gives a function that can be computed by a formula of depth $k$ and size $n$ but requires formulas of depth $k-1$ of size $\exp(n^{1/k})$. 

NP is ofter characterized as a proof system in which the prover sends a polynomial-length proof to a deterministic polynomial-time verifier, and after which there is no interaction. The class of recursively enumerable languages can be characterized similarly by replacing "polynomial" with "finite". Also, since the class of recursive languages R is the intersection of RE and coRE, you can characterize R as a proof system in which an all-mighty prover can convince a finite time verifier both in the validity of correct claims and in the invalidity of false claims. The class EXP has a characterization in terms of a proof system with "competing provers" - i.e., a proof system in which there is a prover that tries to convince the verifier that the claim is true and a refuter that tries to convince the verifier that the claim is false. See the paper "Making games short" of Feige and Kilian for more details. 

If you just need any code $E : \{0,1\}^n \to \{0,1\}^m$ where $m=O(n)$ and where the distance is linear in $m$, then what you are looking for is called an "asymptotically good code". There are many explicit constructions of such codes, and you can find the basic ones in lecture notes of courses about coding theory. For example, you can find a description of a classic construction in Lecture 7 here. Another example of a construction is expander codes, which are described in Lecture 14 there. If you are looking specifically for codes where the distance between any two codewords is close to $\frac{m}{2}$, and in particular is upper bounded by $(1+\delta)\cdot \frac{m}{2}$, then things are a bit more complicated. Such codes are tightly related to objects called "$\epsilon$-biased sets", which have been studied for quite a while in TCS. You can find a very recent construction of such codes here. The earliest constructions can be found here and here (although they only give you $m = \rm{poly}(n)$). 

Suppose we consider $s$-folded Reed-Solomon codes that are based on polynomials over a field $\mathbb{F} = \mathrm{GF}(p^t)$. Then the alphabet of those codes is of size $p^{t \cdot s}$. Hence, in order to be linear, those codes should be closed under multiplication by scalar from the field $\mathbb{F}' = \mathrm{GF}(p^{t \cdot s})$. There is no apparent reason why they should be closed under such operation. However, it is true that they are closed under multiplication by scalar from $\mathbb{F}$. In other words, the codes are $\mathbb{F}$-linear. 

I don't know whether such algorithms are used in practice, but it seems that Markov's inequality should give you a sufficiently good bound on the probability of running for too long time, right? 

If I understand correctly, this is exactly equivalent to local decoding of the Hadamard code. We can think of the oracle (with the noise) as being chosen beforehand, and after that we can view the oracle as a corrupted codeword of the Hadamard code. Learning the parity in this case is exactly the task of local decoding the original message from the corrupted codeword. This is well-known to be possible using $O(n \log n/\delta)$ queries, using the Goldreich-Levin theorem, e.g.: $URL$ On the other hand, it clearly can not be done using $o(n)$ queries, since the algorithm has learn $n$ bits about the parity - i.e., its attributes. 

There is a simple construction: Take any $d$-regular non-bipartite expander $G=(V,E)$ - there are several constructions of those, e.g., Margulis, or the Zig-Zag construction. Now, turn it into a bipartite graph $G' = (V_1 \cup V_2, E')$ as follows: $V_1$ and $V_2$ are copies of $V$. Two vertices $v_1 \in V_1$ and $v_2 \in V_2$ are adjacted in $G'$ if and only if the corresponding vertices in $G$ are adjacted. The expansion of $G'$ follows from the expansion of $G$. However, one thing that should be noted is that the expansion factor in all those constructions is at most $d/2$. We do not have bipartite expanders that have a better expansion factor and are balanced (i.e. have degree $d$ on both sides). 

Every $\epsilon$-biased set gives a code whose minimal relative distance is $0.5 - \epsilon$ and maximal relative distance is $0.5 + \epsilon$. To see it, write the elements of the set as the rows of a matrix, and then define the code to be the span of the columns of the matrix. The $\epsilon$-biased property of the set is equivalent to saying that the relative distance between codewords is always between $0.5 - \epsilon$ and $0.5 + \epsilon$. One particular construction of such sets will give you a code whose dimension is linear in its block length. It is mentioned here: $URL$ Basically, the idea is to take AG codes of constant rate and relative distance close to $1$, and concatenating them with the Hadamard code. 

First of all, would you agree that DNFs and CNFs are a natural object to study? At the very least, if you believe that decision trees are a natural object to study, then so should be DNFs and CNFs, since they are the non-deterministic and co-non-deterministic versions of decision trees. If you accept that, then I would argue that that $\mathbf{AC}^0$ is a natural generalization of DNFs and CNFs, and therefore it is natural to study it. We can also think about $\mathbf{AC}^0$ as the analogue of the polynomial hierarchy for decision trees. Indeed, there are connections between questions about $\mathbf{AC}^0$ and questions about the polynomial hierarchy relative to oracles (see here for a recent example for exploiting such a connection). That said, the main motivation for studying $\mathbf{AC}^0$ is that it is a good "relaxation" or a "toy problem" for studying larger classes of circuits. A holy grail of complexity theory is to prove that $\mathbf{NP}$ does not have polynomial-size (general) circuits, and we are not anywhere close to proving that. Hence, a natural way to make progress toward this goal is to first try to tackle the following toy problem: prove that $\mathbf{NP}$ does not have polynomial-size $\mathbf{AC}^0$-circuits. This is problem was solved in the celebrated result that showed that such circuits cannot compute the parity function (here and here). This motivation is even more important for a class such as $\mathbf{ACC}^0$. Indeed, this class is less natural by its own right. However, it provides us with another important "toy problem": prove that $\mathbf{NP}$ does not have polynomial-size $\mathbf{ACC}^0$-circuits. This "toy porblem" is still open, and has been open for over 20 years. The strongest result we have is this work of Ryan Williams, which, while very impressive, is still far from solving the problem.