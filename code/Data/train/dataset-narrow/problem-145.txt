If your goal is isolation of traffic then you'd have downstream ports on the switch that would be in VRF's A or B and then configure routing within the respective vrf's (either BGP or just a static default). You'd use different IP ranges in the two vrf's as, ultimately, the Internet is one routing table and has no way to differentiate between identical IP's across your vrf's. I'm inferring a bit but I don't think vrf's are the tool to accomplish your stated goal of adding capacity. VRF's are fundamentally about segmentation - so in the example above, the switch would be logically split into two distinct operational units. You'd much more typically just hook up the second connection to the same vrf, add it to BGP, advertise the same routes and then potentially adjust policy to shift some traffic to the new line (...this may not be necessary depending on which carriers and what views you're receiving). If this isn't the case then, again, two vrf's with their own sets of interfaces and routes and you're all set. Logically (at least from a routing perspective) it's basically behaving like two independent switches. 

The first command sets the AD for all routes learned by OSPF. This is basically the traditional AD command, as seen in IOS since they invented dirt. The second allows you to set different AD's for different types of OSPF routes - so different values for externals vs inter-area vs intra-area. I suppose the idea here is to be able to prefer, say, a route learned within an OSPF area to one learned from IS-IS but to continue to use the IS-IS route if the OSPF route is external (i.e. redistributed in). I guess it's nice that such a knob exists and I'm sure it can be useful in some circumstances but can't help thinking that it'll all end in tears more times than not. It's been my experience that if the answer to a question is tweaking AD that one is almost always better off asking a different question. Also - the default AD of OSPF on anything Cisco is 110. EIGRP is 90. 

The difference here is incredibly trivial - literally on the order of single-digit nanoseconds. The amount of time it takes to get the packet organized and across the switch is likely 2- to 3- orders of magnitude higher (and you're still in microseconds at that point) - and that's before you consider how long a host takes to actually receive, decode and process the packet. To accurately measure the differences here would require some very specialized equipment (read: hundreds of thousands of dollars) but, yes, a .5M passive twinax cable has less propagation delay than a 10M AOC. 

There are some Cisco platforms that can translate multicast source/destination groups. As an example the Nexus 3500 has a feature called multicast service reflection that can work this way. There are some other platforms running XE that also support this feature, as well as some older boxes (7200/6500, for example). In any case, service reflection works as follows: the configured router will join the source multicast group - call it (S1,G1) and then originate the same traffic into a new group with a source on a virtual subnet: (S2,G2) that clients can then join via normal means. There are definitely some potential design caveats here in terms of loops and such, so exercise caution. I've honestly primarily seen this used in large enterprise multicast environments where streams were coming from third parties and either the source had to be fixed to keep RPF from going haywire or there was a need to disambiguate groups in very dense environments (or, occasionally, both). I understand that it may simply be for purpose of example, but it's also worth noting that the lowest subnets in the multicast/class-D space have special meaning. 224.0.1.0/24 is explicitly defined in RFC 5571 and there are static definitions maintained by the IANA. It's legitimate to route this traffic but is likely bad form for typical end-user applications. 

Separate connectivity into tiers - so, for example, a few external aggregation boxes might feed into a larger number of boxes focused strictly on encryption and decryption that, in turn, connect to infrastructure that separates customer connections into multiple logical tenants that are then cross-connected through a layer that can provide appropriate policy based controls and security. Greater flexibility in this regard will let you make more optimal choices about horizontal versus vertical scale - so more cheap boxes vs fewer high-density/exotic ones, etc. Good management invariably means a very deep understanding and integration of operational processes into orchestration. You don't bolt automation on. Like security you really have to bake it in from the start. As you put together your infrastructure you want to choose product sets that integrate with your toolsets and you want to provide a product that integrates with your customer's toolsets. Success in anything like an SP network is going to be pretty directly tied to the extent you can keep humans out of day-to-day provisioning, troubleshooting, etc.. and to maximize the effectiveness of those humans when their attention is required. Visibility and measurement are worthy investment targets. Be able to gather data from each and every tier and device and have an efficient and sane way to store and analyze this information. These areas are often neglected earlier in the process and it causes a lot of pain. This can be the means by which business and infrastructure can be well done and can be a significant competitive advantage and source of revenue. 

As its name implies the PHY chip is responsible for getting an actual fully formed Ethernet (or other transport) frame on to / off of the wire. This will tend to include clocking, carrier, calculating checksum, possibly multiplexing (for some types of optics), MAC framing and the like. Its job is generally to present a normalized and (hopefully) correct frame to the fabric or controller, which is tasked with lookup and forwarding - either to another PHY or, possibly, to a higher-order fabric/bus controller. How do these elements communicate? There are a variety of topologies and types but there are some number of lanes running between the PHY and fabric/controller. Depending on the vendor's particular design and implementation there may be some amount of over- or even under- subscription at this point. Keep in mind that the PHY is likely stripping some amount of extraneous framing information while also potentially adding some amount of locally significant tag info (again - varies by specific design). There are actually a number of different ways in which a given frame could be represented under the covers, either as whole/partial frames or as fixed-sized cells. Each approach has its own set of advantages and disadvantages. 

Globally enable multicast routing ("ip multicast routing"). Set up an RP - "ip pim rp x.y.z.q" If you don't have an RP in your network just set up a loopback interface on your switch and use its address. On each L3 interface (SVI, routed switchport) you want to support multicast routing enter "ip pim sparse-mode" 

If the ASBR at the edge of an NSSA receives- and redistributes- a default route then it will show up within the area as an N1 or N2. This is basically the type-7 equivalent of the usual type-5 E1/E2 external routes in a standard area. A default route originated by the ABR with the command you listed will show up within the NSSA as an inter-area summary route (i.e. type-3). To the various routers within the NSSA a type-3 is going to be preferred over an external route. This is just the basic behavior of route selection in OSPF (O > O IA > E1 > E2 > N1 > N2). 

I'm assuming this pertains to Cisco routers/switches. The software itself is going to create a running configuration that contains start and end markers, some default values and an enumeration of the hardware in the box (i.e interfaces). 

Yes - the BRAS is both stripping the PPP[oE] frame (...and some L2/L3 underlay information, depending on the design in use) off of frames coming from the subscriber device and is adding both the PPP[oE] header and associated L2/L3 mapping information to get packets from the Internet back to the subscriber. Part of the process of the subscriber device logging in to the BRAS is establishing a mapping between the subscriber's assigned IP and the necessary underlay information required to get inbound packets encapsulated and on their way to the edge. This is, in fact, a session table and it's one of the reasons why BRAS are fairly specialized devices (read: lots of state + high traffic volume). 

In larger carriers it's actually a lot more common to actually generate the route-maps (and associated peering configurations) from some kind of provisioning database via a script. The idea is that things like series of includes, conditionals based on peer, etc are more easily achieved off the box. This is also a win because it enables provisioning consistency, allows for first-tier folks to provision without CLI access, etc. This was the comment I made a while ago, reposted as an answer. I'll also add that the other benefit of generating the route-maps is that variances in syntax (between vendors or within vendors using multiple operating systems) are normalized. It also ultimately allows for possibilities like downstream customers to edit their individual advertisements/policies in some kind of limited/sane way and to (hopefully) allow for careful, sane netizenship on the part of carriers to prevent bogus advertisements from making their way to the wider Internet (ok..this one is just wishful thinking!) 

So - if we assume the above to be true then there are actually a few ways you can approach the problem. The q-in-vni approach mentioned isn't really one of them, though, as that's really more intended to tie together pure L2 environments. This would be applicable if, for example, if you had a client with a few 3750's spread across the network and you wanted to allow them to run a full set of VLAN's among them without having to provision each one on each VTEP. Put another way, there's no gateway involved. Anyhow - back to what would work. Let's assume you assign a unique VXLAN VNI to each customer network you'll be provisioning. This VNI can show up on any of the VTEP's as needed. Wherever it appears you'll associate it to an SVI which, in turn, would be configured with the customer's vrf and an identical anycast address (i.e. the same default gateway will be present everywhere this customer's network appears). Keep in mind that wherever a given customer's vrf appears you'll also need to create an SVI to associate with an L3VNI. The SVI's don't need to be identically numbered across your VTEP's, as you'll be establishing a mapping on each box between VLAN and VNI numbers. You'll also need to establish an underlay IP network amongst the N9K's and the ASR9K's. It's possible to do this with pure eBGP but I'd strongly recommend using a limited IGP deployment and then setting up iBGP - either with a full mesh or, preferably, a few route reflectors. There's a great deal written about this on various design guides. There are also some considerations that come in if you'd like to use VPC between pairs of VTEP's to provide redundancy to your downstream clients. So - the two ways in which you can now manage VLAN numbering on the VTEP is either using the traditional mechanism, which is to globally assign a given VLAN on VTEP-wide basis. You'd say, for example, that VLAN 100 gets mapped to VNI 10100 and then simply allow that VLAN over a trunk to a 3750 that contains said customer. The next customer's VLAN might be VLAN 101 that gets mapped to VNI 10101. Put another way, any reference to VLAN 100 means the same thing on any port on that box. The other way this can be managed is with port-local VLAN mapping. In this case you can actually map the VLAN identifier on an individual port to any other VLAN on the box. In this case you might create a policy that says that every downstream customer is going to see VLAN 100 but that you'll actually map 100 on the customer facing port to whatever value you want on the box. As an example - perhaps you assign VLAN's 2000-3900 to individual customer VLAN's (..each mapped to their respective VNI, obviously). On the first customer-facing port you might map incoming VLAN 100 to local VLAN 2000. On the next customer-facing port the mapping might be VLAN 100 to local VLAN 2001, and so forth. This potentially simplifies client configuration (...everyone just uses VLAN100 from their point of view, after all) and hides the details of your network configuration. Now - regardless of your choices above, the ASR side is actually not that big of a deal. The clients are actually routing to default gateways on the Nexus 9K's which, in turn, are using BGP-learned EVPN prefixes to send traffic on to the ASR - which can then be configured, for example, to cross-import these routes to another vrf with Internet access (...again, just an example). The opposite is also true (the N9K's are advertising routes and MAC addresses via BGP to both each other and the ASR's). I've glossed over a number of details about EVPN design and implementation and would strongly recommend that you read some of the design and configuration guides out there to get a better feel for how these things work together. It's pretty approachable and familiar if you've done a fair amount with non-trivial BGP and self-managed MPLS but might be confusing if these things aren't in your background. It's also worth mentioning that the above solution is going to provide full L2 reachability within common VNI's for a customer and will provide routing between VNI's if a customer has multiple networks within their vrf. 

That's pretty much it. There are some other considerations if you want to route the multicast to hosts on other switches or if you want to support SSM but this should get you started with basic multicast routing. 

The column "Bytes(%)" refers to the total volume of data coming from and going to that host during the sampling interval (~960 seconds for 3 of the flows and ~260 for the fourth) and the percentage of all data this number represents in the overall interval while, in contrast, "bps" refers to average bits per second during the individual interval - so ostensibly (bytes * 8) / (interval in seconds). The percentages add up to over 100% - which is why I think this table is actually capturing total data to- and from- the IP's in question and that it's likely that some amount of this traffic could be between some of these hosts. A top-n table based on source + destination or, better still, source IP + destination subnet might be more illuminating and should be pretty easy to generate from the data. 

Some folks have scripted this such that all the macvtap interfaces are set as such during initial setup and some integrate at other points. Hope this helps- 

The basic architecture of general purpose compute is just that - general purpose. It's great at applying significant amounts of processing power to a huge variety of workloads but it doesn't truly excel at many of them. As an example - GPU's are used because they're incredibly good at certain kinds of calculations and can perform the work that would equivalently be performed by dozens or hundreds of physical CPU's, all while consuming vastly less power and space while producing much less heat. Could we build performant graphics cards out of tons of CPU's in parallel and just use software? Probably, but it would kind of ridiculous. The same applies to networking. Applying the problem of how to move massive amounts of I/O in a distributed fashion also doesn't map well to an architecture where all processing is basically handled on a small number of big, central CPU's. As impressive as the amount of bandwidth is between a CPU and main memory nowadays it's a fraction of the aggregate throughput of even a few 40GE connections. What's worse is that outside purpose-built hardware architectures such CPU's have to process interrupts for every single packet to be forwarded. Oh, and this is just forwarding packets - no crypto, no translation, encapsulation or other intensive processes. Do the math on how many packets per second need to be forwarded even on a 10GE link is about 14.8 million per second in one direction (with small packets). A basic commodity 48-port 10GE switch has to read, process and regenerate on the order of 1.4 billion packets per second (..on top of any kind of control plane functions). That's for a switch that was considered novel technology a decade ago. At this point switches with nearly 300 100 gigabit ports are readily available, with higher densities and speeds just a matter of product ship dates. The way this has been approached is in a distributed fashion - pushing as much of that processing load to semi-autonomous processors running in dedicated silicon close to the ports themselves. Processing terabits on a single piece of silicon is sci-fi. Processing hundreds of gigabits on hundreds of parallel port ASIC's, though? Done. So why don't SP's just move the network to a bunch of Linux boxes or, better yet, push it to a public cloud? Because all that compute can only exist in a useful way when its compute is surrounded by dedicated hardware that can feed it data (..and in turn that network hardware is only relevant because of the compute, of course). Is there some cross-over in terms of programmability for network gear? Absolutely. Is there some common hardware for control planes? Sure. Cloud routers? Totally. Ultimately, though, just like the machine learning guys started on general purpose compute but have moved to GPU farms and, more recently, dedicated ASIC's so too can many basic networking tasks be run on general purpose compute (the vast majority of network gear never sees anything close to 100% utilization at any point in its life) but ultimately economics and practical engineering mean that there will always be dedicated network hardware. (Oh - and to let you in on a dirty secret, for a very, very long time now pretty much all networking has been software-defined.)