Negative account balances are allowed, but transaction amounts must always be positive. So at this point in time Emma has an account balance of -50, Alex has an account balance of 40, and the company has an account balance of 10. For the system as a whole to become solvent, Emma must deposit (make a payment) and Alex must withdraw (be given a paycheck). As these occur, we add additional transactions: 

Since I was going to have a table anyway, adding this "meta-relationship" table is less redundant than representing the full triplet. However, it now becomes a lot more abstract and difficult to understand. Which way is more correct? 

So under this model Emma has fully funded the cost of the service, and Alex has "withdrawn" $50 from his account (though he only gets to keep 40 of it). Thus, Alex ends up with a -$10 account balance. Alex's account will become solvent again when he does more work, or pays his balance to the company. Is this a good idea, even though the reality is just that Emma handed Alex $50? 

If that looks too simple - keep in mind that you will have to drop and later recreate foreign keys if you have any. :) 

Usual rules apply. Code changes should not be required but something will break if code is more complex. Also there will be some SQL plans regressions. Official documentation about deprecated features is there: Deprecated and Desupported Features for Oracle Database 12c Also you can check Metalink note 567171.1 about parameter. This parameter is used to enable or disable certain bugfixes which affect query behaviour. In 11.2.0.4 the view contains 854 rows which means that there are so many potential quirks while upgrading to the newer version of Oracle Database. I am sure that 12c contains much more rows. 

First step has to be run on all nodes of the cluster. Second step though - just on one node which will rebuild all the indexes. For foreign keys you can try to split first step into two: 

(Plan XML here) Have I stumbled upon some kind of bug or am I doing something wrong on my end that I should be doing differently? (P.S. I know what the warning means and how to fix it, I am more interested in the warning showing up in one place but not another.) EDIT: Here is the version information for my SSMS from the "About" help page. 

From what I have gathered the 256 limit is a hard limit for "supported configurations", once you go over 256 some features may fail to work. For example I have a database with 629 merge articles and it works fine, but if I add a filtered article to it it will blow up with a error similar to this error "Message: Too many table names in the query. The maximum allowable is 256." when you try to build the snapshot. 

XY Problem background info: I have a pull replication publisher in which I want to add or alter a index and have those changes be applied to the subscriber. The solution to that problem I would like to do is to generate a new snapshot and re-init the subscription. 

I do that now and then to create test environment. Just for such more complex directory structure I prefer . With it you can resume copying if you run into some disk space or network issues. 

2nd step is to drop all the active connections and unfinished transactions so that node is free from user queries. And all the applications continued to work on the other SQL node. 

I was using the following steps to perform backup and restore. In the first step I generate dump script to make schemes structures backup. 

Everybody here are missing one point - SQLs in question are retrieving 1000 rows. And one cannot retrieve 1000 rows fast unless most of the data is cached. If data is not cached one has to do 1000 random reads sequentially to retrieve the data. Good disk based storage with fast disks will give you up to ~200 reads per second. Common disks even in RAID I doubt that manage even 100. That means 10+ seconds to get results even with the best indexes. So in the longer run such data model and queries won't work. Now the queries run fast when you hit cached data. 

I looked in to the declaration of and found out the way it invalidates the snapshot is a simple query to update the table 

My Question: How do I, from the publisher, mark a pull merge replication publication as having a invalid snapshot such that if I did the column would return 0. Doing will cause the subscriber to re-initialize but it does not mark the snapshot as invalid. I know I could change a publication or article property then change it back and cause to invalidation to happen that way but I would really like to invalidate as the "primary action" instead of having the invalidation be a side effect of some other action. I am looking for something similar to transactional replication's procedure which has a parameter, but I could not find the equivalent for merge replication. Is there any way to invalidate a merge replication snapshot only without making some other kind of change that has snapshot invalidation as a side effect? 

Notice that these are not captured by the binary many-to-many relationships I described earlier - John can be tutoring Maria, and Maria can be enrolled in History, but that doesn't mean that John is tutoring Maria in History. The way I see it, I have two options: Option 1 - A single table containing triplets Table : 

I'm developing a new double-entry accounting system for my company, which has Customers and Workers. Customers pay the company for services rendered, and in turn the company pays Workers for providing these services (sort of an Uber model). The idea is to model an Account for each Customer, Worker, and the company itself. Each transaction therefore, is either a transfer of funds between two of these accounts, or a transfer of money in/out of the system as a whole. The latter case would be, for example, when a Customer makes a payment or when a Worker receives a paycheck. For example, consider the following accounts: 

Maybe db_cache_size, shared_pool_size, sga_target or other memory related parameters are set to non zero? Remember that when using AMM those parameters specify minimum memory allocated for particular pool. So if sga_target is 6GB you will not be allowed to set memory_target to 4GB. Also sum of internal variables __sga_target, __db_cache_size, etc. may be more than your specified value of 4GB. If you see those symptoms you can cleanup pfile bounce Oracle with pfile and recreate spfile. In the same step you can also set to zero. 

Now you can unconfigure and remove old machines from old cluster. Of course Oracle version on new machines has to be the same as on old machines. Or it is possible to do upgrade right away on new machines. You should test procedure of course. There are quite a few possible problems on the way. The idea is that Oracle DB does not store anything in "cluster". All the data is in datafiles, controlfiles, redo logs and spfile. Which is stored on ASM can can be mounted on another server. 

This is nice because it explicitly lays out the full relationship in a single row. However, I now need to maintain this table in addition to my simple relationship tables. Option 2 - Meta-relationships Table : 

I am designing a database which needs to store some fairly complex relationships among many different types of entities. In the simplest example, let us say I have three types of entities - "Student", "Tutor", and "Course", each with their own tables (, , ). First of all, I need to represent simple many-to-many relationships between each possible pair of entity types: 

Suppose Alex provides a service to Emma. Emma is charged for it, and Alex is paid some percentage. In the table, this would look like: 

Transaction 3 is essentially "Emma funding her account", while transaction 4 is "Alex withdrawing from his account". Any account's current balance can be derived by adding all of the amounts where their account appears in the , and subtracting all of the amounts where their account appears in the . Our "accounts receivable" would simply be the sum of all negative balances, and "accounts payable" would be the sum of all positive balances. This seems like a good approach, but it doesn't really reflect the actual flow of money. When Emma pays us from her credit card, from the banks' perspective there is no "Emma's internal account" - the funds are simply transferred directly from Emma's bank account to the Company's bank account. Is it a problem for us to model "Emma's internal account" anyway? There is also the possibility that Emma will pay $50 to Alex directly when services are rendered, and then Alex needs to pay the Company their share ($10). Under my proposed system I would model this as follows: 

The problem is from in the first outer join. can contain text in the ENTRY_CODE column, however all records that have set to the line number from where will always be numbers only. I perform the cast as may or may not have leading 0's and spaces so I am trying to get them to be well formed. What appears to me to be happening is adding to the outer query causes the clause not to be evaluated before the cast to int in the inner query. I have tried things like adding to both the inner and outer queries but it has no affect. 

When working with virtual machines often it can be very useful to have a base read-only VM then have several smaller VM's that use that larger VM as a base and write their changes to their own writeable copy. Is something similar available in Microsoft Sql Server? The situation I am in is we host a demo copy of our product that our clients can connect to for about 30 days or so. When we create a demo account we have to create a new copy of the base database the program uses and have the software point at that. Each image is a little over 2GB, but on average the demo users will only change about maybe 100MB worth of data in the database. What I would like to do is have a read-only database that acts as a base then have the demo databases create a "differencing database" and write out it's data and log information to that differencing database. Is this possible? I have been searching through the MSDN documentation but I have yet to find anything and unfortunately google searching has been fairly useless as searching for gets polluted with all the pages about doing differential backups (doing did not return any useful results either, all I got was information about differential deployment scripts). 

You will need some more space for row overhead and PK storage. More information you can find in MySQL Documentation. For the most detailed information you can use ndb_size.pl utility. 

impdp/expdp unlike imp/exp does not move the data. They only invoke DBMS_DATAPUMP package and actual data movement is done by the Oracle instance. So data pump can access external data the same way as all other Oracle procedures - loading files via directory object or SELECT data via dblink. 

Ultimate source for such answers is Oracle Database Licensing Information. Sadly to downgrade from Enterprise to Standard Edition you have to export all the data install Oracle Standard Edition and import data (Doc ID 139642.1). From your list "Automatic SQL Tuning Advisor" is Enterprise Edition only feature. As a rule of thumb - features which needs AWR are Enterprise Edition only plus you need to buy . 

Also one small note for the multiple schemas approach - put different applications data into separate tablespaces. This will add just a few minutes while creating users but may save a lot of maintenance time later. Trust me. :)