In addition to the above major considerations, you also need to consider licensing and support requirements (open source or closed source; in house support, third party support or vendor support) application/language requirements (the connectors for many databases can be important; is your app compiled? Do you have access to the source code? Can you recompile it, or is it provided by a vendor? Or does it run on an interpreted language?) political requirements (Does your organization only trust Oracle? Do they hate oracle? Do they not trust MySql? How do you feel about MariaDB or Postgres?) and database engine (innoDB? MyISAM? Blackhole? NDB Cluster? Spider?) and historical or compatability requirements (we used PL/SQL for years and half our code is built into the oracle engine! How could we ever port over to MariaDB?!?) All of these things (significantly) impact the tools available to you. 

You are probably unable to find data on this for two main reasons. Firstly, because it varies from system to system. Does an outage cost you money due to contractual obligations? Or only in lost reputation and in user perception? How much does that cost? This varies from organization to organization. But I think that might be what you are getting at with the Bernoulli variable... ...Which brings me to the second thing. In order for you to be able to model risk, you need research in which someone has already statistically proved or measured the efficacy of DevOps methodologies (which means you have to define exactly what DevOps is first, a notoriously tricky issue.) You also need to be able to compare that to old methods - meaning that someone must also have measured data for the Dev/Ops segmented model. I'm not sure that research exists or is settled. Probably because the efficacy and cost of Dev/Ops will also vary widely from organization to organization. In that respect it may provide an opportunity to measure the old methodology and then show the average improvement of changing methodologies if you can find enough research subjects. In short, it is doubtful that such a formula for risk modeling exists. If it does, it likely exists as paid marketing research to a consulting firm and is unlikely to be available to the public and I would personally be very dubious of the accuracy of it's results due to the subjectivity of the input variables measured. 

Amazon's IAM roles generally grant a role access to a particular ARN (Amazon Resource Name). Amazon notes on their pages that for S3 a resource 

I have found however, that I am unable to pipe the formatted results (from jq) into additional tools in my standard toolset, such as grep, sed, awk, etc. How can I make make this output pipe-able? Eg, this doesn't work: 

It seems you have missed DNS. First, don't reference the device by IP, instead use a FQDN. If you need, you can set an entry for 192.168.99.100 in /etc/hosts for now. Secondly, type "hostname" on the node and verify it does not return "localhost" (it likely will). Then type "hostnamectl set-hostname newhostname.fullyqualified.com" and then regenerate and re-sign the certificate. 

Yes, but you will first need to log in as the master or root account, and then in order to log in as that user. There is no way to log in as any user without first going through a privleged user's account. An additional caveat is that the user you are trying to log in as needs to have a shell of or similar valid shell in . If the user has a shell like or you will be unable to change to this user's account (but neither will anyone else for that matter). 

RTO - RTO or Restore Time Objective for backups are one of the most important drivers of database backup solutions. While, at first it might not appear relevant to most of the other problems, it becomes extremely relevant when you ask "What if I used my backup solution for creating or seeding new instances?". I'll cover some techniques for doing so in the next section. RPO - RPO or Restore Point Objective for backups defines A) how far back you are able to restore (minutes, hours, days, weeks, months, or years) B) The backup interval at different tiers and C) how granularly you can restore. For example, for E-mail databases, Message Level Backups - restoring a specific E-mail - are often sought. Similarly, you may find that data over a few days is completely useless - so there is no point restoring back a year. Size of your dataset - This is important because for a 1MB database, your RTO may be achieved with most backup products and solutions. For a 10TB database however, you will find that a full, row level backup to LTO 3 tape probably won't achieve your RTO and could interfere with your RPO because backups begin exceeding your backup window. You can't exactly just do a mysqldump on this large of a dataset, but could probably get away with that on the 1MB database. Database consistency - A final thing that makes an immense difference in continuous deployment, site reliability, scalability and high-availablity when working with databses is your need (or lack thereof) for consistency. There are three basic types: immediate consistency, Just-In-Time (JIT) consistency and eventual consistency 

Depending on the answers, it might be ideal to use a single version numbering system led by marketing - but marketing will need to do a better job of understanding the risks of under-the-hood changes. If this is needed as a designator for setting deadlines for change integrations, then two version numbering systems might be approperite: Side note: git tags would be great for any version numbering scheme you have mentioned. 

You may wish to try scripting the actions you want. Then use git hooks to trigger your scripts to release the latest code to your environment. Alternatively, you could use something like Jenkins to configure a "push on green" system if you are working with compiled code. Lastly, (though I haven't used them) Amazon claims that their CodePipeline product can do this (and perhaps CodeBuild can as well), though my personal preference is to keep things as generic as possible and avoid drinking Amazon's Kool-Aid in order to keep things portable. Like a drug dealer, the first hit may be free with Amazon, and things are great until they have you hooked and decide to jack up the price. Utilizing my own open source system ensures that I can easily take my ball and go to another cloud or home altogether (and host it myself) in the event that I need to make a rapid exit from Amazon's cloud and avoids lock-in. 

But finally, to answer your question, use the three smaller hosts. You are clearly virtualizing anyway and this provides you redundancy. If your single large hosts crashes or reboots, it affects your entire system. If you have a problem with one instance, it affects 1/3rd of your environment and you might even be able to seamlessly migrate your VM to another host if your system is properly configured. 

I am using Salt for deployment and configuration management. In order to attach to a FreeIPA server, I need to randomly generate an OTP token and then use it to attach to IPA. How can I share a variable between SLS files? For example, init.sls: 

and links to the above image. This image is attempting to comically illustrate the confusion many feel with the world of DevOps. There is a smorgasboard of toolkits and frameworks out there, and the bullets in the image are a commentary of the nonsense of the diagram to the right. This is seen in the first bullet: 

The most likely scenario is that 1. The server doesn't trust the client's signing certificate authority since the server doesn't verify DNS for the client certificate and the error indicates this is a not on the client. Most likely this means the client's certificate was signed on the client itself, not on the server. You will need to make sure to generate the certificate on the client, have the server sign it and then transfer the certificate back to the client. Otherwise, you need to add the server's CA certificate to the Client's keystore.. You can ensure you do not have a DNS/DN mismatch by setting hosts file entries. 

You aren't actually using file.managed, you are using file.append - so salt is expecting to add onto the end of an existing file. This may be the cause of your issue. Instead try: 

(this issue arises because puppet expects 2 GB or RAM out of the box) You are bound to find that running all of these on a single server is debilitating and painful with only 1 GB of RAM. You should probably consider using something like virtualbox on your PC or laptop so that you can get some more ram than that offered by the AWS free tier. I'd say you want at least 4 or maybe 8 GB of RAM. While this probably can be done, I don't recommend it. You may have to stop one service to run another which then makes it difficult to have one talk to the other (eg, ansible talk to Jenkins while puppet is running, for example.) 

You can then execute environments for that state with the command However, it appears that the same cannot be said for an arbitrary command. Running will run on all minions instead of just the minions in the "example" environment. How can I run commands for only a specified environment? 

Expense Technology can be expensive. Buying a solution only to discard it later cares with it hugely wastful expenses. But even more expensive is the time expended by personnel. Salaries are every companies' largest expense. Wasting time developing a solution only to discard it and develop a replacement is wildly inefficient. 

You shouldn't hire a DevOps engineer because DevOps encompasses such a wide variety of disciplines that one individual cannot possibly be an expert in all aspects of these disciplines. By hiring a jack of all trades, you would be hiring a master of none. DevOps is necessarily a team based endeavor and you cannot possibly expect one individual to support the expectations of an entire team. Consider the scope of DevOps. One person cannot possibly: 

This sounds like an asymmetric or n-path routing issue. Here is what is probably happening: Machine A at IP address 192.168.1.1 makes initiates a [SYN] request through the LB at 192.168.1.10. the LB then proxies the payload to Machine B at 192.168.1.2, so the payload now has source: 192.168.1.1 and has has destination: 192.168.1.2 (which used to be 192.168.1.10). So what happens now when 192.168.1.2 responds with a [SYN, ACK]? What should happen is that Machine B should respond to Machine A through the load balancer - typically because of a default route or gateway on the server that routes traffic through the LB. In this case however, the machine is on the same subnet, so the route/gateway is not used and the routing table ignored by the server. This means that when the server responds, the [SYN,ACK] appears to Machine A to come from an IP different than the IP that Machine A initiated the request with - it was expecting a source IP of 192.168.1.10 (the LB) but is seeing a [SYN,ACK] coming from 192.168.1.2 (machine B) and thus the LB is unable to establish a connection with the machine B in this scenario because the response went to the wrong device. The reason this works for external traffic is because of your default route - the responses to everyone else are routed through the ELB. The ELB sees that it was initiating a connection and automagically intercepts the response and swaps the source of 192.168.1.2 back to 192.168.1.10. So, for one solution to this issue, you could implement one-armed load balancing (also known as a load balancer on a stick). What this will do is use a Source NAT on the inside interface of the load balancer (so assume you had outside interface 192.168.1.10 on your load balancer and 192.168.1.11 on the inside interface). This will make all traffic appear to be coming from 192.168.1.11 from the perspective of Machine B which should solve your connection issue. It appears however, that your AWS ELB doesn't support SNAT, so you will either need to put your hosts and ELB on different subnets or use something that supports SNATs like F5's Virtual Edition which comes in hourly or BYOL flavors. Beware connection limitations with SNATing though - if you need over about 30k simultaneous connections you will run into SNAT port exhaustion and need to start using a SNAT pool.. Hence, you best solution (for cost and to prevent future issues) would be to make sure the client and server are on different subnets. The best way to confirm would be to use tcpdump on the connecting host and/or back-end server and look for responses coming directly to/from the back-end server instead of going through the load balancer. You can then load your dump file into WireShark to figure out exactly what is going on. 

Tuning the parameters you have identified will certainly have an effect - to degrade performance. For example, net.ipv4.tcp_window_scaling is an on or off option. Similarly, net.ipv4.tcp_sack is either on or off. Both default to a value of on on Linux and on all load balancers. Both of these were defined in RFC1323 which was published in order to give us the ability to achieve 10/100 Ethernet networks. Without this RFC, networks were limited to 10 Mbps a second, so disabling these will result in a huge, immediate and noticable performance hit. These performance benefits come because there is a huge difference between "almost" zero latency and zero latency. Using a sliding TCP window allows equipment to use Selective Acknowledgements (SACKs) to acknowledge the receipt of hundreds of packets simultaneously. So, if latency is 10ms on a network, without SACKs, every packet has to be acknowledged and that 10ms latency adds up very quickly - even at theoretical minimum latencies. This means that I send packet A, then have to wait for a response to packet A before sending packet B. But as it turns out I can send the rest of the alphabet and then some while waiting for the ACK to packet B. Enter SACKs - These allow the receiver to signal that it is has successfully received up to and including packet Z. Meanwhile, the sender is already sending packet AA-ZZ. This results in something known as a "TCP Window Size" which is the maximum amount of data the client can buffer/handle. Since memory can be a concern, if the recipient is only capable of storing and processing Packets A-F, if we send A-Z some amount of data is going to be lost. Therefore, it becomes necessary for the recipient to signal their ability to receive data - that is "I can only buffer this many packets). This is really CPU dependent on how fast the recipient can empty their buffers and transmit the data up to layer 7. It is not uncommon to see TCP windows scale from empty to full (Max window size goes from 0 indicating the window is completely used to max indicating the window is completely available) and this is one way throughput bottlenecks can be identified on your network. TCP Window Scaling offers two primary benefits: