Yes. With M=(λx.x x) per the question, consider the rewrite ζ that takes M M p to M M. It is confluent, and so characterises a reduction system over the lambda calculus. Argument sketch for confluence: since M M is closed, we only need to consider critical pairs of the form M M N1 ... Nk. These can be resolved. It is an invisible equivalence, because terms of the form M M I ... I (with zero or more I s) are closed unsolvable terms that reduce only to themselves in the base lambda calculus, thus are distinct, and so the infinite set of these terms is nontrivial, and is obviously equated by ζ. I don't like accepting my answer to my question, so I'll accept an answer from someone who provides a less absurdly incomplete confluence argument. 

It leads nicely into talking about computation in biology, the role of logic in computer science, &c. 

The $\lambda\pi$ calculus —which is essentially the core of LF, in turn the most widely reimplemented approach to higher-order logic— is by far the simplest dependently typed system you can learn, since it consists just of the extension of the type system of the simply typed lambda calculus with dependently typed quantifiers. So the key intuitions needed to master LF are intuitions you need to master with any theory with dependent types. Twelf is a good theorem-proving system based on LF. Looking over the advanced course notes offered by Frank Pfenning are a good introduction to the theory and practice of LF. That said, it is perhaps not the best first system to learn if your interest is in constructive mathematics rather than the essentials of type theory: LF means logical framework and it is a system used to axiomatise theories, and is not so often worked in as a target system directly. Using a system based on Martin-Loef's type theory is probably the best introduction -Dave mentions Agda, among others- is maybe a better starting point if this is your goal, since you can get going with arithmetic and inductive types more quickly in such a framework. 

Use specialised software to manage submission of papers, assignment of referees, receipt of referee reports, gathering PC member assessment of reviews, and communication of outcomes. Conference management software on the Edutech wiki lists some options. 

No, not recursively. The idea that randomness is absence of predictable pattern is usually formalised by Martin-Löf randomness (see WP, Algorithmically random sequence). Martin-Löf randomness consitutes a recursive degree (as a mass problem) stronger than the base recursive degree, so it cannot be generated by a machine. See Simpson, 2005, Mass problems and randomness. 

The two inference rules are different, because the first requires that x:T_1 is the only assumption, while the second allows side assumptions. This can have subtle effects of the consequence relation for the type theory prevents the type theory from modelling weakening by having as the hypothesis rule: $$ \frac{}{\Gamma, x:A \vdash x:A} $$ In your English gloss, I would have under the sole assumption, to avoid confusion with the assumption-gobbling inference rule: $$ \frac{\Gamma, x:T_1 \vdash t_2 : T_2}{\vdash \lambda x:T_1.t_2:T_1\to T_2} $$ 

Currently, the most systematic proof theory that allows many modal logics to be layered upon many substructural logics is Belnap's display logic, which has received a decent treatment at the hands of Marcus Kracht —see in particular his Power and Weakness of the Modal Display Logic, 1996— and Heinrich Wansing, Displaying Modal Logic, 1998. Display logic has problems handling noncommutative logic, which was one of the motivations behind a couple of MSc theses I supervised some years back, to apply some ideas about representing modalities in the Calculus of Structures, which is very powerful for representing substructural logics, but ran into problems because of the unusual way cut-elimination is proven in that setting. Robert Hein's work on generating rules for modal logics from families of axioms, summarised in Purity through Unravelling, 2005, covers most of the usual logics (the most important axioms not covered are B, CR, and L), and there is fairly strong circumstantial evidence to believe the cut-elimination conjecture. None of this work actually treats substructural logic, but if a stronger kind of cut-elimination theorem were proven for these modalities, the so-called splitting lemma, this would make the logic very modular and cut-elimination should follow easily for all ways of gluing together the logics. Substructural logic doesn't really have a uniform notion of semantics, but for modal substructural logic we do have a kind of recipe for turning semantics of the base logic into semantics of matching modal logics, by extending a trace-like semantics with a notion of frame or an algebraic/categorical semantics with a notion of operator. Kracht and Wansing do some work in both of these directions. 

Retoré 1997, Pomset logic: a non-commutative extension of classical linear logic Strassburger 2003, Linear Logic and Noncommutativity in the Calculus of Structures Kahramanogullari 2009, On Linear Logic Planning and Concurrency, Information and Computation 207:1229 - 1258. 

Keep prices down: you can get lovely hors d'oeuvres at a -per-seat hotel-hosted conference, it is true, but it tends to detract from the intellectual atmosphere you can get with a -per-seat university-hosted conference. Also, you don't just get the people attending who deliver talks. Allow students at the university to attend for free; Space parallel sessions, so that people can really move between them; But also encourage interaction between speakers and attendees at parallel sessions beforehand, so that parallel sessions are also autonomous communities. This encourages interaction, and the sense that parallel sessions are moving the discussion in their subfield forward. Some conferences (e.g., the German linguistics conference, DGfS) have taken this to the conclusion of having all parallel sessions be independent workshops. 

Resolution only applies to a subfragment of FOL, the Horn clauses, that lack disjunction and existential quantification, but combined with Herbrandisation, this is sufficient to encode satisfiability in the following sense: for any formula of FOL, we can find a Horn clause such that each is satisfiable if the other is. Resolution is related to cut-elimination, but is more efficient. Extending resolution to the whole of FOL requires a more sophistcated approach to unification than Robinson's algorithm. Uniform provability offers a technique for reading off such a computational mechanism from a cleaned-up proof theory of intuitionistic logic, higher-order unification, and the double-negation translation. 

Barry Jay's SF calculus is able to look into the structure of terms it is applied to, which is non-functional. Lambda calculus and traditional combinatory logic are purely functional, and so cannot do this. There are many extensions of the lambda-calculus that do things that violate purity, most of which require fixing the rewrite strategy to some degree, such as adding state, controls (e.g, via continuations), or logic variables. 

The lambda calculus is older than Turing's machine model, apparently dating from the period 1928-1929 (Seldin 2006), and was invented to encapsulate the notion of a schematic function that Church needed for a foundational logic he devised. It was not invented to capture the general notion of computable function, and indeed a weaker typed version would have served his purposes better. It seems to be incidental to the purpose of that the calculus Church invented turned out to be Turing complete, although later Church used the lambda calculus as his foundation for what he called the effectively computable functions (1936), which Turing appealed to in his paper. Church's simple theory of types (1940) provides a more moderate, typed theory of functions that suffices to express the syntax of higher-order logic but does not express all recursive functions. This theory can be seen as being more in tune with Church's original motivation. References 

A proposition $P$; and A propositional attitude $\cal A$ – this is what philosophers call it, cf. propositional attitude reports which tie propositional attitudes to the speech acts Kaveh talks of; 

Here, where public interest in the problem has been expressed, I think it's good for science to make known the fact that you believe you have solved the problems and provide some concrete details of what you have achieved. If you have reason not to show your hand right away, I think the goal should be to figure out what you are now comfortable saying in public, and how to present it. So I argue against private email. Something I've seen some people do is write research bulletins, a bit like personal technical notes, that summarise findings on a topic in some degree of rigour, but without attempting to be comprehensive in the way that makes writing papers for peer-reviewed publication so time-consuming. The progress reports that Harvey Friedman used to send to the Foundations of Mathematics mailing list would be a good exemplar of that kind of thing, e.g., Self-contained posting 82: Simplified Boolean Relation Theory. Starting such a bulletin series to deal with this, and then posting an excerpt here seems like a good strategy, since it allows you to quickly identify your achivements, while being in control of what details you make public. I don't recommend regular blog posts for this, since they carry some unwanted associations, that they are conversational, open to revision, and not properly referenceable documents. Writing in a form for publication on Arxiv would make sense, but a Research Notes section on your publications page linking to an html page would work. 

Hilbert-style systems are good for characterising the logical consequence relation of a logic, and they are usually good for categorising several logics, such as rival modal logics; Tableau systems are good for formalising decision algorithms. Typically if a logic is decidable, one can find a terminating tableau system as a decision algorithm, and if not one can find a potentially non-terminating tableau system that provides a semi-decision procedure. If one wants to show an upper bound on the complexity of decidability (i.e., the complexity class of a logic), tableau systems are generally the first place one looks. Analytic proof theories, such as Gentzen's natural deduction and sequent calculus, give representations of proofs that are good for reasoning, and offer the notion of analytic proof, which is useful for proving useful properties such as interpolation for a theory. Tarski-style model theories are often even better for reasoning about logics, because they nearly completely abstract away from the syntactic details of the logic. In modal logic and set theory, they are so much better at delivering the results that those logicians tend to have very limited interest in tableau and analytic proof theory.