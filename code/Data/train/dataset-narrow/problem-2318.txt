I am currently searching for a Ph.D. project and this is the kind of stuff that interests me. I would certainly like to contribute to solving this problem, if it is still open. That is why I would like to know some of the recent developments on this. Is this still an open problem? What contributions were made in the last decade? 

I wouldn't think so. I'm assuming your $a_n$'s are normalized, i.e. that $||\sum_n a_n |n\rangle|| =1$. But then your wish output is not normalized since $$||\sum_n e^{ia_n} |n\rangle|| = \sqrt{\sum_n |e^{ia_n}|^2} = \sqrt N$$ where I assume there are $N$ terms in the sum. Quantum (unitary) operations have to preserve the norm so there can't be one that maps the first state to the second one. 

For computational complexity, there is no proof that quantum computers are better than classical computers because of how hard it is to obtain lower-bounds on the hardness of problems. However, there are settings in which a quantum computer provably does better than a classical one. The most famous of these examples is in the blackbox model in which you have access via blackbox to a function $f:\{0,1\}^n\mapsto \{0,1\}$ and you want to find the unique $x$ for which $f$ evaluates to 1. The complexity measure in this case is the number of calls to $f$. Classicaly, you cannot do better than guessing $x$ at random which takes on average $\Omega(2^n)$ queries to $f$. However, using Grover's algorithm you can achieve the same task in $O(\sqrt{2^n})$. For further provable separations, you can look into communication complexity where we know how to prove lower bounds. There are tasks that two quantum computers communicating through a quantum channel can accomplish with less communication than two classical computers. For example computing the inner product of two strings, one of the hardest problems in communication complexity, has a speedup when using quantum computers. 

There are several recent papers that address your question, e.g.: Margareta Ackerman and Jeffrey Shallit. Efficient Enumeration of Regular Languages. Conference on Implementation and Application of Automata (CIAA) , Lecture Notes in Computer Science, 4783, Springer-Verlag, Berlin Heidelberg, pp. 226-241, 2007. Margareta Ackerman and Erkki Makinen. Three New Algorithms for Regular Language Enumeration, Computing and Combinatorics: 15th Annual International Conference (COCOON) , Lecture Notes in Computer Science 5609, Springer-Verlag, Berlin Heidelberg, pp. 178-191, 2009. You can get these two papers from here. 

Let the "DFA $\to$ NFA" problem denote the following: Given a DFA $A$ and an integer $k$, is there an NFA with at most $k$ states equivalent to $A$? Similarly, let "DFA $\to$ RFSA" denote the problem obtained from the above if we replace "NFA" with "residual finite state automaton". Jiang and Ravikumar showed that the "DFA $\to$ NFA" problem is PSPACE-complete by a reduction from the "DFA union universality" problem. The latter problem has given a list of DFAs $A_1,A_2,\ldots,A_n$, and asks if $\bigcup_{i=1}^nL(A_i) = \Sigma^*$. Their reduction goes by defining a language $L$ from these DFAs and a suitable integer $k$, such that a DFA accepting $L$ can be constructed in time polynomial in the size of the DFAs $A_i$. Then they show that every NFA (thus a fortiori every RFSA) accepting $L$ needs at least $k$ states in case $\bigcup_{i=1}^nL(A_i)$ is universal and at least $k+1$ states otherwise. Then they construct a $k$-state NFA $N$, which accepts $L$ iff $\bigcup_{i=1}^nL(A_i) = \Sigma^*$. This proof was reconsidered later by Gruber and Holzer (Developments in Language Theory '06). They use the same reduction to show a slightly different result, which concerns the computational complexity of lower bound techniques for NFAs: An (extended) fooling set for a regular language $R$ is a set $S$ of word pairs $(x_i,y_i)$, such that for each $i$ holds $x_iy_i\in L$ but for all $i\neq j$ holds: $x_iy_j \notin R$ or $x_jy_i\notin R$. For the above reduction, they show that there is an extended fooling set $S$ of size $k$ in case $\bigcup_{i=1}^nL(A_i)$ is universal. By inspecting the proof by Gruber and Holzer, one readily notes that each "left word" $x_i$ in the set $S$ is such that the NFA $N$ mentioned above can go from the initial state to only a single state $q_i$ on reading $x_i$. That is, the language accepted from state $q$ equals $x_i^{-1}L$, and thus $N$ is a residual finite state automaton. If the above reasoning contains no mistakes, this yields a reduction from the DFA union universality problem to the "DFA $\to$ RFSA" problem, and thus it is PSPACE-hard. Membership in PSPACE follows along the same lines as for the DFA $\to$ NFA problem, so the "DFA $\to$ RFSA" problem is PSPACE-complete. Being more general but still in PSPACE, the "NFA $\to$ RFSA" problem is PSPACE-complete as well. For those who want to reconstruct the above argument (please don't take it for granted, my writeup was a bit hasty), I recommend reading the proof of Theorem 15 in the ECCC report cited below. Especially, Figure 5 on page 18 depicts the automaton $N$ which I claim to be a RFSA. T. Jiang and B. Ravikumar. Minimal NFA problems are hard. SIAM Journal on Computing, 22(6):1117–1141, December 1993. Hermann Gruber and Markus Holzer. Finding Lower Bounds for Nondeterministic State Complexity Is Hard. In Oscar H. Ibarra and Zhe Dang, editors, 10th International Conference on Developments in Language Theory (DLT 2006), Santa Barbara (CA), USA, volume 4036 of Lecture Notes in Computer Science, pages 363--374. Springer, June 2006. Hermann Gruber and Markus Holzer. Finding Lower Bounds for Nondeterministic State Complexity is Hard. Technical Report ECCC TR06-027, Electronic Colloquium on Computational Complexity, 2006. 

The question has been answered in the comments by Tsuyoshi & Chandra! I am adding this CW answer so I can accept it to indicate the question is closed. Thanks, everyone! 

I am interested in learning connections between "chaos," or more broadly, dynamical systems, and the $P{=}NP$ question. Here is an example of the type of literature I am seeking: 

I feel I should know this... But I am not finding a definitive reference. Is it $\Omega(n^d)$? How about the $d{=}2$ specialization: The largest area bounded cell in an arrangement of lines? 

Let $G$ be a graph with (positively) weighted edges. I want to define the Voronoi diagram for a set of nodes/sites $S$, to associate with a node $v \in S$ the subgraph $R(v)$ of $G$ induced by all the nodes strictly closer to $v$ than to any other node in $S$, measuring the length of a path by the sum of weights on the arcs. $R(v)$ is $v$'s Voronoi region. For example, the green nodes below are in $R(v_1)$, and the yellow nodes are in $R(v_2)$.            I would like to understand the structure of the Voronoi diagram. As a start, what does the diagram of two sites $v_1$ and $v_2$ look like, i.e., what does the 2-site bisector look like (blue in the above example)? I think of the bisector $B(v_1,v_2)$ as the complement of $R(v_1) \cup R(v_2)$ in $G$. Here are two specific questions: 

I think that this question has been studied previously. Mike Domaratzki wrote a survey on research in this area: "Enumeration of Formal Languages", Bull. EATCS, vol. 89 (June 2006), 113-133: $URL$ 

The following paper reports on an implementation of the Kameda-Weiner algorithm for computing a minimal NFA, as well on an approach using a SAT solver. I don't know whether the implementation is available, but perhaps you can contact the authors about this. Jaco Geldenhuys, Brink van der Merwe, and Lynette van Zijl. Reducing Nondeterministic Finite Automata with SAT Solvers. Revised Selected Papers from the 8th International Workshop on Finite-State Methods and Natural Language Processing (FSMNLP 2009), LNCS 6062, Springer, pages 81-92, 2010. 

Devdatt Dubhashi and Alessandro Panconesi: Concentration of Measure for the Analysis of Randomised Algorithms. A first draft is available at $URL$ (via geomblog) 

Let me second Michael's judgment, this is indeed an interesting question. Michael's main idea can be combined with a result from the literature, thus providing a similar lower bound with a rigorous proof. I will refer to bounds on CFG size in terms of the total number of alphabetic symbols in the $n$ regular expressions. Let this number be denoted by $k$. (As john_leo noted, we will not find any useful bounds in terms of the number of regular expressions taking part in the intersection.) Neither the OP nor Michael did find it necessary to mention this, but an upper bound of $2^{k+1}$ (on the number of states) for converting an intersection of regular expressions into a NFA can be easily proved. For the record, here it is: Convert the regular expressions into Glushkov automata, which are all non-returning. Then apply the product construction to obtain an NFA for the intersection of these languages. (I suppose that one can improve the bound to $2^k+1$ or so.) An $s$-state NFA can be converted into a right-linear grammar (which is a special case of a CFG) of size $O(s^2)$ (if we measure grammar size as total number of symbols on the left- and right-hand-sides of the productions), thus giving size $O(4^{k})$. This bound of course sounds horrible if you have practical applications in mind. Trying to prove a better bound using nondeterministic transition complexity instead of nondeterministic state complexity for estimating the size of the NFA may be worth the effort. The other part is finding a witness language that can be succinctly expressed as the intersection of regular expressions, but is necessarily cumbersome to describe with a CFG. (Here we need to establish a lower bound on the size of all CFGs generating the language, of which there can be infinitely many.) The following argument gives a $2^{\Omega(\sqrt{k}/\log k)}$ lower bound. Consider the finite language $L_n = \{\,ww^Rw \in \{a,b\}^*\mid |w|=n\,\}$, where $w^R$ denotes the reversal of $w$. Then $L_n$ can be expressed as the intersection of the following $2n+1$ regular expressions: 

To more directly address your question, perhaps this is the most natural interpretation for $m=1$: Pollack, Sharir, Rote, "Computing the Geodesic Center of a Simple Polygon," Discrete Comput. Geom. 4:611-626 (1989): 

Here is a figure from the first:     Note that the shapes mentioned in Yoshio Okamoto's post appear in this hierarchy. And if you are wondering what a (Barbados-induced?) "palm" polygon looks like...              

(2) I also found a 2007 German Ph.D. thesis, "Facility Location and Related Problems," by Martin Romauch (PDF link), that includes a chapter on the "Vertex Guard Double Cover problem," showing that it is NP-hard for polygons with holes. He also shows that the right combinatorial bound is $\lfloor 2n/3 \rfloor$ (disappointingly obvious!). I have only skimmed through this, but it is certainly worth a look. 

This is not a definitive answer, just some references. First, you might look at the constrained shortest-path first algorithm, about which I know little, and which doesn't look all that sophisticated. Second, the paper "Polygonal path approximation with angle constraints," by Danny Chen et al. (SODA 2001) might be relevant. From the Abstract: 

I am working on an algorithm for which the running time is a random variable $X$ that has finite expected value, but infinite variance. Are there examples of other algorithms for which this is the case? Are any such algorithm used in practice, or does the infinite variance make the running time too unpredictable? 

In stochastic simulation, we are often interested in estimating the expected value of a random variable. The expected value of a continuous random variable is an integral over the real numbers. To estimate this quantity, we use the Monte Carlo method which consists of generating instances of this random variable from pseudorandom uniform variables. From these uniform variables, we can generate random variables from a given distribution by inverting the cumulative distribution function which is defined itself as an integral. 

Lov Grover published an article in 1997 in which he shows that if you can query the database on multiple items, then a single query suffices to find the marked element. However, it requires a number of preprocessing and postprocessing steps in $\Omega(N\log N)$. If you let $S_1, \dots, S_N$ denote the elements of the database, you query the oracle with the string $S_{i_1}, \dots, S_{i_\eta}$ for some number $\eta$ and the oracle returns $1$ if the marked state appears an odd number of times in the string and $0$ if it appears an even number of times. You query this oracle on a superposition $(|S_1\rangle+ \dots+ |S_N\rangle)^\eta$ and then apply the inversion about the mean operator from Grover's algorithm. Now in each the the $\eta$ subsystems, the marked element has a greater amplitude than the unmarked ones. Measuring all subsystems yield the marked state with greater probability and to have sufficient certitude about the resulting state, $\eta$ must be in $\Omega(N\log N)$. 

Computing the Cheeger constant of a graph, also known as the isoperimetric constant (because it is essentially a minimum area/volume ratio), is known to be NP-complete. Generally it is approximated. I am interested to learn if exact polynomial algorithms are known for special classes of graphs. For example, is it still NP-complete for regular graphs? For distance-regular graphs? (I have not studied the existing NP-completeness proofs to examine their assumptions.) Literature pointers appreciated—thanks! 

I wrote a paper long ago that detailed an linear-time algorithm for finding the smallest area triangle enclosing a point set (or a polygon): 

This does not answer your (interesting) question, but it may be worth mentioning that this variant (which does not use geodesic distance) has been studied: Thomas Shermer, "Hiding people in polygons," Computing, Volume 42, Numbers 2-3 (1989), 109-131 (Springer link): 

After hearing Emo Welzl speak on the subject this summer, I know the number of of triangulations of a set of $n$ points in the plane is somewhere between about $\Omega(8.48^n)$ and $O(30^n)$. Apologies if I am out-of-date; updates welcomed. I mentioned this in class, and wanted to follow up with brief, sage remarks to give students a sense for (a) why it has proved so difficult to nail down this quantity, and (b) why so many care to nail it down. I found I did not have adequate answers to illuminate either issue; so much for my sageness! I'd appreciate your take on these admittedly vague questions. Thanks! 

Is the graph coloring problem complete for poly-APX under C-reductions (alternatively, under AP-reductions)? For the graph coloring problem, speaking of a feasible solution means a proper coloring for all vertices of the given graph. The complexity class poly-APX contains all NP optimization problems that can be approximated within a factor that is polynomial in the size of the input. The notion of C-reducibility concerns approximation preserving reductions, which keep the performance ratio of the feasible solutions under consideration within a linear factor. For definitions regarding approximation preserving reducibilities, see P. Crescenzi: A short guide to approximation preserving reducibilites, CCC '97. EDIT (1.8.2014): Somewhat related, I've found in the paper "on syntactic versus computational views of approximability" by Khanna, Motwani, Sudan and Vazirani (SICOMP 28(1):164-191) a remark stating that GRAPH COLORING and MAX CLIQUE are both in poly-APX-PB and interreducible (Remark 6 in that paper). I understand this is meant with respect to E-reducibility defined in that paper. Later, in the sketch of proof of Theorem 6 in that paper, I understand that they imply that MAX CLIQUE is complete for poly-APX-PB under E-reductions. I would also be grateful for a proof that GRAPH COLORING is complete for poly-APX-PB w.r.t. E-reducibility. 

Back in 2005, Scott Aaronson posted a list of 10 "semi-grand" challenges for quantum computing theory which contained the following challenge: 

The decisional version of your problem can be written in the folowing form: $$EQ(s_1,t_1)\lor EQ(s_2,t_2)\lor \dots \lor EQ(s_n,t_n).$$ This is very similar to the inner product $$x_1y_1\lor x_2y_2\lor\dots\lor x_ny_n.$$ The communication complexity of those two problems is equivalent, since equality can be done at constant randomized communication cost. Also, finding an $i$ such that $s_i=t_i$ is at least as hard as the decisional version of the problem. Therefore, your problem is as hard as solving $IP$ when only one clause is satisfied. I have no direct proof, but my intuition lets me believe that $IP$ with only 1 satisfied clause is as hard as general $IP$. So the randomized complexity of your problem would be in $\Theta(n)$ if I am not mistaken. I hope this helps. 

Not sure if this is directly linked to your question, but reading it made me think about an article by Peter Høyer I read some years ago. In it, he shows how the most popular quantum algorithms like Grover's or Shor's follow the same pattern of applying what he calls "conjugated operators" and he builds new algorithms also based on that same pattern. As I said, it's been a few years since I've read it so my description is a bit sloppy, but here's the link in case you want to check it out. $URL$