So we have 5 times c0 which means n0 = 5, 2 times c3 means n3 = 2 and n2 = 1. Finally bi is the number of bits equals to 1 in c1 which b0 = 0, b1 = 1, b2 = 1, b3 = 2. This completes all the values you need for computing the lambda collocation scoring metric. Also take into account you could add smoothing to ni to avoid 0 counts. Now with respect to 

Before trying using Hadoop or Spark, I recommend you to follow some optimization tricks (tips): [1] A Beginnerâ€™s Guide to Optimizing Pandas Code for Speed [2] Using pandas with large data This link (Don't use Hadoop - your data isn't that big) can be useful too. Also for a comparison of SAS and pandas you can read the pandas documentation on such comparison or this. 

The resulting feature is determined by the pooling function in the case above it computes the arithmetic mean. See this for a more in depth usage example. 

If you want to reduce the dimension of your dataset to 2 dimensions, the algorithm clusters together feature1 and feature2, and leaves feature3 unchanged, the new matrix is: 

There are several tools for this you could check the Stanford Simple Annotation Tool, also Brat has online demos. For a comprehensive list you could check this question on Quora 

The main consideration is the number of classes, assume you have $N$ different classes: "one vs all" will train one classifier per class, so $N$ classifiers in total. For a given class $c_i$ the classifier assumes samples with $c_i$ as positives and the rest as negatives. Obviously, this leads to imbalanced datasets, moreover, in "one vs all" the outputs must be calibrated to produce comparable results. In "one vs one" you train a separate classifier for each different pair of classes, so $\frac{N(N - 1)}{2}$, obviously this is computationally expensive. For more details see this: 1 and 2. 

In your description the agents are drivers and the tasks are "driving to the pickup location", the cost can be either the eta or the distance of the driver's location to the pickup location. There are a number of algorithms for solving this problem, one of the most famous is the Hungarian algorithm also known as Kuhn-Munkres algorithm. There is an implementation of the algorithm in the scipy library (this is in Python), under the name of linear_sum_assignment, the documentation is here. 

If I understood correctly your question, you want a function that takes a signal (a fixed window) and outputs a 32-bit representation in a way that the correlation between it and any other signal is preserved. Mathematically speaking given signals $s_1$ and $s_2$ in $S$ and a correlation function $corr(s_1, s_2)$ you want some function $f : S \rightarrow B$ (where $B$ is the space of 32-bit binary numbers) that you could use another correlation function for instance $corr_f(b_1, b_2) \approx corr(s_1, s_2)$. If that is what you want you should at hashing techniques in particular learning to hash. Essentially in hashing want you do is you represent your input by binary numbers in a way that the hamming distance between the binary numbers preserves some target similarity (distance) function such as the correlation. In particular for cross-correlation (inner product) you have some methods based on random projections. So once you have learned (designed) your hashing function $f$, what I would do is: 

I guess the measurements from accelerometer-gyroscope-magnetometer are noisy and redundant in some sense. This means you can find some sort of correlation among the values of the measurements, for instance a correlation between the values from the accelerometer and the gyroscope. PCA captures the principal directions of variation on your data, removing the correlation between the measurements and also reducing the noise, therefore increasing the accuracy. From the graph it can be seen that the accuracy just diminishes a little when using all the features. Other factor I will consider is the magnitude of the features, a feature with a very large magnitude affects the behavior of K-NN. 

My question is: I still need to use the training data (with both features and labels) to make predictions on the test data (with features only). In this case, how should I deal with this feature variable? I was thinking about remove the top 1% data in both training and test data set, but as I still need to make predictions on that 1% test data, so this is not a good idea I guess. In this example, as the empirical distribution of this variable in both training and test data sets look the same before and after removing the "outlier", should we just leave this variable unchanged? Also, in general, how should we handle the outlier before we put the feature into the machine learning algorithm? 

Assume that we have a training data set (with both features and labels) and a test data set (with only features). When we build a machine learning model that requires normalization of the features, the correct way of doing normalization (to prevent information leakage) is to only use the training data set. Namely, a wrong way of doing normalization is to stack the training (exclude the Y column) and test data set together, and perform normalization (i.e., using the mean and variance of the entire training + test data set). The intuition here is very clear: if you want to get an unbiased estimator of your model performance in production, then when you train your model, you shouldn't instill any information in the test data set that will be used to gauge the actual performance of your model. My question is the following: When we have trained the model correctly and about to use this model to do prediction, how should we normalize the test data set? I believe the correct way to normalize the test data set is: using the mean and variance obtained from the training data set to do normalization on the test data set. However, why not just normalize the test data using its own mean and variance? Or why not stack train and test data set together and using the overall mean and variance to normalize the test data set? In the prediction stage, the idea and intuition of data snooping and information leakage is not clear to me. 

Assume $D$ is the training data set with both the value of the predictors $\mathbf{X}$ and the value of the response variable $Y$. I have a loss function $L$ and two models $f(\mathbf{X};\beta)$ and $g(\mathbf{X};\lambda)$, where $\beta$ and $\lambda$ are model parameters. Our goal is to estimate \begin{equation} e(f)=\mathbb{E}[L(Y,f(\mathbf{X};\beta))|D]~\mbox{and } e(g)=\mathbb{E}[L(Y,g(\mathbf{X};\lambda))|D] \end{equation} Note that it is the expectation of the generalization error of the model $f$ (and $g$) that is trained on the specific training data set $D$, where the expectation is taken based on the same distribution that generates $D$. Now, if we do a leave-one-out procedure, specifically: let $N$ be the total number of observations in $D$, let $D_{-j}$ be the data set that removes the $j^{th}$ observation. Then,$L(Y_j,f(\mathbf{X};\beta))|D_{-j}$ should be an "almost" unbiased estimator of $e(f)$ right? Theoretically, to get $e(f)$, once should generate infinite new $(Y_i,\mathbf{X}^{(i)})$ from the distribution of $D$, then train the model on $D$ and use that model to make prediction on the new infinite data set and take the average. Now we fit the model on $D_{-j}$, which is only slightly different from $D$. So $L(Y_j,f(\mathbf{X};\beta))|D_{-j}$ should be an "almost" unbiased estimator of $e(f)$. Then you go through all $N$ data points in $D$ to obtain the value of $L(Y_j,f(\mathbf{X};\beta))|D_{-j}$, where $j=1,2,...,N$ (assume $N$ is large), then you take the average, then the result should be very close to $e(f)$ right? Then we do the same thing to model $g$. Then in this case, we get very good estimates of $e(f)$ and $e(g)$ so we can do model selection based on $e(f)$ and $e(g)$. Specifically, if $e(f)<e(g)$, then I should expect that on a LARGE new independent data set $T$, $f$ should perform better than $g$ correct? Also, the quantity $e(f)$ and $e(g)$ computed from $T$ should be very close to the quantity computed from $D$, assuming both $D$ and $T$ are large. Is that correct? If all the above are correct, then it seems that I did model selection and model assessment in one step. But should I partition the data set into 3 pieces, i.e., first train 2 models on one piece, then apply the 2 models on another piece to do model selection, then apply the 2 models on the 3rd piece to do model assessment? 

You should look at the Jaccard Index, is the de facto similarity between set of items, where the sets are represented using a boolean vector. In this boolean vector each coordinate represents an item, 1 means the item is present, 0 otherwise. For example: for an universe of items banana, orange and apple. the set banana, orange will be represented by (1, 1, 0). The Jaccard Index is the intersection of the sets over union the sets so for a set (1, 1) it's value is 1. Cosine similarity is for real-valued vectors, but whether cosine similarity is better than the Jaccard index and vice versa depends on the application. You should do a test on your data and verify which is better, for a discussion see this question 

I suggest you take a look at the Python library Spacy it has a model for spanish language that includes NER. 

In general the quality of the quantization is measured using the mean squared error (MSE) between the input vector $x$ and the reproduction value $q(x)$ ($w_k$ in the notation you used in the question). For the MSE the best partition is the one defined by Voronoi regions under the euclidean distance ($l^2$ norm), i.e: $$V_j = \{ x \in R^d : \|x - w_j \|^2 \leq \|x - w_i \|^2\ for\ all\ i \}$$ You can find the proof in here. Also here and here the euclidean distance is used. 

Assuming you have a pair of csv files: "replace.csv" representing the first table and "table.csv" representing the second table i.e. "dict". Use the following code: 

Last, for the trim of the car I would use a function that returns 0 if the categories were different 1 otherwise. 

I will try to illustrate the lambda collocation metric, first we have to define a function c(x, z) that receives two K-word expressions where x is the K-word expression you want to score and z corresponds to all possible values of a K-word expression in the corpus. c(x, z) returns a binary vector (j1, ..., jk) such that ji = 1 if xi == zi, 0 otherwise. Now a binary vector can represent a decimal number up to 2^K = M, so c(x, z) = ci means that the binary vector returned was equal to the binary representation of the decimal number i. Now as it says on the documentation ni corresponds to the number of times the c(x, z) function was equal to ci through the corpus. Now an example, consider the following sentence: 

The definition of a contingency table for the array data is simply the number of times a given value appears (i.e. its frequency). See this and the usage of the table command for more information. For the example above we have the following array data: 

Now the similarity using the same weights is S(a, b) = 0.9. Alternatively you could use a similarity matrix for the trim of the car, this is base could be more similar to mid thant to top. For example: 

Python is not a matrix-based language, as far as I know the language does not offer by default any capabilities for handling matrices. I think your referring to the numpy/scipy stack, this is a separated library. 

The values of the matrix have to be adjusted to fit your needs. For a more in depth comparison between similarity functions between categorical attributes (trim) see this for a comparison of different similarity measures for continuous attributes (year, km) see this. Again to see which similarity is the best will depend on the data and on your perception, I would select a random set of pairs of cars and manually assign a similarity. Then split this set into a train set and test set. Use the train set to adjust the parameters of the different possible similarities, in the case of my proposal Wkm, Wyear and Wtrim. Then once your satisfied evaluate on the test set.