From the second graph it seems pretty easy to identify the outlier. You could probably just fit a simple polynomial (or some other function) and then flag all points that have a distance greater than 2 standard deviations (or whatever seems appropriate) from the fitted curve. 

Then, for the circular contours, the optimal solution will be where the line $w^*_2 = C- w^*_1$ (for the boundary region) and the line through the red cross at $(r_1,r_2)$ running orthogonal to the boundary region intersect. You can see that the latter has the form $w_2 = f(w_1) = 1\cdot w_1 + r_2 - r_1$. (Orthogonality gives you slope +1 because the boundary region has slope -1.) Setting them equal gives you the intersection at coordinates $w^*_1 = \frac{C+r_1-r_2}{2}$ and $w^*_2 = \frac{C-r_1+r_2}{2}$. Now, if $r_2=r_1$ you have $w^*_1=w^*_2$ independent of $C$ and the solution won't be sparse. If $r_1>r_2$ (i.e. you are below the first diagonal) shrinking $C$ to $r_1-r_2>0$ yields $w^*_2=0$ but $w^*_1>0$. You end up in the right corner of the regularization region. (Similar to my image.) If $r_2>r_1$ (i.e. you are above the first diagonal) shrinking $C$ to $r_2-r_1>0$ yields $w^*_1=0$ but $w^*_2>0$. You end up in the top corner of the regularization region. (Similar to your image.) As Emre mentioned in the comments, this holds in general and you can "see" this from the Kuhn-Tucker conditions of the optimization problem. 

I think the documentation, at least for your example, is not too obscure. It doesn't tell you what kind of interpolation it uses because it doesn't use any: is a deferred operation. In order for it to work you have to call it in conjunction with a function that performs the interpolation, e.g. or (as can be seen from the examples in the documentation). Sometimes, it can be helpful to also look into the release notes for additional information. Scipy and Numpy are sometimes not very detailed, but they contain at least a decent number of references with more information. (There is also a section on Matlab versus Numpy here. If you are familiar with in R , then the pandas cheat sheet can help as a quick comparison between R and pandas for typical data wrangling operations.) I'm not aware of a "second more detailed documentation" that encompasses the entire libraries. You probably have to fall back to books (which, of course, aren't complete or always up to date). 

Keep in mind that while Naïve Bayes is a decent classifier for many applications, the generated probabilities are usually not very representative. 

I am looking for a machine-learning (classification) model that can adapt to new data. By that, I don’t mean completely new samples in the online-learning sense, but rather additional details about an existing sample. Over time, for an existing sample, I learn an additional feature value that was missing before. (Once it is learned it doesn't change.) When I run an updated sample through the model, I would like to get a "more accurate" prediction. The time and order at which new data becomes available are not consistent and features can still be missing at the end. In my training data, I only know the data that is available at the end and not when which feature value became available. Most of the features are categorical and I am wondering if it would be enough to train a model by creating many synthetic samples where I randomly replace some of the categories with a “missing category” value. It seems to me though that this approach would not make sure that additional data would lead to a more confident prediction. The model would just see it as a new and different sample. How can I make sure that an update leads to a better prediction? Any hints are appreciated. 

I bet this would be good enough. But, if there are a lot of false negatives, you could ask the dictionary for the most similar words and then apply a word distance measure. Here I will use Levenshtein distance as implemented in python: 

Use . This will truncate words that appear in more than that percentage number of documents. The TF-IDF part that punishes common words (transversal to all documents) is the IDF part of TF-IDF, which means inverse document transform. Several functions may be used as your IDF function. Usually, IDF=$\log\frac{\text{#documents}}{\text{#documents where word appears}}$ is used. You could try a more punitive IDF function. sklearn does not seem to allow to specify it, but you can use or or easily implement your own TF-IDF vectorization. It needs no more than five lines of code. 

I think this last suggestion of mine of using Levenshtein distance to the closest English words is overkill. If your documents involve a lot of urban words or hip terms, you could also filter based on the number of Google results, instead of using a common dictionary. Your word hdhhdhhhhhjjj, for instance, gives me only two Google results, one of which is this stackexchange. Data science is about being creative. :) There probably are heuristics or statistics based on the number of consonants in a word, and letter combinations, which you could use to probabilistic shrink your documents' dictionary, but I wouldn't go there. It will be too much work and very brittle. 

Machine learning can be divided into several areas: supervised learning, unsupervised learning, semi-supervised learning, learning to rank, recommendation systems, etc, etc. One such area is PU Learning, where only Positive and Unlabeled instances are available. There are many publications about this, usually involving a lot of mathematics... When looking at the literature, I was expecting to see methods similar to self-training (from semi-supervised learning), where labels are adjusted gradually according to the classifier margins. I don't think these is what practitioners from the area do, and I was unable to navigate the mathematics or to find a survey on PU learning. Could someone from the area perhaps clarify what said practitioners do? Why can they not just use a binary classifier where the negative class=unlabeled? Can negative labels exist among the unlabeled data? What is the goal and what metrics exist to evaluate said goal? 

usually performs better after one-hot encoding. Otherwise, it will treat your categorical variables as numerical variables. But most other tree packages support categorical variables; in other words, they support rules such as: Again, , unfortunately, does not. Therefore, you must convert your categorical variables to binary variables so that it can do: 

Since I asked this question, it seems the documentation was expanded: We need to provide a ".group" file. For example, the file could be 

This is not necessarily an answer to your question. Just general thoughts about cross-validating the number of decision trees within a random forest. I see a lot of people in kaggle and stackexchange cross-validating the number of trees in a random forest. I have also asked a couple of colleagues and they tell me it is important to cross-validate them to avoid overfitting. This never made sense to me. Since each decision tree is trained independently, adding more decision trees should just make your ensemble more and more robust. (This is different from gradient boosting trees, which are a particular case of ada boosting, and therefore there is potential for overfitting since each decision tree is trained to weight residuals more heavily.) I did a simple experiment: 

The embedding layer maps your vocabulary index input to a dense vector, so it acts as lookup layer and (if set to trainable) will be influenced on some weights only, by the words occurring in a batch of training data. Having a linear layer, it would be sequentially trained by all the data batches and would not provide a lookup functionality (each word given to the input would share the same weights). Also, you're right considering word2vec differently. When using a custom trainable embedding layer, the dense vectors will be optimized (by SGD) for the task you are considering whereas models like word2vec act like language modeling and find a semantic optimum representation in the embeddings. Therefore, depending on data size, the representation found during training might be better for your task than the one found by a neutral word2vec or other model. 

First of all, you should use to get you predictions vector, so that you followed approximately the same validation scheme to get them : 

Reducing from 1048576 to 100 features makes more than a 99.99% reduction of the input features dimension, knowing that SVD does not focuses on finding interesting features for classification, but creates an uncorrelated vector space from the initial one. You should try to increase the dimension resulting from the SVD (have you looked at the retained variance for 100 components ?) but more specifically I suggest you to use a feature selection pipeline using a metric such as chi2 score, which gives better results for a classification goal (it may however be longer to compute). 

It will search for ipynb files. Otherwise try to search for ipynb files from the root folder of your OS. 

There are plenty of different approaches you can use, and none is the universal best solution. However, in general, preprocessing in twitter data, especially for Doc2Vec, follow: 

You add a bicycle class or other classes that can be problematic in your opinion due to close features with your base classes, and train again. Complicated to be exhaustive with all situations. You define a threshold of confidence of your model: for example if there is no probability more than 95% then the model is not confident in its choice and you should not trust him. Good and larged models should stay at equiprobable choices at the softmax mater when the features are not recognized well. 

In effect it is ordinal regression/classification. I suggest you to go for Mean Absolute Error to take into account the missclassification that play a bigger role when far from the ground truth : $MAE = \frac{\sum_{i=1}^n|h(x_i) - y_i|}{n}$, creating your own scoring function in python. Given the number of different outputs you have, I recommend you to go for regression. Anyway, rounding any hypothesis output would only play on the score, it's up to you to decide if it makes more sense to see wine quality scores as classes or not. 

This website explains it very well. While trees are trained sequentially, each individual decision tree is trained in parallel by using highly creative techniques. Most importantly, nodes in the same depth do not depend in each other, so you can paralelize them. Long story short: the loss function is applied between training models. When it comes to making predictions, then the entire ensemble can be used in parallel. 

What data mining package do you use? In sklearn, the DecisionTreeClassifier can give you probabilities, but you have to use things like in order to truncate the tree. The probabilities that it returns is $P=n_A/(n_A+n_B)$, that is, the number of observations of class A that have been "captured" by that leaf over the entire number of observations captured by that leaf (during training). But again, you must prune or truncate your decision tree, because otherwise the decision tree grows until $n=1$ in each leaf and so $P=1$. That being said, I think you want to use something like a random forest. In a random forest, multiple decision trees are trained, by using different resamples of your data. In the end, probabilities can be calculated by the proportion of decision trees which vote for each class. This I think is a much more robust approach to estimate probabilities than using individual decision trees. But random forests are not interpretable, so if interpertability is a requirement, use the decision tree like I mentioned. You can use grid search to maximize the ROC AUC score by changing hyperparameters such as maximum depth to find whatever decision tree gives the most reliable probabilities. EDIT: In case I wasn't clear enough, I think it's awful to use a single decision tree to predict probabilities. I have extended my answer in a blog post. 

One last piece of advise: if you need this forecasting to interoperate with other python code, my recommendation is to do it via CSV files. Just have your python code do the pre-processing and generate a CSV file, then R doing its thing and generating another CSV file, then use this file for the post-processing in python, etc. Some people try to use things like rpy2, but I find it to be overengineering. 

I know that neither xgboost, nor sklearn offer what you want. I have checked R, and didn't find it either. But random forests are easy models to implement, so you can just produce one by yourself: Here, in python using sklearn: 

Yours is not an example of nested cross-validation. Nested cross-validation is useful to figure out whether, say, a random forest or a SVM is better suited for your problem. Nested CV only outputs a score, it does not output a model like in your code. This would be an example of nested cross validation: 

The opposite of what you describe is used more often. You start with many features and then prune the neural network at the end. I don't believe any python neural network package does what you want. But it's very simple to implement. For example, using sklearn, you can just pass , so that it doesn't reinitialize weights every time you call , and then you can increase the number of nodes in your feature layer, or any layer you wish.