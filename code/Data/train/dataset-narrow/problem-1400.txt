It looks like valid implementation, but it could be simplified. Also there are some naming issues: in method is actually a and not a string. carries a single char but is declared as . You have a lot of trie management code inside class while it would be more readable to have the logic in class: each node would be responsible for making decisions on its own level, delegating further decisions to its children. Also there is a common trie optimization that groups several chars in one node, e.g. if you have only and words you would have 3 nodes: din with children (din)e and (din)ner 

I've seen many attempts to abstract particular ORM from application, and all of them at a certain stage of maturity had to break this abstraction. One of the reasons for that is when you need to optimize certain use cases (like eager-loading related entities, or combining several round trips to server into one) you'll need to use ORM specifics that you're abstracting from. And another (more obvious) reason why I vote against abstracting ORMs from the code is that ORM is already an abstraction, so what you do is an abstraction on top of another abstraction that brings little benefits. There is a very good series of articles that describes best practices for managing NHibernate in ASP.NET: 

In addition, starting with SQL 2008 stored procedures support Table-Valued Parameters, which let you pass a multi-column, multi-row recordset as an argument to the stored proc. This can be useful in situations where (because of its limitations) BulkCopy is not a feasible solution. In particular, because it's "one table at a time". 

Which brings me to the one thing I would change about your query: locking. Unless you really need per-transaction integrity when serving articles to the readers (and in most cases, for a publishing system, you probably don't) you can hint to the database that you'd like a more permissive transaction isolation level. In layman's terms, you can volunteer to let the other guy win if you both need a lock. So if you are doing a SELECT at the same time an UPDATE is occurring to the same article, you can allow a READ UNCOMMITTED transaction isolation level on your query to avoid taking unnecessary locks. The syntax for doing this varies from one DBMS to another, but usually it's some variation on the theme of SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED. Some (MS SQL, for example) also have a shortened syntax such as WITH (NOLOCK). Finally, a couple of caveats to this advice since you didn't specify your DB system. Some DB systems don't fit this mold. For example, Oracle implements non-blocking SELECTs without requiring dirty reads, so there's no equivalent to NOLOCK there because it's a built-in assumption. MySQL, on the other hand, has InnoDB and MyISAM tables, and on the latter type you are not going to be able to use READ UNCOMMITTED due to the underlying architecture of the table. However, even though you will be taking a table lock the architecture is designed to efficiently allow many concurrent reads by sharing the lock to multiple readers. To sum up, your queries look fine but explore whether a NOLOCK-type option is available. 

What you have implemented is a sort of Active Record where the record itself knows how to communicate with the storage. What is bad about your design is that this kind of code will be extremely hard to unit test. Imagine that you need to write a unit test for a class that uses objects. How can you prevent it from calling ? The proper solution for your problem depends on use cases. 

It's hard to suggest good solution based on the information you've provided... Will the number of entities change over time? Can they "loose" SpriteComponent? Assuming that the number of entities is the same and only Z will slightly change you can cache the sorted list from previous run, and apply TimSort or Insertion sort (you would need to implement it yourselves or grab from Wikipedia) to nearly-sorted list. 

I guess this plugin could be useful, however I think you can drop the async part since it's not any harder to call directly. Also, I am not a huge fan of error swallowing or ignoring invalid calls since it makes the code harder to debug. If there's a call to without providing a callback, you should let the developer be notified. I also like to allow defining the value for the callback function so that you do not need to use for that purpose. Finally I allowed to return a value from the callback to change the target object for the rest of the chain. However I am not so sure about this feature since it could harm code comprehension but I am leaving it there as an idea. Basically it would be as simple as: 

EDIT 3: Actually I've created a performance test that compares different solutions and it seems that using a map isin't the fastest way and it's actually slower than performing a double replace. I must say I am quite surprised. Anyway, here's the fastest implementation I could write: PERFORMANCE TESTS (includes @200_success solutions) 

Nice thing in participating/answering in forums like this is that you learn while you answer questions. I haven't heard about SpecsFor framework. Looks a bit tricky, but will definitely have a look later. Ok, back to your question :) About your first question, setting up the mock - you can definitely do that, there are a number of overloaded methods accepting delegate/lambda, depending on the number of parameters in the method being setup (here I setup the method to always return the same query regardless of the query passed: 

It's better to use new asynchronous API since it provides more features for combining asynchronous tasks. Ideally , and should expose asynchronous API if a certain operation is not trivial, so (as you mentioned in comments you wrote them) I would implement asynchronous API for them first, e.g.: 

As you see users of this interface don't know about those request-reply wrappers that you have in the underlying communication protocol, they are dealing with business entities. And implementation of this interface may look like this (just as example): 

Best practice is to do away with cursors. Both of them. There is always a better-performing solution. If possible, reduce the problem to one that can be solved with sets instead of iteration. But, if this nested iteration cannot be avoided, you will still get far better performance using table variables and WHILE loops instead of a cursor. For example: 

The performance with simple queries such as yours will depend more on what is going on in the database than what is going on in your code. Here are a few things to consider if you are maintaining the DB yourself, or to bring up with the DBA if you are not: 

If the field is an integer, and ideally a surrogate key (e.g. generated by an identity function instead of being a natural key using date/time, articlename, etc.), that will perform significantly better. Integers can be easily and swiftly ordered and generate relatively small indexes that can be searched quickly. A surrogate key improves performance for many reasons, one of which improved index performance by pushing data INSERTs to the leading edge of the index. If the article table has a clustered index on the id field, that's even better for your performance as it will reduce the cost of both the SELECT and the UPDATE in most cases. The clustered attribute means that the data is already ordered by your ID column, significantly improving search performance. If you don't have a lot of competing queries that try to lock bigger sets of data than a row at a time, that will also be good for the performance of these queries. You want to avoid contention for locking the same data as much as possible. For example, you don't want a big 100,000-row update that runs every 15 minutes competing with your single-row update.