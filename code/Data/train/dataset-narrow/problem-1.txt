It is worth separating your Continuous Integration "Build" from your Continuous Deployment "Pipeline". For the build portion, CircleCI seems to be your tool of choice, for the latter I recommend using something like AWS CodeDeploy which natively supports Blue-Green Deployments, for completeness you can use CircleCI to orchestrate AWS CodeDeploy: 

I can think of two architectures that would support the answering of these questions, however, the enormity of the problem could well be clouding my judgement: Approach #1: Walled Garden Effectively firewall off the sources of these open source packages, i.e. npm, Docker Registry, nuget, etc., then create an internal repository of approved packages, implementing some process to whitelist packages. 

As far as I am aware there are no statistical data about the enterprise adoption of each of these frameworks and whether they have adopted DevOps practices in addition to Agile practices, however from experience many organisations appear to gravitate towards Scaled Agile Framework. Kristof Horvath discusses the application of DevOps in these enterprise-grade agile frameworks in his article: Scaling Agile in Large Enterprises: LeSS, DAD or SAFeÂ®?. It is worth noting that adopting an "enterprise-grade" framework is not a requirement in an enterprise, at the end of the day Agile, DevOps and Lean are about adopting the right practices for your organisation, not the most popular ones. 

This makes it easier to share code across projects, document what has been done so far and handover the code to other or future developers. 

Important: Chat and ChatOps is very specific to an organisation, think of Chat as the Fabric for your team's bots to existing within it's the bots that provide the functionality that enables business and technical processes. With the brief lesson in ChatOps Theory over I can talk about the experiences that I have had with ChatOps: DevOps Support Slack and PagerDuty has fantastic integration allowing any newly raised incidents in PagerDuty then posted in one or more Slack Channels with information about the incident and buttons for acting upon the knowledge: 

Frequency of deployments is one minor update on most business days, one major update on a weekly basis. Source: How Facebook Does Deployment 

I've got ephemeral storage setup in Scalr as part of the role. The instance is spinned up by Chef and it seems it is running fine (it's using device mounted as drive). I've tried to use command, but here is no any indication that the ephemeral device is attached to the instance (unless I don't know what I'm looking for). How I can access ephemeral storages of the running instances using via AWS CLI/API? 

Note: I've changed it slightly for privacy reasons. How can I decrypt its original password based on the string above? 

According to New EC2 Run Command news article, AWS CLI should support a new sub-command to execute scripts on remote EC2 instances. However I've checked in , but I can't find the relevant command. I've installed via : 

I've freestyle jobs which aims to run EC2 instances via Vagrant (), however when I cancel the job manually (by clicking X button), the instances on EC2 are still running. How can I make sure that the instances are terminated when the job is cancelled? 

As for workaround, the actual memory can be checked by invoking Groovy commands directly in Script Console (at ). Example command: 

JetBrains TeamCity docker image describe build steps for already installed TeamCity. Is there any way to automate installation of TeamCity it-self? For example creating the admin account, enabling standard authentication and setting up the project? 

Plus adding rule with Log text as "Build was aborted" make sure that above command is only invoked on aborted builds. For example: 

I've taken over the project where a lot of Jenkins credentials has passwords or passphrase strings which I need to know in order to progress with the project, unfortunately these weren't documented anywhere. I've checked the file where these credentials are stored, but they're in not plain text, e.g.: 

ChatOps is an absorbing topic; practically it means something very different to each team using it. So much so that Atlassian have put together what could be considered to be a Chat maturity model. 

Part of adopting the Immutable Infrastructure Pattern is decomposing your system into small manageable pieces that can move through CI/CD Pipeline very quickly, this means that OS patches can be done quickly and in a controlled manner. I often see clients ending up with a halfway house where infrastructure is mostly immutable. However, there are a few approaches to this which I have used in large-scale deployments of cloud architecture, typically I implement more than one as part of a Defense in Depth strategy: 

Since Google's Site Reliability Engineering Book was released, on more than one occasion I have been told that SRE is an extension of the existing Operations or Application Support model. We've had a couple of questions that defined differences between Sys. Admins, DevOps Engineers and Site Reliability Engineers: 

If for example the services as a whole were scaled to support 80,000 requests per seconds and run at about 80% of capacity, a spike in traffic that caused the service to receive 101,000 requests per second would cause 1,000 of those requests to fail. When the retry policies kick in, you end up with additional 1,000+ requests, depending on where the failure was detected which would push the service as a whole up to 102,000 requests per second - from there your service goes into a death spiral doubling the number of failed requests every second. Other than massive over-provisioning of services beyond the projected peak transaction, which would be inefficient. What strategies can you employ to avoid "retry storms"? 

If you were to apply IT Service Management (ITSM) or ITIL language to the same situation you would likely call it an IT Service Continuity Plan or Recovery Plan: 

But it disappears after Ansible playbook command is finished. Workaround The following one-time workaround works to recreate the folder: 

Which sub-command I should look for and what's the syntax to run, let say in PowerShell on the remote EC2 instance? 

The installation of Windows application can be automated using AutoHotkey (AHK), similar as Winetricks does this for all common apps which support. Here is example of AHK script which aims to install mentioned application: 

Problem It seems after installing and running during provisioning, the container have the problem running command as below: 

To summarize, the main point in order to avoid spending too much time on merging complex branches, introduce Pull Request, so each requester is responsible for mergeability of his own code and other developers are responsible for reviewing the change, so no any unpredictable branches or conflicts are possible in this case. So yes, Agile methodologies can scale for massive scale projects. 

which stops the job at the very start. I've no idea why EnvInject plugin would want to remove some Java files (such as ). Any idea how to solve this problem? 

however, the above sub-command doesn't support parameter. I've checked and there are and parameters, but I'm not sure about the syntax. What would be the easiest way to display the description of the security group of the instance? 

In Jenkins when clicking on Build Executor Status I can only see free disk space related statistics (URI: ). How I can monitor free system memory (RAM) in Jenkins? I'm asking, because sometimes when I had too many executors (despite having swap space configured, but not the one below), Jenkins was freezing or crashing a lot. 

Although I cannot see in which bucket it has been imported. In addition, there is a command which imports a disk from the S3 bucket. So basically I'm looking for some way of exporting EC2 snapshot into S3 bucket where I can see the image file in order to download it. How I can do it? 

Notes Master is protected as it represents the current state of production, to do this practically you may have another "Release" branch that deployments are made from and only when successful merge into the Master branch. Key Points The Blue Development branch is basically a free-for-all. Hotfix is kind of a free-for-all but any deployments trigger a type of Break Glass by notifying a non-development function who will perform the post-approval and in the process will merge the change into Master. It's essential merges into Master stop while Hotfix is ahead of Master to: 

Practically an organisation could use either solution or if necessary both solution to provide a degree of checks and balances. Are their other architectures or tools that support the management of Free and Open Source dependencies? 

The layout of your repository depends in many ways upon the context you are developing the automation in. If, for example, you are building out the infrastructure for a product as part of a product team, then it would make sense to tie the infrastructure to the product - i.e. keep the infrastructure in the same repository as the software source code. If however, you are building out common infrastructure components to stand-up basic infrastructure for DNS, File Storage, Email, etc. then it probably makes more sense to have a single repository. Some of this will be tied to the tool you are using, Terraform for example leads you down the path of having a single repository for all of your environments to support the construct. Reusability You quite rightly called out reusability and code sharing as a problem you are going to encounter sooner rather than later. This is an important principle in Software Development called Don't Repeat Yourse - or DRY for short. Most all DevOps tools allow you to modularize your code in such a way that you don't need to copy/paste code: 

I'd like to find a route table id associated with the given EC2 instance. How can this be achieved using AWS CLI? 

I'm aware that Apache Hive provides SQL-like interface to query stored data. So I would like to ask, what are the main limitation of Hive-based SQL-like compared to other relational SQL query languages (such as MySQL)? 

I'd like to understand the differences between Puppet and Ansible, especially what kind of Puppet limitations has in comparison to Ansible. Are there any things you cannot do it in Puppet, but you can in Ansible? In other words, why some people moving away from Puppet to Ansible? 

The route table is associated with VPC, which is associated with the instance. Given shell variable has the instance ID, e.g. 

There is a utility from GitHub which extend your git with extra features and commands. This includes creating the releases on the GitHub. 

These big companies are using unconventional solutions, such as the BitTorrent deployment system and geographically distributed content delivery networks (CDNs). When Facebook updates its code or generates a new build, the binary files needs be pushed to all of the company's servers (~1.5GB), so they created its own custom BitTorrent tracker designed to obtain data slices from other servers that are on the same node/rack which reduces total latency. Source: Exclusive: a behind-the-scenes look at Facebook release engineering Facebook deployment takes the following steps: 

For GitHub the only workaround is to push tags (e.g. ). Or solution is to use different supported provider. 

This can be achieved by using Post build task plugin and by adding the following Script command in Post-build Action, Post build task solved the problem: 

where is the file to contain above AHK script which can be dynamically generated or downloaded from some repository. 

Here is a helper Bash script which uses with parameter to run the command and the result is stored in the S3 bucket, then displayed to the standard output. 

Potentially could create the above from scratch, or transition from a partially deployed state to the above desired state. I am aware that Terraform splits its work into the execution plan stage and the application phase which actually makes changes to the target architecture. Can this be used to write tests against the execution plan, if so are there frameworks to help write these? 

So having gone backwards and forward over this for a couple of weeks, Azure has confirmed to me in-person that the only way to utilise FIPS-140 Level 2 certified hardware security modules in Microsoft Azure is to use Azure Key Vault. 

Within a traditional separate Development and Operations organisations, procedures and processes are put in place to ensure that relationships between software vendors and suppliers are maintained. These relationships exist so that when for example a vulnerability is discovered operations teams are able to identify the business impact and mitigate appropriately. If that organisation was built around ITIL service management principals then they would likely be doing one or all of the following: 

To be absolutely clear, these are functions that used to belong to the operations organization and are now owned by the Agile/DevOps organization. There are existing KPIs that drive bad behaviors are: 

I am creating an AWS IAM policy for a Serverless Framework project, I am applying the Principle of Least Privilege to ensure the policy is as tight as possible. To restrict the scope of the policy I need to be able to identify the ARN for each of the resources, i.e.: 

Both component parts are within their SLA but the total system was unavailable for 2 hours out of 24. Serial and Parallel Availability 

There is a fair bit of configuration to plug in to get this setup including setting IAM policies and adding EC2 tags to the instances you want to be party to your cluster. If you were to use AWS Autoscaling Groups then you would add the following to your :