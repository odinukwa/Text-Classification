Important note: all photos are property of their respective owners. All images on Flickr have specific license conditions, which you can also query through the API. A list of the available licenses on Flickr is available on their website. You have to make sure you don't infringe the copyrights of the respective owners. Especially if your work is commercial, this complicates things. References [1]: Gordo, A., Almazan, J., Revaud, J., & Larlus, D. (2016). Deep Image Retrieval: Learning global representations for image search. arXiv: 1604.01325. 

which would create vector which contains either or , drawn randomly for each probability in . As you probably have noticed, nobody does that! E.g. describes it in his Practical Guide to Training RBMs, as 

There are many different publicly available datasets out there, and most come with a paper describing how the dataset was acquired. Almost nobody takes a camera and starts taking thousands of pictures themselves. You may find some inspiration by looking at those papers and adapting their methods for finding images. A very popular way is to download the images from Flickr: This is a photo platform where users share their photos and add comments or tags, describing the contents of the images. Flickr also has an API to find and download images. A couple of test queries show that there are thousands of photos available: 

If I understand correctly, the function computes the cosine similarity of the vector with all other vectors and finds the closest one. The vectors then viewed from a certain origin of (0,0,0,..0) for N dimensions (for N dimensional word vectors). Is there by any means that I could compute the angle of these vectors from the origin ?. If I compute the similarity between the vector and the origin vector and take the inverse cosine, will the resultant angle be the angle of the vector on origin space. If I get such an angle, on which plane does it lie as the vector is high dimensional. The reason is that, when we try to reduce it to 2 dimensions and plot them, only magnitude of the vector is considered right, so the direction information is missing. I would like to plot it in a polar plot to view both. Is there any such function in ?. 

In this way, The memory is loaded with only one sentence at a time and when it is done the memory loads the next one. For building the vocabulary, you can do the whole iterating via all the documents to build the vocab first and then train the data, depending upon the word-embedding functions implemented. 

The objective function they use to train the CNN minimizes the squared L2 distance (i.e. the squared Euclidean distance) between two similar (positive) images and simultaneously maximizes the distance between two different (negative) images. That means, the (squared) Euclidean distance between two representations is a measure of their similarity. Then, recognizing a face in a new image is as simple as 1) running it through the CNN and 2) finding its nearest neighbors with a KNN algorithm. The last paragraph was only about images - in the Youtube Faces DB, we are handling videos of different persons. In section 5.7 of the paper, they describe how they evaluate performance: 

If this is just a one-time case, you can simply re-train the neural network. If you frequently have to add new classes, then this is a bad idea. What you want to do in such cases is called content-based image retrieval (CBIR), or simply image retrieval or visual search. I will explain both cases in my answer below. One-time case If this just happens once - you forgot the 11th class, or your customer changed his/her mind - but it won't happen again, then then you can simply an 11th output node to the last layer. Initialize the weights to this node randomly, but use the weights you already have for the other outputs. Then, just train it as usual. It might be helpful to fix some weights, i.e. don't train these. An extreme case would be to only train the new weights, and leave all others fixed. But I am not sure whether this will work that well - might be worth a try. Content-based image retrieval Consider the following example: you are working for a CD store, who wants their customers to be able to take a picture of an album cover, and the application shows them the CD they scanned in their online store. In that case, you would have to re-train the network for every new CD they have in the store. That might be 5 new CDs each day, so re-training the network that way is not suitable. The solution is to train a network, which maps the image into a feature space. Each image will be represented by a descriptor, which is e.g. a 256-dimensional vector. You can "classify" an image by calculating this descriptor, and comparing it to your database of descriptors (i.e. the descriptors of all CDs you have in your store). The closest descriptor in the database wins. How do you train a neural network to learn such a descriptor vector? That is an active field of research. You can find recent work by searching for keywords like "image retrieval" or "metric learning". Right now, people usually take a pre-trained network, e.g. VGG-16, cut off the FC layers, and use the final convolutional as your descriptor vector. You can further train this network e.g. by using a siamese network with triplet loss. 

In a draft copy currently being written by Andrew Ng, he discusses about the amount of data in train-test dataset. My understanding from the book, The traditional and most common value is 70-30 or 75-25. If you have 10k or 30k samples, it is fine to go with 70-30 split. But when dealing with Big-data, for example if you have 1 million samples, it is not recommended to have 30k samples as test data, so in that case, 90-10 is actually okay. Because 10k test samples can pretty much provide an intuition about the model. in brief: for less samples, go with recommended 70-30 split, for much higher samples, go with number of samples Draft copy link : ML Yearning 

Just another point of view, Dig the topic names a bit deeper. - mining of text (just as data mining, and the data is text data). mining is about extracting useful information from the available data. information could be patterns in text or matching structure but the semantics in the text is not considered. The goal is not about making the system understand what does the text conveys, rather about providing information to the user based on a certain step by step process. - Natural language is what humans use for communication. processing such a data is NLP. The data could be speech or text. Thus, the main goal is towards understanding what is the semantic meaning conveyed in it. Now you know why we care about grammatical part of speeches and the lexical relations among them. speech recognition systems could be a part of NLP, but it has nothing to do with text mining. And, it seems like NLP is the bigger fish and it uses text-mining, but its actually the other way around. text-mining uses NLP, because it makes sense to mine the data when you understand the data semantically. 

You are making a mistake regarding what is given: during training, you don't have a radius $R$. You have the coordinates $\vec{x}$ and the label $y$ for each point: $$ \vec{x}_1 = [-2;1] \quad \vec{x}_2 = [1;2] \quad \vec{x}_3 = [0;-1] \quad \vec{x}_4 = [1;0] \quad \vec{x}_5 = [1;1] $$ and $$ y_1 = T \quad y_2=T \quad y_3=T \quad y_4=S \quad y_5=S $$ with that, your "trained" centroids are $$ \mu_T = \frac 13 [-2 + 1 + 0; 1 + 1 - 1] = \left[-\frac 13, \frac 13\right] $$ $$ \mu_S = \frac 12 [1 + 1; 0 + 1] =\left[1, \frac 12\right]$$ You calculate these centroids before you get any test values. Then, for testing, for your observed point $\vec{x} = [1,1]$, you calculate the Euclidean distance between the point $\vec{x}$ and the centroids $\mu_T$ and $\mu_S$: $$ \| \vec{x} - \mu_T \| = \left\| [1;1] - \left[-\frac 13; \frac 13\right] \right\| = \left\| \left[\frac 23; \frac 43\right] \right\| = 1.49 $$ $$ \| \vec{x} - \mu_S \| = \left\| [1;1] - \left[-1; \frac 12\right] \right\| = \left\| \left[0; \frac 12 \right] \right\| = 0.5$$ Finally, the term $\hat{y} = \arg\min_{l \in \mathbf{Y}} \|\vec{x}-\mu_l\|$ is used to find the estimated class $\hat{y}$ (the hat symbol is to denote that this is an estimated $y$, not one we knew before.). $\arg\min$, means that you find the minimum value - which is 0.5 in our case, and chose which "argument", i.e. which class, leads to that minimum value. In our case, the class which leads to the minimal distance is $S$, so the result is $\hat{y} = S$, and our test point is a square. 

Start with , it gives an idea which part of posting is requirements and what part is description and then you could try just with bag of words. As the topic structure and vocabulary is standard and no weights are needed for word importance, I dont find a reason for TF-IDF to be used here. 

how about Doc2vec. Mikolov released the journal after the successful attempt of word2vec. A single feature vector for a whole paragraph or sentence or document (depending upon your use case) could be generated. Personally, I didn't get quite good results with it for sentence classification problems. Rather I went with averaged word vectors for sentences. 

Word2Vec is not a combination of two models, rather both are variants of word2vec. Similarly doc2vec has Distributed Memory(DM) model and Distributed Bag of words (DBOW) model. Based on the context words and the target word, these variants arised. Note: the name of the model maybe confusing 

This is kind of a common issue in Seq2Seq model. I haven't tried machine translation, but I have tried text generation and got stuck with this problem of repeating words. The problem is in learning weights I believe. Even with GPUs it takes almost a week to get a considerable good result. I guess, you have used CPUs. for sure, CPUs takes much more time than 44 hours. 

As you correctly say, we calculate the probability of a hidden unit $h_j$ being one and then make it binary. That probability is given by $$p(h_j=1) = \sigma\left(b_j + \sum_{i=1}^V w_{ij}v_i \right)$$ where $\sigma$ is the sigmoid function, $b_j$ is the bias of hidden unit $h_j$, $V$ is the number of visible units, $v_i$ is the (binary!) state of visible unit $i$, and $w_{ij}$ are the weights. So, your MATLAB code for obtaining the probabilities is something like this (we write the sum implicitly by making a matrix multiplication): 

When you are unsure about how something (e.g. RBMs) should really be implemented, it is often useful to look at the code of others. G. Hinton himself has published a MATLAB-script (here) which demonstrates the training of an RBM. There, you can see that for each mini-batch, he does the positive phase, then the negative phase, and finally updates the weights - and that's it. So he doesn't iterate between the visible and hidden states. However, this is not the full truth: for the weight updates we need to know the probability $p(v,h)$. This is very complicated to calculate, as it would contain a sum over all possible states of the RBM. There is a "mathematical trick" called Gibbs sampling: it allows us to iterate back and forth between visible and hidden units to calculate this probability $p(v,h)$. But: for the result to be correct, we have to iterate forever, which is not really practical. So what Hinton proposed is to iterate for only 1 step instead (this is $CD_1$), so he only goes back-and-forth once. But, you can also iterate any number of times $k$, which is denoted by $CD_k$. While for Hinton's $CD_1$, you would do 

Word2vec works in two models CBOW and skip-gram. Let's take CBOW model, as your question goes in the same way that predict the target word, given the surrounding words. Fundamentally, the model develops input and output weight matrices, which depends upon the input context words and output target word with the help of a hidden layer. Thus back-propagation is used to update the weights when the error difference between predicted output vector and the current output matrix. Basically speaking, predicting the target word from given context words is used as an equation to obtain the optimal weight matrix for the given data. To answer the second part, it seems a bit complex than just a linear sum. 

Source : $URL$ Update: Long short term memory models are currently doing a great work in predicting the next words. seq2seq models are explained in tensorflow tutorial. There is also a blog post about text generation. 

I'm working on Seq2Seq model using LSTM from Keras (using Theano background) and I would like to parallelize the processes, because even few MBs of data need several hours for training. It is clear that GPUs are far much better in parallelization than CPUs. At the moment, I only have CPUs to work with. I could access 16 CPUs(2 Threads per core X 4 cores per socket X 2 sockets) From the doc of multi-core support in Theano, I managed to use all the four cores of a single socket. So, basically the CPU is at 400% usage with 4CPUs used and the remaining 12 CPUs remain unused. How do I make use of them too. Tensorflow could also be used instead of Theano background, if it works. 

As you know, a deep belief network (DBN) is a stack of restricted Boltzmann machines (RBM), so let's look at the RBM: a restricted Boltzmann machines is a generative model, which means it is able to generate samples from the learned probability distribution at the visible units (the input). While training the RBM, you teach it how your input samples are distributed, and the RBM learns how it could generate such samples. It can do so by adjusting the visible and hidden biases, and the weights in between. The choice of the number of hidden units is completely up to you: if you choose to give it less hidden than visible units, the RBM will try to recreate the probability distribution at the input with only the number of hidden units it has. An that is already the objective: $p(\mathbf{v})$, the probability distribution at the visible units, should be as close as possible to the probability distribution of your data $p(\text{data})$. To do that, we assign an energy function (both equations taken from A Practical Guide to Training RBMs by G. Hinton) $$E(\mathbf{v},\mathbf{h}) = -\sum_{i \in \text{visible}} a_i v_i - \sum_{j \in \text{hidden}} b_j h_j - \sum_{i,j} v_i h_j w_{ij}$$ to each configuration of visible units $\mathbf{v}$ and hidden units $\mathbf{h}$. Here, $a_i$ and $b_j$ are the biases, and $w_{ij}$ are the weights. Given this energy function, the probability of a visible vector $\mathbf{v}$ is $$p(\mathbf{v}) = \frac 1Z \sum_{\mathbf{h}} e^{-E(\mathbf{v},\mathbf{h})}$$ With that, we know that to increase the probability of the RBM generating a training sample $\mathbf{v}^{(k)}$ (denotes the $k$-th training sample), we need to change $a_i$, $b_j$ and $w_{ij}$ so that the energy $E$ for our given $\mathbf{v}^{(k)}$ and the corresponding $\mathbf{h}$ gets lower. 

1- The number of features: In terms of neural network model it represents the number of neurons in the projection(hidden) layer. As the projection layer is built upon distributional hypothesis, numerical vector for each word signifies it's relation with its context words. These features are learnt by the neural network as this is unsupervised method. Each vector has several set of semantic characteristics. For instance, let's take the classical example, and each word represented by 300-d vector. will have semantic characteristics of Royality, kingdom, masculinity, human in the vector in a certain order. will have masculinity, human, work in a certain order. Thus when is done, masculinity,human characteristics will get nullified and when added with which having femininity, human characteristics will be added thus resulting in a vector much similar to . The interesting thing is, these characteristics are encoded in the vector in a certain order so that numerical computations such as addition, subtraction works perfectly. This is due to the nature of unsupervised learning method in neural network. 2- There are two approximation algorithms. and . When the sample parameter is given, it takes negative sampling. In case of hierarchical softmax, for each word vector its context words are given positive outputs and all other words in vocabulary are given negative outputs. The issue of time complexity is resolved by negative sampling. As in negative sampling, rather than the whole vocabulary, only a sampled part of vocabulary is given negative outputs and the vectors are trained which is so much faster than former method.