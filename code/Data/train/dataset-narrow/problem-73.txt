All these values are very low; even crystals with high refractive indices like diamond ($F_0=0.17$) and moissanite ($F_0=0.21$) hardly exceed 20%. Yet most metals have $F_0$ values above 50%. Moreover, I have read multiple times that the formula mentioned above doesn't apply for metals (which can be easily confirmed by trying to use it and see completely wrong results), but I haven't found any further explanation. What phenomenon explains this difference? How can I calculate $F_0$ for a metal (in particular if the medium it is in contact with has an IoR different than 1, like water)? 

Yet, I don't think I've ever seen it written this way. I have seen the specular term being weighted according to the Fresnel term, but not the diffuse one. In his largely referenced article on PBR, SÃ©bastien Lagarde even states that using $(1 - F)$ to weight the diffuse term is incorrect. What am I missing? I would much welcome an explanation that highlights in an obvious manner why this would be incorrect. 

Back to the problem So we want to know the color of the points of an object, we know that to compute it we need to integrate light in all directions, and we have a cube map that represents light coming from all directions. So far so good. A problem though is integrating basically means we need to go over every single texel of the hemisphere (so half the environment map: $(64 \times 64 \times 6) / 2$ texels), do some math, and add that to the result. That's a lot of computation for each point, which we'd like to avoid if possible. We know that the contribution of one light depends on the light direction. If we consider the environment to be static (the lighting doesn't change), then we can isolate the part of the computation that only depends on the light and surface normal (and not on the material or the observer), pre-compute it and store it to use later. For the diffuse term, that's the diffuse irradiance mentioned earlier, and it typically looks like the center figure of the illustration. Each pixel represents the irradiance term for a given surface normal. The right figure is the specular environment map, computed in a similar way but with a different integral. 

The problem of reducing noise is still under active research though, and there is no silver bullet. At this point you will need to read what are the latest developments. 

The well known Schlick approximation of the Fresnel coefficient gives the equation: $F=F_0+(1 - F_0)(1 - cos(\theta))^5$ And $cos(\theta)$ is equal to the dot product of the surface normal vector and the view vector. It is still unclear to me though if we should use the actual surface normal $N$ or the half vector $H$. Which should be used in a physically based BRDF and why? Moreover, as far as I understand the Fresnel coefficient gives the probability of a given ray to either get reflected or refracted. So I have trouble seeing why we can still use that formula in a BRDF, which is supposed to approximate the integral over all the hemisphere. This observation would tend to make me think this is where $H$ would come, but it is not obvious to me that the Fresnel of a representative normal is equivalent to integrating the Fresnel of all the actual normals. 

Your first quote is referring to "Split-sum approximation" presented in "Real Shading in Unreal Engine 4" by Brian Karis, and also referred in the paper [Kar13]: $$\frac{1}{N}\sum_{k=1}^N \frac{L_i(l_k)f(l_k,v)cos\theta_{l_k}}{p(l_k,v)}\approx \bigg(\frac{1}{N}\sum_{k=1}^N L_i(l_k)\bigg)\bigg(\frac{1}{N}\sum_{k=1}^N \frac{f(l_k,v)cos\theta_{l_k}}{p(l_k,v)}\bigg)$$ In split-sum approximation the incident radiance (cubemap) is pre-convolved with GGX kernel using for example importance sampling, and normal-incident specular reflectance ($F_0$ or "specular color" you refer to) is multiplied with another pre-convolved 2D look-up-texture (referred as "an auxiliary specular color map") shown below. 

Lowering register pressure doesn't necessarily give you any performance boost though. I recently went through this exercise myself on GCN architectures (for a simple ray tracer) and reduced register pressure so that it increased occupancy from 2 to 4, which had no impact on performance. It's generally a good idea to reduce the pressure if you need to hide memory latencies, but it really depends on what the real bottlenecks in your shaders are. Since you are working on a GPU ray tracer, you might get better improvement in performance by paying attention to the divergence of threads and try to improve this instead. For example by trying to group rays based on location and direction to reduce divergence in KD-tree traversal per wave. I'm not familiar with the paper you are referring to, and you may already have considered this though. 

For light sources with larger solid angle and where the shadow caster is relatively closer to the light than the receiver, you get notable soft shadowing effect. So if you render larger light sources closer to the shadow receiver it's important to handle soft shadows properly for realistic lighting. Even with the Sun which has quite small solid angle, you can still see fairly large penumbras from tall buildings. PCSS is fairly simple algorithm for implementing contact hardening shadows, but it has some notable issues. PCSS fails when you have soft and hard shadows intersecting (blocker search fails), so it's not a good algorithm to use when large penumbras are needed. See in the below image where shadows from the box & triangle intersect. This artifact is particularly disturbing when animated. 

It's happening because they just happened to define the rotation matrix with counterclockwise rotation direction, which is the common convention for polar$\rightarrow$cartesian coordinate system transformation. If you google for the transformation, you'll see that the angle is universally shown to rotate to the counterclockwise direction like below 

Normally $sin(\theta)$ isn't written as part of the pdf, because it's an artifact from using double integral for spherical integration. The double integral does the integration over a rectangular domain (in you case of size $2\pi$ x $\pi/2$), while you want integration over a sphere/hemisphere, so $sin(\theta)$ is added there as a weight function to transform rectangular to spherical integration. A common notation is to use integration over solid angle with single integral and differential solid angle $d\omega$ denoting 2D integration over sphere, and you won't see the $sin(\theta)$ term in there. To clarify, the notation with single integral has constant infinitesimally small solid angle $d\omega$ over the sphere. However, for the double integral over $\theta$ and $\phi$ the solid angle isn't constant (think of sphere tessellation) but a function of $sin(\theta)$. In case of Monte Carlo integration and even distribution of samples over the hemisphere, each sample represents solid angle of $2\pi/N$ steradians and there's no $sin(\theta)$ solid angle weighting required. 

To be able to give a good answer, we need to know what is the compiler error you are referring to. At first sight your shader looks ok, although: 

I have no knowledge of the literature on the topic, but I did something very similar to what you're asking some time ago: I wanted to generate lathe meshes and bend them according to a spline. I think the same technique could be adapted to your case quite easily. First you would need to define what your default axis is: if the input mesh corresponds to the case when the spline curve is a straight line, where is that line in 3D? It could be defined arbitrarily as the Y axis, the PCA of the mesh, or some manually defined axis. The algorithm is then: 

So you simply need to make sure texture coordinates generation is disabled for the color texture, but enabled for the shadow texture. As you guessed, this is done with , by writing for example: 

At CEDEC and GDC in the early 2000s, Masaki Kawase has presented a series of fast post-processing based lens effects, including large bloom. This presentation from GDC 2003, Frame Buffer Postprocessing Effects in DOUBLE-S.T.E.A.L (Wreckless) (see slides 15 and upwards: "Bloom"), gives a first version of the bloom. It consists in doing a Gaussian blur by sampling at exponentially increasing distances. In this presentation from GDC 2004, Practical Implementation of High Dynamic Range Rendering (see slides 44 and upward: "Glare generation"), he updates the technique. Instead of varying the sampling distance, he uses downsized versions of the original image, using a carefully crafted equation to achieve a large yet spiky Gaussian blur. 

This being said, you're adding to yourself a lot of trouble by sticking with fixed pipeline instead of using GLSL. 

Make sure you still take the paths that hit nothing into account when averaging the color of the pixel, otherwise this will also introduce a bias. As joojaa said in the comment, you should try using a simpler scene: a few spheres or cubes, no texture. You can also reduce the number of bounces to one or two to see only direct lighting. If necessary, maybe even use a constant sky dome first instead of a light area. 

A note first From the look of your screen capture, I suspect there might still be a bug in your code. Noise is to be expected with only 16 spp, but your picture still looks surprisingly dark to me. For comparison, here is what my implementation of SmallPT looks like with 16 spp, 15 bounces, and no next event prediction: 

You need to implement your own RNG class and not use the clib RNG implementation that shares the state across threads. What I'm using is a simple & efficient multiply-with-carry RNG by George Marsaglia, that I initialize with a seed based on pixel coordinates (e.g. simple x+y*width) that I'm calculating (state per pixel instead of global state) 

Instead of subdividing until reaching certain edge length on screen, you should look into GPU Gems 2 article "Adaptive Tessellation of Subdivision Surfaces with Displacement Mapping" by Michael Bunnell. It uses edge flatness test to adaptively tessellate the geometry which results in better quality with the same number of triangles. With your current adaptive tessellation strategy you'll end up splitting flat surfaces, which unnecessarily increases your triangle count. This is particularly bad on GPU's as it increases quad overdraw for the resulting microtriangles. About your question for the name of your subdivision scheme, I don't think there is specific name for it but it just falls under more generic "adaptive tessellation". 

Next about your second question of importance sampling being inefficient due to band-limited signal - It simply means that the incident radiance is sampled with finite resolution (cubemap resolution) and thus higher frequencies have been filtered out. But because importance sampling biases samples to important regions of the function (peak of the GGX BRDF) this region may be sampled in unnecessarily high frequency thus wasting samples. 

I'll explain the "geometric way" of solving the problem since that's the solution you seem to be after, instead of using rasterization that Dan suggested in the comments. You can get the list of potential silhouette edges without 2D projection by first iterating through all edges and taking dot product of the normal of the attached two triangles against the eye->edge vector (any point on the edge will do, e.g. one of the end-points). If the dot products for the two triangles on the edge have different signs, then it's a potential silhouette edge. If the object is convex, then you are done and got the list of silhouette edges. For concave objects it gets more complicated. For concave objects it could be that an edge is only partially a silhouette edge and can be potentially split to N fragments that all belong to the silhouette. So you need to process the list of potential edges and clip them with the geometry. The way you can do the clipping is to extrude every triangle in the geometry away from eye and use this volume to clip the list of potential silhouette edges. The remaining edge fragments that are not clipped away are your silhouette. 

While browsing to properly write my question, I actually found the answer, which happens to be very simple. Another Fresnel term is also going to weight in as the photons make their way out of the material (so being refracted into the air) and become the diffuse term. Thus the correct factor for the diffuse term would be: $$(1 - F_{in}) * (1 - F_{out})$$ 

Reduce shading when possible Lens distortion Part of the NVidia VRWorks SDK is a feature, Multi-Res Shading, that allow to reduce the amount of shading, to take into account the fact that some pixels contribute less to the final image due to lens distortion. In Alex Vlachos' GDC 2015 presentation, he also mentions the idea of rendering the image in two parts of different resolution, to achieve the same goal. Foveated rendering Finally, like your question mentions, foveated rendering aims at scaling the resolution to take into account the fact that eyes have more precision in the fovea and less in the periphery. There is some research on the topic, but this requires eye tracking like in the Fove HMD. 

Addendum To use depth or normals, you would need to save them in a texture if that's not done already. When you create the frame buffer for your regular rendering pass, you can attach various textures to it (see ) and write other information to them than just the color of the scene. If you attach a depth texture (with ), it will be used automatically for depth. If you attach one or several color textures (with ), you can write to them by declaring several outputs to your fragment shaders (this used to be done with ; either way, see ). For more information on the topic, look up "Multi Render Target" (MRT). 

So in the case #2, the diffuse is left out, which is equivalent to a 0%, pure black, diffuse color. In case you haven't checked it already, this presentation is an excellent code explanation of SmallPT, dense but thorough: smallpt: Global Illumination in 99 lines of C++. 

This approach should give the expected result, but the problem with it is the first part, which is not trivial at all. Idea B: Approximate the mesh with fat particles, that are big enough to hide points in the background. 

First render the scene into a frame buffer, without the distortion effect. Copy the entire frame buffer into another frame buffer using a custom shader. The shader will apply some offset to the texture coordinates (either directly or maybe using a distortion texture), thus distorting the image.