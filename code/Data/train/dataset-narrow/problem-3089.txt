I am trying to build an Autoencoder with LSTMs. My input data has been one hot encoded, which results in around 13.000 features. My problem at this point is, that I get the following error message: 

ValueError: Cannot create a tensor proto whose content is larger than 2GB. How can I reduce the size of the tensor without secraficing the deepth of the model respectively without rewriting the code in tensor myself? I am using tesnorflow as my backend. My model looks as following: 

From my gut feeling I would say that you don't have enough data to train the AE properly. Keep in Mind, the MNIST database contains 50,000 image. And you need enough variance in order to not overfit your training data. Tree based approaches are, at least in my experience, easier to train. If you like to stick at the anomaly detection part, which I recommend since you don't know what anomalies you will face, try the Isolation Forest Algorithm. But for a solid recommendation I would need to know how your data looks. Btw, A good metric to use in such a case is the ROC score, which basically tells you how likely it is that your model will classify new data points correctly. Check out the link for an visual explanationROC explained So Baseline is, try less complex approaches until you a certain that they are not sufficient enough. 

Assuming, you have a dataframe, is strictly label based. Since you're using it accesses the column you're specifying inside the brackets and that is the reason you're getting . Read documentation for better explanation. Here is another link that has answers similar to your question. 

After this you can use function to make your time-series stationary by taking off trend and seasonality. Refer to this tutorial, which has in detail to make your time-series stationary based on your data. Hope this helps. 

I am not sure if there any is any library that can directly make your data stationary in python. However, you can plot your time series, which will visually tell you the components such as trend and seasonality of your data. using you can take each a look at each component of a time-series like this, 

Well, it depends. Auto Encoder are a quite broad field, there are many hyperparameters to tune, width, depth, loss function, optimizer, epochs. 

What you are facing is a small but crucial definition difference: novelty detection: The training data is not polluted by outliers, and we are interested in detecting anomalies in new observations. outlier detection: The training data contains outliers, and we need to fit the central mode of the training data, ignoring the deviant observations. OneClassSVM is an Unsupervised Outlier Detection. Therefor your data needs to have outliers in order for the algortihm to detect them. My best guess, why its prediction every input as an outlier is, that if there are no real outliers, everything must be an outlier. Let me demonstrate this quickly. I adjustet the kernel to linear 

Gaussian models are often used (and maybe sometimes over-used) because of their mathematical convenience (many statistical models can be found as built-in functions, when based on the Gaussian distribution, in some libraries such as mixture models, hidden Markov models,...). Also, when one has no idea about what distribution could best model the data, given the relationship of the Gaussian distribution with the Central-Limit Theorem, it is often a reasonable assumption to make. However, if say, your goal is to generate new data similar to some training data and that you know that the data you would like to generate should be strictly positive, then a Gaussian assumption may not be the best one. Indeed, generating data from a Gaussian-based model gives no guarantee that these data will be positive (the Gaussian has an infinite support). Then one could think of basing the model on some distribution that insure generated data to be positive such as the Beta or the Dirichlet distribution. However, it is always a good start to take the Gaussian as an assumption, get some results and if they are not good enough, try other assumptions and compare. This can enhance the accuracy but can also require a big amount of work as most classical machine learning algorithms are often not implemented in main libraries for assumptions other than the Gaussian. To summarize: 

The baseline of my answer is certainly, try the simplest approach first, then go on. Machine Learning is not about heavy lifting, it is about smart usage of your tools. The choice of your algorithm depends mostly on your data and the types of anomalies you would expect. Do you have timeseries data with sesonality effects and trends, maybe the twitter anomaly detection package would be a good start. For instance. If your CPU is hot you would expect it to run with high load. If the load is low maybe it is an anomaly or it has suffered from a high load. So one of your questions would be, can you spot anomalies or identify normal behaviour by just looking at one sample? Therefore the, "No-free-Lunch", theorem applies again. That being said, SVM and Isolation Forest are a good start. Even if you have a time series problem, where you want to track short term dependencies. Just include "old" rows to your current set:Link You can even try simpler approaches like Gaussian distribution of your values, calculating the probabilities of each permutation (if you features are small enough) or building a markov modell. Unsatisfying results? Dive deeper, maybe Auto Encoder do the trick, maybe LSTMs, maybe a combination of both. A short summary of unsupervised and semi-supervised ML algorithms I recently used, sorted by complexity (kind of): 

Apparently, I misunderstood your question. There are several methods for finding the k-best paths with extending versions of the Viterbi algorithm. My first advice would be to look at this question on SO that is similar to yours and has a good illustrated answer. Then, I would refer you to two articles/thesis that are publicly available and from where one can extend his/her research. (Disclaimer: these references may not be the "best" one but I've chosen them because they are publicly available and provide a good number of reference to deepen research on the topic) 

The first thing to consider is that, even continuous measurements are actually discrete. For instance, temperature is always considered as continuous but every time you process temperature data, these are discrete data to the precision of the sensor used to measure them. Same is true for every physical, measurable quantity (including intensity). That being said, with our modern physics models, it makes no sense to consider that temperatures follow discrete mass probabilities with several thousands possible output and everyone studies them under continuous assumptions. So where do we set the boundary? When do we consider that there are "too many" possible outputs to work with probability mass functions and choose to work with continuous distributions? This is an open question! :) Here is the thing, we do not lose information when processing discrete data in a continuous way so there is no restriction for doing so! Now consider the pixel intensities of an image, most of the time, we would need to consider discrete mass probabilities with 256 possible outputs, meaning 256 parameters (or weights) to be estimated or tuned. If we model the intensity by a continuous distribution, the number of parameters is drastically reduced! For a Gaussian for instance: 2 parameters. Even using more complex distributions or mixture models, we will not need to tune hundreds of parameters. This is the main advantage! Compact representation! About the choice of the Gaussian specifically, let say that it has been empirically proven to work great. The Central Limit Theorem also supports this common choice. And now, on the top of this, many algorithms are readily available under the assumption that the data follows Gaussian distributions making them even more used by people in the field. Doesn't mean that nothing else could be used but this is the reality of things. 

In your first case, what you're doing is copying a list, which is referencing a list to another list (ex: list is referenced by list ). In order to avoid that, you have to consider in python in order to safely make edits to the copied list without those changes being reflected in the original. For example, 

I am trying an model using following this tutorial . I am having trouble understanding why am I getting an error in my test set when I try to invert scaling for forecast (line 86 in the tutorial). After loading the dataset and created featured, following is what I did for dataset, 

It can indeed be worth it (and little work) to transform the data and see the impact on the results. 

My previous answer: For anyone coming across this question looking for a way of computing some of the typical state sequences of an HMM (as I thought first this question was about), just know that such a concept of most probable sequence without specifying data is not really something used in any theory about HMMs, as far as I know. However, one can follow these steps: As a first try, I would implement something like this: Get the state at time $t=0$ Draw an initial state $s_0$ from the initial state probability mass function (pmf) Get the state at time $t+1$ Draw the new state $s_1$ from the pmf defined by the $s_0$-th row of the transition matrix Repeat this step as many times as needed to draw states up to $s_N$ Then you can repeat the entire procedure $X$ times in order to get as many sample paths as you wish. This is very fast and easy to implement and it will give you what you want. Many scientific libraries in many languages have a built-in function for drawing a random sample from a pmf.