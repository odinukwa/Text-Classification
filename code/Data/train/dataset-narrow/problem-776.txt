Just answering your specific scenario. I would create a table for employees and the common attributes they have. Name, DOB etc. I would then create a table for thier skills or qualifications. I would create a foreign key linking to the employee table and use a name/value pair. So you have something like: 

I think the answer depends upon what you are trying to achieve and how the data will be used. Think more about the application design requirements. From what you describe, I would recommend a single table with 1 key, datetime & value per row. ( I think this is what you describe in your 2nd option) The reason I say this is this is that its easier insert the new 'now' record with today's date time. The newest record is current. Or you could choose to add an 'is_current' flag. This approach would make it easier to store and maintain a rolling set of 4 values but doesn't stop you from having more or less values. It also doesn't force you to have fixed intervals. And it doesn't require you to update records to 'shift' records from one column to another if you have to maintain a rolling window. To get the latest / current /now record you: select id, value, max (timestamp) From tab Group by id, value 

All relational dbs should be "good at aggregations". Thats what they're designed for. Most db's follow similar design rules. To produce your result, they all have to do the same math. Im not familiar with other products but to consistently achieve response times like that you'll need to have the data in memory already (cached) and/or pre-calulated. Either using an aggregate table or an OLAP cube. The first time your query runs will be the slowest. If you can reuse the same query it will be faster. The more you can reuse the same query - the faster on average it will be. If you can pre-execute the query ahead of time your client may never experience the delay. Re-use and cacheing depend upon how often the data changes and how often the same query is executed. If the underlying data or the query changes, you go through the over head of calculating the execution plan and io again. If the data changes how quickly do you need the aggregate to reflect the change? If you are developing a proof of concept sql developer edition has column store indexes and memory optimised table features which may help, but if this goes into production you'd need Enterprise edition which isnt free. You could use an SSAS OLAP cube with SQL standard but there will be a lag while processing. However SQL Std isnt free either. It may be possible to achieve this with SQL express and standard query tuning and/or aggregate tables. Frequently executing the query will assist with caching which will help performance. Stored procs and parameterisation will also help. I doubt just picking a different product will magically make this query go faster without some other compromise. They all have to do the same math. They all have to write to and read from disk at some point. Based upon the volumes you have described and the desire to use 'free' dbs i would consider sql express and aggregate tables updated periodically. 

I need to migrate some data from another system, and so I'm trying to insert data into this field/XML. So far my attempts have resulted in singleton nodes which the app doesn't like. Using some examples i found in the web I've tried a the following: 

You can't connect to SSAS with SQL authentication. SSAS only uses Windows authentication.. On the server side users must be defined as part of a security role. The role will define what cube privileges you have. To connect to another server you type the server name in field where it says local host. 

I believe what you are describing is technically ETL. However I don't think it is what people in the industry would typically consider as ETL. And Excel would not commonly be considered an ETL tool. There is a difference between experience with ETL and experience with a specific ETL tool or product. However I do think that if you explain yourself carefully that it would be beneficial to your CV. Eg: "Automated Extract, Transform & Loading of data between databases using VBA and Excel" is honest and unambiguous. In my opinion it shows a degree of understanding, technical proficiency, problem solving and experience. And I think that has merit. 

The short answer is: define your end result ( requirement) and work backwards from there. Normalization goes out the window pretty quick with a DW schema. In my case we have 3 schemas for our ETL. 

There's a few things wrong with your post. But cutting to the point: Judging by your code samples, the fastest/easiest way to create a table as a copy of another is to "select * into tableA from tableB" In your case, assuming you only have (or want) the 2 columns you have listed you could use the following: SELECT f_name, l_name INTO dummy_driver FROM driver; The limitation of the above statement is that you can only use it once. If you try to execute the same statement again it will fail because the target table already exists. 

I've just upgraded our Data warehouse to SQL 2016. I'm seeing some really interesting graphs in the Query Store (I love this feature!). Below is the weirdest example i've seen. 22 plans for the same query. 

I have curves consisting of X/Y points (an array?) stored in a string with the same delimiter. There is 1 curve per row. Below is a simple example: 

I think the answer depends upon how your app is intended to work. Do you envisage users being able to reply to many messages? Or only to a single message? Personally id keep all messages in a single table with a self join. (Like a employee heirachy where each employee has a manager). To speed up searching and selection you could keep all related replies together with a "conversation id". 

I found a workaround, but not one I'm especially happy with. We managed to use our Website monitoring software to hit the SSRS report server, and it seems to do just enough that SSRS web component starts up. The first user of the day now gets a response within 4 seconds. I have spent hours trying to get powershell to do this for me, and got very close. So I'll leave this here. If someone else can solve the last piece of the puzzle and make this work I'll credit them with the answer: 

It's making me consider performance tuning of my ETL process and the pros and cons of temporary tables and how you could influence execution plan behavior. My ETL process uses a number of stored procedures which use mix of standard and temporary #tables as staging tables. The #tables are typically used once and then dropped. Some are only a few thousand rows. Some are millions. SSMS advises that there are missing indexes, but on smaller tables would they make enough of difference to be worth the effort of adding them? Are better statistics sufficient? I've just read this Brent Ozar blog post about Statistics on Temp tables, and Paul White's article on Temporary Tables in Stored procedures It says that statistics are created automatically, when the #table is queried, and then presumably used by the optimizer. My questions are: Is there much point or benefit in creating an index on a #table. And/or: Is it worth explicitly updating statistics as a step in the stored procedure before using it in queries given they're only used once. Are the additional steps and overhead worth it? Would it result in significantly better or different execution plans? 

I'm sure there's other more efficient ways of doing this, but the point being the subquery executes and correctly returns valid records. in this case id = 4. Which has a valid date 1 July 2016. The outer query should simply now be comparing the converted date with today's date. But instead i get: 

The 3 options you have suggested are correct. Another option is simply upgrading SQL Server in place. If this a development instance on the same machine I would consider that as an option. Which option is best will depend on your preferences, the size of the db's and whether you need to move machines and/or whether it is important to have a fresh installation. As the difference between 2016 & 2017 is comparatively small, you could detach, move and reattach or backup and restore. In each case you should check the compatibility level and raise is it if you wish to use new 2017 features. If you are moving to a new SQL instance logins will need to be reattached. Yes you could also script the the data, export/import, use a wizard or a data comparison tool. However scripting comes with some risk depending upon the tools and formats you use to export and import the data (eg .csv BCP etc). The bigger and more complex the db is the less feasible or attractive this becomes. 

The links and information are confusing but we did find a solution. The short version is you need to install Visual Studio 2015. I used the free Visual Studio 2015 Community Edition. That includes the components needed to run Team Explorer. Once we installed VS the Team menu is now visible in SQL Server 2016 SSDT. You can then add and configure TFS as before. 

This has now run successfully for ~1.5 weeks. The more I think about it I believe that the ssis error (posted in title) was a side effect, and was the result of the timeout caused by the dataload connection. I can't prove or disprove if installing SP3 had any effect at all. In my opinion the comment from @TheGameiswar is the best indicator pointing to root cause. 

If you want to create a table using DDL (data definition language) you can use a create table statement You don't need the start or end position. But you do need a data type and field length. With a bit of clever work in excel, you can probably generate a create statement. Unless you are repeating this for lots of tables it might be easier just to create the table directly in Access. Another option is to mock up a sample table in excel with your field names and a sample row of data and import it into access. That way access will select suitable data types. 

I would highly recommend that you take the time and effort required to connect your app to a newer version of SQL, preferably one that is officially supported. i.e 2012+. (SQL 2000, 2005 and 2008 and 2008R2 are all officially obsolete.) If you have issues with deprecated features or connectivity, SQL 2008R2 supports database compatibility back to 2000. It's still obsolete, but is still a much more sophisticated product than 2000. Edit: Technically Support for 2008 & 2008R2 is still available from Microsoft through their MS Lifecycle Support Site. In most cases apps are oblivious to which version of SQL they are on as long as they can connect and the features and functions still work. Even if you can't do a backup and restore, you should be able to create a new db and use SSIS to import the data. If this is a current application I highly recommend you go back to the vendors for assistance and ask/insist they support current db's. Requiring a 17 yr old db to be able to install or run their software is terrible. If this is a legacy app and no longer supported I suspect you have a much bigger issue looming than just moving a db. If this is a legacy app and you just want to keep it running but need to replace hardware, I would consider virtualising the server. 

My best and most speculative guesses are: A lookup table. Used as a control/pick list for an application or report A dimension. Used in a data warehouse. A log table. Used for logging. Left over junk fron an adhoc query, data migration, report filter etc. Aaron was right... But I couldn't resist ; ) To me, the names imply that it's to distinguish between academic and non academic staff. But unless you can test the application exhaustively in order to reverse engineer it you may never know the true/complete purpose for it. 

I believe you are asking about a table design or schema. Whether you use 1 or many tables is up to you. The increased complexity of normalisation is a trade off between efficiency and convenience. The easiest option to begin with is to use a single table to describe the car, and create a field for each piece of information you want to store. Start with number plate, make, model, year... If you want to limit or restrict the data that can be entered you can add reference tables with foreign key constraints. If your model is going to be be detailed enough to describe things like engine models, individual parts or a detailed service history then you will probably want to use seperate tables. The key question here when thinking about the data is: "When i think about one car, does it have one or many of these things?" That will give you a good indication if you will need to add a table for that 'thing' Sometimes we dont have all the information we would like to have. Thats just a fact of life. You can either collect the missing info, or design your table(s) so that the data is not mandatory. You need to decide if the lack of detail is going to be a problem for you. Can you achieve want you want with the data you have? Will more data improve the accuracy or output of your database? 

Running SQL Server 2012 SP1 and SSDT from same distribution. (VS2010) When I display a report in SSDT the date selector on a date parameter is dynamic or responsive. When you click on the date (month/year) the date picker can zoom in/out. Or drill in/out (Apologies if I'm using the wrong terminology). However looking at same report in SSRS, the date picker doesn't have the same functionality. You can click left and right arrows to get last/next month but clicking on the date does nothing. Does this only work in SSDT? or should it also work in SSRS? If it should work in SSRS, any suggestion of where to start looking would be greatly appreciated. Is it report setting? an SSRS setting? a browser setting? If there is a better description of this feature please let me know and I'll update my question. Thanks Peter 

Systems I have worked with in the past stored for each customer a row for each recurring product they have. Each row has a product id, start date and anticipated end date, and a "billed to date". Each month when you bill them you increment the date. From that you generate an invoice (storing the values to make up the invoice), and they customer makes payments against that. Their balance is calculated from opening balance + invoices +/- payments. It gets complicated when product prices change over time, or discounts only apply for a specific period. You need to store enough detail that you can recreate the invoice correctly at that point in time. If you give a refund that you don't want to refund too much, and if required can you pro-rate adjustments? A good approach is to ask yourself what level of detail do you need to see on an invoice. If I had to recreate the invoice, adjust it or provide a refund, where would I get the data from. You need to either store the detail or be able to reproduce it reliably. Can you? The next level to consider is, what accounting information to I need to report? How much did we sell last month? what discounts applied? how many customers have paid? who owes us money and how much?(debt mngt) How much tax have i charged? (is anything exempt? different rates?) Look and the end results you need and work back from there.