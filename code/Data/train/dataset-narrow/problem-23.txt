get some kind of checkpoint in place to get security in the pipeline keep a backlog of unmanaged security concerns (gaps in the tools? CVEs?) keep the backlog prioritized by what you can get done now, at current funding get involved in devops development and release planning deliver software confident that you did the best with the resources you had 

Put your pubkey in that file, and restart the sshd. See: $URL$ The default behavior (which we are all probably familiar with) is 

I'm going to add more story to the answer @esoterydactyl gave, in case it goes over too many people's heads. Instead of a monolithic build process, maybe you want to build intermediate packages that can be versioned and deployed to your workspace without all the intermediate build steps. Docker is one packaging solution which is super popular in this case. You could also use .deb or .rpm or non-Docker tarballs, or make CF "StemCells" or even zip files. Also consider Python has prebuilt "wheel" packages now. Problems: 

Laravel Framework for building a full MVC Web application in PHP. It is the equivalent of Spring-boot with Java or Rails with Ruby. Artisan Laravel's console driven interface for development. It provides tools such as the web server coming with PHP 5.4+. When using the "serve" command a simple web server will start. This is perfect for development, but I would suggest you use a server that gives you proper configuration flexibility in production such as Apache or nginx. 

The confusion might come from the concepts behind the tools you are using. So let's start with a little definition. 

If you consider this solution to track binary versions, I would suggest you go for binary repositories instead such as: 

When to decide whether a file should go to git LFS Your files can go to git-lfs at any time, but it would be inneficient to implement this solution for few files. Specially if it's basic gradlew jars. I have used the gradlew strategy previously. The idea is to push this "wrapper" along your code to make the build process portable. Any platform with the proper JRE will be able to build the application in a similar fashion. This removes the headache of enforcing teams to install specific versions of gradle. The purpose of gi-lfs is to leverage the benefits of git along with the ability to store large files remotely by caching them and indexing them. It reduces the burden of downloading a big repository when binaries are stored. In my opinion you should start consider this solution when: 

The point of DevOps, is that development shouldn't oppose operations, instead they should support each other. Traditionally, due to waterfall deployments and large scale updates, development would cause operations a variety of problems when deploying due to inadequate testing, changing server environments, the list goes on and on. Essentially, the updates were too large for the operations team to be able to effectively deploy them without some problems arising in the process. These problems might be why you believe that development opposes operations. On the other hand, DevOps works to reduce update size, decrease rigid environments, and generally improve the handoff of the application between development and operations by increasing the amount of times the handoff occurs each year. With the increased number of deployments comes less headaches for operations, because they have either automated a large amount of work required to update the products, or they better anticipate and prepare for the updates. Tldr: DevOps aims to nullify the theory that development opposes operations by creating a mindset where operations and development work together to frequently deploy products in a timely and easily reproducible way. 

2017 expectations Docker is about getting some freedom from host management at the expense of reinventing all the things package management does. Docker images are just another package format. This is all very new, so I think this question is part of the growing pains of the Docker ecosystem. basic requirements/criteria For secure trusted containers, only use them if they have been 1. tested by current standards 2. cryptographically signed 3. checked against a vulnerability database for insecure dependencies Maybe there is a way, but I don't see a way to un-trust a signed, trusted Docker image after a researcher finds a new vulnerability. This is just one gap in the toolset cited in the question. Are you planning to use a cloud Docker service? Are the containers' memory encrypted or at least obfuscated at the kernel (containers are just processes)? Are your Docker hosts secure? $URL$ $URL$ economy of security In the end, you can never escape the economics of checking everything, so there needs to be some respect paid to the speed Docker provides by not worrying about all of this stuff. When you know you are handling sensitive information and you know you need to lock down the execution, it will require measures that are not available yet as commodity solutions (2017). Anything is better than nothing, so get some tools deployed and get your foot in the door. Hubblestack? Keep your eye on HubbleStack. It provides a Docker image which can be dropped in to run audits (and remediations) against a live database of vulnerabilities. I'd be surprised if they left Docker trust out of scope forever. Also, if you own the Docker hosts, you can use root to scan all the containers. This kind of thing belongs in the DevOps automated test and validation cycle. 

My team and I are responsible for developing "one-offs", products that once finished are given to the client for upkeep or in some cases managed by us for a fee. We still need to maintain a solid development pipeline to handle the constant feedback from our clients in order to ensure that we ship them something reliable and proven to run. While the client doesn't care about DevOps (in most cases), it is still helpful for us. With DevOps, we can rapidly push new builds, so clients can see feedback in minutes not hours, and we are also able to catch any errors/bugs with our testing via Jenkins/Travis. To ensure our deployment strategies are the same across projects, we focus on containerizing our applications. Using Docker, we are able to easily hand off the application to our clients. The cost saved by DevOps is hard to determine. We do have extra costs in the form of software we choose to use for the pipeline (Travis, Jenkins, Puppet, what have you), but we also save time and money by fixing bugs/ giving the clients feedback quickly. Our quick response time keeps our customers happy, in turn, keeping our wallets happy. 

Solution with down time When upgrading Service A with breaking changes, move these changes to another "version". The goal is for you to proactively create a new interface version (i.e. API version) that will implement the breaking changes. In some situation, this might require heavy refactoring. In other (depending if your code base was properly layered), not. Then you can start implementing "contract" tests between Service A and B to make sure A doesn't break the contract with B. The problem here is that when deploying this solution, for a short while you will not have access to neither API. Solution without down time Make the same change as the solution above, but also consider a deployment strategy where Service A.1 is always up while we introduce Service A.2. When no requests to A.1 is incoming, it may go down to leave only A.2 alive to serve all the requests. This is the blue-green deployment strategy. This is more a cherry on the top if your overall solution requires minimal downtime deployment. 

Immutable servers are servers on which no changes can be made (other than updates and security patches ideally). Instead of changing the software on the server, you spool up a new server with the desired software and then terminate the older one. This concept helps to ensure that your test, development, and QA server are all identical, which is important for multiple reasons out of the scope of this question. Another benefit of immutable servers is the ability to rollback the application onto an older server. For example, I need to change K on production server 1, so I spool up server 2 and change K. Now after 10 minutes, I notice that K broke something with my application, rather than having to fix it right away which could take hours and potentially cause downtime for my customers, I redirect the traffic back to server 1, while I figure out what is wrong with 2. 

Does the existing CI configuration will break If you decide to go git-lfs, the normal git commands won't work. You will need to either create symbolic links toward git-lfs after doing the setup or add after in your CI tool. i.e. becomes . 

No matter which language you use, choose a library or a framework that will give you the tools to implement your APIs. Since Laravel is your framework, you can take a look at this tutorial to get started. 

Java/Ruby/PHP/Javascript Are programming languages. Node.js is like an ecosystem that allows you to run client/server/middleware/tools Javascript based applications. Someone did a great job of comparing it with a Java ecosystem. Right here. 

In theory, yes. It would require your development setup to be in production as well. Hence from this assertion, the answer becomes self evident for security, operability, configuration and performance to name a few. You will need a different configuration for production.