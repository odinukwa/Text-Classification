At my current employer, the developers are branching for development per story and merging for release in Git - and frankly I'm jealous. ...but I haven't been able to wrap my head around how a SQL Server DB workflow should work in this manner, especially considering tooling that I am aware of / using. So, I'm curious if anyone has some thoughts. ~Our SQL Server environment 

Each team has a dev server stack where stories are worked. There are 25+ databases per server. When product/project determine which stories are to release, those changes are merged to Dev Release - and eventually flow back to team dev. Dev Release is then used to script the deployment. Thus any ongoing projects (long term / third party integrations / etc) aren't included with the release. This means that there is not a point guaranteed where dev release and any team dev are fully synchronized. Also of note, the sequence of changes made to dev release are not related to the sequence that changes are applied within team dev... Some ponderings: One way (problem): If we have a single branch for each team dev server - it appears to be easy to keep in sync with tooling (RedGate Source Control, etc)... ...but prepping for a release (in Git) either means cherry-picking commits (too much work) or git copying files and losing dev commit history (too much work). (I assume that SVN's file deltas would preserve dev history - but would still be a pain (as its not a merge)... guessing) Although dev history is lost, we would still have a list of release changes - but that doesn't work for me. This feels wrong. Another way (problem): If we branch per story - then it would be fairly trivial to merge to dev release and preserve object history. However, there wouldn't be a single branch that matches the state of the database, and thus tooling (RedGate, etc) will always be complaining that the DB and the repo are out of sync. This feels annoying. At my previous employer, we branched for release using SVN. All forward development teams worked towards a common release - thus forward changes were developed against the trunk (or a dev branch reconciled to the trunk), and hotfixes were applied to the release branches. Depending upon the product, we had different workflows for publish - but the idea was similar. Most all the links I've found (even on this site) seem to point to that workflow - as described very well here: $URL$ However, all the teams (excluding hotfixes) were working towards a common release - thus the dev branches had to reconcile / merge. So, if we had a project / integration that wasn't in the release, we'd have to back out changes of the trunk for the freeze - and reapply once the next release cycle started. Annoying - but doable. Of note, the changes to the database could be applied in a serial fashion - as they were (mostly) all related. So, it's no surprise that tooling SSDT / RedGate / etc seem to be geared for this... So, just curious if anybody has any pointers, helpful URLs, or can point out where my ponderings have gone awry, I'd be grateful for the info. 

Will this work as intended? DO I need to change the transaction isolation level? Is there a better way to do this in MySQL? Note I'm using MySQL 5.6 in one environment and Amazon RDS in another environment, so it should be as compatible as possible. 

However, I'm very worried about the synchronization of all this and making sure we don't end up in a bad state if multiple queries are running concurrently. Because we have multiple nodes that could be accessing the database, concurrency at the application level is very tricky (we would need to use redis or something for locks, which I'd rather not do). I can use GET_LOCK and RELEASE_LOCK with a named lock, or I can use table locking. My fear with these methods is: what will happen if something goes wrong? The lock is held for the length of the session, right? We're using connection pooling (some services using Apache DBCP and others using Hikari), which reuses connections, and even without that, we might have transactions that do multiple operations. Since MySQL lacks any mechanism, I'm afraid we might get into some sort of deadlock situation. What's the best way to implement concurrency with this approach? Note that I only care about concurrency within the triggers/update process itself. It would be nice if other clients don't read an intermediate state, but it's not the end of the world. As long as it's eventually consistent, I can handle phantom/stale/whatever reads from clients that aren't doing updates. 

Multiple upstream Kapacitor servers are sending notifications to a load-balanced django app. If all the upstream servers are working correctly, the app will always receive duplicates of these notifications (since all the upstream notifiers should send the same notifications; it's just for redundancy). I want to be able to filter out these duplicates. However, since the python app is load-balanced, the only place we can check for these duplicates is in the database. This means I'm using this stored procedure to control application logic, just inserting the data with the hash into the database and ignoring duplicates is not an option (the application might do something like send someone an SMS message based on the contents of the alert, so we definitely don't want dupes) To do this, I'm hashing the messages, then invoking a stored procedure in the database that checks if a message received in the last 10 seconds had the same hash. I want to be 99% sure the stored procedure is safe against race conditions. Here's some SQL code that seems to work: 

CONTEXT: this isn't exactly a closure table. We need to have the full hierarchy in these other tables for quick access. For example, we need to be able to do joins on the complete set of parents (since stuff like permissions are inherited by sub-folders, so if a user has access to a parent, they have access to all subfolders). Therefore, the and (sorry about the naming conventions; I don't control that part) need to have a record of every item and every child of that item (transitively). Basically, we need to be able to check if a user tries to access a folder 5 levels deep what level of access that user has access to it, which can be granted at any level of the hierarchy. We also need to do some aggregations/sorting on stuff that joining the subfolders table would be helpful for (ie "sort on total number of items in this folder and all its subfolders"). The reason there are two tables is that we have one table for folders, and one for items. Folders and items have different ID spaces. Think of it like a filesystem with directories and files treated separately. A file ("watchlist") can't have any children, but a folder can have either folders or files as children. The code I posted above works for this. My concern is just with the concurrency and correctness in the face of multiple simultaneous clients. 

Using correlated subqueries is not a recipe for blinding speed, but the advantage of this is that you don't have to rank everything, you just have to count entries in an index relative to a user's particular scores. Of course, only your query optimizer will know for sure. 

Trying to normalize addresses is generally a bad idea. There isn't a lot of value to normalizing addresses. Both of your designs are inappropriate for the vast majority of systems. There are two things you typically do with addresses: 

Both designs are technically 3NF, depending on your data population. What a lot of people who have only a cursory understanding of formal normalization theory don't understand is that redundancy in foreign keys isn't really redundancy by definition. Transitive dependencies and partial dependencies apply to the way non-key attributes relate to key attributes. If your composite keys happen to contain (or even consist of) foreign keys then whether your foreign key has the look of being denormalized isn't relevant. (i.e. Design 1 isn't redundant in a way which is significant to the formal definition of normal forms.) The practical issue comes down to what the values of your keys are. If you have natural keys and the key consists of =123 and =1 - i.e. your IDPart values are natural number sequences which are only unique within the context of a given IDObject value, then you need to use design 1 (weak entities). However, what a lot of people do for good (and less good) reasons is apply surrogate primary keys to every or nearly every table. I'm not going to get into whether or why to use surrogate keys. That is a holy war for another question. However, if you happen to have table-wide unique (e.g. IDENTITY) values for IDPart, then your primary key for is more complex than it needs to be using design 1. In that case, there is no need for in the PK of because the table already has a unique key just using . In such a case, design 2 is sufficient and simpler, therefore better, as a rule of thumb. In practice, I would say weak entities are much more common in logical models than in physical models, if for no other reason than that most physical models tend to use surrogate primary keys. 

The theory behind using stored procedures for increased security is that you don't give DML access directly to users. The idea is that users never get anything other than execute permissions on the procedures. There's also a slightly less paranoid version of this approach where all writes are by stored procs, but table/view reads can be done by users. The reasoning goes that if users only have execute permissions on stored procs and no direct DML on tables or views, then they can't manipulate data in any unpredicable ways. To take an example, let's say you've got a bank account system and the only way to make a funds transfer is through a stored proc, you can have this proc audit log details about when it happened, where the money went and so forth. If you allowed direct DML access to the base tables then a compromised account could be used to take money without leaving an audit trail. You are right that a compromised account is still a big security hole - big enough to drive a truck through. If all the compromised account's access is restricted to stored proc execution however then at least the hole will only fit a smaller truck.