It seems that you are looping through the entire job data set 3 times (once each for salaries, descriptions and titles). You will be able to speed this up three-fold, if you extract all the info in one pass: 

Other than the few things pointed out above, it might be 'cleaner' to use a helper function to test primality. For example: 

There is more than likely a more elegant way to implement this using list comprehension. But conceptually, does this work for your needs? 

as you say, it does look like this code is doing exactly what you want. But it also looks like you are repeating yourself in a couple of places. If it was me, I would prefer to loop through the file only once and check for both and at the same time, and then only print out if the sample text is there: 

By testing for both, you avoid having to use and start looping through the file again. Of course, this is based on the sample input above - and assumes that there isn't multiple lines with 'Time taken' or 'Sample Text' in them. Hope this helps 

RECURSIVE SOLUTION: NOTE: it assumes that the input is already sorted. You could sort this at every call of the function, but this seems like unnecessary overhead. 

Now, I know you didn't want any "fancy-pants" coding, but this is actually really really similar to your iterative approach, just using recursion. If we look at each bit we have the following: : this tests whether the value is prime or not. Just like in your code, it takes the modulo of against all of the primes up to the square root of . This isn't too tricky :) The function gathers all of these tests and returns a boolean (True or False). If all of the test (from 2 up to sqrt(x)) are False (i.e., every single test confirms ti is prime) then it returns this finding, and we know x is prime. : ok, this is recursive, but it isn't too tricky. It takes 3 parameters: 

EDIT Added loadData(); slight tweak to return a dictionary of dictionaries, instead of a list of dictionaries: 

the rest just sets up the boundaries (i.e. the kth limit). So once you have seen a bit of recursion, you intuitively zoom in on the important lines and sort of 'ignore' the rest :) As a side note, there is a slight gotcha with using recursion in Python: Python has a recursion limit of 1,000 calls. So if you need to recurse on large numbers it will often fail. I love recursion, it is quite an elegant way to do things. But it might also be just as easy to do the function as an iterative function, or using a generator. 

Even if you address my concerns, I'm no cryptologist, so I would be very surprised if we can come up with a robust crypto system on this forum. I would strongly encourage you to look into one of the many libraries that provide the AES or a modern stream cypher like Spritz or HC-128 if performance is a constraint. 

I'd encourage you to use the operator instead of . The reason being it gives you a kind of duck-typing with collections: your code (at a source compatibility layer) stops caring about the specific implementation of the add like thing, and instead asks if the type implements a fairly common operator. The single most obvious advantage here is that it allows you to switch from to completely without (source-compatability) issue. 

I'm going to post a slightly different opinion; the previous answer is perfectly sensible but they aren't the changes I'd make. 

as an ANTLR fan, and assuming you dont have a raging hatred for auto-generated code, I'd be remiss if parser generators didn't get mentioned here. With a parser generator, you'd specify your desired syntax in a grammar file and the interpreter in a listener (or 'walker' or 'visitor') C# file. 

You will also have to write some 10-20 lines of boilerplate to get ANTLR to lex and parse the text, then run your listener over it. The two big wins in this approach are that you get to specify your language in EBNF (or an ASCII version of it), and you don't have to write the branching logic yourself. This makes the listener and grammar vastly easier to read and reason about (and have interns/newbies modify) than hand-written parsers. Another cool win is that ANTLR does a reasonable job at best-effort recovery. You can configure it to act differently, but by default it will coalesce the tree to a close legal approximation, and give you warnings (rather annoyingly pushed to std-err by default) when it does. then the nasty bit: you'd have to modify your build system to tolerate the generated code that comes out of your parser generator. The tools for doing this are reasonable on the java side (ant, gradle, and maven tasks all ready to go), but I don't know how they are in .net land. Further, for your situation, this is a pretty heavy-handed measure. If you can get away with writing your parser by hand and never touching it again, doing that is the best option. If you find yourself going back to that parser time and time again I would urge you to consider a solution involving a parser generator. 

I think this problem is of interest because pre-emptive concurrency doesn't seem like a good way of solving it -- most concurrency libraries do their best to help you avoid deadlock, not model processes that have got into deadlock. (But perhaps I'm wrong, and this is easily modelled with a standard concurrency library - I'd be keen to hear.) Also it gave me a good reason to look at some of the streaming data packages (conduit, pipes and streaming), all of which I believe are modelled around the idea of processes that can "yield" data "downstream", or "await" it from "upstream", which is exactly what this problem requires. Here's my code: (NB: contains possible spoilers for the Advent of Code 2017, day 18 problem, Part 2 - but this is not relevant to my question, which is about modelling deadlock with coroutines.) 

I'm modelling two processes which have been put into a cyclic pipeline - the output of each feeds into the input of the other - so as to work out when they've deadlocked (i.e., neither can progress since they're each waiting on output from the other). I've used the conduit package for this, as I couldn't see an easy way to do it using pipes, and streaming doesn't really look suited to this sort of task. It looks like monad-coroutine would be another possibility for this, but I didn't investigate it further. Define the problem as follows: 

(Possibly a hand-coded loop might be even faster than , but I didn't check that.) I'd also suggest splitting out the parsing of input from the actual solving of the problem - no advantage is gained by putting everything into . I'd further suggest that writing makes your intent clearer than ; and that you use s instead of , since this will (a) get rid of extra bits of cruft in the code and (b) allow you to use the package, which already contains a binary search function. It's usually far better to use someone else's de-bugged search routine than write your own. However, if you are submitting for an online competition, that package may not be available. Nevertheless, it provides useful inspiration on how the search may be sped up. This gives code along the following lines: 

This may not be as concise as your code, but should be much easier to follow and to modify - and importantly, for your purposes, it runs in less than half the time. Other notes - the versions of will normally be faster than the boxed versions, if you can use them; if you're submitting code for review, then I'd think a few more comments in your code wouldn't go astray. I hope that helps.