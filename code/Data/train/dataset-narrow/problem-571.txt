Let's step back a little: you're basically switching from a SQL Server that someone else manages, to managing your own server. Q: What are the security challenges involved with running your own server? You can get hacked, or you can incorrectly set permissions to let strangers access your data and publish it. Here are some resources to get you started: 

In SQL Server, what does the Perfmon counter "Workload Group Stats:Suboptimal plans/sec" measure? It's one of the Resource Governor counters, so it's measured at the workload group (which you have whether you configure RG or not.) I'm not asking what makes a good plan or a bad plan, but what specifically does that counter measure, and where do you go to find the exact corresponding plans? For example, I can't find anything in Extended Events that would seem to match to a query triggering the "Suboptimal plans/sec" counter. 

Look at the execution plans. SQL Server will build different execution plans based on the different number of rows that it expects to return. (Even though the ACTUAL number of rows that may be returned could be the same, the ACTUALS could be different.) To get the actual execution plans in SSMS, click Query, Include Actual Plan. Then you can compare the different plans to see if they have different shapes. If you're using SQL Server Management Studio 2016, you can use the Compare Showplan feature to make that easier. (It works even for querying 2008 boxes.) If you can share the execution plans publicly, use PasteThePlan.com to upload the two different execution plans, and then folks can point out what's different between them. (Be aware that this tool is completely public - everyone will see the full text of your query, and your parameters.) 

That is the equivalent of shrinking. To prove it, check the box and click the Script button at the top to see the shrink commands that it produces. Generally speaking, that’s a great learning tool too - whenever you want to learn what something in the UI does, try that script button. 

That was fixed. Upgrade to the current version, v5.3 via the Github repository or our site. (Disclaimer: I'm Brent Ozar.) 

Recovery model is changed by a specific ALTER DATABASE statement, and as far as I've seen, there's never been a bug that would change it. Some common sources of that statement include: 

It's possible, but not just with backup and restore. You've got three separate problems to solve: Problem 1: How do you delete all but one day at the office? You can do this with delete statements that touch every table in the system, but if you've got complicated relationships or lots of tables, this can be painful. You have to make sure you only delete tables with changing data, like sales, and not tables that need to be kept, like config tables and customer lists. If you've got large quantities of data, this can also be painful for the long-running deletes. Table partitioning can be a good fit for scenarios like this when you've got, say, >100GB of data involved. Problem 2: How do you get the data from office to home? You could solve this with backup/restore, but given that this is a solution you want to implement permanently, something like replication might make more sense. Transactional replication lets you stream the logged transactions to the second machine, and then your data can look different between the primary machine and the secondary machine. For example, the home machine might have different indexes depending on how much data you're trying to store. Problem 3: How do you keep adding the historical data at home? If you use the backup/restore approach, you'll be restoring a new database every day. The historical database should be under a different name, and then write scripts to insert the data from the restored database into the backup database. Again, more tables = more problems here. If you use the transactional replication approach, this will be much easier. All three problems are pretty large, so consider breaking this work up into more digestible steps. Short story, though, this isn't a built-in feature. 

No, as of 2018/04/25, you can't choose the filegroup where Query Store data is placed. In the databases where you enable Query Store, the data goes into the default filegroup. It was a highly upvoted Connect item, so other folks share your pain - so I would expect that to be an enhancement added in the future. Thanks to @klitzkrieg, here are the current feedback items: 

Then, when you get the list of roles, join to this table, and sort by ranking descending (or ascending, if you're looking for lowest privilege). 

Generally, for best results on Stack, you want to break each question up into its own question, but here we go: Q: When I run this command, is the data left unencrypted on the disk at any point? Not by the key change process, but be aware that filestream and replication data aren't encrypted regardless. For more details, check Books Online's section on TDE. Q: i.e. Does this command first unencrypt all the data using the old key/certificate and then re-encrypt it using the new one, which would mean that the data is unencrypted at some point in the process. No. For more details, see Microsoft's post on encryption key management. Q: Bonus question, what happens to the log file during the key change? Changes to the database are logged operations, so you'll need to watch the size and speed of your transaction log just like you did when you first applied TDE to the database. 

Beyond the Cost Threshold setting, SQL Server appears to treat parallelism differently for columnstore indexes depending on your SQL Server version (2012 vs 2014) and even the datatypes in your table. I'd start with Joe Chang's post benchmarking decimal vs float datatypes, and read the comments on that post as well. If you want to get exactly the right MAXDOP and Cost Threshold for Parallelism settings for your system, you'll need to perform the level of detailed testing that Joe does in his post, and that takes a lot of work. Because of that, I would focus on your system's primary bottleneck first - use wait stats to make sure parallelism or CPU pressure are problems for you, and then start by tuning the most CPU-intensive queries rather than making system settings changes. 

Even if it did, it would include things like backups, index rebuilds, statistics updates, etc. Not sure that’s be totally useful by itself. Zoom out a little - what’s the problem you’re trying to solve, and there may be a better answer to a different question? 

If you frequently use the query in the example (with TaskExecUpdateDate IS NULL), then you might check out filtered indexes. They're a new feature in SQL Server 2008 that allows you to put a where clause on your index, basically. $URL$ 

Use the one you're most familiar with. You're better off making smart decisions on a database platform you know well than making first-time-user decisions on a strange platform. All RDBMS's can scale to millions of records these days. If you're going to scale to the billions of records, you'll want to hire a database administrator and architect who's done this kind of thing before. 

DBCC CHECKDB isn't a good storage test. It does logical tests too, not just reads from disk - for example, it compares data between multiple indexes on the same table to make sure they all have the same values. These checks consume CPU cycles. If you want a better pure storage test, consider setting an artificially low buffer pool number and running multiple simultaneous SELECT COUNT(*) queries across multiple large tables with no nonclustered indexes. 

It might be that someone accidentally created objects in master, and they're being queried. These results will help you figure out the mystery. Enjoy! 

Try running sp_Blitz, a free health check for your SQL Server (disclaimer - I'm the author). You can also run it with @OutputType = 'markdown' if you want to share the results here at Stack. I've got a hunch that you're running into THREADPOOL waits, and sp_Blitz alerts about that. THREADPOOL means your SQL Server ran out of worker threads to service incoming queries. It won't show up in the OS or SQL Server error logs. When it's happening, you'll be able to connect to SQL Server using the Dedicated Admin Connection (DAC) (disclaimer: that's a blog post on my site.) The DAC is a set-aside CPU scheduler used just for emergency troubleshooting. From there, you'll be able to see which queries are burning up all the worker threads - typically it's a blocking problem. 

The challenge here is that numbers don't take into account end user experience. Great example: I have database server used to track every web site that the company employees visit. I don't care if it can't keep up with inserts during peak loads because the front end app batches the inserts off periodically, and slow inserts don't cause a problem for users. Users are still able to surf the web without being held up by slow inserts. At SELECT time, the HR department just fires off reports when asked for suspicious browsing history for a given employee, but they don't care how long the reports take - they just open the report and go off to do other things. Performance needs to start with a question: are the users happy with performance? If so, leave the system where it is. 

So to start the discussions with your management, lay out the differences in just one of the complex tables, and white-board out what would happen if they had to fail back from the new to old schemas. Talk about the ways you could lose data, and how much work it would be to build apps that would migrate the data back and forth. 

There's nothing stopping you from doing that - unless you specify something about the table that enforces uniqueness. 

That's going to result in long downtimes for a 200GB database. To shorten it, you can use tools like Red Gate's Data Compare or build your own comparison tool to detect what changes have been made to your database. For example, on one project, the developers added LastUpdated timestamp fields to all the tables, and maintained those with triggers. Then, after our restore, we could shut down the apps, copy across the specific records that had been updated, and go live with only a few minutes of downtime. Q: What time zone should I use for my EC2 servers? Just as RDS was configured with UTC, so should your SQL Servers. Just set your EC2 boxes to be in UTC time zone regardless of their location, and then you won't have problems when you fail over from one region to another. 

Side note first - hmm, there's something a little suspicious in those results. They all say the same table, but at the same time, they say "NC indexes on table: 1". That means we think there's only 1 index on the table - but you're showing 4 here. These could be in different databases perhaps, or maybe there's a bug in the sp_BlitzIndex code. That aside, you have a couple of different questions in here. How do I know if I have a blocking problem? Look at wait stats, which tracks what your SQL Server has been waiting on. My personal favorite tool for that is sp_Blitz @CheckServerInfo = 1, @OutputType = 'markdown' because you can post the results here on Stack in your question. (Disclaimer: I work for the company that started those scripts.) One of the sections will show your most significant waits - focus on those first. If your primary wait types include LCK*, then you probably have a blocking issue. How do I find queries that are taking locks? This one can be kinda tough without monitoring software. I'd start with sp_BlitzCache @SortOrder = 'duration' That'll show you the longest-running queries, which can sometimes point out queries involved in blocking. You can't really filter by table, though, at least not quickly. How do I design the right indexes to support those queries? That's kind of a big question. Generally, you want as many indexes as you need to support your workloads, but not so many that delete/update/insert (DUI) operations slow down due to the wide number of locks they have to get. If your table really does only have one index, then DUI queries are probably doing table scans in order to find the rows they want to update. It's time to look at how you're accessing those tables when you do updates. For example, say you have the classic white pages of the phone book - organized by last name, first name. If you try to update all users whose first name is 'Brent', you're going to have to scan the entire phone book, and you can end up with some nasty locking situations. You'd want to create a separate index on first name so that you can quickly find the rows you need to update, and then get out of the table.