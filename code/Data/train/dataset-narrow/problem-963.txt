Concerning , the main problem is that the function isn't total. This means that for it will fail badly and unconditionally. If you declare the functions from and as named functions in a (or block), you will be able to check for as well in pattern matching. Also, you can combine several constructors in a pattern. There are two options how to fix : Either return some for , or remove such entries. I'll show an example for the latter option using : 

This one reads one or more digits, consumes if there is one, and converts the digits into a number. (Combinator runs two actions sequentially, but keeps only the result of the first one.) 

In addition to the other answers: If you want to have fast, imperative solution in Haskell, the way to go would be the [ monad]. You can use or even (unboxed, which means it's strict and memory efficient - no thunks): 

In we separate the logic that prints out the output. For each pair it receives it prints the line number and its content. Combining and running these conduits is then easy: 

You could also generalize to general monoids. That is, use to concatenate several values and (or its synonym ). This will give you the implementation of in : 

We're traversing the list at two different "speeds" at the same time in a tail-recursive loop, to get to the middle. During that we compute the reverse of the traversed part in . When we hit the middle after n/2 steps, we just compare the accumulated reversed first half with the rest. The combined length of and is always n and if we find out in the middle a difference, like in , we finish and release resources early. 

Probably it'd be possible to implement a lot of the operations as folds/zips on tuples, although if you care about performance, you'd have to check if it won't degrade too much. A small remark, for implementing the instance it's often better to implement . It doesn't matter much for such data type that converts only to short strings, but it's a good practice. 

It's better to base recursion at the length 1 or 0. It's usually trivial and reduces the chance of making an error. In your case, the code doesn't work for lists of length 1, and this can be easily fixed by setting the base case to . The costly operations in your code are repeated traversals of the input list. In particular, is called every time, and as lists in Haskell are lazy linked lists, it costs you O(n). You could pass the length of the list as another argument instead. Similarly and are O(n). You could use to traverse the list just once, or even better, rotate the other way around, something like where you need to traverse the list just once (for ) and pattern matching is also somewhat safer than using partial functions such as ///..., especially if you cover all cases and use . You might also consider using which has O(1) costs for manipulating its ends and O(log n) splitting/merging sequences in the middle, but has higher constant factor. Another source of inefficiencies could be the in the branch, as needs to traverse the whole left argument. You might again try out , or constructing the result using difference lists, which eliminates this problem. You could solve several of these problems by introducing a helper function that'd return all possible splits of an input list, something like 

The above functions allow creating modules and combining them together in various ways. Now one of the main functions is to step a module with no input/output, producing its next state: 

Similarly tihs second function reads a string until it hits , consumes the optionally and returns the string. Then it's easy to construct a instance. The parser library already has a handy function that converts a parser into a function: 

I'd like to add a completely different answer. The task can be very well solved with a concept called pipes or conduits (also iteratees). I'll give a solution using my experimental library called scala-conduit (disclaimer: I'm the author). A is a thing that can read input elements of type from one side () and emit output elements of type on another side (). When it finishes, it produces a final result of type . It decides on its own when it requests and when it responds. This is ideal for the task - we receive inputs and decide when to emit the cluster of equal elements. A pipe also knows when the input finishes. This was problematic with using s or other native Scala collections, because folding, mapping, etc. don't notify the function that it hit the end. We didn't know easily when to output the last accumulated buffer. With conduit it is easy, our pipe will get notified when it runs out of input. The solution would look like this: 

This function takes a file name and creates a - a conduit that takes no input, but produces output. It reads a file line by line and sends each line down the pipeline using . Using we ensure that the file will get closed no matter what happens to the pipeline. 

There are multiple reasons for separating the loop in an inner function. One (that doesn't really apply here) is that by having the top-level function non-recursive, it allows it to be inlined. The other is that this clearly expresses what changes during the loop and what not. 

It's somewhat more verbose, but separates the general properties of distributions (the instance) from the final computation, which then becomes trivial. Another advantage is that the allows to hide the internal representation, which can be potentially replaced by something else, while keeping the interface intact. 

Instead of having an explicit key function I used an implicit equality witness. After applying this function, we have a stream of the same length, whose elements are those sums. What we need to do is get exactly those which are followed by a shorter (or equally sized) sequence. We can do this by zipping the stream and its tail augmented with an empty sequence and filter out just those where the size of a sequence doesn't increase: 

is redundant. There is no need to distinguish this case, also there is no need to call on a temporarily created value. Other than that, it looks good. Just a remark, using with probably isn't very meaningful, monoids that provide useful information (depending on the use-case) would be , or . For testing, I'd suggest you to study QuickCheck. It allows you to define properties of your operations and automatically test them for various inputs. It's important to define good properties, in particular they should be distinct from the implementation of your operations. For example, you might want to test: For any s and 

For the former, repeated sorting: Let's consider, how would a human create such sequence. Probably she would keep two lists. One would some part of the sequence, and the other would be candidates yet to be added to the sequence. At each step, the smallest candidate n is appended, and 2*n+1 and 3*n+1 added to the candidates. Translating this into code yields 

One possibility to solve 2. and 3. is to use conduits. This may seem as somewhat complex subject, but the idea is actually very intuitive. A conduit is something that reads input a produces output, using some particular monad. This allows to break your program into very small, reusable components, each doing a single particular task. This makes it easier to debug, test and maintain. For example, your code could be refactored as follows. (First some required imports.) 

My suggestion would be to create a special result value (similar to Future) that would represent an asynchronous computation: 

Their combination is a module with no input or output, and stepping them prints the counter each time: 

Minor change in the definition: Instead of using , it's more natural to define it using , especially for trees. This can also have significant performance benefits in certain cases, like when using the monoid (untested). 

Processing of pipes is single-threaded and fully lazy. A pipe is processed only if its downstream pipe requests an input and only until it produces its output, then it's suspended again. And if a pipe terminates (means stops receiving input), its upstream pipes are notified and any registered finalizers are executed. For example we could read the source values from a file, process it with and do something with the clustered output. And if the right-most (sink) pipe decided to terminate early, the file would be automatically closed, without reading unneeded input. (We can't convert a pipe to an or to a lazy . We must run it and extract its final, definite output. The reason is that if extracted values lazily, we wouldn't be able to detect when no more values are requested and finalizers should be run. However this is not a big problem, we simply build the whole computation from s.) 

While the output should be a balanced tree, it seems it's not required to be ordered in any way (at least the given example there isn't). So it's definitely not a heap. And the solution is required to use , which yours doesn't satisfy. You need to split the task into two parts: 

However, using and makes the notation usually short enough so that we use the combinators directly without the need to define such helper functions. 

Not only they are very useful on their own, but they can be also used to easily implement other operations such as insert, delete, merge and intersect. The general idea is that when we want to do an operation on two trees, we can split the first one using the root key of the second one and perform the operation recursively on the subtrees of the second one. Splitting two s is more-or-less straightforward, but merging them is more problematic, as for merging we need to start from the bottom, not from the top, and we don't have information about heights of trees. We could traverse both trees until the bottom layer, unwinding the visited nodes into a heterogeneous lists that holds trees of increasing heights and then merge them backwards up. Or we could embrace this idea into the data structure itself and implement finger trees based on .