Also, feel free to vent about your experiences with tasks like these. I'm sure I'm not the only one who has been handed down tasks like these. 

I have a table that stores version information in multi-dotted strings, ie '4.1.345.08', '5.0.1.100', etc. I am looking for the quickest way to find out which is larger. Is there a faster, or easier way on the eyes than what I have come up with below? One caveat is that I am 90% sure that we should ONLY find dotted numeric values here, but I can't guarantee that in the future. 

Is there a best practice method to handle localized strings via a view? Which alternatives exist for using a as a stub? (I can write a specific for each schema owner and hard-code the language instead of relying on a variety of stubs.) Can these views be simply made deterministic by fully qualifying the nested s and then schemabinding the view stacks? 

That seems like a complex plan for a view that has less than a thousand rows that may see a row or two change every few months. But it gets worse with the following other observances: 

I want to SELECT or UPDATE to remove a row if the first or last (with id order) is (a given value). In other words, I want to skip the first and last row of a category IF . I cannot use as it is a table. 

Since, it is a temporary table, I cannot use . How can I from this table to have pairs of key/value in each row? 

In a project, we need to frequently the tables to delete or add columns. I wonder if this weakens the database performance in long term. If it has a considerable negative impact, what is the approach to keep the database healthy and efficient? I was thinking of the following approaches: 

is not enabled by default. Normally, it should be enabled by placing in . But it does not work for all installations. In my experience, it worked for Debian 7, but not Debian 7 minimal, though both installations come from the same precompiled deb package. Both on OpenVZ VPS. How to debug why does not work for an installation, and how to safely activate ? 

In this typical example, all three columns are . When the fails, I do not know which col caused the error to change its corresponding value. How can I make a query to change to upon duplicate error? 

Create a new String table populated with a cleanly joined set of data from the original base tables Index the table. Create a replacement set of top-level views in the stack that include and columns for the and columns. Modify a handful of s that reference these views to avoid type conversions in some join predicates (our largest audit table is 500-2,000M rows and stores an in a column which is used to join against the column ().) Schemabind the views Add a few indexes to the views Rebuild the triggers on the views using set logic instead of cursors 

It sounds like you need to add another column in your data table to account for the quantity of ingredient. In the example you linked, it appears that the site calculates how many grams each ingredient adds to the total sum of the final product. But it will be up to you to decide how best to generate these. For example, if your site is all about home made juices, perhaps you can come up with a formula to determine how potent or how much flavor a specific ingredient adds to the final product. But I digress. If you have an additional column for grams you could take the weight of one ingredient, calculate the SUM(Weight) of all ingredients in your recipe and divide the two together, like in this Excel example. 

I stumbled across this blog post, which while the author didn't have the exact same problem as me, our issues seemed similar enough that his fix might work for me to. I downloaded the SSDT-BI 2012 package and installed it, and lo and behold, things work fine again. I'll just not even consider that somehow things worked fine for a year or more without downloading and installing SSDT-BI 2012, so I might be able to retain some portion of my sanity. 

The thing is, none of the column really need to be LOBs. There's a few that are TEXT types, but could fit easily within a varchar(max). Even stranger, though, most already are varchars, but it seems anything over varchar(128) is being treated as if it was a LOB (in advance properties, the data type is DT_NTEXT). I event tried doing a manual SQL command where I explicitly casted every string type to a varchar of an appropriate length in the select statement, and they're still being set as DT_NTEXT in the ODBC source. I'm not a DBA, so it's entirely possible I'm doing something really stupid. I would just like to know the best way to ensure that the types end up as varchars so I can batch fetch. Any ideas? In case it matters, I'm using SSIS-BI 2014 inside Visual Studio 2013. 

Say for example, if a defect is reported in this spaghetti mess, for example a column may not be accurate in certain specific circumstances, what would be the best practices to start troubleshooting? If debugging this was a zen art, how should I prepare my mind? :) Are there any preferred SQL Profilier filter settings that have been found useful in debugging this? Any good tactics, or is it even possible to set breakpoints in nested procs when using debug mode? Tips or suggestions on providing meaningful debug logging when dealing with nested string builders? 

Here is why s are being used as predicates. The column is formed by concatenating: During testing of these, a simple from the view returns ~309 rows, and takes 900-1400ms to execute. If I dump the strings into another table and slap an index on it, the same select returns in 20-75ms. So, long story short (and I hope you appreciated some of this sillyness) I want to be a good Samaritan and re-design and re-write this for the 99% of clients running this product who do not use any localization at all--end users are expected to use the locale even when English is a 2nd/3rd language. Since this is an unofficial hack, I am thinking of the following: 

I need to build a table accepting one row for each minute. If I set the type of primary key to , 60 records can be added for each minute. Is there any trick to design a table to keep unique? 

I want to concatenate the objects if the sum of frequencies reach a value. Currently I am doing this in as 

is a big table, and I want to make the only if has found something. In other words, I do not want to make the for . 

Since, make the calculation for the entire column, apparently, I need to reset the value/process of after reaching the maximum item in folder. 

The slow performance for using in large table has been widely discussed in various blogs, and the most efficient way is to use an as 

I have scheduled heavy queries at administrator level, and since they are heavy s with multiple s, consume a huge amount of resources. I do not want them to stop end users from basic tasks when performing such heavy tasks. Is for this purpose? or I should follow another strategy? 

and id is a in 5 child tables. Is it possible to drop TABLE , and force drop of all 5 child tables having ? instead of dropping child tables one by one? 

This seems pretty rotten to me, but I only have a few years experience with TSQL. It gets better, too! It appears the developer who decided that this was a great idea, did all this so that the few hundred strings that are stored can have a translation based on a string returned from a that is schema-specific. Here's one of the views in the stack, but they are all equally bad: 

While profiling a database I came across a view that is referencing some non-deterministic functions that get accessed 1000-2500 times per minute for each connection in this application's pool. A simple from the view yields the following execution plan: 

I recently inherited a MESS of a search. It's around 10,000 lines of code in the procs/functions alone. This search is aptly named the "Standard Search." It's a proc that calls about 6 other procs, which are entirely composed of string builders, in which each proc has between 109 and 130 parameters. These procs also call deeply nested functions which generate more strings to be assembled into a final query. Each proc can join up to 10 views, depending on the logged in user, which are abstracted from the table data by between 5 and 12 other views per primary view. So I am left with hundreds of views to comb through. On the plus side, it does have a parameter to PRINT out the final query, (unformatted of course!) It's a hot mess. The string builders also have no formatting, and they don't logically flow. Everything jumps around everywhere. Although I couid do an auto-format, to pretty print the code, that doesn't help with the formatting on the string builders, as that would involve refactoring the content of strings, which is a no-no. 

How we can change two variables in one condition. For example, consider that when , we want to change not only , but also re-assign . In fact, regardless of the condition 2, we want to increase , if the first condition fails (the second part). Is it possible to re-assign two user-variable in a or we need to add another ? This will be equivalent to 

is a many-to-many relationship between tables and . In this query, mysql will find all posts tagged by any of these words, and order them by PRIMARY KEY. How can we order the results by tag weight: 

How can I reduce the number of subqueries, and count all values in one subquery, then updating all columns? 

I want to catch X rows, thus, I set ; but how can I simultaneously count the total number of rows too? Currently, I use two separate queries to do so as 

It is widely recommended NOT to use . I understand this, as it is not a good idea to bring the entire row into memory, when only a few cells are needed. 

There are many articles exaggerating (IMHO of course) the need for . I understand that with , there should be a better control over the individual tables; like backup each table separately. However, the claim for better performance is questionable. In my test, there is no difference in performance of and for a database of 60GB. Of course, it was a simple test with normal queries, and the situation can be different for complicated queries in real life (this is the reason that I asked this question). 64-bit linux with can effectively handle large files. With , more disk I/O operations are needed; and this is significant in complicated s and constraints. Tablespace is shared on single ; how dedicated tablespaces for separate tables can save disk space? Of course, it is easier to free table space for each table with , but it is still an expensive process (with table lock). QUESTION: Does has an effect on a better performance of mysql? If yes, why? 

I have a client agent application that stores a replica of user metadata for the entire organization in a or earlier database. It gets accessed fairly often based on user activity so low latency is appreciated by everyone. Normally this is all well and fine, but some organizations have to store multiple orders of magnitude more metadata, which would total around 10-300 million rows, instead of 1 million or less that a typical agent uses. The Jet DB runs on each client workstation, and must operate locally on aging laptops without consistent network access. Full user data syncs get pushed to it every 24 hours or the next time it can reach the servers upstream at corp, and metadata stored while offline gets replicated back to corp whenever convenient. I have a copy of the schema that works for the server side agent which should have not changed since it was first implemented back in the early 2000's, should have very similar data access patterns, and runs on SQL Server 2008 R2 and up. The server agent does a substantial amount of reads and writes in a RBAR fashion, so I am going to assume the same access patterns are used in both. If I profile the activity for a few hours on the server agent to generate some queries to replay activity to benchmark, how can I run it against a Jet database? Are there testing or development utilities that can create a new Jet database and populate it with a schema and data? I am wondering if there would be any benefit in pushing for the development team to upgrade the client database engine to one of those new SQL Server engines that get loaded up only when the application code needs it. Not sure what the best options are out there, or if there is either a useful benefit either in performance or memory consumption. The client agent code runs in an old version of Java, .Net, and C++, but I have not been able to determine which language the data access layer is written in. 

Anyone have any clue what's going on here? FWIW, I have tried closing all instances of Visual Studio and reopening my solution with the SSIS package. The same behavior persists. 

I'm working on an SSIS package in Visual Studio 2013. For any script component throughout the package, if I double-click on it and then click the "Edit Script..." button, Visual Studio 2012 opens but no editor window loads. It's just an empty window like you closed all documents. The Solution Explorer shows items, but notably "ScriptComponent" is an empty folder: 

Apparently this just boils down to SSIS treating any varchar larger than 128 as NTEXT. Not sure why. I can, however, go into the advanced properties of the ODBC source and change the types back to something like DT_WSTR. Which seems to work for the most part. However, I did determine that a few of the tables I'm dealing with actually are carrying upwards of 4000 bytes in some of the their TEXT columns, so I unfortunately have to leave those colums as DT_NTEXT to prevent truncation (SSIS won't let you set a DT_WSTR type with more than 4000 bytes). I suppose in these instances, I'm just stuck with row-by-row fetch, but at least I was able to fix a few tables. 

I use table for calculation of temporary data, but tables in has a big limitation, which made it useless for me. A table cannot be re-opened in a query. This mean I cannot the table with itself, and add sub-query. It is almost impossible to process data without and . Thus, I must give up table. A possible alternative is to create and delete a permanent table on each script run. However, I think it is not good from database administration point of view to and tables. Is there any practical alternative to tables with possibility of re-opening the table? 

as I want to update another table with these values, and with this row structure, I can update the country table row by row as I get from the above query. 

I need to make heavy statistical analysis to deliver some data to users. Currently, I catch the data from mysql and process them via arrays. However, mysql temporary tables are quite better (extremely more efficient and faster than PHP arrays; obviously due to their mechanism). My tentative solution is to form a temporary table upon each request (i.e. connection) to import and process the data. However, I am not sure if there is a drawback for creating many temporary tables simultaneously? Can this make problem for the mysql server? Or I can use it as an alternative to PHP arrays in numerous simultaneous requests?