An attacker would still have a copy of the certificate even if it was encrypted so it would be your duty to revoke it anyway. These days its far easier for an attacker to obtain a valid certificate for your site via social engineering than to steal a working copy of one. Certificates naturally expire making their attack surface limited. Host based security systems such as traditionally permissions and SELinux offer a robust means of protecting certificates on the platform. A certificate isnt a be-all and end-all of a secure system. There are many other aspects to consider such as the data you store, the media you store it on and the value and/or personal nature of the data. 

If I had to guess, I would suspect that the vast majority of HTTP requests when the server is "choking" is blocked waiting for something to come back from tomcat. I bet if you attempted to fetch some static content thats directly served up by apache (rather than being proxied to tomcat) that this would work even when its normally 'choking'. I am not familiar with tomcat unfortunately, but is there a way to manipulate the concurrency settings of this instead? Oh, and you might need to also consider the possibility that its the external network services thats limiting the number of connections that it is doing to you down to 300, so it makes no difference how much manipulating of concurrency you are doing on your front side if practically every connection you make relies on an external web services response. In one of your comments you mentioned data goes stale after 2 minutes. I'd suggest caching the response you get from this service for two minutes to reduce the amount of concurrent connections you are driving to the external web service. 

However because the endianness is not incorrectly handled, the following comparison is made instead. On A: 

So, he who has the biggest IP address will win. In keepalived, the way this is done is basically wrong. Endianness is not considered properly when doing this comparison. Lets imagine we have two routers, (A)10.1.1.200 and (B)10.1.1.201. The code should perform the following comparison. On A: 

This will give you a quite detailed outlook of the running of the system at ten minute intervals. I would suggest this be your first port of call since it produces the most valuable/reliable data to work with. There is a problem with this, primarily if the box goes into a runaway cpu loop and produces huge load -- your not guaranteed that your actual process will execute in a timely manner during load (if at all) so you could actually miss the output! The second way to look for this is to enable process accounting. Possibly more of a long term option. 

I got this to work in hfsc. I assume "X" in your example is 100mbit, but that could be anything of course.. The trick here is to create a tree class like so: 

Bring this up on the kernel mailing lists to see if this really is a bug, behaviour expected or nothing at all to do with what I'm saying. Request that linode set the 'available memory' on the system to be the same 1GiB assignment as the 'current memory'. Thus the system never balloons and never gets a Normal zone at boot, keeping the flag clear. Good luck getting them to do that! 

Confining what is in effect a programming language can be pretty tricky because of how robust the policy needs to be to work with many different web applications. 

It measures the CPU time spent performing a CPU intensive peice of work (spinning in a spinlock) in both a threaded model and a forked model, both at the same time using all the systems CPUs. Then reports the CPU statistics. My results show on a 4 CPU box: With autogroup DISABLED 

I believe in RHEL6 (before systemd anyhow) session management is tracked by . Try and see if that shows you anything. For completeness, on Fedora 16 and 17 this feature was deprecated in favour of which you can list sessions with using . 

initially creating the large file with then writing into it. Setting dirty_background_bytes much much lower (say 1GiB) and using CFQ as the scheduler. Note that in this test it might be a better representation to run the small in the middle of the big run. 

This will enable process accounting (if not already added). If it was not running before this will need time to run. Having been ran, for say 24 hours - you can then run such a command (which will produce output like this) 

So, overall based off of the information in the question I assume you have omitted something or made a typo. 

and basically read the data from . If you change the permissions of it to say 660 that will prevent users being able to read it. 

I think theres something odd going on in that policy of yours. If you check the audit logs, it says whilst the SELinux source context is correctly labelled as the target context is labelled as . This is despite what your policy says, that it should be . This means whats in the kernel and whats in policy dont match. The port is still 6379 though. You may want to check what you have configured for your as well as your . As far as I understand, port policy bindings can only have one label per port/protocol, so I suspect whats in your policy store does not reflect whats in your server presently. You may want to try doing a to rebuild and reload your policy to try to fix the synchronization problem. If no luck, search whats in the port listings for and update the question. 

As your varnish cache size is 10G it will never fit completely in memory, thus the following formula is relatively representative 

This is less to do with mysql and more to do with python. When you start a process, it inherits a lot of information from the process that created it, including its file descriptors. What your finding is that mysql inherits the listening socket your process created when it spawned. To resolve this, you need to change your python code to not use and instead use Alternatively, setting the CLOEXEC flag on your socket will prevent it ever being inherited at all -- albeit you need to test this behaviour works throughout all your code paths and nothing strange occurs because of it. The following is the basis for doing this. 

I think you will need to create multiple route tables. One is your default meant for normal traffic, the next one is your special SSH table which contains route entries only for the ISP connection you want to use. Next, setup iptables to mark packets which are from your IP and listening SSH port. Finally, you can setup ip rule entries to route based off of the firewall mark. Theres lots of info (but you might have already seen it seeing as your this far) in the LARTC documentation: $URL$ So, as an example: 

This answer functions in a way which avoids you having to manipulate the dirty_bytes/dirty_background_bytes system global which may affect other applications when not doing a backup. Its a bit of a hack to be honest, but I leave it in case its useful to you. 

Do nothing. Really, do nothing. If your purpose here is to spread your resources as efficiently as possible, the proper thing to do is leave the operating system to move the processes to relevant CPUs on demand and as necessary. 

This changes the behaviour such that when a primary IP is removed, it will not flush the remaining addresses and instead will promote a new IP address as the primary. 

Note its a 1900 sized byte length with a dont fragment option set on the packet. Typical MTUs tend to be between 1400-1500 bytes. Your probably getting packet too big ICMP messages back but your dropping all ICMP traffic inbound at the site A firewall. To test for this you'd have to do the packet trace on your firewall for icmp and tcp 22. Make sure you permit ICMP packet too big messages inbound at site A. Alternatively you could try setting the MTU on your Linux boxes at Site A to something under the size of your network MTU. I am hazarding a guess that on Fedora you have jumbo packets enabled but on Windows you do not. 

I assume you are attempting to request a username with a '' in it (probably an indication of someone thinking file globbing works on usernames) or responding messages coming out of ldap contain a user with a '' in it. You could either attempt to resolve the offending username or alter the regex such that it accepts '*' as a valid user name. Note that doing this may lead to unexpected problems, especially in shell scripts where '*' is used as an expression if its not properly escaped or quoted. 

Is the most concise way to do this. Edit: Found a more concise way. This will not confuse stray "$:" that exist in GECOS fields, for example. 

So, to start this creates a 512M file that is the basis of our virtual block device which we will punch a 'hole' in. No hole exists yet though. If you were to you'd get a perfectly valid filesystem. So, lets use dmsetup which, using this block device -- will create a new device which has some holes in it. Here is an example first 

This worked for me, given it added the entry to the later files and the 'last regex won' when I did that. It actually changed the regex to this in the file: 

Now, it could be that something is wrong with the process, such as the other end is expecting a response from you first before sending more data, or a previous response from the other end anticipates SVN to do something else before requesting more data. Suppose for example an error response came back which should force the client to resend some information. You cannot fix this gracefully because its impossible from the information you have to determine what the sender of this data is expecting you to do. However there are a few possible ways to avoid the problem and report it. 

It will support that many rules, but you really wouldn't want to traverse a chain of 4500 rules. As @Zoredache pointed out you could also binary split the chains. If you did it perfectly you could drop the number of chain traversals to 13. The simplest way to do this is with ipsets. I am using EL6 which provides support for this. Obviously I dont know all the chinese netblocks so I'm just filling this with garbage.. 

This will mean that your problem (emulate buffered I/O on a non-buffered write) will be resolved. That is because the new 'limit' on your FIFO will in effect become the speed of whatever utility is writing what is in the pipe to disk (which presumably will be buffered I/O). Nevertheless, the writer becomes dependant on your log reader to function. If the reader stops suddently reading, the writer will block. If the reader suddenly exits (lets say you run out of disk space on your target) the writer will SIGPIPE and probably exit. Another point to mention is if the server panics and the kernel stops responding you may lose up to 64k of data that was in that buffer. Another way to fix this will be to write logs to tmpfs (/dev/shm on linux) and tail the output to a fixed disk location. There are less restrictive limits on memory allocation doing this (not 64K, typically 2G!) but might not work for you if the writer has no dynamic way to reopen logfiles (you would have to clean out the logs from tmpfs periodically). If the server panics in this method you could lose a LOT more data. 

Bear in mind, trusting third party sources for your IP might be problematic especially if what your doing with that data has special meaning. A more trustworthy way is to pick a known, trustworthy DNS server (ideally running DNSSEC) and query the hostname of the box with it, providing your DNS server contains such entries; 

OK, so lets go through each bit. Active memory is regions of memory that get thrown to the top of the LRU stack (basically get called a lot). Inactive memory is stuff thats not being used a lot and is a swap nomination should memory need to be swapped. Free is genuinely free memory About 40Mb. What gives? The clue is in these lines: 

You only give a small amount of CPU time to this process when other processes are busy. If there is nothing on the CPU or a CPU is idle your task spends 100% of the time on it. All the processes in the process group inherit the same niceness. 

I have done this very recently to push an update out for a few thousands clients. Yes, you need a custom installer to do this. The 'silent' install is not quite so silent and is prone to erroring, particularly where an existing deployment already is available. Another big problem you'll have is silent installs will generate a 32 byte random string as the password, the standard installer offers no easy way to avoid this. To solve this I created a silent installer which took arguments to set a specific director and password. I also squashed some of the bugs pertaining to existing deployments being upgraded. All in all - without altering the windows installer to do this it will be pretty difficult to do this without invoking some other scripting language to alter text files. 

I've done a bit of research into your problem. Not easy but looks feasible. The area of code breaking you is this (well, in newer kernels): 

The 'Cached' entry is counting the number of pages marked as 'shared'. The mappings given are marked as shared. The kernel internally sees no difference in memory that is set with the shared flag and a file thats simply been catted by the operating system and stored in cache (all files in cache are effectively shared mappings). The fact this is backed by /dev/zero is inconsequential. The reason they are shared is almost certainly because the same memory data needs to be made available to all child processes that modify the data. From a semantics perspective it behaves like normally allocated memory (or anonymous memory) given there really is no usable file you can evict pages to (/dev/zero is a not really a file you can save into) and the pages would get moved to swap if they were not in use. So - confusingly - the data accounts as 'cached' but actually is treated like anonymously backed memory. You can see this exactly is the case in your meminfo: 

The memory on your router is split up into zones, one zone -- HighMem contains 128MiB of memory, the other zone, Normal contains 128MiB of memory. The zone are functionally independent of one another. The memory manager works in each zone in an isolated way. oom-killer is invoked in the normal zone. You can practically eliminate 128Mib of memory when you are looking at your request. Now to the actual anomaly: 

Many kernel level rootkits can be effectively squashed at attack time by enabling the kernel command line . $URL$ 

Each line should report the same result, certainly not a second out. You should try it a few times to make sure you just didnt cross a 1 second threshold during the test. The bug I came across here was that one processor was 1 second out from the other processor. This lead to a condition where mysql would ask for the time, then compare the time again with the next new request (which was -1 second away). Given this is unexpected it would underflow and mysql thought the connection was four billion seconds older than what it was. If this is the problem, you should change the clocksource on the host from jiffies to , or and the problem should go away.