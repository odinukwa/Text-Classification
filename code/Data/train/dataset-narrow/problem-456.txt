Ran in like one second At the database the query runs in like one second This is just a data volume thing getting data to the client. 22 minutes seems too long but that is the problem to solve. 

I don't like to use plural for table names Most people would expect a table to have multiple rows If the properties (columns) of character are the same if they are in a movie of a show then just one 

15K in 5 minutes 50 / second - that is reasonable The likely culprit is page splits and index fragmentation Give this a try On the insert sort by the order of the PK on the insert Put a fill factor of like 50 on all the indexes Try an insert If this work then tune the fill factor and add in index maintenance Maybe even tune the order of PK to put those with a more common insert sort early - like if one FK is a customerID and you load by customerID then have it early If it does not work then go back to your prior fill factors You might also try tablock holdlock Some times it is faster just to take a tablock get your business done and get out If you have lock contention you can create a test table or test when usage is low 

Since no one is advising 3 tables I will advise 3 table Join on pk to indexed fk is very fast OP said would search details some times It would avoid dual updates of the data (if in two tables) 

If it works then you can just make two stored procedures where you pass in the 5 conditions for the base select and the syntax becomes pretty simple. Materializing is the safe path. But use #temp over table. An index on [IntField] may help but with only 3 unique values it may not help. Even with materializing make some stored procedures to reduce syntax. 

There is no constraint on that table to stop multiple or Put a PK or unique constraint on worker_name, in_out 

First txn_status should be an tinyint with FK and index Do you need bigint for amount I trust there is an index on date And consider 4 different queries In the three below the first was 86% of the cost Case was an index scan Where is an index seek Assign current_time to a variable so current time is the same for all The older transactions are going to be in pages most likely not use by inserts You would only get contention the newest - make that short and sweet This was tested on MS SQL-Server 

this needs a lot more information they are going to have registry, church, and a LOT of other stuff you need to read a book on database design or do some tutorials as your design is not very good starts with weddings plural they are not going go want a separate database for each wedding if everyone needs to logon then you need one table for that 

If you change the fill factor before then the rebuild will build in space. That is probably what you want to do. If you only want new insert to have the fill factor then do it after the rebuild. 

I think this will do it I changed this answer as I may have read the question incorrectly Look at edit history if you are looking for something different as for the fk 

just 9 joins if ID is indexed the join will be efficient if you have gaps in ID that needs to be compressed then this does not work 

I suspect this is a duplicate but I cannot find it Typically another table and even some definitions state another table Repeating values is the norm for a FK FK point to the PK of another (or the same table) is the common configuration. In MSSQL it can be a column(s) with a unique constraint (and not PK). The row must be uniquely identified. Here is an example of a single table Assume each employee has exactly one boss The person at the top works for them-self BossID would be a FK in the same table Yes BossID would repeat if more than one person worked for that boss 

this is the revision - not 100% sure about it but (maybe) give it a try even one OR is probably going to be a problem this would break on ActiveDirectoryUser null 

above find the violations if you just want the good then flop the where and drop the group by and min from the question it is not clear what you want to do with this counter and you don't seem to address the counter in the task as for the task 

If so I am not seeing the value here That is a messy join to avoid an ass_users_events table And you lose declarative referential integrity How is the first causing the ORM to do unnecessary work? Dedicated tables are performant and sustainable 

Still give them access to the View for those that need current data and are willing to pay the price. 

A transaction like Kenneth Fisher answered And look at optimizing Why are you updating after the insert? Why not just insert a hard coded TargetAmountSetTable.isModified of 1 in the insert? Another option is to output the whole lot of joins once to a #processTable 

Do it at the application level Even .NET only goes down to ±5.0 × 10−324 Multiple by 10^300 and insert After you select you can scale If you are dividing then you don't need to scale A * B would be A * B * 10^300 * 10^300 and you are going to be outside the range You might be better off with BigInt, SmallInt with SmallInt for the exponent 

Loading a file on each request is not an option. That is around a TB of data. Many conventional data bases (e.g. MSSQL will handle that). Clustered index on min_latitude, max_latitude, min_longitude, max_longitude may be the way to go. Since you don't have any relationships a document db may be the way to go. Consider a Cloud solution that can quickly scale. In don't know much about big data. Hopefully you will get more answers. 

With one table you would need to limit it to a single row. I think a trigger on InUse would be a better solution. 

try this but I am not getting this o.created_at it should be in some type of aggregate do you mean max(o.created_at) ? 

This is a bit of a rant but I had an application that could have 100 people making edits and loading over 100,000 rows during working hours. It was smoking fast. I only gave them a next prev and they insisted on a page number and I told them page number was typically only valid for a few seconds when the system was active. They said but our old system did. I told them your old system took a static snapshot of 100,000 and a user did not see any updates. The said we need a goto page so I did it. Sure enough a user blew up when goto page 60 was not the same after lunch. I just take the values of the sort for the last row and then fetch the next X. 

I think the Notes table is missing a userID FK column (and put an index on it) You need to identify the owner of the Note The UserNotes table does not need an ID just: 

If the is what the book says then it is misleading at best At least in MSSQL 2012 The insert (col1, col2) is just index based The first item in the select goes into col1 The data type must be the same The select stands on it own and is not aware of column names in the insert Example1 would only be valid for just need some data and don't care Just need some data for test This would only be valid is there is a col1 in table_name2 If that is the case it is a terrible example as many people would assume it means table_name1.Col1 

Looks like the delete itself is what is taking the time. You should test disable, delete, and then rebuild the indexes. I know you said you tried a join but this would be a quick test 

The query planner is more efficient with #temp. On a table variable it only considers the first few rows. Your table variable (and #temp if you use one) would likely benefit from declaring a primary key. Put a key on #AutoData and only populate with necessary rows. Sort by key as you add rows. I suspect below can be optimized with a row_number() 

Just trying to made some sense of this. Odd a trx_id_fk can have more than one SEARCH_NUMBR but SEARCH_NUMBER is not reported. For that matter trx_id_fk cazn have multiple account_FK and account_FK is not reported. OP it is not clear what you are trying to do. I am not sure this is what you mean to to.