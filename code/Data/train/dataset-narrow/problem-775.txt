What you are planning is called side-by-side upgrade. It means that wholly new system is installed. Its alternative is in-place upgrade. It means that only the Sql Server is upgraded on existing computer. As you are going to upgrade the operating system too, side-by-side is the only option. Microsoft has released a white paper about how to upgrade Sql Server. The pdf has 400+ pages, but not all parts are likely to be relevant to your case. Read the documentation and ask for clarifications about things you didn't quite catch. As a side note: though migrating from Sql Server 2000 to 2008 is certainly an improvement, you really should migrate to more recent version. That is, Sql Server 2014 or 2016 should be your migration target. Be aware that Sql Server 2000 is so old that it cannot be migrated directly to Sql Server 2012. An intermediate step into Sql Server 2008 is needed. 

This happens (based on the discussion in comments) because the tables exist in the system database. As per the documentation (my emphasis): 

Sql Server updates are incremental. As long as the service pack level isn't changing, there is no need to install any intermediate updates. In your case, SP2 CU2 can be patched directly to SP2 CU8. There is no need - or use - to install CU3 - CU7 before the CU8. Release notes are not cumulative. This means that to see all the changes between CU2 and CU8, you need to read the release notes for CUs 3-8. Service packs are similar, RTM can be upgraded to the desired SP without installing other SP version(s) before. Edit: As per the comment, a replication setup needs special steps for update installation. This is documented in Microsoft's support documentation and depends on topology. 

You'll need to put a trigger on the siteArea table so that whenever it's updated or inserted, it'll call a stored procedure with an argument representing the new site area and that procedure will calculate the needed values and insert them in a table. Your question is too ambiguous for me to describe further. 

Here's the weird thing, the following query DOES work, the only difference being whether rating needs to be larger than or smaller than 1. Suffice it to say that I'm utterly perplexed. Is this a bug? I'm on 11.2.0.4.0. Could it be something to do with JARO_WINKLER? 

I've been using sysdate to time procedures, but a lot of my procedures are very small procedures that get called thousands of times. Is there a way to get milliseconds elapsed instead of seconds? I know it has something to do with timestamps, but I can't find any exact way to do it. 

Sorry if the way I phrased that is confusing. I'm writing a program that acts kind of like a filter - it retrieves an answer to a query, counts the rows, then I want to do a slightly more narrow query. I want to do those queries until I get a count of 0, which means the query just before it is the narrowest I can get. I then use a similar version of that query, except instead of counting I actually retrieve the records. The best implementation I know of is to just write all of the different queries, put them into different statements and feed those statements into different result statements, and retrieve the counts from there. Then, I use the last one that counted > 0 and use that main query without a count, and use those results. This seems like a horrible implementation. It seems like I'm just going to be bombing the database over and over. I'd prefer to hit it once, and then narrow the resultset on the client. Any ideas? 

This creates multiple smaller files with 400MB each. This works fine. But is there any configurations I can change to keep the connections alive for large file? I tried to set and and , but the connection still lost after 10-15 minutes. 

Inherits: messages There is a problem when I try to query parent messages. A parent message can have multiple child messages that posted at different date. I fetch the child messages using the date range, and select the from the child messages. Then I try to fetch the parents messages, but I don't have the date range for the parent messages, the parent messages can be outside the child messages' date range. What I can do is add a in the table. But updating all the existing messages will take very long. Also it's not ideal to add parent info in the child messages. Is there any better way? My question maybe more related to the design of the DB, should I use partition for this case or maybe partition differently? EDIT: Here's some example rows: I want to all 2015 May messages. So I selected [3,4,5,6]. But [5,6] are referring to [1,2]. To select the May messages, I have the date range, so it fits the check contraints, but after that I have to find the [1,2], the problem is I do not know the parent's posted_at, I only have the ids. So it will scan through all the partition tables. Maybe I should not partition by posted_at, instead I should partition by feed_id? 

I am returning top 1000 is because currently and is not 1 to 1, same can have like 1 to 5 s, so just return 1000, and then group by later to approximately get all the counts for the same . But after this, I cannot figure out a way to append the average score from table a. Currently the data in table ab, for each , there can be 1 to 1000000 or more s. 

Consider using Sql Client Aliases to abstract the data source name. The aliases are set on client computers by either adding keys to registry or by running (for x86, x64 systems) and (for 32-bit applications in x64 system). A Sql Client Alias is quite a bit like the additional DNS record you described. What's more, aliases support instance names too, a feature that DNS record won't do. After an alias has been created, configure the client to use it instead of real Sql Server instance. Should you ever need to change the server, just modify the alias. As a caveat, there seem to be some applications that won't work well with aliases. These often are legacy ones, like those using ODBC or proprietary database drivers. For modern applications, aliases will work just fine. 

Be aware that there are a few catches. For example, sequence values are reusable, are not unique by default and can contain gaps. Pay attention to the Limits part in the docs. 

By splitting the delete in batches, you should lock way less data than with a all-encompassing single-pass delete. YMMV, depending on the selectivity and other activity in your table. In addition, keep in mind that is logged and you might get a nasty surprise with your transaction log size. This article has nice an overview about how to batch delete data and mind the logs too. 

Develop the packages on development environment. Push those to the test and let production admins move packages into the production. 

Based on the comment thread, the first step is to update the system. That is, install Windows updates, patch the Sql Server and update all the device drivers and firmwares. On virtual machines (which you seem to have), the VM guest drivers and the host itself should be updated, but that's usually not DBA's domain. Lack of patching is often a red flag about more widespread infrastructure problems. Please make sure that the system is fine in other ways. That is, you have a working restore plan [1], service accounts, passwords and user rights are documented and so on. To illustrate the patching effect, SentryOne has a handy list about the number of fixes included in service packs and cumulative updates. Just SP2 fixes some 133 issues, of which "only" 53 are public ones. Reading through all the release notes about if the problem is mentioned is going to take a while. As a side note: Microsoft's support policy is restrictive. One needs quite strong a business case (read: lots of money is at stake) if unpatched software problems are looked into. The advice usually is akin to "install all the patches and see if problem occurs again". [1]: "Working" means you have tested recently enough that the backups can be restored and contain the stuff they should. It's all too common that backup process looks successful, but doesn't quite do what was intended. 

Turning the original select into a table is not an option. Ideally for readability, I'd like to not copy/paste the select; I'd rather alias it somehow but I feel like my syntax for that isn't quite right. Here's my actual code. It is a horrible mess (actually if you have any suggestions for making it less of a mess that'd be great). It gets on the top layer - the layer below that works fine (producing the first table above, essentially) 

Very weird question: Is there any way to artificially inflate the amount of time a query will take? An infinite loop would be great. 

I want a PL/SQL procedure to give me progress updates as it runs. However, DBMS_OUTPUT seems only to give me the output when the whole procedure is done. Is there a way to make it give me updates during runtime? Thanks! 

So, I have an insert statement in a Java program I'm writing. Under some conditions, I want it to insert some values as null. However, before I can execute the statement, I have to set all of the tokens to values. How can I make it set something to null instead? Edit: Example: 

I'm using UTL_MATCH's Jaro Winkler similarity function and it seems to be performing well. However, I would like to adjust the prefix scale according to the situation. Is this possible? Is it possible to see what the default prefix scale is? I could not find any documentation on this, but it seems that in order to be a J-W distance, it must use a prefix scale. 

Are there any other probable causes for disabling the BPE than human error? There is a KB article about I/O errors that might cause loss of BPE, but the current patchlevel should include the fix, and there are no log entries about I/O errors anyway. The tempdb files have been autogrowing around the same time, but there was plenty of free space on the SSD. The server is up-to-date with OS (Windows Server 2012 R2) and Sql Server patches (SQL Server 2014 SP1-CU6 X64 Enterprise Edition). 

It is very poor practice to misuse reserved words. Frankly, messing with multiple periods , the table/column separator operator, you deserve any hardships coming to your way. There is no need for funky table names, barring some odd edge cases. The DB should be transparent to applications anyway, so stick to sensible naming. Escape the inner quotes. As how to do that, depends on your shell. The error message looks like *NIX shell, so try backslash . Like so: 

This very much sounds like a policy. From technical point of view, there is no reason why direct connections would not be possible. That being said, production admins are responsible for the production environment not going haywire, so this kind of restrictions are quite common in the industry. The current buzzword DevOps is pushing this limit a bit, but on more traditionally oriented houses there is little if any change. The reason why production staff are quite strict about the access limits is simple. If something breaks in the production, it's their fault. Even if the real culprit was a developer running a cross-join-o-matic, dropping indexes and generally being a clown. Production admins are supposed to have the environments running at top performance, and letting those down for any reason is a sign for problems. For a solution, talk with your team lead and boss. Write a memo that outlines the pros and cons the new method would introduce. Don't just list the obstacles, tell what's good too and propose solutions. The memo should give an impression that you are a problem solver, not a whiny complainer. After that, it's your boss' call what to do.