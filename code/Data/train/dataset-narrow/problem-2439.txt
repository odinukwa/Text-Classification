If a node has fewer than $n^{1/3}$ neighbors, then add all edges incident on that node. From your remaining nodes, greedily select the remaining node with the most neighbors. Declare this to be a "cluster center," and then temporarily set aside this node and all its neighbors. Repeat until you're out of nodes. Let $C$ be the final set of cluster centers. There is a proof in the paper mentioned above that guarantees that $C$ is only $O(n^{2/3})$ big. Add to $H$ the edge from each cluster center to each of its neighbors. For each $c, c' \in C$, add to $H$ a single weighted edge from $c$ to $c'$ of weight $\delta_G(c, c')$. 

This is intended as a comment, but it's too long to post as a comment. You might also be interested in graph spanners or emulators for your purposes. A spanner of a graph $G = (V, E)$ is a subgraph $H = (V, E')$ with few edges, but approximately preserved distances. An emulator is a graph $H = (V, E', w)$ whose edges are allowed to be weighted. The best result for spanners is $O(n^{4/3})$ edges and an additive error of +6 on distance estimates in the graph. The best result for emulators is $O(n^{4/3})$ edges and an additive error of +4. It is not known for either if we can beat $O(n^{4/3})$, even if the error is allowed to be polylogarithmic. If this sounds useful, I can try and dig up the relevant constructions for you. 

By the claim, the total distance traveled (in all rings) is at most $$\sum_{i=0}^\infty 10 (0.99)^i ~=~ 1000.$$ Obviously the constant factor here is loose. For example, if the bug travels in the first ring at an angle of 89 degrees or more, this immediately kills almost half the points in the disc of radius 1 (not just the points in that one ring). 

Assuming the given problem is feasible, the algorithm returns an $x$ such that $Px\le 1$ and $Cx\ge 1-O(\varepsilon)$. The number of iterations is $O(m\ln(m)/\varepsilon^2)$, because each iteration increases some constraint by $\varepsilon$, and this can happen for each constraint at most $N$ times. The proof of correctness is via the invariant $$\mbox{Lmax}(Px) \le 2\ln(m) + (1+O(\varepsilon)) \mbox{Lmin}(Cx).$$ The invariant implies $$\max Px \le 2\ln(m) + (1+O(\varepsilon)) \min Cx.$$ At termination the left-hand side is $\Omega(\log(m)/\varepsilon)$, proving the performance guarantee. In Step 2.1, the desired $j$ must exist as long as the original problem is feasible. (This is because, for any feasible $x^*$, and any $x$, if we were to choose a random $j'$ from the distribution $x^*/|x^*|$, the expected value of the partial derivative of Lmax$(Px)$ with respect to $x_{j'}$ would be at most $1/|x^*|$ (see the previous proof sketch for Set Cover). Likewise, the expected value of the partial derivative of Lmin$(Cx)$ with respect to $x_{j'}$ would be at least $1/|x^*|$. Thus, there is an $j$ such that the partial derivative of Lmax$(Px)$ with respect to $x_{j'}$ is at most the partial derivative of Lmin$(Cx)$.) Then the invariant is maintained in each iteration because, by the choice of $x_j$ and $\delta$, and the smoothness of Lmin and Lmax, increasing $x_j$ to $x_j+\delta$ increases Lmax$(Px)$ by at most $1+O(\varepsilon)$ times the increase in Lmin$(Cx)$. Learning (following experts / boosting) One reference for understanding this connection is Adaptive game playing using multiplicative weights, by Freund and Schapire. Here is a quick summary to give the technical idea. Consider the following repeated game. In each round $t$: 

A +2 spanner on $O(n^{3/2})$ edges A +4 spanner on $O(n^{7/5})$ edges, due to Shiri Chechik A +6 spanner on $O(n^{4/3})$ edges 

If I understand right, you are interested in finite projective planes. These are the maximum combinatoric set systems in which all sets intersect each other exactly once. To briefly answer your question, you can "improve" your construction by getting up to $\Omega(n)$ sets of size $\Omega(n^{1/2})$ and still enforcing the property that any two sets intersect on exactly one element (note: this only works for certain $n$, e.g. $n$ = square of a prime). I assume you have some unstated restriction on your sets that rules out the construction $\{1, 2\}, \{1, 3\}, \{1, 4\} ...$ in which case your sets have size two but all pairs intersect on exactly one element? 

Input: A set of $n$ points in $\mathbb{R}^3$, and an integer $k \le n$. Output: The smallest volume axis-aligned bounding box that contains at least $k$ of these $n$ points. I'm wondering if any algorithms are known for this problem. The best I could think of was $O(n^5)$ time, loosely as follows: brute-force over all possible upper and lower bounds for two of the three dimensions; for each of these $O(n^4)$ possibilities, we can solve the corresponding $1$-dimensional version of the problem in $O(n)$ time using a sliding window algorithm. 

Here are loose lower and upper bounds. Fix $d \le n$ as in the post. Let $k^*$ denote the largest possible value of $k$ meeting the conditions in the post. We show that $k^* = \exp(\Theta(d\log d)$) for $d\ge 3\sqrt n$, but $k^* \le 2$ for $d\le n^{1/3}$. I suspect the latter bound can be strengthened (e.g. shown for larger $d$). Lemma 1. $d!/{n\choose d} \le k^* \le d!$ Proof. The upper bound $k^*\le d!$ holds simply because each chosen permutation induces a distinct permutation on the $d$-subset $\{1,\ldots,d\}$. For the lower bound, say that two permutations of $[n]$ conflict if, for some $d$-subset $T$ of $[n]$, they induce the same relative order on $T$. A given permutation $\sigma$ induces a relative order on $n\choose d$ such $d$-subsets. Each $d$-subset $T$ and relative order on $T$ is induced by $n!/d!$ permutations of $[n]$. It follows that any given permutation $\sigma$ conflicts with at most $c = {n\choose d} \times n!/d!$ other permutations. So, any maximal set of pairwise non-conflicting permutations must have size at least $n! / c = d!/{n\choose d}$. $\diamond$ Corollary 1. If $d\ge 3\sqrt n$, then $k^* = \exp(\Theta(d\log d))$. Proof. By Lemma 1 and calculation from Stirling's approximation, $$k^* \ge d!/{n\choose d} = \frac{d^{2d+1}\exp(d^2/2n - 2d+O(1))}{n^d} = \exp(\Omega(d\log d))$$ for $d\ge 3\sqrt n$. For the upper bound, note $k^*\le d!= \exp(O(d\log d))$. $~~~~\diamond$ Lemma 2. If $2\le d\le n^{1/3}$, then $k^* = 2$. We give two proofs. Proof 1. Consider any $d$ with $2\le d\le n^{1/3}$. To see that $k^*\ge 2$, note that (as Zihan noted in his comment) for $d\ge 2$ every $d$-subset is distinguishable by the identity permutation and its reverse. To finish we show $k^*\le 2$. Fix any three permutations $\sigma_1, \sigma_2, \sigma_3$. Assume WLOG that $\sigma_1$ is the identity permutation: $\sigma_1(k) = k$ for $k\in [n]$. We show that some $d$-subset of $[n]$ is not distinguishable by some pair of the three permutations. If $\sigma_2$ has an increasing subsequence of length $d$, then that subsequence induces the same permutation on its $d$-subset $T$ that $\sigma_1$ does, so $T$ is indistinguishable by $\sigma_1$ and $\sigma_2$, and we are done. Otherwise, by the Erdős–Szekeres Theorem, $\sigma_2$ has a decreasing subsequence of length $n/d \ge d^2$ on some subset, say $T'$, of $[n]$. On $T'$, $\sigma_1$ is increasing and $\sigma_2$ is decreasing. Applying the theorem to $\sigma_3$ restricted to $T'$, $\sigma_3$ has either an increasing subsequence of length $\sqrt{d^2} = d$ on some $d$-subset $T''$ within $T'$, or a decreasing subsequence of that length on some $d$-subset $T''$ within $T'$. In the first case, $\sigma_1$ and $\sigma_3$ don't distinguish $T''$. In the second case, $\sigma_2$ and $\sigma_3$ don't distinguish $T''$.$~~\diamond$ Here's an alternate proof of Lemma 2, by adapting the proof of the Erdős–Szekeres Theorem, for those who are interested in the details. Proof 2. Consider any $d$ with $2\le d\le n^{1/3}$. To see that $k^*\ge 2$, note that (as Zihan noted in his comment) for $d\ge 2$ every $d$-subset is distinguishable by the identity permutation and its reverse. To finish we show $k^*\le 2$. Fix any three permutations $\sigma_1, \sigma_2, \sigma_3$. Assume WLOG that $\sigma_1$ is the identity permutation: $\sigma_1(k) = k$ for $k\in [n]$. Assume for contradiction that every $d$-subset $T$ of $[n]$ is distinguishable by each pair of these three permutations. For each pair $i, j$ with $1\le i < j \le 3$, define $I_{ij}(k)$ to be the maximum size of any subset $T\subseteq [k]$ that contains $k$ such that the order imposed on $T$ by both $\sigma_i$ and $\sigma_j$ is monotonically increasing. Define $D_{ij}(k)$ to be the maximum size of any subset $T\subseteq \{1,\ldots,k\}$ that contains $k$ such that the order imposed on $T$ by both $\sigma_i$ and $\sigma_j$ is monotonically decreasing. Define 3-tuple $$s(k) = \big(I_{12}(k),\,I_{13}(k),\,D_{23}(k)\big),$$ and $S = \{s(k) : k\in[n]\}.$ Claim 1. For every $k,\ell$ with $1\le k<\ell \le n$, we have $s(k) \ne s(\ell)$. Before we prove the claim, note that it implies the lemma as follows. The claim implies that $|S| = n$. Each of the three values in any 3-tuple $s(k)$ is in $[d-1]$ (because, for example, there is a subset of $[n]$ of size $I_{12}(k)$ that is not distinguishable by $\sigma_1$ and $\sigma_2$). So $S\subseteq [d-1]^3$, so $n = |S|\le(d-1)^3$, implying that $d>n^{1/3}$, contradicting that $d\le n^{1/3}$ and proving the lemma. To prove the claim, consider any $k,\ell\in[n]$ with $k<\ell$. Consider the relative order that each of the three permutations induces on the pair $k, \ell$: for each $i\in[3]$, either $\sigma_i(k) < \sigma_i(\ell)$ or $\sigma_i(k) > \sigma_i(\ell)$. Since $\sigma_1$ is the identity, $\sigma_1(k) < \sigma_1(\ell)$. First consider the case that $\sigma_2(k) < \sigma_2(\ell)$. By definition of $I_{12}$, there is a subset $T$ of $[k]$ that contains $k$ such that $|T| = I_{12}(k)$ and $\sigma_1$ and $\sigma_2$ are monotonically increasing on $T$. So $T' = T\cup\{\ell\}$ is a subset of $[\ell]$ that contains $\ell$ such that $|T'| = I_{12}(k)+1$ and $\sigma_1$ and $\sigma_2$ are monotonically increasing on $T'$. It follows that $I_{12}(\ell) \ge |T'| > I_{12}(k)$. It follows that $s(\ell) \ne s(k)$ in this case. For the case when $\sigma_3(k) < \sigma_3(\ell)$, by the same argument $I_{13}(\ell) > I_{13}(k)$, so $s(\ell)\ne s(k)$ in that case. In the remaining case, $\sigma_2(\ell) < \sigma_2(k)$ and $\sigma_3(\ell) < \sigma_3(k)$. By a similar argument $D_{23}(\ell) > D_{23}(k)$. So $s(\ell)\ne s(k)$ in all cases, proving the claim. $\diamond$ EDIT: Comment on the tightness of the proof of Lemma 2. The proof of Lemma 2 shows the existence of a $d$-subset $T$ which is indistinguishable by two of the permutations and that has the following stronger property: the two permutations are both increasing or both decreasing over $T$. Given this, I believe the bound of $\Theta(n^{1/3})$ on $d$ is best possible. That is, I believe that for, say, $d\ge 2n^{1/3}$, choosing appropriate random permutations w.h.p. yields $\exp(\Theta(d\log d))$ permutations such that, for every pair $\sigma$ and $\sigma^*$ of the permutations, there is no $d$-subset $T$ on which $\sigma$ and $\sigma^*$ are both increasing, or both decreasing. (The argument is similar to the proof of Lemma 1 above, with the additional observation that, for a random permutation, w.h.p. there are only about ${n\choose d}/d!$ $d$-subsets on which the permutation is increasing or decreasing.) So, to strengthen the lemma, the proof would have to somehow consider indistinguishable subsets on which the two permutations are not monotonic. 

If I understand the problem properly, the answer should be $2 \log_2(n)$: make a graph with $\log_2(n) + 2$ layers, where the first layer is just $A$, the last layer is just $B$, and the intermediate layers have $2$ nodes each; we then put a directed edge from each node to the node(s) in the next layer. We then have $2^{\log_2(n)} = n$ paths from $A$ to $B$, and the union of these paths contains $2 \log_2(n)$ intermediate nodes. Does this fit the constraints of your problem? 

I found one related consequence. Let's say $NEXP$ contains $DTIME(2^{O(t)})$, where $t = n^{\omega(1)}$. It turns out this is just enough time to diagonalize against $P/poly$. Specifically, build the following machine: On input $x$ of length $n$, consider the $n^{th}$ Turing machine $M$. For every possible advice string of length $t$ and every possible bitstring $b$ of length $n$, run $M$ on $b$ with advice $a$, and reject after $t$ steps if you haven't accepted yet. Record your results in a table. This procedure runs in $DTIME(2^{O(t)})$. On input $0^n$, if at least half the advice strings cause $M$ to reject, then instead we define it to be correct for our algorithm to accept (otherwise, it is correct for our algorithm to reject). Any advice strings that caused $M$ to get $0^n$ wrong (that is, at least half the advice strings) now get thrown out of the table. We then repeat the process on input $0^{n-1}1$: if at least half the surviving advice strings cause $M$ to reject, then our algorithm will accept (and reject otherwise). Continue like this for all inputs of length $n$ (although really, only $t$ of them are needed - after that many inputs, we have thrown out all possible advice strings). Clearly this language can be decided in $DTIME(2^{O(t)})$, which we have assumed is in $NEXP$. On the other hand, it cannot be in $P/Poly$: the set of length $n$ inputs diagonalizes against the prospect of $M_n$ being used to decide the language. So we get $NEXP \not \subset P/poly$, which would be interesting. I'm going to leave the question open in case someone comes up with something else. 

Upper bound of $O(\log N)$ I only sketch the proof. Fix any sequence of circles. We will argue that as $N\rightarrow \infty$, the total distance traveled by the bug in the first $N$ steps is $O(\log N)$. Assume without loss of generality that the first circle has radius 1. Fix an arbitrarily large $N$. Let $p$ by any point in the intersection of the first $N$ circles. Note that because of the way the bug moves, in each step that the bug moves it gets closer to $p$. First, consider steps where the following ratio is at least $1/\log N$: $$ \frac{\mbox{the reduction in the distance to } p}{\mbox{the distance traveled in the step}}.$$ The total distance traveled in such steps is $O(\log N)$, because the total distance traveled in such steps is $O(\log N)$ times the initial distance to $p$. So we only need to bound the total distance traveled in the other steps --- those in which that ratio is at most $1 / \log N$. First, we argue something slightly weaker: that the total distance traveled in such steps before the circle radius decreases to 1/2 or less is $O(\log N)$. (We show later this is enough to give the bound.) Consider any such step. Let $a$ and $b$, respectively, denote the locations of the bug before and after the step. Let $o$ denote the center of the current circle. Let $b'$ denote the point on the ray $\overrightarrow{pb}$ such that $|pa| = |pb|$: 

I'm told that, based on some implementation details of the proof of this theorem, this can be viewed as a continuous analogue of the claim that $\exists k \, \, \, P \subset SIZE(n^k)$. Sorry I'm not qualified to be more precise than this -- if anyone else has heard this idea, maybe they could help me out. 

The class $PR$ (primitive recursive functions) strictly contains the class $ELEMENTARY = TIME(2^n) \cup TIME(2^{2^n}) \cup TIME(2^{2^{2^n}}) \cup \dots$. You say "Perhaps NP-hard problems are solvable with primitive recursion but not as efficiently." This is true. Even the total recursive function model of computation does not carry a notion of time complexity that agrees with that of Turing Machines. To see a simple example, how do you define the predecessor function in primitive recursive terms? You'd have to do something like: use the unbounded search operator to find the smallest $y$ such that $S(y) = x$. That is analogous to computing $f(x) = x-1$ with the function: $i = 0$ $While \, (i + 1 \ne x) \, \, \, \, i++$ Which takes $2^n$ time. Obviously, there is a better way! 

The solutions in the proofs of Lemmas 1 and 2 are well-behaved, in that, for sufficiently large $n$, for each solution $P(n)$ that reaches position $n$ there is a position $k\le n/2$ such that only one pebble is ever placed at position $k$, and the solution decomposes into a (well-behaved) solution $P(N-k)$ for positions $k+1,k+2,\ldots,n$ and two (well-behaved) solutions $P(k)$, each for positions $1,2,\ldots,k$, connected by the pebble at position $k$. With an appropriate definition of well-behaved, let $V(n)$ denote the minimum pebble volume (the sum over time of the number of pebbles at each time) for any well-behaved solution. The definition implies that for sufficiently large $n$, for $\delta=1>0$, $$V(n) \ge \min_{k<n} V(n-k) + \max(n/2, (1+\delta) V(k)).$$ I conjecture that for every sufficiently large $n$ there is a well-behaved solution that minimizes pebble volume. Maybe somebody can prove it? (Or just that some near-optimal solution satisfies the recurrence...) Recall that $\epsilon(n) = 1/\sqrt{\log n}$. Lemma 3. For any constant $\delta>0$, the above recurrence implies $V(n) \ge n^{1+\Omega(\epsilon(n))}$. Before we sketch the proof of the lemma, note that it implies that any well-behaved solution that reaches position $n$ in $t$ steps has to take space at least $n^{1+\Omega(\epsilon(n))}/t$ at some step. This yields corollaries such as: