After the Data Munging, this is the most difficult task on a prediction model. However, in order to answer it, we need more details. What do you mean by the "best model"? Do you want accuracy and long training time? Do you need something really fast with lower accuracy? Something between the two of them? What are your features? Have you just taken them or have you created new features from them? In any case, I suggest that you should spend some time and read this perfect tutorial from Microsoft about Machine Learning. A part of the tutorial to understand what I mean: Regression 

With this way, I will avoid to make an offer to someone who would buy with the regular price. So, I am still in the brainstorming and trying to find a way for implementing the 1-2. Should I create two separate models to predict the 1) and then the 2) with the second model? Or should I join both in one prediction model? 

Let's say I have a new input with the original values (before rescaling). How can I use the above information to rescale all the features on the new object I have to predict the results? 

Data Science is not an algorithm to run on your data. It is the process that helps you answer a specific question. The key to be a data scientist is to ask the right questions. So, first, since you want to be familiar with machine learning, examine your data and try to understand what they can answer for you. Examples: 

When you use and in general many other pandas features, it doesn't update your dataframe. It creates a new, temporary dataframe. So, you either need to assign it back to your original dataframe or use wherever it is available, like: 

In general, Machine Learning algorithms handle volumes data. This doesn't mean that you cannot extract information from "small" data. Keep in mind: 

Models to use To avoid overfitting, you need to avoid complexity. As a result, you need a model that has as less of parameters as you can. For example, linear/logistic regression could be what you need. Naive Bayes and SVM might work well, but this is not a generalization. Cleaning up your data Together with the outliers, you need to be careful with the rest of the cleaning process. Noise can skew a lot your data. This is a nice blog post about "small data" and what you can do when you are working with them. Highly recommended. What to do with “small” data? (7 min read) About Outliers I recommend the answer from this Data Science Stack Exchange question. Short story. You need to examine the outliers one by one. There isn't any rule written in stone. But you need to identify if those outliers are just wrong input data or data that makes sense to be there, even as an outlier. For example, if you have the sales of an eshop per day and in your data you have a specific day where the sales are 10x more, you need to understand if that was an error or something happened that day to increase the sales (maybe a discount campaign or email advertisement). Also, it depends on what kind of problem you have. If your problem is an anomaly detection, then you don't want to remove the outliers. Another suggestion (I don't have any source to link) is to update the values of those outliers with the mean, mode or another metric from your data. If you have only a few rows of them, removing a dozen will make it more difficult. In general, this should be part of your model process. You try once with outliers, a few times without or with updated values and try to understand if it is underperformed or overfitted. 

Actually, there isn't any limit of what you can do, if you have the right data. I can remember a startup that was working on identify anxiety and suicidal thoughts on Tweets and Facebook status. First, they created a system where a user could raise a flag in case of a related Tweet/status about anxiety. They used those data to create a labeled dataset. If you have labeled data and you know the psychological intentions from them, then you can just train a model to classify all the other dataset to one of these labels. I am not aware if there is any public dataset with already labeled text, but you can either search for one or create your own. 

You don't need a prediction model for this. Maybe if you have had users' data. But without anything else, then you just need labeled data. Historical data that you know if it was a real accident or not. When you have your labeled data, then you can follow a process like this, which is still heavily dependent on the kind of your data. Start iterating on your labeled dataset and calculate the accuracy of a real accident's report for different combinations (5, 10, 15, 20, 25, 30 ... mins) and (1, 2, 3, 4, 5, 6, 7, etc users). You will have a 2D matrix. I guess, acting fast on an accident is important in your case. Set an acceptable accuracy and choose the combination with the smallest interval, above that accuracy. 

It's not easy to compare two RAMs with different frequency and latency, since both of them affect your performance with not the same way. The short answer is: 

Let's say I have an expertise in the domain knowledge of the dataset I am working on. I know that part of my dataset acts 100% differently than the rest. Also, it is straightforward to check if a row belongs to one part of the dataset or the other with a couple of simple if-else. Should I split my dataset upfront and create two different prediction models for the two parts of the dataset? Or should I keep one model and try to improve it? Then when I want to use the model for predictions, I check the input values and see in which category it belongs and I call the appropriate model for it. 

As far as the algorithms are concerned, I am not keen on choosing one, because it is popular. If you have done all the Data Munging and the feature selection, you have the 90% of the job and the algorithm that you will choose is 2-3 lines on the code (in case you are using a language like Python). What I usually do, is to check all the possible algorithms and evaluate their accuracy. Then I either use a combination of them or the one with the highest accuracy. 

The easiest and most efficient way to do it, is to use the One Hot Encoding ScikitLearn This is an example: 

ATM's code of the withdrawal. People use most of the time similar ATM in their daily routine. If you know the lat and long of their previous ATM, you can check if one of them is far away and combining it with the other features, you will increase your accuracy. Seconds spent on the ATM for each withdrawal. People tend to follow specific patterns when they withdraw money. If all of their previous data are similar on the spending time and then you see lower or higher time on a data point, you will be able to increase the accuracy of the model. Labeled data. In models like this, it is far better if you use supervised algorithms instead of unsupervised. Thus, I would seek for labeled data for fraud usages. This will also let you to calculate the actual accuracy of your model. Time between the two withdrawals. As I said before, people tends to follow patterns. An "anomaly" on this with a sooner withdrawal than the expected will also raise your accuracy. 

How well those notifications are spread on the period of 60 minutes? Do we have parts of this timeline that don't contains notifications and are the rest clustered in a specific time? Changing the random functions in the implementation, how the above answers will be affected? 

When I have a similar question and I don't know which one to choose, I usually end up to 3-4 different algorithms based on the cheat sheet of Microsoft or the one from the scikit-learn, try them all and choose one or two of them with the best results. 

I have historical data from an e-shop transactions. I want to write a prediction model and check if a specific user will buy with or without a discount, so I can do some targeting offers. The idea is: 

If a user will buy the regular price, will not have an offer. If a user will not buy the regular price, check if he/she will buy with an offer. 

What you described is the Filter Method. One of the three methods of Feature Selection. The most common mistake that one could do is to make the feature selection as an independent step on your whole process and then decide which model he will use. The most appropriate way to do it is to include your Feature Selection process in your accuracy test and try different kind of feature selections. The best way for the feature selection is the domain knowledge. If you don't have it, then you start working with one of the three methods, like the Filter Method. What you need to do is to test the chisquare or mutual information on your features with your label column. This will give you which of the features have low effect on your prediction and you can remove it. You can find a briefly detailed article about Feature Selection here and here. 

On model settings, I used which uses the . Dataiku also exports this file with details about the rescaling: 

Ordinal regression: Data in rank ordered categories Poisson regression: Predicting event counts Fast forest quantile regression: Predict a distribution Linear regression: Fast training, linear model Bayesian linear regression: Linear model, small data sets Neural Network regression: Accuracy, long training time Decision forest regression: Accuracy, fast training Boosted decision tree regression: Accuracy, fast training