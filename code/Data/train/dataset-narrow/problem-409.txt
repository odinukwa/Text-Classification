I'm still working towards getting something others can try because, of course, a bug report is useless if I say that it only works on my data. UPDATE 3: I ran it again with different data and ran into the same problem with Cyrillic characters and . Making a table containing them didn't reveal anything. Same problem as with the Arabic text, I think. Something is getting stripped along the way. UPDATE 4: This is definitely a bug. I'm still trying to figure out how to report this. I found a workaround: 

I have PostgreSQL configured mostly how pgtune recommended, and the settings are supposedly conservative. PostgreSQL is using more memory for a single connection than the settings would suggest is allowed, causing my Linux system to run out of RAM. Here are my settings for a system with 64GB of RAM: 

Does it matter what order I add foreign key constraints to a table in, or what order I write them in a statement? MySQL clearly at least remembers the order; if I run the DDL statements below... 

... then we can see that MySQL remembered that the foreign key pointing to comes first in , but second in . It's possible to imagine this order making a difference; for instance, if MySQL, upon insert into a table with foreign keys, takes out locks on referenced tables in the order that the foreign keys are declared, then simultaneous inserts into and in my example above would have the potential to deadlock, and so it would be a best practice to ensure that foreign keys were always declared in the same order on all tables (e.g. by alphabetical order of the referenced table name). Do any such considerations exist in reality, or does the order of foreign key constraints have no effect on anything? 

RES is ~42GB for that one. Combined with those other two processes, that makes ~64GB of RAM used, so something is probably page faulting. Shouldn't it be less than or equal to 32GB per process? I thought maybe the was not included in (documentation doesn't explicitly state it), but that couldn't take 10GB of RAM by itself unless I had 26 joins at once, and I have at most 4. So I'd expect at the very most 34GB RES for one process. What am I missing here? 

This should never fail, right? It fails: violation of unique constraint due on key . And it has worked many times before with other data sets containing millions of rows in PG 9.3; I don't know whether was in the data back then. I know Arabic has decorations you can put on the letters, so I wonder if that's tripping it up. Does anyone have an alternative explanation, or should I report this as a bug once I can reproduce it more easily? UPDATE: Confirmed that the query runs successfully on a PostgreSQL 9.3 server with the same data. There are some moving parts here, so I'm trying to find exactly what the problematic strings are so I can make a simple list of queries anyone can run to expose a bug. UPDATE 2: Argh, I can't get my database to give me a set of strings I can copy into a table and expose a bug. I've been trying to do it with . Something along the way keeps stripping the Arabic text of the differences that are making it fail, I think. But I tried a simpler query, and it's also failing. It's more obvious that this should work: 

Since MySQL 5.6 introduced online DDL, the command can optionally have either or specified. The overview of online DDL notes that, by default, is used wherever possible, and implies (without ever quite stating it) that the algorithm is cheaper than the one is. So what reason would I ever have to specify on an statement? 

This seems to be a bug that was introduced in MySQL 5.6.10 and has since been fixed (although there is no bug report on $URL$ that I can find). The behaviour described in the question does not occur in the latest version of MySQL. Observed behaviour in MySQL 5.7.5-m15: 

The documentation doesn't shed any light on which of the above models MySQL uses. If #1 is true, then our plan will work fine. However, if #2 is true, then trying to replicate queries from the master to the slave is going to go wrong in some way if one of the columns involved has a different character set. For instance, once we've converted our tables from latin-1 to UTF-8 on the slave, the slave might try to store latin-1-encoded strings passed across from the master in a UTF-8-encoded column, resulting in either the stored text not being validly encoded in UTF-8 or representing the wrong characters. The MySQL docs on Replication and Character Sets shed little light on the problem; they discuss potential issues resulting from differences in the global character set of each server, but don't touch on potential problems caused by differences in table or column character sets at all. Is our plan sound, or are we at risk of ending up with corrupt data on the slave? 

I have a PostgreSQL 9.3.7 database scheme representing a directed graph of vertices and edges with some many-to-many relations. I'm only ever interested in getting all vertices that a given vertex points to or getting all vertices that point to a given vertex. For each many-to-many relationship, I could simply give each of the two tables a bigint[] (array) column to represent what the object points to or is pointed from. When adding an object, I would go through all its bigint[] columns I'm inserting and append itself to the corresponding bigint[] columns (inverse relations) in other tables. Edit: Actually, I have some other many-to-many relations that need to be flexible, so I use separate tables for those. But I am guarenteed that for the ones in question, I only need the queries described above. This seems more efficient than the conventional approach of making a separate table for the relationship, plus it's less complicated. But I've never heard of anyone doing this, so I'm worried. Would such an approach have any flaws that I'm not considering? 

I upgraded my browser (Firefox) to the latest version 37.0.1. Thereafter, OEM 11g is not accessible. Here are the errors: 

Based on the readings, my I/O thread is doing not too bad. Catching up with master log at masterA-bin.000117. But the SQL thread is lagging behind. . SQL thread is replaying at . But when I check my OS directory /log, I can see slaveB-relay-bin.001341 as the latest generated. The slave is continuously lagging behind master. I have set 

I do not wish to skip the slave error as I need these tables to be in the new slave. Any thoughts to remedy this please? 

Not sure if anyone has a workaround or solution. I am sure ver 37.0.1 has changed the https behavior. 

The values are stored as unicode character sequences (i.e. the binlog knows what encoding they are encoded in sends that information to the slave so that it can convert the values to match the character set of the target column on the slave.) or... The encoded values are just stored as byte sequences, with no record of what the character set was. 

How (if at all) can I get around this and add a constraint while permitting concurrent DML? In case it's relevant: my exact MySQL version is 5.6.16 and my SQL_MODE is . 

I'm trying to use MySQL's online DDL feature to add a constraint to an InnoDB table without blocking writes to the table. According to manual, this should be possible - the linked table summarising MySQL 5.6's online DDL capabilities has a 'YES' in the Allows concurrent DML column of the Make column NOT NULL row. However, when I actually try this, it fails under absolutely all circumstances I've tried. Even if the column I'm trying to add the constraint to does not contain any values, I get the following strange error: 

I copied .frm .myd .myi files for MyISAM tables from server to . Then I did a mysqlcheck on DB and repair tables whenever necessary. All is good for MyISAM tables in the new_prod. What happen is I have a slave configured to replicate from the . Error from slave, 

But it helps a little only, reducing probably 1-2 seconds of lag. I am using STATEMENT binlog, will it matters if some tables does not have primary/unique key? I'd to know if there are other means to speed up the replication without multithreaded slave? 

This transaction is processing like 0.5s for every update. The total rows count for the table is just under 100,000 rows. I am not sure if my expectation is right that it should be fast. I turn on general_log to capture for several seconds and switch off back. Excerpt: 

We have some huge tables in our MySQL 5.6 database that have latin-1 as their character set, and want to convert them all to UTF-8 (along with converting the database's global character set). The necessary statements take, between them, about an hour to run. We'd like to minimise our downtime. Our plan is to set up replication, make the encoding changes on the slave database, let the databases sync up, and then briefly take down our webserver and point it at the slave database. Will this work, or is there a risk of it causing data corruption in the slave? My concern about this stems from there being two obvious possible approaches for how string values (that are being ed or ed) might be stored in MySQL's binlog and sent across to the slave. Either...