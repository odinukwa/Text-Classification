What's worng with me? (If I changed the dvec4 in the above shader to vec4, the triangle was drawn, but maybe it's not double anymore. I want to use double in shader.) 

Supposing a behavior function $C(x_1, \ldots, x_n)$, then we have a scalar potential energy function $E = \frac{k_s}{2}C\cdot C$ where $k_S$ is stiffness constant. Hence, the force is as follows $f_i = -\frac{\partial E}{\partial x_i} = -k_sC\frac{\partial C}{\partial x_i}$ I caught up so far, but couldn't understand when adding damping. The paper read $f_i = (-k_sC - k_d\dot{C})\frac{\partial C}{\partial x_i}$ , where $k_d$ is damping constant and $\dot{C}$ is derivative of $C$. How to derive the damping part? I know damping equation is $f_d = -k_dv$, In detail, supposing there are two end of points in a spirng, called $p_a, p_b$. then the damping force $f_d = -k_d\frac{\dot{l}\cdot l}{|l|}\frac{l}{|l|}$ in 3D sapce ($\dot{l}$ is $v_a - v_b$, $l = p_a - p_b$) However, I could not correspond this equation to the above damping part. 

Let's suppose the start point of the ray $R$ is $(x_0, y_0, z_0)$ and direction is $(x_d, y_d, z_d)$, then $R = (x_0, y_0, z_0) + (x_d, y_d, z_d)t$ , and the plane is $ax + by + cz + d = 0$. To check intersection between the ray and plane, calculate the below equation. $t = \frac{-(ax_0 + by_0 + cz_0 + d)}{ax_d + by_d + cz+d}$ If $t \ge 0$, then there is a possibility of intersection, so go second step. else there is not intersection. 

I just want that if I click the item, the white space item(current is 'None') is converted as selected item. 

After binding the data to the vertex buffer object as follows, how to modify the buffer data? (Each subdata size is 3$*$ data size (three elements per index) 

I'm using GLUI for user interface. I tried to add Listbox with several items. I could add items like below image, but if I clicked the one of them (None, AAA, BBB). The program was stopped. It looks like accessing NULL data. 

I have a spectral power distribution (SPD, 5 nm steps) for all light sources in the scene and SPDs for the reflectance of all surfaces under any light / viewer angle in question. I'm going to calculate in floating point precision and not clamp any values. Except for pathological cases like monochromatic light or special things like dispersion effects, does it make a difference whether I render using the spectrum, XYZ color space or linear RGB color space? Note that I will convert the result to sRGB as a last step in any case. Specifically, is it invariant at which point (before or after multiplication of surface BRDF with light intensity) I integrate to convert spectrum to XYZ? Does the introduction of reference white in XYZ / RGB distort the colors when working with light sources of a different color / SPD? 

By using smooth shading (i.e. interpolation of normals), an object can have a smooth look despite low polygon count. A nasty artifact that can happen when casting shadow rays is the Shadow Line Artifact. Since shadow rays originate from a certain position and don't usually take the normal into account, a complete face on a smooth object will either be shadowed or not, revealing the hard edges again: 

Now with the trend going towards physically-based rendering and thus material models in engines such as Frostbite, Unreal Engine or Unity 3D these BRDFs have changed. For example (a pretty universal one at that), the latest Unreal Engine still uses Lambertian diffuse, but in combination with the Cook-Torrance microfacet model for specular reflection (specifically using GGX/Trowbridge-Reitz and a modified Slick approximation for the Fresnel term). Furthermore, a 'Metalness' value is being used to distinguish between conductor and dielectric. For dielectrics, diffuse is colored using the albedo of the material, while specular is always colorless. For metals, diffuse is not used and the specular term is multiplied with the albedo of the material. Regarding real-world physical materials, does the strict separation between diffuse and specular exist and if so, where does it come from? Why is one colored while the other is not? Why do conductors behave differently? 

Hnece, we can get the $\alpha, \beta, \gamma$. If $0 \le\alpha, \beta, \gamma \le 1$ and $\alpha+ \beta + \gamma = 1$, then there is intersecton. else No intersection!. (The page 19, 20 is explaining how to calculate the 'Area'. (Actually, we can project the 3D space points to 2D space for performance, but avoid the perpendicular projection!) ) 

In this case, I wonder how to work fourth work group. In my thought, the fourth work group have overflow problem because the total number of points is 900. Do we need to deal exceptional case(overflow)? For example, if idex > total num, then 'pass' Or The Opengl deals this problem automatically? (ex, There is no problem situation.) I tried to check by myself. Even though I make overflow situation, there was no warning. 

In detail, supposing I want to change the second vertex element to (1.0f, 1.0f, 1.0f). I know it with compute shader, but I just wnat to know how to do it without compute shader or OpenCl. Is it possible to change the buffer data uploaed directly without uploading the whole data again? 

I'am curious about compute shader in OpenGL. Let's assume the number of points (vec4) is 900 and the work group size(= the number of work items) is 256 Then, We would have four work groups because 900/256 = 3.xx, so need to plus 1. The code is as follows: 

I'm doing a project with assimp. I got confused with the weird situation for me. I think the both code are exactly same, but the result is different. Why the codes act differently? (vertex shader) 

Why sending data from gpu to cpu is slower than cpu to gpu? I heard the relation is similar to network situation, in detail, the upload is slower than download. However, I could not understand the meaning. In my opinion, the sending operation should be same because the two sending buses (cpu to gpu and gpu to cpu) can be made with the same performance (I mean bendwith). 

Keep in mind that Game Engines may be a viable way for 2D applications too and can make your life a whole lot easier. 

I'm in the process of writing a Ray Tracer using DirectCompute / HLSL. First, eye rays are generated (one per pixel). Then, rays are traced, shaded and reflected in a loop. Also, shadow rays for each light source are generated and tested for occlusion. As a scene structure, I'm using a KD Tree with Ropes$^1$. I have analyzed the shader with Joshua Barczak's Pyramid and it seems that I have a major problem with register pressure, mainly vectors. On a Fiji GCN architecture, the shader uses 85 SGPRs and 81 VGPRs, limiting the number of simultaneous wavefronts per SIMD to 3 (VPGR). Furthermore, I have implemented a simple counter in my shader, that atomically increases a value when a thread starts, decreases it when a thread ends and keeps track of the maximum number of shaders that were concurrently running. I've managed to get rid of some data and bring it up from 8192 to 9216, resulting in a correlated speedup of ~%13. When I use an empty dummy shader, I get 16384 concurrent threads. I've tried to get rid of excessive variables, especially vectors, by keeping their lifetime as short as possible, and also tried storing some variables in groupshared memory and reading / writing directly from there, all without any change in register count whatsoever. Are there any practical tips on how to take pressure of the registers? Is this even as much of a problem as I think it is? I have also thought about splitting my Ray Tracer into various kernels. This should, the way I understand it, take pressure of the registers significantly, but also takes quite some coding effort on my part. Does this sound like an idea worth trying? 

In the case of a convolution with a cosine lobe for rank 3 SH this results in a simple scaling of the bands with the factors 

I'm working with OpenGL and facing some difficulties because I'm not familiar with OpenGL. I tried to search related example in Google, but I could not find some useful code. There are five arrays. I bound and changed the data in compute shahder as follows: 

for better accuracy, instead of GLfloat, I bound double data and tried to use the data in shader as follow: (I just tested to draw single triangle.) cpp. 

I'm studying spring model. There is a suggested equation (Hooke's Law vector form) But, I couldn't understand how to derive that equation. I'm reading 'Computer Animation Algorithms and Techniques Third Edition. In the page 241, (7.88) The equation is as follows: ($|v_{1}^* - v_{2}^*|$: the rest length) ($|v_{1} - v_{2}|$: the current length) $F_s = (\frac{k_s|v_1 -v_2| - |v_{1}^* - v_{2}^*|}{{|v_{1}^* - v_{2}^*|}})\frac{(v_1 -v_2)}{|v_1 -v_2|}$ // I think the $k_s$ is typo. (I referred to $URL$ So, maybe $F_s = k_s(\frac{|v_1 -v_2| - |v_{1}^* - v_{2}^*|}{\color{red}{|v_{1}^* - v_{2}^*|}})\frac{(v_1 -v_2)}{|v_1 -v_2|}\cdot\cdot$(1) is correct. However, I wonder why the red part divides the equation? If I know correctly about Hooke's Law, $F=-k_s(current_{lenght} - rest_{length})\cdot\cdot$(2). It means that the red part should be removed like that $F_s = k_s(|v_1 -v_2| - |v_{1}^* - v_{2}^*|)\frac{(v_1 -v_2)}{|v_1 -v_2|}\cdot\cdot$(3). For example, in 3D space, there is a spring that has the spring term as $k_s$. The rest length is '3', so let's assume the left rest point, $v_2^*$, is fixed at (0,0,0) and the right rest point, $v_1^*$, is (3,0,0). And then we moved $v_1^*$ to $v_1 $=(5, 0, 0) with some force. We can calculate the force with (2), and the force is $-2k_s = -k_s(|(5,0,0) - (0,0,0)| - |(3,0,0) - (0,0,0)|)$ But if we use equation (1), the result is different. $F_s = -k_s(\frac{|v_1 -v_2| - |v_{1}^* - v_{2}^*|}{{|v_{1}^* - v_{2}^*|}})\frac{(v_1 -v_2)}{|v_1 -v_2|} \\\quad= -k_s(\frac{|(5,0,0) -(0,0,0)| - |(3,0,0) - (0,0,0)|}{{|(3,0,0) - (0,0,0)|}})\frac{((5,0,0) -(0,0,0))}{|(5,0,0) -(0,0,0)|} \\\quad= -\frac{2}{3}k_s(1,0,0)$ The force amount of $-\frac{2}{3}k_s(1,0,0)$ is totally different from $-2k_s$ Could you explain what I'm wrong?