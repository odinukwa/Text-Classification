I would never concatenate previous payments into one opaque string. I also wouldn't worry about performance yet: this is premature optimisation. If you do expect 100s of millions of rows, there are other techniques to improved performance. Otherwise, concentrate on data integrity and correctness 

Torn page usually. Do you have backups? DBCC may be able to fix it though The king of DBCC is Paul Randal: to avoid my copy/paste, read these from him 

An very blunt way of forcing a stats update... It may be that a full scan probably isn't needed, just up to date stats. Now, if you have enough data changes to trigger a auto update, then you probably have logical fragmentation too. I would try doing hourly on-line index builds on the offending tables/indexes used 

You can't hide a column dynamically unless you use dynamic SQL. This quickly gets complicated: what if the zeros are in different columns on different rows? What should the output be If the zeros/nulls are in the same column, then you can "hide" a column by not selecting it. 

tells SQL Server to discard the execution plan You have no further need to clear buffers or caches: you are testing a query, not the IO stack to read data back into memory. If you're tuning a query that runs frequently, then data will be in cache most likely in real life. If it runs Sunday 3am once, who cares...? And maybe you can't run these DBCC commands anyway unless you have sa rights... Edit, Jan 2012 For SQL Server 2008+, you also have the hint (also see this) which gives a more general query plan 

The name of the default could be user supplied or system generated. To find it and generate the SQL needed: 

Yes Service_account_name = DomaninServiceAccount = Domain user A service account is actually a domain user, just with some extra settings like "no interactive login" and "password does not expire". Dunno why they would use "DomaninServiceAccount" too. It's just Service_account_name Is it certain? Just have to test, but it looks usual for non-Windows applications (Atlassian does the same, and I have other Java apps that have AD authentication too) 

If all data access is via stored procedures, then permissions on the table are not checked of the same user (dbo probably) owns both procedure and tables. This is called ownership chaining This also means the no permissions are needed on the tables at all. You don't need to deny or grant anything on the tables because they won't be checked. Lack of GRANT means no rights anyway, so you don't need DENY Now, this means nothing to folk with elevated privileges: sysadmin, db_owner etc. These will always have rights. You can only use triggers to block these: but they can disable or drop triggers of course. I assume that your "end users" are not running as db_owner or sysadmin... 

Query results are not cached However, the source table and index data and metadata will be cached after the 1st use (subject to continued use, load and memory pressure though) That is, the results of a query will be evaluated every execution but the tables(s) (and any indexes etc) used by the query will most likely be in memory already. The compiled execution plan will be cached which is where the confusion comes from I suspect 

The query is run in DatabaseA, not DatabaseB, so I'd say by design. That is, SQL Profiler captures "what is the connection database context" not "In what database is the object am I accessing" This makes sense: each database is logically isolated from each other. Using a synonym or other cross database query is a special case because you can't ensure all databases are in synch (especially around restores) nor enforce foreign keys etc An other example... 

When I use Access, I use both techniques as some form of Command Query Separation (CQS) That is, the form can read the base tables but all writes are via stored procedures. In my case, I chose this for 

SET CONTEXT_INFO exists for SQL Server 2000 too. However, you have to query to read it back. was new for SQL Server 2005. 

Your sample code above is confusing (you have multiple parents for the same Code column) so I'll give you what I understand 

A boat and a car are separate entities: this means a separate table. There are cases where you want to maintain some kind of uniqueness of "Vehicle" where the non-shared attributes are in child tables and of the parent table. But in this case the simple solution is to have 2 tables. As soon as the attributes diverge you'll have refactoring of SQL code and the client to deal with separate stored procedures etc Edit: as mentioned in comments and other answers, we need more information. 

Yes, it's possible (as per JNK's answer: 2 overlapping MAX reads, both get 6, one fails with duplicate assuming a constraint) Use IDENTITY columns for this very reason. If you have some odd notion to not use IDENTITY, then you have to use lock granularity (TABLOCKX ) or semaphores (sp_getapplock) to restrict concurrency and allow only one process to run MAX at a time. Edit: Two requests that come in at the same time (or very close to it) run concurrently. They are both separate from each other but overlapping. So each process will read the table using MAX and both get 6. At this point, both processes and reading and the INSERTs have not started yet. The leading processes INSERTs 6, closely followed by the lagging process. The lagging process will get the unique key violation (you have uniqueness, right?) If the gap between processes is enough so that the 2nd read happens after the 1st INSERT you're OK. However, if you have enough calls that you have this risk of duplicate, then using locks/semaphores to decrease concurrency is madness... 

Very simple answer... Why not 2 separate tables? And UNION as needed or in a view when required combined? As I see it, a "Collection Note" and a "Delivery Note" are separate entities. At some point, the definition of each will be different. Are they really the same now anyway: I'd expect some differences? Also, SQL Server 2012 support SEQUENCE natively too 

If not an indexed view... You'd need a third table that contains DateCreated and a key on an IDENTITY and a SubTable column that contains 1 or 2 only. And an index on DateCreated This 2-column key becomes the key of the other 2 tables too. You use a CHECK constraint on SubTable to ensure an IDENTITY value is used in one table only. Each bit of the UNION is a JOIN using Table3. Then the index spans all DateCreated values as expected. The indexes you have now are per table: so SQL Server needs to sort the 2 sets of data. 

Now, this isn't going to be the best of queries. Consider full text search instead (I'm not too familiar with it) This OR could be split into a UNION 

The client library used to connect does not care at all about the compatibility level of the database. The code issued by the client may have some issues (for example, you needed 90 to use APPLY many years ago). But the mechanism used to send the code and get results does not care 

You can see this in SSMS in the Security nodes on Object Explorer. I suspect you have "orphaned" users. That is, the users in your restored database no longer have an associated login to map to. This need recreated (Windows logins) or remapped (SQL logins) The MSDN article Troubleshooting Orphaned Users has checks, fixes and more details. I'd simply copy/paste or re-hash this so I recommend you look at this. 

1. Separate tables That is, you can split off the separate languages into different tables. This allows table level collations rather than column level ones It allows allows more rows per page and more chance of in-row LOB storage PageParent 

There are some cases where the DataSet calls a SQL Server database that uses a linked server: but I suspect you aren't that far yet. If you think you need to use a "linked server" to call one database only to get data from another database, then why not call the other database directly. And stop thinking of linked servers 

I suspect the one of the values is NULL, so the whole expression concatenation is NULL. You'd need this: 

No code goes into the Data schema. No tables live outside the Data schema. This can extended of course to have a Lookup or Staging schema if you wish. For views we use but this was to distinguish them from tables in SQL Server 2000 and it's kinda legacy now 

Edit: Securityadmin allows someone to bootstrap themselves to sysadmin. Not good. Don't know how to workaround this at the server level For jobs, look at the "SQL Server Agent Fixed Database Roles" For maintenance plans, it looks like "sysadmin" only 

If you have foreign keys then it is impossible to use TRUNCATE TABLE and trickier to use DELETE. You'd have to have multiple passes of DELETE. Ditto for DROP/CREATE TABLE A DELETE with no WHERE is quite easy to do with your app uses dynamic SQL and some WHERE logic is removed with NULL concatenation. DROP/CREATE and TRUNCATE TABLE also require very high levels of privilege. Summary. One or more of: 

@Sankar Reddy's solution should work for parsing the XML but for performance I'd parse the XML outside of the database. I've seen problems before with high load parsing complex XML in the databases although I do use XML parsing but not for high loads. I'd pass the shredded data in as a table, using a table valued parameters for SQL Server 2008+ or a temp table. At least, separate the XML parsing into one stored proc that calls another that stores into the database. This way you can see any INSERTs with the final INSERT that would be masked by the XML parsing that tends to take 98% of a query in the graphical execution plans 

It looks like the pre SQL Server 2012 behaviour is a bug. I infer this from this Connect article: $URL$ I could be wrong of course... 

You shouldn't need to uninstall/install: your data and log files should be on separate disk arrays/SAN Luns from the binaries. Saying that, 64k NFTS cluster is highly recommended all over the place. SQL Server does IO in extents which is 8x8k pages = 64k, basically. For actual numbers on performance differences, I can only find this $URL$ (Azure, but still SQL Server) 

If your data can be pre-calculated and is static intra-day then why not? After all, isn't this what a data warehouse does? As for having up to 50 million rows, this is peanuts. 

To make them the same you'd need DISTINCT on the first one: which doesn't make it more readable. Going further, the 2nd one can be expressed as 

That's a long question. First off, my current project (I'm the database guy, there are MMO engine experts to deal with that) is a form of MMORPG based on an off-the-shelf engine. Volumes would be like Eve Online" or "World of Tanks" volumes. Now for an orthogonal short answer: 

Do both your Dev and Prod link to the same Navision DB? Compare the rows in sys.servers, especially the column Is the linked server via an ODBC/DSN or is it simple "SQL Server"? If ODBC then are settings the same in the DNS? 

SSMS is creating a copy of the table, populating it, dropping the original. hence the error. I'd create a new filegroup, rather then adding files to the primary filegroup because of how the files are used within a file group You can move a table "in-situ" to a new filegroup by rebuilding the clustered index using 

R and Ř are different letters, as opposed to modified base letters(?) like German umlauts with ö vs o From Czech language on Wikipedia (my bold) 

An ORM requires information about the first write (SCOPE_IDENTITY or such) to complete the second write. This means 2 (with an OUTPUT clause) or 3 database (with SELECT SCOPE_IDENTITY) calls in general in a client side transaction. No amount of tinkering with isolation levels will eliminate these 2 or 3 calls. If you want more performance, then the best concept stored procedure to do an atomic write (in a transaction) to both tables in one database call. This means 50% or 66% reduction in database calls Going further, you have to decide what "superfast" really means For example, should you be using SSDs for your transaction log files: the drive hosting the log files determines overall write speed because of write ahead logging. Then there the design: are you using IDENTITY columns or GUIDs or Hibernate style nvarchar keys? Has the ORM designed the database for you? 

The SSIS service doesn't actually do much and you can run packages without it or with it disabled. It's used by SSMS amd SQL Agent for example. See the KB article "Description of the SQL Server Integration Services (SSIS) service and of alternatives to clustering the SSIS service" What you need are the base SQL Server binaries which are installed with the DB engine. So you need a SQL Server Instance of some flavour to have dtexec running. Whether it's the DB Engine or the SSIS service installed but disabled. Running packages in BIDS is a special case. Run the package on the same PC as BIDS via dtexec and it will fail unless you have installed server components (= a SQL Server Instance) 

Since SQL Server 2005, both mean "fill all pages including all b-tree index levels" Quotes from BOL (My bold) SQL Server 2000: 

Basically impossible for desktop clients. Security when you connect to the database doesn't care about what client is used, SSMS is just another client. Every PC with Office installed since Office 95 at least has msqry32.exe which is a client. You have the ODBC Control Panel item too. Note: the APP_NAME() function in SQL Server returns what is in the Connection String which can be set with anything. 

For point 3, doing a loop inside a transaction is pointless: is this it? At some point, a statistics update may be needed if you delete enough rows but I don't think it's that. Other options: