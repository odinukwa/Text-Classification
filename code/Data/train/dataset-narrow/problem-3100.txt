Typically you want a smaller RMSE and without getting into detail it should be sufficient to just take the smaller one. However, I am concerned because you state that the models were ran on the same dataset but at different timeframes. Since RMSE scale depends on the dependent variable scale, it's entirely possible that these two timeframes are scaled different. A somewhat contrive example would be energy consumption. I would expect a model trained on daytime consumption to have a higher RMSE than for one trained between 1am and 3 am. In that case, comparing the RMSE may be meaningless. You can try to normalize your data and RMSE to help with this, but i'm unsure if AWS provides this ability. As for your second questions, you really won't get a 75% accurate number for regression. You can look at the deviations of the residual or do cross valiadtion and see how well the model performs. Again this may not be possible in AWS. edit: I juse realized that the histograms were residual plots. Do three things Increase bin size. Check to see if the residuals are centered around 0 and then check if there is skewness in the data. If the data is centered around 0 and symmetric then you can say the model error is basically random and does not favor over or under predicting. If the data is not centered around 0 and there is skewness, then the errors can be systematic and then in that case considering adding more variables. 

Depends on the implementation but commonly used is a certain cursive partitioning method called CART. The algorithm works as following: It searches for every distinct values for your predictors and chooses to the split based on what minimize the SSE for two groups of dependent variables. The difference is within the SSE is usually the difference between the actual value and the average of the sample or the difference between the actual value and the output of a linear regression. For each group, the method will recursively split the predictor values within the groups. In practice, the method stops when a certain sample size threshold is met. The node with the lowest SSE becomes the root node. Reference: You can read more about tree based regressions here: Tree-based regressions 

Prior to GAP, one would flatten your tensor and then add a few fully connected layers in your model. The problem is that a bunch of parameters in your model end up being attributed to the dense layers and could potentially lead to overfitting. A natural solution was to add dropout to help regulate that. However a few years ago, the idea of having a Global Average Pooling came into play. GAP can be viewed as alternative to the whole flatten FC Dropout paradigm. GAP helps prevent overfitting by doing an extreme form of reduction. Given a H X W X D tensor, GAP will average the H X W features into a single number and reduce the tensor into a 1 X 1 X D tensor. The original paper simply applied GAP and then a softmax. However, it's now common to have GAP followed by a FC layer. 

This is a common thing with neural networks and different batch sizes. The training loss is the average of losses for the minibatch. Naturally for the first few batches you'll have a higher loss and as it goes through the data the loss gets smaller. Mean while the loss for the validation set is calculated against the entire dataset. 

So essentially you just chain a bunch of task and define your run function to call your previously built scripts using something like subprocess. In requirement, you would reference the last step, and for output you would point towards where the file is being written. The pro for doing this is that if your process breaks in task 50 out of 100, you don't have to rerun all 50 task, luigi will go down the dependency tree until it finds a requirement not fulfilled. Calling R from Pytho Luigi 

You more than likely do not have enough training data for a neural network. Your class imbalance problem is probably an issue. Instead of using accuracy as a measurement trying some type of F-score. Batch normalization should be applied between the convolution layer and the activation function. If you think you have a vanishing or dying activation problem, plot the gradients or the sum of gradients. It'll give you an idea if you're right or not. 

As you can see here, keras predict_proba is basically the same as predict. In fact, predict_proba, simply calls predict. As for why it is not working, I have no idea, it seems like it should work. However, it's mostly irrelevant since predict will give you the probabilities and predict_class will give you the labels. 

Depends on the model. I would say most of the time the new level is dropped or mapped to a NA level. If possible, you should try to do a stratified sample for your training set to ensure that you get every level you care about. 

Toxic comment classification challenge might be a good place to start. It contains a set of comments and 6 binary classifications indicting if it's a toxic comment and of which type. I imagine this would be a sufficient start. 

In responses to the comment requesting real world application. I have not found models like GAM used particularly often in the industry, but there were notable exceptions in my career. Insurance companies, especially life insurance companies, tend to prefer an explainable model over one that just makes good predictions. In this space, I have used GAM to model spatial distribution. I can't go into particular details, but to give you a related example, imagine an experiment design regarding the survey of fish. A properly designed survey will stratified an environment; however, within the environment there exist covariates. The traditional approach would be at apply a GLM and add covariates into the model. However, it's easy to see that some predictors may not be linear. For example, fish are drawn to different temperature, so the relation for the environment of the water and the temperature of the wish should probably not be modeled in a linear fashion. Thankfully we could use a cubic spline instead and produce a readily interpretable result. Another example, would be in credit scoring. In practice, there's usually a myriad of mixed models working together to produce a credit score and there exist different types of models for different level of products and risk. Generically speaking though, some of these models can be hard to understand and find what driving factors effect it. MARS can be used as a good baseline piecewise model for credit scores. In short, it's like any other model. There's a use, time and place for it. I've found that companies that rely heavily on more traditional statistics and process understanding are more inclined to use models like GLM or GAM, while newer tech heavy companies favor the more blackbox quick iteration push to production approach. Neither is better than the other, just that each industry has its own standards and goals. 

I haven't tested this out but that should be the general flavor for how you go about it. If it doesn't work let me know and i'll retry over the weekend. I'm also assuming you have your own f1 score implemented already. If not just import for sklearn. 

I was interestedin this problem awhile back ago. I still have this paper and should serve as a good primer. Guess Who Rated Thos Movie:... 

I also advise you to combine these techniques. For example, give your model a prior and use the F1 score. I think you'll find good results going this route. 

Requirement - what needs to exist before this task runs? Output - where is the output going? Run - what is the task? 

Throughout the hashing process those words may map to the same indices and for the example you gave, i'll say it's extremely likely you'll have collision. How the hashing table chooses to handle that is up to the implementation. Feature hashing may purpose is when you have a large amount of data and that data tends to be sparse. Words tend to be a good example because word matrix of counts tend to be rather sparse. The hashing trick is useful for large datasets because the more traditional ways of handling text data is essentially by making two passes on the dataset. You basically have to go over the dataset once to create a dictionary and then make your transformation. For large data this can be rather expensive process. Feature hashing allows for a one time pass of the data. This also enables the ability to do online learning, which brings me to this point. I wouldn't say that feature hashing helps with new words rather that it basically handles it for you. In traditional approaches you have to ignore the new word or create a new dictionary. Feature hashing will simply just map it to an existing index or a new one if there's space. The last point is that feature hashing rarely if ever improves performance of a model. It also hurts the ability to be introspective. You are going to lose information, and in fact, you can view feature hashing as a way to introduce noise. However, research indicates that in large sparse data, the noise does not impact performance in major ways. Hashing Function Large Scale Hashing Short idea behind hashing 

Nomially, I would simply just try to write a bash script (or powershell in windows) and just string the commands together. However, this approach is rather fragile as in things get overwritten, and if it's an end to end process that has long batches. I tend to use a workflow packages like luigi or airflow when stringing dependent task together. The idea for Luigi is that you can break each action into a task. Each task has three needed functions. 

Compute the partial derivative of -$L(y_{i}, F(x_{i}))$ with respect to $F(x_{i})$ for all i to n Fit a tree $h_m$ to the the result from above. solve $\lambda_m =arg min \sum_{i=1}^{n}L(y_{i}, F_{m-1}(x_{i}) + \lambda h_{m}(x_{i})$ 

Why are you using MSE? It's not completely impossible but the usual framework is done against the probabilities and usually for forecasts. You can read more about it by researching Brier scores. However, it doesn't appear you're doing this and instead just minimize against the labels. You should use a more appropriate measure like cross entropy. 

There's a myriad of approaches you can do here but each really depend on your end goal. You can use a BOW approach and create a large sparse matrix like you've seen before with CountVectorizer or TfidfVectorizer. You can computer similarity scores between sentences using cosine distances. You can score them using bm25+. You can show average idf score. You can show co-occurance of terms in sentence when compared to values in same target or different target or all targets. You can embed the words into a continious vector space using word2vec, fasttext etc. In the end, it really comes down to what you're overall goal. 

I tend to recommend Applied Predictive Modeling. It serves as a nice bridge between theory and application. The organization of the book, which is essentially data prep, pick model, and validate is what one does in the real world (at least at a high level.) Plus there's plenty of R examples and datasets you can try things. As far as statistics, I think you just have to go out and learn more of it. Basic statistics is often insufficient to do anything meaningful in the real world. The only real way that it comes together is by learning something and then applying it to something. A comment suggested kaggle. I personally wouldn't do kaggle for the competitions but rather for the data. I like to find relevant datasets and use models i'm toying around with as a way to learn more about them. For example, when I have interns, I give them the zillow housing dataset and show how different generalized linear models work and how each change can improve or decrease model performance. In short, I suggest reading Applied Predictive Modeling, take some courses in Coursera and apply that knowledge to real datasets. By learning and then doing, you'll be amazed at how quickly you learn and how much easier it becomes to grasp more advance techniques and applications. 

I don't think there really is a right or wrong answer to the "removing stopwords" question. Some people will argue that throwing away information will reduce model performance, while others argue that it'll increase noise. I personally follow a simple rule of thumb. If my model depends on sentence structure, then I keep stopwords. If i'm modeling topics and more interested in important phrases, then I remove them. This seems to work well. If you're looking for ways to reduce you matrix, then yes removing stopwords is a perfectly acceptable idea. Another thing you can do is apply common feature reduction techniques like LSA or chi2 to find the most important words and reduce your input space to the most meaningful words. However, doing this may dramatically effect the performance or your word2vec model. But if it is your only choice, then why not give it a go. 

The probability that a vector $x$ is drawn from $p(x)$ in some region $R$ of a sample space is given by $ P = \int_{R} p(x')dx'$. Given a set of N vectors drawn from the distribution; it should be obvious that the probability k of these N vectors fall in $R$ is given by $P(k) = \binom{N}{k} p^{k} (1-p)^{N-k}$. From the properties of a binomial p.m.f the mean and variance of the ratio $\frac{k}{N}$ are ${E}[\frac{k}{N}] = P$ and ${var}[\frac{k}{N}] = \frac{P(1-P)}{N}$. Therefore, as $N \rightarrow \infty$ the distribution becomes more defined and the variance smaller. Hence, we can expect a decent estimate of the probability P to be obtained from the mean fraction of points that fall within the region $R$. Hence $P \cong \frac{k}{N}$, Now consider if the region $R$ is small such that $p(x)$ does not vary considerably within it, then $\int_{R} p(x')dx' \cong p(x)V $. Combining this result with the one above. We see that $p(x) \cong \frac{k}{NV}$. That's where the formula you found basically comes from. Therefore if we want to improve $p(x)$ we should let V approach 0. However, then $R$ would become so small that we would find no examples. Thus we really only have two choices in practice. We have to let V be large enough to find examples in $R$ or small enough such that p(x) is constant within $R$. The basic approaches include using KDE (parzen window) or kNN. The KDE fixes V while kNN fixes k. Either way, it can be shown that both methods converge to the true probability density as N increases providing that V shrinks with N and that k grows with N. The formulas used in the picture are just arbitrary examples that fulfill this requirement. 

Sample N examples from your population with replacement (meaning examples can appear multiple times). At each node do the following 

I actually do not think your method is a good way to find subtopics. Consider a document X with a distribution of topics z. X is made up of a mixed model distribution of topic Z. If you just give a document the most domiant distribution, and then run lda again, you might find subtopics but you'll also refind the topics that should should perhaps not be considered subtopics. For example, let's consider a document that talks about food and exercising and why certain foods and cardo is good for heart health. Let's assume that we find topics 1 and 2 such that the main topics are food, and exercise. Suppose that the domiant distribution is topic 1. Then running LDA on that particular document again, you're going to refine food, exercise and perhaps nutrition, viatims, etc. However, there's no real reasons what should be considered the topic without inspecting each one and making an inference regarding it. Also you'll have no real way to discern how deep you should go down this tree. The original paper on hLDA can prove useful and exploits the chinese restraurant process and its relationship to dirichlet distribution to form the topic relationship and tree depth problem. Also on David Blei github (bleilab) has an implentation in c++. You can essentially setup a little shell script after processing the data in a langauge you may be more familar with and use his code. 

A common way to do this is flatten your output after your last convolution layer, and pass it through a fully connected layer. 

For classification problems, not just decision trees, it isn't uncommon for unbalanced classes to give overly optimistic accuracy scores. There's a few common ways to handle this. 

If you're asking if the group homomorphism makes the the process symmetric then no it doesn't directly. However, they use the fact that they require a group homomorphism to show that $w_{i}^{T} \tilde{w}_k = log(P_{ik})=log(X_{ik}) - log(X_{i})$ This nearly gives us symmetry. Finally by adding $\tilde{b}_{k}$ into the equation you restore symmetry. So in short $w_{i}^{T} \tilde{w}_k + b_{i} + \tilde{b}_{k} = log(X_{ik})$ is what ensures symmetry, and the group homomorphism is a tool to get there. Update: Some more details Essentially, what we want is the ability to peform a label switch. Group homomorphism helps with this process because it perseves a mapping between the $(R, +)$ and $(R, x)$. $F((w_{i}^{T} - w_{j}^{T})w_{k}^{'})=F(w_{i}^{T}w_{k}^{'}+( - w_{j}^{T}w_{k}^{'})) = F(w_{i}^{T}w_{k}^{'}) \times F(-w_{j}^{T}w_{k}^{'} )= F(w_{i}^{T}) \times F(w_{j}^{T}w_{k}^{'})^{-1} = \frac{F(w_{i}^{T}w_{k}^{'})}{F(w_{j}^{T}w_{k}^{'})}$ The group homomorphism here allows for that to occur. Therefore we can see that by setting $F(w_{i}^{T}w_{k}^{i}) = \frac{X_{ik}}{X_{i}}$ Now finally we can say that $w_{i}^{T} {w}_k^{'} = log(P_{ik})=log(X_{ik}) - log(X_{i}).$ So as far as your comment, it is the most sensible chocie for their method and of which they buld the core mathematicals to GloVE. Changing it, I imagine wouldn't be a trivial thing. I imagine if you did, much of what is derived, including the loss function would change. But with that said, I imagine there are otherwise to achieve label switching.