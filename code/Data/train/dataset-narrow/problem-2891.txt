My question is, how can I move the objects out of each other? (Before performing the 'actual' collision handling, i.e. setting velocities etc). 

(I asked a similar question, but this one is far more specific). How can I handle collisions without having to do a lot of type checking and statements? People here suggested that when spotting a collision, the collision detector should notify both entities in the collision, and the entities themselves will encapsulate the logic to react to the collision. I like the idea of having the entities themselves decide how to handle the collisions. However I'm not sure how to avoid doing a lot of type checking and statements inside the methods. For example (in psuedocode): 

Called every frame to apply global forces on all entities (e.g. gravity). Called by the to do the physics part of collision response. 

I'm making a collage of lots 16x16 renders on a 512x512 texture, of the same scene, from various viewing positions and angles, preferably lots of times per second. I've profiled my program (which contained a glDrawElements call per mesh), and the multiple glDrawElements calls seemed to slow it down a lot. In order to optimize, I've resorted to instanced rendering. However, the main problem I'm having is changing the viewport between, say, every 3 instance renderings, or so. I was thinking of adding a fourth matrix, which would scale the perspectively-projected vertices of a would-be 16x16 picture, translate them so that the little images don't overlap, and based on the little picture's position and size (16x16 pixels) on screen, use the `discard' command in the pixel shader. How do I scale the projected vertices from the current large viewport into a 16x16 smaller version of it and translate them inside the former at a certain position? Or, more clearly, how do I change the viewport during a glDrawElementsInstanced call, every N instances? EDIT Here's a visualization of what I'm trying to achieve: Keep in mind, I can't change the viewport as I want to do this during a call to glDrawEleemntsInstanced, every 3 instances, or so. How do I compute a matrix or what do I have to do to get the post-projection vertices scaled and translated so that I'll have the full image scaled in a 16x16 portion of the screen? 

I have got a floating character, which should be a robot like thing, floating over the ground, whereby I am using the »hovercar« effect described here. Zhe Image below outlines the basic setup of the physics 

I would like to Ray Cast the upper left and upper right edge of the characters AABB downwards to the ground and find the lines marked in Red. Right now I have good Idea, I just thought to use a whole bunch of Rays, but this might become a performance bottleneck and is not that precise anyway. How could I solve the »Shadow-Problem« properly? Thanks in ahead! EDIT As asked in the comment, here some additional notes. The angular line at the bottom is meant to be the ground and the rectangle just below the character represents a platform. These are the shapes which should receive the shadow. The light should go from top to bottom, like raindrops falling down if no wind is present. So the extends of the AABB in x-direction represent the left and right region which should create the shadow, or, in other words, the width of the shadow. 

OK, I have fixed it. I used a more simple shape as per Nathan Reed's suggestion. Turns out the eye was too close to the projection plane. I have set a d of 1000 units and now it appears to render properly. 

I'm trying to make a real-time GPU (CUDA) ray tracer, and for now I'm tracing single rays, but I've ran into a problem: the BVH. This [PDF]paper has been my inspiration for the theoretical part, and as you can see, the BVH is composed of Axis Aligned Bounding Boxes, however, the stackless rope-based algorithm for the ray-AABB intersection does not take into account overlapping siblings, which occur quite a lot with the AABB creation algorithm I've read about in multiple places on the Internet, which is averaging the centroid of each triangle in the current triangle list and deciding in which child to place each triangle based on the projection of the average on the axis parallel to the longest edge of the parent box. The use of AABBs in the paper indicates that there indeed exists a method to efficiently (in terms of speed) make AABB trees without overlapping siblings. Unfortunately, I can't find such a method. Would someone please describe a fast method for creating an AABB tree without overlapping children? I'd also appreciate it if they'd post pseudocode too. Thank you. 

Currently, (almost) all physics-related logic goes through this class (including single-line things like ). My question is, from an OO design perspective: is it okay for an entity to do simple physics logic by itself, or should all physics logic (even simple things) go through classes dedicated to it? For example: in my game, when spotting a collision the notifies both entities to take care of gameplay related logic, and the to take care of physics related logic. Looks like this: 

As you can see, both and are instances of the class. But they are composed with different components. Is this how 'entity component systems' are supposed to be? No inheritance hierarchy at all? Or is there still a basic inheritance hierarchy, but most of the logic is inside components? 

Also, virtual methods tend to be slow as well. It may not be the most professional way of dealing with this, but I think it's the fastest. 

I'm making a software renderer which does per-polygon rasterization using a floating point digital differential analyzer algorithm. My idea was to create two threads for rasterization and have them work like so: one thread draws each even scanline in a polygon and the other thread draws each odd scanline, and they both start working at the same time, but the main application waits for both of them to finish and then pauses them before continuing with other computations. As this is the first time I'm making a threaded application, I'm not sure if the following method for thread synchronization is correct: First of all, I use two global variables to control the two threads, if a global variable is set to 1, that means the thread can start working, otherwise it must not work. This is checked by the thread running an infinite loop and if it detects that the global variable has changed its value, it does its job and then sets the variable back to 0 again. The main program also uses an empty while to check when both variables become 0 after setting them to 1. Second, each thread is assigned a global structure which contains information about the triangle that is about to be rasterized. The structures are filled in by the main program before setting the global variables to 1. My dilemma is that, while this process works under some conditions, it slows down the program considerably, and also it fails to run properly when compiled for Release in Visual Studio, or when compiled with any sort of -O optimization with gcc (i.e. nothing on screen, even SEGFAULTs). The program isn't much faster by default without threads, which you can see for yourself by commenting out the #define THREADS directive, but if I apply optimizations, it becomes much faster (especially with gcc -Ofast -march=native). N.B. It might not compile with gcc because of fscanf_s calls, but you can replace those with the usual fscanf, if you wish to use gcc. Because there is a lot of code, too much for here or pastebin, I created a git repository where you can view it. My questions are: 

I have got it working now! I just missed one vector difference and one multiplication. here is the code: 

As mentioned above, the framerate increased between 20% and 40% on each platform, but flickering and kind of sloppy rendering began. What could I do to gain a more fluent rendering? 

The Green rectangle is the chassis, its not allowed to rotate and held up by the »hover-power«. Attached to this is the robots head with an revolute joint, and attached to the head is the robots body, with another revolute joint. There are some more distance Joints for damping this, but not of interest for now. In the next image one can see how the body of the robot should be tilted when it is in motion to the right or left. 

After I get the result, I divide x and y coordinates by w to get the actual screen coordinates. Apparenly, I'm doing something wrong or missing something completely here, because it's not rendering properly. Here's a picture of what is supposed to be the bottom side of the Stanford Dragon: 

Why does adding these two threads slow down my application? Why doesn't it work when compiling for Release or with optimizations? Can I speed up the application with threads? If so, how? 

Thanks to everyone for their responses, I have finally figured out the problem. Turns out I had to interpolate the positions, as they are transformed in view space, instead of the light direction. Apparently light direction does not linearly interpolate properly. Here are my modified shaders: 

the fun is the line where get calculated, because there the tween functions come into play and it is possible exchange them against each other, what makes it very easy to test different easing. Btw: It is working for dynamic bodies too. 

Right now, the movement of the character is implemented so that forces and impulses are applied to the chassis body. Especially if there is an Impulse to the left or right, the desired tilt of the robots body comes to happen, but it fades away when the velocity becomes more constant, or acceleration comes closer to zero. What can I do to keep the robots body tilted, when it is in motion? 

I have created a jump'n'run browser-game based on SVG. The »World« grew large (~80px * ~20000px, before scaled to viewport height) and rendering went slow. In consequence I included a range searching algorithm to exclude elements from the render tree, if they are out of the visible area. To get all elements inside the viewport, I am using a range query on a AVL-Tree. Basically the approach was very effective and increased the performance up to 40%, unfortunately it also results in flickering of the graphic if the player is in motion. The initialization looks like this: 

The third matrix (C) is the one that transforms from world space, into camera space. This matrix is a translation matrix with a translation of (0, 0, 10), because I want the camera to be located behind the object, so the object must be positioned 10 units into the z axis. 

I'm implementing a software renderer with this rasterization method, however, I was wondering if there is a possibility to improve it, or if there exists an alternative technique that is much faster. I'm specifically interested in rendering small triangles, like the ones from this 100k poly dragon: 

I'm applying phong shading onto a single giant triangle, and I'd like the light's coordinates to coincide with the camera's coordinates in 3D space. In order to do this, whenever I update the camera's coordinates, I also update the light's coordinates. However, diffuse and specular lighting don't "focus" exactly at the camera position when I bring the camera close to the triangle, instead they do it a few units too far. The triangle is .5 units below the XZ plane and parallel to it. Here is a picture demonstrating this effect.