You are right to be confused. What's going on is that the hyperparameters refer to different formulations. On the one hand there is a generic empirical risk minimizer $$\lambda\lVert \mathbf w \rVert^2 + \frac 1 n \sum_{i=1}^n \max\left(0, 1 - y_i(\mathbf x_i^T \mathbf w + w_0)\right)$$ On the other there is the soft-margin objective $$\frac{1}{2} \lVert \mathbf w \rVert^2 + C \sum_{i=1}^n \xi_i$$ such that $$\xi_i \geq 0, y_i\left(\mathbf x_i^T \mathbf w + w_0\right) \geq 1-\xi_i, \forall i$$ The first one places SVM in the framework of empirical risk minimization, in which the objective is an expected loss plus the $L_p$-norm of the main parameter. The purpose of introducing slack variables and constraints in the second formulation is to allow a fraction of points (adjustable by $C$) to lie on the wrong side of the margin, and eliminate the non-differentiable $\max$ function. Moreover, it draws attention to the fact that SVMs encode sparsity in the loss function, rather than the prior. This is called the primal form and can be solved using quadratic programming. Wikipedia currently explains this in the Computing the classifier section. You should be able to see that $C=1/{2n\lambda}$. Some texts omit the $1/2$ or the $n$. 

If you can afford to do the full join once, do it and learn which columns are useful through feature selection. Then you can only SELECT these columns for subsequent iterations, when the database is updated. Here's a survey: Feature Selection for Clustering: A Review 

It simply means that you should set the bandwidths through binary search. The way it works is that you start with a preset target perplexity (Mark's link suggests values from 5 to 50 as reasonable values), and bounds for the bandwidth. If the target perplexity is inside the interval defined by the boundary perplexities, you iteratively halve the search space until you converge to the target: $$2^{H(p; \sigma_L)} < PP_\mathrm{target} < 2^{H(p; \sigma_U)}$$ If the target was not in the initial interval, you expand the interval and try again. 

Transcribe then topic model the recordings. This will let you know which podcast talks about the subject of interest, then you can search the transcript. If you really need it to pinpoint the moment when the subject was discussed, you'll need a temporal topic model like so but that would add a good bit of complexity. Google has a good service for the first part. You can use an open source library like gensim for the second part. 

Take the taxi routes and combine them with civilian car routes to form a data set for classification. Using a map (say, from Google) break down each route into a sequence of roads segments, from intersection to intersection. If you only have GPS traces this will involve spatio-temporal segmentation. (Intersections/terminuses are places where cars go but stop at, and the road segments are the places cars move through to get to from one intersection to another). Model these as categorical variables (road segment 1, 2, 3, etc); abstract out the physical location. Then train a classifier than accepts a sequence as input (e.g., a recurrent neural network). Use the time of departure as another feature, modeled as two real variables, $\cos(2*\pi*t/24), \sin(2*\pi*t/24)$, where $0<t<24$. If you have really accurate GPS information, I would also try to estimate the rate of lane crossing; taxi drivers are well known for their aggressive driving, and the rate of lane crossing would capture this well. Finally, use a pair of boolean variables to record whether the points of departure and destination are parking areas, if you can obtain this information. I imagine taxis will be more likely to start and stop at prohibited places. The rest is your usual hyper-parameter optimization black magic. If you want to be able to do anything besides classify a given route as taxi-driven or not, please say so. 

Instead of collaborative filtering I would use the matrix factorization approach, wherein users and movies alike a represented by vectors of latent features whose dot products yield the ratings. Normally one merely selects the rank (number of features) without regard to what the features represent, and the algorithm does the rest. Like PCA, the result is not immediately interpretable but it yields good results. What you want to do is extend the movie matrix to include the additional features you mentioned and make sure that they stay fixed as the algorithm estimates the two matrices using regularizastion. The corresponding entries in the user matrix will be initialized randomly, then estimated by the matrix factorization algorithm. It's a versatile and performant approach but it takes some understanding of machine learning, or linear algebra at least. I saw a nice ipython notebook a while back but I can't find it right now, so I'll refer you to another one which, while not as nice, still clarifies some of the maths. 

You are probably aware that deep learning is all the rage these days, and it has touched NLP too. There is a tutorial on it from a recent conference: Deep Learning for Natural Language Processing (without Magic) by Richard Socher and Christopher Manning, who are from Stanford. 

The current thinking is that it is easier to fit an overparameterized neural network, since the local extrema are different ways of expressing the same thing, whereas in a minimal neural network you have to worry about getting to the global extremum: 

Study multiclass logistic regression; it's similar and has many tutorials. Good books include Larry Wasserman's All of Statistics, and An Introduction to Statistical Learning. The way to understand research papers is simply to read more of them. Whenever you encounter something you don't understand follow the references or look it in one of the aforementioned books. 

My generic answer to the title is to use the extra data for regularization in representation learning; a transformation of your features into a space conducive to your main task: regression (prediction, forecasting). Here's a survey [PDF]. For your example, you could build a model that takes the delay of the target time from the present as an input, so you can predict arbitrarily far into the future, though it probably would not predict as well as a simple regressor that has a fixed horizon since it is trying to learn a more complex function. 

The formula is defined in section two of Gower's A general coefficient of similarity and some of its properties.