The decision boundary in (4) from your example is already different from a decision tree because a decision tree would not have the orange piece in the top right corner. After step (1), a decision tree would only operate on the bottom orange part since the top blue part is already perfectly separated. The top blue part would be left unchanged. The boosted stumps, however, operate (as you mentioned) on the full dataset again, which can lead to different results. 

In the case of a line segment, the same idea applies. If the end points are labeled differently than one of the other points, they cannot be separated by a hyperplane. Since we covered all possible formations of 4 points in 2D, we can conclude that there are no 4 points that can be shattered. Hence, the VC dimension must be 3. 

The biggest one I know is O'Reilly's survey (here the last available from 2016). There are also polls on KDNuggets (older version) and other websites. But, in general, most of them are not very representative. You can also go look at the national averages on glassdoor.com (example India) or other job sites. 

Your model can be worse than random, for example, if some fundamental assumptions are violated, in an imbalanced setting when your using accuracy as your baseline or you have noisy data etc. However, in a binary setting, if your classes are perfectly balanced and if your classifier is consistently making false predictions (not due to randomness), you can always adjust the model to be better than random by predicting the exact opposite of your model. 

I think with your red cross you have in mind the case where $w_1 = w_2$. You are right that in this scenario the solution will not be sparse, since you will also get $w^*_1 = w^*_2$. However, whenever the coordinates of the red cross are off the line $y=x$ ($w_1\neq w_2$), you can shrink the regularization region so much that eventually you will end up with a similar picture as Figure 3.4 above. Say your red cross has the coordinates $w_1=r_1$ and $w_2=r_2$ and say your regularization strength is such that $||w||_1 = |w_1|+|w_2| \leq C$ where $C$ defines the size of the shaded region. 

Weather and Wind both produce only one incorrect label hence have the same accuracy of 16/17. However, given this data, we would assume that weak winds (75% YES) are more predictive for a positive outcome than sunny weather (50% YES). That is, wind teaches us more about both outcomes. Since there are only few data points for positive outcomes we favor wind over weather, because wind is more predictive on the smaller label set which we would hope to give us a rule that is more robust to new data. The entropy of the outcome is $ -4/17*log_2(4/17)-14/17*log_2(14/17)) =0.72$. The entropy for weather and outcome is $14/17*(-1/14*log_2(1/14)-13/14*log_2(13/14)) = 0.31$ which leads to an information gain of $0.41$. Similarly, wind gives a higher information gain of $0.6$. 

L2 and L1 regularization are controlled via the and parameter respectively. Higher values of mean more L1 regularization. See the documentation here. 

You only have to call the method on your matrix, which returns an array of for inliers/outliers, assuming your IsolationForest is already fitted. Note that the matrix should be of the form (rows=samples x columns=features). Something like this should do it 

respectively. (Actually, $x_1 < x_2$ can be assumed w.l.o.g. but it is enough to find one set that can be shattered.) Now, consider three arbitrary(!) points $x_1$, $x_2$, $x_3$ and w.l.o.g. assume $x_1<x_2<x_3$, then you can't achieve the labeling (1,0,1). As in case 3 above, the labels $x_1$:1 and $x_2$:0 imply $a<x_1<b<x_2$. Which implies $x_3$ > b and therefore the label of $x_3$ has to be 0. Thus, the classifier cannot shatter any set of three points and therefore the VC dimension is 2. - Maybe it becomes clearer with a more useful classifier. Let's consider hyperplanes (i.e. lines in 2D). It is easy to find a set of three points that can be classified correctly no matter how they are labeled: 

Let's look at the example of linear regression. Instead of deriving it from solving the normal equations, we can motivate it by thinking about it as finding the conditional distribution $P(y|x)$. Let's assume that this distribution follows a Gaussian distribution with fixed variance $\sigma^2$ and mean $\hat{y}(x,w)$ and weights $w$. Assuming that samples are i.i.d. we can easily show that applying maximum-likelihood gives the same values of $w$ than minimizing the mean squared error. See example 5.5.1 for a mathematical explanation. If we additionally assume a prior Gaussian distribution for $P(w)$ (w.l.o.g. we can assume unit variance $I$), we can now show that the posterior $P(w | x,y)$ of $w$ is also Gaussian distributed (that is because the prior is "conjugate") with variance $\frac{1}{\alpha} I$ and we recover linear regression with weight decay $\alpha w^T w=\alpha \lVert w \rVert^2_2$ (i.e. $L^2$ or Tikhonov regularization). Intuitively, a Gaussian distribution's probability mass is centered around the mean (assumed here to be zero). If we assume such a distribution for the coefficients in the linear regression (as our prior knowledge), linear regression will favor coefficients close to zero. All of this is nicely explained here (chapter 5) or here (chapter 18). 

When you go backward for max-pooling you keep track of the position of the maximum: $$ X = \begin{bmatrix} 1 && 2 \\ 3 && 4 \end{bmatrix} \quad \rightarrow \quad dZ *\begin{bmatrix} 0 && 0 \\ 0 && 1 \end{bmatrix}$$ This gives you the position of the input value that ultimately influenced the cost/output. The gradient will then be "propagated" back to this value. For average pooling, all values influence the cost equally and therefore it should look like this: $$ dZ = z \quad \rightarrow \quad \begin{bmatrix} z/n && z/n \\ z/n && z/n \end{bmatrix}$$ where $n$ depends on the filter size, here $n=4$. The gradient gets distributed evenly back to all input values. 

I can recommend Genetic Algorithms in Search, Optimization, and Machine Learning by Goldberg. In particular, chapter 1 gives a great "introduction to genetic algorithms with examples." The code examples are unfortunately in Pascal but readable even if not familiar with the language. The book by Thomas Back is a little more advanced but also more complete (more "evolutionary programming"). Also, Genetic Programming by Koza is a good choice with many examples but less tutorial. 

I have a dataset that has (among others) a categorical variable with many levels and further attributes associated with each level. For example, consider predicting machine failure based on its last repair report. 

One reason why functional programming could be useful for data science is that it lends itself more easily to parallel and distributed programming, e.g. the popular frameworks Apache Spark for cluster computing and Apache Kafka for stream-processing are both written in Scala (and Java). Other than that "functional programming" as a skill is not directly related to data science. It's a tool that may facilitate some practicalities of data science and therefore more relevant for the "data engineering" aspect of data science. It's useful but probably not necessary. It depends on your interests. 

If you would use the on the full dataset you would provide the algorithm with some information about the values in the test set that it would not have otherwise. This additional information ("data leakage") from your test set to your training set is problematic because it leads to unreliable performance estimates. It is therefore not surprising that you achieve a higher R-squared. Due to the data leakage, this R-squared might be overly optimistic because it could depend on the additional information that you introduced into the training set. This is particularly true for the because it is by definition very sensitive to outliers. The effect will probably less problematic if you use the (or even the ). The same holds for other preprocessing steps like outlier removal, feature selection etc. If you are worried that your training data does not adequately reflect the true distribution, you can fall back to a cross-validation approach with multiple folds so you can estimate the effect across multiple splits of the data. Again, remember to fit the on all the training folds and apply it to the test fold for every iteration. 

Therefore, for regression often $N/3$ is recommended, which gives larger values than $\sqrt N$. In general, there is no clear justification for $\sqrt N$ or $\log N$ for classification problems other than that it has shown that lower correlation among trees can decrease generalization error enough to more than offset the decrease in strength of individual trees. In particular, the authors note that the range where this trade-off can decrease the generalization error is quite large: 

There are at least two uncomplicated ways to deal with this issue. If your data has a lot of zeros etc. it might be worth trying to load it as a sparse dataframe. One thing that always works is to process your data in an iterative fashion. For most datatypes pandas can let you iterate through your file, e.g. with . This is not available for JSON data, but you can use the package instead. Many scikit-learn algorithms can also be trained iteratively on one or a few samples (mini-batches) at a time. In particular, the flexible linear models with stochastic gradient descent (SGDClassifier and SGDRegressor). For clustering, you can use MiniBatchKMeans. You can read more about this in the documentation. If your data fits almost into memory, you may just read in as much as possible. If most of it doesn't fit, there is probably no way that lets you use pandas/sklearn without some adjustments. 

Histograms and methods based on binning have a number of well-known problems. Different anchor points etc. can introduce artificial patterns that make interpretation unreliable. Smooth kernels don't use a grid and thus smooth out the noise. This also has the advantage that it makes it easier to get a single overall picture of the data because it takes into account neighboring points and smooths the data into areas where no data is observed. Smooth kernels can also be justified by their favorable statistical properties. Popular methods like fastKDE use the fact that one can find "an empirical kernel that is optimal in the sense that the integrated, squared difference between the resulting KDE and the true PDF is minimized." 

One thing your problem can depend on is the number of categorical variables. If you have many categorical variables that are encoded as dummy-variables it usually makes sense to increase the parameter. Again, from the Random Forests paper: 

If one requires that $H = C$, then this is called the "proper PAC" framework compared to "PAC prediction" where we don't care about the representation of $h$ as long as the prediction error is small enough (i.e. we allow $H$ to be the class of all time-polynomial programs). You can think of a concept as the set of inputs that produce the same label (e.g. all the images that show a chair form the concept "chair" or all the points in the same half space form the concept "true/false"). 

A tuple of the form $(i_1, i_2, i_3, ... , i_n)$ gives you a network with $n$ hidden layers, where $i_k$ gives you the number of neurons in the $k$th hidden layer. If you want three hidden layers with $10,30$ and $20$ neurons, your tuple would need to look like $(10,30,20)$. $(100,1)$ would mean that the second hidden layer only has one neuron. 

If the parameter is set to , the object will have the attributes , etc. ("Best" measured in terms of the metric provided through the parameter.) For other models you can only access the cross-validation scores from . The models are not stored. If you want to use them to make predictions, you have the re-train the model where you explicitly set the parameters. If you want to use all the models to make predictions, it would be better to write your own code and simply loop over the parameter constellations without using . 

The algorithm works by adding or subtracting the feature vector to/from the weight vector. If you only add/subtract parts of the feature vector your a not guaranteed to always nudge the weights in the right direction, which could mess with the convergence of the procedure. The idea is that in the weight space every input vector is a hyperplane. You need to find a weight vector that is on the correct side of all the hyperplanes of your data inputs. The correct weight therefore is in a convex cone. If you observe a misclassification, that means your weight vector is on the wrong side of the hyperplane and therefore outside the convex cone of possible solutions.