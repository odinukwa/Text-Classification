We pass an additional boolean flag that tells if a particular step was an improvement or not. Note that we don't need to check for the final condition - if everything succeeds, there will be one final step with an empty list and then we'll stop. However, I like a bit more yet another solution. Notice this: Let's say the first removal succeeds, the second fails and because of this, everything else. Then we iterate through the whole list, and then once more, before we finish. But in fact we know that we're bound to fail when we fail to remove all the other elements. In this second solution, we loop over all elements without distinguishing a full pass. Instead we "reset" the state every time we succeed and try all the failed elements again. I also like its simplicity, compared to the previous one: 

First, you should decide if the height of a tree will be a field of or not. If yes, then there is no need to compute it. Instead, your tree module should hide the constructors and only expose smart constructors: 

where we start with an infinite list of zeros. This is safe, as we always only access a finite number of elements of this starting list. While the algorithm remains the same, avoiding explicit indexing makes it more readable. 

(Again compiled but untested.) Note that in we change the order of elements. This shouldn't be a problem, as we need to try out all before we fail anyway, but if you don't like it, you can use instead of and maintain sequential order. Or you could also replace it with , but only if the total number of elements is moderate, otherwise you could start having problems with its O(n) complexity. 

(I suggest you to try out hlint, which gives many interesting hints how to improve a Haskell program, including the two above.) 

If you're not sure how to implement , hover over the following box with another hint (but I'd strongly suggest you to try without it first): 

where waits until the underlying computation finishes and returns the reuslt. Then let all your methods return instances of asynchronously. If you want a synchronous invocation, just call 

On the other side of the spectrum, a way how to look at the problem is how to construct the simplest histogram - from a single value, and then how to combine histograms together. A one-element histogram (represented as ) is and they can be combined together by combining maps with (here generalized to arbitrary values that can be ordered): 

for example = . And then recursively process the second part, prepending the picked element to all sub-results. It's good that you provided the type of the top-level function. Also can be abbreviated to using η-reduction. 

Or you might want to keep without the height, and compute it separately. Implementing won't help, as it only exposes the elements of a data structure, while you need to examine the structure, not the elements. What you can do is to create a generalized folding function over , so called catamorphism, which allows you to express functions that consume trees: 

Then your folding function will only fold over 1 and 2, but not over 3. If you have a recursive structure like this, a folding function over it must also be recursive. Otherwise it won't be able to traverse arbitrarily large recursive structure. For the other question: If you specialize the folding function as 

Your code is correct, fast, and I'd like to emphasize that you thoroughly include types for your functions, which greatly helps readability. While arrays offer fast performance, I'd say that in this case operations with indexes and bounds of the arrays cloud the main idea considerably. Therefore I'd offer to use lists and functions native to lists that avoid indexing (the same could be also achieved with arrays, by writing similar functions for them). The main idea, computing the maximum path lengths for one row, can be then nicely described by zipping: 

I'd say it's hard to improve , as it's just a call to the primitive function . However, there are other areas for improvement. In particular, going through div is a very inefficient method for listing divisors. A much more efficient method would be to factorize and then compute all its divisors from that (see Divisor function, in particular the formula specialized for σ₁(n)). So my suggestion would be: 

Great that you included the tests. I'd suggest to add property-based tests too, it seems highly appropriate for this. One thing that could simplify the implementation would be to use heterogeneous tuples, like the ones provided by tup-functor, for example 

The point of the approach is that you don't manage any state globally. Instead, each component () manages its own state internally and you just express how they are connected together. To give a simple example, let me first implement the standard type classes for : 

you let lens generate so-called lenses for your data type, so for example won't be of type , but , and then the function that modifies the field inside would be 

Nitpicks: I'd put before so that the functions are in logical order, but that's just a matter of personal preferences. Otherwise I also quite liked the code. Update: Some further thoughts: Function is concise, but it also mixes several concepts together. Namely: 

Note that operators (.&.) and (.&&.) are very different! I'd probably prefer yet another variant using , which spares you of the explicit recursion. By zipping a list with its tail we get all consecutive pairs, and then we just express that each of such pairs must satisfy . 

There are many options how to avoid having everything inside . One possibility is to use the prompt monad (see my comment). As an example, let's modify using . First, we'll create a data type that represents all possible actions can perform: 

A tiny nit, unrelated to the main problem: Haddock comments allow you to generate nice documentation. 

and . (Unfortunately I don't have more time to review the rest, maybe later or someone else can continue.) 

Yes, defined such that it shrings the result to the size of the shortest list isn't an involution. Your idea of viewing lists as maps is nice, but I suppose once you leave s, there are other easier options. As you observed, is also a that works on integers that are between and the size of the list. Now if we have a list of lists, it's . Now we can convert such functions into a single function by a process called un-currying. While Scala has for plain functions, it lacks it for s. But it's not difficult to define it: 

using .) Furthermore, since calls itself recursively and examines the result of the recursive call, it can't be optimized to tail recursion, so you're building a chain of calls on the stack. Another thing to notice is that the whole original list is kept in memory until the whole chain of s finishes. 

Otherwise nice program! I also like that you meaningfully named variables, this really helps reading the code. 

Now has no reference to or other particular monad, it only uses our given set of actions. For example, we could create a testing instance that simulates user input, checks that it returns (s) the correct reply, checks how updates the database etc. If we converted the whole module, we'd most likely add all the actions our functions need and we'd let the main dispatcher provide the correct implementations. Since we converted only , we could implement the old type as 

In particular, when recursively operating on lists, there are two approaches: left-fold-like and right-fold-like. 

Your approach is definitely valid, I've seen something similar used in GHC's implementation of atomicModifyIORef. However I have some doubts if such a class is really useful. If the computations are potentially long running, the performance advantage of using instead of some standard synchronization primitive is negligible. Moreover, if there are several updating threads, it will result in repeated recomputation, wasting CPU power on it. So using plain on some private lock object (so that no other part of code can lock on it) seems simpler, safer and with better performance. 

There is no reason against using in this case. Some people actually prefer it over so that the code can be read top down. I'd like to point out that the time complexity of your algorithm is O(n^2). There are two reasons: 

This looks very much like a monoid (see also Monoid on Haskell Wiki). And this is what you indeed want from your parser - to be compositional, and to adhere to certain laws: associativity and identity. So it'd make sense to have a instance for . Note that then becomes just mconcat. Another instance would be , whose identity would be a matcher that's always failing . This is very much like there are two monoids for natural numbers - multiplication with 1 () and addition with 0 (). We'll back to this later. The next thing you might consider is to return a value from your matcher. For constant matchers this isn't that useful, but there are multiple reasons why this can be useful: 

So we can apply immediately on (or rather to to avoid O(n) access for lists) and we get a functional representation that accepts a pair of indices and gives the result. If we swap the indices in a pair, we get a transposed view. The problem with this representation is that we can't enumerate all matching indices easily. If this is required, we could instead convert a list of lists into , perhaps a so that we can enumerate the indices in a proper order. Again, this will be a , but also with this additional ability. 

Now we can rewrite as follows. It doesn't carry around any state nor database and runs in any monad that is an instance of (this signature requires FlexibleContexts). 

Nice and well documented code. Some ideas: In this case you could use , as you create a vector only to thaw it. This will save you copying it during , but of course you must be careful and aware of the consequences. If using arrays, another option is . When converting back to a list, for arrays you could either use which again saves copying the entire array (as opposed to using ). Or instead of followed by , use . For vectors, there is too. While using a counter for the middle part certainly works, it's a bit imperative style for Haskell. The more common approach would be to have a recursive function where you pass it as an argument and return it back. Somewhat related is looping using over a list. Probably passing it as an argument would be faster, as producing/consuming the list is likely to cause memory (de)allocation inside the loop, although less idiomatic. So it depends on what are your goals. Finally, declaring functions using inside is indeed often convenient, but can impact readability, if they're long. I'd prefer to declare them separately so that the main function is just "thaw - go - freeze" and then the definition of follows (or precedes). If the helper functions aren't recursive (that is, the recursion is hidden inside), they'll be properly inlined as needed by the compiler. Also is equivalent to or just . Here is a possible variant with the ideas above, and a few minor more: 

At every step you compute , which is O(n). And at every step you also append to from right, which is also O(n). 

Some more remarks: I like that you included types for all functions, including helpers, that's often very convenient. And that you gave descriptive names to the types. From the types it's not that obvious how are graphs represented. If an edge is 

Number (1) is easy to solve, you need to compute only once and share it within the computation. You could use nested or (or their combination), or separate the computation into multiple functions. Number (2) is somewhat harder. One solution would be to create an array from the list first one, that is O(n) (see listArray), and then use to look up elements in O(1). If the number of required elements k is small compared to the length of the list n, you could try to devise an algorithm that'd use this advantage and work in O(n k) or perhaps even O(n log k), still keeping the uniform distribution of the elements in the returned list - this looks like a nice exercise. Also in the case you'd like to use monadic computations, have a look at MonadRandom, which eliminates the problem of passing the random generator around. 

Also I'd suggest you to have a look at various balanced tree implementations (unless you want to explore it yourself, which is a good exercise). On the other hand, if your aim is only to have a balanced tree, without the need of modifying it later, you could somewhat relax your requirement, and just require that the height of the tree is at most 1+log{2} n. Then you'd be able to create the tree incrementally, without knowing the total length of the input list: at a particular point, the algorithm tries to fill a node of height h. If it's full and there are more incoming data, it starts to build another node of height h+1. So the left subtree will be always a full binary tree. From the syntactical point of view, it's very good that you annotate all functions with their types, the only thing I'd improve is to have a fixed line length limit to increase readability. 

I didn't know the trick, nice. Given the semantics, the 5-clauses don't seem to be really problematic. You need to branch on these conditions one way or another. Some ideas, neither seems to be perfect: 

As far as I know, Scala doesn't have anything like you suggest. Your solution is short and fast. I tried to think of a solution that doesn't use mutable state and uses existing functions, without examining the structure directly. It is based on s instead of s, which are imperative in their nature. The main idea can be expressed using a scanning function that sums together elements that are equal according to a given comparison function: