A recent one, presented at ICALP is Markus Lohrey, Christian Mathissen: Isomorphism of Regular Trees and Words. ICALP (2) 2011: 210-221 You will find the paper both on arxiv and here. Another example is Mostowski epimorphism (see P-completeness and efficient parallelization by Satoru Miyano, and the paper by Dahlhaus): Dahlhaus E, Is SETL a suitable language for parallel programming - a theoretical approach, Computer science logic, 1st Workshop, CSL ’87, Karlsruhe/FRG 1987, Lect. Notes Comput. Sci. 329, 56-63, 1988) Instance: a directed acyclic graph $D = (V, A)$ satisfying the axiom of extensionality and two vertices $x_1, x_2 \in V$ Problem: Decide whether $M_D(x_1) = M_D(x_2)$, where $M_D$ is the Mostowski epimorphism for $D$. 

You can partition your domain for subsequent parallel processing as follows. There are a few possible options. You can try a static geometric decomposition, including recursive bisection, quad or oct-trees and a space-filling curve. Recursive bisection techniques are used to partition a domain (e.g., a finite element grid) into subdomains of approximately equal computational cost while attempting to minimize communication costs, that is, the number of channels crossing task boundaries A divide and conquer approach is taken. The domain is first cut in one dimension to yield two subdomains. Cuts are then made recursively in the new subdomains until we have as many subdomains as we require tasks. Notice that this recursive strategy allows the partitioning algorithm itself to be executed in parallel. Advanced variants include recursive graph bisection and recursive spectral bisection. Here is an example. In the following picture, assume that magenta points requires twice as much work as cyan ones. 

If $f$ is an elementary symmetric polynomial over a finite field then it can be computed by polynomial-size uniform $TC^0$ circuits. If $f$ is an elementary symmetric polynomial over a characteristic $0$ field, then it can be computed by polynomial-size depth three uniform algebraic circuits (as you already mentioned the Newton polynomial; or by the Lagrange interpolation formula); and so I believe this then translates to polynomial-size uniform Boolean circuits (though perhaps not of constant depth) (but this may depend on the specific field you're working in; for simplicity you might consider the ring of integers; though for the integers I presume $TC^0$ is enough to compute symmetric polynomials in any case.) If $f$ is a symmetric polynomial over a finite field then there is an exponential lower bound on depth three algebraic circuits for $f$ (by Grigoriev and Razborov (2000) [following Grigoriev and Karpinsky 1998]). But, as mentioned in 1 above, this corresponds only to constant-depth Boolean circuit lower bounds (while there are small uniform Boolean circuits in $TC^0$; meaning also that the polynomials are computable in polynomial-time). 

For strong enough proof systems the graph representation of a proof in the system seems less consequential, since (as Joshua Grochow already commented), DAG-like and tree-like Frege proofs are polynomially equivalent (see Krajicek's 1995 monograph for a proof of this fact). For weaker proof systems such as resolution, tree-like is exponentially weaker than DAG-like proofs (as Yuval Filmus described above). Beckmann and Buss [1] (following Beckmann [2]) considered restricting the height (equivalently, depth) of the proof-graph of constant-depth Frege proofs and investigated the relationship between DAG-like, tree-size and height of constant depth Frege proofs. (Note the distinction between restricting the depth of the proof-graph and restricting the depth of a circuit appearing in a proof-line). There might also be separations between tree-like and DAG-like Nullstellensatz (and polynomial calculus) proofs, which I currently don't remember. 

To me, a substantial contribution to software design may be related to many different things. For instance, rethinking the whole architecture, rethinking algorithms and data structures used, augmenting modularity, making the code easier to maintain and, in particular, applying correctly what in Software Engineering is usually called "design for change". According to your description, making the code more logical and adaptable may deserve authorship: it just depends on how much the overall code quality has changed, and it is up to you to judge this. I understand that, being a biologist, this is of course almost impossible to judge correctly on your side. You may ask a CS colleague about. On the other hand, making the code more readable is not necessarily so important. Indeed, code written by seasoned professionals programmers (especially in C and C++) is usually more difficult to read (because it is more concise and the typical style adopts statements not immediately understood by the majority of the people) w.r.t. code written by junior programmers and students. Even though every Software Engineering textbook stresses that code readability is important, especially in connection to software maintenance, practice is quite different from theory, at least for professional grade and commercial software. 

Frequent items can be found using either $count$-based or $sketch$-based algorithms; these algorithms may be adapted to answer top-$k$ queries as well. For count-based algorithms, Space Saving can report frequent items in time linear in $O(n)$, the total number of items using $k$ counters. The Frequent algorithm reports frequent items in $O(n)$ time using $k-1$ counters. Among the sketch-based algorithms, relevant algorithms include Count-Min which uses $O(1/\epsilon \log 1/\delta )$ space and hCount whose space complexity is better, being $O(1/\epsilon \log(-M/(\log 1- \delta)))$, where $\epsilon, \delta, M$ are input parameters. These algorithms also works for streams. Note that the space complexities reported are for heavy-hitters and not for top$k$ queries. See Sasho's answer. 

If I understood correctly the question, the so-called Buss-Pudlak game provides a simple transformation from a proof system to such a decision tree (see Buss-Pudlak '94 $URL$ The queries are formulas (not just variables). The tree is also completely deterministic. Other decision trees that correspond to different propositional proof systems exist: e.g., Linear Decision Trees correspond to Res(lin) refutations (cf., $URL$ and $URL$ But there are many other examples (cf., Tonian Pitassi's work on CP-like proof systems). 

To my knowledge, such a reduction is in fact known: Hrubes and Wigderson ITCS 2014 show how division gates can be eliminated from non-commutative circuits and formulas which compute polynomials. They also provide exponential-size lower bounds for non-commutative formulas with division (not circuits) that compute any entry of the matrix inverse function $X^{-1}$. Moreover, your main question about lower bounds for non-commutative circuits, is not known (while for formulas it is known as mentioned above), because non-commutative circuits in which each gate computes a polynomial (not a rational function) with division constitutes a class which is at least as strong as non-commutative circuits. But there is no known super-polynomial non-commutative circuit lower bound (see [Hrubes, Yehudayoff and Wigderson STOC 2010] on this). 

The following result by Raz (Elusive Functions and Lower Bounds for Arithmetic Circuits, STOC'08) is aimed at $VP\neq VNP$ (and not directly $P\neq NP$), but it might be close enough for the OP: A polynomial-mapping $f:\mathbb F^n \to \mathbb F^m$ is $(s, r)$-elusive, if for every polynomial-mapping $Γ : \mathbb F^s → \mathbb F^m $ of degree $r$, Image($f$)$\not⊂$ Image($Γ$). For many settings of the parameters $n, m, s, r$, explicit constructions of elusive polynomial-mappings imply strong (up to exponential) lower bounds for general arithmetic circuits. 

The best general-purpose geometric load-balancing comes from space-ﬁlling curves. The order in which points are visited in the space-ﬁlling curve determines how the geometric objects are grouped together to be assigned to the processors. The following image shows an Hilbert space-filling curve. 

If you do not want to delve into the gory details, a very good introduction to parallelization design patterns is provided by the book Patterns for Parallel Programming by Mattson, Sanders and Massingill. You will find general, widely applicable solutions to parallelization and even a brief introduction to both OpenMP and MPI. The book starts by introducing design patterns and concurrency. Then, the authors proceed to illustrate how to exploit the concurrency, how to structure the algorithm, and how to actually implement the algorithm taking into account synchronization and communication. Again, this is not a textbook on parallel algorithms. It does a very good job of presenting materials strictly related to parallel software engineering, with both a practical and theoretical focus. Therefore, it should suit perfectly your needs. 

Then, you continue applying recursive bisection by cycling through the axes until # pieces = # processors, obtaining something like the following: 

I am not sure, but you may be interested - if not, I beg your pardon - to the following paper, which is not related to a natural kind of optimization problem, but deals with a problem which can be reduced to a particular optimization problem, whose solution is in NC. Igor Averbakh, Oded Berman, "Parallel NC-algorithms for multifacility location problems with mutual communication and their applications", Networks, Volume 40, Issue 1, pages 1–12, August 2002, Wiley DOI: 10.1002/net.10027 Abstract The generic problem studied is to locate p distinguishable facilities on a tree to satisfy upper-bound constraints on distances between pairs of facilities, given that each facility must be located within its own feasible region, which is defined as a subtree of the tree. We present a parallel location scheme (PLS) for solving the problem that can be implemented as an NC-algorithm. We also introduce parallel NC-algorithms based on the PLS for the minimax versions of the problem, including the distance-constrained p-center problem with mutual communication. Combining the PLS and the improved Megiddo's parametric technique, we develop strongly polynomial serial algorithms for the minimax problems; the algorithms have the best complexities currently available in the literature. Efficient parallel algorithms are given for obtaining optimal regions of the facilities.