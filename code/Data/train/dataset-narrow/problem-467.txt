If does not work, rerun these lines using EPILOGUE Neither of these techniques will allow operations while the file is live. 

AFAICT you would only need to consider the size of the and files. Since SMALLINT is 2 bytes, and MEDIUMINT is 3 bytes, look for the files to increase byte 1 byte for each record. Thus, 1,000,000 rows in a 10MB MYD will now be about 11MB. MyISAM tables with a wider row may need some adjustments (increases) in things like 

Perhaps you could add a counter to the loop so that every 100 rows, you run SLEEP for one second. I will add that code: 

If you are a little adventurous, you could take matters into your hands by performing the ALTER TABLE in stages you can see. Suppose the table you want to change is called WorkingTable. You could perform the changes in stages like this: 

OBSERVATION #1 You need to look at what the Query Optimizer sees and what it is interpreting First, look carefully at the query 

I think that is the new way to express home based on the IPv6 protocol. It's new to me as well. By this, my guess is your WAMP installed MySQL 5.5. Just remember two things 

That's it !!! If other accounts are messed up in the same way, set , and then execute the code. Here is That Code Formatted: 

You will some due diligence to go through. In other words, bite the bullet and optimize your queries. In your particular case, there are two things you should do with that delete query: REFACTOR THE QUERY AS A DELETE JOIN Instead of 

Right now, you are in a very fortunate position. I noticed you have defined. This is preventing you from experiencing "Table is Full" errors. Why is this good? Whenever you get "Repair With Keycache" as a status, you have no free space to do file sorting. Making sort_buffer_size bigger isn't necessarily the answer since temp tables become disk files immediately. You have two options OPTION #1 : Increase Diskspace for datadir The data volume where (or whatever is) resides may not have enough room to house a materialized temp table on disk. I would suggest increasing the disk volume's size to, at least, twice its size. DRAWBACK : A one-time maintenance to move the database to the bigger disk. OPTION #2 : Separate Disk for Temp Tables Perhaps having a separate disk volume who sole purpose in life is to house temp tables should be set up. Try this 

You can set to choose how granular to scan for changes. If many tables or the same set of tables appear from this query, you will see which tables are experiencing the most frequent writes and the most growth. I will leave it to your to determine the weirdness from the results. 

ALTERNATE SUGGESTION This may seem unorthodox and probably smells like SQL Anitpattern, but here it goes... 

OBSERVATIONS You doing an INSERT with two different account_ids: 561 and 563. They are unique and should not have issues, right ? WRONG !!! Due to InnoDB's Clustered Index, there can still be a deadlock. Why ? Look back at your two INSERTs. The on id in not specified. It must be auto generated. Any key other than the PRIMARY KEY (unique or non-unique) will have the PRIMARY KEY attached. Please note the MySQL Documentation on how a Secondary Index and a a Primary Key are intertwined: 

There is also a forum for users of this product (As of this posting, there are 5 active questions that have activity this month) MANUAL ALTERNATIVE If you have the patience, you could do the following STEP 01) Create a table using the CSV Storage Engine When you create the table like this: 

If you want to run later on, set up a cron job to run it in every midnight or perhaps once a week. You will have to do this inevitably if you expect queries against your table to perform well after an optimization. SUGGESTION #2 : Change the MyISAM table to InnoDB During the generation of an EXPLAIN plan, InnoDB will perform reads against the index pages of a table by traversing 2-3 levels of nonleaf nodes and literally approximate (or really guess) the cardinality of all column levels. You can tell this is happening to an InnoDB table when you run because the cardinality changes with each run. In light of this, you can simply run 

ALTERNATIVE What you need is to simulate the BENCHMARK function yourself. Here is some sample code for you to try 

You forgot to give 'john91'@'localhost' grant option. Thus, you cannot grant privileges to anyone else. You can fix that by doing the following 

When an in-memory temp table exceeded the minimum of (tmp_table_size or max_heap_table_size), mysqld does the following: 

the code for the trigger definition is included. However, there is a hardcoding situation you want to be cognizant of 

Once you have the accessibility, you can create the DBs directly in the Slave. I would normally say "Give it a Try !!!" but I never did this before. So, try it out and tell us all if it worked. 

Give it a Try !!! UPDATE 2014-09-22 15:07 EDT This will group by date and hour with a summary by date and an overall summary 

I'll leave the FW/SecurityGroup stuff to you. Once these are cleared up, run the with the proper private IP. Please don't use . 

PROBLEM It is logically possible because --single-transaction gets "thrown under the bus" if any commands are launched intermittently during the mysqldump (See this post from mysqlperformanceblog.com). What happens when a dump faces off against an ALTER TABLE ? 

If you look at Dump of foreign key constraints with mysqldump not sorted, you will note that this was a feature request for InnoDB long ago. This same feature is probably not properly implemented in NDB. I am sure if you change the to in your script and run it, it will work without incident. So, YES, is being ignored. 

For every table that uses INT and you want to switch to BIGINT, you must forecast how much additional space to expect. For example, for figure out how much space will increase when you shift all INT(10) columns to BIGINT, run this query