Xtrabackup is a useful tool. If you have replication, additional tools such as Maatkit will be extremely helpful. 

No, not on the network layer. You could perhaps achieve your goal using something like a proxy. You could also emulate the behavior using a script but it would likely be fallible. To be clear, my point was that you cannot dynamically route based on hostname. I am not contesting what splattne said. 

Plenty of data provided by the companies selling their anti virus software. Best practices and regulatory practices in certain industries require its installation. For example, the PCI DSS requires it. If you have a workstation that gets compromised by a self propagating worm, it's likely that any Windows servers on the same subnet will be compromised as well. Unless the servers are storing restricted data, the only risk is to availability. If you are able to justify the potential risk, go for it. I believe the argument you provide is technically legitimate. You also risk the perception of those who can influence your success within your career, as most people believe it to be absolutely necessary for Windows. Of course, if you want to make this risk, you should enforce certain practices including but not limited to: 

sets the default permissions for the user creating files. This is set in the shell environment. For bash system wide, that's . Or, you can set it for the user in or depending on how the shell is executed. 

The size of the company and the industry largely influences the structure of the IT department. In some cases, a little too much. My professional focus is an Internet technology infrastructure. I encourage the separation of duties between primary production and internal intranet support. Even with a technology company, there will be internal only applications and support demands. Often, helpdesk would fit within internal intranet support. Depending upon the size of your environment, you could potentially justify separating the duties in production between architecture and maintenance. I like the junior to senior level approach, which is documented well by SAGE's job descriptions for system administrators. This is a reasonable rule of thumb to follow in general. In a smaller environment, separating duties too much will not be justified and may encourage siloing within a department. Smaller environments benefit from shared knowledge and responsibilities. As the company and IT department grows, it may make sense to start separating duties more specifically. Key areas, which often benefit from some separation of responsibilities: 

The filesystems at boot in most distributions use . It mounts all filesystems in fstab with specified, which is part of the specification. For ext2/ext3, can modify and display the settings that cause the filesystem to be fscked. For example, allows you to specify how many mounts until the filesystem is fscked. Ultimately, your assumption regarding the boot process is not accurate. is the solution to see if it's mounted. With ext3, I do not believe the "mount status" is stored with the partition. If you describe why you're incertain be it shared storage or NFS, we might be able to provide a recommendation applicable to your specific situation. 

Intrusion Detection Systems (IDS) and Intrusion Protection Systems (IPS) are a rather broad topic. As such, my answer here is far from comprehensive. The types of IDS include network and host based. Network based IDS, such as SNORT, analyze and log network traffic based on a set of rules. These rules would match potential vulnerabilities, thus potentially providing advance warning of attempted intrusion and forensic data after the fact. Host based IDS include software such as AIDE, which compare hashes of the files on the filesystem on a recurring basis. This would allow someone to monitor changes on the system and identify unauthorized changes. Central logging could arguably be part of your host based IDS solutions. Central logging enables control and auditing of your logs in a central place. Additionally, keeping logs in a central place minimizes exposure and allows an additional audit trail, in case a system is compromised and the logs are no longer trusted. Packet filtering (firewalling) is a security mechanism for controlling traffic to and from your network. Firewalls are not IDS. A well run IT infrastructure includes many of these technologies and many professionals would not consider them optional. 

There's a command line utility called blat that can be used to send e-Mail in Windows. In UNIX, you could use the userland tool to interact with the queue. Most MTAs have equivalents symlinked from the same location. The mail utility is available on most systems as well. I'm entirely on board with Chris' monitoring recommendation, if you don't already have monitoring. Otherwise, verification of functionality after a change makes plenty of sense. 

In a nutshell.. System wide with RHEL/CentOS, the initialization scripts are contained within , which is System V style initialization. Others, such as Slackware, use BSD style initialization. The scripts are started by init, which is configured in . On the user level, it is going to be unique to your shell. With , the default shell in most Linuxes, an interactive shell is going to use both and . Non-interactive will use . There are a lot of details here and if you want a particular answer, you will need to clarify. 

Prolexic is a Denial of Service (DoS) mitigation service that I've had pleasant experience with but it isn't low cost. Otherwise, it's going to be circumstantial. Without huge pipes, expensive networking equipment, and strong technical knowledge these situations are difficult to deal with. Some ISPs are more helpful than others. 

You could bind Apache to the tunnel interface. Having any part of the server on the Internet substantially increases the risk and you would not be able to consider the tunnel fully secured. 

Quotas are enforced on a per filesystem basis. If /tmp has enough space available, there's a good chance you can use it. To pipe tar through SSH: 

Consider specifying in the cnf instead. Typically, flags are specified in the init script, which would be located in . 

You have to change the password on the server itself. Typically in UNIX this will be the system user account. Once you had a shell session established, you would run the command. 

To prevent a Common Name mismatch, you will need to purchase either a wildcard certificate or a Subject Alternative Name (SAN) certificate. The SAN certificate will allow you to specify multiple CN. SAN certificates are also known as Unified Communications Certificates. If cost is an issue, it is not unreasonable to only use SSL for and not . It is often more difficult to justify the additional cost for wildcard certificates. 

You can use mod_alias and Redirect based on directory. The linked document identifies additional details. 

The domain name registrar will not affect the availability of your domain. There is substantial architecture that is necessary to reach "6 nines" availability. You will not insure that level of availability by simply having multiple DNS providers. 

I hear that Web sites are pretty cool to run and use, there's always Apache. Fun things but could be applied to work as well.. 

Looks like it might be a hard disk issue. Use a boot disk (UBCD?) to run disk diagnostics. You could also use a Linux rescue disk to attempt to access the filesystem and verify the configuration. System was working before, right? If so, have any changes been made to the system recently? 

Dell hardware RAID solutions that include hot pluggable backplanes will automatically rebuild in swap. I use the RAID utilities to verify successful rebuilding. I cannot specifically speak for HP hardware but I would be shocked if it differs much. No disk formatting or filesystem creation is necessary. 

If you have already isolated the server without a doubt.. iptraf is useful for bandwidth measurement on a local server. ntop can be useful to breakdown on a higher level. If you install and start to running now, it can analyze past traffic. Ultimately, tcpdump and wireshark should be all you need. Otherwise, I would spend time on the network equipment to isolate and analyze the traffic. Netflow is something network admins like to pay a lot of money for but the OSS tools aforementioned can accomplish the same. Ultimately, you're unlikely to have evidence specific to traffic in logs if you have not already configured something to do that. Depending on the protocol, you could analyze Apache logs and other application logs for historical patterns. If you have not isolated on any level, you are going to have a difficult time without enabling additional tools. 

If I understand correctly, you have mod_ssl installed and operational with the configuration but still receive an error when running configtest. Are you using the appropriate to your installation? If you have a package installed and it compiled from source as well, you may be running apachectl with a binary that isn't mod_ssl aware.