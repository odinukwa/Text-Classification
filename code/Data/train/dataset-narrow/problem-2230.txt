Something you might be interested in is the notion of quantum oracle introduced by Aaronson and Kuperberg in arXiv:quant-ph/0604056. Quoting from their paper: 

I am talking about publications in journals, not blog posts or technical reports. Also, I tagged it as big-list, with the hope that it will actually be. 

This paper came up on the arXiv today and it improves on the upper bound on $bs(f)$ in terms of $s(f)$. They prove the following bound: $$ bs(f) \leq 2^{s(f)-1}s(f). $$ This along with the connection that Marcos mentioned in his comment should give better bounds than previously known. 

Moved my comment here after Suresh's request. An example of a natural problem for which we only know algorithms that require error on both sides is the following: given three algebraic circuits, decide whether exactly two of them are identical. This comes from the fact that deciding whether two algebraic circuits are identical is in co-RP. Reference: see the post How Many Sides to Your Error? (Dec 2, 2008) about the very same question on Lance Fortnow's blog and the comments below his post for a discussion about the naturalness of the problem. 

From what I understand, this can be proved by showing that "small" $AC^{0}$ circuits with a MAJORITY gate at the root fail to compute the PARITY function even on a (1/2+small)-fraction of inputs. I found such claim in Theorem 6 of [3]. Does this reasoning make sense? Am I interpreting Klivans' theorem in the right way? I wrote "largest" class in the title, because I am also interested in classes that have this same property and that are uncomparable with $PP^{PH}$. 

This is amazing and it's the first time that I have seen it. I have always wondered why authors of paper don't write how they got to the proof, including the failed approaches they tried before getting to the track that led the solution. When I saw Ryan's paper on the arXiv, I felt very motivated to read it. I consider it a revolutionary paper from this point of view. Most of the time the only thing you can do with a paper is verifying its correctness. The question is the following: 

M. G. Parker and V. Rijmen have studied the quantum entanglement of binary and bipolar sequences in arXiv:quant-ph/0107106. I remember reading their paper long ago. They use a lot of terminology from coding theory, so now by just skimming through, I can't understand exactly what their result is, but you can have a deeper look. Notice also that graph states are a special case of your states. In particular, $\psi$ is a graph state if and only if the sequence $x$ is generated by a quadratic polynomial in GF(2). 

Another problem with this phenomenon is the MINIMUM $t$-SPANNER problem on split graphs. For a constant $t$, a $t$-spanner of a connected graph $G$ is a connected spanning subgraph $H$ of $G$ such that for every pair of vertices $x$ and $y$, the distance between $x$ and $y$ in $H$ is at most $t$ times their distance in $G$. The MINIMUM $t$-SPANNER problem asks for a $t$-spanner with minimum number of edges of a given graph. A split graph is a graph whose vertex set can be partitioned into a clique and an independent set. In this paper it was shown that MINIMUM 2-SPANNER on split graphs is NP-hard while for each $t \ge 3$, MINIMUM $t$-SPANNER is easy on split graphs. 

I like this textbook very much: Sanjoy Dasgupta, Christos Papadimitriou, and Umesh Vazirani: Algorithms Published by McGraw-Hill 2007. I don't calculate your suggested ratio but I think you will also like it :) 

This is not a new answer but rather a clarification the first and easy-to-obtain reference for hardness of INDEPENDENT SET in triangle-free cubic planar graphs: The note by Owen Murphy, Computing independent sets in graphs with large girth, Discrete Applied Mathematics 35 (1992) 167-170 proves that 

I cannot beat David in the elegance of his answer. But after spending al lot of time on thinking about this problem I would like to betray my solution to you though ;) Let $k \ge 2$ be a fixed interger. Given $G$, construct $H$ as follows:Take two copies $G_1$, $G_2$ and a clique $Q$ on $k$ vertices $x, x_1, x_2, \ldots, x_{k-1}$, a new vertex $y$, fix a vertex $v_1 \in G_1$ and a vertex $v_2 \in G_2$. $H$ is obtained from $G_1, G_2, Q$ and $y$ by joining $x$ to $v_1$, joining $x_1, x_2, \ldots, x_{k-1}$ to $v_2$ and joining all neighbors of $v_1$ in $G_1$ and all neighbor of $v_2$ in $G_2$ to $y$. Then it can be easily seen that $G$ has a Hamiltonian cycle if and only if $H$ has a completeness tree with at most $k$ leaves. 

While the problem "is the crossing number of a graph at most $k$?" is trivially in NP, the NP-membership of the related problems for the rectilinear crossing number and the pair crossing number are highly not obvious; cf. Bienstock, Some probably hard crossing number problems, Discrete Comput. Geometry 6 (1991) 443-459, and Schaefer et al., Recognizing string graphs in NP, J. Comput. System Sci. 67 (2003) 365-380. 

Something that has not been mentioned so far (as far as I can see) and that holds in the unrelativized world is the following: $$PH \subseteq PP \quad\mbox{ if }\quad QMA = PP.$$ This was observed by Vyalyi in this paper and comes from the strengthening of two theorems: 

A nice description of an oracle that separates P and BPP is given by Greg Kuperberg in one of the comments of this interesting blog post, where Terence Tao describes Turing machines with oracles and complexity results relative to oracles in the form of an allegory. 

Toda's theorem - Vyalyi shows that one query to a $\sharp P$ oracle is enough for a "$P$ machine" to simulate $PH$. The inclusion $QMA \subseteq PP$ proved by Kitaev and Watrous. Vyalyi proves that $QMA$ is also in $A_0PP$, a class that is contained in $PP$. 

First of all, there is a formal definition of "quantum-NC", see QNC on the zoo. GCD is indeed a good candidate for a problem that could be shown to be in QNC, but it's not known to be in NC. However, finding a QNC algorithm for GCD is still an open problem. The feeling for which this is believed to be true comes from the fact that the Quantum Fourier Transform can be done in QNC. Reference: Conclusion section of "R. Cleve and J. Watrous, Fast parallel circuits for the quantum Fourier transform", arXiv:quant-ph/0006004 

Frederic Green, An oracle separating $\oplus P$ from $PP^{PH}$, Information Processing Letters, '91 James Aspnes, Richard Beigel, Merrick Furst, and Steven Rudich, The expressive power of voting polynomials, STOC '91 Adam Klivans, On the Derandomization of Constant Depth Circuits, RANDOM '01 

There isn't a one word answer to your question, but you can have a look at Xiaotong Ni's master thesis, where commuting circuits with several restrictions are considered and compared to classical classes. There you can also find the definition of the class IQP, which is a subclass of polynomial size commuting Pauli circuits. 

This doesn't directly answer your question about a definition of a complexity class that represents the model you describe. Still, the notion of quantum oracle has relevance in complexity theory: in their paper Aaronson and Kuperberg use a quantum oracle to give a separation between QMA and QCMA. 

Basically everything that is known about the Quantum PCP conjecture has been collected in this survey by Dorit Aharonov, Itai Arad, and Thomas Vidick: The Quantum PCP Conjecture See also Thomas' blog post on the topic. 

The perceptron algorithm as such is a simple case of Gradient Descent (which is widely used for convex optimization). It minimizes a convex function (here the sum of inner products of misclassified examples with the current hypothesis) by making a step in the direction of the gradient of the convex function. As such perceptron is not necessarily an online learning algorithm. There is however work on primal dual interpretations of online learning algorithms that seem to be relevant to you: $URL$ 

Whenever $f(S)$ counts the number of edges $(u,v)$ satisfying some Boolean predicate defined in terms of $u\in S$ and $v\in S$, then what you wrote is just a Boolean 2-CSP. The objective function asks to maximize the number of satisfied clauses over all assignments to the variables. This is known to be NP-hard and the exact hardness threshold is also known assuming UGC (see Raghavendra'08). There are many natural positive examples when you want to maximize over subsets of edges, e.g, Maximum matching is one example of a polynomial time problem in this case. 

The standard reference for such a positive result is Piotr Indyk's paper on stable distributions: $URL$ He shows a dimension reduction technique for $\ell_1$ where the distance between any pair of points does not increase (by more than factor $1+\epsilon$) with constant probability and distances do not decrease (by more than factor $1-\epsilon$) with high probability. The dimension of the embedding will be exponential in $1/\epsilon$. There are probably follow up works that I'm not aware of. 

There is basically an entire course devoted to this question: $URL$ The course is still ongoing. So not all notes are available as of writing this. Also, some examples from the course were already mentioned. 

You are right in that the integrality gap of a relaxation has as such nothing to do with any rounding algorithm. These are two different notions. An integrality gap is a property of a particular relaxation. That is, how much larger is the value of that relaxation compared to the optimal integral value? Why do we care about linear/convex relaxations? To efficiently approximate an integral value. Hence, we typicaly talk about relaxations only in cases where the optimal value is hard to compute and we're interested in efficient approximations. Integrality gaps show us the inherent limitations of what can be achieved by such techniques. So, why do we care about rounding algorithms on top of the relaxation? We use rounding algorithms to solve the algorithmic problem of finding a near optimal solution as opposed to just approximating the value of an optimal solution. Moreover, often rounding algorithms are used to bound the integrality gap of a relaxation in the first place.