The terms "stratification" and "boxes" come from proof nets. Elementary linear logic ($\mathbf{ELL}$) was originally introduced by Girard as a variant of light linear logic ($\mathbf{LLL}$) and its execution was formulated in terms of proof nets. It is on proof nets that the elementary bound is satified, i.e., every $\mathbf{ELL}$ proof net $\pi$ may be reduced to its cut-free form in a number of steps bounded by $$\left.2^{\vdots^{2^s}}\right\}d$$ where $s$ and $d$ are the size and the depth of $\pi$ (maybe the height of the tower is not exactly $d$, it is linear in $d$ I guess). The size of a proof net is similar to the size of a $\lambda$-term. The depth, on the other hand, is the maximum number of nested boxes in $\pi$. A box is basically a sub-proof net which may be duplicated or erased: remember that in linear logic there is no free contraction or weakening, which means that proofs in general may not be duplicated or erased; those that can must be marked with a special construct, called box. Stratification refers precisely to the depth. It is an informal word that people in the linear logic community use to describe the restricted cut-elimination dynamics that is typical of systems like $\mathbf{ELL}$. In full linear logic proof nets, cut-elimination may completely alter the depth: if $\pi\to\pi'$ by means of a cut-elimination step and $a$ is a node of $\pi$ at depth $i$ which has a residue $a'$ in $\pi'$, the depth of $a'$ may be anything between $0$ ($a$ is "pulled out" of all boxes) and $2d$, where $d$ is the depth of $\pi$ ($a$ is at maximal depth and it "enters" inside a box which is also at maximal depth). On the contrary, $\mathbf{ELL}$ proof nets, because of the structural constraints that define them, have the remarkable property that the depth is invariant under cut-elimination: in the above case, the depth of $a'$ is exactly $i$. This stratification property is essential in proving the complexity bounds of light logics (elementary for $\mathbf{ELL}$ and polynomial for $\mathbf{LLL}$). From the point of view of Lamping's algorithm, the depth corresponds to the integer label of fan nodes. Stratification means that a sharing graph corresponding to an $\mathbf{ELL}$ proof net will need no brackets and croissants to be evaluated, because the labels of fan nodes do not change. After Girard, people started applying the principles of light and elementary linear logic to define type systems for usual $\lambda$-terms (instead of proof nets) which would ensure interesting normalization properties. The paper you mention falls in this line of work, which explains the terminology they use. Informally, typing a simply-typed $\lambda$-term amounts to decorating it with "boxes", which is what their so-called pseudo-terms are for. For the rest, $EAL^\star$ is just like any other non-trivial type system for $\lambda$-terms: there is no simple description of what a typable term looks like; the shortest description is its type derivation! 

For what concerns the use of non-flat domains, babou already gave examples. I can add that sometimes it may even be useful to see integers as streams: there's ⊥, above which there are 0 and S⊥, above the latter there are S0 and SS⊥, and so on. I know that in the early 90s Loïc Colson worked on models using the above interpretation of integers, although I don't know exactly for what purpose. So the usefulness of non-flat domains may be taken as understood and I will add some motivation as to why flat domains tend to be the "default choice" for interpreting base types. In languages like PCF, the normal forms of every base type plus the equivalence class of all diverging terms of that type form a flat cpo with respect to the observational preorder. Therefore, if one is interested in full completeness (or even just adequacy) for PCF, interpreting base types as flat domains is a natural choice. It works too: in all fully abstract models of which I am aware of (Milner's syntactic model and both Abramsky-Jagadeesan-Malacaria and Hyland-Ong-Nickau games models) base types are flat. In fact, the difficulty in achieving full abstraction is to capture the sequential behavior of PCF (and, more generally, the operational behavior of its extensions, such as non-deterministic, probabilistic, quantum or whatever) and this has nothing to do with the flatness/non-flatness of base types (in fact, as I said above, from the syntactic point of view base types are flat). So, in the context of adequacy/full abstraction, which is very important (and historically fundamental), there is no need to go beyond flatness, or at least we have not yet found a compelling reason to do so. 

The concept of primitive recursion function easily generalizes to any inductive data structure. For example, you can define the primitive recursive functions on binary strings as the smallest class of functions containing $\epsilon$ (the constant "empty string"), $S_0,S_1$ (append $0$ or $1$), projections and closed under composition and the scheme allowing to define a function $f$ by $$ \begin{align*} f(\epsilon,\vec x) &= g(\vec x), \\ f(zi,\vec x) &= h_i(z,\vec x,f(z,\vec x)), & i\in\{0,1\} \end{align*} $$ as soon as $g$ and $h_0,h_1$ are already in the class. In the above setting, the usual numerical functions may be encoded either by considering only unary strings (i.e., $n$ is represented by $1^n$), in which case you can mimick the usual primitive recursive functions, or, more interestingly, by representing integers in binary. In that case, the successor function is not part of the basic functions but must be defined (and this is not immediate). As you point out, traditional recursion theory considers primitive recursion on unary strings (or tally integers) because in that setting complexity is not a main concern. On the other hand, whenever complexity classes are defined in terms of recursive functions, primitive recursion on binary strings (or binary integers) is routinely used. See for example the seminal paper by Bellantoni and Cook "A New Recursion-Theoretic Characterization of the Polytime Functions", Computational Complexity 2(2):97-110. 

Concerning the system you are referring to (from the ATTPL book), I am pretty sure it cannot decide every language in $\mathsf P$. It certainly cannot compute every function in $\mathsf{FP}$: as mentioned in the notes of that chapter, that system is taken from Martin Hofmann's LICS 1999 paper ("Linear types and non-size-increasing polynomial time computation"), in which it is shown that the representable functions are polytime and non-size-increasing, which excludes lots of polytime functions. It also seems to give a serious limitation on the size of the tape of the Turing machines you can simulate in that language. In the paper, Hofmann shows that you can encode linear space computation; my guess is that you will not be able to do much more, i.e., the class corresponding to that system is roughly the problems solvable simultaneously in polytime and linear space. Concerning your second question, there are several $\lambda$-calculi that can solve exactly the problems in $\mathsf P$. Some of them are mentioned in the notes of the ATTPL chapter you are referring to (Sect. 1.6): Leivant's tiered $\lambda$-calculus (see his POPL 1993 paper, or the paper with Jean-Yves Marion "Lambda Calculus Characterizations of Poly-Time", Fundamenta Informaticae 19(1/2):167-184, 1993), which is related to Bellantoni and Cook's characterization of $\mathsf{FP}$; and the $\lambda$-calculi derived from Girard's light linear logic (Information and Computation, 143:175–204, 1998) or from Lafont's soft linear logic (Theoretical Computer Science 318(1-2):163-180, 2004). Type systems arising from these latter two logical systems and ensuring polytime termination (while still enjoying completeness) may be found in: Patrick Baillot, Kazushige Terui. Light types for polynomial time computation in lambda calculus. Information and Computation 207(1):41-62, 2009. Marco Gaboardi, Simona Ronchi Della Rocca. From light logics to type assignments: a case study. Logic Journal of the IGPL 17(5):499-530, 2009. You'll find lots of other references in those two papers. To conclude, let me expand on Neel Krishnaswami's remark. The situation is a bit subtle. All of the above $\lambda$-calculi may be seen as restrictions of more general calculi, in which you can compute much more than just the polytime functions, say for example system F. In other words, you define a property $\Phi$ of system F programs $P:\texttt{string}\rightarrow\texttt{bool}$ such that: soundness: $\Phi(P)$ implies that the language decided by $P$ is in $\mathsf P$; completeness: for every $L\in\mathsf P$, there is a system F program $P$ deciding $L$ such that $\Phi(P)$. The interest is that the property expressed by $\Phi$ is purely syntactic and, in particular, decidable. Therefore, completeness can only hold in an extensional sense: if $L$ is your favorite language in $\mathsf P$ and if $P$ is your favorite algorithm for deciding $L$ expressed in system F, it may be that $\Phi(P)$ does not hold. All you know is that there is some other system F program $P'$ deciding $L$ and such that $\Phi(P')$ holds. Unfortunately, it may happen that $P'$ is much more contrived than your $P$. Indeed, completeness is proved by encoding polynomially-clocked Turing machines as system F terms satisfying $\Phi$. Therefore, the only guaranteed way of solving $L$ using your favorite algorithm is implementing that algorithm on a Turing machine and then translating it in system F using the encoding given in the completeness proof (your own encoding may not work!). Not exactly the most elegant solution in terms of programming... Of course, in many cases the "natural" program $P$ does satisfy $\Phi$. However, in many other cases it does not: in the LICS 1999 paper mentioned above, Hofmann gives insertion sort as an example. Intentionally complete type systems, which are able to type exactly the polytime programs of the wider language (system F in my example above) do exist. Of course, they are undecidable in general. See Ugo Dal Lago, Marco Gaboardi. Linear Dependent Types and Relative Completeness. Logical Methods in Computer Science 8(4), 2011.