The image shows two upscaling-techniques and shows how different the results can look if different techniques are used. (A nearest-neighbor scaling on the left and a 2*Sal on the right.) Imagine that there are also such differences if you downscale an image. You can find a nice overview on different scaling techniques on the wikipedia page for Image scaling. 

A raster display draws every pixel on the screen in every frame whether there is something to show or not. A random scan display activates only those pixel which are occupied by an geometric primitive. So yes, they both use pixels, but the difference is in how they draw the pixels onto the screen. 

But each this call takes nearly half a second. I think optix does quite some work in the background when I switch this object out. This delay is not acceptable for me as it makes the playback of my "animation" stutter. Has anyone a idea how to do this more efficient? 

Let us take one step back. When you do path tracing, you are doing a Monte Carlo integration. What does this mean? You try to solve $\int f(x)\mathrm{d}x$ by sampling. The Monte Carlo integration says for a sufficiently big $n$: $\frac{1}{n}\sum_{i=0}^n f(x_i) \rightarrow \int f(x)p(x) \mathrm{d}x$ If we instead sum $\frac{f(x_i)}{p(x_i)}$ we get $\frac{1}{n}\sum_{i=0}^n \frac{f(x_i)}{p(x_i)} \rightarrow \int f(x) \mathrm{d}x$. Now your $f(x)$ is the rendering equation and you got yourself a way to approximate the rendering equation. Now that we are on the same page about the basics, let's consider what you do. You generate a path of light, that will become one of the samples $\frac{f(x_i)}{p(x_i)}$. In order to do this you model the interaction of the light with the surface at each step along the path. And you always update your current path probability $p(x_i)$. What is now the correct way to handle a material that has multiple different ways that a ray could travel? There is no one correct way. The only important part is, that your values $n$, $f(x_i)$ and $p(x_i)$ stay correct. When you diverge the path and spawn two rays, you have to increase the number $n$ of total samples accordingly because now you have done two samples. Then handle both pathes as if they were separate from the beginning. If you want to only continue one path, you can do it with the russian roulette way. Define three intervals (one common way is to chose the intervals to be a partition of the numbers between 0 and 1). The relative sizes of these intervals have to be proportional to the material. The tricky part is how do you design this. In my class we defined a number reflectivity $\rho$. We draw a random number between 0 and 1, if it is lower than $\rho$ the path ends. Else we reflect. The BRDF will take care to weight the specular vs diffuse part if you do enough samples. Of course you can improve this via importance sampling, and that is probably where you have problems. For a very specular surface design a random distribution that just draws more directions from the specular reflection direction. Rest will be diffuse. I don't have enough experience with actually bulding path tracers to help you how to design one, but your sources will probably have something here. Important here is again: Design it so that you know how probable a specific direction is, and then keep your $p(x_i)$ up to it, so you can in the end divide by this and get a correct Monte Carlo estimate. TL;DR There is no correct way. It is ONLY important to keep the probabilities of samples in mind. I hope I haven't missed the point of your question entirely. Addition: Monte Carlo samling, as long as you correct weighting by probability, will always with infinite samples converge against the correct value. This means even if you use a complete stupid way to generate samples, if you do it long enough, that will work. Maybe not in your lifetime or that of your great grandchildren, but it will. 

I think you can calculate some surface-bound barycentric coordinates for each point on the surface, and then use them to check for inside or outside of the triangle. I don't have an exact algorithm at hand but I found this following paper which does seem to handle exactly this kind of coordinates. Barycentric Coordinates On Surfaces 

A quote from 'Multi-View Stereo: A Tutorial' by Yasutaka Furukawa and Carlos Hern√°ndez. So to paraphrase: We have a set of images that is larger than two, and use them in a pairwise manner by applying techniques that finde stereo correspondences to reconstruct the object shown in them. If you want to know more about MVS algorithms that paper I linked seems to be pretty good and at least Furukawa is a name that I know and have read papers from, so he seems to know the topic. 

I think getting the real data with high precision is not easy without some API. The first idea would be just to manually use some color based selection tool that you can tell to select all areas in the screenshots according to the lookup table. But that would not result in some 3D data, but only segmented images where you can annotate the values. Another method would be to make a UV-map of the mesh, and then by making screenshots from various regions try to extract the color-coded texture. Then you could read the interpolated color-values of the texture and map them back to the actual values. The last but maybe strangest idea would be to make a huge number of screenshots and then use some multiview object reconstruction algorithm (I don't know if there is software or if you have to write it yourself.). There are algorithms that not only reconstruct the geometry, but also the texture. This would then give you a model with the color-texture that again you could map back to the values. But ideally there would be some export function in the software you used. Maybe this is a question you should ask the developer of the software. Maybe they have this function somehow but it is a bit hidden. 

Both images are taken from this tutorial where you can also learn more about how shadow mapping works, how shadow acne is created and solved and what Peter Panning is. 

I have a Optix Raycasting renderer which loads and renders VTK files. For a single static model this works fine. But now my dataset consists of multiple timesteps which I want to display in a animated fashion. Each couple of frames or seconds the displayed model has to be switched out with the next one. If possible I want to refrain from rebuilding the acceleration structure each time this happens. So my question: Can I just load all models at once, place them in the global acceleration structure and then just switch all off except the one currently displayed. My hope is that this would be more efficient. Edit 1: I have now build a variant in which I create optix::Acceleration and optix::Group for each of the timesteps separate and each time the active timestep changes I just 

With help from the Nvidia Forum and two very helpful and friendly Nvidia employees I found the perfect solution. In OptiX there is a so called selector node. This is a special scene-graph node that utilizes a user specified program to select which child a ray should traverse and which not. 

$ c = param * c_i + (1-param) * c_{(i+1)}$ If I haven't messed up something in my mind, this should work for any piecwise linear function, and not only if your $t_i$ values lie between 0 and 1. 

I use Nvidia OptiX for Ray casting and tracing purposes. It is based on CUDA and gives you a library that runs on GPU and is capable of building and traversing efficient acceleration structures. You just supply a few programs such as intersection tests and shading and OptiX does the rest. I personally think it is really easy to work with it but your milage may vary. With this it should be very doable to make your project and even with decent running speed. 

As you see in the picture on the left the shadow is disconnected from the shadow casting wall. This gives the impression that the geometry hovers over the ground (just like Peter Pan can hover, hence the name Peter Panning). To solve this problem you have to use "thick" geometry that has a volume and then render the shadow map using the back-faces. If the offset is smaller than the thickness of the geometry there will be no Peter Panning. 

The only cost is one additional method call for each traversal and thereby also a slightly bigger stack which caused me some problems with a stack overflow so when using this, keep an eye on the stack size. 

If you scale an image down to a screen that has a lower resolution than yout image, you have to descide what you do, there is no single way of doing this. There are many different techniques that yield very different results. This is very similar to displaying a low resolution image on a high resolution display where do the inverse thing by desciding how to fill the "more" pixels on the screen. There are also many techniques for doing this. 

In general as I learned the distance between emitting and receiving surface is only important for Radiance because the projected area will be smaller if they are farther apart. The time you are missing in your definitions seem to come from the conversion of Watt to Joules Per Second. 

It seems that nothing was wrong with the code I posted. I deleted my complete build and rebuild it, and now it seems to work. I don't see anything on screen but i think that error is unrelated. I can see the matrix in the nsight debugger. 

I can only cite what I have learned in my lecture on Global Illumination techniques which was unfortunately some time ago: 

The problem with your calculations is that you use a uniform for viewPos which is the camera position. The rest of your calculations is in Camera-Space, so the viewPos should always be $(0.0, 0.0, 0.0)$ and thereby the $viewDir$ becomes $-1*\text{fragPos}$. In general: Always be careful about the different reference frames your calculations can be done in and aim to keep this consistent. 

Regarding optimizations: Could you calculate the min and max corners in modelspace beforehand? Then you could check on the CPU-side the NDCs of these two points and skip the processing of the whole mesh if it is outside of the desired range. This needs only two matrix-vector multiplications on cpu side. And since for rigid bodies these two points do not change over time, you don't have to iterate the meshes in each frame. 

This is only a rough idea, though I hope it may inspire better answers. The closest point on a surface is as far as I know always forms together with the red point a line that is orthogonal to the surface. If you know which yellow vertices form the surface that the green point lies on, this is easy. You could just formulate the normal form of the surface (which is piecewise linear so it works out) formulate the orthogonal line to the surface that goes through the red point and solve for the intersection point. Though I don't have the exact math here, this step should be easy to look up. Now the problem is: Which linear piece of your surface is the closest one. Either you calculate the green-point candidates for all linear pieces and also the distance and take the closest one, or you find some other possibly faster way for this. For that I don't have a more clever idea. 

Topologically correct is a very vague. I believe you think of Polygon-Meshes when you say meshes, which is to represent the surface of the object by a patch of polygons, in most cases triangles or quads. There are other options to model meshes, one idea would be to use signed distance functions as for example in constructive solid geometry. So back to the question. Is it correct to mix the type of polygon? Yes. There is, mathematically and theoretically nothing stopping you from doing it. The only real 'correctness' criteria that comes to my mind is the one of the mesh being a manifold. This is in (overly) simplficated the property that the mesh has a defined outside and inside and no strange places where you can't decide. (This happens for example if the mesh intersects with itself, but it get's freaky). And having different types of primitive does not violate the manifoldness of the mesh. But most algorithms can actually deal with non-manifolds. There is no problem in rasterizing or raycasting a non-manifold as these algorithms process each primitive separately. The more interesting question here is: Why DONT we mix the type of primitive? And here the answer is very simple: Efficiency. You want to design an algorithm that handles your mesh as efficient as possible, and so you assume it's only one type of primitive. And at some point in time, because triangle-meshes are really handy, hardware and hardware-api began to adpot mainly to processing triangles. And now they are good at that and even other primitive types get split into triangles since splitting and rendering triangles is still faster than rendering the polygons directly. So in short: You can mix polygon types, but if you want to be fast, use only triangles since they are simple and widely used. 

This is more a Computer vision question I think. I don't know how exactly you can achieve that but some ideas and keywords you may use to find additional resources: Feature tracking of hand and pen can yield the movement of said things. Assuming you know that the camera is not moving I think that would give you good results. Related is maybe motion tracking. Any information you have about the hand and the camera can in some way be used to make more accurate estimates. Removing the hand from the video could be much harder. I don't remember that part from my lecture. We copied parts of the image onto the part we want to remove and used image metrics to make sure it fits best possible... But since the hand covers a drawing we don't know much about this could be very hard to do in an convincing way. 

I think the most common way to have a the "up and down" movement of a vehicle is to actually build a physics simulation. It does not have to be a very sophisticated one and how much realism you want is up to you. But just in each frame update the velocity of the car in downward direction (gravity) until it hits the road. Then if the road has an upward slope it will also push the car up (by colliding with the car). Now you have a new problem and that is the problem that the car remains parallel to the ground-plane. And the physical calculation behind this is a bit more complex. When detecting if your car hit the ground you would have to detect the exact position of the hit (which part of the car hit the floor?). Then using the gravity, mass and some material properties (all of which you could just set globally and assume as easy values for simplicity) you can calculate how the reaction force to the collision will be. And you know that the force will be exerted on the collision point. Now when you have the center of mass of the car, and the collision point, and the force created by the ground-collision, you can calculate how to rotate the car. This is only a very rough sketch of how I learned to handle such things in a "real" way. I hope this was not too confusing, if you have additional questions I can rework the answer later. 

A simple way would be to think about how you can decompose your star into triangles. And then just draw these triangles. Not as a triangle-fan since those may be a bit strange to use. Since triangles are one of the most basic and standard ways to draw all kinds of meshes and shapes (every polygon can be triangulated as far as I know), I suggest du don't use the polygon draw as a beginner. Begin with the basic stuff everyone uses.