I've got a Linux server I can only connect to remotely. I want to backup it, but it'll have to be over the Internet, and I've chosen Google Drive to hold the backups. The only piece of puzzle I still don't have is how to package and compress all the files. I want compression, because space on the Google Drive is limited, and it would also reduce upload times. I could of course use the standard tar+gzip/bzip, or zip, or maybe even something fancy like 7z for best compression. But what I'm wondering about is this - many of the files that will need to be backed up will be things like JPEG images, which don't compress well at all, no matter which compressor I use. It would be faster if those files were copied to the target archive as-is, rather than compressed. Other files are text files, which compress better with a specialized algorithm (can you tell yet that I'm backing up websites?). Is there some kind of archiver which recognizes such files (by file extension would be fine) and applies a different algorithm for them? I think I've seen one somewhere, but I don't remember which one it was and if it has a Linux version. Or perhaps I'm overthinking this? 

I understand that this question is not OS-specific, right? Under Windows, I tend to give all my machines as few partitions as possible, but no less than two - SYSTEM and DATA. If the machine has two physical disks, then one (smaller) will be SYSTEM, the other DATA. If there is just one disk, I split it in two partitions. The reason for that is just one - when I need to reinstall the machine (and there will be such a time), I don't have to worry about the contents of the SYSTEM partition - I just do a full format on it and a clean install. This of course means that my Documents (and preferrably Desktop too) has to be mapped to a folder on DATA, but that's easy to do, especially on Vista and later. I've also tried making more partitons (like GAMES, MUSIC, MOVIES, etc.) but that only resulted in some of them overflowing into others, creating more mess than order. 

My guess is that it should be at least as good as tapes. Perhaps even better, because the magnetic disk itself is in a contained environment and magnetically shielded. However, I'm pretty sure that nobody really knows what happens to a HDD after, say, 20 years of unuse. It's the same thing as with CD's and DVD's (the archival-grade ones) - they simply haven't been around that long. Well, OK, hard drives were there 20 years ago as well, but comparing them to hard drives of today is pointless. 

In a course about Windows Server administration that I took at the university they told us that a normal user could add 10 computers to a domain. Over the years however I've added and removed way more than 10 computers (virtual machines included) to the domain without any problems, while some colleagues of mine swear that they can't add any computers. My account is not priviledged in any aspect, I'm just a regular user like everyone else. So... what are the rules governing this? Can they be changed by the admin? Is there any way I can find out with my user-level-priviledges? Just curious. :) 

Actually, Microsoft Virtual PC already has an option called "Compact virtual disk". Read about it in the help file. However, to achieve the best reduction you will need to zero-fill the empty parts of the disk. For that you will need a third party tool - but there are plenty of those. Just google. Sorry, misread your post. You already tried that. In that case you didn't read the help file hard enough. I quote: 

I've bought a domain, say . I'd like to set up google apps to work with it. As such, I'd like to create subdomains like , , etc. To integrate it with google, I must add to all of them CNAME records to . However my domain registrar says that they can create only one CNAME in my domain, for which they chose . The rest of them, they say, I must create as A type records. Are they being lazy, or is there some limitation here that I'm not aware of? 

It's all about marketing and getting more money for Microsoft. Therefore the desktop versions of Windows have a severe limit on concurrent requests in IIS. Don't know the numbers by heart, but it would choke immediately if used under production load. You can however run 3rd party webservers just fine. I just don't know if any of them support ASP.NET. 

There is no open-source alternative that can do all that. Samba can do a useful subset. Why are you asking? 

What speed is the link between the 2 sites? What is the rate of change? 100MB/hr? 1GB/day? That will determine the tool to use. Tools like robocopy (and FRS) copy the entire file even if a single byte changed. I suspect rsync does too. DFS since Windows 2003 R2 is the successor to FRS and does byte-level replication. That is, only the changed bytes are transmitted, not the entire file. This can lead to substantial transfer time savings. 

How big is your intranet? As an alternative you could try the free MS Search Server Express which we use on our intranet (2000 employees). There are some docs on the web comparing the two. 

The jobs are stored in the system database MSDB. Did you back that up? You need to restore it to get your jobs back. 

You can have all the whizzbang technology in the world. Customers still won't be happy. You are simply enabling them to do their job. Their job may make them happy, but most times IT is invisible to them and only rears it's ugly head when things go wrong. 

You would have to get the hashes (either LM or NTLM) and do a dictionary attack on them. Anyone with a moderately complex password would be nearly impossible to discover. Don't enable "reversible encryption" - you need to change the way you manage passwords. 

I need to remote control a PC that is behind a SOHO ADSL router (netgear or similar). I do not have access to the password for his router so I cannot make changes on it. I can install software on his PC though. What remote control software is best that can work through http/https (I am assuming his router only has those ports open)? It also needs to work without having a person at the other end, as I will be accessing the PC out of business hours. 

For example, say I have a full backup done using robocopy. Can I then use rsync to replicate just the changes? Or will rsync do another full copy? I don't want that to happen because it's over a slow WAN link. 

Generally I ask "What has changed that might have caused this problem"? Most issues are caused by changes to known good configurations. If you can isolate who did the change then you usually get your answer. 

One of the best I've used is PRTG from Paessler. It is relatively cheap for a small number of servers. They charge based on the number of things being monitored (ping, free disk, cpu load etc). It's completely web-based and no agents are required. It comes with a load of predefined WMI queries and of course you can define your own (they supply templates you can modify). Plenty of charting and reporting available to impress management. 

We've seen users sneak Firefox onto their machines, even though they are not local admins. As it doesn't "install" you just need the expanded distribution on a network share and just run firefox.exe direct over the network. They found this out themselves when they discovered that IE6 (our SOE browser) is incompatible with a growing number of apps. So it can sneak in the backdoor! A big reason for not using Firefox is the lack of official ActiveX support. We have a number of these around the business. 

What Windows backup software do you use? Most have Linux agents (Backup Exec, Arcserve etc). If you are using NTBackup then there is no Linux integration. We have a similar situation. We just have a cron job that runs on the Linux box and copies files to a Windows server using Samba. From there the Windows backup software backs it up. Yes it's not very "Windows-ey". For that you really need a Linux agent which will have the smarts and integrate into the main backup console. 

Do you have a "delete first ask questions later" policy for music and movie files? Or are you more lenient? Just wondering if there's a difference between how large and small organizations deal with the issue. 

You can add custom reg keys by creating a custom adm file and importing it as a template into the Administrative Templates section of a Group Policy Object. Then link that GPO to your OU. There are docs at MS about how to do this, or you can look at the adm files that already exist on the server (somewhere under Sysvol I think). This process is called "tattooing the registry" and it means you are outside the control of group policy removal i.e. the reg entries will remain even if the policy is removed. You need to create a "reversed" reg key and deploy it (or just delete it).