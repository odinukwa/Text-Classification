is this even feasible? Yes. I would suggest you look at TXT records for holding this information what are the pitfalls? Not sure how do I size the DNS servers. Not sure, but as far as I understand it DNS is a fairly low requirement service using UDP to handle requests and replies - as such it is up to the client to determine if it got an answer or not and re-ask if it was unsuccessful. 

I have set up a business that ran purely on Open Source. SuSe desktop, Mitel SME for fileserver / eMail. It all ran beautifully until... 

People wanted to exchange files with the outside world (Open Office vs Microsoft Office) They wanted Salesforce integration - MS Office only They wanted screenpop / telephony CTI They wanted some industry specific applications (Windows only) 

There used to be an overriding reason to go with Linux for 64bit support (and therefore better memory management) - this is now reduced with the more stable support of x64 WIntel platforms. I cannot give you a definitive answer for why, but I looked after the IT support for a smallish development house who wrote applications against Oracle 7i, 8i, 9i and 10i - all database hosts were done vs Linux - on RHEL 3, 4 and 5. The primary reason for this was given as it was the most stable supported Host OS that Oracle ratified for use with their database. With the plethora of patches that Microsoft release, it was almost impossible to keep the host OS at a level that Oracle had tested and would support. Having used Oracle on Windows 2003 x64 and RHEL 4 x64 - the same database performed a lot better on Linux than on Windows - the back end storage was a 300Gb fibre presented raid 10 partition on an EMC array. Both systems were 'untweaked' 

Can you boot in vga mode? if so - try a less capable video card? I had issues after installing Vista on an nVidia machine. After rebuilding it a couple of times, I discovered that it was a known problem, and there was an updated nVidia driver (the 'even more recent' one on windows update was no good) 

If the hosting company is not the registrar you can go bug them. However most likely it is in which case you have to deal with the hosting company. Redemption period means that even tho its expired the domain you can still retrieve it from the registrar. I am surprised they are not agreeing to it. You can go to $URL$ and list a complaint. Nothing will happen probably but it will be a stain on their record. You could get a technical lawyer and see if you can send a nasty gram to them. Companies tend to listen when threat of litigation looms over them. However, it probably isn't worth the money and waiting would be a good bet. With the changes ICANN made domains are not being squatted nearly as much anymore since it costs them money now. 

Well if you got a ton of VMs running at the same time. having one VM hog all of the CPUs would be very bad no? With the CPU choice you could have 4 VMs each with dedicated access to one CPU without any context switching happening. 

From my perspective I would just pop it on EC2, why do the extra step. I wouldn't use the HSQL memory database but set up a different one such as mysql as I have had a bad experience performance wise with the default memory db. Also, the default amazon AMI has been a pain imo to set up graphical programs as X Window System is not listed under yum grouplist; so you will end up having to do some manual config me thinks. As far as the amazon reservations go. On Demand is the most expensive hourly. if you run it all year your bill will probably be around 30% more. Then there are the 3 RI levels: light,medium, heavy. All require some intial money down. light means the least money down but also the least savings from on demand. heavy requires the most down but has the most savings hourly compared to on demand. If you going to run the instance all year round for sure then go heavy. As as far as performance of a micro goes. Well, it is the cheapest but if I remember its not guaranteed even 1 cpu; its more for burstable hits. If its going to be used lightly you might be able to get away with it, but if you got a lot of developers and they are going to be generating those reports you might wanna aim for something a little more beefier. It is a shame m1.smalls are still 32-bit. 

I would advise against NFS. Simply put - we had a web server farm, with JBoss, Apache, Tomcat and Oracle all using NFS shares for common configuration files, and logging. When the NFS share disappeared (admittedly a rare-ish occurrence) the whole thing just collapsed (predictable really, and I advised the 'devlopers' against this config time shortcut). There seems to be an issue with the version of NFS we were using that, if the target disappeared during a write, the client would drop into a never ending wait loop, waiting for the NFS target to come back. Even if the NFS box reattached - the loop still did not end. We were using a mix of RHEL 3,4,5. Storage was on RHEL4, servers were on RHEL5, storage network was a separate lan, and not running on vlans. If there is a load balanced front end, checking single storage - would this not bottleneck your system? Have you considered a read-only iSCSI connection to your storage, with an event driven script to move the uploaded file to the storage via ftp/scp when a file is uploaded? The only time I have implemented a successful centralised storage for multiple read heads was on an EMC storage array... All other cost effective attempts had their drawbacks. 

A client of mine has been in this precise situation. He left his WiFi open to all as a gesture of good will, two months after he did, he got a warning from his ISP about his data usage, and a week after that he received a cease and desist letter from the RIAA He no longer has open WiFi. 

There was also some fairly major headaches getting single sign-on working with NIS/Kerberos and all in all, I think we spent more time in support than we saved on licenses from MS From a purely non-tech point of view, the differences between Open Office files and Microsoft files caused a significant headache. I know that OO saves as MS Office files, but it just did not work in the real world. To take eMail as an example - People also get comfortable with the Outlook instantaneous update you see with exchange - not so happy with a 5 minute poll on a local IMAP / POP server. 

One way would be to install a free VNC server/client on each machine. www.tightvnc.com . They would then login to each machine to see that machine. a free two way non setup way to see someone elses computer is $URL$ . However I believe this is only good for 1 to 1 at most. Now instead of having multiple machines login you can broadcast your desktop using $URL$ . However this only works on the LAN I believe. 

You can write a script that parses the log file of say ftp daemons such as vsftpd and proftpd and do something once it finds a line that matches what you want. However it would be different for each daemon as each has their own log style. On top of which, there will be delay as you will probably have to poll the file to see if its changed. A perhaps more portable solution and better response time would be to make a PAM module(WAY MORE WORK THOUGH) A lot of daemons have PAM support built in. so when someone logs into ssh for instance it will query PAM and the PAM config will pick for the login whether LDAP or UNIX file. then it has like password modules like cracklib to check if it works. it short you can append this line to the pam config of the particular service once you make a pam module. session required /lib/security/pam_yourmodule.so $URL$ for a quick primer on PAM. 

I'm surprised you were able to generate a certificate at all on their servers. Anyways, I doubt it'll work. You need to configure the web server to return that SSL cert and that probably requires admin priveledges that you don't have. So when you go $URL$ it will show their default SSL they set up on the site. Its ok to self sign your certs using your own CA in general. There is nothing less secure about them if you know they are yours. What you pay for from a CA is their signature saying your verified and webbrowsers store that CA's cert with the default value of trust. So it doesn't bitch like a self signed one does. However, again, I doubt you can do what you are planning since it requires access to places a normal web host shouldn't have. It is perfectly normal for most webhosts to charge for an extra IP and installation. 

The only effective way to do is is with mod rewrite rules. If the referrer is not from your own domain, then rewrite the image url to be one that is non-existant, somewhere else, or a 1px by 1px transparent gif. A lot of people do this with an image that says 'No leeching, buddy' or similar 

It is not possible to snoop on your machine without extra software. Remote Desktop does not allow a 'view' or share - Remote Assistance does but requires you to initiate it. VNC/RAdmin etc require that you install software on your machine, and although it can be push installed from a remote location, it's not generally done, and will alert you that it is installed or that someone is using the connection. The same applies to LogMeIn and GoTomyPC Back Orifice will do exactly what you are afraid of, but is detected by even the lowliest of Antivirus products. Dameware allows surreptitious observations of a machine (most others tell you that you are being observed) and also allows the remote administrator to push the installation onto your machine as standard. The real question you should be asking is this ... If you are scared of being discovered - should you really be doing it in the first place? Is it breaking acceptable usage policies? Are your actions illegal / against your employers company interest? If so - do not be scared of doing things you could be disciplined for by SIMPLY NOT DOING IT! Of course it is also possible to identify the port your PC is connected on, set that as a monitor port and send all its traffic to a packet capture station (thats what most big corporates do in cases where monitored activity is required) There - they see everything. I have before now installed SSH on windows machines, so that I can run command line utilities such as PSLIST and PSKILL to see if people are doing things they shouldnt and stop them) So the short answer to your question is 'No, not easily' The long answer to your question is 'Yes they can monitor everything you do, the level of their monitoring will take different amounts of effort, and often the effort is too much for the end results' 

They probably mean set the block size to 4k. $URL$ I think the default used to be 512Bytes for a lot of file systems but people moved to using 4k. There should be a way of setting it when you create the filesystem. 

if you want a script to start once at startup and theoretically it stays up then you should add it as a service to init.d with the appropriate runlevel set. Of course if it should go down then you would want to to come back up. to do that you can have a shell script run ps -aux | grep 'nameOfYourScript' like that. Of course don't include the grep command which will match as well lol. have that script check every five minutes with a cron like this */5 * * * * user checkScript.sh the checkScript you make could be written to start up the program. 

not quite sure what you mean. If you mean can a host set up a maximum file usage then yea. My host, hostgator has a 50,000 inode limit or something like that. that means any more than 50,000 files and I get banned. If you mean like filesystems if you can set that up quotas. You can in Linux, I am sure there must be some mechanic for windows as well. $URL$ 

Sounds like you might want a distributed file system $URL$ I had a Grid project and we had to built a compute cluster and decided to use OpenAFS which is the only one I know. It kinda sucks from a learning point of view cause its not easy imo. However that link I posted has a list of them. Some support replication and striping (fault tolerant and parallel. Just research those and pick one that you like). 

I use HostGator, they've been helpful on their forums and are generally explicit about what they don't and do accept. 

I was wondering if I am editing a file such as /etc/hosts or /etc/sysconfig/network should I put a . at the end of the name. such as test.example.com. Is there a difference? Would anything break either way. 

I think the most stupid thing I ever did was to remove the default route on out external facing firewall cluster - whilst vpn'ed to my desktop over a hundred miles away. Fortunately it was in a period designated as planned downtime (just in case), but that did not save me the 200 mile round trip to go and reconfigure the firewall onsite. It also did not help that this took out our internet exposed production systems for the duration of my travel and subsequent fix. We all know the definition of a nano-second. An ohno-second is even smaller, and is the time between hitting 'enter' and realising your mistake. 

The first indication that your DNS was not working, would be sporadic enterprise wide lookup failures. Note also, that almost every computer system that makes these requests, will cache the result for 30 mins or up to the records ttl value if its less than 30 mins. So careful handling of these values would be needed. 

Personally, I tend to use binary for every transfer. I was stung once with a file that had been placed from a windows machine onto a unix machine using binary transfer. As a result the txt file still had CR LF endings. ASCI mode looks at source and dest platforms and performs line-ending translation, so the resultant file I got had CR CR LF line endings (LF was translated by ASCI to CR LF as it was unix -> windows) Sounds petty - but it was a 20Gb log file, and I only had one time window in which to collect it. I use eol conversion utils on the local machine if the necessity arises. EDIT: I was getting the file back onto windows from the unix host 

I used a product by Castle Rock called SNMPc - its not the most polished of tools, but it does everything that you could want and wont break the bank. Its basically an SNMP statistics collation tool, that can baseline and warn if baselines are deviated from. It can be given thresholds for growth and decline warnings and works well with any SNMP capable device. Enabling SNMP in *nix is simple, as it is within Windows. Extensibility of SNMP is quite easy too (at least on *nix) SNMP is free - there are 3 levels; all to do with security. SNMP 1 is plain text and very 'insecure'. SNMP 2 is encrypted, but its trivial. SNMP 3 uses certificates. It can be a bit of a chore getting it to work the first time though. Because there are so many counters and statistics that you can pull, it can also take a while working out which ones are right for you - but once this is done, its very straight forward. You pay for front end collation and trigger on events to make SNMP useful. You can do it with open source software, but I wanted a modicum of commercial support. Data can be polled from the devices (normal) and on critical systems, you can get the individual system to send a trap event notifying the trap manager that something went wrong, and they need to know now, and not wait for the next poll period. Polling remote devices can be done by using a collection agent - same sort of thing as the console, but without all the reporting wizardry - that then pushes the stats at the central console periodically. Of all the monitoring systems I have used, SNMP kept supplying what I was asked for, and within the budget I was given. There is a product for Microsoft Servers called MOM 'Microsoft Operations Manager' where the 5 server workgroup version is (or at least was) free... but extending it to keep an eye on enterprise systems such as Exchange and SQL could cost a lot in licenses and connectors. Beyond that - My experience is limited to SNMP, MOM, and Spotlight (by Quest) which was awesome and a bit too far beyond our budgetary range for all but the most critical of Oracle Databases.