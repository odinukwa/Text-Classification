Let define the $(3,2)_s$ SAT problem : Given $F_3$, a satisfiable 3-CNF formula, and $F_2$, a 2-CNF formula ($F_3$ and $F_2$ are defined on the same variables). Is $F_3 \wedge F_2$ satisfiable? What is the complexity of this problem ? (Has it been studied before ?) 

You can easily generate directly Unique SAT formulas with reasonable size $(|F|<n+2^k)$ Let $m$ be the unique model - say $m$ contains only "0"s (rename the variables later if needed). Let $F$ a $k$-SAT formula satisfied only by $m$ - the maximum size of $F$ is the total number of clauses satisfied by $m$ i.e. $(2^k-1) \binom{n}{k}$. Take the $\binom{k}{1}$ clauses that eliminate all models assigning exactly one "1" among $x_1,x_2 \ldots x_k$: $(\lnot x_1, x_2 \ldots x_k)(x_1, \lnot x_2 \ldots x_k)\ldots (x_1,x_2 \ldots \lnot x_k)$ Take the $\binom{k}{2}$ clauses that eliminate all models assigning exactly two "1" among $x_1,x_2 \ldots x_k$: $(\lnot x_1, \lnot x_2, x_3 \ldots x_k)(\lnot x_1,x_2, \lnot x_3 \ldots x_k)\ldots (x_1,x_2 \ldots \lnot x_{k-1} \lnot x_k)$ Keep going until taking the only $\binom{k}{k}$ clause that eliminates all models assigning "1" to each variables among $x_1,x_2 \ldots x_k$. The only models which are not yet eliminated assign all $x_1,x_2 \ldots x_k$ to "0". Since $m$ is a model, then take any set of $n-k$ clauses that eliminate all models assigning "1" to $x_i (k<i\leq n)$ and $0$ to any $k-1$ variables among $x_1,x_2 \ldots x_k$, for instance: $(\lnot x_{k+1}, x_1 \ldots, x_{k-1}) \ldots (\lnot x_n, x_1 \ldots x_{k-1})$. Then $|F|=\sum_{i=1}^k \binom{k}{i} +n-k = 2^k-1+n-k$ To get more clauses, add any clause containing at least one negated variable. To get an unsatisfiable formula, just add a clause with $k$ unnegated variables. 

[1] Erik Waingarten and Amit Levi. Lower Bounds for Tolerant Junta and Unateness Testing via Rejection Sampling of Graphs, 2018. arXiv:1805.01074 [2] Eric Blais. Improved Bounds for Testing Juntas. APPROX-RANDOM, 2008. [3] Roksana Baleshzar, Deeparnab Chakrabarty, Ramesh Krishnan S. Pallavoor, Sofya Raskhodnikova, C. Seshadhri. Optimal Unateness Testers for Real-Valued Functions: Adaptivity Helps. ICALP, 2017. [4] Xi Chen, Erik Waingarten, Jinyu Xie. Boolean Unateness Testing with $\tilde{O}(n^{3/4})$ Adaptive Queries. FOCS, 2017. 

For randomized algorithms $\mathcal{A}$ taking real values, the "median trick" is a simple way to reduce the probability of failure to any threshold $\delta > 0$, at the cost of only a multiplicative $t=O(\log\frac{1}{\delta})$ overhead. Namely, if the $\mathcal{A}$'s output falls into a "good range" $I=[a,b]$ with probability (at least) $2/3$, then running independent copies $\mathcal{A}_1,\dots,\mathcal{A}_t$ and taking the median of their outputs $a_1,\dots,a_t$ will result in a value falling in $I$ with probability at least $1-\delta$ by Chernoff/Hoeffding bounds. Is there any generalization of this "trick" to higher dimensions, say $\mathbb{R}^d$, where the good range is now a convex set (or a ball, or any sufficiently nice and structured set)? That is, given a randomized algorithm $\mathcal{A}$ outputting values in $\mathbb{R}^d$, and a "good set" $S\subseteq \mathbb{R}^d$ such that $\mathbb{P}_r\{ \mathcal{A}(x,r) \in S \} \geq 2/3$ for all $x$, how can one boost the probability of success to $1-\delta$ with only a logarithmic cost in $1/\delta$? (Phrased differently: given fixed, arbirary $a_1,\dots, a_t\in \mathbb{R}^d$ with the guarantee that at least $\frac{2t}{3}$ of the $a_i$'s belong to $S$, is there a procedure outputting a value from $S$? If so, is there an efficient one?) And what is the minimum set of assumptions one needs on $S$ for the above to be achievable? Sorry if this turns out to be trivial -- I couldn't find a reference on this question... 

Lemma: No LTL formula recognizes the language $L_{even} = $Traces$(EVEN)$. A string $c \in L_{even}$ iff $c_i = a$ for even $i$. See Wolper '81. You can prove this by first showing that no LTL formula with $n$ "next-time" operators can distinguish the strings of the form $p^i\neg p p^\omega$ for $i> n$, by a simple induction. Consider the following (infinite, non-deterministic) transition system $NOTEVEN$. Note that there are two different initial states: 

The 2-approximation algorithm for Knapsack: First, consider the trivial algorithm: select the highest value item that fits. This can obviously be arbitrarily far from optimal. Now consider the greedy algorithm: greedily select the highest value density items. This can also be arbitrarily far from optimal. Now, the 2-approximation algorithm: Run Trivial and Greedy. Take whichever solution has the highest value. This is guaranteed to be within a factor of 2 of optimal. 

Any problem complete for $NEXPTIME$ or 2$EXPTIME$ is known not to be in $NP$ (by the time hierarchy theorem). Similarly for $NEXPSPACE$ and $EXPSPACE$ (by space hierarchy + simulation). You can often get "fake" problems by padding, but natural problems complete for these classes don't seem to be quite so common (probably because they're so incredibly hard!), but here are a few: EXPSPACE: Regular expression equivalence with exponentiation operator 2-EXPTIME: Satisfiability for CTL* (a temporal logic) Satisfiability for ATL* Decision problem for Presburger arithmetic 

As mentioned in a comment above, the Boolean Hidden Matching Problem introduced and studied in [BJK04,KR06] seems to (almost) meet your requirement. The input size is roughly $n\log n$ (as an input is of the form $(x,M,w)\in\{0,1\}^{2n}\times\{0,1\}^{n\times 2n}\times \{0,1\}^{2n}$, where $M$ is a very sparse matrix that can be encoded with $n\log n$ bits); and $\textsf{yes}$- and $\textsf{no}$-instances of the promise problem have distance $\Theta(n)$, The one-way randomized communication complexity of $\textsf{BHM}_n$ is $\Omega(\sqrt{n})$, as shown in [KR06]. 

using the notation of the book. However, if you look at the proof of Theorem 6.7, $(4) \Rightarrow (6)$ follows from the Np-Free-Lunch theorem (Theorem 5.1), or rather its implication in terms of VC dimension (Corollary 6.4, and then Theorem 6.6). I'll let you check the details, but essentially it proves the stronger-looking statement you need: 

[1] Tight Bounds on the Fourier Spectrum of $\mathsf{AC}_0$, A. Tal. CCC'17. [2] On polynomial approximations to $\mathsf{AC}_0$, P. Harsha and S. Srinivasan. RANDOM 2016, 

Here is a detailed outline, not entirely made rigorous. Setting $b_n \stackrel{\rm def}{=} \frac{1}{6}-Y_n$, with $b_0 = 1/6$, we have $$ b_{n+1} = \frac{4}{3}\left( \sqrt{1+\frac{3}{2} b_n - \frac{9}{2} b_n^2} - 1\right)\tag{1} $$ (I like to set things near zero.) By induction, $b_n \geq 0$ for every $n$, and a simple computation shows that $b_{n+1} - b_n \leq 0$ for all $n$. By monotone convergence, $(b_n)_n$ converges, and the fixed point being zero we have $$\lim_{n\to \infty}b_n = 0\,.\tag{2}$$ We can do a Taylor expansion of (1) to get $$ b_{n+1} = b_n - \frac{27}{8}b_n^2 + o(b_n^2)\tag{3} $$ i.e., summing, $$b_{n} = b_0 - \frac{27}{8}\sum_{k=0}^{n-1} \left(b_k^2 + o(b_k^2)\right)\,\tag{4}$$ Solving the recurrence $$ \tilde{b}_{n} = \frac{1}{6} - \frac{27}{8}\sum_{k=0}^{n-1} \tilde{b}_{k}^2$$ yields $\tilde{b}_{n} = \frac{1+o(1)}{6+\frac{27}{8}n} \displaystyle\operatorname*{\sim}_{n\to\infty} \frac{8}{27n}$. (For instance, solving the continuous version $h' = -\frac{27}{8}h^2$ with $h(0)=1/6$.) "Therefore", $b_n \displaystyle\operatorname*{\sim}_{n\to\infty} \tilde{b}_{n}$ and $$ Y_n = \frac{1}{6} - \frac{8}{27n} + o\left(\frac{1}{n}\right)\,.\tag{5} $$ 

Context: Kavvadias and Sideri have shown that the Inverse 3-SAT problem is coNP Complete: Given $\phi$ a set of models on $n$ variables, is there a 3-CNF formula such that $\phi$ is its exact set of models ? An immediate candidate formula arises which is the conjunction of all 3-clauses satisfied by all models in $\phi$. Since it contains all 3-clauses it implies, this candidate formula can easily be transformed into an equivalent formula $F_{\phi}$ which is 3-closed under resolution - The 3-closure of a formula is the subset of its closure under resolution containing only clauses of size 3 or less. A CNF formula is closed under resolution if all possible resolvents are subsumed by a clause of the formula - a clause $c_1$ is subsumed by a clause $c_2$ if all literals of $c_2$ are in $c_1$. Given $I$, a partial assignment of the variables such that $I$ is not a subset of any model of $\phi$. Call $F_{\phi|I}$, the induced formula by applying $I$ to $F_{\phi}$: Any clause that contains a literal which evaluates to $true$ under $I$ is deleted from the formula and any literals that evaluate to $false$ under $I$ are deleted from all clauses. Call $G_{\phi|I}$, the formula that derived from $F_{\phi|I}$ by all possible 3-limited resolutions (in which the resolvent and the operands have at most 3 literals) and subsumptions. Question: Is $G_{\phi|I}$ 3-closed under resolution ? 

In his CCC'17 paper [1], Avishay Tal improved the bound to $$ \left(\log\frac{m}{\varepsilon}\right)^{O(d)}\,. \tag{1} $$ You may want to check p.15:4 for a discussion. It also refers (see Footnote 30 to a paper of Harsha and Srinivasan, which improves on (1) and answers Tal's conjecture: $k$-wise independent, for $$ k = \left(\log m \right)^{O(d)}\cdot\log\frac{1}{\varepsilon}\,. \tag{2} $$ suffices to $\varepsilon$-fool size-$m$ depth-$d$ AC0 circuits. 

[BCOST15] Eric Blais, Clément L. Canonne, Igor Carboni Oliveira, Rocco A. Servedio, Li-Yang Tan. Learning Circuits with few Negations. APPROX-RANDOM 2015: 512-527 

My apologies if the question is a tad vague—I did try to search the literature for more, but didn't find anything (the similarity between the keywords "Takens" and "taken" on Google may be partly to blame). If I understood correctly the Wikipedia page on Takens' theorem, and some discussions I had with people in applied (physics-related) fields that mentioned it to me, Taken's theorem essentially states that 

Yes. The $$O\left(\frac{d}{\varepsilon}\log\frac{1}{\varepsilon}\right)$$ sample complexity upper bound holds for proper PAC learning as well (although it is important to note that it may not lead to a computationally efficient learning algorithm. Which is normal, since unless $\mathsf{NP}=\mathsf{RP}$ is it known that some classes are not efficiently proper PAC learnable. Cf. e.g. Theorem 1.3 in the Kearns—Vazirani book you mention). This is actually shown in the Kearns—Vazirani book (Theorem 3.3), since $L$ there is a consistent hypothesis finder with hypothesis class $\mathcal{H}=\mathcal{C}$. See also [1]. Unknown. Hanneke's algorithm [2] is an improper learning algorithm. Whether this extra $\log(1/\varepsilon)$ factor in the sample complexity can be removed for proper PAC learning (information theoretically, i.e. setting aside any computational efficiency requirement) is still an open question. Cf. the open questions at the end of [3]: 

Its traces are precisely $\{a,\neg a\}^\omega - L_{even}$. Corollary to the Lemma: If $NOTEVEN \vDash \phi$ then $EVEN \not\vDash \neg\phi$ Now, consider this simple transition system $TOTAL$: 

Reading Baier and Katoen closely, they are considering both finite and infinite transition systems. See page 20 of that book for definitions. First, take the simple transition system $EVEN$: 

Its traces are clearly $\{a,\neg a\}^\omega$. Thus, $NOTEVEN$ and $TOTAL$ are not trace equivalent. Suppose they were LTL inequivalent. Then we would have an LTL formula $\phi$ such that $NOTEVEN \vDash \phi$ and $TOTAL \not\vDash \phi$. But then, $EVEN\vDash \neg\phi$. This is a contradiction. Thanks to Sylvain for catching a stupid bug in the first version of this answer. 

Any value. It depends upon which $t$ you are given. A term of type $\exists y.(\neg(0 = 0) \Rightarrow 0 = S(y))$ is a pair of an int $y$ and a function that takes a proof of $\neg(0=0)$ and gives you a proof of $0 = S(y)$. You can use a term of type $\neg(0 = 0)$ and type $0 = 0$ (from reflexivity) to derive a term of any type you want. This includes a term of type $0 = S(0)$, $0 = S(1)$, $\ldots$. So, you can make $y$ any integer you want. 

(There are others -- see e.g. the surveys referenced by Ronitt Rubinfeld here for property testing in general, and some discussion of distribution testing.) Some slides: $URL$ 

On the other hand, Blais [2] showed that non-adaptive testing of $k$-juntas had query complexity $\tilde{O}(k^{3/2})$, hence the separation. 

Yuval Peres gave the answer in terms of the Kullback-Leibler divergence. Another way is to recall that the sample complexity will be captured by the inverse of the squared Hellinger distance between the two coins. Now, letting $D_p$ and $D_{p+\varepsilon}$ be the distributions of a Bernoulli random variable with parameter $p$ and $p+\varepsilon$ respectively, $$\begin{align} d_H(D_p,D_{p+\varepsilon})^2 &= \frac{1}{2}\lVert D_p-D_{p+\varepsilon}\rVert^2_2 = \frac{1}{{2}}\left((\sqrt{p}-\sqrt{p+\varepsilon})^2+(\sqrt{1-p}-\sqrt{1-p-\varepsilon})^2\right) \\ &= \frac{1}{2}\left({p(1-\sqrt{1+\varepsilon/p})^2+(1-p)(\sqrt{1}-\sqrt{1-\varepsilon/(1-p)})^2}\right) \end{align}$$ Assuming wlog $p\leq 1/2$, we can see easily by a Taylor expansion that this is $$\begin{align} d_H(D_p,D_{p+\varepsilon})^2 &= \Theta\left(\frac{\varepsilon^2}{p}\right) \end{align}$$ leading to the same answer as Yuval Peres' (from a different method). Interestingly, this also shows the usual observation, that the quadratic relation between TV and Hellinger distance can matter a lot: for $p=1/2$, the bound $1/TV$ (i.e., $\Omega(1/\varepsilon^2)$ here) is tight; but for $p=O(\varepsilon)$, then it is quadratically worse than the optimal, which is $1/d_H^2$ (that is, $\Omega(1/\varepsilon)$). 

According to [MOS04] (Section 5.2), the class of $k$-juntas over $\{-1,1\}^n$ can be learnt with membership queries in time $\operatorname{poly}(2^k, n)$ — cf. also footnote 8 of [BL97], p. 17. Edit: as pointed out in the comment, this is probably not what you were looking for (in terms of attribute-efficiency). 

That question was about a translation from an arbitrary instance of SAT to a HORN-SAT instance with the exact same satisfying assignments. The translation property needed for P$\neq$NP is just equisatisfiability, not equality of assignments. That is, it simply requires them both to be satisfiable, or unsatisfiable. If satisfiable, they may be satisfied by different assignments. For example: $a \wedge \neg b$ and $\neg a \wedge b$ are equisatisfiable (they are both satisfiable), but they have different satisfying assignments. The answer was that there can not be a satisfying assignment preserving translation, but this doesn't tell us whether there is a polynomial time satisfiability preserving translation. 

I've seen (and heard) it claimed that it is safe to add the classical axiom of excluded middle to Coq, but I can not seem to find a paper supporting this claim. The papers I see listed on the Coq wiki about excluded middle are showing inconsistency with impredicative Set. Indeed, it seems that Coquand states that adding Excluded Middle (an inhabitant of $A+\neg A$) is inconsistent for CoC in section 4.5.3 of his description(PDF) of the metatheory of CoC. However, this section is a bit abstruse to me, so I may very well be misreading him. 

Tutorials on the Foundations of Cryptography (edited by Yehuda Lindell) Dedicated to Oded Goldreich From the Springer page: 

[1] A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. Learnability and the Vapnik-Chervonenkis dimension. Journal of the ACM, 36(4):929–965, 1989. [2] S. Hanneke. The optimal sample complexity of PAC learning. J. Mach. Learn. Res. 17, 1, 1319-1333, 2016. [3] S. Arunachalam and R. de Wolf. Optimal quantum sample complexity of learning algorithms. In Proceedings of the 32nd Computational Complexity Conference (CCC), 2017. 

I realize the problem itself would need a more thorough and precise formulation to be tackled, as in the above we deal with full real numbers (so "hiding" information in a single real number, say the weight of a node in the middle layer, would give a very easy way out). But with this dealt with appropriately, hopefully there are non-trivial statements to be made with regard to what compression can be achieved, with respect to some distribution over the inputs? Has this type of question been looked at from a theoretical viewpoint, in our community or another? 

Using the relation between total variation and $L_1$/$\ell_1$ distance of the probability/distribution/mass functions, we have $$\begin{align} d_{\rm TV}(D_1, D_2) &= \frac{1}{2}\lVert D_1-D_2\rVert_1 = \frac{1}{2}\lVert \beta D_2 +(1-\beta)D_3 - D_2\rVert_1\\ &= \frac{1-\beta}{2}\lVert D_3 - D_2\rVert_1 = (1-\beta)d_{\rm TV}(D_2, D_3). \end{align}$$ 

The Risch algorithm for computing elementary antiderivatives. According to Wikipedia, no software package is known to implement the full algorithm due to its complexity. 

Nope. Let $A = \{a\}$, $B= \{b\}$. Then $A^* = \{\epsilon,a,aa, \ldots\}$ and similarly for $B$. So, $A^*\cup B^*$ contains strings of repeated "a" or repeated "b". But $(A\cup B)^*$ contains strings like $ab$, which you can't get from $A^*$ or $B^*$. I'd suggest looking at a book like Sipser for a good coverage of languages like these and how to prove properties like equivalence. In particular, he has a great number of exercises to work through. Good luck! 

Rob's answer reminded me of a similar Cornell reading group that Michael Clarkson organized for a few years: Cornell Security Discussion Group. Might be worth skimming through there for some papers. 

Just to recap: in ND, the elimination of a connective tells us how to use it: $A \wedge B$ $---$ $A$ $A \wedge B$ $---$ $B$ are the elim rules for conjunction. They say that we can use $A\wedge B$ to get $A$, or to get $B$. Similarly, in SC: $\Gamma,A\vdash \Delta$ $------$ $\Gamma,A\wedge B \vdash \Delta$ $\Gamma,B\vdash \Delta$ $------$ $\Gamma,A\wedge B \vdash \Delta$ What the SC rules are telling us is that if we "need" $A$ or $B$, then we can use $A\wedge B$ in place of $A$ or $B$. In general, SC left introduction rules tell us "when" we can use a connective, and ND elimination rules tell us "how" to use a connective. Now, for implication, in ND we have: $A \rightarrow B$       $A$ $------$ $B$ In SC: $\Gamma \vdash A,\Delta$       $\Sigma,B\vdash \Pi$ $----------$ $\Gamma,\Sigma, A\rightarrow B \vdash \Delta,\Pi$ Now, the first thing to do with the SC rule is to ignore the "crap". $\Delta$ and $\Sigma$ can be ignored for the purposes of intuition: $\Gamma \vdash A$       $B\vdash \Pi$ $-------$ $\Gamma, A\rightarrow B \vdash \Pi$ What this says is that if we know how to use what we "have" ($\Gamma$) to prove $A$, and we know how to use $B$ to prove what we want $(\Pi)$, then we can use $A\rightarrow B$ to get what we want ($\Pi$). That is, once again the left introduction rule is telling us "when" we can use the connective.