Here is a counterexample: $$ (a \lor y \lor s) \land (\overline{y} \lor b \lor z) \land (\overline{z} \lor c \lor d) \land (\overline{s} \lor b \lor t) \land (\overline{t} \lor c \lor d). $$ This formula implies $a \lor b \lor c \lor d$, but only the five listed 3-clauses, no two of which resolve to $a\lor b\lor c\lor d$. 

Cygan, Kratsch and Nederlof give a $(2+\sqrt{2})^{\texttt{pw}} n^{O(1)}$ algorithm for hamiltonicity on graphs with $n$ vertices and pathwidth $\texttt{pw}$ (assuming you are given the pathwidth decomposition). Assuming SETH, they show that the constant $2+\sqrt{2}$ cannot be improved. Thus their results give a $(2+\sqrt{2})^n n^{O(1)}$ algorithm, and their results plausibly imply that the constant $2+\sqrt{2}$ cannot be improved (assuming SETH). This shows that when you carry out your program starting with the algorithm of Cygan et al., you get a $2^n n^{O(1)}$ algorithm for SAT, so there is no contradiction. 

Here a submodular function is one satisfying $$ f(A) + f(B) \geq f(A \cup B) + f(A \cap B) $$ for all $A,B \subseteq D$. Buchbinder et al. gave a $1/2$-approximation algorithm for this problem. This is best possible in the value oracle model: Feige et al. showed that achieving a $(1/2+\epsilon)$-approximation requires exponentially many value oracle queries, for any $\epsilon > 0$. What if the function $f$ is given explicitly? Dobzinsky and Vondrák gave a class of explicit functions for which no $(1/2+\epsilon)$-approximation can be achieved unless $\mathsf{NP} = \mathsf{RP}$. My question is: 

The operation is discretization to steps of size $0.05\mathrm{Hz}$. In other words, in the computer the values stored are in a new unit which is 20 times smaller than a Hertz. This corresponds to decimal values after the dot of the forms $.X$ and $.X5$. So two steps correspond to a change in the first digit after the dot. In fixed-point arithmetic, there is a base (usually base 2), and you take a fixed number of digits after the dot. This works (somewhat artificially) in this case if you use base 20. 

Let $D$ be the distribution on inputs that w.p. $p = 1/(1+\epsilon)$ picks $(x,x)$ for random $x$, and w.p. $1-p$ picks $(x,y)$ for random $x \neq y$. An appropriate choice of random coins leads to a deterministic protocol that never makes mistakes and outputs MAYBE w.p. at most $1/2$ with respect to $D$. In particular, it must answer YES on at least $1-(1/2)/p = 1/2 - \epsilon/2$ of the nodes $(x,x)$. Suppose that the protocol uses $C$ bits of communications. It therefore has at most $2^C$ leaves. Each leaf is a monochromatic combinatorial rectangle, and so each of the at least $(1-\epsilon) 2^{n-1}$ YES answers must end up in a different leaf. We conclude that $C \geq n-1$. 

There are many open algorithmic problems. All problems below (other than the last bullet) are NP-hard, so we are interested in the best approximation ratio we can achieve in polynomial time. The following are just a sample: 

Like many other questions, the answer to this one can be found in Stothers' thesis. A local USP is a CWP in which the only way in which a 1-piece, a 2-piece and a 3-piece can fit together is if their union is in $S$. Clearly a local USP is a USP, and a construction from [CKSU] shows that the USP capacity is achieved by local USPs (we are going to show that constructively). Coppersmith and Winograd construct an almost 2-wise independent distribution $S$ on $2^V$ with the following two properties: (1) $\Pr[x \in S] = (|V|/2|E|)^{1-\epsilon}$, (2) For any $x,y,z \in V$ such that the 1-piece of $x$, the 2-piece of $y$ and the 3-piece of $z$ together form a vector $w \in V$: if $x,y,z \in S$ then $w \in S$. We choose a random subset $S$ of $V$ according to the distribution, and for each edge $(x,y) \in E$, we remove both vertices $x,y$. The expected number of vertices left is roughly $(|V|^2/2|E|)^{1-\epsilon}$. The resulting set $T$ is a local USP: if there are $x,y,z \in T$ in which the 1-piece of $x$, the 2-piece of $y$ and the 3-piece of $z$ fit, forming a piece $w$, then $x,y,z,w \in S$, and so all of $x,y,z$ are removed from $S$. 

Your question is related to the well-known question about computing the minimum and maximum of a list simultaneously using the minimum number of comparisons. In that case the answer is $3\lfloor n/2 \rfloor$. The clever algorithm proving the upper bound translates to an AND/OR circuit with same bound you get, since one of the comparisons computes both a minimum and a maximum. However, the lower bound (given by an adversary argument) does seem to translate, at least in the case of monotone circuits (since an AND/OR circuit translates to a max/min algorithm). This would imply a lower bound of $3\lfloor n/2 \rfloor$. Perhaps a tight lower bound can be obtained by analyzing the adversary argument. The upper bound appears in "Introduction to Algorithms", where you can also find the easy argument showing that max/min comparator circuits are valid iff they work for boolean inputs (use an appropriate threshold). The lower bound can be found e.g. here. 

Levin's universal algorithm is an algorithm such that $t(U,A) < s_V t(V,A) + t_V$. By modifying the algorithm (see for example Hutter's The Fastest and Shortest Algorithm for All Well-Defined Problems), you can make $s_V$ a universal constant, though definitely not $1$ as you require. For related work, consult work by Hirsch, Itsykson and their students, for example this technical report. Edit: As Squark comments, the runtime of Levin's algorithm also depends on the runtime of $A$, since it has to verify its answers. To get a constant $s_V$, all you need to do is to set the speeds of the various algorithms in geometric progression (rather than arithmetical progression, like in Levin's original algorithm). 

The problem of circularity is coNP-hard. Suppose we're given a 3SAT instance with $n$ variables $\vec{x}$ and $m$ clauses $C_1,\ldots,C_m$. We can assume that $n = m$ (add dummy variables) and that $n$ is prime (otherwise find a prime between $n$ and $2n$ using AKS primality testing, and add dummy variables and clauses). Consider the following language: "the input is not of the form $\vec{x}_1 \cdots \vec{x}_n$ where $\vec{x}_i$ is a satisfying assignment for $C_i$". It is easy to construct an $O(n^2)$ DFA for this language. If the language is not circular then there is a word $w$ in the language, some power of which is not in the language. Since the only words not in the language have length $n^2$, $w$ must be of length $1$ or $n$. If it is of length $1$, consider $w^n$ instead (it is still in the language), so that $w$ is in the language and $w^n$ is not in the language. The fact that $w^n$ is not in the language means that $w$ is a satisfying assignment. Conversely, any satisfying assignment translates to a word proving the non-circularity of the language: the satisfying assignment $w$ belongs to the language but $w^n$ does not. Thus the language is circular iff the 3SAT instance is unsatisfiable. 

Classical work of Coppersmith shows that for some $\alpha > 0$, one can multiply an $n \times n^\alpha$ matrix with an $n^\alpha \times n$ matrix in $\tilde{O}(n^2)$ arithmetic operations. This is a crucial ingredient of Ryan Williams's recent celebrated result. François le Gall recently improved on Coppersmith's work, and his paper has just been accepted to FOCS 2012. In order to understand this work, you will need some knowledge of algebraic complexity theory. Virginia Williams's paper contains some relevant pointers. In particular, Coppersmith's work is completely described in Algebraic Complexity Theory, the book. A different strand of work concentrates on multiplying matrices approximately. You can check this work by Magen and Zouzias. This is useful for handling really large matrices, say multiplying an $n \times N$ matrix and an $N \times n$ matrix, where $N \gg n$. The basic approach is to sample the matrices (this corresponds to a randomized dimensionality reduction), and multiplying the much smaller sampled matrices. The trick is to find out when and in what sense this gives a good approximation. In contrast to the previous strand of work which is completely impractical, sampling algorithms are practical and even necessary for handling large amounts of data. 

Here's a somewhat interesting solution for $n=4$. The same idea also works for $n=6$. Start with the switches $(0,1),(2,3)$ with probability $1/2$. Reducing $0,1$ to $X$ and $2,3$ to $Y$, we are in the situation $XXYY$. Apply the switches $(0,3),(1,2)$ with probability $p$. The result is $$ \begin{align*} XXYY &\text{ w.p. } (1-p)^2, \\ YYXX &\text{ w.p. } p^2, \\ XYXY &\text{ w.p. } p(1-p), \\ YXYX &\text{ w.p. } p(1-p) \end{align*} $$ Our next move is going to be $(0,2),(1,3)$ with probability $1/2$. Thus we really only care if the result of the previous stage is of the form $XXYY/YYXX$ (case A) or of the form $XYXY/YXYX$ (case B). In case A these switches will result in a uniform probability over $XXYY/XYYX/YXXY/YYXX$. In case B they will be ineffective. Therefore $p$ must satisfy $$ p(1-p) = 1/6 \Longrightarrow p = \frac{3 \pm \sqrt{3}}{6}. $$ Given that, the result is uniform. A similar idea works for $n=6$ - you first randomly sort each half, and then "merge" them. However, even for $n=8$ I can't see how to merge the halves properly. The interesting point about this solution is the weird probability $p$. As a side note, the set of probabilities $p$ which can conceivably help us is given by $1/(1-\lambda)$, where $\lambda \leq 0$ goes over all eigenvalues of all representations of $S_n$ at all transpositions.