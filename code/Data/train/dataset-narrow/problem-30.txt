If your environments are per customer, I would suggest in your specific case to have a repository per customer. (In general it is repository per environment.) This repository would have a standard directory structure for environment variables, ansible variables and inventories, strongly encrypted secrets (account access tokens, private keys, etc.). You would git submodule the code into those repositories. I would probably do it in multiple repositories. One for ansible roles and modules, one for maintenance and deployment scripts, one for each major application running in the environments. Now you can optionally actually fork the code or otherwise pin the submodule at specific tag for release, making sure that the code managing the customer's environment would not change unless tested and released. If you are using an artifact repository, make sure the artifacts are properly versioned and those versions are specified in the environment variables properly. Automation is important because the environment variables should not be updated by humans where possible, but generated by scripts. Make sure that there are nearly no manual updates in the per customer inventory and developers only update the code repositories. If they want to make configuration change, it should be done to one of the generating scripts, which is then run to generate the variables and the diff is committed into the customer repository. It pays to setup continuous integration for this process. Without this at some point there will be too many repositories to maintain. 

Poor organization and planning of work Doing work someone else should be doing Doing work that should not be done at all Being actually understaffed 

Product Management Software Development Tools Development Security and Compliance Quality and Testing System Operations (SRE) 

What you are looking for is at the bottom of the GitHub README describing Job DSL for the Promoted Builds Plugin. Link to GitHub is on every plugin's page. There are also links to plugins it depends on and you should read the documentation for those like the Job DSL plugin wiki. You can also ask questions in the Job DSL forum or simply read the documentation. Here is a plugin that extends the promotion syntax of the DSL. 

Until Puppet 4.0 there was no easy way to orchestrate application spread over multiple servers or services, as it was hard to specifically order actions in Puppet, which was a design choice. Ansible was better at orchestrating and ordering the steps, especially across multiple servers. This was especially significant in applications where the wrong order of steps could lead to errors unrecoverable through repetition of those steps until an eventual consistency could be reached. That is no longer an issue and so the distinctions are largely preference based. 

We have redis connected through stunnel. That way you don't have to always establish the ssh connection, which can be a problem. Here is an article on how to setup redis through stunnel, which I won't copy into this answer. Feel free to edit. $URL$ 

DevOps does not have any KPI. It would be like asking what are the KPI of Love. But some of the things you mentioned (Problem and Incident Management, Capacity Management, Change and Release Management) do have good KPI, some of which are based on the theory behind DevOps. In general, for any business process, you can create a Value Chain Map describing how value flows from Customer, through the enterprise back to Customer. The entire loop always has to start and end with Customer, but sometimes, for a service organization, the Customer can be internal. The throughput of value through such chain can be a good way to design your KPI in a tamper-proof way. Measuring any KPI in any individual link of the value chain only makes sense as long as that particular link is the bottleneck of the process and you try to exploit or elevate the bottleneck. A common problem with KPI is when it starts halfway through the chain. For example, a Change and Release process often starts with developers and ends with deployment. This process excluded: 

TL;DR: Infrastructure as Code is a way to automate and backup your environment. In ideal case, after a disaster, you could restore your Infrastructure fully and automatically by Provisioning new resources, Restoring Configuration from Code Repository and Recovering Data from Backup. Overview Infrastructure as Code relies on three main concepts: 

Failure is Inevitable Failure in the System is inevitable, something will always go wrong. You might not be able to choose what, but you can try to choose when. By introducing small errors throughout the day, you ensure that your engineers are present. By killing non-conforming services quickly, you ensure that failures happen often before deployment. By making the environment more adversarial, you ensure that it will be the developers who run into issues long before any service makes its way into production. Failures will be quickly apparent in integration phase of new services with the old ones, but that is ok, because the old production services are already resilient. Cattle not Pets Everyone will tell you lately: Do not treat your servers as pets. There is a power in numbers and any single point of failure will bring down the system. No matter how well you can tune and optimize your server, no matter how beefy hardware you can get, how much it can handle, it will never be a match for herd of small scalable instances. Chaos Monkey encourages you to think about removing all points of failure, because sooner or later, the Monkey will come! Everyone fails and even the Amazon S3 had an unpredictable outage. Anti-Fragile So what is the theory and why does it work? Nassim Nicholas Taleb in his book Antifragile describes a concept where living self-aware systems, will benefit from a small levels of randomness and actually become better in face of adversity. This is similar to annealing. He does also describe an evolutionary way, where fragility of parts in a system is transferring into antifragility of the whole. The transfer occurs on two levels: 

Here is a GitHub Gist code snippet of exactly what you are asking for by Daniil Yaroslavtsev. It uses the list of all images and their snapshots and compares the IDs to list of all snapshot IDs. Whatever remains are the orphaned ones. The code works in the same principle as the answer above, but is better formatted and slightly more readable. The code takes advantage of the JSON query with option (you can also use jq command line utility for that, it is the same JSON query language). The formats the output as text with . Here is a link to API reference and few examples. It is slightly more elegant than a long chain of grep/awk/sort/uniq/tr pipes. 

I had a perfect server, it was so pretty and rock solid and so I named it Petra. It was perfect in every way, everything was configured and tuned just right, it had perfect 100% service record and 753 days of uptime. I've spent a lot of time and effort making sure it run so well. No other server in the company had been this good. But last night this evil monster crashed my server for no reason. 

Business Projects - what you do for other teams in the organization Internal Projects - what you do to make your work easier in future Scheduled Maintenance - what you do to keep the lights on Unplanned Interrupts - what you do because something went wrong 

You have to figure out which stage your company is in and/or moving into and act accordingly. There is no "one size fits all" solution. 

Start with a review of the first three points. Read the Phoenix Project on ideas how to do the first. Ask yourself for every task you help anyone with if it should be done at all and if it is you that should be doing the task or if you should simply enable whoever needs it done to do it themselves. This will give you some documentation on why all the work you do is necessary. Next review the four types of work mentioned in the Phoenix project: 

By a small random variations - developers making changes - the most fit for the environment will survive and propagate - pass tests and get deployed. Standard Development Life Cycle. By failure of parts not capable to withstand a larger level of randomness in the environment, the remaining parts that were able to withstand it compose a system that is as a whole better able to deal with changing environment than before. This is essentially Chaos Monkey. 

Automation Configuration Management is in its 3rd generation of tools. Building on CFEngine a new set of tools for Automated Configuration Management is being widely deployed now. The most popular in alphabetical order are Ansible, CFEngine, Chef, Puppet, PowerShell DSC and SaltStack. Each will have a Language to describe state of your infrastructure, Code Modules to apply those changes and provide ability to extend the tools, some Agent to execute those on the servers and a Central Repository of information. They will generally operate in push or pull mode, either connecting to servers from a central location(s) and executing changes remotely or running on each server and pulling information about state from central location and that either in client/server model or in a distributed way. The important concept is for the system administrator or site reliability engineer to not make changes directly to the infrastructure, but let the automation do changes. Anything done manually by a human should be considered either perishable, being soon corrected back by automation or in stricter form violating the integrity of the infrastructure and triggering destruction and rebuild of the affected components. Code Repository Code Repository, ideally separate from Repository holding Software, would be used to manage all changes to the Infrastructure and related Automation. It should hold Configuration files and templates, Playbooks (Cookbooks) describing process of changes to be reviewed, Code extending the CM automation tools, Provisioning configurations, Infrastructure Tests and Alerts, Staging/Deployment Tests, Documentation, Manual (not yet automated) Process Descriptions. The important concept is to institute peer reviews for changes, to have record of all changes and ability to automatically revert to previous state in case of unpredictable and/or untested issues, ability to deploy to staging environment and test configuration changes and ability to automatically deploy changes without variation caused by human error. Managed Infrastructure Managing Physical Infrastructure is a real world task that goes beyond software and requires very different set of skills. By being able to abstract this layer through Cloud Computing or a Managed Datacenter, you have your team focus on the part of managing Infrastructure that adds business value. While Cloud Computing offers a way to start and scale quickly at a later stage companies often realize some benefits and even significant savings in moving parts of the Infrastructure in their own data centers for a hybrid model. Owning or renting the hardware does not mean that you also have to employ the people who handle it. At this scale you need data centers geographically distributed around the world and having people with all the required skills in all places would be very expensive. Flying them around the world adds high latency to any changes and additional level of operational inefficiencies, which is another reason for outsourcing the datacenter management. The important takeaway is that Managed Physical Infrastructure is often forgotten or overlooked concept, but just as important. Even if you've got everything automated, all configuration stored in a backed up code repository, unless you have a way to quickly provision, you have a huge bottleneck, which could easily erase all the benefits you've gained by the other two steps.