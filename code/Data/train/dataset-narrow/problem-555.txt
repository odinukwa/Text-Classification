A login - this gets you into the server itself. This is not a database user, but a login at the instance level (In SSMS this is the top most security, not the security under a database.) That login being mapped to a user - now there are exceptions but for the purposes here - you need that login to then be granted access to the database. This would be security found at the database level. Permissions - you'll need permissions of some sort to do something in SQL. SQL Server needs to have the protocol enabled that you are trying to connect with. Also if there is a firewall, the port (default is 1433 for TCP port) needs to be open. 

You can put the SQL Server binaries/instance files in the same place you tend to put your other program files. But if you do that - at least make sure you take your system database files and potentially your default backup location and move it someplace else.. Here is what I tend to do when given an unlimited number of drive letters to play with (at a minimum.. Letters aren't important here): 

The relationship between and is one-to-one, so if you wanted to simplify your table structure, then these two tables could be merged into one. Are you 100% certain that every question will have only 5 (or less) options? For flexibility reasons, it might make sense to have only one answer option per record. In that case, the table must remain separate from the table, and it becomes a one-to-many relationship. Which is the right answer option? You should probably indicate that somehow. In the table, the is obviously the primary key. The is a foreign key. And you seem to have forgotten a foreign key to , as well as a foreign key to a table to indicate who gave this answer. A minor issue: In the table, the column is a . This is up to 16 million bytes. Do you really need that many? A column can store up to 64K bytes, maybe that would be enough? Also, you would want to use for your columns. The reason for this is that auto incrementing ints start at 1, so by allowing signed s you're effectively wasting a bit and limiting the range of values you can use. There may be other opportunities for improvements, these are just a few I noticed. 

2.) So a restore is being attempted and it is failing because the backup device can't be opened. SQL didn't retrieve the text of the windows error message, but we still have the windows error ID. 3.) I happen to know that the windows error 3 is a path issue. But if you didn't know that you can open up a command prompt and type this shows us the OS error for 3.. In our case: 4.) It is obviously a case where the path is invalid. The path isn't right, so double check the path and the issue should be resolved, as the OP discovered through the comments. 

The date or date key can be an EXCELLENT choice in clustering and partitioning for a fact table in a lot of cases. If you are often scanning by a date range, including date in your criteria this can work well. By partitioning on the date and aligning your clustered index by changing it (yeah that is a bit of work for the DBA but not much and worth it.. I actually like Identity Clustered Indexes and surrogate keys in a lot of tables, but for fact tables I tend to go with a date value.. especially when date is included in most of your queries to the fact table, and in a lot of cases it is... Lot of range scans by date, for instance) you can really take advantage of partition elimination during work. Say you normally query by month, partitioning by month and aligning can potentially eliminate all of the partitions except for the month you are querying. Now you are scanning along that one partition instead of the big table as a whole. You really need to test as SQL-Learner says.. But date is monotonically increasing in inserts typically.. Would be aligned if you changed the clustering key (really no reason to not change clustering key.. Do you ever even use that identity for ANYTHING on your fact table? If this were a dimension, then I would be having a different discussion. In that case, I would ask which key you relate your dimension to your fact table by. It would either be a natural key or a surrogate key and in that case, the identity column would be a fine clustered key for the dimensions. The fact table is a little different. In most warehouse applications, the fact table isn't usually being joined to other tables by the fact table's key, but it is being joined to dimensions by their keys. The date clustered index can be a good approach to at least try in warehouse fact tables. 

It's because you're calling the stored procedure with empty string values rather than NULL values. The IFs and ELSEIFs in the SP body are testing for NULL, not empty string. 

And you may also want to consult this page in the MariaDB Knowledge Base: Optimizing table_open_cache. 

mysqldump and mysqlimport (as described by the documentation you cited) should be fine for your regular tables, especially since you don't have any foreign keys. However, due to the differences in MySQL versions, the tables in the database will be different, so I suspect migrating that database probably can't be done with this particular method. If you need to migrate the database, there are better ways of doing that. 

It's not entirely clear what you're asking, and your SQL statements don't work, but assuming you mean something like this: 

You can manipulate system variables like innodb_old_blocks_time (increase this - 1000 = 1 second) and innodb_old_blocks_pct (default is 37 - allowed range is from 5 to 95, set a smaller value to evict data from and similar faster). Both these variables are dynamic, so they can be given special values just before you run mysqldump, and then restored to the original values once it has completed. For details, see Making the Buffer Pool Scan Resistant. With MySQL 5.6+ (or MariaDB 10.0+) it's also possible to run a special command to dump the buffer pool contents to disk, and to load the contents back from disk into the buffer pool again later. (See MySQL Dumping and Reloading the InnoDB Buffer Pool | mysqlserverteam.com.) This way you can still use or other tools that "pollute" the buffer pool and then restore it afterwards. A way to prevent that running backup is unintentionally evicting your working set data at all would be to replace your backup method with Percona Xtrabackup or another physical backup tool that doesn't access the InnoDB buffer pool as such. Physical backup methods are also faster, and can be less disruptive than mysqldump. The disadvantage is that you'll need the exact same MySQL version and configuration on the system where the backup is restored. 

So I can say for sure in my limited testing on SQL Server 2012 the traces of the DB were gone from the buffer pool and the plan cache completely. Which makes sense because the DB that they related to disappeared, that is the expected behavior. That said - after a restore one thing you definitely should consider doing is updating your statistics. I like to start with clean stats build up and I really like to see people using production like data in dev and I like starting with clean statistics. So good on you for using real data in dev. (Caveat - sometimes you need to deidentify your data before doing this, but yay for testing with real data distributions and sizes.) 

For most applications hitting SQL Server 2008 R2, you would be fine going either way, I think. support is being removed after SQL Server 2012 but until it is, you'd likely be fine either way based on your standards and your needs. That said, I'd look to future supportability and consider going with the Native Client. I would say a more important decisions would be to consider using the Native Client for your .net apps if you aren't already. 

Use case: A measurement creates a given number of images. For each image we need to store a small set of quality indicators (floats, doubles) along with an image integer [1 ...N], a timestamp and one or two foreign key values. This should then be plotted in "real time" in a web application (PHP) for users to evaluate. Each web client polls the database every 5s. Storage + retrieval of each set of quality indicators should ideally take < 2s (approximately). In a worst case scenario there can be ~30 simultaneous web clients polling and around 10 measurements could be writing simultaneously, leading to write bursts of approx. 1000 sets of quality indicators per second. In a programming language, this sort of data would probably be stored in arrays or lists. As I'm not aware of anything similar in the MariaDB / MySQL world I'm just using a regular InnoDB table with a column for each of the values mentioned above. This already has 90+ million rows and is expected to grow faster in the coming months. Is InnoDB overall the best storage engine for this, or should I consider others? Is it best practice to archive data after a while, perhaps once all the measurements' images have been processed? Would it help to enable compression, or would that have very negative impacts on performance? 

I'd echo @Mark Storey-Smith's comment - a competent DBA is the best way to go here. You can't really automate a well tuned SQL Server but a good DBA can setup various maintenance items to keep it running well. Sounds like you were asking a lot about maintenance so one great spot to look for some scripts to help setup a best of breed monitoring solution is Olla Hallengren's Maintenance Solution scripts explained on Olla's site here. That will help ensure you are at least doing the important maintenance items (Index rebuilding/reorganizing, statistics updating, backups, updating statistics, checking database integrity, etc.) As far as the ongoing optimization, I'd suggest picking up a copy of the Professional SQL Server 2008 Internals and Troubleshooting book. It has just the right amount of internals knowledge to help you understand the "why" behind best practices and contains plenty of practical examples for implementing the best practices. Or I'd recommend the same Internals & Troubleshooting book but for SQL Server 2012. Contains some great chapters just for your question. Like how to perform a SQL Server Health Check, by Glenn Berry. 

Like Swasheck and Mark I started playing when this one came out. Please don't accept this answer, accept Mark's, in fact don't even give me points, I have no screenshots when I saw Swasheck post what I had started working on, but this is too long for a comment. So the short answer is: Yes, you are fine That said, I have heard of reports in past versions of SQL where people have seemed to think this was required. So I wanted to be sure and I did a test. I created a database, created some tables and then used Glenn's query to see how much of that database was in the buffer cache before the restore. Before I had about 50.2MB of data, after I had about 1.5MB - this is most likely because of something after the restore, not because of data hanging about. I also ran several queries before and included table aliases like "foo" so I could find them in the cache using the query below. I saw them all there, did the restore and they were gone.