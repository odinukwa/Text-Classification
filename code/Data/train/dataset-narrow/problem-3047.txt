Doc2vec might help. Embed each document into a high dimensional space. Then use a classification algorithm (e.g., k-nearest neighbors) on the created vectors. Gensim is a Python package for doc2vec. 

One reason to discretize continuous features is to improve signal-to-noise ratio. Fitting a model to bins reduces the impact that small fluctuates in the data has on the model, often small fluctuates are just noise. Each bin "smooths" out the fluctuates/noises in sections of the data. 

It depends on how "generalized" you want your methods. If you have known and fixed set of quantitative relationships (=, <=, >=, …), then you can consider it a classification problem. Given some input string, which category does it most likely belong to? The same goes for entities. Are you assuming it will always be two entities? If the system should handle an unlimited number of entities with unknown and possibly infinite number of relationships than the problem quickly becomes intractable. If you assume there will always two entities, then you can apply Resource Description Framework (RDF). RDF goal is to find semantic entities and their relationships. RDF finds triples: a subject, a predicate, and an object. The predicate is "quantitative relationship" between the subject and object. There are a variety of frameworks for extracting RDF from a sentence. The most effective ones depend on building a parse tree of the natural language. 

What's more interesting -it seems to get worse with smaller learning rate! 0.4 is good, and 0.0001 results in smaller updates (as expected) but larger relative fluctuation (caught me by surprise). Why is it so? Edit: Just tried 100 layers (each 63 neurons). It's very noisy, but I am able to more-or-less steadily reduce the error. It's less stochastic if I set learning rate to 10 ("ten", lol!) and will get only noisier if it's say 0.5 which is incredibly weird Of course, this learning rate is so high that the error does occasionally snap to a high value, but seems to work with 100 layers... Notice - I am using full-batch gradient descent, with 50 elements 

I've implemented Nesterov Accelerated Gradient (NAG) (link, section: "Nesterov Accelerated Gradient"), however not sure it's viable. $$v_t = \gamma v_{t-1} +\eta\nabla_\theta J(\theta - \gamma v_{t-1})$$ $$\theta = \theta - v_t$$ I attempted the following (consider a minibatch consisting of merely 1 training case): 

Pick an asymmetric loss function. One option is quantile regression (linear but with different slopes for positive and negative errors). 

If you want to model the unique meaning of commonly occurring n-grams, often times called collocations, the best solution is to train a new embedding space to model those specific semantics. Aggregating existing embeddings will only capture part of the collocation meaning. If you do not have very much data, you can do fine-tuning/transfer learning. Fine-tuning/transfer learning takes an existing model architecture and weights, then does additionally training with more data. In this case, take Google's Wikipedia model and train it with your collocations. An example of fine-tuning for word2vec in Keras can be found here. 

scikit-learn's FeatureUnion concatenates features from different vectorizers. An example of combining heterogeneous data, including text, can be found here. 

For example, assume we are working on the the i'th element that is at index 3 and $w^g_t$ has 5 entries. Like in the paper, let's allow $s$ to be able to perform a shift of -1, 0 or 1. Thus, $s$ has 3 dimensions. Now, I want to shift every element in $w$ by +1 forward. I have a feeling I will get out of bounds error, and modulo won't help too much. We can unfold this computation for the 3rd element: 

Some people use recurrent weights with Input, Forget, Output - notice, their equations don't even mention blockInput, they start from describing the $f$ or $i$ gate (1), Wikipedia: (2) Lke this: 

In the first case, I am able to get 4 loss-values, but in the second example, I only have 1 source of gradient, at _t3 My main concern is in first example, I demand LSTM to make prediction of 'b' and 'c' without supplying it enough previous context. It's fine for 'd' and 'e', but asking for answer at timestep 0 and 1 is a bit unfair? What would be best for this particular example? 

This blog post gives a general overview of fitting Gaussian Processes, including scikit-learn's GaussianProcessRegressor. 

NLP stands for Natural Language Processing. Programming language source code are synthetic (or unnatural) languages. Thus, NLP tools are not useful for processing programming language source code. Understanding programming language source code is done by the compiler or interpreter. Compilers and interpreters perform many functions, including lexical analysis, parsing, and semantic analysis. For Python language, language analysis is relatively straightforward since the main Python implementation, named CPython, compiles Python programs into intermediate bytecode, which is executed by the virtual machine. The dis module supports the analysis of CPython bytecode by disassembling it. 

John Langford's documentation on GitHub could help. You can find something on the Learning to Search Sub System page. 

Might not be the answer you are seeking, but I'll still have a go: First, quick review of word2Vec, assume we are using skip gram. A typical Word2Vec train-able model consists of 1 input layer (for example, 10 000 long one-hot vector), a hidden layer (for example 300 neurons), an output (10 000 long one-hot vector) 

Edit: to make things worse, the "LSTM Peephole paper", page 121 describes 3 connections (no peephole for blockInput). This destroys the equal-sizes of my matrices, because now 3 gates use the Usual, Recurrent and Peephole weights, but the very first gate only uses the Usual weights 

I am building an LSTM to recognize if the person is sad, happy, angry or neutral. This is done by feeding-in his the wave of his voice into the network, as a sequence of bits (0-to-255). The problem is, my dataset is not large enough, are there efficient ways I could augument my dataset? I am training on short 1.5 second clips and I have 800 of those, which is not enough. My current augumentation is: 

Each distribution could be estimated with kernel density estimation (KDE). KDE is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is smooths the data / summarizing the data. It has the advantage of not having to pick a specific distribution apriori. The disadvantage of a KDE is the data is not summarized in a single variable, the data is summarized as a function. 

A working example of a Variational Autoencoder for Text Generation in Keras can be found here. Cross-entropy loss, aka log loss, measures the performance of a model whose output is a probability value between 0 and 1 for classification. Cross-entropy loss goes up as the predicted probability diverges from the actual label. In the case of character-by-character autoencoder, each character in the vocabulary would be a label. Cross-entropy works if the input and output are the same size, that is the case in character-by-character autoencoder. Often times in text analysis, the input and output sequences are different lengths so a second term, encoder loss, is added to the objective function. 

I've just realized my prediction approach for LSTM might not be correct. I am trying to predict character by character, by reading over the book. The way I've approached the problem is as follows: 

I am reading the paper on Neural Turing Machine, page 9, and got stuck on just 1 pesky point. I can't understand how the shift is done: 

Reversing the sequences doesn't seem to be applicable, after all, my network will be predicting non-reversed speech when it's fully trained. 

There is a matrix E between Input-Hidden, describing the weights to make your one-hot into an embedding. You don't need to perform any activation on these 300 neurons and can use it straight away as an embedding in any future task. 

I've been reading about word2vec and it's ability to encode words into vector representations. Coordinates (probabilities) of these words are clustered together with their usual context-neighbor words. For example, if we have 10k unique words in our dataset, we will feed 10 000 features into the network. The network can have for example 300 hidden neurons, each with linear activation function. The output of the net are 10 000 neurons, each with a softmax classifier. Every such a "soft-maxed output" represents probability to select the appropriate word from our 10k dictionary. 

Word Mover’s Distance (WMD) is an algorithm for finding the distance between pairs of strings. It is based on word embeddings (e.g., word2vec) which encode the semantic meaning of words into dense vectors. 

Another way to reframe the problem is approximate/fuzzy string matching. FuzzyWuzzy is an approximate/fuzzy string matching package for Python. The package can find the nearest element in a set to a user's inputted value with a level of confidence. Something like this: 

Name parsing does not appear to built-in to Stanford CoreNLP. . One option is writing a series of Regular Expression using Stanford TokensRegex to parse and label name tokens. Another option is using a third party package, such as nameparser in Python. 

You could represent the different models with different colors on the same generalized space. The figure could be interactive with the ability to select one or more models at a time.