ADC being Active Directory Connector for Exchange? If so, being an ADC doesn't make the server a domain controller also. If this is the case, you're out of luck as you've lost your only functional DC. However if I'm reading that wrong and DC2 is definitely a full domain controller, then I think your problem lies in trying to use restore mode. You shouldn't need to. Instead, bring DC2 up into its regular windows environment, then follow the 'Seize FSMO Roles' guide found here: $URL$ Note: Ignore the 'Transfer' guide. It's not relevant for your scenario. Edit: I just found the other question where you got the term 'ADC' from. This term is widely recognised as 'Active Directory Connector for Exchange' and not 'Additional Domain Controller' which I think is your usage. I don't recommend the use of acronyms that aren't widely used, as it often just causes confusion. 

In theory, the clients will check in with Windows Update, see they need some patches, check their cache, and realize they already have it. I haven't verified this, but it's worth investigating. We're based in NZ and do some work out of an Australian office. We hit issues with bandwidth caps and the speed across the Tasman all the time. 

My switching knowledge is sketchy, but: You can't send more than one untagged VLAN to a port. As soon as you start sending more than one VLAN you HAVE to use tagging. Also I'm not sure there's such a concept as 'no PVID setup', I think that the PVID is already set on all ports, but you can change the value. 

According to Sun's specs the max RAM it'll take is 64Gb. That said, there's no reason not to take ESXi or that server to its max ram. Unless your colo techs can come up with a very specific reasons. We're running ESXi servers quite happily with >100Gb of RAM on Dell and HP hardware, so unless there's some technical limit on that particular server model, I'd say you're good to go. The server is on VMWare's ESXi Hardware Compatibility List with a max RAM listing of 64Gb. In my experience, with 2008 becoming more common as a VM Guest OS, you almost always run out of RAM on a virtual host before you run out of CPU. It depends on workload, of course, but VMs LOVE RAM, and this trend seems to be getting more and more pronounced. I would warn that 8Gb RAM sticks are uneconomically expensive (3x or more the cost of 4Gb sticks). 4Gb sticks seem to be the sweet spot. Another suggestion: If you're considering taking a single host to such a high level of RAM and this is your only ESXi host, you are putting a lot of eggs in 1 basket. A good option that gets you both scalability and resilience is to buy a second identical server and run them as a pair with less ram per server... Then if your one server falls over, you won't lose your environment. EDIT: Just to illustrate the CPU-vs-memory usage in a production environment, here's a snippet from an email I sent out a few days ago, showing CPU and RAM utilization on virtual hosts at one of our datacenters. The servers in question are HP DL360 G6 dual-quads with 70Gb RAM per host, running ~160 VMs across the 5. I'd love to see if others are seeing similar trends, or if this is just a characteristic of our workloads. 

LG are doing some very cheap LED-backlit monitors at the moment, and the viewing angles appear to be excellent. I'll try spinning one and let you know the model number if it looks good. I also found that running f.lux highlights the problem much more on panels with bad viewing angles. We run some nice dell units with swivel stands, but they look terrible when rotated. 

Just spotted this question and realize I'm a bit late to the party. Anyways... a mapped drive network disconnection on a workstation OS after 15 minutes indicates that the server service believes the connection is idle, and is disconnecting it. To fix this, enter this on the command line: net config server /autodisconnect:-1 

You can repeat the above as necesarry to re-create your test environment quite quickly. There's also nothing stopping you raising multiple environments at the same time, as long as you create a seperate host-only network for each instance. There are some limitations of this setup (mainly that having all the machines in isolates networks makes it a hassle to load data) but it'll allow you to setup quickly and ensure you're always returning to a true reference baseline. 

No, google apps for business is run from the same infrastructure as the rest of google apps. There are some that run on a separate infrastructure (early adopters, I believe), but these are being transitioned back into the main infrastructure. You can tell if a given google apps account is being run from the core infrastructure by trying to load one of the apps email accounts alongside a regular gmail account on the same browser session. If you can open both at once, they're on separate infrastructure. If you can only open one at a time, they're in the same infrastructure. 

If the server is edge, chances are it's out in your DMZ. You'll need to ensure that there's sufficient rules punched between the server and its domain controller, through the firewall. If possible look at firewall traffic logs between that server and the DCs, and you should find some dropped traffic. 

That IP is definitely one I would avoid. Even if the end users devices can handle a .255, there's a chance some bad config anywhere between your server and your users might also be mishandling the traffic. Guessing it's a hold-over from the old class-based way of handling subnets. 

Ultimately you can target a tiered storage solution that stacks up like this, in order fastest to slowest: - RAM - Pagefile - SSD - Fast SATA/SAS stripe - Bulk Disk array 

You should also be able to solve this at the firewall level using two NAT rules, assuming the websites can be bound to seperate IPs. You NAT one of the websites through from external port 80 to internal port 8080. As an aside, I prefer a port other than 8080, as that's conventionally used for Web Proxies. 

You can get by on either route. The certifications would be my choice in your position for the following reasons: 

Sure. Run ifconfig on the server and it will show you all the network attachments on the server and their currently bound IP addresses. To add an additional IP to one of the network adapters, edit it into the network configuration file for that adapter. The files are explained here: $URL$ 

As Ward mentioned, a virtual machine would be ideal for your purposes. You can snapshot the disk before installing a piece of software, then roll it back to the point before you installed it, and it's completely gone. However if you're determined to go with the app virtualisation route, look at Altiris SVS. It's an enterprise-grade product but a non-commercial version was released for free a few years ago. It does pretty much what you describe and isolates both the application and any data that application writes (ANY data). 

Consider an external Capture-And-Release style message filtering service. Symantec offer this in the form of Messagelabs Google offer this in the form of Postini These will be more effective at controlling spam at the edge of your network, while still allowing users to manage their caught spam and release messages if necessary. Disclaimer: There are probably lots more companies offering similar services, so shop around to find the best fit for you. I'm just most familiar with the 2 above. 

At this point I would suggest that you take a solid backup of everything you have then re-build the SBS from the ground up. You have all your mailbox data and presumably can backup all your file data - An SBS AD structure is typically not massive so it should take under 1 day to set all that back up again. You may be able to recover by removing and re-adding DNS Role on the server but at this point I suspect your troubles run deeper than this. You'll almost certainly need to remove and re-join all of the machines to the domain once more. You can also try a cheat - try adding the domain name to the HOSTS file entry on the SBS, then re-try the /fix process. The issue may be that the server cannot resolve the DNS name of the domain (because DNS is busted!) and this may side-step it. This is a powerful example of why snapshots should never be taken on domain controllers, but that advice is a little bit late for you ;( 

Bearing in mind that 24Gb is the most RAM that i7 chip can address, and it's non-ECC and at a lower memory bandwidth, and that you're not able to scale to multiple i7s.... i7 - 3.33Ghz * 6 cores = 19.98 E5620s - 2.4 * 8 cores = 19.2 Whether your application is highly parallelized or not, the i7 still looks like it will narrowly edge out the E5620s for raw processing. However, there are better E56xx chips available and they'll exceed the performance of the i7, especially in 2-chip setups. You also need to pay attention to the memory bandwidth as all that extra processing power becomes useless if you transfer the data at suitable speeds. Outside of these issues, also bear in mind that the intel server chips are designed for continuous operation and stress tested more completely than the i7s which are desktop targeted and generally no as robust. 

Any one of the factors above (and probably a whole bunch more) will impact the reliability of a real network. So you can see that a holistic view must be employed to properly gauge a real network's reliability. Applying computer science directly to IT scenarios tends to be problematic because real-world factors are rarely considered in detail Here's a few real-world examples I've encountered where the reliability of a network couldn't be measured by k-connectivity: 

A similar concept, yes. But usage of the terms are a bit ephemeral. A private cloud is one that is single-tenant. That is, your VMs run on hardware dedicated to the task, with no unrelated company sharing that tin. This may be for example a VMWare or Microsoft Virtual Infrastructure that you host and maintain, or it could be hosted by a third party but dedicated to your use. In theory there could still be an automated billing and provisioning system for this, but I don't know of a system that's active today that'll do fully automated provisioning yet dedicate your own piece of hardware. A public cloud is shared infrastructure, multi-tenant environment. Your VMs may be executing on the same physical hardware as another client's VMs. In both cases your VMs and networks are still distinct units in their own right, it's just dedicated hardware vs shared tin. So that's my yardstick, but it's not universally accepted. Off the top of my head you could also differentiate on the basis of availability - Anything home-rolled is a private cloud. Anything available to public via a web interface is a private cloud. But then what about business to business cloud solutions that aren't widely available? Likewise, you could divvy it up by network connections - Clouds connected back solely to your environment are private, and setups that can link straight out to the web are public. Even Wikipedia disagrees: $URL$ 

You can use a fully qualified domain name that you do not own. However, if you're going to go down this route, make sure you pick an FQDN that is not and could never be owned by a third party. So pick one ending with .lan or .local, as Zaid suggests. The reason for doing this is that if any of your machines are ever used off your local network (e.g. one of them is a laptop and you take it to a cafe), that machine will be trying to resolve network names with the FQDN you chose, on the public internet. If someone else owns that domain on the 'net, then those requests are going to end up on their doorstep. Thanks to a DNS client feature in Windows called 'DNS devolution', even if the exact target DNS name doesn't exist on that remote network, the request will get re-sent with just the base domain name e.g. if you try and resolve mypc.domainname.com and it fails, the machine will then go out and just request domainname.com, then finally just com. Long story short, either buy an externally recognised domain name (it's cheap!), or go with the not-fully-supported-but-still-acceptable .lan or .local suffixes.