If you truly are, as you say, "lazy", then perhaps game development is not the hobby for you. Game development requires work and effort. If taking a couple of hours to compile and install a Python module for a library is too much for you, then what are you going to do when you hit a real problem? When you can't figure out how to make the various objects interact correctly? When your AI seems to be doing the wrong thing and you can't figure out why? That's no excuse for a difficult install process. But if you need this library, then you should be willing to expend some effort to getting access to it. 

So just make sure that your game is interesting to the player, either gameplay-wise or story-wise. Easy, right? 

That requires transforming the camera-space light direction into model space, so you need a matrix to do that. Or you could do that transformation on the CPU and save time, so you'd be passing the model-space light position/direction. So there isn't just one possible TBN matrix; there are many forms. It's all about what space you want to transform from and to. And this is just my personal opinion, but if you see a tutorial that suggests you do lighting in world space, stop reading it. Read something written by someone who's not an idiot. As for transpose guy, he is simply wrong. The transpose of a pure rotation matrix is its inverse, so what he's doing is take a matrix that transforms from normal space (the space the normals are in, which as shown above is dealer's choice) to tangent space and doing it backwards. The goal of that is to transform the normal fetched from the tangent space bump map into the space of the normals, so that you can do your lighting computations in a space other than tangent space. His erronous assumption is that the TBN matrix is a rotation matrix; it isn't. TBN matrices will almost never be orthonormal; even if you compute the bitangent by cross-product, the normal and the tangent do not have to perpendicular, and they often will not be. Indeed, they should only be perpendicular if the surface is flat. The tangent and bitangent point along the flow of the surface in the S and T components of the UV. That's almost never in the plane of the normal. 

If this is just simple, local information, then storing it in any kind of "database" is massive overkill, let alone something like MySQL. SQLite might be appropriate. These data seem to be for game concepts. So don't forget: you need to author this data. It's a lot easier to hand-edit XML/JSON/etc than it is for some kind of database. You can even develop tools to validate XML, thus making sure that values are within acceptable ranges. If you use something like a Lua script, you can even have dependent properties, where changing one value can change it globally across all scripts. Considering how easy it is to author text, and how hard it is to author databases (unless you want to write a specialized editor program), I see no reason to use databases for this at all. 

If you're using a rendering API, then you only need to worry about what that API tells you to worry about. OpenGL doesn't say anything about quad-based or triangle-based rendering systems. So you don't need to concern yourself with it. In any case, all consumer-grade GPUs use triangles, not quads. 

I personally don't care much for answers that are just links to other stuff, but these two tutorials explain matrices and transformations really well. They explain why it is that creating a Y-rotation matrix doesn't rotate about world-space Y, and the second link explains how to rotate things in different spaces, and how to overcome these kinds of issues. Yes, the tutorials are for shader-based programming. But transformation matrices are the same, whether using shaders or fixed-function. In the interest of full disclosure, I wrote these, so take the above for what it's worth. 

Assuming that you are referring to Bidirectional Reflectance Distribution Functions, a BRDF is nothing more than a fancy name for a broad class of lighting equations. These are Functions that compute the Distribution of Reflectance from a surface based on two directions (Bidirectional): the direction of the incident light and the direction of the viewer of the surface. The BRDF directions are both defined in terms of their relative direction with respect to the normal, so the normal is effectively an implicit parameter. This is a very generalized category of lighting equations. Standard Lambert, Blinn, Phong, and so forth all qualify as Bidirectional Reflectance Distribution Functions. They can all be stated in terms of the direction towards the light and the direction towards the viewer. However, since you're comparing it to Matcap, I assume you are referring to physically based BRDFs, not the general category of all BRDFs. That is, BRDFs generated from tables computed by using machines to detect the light interaction properties of real materials with a changing view and light direction. Assuming the Matcap you are referring to is something like this, I would have to say, no. At least that particular implementation of Matcap is not. If you want to be exceptionally generous, then it is a BRDF. But only in the loosest possible sense. Why? Because the reflectance computed does not change with either of the two directions. Just look at the shader; the only directional information it uses is the surface normal. And while it is a property that is used to compute the color, from the perspective of a BRDF, it's a constant. If you want to be technical, Matcap uses a BRDF. Namely, the sphere texture. That is what the lighting equation actually is; that's the reflectance distribution, based on a specific view and light direction. A proper physically-based BRDF is effectively a 4-dimensional lookup table, based on the angles of the two directions. Matcap uses a 2-dimensional lookup table, and the lookup is not really based on either the view or the light direction; just the normal. Matcap is basically cheating, using a single, static 2D slice of a proper 4D BRDF. So no, Matcap is not a BRDF, much like your code that uses the result of a BRDF to color a particular pixel is not itself a BRDF. 

The first step in the most efficient angle/axis->matrix transform is converting the angle/axis matrix into a quaternion. So if you already have a quaternion, there's no point in turning it into angle/axis, only to give it to someone else who's going to turn it back into a quaternion to make it into a matrix. The D3DXMATRIX class has several constructors, one of which takes an array of floats. If your array and D3D's array order their components the same way, then you can just use that. If not, then you can use it anyway and simply transpose the matrix afterwards. 

This is called a "feedback loop". It is decidedly wrong. And while texture barrier could save you in certain cases, it can't save you when you want to do a blur filter. You're going to have to render to a different texture than you read from. 

When you draw your shadowing layer, what you do is turn on the stencil test and have the test function be equals. All of your foreground should be rendered with a stencil value of 1. The code you would use before rendering your shadowing layer would be (warning: untested): 

The purpose of anything that you would actually call a "database" is primarily to make searching and finding specific information easy and fast. Unless you have some need to find entities via specialized queries based on many of their properties, you don't need a database. You just need a simple array or map or whatever. A simple thing like "find entity by name" is something that your programming language's standard library can cover. In C++, the first go-to type for this would be . 

It's "easy enough" to ensure that this happens in C++. In C#, however, that's going to be a lot harder to deal with. This is thanks to JIT. What do you suppose would happen if the interpreter ran your code interpreted once, but then JIT'd it the second time? Or maybe it interprets it twice on someone else's machine, but JIT's it after that? JIT is not deterministic, because you have very little control over it. This is one of the things you give up to use the CLR. And God help you if one person is using .NET 4.0 to run your game, while someone else is using the Mono CLR (using the .NET libraries of course). Even .NET 4.0 vs. .NET 5.0 could be different. You simply need more control over the low-level details of a platform to guarantee this kind of thing. You ought to be able to get away with fixed-point math. But that's about it. 

Well, that's just not going to happen. Shaders, yes. OpenGL 3.2+ core profile? No. There is no engine on the market that works against the core profile of OpenGL. None. All of them use some of the removed functionality to some degree, even if it's just GL_QUADS. All of them. There is no need for the core profile requirement anyway. Especially since you're making a game for Android, which does not support OpenGL at all. It supports OpenGL ES, which is not the same thing as desktop OpenGL. ES has no concept of profiles at all, let alone a core profile. Furthermore, you can't just take a desktop-based OpenGL engine and throw it onto OpenGL ES 2.0 and expect it to work. That will involve porting, which (given the significant differences between desktop GL and GL ES) will involve quite a bit of work. Therefore, you should be looking at engines that have OpenGL ES support. 

It is not necessarily the "only" way, but it is the best way. It is the way that works on anything, from the lowly Voodoo 1 all the way to the Radeon 6990. It doesn't require shaders (though they can be useful with this). All it requires is a bit of pre-processing on your end to put all of the images in one large image. This is hardly an onerous burden. That being said, how many sprites do you intend to draw per frame? If it's less than a couple of hundred, an atlas isn't that important to your rendering performance. Texture atlases matter most for things like particle systems and fonts, because you're drawing lots of them. 

Just build your rooms already connected. Start with one room, then build 1-3 corridors to other rooms. Then recurse until you've added enough rooms. 

As to the code you posted, the camera in camera space is at the origin. That's why it's called "camera space". You don't need to pass it to the shader. Also, the dot product is between the direction from the camera to the fragment. So you need to compute that. It isn't something you can pass to the fragment shader. You either need to send the camera-space position as a per-vertex varying, or you need to reverse-transform to get the camera-space position. 

Sounds like a driver bug. A simple way around this is to write your diffuse and/or specular colors out in linear values. To maintain proper color resolution (the sRGB-to-linear conversion happens in floats, but storing linear values in 8-bit ints loses some precision), you should use 10-bit RGB images. Obviously this will not work if you need a real alpha component, but maybe you can hide that within your normal or something. 

This question is confused. Animations are composed of sprites. They're not separate things; one is a superset of the other. Typically you have a big image that holds multiple sprites that, when you flip between them, form animations. 

The fundamental problem with this question is that it conflates two concepts that have nothing to do with one another: 

The homogenous representation of P, which is a vector position, has a W equal to 1. The homogenous representation of N, which is a vector direction, has a W equal to zero. 1 * 0 is still 0, and therefore the W term of neither P' nor N' will have any effect on the result of the dot product. There are no two 3D vectors P and N such that and that So the example is simply impossible. 

If takes radians, you can feed that directly to it. If it takes degrees, you'll have to convert it to degrees by multiplying by the radians by 180/Pi. 

When a script changes, you simply update the functions in the . There are more complex ways of doing it, by giving C++ code tables that contain metamethods for that will go track down the actual function or whatever. But this is the simplest and most obvious method of doing it. This sort of complexity is generally why I don't give Lua this power. I want to focus my Lua scripts on doing their actual job, rather than updating functions and other nonsense. So I give C++ the responsibility for managing Lua-created object loading and destroying. I tend to use Lua scripts as an alternate means of instantiating C++ derived classes. So "reloading a script" means destroying an object and replacing it with a new one. 

This makes reference to , which draws the given face of the cubemap. That is implemented as follows: 

I'm looking for a way to create sRGB raster images. Now, I could use Inkscape and just have it render to a raster format, but are there any image drawing tools other than Photoshop that work in sRGB? Here's what I want to be able to do more specifically. Let's say the image is all one color. I want to be able to pick color 0x808080, which is in the sRGB colorspace. I want the file to write an image where every pixel is 0x808080. And then, when I read it, I can tell OpenGL that it's sRGB color data, so it will treat the 0x808080 color has being from the sRGB colorspace. The important part is that, when I'm selecting colors in the tool, the colors I'm selecting are from the sRGB colorspace. 

If that's really your question (which is most assuredly not what you said in your actual question text, which ended in 4 questions, none of which were asking where you might find a resource), then the answer is simply: There isn't one. The majority of C++ programmers don't have to care that much about the overhead of standard library structures, cache performance of them (which is highly compiler dependent anyway), or that sort of thing. Not to mention, you usually don't get to pick your standard library implementation; you use what comes with your compiler. So even if it does some unpleasant things, the options for alternatives are limited. There are of course programmers who do care about this sort of thing. But they all swore off using the standard library a long time ago. So you have one group of programmers who simply don't care. And another group of programmers who would care if they were using it, but since they aren't using it, they don't care. Since nobody cares about it, there is no real information about this sort of thing. There are informal patches of information here and there (Effective C++ has a section on std::string implementations and the vast differences among them), but nothing comprehensive. And certainly nothing kept up-to-date.