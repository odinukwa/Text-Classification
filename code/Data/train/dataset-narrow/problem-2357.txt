Krawczyk describes a scheme to reduce the amount of randomness basically by letting $a_i$ be the $i$th row of a Toeplitz matrix. However, the Krawczyk scheme works over $GF(2^b)$, not arithmetic modulo $2^b$. 

This arises from a practical application, so if you can't solve this problem, feel free to solve any variant or interesting special case. For instance, feel free to instantiate any of the parameters or thresholds in any way that's convenient for you. You can assume the circuits are not too large (polynomial sized, or something). Feel free to replace graph edit distance by some other measure of near-match of implementation. Also, in practice SAT solvers are often surprisingly effective on the structured circuits that arise in practice, so it's probably fine to invoke a SAT solver as a subroutine/oracle (at least, if you're invoking it on something like a SAT instance derived from a circuit like $C$). Alternatively, lacking any algorithms, I would also be interested in the existence question: for an "average" circuit $C$, what's the probability that there exists some $D$ that meets all the criteria? (I'm hoping this probability is very low, but I have no clue if that is the case.) 

The practical application is to test whether a circuit $C$ might contain a malicious backdoor / hidden Easter egg. The hypothesis of how such a thing might get inserted goes like this. We start with a "golden" circuit $D$, that computes the desired functionality and has no hidden backdoor. Then, the adversary makes a small change to $D$ to introduce the hidden backdoor, obtaining a modified circuit $C$. The purpose of the backdoor is to change the function computed by $D$ in some way. If $\Pr[C(x) \ne D(x)]$ is not too small, the change can plausibly be detected by random testing, so an adversary will probably try to keep $\Pr[C(x) \ne D(x)]$ very small. Similarly, if $C$ differs from $D$ in too many places, this might be noticed by random inspection of the circuit, so an adversary will probably try to minimize the number of changes. (And, there may be a test suite of $x^i,y^i$ pairs that represent instances of the desired functionality, so we know that whatever the "golden" circuit $D$ is, it satisfies $D(x^i)=y^i$ for all $i$.) Ultimately, we are given the circuit $C$ (but not the "golden" circuit $D$), and we want to know whether $C$ might be a modified version of some $D$, where the modification was made to introduce a hidden backdoor of this sort. 

A similar method can be built to refine $A,B$ to get $B^*$. You basically reverse things above and flip some signs: e.g., instead of $d+B$, you look at $-d+A$. How to compute an initial over-approximation. To get our initial over-approximation, one idea is to assume (wlog) that $b_1=0$. It follows that each value $a_i$ must appear somewhere among $D$, thus the list of differences $D$ can be used as our initial over-approximation for the $a$'s. Unfortunately, this doesn't give us a very useful over-approximation for the $b$'s. A better approach is to additionally guess the value of one of the $a$'s. In other words, we assume (wlog) that $b_1=0$, and use $A=D$ as our initial over-approximation of the $a$'s. Then, we guess which one of these 36 values is indeed one of the $a$'s, say $a_1$. That then gives us an over-approximation $B=a_1-D$ for the $b$'s. We use this initial over-approximation $A,B$, then iteratively refine it until convergence, and test whether the result is correct. We repeat up to 36 times, with 36 different guesses at $a_1$ (on average 6 guesses should be enough) till we find one that works. A full algorithm. Now we can have a full algorithm to compute $a_1,\dots,a_6,b_1,\dots,b_6$. Basically, we derive an initial over-approximation for $A$ and $B$, then iteratively refine. 

Build an interval tree storing all of the ranges that have been written. That data structure lets you efficiently determine, for any given memory address, the time when it was most recently written. In particular, scan through the operations in chronological order. When you see a write operation, update the interval tree. Also, for each operation (read or write), check the interval tree to find all intervals that overlap it, and add them to the DAG. This will build up a dependency DAG with no redundancies. Each update to the tree can be done in $O(\log N)$ time. Also, the overlap check can be done in $O(k + \log N)$ where $k$ is the number of intervals that overlap for that particular query. The running time will depend on the size of the resulting dependency graph, but if that graph has $M$ edges, then the total running time will be $O(M + N \log N)$. In particular, since this DAG can have at most $O(N^2)$ edges, the algorithm certainly runs in $O(N^2)$ time -- and possibly much faster, if the resulting dependency DAG is sparse. 

Yes, it is safe to claim that most theorists believe that $\mathsf{NP}$ is not equal to $\mathsf{ZPP}$ -- and that most theorists believe that $\mathsf{NP}$ is not equal to $\mathsf{BPP}$. One consequence of $\mathsf{NP} = \mathsf{ZPP}$ is that computational cryptography is impossible. This is described as the world Algorithmica in Impagliazzo's famous five worlds: 

You can solve this in $O(n \lg n)$ time through an appropriate use of interval trees. I'm going to explain how to process the $x$'s in an incremental, streaming fashion. So, suppose we've already received $x_1,\dots,x_n$. Define the sequence $m_1,\dots,m_n$ by $m_i=\max(x_i,x_{i+1},\dots,x_n)$. In previous processing, we'll have accumulated a data structure that summarizes the sequence $m_1,\dots,m_n$. This data structure supports two kinds of operations: 

The obvious algorithm is to use binary search. You'll need $\lg n$ iterations of binary search, where $n$ = the number of convex sets in your sequence. (If you have an infinite family of convex sets, $\lg(|f(x)|/\epsilon)$ iterations are enough to approximate $f(x)$ to within $\epsilon$.) In each iteration, for some value of $\lambda$, you test whether $x \in C_\lambda$. You haven't specified the representation of the family of convex sets, but for typical representations, testing whether $x \in C_\lambda$ can be done in polynomial time. So, the total running time will be polynomial. To do better than this, I suspect the algorithm will need to depend upon how the sequence of convex sets $C$ is represented. (Do you have a requirement for it to be represented in a particular way, or are you OK with any reasonable representation? You might want to edit the question accordingly. You might also want to specify whether this is a finite sequence or an infinite sequence, and how the sets are related.) 

If $m=2^k-t$ where $t$ is small, then you can reduce modulo $m$ faster. In particular, $a \cdot 2^k + b \equiv at+b \pmod{m}$, so reducing an $\alpha k$-bit number modulo $m$ typically requires $\alpha$ multiplications by $t$ and $\alpha$ additions. This can be significantly faster than the normal algorithms, if $t$ is small enough. Something similar holds for $m=2^k+t$, too. So, one plausible approach is to choose a modulus $m$ that is a product $m=m_1 m_2 \cdots m_n$ where each $m_i$ has the form $2^k \pm t$, where $k$ is the word size of your computer (e.g., $k=64$). 

I feel like there ought to be some way to work modulo $2^i$, $3^j$, and $5^k$, and at each step separately increment either $i$, $j$, or $k$, but I haven't worked out the exact details of how to do that yet. If you can make that work, the resulting scheme would be significantly more efficient. 

Here's an observation that I think gives you a foothold, possibly enough of one to solve the problem. Suppose we have four differences $a_1-b_1$, $a_1-b_2$, $a_2-b_1$, $a_2-b_2$ that arise as the pairwise differences between two $a$'s and two $b$'s. Call this a quartet of differences. Notice that we have a non-trivial relationship: $$(a_1-b_1)-(a_1-b_2) = (a_2-b_1)-(a_2-b_2) \pmod N.$$ You can try to use this relationship to identify potential quartets out of the list of $K^2$. For instance, pick four differences out of the list; if they don't satisfy the above relationship, then they definitely don't arise from a quartet structure; if they do satisfy the relationship, they might arise from a quartet. There are many ways you can take things from here, but I suspect this is going to be enough. I especially suspect that, for your example parameter settings, the problem is going to be pretty easy, because the above test for recognizing a quartet probably won't have too many false positives. Our of all ${K^2 \choose 4}$ ways of choosing 4 differences from the list, there will be ${K \choose 2}^2$ quartets (which will all satisfy the relationship) and the remainder are non-quartets (which satisfy the relationship with probability $1/N$, heuristically). Therefore we expect to see about $({K^2 \choose 4}-{K \choose 2}^2)/N$ false positives, i.e., 4-tuples that pass the test even though they aren't quartets. For your parameters, this means we have 225 quartets and $(58905-225)/251 \approx 234$ other false positives; so about half of the 4-tuples that pass the test are actually quartets. This means that the above test is a pretty good way to recognize quartets. Once you can recognize quartets, you can really go to town on recovering the structure of the list of differences. 

The final value of $T[\cdot,\cdot]$ will have the desired form (it is a minimal fixpoint subject to all of the conditions above), and thus will be a valid solution to your problem. Each $T[u,v]$ contains the set of minimal clobber sets, taken over all paths from $u$ to $v$. Notice that the above algorithm does not specify the order in which you pull items out of the worklist. Good scheduling algorithms can improve the performance of this scheme. One standard heuristic is to use reverse post-order (derived from a DFS on the graph) to prioritize which elements to pull out of the worklist. Also note that the running time of this algorithm could be exponential, if your graph is unfortunate. This is unavoidable: there might be exponentially many clobber sets, so the output size might be exponential in the size of the input in the worst case. 

For instance, for TSP, you could do something like the following. Generate a random problem instance by picking a random $U(0,1)$ cost matrix. Then, adjust the problem instance to hide a much-better solution in it: randomly select a tour that visits each vertex exactly once, and reduce the edge weights on that tour (e.g., generate it randomly from $U(0,c)$ where $c<1$; reduce the existing weight; or modify the existing edge with some fixed probability). This adjustment procedure ensures that the optimal solution will, with high probability, be that special tour that you selected. If you're lucky and you select a reasonable embedding, it will also not be so easy to recognize where you hid the special solution. This approach is derived from general ideas in cryptography, where we want to create trapdoor one-way problems: where the problem is hard to solve without knowledge of the secret trapdoor, but with knowledge of the secret trapdoor, the problem becomes very easy. There have been many attempts to embed secret trapdoors into a variety of hard problems (while still preserving the hardness of the problem even after the trapdoor has been added), with mixed degrees of success. But this general approach seems like it might workable, for your purposes. The resulting problem instances might be hard, but will they be interesting, from any practical perspective? I don't know. Beats me. They look fairly artificial to me, but what do I know? If your primary goal is to select problem instances that are practically relevant and representative of real-world applications of TSP, my suggestion would be take a totally different approach. Instead, start by surveying real-world applications of TSP, then looking for representative instances of those problems, and convert them to their corresponding TSP problem instance -- so you are working with problem instances derived from a real world problem. 

My problem: I am given a circuit $C$, and I want to know whether there exists a circuit $D$ that is similar to $C$ but not identical to $C$ (i.e., where there exists $x$ such that $C(x)\ne D(x)$). Can anyone suggest an algorithm to solve this problem? If it helps, we can restrict attention to circuits $D$ that are smaller than the given circuit $C$ (i.e., we want to know whether there exists a circuit $D$ such that $D$ is smaller than $C$, $D$ is similar to $C$, and there exists $x$ such that $C(x)\ne D(x)$). If it helps, you can additionally assume we are given known-good test cases $x^1,\dots,x^m,y^1,\dots,y^m$ such that $C(x^i)=y^i$ for all $i$, and we can further restrict attention to only circuits $D$ such that $D(x^i)=y^i$ for all $i$. 

Their solution is actually even better than you asked for. They also provide security against insider attacks: even an authorized recipient does not obtain any information that would allow the recipient to jam the channel and prevent others from receiving the broadcast. 

Given your edit, the question now looks trivial. The structure on $\mathcal{O}$ is irrelevant. Basically, you are given a set $\mathcal{O}$ and a set of pairs $a_i,b_i$ with the promise that $a_i < b_i$ for some total order $<$. You want to find all pairs $a',b'$ such that there exists some total orders $<_1,<_2$ that are consistent with all the examples and such that $a' <_1 b'$ and $a' >_2 b'$. Assuming I've got all that right, that's an easy problem to solve, computationally. Basically, build a DAG with an edge $a_i \to b_i$ for each $i$. Compute the transitive closure of the DAG. Now, if there's an edge $a' \to b'$ in the transitive closure, then you know $a' < b'$ in all possible orders. On the other hand, for any pair of vertices $a',b'$ that are not connected by an edge in the transitive closure, you could have either $a' < b'$ or $a' > b'$. So, that's the region of uncertainty: the set of pairs of vertices $a',b'$ that are not connected by an edge in the transitive closure of this DAG. If I haven't got all that right, you might need to provide a clearer statement of the problem in the question. 

One issue is that many of the theorems we're used to in information theory, don't hold in the computational world. Therefore, even if we formalized a computational analog of entropy, the resulting theory might not look like information theory any more. For instance, if $f$ is a deterministic function, then $H(f(X)) \le H(X)$. However, for any plausible computational notion of entropy, this will no longer hold: think of a pseudorandom generator, for instance, which stretches a short seed into a long pseudorandom output. By any conceivable definition of computational entropy I can imagine, that long pseudorandom output will have large computational entropy (it is computationally indistinguishable from a uniform distribution on those long strings), thus violating $H(f(X)) \le H(X)$. 

The length of the solution is something like $O(n^3 \lg n)$. The running time of the procedure is something like $O(n^5)$. This is all polynomial. 

Yes, when implementing this sort of thing in practice, one has to be careful that this doesn't screw you up. If you're not careful, when trying to estimate the probability of a term that doesn't appear in the document, you might mistakenly estimate its probability as 0 (e.g., if your estimate of its probability is the number of times it appears in the document divided by the number of words in the document). However, the actual probability is almost certainly not 0. A standard workaround is to use Laplace smoothing, add-one smoothing, Bayesian smoothing, or another similar method. 

I suggest you use the framework found in the following paper: How Far Can We Go Beyond Linear Cryptanalysis?, Thomas Baignères, Pascal Junod, Serge Vaudenay, ASIACRYPT 2004. The crucial result says that you need $n \sim 1/D(D_0 \,||\, D_1)$, where $D(D_0 \,||\, D_1)$ is the Kullback-Leibler distance between the two distributions $D_0$ and $D_1$. Expanding out the definition of the K-L distance, we see that in your case $$D(D_0 \,||\, D_1) = p \log \frac{p}{p+\epsilon} + (1-p) \log \frac{1-p}{1-p-\epsilon},$$ with the convention that $0 \log \frac0p = 0$. When $p \gg \epsilon$, we find $D(D_0 \,||\, D_1) \approx \epsilon^2/(p(1-p))$. Thus, when $p \gg \epsilon$, we find that you need $n \sim p(1-p)/\epsilon^2$ coin flips. When $p = 0$, we find $D(D_0 \,||\, D_1) = -\log(1-\epsilon) \approx \epsilon$, so you need $n \sim 1/\epsilon$ coin flips. Thus, this formula is consistent with the special cases you already know about... but it generalizes to all $n,\epsilon$. For justification, see the paper. 

There is no standard notion of "fundamentalness". It's not clear that what you are looking for is meaningful or makes any sense. What is "fundamental" is a matter of perspective; I don't see any reason to privilege insert as more fundamental than splice, or vice versa. So, if you need a concept of "fundamental" for some reason, you'll need to define for yourself what you mean by "fundamental". If you're going to head that path, I would suggest that you start by asking yourself, "Why do you care?" Personally, I suspect an XY problem. I suspect you're asking the wrong question, and you should go back and ask yourself why you want to know and what you will do with the answer -- and that might help you identify a more sensible question or a more pragmatic path to achieving your real goals. 

I don't think there will be any result of the form you want. There's no single "most general system" that has this property. (Instead, it'd be better to choose some specific pair of classes of systems you're interested in and ask specifically how to distinguish those two; that's more likely to yield an answerable question.) To learn about work in the computer science literature that might be applicable to your situation, here are some subjects you should probably take a look at: