How to recover SQL Server data from accidental UPDATE and DELETE operations How to recover SQL Server data from accidental updates without backups Disclaimer: I work for ApexSQL as a Support engineer 

Start the Command Prompt Go to the Tools\Binn folder Run code such as osql -H TestServer -S Fujitsu\SQL2012 -i D:\Test\sync.sql -U sa -P sqladmin 

If you're not using a third party tool, I recommend the fn_dblog function. As it's undocumented, it's not easy to use and the results it returns are not easy to read. Try with the scripts someone has already tested: SQL Server â€“ How to find Who Deleted What records at What Time How to recover deleted data from SQL Server Keep in mind that fn_dblog can read only the online transaction log For transaction log backups, use fn_dump_dblog 

You can catch values for every inserted column in a row and save them into the table where you save other audit data. There are third party tools, such as ApexSQL Audit, that create such triggers, captures data (for UPDATEs both old and new) and shows reports. When a row is inserted into a table with the following columns: All column values are shown as separate columns 

There are several options you can use: You can use Use Dynamic Management Views (DMVs), such as sys.dm_exec_sessions, sys.dm_os_performance_counters, sys.dm_os_memory_brokers, sys.dm_os_memory_nodes, sys.dm_exec_procedure_stats, sys._dm_os_sys_info, sys.dm_exec_requests, sys.dm_exec_requests, and many more, depending on what you actually want to monitor. As SQL Server doesn't store the performance metric values in an archive table, you can query the views on a schedule and insert the results into a table that you will use as a repository.You cfan query and analyze the records easily then. Another question is - what counters to monitor. For processor usage: Processor : % Processor Time, Processor Queue Length For memory, Available memory bytes, Total server memory, and Target server memory You can find a complete list of counters recommended for monitoring here: Performance Monitor Counters Note that when you use the dm_os_performance_counters view, it's essential to understand the counter_type. There are five different values, and the current value is calculated differently for each value type. You have examples and explanations here: sys.dm_os_performance_counters Another option is to use a third party tool that collects the metrics you want to monitor, stores them in a repository and shows historic data for the time period you select. As the history data is stored in SQL tables, you can also easily query the data yourself and create reports. Such a tool is ApexSQL Monitor, and it has built-in graphs for the last day, week, and month. It will have the reports available soon. 

However, I suggest you check who has the "sa" login privileges and whether these are the right persons. A monitoring tool can be an answer, as it will show you everything that the "sa" login did on your instance. 

Note: this feature will be removed in future SQL Server versions You can fine all arguments available for the utility here: osql Utility 

The transactions are not actually removed after a check point. Their state is only changed to inactive, so they are ignored by the function Even when the transactions are in the inactive parts of the online transaction log, it doesn't mean that they are deleted from the LDF file 

Disk Writes/sec depends on disk specification. For an array system, the values shown are for all disks. So, there is no specific threshold, nor it's limited by SQL Server / Windows server / any other thing. More useful info here: Windows Performance Monitor Disk Counters Explained 

In SQL Server 2012, the Create Audit dialog enables to specify the audit files size and number. You cannot specify the time the times are saved, but the file size and number might be enough to accomplish what you're looking for Maximum rollover files - the number of files kept in the system. When the maximum number is reached, the new files overwrite the oldest ones. The default value is unlimited Maximum files - the number of files kept in the system. When the maximum number is reached, the old files will not be overwritten, and storing new audit information will fail Maximum file size (MB) sets the size of the target file. When the specified size is reached, a new file is created. The default value is unlimited 

This depends on how long you want to keep your old backups. Does your company have a policy for that? Do you move your backups to a storage after a while? If you do, you can specify any number of days for expiry, as long as you copy the backups before the expiry time comes. 

SQL Server Profiler shows all queries executed against a SQL Server instance. The queries can be executed by a user, application, SQL Server itself,etc. To be able to see the queries, a SQL trace must be configured and running: 

It's definitely SQL Azure. You can easily recognize it as the icon is azure, unlike the green one for a regular SQL Server instance You can see the the video by Thomas LaRock here How To Connect to SQL Azure database using SSMS 

Note that the transaction is tied to the Primary key column, not to the column used in the original where clause. Similarly, the undo script will be: 

From MSDN Back Up Database Task (Maintenance Plan) Backup set will expire Specify when the backup set can be overwritten by another backup set. So, there's no deleting of old backups, just overwriting with new ones. 

Besides using the script (which is preferred in most cases), you can do the same in SQL Server Management Studio: 

As steoleary said, SQL Server 2012 Express doesn't support SQL Server Agents Only the following SQL Server 2012 editions support SQL Server Agents: Enterprise, Business Intelligence, Standard and Web See all the features by edition on MSDN: Features Supported by the Editions of SQL Server 2012 

I guess the problem is that the database is in suspect mode, not suspend First of all, it's not recommended to detach it, if you do that, most probably all you're left with is a third party tool, such as ApexSQL Recover that can read the MDF file to recover the table records. Use the Recover from a corrupted database or a detached MDF file option While the database is still attached to SQL Server, you can try the steps suggested by Paul Randal - use the emergency mode, switch to the single user mode, and try with ATTACH_REBUILD_LOG and DBCC CHECKDB You have detailed explanation here: Creating, detaching, re-attaching, and fixing a SUSPECT database EMERGENCY-mode repair: the very, very last resort Disclaimer: I work for ApexSQL as a Support Engineer 

You can find more useful info here: Auditing triggers in SQL Server databases Disclaimer: I work for ApexSQL as a Support engineer 

No These are the dialogs that SQL Server Management Studio shows when connecting to a Database Engine. All you have to do is enter the machine\SQLinstance names (in the example machine is Fujitsu and the instance is SQL2012) 

Just to add that restoring a database from an older version (e.g. SQL Server 2005) to a newer one (e.g. SQL Server 2012), i.e. upgrading is usually smooth. The issues are encountered when restoring SQL Server 2000 and older databases to SQL Server 2012 and newer. Downgrading - restoring a SQL Server 2012 database to a SQL Server 2005 instance is troublesome. 

4.In the Set Scripting Options tab, click Advanced and make sure the Types of data to script option is set to Data only Note: If you select Schema & Data the generated schema script will be identical to the script generated in the first method in this answer. 

Run SQL Server Management Studio In Object Explorer, expand Databases Expand Tables Right click the table and select Script table as | CREATE TO | New Query Editor Window 

SQL Server recovery tools cannot help here, they can only recover lost information from database files (mdf, ldf, ndf, bak, trn). You should search for a way to recover overwritten files in Windows One of the solutions is to use the File History option in Windows. Your Windows version must support it and it had to be enabled at the time when the files were overwritten and "File History saves copies of your files so you can get them back if they are lost or damaged" Restore files or folders using File History 

Yes, they read the LDF file (online or detached) and trn files (transaction log backups), find what transaction has happened, and create a script that will do the same, or the opposite. Note however, that the undo and redo script don't have to be exactly the same as the ones executed, but the effect will be exactly the same. For example, if the executed script was: 

Like Sebastian said, ApexSQL Recover can be used to recover from corruption, but it doesn't seem to be a right solution in this scenario. The option to recover from a corrupted database or an MDF file reads only the online databases and detached MDF files. It doesn't read corrupted database backups There is another option in ApexSQL Recover that reads corrupted backups, that's Recover table data from a database backup. However, note that it recovers only table data and creates an INSERT script, so it cannot be used to move the whole database to a newer SQL Server version There are other ApexSQL tools that can help, as they read database backups - ApexSQL Diff and ApexSQL Data Diff. As the backup is not severely corrupted, they might be able to read it. Both tools read both online databases and database backups, they can also be used if you have an online SQL Server 2005 database that you want to upgrade to SQL Server 2008 R2 ApexSQL Diff is a SQL Server database comparison and synchronization tool which detects differences between database objects in live and versioned databases, backups, snapshots, and script folders. Comparing your live SQL Server 2005 database or database backup to a blank SQL Server 2008 destination creates the SQL objects that exist in the source (i.e. SQL Server 2005 database) Once you recover the database structure, use ApexSQL Data Diff to recover data You can find the recommended steps here: Migrate a SQL Server database to a newer version of SQL Server Create a database script from a backup without restoring it Disclaimer: I work for ApexSQL as a Support Engineer 

Before restoring from the latest full database backup, you can try Paul Randal's advise - set the database to the EMERGENCY mode and use a login that is a member of the sysadmin SQL Server role to access it. Keep in mind that you will be able only to read the data, as the database will be in the read-only mode. Search Engine Q&A #4: Using EMERGENCY mode to access a RECOVERY PENDING or SUSPECT database 

For the stored procedure that has already been deleted, the only option is to dig into the online transaction log or transaction log backups (in case the database is in the full recovery model). The above mentioned fn_dblog provides the requested info only if the transaction is still in the online transaction log. To read transaction log backups, use fn_dump_dblog. Check out Paul Randal's article: Using fn_dblog, fn_dump_dblog, and restoring with STOPBEFOREMARK to an LSN To read the transaction logs (online, detached, and transaction log backups), besides using undocumented functions, you can use a third party tool such as ApexSQL Log. It can show who and when deleted the stored procedure, and will also provide the script to re-create it. 

The lazy writer process is closely related to checkpoints, so I'll start with that first Best SQL Server performance is achieved when pages are read from the buffer. To provide enough free space in the buffer, pages are moved from the buffer to disk. These pages are usually moved at a check point, which can be: 

Yes, this is normal. When you created the database objects and inserted data into tables, the database pages were populated serially, row after row. Understanding Pages and Extents When you use the database and for example delete a table, it doesn't mean that the database MDF file will become smaller for the amount of deleted data. The MDF file will contain the deleted table data, just marked to be overwritten. There is a nice article by Brent Ozar that will help you understand the database files: How Does SQL Server Store Data? 

At a checkpoint, all dirty pages are flushed to disk and the page in the buffer cache is marked for overwriting 

Besides sqlcmd, SQL Server provides the osql utility The same as sqlcmd, osql is stored in the SQL Server's installation Tools\Binn subfolder, and is used from the Command Prompt The syntax is 

3.Select Device and navigate to the .bak file SQL Server will in the Database field automatically insert the name of the original database, read from the restored backup 

It is possible to rollback wrong updates without any downtime using ApexSQL Log. For maximal UPDATE reconstruction, the database should be in the Full recovery model, and a full chain of transaction logs is needed. A full chain of transaction logs starts with a full database backup and is followed by all subsequent transaction log backups up to a point of the rollback. 

There is a huge difference between the records logged in a transaction log file for a DELETE and for TRUNCATE TABLE statement A DELETE statement records what exactly has been deleted, i.e. the value of the deleted row e.g. 'JohnSmith', so you can read the transaction log content (you can use fn_dblog), see what was deleted and re-insert the deleted record if you want. With the TRUNCATE TABLE statement, you cannot do this, as the statement is 'minimally logged'. The exact deleted values are not logged in the transaction log, only the IDs of the pages that held the truncated records are logged. These pages are marked for overwriting in the database data file and the truncated data will be gone for good when the new transactions are written to these pages. Therefore, reading only the transaction log cannot provide the values that are lost due to the TRUNCATE TABLE statement. The only chance you have to recover the truncated records is to read the page ID in the transaction log and find it in the MDF file, in case it hasn't been overwritten 

Right-click the database (not the table!) Open Tasks | Generate Scripts On the Choose Objects tab, select the table to script 

Start SQL Server Profiler On the File menu, select New Trace In the Connect to Server dialog, select the SQL Server instance and provide credentials Click Connect In the General tab, specify the trace name Open the Events Section tab Select the Show All Events check box. Make sure that the event type you want to audit is selected 

There are several methods to audit database transactions, but they all affect performance more or less SQL Server Change Tracking and SQL Server Change Data Capture don't show who, when, and how executed the transactions. On the other hand, SQL Server Change Data Capture shows the old and new values for the UPDATE statements. Here is a useful set of comparison notes: SQL Server 2008 Change Tracking (CT) and Change Data Capture (CDC) and Comparing Change Data Capture and Change Tracking SQL Server Auditing shows who, when, and how, but doesn't show the old and new values for the UPDATE statements The methods that read the database transaction log files don't add overhead, as there is not additional change capturing. Besides fn_dblog, that can return results not easy to understand, there are third party tools, such as ApexSQL Log There are two more auditing tools from ApexSQL - ApexSQL Audit which uses triggers, so can impact performance on a high transaction database, and ApexSQL Comply that uses SQL traces SQL Server database auditing techniques Disclaimer: I work for ApexSQL as a Support Engineer 

In future, you can also use the SQL Server Audit feature, just make sure you have specified all events you want to audit. Disclaimer: I work for ApexSQL as a support engineer 

The transaction log will record that the row in the table with the column values 9, 'New Loc22', '41BC2FF6-F0FC-475F-8EB9-CEC1805AA0F6', and '2002/06/01 00:00:00.000' is deleted. From the table structure, the tool will read that the Primary key is the AddressType column, and will create the following redo script: 

Keep in mind that the frequency of transaction log backups depends on how busy your database is and how much data you can afford to lose. If your databases are quite busy, consider creating transaction log backups every 15 minutes. That will also provide that the maximum potential data loss is less than 15 minutes of data. If you don't create transaction log backups often enough, besides more data you can lose, the online transaction log might grow more than you prefer. Here's the SQL Server Management Studio Backup dialog for a database in the Simple recovery model 

The generated script will be shown in the Query Editor tab. Make sure you change the table, primary and foreign key , and constraint names, as these names have to be unique. Otherwise, you'll get an error message saying something like 'There is already an object named 'Address' in the database.', or 'The operation failed because an index or statistics with name 'AK_Address_rowguid' already exists on table 'Person.Address'.' To create a script for an index Use the steps similar to the above: 

Understanding Logging and Recovery in SQL Server An inactive VLF can and will be overwritten, but not immediately after the checkpoint, so the transactions will be there for a while. They will be gone when new transactions overwrite them. That can be in 5 minutes, or in 2 days. It depends on several factors Please note that 'truncate' doesn't mean 'deleted', just marked so it can be reused, i.e. overwritten.