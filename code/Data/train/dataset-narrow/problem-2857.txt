Step 3 - Knowing how to pass a Matrix to SpriteBatch One problem with the class is that its method doesn't know how to take a world matrix directly. Here's a an helper method to bridge this problem: 

So that's where the real problem lies. You should have been more clear about it instead of giving focus to the dt issue. In that case, you could consider using a fragment shader solution, but I think that the first option will be simpler to implement. In particular, I think it would be a good idea to start by encapsulating the vertex/color generation code in a method that given a simple array of colors, would return the vertex/color arrays required for calling glVertexPointer and glColorPointer. Then use this method to generate each of the gradient sprites, and finally, given the determine which two of these sprites should be interpolated and by what amount. Then draw them on top of each other and interpolate the alpha value so that there's a cross-fade between them. The alternative that would only require one sprite would be to do the gradient on a fragment shader. The first implementation that comes to my mind would be to pass it two 1D textures where each pixel corresponds to one of the colors of the gradient, along with an interpolation value between 0 and 1 to decide how much to fade between texture A and B. Then just render a single quad, and use the information above to calculate the color at each fragment. I'm not sure but you may also need to pass it either the width of the textures or how many colors there are in each of them in order to calculate the correct texture coordinates from which you should sample. There are probably other solutions though. 

Since the red border "eats" into neighbour tiles I draw a 3x3 radius around the last mouse position instead: 

If all you want is for those objects to be unaffected by the camera, why don't you just use a separate view matrix for them (one that never changes)? Simple! And since you probably want them to always appear on front of everything else, don't forget to clear the depth buffer, or use separate render targets. Edit 

If you are copying that list around (i.e. passing in the list by value) then the overhead of copying a list of pointers is the least of your concerns - it probably won't even work as you expect. Each Screen will have its own separate list, and modifications won't propagate to other screens. You would need to share the same list between all screens. There are two simple ways to solve this: 

In my opinion you're asking for information on a topic that is so broad and extensive - a topic for which entire books (or a great part of many books) have been written - that it might be extremely difficult to try and summarize everything in a concise fashion. This is just much more likely the subject of an entire article, chapter, or book, than something that could be easily written out as an answer. I can also tell that your understanding of the topic is very hazy, even on the parts you claim to understand. For instance, most maps that you mention are not just textures that are applied by the graphics cards! In fact they're usually nothing more than data (of any kind) encoded in image format in order to be used in computations further down the pipeline (e.g. in the pixel shader). This data may range from storing the surface normals at each pixel of the scene, to the coefficient of specularity, motion vectors, pretty much anything. So my recommendation would be to take the long trip, and turn to the bibliography. In particular I'd recommend Real-Time Rendering 3rd Edition as the flagship of all graphic programming books in order to provide a real in depth coverage of this and many other related topics. Well, but I guess could try to give a basic overview of the pipeline. But think of it as watching the earth from space. You can get a grasp of the shape of the continents, how there's oceans, etc. But if you need a more detailed view, you'll need to come down and do a lot of exploring on your own. 

Note: XNA's stores ints, not floats, so you'll have to convert between and (and cast the vector components to ints) to use or use a structure such as this instead. 

Since trigonometry functions are somewhat expensive, and using a source rectangle requires the same amount of work as not using one, I think using the sprite sheet approach will probably be a bit faster than rotation, although the difference is probably negligible. 

Watch this video for some ideas that you can add to your system, in particular modifiers, which can be used to interpolate particle properties over time, such as color or opacity, or to apply complex gravitational forces to the system. These are often enough to create some really cool effects, when combined with a basic particle emitter. Afterwards, you might want to check the documentation of some existing particle engine to see what else they provide. The first one that comes to my mind is the Mercury Particle Engine, which also seems to be built around the concept of particle emitters and modifiers. Give it a test drive and it should be enough to answer your question. 

Which gives the following result (using the texture on the right side which I zoomed in for clarity): 

This seems like a big misunderstanding on your part, and I think the comparison doesn't make much sense because in general they serve different purposes: 

Using this scheme you can have any type of slope you want just by varying the tile's collision mask. This is how Sonic games were implemented back in the day, and as such I suggest reading the following page in its entirety for a better understanding: $URL$ Then in order to process collisions between your character and the environment, I suggest defining a few sensors or collision points on your character, such as the magenta dots on the image below: 

Alternatively you could use a cube map with a sky texture instead of using perlin noise. And I also think that will make your scene look better due to having more detail and variation in the environment mapping. That's exactly what I did when I implemented my raytracer. Another reason I recommend this, is that the environment does not need to be only clouds. It can have mountains, it can be an indoor scene, can be a scene from mars. Using a cube map the only thing you need to do is swap the texture and you're done. So the first step will be to find a cube map that fits your needs. It could be stored as six separate textures, or as a single texture like the one below. Also check this link for more info on how to create your own: 

As a complement to Andrew's link, here's a more recent post Shawn wrote on this: $URL$ But I don't recommend it either. That technique might be neat to create billboards and impostors, or placing text within a 3D world. But not to generate the floors and walls of an entire level. If you're thinking about taking your 2D level data and procedurally generating a 3D level, I think you'd be better off writing an algorithm to generate all of the geometry from it (i.e. create and fill vertex index buffers, etc.). 

And if the answer is no, make it abstract. Interfaces Should be used when you have a set of operations that you would like to see supported by several classes, but you want those classes to still be able to inherit (i.e. extend) from other classes. Classes in java can't inherit from multiple classes, but they can implement multiple interfaces. So let's say you had this interface and these two classes (sorry if the syntax is wrong, I'm a bit rusty on Java): 

The result would be that you could then call it by writing or any other valid button name. Not that I would really recommend going this route, but it's good to be aware of the alternatives. And of course it only works if you pass it a valid property name of type , otherwise it will crash. 

I wouldn't call it an elegant solution or anything but at least it succeeds in solving the hovering problem without having to redraw the entire map. It redraws a 3x3 radius around the last mouse position instead, since that's the amount of tiles affected by your hover rectangle. Here's the new fiddle and below are the changes I made to the original one. 

Then on your update method, you move the object by adding a multiplication of , and to its position. After that, to check if the movement is over, you see if the distance between the start point and the object's current position is greater than the initial distance you calculated. If that's true, we snap the object's position to the end point, and stop moving the object: 

It might be true that pixel shaders are used a lot more than vertex shaders, but that's because there are a lot of effects that require that extra control over the color of each fragment that are only possible with pixel shaders, e.g. things such as bump mapping, or any full screen post processing effect. Vertex shaders have different uses, and are used mostly for other types of effects such as creating waves in the ocean, and other types of distortions. As for your performance question, if you can implement the exact same effect, with same visual quality either on vertex shader or pixel shader, then I think the vertex shader version will probably be a lot faster, because the number of vertices will usually (always?) be a lot smaller than the number of pixels covered by the primitive. 

Just this! Then all you have to do is change the value and everything will be panned for you at no performance cost. (By the way, notice the minus sign). Or better yet, since the camera will always be centered on the player: 

I'll use some linear algebra structures since it's easier to describe the operations that way. In case you don't know how to implement these vector operations I'll give a quick explanation at the end. So let's say you start with these values: and mark the end points of the movement, is how many pixels it should move by second, and is the rate at which you'll update your object's position (some engines already provide that value for you): 

The Projection Matrix describes how to perform the mapping between 3D and 2D coordinates. However, this mapping does not happen by applying the matrix - the matrix only prepares the values for the projection. The actual projection is done in a separate step called the homogeneous divide (or perpsective divide) which is basically doing the following: 

This entire series is filled with secrets of the trade from all areas of game development, from programming to graphics, sound, math, AI to networking. They're rather expensive, but I kept an eye open on the used books at amazon and managed to get most of them at a fraction of the price. There's 8 volumes so far, but I find the very first volume to be the most valuable for a beginner since it covers so many things that a game programmer simply needs to know. Check the table of contents of each book online here. And finally, I know you didn't mention graphics but you will probably eventually get interested in it. For graphics my opinion is... Realtime Rendering 3rd Edition LINK 

I decided to give this problem another try today and finally managed to load an OGG file at runtime into a object. Here's what I did! First download the library below which contains a class capable of decoding OGG files: Prerequisite - Download library 

It uses a trick found here to cast to int. I also got rid of those null checks and replaced them with a default value that does not do much harm. Once again, updated fiddle here. 

I think when you create a physics body, the body's position is considered to be at its center of mass - which in this case since it's a rectangle, the center of mass is exactly in the middle of the rectangle. But you're drawing the sprite using the physics body position as if it were the top left corner of the sprite, instead of its center. For starters you should use another overload of Draw, the one that takes a Vector2 position and a Vector2 origin, and set the origin to be . The one you're using is only really useful when you want to directly specify a sprite size. 

As for an Oblique Projection, I think it will be a bit more complicated. I've actually never seen an implementation of this before, so I'll just be going with my intuition. Someone let me know if there's a better, known solution to the problem. You're going to need a vertex shader. Here's the steps: 

This basically means, make your current position replace the first node in the path, instead of appending it like you were doing. Example: 

Context In the game I'm working on (a sort of a point and click graphic adventure), pretty much everything that happens in the game world is controlled by an action manager that is structured a bit like: 

Solution - Complex Version (Faster) Finally, the approach I use on my projects is to use the GPU to do the processing instead. In this method you need to create a render target, set up some blend states properly, and draw the image twice with a SpriteBatch. At the end I go over the entire RenderTarget2D and clone the contents into a separate Texture2D object because the RenderTarget2D is volatile and won't survive things like changing the backbuffer size so it's safer to make a copy. The funny thing is that even with all of this, on my tests this approach performed about 3 times faster than the CPU approach. So it's definitively faster than going over each pixel and calculating the color yourself. The code is a bit long so I placed it in a pastebin: $URL$ Just add that class to your project, and using it as simple as: 

As opposed to what you were doing, which was something like the following code (mirroring the same structure as the code above): 

They don't have a different rotation, that's just the effect of perspective. Use an orthographic projection instead if you don't want it to appear that way. Edit 2 

Beginning C++ Through Game Programming Simple introduction to both C++ and making games. Starts from zero and introduces language features in the context of simple games. A warning though, all examples are text based and focused on the game logic, so there are no graphics covered in the book. Intermediate Level (Knows C++ / Doesn't know how to use it efficiently for games) 

Push this matrix before rendering the cylinder, and pop it afterwards. Draw with backface culling enabled. And if you want to limit the projected shape inside the quad's boundaries, look into how to use the stencil buffer. 

What you're trying to do is simple to solve without having to resort to doing any timing on the input (in fact I recommend you not to do so in this case - timing is more useful for scenarios such as limiting the firing rate of a spaceship). If you want for only the initial press of the key to be registered, the way it's usually done is: 

You could also use the parameter instead of the and let the determine the size. Alternatively do the tiling manually with multiple calls. Basically calculate how many times you need to draw the texture to fill that space, and draw them in the loop. In the case of an horizontal strip like your example, the last draw call will probably need a source rectangle, to crop only the remaining portion that fits that space. You could also probably draw using a special pixel shader effect that calculates a new texture coordinate that wraps around the original source rectangle without stretching, but this might be tricky. 

Edit: Okay, I've just read from one of your comments that you don't want automatic navigation. In that case, only consider the final section in this post ("Walking that Path") with the simple case that the list only stores the place where your mouse clicked and clears itself when an obstacle is found in the way. I also just noticed that you mentioned tiles once in the post. In that case there's no need for a visibility graph, the grid itself can be used to run A-star on. Anyway I'm still posting this as a more general solution to the 2D point and click navigation problem. What you're asking for is how to do pathfinding on a 2D environment. I've written an article before outlining one technique that is capable of solving that problem. I'll start by laying down a link to the article, and then add a brief explanation of the algorithm. $URL$ Of course, this is not the only way to solve it. I'm using a visibility graph. You could also use a navigation mesh. Or a grid. The visibility graph has one particular advantage that it always returns the most direct path between points without needing to do any path straightening. And building the visibility graph on top of a polygon you can specify the walkable regions precisely. Concept The main idea here is to represent your walkable region as a polygon and construct a visibility graph using the polygon's concave vertices. If the polygon contains holes you'll use their convex vertices too. Creating a visibility graph means taking every node (or vertex in this case) of the graph and connecting it to every other vertex that it can "see". You need some line-of-sight check to do this. The one I used is built on top of a simple line segment intersection test, with a few added checks. Then whenever you want to find the path between two locations, you add those temporarily to the visibility graph and just run a classic A* pathfinding algorithm on it. Here's what the whole structure looks like: