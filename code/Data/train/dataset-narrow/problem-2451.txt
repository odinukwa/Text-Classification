Comparing two programming languages is difficult is a difficult problem, and far from being solved. The key issue is that there are many different ways languages can be compared, and none of them is compelling. The most widely used approach, coming from logic, is to consider translations between the languages to be compared. The general idea is as follows: if we have a translation into from $L_1$ into $L_2$, say, then $L_2$ is at least as expressive as $L_1$. Unfortunately, things are not quite that easy. By the Church-Turing thesis you know that you can always find such translations (for Turing-complete languages). So the very existence of translations is not discreminating enough to distinguish programming languages. And yet, everybody who has programmed in different languages (e.g. Assembler vs Scala) knows that some languages are easier to program than others. This conundrum is usually approached by putting restrictions on what counts as a translation functions. The idea here is that the translation functions 'measure' how much 'rearrangement' is necessary when translating from one language to the other: the more 'rearrangement' required, the less expressive the target language. Restrictions on translations control what kind of 'rearrangement' we consider. If we can show that no translation exists that satisfies a bunch of restrictions, then the languages are substantially different in expressivity. In this sense e.g. the $\pi$-calculus is very expressive because we can often find translations meeting various restrictions when translating from a language $L$ into the $\pi$-calculus. Conversely, purely functional languages are not so expressive, because when we translate $L$ into a purely functional language, the translation functions often violate various constraints. Here is a far from exhausting list of popular restrictions for this purpose: 

One way of looking at these two concepts is to say that pattern matching is a feature of programming languages for combining discrimination on constructors and destructing terms (while at the same time selecting and locally naming term fragments) safely, compactly and efficiently. Research on pattern matching typically focusses on implementation efficiency, e.g. on how to minimise the number of comparisons the matching mechanism has to do. In contrast, term rewriting is a general model of computation that investigates a wide range of (potentially non-deterministic) methods of replacing subterms of syntactic expressions (more precisely an element of a term-algebra over some set of variables) with other terms. Research on term rewriting systems is usually about abstract properties of rewriting systems such as confluence, determinism and termination, and more specifically about how such properties are or are not preserved by algebraic operations on rewrite systems, i.e. to what extent these properties are compositional. Clearly there are conceptual overlaps between both, and the distinction is to a degree traditional, rather than technical. A technical difference is that term rewriting happens under arbitrary contexts (i.e. a rule $(l, r)$ induces rewrites $C[l\sigma] \rightarrow C[r\sigma]$ for arbitrary contexts $C[.]$ and substitutions $\sigma$), while pattern matching in modern languages like Haskell, OCaml or Scala provides only for rewriting 'at the top' of a term. This restriction is also, I think, imposed in Jay's pattern calculus. Let me explain what I mean by this restriction. With pattern matching in the OCaml, Haskell, Scala sense, you cannot say something like 

The !-operator also has a process-theoretic interpretation: if $A$ is read as a process, then $!A$ can be read as running infinitely many processes $A$ in parallel. In this reading the axioms $A \vdash A$ of linear logic become simple 'wires' that forward messages from processes $A^{\bot}$ to processes $A$. This interpretation of axioms is already in Girard's proof nets (3). This process-theoretic interpretation has been influential and given rise to a lot of follow-up work like e.g. (2) for session types. Nevertheless, there are some edge cases that make it a bit awkward, and to the best of my knowledge it hasn't been made to work perfectly for full linear logic even in 2017. 

Mutation Testing Repository: Mutation Testing Theory. R. A. DeMillo, R. J. Lipton, F. G. Sayward, Hints on Test Data Selection: Help for the Practicing Programmer. R. G. Hamlet, Testing Programs with the Aid of a Compiler. S. Berghofer, T. Nipkow, Random testing in Isabelle/HOL.. 

Regarding recommendations, that would depend on your background and specific goals. The Milner and Hoare books should be easy to access. After those you can read the original papers. 

The textbook that might be most relevant to your question is Principles of Program Analysis by Nielson, Nielson and Hankin. It does cover dataflow analysis and its relationship to denotational semantics. It does not deal with axiomatic semantics though. 

$\lambda\underline{\omega}$ is the simplest extension of the simply typed $\lambda$-calculus allowing type-level computation. Note that $\lambda\underline{\omega}$ doesn't have parametric polymorphism or type dependency. One way of thinking about $\lambda\underline{\omega}$ is that type-level computation is carried out by having another $\lambda$-calculus, but this time at the type level. You can think of type-level computation as being run at 'compile time'. Why use a typed language to run type-level computation? Why not use the untyped $\lambda$-calculus at the type level? Because the things that could go wrong at the term level with untyped terms (e.g. ill-formed programs like $3 + hello$) could now go wrong at the type level (e.g. $\mathbb{B}\; Pair$). So $\lambda\underline{\omega}$ needs a way of preventing ill-formed type-level computation. But how? Well, let's use the simply typed $\lambda$-calculus again, but now at the type-level, to carry out, and constrain type-level computation? In order to avoid terminological confusion, we speak of kinds of for this second simply typed $\lambda$-calculus. In summary: 

H. Barendregt, Introduction to generalised type systems. H. Barendregt, Lambda Calculi with Types. B. C. Pierce, Types and Programming Languages. 

None of these additional structural congruences does anything 'unusual' with extrusion, so, as Damiano says, whether $M$ substitutes for $x$ in $N$ has nothing to do with restrictions 'on the outside' in $(\nu \vec{n})(\{M/x\} | N)$, but only with $x$ occurring in $N$ or not. 

Assume fixed an untyped programming language $L$ and a well-founded partial-order (or preorder) $\sqsubseteq$ on $L$ programs. We are also given a typing system for $L$. We write $\Gamma \vdash P : \tau$ to indicate that program $P$ has type $\tau$ assuming $P$'s free variables are typed as described in the typing environment $\Gamma$. 

Edit: The question is clearly very context dependent and is subjective. I was hoping that the above five conditions, however, will allow getting some suggestions. I am happy to further modify the question and give more details, if needed to get an answer. Thanks! 

For instance, talking about graph traversal, breadth-first and depth-first traversal satisfy the above two conditions; for shortest-path computations, breadth-first and Dijkstra's algorithm satisfy the above two conditions (on unweighted graphs, of course); etc. Are these also sufficient conditions? More specifically, suppose two algorithms satisfy the necessary conditions to be similar. Would you indeed call them similar, if 

I do not work in theory, but my work requires reading (and understanding) theory papers every once in a while. Once I understand a (set of) results, I discuss these results with people I work with, most of whom do not work in theory as well. During one of such discussions, the following question came up: When does one say that two given algorithms are "similar"? What do I mean by "similar"? Let us say that two algorithms are said to be similar if you can make either of the following claims in a paper without confusing/annoying any reviewer (better definitions welcomed): Claim 1. "Algorithm $A$, which is similar to algorithm $B$, also solves problem $X$" Claim 2. "Our algorithm is similar to Algorithm $C$" Let me make it slightly more specific. Suppose we are working with graph algorithms. First some necessary conditions for the two algorithms to be similar: 

Admittedly, this is less of a research-y question (and admittedly, a little non-algorithmic trivial question), but I am hoping to get some meaningful answer. Given an undirected graph $G = (V, E)$, with $n = |V|$ vertices and $m = |E|$ edges, what is a good upper bound on $\sum_{v \in V} \deg^2(v)$ in terms of $m$ and $n$. Two obvious upper bounds are $O(n^3)$ and $O(m^2)$, which are quite weak for extremely sparse and extremely dense graphs respectively. Is there any reference to find a non-trivial (asymptotic) bound? For example, can we bound it as $O(m^\alpha n^\beta)$ for some $\alpha < 2$ and $\beta < 3$? 

The above implies that given a graph with average degree $\mu$, we can replace it with a graph with maximum degree $\mu + 2$, and build the distance oracle on this new graph instead of the original graph. 

Among many possible notions of the sparsity, I am particularly interested in the one that relies on edge density (or alternatively, average degree). 

There is a three-way trade-off between space, query time and approximation in the distance oracle problem. One can trivially answer each query exactly (approximation = $1$) in $O(1)$ time by storing the all-pair distance matrix; or in $O(m + n\log n)$ time by running a shortest path algorithm. For massive graphs, these two solutions may require prohibitively large space (to store the matrix) or query time (to run a shortest path algorithm). Hence, we allow approximation. For general graphs, the state-of-the-art is the distance oracle of Thorup and Zwick, which for any given approximation $k$, requires optimal space. It also gives you a nice space-approximation trade-off. For sparse graphs, a more general space-approximation-time trade-off can be shown. 

Here is a rough proof to the above claim: We start with a copy $H$ of $G$. We scan the vertices of $H$ one by one. If a vertex $v \in V(H)$ has degree $\leq k$ then we leave it as it is. Otherwise, consider $v$ and the edges coming out of it. We replace $v$ by a list of $\lceil \deg(v)/(k-2) \rceil$ vertices, all of them connected by a new path of edges having weight $0$. Each of these new vertices is assigned at most $k-2$ edges that were adjacent to $v$ (these reassigned edges keep their original weight). We repeat this process till the degrees in the new graph are bounded by $k$. The number of new vertices created during this process to replace a single vertex $v$ is $ \lceil \deg(v) /(k-2) \rceil$. The number of new edges created for $v$ is $ \lceil \deg(v) /(k-2) \rceil - 1$. Clearly, the new graph has $\sum_{v \in V(G)} \lceil \deg(v)/(k-2) \rceil \leq n + \sum_{v \in V(G)} \deg(v) /(k-2) \leq n + 2m/(k-2)$ vertices. The number of new edges added because of $v$ is at most $2m/(k-2)$. In the new graph, we replace every vertex by a path (with all its edges having weight $0$), and kept the original edges. As such, the distance between two vertices in the modified graph is the same as the distance between the corresponding vertices in the original graph. A corollary of the above claim is: 

In their paper Approximate Distance Oracles, Thorup and Zwick showed that for any weighted undirected graph, it is possible to construct a data structure of size $O(k n^{1+1/k})$ that can return a $(2k-1)$-approximate distance between any pair of vertices in the graph. At a fundamental level, this construction achieves a space-approximation trade-off --- one can reduce the space requirements at the cost of a lower "quality" of the solution. 

Has this problem been studied? Does this problem have a name? Are there any known lower and/or upper bounds on $\mu'$? Are there efficient algorithms to construct, if at all possible, such a graph $H$? What if I am also interested in the poly-log factors? Do the answer to any of the above questions change if we consider the special case of unweighted graphs? Clearly, the problem is of interest only if $\mu = \Omega(|V|^{\varepsilon})$ for some $\varepsilon > 0$. Also, note that the problem is extremely trivial if we do not require the second condition. Also, assume that $G$ is a connected graph. 

I am interested in the case of both static and dynamic, weighted and unweighted, undirected and directed graphs. Thanks. 

I am not aware of a general theory being developed on approximation algorithms for problems in P. I know of a particular problem, though, that is called approximate distance oracles: 

they have different asymptotic performance? for a special class of graphs, one algorithm requires $\Omega(n)$ time while the other requires $O(n^{1/3})$ time? they have different terminating conditions? (recall, they are solving the same problem) the pre-processing step is different in the two algorithms? the memory complexity is different in the two algorithms?