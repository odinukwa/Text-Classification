Your EER diagram meets all the business rules you have documented a few additions to your design: a) You need to track actions on a ticket i.e, when it was opened, when a watcher was added, when status changed etc for auditing purposes b) Column naming - you use users_id throught, but each foreign key should be named according to the purpose for example: - issue_tickets - created_by or submitted_by - issue_comments - submitted_by c) table naming - this is just my style, but I recommend naming the tables using singular, issue_ticket etc 

Remember these are just starting points you can fine tune specifics as you move along, but before you fine tune, measure tune then measure again 

I would recommend modeling it as a sale with an adjustment for the trade-in, assuming that as a car dealer you accept trade-ins without a related sale of a car. Therefore within your system you re-use the purchase features, and can later report on metrics like how many sales are linked to trade-ins, what values of sales are linked to trade-ins, whether trade-ins boost sales, etc 

Either approach above will require you to save the coefficient in the rentals table for historical reporting/auditing purposes and also maintain a note on what data was used for computation at that time 

Schedule a) and b) as cron jobs, but remember to removed files which have been loaded so that you do not get duplicated data These are just guidelines and will be made more complicated depending on the formats of the data and of the database into which they are loaded 

Why not add two indexes: a) A two column index for (state, city) - handles queries for state only, then state and city b) An index on city - queries on cities only To tune for performance further you may be able to add lookup tables for state and city then use numeric keys to speed up query performance. 

I have scheduled heavy queries at administrator level, and since they are heavy s with multiple s, consume a huge amount of resources. I do not want them to stop end users from basic tasks when performing such heavy tasks. Is for this purpose? or I should follow another strategy? 

I need to make heavy statistical analysis to deliver some data to users. Currently, I catch the data from mysql and process them via arrays. However, mysql temporary tables are quite better (extremely more efficient and faster than PHP arrays; obviously due to their mechanism). My tentative solution is to form a temporary table upon each request (i.e. connection) to import and process the data. However, I am not sure if there is a drawback for creating many temporary tables simultaneously? Can this make problem for the mysql server? Or I can use it as an alternative to PHP arrays in numerous simultaneous requests? 

In a project, we need to frequently the tables to delete or add columns. I wonder if this weakens the database performance in long term. If it has a considerable negative impact, what is the approach to keep the database healthy and efficient? I was thinking of the following approaches: 

I want to SELECT rows in which there are duplicates against . For the example table, SELECT query should return 

In this typical example, all three columns are . When the fails, I do not know which col caused the error to change its corresponding value. How can I make a query to change to upon duplicate error? 

Since external_id-number couple is unique, it will not update if the new value of number exists. How can I use something like to increase the value of by to reach an available value according to the UNIQUE Constraint and complete the task? 

Must the partition column be part of the primary? Is it recommended one way or the other? Do I have to create an index for the partition key, or does the DBMS do it automatically on its own? 

I've inherited a very volatile table which is a map of who holds what resource in the system. At any given moment, there could be a dozen inserts/deletes/reads going against that table. However, there are never any more than 30-40 rows in the system. The system was written in the SQL 2000 era and the access to the table is serialized via sp_getapplock/sp_releaseapplock system sprocs, so that only 1 request is modifying the table. In addition, the INSERT & DELETE statements execute . Reading the notes from a decade ago, it states that without these restrictions, the system would experience non-stop deadlocks. I've ported the database to SQL Server 2016 Enterprise Edition. Now that the throughput of the system has increased 10 fold, this table is easily the biggest bottleneck. What are my options for a table as volatile as this with SQL 2016? I am looking for fast (hopefully concurrent) access and no deadlocks. 

I have an SQL Server 2014 Enterprise Edition with a lot of data. I would like to mirror the data from that to at least 3-4 servers. The mirrors are all SQL Server 2014 Standard Edition (no more money is available for Enterprise licenses). How do I mirror the data from my main box (with the Enterprise Edition license) to other boxes? I tried the mirroring feature, but it seems that it only allows single mirror. I could you use Always On Availability groups, but that would require that all mirrors also be Enterprise Edition (unless I am reading the docs wrong). At least one of the mirrors needs to be there almost real-time (1-2 minute delay is fine) data replication. The other mirrors could have 1-2 hours delay. So what are my choices? P.S. All the secondary servers are just read only. P.S. The purpose of the mirrored boxes are partially to off-load readonly queries to them. These mirrors need to have near real-time data replication. Another purpose is for analytics, which is a heavy load. Today everything is on the same box and we are forced to do analytics at night so as not to disrupt users and there is just not enough time. P.S. The servers are nearby each other - on the same subnet, connected via a 10Gb link. P.S. Our license also allows a no cost upgrade to SQL Server 2016 when it becomes available. Does that change anything? 

First of all since you are using MySQL, go for InnoDB which allows you to create relationships between the different tables (not dbs) You need three tables: 

Add a movie genres table - as a movie can have one or more genres Split up the movie_showing table as follows: 

MySQL does not properly utilize CPUs with more than 4 cores, so you would go for 4-cores max, and as much RAM as you can get to be able to carry out as much of the processing in memory as possible. 

Do you have an index on account_id? The second problem may be with the nested sub-queries which have terrible performance in 5.0. GROUP BY with a having clause is faster than DISTINCT. What are you trying to do which may be better done through joins in addition to Item #3? 

You can use the "free" MySQL workbench if you are using MySQL $URL$ to reverse engineer the model from an SQL file then re-organize the model into multiple pages. 

I follow the following rules for primary keys: a) Should not have any business meaning - they should be totally independent of the application you are developing, therefore I go for numeric auto generated integers. However if you need additional columns to be unique then create unique indexes to support that b) Should perform in joins - joining to varchars vs integers is about 2x to 3x slower as the length of the primary key grows, so you want to have your keys as integers. Since all computer systems are binary, I suspect its coz the string is changed to binary then compared with the others which is very slow c) Use the smallest data type possible - if you expect your table to have very few columns say 52 US states, then use the smallest type possible maybe a CHAR(2) for the 2 digit code, but I would still go for a tinyint (128) for the column vs a big int which can go up to 2billion Also you will have a challenge with cascading your changes from the primary keys to the other tables if for example the project name changes (which is not uncommon) Go for sequential auto incrementing integers for your primary keys and gain the inbuilt efficiencies that database systems provide with support for changes in the future 

as I want to update another table with these values, and with this row structure, I can update the country table row by row as I get from the above query. 

I want to search for the occurrence of each country in the column. I use multiple subqueries to do so 

I need to build a table accepting one row for each minute. If I set the type of primary key to , 60 records can be added for each minute. Is there any trick to design a table to keep unique? 

and no special features is needed, just creating table and performing SQL queries. or other advanced features are not needed. 

For dropping a table in which a is used in other tables, it is necessary to drop all other tables having connection with the corresponding table. Is there a short way to drop a table, and its all child tables (in which they have to the parent table)? For example, 

Since, make the calculation for the entire column, apparently, I need to reset the value/process of after reaching the maximum item in folder. 

and id is a in 5 child tables. Is it possible to drop TABLE , and force drop of all 5 child tables having ? instead of dropping child tables one by one? 

I use table for calculation of temporary data, but tables in has a big limitation, which made it useless for me. A table cannot be re-opened in a query. This mean I cannot the table with itself, and add sub-query. It is almost impossible to process data without and . Thus, I must give up table. A possible alternative is to create and delete a permanent table on each script run. However, I think it is not good from database administration point of view to and tables. Is there any practical alternative to tables with possibility of re-opening the table? 

I've setup a test SQL Server 2016 server. I've installed 2 instances, restored a backup on the primary. Then restored a backup on the secondary with , then restored the transactional log on the secondary, also with . I then followed the prompts in the Mirroring Wizard off the Database Properties page and ended up getting an error: . What am I missing? 

What does Table Scan (HEAP) mean for a partitioned table? Does it indeed use an index, perhaps behind the scenes? Is there anything I need to do to improve efficiency? 

With SQL Server 2005, you could look at the Task Manager and, at least, get a cursory look at how much memory is allocated to SQL Server. With SQL Server 2008, the Working Set or Commit Size never really goes above 500 MB, even though the SQLServer:Memory Manager/Total Server Memory (KB) perf counter states 16,732,760. Is there a setting where it will actually show the server memory in the Task Manager? Or is it a result of them changing how memory is used in SQL Server 

I have an app that's local to the SQL Server and thus has a Shared Memory connection to it. I was wondering whether the RAM taken by the connection (including data transfers) counts against the max memory limit set for the SQL Server. The reason I am asking is that the SQL Server is maxed out on memory (e.g. Target Server Memory = Total Server Memory and other metrics). If the RAM taken up by Shared Memory connection counts against it, wouldn't I be better off using TCP connection to the SQL Server? 

I have table partitioned on (int). I also created a non-clustered index for on the table. When I run the following: 

Since, it is a temporary table, I cannot use . How can I from this table to have pairs of key/value in each row? 

How we can change two variables in one condition. For example, consider that when , we want to change not only , but also re-assign . In fact, regardless of the condition 2, we want to increase , if the first condition fails (the second part). Is it possible to re-assign two user-variable in a or we need to add another ? This will be equivalent to 

is a big table, and I want to make the only if has found something. In other words, I do not want to make the for . 

How can I reduce the number of subqueries, and count all values in one subquery, then updating all columns? 

is a many-to-many relationship between tables and . In this query, mysql will find all posts tagged by any of these words, and order them by PRIMARY KEY. How can we order the results by tag weight: 

There are many articles exaggerating (IMHO of course) the need for . I understand that with , there should be a better control over the individual tables; like backup each table separately. However, the claim for better performance is questionable. In my test, there is no difference in performance of and for a database of 60GB. Of course, it was a simple test with normal queries, and the situation can be different for complicated queries in real life (this is the reason that I asked this question). 64-bit linux with can effectively handle large files. With , more disk I/O operations are needed; and this is significant in complicated s and constraints. Tablespace is shared on single ; how dedicated tablespaces for separate tables can save disk space? Of course, it is easier to free table space for each table with , but it is still an expensive process (with table lock). QUESTION: Does has an effect on a better performance of mysql? If yes, why?