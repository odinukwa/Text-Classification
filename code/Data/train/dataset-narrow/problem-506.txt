They way that is listed in the other answer given is a mix of the pre-staging way with how to create multiple listeners. Additionally it has improper configuration when it comes to the security and will most likely come back to bite you if you do it that way. 

Yep. If you wanted to do it inside of SQL Server you could write some CLR code, but I wouldn't advise it. 

Adding a listener does not require the AGs to be set to a specific availability mode. AGs can have 0 or more listeners (only one available to be added via the GUI) and be in any availability mode. 

That's the gist of it, if you can't truncate and are growing log that may eat up CPU/Disk time that is wasted or cause an issue with the disk growing too much and eating all available space. Don't laugh (or do!), I have witnessed it multiple times - it really does happen. The other item which isn't discussed is synchronization and redo. Assuming synchronization stays nominal (synchronized) the redo may fall behind. Yes, at some point it might "catch up" but the SLA was already missed. REDO queue build up is a thing in extremely high performant environments and even though SQL Server 2016+ has the potential for parallel redo it doesn't mean it'll always be faster or that the database is using parallel redo. I'm assuming we don't care about automatic failover or failover times (database startup times will be affected). 

Not knowing much else about your problem, if your database is online and you're not using anything that may be actively using part of the log - such as Mirroring, AGs, Replication, etc, you could attempt removing that part of the log from the sweep by changing to the simple recovery mode (if in bulk or full) and issuing a checkpoint. Then go back to your normal recovery model and either take a full or differential to restart a valid LSN chain. This may or may not work for you depending on what the actual underlying cause is, but should work for most cases caused by a state of 6. Please note: I am not advocating that you do anything to compromise your database. If the database is online and functioning, this won't cause any data loss nor would it be destructive other than to your LSN chain which can be bridged. 

Stop trying to use the transaction log for these types of situations. Take any notion you have of using the log for data mining and defenestrate it. 

These all measure different aspects of the AG and where performance might be lacking. For example, we'd want to troubleshoot a large log send queue much differently than a low redo rate. Sure, they may be related but we'd be looking in two different areas at first. It would be best to build in some logic for the alerts. I would still keep the redo alert and add one for blocked redo threads (since that one wasn't mentioned), but I would monitor the full stack as redo rate is just giving you one small piece of the puzzle. 

There is no such thing as a "trusted user or login". Local windows accounts are only known to the local server, thus you could not use webserver\localuser unless the database server was also on the webserver. Since it is not, this is not possible. Since the local user is not in active directory, this will also not work. Thus, no, you cannot use that method. Your best possible scenario for this is to use encryption of the connection string at rest and potentially even SSL connections (though that might be overboard and doesn't make the password kept in the registry and more or less secure, it just encrypts the traffic on the line). 

Yes, and it should. If it didn't then someone deleted the user and re-created it. Users exist inside the database and thus their information is sent to each replica. Logins, however, are server level objects stored in and thus are not part of the AG traffic as it is outside the bounds of the database. That's how this happens (mismatched SIDs). Also, this is only an issue with a SQL Login - as Windows logins obtain their SID from active directory and shouldn't change (note that it can but takes admin intervention and commands). 

This goes back to pre-configuration only replicas for adding into Read-Scale replicas. Again, surprised you didn't run into a few different issues as Read-Scale isn't made for HADR. 

Three may in fact be redundant, or it may not. It completely depends on how your host cluster is setup (physical interfaces and vSwitches) and how the physical network is setup (physical switches and networks). If the host has a few physical interfaces that all go to the same vSwitch, etc., then you'll get isolation at the windows level (more RSS queues, etc.) which can help identify issues or allow for other QoS options but it won't magically give you more throughput. This all assumes you'll be going outside of the same host for networking. 

Quorum is a majority agreement, it is not a resource. It used to be a resource until 2003 where the only option was a disk, but this hasn't been the case since Server 2008. There can only be a single witness, you can't have both a disk and a fileshare. The choice of which to have is going to be based on your favorable scenario during a failure and your internal implementation limits. For example, a company I work with has said absolutely no shared disk to any server, so that rules out a disk as a potential witness. Most organizations want the whatever is considered the optimal side for their business (though, proper planning and implementation there wouldn't be an "optimal" side) to stay running. In most cases we call this the "primary" side. Generally speaking the witness should be closest to the location that should stay up in case of a true split situation when using windows server 2012R2 or lower. If you're using Windows Server 2016 you will want to look into sites. 

You'll notice that the secondary replica is missing The fix is to make the secondary have identical paths for the databases involved. 

I've been seeing this when a configuration only replica (came in CU1) isn't used with Read-Scale AGs that are being used to fail over. Read-Scale wasn't made to fail over and all that jazz, it was made to horizontally scale out read copies for intense read situations (or as a way to replica across Windows/Linux for migrations). I must reiterate, "clusterless" AGs are not made for HADR. If this is part of your use case, use WSFC or Pacemaker (Linux). Info on Configuration Only Replica. 

No, this is the name of a specific version of the filter driver. For example, here is a system with the 2016 one loaded . ReFS is a file system, this is a filter driver that sits between the filesystem and the miniport driver. It's actually quite disconcerting that this is a legacy filter driver as denoted by the .10 at the end of the altitude... hmm. You'll also notice it has quite a low altitude, which is generally not acceptable for 3rd party filter drivers. 

The one without the listener might be working fine... does it still work fine on a failover? Are they using some other DNS alias to "act" like the listener may. The one with the listener, what does their connection string look like? If it isn't using the listener name in the connection string then of course it isn't going to work... they need to point it to the right place, that's why the listener exists! The connection to the listener may also fail if there are multiple subnets, older client libraries are used, or certain keywords aren't in the connection string. If there are multiple subnets, the client driver should be something that supports the MultiSubnetFailover keyword and this should be set to TRUE. 

Install a supported OS for SQL Server 2016 on the old 2008 server. Install SQL Server 2016, then setup log shipping as you normally would per your above quote. 

Most of the corruption is involved in non-clustered indexes. This means that if the underlying table is clean, the indexes can be rebuilt to fix the corruption. Object IDs 1345738380, 1761739862 (this has clustered index corruption as well), 2056692908 It doesn't seem like any PFS/GAM/SGAM/etc system pages were damaged. 

Personally the name of the nodes shouldn't really matter. Yes, I get naming conventions and whatnot, but at some point (just like which node in a cluster owns a specific role) there is diminishing returns. This is one of those times where you're going to put way more effort into changing it than I personally believe you'll get out of it. 

the overall issue with your endeavor, while I commend it, is that getting the log isn't the hard part. Reading the log is, as it isn't documented at all. There are 3rd party products that can help you, but you reading the log and asking these questions isn't going to work out very well. This wasn't written to put you off, merely to express that the log isn't very human readable, documented, or otherwise made for people to go slogging through it. With that, the below should help you: 

That's not quite correct. The listener always points to the primary replica and the main purpose of it is to facilitate transparent failover so that the connection strings do not need to change. One additional use is in read only routing as you've said but the primary use is actually for transparent failover. 

Nothing that is going to give you the "messages" such as the PBM policies does. Looking at the DMVs as you have above are going to give you the best information for what's currently happening now. In the example you have, looking at sys.dm_hadr_database_replica_states will have a synchronization_state_desc column that will tell you what's going on. There are additional DMVs which the script you already posted does look at to help identify what's going on. If you wanted this logged, there is nothing stopping you from creating your own policies and having them run on a schedule - then using an agent alert to give you a feedback mechanism for the PBM policy failures... it will just tell you something happened, though, and won't give you pretty text. The best would be a roll your own monitoring with common problem finding or looking at other monitoring solutions which may give you nice pretty formatted text in the alerts. 

In SQL Server there is the logical structure of your database and the physical. Since you're asking about the physical structure, there are two things at play here: 

Yes, since the same as above applies but in reverse order. Data was encrypted on disk, is being read and transferred in the encrypted state. Then it gets to the instance and loaded into the buffer pool where it is unencrypted as a step on the way. 

This is not accurate at all. Ther, in fact, is GREATER latency is getting the transaction blocks hardened on the asynchronous commit replicas it's just that we don't wait for them to be hardened like we do with synchronous. Thus, there is still a delay (your network doesn't magically become a non-wait environment) it's just not counted in that specific counter as that counter ONLY contains information for synchronous commit replicas and transactions. 

Overall, suppression or inclusion of logging is generally accomplished via trace flags - you are correct. This, however, does not mean that: 

No, there are no changes in SQL Server. There is a change in .NET 4.6.1 that will use logic if the initial connection doesn't happen in the first 500 miliseconds if was not added to the connection string. 

That's dynamic quorum kicking in with a 2 node cluster and no witness. I definitely wouldn't continue to run this way, either add another node or add a witness. 

This means, in order to OPEN the DEK to decrypt the database data you need to open the Server Certificate. To open the Server Certificate you'll need to open the Database Master Key (DMK). To open the DMK you'll need to open the SMK. This is normally done automatically for you as part of automatic key decryption if all levels of the hierarchy are setup properly. Thus, if we attempt to restore the database using TDE to another instance and that instance does not have the server certificate then it will not be able to read the database and cannot open the database. This is why the Server Certificate is required and we can open it properly. Encryption Hierarchy: $URL$