After some more research I discovered that when I create an AG with autoseeding SQL Server creates a new worker (that executes the VDI_CLIENT_WORKER command) for each scheduler. To remove those workers I need remove every 'autoseeded' AG and restart the SQL Server service. If I then create the AG with full backup/restore none of these workers are created. 

On the secondary dm_hadr_database_replica_states is empty and the database doesn't show up in sys.databases. When I look in SSMS the database is shown in 'Availability Databases' Is this a bug in SQL Server 2016 SP1 Standard CU2 or is there something wrong with my T-SQL statement? 

To me it looks like overkill to set processor affinity so a core would be free for Windows. What is your advice about this? 

I've a database on SQL Server 2016 SP1, in the db there is only one table with a Filestream column and a few columns of type . The table is partitioned and every month a new partition is added by scripting. Each filestream partition has its own filegroup, while the other partitions are in the primary filegroup. New data is inserted in the new partition + old data is not be updated. The partition function is created with . I want to hold one year of data (12 partitions) on SSD and archive the rest to slower storage. This will be an automatic process (SQL Job/Scheduled task). 

I need to rebuild some big indexes and I'm doing some tests with the various options (sort_in_tempdb, maxdop, online) of the statement on an test index with 4 levels and 800000 pages on leaf level. I noticed when I'm running the statement with the intermediate pages (level 1) of my index are higher fragmented as before (89% in stead of 3%). The intermediate pages only get defragmented when I'm setting . With the options the level 2 fragmentation jumps from 0 to 100. This are the statements that caused an increase of fragmentation on level 1: 

I'm a newbie regarding the query store and have some problems understanding what I'm seeing. We have a third-party application (running on SQL Server 2016 Enterprise SP1 CU7) that uses the query-hints and . When I do some monitoring in the querystore I see that some queries has multiple plan ids in the plan summary window. Why can a query that uses and have multiple plans (sometimes completely different, sometimes the same)? And the second question, if there are plans that look exactly the same (same physical operators, same set options, same queryhash), how can one plan have a missing index and the second plan not? This are two anonymised plans: Plan 1 Plan 2 

Somebody told me there's a difference in the internal working (I not mean a difference in features or supported resources) of SQL Server Enterprise and SQL Server Standard. One difference I remember was that the Enterprise version uses multiple threads for certain operations and the Standard version only one (I forgot the details). I've searched the web for an overview of the differences (+ some explanation) but I was unable to find such a list. I now wonder is there a difference in the internal working between the two editions? 

I want to setup 2 node master-master replication. Im aware of the point of failiures in this replication. Its a huge database around 1.5TB. Heavy OLTP is going on. But its mandatory to implement right now. I have 2 servers. 

Then I tried to query the actual database, and I can see the primary key value twice. And tried to reindex and got this error. 

I have a PostgreSQL master/slave setup. Today I created a new slave from the existing slave (cascading replication). 

Any suggestions to achieve this? I need to map multiple Linux users. The above one is for who all are using root user. 

Im using Postgresql 9.2. I have disabled auto vacuum in Conf, but still, I can see this auto vacuum is running on few tables every day. Can anyone help me find out why its running? 

Restore collections with different pattern There is no straight forward method for this, but we can create a loop to restore this. create a file with list of collections that we need to restore 

I have some huge tables which have around 10years data, and each table has approx 10000000+ rows. Basically, I want to remove older than 2 years data and make a copy of deleted data in a separate table(I'll export the separate table to CSV and drop it later) I found this great article: Archive huge record I want the same thing, but I don't have an ID column in my tables. So the process should like this, 

There are few simple solutions. Snapshot and restore Take a Snapshot of the old RDS and create your new RDS from this snapshot. $URL$ $URL$ Mydumper Use mydumper to backup and myloader for restore the db. This will be faster than native mysqldump. $URL$ Tune the parameter For Faster restore use this parameter. MySQL any way to import a huge (32 GB) sql dump faster? 

Here somehow my sec_file get deleted so I want to restore latest NDF backup, how can I restore it? Output of restore headeronly 

Keep your databases in SSD will handle more IO in very less time. Keep temp DB in SSD, obviously, it will improve the performance of the queries. We can use SSD as buffer pool (extension). 

I also had the same problem while implementing mirroring to one of my customers, the problem was SQL server Service account for both instances must be a domain name. Once again confirm that. And try this too. 

Basically we don't need any Postgresql server to take the backup. Postgresql client is enough for this. But you must have sudo privilege to install this client. For Ubuntu: 

Im using mysql 5.5 All of my Tables are in innoDB. Today my app went down and the mysql was unable to start. Here are my findings. 

So you can create a view to converting your datetime column to the specific date format. Create table with Datetime 

I need to setup a Highly available Postgresql Cluster. So I found Pacemaker and DRBD with pgpool load balancer are the best options. So the architecture will look like this. 

I've setup an AlwaysOn AG on SQL Server 2016 SP1 Standard. Then I created an AG and added a database with autoseeding (synchronous mode). I used SSMS 2017 to create my AG and to add the database. Everything works fine. But when I check the wait stats I get waits of type VDI_CLIENT_OTHER (80%) on the primary with an average resource time of 42 seconds. After some research I found out that the waits are generated by 4 sessions that execute the command VDI_CLIENT_WORKER. As I understand the wait means that a thread is waiting for work when seeding a new AG. But what I do not understand is why I have those waits because my AG is ready and why do I have 4 sessions that execute the VDI_CLIENT_WORKER command? I found out that each scheduler has one VDI_CLIENT_WORKER Can somebody try to explain what VDI_CLIENT_WORKER-command does and how can I solve the problem of the many VDI_CLIENT_OTHER waits? 

A while ago a consultant said that I should set processor affinity when I'm running SQL Server on VMWare. The advice was to disable CPU0 so it was free for the OS. When I read the "Architecting Microsoft SQL Server on VMware vSphere"-PDF on the site of VMWare I find following: 

I'm trying to setup an AlwaysOn AG with T-SQL commands but my secondary is always in a disconnected state (when I failover, my old primary becomes disconnected). This is not an firewall/network issue, I'm able to send messages from SQL1 to SQL2 with a TCP-sender/receiver. The option New Availability Group... in SMSS also generates the same problem. However when I'm creating an AG with New Availability Group Wizard... it works perfect. This is the T-SQL statement I'm using: 

Is fragmentation on intermediate pages something to worry about and what is causing the increase in fragmentation? 

The second method is twice as fast as the first one. But the disadvantage is that after a few years the archive disk will contain a lof of folders/partitions containing the filestream data + database needs to be offline. As with the first method there is only 1 folder/partitions (or I can split that big partition by year). I hope my explanation is clear enough because it was hard to put all this in writing. What is the best method to accomplish the archiving or am I missing something? 

We have a SQL Server 2016 SP1 CU7 Enterprise server where we enabled the query store on a database. This is a highly used database.The has been set to 200MB and is 1 day. I've used to check the properties of the query store and everything works perfect as long as stays under the 200MB. I've set to Auto but no cleanup happens, normally a cleanup should be triggered when the query store is 90% full. What happens next is that the query store goes into readonly mode because there is no available space left. I've tried changing to Auto but nothings changes.If I then change to 30 days the changes to READ_WRITE and the query store starts capturing again. The just becomes higher than the and after a while the goes back to READ_ONLY. Can anybody explain this behavior? 

This is result of dropping 22 tables... over 770 seconds! This used to be working much faster and then this happened. 

I am wondering what is the maximum setting for hash_area_size parameter? I'm trying to perform a huge join which ends up allocating 10Gb of temp space and completes in an hour. 10Gb is not that much by today's standards. I have the RAM available in the system. Should be possible to process that in memory. Is there a way to tell oracle to do that? When I try to set hash_area_size to 10Gb it complains because it seems to be limited to 32bit integers. I've also tried alter session set "_smm_max_size" = 10000000; -- 8 zeros because I read that it is in Kb, but it doesn't seem to make a difference. UPDATE: 11g Enterprise Edition Release 11.2.0.3.0 - 64bit The query is a join of 3 wide tables of equal size by 4 columns. Each column has about 10M rows. No indexes. The plan is 2 HASH JOINs. Hash joins are performed in TEMP space. 

launch sqlplus and connect to the readdb run the read query, get status 'ACTIVE' run the update query against the writedb in a different sqlplus go back to the sqlplus from step (1), launch the read query from step (2) and get 'ACTIVE' again repeat read query same as in step 4 -- get the correct 'DONE' response. 

I don't think this is even a question, but in case someone has seen something like this before or has any thoughts besides 'file a bug with Oracle' (which I'm doing), I'd love to hear it. 

and I don't understand how to get insight into what this "scheduler" wait class is. It seems like "scheduler" that it refers to is the process that runs scheduled maintenance jobs? But can I see which jobs they are? All I see is normal queries waiting for the "scheduler". The wait even class is "resmgr: cpu quantum" 

We are seeing table drops consistently take tens of seconds on our Oracle 12c RAC. It appears to be spending on its time on reliable message waits. Which I think means that it asks the other node to do something and waits for it to respond... but there is no further visibility into it. Here's the portion of trace which appears relevant to me: 

While under massive write (insert and create table as) load, when Oracle RAC needs to extend tablespace, all write activity gets blocked for however long it takes it to finish extending (tens of minutes), with all sessions blocked by "data file init write". Is there way to configure Oracle to extend the file proactively so that things can keep moving while it is doing it? 

I am trying to investigate some concurrency issues on Oracle 11 RAC. So far the most helpful tool I have is "blocking sessions" view of the Enterprise Manager, which tells me which query is blocking everyone else and what it is waiting for. However, to take advantage of that view, I need to catch the problem as it is happening. So I'm looking for a way to query oracle for historic data which would give me data similar to what "blocking sessions" screen has to offer in Enterprise Manager.