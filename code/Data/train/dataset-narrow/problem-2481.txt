While I don't know the answer to this question, it seems crucial that the phenomenon isn't limited at all to theoretical computer science. I believe SIGGRAPH plays the same sort of role for graphics, NIPS for machine learning, ISCA for architecture, etc. that STOC and FOCS play for theory. Yet it's true that the emphasis on publication in conference proceedings is a striking feature of computer science as a whole, one that isn't shared by any other academic field that I know about. (But maybe there are other such fields?) 

An important 2003 paper by Childs et al. introduced the "conjoined trees problem": a problem admitting an exponential quantum speedup that's unlike just about any other such problem that we know of. In this problem, we're given an exponentially-large graph like the one pictured below, which consists of two complete binary trees of depth n, whose leaves are connected to each other by a random cycle. We're supplied with the label of the ENTRANCE vertex. We're also supplied with an oracle that, given as input the label of any vertex, tells us the labels of its neighbors. Our goal is to find the EXIT vertex (which can easily be recognized, as the only degree-2 vertex in the graph other than the ENTRANCE vertex). We can assume that the labels are long random strings, so that with overwhelming probability, the only way to learn the label of any vertex other than the ENTRANCE vertex is to be given it by the oracle. Childs et al. showed that a quantum walk algorithm is able simply to barrel through this graph, and find the EXIT vertex after poly(n) steps. By contrast, they also showed that any classical randomized algorithm requires exp(n) steps to find the EXIT vertex with high probability. They stated their lower bound as Ω(2n/6), but I believe that a closer examination of their proof yields Ω(2n/2). Intuitively, this is because with overwhelming probability, a random walk on the graph (even a self-avoiding walk, etc.) will get stuck in the vast middle region for an exponential amount of time: any time a walker starts heading toward the EXIT, the much larger number of edges pointing away from the EXIT will act as a "repulsive force" that pushes it back toward the middle. The way they formalized the argument was to show that, until it's visited ~2n/2 vertices, a randomized algorithm hasn't even found any cycles in the graph: the induced subgraph that it's seen so far is just a tree, providing no information whatsoever about where the EXIT vertex might be. I'm interested in pinning down the randomized query complexity of this problem more precisely. My question is this: Can anyone come up with a classical algorithm that finds the EXIT vertex in fewer than ~2n steps---say, in O(2n/2), or O(22n/3)? Alternatively, can anyone give a lower bound better than Ω(2n/2)? (Note that, by the birthday paradox, it's not hard to find cycles in the graph after O(2n/2) steps. The question is whether one can use the cycles to get any clues about where the EXIT vertex is.) If anyone can improve the lower bound past Ω(2n/2), then to my knowledge, this would provide the very first provable example of a black-box problem with an exponential quantum speedup, whose randomized query complexity is greater than √N. (Where N~2n is the problem size.) Update: I've learned from Andrew Childs that, in this note, Fenner and Zhang explicitly improve the randomized lower bound for conjoined trees to Ω(2n/3). If they were willing to accept constant (rather than exponentially-small) success probability, I believe they could improve the bound further, to Ω(2n/2). 

I am not sure what is the appropriate way for this, but I would like to collect different possible definitions/variants of Universal Turing-machines. Here are the ones I know, post below if you know other definitions/comments/don't agree. For simplicity, I assume that we talk about two-tape machines with alphabet {0,1,*}, but let me know if in some definition this might make a difference. I denote by T(x,..,z) the output of Turing-machine T when the inputs on its tapes are x,..,z, if it halts, and "running forever" otherwise. Exists. U is universal if for every T there is a p such that for every x we have U(p,x)=T(x). Computable. U is universal if there is a computable encoding E of Turing-machines (labeled directed graphs) into strings such that for every T and x we have U(E(T),x)=T(x). Fix. Fix a (computable) encoding E of Turing-machines into strings. U is universal if for every T and x we have U(E(T),x)=T(x). Strict. In both the Computable and the Fix definition, we can also demand that for every input that is not of the above format, U does nothing useful (halts without output, outputs the empty string, or runs forever). 

Well, one crucial observation is that the new algorithm apparently only works for groups of the form $Z_{p^k}$ where $p$ is small --- it doesn't give a speedup for groups of the form $Z_p$. The latter is the much more common setting that people talk about, both for cryptography and for Shor's algorithm, and the new algorithm doesn't threaten the quantum speedup there. On the other hand, yes, unless I'm mistaken it does make the speedup much smaller in the $Z_{p^k}$ case. 

Recently, when talking to a physicist, I claimed that in my experience, when a problem that naively seems like it should take exponential time turns out nontrivially to be in P or BPP, an "overarching reason" why the reduction happens can typically be identified---and almost always, that reason belongs to a list of a dozen or fewer "usual suspects" (for example: dynamic programming, linear algebra...). However, that then got me to thinking: can we actually write down a decent list of such reasons? Here's a first, incomplete attempt at one: (0) Mathematical characterization. Problem has a non-obvious "purely-mathematical" characterization that, once known, makes it immediate that you can just do exhaustive search over a list of poly(n) possibilities. Example: graph planarity, for which an O(n6) algorithm follows from Kuratowski's theorem. (As "planar" points out below, this was a bad example: even once you know a combinatorial characterization of planarity, giving a polynomial-time algorithm for it is still quite nontrivial. So, let me substitute a better example here: how about, say, "given an input n written in binary, compute how many colors are needed to color an arbitrary map embedded on a surface with n holes." It's not obvious a priori that this is computable at all (or even finite!). But there's a known formula giving the answer, and once you know the formula, it's trivial to compute in polynomial time. Meanwhile, "reduces to excluded minors / Robertson-Seymour theory" should probably be added as a separate overarching reason why something can be in P.) Anyway, this is specifically not the sort of situation that most interests me. (1) Dynamic programming. Problem can be broken up in a way that enables recursive solution without exponential blowup -- often because the constraints to be satisfied are arranged in a linear or other simple order. "Purely combinatorial"; no algebraic structure needed. Arguably, graph reachability (and hence 2SAT) are special cases. (2) Matroids. Problem has a matroid structure, enabling a greedy algorithm to work. Examples: matching, minimum spanning tree. (3) Linear algebra. Problem can be reduced to solving a linear system, computing a determinant, computing eigenvalues, etc. Arguably, most problems involving "miraculous cancellations," including those solvable by Valiant's matchgate formalism, also fall under the linear-algebraic umbrella. (4) Convexity. Problem can be expressed as some sort of convex optimization. Semidefinite programming, linear programming, and zero-sum games are common (increasingly-)special cases. (5) Polynomial identity testing. Problem can be reduced to checking a polynomial identity, so that the Fundamental Theorem of Algebra leads to an efficient randomized algorithm -- and in some cases, like primality, even a provably-deterministic algorithm. (6) Markov Chain Monte Carlo. Problem can be reduced to sampling from the outcome of a rapidly-mixing walk. (Example: approximately counting perfect matchings.) (7) Euclidean algorithm. GCD, continued fractions... Miscellaneous / Not obvious exactly how to classify: Stable marriage, polynomial factoring, membership problem for permutation groups, various other problems in number theory and group theory, low-dimensional lattice problems... My question is: what are the most important things I've left out? To clarify: 

If toy problems are fine: Let $g\in\mathbb{N}$ and let $H$ be some graph of genus $g+1$. For $\phi$ a CNF-formula, let $G_\phi$ be some encoding of $\phi$ as a planar graph plus a disjoint copy of $H$. Given $G_\phi$, which is a graph of genus $g+1$, it is NP-hard to decide whether $\phi$ is satisfiable. This problem however becomes trivial when restricted to graphs of genus $\leq g$. 

It is known that, for $G$ of bounded treewidth, the Tutte polynomial $T(G;x,y)$ can be evaluated at any $(x,y)$ using $O(n)$ arithmetic operations. If $G$ is connected, then $t(G)=T(G;1,1)$. 

An addition chain for $n \in \mathbb{N}$ is a sequence of natural numbers $$1 = a_0,\ldots,a_l =n$$ such that each $a_t$ is the sum of two previous elements in the sequence. The length of minimal addition chains for natural numbers has been studied intensely. I wonder if "graph addition chains" have been studied, most probably under different names. For a graph $G$, a graph addition chain could be a sequence of graphs $$G_0,\ldots,G_l =G$$ such that $G_0$ is contained in some set $\mathcal{F}$ of elementary graphs, and each $G_t$ is some sum of two previous elements. Depending on $\mathcal{F}$ and the sum operation, one could ask about the length of minimal graph addition chains (if they exist). As an example, set $\mathcal{F} = \{K_{k+1}\}$ and let the sum operation be the $k$-sum of graphs. Graph addition chains with these parameters produce partial $k$-trees, and we ask to construct partial $k$-trees using the minimal number of $k$-sums. For $k=1$, graph addition chains for path graphs correspond to addition chains for natural numbers, making this problem already $NP$-hard. I would be thankful for any references to algorithmic and structural results about this or related concepts. 

This is a known (wonderful) open problem that I've worked on from time to time without success. Avi Wigderson and I mentioned the problem in our algebrization paper, where we raised the question of whether or not containments such as coNP ⊆ IPNP can be proved via algebrizing techniques. (Here IPNP denotes IP with a BPP verifier and a BPPNP prover.) If (as I conjecture) the answer is no, then that would provide a formal reason why any interactive protocol like the one Peter asks for would require non-relativizing techniques that go "fundamentally beyond" the ones used for IP=PSPACE. An analogous question is whether or not BQP = IPBQP, where IPBQP means IP with a BPP verifier and a BQP (quantum polynomial-time) prover. That question is also open---although a recent breakthrough by Broadbent, Fitzsimons, and Kashefi showed that a closely-related statement is true. 

$NEXP^{NP}$ is (probably) bigger than NEXP, as we can ask questions of exponential length from the oracle. That NP in the power is practically a NEXP there, so eg. co-NEXP is contained in $NEXP^{NP}$. 

Ok, so let me try to prove that two is enough, that is $cc(R)\le 2\log_2(pp(R))$. Sorry but sometimes I write leaves instead of number of leaves/pp(R), whenever the number is smaller than 1, I obviously mean this. Also, I usually write < instead of $\le$ to enhance non-tex readability. Indirect suppose that there is an R for which this is not true and let us take the R with the smallest possible pp(R) that violates the inequality. We basically have to show that using two bits, we can halve the number of leaves in all four outcomes of the protocol tree, then we are done using induction. Denote the possible set of inputs of Alice by X and of Bob by Y. Take the center of the protocol tree that achieves pp(R) leaves, i.e. the node deleting which the tree falls into three parts, each having at most 1/2 of the pp(R) leaves, and denote the corresponding inputs by X0 and Y0. Without loss of generality we can suppose that Alice speaks at the center and she tells whether her input belongs to XL or XR, whose disjoint union is X0. Denote the ratio of the leaves to pp(R) in XL $\times$ Y0 by L, in XR $\times$ Y0 by R and in the rest by D. Now we divide the rest into three more parts, similarly to Doerr, denoting the leaves whose rectangle intersect Y0 $\times$ X by A, whose rectangle intersect X0 $\times$ Y by B and the rest by C. Notice that A+B+C=D. Now we know that L+R>1/2, L,R<1/2 and without loss of generality we can suppose that L is at most R. We also know D=A+B+C<1/2. It follows that 2L+A+B<1, from which we know that either L+A<1/2 or L+B<1/2, these will be our two cases. Case L+A<1/2: First Bob tells whether his input belongs to Y0 or not. If not, we have at most D<1/2 leaves left. If it does, then Alice tells whether her input belongs to XR or not. If not, we have at most L+A<1/2 leaves left. If it does, then we have R<1/2 leaves left. Case L+B<1/2: First Alice tells whether her input belongs to XR or not. If it does, then Bob tells whether his belongs to Y0 or not, depending on this we have R or B leaves remaining. If the input of Alice is not in XR, then Alice tells whether her input is in XL or not. If it is, then we have L+B<1/2 leaves remaining. If not, we have at most D<1/2 leaves remaining. In all cases we are done. Let me know what you think. 

To complement what Joe wrote, and maybe explain this question a bit more (without answering it!): The computational complexity of simulating "realistic" quantum field theories has been considered an open problem for a long time. One of the main problems, as I understand it, is that (3+1)-dimensional QFTs aren't sufficiently well-defined mathematically for it to be clear what model of computation should correspond to them. But the situation is different for the (2+1)-dimensional QFTs called topological quantum field theories (TQFTs). For those, there is a rigorous mathematical description based on the Jones polynomial, due to Witten from the 1980s. It's that description that led to the deep and celebrated result of Freedman, Kitaev, Larsen, and Wang, which showed that simulating TQFTs is indeed BQP-complete, as one would hope and expect (see Aharonov, Jones, and Landau for a more computer-scientist-friendly version). This remains essentially the only rigorous result we have about the computational complexity of quantum field theory. Now, the questioner is asking whether some new work by Witten could give a handle on the computational complexity of (3+1)-dimensional QFTs. I don't know the answer to that, but it seems obvious that whatever it is, it would involve a significant research effort, and probably not fit within the margins of CS Theory StackExchange. :-) Addendum (Oct. 12, 2013): I just saw this answer again, and I thought I should add a note that, shortly after I posted it, Jordan, Lee, and Preskill released an important paper showing how to simulate "φ4 theory" (a simple interacting quantum field theory) in quantum polynomial time, in any number of spacetime dimensions. This doesn't directly address the OP's question, but it does render obsolete my comment about Freedman-Kitaev-Larsen-Wang remaining "essentially the only rigorous result we have about the computational complexity of quantum field theory." 

There is nothing wrong with your argument, but there is no contradiction. You prove that from some large enough $N$ the Kolmogorov complexity of the spectrum of $f_n$ is always at least $T$. But this statement is trivially true! Although we cannot prove that the Kolmogorov complexity of one string is large, if we have a sequence, then from some point it must contain only strings of large complexity. So what is this $N$ that you got? It must satisfy $N>g^{-1}(c \cdot BB(T))$, what is a number that we cannot compute (because of $BB$), so there is no problem at all. 

The private coin (one- and two-way) randomized complexity of ANY function is at least log log |size|, so e.g. in your case at least log log {U \choose m}, which would be log log U if m is small, which can give a better lower bound. This result is mentioned in Yao's seminal paper on CC, you can find the proof in my master's thesis, lemma 3.8 and around: $URL$ Of course this is just a lower bound, maybe their is a matching upper bound like m + log log U. 

Let $k\in\mathbb{N}$ and denote by $G_k$ the set of all graphs that can be embedded on a surface of genus $k$ such that all vertices are situated on the outer face. For instance, $G_0$ is the set of outerplanar graphs. Can the treewidth of graphs in $G_k$ be upper bounded by some function of $k$? The other direction obviously does not hold, since constant treewidth does not even imply constant genus: Let $H_n$ be the union of $n$ disjoint copies of $K_{3,3}$. The treewidth of $H_n$ is constant, its genus however is $n$. 

The problem admits an FPTRAS. This is a randomized algorithm $\mathbb{A}$ that gets a graph $G$, a parameter $k\in \mathbb{N}$, and rational numbers $\epsilon >0$ and $\delta \in (0,1)$ as inputs. If $z$ is the number of $k$-vertex sets you are looking for, then $\mathbb{A}$ outputs a number $z'$ such that \begin{equation} \mathbb{P}(z' \in [(1-\epsilon)z,(1+\epsilon)z]) \geq 1-\delta, \end{equation} and it does so in time $f(k)\cdot g(n,\epsilon^{-1},\log \delta^{-1})$, where $f$ is some computable function and $g$ is some polynomial. This follows from Thm. 3.1 in (Jerrum, Meeks 13): Given a property $\Phi$ of graphs, there is an FPTRAS, with the same input as above, that approximates the size of the set \begin{equation} \{S \subseteq V(G) \mid |S| =k \wedge \Phi(G[S])\}, \end{equation} provided that $\Phi$ is computable, monotone, and all of its edge-minimal graphs have bounded treewidth. All three conditions hold if $\Phi$ is the graph property of admitting a perfect matching. 

What happens if we define ${\bf PPAD}$ such that instead of a polytime Turing-machine/polysize circuit, a logspace Turing-machine or an ${\bf AC^0}$ circuit encodes the problem? Recently giving faster algorithms for Circuit satisfiability for small circuits turned out to be important, so I wonder what happens to rectricted versions of ${\bf PPAD}$. 

Please see the comments below by Emil Jeřábek, so I am not that sure anymore that the problem is harder. No, it is not known but it is harder than PPP :) Here I focus on the $M=2N+1$ case, so $t=3$, that is, we want $3$ inputs that map to the same output. In Papadimitriou's seminal paper "On the Complexity of the Parity Argument" he defined PPA-3 similarly to PPA as the search problems reducible to the argument "If in a bipartite graph a node has degree not a multiple of 3, then there is at least another such node." Let me define here PPAD-3 in a way similar to PPAD as the search problems reducible to the argument "If in a balanced, directed bipartite graph $(A,B;E)$ there is a node in $A$ whose outdegree is not $2$ or whose indegree is not $1$, then there is at least another such node in $A$ OR there is a node in $B$ whose outdegree is not $1$ or whose indegree is not $2$." Just like PPAD$\subset$PPP, you can show that PPAD-3 is a subset of the problem you have defined. Unfortunately this is all the evidence I can give you as (as far as I know) PPA-3 has not been investigated much and no oracle separation results are known for it, though I don't think it would be hard to obtain some. Papadimitriou defining it as a separate class should be enough evidence that there is a good chance that they differ.