Say that a node of a circuit is small if it has fan-in at most 2 and large if it has fan-in greater than 2. The weft of a circuit is the maximum number large nodes in any path from an input node to an output node. Let $C_{t,d}$ be the class of circuits of weft at most $t$ and depth at most $d$. The notion of weft is used fundamentally in parameterized complexity theory to define the W hierarchy. Namely, a parameterized problem P belongs to $W[t]$ if there is a parameterized reduction from P to $WCS[C_{t,d}]$ for some $d>1$, where $WCS[C_{t,d}]$ is the problem of determining whether a circuit in $C_{t,d}$ has a satisfying assignment of Hamming weigth exactly $k$. I'm interested in circuits in which only OR gates are allowed to be large. More precisely, say that a circuit $C$ has OR-weft at most $t$ if the following conditions are satisfied. 

All negation gates are in the bottom layer. $C$ has weft at most $t$. All AND gates have fan-in $2$. 

For every linear objective function, $(E,\mathcal{F})$ has an optimal basis. $(E,\mathcal{F})$ is a matroid embedding. For every linear objective function, the greedy bases of $(E,\mathcal{F})$ are exactly its optimal bases. 

They show that for linear objective functions (where the objective value is the sum of element weights), the greedy algorithm will work exactly on the structure they define as a matroid embedding; They give a similar characterization for so-called bottleneck objectives (where the objective value of a set is equal to the minimum over the individual element weights); and They give an exact characterization of which objective functions (beyond linear ones) are optimized by the greedy algorithm on matroid embeddings. 

Actually, the complete and general description of a problem that can be solved by a greedy algorithm is a matroid embedding, which generalizes both the concept of a matroid and that of a greedoid. The answer is no—a problem solvable by a greedy algorithm need not have a matroid structure, but it will have the structure of a matroid embedding (which is, alas, much more complicated). A mental model for some of this could be finding minimum spanning trees. The structure used by Kruskal's algorithm is a matroid, but that used by Prim's algorithm (which requires a start node) is not. (It is, however, a greedoid—and a matroid embedding.) Helman et al. (1993), in their paper An Exact Characterization of Greedy Structures define their notion of a greedy algorithm in terms of set systems, which is the same formalism that is used for matroids and greedoids. A set system $(S,\mathcal{C})$ consists of a set $S$ and a collection $\mathcal{C}$ of subsets of $S$, the so-called feasible sets. A basis for the set system is a maximal feasible set, that is, a set that is feasible but not contained in any other feasible set. An objective function $f:2^S\rightarrow\mathbb{R}$ associates each subset of $S$ with a value. An optimization problem, in this formalism, consists in finding a basis of maximum objective value for a given set system and objective function. The greedy algorithm, defined in terms of this formalism, is quite simple: You start with the empty set, and successively add a single element until you reach a basis, always ensuring that (i) your set is feasible at each step, and (ii) the element you add maximizes the objective function of the resulting result, wrt. all the alternative elements you could have added. (That is, conceptually, you try adding all feasible alternatives, and choose the one yielding the highest objective value.) You could, perhaps, argue that there might be other forms of greedy algorithm, but there are several textbooks on algorithms and combinatorial optimization that describe this set-system based algorithm as the greedy algorithm. That doesn't prevent you from describing something that doesn't fit, but could still be called greedy, I suppose. (Still, this does cover anything that could potentially have a matroid structure, for example, though it is much more general.) What Helman et al. do is that they describe when this algorithm will work. More specifically: 

A famous theorem due to Impagliazzo and Wigderson states that if some function in $E=DTIME[2^{O(n)}]$ requires circuits of size $2^{\Omega(n)}$ then P=BPP. When can we change $P$ with some complexity class $C$ such that the same result works? In other words, for which complexity classes the following statement is true? Statement: If some function in $E$ requires circuits of size $2^{\Omega(n)}$ then $C= BP-C$. Here $BP-C$ is the bounded probabilistic version of $C$. In particular does it work for formulas? I.e. for $C$ equal to logspace uniform $NC_1$ ? In this case the statement would be: If some function in E requires formulas of size $2^{\Omega(n)}$ then $NC_1 = BP-NC_1$ 

What is the simplest machine model accepting the following language? $$ L = \{(w\# )^{k}\;|\; w\in \Sigma^{*},\;k\in \mathbb{N}\}$$ In other words, $L$ is obtained by taking each string $w$ in $\Sigma^*$, and then creating the strings $w\#\;,\; w\#w\#\;,\; w\#w\#w\#\;,...$ Here $\#$ is a separator symbol which is not in $\Sigma$. The simplest I could think of was queue automata with $2$-queues. But these are already Turing complete. Is there some well studied class of machines that is not Turing complete and accepts $L$? 

In the first edition of Introduction to Algorithms (Cormen et al., MIT Press, 1990), the discussion of parallel algorithms is based on the PRAM model. In the second edition, paralellism has been eliminated, but in the third edition (Cormen et al., MIT Press, 2009), the topic is reintroduced, but with a dynamic threading model (based on Cilk). The chapters are very different, for sure, and the models seem to be, as well, at least superficially. But I'm wondering: What are the differences in the underlying computational model or abstract machine here? Their underlying model is still a shared-memory RAM machine with multiple processors. How is this different from the PRAM? Is it the case, perhaps, that they are in fact using the same underlying model, but approaching it differently? The threading is certainly handled differently in the classic PRAM algorithms – more in line with static threading, where you manually schedule which threads/processes are to run on which processors, rather than simply express concurrency/potential parallelism and have some automatic scheduler use the processors available. But still: Are there more fundamental differences? In their chapter notes (3rd ed., Chapter 27), Cormen et al. write, “Prior editions of this book included material on […] the PRAM (Parallel Random Access Machine) model.” This seems to indicate that they do not view their dynamic multithreading as being built on this model. Is this so? If so, what differences am I missing? 

Group isomorphism can be solved in time $n^{O(\log n)}$ Graph isomorphism can be solved in time $n^{\log^{O(1)} n}$ Isomorphism of linear codes can be solved in time $2^{O(n)}$ ... 

Let $p_1,...,p_n$ be a list of numbers, each specified by $n^{O(1)}$ bits. Let $\mu = \sum_{i} p_i$ be the sum of all numbers in the list. I want to sample from the set $\{1,...,n\}$ where each $j$ is drawn with probability $p_j/\mu$. I'm looking for a reference where the complexity of this problem is analyzed explicitly taking the bit size of the numbers into consideration? I was only able to find references where the numbers $p_j$ are treated as ideal real numbers. In this case, on a turing machine operating over the reals, the complexity seems to be $O(n)$ pre-processing time and $O(\log n)$ per sample. 

Does sampling functions from $X$ uniformly at random in time $n^{O(1)}$, implies the existence of pseudorandom generators of small seed-length that fool all functions in $X$? I remember of having read somewhere that the natural proofs method does not encompass Razborov lower bounds for monotone Boolean circuits because one does not know how to sample n-bit monotone Boolean functions efficiently. Why would such a sampling algorithm imply lower bounds based on cryptographic assumptions? 

If you don't use the flow per se, but use the Ford-Fulkerson algorithm (or some version, like Edmonds-Karp), you can get both the max-flow and the min-cut directly as a result. When looking for augmenting paths, you do a traversal, in which you use some form of queue of as-yet-unvisited nodes (in the Edmonds-Karp version, you use BFS, which means a FIFO queue). In the last iteration, you can't reach $t$ from $s$ (this is the termination criterion, after all). At this point, the set of nodes you reached forms the $s$-part of the cut, while the nodes you didn't reach form the $t$-part. The leaf nodes of your traversal tree form the “fringe” of the $s$-part, while the nodes in your traversal queue form the fringe of the $t$-part, and what you want is the set of edges from the $s$-fringe to the $t$-fringe. This can also easily be maintained during traversal: Just add an edge to the cut when it is examined, and leads to an unvisited node, and remove it if it is traversed (so its target becomes visited). Then, once Ford-Fulkerson is finished, you'll have your min-cut (or, rather, one of them) right there. The running time will be (asymptotically) identical to Ford-Fulkerson (or Edmonds-Karp or whatever version you're using), which should give you what you were looking for. 

Where for each $n$ we assume that an isomorphism is a permutation of the set $\{1,...,n\}$. Is there a well studied variant of isomorphism problem that is known to be solvable in time $2^{O(n\log n)}$ but not in time $2^{O(n)}$? Obs: Note that $n!\cdot n^{O(1)} = 2^{O(n\log n)}$ is roughly the time necessary to test all permutations. 

Obs: Note that sampling uniformly from X is not the same as sampling uniformly from the circuits representing function from X, since many circuits may represent to the same function. 

It is well known that each resolution refutation $\Pi$ for an unsatisfiable CNF formula $F = C_1\wedge C_2 \wedge ... \wedge C_m$ over variables $X$ can be translated in polynomial time (in the size of $\Pi$) into a deterministic branching program $P$ solving the following search problem: 1) $P$ has one source node and one sink node for each clause $C_i$. 2) For each assingment $\alpha:X\rightarrow \{0,1\}$ there is a consistent path in $P$ from the source node to some sink node associated with a clause that is falsified by $\alpha$. Question: Is there a proof system strictly stronger than resolution where each proof $\Pi$ can be translated in polynomial time (in the size of $\Pi$) into a not necessarily deterministic branching program $P$ solving the search problem above? 

I’m sure there are several ways of dealing with this, but one I’ve come up with for my current research code lets you do 1., 3. and 4. in constant time with a really small memory overhead. The structure assumes that every member is represented by an integer $0\ldots n-1$, and that you have two tables of size $n$ that can accomodate such integers; let's call these $\pi$ and $\pi^{-1}$. You can then use these to represent a permutation of the members — as well as the inverse permutation. Basically, the inverse permutation $\pi^{-1}$ is simply an array of members (answering the question “Which member is in position $k$?”), while the permutation ($\pi$) gives you the location of any given member. In addition, you store the number of remanining members, $m$. Initially, you need to fill these two arrays so that $\pi = \pi^{-1} = \langle 0, 1, 2, \ldots, n-1\rangle$, as well as set $m=n$. To reset it, though, you only need to set $m=n$ (constant time). Adding and removing elements only requires you to swap the given member into the position just inside/outside the “cutoff point” given by $m$ (updating both $\pi$ and $\pi^{-1}$), and then to increment or decrement $m$, as needed. To make best use of this setup, you would only have to compare $m$ to size of the neighbor array of the current node in your BFS. Iterate over whichever is smaller, and do the lookups in the other one. 

Has some notion similar to the OR-Weft hierarchy been studied in parameterized complexity theory? What kind of functions can be computed by circuits of constant OR-weft? 

Let $X$ be a set of $n$-bit Boolean functions of the form $f:\{0,1\}^n\rightarrow \{0,1\}$. For instance, $X$ could be the set of $n$-bit monotone Boolean functions, or the set of $n$-bit functions computable by circuits of size $s$, or the set of n-bit Boolean functions computable by branching programs of width $w$, etc. What are the implications of an efficient algorithm that samples a function from $X$ uniformly at random? Examples of concrete questions are the following. 

Let $D_{t,d}$ be the class of circuits of depth at most $d$ and OR-weft at most $t$. Say that a problem $P$ belongs to OR-$W[t]$ if there is a parameterized reduction from $P$ to $WCS[D_{t,d}]$ for some $d$. Questions: 

Let $X={x_1,...,x_n}$ be a set of variables and $\pi:[n]\rightarrow [n]$ be a permutation of the $n$-element set $[n]=\{1,...,n\}$. A $\pi$-OBDD is an oblivious, read-once branching program where the variables are tested in the order $x_{\pi(1)}x_{\pi(2)}...x_{\pi(n)}$. The width of an OBDD $F$ is the maximum number of nodes in a layer of $F$. Let $c$ be a constant. Given a $\pi_1$-OBDD $F_1$ and a $\pi_2$-OBDD $F_2$, both of width at most $c$, where $\pi_1$ and $\pi_2$ are possibly distinct permutations, what is the complexity of determining whether $F_1$ and $F_2$ represent the same function? Obs: If $\pi_1 = \pi_2$ then testing whether $F_1$ and $F_2$ represent the same function can be done in polynomial time via classic automata-theoretic techniques.