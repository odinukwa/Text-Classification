I'd like to draw a large number (multiple thousands) of simple meshes (each maybe... a maximum of 50 triangles, but even that is a very large upper bound), all of which are exactly the same. As a simple brute force way, right now I'm just doing thousands of draw calls, where I just change the transformation matrices that the shaders need and then draw the same objects again. Of course this is hideously slow, since (I'm guessing) there are too many draw calls for the driver and my PC to handle efficiently. What can I do to make it faster? 

As you can see (from the shadow outlines at the top right), the scene is lit by a directional light source somewhere above the scene to the top left. And since there is only one cell separating the outside of the atrium and the inside, the light bleeds through and the wall to the left is incorrectly illuminated. Actual question The author suggests a form of manual anisotropic filtering to fix this. He gives a radiance gradient (I'm assuming of the SH coefficients sampled from the current cell) towards direction of the surface normal n as: 

Before the collision detection happens, save the players velocity to some temporary variable. After you've done your collision response, check if the players X or Y position has been corrected If X position has been changed, manually reset (as a kind of "safety reset") the players Y velocity to the one that he had before the response. 

So, I want it to be very easy to create all the entities of my game and for other people to come in and do the same. I was thinking I could just let the users/myself create an XML sheet the stores all the properties of each block (Like a Terraria or Minecraft voxel) and add Lua scripts that are referenced in the XML for additional functionality of any of the blocks. I'm starting to think It would just be easier to let the user create a JAR file full of classes for each block. And then that JAR file could easily be used to get all the blocks. It'd just be interesting to reference all the blocks by a block id without storing all the classes by ID. Or I could give each class a static id. But that's not important. Okay, so my short question is what are the pros and cons of storing all the the different types of blocks as classes versus in an XML sheet with Lua for additional functionality? UPDATE: I will probably go for the only use Lua approach. I found about it just recently and it appears like it could be the best method as of right now for me! 

I'm about to begin on the engine for the game I am making. However, I am quite uncertain how I should design it. I have heard lots about threaded games, component and asynchronous systems. I'm wondering which design will give the best performance using the LWJGL with a 2D game. I know that how I build it will greatly impact the performance. I just want to know what direction I should go in for the engine based on what has worked for others. 

OK been googling nonstop for ages... My problem is, i need transparent models in my game. After a lot of annoying fiddling around with my code, i changed the blendstate to alphablend. This allowed me to start having transparency in my models. However now i face another problem. Any other part of the same model that is meant to be view THROUGH the transparent part of the model, is not shown. Basically it is only showing the outermost part of the model, even though an inner part is meant to be seen through the transparent part. Any help? Thanks 

Diverse terrain can be created using various arithmetic adjustments of randomly generated numbers. As I'm sure you realize, adjusting your generated values (for example, multiplying by two) would change the result you have. If you are familiar with programs such as Photoshop or Paint.net you've probably heard of the concept behind multiplying colors, or burning, adding, etc. These are pretty simple concepts which wouldn't take much googling at all to find code for. If you had a variety of heightmaps, using these effects you could start creating pretty diverse terrain. So where am I heading with this? Expand your skills with random generation. Change your code so that it merely generates a single canyon throughout a flat terrain (or 'flat' around your planet). Try using diamond square algorithms and mucking around with the variations to create extremely mountainous terrain. By creating all sorts of effects, you're essentially creating a library of algorithms which you can now mix and mash to get your final product. 'But these are still all the same effect around the entire planet!' Well this is where those image effect concepts come back in. Say for example, I am making a terrain out of two perlin noise maps and a diamond square generation. For a single vertex, I generate the height as usual for each map. This may give me the values p1, p2 and ds1. In your current implementation, you are straight off using p1 as the height. What I am suggesting, is you use something like (p1 * p2) + (ds1 * (1 - p2)) as the height. Just to clarify, the 'lower' the second perlin noise point is, the more the vertex is based off the first perlin noise map. the 'higher' the second perlin noise point is, the more the vertex is based off the mountains diamond square calculation. Given the nation of perlin noise (especially when scaled up a lot), this will create effects where some patches are mountainous, and then that dies out to be the hilly nature of perlin noise in other areas. Of course this is an extremely basic example for the potential behind the idea. TLDR; mash multiple terrain generations together, not necessarily simply with averages or additions. Multiply some generations by others, then add them to a third and then multiply by a fourth! Think photoshop layer effects 

What techniques should someone look at when getting into real time global illumination (for dynamic lighting scenarios)? It's hard to get a real grip on the different mentioned techniques without knowing anything about the subject. If I wanted to incorporate GI into my graphics engine, where should I start reading as a baseline? Which algorithms are implementable in a reasonable timeframe by someone new (i.e. not a highly specialized algorithm that gains minor performance for months of work) and which are not completely outdated by now (i.e. straight up replaced by something simply prettier and faster)? Photon mapping seems to come up a lot, and I've read about things like voxel cone tracing and more specific algorithms, e.g. this technique used in CryEngine 3. However, knowing next to nothing about the field it's hard to decide where to start. What general techniques are worth looking into? 

This seems really basic, but I'm confused and can't find a clear answer anywhere. What happens to a target fragment of a rendertarget if the fragment is written to multiple times within one pass (e.g. by drawing two identical triangles above each other), assuming depth and stencil testing is disabled? Does this just use the behavior specified with the current blend state? If not, can I modify this behavior? 

etc. From a superficial glance, Lua, AngelScript etc. seem to be a little bit bloated for my simple needs (Although I must admit I haven't put tons of time into those two). Can you recommend something better? 

which disables a second shot from firing, as your game so far only works with one bullet (without this code, the 'old' bullet would disappear and the 'new' bullet would appear). Instead, try implementing an array to hold new bullets, instead of limiting yourself to only one. (Though you may want to implement a cooldown on firing, otherwise space could be held for an extremely rapid fire) EDIT: Quite a few changes to the code had to be made... As i havn't used javascript or HTML5 before, i couldn't really test all of this, though it should be fine. I commented out lines of code instead of deleting them, and tried to point out where i modified stuff. 

For a section of my game, instead of having a perspective projection, i would like to display an isometric view of an object. So i don't want things to appear smaller if in a distance, etc. Basically a proper isometric image. Is there any way to do this in XNA? 

Ended up using the example on the site. Realized i was using it wrong and should have used it more like a library. 

DisplayBox.SetValue is a function which goes through and sets the texture. This is working! DrawIsoCube() draws the cube (the test model) and this is working too. The problem is definitely something to do with the unreusability of the render target. Pretty simple yet it's not working. It appears that whatever the last image being rendered is, is shown in every image of the allBlockIso array. Anyone know a fix? Cheers. P.S. sorry if this wan't explained well. I'm not very good at explaining :P 

Whenever I try to use textures, I have to at least specify the sample filtering parameters (GL_TEXTURE_MAG_FILTER, GL_TEXTURE_MIN_FILTER) for textures to work at all. If I don't, sampling the textures in a shader will usually just return a value of 0 for all texels in the texture, even though glReadPixels return the correct values, and rendering to the textures actually works. Debugging tools like Nsight and gDebugger confirm this. I tested this on two NVIDIA GPUs (GTX 970, GT 555m) with the latest driver versions (347.09), and the behavior is the same. However, I can't find any reference to this in docs like this or this I assumed that not specifying these parameters just made them take on their default values. So is this actually intended standard behavior (probably meaning "undefined behavior") or can I assume this is a driver bug? 

Such a Sponza scene doesn't exist. Seems like all demos just use the basic Crytek Sponza model, some with the big flag in the middle removed. 

Simple question: In GLSL, is there a way to share functions across multiple shaders, or do I have to define all functions in every shader that needs them? 

The Gamedev.net Help Wanted forums is great place to search. When I've searched for artists in the past (paid and unpaid), I've usually received replies within a couple of hours. 

Are there strategies to minimize depth buffer precision problems with hyperbolic depth buffers, such as the ones resulting from perspective projection matrices, or depth buffers in general? For example, graphics APIs usually give an option to change the depth range, which might influence precision. It's possible to linearize non-linear depth buffers, for whatever reason. There's the option of floating point depth buffers, and non-floating point depth buffers. It's possible that changing the information in projection matrices has a result on the resulting range & precision of the depth buffer. How do all of these things interact with the resulting range & precision, or with each other, and how do I get the maximum out of my depth buffer? Are there general good practices one should adopt, regardless of project specifics? 

So, I'm not really sure where to start with this question. Feel free to tell me I'm stupid and Off Topic but I'd also like you to tell me why. So, the maps in the game I am designing will consist of lots and lots and lots of voxels. Most of the voxels will just sit there and some of them will have methods run when they are touched or something happens around them. However, some blocks will constantly being doing something. Now, I'm not sure if I should have a array in every chunk (Group of voxels) that contains all the 'active' blocks or if there is another way. I know that some voxels will run code when they are loaded that makeup for all the time that they hadn't been doing anything but I may need to constantly run some voxels. (Machine that overheats or powers something can't just be loaded in 20 minutes after it blew up; or can it?) So, now that that basic background is over, here is my question: Will an array storing all active entities in an area suffice as a way to run all the frame by frame methods of the blocks? Or is there a "normal" way to do this that I am unaware of? EDIT: I just realized that the client that does rendering will not do a majority of the operations for the game engine. I will have a server that does it. The answer to this question is still largely relevant and helpful though. I just thought I'd mention my change. 

Apart from the superior code-design, it's also a lot faster than the current version of SDL and has more features (if you use the basic SDL libs). I can really recommend it if you're using C++. Also, if you're going to pick SFML, start with SFML2 instead of 1.6. It isn't officially released yet, but it is going to be very soon and not a lot will change in the API until then. The latest SVN snapshots of SFML are always available on the download page. 

and call them like this (never mind attribute access, I just made everything public for debugging comfort, will switch to private soon): 

While loading a model I get a big array of 32 bit floating point texture coordinates. I just upload that to a single vertex buffer (along with other data, such as vertices, normals, etc.) and use it while rendering via 

Where exactly does the cost from switching textures come from? Would it be a good strategy to just fill all available texture slots from 0 to GL_MAX_IMAGE_TEXTURE_UNITS - 1 using glActiveTexture and glBindTexture to avoid the amount of state changes? Or is it better to minimize the amount of texture slots that are used at the same time, and rely on sorting by material to keep texture changes for each used slot as low as possible? Also, seeing as bindless textures are available in OpenGL 4.4+, can it be assumed that they perform better in general, i.e. should I prefer changing the uniform value to point to a new texture over re-binding a texture to a certain texture slot? 

I am trying to have an explosion appear when a player lands on a mine. I checked out the particle example on the XNA website but it seemed to over complicate it a lot. So any simpler neater explosion particle effect would be cool to be linked to ;) But on to the problem, i can't even see the explosion though i cannot see what is going wrong. Here's my explosion class (a lot of this is from the particle example): 

I decided it was time to stick in a bit of multiplayer into my game, but came across my first hurdle when i quickly downloaded an example from the XNA website. It prompted me with a login to live. However i really DO NOT want my game to use live accounts. This is PC btw. Basically i wanted to know, is it possible to have multiplayer with XNA without logging in through a live account? Also, is it better to stick with the built in XNA networking, or use a totally different library such as lidgren? 

For balancing mechanics, Grab some friends, and do it manually, the fun way. If your game is good i doubt they would disagree, just spend a weekend playing your MMO for a bit. This should help get the basics out of things like whether bleeding needs a nurf or buff, whether being crippled needs to slow down a character more or less, etc. For balancing moves and such, try a beta, open or closed and give players the chance to give feedback. Given the fact that your balancing is for the players, maybe players should have a say in the balancing? I know this isn't about documentation, but it is still THE BEST option out. Or i could save my answer by saying "Documents from players beat general guideline documents".