Yes, here (this was a class project; beyond that I know almost nothing about that code). The author of that code has a nice writeup of the data structure as well. Some more recent academic work on that data structure has experimental results. You may try looking through those as well (though of course, many are variations on the original concept). 

Related: Rengo Kriegspiel, a blindfolded, team variant of Go, is conjectured to be undecidable. $URL$ Robert Hearn's thesis (and the corresponding book with Erik Demaine) discuss this problem. They prove other problems undecidable through "TEAM COMPUTATION GAME", which is reduced directly from Turing machine acceptance on empty input (see Theorem 24 on page 70 of the thesis). So it seems to me that such a reduction would imply Rengo Kriegspiel is Turing complete. On the other hand, their discussion says that this reduction would be very difficult (see page 123). So while this is a potential avenue, it appears that it has been looked into previously. 

Traveling salesman is open on sold grid graphs, but Hamilton cycle (the unweighted variant) is known to be polynomial. Discussion of both on the open problems project: $URL$ 

No, so long as there aren't too many elements. This is fairly easy to see---so long as you don't have too many elements, you can distribute the elements in the whole array evenly, meeting the density bound of the top array. But the arrays have looser requirements as they go down. So if the top array fits in density (and the elements are distributed evenly), the entire data structure is correctly balanced. 

As @Joe points out, chess is trivial to solve in $O(1)$ time using a lookup table. (An actual implementation of this algorithm would require a universe significantly larger than the one we live in, but this is a site for theoretical computer science. The size of the constant is irrelevant.) There is obviously no canonical $n\times n$ generalization of chess, but several variants have been considered; their complexity depends on how the rules about moves without captures and repeating positions are generalized. If a draw is declared after a polynomial number of capture-free moves, or after any position repeats a polynomial number of times, then any $n\times n$ chess game ends after a polynomial number of moves, so the problem is clearly in PSPACE. Storer proved that this variant is PSPACE-hard. For the variant with no limits on repeated positions or capture-free moves, the number of legal $n\times n$ chess positions is exponential in $n$, so the problem is clearly in EXPTIME. Fraenkel and Lichtenstein proved that this variant is EXPTIME-hard. 

Theorem 3.1 requires that the hierarchical representation of $P$ is compact. One of the requirements for compactness is that the degree of $r_i$ in $P_{i-1}$ is bounded by a constant. See the bottom of page 3. The definition and construction of the Dobkin-Kirkpatrick hierarchy is much more explicit in their earlier papers (references [9,10,11] in the paper you're reading). I strongly recommend reading them first. 

While this doesn't answer your exact question, CFG parsing is a decision problem that was reduced from matrix multiplication (so it is as hard as matrix multiplication in a sense). Specifically, in [1] it was shown that CFG parsing is as hard as boolean matrix multiplication. In particular, if CFG parsing (a decision problem) can be solved in $O(gn^{3-\epsilon})$ time, boolean matrix multiplication can be solved in $O(n^{3-\epsilon/3})$ time. An interesting aspect is that matrix multiplication can also be used for fast CFG algorithms, so the problems are computationally equivalent in a sense. The reduction has some unusual aspects because boolean matrix multiplication requires $n^2$ output bits, whereas CFG parsing only requires one. To deal with this, the paper assumes that the CFG parser solves certain subproblems when parsing the string (and argues that this is a reasonable assumption to make). The reduction makes $n^2$ queries to these subproblems to obtain the product matrix. Thus CFG parsing is a decision problem that is computationally as hard (in a sense) as matrix multiplication. However, this is not specifically a decision version of matrix multiplication, and furthermore, the reduction relies on the idea that CFG parsing is actually made up of $n^2$ decision subproblems. 

The first thing to notice is that an $n$-vertex polygon polygon $B$ is inside circle $A$ if and only if all of the vertices of $B$ are inside $A$. (1) So the solution to your problem is exactly the area of $n$ arbitrary intersecting circles, each of radius $r$ and centered at a vertex of $B$. (You then divide by $S$ to get the final probability). There's an interesting blog about this problem here which includes some heuristic solutions which may be sufficient for your use case. From a theoretical point of view, the furthest-point Voronoi diagram gives us a good starting point. Let's look at a point $p$ inside a cell of the furthest-point Voronoi diagram. We want to know if the circle $A$ centered at $p$ contains $B$. By the above, this happens if and only if all vertices of $B$ are within distance $r$ of $p$. Now, since we're inside a cell of the furthest-point Voronoi diagram, we can just test whether or not $p$ is within distance $r$ of that furthest point. In particular, the area we want within a given Voronoi cell is the intersection of that Voronoi cell with a single circle of radius $r$. This can be easily calculated in time linear in the number of edges of the cell. Since the furthest-point Voronoi diagram has $O(n)$ cells with $O(n)$ total edges, this takes $O(n)$ time in total. Then the total running time is dominated by the time to calculate the furthest-point Voronoi diagram, which is $O(n \log n)$. A similar strategy gives you an explicit region $C$ in the same amount of time. (1): The "only if" direction is obvious. To see the "if" direction, we can first note that by convexity if all vertices of $B$ are inside $A$, then all edges of $B$ must be as well. The same argument then extends to all interior points of $B$ since they must lie on a line between two vertices or edges. 

As the figure hopefully suggests, this algorithm breaks the input array into chunks of size $k/2$, and then applies bubblesort to the chunks, using the $k$-sorter in place of a compare-exchange operation. Correctness follows by observing that the network correctly sorts any array of 0s and 1s. As @VinayakPathak's comment suggests, the $O((n/k)^2)$ bound can be reduced by replacing bubblesort with a different sorting network. For example, Batcher's even-odd mergesort reduces the number of black-box calls to $O((n/k)\log^2(n/k)) = O(\sqrt{n}\log^2 n)$, and the AKS sorting network reduces it to $O((n/k)\log (n/k)) = O(\sqrt{n}\log n)$. This last algorithm matches Neal's non-oblivious algorithm up to (large!!) constant factors. 

The results in "The Geometry of Binary Search Trees" by Demaine, Harmon, Iacono, Kane, and Patraşcu were developed with the help of software to test various charging schemes and construct optimal asses for small access sequences. (And yes, "asses" is the correct term.) 

Here's a possible alternative to Bob's reduction, this time from (undirected) Hamiltonian cycle. I'm not 100% confident that the details are correct—I've already found and fixed several issues—but I'm sure it can be massaged into a correct proof. As Bob points out, this reduction has a serious bug; the white king can easily stray from its canonical path through the board. This bug can be fixed by adding Bob's cross-over gadget at appropriate locations (I think), but then it's not significantly different from his reduction. Let $G$ be an undirected graph with $n$ vertices and $m$ edges. Draw $G$ in the plane by placing its vertices at regularly spaced points on a line with slope $-1$, and drawing every edge as a horizontal segment plus a vertical segment, both above the vertex line. Now we reduce this drawing to an $O(n^2)\times O(n^2)$ board (rotated 45 degrees) with $O(n^2+m)$ black checkers and one white king. We need three types of gadgets: corners, splitters, and hordes. A corner contains two black pieces that can only be captured together by changing the white king's direction. A $k$-splitter contains a single piece that must be captured in a particular direction, with $k$ special locations for the capturing king to jump into. Finally, a hoard is a large box full of $hn$ black pieces that must be captured in a particular order, for some large constant $h$. In the figures below, the gray circles are pieces that cannot be captured. 

A modified Bentley-Ottmann sweep algorithm (here, for example) will find all line segment intersections in $O((n+k)\log n + k)$ time, where $k$ is the number of intersections. It's fairly straightforward to extend the algorithm to handle circles. A good walkthrough is here under "finding intersections". $k$ can be $\Omega(n^2)$ in theory (in which case pairwise checking is faster), but it seems difficult to avoid the $k$ term, and in any case you seem to say $k$ will be small in your question. Granted, this recomputes the intersections from scratch each time you run the algorithm, which may be inefficient for your purposes. You may be able to get around this a bit by maintaining the list of circles sorted by $x$ coordinate, and modifying this list online as the circles move. This won't help your asymptotic running time, but it avoids sorting each circle every time. If the circles stay relatively sparse (few cross the same vertical line), the binary search tree on the $y$-coordinates will be cheap to maintain. The other issue is that knowing two circles intersect already does not improve the running time at all. Once two circles attach to each other, you will calculate their intersection every single time, which is a waste. 

Lee, Lillian. "Learning of context-free languages: A survey of the literature." Techn. Rep. TR-12-96, Harvard University (1996). 

I assume you're talking about maintaining keys, in order, in an $O(N)$-sized array, with $O(\log^2 N)$ amortized worst-case update time. 

Have better things to write about. Write about them better. Stephen King's advice for writers is surprisingly relevant: $URL$ 

I'm not familiar with details of the ellipsoid method specifically for semi-definite programs, but even for linear programs, analysis of the ellipsoid method is pretty subtle. 

Klein's algorithm computes a compressed representation of all shortest-path trees rooted at vertices of the designated face $f$, in $O(n\log n)$ time and space. This representation allows us to retrieve the shortest-path distance from any vertex on $f$ to any other vertex in $O(\log n)$ time. An explicit representation of these shortest-path trees would require $\Theta(kn)$ space, where $k$ is the number of vertices in $f$; in the worst case, this space bound is $\Theta(n^2)$. Klein's observation, at least intuitively, is that the shortest-path trees for two adjacent source vertices are nearly identical; his data structure stores only the differences between adjacent trees. Klein's algorithm works just fine for bidirected and undirected graphs. For purposes of the algorithm, a bidirected graph is just a directed graph where the reversal of every arc is another arc, and an undirected graph is jut a bidirected graph where every arc has the same weight as its reversal. In other words, a bidirected graph is a directed graph whose adjacency matrix is symmetric, and (in this context) an undirected graph is just a directed graph whose weighted adjacency matrix is symmetric. Klein's algorithm requires no modifications whatsoever to handle these special cases. Klein's algorithm does not compute shortest-path distances between arbitrary pairs of vertices; in every pair, the source vertex must lie on the designated face. For a solution to the more general problem, see Sergio Cabello's paper "Many distances in planar graphs" [SODA 2006 and Algorithmica 2012]. You might also check out my upcoming journal paper with Sergio Cabello and Erin Chambers, which describes a different multiple-source shortest-path algorithm, which also works for higher-genus graphs. 

It depends on what you mean. Do you mean "when I insert a single element, so $N\gets N+1$, should I update the chunks?" then the answer is no. On the other hand, if you mean "when I run out of room in the data structure, and double its size, rebuilding the entire data structure, should I update the chunks?" then the answer is yes. 

At the beginning, you will have one chunk. Ordered file maintenance doesn't make sense for very small arrays. Pick a smallish number (like 64 or 1024), and only start this whole chunking method once you grow above this small number. In the small array do something naive, pushing elements around as necessary. 

Yes, you can use standard dynamic array methods, like doubling in size when it gets too large. Some data structures like to avoid this, as a full rebuild is a really large cost. But this isn't a big deal for us, as this data structure is highly amortized anyway. In other words, you have to do about $\Theta(\log N)$ rebuilds of the entire data structure every $N$ insertions anyway, just to maintain balance. This just adds an extra one. 

A paper by Abboud et al. recently accepted to SODA 2016 shows that subtree isomorphism cannot be solved in $O(n^{2-\epsilon})$ time unless the strong exponential time hypothesis is false. Of course, we can verify an isomorphism in linear time. In other words, the SETH gives us a natural problem in $\sf{P}$ with an $\Omega(n^{1-\epsilon})$ gap between finding and verifying. In particular, a $O(n^2/\log n)$ algorithm is known for rooted, constant-degree trees (for which Abboud et al.'s lower bound results still apply). So under SETH, the almost linear find-verify gap is essentially tight for this problem. 

I strongly suggest reading the more recent followup paper by Ailon and Chazelle, which avoids the whole infinitesimal issue entirely. If you want to stick with my paper, please read the journal version (Chicago J Theoretical Computer Science 1999). The SODA version has a major bug in Section 5, and (I think) the journal version explains the main proof much more clearly. 

It is NP-hard to approximate the genus of a graph to within an additive error of $O(n^\epsilon)$. There are polynomial-time algorithms that compute an embeddings of genus $O(g\sqrt{n})$ or $\max\{4g, g+4n\}$, where $g$ is the true genus and $n$ is the number of vertices. A significantly better approximation algorithm, spectral or otherwise, would be a significant breakthrough! See: Jianer Chen, Saroja P. Kanchi, and Arkady Kanevsky. A note on approximating graph genus. Information Processing Letters 61(6):317–322, 1997. 

Richard Bellman's autobiography suggests that he chose the term “dynamic programming” to be intentionally distracting. 

Finer distinctions between these categories are pointless. Other metrics like the number of issues/articles per year, are utterly irrelevant. A long time to publication is frustrating, but also ultimately irrelevant; this is only an issue of latency, not throughput. Assume it will take a year and plan accordingly. Meanwhile, you're already published your results in a good conference, right? And you've put a full version of your paper on the web, right? Ultimately, your work is not going to be judged on the venue in which it's published, but rather on the impact it has on the research community. There are crap papers in JACM, and there are groundbreaking papers in write-only journals. So where should you aim your paper? Be respectful but brutally honest with yourself. If you really have a once-in-a-lifetime breakthrough result, send it to an elite journal. If you have a good paper that will be interesting to a large segment of some intellectual community, send it to a good journal. If you have some decent results that could be published somewhere but only a handful of people would care about, I suppose you could send it to a write-only journal, but why bother? In other words, stop worrying about how to play the game and just do good science.