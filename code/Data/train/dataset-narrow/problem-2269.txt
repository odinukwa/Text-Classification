I have a range space $(X,R)$, were $R$ is a collection of subsets of $R$ and I have an upper bound $d$ to the VC-dimension of $(X,R)$. Suppose for simplicity that $X$ is finite. Given $\delta\in(0,1)$ and an integer $m>1$, I know that if I have a random sample $S$ from X of size $|S|=m$, then for $\varepsilon=\sqrt{\frac{d+\log(1/\delta)}{m}}$, we have $\Pr\left(\exists r\in R ~:~ \left|\frac{|r|}{|X|}-\frac{|r\cap S|}{|S|}\right|>\varepsilon \right)<\delta$. Notice how $\varepsilon$ does not depend on the sizes of the members of $R$. I was wondering whether I can get a smaller $\varepsilon$ if I know that all members of $R$ have a specific size, or better, if I know that there is a $\phi\in(0,1)$ sucht that $\frac{|r|}{|X|}=\phi$ for all $r\in R$. Thanks. 

One can bound the Rademacher average $R_n(A)$ of a finite set of vectors $A\subseteq\{0,1\}^n$ using Massart's Finite Lemma: $$ R_n(A)\le \max_{a\in A}\|a\|\frac{\sqrt{2\ln|A|}}{n} $$ where $\|\cdot\|$ is the Euclidean norm. Then, using Sauer's Lemma, one can obtain $$ R_n(A)\le C\max_{a\in A}\|a\|\sqrt{\frac{V\ln\frac{n}{V}}{n}},$$ where $V$ is the (empirical) VC-dimension. Using chaining and a bound on covering numbers, one can get rid of the logarithmic factor and obtain $$ R_n(A)\le C'\sqrt{\frac{V}{n}}. $$ Looking at the proof that uses chaining, I can't seem to find a way to have $\max_{a\in A}\|a\|$ in the second bound. Is it even possible? It may not change much in theory, but it does in practice, and (in my opinion), it intuitively makes sense that the bound should depend on it. 

Ezra and Sharir showed the $O(n^2\log^2 n)$ linear decision tree complexity for $k$-SUM problem [1], which improves the $O(n^3\log^3 n)$ complexity result of Cardinal et al [2]. It is known that $k$-SUM and Table-$k$-SUM problems are related and can be reduced to each other in linear time[3] and both problems can be solved in polynomial time $O(m^k).$ $\textbf{$k$-SUM Conjecture}$ [4]: There does not exist a $k ≥ 2$, an $ε > 0$, and a randomized algorithm that succeeds (with high probability) in solving $k$-SUM in time $O(n^{ \left \lceil{k/2}\right \rceil−ε})$. What's the consequence of the above new decision tree complexity results? What can this lead to a new bound for the time complexity of $k$-SUM problem? [1] Ezra, Esther, and Micha Sharir. "The Decision Tree Complexity for $ k $-SUM is at most Nearly Quadratic." arXiv preprint arXiv:1607.04336 (2016). [2] Cardinal, Jean, John Iacono, and Aurélien Ooms. "Solving $ k $-SUM using few linear queries." arXiv preprint arXiv:1512.06678 (2015). [3] Woeginger, Gerhard J. "Space and time complexity of exact algorithms: Some open problems." International Workshop on Parameterized and Exact Computation. Springer Berlin Heidelberg, 2004. [4] Abboud, Amir, and Kevin Lewi. "Exact weight subgraphs and the k-sum conjecture." International Colloquium on Automata, Languages, and Programming. Springer Berlin Heidelberg, 2013. 

Given a Graph $G=(V, E),$ it takes $O(|V|^3)$ delay and $O(2^|V|)$space to find all bicliques in lexicographical order in $G$ [1]. There is no polynomial-delay enumeration algorithm for all bicliques in reverse lexicographical order unless P=NP [2]. Given a bipartite graph $G=(U, V, E),$ it takes $O((|U|+|V|)^2)$ delay with exponential space to find all maximal bicliques in $G$ in lexicographical order in $U$ [3]. [1] Vania M.F. Dias, Celina M.H. de Figueiredo, and Jayme L. Szwarcfiter. “Generating Bicliques of a Graph in Lexicographic Order”. In: Theoretical Computer Science 337.1-3 (June 2005), pp. 240–248. doi: 10.1016/j.tcs.2005.01.014. [2] Wasa, Kunihiro. "Enumeration of enumeration algorithms." arXiv preprint arXiv:1605.05102 (2016). [3] Alain Gely, Lhouari Nourine, and Bachir Sadi. “Enumeration aspects of maximal cliques and bicliques”. In: Discrete Applied Mathematics 157.7 (Apr. 2009), pp. 1447–1459. doi: 10.1016/j.dam.2008.10. 010. 

Let $A$ be a set of size $k$ and $B$ be a set of size $\ell$, for fixed $k$ and $\ell$, and such that $A\cap B=\emptyset$. What is the (or a) Sperner family $\mathcal{F}$ on $A\cup B$ for which $\mathcal{F}_B=\{C\cap B ~:~ C\in\mathcal{F}\}$ is maximized? I actually just need an upper bound to $|\mathcal{F}_B|$ (possibly something better than $2^\ell$, which seems to be loose if $2^k<\ell$) Any hint or reference where this kind of information or relevant material could be found would be much appreciated. Thanks. 

For the VC dimension to be exactly $\log_2(|\mathcal{S}|)$, $\mathcal{S}$ must be the powerset of some subset of $X$ of size $\log_2(|\mathcal{S}|)$, so you would need to check whether this is true, which takes, I'd say, time $O(|\mathcal{S}|)$. 

Given a set $I$ of $n$ items, and a collection $D$ of $m<2^n$ subsets of $I$, a closed itemset is a subset $A$ of $I$ that is contained in strictly more elements of $D$ than any of its proper supersets. Closed itemsets are important in data mining because they provide a compact representation of the dataset. I'm trying to find some non-trivial upper bound to the number of closed itemsets. It seems a natural combinatorial question and I'm surprised I can't find anything non-trivial. Any suggestion/reference? Thank you. 

In uniform converge results of means or averages to their expectations (think of the typical results involving VC-dimension, covering numbers, Pseudo-dimension, fat shattering dimension, ...) , the estimator used on the sample is the sample average. Is it possible to use a different estimator? Is it required that the estimator has some specific properties (I assume at least unbiasedness) ? Thanks 

2-3. $\Psi(\mathcal{C})$ will typically become more difficult than $\mathcal{C}$ and this is a good thing. The difficulty of a proof-of-work problem needs to be finely tunable, but the original problem $\mathcal{C}$ may or may not have a finely tunable level of difficulty (remember that the difficulty in mining Bitcoin is adjusted every two weeks). The difficulty of problem $\Psi(\mathcal{C})$ is equal to the difficulty of finding some suitable $(k,x)\in D$ multiplied by $\frac{2^{n}}{C}$. Therefore, since the constant $C$ is finely tunable, the difficulty of $\Psi(\mathcal{C})$ is also finely tunable. Even though the problem $\Psi(\mathcal{C})$ is more difficult than the original problem $\mathcal{C}$, almost all of the work for solving the problem $\Psi(\mathcal{C})$ will be spent on simply finding a pair $(k,x)$ with $(k,x)\in D$ rather than computing hashes (one cannot compute whether $H(k||x||\textrm{Data}(k,x))<C$ or not until one has computed $\textrm{Data}(k,x)$ and one cannot compute $\textrm{Data}(k,x)$ unless one verifies that $\textrm{Data}(k,x)\in D$). Of course, the fact that $\Psi(\mathcal{C})$ is more difficult than $\mathcal{C}$ presents some new concerns. For a useful problem, it is most likely the case that one would want to store the pairs $(k,x)$ where $(k,x)\in D$ in some database. However, in order to receive the block reward, the miner must only reveal a pair $(k,x)$ where $(k,x)\in D$ and $H(k||x||\textrm{Data}(k,x))<C$ instead of all the pairs $(k,x)\in D$ regardless of whether $H(k||x||\textrm{Data}(k,x))<C$ or not. One possible solution to this problem is for the miners to simply reveal all pairs $(k,x)$ where $(k,x)\in D$ out of courtesy. Miners will also have the ability to reject chains if the miners have not posted their fair share of pairs $(k,x)\in D$. Perhaps, one should count the number of pairs $(k,x)\in D$ for the calculation as to who has the longest valid chain as well. If most of the miners post their solutions, then the process of solving $\Psi(\mathcal{C})$ will produce just as many solutions as the process of solving $\mathcal{C}$. In the scenario where the miners post all of the pairs $(k,x)\in D$, $\Psi(\mathcal{C})$ would satisfy the spirit of conditions 2-3. 

Berman–Hartmanis conjecture: all NP-complete languages look alike, in the sense that they can be related to each other by polynomial time isomorphisms[1]. I am interested in a more fine-grained version of the "polynomial time", that is, if we use parameterized reductions. A parameterized problem is a subset of $Σ^∗ × Z \geq 0$, where $Σ$ is a finite alphabet and $Z\geq 0$ is the set of nonnegative numbers. An instance of a parameterized problem is therefore a pair $(I, k)$, where $k$ is the parameter. A parameterized problem $π_1$ is fixed-parameter reducible to a parameterized problem $π_2$ if there exist functions $f$, $g$ : $Z≥0 → Z≥0$, $ Φ : Σ∗ × Z≥0 → Σ^∗$ and a polynomial $p(·)$ such that for any instance $(I, k)$ of $π_1$, $(Φ(I, k), g(k))$ is an instance of $π_2$ computable in time $f(k) · p(|I|)$ and $(I, k) ∈ π_1$ if and only if $(Φ(I, k), g(k)) ∈ π_2$. Two parameterized problems are fixed-parameter equivalent if they are fixed-parameter reducible to each other. Some NP-complete problems are FPT, for example, the decision version of vertex cover problem is NP-Complete, it has a $O(1.2738^k + kn)$ algorithm[2]. Finding better fixed-parameter reductions of a FPT problem which is NP-Complete can lead to a better algorithm, for example, by invoking a reduction to an "above guarantee version" of the Multiway Cut problem can lead to an algorithm in time $O^*(4^k)$ for AGVC(Above Guarantee Vertex Cover) problem[3], which is better than the original $O^*(15^k)$ algorithm [4]. $\textbf{My Conjecture: All FPT NP-complete languages are fixed-parameter-isomorphic.}$ Is that conjecture true? [1] Berman, L.; Hartmanis, J. (1977), "On isomorphisms and density of NP and other complete sets", SIAM Journal on Computing 6 (2): 305–322. [2] J. Chen, I. A. Kanj, and G. Xia, Improved upper bounds for vertex cover, Theor.Comput. Sci., 411 (2010), pp. 3736-3756. [3] M. Cygan, M. Pilipczuk, M. Pilipczuk, and J. O. Wojtaszczyk, On multiway cut parameterized above lower bounds, in IPEC, 2011. [4] M. Mahajan and V. Raman, Parameterizing above guaranteed values: Maxsat and maxcut, J. Algorithms, 31 (1999), pp. 335-354. 

I have a graph $G=(V,E)$, with positive weights $w_e, e\in E$ on the edges, and I would like to randomly perturb the weights of the edges so that for each pair of distinct vertices $(u,v)$ such that there is a path from $u$ to $v$, there is, after perturbation, a unique shortest path from $u$ to $v$, and this shortest path is one of the "original" shortest paths, that is, one of the shortest paths before the perturbation. It seems to me that the perturbation can only be additive and at most equal to the minimum difference between two edge weights divided by the maximum number of vertices in a shortest paths between pair of vertices in $G$ (let this number be $\Delta$). That is, the perturbation that I add to the edge $e\in E$ should be some $$ \varepsilon_e \in\left[0, \frac{\min_{\ell,r\in E}|w_\ell-w_r|}{\Delta}\right] $$ and distributed according to some distribution (is uniform sufficient?) on the interval. Possibly, I would also like the "perturbed" shortest path to be chosen uniformly at random from the set of original shortest paths. Is this possible? Any reference or hint is appreciated. Thanks. 

A range space is just a pair $(X,\mathcal{R})$ where $X$ is a finite or infinite set and a $\mathcal{R}$ is a finite or infinite collection of subsets of $X$ (the elements of $\mathcal{R}$ are called "ranges"). Note that a range space is not used to "represent" the VC-dimension (whatever meaning of "represent" you are using). Rather, the VC-dimension is a property of the range space. 

$\textbf{Other Advantages of this technique:}$ The SLT offers other advantages than conditions 1-4 which are desirable or necessary for a proof-of-work problem. 

Mining pools are more feasible: In cryptocurrencies, it is often very difficult to win the block reward. Since the block rewards are very difficult to win, miners often mine in things called mining pools in which the miners combine their resources in solving a problem and in which they share the block reward in proportion to the amount of “near misses” they have found. A possible issue for $\mathcal{C}$ is that it may be difficult to produce a qualitative notion of what constitutes as a “near miss” for the problem $\mathcal{C}$ and the algorithm for finding a near miss may be different from the algorithm for solving $\mathcal{C}$. Since the pool miners will be looking for near misses, they may not be very efficient at solving $\mathcal{C}$ (and hence, few people will join mining pools). However, for $\Psi(\mathcal{C})$, there is a clear cut notion of a near miss, namely, a near miss is a pair $(k,x)$ where $(k,x)\in D$ but where $H(k||x||\textrm{Data}(k,x))\geq C$, and the algorithm for finding near misses for $\Psi(\mathcal{C})$ will be the same as the algorithm for finding solutions to $\Psi(\mathcal{C})$. Progress freeness: A proof-of-work problem $P$ is said to be progress free if the amount of time it takes for an entity or group of entities to find next block on the blockchain follows the exponential distribution $e^{-\lambda x}$ where the constant $\lambda$ is directly proportional to the amount of computational power that entity is using to solve Problem $P$. Progress freeness is required for cryptocurrency mining problems in order for the miners to receive a block reward in proportion to their mining power to achieve decentralization. The SLT certainly helps mining problems achieve progress freeness. 

The convex hull problem is to compute the facets of the convex hull of finitely many given points in $\mathbb{R}^d.$ By cone polarity it is equivalent to computing the vertices and rays of a polyhedron which is given in terms of finitely many linear equations and inequalities. One way to measure the complexity of an enumeration algorithm is the total time used to compute all solutions. An output-sensitive algorithm is an algorithm whose running time depends on the size of the output, instead of, or in addition to, the size of the input. The number of solutions may be exponential in the size of the input, therefore a problem is tractable and said to be output polynomial when it can be solved in polynomial time in the size of the input and output. The precise complexity status of the convex hull problem is unsettled: it is not known whether or not there exists an algorithm whose complexity is bounded by a polynomial in the combined size of the input and the output[1]. Unlike the well known convex hull algorithms, there is "the ultimate convex hull algorithm" which is an optimal output-sensitive algorithm by Kirkpatrick and Seidel in 1986 and a simpler algorithm was developed by Chan in 1996 [2]. Are there any known related results about the complexity of the convex hull problem? Even it is not output-sensitive. What caused the difficulty of this problem? [1] Assarf, Benjamin, et al. "Computing convex hulls and counting integer points with polymake." Mathematical Programming Computation (2016): 1-38. [2] $URL$ 

It is well-known that every permutation can be written as the composition of two involutions. Suppose that $p$ is a polynomial. Then does there exist a polynomial $q$ such that if $f:\{0,1\}^{n}\rightarrow\{0,1\}^{n}$ and its inverse are permutations which are computable by combinatorial circuits with at most $p(n)$ gates, then there exists involutions $g,h:\{0,1\}^{n}\rightarrow\{0,1\}^{n}$ such that $f=g\circ h$ and where $g,h$ are computable by a combinatorial circuit with at most $q(n)$ gates? 

a. Alice pays a large fee (the fee will cover the costs that the miners incur for verifying the algorithm) and then posts the algorithm which we shall call Algorithm K that breaks Problem $A$ to the blockchain. If Algorithm K relies upon a large quantity of pre-computed data $PC$, then Alice posts the Merkle root of this pre-computed data $PC$. b. Random instances of Problem A are produced by the Blockchain. Alice then posts the portions of the pre-computed data which are needed for Algorithm K to work correctly along with their Merkle branch in order to prove that the data actually came from $PC$. If Alice's algorithm fed with the pre-computed data $PC$ quickly, then the problem is removed and Alice receives a reward for posting the algorithm that removes the problem from the blockchain. This problem removal procedure is computationally expensive on the miners and validators. However, the SLT removes most of the computational difficulty of this technique so that it can be used if needed in a cryptocurrency (instances which this technique is used will probably be quite rare). 

The connection between $c^kn^{O(1)}$ for $c<4$ and exact exponential-time algorithms beating brute-force $O(2^n)$ algorithms has been known for a long time. However, when $c\geq 4,$ there are not many known results. Fomin et al. [1] showed a technique to handle this case, can improved a lot of exact exponential-time algorithms. For example, for the Interval Vertex Deletion Problem, the fastest previously known exponential-time algorithm is $O((2-\epsilon)^n)$ for $\epsilon <10^{-20}$ [2]. The fastest parameterized algorithm is $8^kn^{O(1)}$ [3]. Combined them together, Fomin et al. [1] got a $1.875^{n+o(n)}$ time algorithm. Do you known any other techniques and known results to improve the exact exponential-time algorithms using parameterized algorithms when $c\geq 4$? [1] Fomin, Fedor V., et al. "Exact algorithms via monotone local search." arXiv preprint arXiv:1512.01621 (2015). [2] Bliznets, Ivan, et al. "Largest chordal and interval subgraphs faster than 2 n." European Symposium on Algorithms. Springer Berlin Heidelberg, 2013. [3] Cao, Yixin. "Linear recognition of almost interval graphs." Proceedings of the Twenty-Seventh Annual ACM-SIAM Symposium on Discrete Algorithms. Society for Industrial and Applied Mathematics, 2016. 

There has been a series of breakthroughs on Dichotomy Theorems by Professor Jin-Yi Cai $URL$ Another dichotomy theorem $URL$ $URL$ $URL$ 

This conjecture is from an expert in Game Theory area, I post it here to draw more attentions of TCS experts. Discussions and comments are welcome. $URL$ An unknown deterministic finite automaton with a known number of states $n$ is put in a black box. The states of the automaton are colored in two colors, 0 and 1. Given an input bit the automaton outputs the color of its current states and moves to a new state as a function of the current state and the input bit. A decision maker trying to predict the behaviour of the automaton by feeding it input bits and observing the output repeatedly. An attempt is called successful if the input bit is equal to the output bit. How many attempts does the decision maker need until he can succeed in 99% of the attempts (in expectation)? Neyman (1997) showed that $O(n \log n)$ is sufficient and conjectured that $Ω(n\log n)$ is necessary. 

So the two-dimensional reversible cellular automaton Critters (which you can simulate online at $URL$ on the Torus does not seem to follow the second law of thermodynamics. For example, if a $10\times 10$ block in the torus is filled with random data and the cells outside of this $10\times 10$ block are all zero, then there is a good chance that after several million generations on a $256\times 256$ grid, one can see the cellular automaton return to its original state. Why is the Poincare recurrence time for the reversible cellular automata Critters so small in this case? The rule Critters is universal for reversible computation, and it exhibits chaotic behavior that one would expect in a reversible cellular automaton. Therefore, one would expect that after running the cellular automaton for enough generations, the entropy would constantly increase and hence one would need to wait a very long time (much longer than several million generations) to observe the Poincare recurrence. Is there any explanation for this phenomenon? Are there other chaotic, reversible, universal cellular automata of dimension 2 that exhibit this phenomenon? 

Suppose that $p$ is a polynomial. Then does there exist a polynomial $q$ where if $f:\{0,1\}^{n}\rightarrow\{0,1\}^{n}$ is a bijection where both $f$ and $f^{-1}$ are computable by circuits with at most $p(n)$ gates, then there exists involutions $\iota_{1},\dots,\iota_{k}:\{0,1\}^{n}\rightarrow\{0,1\}^{n}$ where $$f=\iota_{k}\circ\dots\circ\iota_{1}$$ and where each $\iota_{i}$ is computable by a circuit with at most $q(n)$ gates and where $k\leq q(n)$.