Every guide to using SSSD for LDAP authentication I've found thus far shows you how to do more than just authenticate a user, such as provide their shell, groups, etc. I don't know how to remove those features without things breaking because there are several moving parts like SSSD, PAM, and NSS. Due to limitations on what information is provided via LDAP (it's not AD), only authentication of users can be done. There isn't even a uid available because the only id provided via LDAP are consistently formatted alpha-numeric strings (won't work on linux). Basically, how can SSSD be configured on Ubuntu to treat ldap as the "shadow" database, but get the uid, groups, and shell from your local system databases (passwd, group). This is currently done with libpam-ldap, but my understanding is there are better alternatives like libpam-ldapd and sssd, the latter of which RHEL has moved to. If I had to guess, it can be done similarly to how we currently do it, which is nss will check local databases first, and if the user doesn't have a shadow file entry, check ldap. As a summary, if I can use SSSD or, as a backup, libpam-ldapd, to authenticate the following way: uid -> /etc/passwd authenticate -> ldap shell -> /etc/passwd groups -> /etc/group Even better if it's possible to stop users from creating passwords locally that would end up in /etc/shadow thus causing it to check ldap at all in future login attempts. Also, all local and service accounts shouldn't be impacted, and ldap authenticated users can be determined with simple regex. I'll be very grateful for any good suggestions on how to handle this. Thanks! 

POP3 is a protocol, Exchange is an MTA, which can deliver mail using many protocols, it defaults to MAPI for user mail retrieval, but you can enable IMAP and POP3 (and even X.400) And it includes a webmail interface. (Which allows mobiles/iphones & remote users to access email). The webmail interface (when configured) allows users with broken Outlook to stay productive anywhere in the world.. ;-) 

Wouldn't it be easier (and kinder) to simply setup a shared "Sales" mailbox, let all the required salesdroids use it. Its never going to be as fast as a simple redirection, but there is no need to script it or anything then. (We use many here, allows users to change roles, get promoted, quit etc without having to worry about redirecting or informing customers of new addresses etc). The other alternative (in Exchange) is a Public Folder, simply create & set permissions on it for all who will need access (using Outlook), then assign the email address[es] you require (in System Manager, will default to "Foldername@domainname.com"), then all mail will go there instead.. has similar/familiar delegation controls/permissions etc. If particular users need access to shared calendars or mailboxes, they can set them as "Favorites". That way, users can "Grab" messages for themselves, by moving them into their own mailbox, or deleting them when done etc.. Also, looks professional when you see: Joe Bloggs on Behalf of Sales@company.com. If its the public folder, turn of "Maintain Per user Read/unread information.." then staff can tell when a message has already been read by someone else. Customers like it, they get an easy to remember address and, their messages aren't lost! (that is the biggie I think) The managers like it, they can keep tabs on open items, weigh in on things and generally be "Managers".. ;-) 

I'm trying to get emails into a Python application. There is an exchange server that marks some emails with a BCC address for archive@somedomain.com. I do not control the Exchange server. The email volume may be high sometimes too. On the server that will be handling the mail send to the archive address, there is a Python application that parses the emails, inserts some header information to some databases, and stores the email in an object store. I'm not sure of the best way to receive the email and get it into the Python program. There are a few issues with a Postfix solution. 1) Postfix is a lot of moving parts for such a simple purpose. Not really a big deal if problem 2 can be dealt with. 2) There isn't a good way that I can find to get the emails from Postfix to a Python program. This is not a Python script that should be invoked each time using the pipe transport. I need a way to stream email contents into the Python application with some kind of deliminator. I'm also trying to avoid using Pythons smtpd library directly as it uses asyncore which should be avoided in favor of asyncio. Is there a good Postfix configuration or perhaps another mail SMTP server application that can do what I need? Or will I have to resort to using a Postfix pipe running a script that will insert the email into some kind of queue? I'm also find with some kind of deliminated rotating log file containing email body and attachment data. 

(Note that any missing frames in the sequence above are due to other traffic I filtered out from this view). From my LAN machine, the ACK is sent back after the LIST command and data transfers work just fine. -- So while I have diagnosed it to this point, I don't know what to do next. The router doesn't say it's dropping any packets or blocking anything due to a firewall rule. Any ideas as to why my LAN machine sends the ACK but the DMZ machine doesn't? 

Edit: I upgraded Open Manage Server Administrator and now I receive the following message when I attempt to assign the new disk as a global hotspare: 

Okay, so I'm still not entirely sure what solved this problem, but the array is now rebuilding. The first thing I did was to remove OMSA completely and do a clean install (when I upgraded before, I went from 7.4 to 8.2 using an upgrade package). After uninstalling OMSA 7.4, I rebooted the server and went into the PERC S300 array manager that is an option on boot. There, it was showing Disk 0 as Ready and Disk 1 as being the Spare (OMSA did not show disk 1 as being a spare). I set Disk 0 to the Spare and unassigned Disk 1 from the Spare. After that, I proceeded to boot and install OMSA 8.2. After installation, I went to view my virtual disk and viola, it was finally rebuilding. 

I'd say it depends on your servers retention policies, what sort of account you are using, how you are connecting, where the data is stored, how you are deleting etc.. if you just push del, it should be moved to deleted items, if you press shift+del it should be gone. However, it might be retained in the "Recover Deleted Items" tools menu, at least until the next backup/scavange cycle (assuming exchange). This question might be better on SuperUser.. 

the "sync" you are referring to, is called "roaming profiles". A simple script implementation is described below that may copy something and log it. Modify the following to your needs (very basic) 

The graphic (below) shows the fault I am seeing in VMWare Infrastructure Client, the drive0 light is also flashing. Not sure why, the other drive lights are green. Can't see any errors in the logs and VM's are running sweet. I got up super-early and rebooted it this morning (production box, so I'm paranoid). It came up and all the status lights went green.. but I get into the office and its back on red.. Is there anything I can do to troubleshoot this? I think reinstalling ESX might fix it, haven't tried that yet. Main points: 

This looks like it is the right target, since it's an "FCP Target" and I have confirmed that the port_name is indeed the WWN of the 15TB storage target I am trying to connect to. The issue is that Linux doesn't create a device in /dev that I can use. I have a sg0 device, but that's mapped to sda, which is from the raid controller for local disks. I have done the following to try and make linux recognize the storage. 

I have a Cisco branded Emulex LightPulse LPE12002 in a Cisco UCS server running Ubuntu 14.04, and I am trying to connect to about 15TB of remote storage configured for it. I don't know where to start, but here's what I have done. First, when I issue , I get a result like the following. 

multiple reboots verified the lpfc driver is loaded None of these seem to work for creating a usable device. I am not using multipath currently, even though it is a 2 port FC card, I am just testing with one port connected and with exposed storage. Any ideas? 

You are checking to see if the connection originates in the /domain/ folder, so you need to specify it with a slash.. otherwise you get a loop.. which may cause the 500 error. 

The 403 page is dependant on Apache serving that html.. you can override it for any or directive simply by using the directive: 

No, no you aren't. By using MAPI internally, you keep the email on the server, and can back it up centrally, and can use public folders to share contacts/messages/todo/calendars etc with multiple users easily. I guess thats it, someone better can probably answer your questions more fully, and I look forward to reading their answers! 

This has a helpful example: $URL$ Uses php to parse the fifo and shunts it to browser... then ajax to refresh 

Then save it somewhere like here: \MyDomain\NETLOGON\logon.bat The "login script" can be applied via group policy, ie, create a new policy applied to your staff's OU 

I am confident in my NAT policies - People can connect to MY FTP server just fine, and I can connect to other FTP servers from inside my DMZ - there's only one specific server where this is a problem. The connection used to work fine before I upgraded my firewall from a Cisco SA520 to the Dell Sonicwall SOHO. I will also note that while I can connect to other FTP servers, those servers connect in passive mode, so it could be an issue with active mode connections. Currently, the problem server only accepts active mode from my IP. My current thinking is that it's a firewall issue on my end, but I can't understand why. My Firewall rules are pretty simple: 

I know there are a ton of similar questions already, but I've read through just about everything I could find and am still having trouble resolving my specific issue. Problem: I am having difficulty transferring data to an external FTP server, but only from an FTP client running on a server located inside my DMZ. Transferring from an FTP client works fine from any machine inside my LAN. A brief overview of my environment: I have a Sonicwall SOHO router/firewall with the following interfaces configured/connected: 

Does 9.1 have SGMon? the logging tool.. it will be in the program directory, you can actually see it working if the status stops updating. It will show you all the "under the hood" you can take! 

Process explorer might be better, allows you to identify which module is in error, where its stuck in the stack, memory/net/etc.. (doesn't stream past like a log-viewer, more like task-manager with better options!) $URL$ MS kb/555021 suggests your profile might be corrupt, try creating a new one. 

Depends, do you want an externally hosted certificate? (Free from here: $URL$ How to use: $URL$ Or an internally created one? Free: $URL$ Once you have OWA using HTTPS, you can start using RPC over HTTPS. (If you have turned it on in Exchange of course, your question seemed to be about the certificate) 

The "violation code" will literally be a random memory address, and will (should) change with every one you see, so google won't be able to help. You could install xdebug and trace it while google-api'ing. When next the error appears, hopefully your output will instead be a huge stack-trace detailing exactly which module is faulting and where. $URL$ 

I've checked windows logs and don't see any failures or errors related to this within the past 24 hours (when I first swapped out the disk). Looking at the OMSA logs, it tells me when I unassign a global hotspare, but there is never an event corresponding with me assigning a global hotspare. Similarly, I see notices when I cancel a rebuild but never when it is initiated after assigning a global hot spare. I've done some looking into similar problems, but most people receive an error or an actual failure during rebuilding, which I am not. I have good backups and the system is still running okay, so my next thought is to just take an image, blow away the array and start from scratch. I am mostly looking for other ideas before going that route. 

So it seems there is something about the disk I need to configure first. I will post the solution once I figure it out. Original Post I have a Dell PowerEdge T410 server with Windows Server 2008 R2. Storage is configured in a RAID-5 array. Everything has been working fine for several years, although I've had to swap out failed drives a couple of times here and there and rebuild. Until now, rebuilding has not been an issue, and the Open Manage software makes it pretty easy to manage. With the latest drive failure, however, I am seeing something different. I bought a new drive (same manufacturer, model number, and capacity as the other three in the array), and after popping it in, I deleted the virtual disk automatically created for it and assigned the disk as the hot spare for my array. From there, the rebuild is seemingly initiated, but progress never goes beyond 0% (it has been about 18 hours since it was initiated). Here is what I am seeing: