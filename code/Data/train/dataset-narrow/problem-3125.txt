Named-Entity Recognition (NER) is one of the techniques you could look at. Different techniques exist, you could first look at pre-trained models of Spacy in Python here. Otherwise, you could train a model on your own training data if you have. Also, more systematic approaches can be efficient. For exemple, looking at POS tags values, do regex identification... More generally, this is the field of Information Extraction (IE) in text mining. Some good resources can be found here and here. Sorry, everything is in Python as I work mainly with it. Good luck! 

In effect it is ordinal regression/classification. I suggest you to go for Mean Absolute Error to take into account the missclassification that play a bigger role when far from the ground truth : $MAE = \frac{\sum_{i=1}^n|h(x_i) - y_i|}{n}$, creating your own scoring function in python. Given the number of different outputs you have, I recommend you to go for regression. Anyway, rounding any hypothesis output would only play on the score, it's up to you to decide if it makes more sense to see wine quality scores as classes or not. 

The document will be at the place you started jupyter. If you’re on Linux and don’t remember where you started jupyter, you can try 

With all of this information, you might be able to come up with a few ways of putting in your custom function. Off the top of my head, I can't see a quick way to simply provide a function. You could for example: 

Machine learning is generally an approach to model and understand data, hopefully in such a way as to allow us to make predictions in the future, given some data that is representational of past observations. It more often than not considers a dynamic world and non-linear functions: given the state of the world, what should the ticket price be? Practically speaking, selling tickets doesn't work that way - it would be very confusing for customers if ticket prices always change! One more point that would concern me with your specific problem, is that your variables (ticket price, frequency of trains etc.) are not mutually exclusive from the number of tickets sold. If you just make each ticket cost $1000 dollars each, profit would be zero, because nobody would by a ticket. What I want to say is: we're not classifying cats and dogs here :-) With your problem and using machine learning, I would be more inclined to try answering questions such as: 

Go over list comprehension if you want a more pythonic way to do it, here is the most understandable to begin. 

I think you are referring to a king of image auto captioning, and why not sentence alignement ! I suggest you to go over the paper Deep Visual-Semantic Alignments for Generating Image Descriptions which is one of the references today (by M. Karpathy). The first step is about recognizing objects in an image and align sentences to them (done with R-CNN and BRNN) and the second is about generating new captions (CNN + RNN) Even though it may be quite difficult to deeply go through, there is some code available on GitHub and it has been remade by a a lot of people with various implementations. Here is the summary. Good luck ! 

It has been a long I am confused on understanding some of the AlexNet architecture : The output of the first conv layer is 55x55x48 (96 considering the division between GPUs but let's stick to 1 GPU so depth 48). Then max pooling is applied and there come my problem. When applying max pooling, the result is 27x27x48 right ? If so, how is applied the next convolution over this result (with 5x5x48 filters) to output 27x27x128 ? I finally don't see how and when to apply max-pooling in between convolutions. I must miss something here... 

Images If you have a set of images in a folder somewhere and you want to get them into a matrix, you can simply read them from disk using . Here is an example for a colour image: 

There is some discussion here on Cross-Validated too, where the selected answer shows some further usage of the term (albeit in a slightly different context). Have a look at this source for some usage of the term, which leads you to my understanding. 

The slice of the model shown below displays one of the auxilliary classifiers (branches) on the right of the inception module: 

EDIT After comments and update from OP: in their case, using (sigmoid) as the final activation negatively affected results. So perhaps it is worth trying out all possible activation functions to investigate which activation best suits the model and data. One further remark, at least within the context of deep learning, it is common practice not to use an activation at the final output of a neural network - for some thoughts around that discussion, see this thread. 

Thought #1 From the short description you give, and assuming you have some data (or can synthesise it), I would have a go at training a Hidden Markov Model. Have a look here for an awesome visual primer. Intuitively, HMMs do what you describe. There are some observables: 

1) Supervised learning is most of the time the process of learning a mapping, e.g relation, of input features x (sample) to an output y (often labels). Unsupervised learning doesn’t not use labels /output y to learn a relation between the samples and possible labels (ex: clustering). 2) Classification and regression are two types of supervised learning (discrete output labels vs continuous). Very good resources exist on the forum and web to go deeper with it if you’d like, don’t hesitate. 

The classification rule being $\underset{c_j}{argmax} \; p(c_j)p(w|c_j)$ with $p(w|c_j) = \prod\limits_{w_i}p(w_i|c_j)$ thanks to the independence assumption. To prevent underflow errors, we often use the log : $log \;\;p(w|c_j) = \sum\limits_{w_i}log\;\;p(w_i|c_j)$ Therefore, the final word is : to classify a document (or paragraph or whatever piece of text), you take each word $i$ from it, and for each class $j$, see the fraction of times it appears in it (in documents from class $j$), in the training data (which is a probability), and add up the log. It will give you $j$ probabilities (one for each class), and you take the class corresponding to the maximum. What is not clear in your question is the relation between labels and sentiment analysis. See this for a detailed explanation and examples. 

execute the search download the file navigate to the next search results page repeat from step 2. until finished 

Freezing two layers (in your case, out of three), intuitively kind of restricts the model. Here is a somewhat contrived analogy that I might use to explain such cases to myself. Imagine we had a clown who could juggle with three balls, but now we want them to learn to use a fourth ball. At the same time, we ask an amateur to learn how to juggle, also with four balls. Before measuring their rate of learning, we decide to tie one of the clown's hands behind their back. So the clown already knows some tricks, but is also constrained in some way during the learning process. In my mind, the amateur would most likely learn a lot faster (relatively), as there is more to learn - but also because they have more freedom to explore the parameter space i.e. they can move more freely using both arms. In the setting of optimisation, one might imagine that position of the pretrained model on a loss curve is already in a place where gradients are very small in certain dimensions (don't forget, we have a high-dimensional search space). This ends up meaning that it cannot as quickly make changes to the output of the weights whilst backpropagating errors, as the weight updates are multiples of these potentially small optimised weights. ...Ok - might sounds plausible, but this only addresses the problem of slow learning - what about the fact that the actual training accuracy is lower that that of the model with random initialisation?? Intial training accuracy 

There is nothing easy that comes to my mind here. Adding a garbage class is not good idea as it would deteriorate the rest of the model. Two solutions come here: 

The embedding layer maps your vocabulary index input to a dense vector, so it acts as lookup layer and (if set to trainable) will be influenced on some weights only, by the words occurring in a batch of training data. Having a linear layer, it would be sequentially trained by all the data batches and would not provide a lookup functionality (each word given to the input would share the same weights). Also, you're right considering word2vec differently. When using a custom trainable embedding layer, the dense vectors will be optimized (by SGD) for the task you are considering whereas models like word2vec act like language modeling and find a semantic optimum representation in the embeddings. Therefore, depending on data size, the representation found during training might be better for your task than the one found by a neutral word2vec or other model. 

In fact, you use 1D convolution. Given that the dimension of the output of embedding layer is 100, that the kernel size is 5, and that the number of filters is 128, You have 100x5x128 = 64000 weights. Add to this 128 biases and you get 64128 parameters. Note that parameter sharing is used, so that there is only one set of weights and biases per filter, in depth. 

in common English, the comma symbol doesn't appear in numerical data too often, or it is not required. E.g., one million would be 1000000. we don't need to commas, but programes like MS Excel show them just for readability, e.g. 1,000,000. in other languages, such as German, the comma symbol uis actually used as an English 'point' or 'dot' to separated decimals from integers, e.g. ("one point five") in English is written ("one comma five). This means that people working exclusively in German don't always used commas as a separator (but might still used the suffix!). probably should have been reason #1 - CSV files are actually still human-readable. Anyone with a text editor can open it up and get the gist of the data. That is a huge timesaver. The following point optimised for something else... People do use other seperators - check out this intersting blog post about using the Icelandic thorn symbol. The idea is to basically use a separator that does not appear anywhere else, so you can parse any file with pretty much zero doubt that something unexpected comes out. The thorn has some downsides e.g. that it doesn't fit into a single byte. There are other symbols that you could go for, such as the vertical bar , the ampersand or even the tilde . Have a look here for a little more discussion with respect to databases. Using whitespace symbols can work well. People use tabs, because they don't usually feature at all in a dataset as information themselves and they also leave the file in a readable format. The newline symbol is of course reserved for newlines (again, to point out the obvious). One could try using something like five spaces, claiming that it wouldn't appear in the English language (or any other natural language), but then the user would need a fancy regular expression (a.k.a. regex) to parse it, so perhaps not the most user friendly. 

Your target y can be whatever you need. If you want to do sequence to sequence for example, y will have the same number of time steps than the inputs (you predict something for each timestep). But you can also define an output with one timestep only, for text classification for example. Your input data has 10 timesteps, but the output is only a prediction at the last timestep. It depends on the problem you’re trying to solve. 

In fact, doc2vec doesn't expect anything, it uses the vocabulary you give. So you decide what you want to keep. A good practice is to experiment different tokenizers and training phases if you have time to see what works best. Also use what has already been done in your field of application. Don't forget that the input of Doc2Vec is an iterator over a list of TaggedDocument. See this tutorial for more. Good luck ! 

You add a bicycle class or other classes that can be problematic in your opinion due to close features with your base classes, and train again. Complicated to be exhaustive with all situations. You define a threshold of confidence of your model: for example if there is no probability more than 95% then the model is not confident in its choice and you should not trust him. Good and larged models should stay at equiprobable choices at the softmax mater when the features are not recognized well. 

The agent might otherwise want to just do the optimal move to keep the pole upright for the next milllisecond, but with this buffer, we are essentially only allowing the agent to consider larger chunks of time. This will take longer to train, but should in theory (well, the assumption behind these rollouts...) lend itself to creating a more thoughtful agent. In other contexts, such as a standard CNN used to classify images, this approach would make less sense as there isn't a temporal dimension to the task. Saving up the errors and making one big update can have several other explanations in general: 

One can of course attempt to frame it as a pure machine learning model, and there are many ways of doing this; it is optimisation, after all. I think the first step, whether using some machine learning algorithm or a linear optimisation construction, is to understand your data, the effects of each factor and their relationship to your target variable to be maximised/minimised (profit/loss, respectively). So, as I was told back when learning about this myself: "The first step is to translate words into linear inequalities". So you could think about the factors you have and understand which ones effect price. If there are constraints/limitations, these should also be considered. For example, if you know that you are not allowed to sell more than 10,000 tickets on one day due to capacity constraints of your system. There are some interesting points made in this thread over on Cross-Validated. Now you understand the data a little better, you could think about which models to try out. Without knowing more about your data, I can't really offer more guidance as to which models might be worth trying, but hopefully this answer helps you along that path. 

It will search for ipynb files. Otherwise try to search for ipynb files from the root folder of your OS. 

There are plenty of different approaches you can use, and none is the universal best solution. However, in general, preprocessing in twitter data, especially for Doc2Vec, follow: 

Then, it is totally possible to have 1 for recall or precision given that these are the scores for the considered positive class for scikit. In fact scikit assigns by default one of your class as the 'positive' one and to metrics inherent to classes are computed according to it. However, as you said you can't have 1 for both and 70% accuracy. The fact is that your clf.predict gave you 100% accuracy while during cross validation it was not the case because in your second scheme: and you train and predict on the same data ! So you obtain 100% accuracy which is common, while during cross validation : 

When doing GridSearchCv, the best model is already scored. You can access it with the attribute and get the model with . You do not need to re-score it in a cross validation. Also, yes, the pipeline is entirely fitted when doing each split during the cv.