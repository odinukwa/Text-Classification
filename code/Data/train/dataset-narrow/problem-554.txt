When you enable archive logging in a database, you're telling DB2 that you want to retain a copy of all transaction logs that have been created. LOGPRIMARY and LOGSECOND only control how many active log files can be in use in the active log path at any given time. Please read the documentation on database logging for more details. 

There's no restriction on the specific name, other than it must be 8 characters or less. So works just fine as a database name: 

The active log path (database configuration parameter ) for a database MUST be a locally accessible directory on the database server. Preferably one with very good read/write performance. (I'd say it must be a local directory, but technically you can specify the path on an NFS mount). When DB2 fills a transaction log file (and all transactions contained within have been committed), the DB2 log archiver process will copy the file according to what you've set in the and configuration parameters. 

However, for remote connections to also be unable to connect, you'll have to restart the DB2 instance. This is required because the DB2 TCP Connection Manager (db2tcpcm) will cache the the database catalog entries, so until the EDU is restarted it will still "know" about the database. After restarting the DB2 instance, remote connections will get the error: 

, and combine to define the maximum size of the transaction log on your server using the following formula: 

Has your database been explicitly activated (with )? DB2 will not evaluate whether a database is a candidate for automatic backups if it is not active. Relying on having at least 1 connection to the database to keep the database activated is a recipe for pain. That said, I moved away from relying on automatic backups a long time ago, instead relying on the consistency and control you get when using a scheduler like . 

requires Administrator authority on Windows, in the command shell you are executing it. Setting your ID as a member of the Administrators group on the local machine is not sufficient. IBM adds "DB2 Command Window - Administrator" in your Start Menu to give you a DB2 Command Window that has administrator privileges (which is the recommended method). Alternatively if you are using a normal command window (cmd.exe), you would have to start it by right clicking "Command Prompt" and choosing "Run as Administrator". It might be possible to use to execute , but it's probably easier to just start the command window directly with administrator privileges. 

This is a matter of looking at physical writes per buffer pool or tablespace over time. will tell you this if you are in Delta mode and look at the "Delta p_reads/s" column. 

It is relatively complicated to do, but it's possible. If your database has only DMS tablespaces that contain table data, you can use alone to get this information: 

You could specify similar options for the system catalog tablespace and the default system temporary tablespace, but I would not recommend doing so. 

In your case, this results in . The maximum transaction log size limits 2 things: 1) the absolute size of a single transaction 2) the "timespan" between the oldest active transaction and the newest transaction. You need to make it large enough to handle both scenarios, although if you're running into problems relating to #2 then you really have a problem with a poorly-behaved app that isn't committing. Choosing an appropriate value for should be based on how regularly you want to archive log files, as this will have an effect on your recoverability in the event of a problem. If your database archives log files only once every hour (or less), you will potentially lose much more data in the event of a serious problem than if your database archives a log file every 2-3 minutes (because every time a log file is archived it should be copied to NetBackup). 

[This answer was also posted to the duplicate question on Stack Overflow] When you perform an online restore, DB2 must lock the tablespace(s) you are trying to restore. The restore process essentially overwrites the file on disk containing the tablespaces' data. This is incompatible with applications using data in the same tablespace while the restore occurs. If your database has all data in a single tablespace, then an online restore is not particularly useful. If you have multiple tablespaces in the database, applications may be able to continue functioning while the corrupted tablespace(s) are restored, but of course this requires some planning in your application and database design. 

You can't "estimate" the size of a buffer pool, because the answer is, "it depends". Try using the command to get started, and enable STMM. Together these will go a long way towards getting your database running more efficiently. I would also recommend you spend some time reading about DB2 Performance Tuning. There is a ton of information available on the web, starting from the entire Performance Tuning section of the manual. 

shows the amount of uncompressed data in this buffer. The line above shows data from an SMS tablespace; DMS tablespaces will also show lines like this, but the will show . So, if you look for lines where starts with , you can add up the of these lines, and find the additional storage space that all SMS tablespaces will use. 

Whoever told you "When rolling forward to end of logs without specifying overflow log path only active logs will be read" was wrong. When you have set to , , or , the DB2 log manager EDU () is responsible for moving log files to and from the archive location. When performing a operation, DB2 will determine which log files it needs, and will retrieve files for processing if they aren't already in the active log path. Now, if you have set to , and you are manually moving log files from the active log path to somewhere else, only then will you have to specify (or move/copy the transaction log files back to the active log path). So: means just what it sounds like: DB2 will replay all transactions present in log files between the backup time and when the restore took place. 

For the most part, application == connection == session when looking at it from DB2's perspective. Someone could probably make an argument that these aren't always necessarily the same in environments using connection concentrator or the database partitioning feature (DPF), but for all intents and purposes it's safe to use the terms interchangeably. You could also make the argument that, from the client-side, a single application may open multiple connections (i.e. a connection pool), but this is dba.stackexchange.com :-) 

When you restore an offline backup, you are not required to roll forward (although the database will be in ROLLFORWARD PENDING state at the end of the backup, which you can avoid by using the option for ). This means that you can remove any archived log files. However, keep in mind that you may want to be careful about which archive log files you remove – if you aren't careful you could accidentally remove files newer than the backup that might be necessary. 

This looks at only records in the last 6 weeks, and then finds records that occurred on Sunday () at 12:00. (Using the predicate should handle minor differences in clocks on your database servers and/or instances when your script doesn't execute exactly on the minute. It may also be possible to write this using OLAP windowing functions, but I'm not sure it would be any more efficient. 

However, if the MQT definition you supply is really this simple (i.e., pre-computing the join), I wonder why you want to create an MQT in the first place? As you said, performance of the query is good, so why wouldn't you just retain the query as written in the application? Alternately, you could encapsulate the join logic in a view. Although I would generally advise against this, if there is some serious performance issue with the join that can't be resolved with proper indexing and , you could replicate this immediate update functionality through triggers on the and tables that insert records into your table: 

You won't get any specific answer to this question, because as you observed, it depends. The answer depends on your company's tolerance for having one DB2 instance adversely impact the performance of another's. You may be able to control or limit this impact if your AIX admins are willing to learn and set up WLM policies at the operating system level (not DB2 WLM). This is really a question of capacity management. Understanding the workloads in your environment is key here, and being able to identify (or predict) when you'll run into limits will influence your decisions about how many instance(s) to put on a single server. 

Although you could solve the problem by making sure that the ID has sufficient privileges for the instance, it would be less confusing to use a different script for each instance and run the script as each instance owner. 

The issue here is that you're reading documentation for Db2 on z/OS, but you're running tests on Db2 for Linux, UNIX & Windows. When you perform this particular alter on Db2 for Linux, UNIX and Windows, it will succeed, but the table will be placed immediately into reorg pending state, which you can see in (see the column). will still show as 'N', even though performing a REORG on the table will rebuild the index. 

The easiest way to do this would be to simply uncatalog and then re-catalog the database with a different name (perform this as the instance owner on the database server): 

You can't do this in a single step. The locking required to truncate the table precludes you querying the table at the same time. The best option you would have is to declare a global temporary table (DGTT) and insert the rows you want into it, truncate the source table, and then insert the rows from the DGTT back into the source table. Something like: 

It almost certainly doesn't really matter. The only significant I/O going to files in the database path would be to the recovery history file (and even that isn't much). Considering that DB2 uses the first storage path by default if you do not specify , I would put it on if that was the only choice. My SOP is to keep DBPATH in the instance owner home directory. This is obviously another important location and perhaps makes more sense than the other two locations. 

This is completely normal. Each of these directories are for each unique log chain. If you are familiar with software version control, each log chain is like a branch. A new log chain is created each time you restore a database and roll forward to a point in time other than . Here is why: You have a database, SAMPLE. It has reached the log file S007500.LOG. If you restore the database back to the point in time that corresponds to the log file S007000.LOG, what should DB2 do with the fact that you now have 2 sets of log files with the names S007001.LOG ... S007500.LOG? They represent 2 unique sets of transactions, and they are called log chains. So, when you create a database, DB2 creates the first log chain, C0000001. If you restore the database, it creates C0000002, etc. By preserving these log chains, DB2 gives you the ability to restore each unique series of transactions for a database. 

DB2 returns dates according to the client application's localization settings. If the client application is using the locale, then DB2 will return dates in the standard format for the US (MM/DD/YYYY). If you change your localization settings to something else (for example, ), you'll get dates back in that country's standard format (for France: DD.MM.YYYY). On Linux/UNIX you can see your current localization using the command. Note that DB2 uses to determine your locale. As long as you have the locale available (i.e. check ) you can just set the environment variable before starting your application: 

One last note: All of this configuration (except perhaps the ) can be done using a response file and . has been deprecated, so using response files is the way to handle this going forward. 

When you detach a partition from a table, the detach operation returns quickly, but DB2 then starts a background process to clean up any global indexes. You can't drop the table containing the detached partition until this process completes. So, have you verified that the asynchronous cleanup that occurs after you issue the is actually finished? 

DB2 does not implement the module in its Oracle compatibility code. DB2 does still offer the and UDFs, but I would recommend against these as they use a weak encryption algorithm and require the use of passwords in SQL. So, you would have to implement your own solution (via UDFs) for application-level encryption like this. It might make more sense to implement this in your application code rather than in the database to avoid having to send encryption keys across the wire to the DBMS. This would certainly be a more platform-independent solution.