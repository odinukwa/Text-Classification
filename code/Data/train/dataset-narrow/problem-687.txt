This error is normal. This is what Foreign Key constraints are for. Basically, what they do is make sure that you can't insert invalid data when referencing some other data In this case, you reference a word (As far as I can tell), but you don't have words that have the ID value (empty string), so it doesn't allow you to insert the row with an incorrect reference. Now that the errors are explained, there's two solutions: 1: Allow your column to be null in your table schema. (This is possible even with foreign key constraints, however it might violate your database design). This way, you can insert a value into the column instead of an actual . 2: Fix your data. Basically, you've built your database in such a way that you don't allow an empty or null value for the column, which coincidentally means that your CSV data is invalid in the way that it is an empty string. You need to update the CSV data to have a correct reference to the column for your data to be 'correct' again. The so called 'correct' approach according to how you've set up your database is the second one, where you fix the data. However, both solutions will work. Just make sure to document the changes to your database if you chose solution number 1 because the application which uses the data from your database might not be expecting values and might break because of this. 

after running that you can change the table structure so it does not allow null values anymore. I recommend checking the code to make sure it doesn't check for null instead of empty strings before changing the way the dB behaves 

is not the same as boolean. They are not interchangeable and SQL Server won't implicitly cast to a boolean datatype when needed (SQL Server does not implement the SQL Boolean datatype.). So you would need to use an expression like 

would not use an index seek either even though it is even more straightforward to simplify than the expression. If you want to search on a string name and get a seek on number you would need a mapping table with the names and numbers and join onto it in the query, then the plan might have a seek on the mapping table followed by a correlated seek on with the number returned from the first seek. 

Looking at the definition of from here it appears to be a heap with a non clustered primary key on . You might consider altering this to be a clustered primary key. You will need to then consider issues such as an appropriate to use and frequency with which to defragment but it seems more useful for the joins against your other user table. Alternatively if the only column you ever care about is then you could drop the primary key and replace it with a unique index instead. 

This is far more generally useful as the most common reason for having an column is to allow SQL Server to manage the values. I suppose it could imply that if there was an extra column in the insert source on an table that it should generate an execution plan for the explicit case but not much benefit to this IMO. 

So the CheckPoint actually physically wrote 8 pages (6 single pages and 1 double page) but shows up in Profiler as being responsible for only 4. From the offsets in the Process Monitor screenshot it is possible to see exactly what pages were written to 

This procedure will create a new employee with the code being the first three characters of the name and a count of the employees already existing in the database with that same name padded to 3 characters with a dash in between. We use the 

In the end what I did was select multiple times instead of just once, so for each result that I had I selected the next date_changed to get the period of time in between the two rows and when I had all the time periods I used different selects to get the results i needed. It's messy code, but it works. 

I'm trying to select different types of data depending on a payment method which is a simple intersection table with columns "id" (INT PK) , "customer_id" (INT FK), "payment_method_id" (INT FK) and "date_changed" (DateTime) Now, the customer may request the admin to change the payment method at any time, however I need to keep track of the payment method even if it's no longer in use simply because if I change the payment method halfway through the month I'll get invoiced the first half of the month using payment method 1 and the second half using payment method 2. Now, depending on payment method, I will need to run different queries (using PHP in the backend) however, I need to find out, what payment method to use for a certain period of time. How could I get to know if there is a payment method more recent, and more outdated, oh and I obviously cannot select the payment methods by month since they may be left unchanged for years on end before seeing a change, and since this is about invoicing my data needs to be trackable for 5 years... How could I go about selecting the different data based on the methods I get back and their "date_changed" ? Thanks! 

This isn't the logic that the cost based optimizer uses (which will involve histograms of the two tables and a rowgoal for the ) but it shows this is not a good strategy anyway. You could try the following to cap the number of seeks at 20,000 per iteration and see if you now get your desired plan. 

It needs to validate that the row you are trying to delete is not a parent of an existing row. You don't have an index on . So it must do the scan. 

In practice it seems that SQL Server does allow some additional cases beyond that mentioned in the documentation however. As you show in your question does in fact work so the restrictions on changes to columns used in indexes appear to be the same as those for user created statistics. Moreover the caveat mentioned about Primary Keys seems to be untrue also. The following works fine. 

As you are repeating this query for multiple months then you will be continually re-aggregating the same rows. For example the rows in the first month will always be brought back by the criteria so will be re-processed for every month. To avoid this I'd probably consider working through it in an iterative way a month at a time from the start. Dependent on the volatility of historic data I might also consider storing the pre-calculated results in the database rather than re-calculating these each month. To calculate this at run time you could create a temporary table with the following structure. 

SQL Server and PostgreSQL return 1 row. MySQL and Oracle return zero rows. Which is correct? Or are both equally valid? 

Takes about 7 seconds on my machine. The actual and estimated rows are perfect for all operators in the plan. 

Hi I am trying to create a SQL Server Failover cluster. Windows Cluster is configured and working as expected but now when I try to install SQL Server it throws the following error: 

I have been working on a SSIS package. Package is extracting data from Access Databases , excel sheets and SQL server databases and loading into SQL Server database. Its been working fine but all of a sudden it has started throwing an error, Its not a run-time error, but whenever I open the solution in Visual Studio, the takes a couple of minutes to validate all the package components (which is expected because the package is huge) at the end of the validation it throws the following error: 

After doing a lot more investigation this is what I have found. It fixed the issue (significant performance gain and WRITELOG has an average wait time of 0.0126 which was initially 14.681) Apparently the issue was with the Number of Virtual Log files in my physical log file. There is a job scheduled to rebuild indexes every night, the job creates 36GB of logs, and until few weeks ago someone had add a job to shrink log file on weekly basis. Log file was being shrunk to 500MB. Since it is a very busy server the log file would grow in size and it was set to auto grow by 3 percent. Each time it grew it added more and more VLFs. As a result my 35.5GB log file had 1600 VLFs. To resolve the issue I did the following: 

Finally found the a solution online which lead me to the actual solution of the problem. Apparently it is an issue with SQL Server 2008 and installing the Service Pack is the answer to this. How can we install a service pack while installing the application: See this article by . I tried to install the Service Pack 1 but it failed with a different error messages: 

No SQL Server won't split a single index access into a multiple index access in some sort of attempt to parallelise the work. 

I'm not sure if it is straight forward to get the same plan on 2008 or maybe it would need an on to simulate it. 

You can only use the keyword in place of an expression, not inside an expression. So you would either need to just use the value of the default directly (could be looked up from the system views to be dynamic) or spilt it into two update statements with appropriate mutually exclusive clauses - one using and the second using an expression - if you really want to use that keyword. 

Which has zero impact on existing permissions, and low administrative effort compared with other possible alternatives such as the solution you propose of dropping and recreating all indexes. 

It's not an operator in its own right. It is an output column from the table scan operator on the heap. It is the "bookmark" that contains the physical address of the row (this is the same bookmark as is referred to in the phrase "bookmark lookup"). This is passed along the pipeline into the update operator so it knows the row it should be updating. In SQL Server 2016 you can see the actual values for this column by using the extended event 

Yes, this is possible. Most DDL statements can be rolled back in SQL Server (There are a few exceptions such as ) 

Because it has to compile a plan that will work correctly for both the cases where is NULL and when it is not. In principle it would not be impossible for SQL Server to create such a plan that works in both cases and still uses an index. The dynamic seek mechanism already copes with the case that contains a leading wildcard and so the whole not null part of the index will need to be seeked - but your requirement is not something currently implemented. The above would only be useful in the event that the index covers the query (as your example does). If the index is non covering then it would not be desirable to do a range seek on the whole index and then require lookups for the whole table. 

Checked the Processor and Memory usage on the sql server (nothing is maxed out) plenty of free resources. Index fragmentation was minimum yet rebuilt the indexes and updated statistics. No changes has been made to the code in application (application code/sql server code) hence poor performance because of the poorly written code is unlikely to be the cause of overall poor performance of the application. Finally got Paul Randalâ€™s script . The result of the script shows that sql server has to wait a lot when writing to log file. The biggest wait type is WRITELOG. 

I have sql server 2008 R2 64bit Developer's Edition Installed on my machine. And Microsoft Office 2010 Professional 32bit. I have been trying to import some Excel data from an Excel sheet into a sql server database. I have used the following query to do this: Query 

I have a pretty complicated situation at hand, let me try to explain. I have a SQL Server and a Web Server in a separate domain (lets call it domain A). SQL Server has the databases for Reporting services, The web server has reporting services installed. The SSRS also have SSL certificates installed and we are using https protocol to connect to the reports manager. Now I need to give access to users from domain B to connect to the Reports Manager (report server on Domain A). The users from Domain B cannot have any logins in the Domain A where the reporting services are installed, I know to access the reports manager I need to add domain logins/groups to the reports manager and assign them appropriate roles to access the reports. What options do I have (if any) to give access to users from Domain B to connect to a Reports Manager on Domain A? Important Note: The access to reports server is via NLB with external facing IP, the reports server (web server) or the SQL server does not have any external facing IP 

The transaction appears in both but has 6 additional entries in the version. These are the 6 rows referring to and they have exactly the same pattern as above. 

Dimensions in Microsoft SQL Server Analysis Services support either ROLAP or MOLAP. Open the dimension in BIDS 2008, select the root dimension node 

with no means it is undeterministic which 100 rows from the remote table end up participating in the join. This is execution plan dependant and can vary. If it is a one to many relationship it may be the case that one batch of 100 rows has more matches on the other side of the join than another different batch of 100 rows. You should specify an (inside the derived table) on some unique column or combination of columns to ensure deterministic results. 

Instead of a synonym you could use a view. Views can be the target of an insert and also support thus meaning no gap between the and to worry about. 

You cannot make schema changes when the database is read only but you could put all your user tables on a new file group and mark that as read only. You can expect a modest performance benefit from absence of locking. On versions of SQL Server prior to 2012 statistics can't be auto created or updated on read only databases. Before making it read only you might as well remove all logical fragmentation and make page density as high as possible. Any non default settings will not be of benefit in a read only environment. Additionally create/update any statistics anticipated to be of use for queries if on version < 2012. 

If you connect via the DAC then the plans seem to be neither used from the cache nor saved to the cache without this optimisation side effect of (which may be a practical solution if you are developing against your own instance). Or failing that on 2008 you can at least be more surgical about just removing the specific plan as shown below.