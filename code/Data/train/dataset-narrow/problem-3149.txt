Inference in the face of uncertainty, incompleteness, contradictions, ambiguity, imprecision, ignorance, etc.: 

Your question is very broad, and therefore I won't offer very specific answers. You asked for "time-saving tips" but there are many and they depend on context. Instead, I'll offer a general set of heuristics that are useful most of the time. 

You could use Topic Modeling as described in this paper: $URL$ They performed Topic Modeling on abstracts of patents (limited to 150 words). They identified papers as "novel" if they were the first to introduce a topic, and measured degree of novelty by how many papers in the following year used the same topic. (Read the paper for details). I suggest that you follow their lead and only process paper abstracts. Processing the body of each paper might reveal some novelty that the abstract does not, but you also run the risk of having much more noise in your topic model (i.e. extraneous topics, extraneous words). While you say that all 500 papers are on the same "topic", it's probably safer to say that they are all on the same "theme" or in the same "sub-category" of Bio-medicine. Topic modeling permits decomposition of the "theme" into "topics". The good news is that there are plenty of good packages/libraries for Topic Modeling. You still have to do preprocessing, but you don't have to code the algorithms yourself. See this page for many resources: $URL$ 

Here's an approach I didn't see mentioned: separate the process into two steps: the first step focused on encoding names so that alternative versions of the same name are encoded the same (or nearly the same), and the second step focused on making them anonymous. For the first step, you could use one of the Phonetic Algorithms (Soundex and variants), applied to first name, last name, and initials in various orders. (See this article, also). It's in this step where you resolve similarities vs. differences in names to balance false positives from false negatives. For the second step, you can pick any hashing or cryptographic method you like, without concern for how that method affects name matching. This gives you freedom to use a method that has the best characteristics for both performance, robustness, and anonymity. 

You may or may not already know this, but here are a few basics about how R and Rstudio work and use resources. Rstudio is a graphical user interface to R, not the interpeter/runtime environment. There is a separate "R session" that actually executes your R programs and returns results for Rstudio to display. Therefore, having Rstudio use more memory won't make any difference in the execution speed of your program. Second, memory allocation and cleanup (a.k.a. "garbage collection") is handled automatically by the R runtime environment. "R holds objects it is using in virtual memory". Virtual memory is a combination of physical main memory and secondary storage, the mixture determined by the server OS configuration. You can use command line to change the amount of virtual memory allocated to processes. (Consult with your favorite Linux expert on this.) Third, your speed of your program may or may not be limited by memory speed. You may be compute-bound. Do some testing to find out what is constraining performance. Fourth, you should first ask yourself whether your program implements an efficient algorithm. Even if you aren't programming with loops and branches, the functions you call may use them, and maybe not efficiently for your application. For example, there are dramatic performance gains to be had by switching to from . Fifth, once you have chosen an efficient algorithm, you can put effort into parallel execution. The simplest way to do this is by using functions that automatically vectorize operations. A bit more complicated is to recode your program use the packages and . With , you can specify the number of CPU cores to use, which on a server may range from 32 to 64 or more. Finally, if your server has a Graphics Processing Unit (GPU), it's possible for some algorithms to reprogram using the GPU commands and get massive parallelism. This option takes the most effort and has the most constraints. 

It is not exactly true that "the strength of an encryption scheme is measured by the randomness (unpredictability, or entropy) of the output". Instead, strength of an encryption scheme is how resistant it is to cryptanalysis. Yes, input $\rightarrow$ output maps that show patterns are relatively easy to break. But the input $\rightarrow$ output map might be very random (high entropy) from many perspectives but still might be defeated through feasible cryptanalysis. You offer another definition: "informally, the strength of an encryption scheme is determined by the number of such input-output pairs needed beyond which it becomes predictable." This is usefully different from your first definition because it focuses on the information that might be gleaned about the encryption scheme from a stream of input $\rightarrow$ output pairs. This is different and easier than the usual cryptanalysis problem where all you have is a data set of (encrypted) outputs. Framed this way, I think you are heading toward a model selection problem. While this may be relatively straight forward with weak encryption schemes, it becomes very hard, very quickly with strong schemes. In fact, I believe you walk right in to the No Free Lunch theorem: 

Here's a crazy idea: talk to the volunteers who know the neighborhoods and who have done door-to-door work before. Get their advice and ideas. They will probably have insights that no algorithm will produce, and those modifications will be valuable to any computer-generated route list. One example: Avoiding crossing heavily traveled streets with slow lights or no lights. Another example: pairs of volunteers working on opposite sides of the same street will feel safer than a volunteer working that street alone. 

Translated: if you find yourself needing to search over all possible cryptanalysis methods, there exist input $\rightarrow$ output streams where no method is probabilistically better than any other. This will defeat any model selection method based on optimization of some objective function. 

You should probably do some reading in the field of "Natural Language Generation", since this seems to relate most directly to your question. But the way you have described the process -- "text mining...text building" -- leads me to wonder if you are aiming for something much more ambitious. It seems as though you aim to automate the process of 1) reading natural language texts, 2) understanding the meaning, and then 3) generate new texts based on that semantic knowledge. I'm not aware of any general-purpose end-to-end systems that can do that, not even specialized systems by the likes of Palantir. What you are aiming for would probably pass the Turing Test for fully capable Artificial Intelligence. 

From a data science point of view, there is nothing very special or unique about this problem. You have three independent variables and one dependent variable. Regression, clustering, and classification methods can be applied. 

"Granularity" refers to the resolution of the variables under analysis. If you are analyzing height of people, you could use course-grained variables that have only a few possible values -- e.g. "above-average, average, below-average" -- or a fine-grained variable, with many or an infinite number of values -- e.g. integer values or real number values. A measure is "fuzzy" if the distinction between alternative values is not crisp. In the course-grained variable for height, a "crisp" measure would mean that any given individual could only be assigned one value -- e.g. a tall-ish person is either "above-average", or "average". In contrast, a "fuzzy" measure allows for degrees of membership for each value, with "membership" taking values from 0 to 1.0. Thus, a tall-ish person could be a value of "0.5 above-average", "0.5 average", "0.0 below-average". Finally, a measure is "rough" when two values are given: upper and lower bounds as an estimate of the "crisp" measure. In our example of a tall-ish person, the rough measure would be {UPPER = above-average, LOWER = average}. Why use granular, fuzzy, or rough measures at all, you might ask? Why not measure everything in nice, precise real numbers? Because many real-world phenomena don't have a good, reliable intrinsic measure and measurement procedure that results in a real number. If you ask married couples to rate the quality of their marriage on a scale from 1 to 10, or 1.00 to 10.00, they might give you a number (or range of numbers), but how reliable are those reports? Using a course-grained measure (e.g. "happy", "neutral/mixed", "unhappy"), or fuzzy measure, or rough measure can be more reliable and more credible in your analysis. Generally, it's much better to use rough/crude measures well than to use precise/fine-grained measures poorly. 

It is proper to label this a "funnel" because a subset of each stage move on to the next stage. Not all "disaffected customers" become "infrequent customers", and so on. There are two keys to defining a defection funnel. First, define the behavioral characteristics for the funnel as a whole, and for each stage (category) within the funnel. In your case, it might be a variation of the familiar RFM ("recency" "frequency" and "monetary") score. Second, you need to identify other behavioral characteristics or patterns that show a propensity to move from one stage to another. These might include: complaints, being victim of trolling, experiencing service interruptions, experiencing betting losses, how long they have been using the service, or maybe it is just demographics. With these definitions in place, you are in a position to build a predictive model and also track over time the population of customers in each category. 

To use your example, each range is a condition: A) 10 <= x <= 22; and B) x < 10. If A is true, then it weighs in favor of certain similarity (distance) scores. If B is true, then it weights in favor of other similarity (distance) scores, but would have less weight compared to other evidence. 

I like Amir Ali Akbari's suggestions, and I'll add a few of my own, focusing on topics and skills that are not adequately covered in most machine learning and data analysis books that focus on math and/or programming. Data Cleaning: 

For specific set of randomized data sets that are 'hard' for these methods (e.r. linearly inseparable n-classes XOR patterns), see this blog post (incl. R code): $URL$ 

This setting is common in reliability, health care, and mortality. The statistical analysis method is called Survival Analysis. All users are coded according to their start date (or week or month). You use the empirical data to estimate the survival function, which is the probability that the time of defection is later than some specified time t. Your baseline model will estimate survival function for all users. Then you can do more sophisticated modeling to estimate what factors or behaviors might predict defection (churn), given your baseline survival function. Basically, any model that is predictive will yield a survival probability that is significantly lower than the baseline. 

By the way, this approach is not dependent on dividing the funnel into "stages", nor does it matter fundamentally how many stages you have. However, defining discrete stages has many practical benefits, including communicating your model and results to other people who are not quantitative or statistically minded. 

Any data consisting of a three element vector of numbers $v =\{x,y,z\}$ could be viewed as topologically spherical if, after converting to polar coordinates, there is only one datum for every value of the 3D angle $\theta$. Thus, every point in the data represents an exterior of a topological sphere. This is taking the term "topological" literally and formally. There are also graph structures (acyclic) that have neighborhood structures that allows the graph to be mapped topologically to a sphere. For example, all graphs that can be mapped to Platonic Solids (i.e. vertexes to vertexes, edges to edges, 1:1) are topological spheres. This can include irregular neighborhood structures, too. As far as practical examples of these, none come to mind immediately. 

Reflexivity - refers to circular relationships between cause and effect. In particular, you could use the definition of the term adopted by George Soros to refer to reverse causal loop between share prices (i.e. present value of fundamentals) and business fundamentals. In a way, the share price is a "model" of the fundamental business processes. Usually, people assume that causality is one-way, from fundamentals to share price. Performativity - As used by Donald MacKenzie (e.g. here), many economic models are not "cameras" -- taking pictures of economic reality -- but in fact are "engines" -- an integral part of the construction of economic reality. He has a book of that title: An Engine, Not a Camera. Self-fulfilling Prophecy - a prediction that directly or indirectly causes itself to become true, by the very terms of the prophecy itself, due to positive feedback between belief and behavior. This is the broadest term, and least specific to the situation you describe. 

Another approach would be to model "churn" (aka "diminished use of the service, including non-use") as a process and not an event. Years ago in retention marketing this was called a "defection funnel", to mirror the "sales funnel" on the customer acquisition side (suspect -> prospect -> trial customer -> repeat customer -> loyal customer). So a defection funnel might look like this: 

There's another approach which involves attempting to identify precursor events patterns or user behavior pattern that foreshadow defection. Any given event/behavior pattern might occur for users that defect, or for users that stay. For this analysis, you may need to censor your data to only include users that have been members for some minimum period of time. The minimum time period can be estimated using your estimate of survival function, or even simple histogram analysis of the distribution of membership period for users who have defected. 

I'll offer a slightly different perspective. While you can help many needy humans -- e.g. individual refugees, poor people, sick people, war-torn people -- to help humanity it's necessary to address root causes: i.e. why are there so many refugees, persistent poverty, persistent (preventable) illness, and chronic violence. The root causes of so many of these are in the domain of the social sciences. So far, data science has only contributed to root cause analysis when it operates within a solid research program, integrating with other types of research and findings. (This research may or may not take place in an academic environment, or maybe in a mixture of environments.) It also places premium value on domain knowledge on the part of the data scientists involved. A related theme is to empower "change agents" (a.k.a. social entrepreneurs) with data science and related services. Nearly all of them are under-resourced and often face severe or menacing opposition. To do this, you will need to make partnerships with some of them to understand their needs and their world.