I recommend you store the primary details including the English names in simpler tables and then each table that has other language equivalents can have a language specific table containing every other language. Something like this: Languages    ID    Name Drugs    ID    Price    Name    Interactions    Mechanism    Uses DrugLanguages    DrugID    LanguageID    Name    Interactions    Mechanism    Uses Companies    ID    Name    Address    Activity CompanyLanguages    CompanyID    LanguageID    Name    Address    Activity You should know that I have never done multilingual design, so my approach may have limitations that I do not fathom. 

My question is what do these row represent? They have file#, block#, and objd columns, but the objd values do not exist in dba_objects. I dumped one of the blocks and it appeared to contain an index, but I couldn't tell anything else about it. There are plenty of rows in v$bh that do match indexes in dba_objects, so the unmatched v$bh rows can't be unmatched just because they are blocks of indexes. Something else must be going on. There are more unmatched objects on one of my systems, but they all have unmatched objects including the 11.2.0.2.6 system. Update: For the system with the most unmatched objects, the majority can be matched when comparing the bh.objd with the dataobj# field from obj$. I'm not sure why these objects aren't reflected in dba_objects, but perhaps an examination of the view (or a separate question) will answer that. For the remaining 391 unidentified entries, here is some other information of interest. 

To add just a bit to the correct answer from Vérace (+1), you should go further than separating different types into different columns, you should also define what your columns are and use them appropriately. This is called design and it is a necessary step for an understandable, scalable, good performing system. Just to hit on the understandable angle, compare this: 

To return all agents with their commissions and commission percentages use an analytic function with no analytic clause so that the partition is over the whole table: 

Doing an export of a number of users causes high CPU and I/O usage. I can mitigate the effects of the CPU usage using Resource Manager and setting the export session to a low priority group, but how can I limit the disk I/O? I am not running the operation in parallel and have already read Oracle's Datapump Performance documentation. Is there anything else I can do? This is on Windows 2008 R2 x64. 

30-90 minutes according to Oracle's Best Practices for Upgrading. This is about the closest estimate you will get given all the unknowns in this situation. The size of the database really matters very little in determining how long the upgrade will take. Here are the main factors effecting the duration (from the Oracle.com upgrade blog): 

As of 11.2 there is no support for nested s without the use of an block and the inner type being an SQL type (see this). As far as I can tell, your options are these: 

The differences between the SQL dialects is too great in the more specialized areas you listed to simplify the problem down to a “rule of thumb”. In these situations you probably should convert the entire statement for each database. By doing this each statement can be optimized for each platform taking advantage of the special features each database can provide. Writing SQL to lowest common denominator (ANSI) may simplify migration, but at the cost of performance. SQL Dialects Reference gives some useful comparisons of the SQL variants. For more information I recommend searching for the specific dialects you are switching between like this:     

Is there a way to tell the command to use a different location with more available disk space? When I run the command in RMAN, it fails with the following error stack: 

I want the product of the c1 column from all the rows. Something like the results of SUM(c1) only I want each value multiplied by they others rather than added. In this case that would be 2 * 1 * 4 * 1 = 8. 

I haven't found definitive information, but the things I have read seem to indicate that since scheduler jobs do not have a login, they cannot use database links that use passed credentials. That would make sense since I get the same error whether the job is owned by SYSTEM or by the user that normally runs the block (and still can when logged in). Can anyone provide further explanation of this situation or more importantly explain ways to work around this issue? I can think of several, but I'm not sure I like any of them. 

Your mileage will vary depending on quite a few factors, but in general an index on myid1 would probably be beneficial. If you haven't already you should read through the Oracle Concepts Guide (pdf) particularly the section on indexing. 

You should also assign permissions and create a django tablespace setting it as the default tablespace for django. 

Yes, you should test the whole chain of events as a unit. So, in your example with a procedure that inserts into a table and causes several triggers to fire, you should write unit tests that evaluate the procedure for various inputs. Each unit test should pass or fail depending on whether it returns the correct values, changes the state of tables correctly, creates the correct email, and even sends the correct network packets if it is designed to do such a thing. In short every effect the unit has should be verified. You are correct, that designing unit tests takes some work, but most of that work has to be done to manually test the unit, you are just saving the work required to test the unit so that when a change is made in the future the testing can be just as thorough and significantly easier. Changing data does make testing more difficult, but it doesn’t make testing less important and actually increases the value of unit testing as most the difficulties only have to be thought through once rather than every time a change is made to the unit. Saved datasets, inserts/updates/deletes that are part of the setup/teardown, and narrowly scoped operation can all be used to make this easier. Since the question isn’t database specific, details will vary. There is no complexity threshold on the high or low end that should stop you from testing or unit testing. Consider these questions: 

The difference is that and can be used in a PL/SQL block, while cannot. The following will work in SQL*Plus: 

Your desire to make a generic solution for all part number generators comes at the expense of making a good one for the domain you are in. VALUES and VALUESET tables could be used to represent almost anything. The components of your part numbers (Fabric Backer, Stitching Type, Stitching Color, etc.) are good candidates for well-defined database tables and the resulting part numbers are also good candidates for being stored in the database. The part number generation itself, however, should probably be in code. This code can (should) be in the database and may come in the form of one or more functions depending on how many different types of part numbers need to be generated. Some of the difficulty in answering your question is that we don’t know the scope of your part numbers. If your part numbers will pull from mostly the same sets of data, then you might even consider referencing the source tables in the table with the generated part numbers. Not only would this enforce valid data, it would also allow joining to see things such as part numbers and their Fabric Color descriptions. The part number itself could then even be a computed column in the table itself. 

Here is another answer that works for nine rows, but not for more. I'm posting it in case it sparks an idea. 

This is a very good question and not just because it shows effort on your part to answer the question. Thank you for asking it and please do not remove it. The answer as you have already discovered is -- You can't do that. You have correctly identified several similar concepts including... 

If a smaller value will work for 98% of the cases, but it takes a Varchar2(4000) to work for 100% of the cases, then you have little choice but to use the larger value. Creating a separate table for 2% of the values and then coordinating inserts/selects etc. would add complexity that would obliterate any memory or performance benefits from not extending the field. 

*Do we have an auto mode for RESULT_CACHE_MODE?* - The Oracle Database Reference shows that the two options for this setting are MANUAL and FORCE. What is the recommended mode for this? - This would depend on what your needs are. To understand the benefits of the result cache, look at the Validation of the Client Result Cache of the Oracle Call Interface Programmer's Guide referenced earlier. 

I am aware that the can be changed and/or turned off, but both would require modifications to the code, the former moves the problem without resolving it and neither resolves the embedded slash issue. The best answer would be a way to allow these statements to run without modification by changing the environment in some way (as sqlblanklines does). If that isn't possible, then perhaps there is a way to programmatically modify the scripts. I am trying to avoid manual changes. 

Based on your description it seems like TRANSLATE can do the work for you. As Jeffrey Kemp suggests, a function based index could be created for this. Setup: 

I don't have the 10g version of Express Edition installed, but if it is like the 11g version, then your first question is answered in the comments section of the Backup.bat file itself. For 11g this means the comments indicate that No cleanup script is needed because the script automatically keeps two backups in the Flash Recovery Area. Whether there is a better method or not will depend on what your recovery requirements are. 

"Null Value Logic" according to $URL$ but most of the other references I have found support Phil's "Null Value" supposition. I haven't found a definitive origin. 

We have used Schema Version Control for our 11g databases, but have had some problems with the software on 11.2. If it weren't for those problems which we are still working through, it would be a great product. 

So, you might see sightly degraded performance if you have a lot of indexes and a lot of DML taking place, but I suspect this is insufficient to cause the appearance of hanging. I suggest you trace a session to see what is slowing the search down. 

Here is what the function might look like if it were done in PL/SQL. It was just thrown together and is only meant to illustrate building the SQL statement and is not meant to be an example of good coding.