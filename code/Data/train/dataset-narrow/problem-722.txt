In addition to MaxVernon's answer (hitting the nail on the head, +1 !!!), you could also consider using tombstone tables to monitor which rows can be soft deleted. See me post from Tombstone Table vs Deleted Flag in database syncronization & soft-delete scenarios. Please consider which way you can live with 

The same page says that MySQL does not warn that an index is irrelevant and switches to a table scan in the case of USE INDEX. Thus, FORCE INDEX can take the MySQL Query Optimizer out of the equation before using the index. Any query using FORCE INDEX properly will reduce I/O. Why do I say properly? Think about it. If the index you choose to navigate through is a covering index and you only need the columns as listed in the covering index, contacting the table for data becomes unnecessary. All I/O is restricted to index pages. All the data requested is retrieved by performing index scans in the worst case. That is indeed a good thing if the requested data needs to be ordered, thus bypass any requested sorting. In terms of "rules of engagements", FORCE INDEX should only be used when 

your auto_increment column was leftmost. You made reference to a second post. Please note that in the answer to that second post, it clearly notes that MyISAM and BDB storage engines support auto_increment ids. When it comes to MyISAM, the auto_increment behavior allows an auto_increment number to be paired with one or more keys in such a way that the number 1 can appear multiple times but be associated uniquely with the columns. I have discussed this a few times over the years: 

UPDATE 2016-11-25 14:49 EDT With Windows, it's a little more involved. Assuming you installed MySQL in Windows using the MSI Installer, there is no . Here is where it gets crazy, but here we go ... STEP 01 Login to the Windows Command Line as Administrator and run 

Therefore, at its very worst, MySQL would consume only 9.58% of RAM. ASPECT #2 You did not mention if the Server is a dedicated DB Server or if you are running a full stack. If you are running a full stack, please lower any memory-sensitive settings in the other parts of the stack (Varnish (more like Vanish), PHP, Munin, Nagios, Tomcat, Hibernate, etc). ASPECT #3 Looking at the first crash around 3:36 AM I see 

Any gotchas, pitfalls, traps with this approach ? GOTCHA #1 : You cannot do the following on the Master 

Move the DATA_ONE.sql file over to the new server and execute the script. That's it. Pure SQL solution. No PHP. If anything does not work, it is probably because I do not know the layout and current contents of the personal, account, & games tables and made assumptions on data. Please post the following in your question: 

Give it a Try !!! CAVEAT : Sorry, it's late and I was not in the mood to do a Stored Procedure !!! If you are interested why this works, this query let's look at the details: 

You can immediately logout and come back later. You could then poll the run log file to see when the mysqldump transfer finished. It will be finished when there are two lines: start datetime and end datetime. 

is actually a little misleading. In the MySQL Documentation, the legal characters for wildcards are , and . (If you want to interpret a literal underscore). The asterisk character is not listed. The above filter is actually looking for a table called . Therefore, you have two options OPTION #1 Correct the filter in to have this 

Give it a Try (Hope it works) !!! CAVEAT : Hopefully, there is some TokuDB Doc you can look up at the Tokutek site. According to one of the Docs, it says: 

There are two problems PROBLEM #1 The needs to be prefixed with PROBLEM #2 It needs to be a , not because you cannot change values in an trigger. Your code should read 

IMHO the second option sounds more viable but you may still want to know individually what each column position has for a value. Here is a sample table 

Move this file over to the MySQL 5.5. machine. Before you load MySQL 5.5, setup replication but do not run . Load the script and it will stop the slave, load the data, and start the slave. 

Step 05) This will take awhile (5 min) because mysqld will create ib_logfile0, format it, create ib_logfile1, format it. Step 06) Login to mysql and load the conversion script 

I used Google Chart a few times and got away from it. It does work, but you have a lot of coding to store the absolute values and calculate deltas for specific variables. Then, you script the URL from the values and render the URL in a browser. GIVE IT A TRY, HERO !!! 

If nothing appears, then you do not have the RELOAD privilege. Consequently, you have no choice but to contact the system administrators to run for you. ALTERNATIVE SUGGESTION Ask the system administrators to configure max_connect_errors in 

These particular values may be too small for your DB Connection to fulfill the query efficiently. These can be set within as follows: 

The second query would make the temp table from the subquery much larger and would take longer to scan for the outside WHERE clause. Go with the first query. Give it a Try !!! 

SOLUTION : Serialize UPDATEs MECHANISM : Would you believe MySQL Replication ? That's right, I said MySQL Replication. You are probably saying right now 

IDEA #1 : USE THE SLOW LOG !!! The Slow Log ??? Yea, I know what I said : USE THE SLOW LOG !!! How can you ??? With the slow log being written to a text file, the header info for each slow log entry has the user@host right on the first line of each entry (ignore the line since one or more slow log entries can be saves within a single time window) For example, the post Mysql slow query log always include "# Time:" has this example slow log: 

Perhaps this is being blocked for users that have % in the host column of mysql.user. You may need to create another user with a hard public IP as I suggested earlier 

DISCLAIMER : Not a User of PhpMyAdmin I can explain what mysqldump does. When you run mysqldump, only data is dumped in SQL statements. Indexed are not copied. Indexes get rebuilt when the SQL is loaded into another server. Here is an example I have a query I run to show index and data usage. Here is that query and its output 

UPDATE 2013-04-12 14:25 EDT I just realized you are running the Windows version of MySQL. What was I thinking ? As point by @yercube's comment, is not available for Windows. Here is the MySQL Documentation on it: 

The SQL Function Syntax between MySQL and PostgreSQL will never converge as you wish UNLESS you are willing to take a chance on some weird way to emulate the IF() function in a convoluted manner: WRITE A STORED FUNCTION !!! The IF conditional function could be called MyIF and do the following: 

You can remove the corresponding files as well. Those are the index file components of teach temporary MyISAM tables. They are never populated because temp tables are never indexed. 

The amount of data you want could be fast to send over a wire, but what is mysql doing to process it and prepare it for transamission ??? Let's first look at the original query 

For more clarification, please read Recover MongoDB Data following Unexpected Shutdown. You may need to run on your data collections to verify integrity. You should also read about Journaling SUGGESTIONS 

EPILOGUE Given your initial idea, you could change the station_arr_id and station_dep_id into a single route_id and store the routes in a route table 

My first guess would be look at the and in . If they have been set to size different from the default, that probably will prevent mysqld's startup. The default for innodb_log_file_size is 5M. SUGGESTION #1 If RoundCube's installation overwrote the , see if you have a physical backup of the previous . Put that back in place and run 

TECHNIQUE #2 : Set the Barracuda Option Before Loading the Dump Login to mysql as and run this from the mysql prompt 

One of the silent killers of MySQL Connections is the MySQL Packet. According to the MySQL Documentation 

I suggest this order because in the query is a constant (35514335). That would make looking up all values. SUGGESTION #2 : Redesign the Query From the looks of the query, you are retrieving first date_sold for for 35514335. 

That being said, you are probably wondering, what privileges are granted for ? Look carefully at the command. You are granting all privileges to the user for the database. Where are database-level grants stored ? That's right, you guessed it. It's in the table , which looks like this: 

IDEA #2 : Poll the Processlist The problem with using either the slow log or the general log is that entry will not entry the log until the action is done. In other words, if a query takes 10 min to run, the entry will not land in the log file until the command is done executing 10 minutes after it started. Wouldn't it be nice to catch the bad queries in the act ??? You should use pt-query-digest. Even with queries that are not done, a histogram will be generated with the pattern of queries that have executed or are still in progress. Rather than write up an example, please see my posts where I discuss using pt-query-digest 

Here is where the meeting of the minds, that is to say, the minds of Developers (DVs) and DBAs, must inevitably happen. Working with Business Logic (BL) and storing such in a database can have an impact that can either glorify or horrify its implementation. For some RDBMS products, there exists superior libraries/tools/APIs for Business Logic and Object Infrastructures one could quickly learn and employ in their applications. For other RDBMS, no libraries/tools/APIs exist. In the past, client server apps made the bridge into BL via Stored Procedures (SP). For products such as Oracle and SQL Server, this was done early. As open source databases such as PostgreSQL and MySQL came into being, those using them were at risk of breaking new ground with stored procedures in BL. PostgreSQL matured very quickly in this, since not only stored procedures were implemented but also the ability to craft customer languages also came along. MySQL basically stopped evolving in the world of stored procedures and came in a stripped-down form of a language with many restrictions. Thus, when it comes to BL, you are completely at the mercy of MySQL and its Stored Procedure language. There only really remains one question: Regardless of the RDBMS, should BL resides in whole or in part in the database ? Think of the Developer. When things go awry in an application, the debug process will have the Developer hop in and out of a database to follow data chanages that may or may not be correct intermittently. It is like coding a C++ application and calling Assembler code in the middle. You have to switch from source code, classes and structs to interrupts, registers and offsets AND BACK !!! This taking debugging to that same level. Developers may be able to craft a high speed method of executing BL in conjunction with language configurations (compiler flags for C++, different settings for PHP/Python, etc) via business objects sitting in memory rather than in a database. Some have tried to bridge this ideology for faster runnng code into the database by writing libraries where debugging Stored Procedures and Triggers is well integrated in the Database and seemlessly useable. Thus, the Developer is challenged to develop, debug, and maintain source code and BL in two langauges. Now think of the DBA. The DBA wants to keep the Database lean and mean as much as possible in the realm of stored procedures. The DBA may see BL as something external to the Database. Yet, when SQL calls for the data needed for BL, the SQL needs be lean and mean. Now, for the meeting of the minds !!! Developer codes SP and uses iteractive methods. DBA looks at the SP. DBA determines that a single SQL statement can replace iteractive methods written by the Developer. Developer sees that the SQL statement suggested by the DBA requires calling other BL-related code or SQL that does not follow normal execution plans of the SQL statement. In light of this, the configuration, performance tuning, and SP coding becomes a function of the depth and data-intensiveness of BL for data retrieval. The more depth and data-intensiveness the BL, the more Developers and DBA must be on the same page for the amount of data and processing power given to the Database. CONCLUSION The manner of data retrieval should be always involving both Developer and DBA camps. Concessions must always be made as to what coding methods and data retrieval paradigms can work together, for both speed and efficiency. If the preparation of data for source code to handle is done only one time before the code gets the data, the DBA should dictate the use of the lean and mean SQL. If the BL is something the DBA is not in tune with, the reins are then in the hands of the Developer. This is why the DBA should see himself/herself and part of the project team and not an island unto himself/herself, while the Developer must let DBA do the fine tuning of the SQL if it does warrant it.