You will design your data model, logical model, using these tools. After that you will export your physical model to different databases. Something like below. 

I would like to use schemas to separate database according to modules and usage patterns. I find that it makes it very easy to understand database tables therefore maintenance becomes easier. I had following schema types in my last sql server project. LT -- Lookup Tables 

I would like to find computed column list in oracle database using Oracle Data Dictionary Views. I would like to add more information. Suppose I have following computed/calculated column in database. 

If your file is created but it is empty. You may still have permission issues . Try to use some file monitor program to see what SQL Server (Agent) is doing in your machine and in your remote machine. Process Monitor is a good utility for this. You can filter file operations and see what your package is doing. You should add this information to your question so that we can help you more. Also you may try to run your package with different credentials to see if your problem is really about permission issues. A handy utility about this is ShellRunAs. 

According to comments. You can connect oracle using easy connect but hr (human resources) account does not exists. Therefore try to use 

We are using 4 node Oracle 10GR2 RAC in Windows x64. 3 instances of our database crashed today and later started normally. I started them by connecting (Remote Desktop) to individual instances and executing startup command in sqlplus. I can connect to database using SYS account but when I try to connect using application user account, I get following error: 

My backup is normal rman backup pieces not backup as copy. I am trying to rename nonexistent files. After this rename process , I will try to restore database from rman backup. But this backup is not piece by piece copy. 

This specific expert system will hold substances and patients. Every rule give an advice according to patient's attributes , which is stored in database. For example: 

$URL$ Third is Directory Server, LDAP Server like Microsoft Active Directory. Here you provide a key and directory server gives you back connection string. You use this connection string to connect to oracle. Fourth one is same as Directory server , you only use other network services. As explained in oracle documentation. "Configuring External Naming Methods External naming refers to the method of resolving a net service name, stored in a non-Oracle naming service, to a network address." According to NIS wikipedia: "Network Information Service, or NIS is a clientâ€“server directory service protocol for distributing system configuration data ...over time other modern and secure distributed directory systems, most notably LDAP, came to replace it." As you can see, External Naming is same as Directory Server, only uses a different server technology (older) than LDAP. $URL$ These are four connection methods. I do think other than where do they get connection string information, they differ. Therefore easy connect should not have less performance than local naming. 

I would like to find given schema, table and column find that if this column is calculated/computed? Wrongly I thought that INDEXES view gives me this information. But following select returns no rows. This gives only functional indexes. 

I would like to offer also INFORMATION_SCHEMA views. These are ANSI Standard and works cross database ,for databases which support them. 

I don't know for sure in mysql but I see a bunch of things that casue performance problems in SQL server. First, correlated subquereis are often performance killers as teh run row by agonizing row insted of in sets. Next, do not use a wildcard as the first character in a where clasue as it prevents index usage. Either your table is incorrectly designed that you need to do this (such as when you store a comma delimited list) or you need to start using some type of full text search instead. Frankly there is no reason why users can't put in at least the first charcters of the email or username, so the wildcard is probably not necessary at all. Look at your requirements and be sure you are not goldplating and harming the system inthe process. Next, no production code should ever use select * espcially when there is a join. You have repeated the join field twice by doing this which is wasteful of server reources and network resources. And if you are not actually using all of the other columns that is wasteful too. Further, I know in SQl server there is a performance hit while it looks up the column names and this may be true for myssql. At any rate it is a SQl antipattern just like using implicit joins is a SQl antipattern. It is also dangerous for maintenance as things which are added to the tables should not automactially be added to the query. You could end up showing things in the wrong place (if some person rearranges the columns) or returing data that you do not use or even showing some fields that you don't want the user to see. This is a very poor practice. 

Is it possible to get the file as a .csv file which can be imported like a text file and will not have this problem? I generally kick back all Excel files and ask for another format because SSIS and Excel do not play well together on many levels. 

Alternative approach if you do not need to disguise the data for privacy reasons. All database changes and values for lookup tables should be in source control. We simply restore the last prod backup and then use source control scripts to add whatever dev changes haven't made it to Prod yet. If you have multiple databases, all should be refreshed at the same time. One advantage of this is that you have roughly the same number of records as prod which makes a huge differnce in testing your database code for performance. It also helps keep your devs using source control for changes and helps practice deployments before you move them to other environments. 

I think any of the major databases can handle the load if designed well. Sadly I would estimate that less than 1% of all databases are designed well. (I have personally dealt with data from literally thousands of different databases performing a wide variety of functions, so I think I have a good idea of the lack of quality that is out there inteh real world.) I would strongly suggest that you get some books on performance tuning for the database you choose and read them thorughly before begining to design. There are many things that will help your database perform better that should be designed in from the start. Just knowing how to write performant queries and design indexes is critical to getting a good design. This type of study and designed-in performance is not premature optimization. There is no reason at all to use known performance killing techniques in the design. Databases need to be designed for performance from the start. 

You create clustered index on only one column. Technically, you can do it on multiple, but the main goal to have it as short as possible. If you created clustered index on you DO NOT NEED any other indexes on that column. I can guess that your performance improved because before there was no Clustered index and indexes were so bad that SQL decided to do full table scan instead. Suggestion: read a book about Indexes, their differences and how they work. 

You did not get the point of SQL programming. You can have 1000 copies on multiple machines of a script to create/alter an object, but until one of those scripts is executed against a server, that object on that server will be unchanged. Yes, you can have multiple tabs/windows with the same procedure, but for SQL Server all of those are different/competing sessions and whoever is last - wins.On another hand: SSMS. You can have multiple copies of a script of the same object in different tabs, which is very handy - you can try different scenarios just by switching tabs. It is not job of SSMS to track your changes and keep records which script is a "Master Copy". That is YOUR responsibility. However, if you are in a project development, you and your boss can't completely rely only on your discipline. Project manager have to choose which way to do a source control. Here is a not full list of things, which can be done: - Implement Change Data Capture on the server; - Restrict permissions on Dev/Test and allow "playing" only on Dev or personal machine; - Use change tracking/deployment/source control tools, which will keep records of any changes. And do not be surprised with SQL. All developers are experiencing that issue with source control. See it here: $URL$ 

Items with big values in and are red flags, such as Table scans and Key Lookups. There can be because of bad indexing, statistics, etc. Wild guess: when you use Temp Table Variable SQL comes up with better query plan. 

Might happen you still have some opened transactions, which hold you from shrinking individual files. See who hols them, close them. Reboot server if necessary. Shrink individual files while nobody accessing the database. switch it temporarily to Single User if necessary: 

Your provides 2 sets of the data: - DB Size + Unallocated space - These numbers include BOTH: Data and log file; - Total statistics of RESERVED space for all objects within the database; I bet your 36 GB of free space are in the Log file. For real numbers use following query: 

So, the question is: How it is possible that procedure created with an option can be successfully executed while SQL Server login does not exist for that user? 

Finally found an answer: You can create database user, which won't have a SQL Login and won't be able to authenticate, but will have permissions and can be specified in clause for stored procedures and functions. The simple way to create such a user is just to use clause during user creation: 

In your sample case SQL Server made very good choice. Here is what it was actually doing: - In the first set of data it extracts 10 rows from table based on the column. Then it extracts 10 corresponding IDs from table using kind of operation for each ID. - In the second scenario, when there are 100 matching rows in table SQL decided not to do operation, but use instead, which is much cheaper from the I/O perspective. Yes, you can use hints for your queries ($URL$ 

When you are building complex queries, you should build them in stages checking the results as you go, not build one whole huge query and then try to figure out what is wrong. Here is what I do. First I list all the columns I want on a spearate line and comment out all the columns except those in the first table. Then I add the from clause and any where conditions on the first table. I note how many records are returned. Then I add each table one at a time and check the results again. I am especially concerned when the number jumps up really high unexpectedly or goes down unexpectedly. In the case of the number jumping up high, you may have a one to many relationship that needs further definition. This is especially true if the field or fields you are getting from the table are almost always the same. You may need a derived table or a specific where condition to resolve. You might even want to do some aggregation. Now I'm not saying it always bad if the record counts go up, only if they go up when you didn't expect them to or when the result set looks suspicious. In the second case, you generally have an inner join where you need a left join. The number of records went down because you did not have a matching record in the joined table. I often check each inner join with a left join to ensure that I return the same number of records. If it does then the inner join is appropriate, if it doesn't then I need to decide if it is filtering records I should be filtering or if I need a left join. When I am not sure why the record counts are off from what I expect, I use a select * (temporarily) just to see all the data, so I can determine why the data is off. I can almost always tell what the problem is when I see all the data. Do not ever use select * in a multiple join query like this or you will be returning much more data than you need and thus slowing the query as at a minumum the join fields are repeated (plus it is truly unlikely you need every field from 20 joins!). To find you issue, you are going to have to repeat this process. Start with just the from clause and the where condtions on it and add tables until you find the one (s) which are causing the query to eliminate all records. Don't stop with the first find, it is possible in a long query like this that you have multiple problems. 

Are you backing up the transaction logs? Not the database but the logs? THe log will grow until it uses up the entire hard drive unless you back it up. I suiuggest at aleasta daily backup or more frequently if you havea lot of transactions. 

Likely you have some data that doesn't fit the datetime column somehwere in the file. Insert to a staging table with a varchar or navachar filed for the column and look at the data. You may need to adjust some of it (or null out bad records) before loading to the real table. 

Merge joins and sort are notoriously slow. Is it possible to write a query for the OLE Db source instead that has joins?