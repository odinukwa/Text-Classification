"Two-Convex Polygons," O. Aichholzer, F. Aurenhammer, F. Hurtado, P.A. Ramos, J. Urrutia, 2009.            

Not an answer, just a few references. First, I wrote a paper (long ago!) on the case where every point in the given set $V$ must be a polygon corner. In that case, it is not surprising that there is (at most) one polygon, and it is easy to find: "Uniqueness of Orthogonal Connect-the-Dots," in Computational Morphology, Ed. G. T. Toussaint, Elsevier, North-Holland, 1988, 97-104. Second, there is a beautiful update to this work by Maarten Löffler and Elena Mumford, in a paper, "Connected Rectilinear Graphs on Point Sets," Journal of Computational Geometry, 2(1), 1–15, 2011. From their Abstract: 

Let $G$ be a graph with (positively) weighted edges. I want to define the Voronoi diagram for a set of nodes/sites $S$, to associate with a node $v \in S$ the subgraph $R(v)$ of $G$ induced by all the nodes strictly closer to $v$ than to any other node in $S$, measuring the length of a path by the sum of weights on the arcs. $R(v)$ is $v$'s Voronoi region. For example, the green nodes below are in $R(v_1)$, and the yellow nodes are in $R(v_2)$.            I would like to understand the structure of the Voronoi diagram. As a start, what does the diagram of two sites $v_1$ and $v_2$ look like, i.e., what does the 2-site bisector look like (blue in the above example)? I think of the bisector $B(v_1,v_2)$ as the complement of $R(v_1) \cup R(v_2)$ in $G$. Here are two specific questions: 

I want to remark that the answer is yes if you consider the input to be a clocked Turing machine, i.e., there is a clock that lets the Turing machine perform $p(n)$ steps and then accepts/rejects. Now checking whether the language decided by the machine is in NP is a syntactic property that boils down to deciding whether the machine is a well-formed nondeterministic Turing machine with a polynomial clock. 

The induced subgraph isomorphism problem has NP-incomplete "left-hand side restrictions" assuming that P is not equal to NP. See Y. Chen, M. Thurley, M. Weyer: Understanding the Complexity of Induced Subgraph Isomorphisms, ICALP 2008. 

I wouldn't say that the 'nature' of the problem changes, whatever that is supposed to mean. All that changes is the parameter, that is, the way in which you measure the difficulty of the problem. Graphs that have a vertex cover of size at most $k$ are so structured that it's possible to efficiently reduce them in size: We can greedily find a maximal matching of size at most $k$ and the rest of the graph is an independent set of size at least $n-2k$. Using reduction rules such as the crown reduction, the number of vertices can be reduced to at most $2k$. On the other hand, graphs which have vertex covers of size at most $n-k$ (or equivalently, maximum independents have size at least $k$) don't seem to have such a simple structure. This can be made precise, as you point out: their structure allows us to encode any $W[1]$-problem. 

When we are discussing the possibility of an NP-complete problem with properties so-and-so, does the ability to disregard from some input strings ("garbage input") ever make a difference? I'm considering 3 ways to view an NP-complete problem: 

What I'm looking for is some evidence or proof that it does make a difference or, alternatively, evidence that it doesn't. Proof could be an example of a property that an NP-complete problem can have if we are allowed to disregard from some inputs, but impossible if we are forced to regard all of $\Sigma^*$ as valid inputs. If, on the other hand, it's often possible to create a bijection between $\{0,1\}^*$ and the valid inputs (maybe even strictly growing w.r.t. shortlex order), polynomial time computable in both directions, then that, I think, is evidence that garbage input isn't a big deal. These questions came up when I was considering the possibility of an NP-complete problem with few easy instances, in a sense. It seemed to me that the ability to disregard from some input strings could possibly have an impact on the answer. 

There is also the sister problem FORM_TARGET_SET_WITH_INTERSECTIONS for which the better algorithm is 

For Eulerian Cycle it's easy to explain that it's a necessary condition that every node must have even degree, but it isn't as easy to explain why it's a sufficient condition. This is the problem that I think so far best meets the specification above: FORM_TARGET_SET_WITH_UNIONS Collection $C=\{S_1, S_2,...,S_n\}$ of sets Target set $T$ Question: Is it possible to form target set $T$ by taking the union of some of the sets in $C$ ? Obvious but ineffective algorithm: 

Techniques for the design of algorithms often help in reductions as well. Therefore it may be good to learn about techniques used to design FPT algorithms, for which the notes of the Spring School on Fixed Parameter and Exact Algorithms (2009) may be a starting point. In particular, you may want to look at the following excellent overview talks: 

Eickmeyer and Grohe (2010) prove that your candidate construction can be made explicit: take $d$ somewhat linearly independent linear hash functions $h_1,\dots,h_d$ and connect left vertices $v$ with right vertices $h_1(v),\dots,h_d(v)$. Eickmeyer and Grohe show that this construction gives $(k,\epsilon)$-expanders with left degree $d=k(t-1)/(2\epsilon)$, whenever $t$ is an integer, the left vertex set has size $n=q^t$, the right vertex set has size $m=dq$, and $q>d$ is a prime power. The hash functions $h_1,\dots,h_d$ are chosen in such a way that any $t$ of them are linearly independent. 

With respect to exponential time complexity, general instances and instances with constant maximum degree are equally hard: The sparsification lemma of Impagliazzo, Paturi, Zane (2002) shows that $n$-variable instances of $d$-Sat can be reduced to instances of $d$-Sat with at most $f(d,\epsilon)\cdot n$ clauses in time $\exp(\epsilon n)$. As observed in joint work with Husfeldt and Wahlén, the sparsification lemma works for the counting versions of $d$-Sat, too, and especially for the case of counting $2$-Sat (which is equivalent to counting independent sets and counting vertex covers). Moreover, counting independent sets in an $n$-vertex graph cannot be done in time $\exp(o(n))$ unless the exponential time hypothesis fails. This is a yet unpublished observation announced in a talk during the Dagstuhl Seminar Computational Counting.