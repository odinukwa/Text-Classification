returns 60 rows. question: is there a way I could modify my function so that it would return the whole 398 lines of the procedure? 

Without considering the indexes, Is there a way to re-write this query and get the same result quicker? 

Here is where I should have something to handle it. Let's keep it simple, and just say - if it breaks by a constraint violation - just don't delete the record. How can I get this done? 

I have been using the default trace to see when this transaction log has been autogrown, but I found none. 

have fixed sqldwdev01 the reason I couldn't connect to UAT instance is because of a couple of reasons 1st was there was a dynamic port specified for the instance under SSNetworkConfig\Protocols for UAT so had to remove that and specify 1435 (as 1434 is in use) restart the service still cant connect remotely to the instance name but can connect using the port name eg sqldwdev01,1435 so it needed sql browser service to be running to direct the port for connecting with instance name i started sql browser and now can connect using sqldwdev01\uat 

there are similar questions without a conclusion: DatabaseMail process is shutting down What is missing? 

Based ONLY on these clauses above, how could I define my clustered index, and other indexes and why? Moderators: I am aware this is not a strictly objective answer, but I consider the contents of the question valuable here is an example of how I am currently using this view: 

Question is: How can I fix this error? Basically get rid of the ghost subscription showing up in the replication monitor and causing it become red flagged? 

It is a known fact that the DMVs dont hold accurate information regarding number of pages and count of rows. However, when you have the stats updated, I can't see why they wouldn't. I am working on a monitoring tool, want to know disk size of each index and data, etc. Eventually I would like to find the right fill factor, and other things etc. The space used by my function and the old sp_spaceused differs a little bit on the space usage, but not on record count. Can you see if there is anything missing in my select? this is the sp_spaceused (then I convert the numbers in MB): 

I need to grant these permissions, what would be the minimum level I could grant so that they can do their job? I am looking at SQL Server Agent Fixed Database Roles and also Database Engine Permission Basics I still could not find exactly, what would be the least permissions to be granted? 

As you can see on the above picture, the amount of free space inside MyDatabase is high. I need a copy of this database so that I can test some partitions operations. Is there a way I can restore this database ignoring the free space in each file? Basically I will use my "new database" to develop and test something, I would prefer to keep it small. How could I achieve that? 

I have been struggling in Configuring Report Server URLs for a certain reporting services server. when I am in the server itself, the url works fine as you can see on the picture below. 

I had a third party company coming over and building ssas databases to produce reports. They did, but now the DBA wants to have a look but I haven't got permissions, to even see those databases, as you can see on the picture below. Can I grant myself administrator permissions? I am system administrator. there is a AD group called and 

This is a typical 1 to many relationship (or parent child relationship). The parent would have the fixed infromation and the child would have the many varying information. The key of the parent must appear in the child. Example: Receipt : {ReciptID, Date} RecieptItem : {ItemID, CustomerPrice, Quantity,TotalPrice,ReceiptID, Status, Date} Item:{ItemID, ItemName, Price} Now each Recipt can have 0,1 or more Recipt Items. Each Item is references on 0,1, or more ReciptItems. The column ReceiptID in ReceiptItem is referred to as a Foreign Key. A constraint could be built to make sure that every ReciptID in ReciptItem corresponds to a ReceiptID in parent Receipt table. Example ERD: 

A relational database would typically contain several related tables. The relationships between tables use what is called a foreign key column which is basically the primary key of a parent table placed in a child table's list of columns (with other rules not listed here). For simplicity, you can say that relationships in a database depend on Primary keys very very much. Now if a value is changed, the system has to perform so many changes to coupe with this change. As an example, say you have a Social Security Number of a person and you keep track about this person's properties, cars, jobs, wive(s), kids and let's say you change the SSN value. The system will have to change the corresponding value in each of those related (child) tables. This process is not always good to perform on-line. Another reason is business related. Say you are issuing an EmployeeID Cards, if the number on the card changes, then the card is no longer usable and you have to print another one, not only that, but this means that the salary information has to change, the insurance information has to change as well as other information related to the EmployeeID. In business, sometimes not all this information is integrated or even automated, so this change leads to a lot of work and possible inconsistency of information in manual systems. Such changes may not only be localized to one organization and may have to affect other external organizations or systems, which would make life very difficult, because you have to ask your business partners to carry such changes on their end too. Yet, another reason for not changing key values is that when you do so, history tracking becomes difficult. All history logs and documents will not reflect the current value making it difficult to interpret or even find data in manual systems. 

Typically you'd have an Invoice table that contains 1 or more sold line items (as a child of Invoice) and each line item's price (as well as possibly other details) would be looked up from a lookup table (that would act as a parent for the line item and would contain the price of each item). A line item in this case has many parents. The price table plays the role of a reference or lookup table. 

In many applications, it is common to use a db-automatically generated number like a sequence number as PK. If the table does not have a naturally key and you don't want to use an artificial key (as I suggested), and the table is not a parent table, you don't have to create a PK at all (you can still create non-unique indexes on any combination of columns you want). 

If you want to do this in an on-line transaction, then you probably have the wrong database design. If you are trying to do this in batch, then select only the fields of interest to a flat file, or export to a flat file then process your data from the flat file. You could even split the output file to several files and run parallel tasks to process the data. This will at least free your database server resources during your processing. This practice is not uncommon in data warehousing applications. 

how can I find out where the backups are taking more time to run? I currently can find which backups are taking longer using the following script: 

quite often while tuning queries and stored procedures I come across the situation where I have a number of different types of the same object, each type has different characteristics but all of them have similarities in the way they behave. for example, please see the code of a view below: 

then I want to see all the permissions of the objects. when I check my stored procedure I run this script: 

I have created 2 table variables in an attempt to match your environment. I have added an UNION ALL to your original query, one query selects all other months but december, and the second selects only december. there are other ways to do this, but in any way, it looks like an expensive query. here we go: 

Questions: what is the methodology (the process) we could follow to conclude which one of the queries above is better for this particular situation? Considering the first query needs less physical reads, but uses more CPU. what are the things to consider in order of relevance? physical reads or CPU? what else? 

I need to deploy packages using visual studio 2013 but I am seen this message, the application is not installed, as you can see on the picture below. what do I need to install? 

in a database that is part of an availability group (alwayson), in the current primary server I create a certificate and sign some stored procedures using it. all is working fine so far. 

I have the following update, that creates a table backup in a database called tablebackups with the records that are going to be updated, and then do 2 different updates. 

question: Can I work out exactly what trace columns has he put in this trace, without having access to the .trc file? 

sometimes the database is involved in replication the permissions are overwritten - the correct way is to save the current permissions before the restore and re-apply them after the restore. Have you checked the current available disk space in all drivers, specially those used in this restore? there might be other people using or working on that specific database, and restoring over it without communication can cause someone's else work being lost in the dev environment the databases can be in simple recovery mode. Have you shrunk the log and changed the database to simple recovery mode. in this particular case, the full backup for every user database is going to run automatically every night. but anyways, have you checked for scheduled backups? have you deleted from the server the backup that you used to do the restore? the folders and drives or data and logs are different in live and test. are you sure you moved the files to the right places? Have you had a look at orphaned users and logins? 

this was really nonsense. I right click on ssms and run as administrator, and added the permission there without a problem. 

this will show your credentials and if you execute as the login you are testing you can see how it looks like. 

that has an identity as a clustered index. however, as it is queried mostly according to these 2 columns: 

Then looking at the sql server log I find this deadlock was caused by one of our procedures (that was the victim) against the replication. 

and that worked for me too. to run ssms press the and on the ssms icon and that will give the options below: 

The process step is not meaningful in your case unless a process version is defined. So you could say that process 1 has its steps executed in this order (a, b , d, c) when the process was in version 1, but in version 2 the step execution order changed to be (a,b,c). So I think that a process version is important. The diagram below represents my suggestion. The silly thing about this is that if you change the order of a step, you have to insert all steps again in the new order, but in this case, it won't matter either in space or time. 

Having a table for each data type has nothing to do with normalization. Nomralization is about not repeating base information in tables. It is valid to have a key for a row and different columns each having its own data type. In fact this is the most common way. 

"integers are cheaper to compare than characters, because character sets and collations (sorting rules) make character comparisons complicated. Integers are very good when using as an index column." - HottestGuide 

To make an efficient compare of all columns between 2 tables, you could use a UNION method with GROUP BY as described in Diff2Tables. Alternatively, if your profile table can have rows deleted from it without cascade delete, you could, drop indexes that are not for PK of the profile, Delete rows having the same key in both tables, and insert the rows from the other table into the profile then re-build the indexes on the profile table again. This may sound bad but in fact it is not that bad. This is a typical scenario in Data Warehousing. 

For the database to be able to guarantee uniqueness of a row, it needs to search the table to make sure that the new incoming row has a unique key. This can only be practical and efficient, in large tables at least, when there is a structure called an index defined on that table that can be searched quicker than searching each row of the table in question. The same index could be used to locate rows by user queries. The index can be built on 1 or more columns of your choice, even on non-key columns to speed the search or to prevent duplicate rows. Sometimes designers would use artificial keys such as auto-incremented keys of data type int for example, instead of long composite keys containing long strings of data. This increases the search speed but still adds an extra index that could affect the insert/update slightly. The shorter the key, the more keys could be loaded and searched in memory, hence improving the overall response time. In your case, you could, use such an artificial key to enhance performance. To guarantee uniqueness, you will still need a unique index defined on your composite key. 

Here the query fails because you are trying to compare a numeric value to an empty string. The data types is not the same. You need to compare values of compatible data types to ensure correct result. I don't fully understand what you mean by Q3, so I will not be able to help with that. Be very careful with Nulls they are tricky. 

I would not use GetDate() for this fixed-period scenario as the end date for queries, because the reports are only meningful for specific periods. A better approach is to use a specific run-date for each reporting period. That run-date is not the current date. For example, let's say one of your KPIs is the sales amount for the first 6 months of 2013. Your system should use end of June as the end date not the current run-date regardless of when the report is run. If you do this, you'd not have to touch the data.