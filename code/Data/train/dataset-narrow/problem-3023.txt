There is a rule called there is no free launch. It means that there isn't a learning algorithm that solves all the problems. You as a machine learning practitioner should decide when and how to use which algorithm. Suppose that you want to recognize faces. This problem is a learning problem which if you increase the number of training data, you will get better results. In these cases neural nets and deep nets are highly recommended. In this case it is not logical to use because it will be so costly and you may not even get good answers. the reason is that deep nets cares about local patterns but considers all the input pattern simultaneously. Actually in your case, I guess your data is categorical. For categorical data, people often use decision trees. To illustrate an example, once I decided to train a simple MLP to distinguish whether an input pattern is in correct position, to solve 8-queen problem. I solve the game using and made data for training the net. The data I brought to net was categorical in some extant. I used it and the net was so good for the trained data, but input features similar to training data which were a bit different had bad recall rate. I trained a decision tree, I get so much better result. Which algorithm depends on your task and your input features. 

You have not specified that what neural network you are using but as comments, you should try to fit your data first. You have to try to find a model that learns your training data. For this purpose you don't have to increase the number of data, at least not at this stage. You should try to find a good model which suits your data. For this purpose you have to change the hyper-parameters of your neural network, e.g. number of layers or number of neurons in layers. You can take a look at here and here which the former can help you and the latter helps you understand the features learned by in case you are using them. For using score in I've not seen but you can implement it and pass it to compile method, take a look at here. 

For increasng your accuracy the simplest thing to do in tensorflow is using technique. Try to use . between your hidden layers. Do not use it for your first and last layers. For applying that, you can take a look at How to apply Drop Out in Tensorflow to improve the accuracy of neural network. 

I recommend you to use the following example and try to manipulate the arguments and adjust them for your work: 

If I get the point right based on the title of the question, in your code you are not making multi layer perceptron. You have tried to make somehow a convolutional network. In MLPs you just have to stack dense layers. Do something like the following code snippet which stacks just dense layers: 

To answer the last question, suppose that you have a binary classification problem. It is customary to label the class as positive if the output of the is more than and negative if it's less than . For increasing recall rate you can change this threshold to a value less than , e.g. . For tasks which you may want a better precision you can increase the threshold to bigger value than . About the first part of your question, it highly depends on your data and its feature space. There are problems which the data is linearly separable in higher dimensions which means you can easily employ just a single neuron for classifying the data by a single hyper-plane. If it has happened that you have such a good accuracy you can not say anything unless you try to find the value of cross validation error. By interpreting the difference between the value of training data, and cross-validation or maybe test data, you can figure out whether your classifier performs well or not. 

If you use just one neuron (linear->sigmoid), you can find the minimum with too much small error or maybe reach to the minimum using approaches like gradient descent or other optimization algorithms. The reason for finding the minimum is that cross entropy is a non-convex shape, but if you use sigmoid function as the activation function of logistic regression, the cross entropy cost function becomes convex and it is easy to find the only global minimum. 

Actually I guess you are making mistake about the second part. The point is that in s, convolution operation is done over volume. Suppose the input image is in three channels and the next layer has 5 kernels, consequently the next layer will have five feature maps but the convolution operation consists of convolution over volume which has this property: each kernel will have its width and height, moreover, a depth. its depth is equal to the number of feature maps, here channels of the image, of the previous layer. Take a look at here. 

The consideration of the number of neurons for each layer and number of layers in fully connected networks depends on the feature space of the problem. For illustrating what happens in the two dimensional cases in order to depict, I use 2-d space. I have used images from the works of a scientist. For understanding other nets like I recommend you taking a look at here. Suppose you have just a single neuron, in this case after learning the parameters of the network you will have a linear decision boundary which can separate the space to two individual classes. 

The easiest way is to replace your labels. The other way is to set importance of the more important class to a higher value so the cost function moves toward direction to take much care for your desired label. You can set the class_weight. Take a look at here and here. 

Based on the comments of one of our friends, the above approaches are named as follows, respectively: 

PCA is used to eliminate redundant features. It finds directions which data is highly distributed in. It does not care about the labels of the data, because it is a projections which represents data in least-square sense. Multiple Discriminant Analysis, try to find projections which best separates the data. The latter considers the label and finds directions that data can be separated the best, although it has some details about the kind of decision that finds. To wrap up, is not a learning algorithm. It just tries to find directions which data are highly distributed in order to eliminate correlated features. Similar approaches like try to find directions in order to classify the data. Although is so much like , but the former is used for classification, it considers the labels, but the latter is not directly used for classification. 

Whenever you have skewed dataset, it means that you know some classes better than some others. In such cases it means that the data is your knowledge and there are learning algorithms for such occasions. Consider an important fact here. Suppose that you have feature vectors of conditions of a nuclear company and they describe whether the company is in danger of nuclear radiation or not. In such case it is clear that it does not happen a lot that you have infected companies so most of your data have label of healthy condition. You have so much knowledge about the healthy class but you don't know much about the infected class because you don't have much data consequently you don't know its distribution and you can not estimate it well. Whenever your data is skewed, it means that e.g. you have 1 million feature vectors of negative class and 5 feature vectors of positive class. Now suppose that you change the feature vectors. In such cases you have imbalanced data-set or you just have the data samples of some classes without some other, you can use anomaly detection. 

Depending on your task may be good or not. In typical classification networks, it is an acceptable size, like the inputs of AlexNet or VGG and such customary nets. For localization tasks, not really. For instance, the input height and width of YOLO is more than that 1. 

In machine learning tasks it is common to shuffle data and normalize it. the purpose of normalizing is clear and is for having same range of feature values, but after struggling a lot I did not find any valuable reason for shuffling data. I have read here about when we need to shuffle data but it is not obvious that why we should shuffle data. Furthermore, I have seen a lot that in algorithms such as or where we need batch gradient descent __ data should be separated to mini-batches and batch size has to be specified. It is vital to shuffle data for each epoch to have different data for each batch, so the data is maybe shuffled and more importantly is changed. Why do we do these? 

Its advantage is that it can model pairwise feature interactions in a translationally invariant manner, which is particularly useful for fine-grained categorization. It also allows end-to-end training using image labels only, and achieves state-of-the-art performance on fine-grained classification. 

will have a value near to one but smaller than that. On the other hand will be so close to zero, multiplying and will result in a small value, which means the value of gradient is so much small. If the value of linear part is so small, then will be close to zero and will be close to 1 but smaller than that. Again, multiplying these will lead to a small value as the gradient. 

It's just the expansion of one dimensional mean and standard deviation. Suppose that you are trying to estimate the weight of a person and you have two inputs, salary and height. For finding the weight, you have two inputs which are of different scales, so you try to find the mean and variance of each feature, salary and height, separately using the data samples. Suppose you have two data samples in a tuple like (salary, height). They are as follows: 

Suppose that you want to have algorithm, in the formulation of average, you have to take the average of each cluster, and then reassign the centers. If you have categorical data, how do you want to take mean? Changing categorical data to numeric data is for translating situations which don't have numerical features to be suited to be used for such algorithms. 

Although your input data is three dimensional, you have to use for your task. I guess is used for data with temporal characteristic, yours is just a simple picture. To illustrate why you should , suppose your input image is and you employ a layer with 10 filters. You have to specify stride and padding in order to specify the output shape. You have to specify dimensions to illustrate the height and width of you filters, also known as kernels, filter size will affect the output size if you assign padding to 'valid'. Here, there is a point. Suppose you have specified the filter size a filter, then if the input shape was , each filter would be of size to fit the input area. Now that the input is of size the size of each kernel is to fit the input volume. Consider in all cases the output of each convolution operation, better to say cross correlation, is a scalar. For more information take a look at videos here and for your case I encourage you watching Convolution Over Volume. 

Images are two dimensional signals. The use of is for one dimensional signals like voice and sound. CNNs are good for these signals too because of taking care of local input patterns. Definitely there are standard one dimensional filters which are most used in signal processing like high pass filters and low pass filters which are so much popular. In order to show you an example take a look at the following figure which illustrates the convolution of two signals: 

Although this may not seem okay but I have seen people that use removing correlated features in order to avoid overfitting but I don't think it is a good practice. For more information I highly recommend you to see here. 

I guess you have a learning problem that has low number of training data. I recommend you a two-step solution. 

You have label of classes which are not mutually exclusive which means as the label of each sample, your data won't be in one-hot-encoding format. Each output vector may have multiple ones. In such occasions you shouldn't use soft-max as the output layer. You have to use activation function for each neuron in the last layer. Suppose you have ten labels and for a typical movie each of them may be activated. So, in the last layer use a dense layer with ten activation function. You can see here which may help you. And as a side answer, each cost function that you are going to use should be categorical, because you have different categories. I respect so much for the other answer and I thank him/her for the answer, but I guess you should use categorical cost function otherwise the code won't work because your output matrix actually is a matrix of samples and not a vector of samples. 

What I'm saying is based on my experience. Optimization itself always lead to overfitting. You have to use generalization techniques to avoid that. To help you figure out the problem, suppose that deep-learning algorithms are able to learn all functions. If you provide them with relatively small number of data, they will memorize a hypothesis, and they do not learn the problem. 

The reason is that in your model, you have specified the input size to vary, as the parameter of your function specifies. You have to change your code as follows: 

As you can read from here plt.subplots() is a function that returns a tuple containing a figure and axes object(s). Thus when using fig, ax = plt.subplots() you unpack this tuple into the variables fig and ax. Having fig is useful if you want to change figure-level attributes or save the figure as an image file later (e.g. with fig.savefig('yourfilename.png'). You certainly don't have to use the returned figure object but many people do use it later so it's common to see. Also, all axes objects (the objects that have plotting methods), have a parent figure object. The parameter is used to set the number of ranges to be used to accumulate the data in those ranges. I don't know where else you have question. 

I guess it is not fair to mention non-free lectures because the owners of other non-free lectures may get disappointed but take a look at complete guide to TensorFlow for deep learning with python, which is very useful. 

As you can see here models contain predict method but they do not have the method you have specified and they actually do not need it. The reason is that method itself returns the probability of membership of the input to each class. If the last layer is then the probability which is used would be mutually exclusive membership. If all of the neurons in the last layer are , it means that the results may have different labels, e.g. existence of dog and cat in an image. For more information refer here. For more information as stated here in the recent version of keras, predict and predict_proba are the same i.e. both give probabilities. To get the class labels use predict_classes. The documentation is not updated. (adapted from Avijit Dasgupta's comment) 

Instead of calling , call . you are referring to the function object now. You have to call the function. 

In this case I don't think you will have a good learning. Anyway, you have to use confusion matrix in this for tracking all classes. Although I guess your model will be over-fitted because of the amount of provided data. 

Based on my experience, not just for ImageNet, if you have enough data it's better to train your network from scratch. There are numerous reasons that I can explain why. 

is not linear. The simple answer is that output is not a straight line, it bends at the x-axis. The more interesting point is what’s the consequence of this non-linearity. In simple terms, linear functions allow you to dissect the feature plane using a straight line. But with the non-linearity of s, you can build arbitrary shaped curves on the feature plane. may have a disadvantage which is its expected value. There is no limitation for the output of the and its expected value is not zero. was more popular than because its expected value is equal to zero and learning in deeper layers occurs more rapidly. Although does not have this advantage solves this problem. You can also refer here and here for more information. 

Pooling layers try to sum up the information in a local neighbourhood and make a higher representation of the inputs. Suppose that the inputs of a pooling layer have nose, eyes, eybrows and so on. Your pooling layers somehow attempt to check whether they exist in a neighbourhood or not. Consequently, convolutional layers after pooling layers usually keep information which may be irrelevant to your task. There is a downside for this interpretation. The information may be distributed among different activation maps as Jason Yosinski et al. have investigated this nature at Understanding neural networks through deep visualization where you can read One of the most interesting conclusions so far has been that representations on some layers seem to be surprisingly local. Instead of finding distributed representations on all layers, we see, for example, detectors for text, flowers, fruit, and faces on conv4 and conv5. These conclusions can be drawn either from the live visualization or the optimized images (or, best, by using both in concert) and suggest several directions for future research and These visualizations suggest that further study into the exact nature of learned representations—whether they are local to a single channel or distributed across several .... A partial solution can be keeping the first layers which find low level features that are usually shared among different data distributions and removing deeper convolutional layers which find higher abstractions. As stated, the main problem of convolutional layers is that the information they find may be distributed among different activation maps. Consequently, you can not be sure by removing a layer you can have better performance or not.