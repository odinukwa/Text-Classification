My test case ran fine on multiple other databases and multiple sources (including oracle support) asked if the ANONYMOUS user had been dropped. Although it had not, there were some XML DB Repository changes that were made without a full understanding. To try short cut the problem I re-installed XDB using note 1292089.1. This allowed my test case to work correctly and after dropping the created types and tables and re-registering our XML schema, everything worked correctly. 

It would be difficult to say whether this solution would be preferred without knowing more about the nature of the data you are storing and how it will be accessed, but here is another idea you may not have considered. Since your data is fairly static and will be accessed from PL/SQL and the application, you could simply embed the data in a PL/SQL package. The package could be as simple as a look-up function to retrieve a value based on a given key. The application could use the same package to retrieve the data. The function would probably be deterministic and probably in a separate package from the code consuming it. The view and/or table may be preferable if you are mostly consuming the data in SQL. By the way, modifying a view is not really any more difficult than manipulating data - assuming DBA privileges. 

These possibilities are from this documentation. If neither is applicable then perhaps you are running into a bug and should open an SR with support. I assume you are running commands like this: 

Phil's answer is good and probably what you want. SQL Developer provides a wizard for this functionality from the Tools menu. Depending on your requirements you may want to do an export (which uses the dbms_metadata package). The Oracle Concepts Guide (required reading for anyone using Oracle) has a section on Oracle Data Pump Export and Import (emphasis mine). 

Phil is right, "No, there isn't a limit", "it /should/ just work", and " this will be difficult to answer without copies of the data or another way to reproduce". Here is a demonstration: 

There may be a simpler way, but here is a solution. If the current row is the minimum foo for the bar, then we always want the next (lead) foo. When there is only one foo for a bar the lead will always be null and for every other case we really do want the minimum foo for the bar. 

There doesn't seem to be any reference to being able to directly manipulate the audit file names in 10g. The closest thing I found is Bug 6023472 which lists the following changes introduced in 10.2.0.5/11.1.07/11.2.0.1: 

Running on one disk indicates that you are comfortable loosing all your work since the last backup. Since you are backing up twice a day it would seem that you want the software to remain available while the backup is taking place. In order to meet these requirements you will need to be in archivelog mode. Use Windows Scheduler to schedule two different scripts that use RMAN. The first run of the day would just backup the archive logs and the second would do a full backup including archive logs. Depending on how much archive data is being backed up and how fast you want your recoveries to be, you could consider doing your full backups less frequently. 

The tnsnames.ora file probably has a syntax problem since you can connect using easy connect. If you want to use tnsnames, I suggest modifying it or recreating it with Network Configuration Assistant. Full syntax can be found here. 

I found the issue. It is documented an oracle note (Doc ID 1086519.1). I used the first workaround listed which is to set CURSOR_SHARING to exact. 

You could store the catalogs tables in a separate schema on production. test could then have a schema with the same name that uses views that select from the production tables using a database link. This would make production fast because the data is local without requiring extra space. If you decided that a particular table was too slow to access over the database link or having too great of an impact on production, then you could create a materialized view on test instead of a view. You could create as many or as few of these as necessary to give the best balance of space and performance. On the other hand, 1GB isn't that much space, so you might be better off importing into both databases even though it would double the space requirements. It would have a speed advantage and would make test more like production. 

If you keep a separate table to maintain whether your procedure is running or not, then you only need to keep a lock as long as it takes you to update the table to ensure that another session is not doing the same. Here is some code to setup the table and then use it to run your procedure. 

It sounds like the "large set of integers" is still considerably smaller than the table with "hundreds of thousands of integers". With that supposition and unless there is a way in MySQL to use an array of your integers as a table in your SQL statement, your second option is probably the best. It should do a full scan of the temp table and the index on the main table. The primary benefit being that it only has to scan the index containing hundreds of thousands of integers one time and only has to send the client the results. Your query could (but need not be) rewritten as the following: 

Yes, there is a way to avoid writing the clause multiple times, but the cure is worse than the problem (and therefore is not recommended). You would have to write dynamic SQL, which would be more difficult to read, require more code, be more fragile, etc. What you have is not difficult to read save that it violates the DRY principle. It is good to recognize this and as TheGameiswar[+1] pointed out, you may one day have a solution without a downside. 

Here are some additional factors you may want to consider that aren't related to the core of the upgrade itself: 

Don't repeat yourself is a good principle in coding that will prevent duplicate debugging, unit testing, bug fixes, etc., however it shouldn't be considered in isolation. Dynamic SQL has the downside (among others) of not showing up in the dependency chain. Package wrappers have the downside (among others) of requiring additional objects. In a case like this, as long as you don't have too many different links you switch to, it may be preferable to simply repeat the small block of code for the other database link and wrap them both in an if/case statement that uses a parameter to determine which database link should be used. This way you don't have to recompile any code to change which link is used. You could of course combine this method with miracle173's and implement a similar strategy for the view. 

Is there a way to know if a has been done on an index? Doing one doesn't seem to update the , , or attributes. Note: I am not referring to the that returns the first non-null expr in the expression list, but the one that is used to make free space in a block contiguous like this: 

The documentation also includes notes on four places that comments should not be used, but these do not include any further differences. 

The phrase "but not use the JOIN ON syntax" may mean you should use the older 8i style of joins that doesn't use words "JOIN ON" at all, or it may mean you should use "JOIN USING" rather than "JOIN ON". Hopefully they meant the latter. 

To understand what will happen in entirety you have to understand the SCOPE parameter and what happens when no SCOPE is specified. From the SQL Language Reference we can see that the ALTER SYSTEM statement can have three different SCOPE specifiers or none at all. 

SAP AG (Often just called SAP) is a company which has a product called SAP ERP (Enterprise Resource Planning). Since Oracle also makes ERP software they are indeed competitors. In fact they are the top two ERP vendors. SAP can be used with quite a few different database systems, but according to one source I found Oracle is dominant comprising around 80% of SAP systems. Oracle recently won a law suit against SAP for intellectual-property theft. ($URL$ Here is some SAP on Oracle documentation. It does seem like a strange scenario for a product to depend so heavily on a competitors product, but it is not unheard of. To some extent this pattern exists for Oracle with their database product running on Microsoft Windows servers. 

=== Update 4/9/2013 === I have an anonymous block that can successfully validate XML based on a schema. This again points to a permissions issue that allows this to work when roles are enabled, but not when they are not available. == Update 4/12/2013 === Every bit of evidence I get seems to indicate that there is something wrong with my ACL or perhaps more likely with the way I set the ACL. Taking the XDBADMIN role away from the user causes the anonymous block to fail with an access denied error even though I have granted all the permissions the role gives according to dba_tab_privs. My setACL follows this form: 

In a perfect world all tuning would be done in the design phase proactively and nothing would be reactive, but the world isn't perfect. You will find that test data sometimes isn't representative, test cases will have been missed, loads will be unexpectedly different, and there will be bugs that cause performance issues. These situations may require some reactive tuning, but that doesn't mean reactive tuning is preferred. The goal should always be to catch these up front. Your planning for retroactive tuning is very pragmatic. When you are testing you should document expected timings and throughput and at times should actually build in analysis that lets you know when production processes are not meeting design specifications. In this way you may be able to identify in advance what code needs to be tuned. You can then determine not just what the problem is, but why you didn't catch it in the design/test phase.