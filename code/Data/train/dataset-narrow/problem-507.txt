There are no strict recommendations. For sure, you want to have the largest redo logs possible for you. The sizes that you cite will negatively impact performance. One rule of thumb is that under the largest load, you shouldn't see more than three redo switches per hour. (Redo switch messages are normally included in your ) For the large redo logs (multi-gigabyte size) I can have three limitations, but they are easily fixable: 

Any query which returns more than 50 rows could seem to execute faster on SQL Developer. This is because SQL developer only fetches 50 rows (and more after you press Page Down), while your application possibly fetches many more rows. The proper way to test is to use sqlplus with . 

gives authoritative answer as far as it goes for the database binaries (i.e. files that are needed to run the engine of the database). is a history of changes to database dictionary (i.e. to the internal Oracle's stuff in your database files). In particular, PSU ships with SQL files. SQL files shipped with PSU should be run manually after as ordained by README and, by Oracle's self-imposed convention, they log themselves in the . This table, as you see, doesn't explicitly contain the current version, just some clues to what the current version could/should be. 

So, Dev could simply back up unused blocks, and there are plenty of reasons why Prod would have no symmetry here. Another possible reason. RMAN will do Level 0 (i.e. full) if it believes you erroneously requested Level 1. Manual says: 

Normally all destinations are optional. But your setting is ok; it could somewhat decrease potential data loss (loss of redo log not yet archived). Further two items to take care of: 

Contact support. ...or determine what exactly has been done, and then having the SQL script(s) revert the actions one by one. There is nothing in Oracle to help you, you need to determine by your own means (some old notes? old screenlogs? clues in old .bash_history?). ...or schedule downtime, expdp everything what is relevant in the entire database, re-create, impdp (of course don't impdp SYSTEM schema). ...or if you are unable to proceed with either 1, 2, 3, don't perform DROPs by any heuristics/guesswork. Continue as you are. Nothing really wrong with your database - just some extra objects in SYSTEM (not in SYS, I hope). They could stand in a way of some future action, but they might not, nobody knows. Normally perform cold backup before upgrade. Chances are the upgrade will be successful. Schedule steps 1,2,3 for later. 

For one thing DBMS_JOB.submit requires COMMIT and you lack that. Another thing is that DBMS_JOB is ugly and long deprecated. Use another guide/howto, one written in this millennium.x And you don't need WITH GRANT. Retaining this answer only to keep informative comments. 

No. The operations go to DR site in exactly the same sequential order as they were performed on the primary site. You can only re-order operations at the level of your application code, the database engine cannot do this for you. The database engine cannot reorder changes to data because it needs to ensure that DR database is bit-to-bit identical to the state of primary database at a very recent point-in-time. Your impression that COMMIT transfers all the data is incorrect. Data is transferred all the time throughout the transaction, and COMMIT is quite a small transfer itself that ends it. The only special thing about the COMMIT is that it is the only transfer for which the primary database has to wait until the COMMIT mark is transferred to DR, written there to disk, and acknowledged back to primary. Anyway, I would strongly suggest to change SYNC to ASYNC in your archive_log_dest_x, which would obviously solve your problem (problem of waiting for COMMIT), at a cost of potential loss of the unsynchronized data in a real DR situation. If your software controls real-time multimillion FOREX transactions, and you have a high-bandwidth high-available network connection, then SYNC is the way to go. If not, you should accept this small risk, as most other users do. 

This needs to be char-by-char identical every time. I see it ends with a slash here, which is non-standard, and I suspect that you've added it by mistake. Adding a slash will result in "ORACLE not available" in so-called bequeath sqlplus, I've just experimented on my system. Try again with one consistent setting like: 

Yes, this scenario is possible in NOCATALOG mode. I did exactly this on 9.2 last Saturday so I can vouch for it :) Your attempt was almost successful. The only thing you did wrong is you restored the control file that was too new, it was above . If you move away it would probably find a bit older controlfile and the restore would succeed. Ensure does not display the new incarnation at all - controlfile shouldn't know about it. 

The best account to login to the Oracle Enterprise Manager Database Control (EMDC) is the SYSMAN user account; with it, you are logging in as the proper EMDC super user. Use NORMAL mode, not . Alternatively, you can use SYSTEM (specifying mode ). This is per 11.1 manual: $URL$ The same manual describes how to, being in EMDC, convert another "normal" database user (the one you use in sqlplus) into an EMDC-capable user. Or, from within EMDC, you can create a new user: a database user that is both EMDC-capable and sqlplus-capable from the beginning. As SYSMAN wouldn't have a password by default after install, you need to assign it. Just login to sqlplus with SYS AS SYSDBA and issue: 

In general imp would work as intended with sys account, it is just not recommended. But exp with sys account could possibly produce inconsistent dump file silently. So it's better to avoid it, for example use . 

Forget copying binaries around. I would integrate remote RMAN executions into your normal backup application/solution (i.e. filesystem backup to fire a script) for the sake of consistency. Especially scheduling of backups would be easier. Re: "In this case parameterfile.par must exist in SRV_PROD_1". Not a problem if you dig into RMAN's own CREATE SCRIPT syntax - you can store the run{ } script inside your catalog. Re: "rman.log will be created also on SRV_PROD_1". Not a problem if you take a look at catalog schema contests - surely there is one large table per each catalog that holds the logs anyway (I can't remember if it's ROUT or RMAN_OUT?). 

KISS principle: Do you have the storage that itself offers you the required availability level (i.e. disk array)? And you do have RMAN backups with a recovery catalog (you need it anyway, for DUPLICATE), don't you? If yes and yes, my version is: no secondary control file at all, one member per each redo group, as there are no secondary datafiles (quite obviously, even for system01.dbf) or no secondary archivelogs. Otherwise your database is unnecessarily doing stuff that belongs on the storage level. I would just leave storage things on the storage level - the mirror is done there, and it is done effectively. Or, if you want cross-site mirror, do it on ASM level. Database is occupied with database-related processing. The software mirroring on database level only makes sense if you use bare hard disks. Most people conservatively stick to having three control files, though. Probably Oracle support would also recommend you to use three. Not only I've never practically benefited from having a second or a third control file; I'm unable to think of a scenario when secondary copy would be justified (i.e. it provides some benefit for the lack of simplicity). Logical corruption in control file would be propagated to a second control file, just as in case of hardware mirroring. Physical corruption wouldn't be propagated in either case. If storage fails, you lose redo or data - you need to restore via RMAN anyway. You lose control file, you can restore it with RESTORE CONTROLFILE, which only costs extra minute at most. If you've lost redo, the recovery is incomplete. If you didn't lost redo (but lost all control files), the recovery is complete de facto, but Oracle insists on OPEN RESETLOGS (loss-less in such case). 

Incremental Level 1 backup in RMAN means all data blocks changed since last Incremental Level 1 or 0 backup (0 is the most basic level, it always contains all data). If you use CUMULATIVE keyword on Dev, the incremental level 1 contains all data blocks changed since last Incremental Level 0 backup. Which means more of them. One possible explanation. RMAN only cares whether the block was written to, there are no efforts here to de-duplicate basing on the data. Maybe the Prod received less of such block writes, and Dev more. In particular, if you import into Dev, this definitely counts here. Another possible reason. Manual says, that unused-never-written-to blocks are always skipped but, in contrast, the unused blocks are skipped only in a very specific set of circumstances: 

Somehow you are trying to keep too many PL/SQL objects (I mean "TYPE AS OBJECT" objects). There is an internal limitation of around 64k of these per session. This is on the client side (not on server side), so this is a programming error, not related to database administration. Not if that matters, but if you are simply curious why this message speaks about "durations" see this doc. 

I vaguely remember I've seen somewhere that 11.1 database in such situation defaults to USE_DB_RECOVERY_FILE_DEST but via LOG_ARCHIVE_DEST_10. In all, your configuration and syntax is correct. 

1) One Tom Kyte maintains that with shared server, the only performance that will surely increase is processing of CONNECTs. For the rest of statements, shared server is inherently a bit slower, unless it happens that pooling/multiplexing is sufficient to maintain advantage. 2) Dedicated is simpler - it has less components. Since much of administrator's job is investigating strange behaviors, tracing, analyzing... simpler means better. 3) Most importantly, everybody uses dedicated, and a best approach in case of any "enterprise-level" software is to take a path more traveled by - my personal opinion. A side-effect would be when contacting Oracle support with problems, you may receive more often the advice to "try again using dedicated server instead" than "well, why don't you try it using shared server instead". So it's one less thing to discuss with those beautiful people. 

You issue from the same operating system where the server (database) is installed. In this case there is no difference between AUTHENTICATION=CLIENT and AUTHENTICATION=SERVER or SERVER_ENCRYPT (by the way, I believe the manuals strongly recommend against CLIENT setting). If you are logged in as an OS user , the DB2 will not doubt that OS authenticated you as such. I am not aware of any trick that would cause DB2 to re-authenticate OS user just to "double check". But a very easy way to accomplish what you require is to be a different OS user while issuing . If you are logged in as and try to , you will be asked for a password, because DB2 can see that OS have not authenticated you as . Hence DB2 asks for a password and performs OS authentication on your behalf. In a default database setup, all users (including ) would be able to connect as themselves, so, as you apparently have security in mind, you'd probably want to first. 

This is a database's feature, not RMAN's (commands are for sqlplus), see doc for the full setup and disclaimers. To more precisely answer your question, RMAN can also be used for the task. The general script is: 

Yes, if the database is open in read-write mode there is always a number of changes to database that reside only in the current redo log and not in any archived log. If you want protect the current redo against hardware-related corruption, you need to add redundant storage (with either a remote replication or local mirror). If you want to protect the current redo from OS-related corruption, you need a "physical standby" database (the marketing term is DataGuard) with "SYNC LGWR" feed. RMAN would not be useful to you, it is not designed for a a rolling backup of current redo. 

TL;DR to be able to restore data+undo without overwriting the entire current undo. When you perform RMAN online backup, some transactions are in the middle of processing, they did change something but not yet performed their COMMIT. Transactional model of work ("atomicity") requires those transactions to be rolled back (completely undone) on recovery. Oracle engine does this automatically. Oracle engine does not allow onlining tablespace if this cannot be done, to protect user from seeing possibly inconsistent data. So do transactions save somehow the "consistent" data before they change it? Yes, in the undo tablespace (or a "rollback segment"). This is why, recovering a tablespace, you cannot "forget" it had undo, you need to recover undo from the same point in time (same SCN). To sum up, Oracle engine needs to have undo data synchronized with from the same point in time as the main data. Undo is, usually, put into a single tablespace despite it is being a mix of pieces of "consistent" information from many different data tablespaces. This is why you TSPITR to an auxiliary instance - to be able to restore data+undo without overwriting the entire current undo. Once auxiliary is online, the auxiliary's data has been made consistent and auxiliary's undo is no longer needed. 

The 11.2 manual says you should also check plain LOG_ARCHIVE_DEST. Although it comes from Standard Edition, it is still valid in case none of numbered Enterprise Edition destinations LOG_ARCHIVE_DEST_n are defined. I haven't tested that personally. In case all are unset, the another 11.2 manual says 

Based on your additional comments, I'd say you could keep the query as is, but create your table as an "index-organized table". This is basically a way to tell Oracle: dear, keep this ORDER BY forever for this data, my queries will use this order a lot.