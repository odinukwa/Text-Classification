First, note that the result states that the only beta redex where the right-hand side is equal (modulo alpha-conversion) to the left-hand side is $(\lambda x. x x) (\lambda x. x x)$. There are other terms that reduce to themselves, having this redex in a context. I can see how most of Lercher's proof work, though there are points where I can't get past without modifying the proof slightly. Suppose that $(\lambda x. A) B = [B/x] A$ (I use $=$ for alpha equivalence), and as per the variable convention suppose that $x$ does not occur free in $B$. Count the number of $\lambda$'s in the left-hand side and the right-hand side. The reduction removes one from the redex, plus those of $B$, and adds as many as there are in $B$ times the number of occurrences of $x$ in $A$. In other words, if $L(M)$ is the number of $\lambda$'s in $M$ and $\#_x(M)$ is the number of free occurrences of $x$ in $M$ then $1 + L(B) = \#_x(A) \times L(B)$. The only solution to that Diophantine equation is $\#_x(A) = 2$ (and $L(B)=1$ but we won't use that fact). I don't understand Lercher's argument for the paragraph above. He counts the number of $\lambda$'s and atomic terms; let's write this $\#(M)$. The equation is $\#(B) + 1 = \#_x(A) \times (\#(B) - 1)$, which has two solutions: $\#_x(A)=2, \#(B)=3$ and $\#_x(A)=3, \#(B)=2$. I don't see an obvious way to eliminate the second possibility. Let us now apply the same reasoning to the number of subterms equal to $B$ on both sides. The reduction removes one near the top, and adds as many as there are substituted occurrences of $x$ in $A$, i.e. 2. Hence one more occurrence of $B$ must disappear; since the ones in $A$ remain (because $B$ contains no free $x$), the extra occurrence of $B$ on the left-hand side must be $\lambda x. A$. I don't understand how Lercher deduces that $A$ does not have $B$ as a subterm, but this is not in fact relevant for the proof. From the initial hypothesis, $[(\lambda x. A)/x] A$ is an application. This cannot be the case if $A = x$, therefore $A$ itself is an application $M N$, with $\lambda x.M N = [(\lambda x. M N)/x] M = [(\lambda x. M N)/x] N$. Since $M$ can't have itself as a subterm, $M$ cannot have the form $\lambda x.P$, so $M = x$. Similarly, $N = x$. 

Comparison sorts cannot be linear It depends what you're sorting and how you're sorting it, but under the most common model, an $O(n\,\log \log n)$ sorting algorithm is impossible. The most common model of sorting is the following, called a comparison sort: 

If we go by the book (or any other version of the language specification if you prefer), how much computational power can a C implementation have? Note that “C implementation” has a technical meaning: it is a particular instantiation of the C programming language specification where implementation-defined behavior is documented. A C implementation doesn't have to be able to run on an actual computer. It does have to implement the whole language, including every object having a bit-string representation and types having an implementation-defined size. For the purpose of this question, there is no external storage. The only input/output you may perform is (to read the program input) and (to write the program output). Also any program that invokes undefined behavior is invalid: a valid program must have its behavior defined by the C specification plus the implementation's description of implementation-defined behaviors listed in appendix J (for C99). Note that calling library functions that are not mentioned in the standard is undefined behavior. My initial reaction was that a C implementation is nothing more than a finite automaton, because it has a limit on the amount of addressable memory (you can't address more than bits of storage, since distinct memory addresses must have distinct bit patterns when stored in a byte pointer). However I think an implementation can do more than this. As far as I can tell, the standard imposes no limit on the depth of recursion. So you can make as many recursive function calls as you like, only all but a finite number of calls must use non-addressable () arguments. Thus a C implementation that allows arbitrary recursion and has no limit on the number of objects can encode deterministic pushdown automata. Is this correct? Can you find a more powerful C implementation? Does a Turing-complete C implementation exist? 

Here's a shorter way of proving this lemma. All the intelligence we really need to provide is to use induction on the proper hypothesis, Coq can figure out the rest with some general hints. 

First the case; it's an inductive case: the property we're working on works for an because it works for the parameter (which is of type ), putting it all together with some straightforward first-order logic. We have a hypothesis of type ; once we've applied , the goal becomes . I'll leave doing that manually as an exercise. The built-in tactic makes short work of this case, as well as the symmetric case for . 

The science of programming language design is very much in its infancy. Theory (the study of what programs mean and of the expressivity of a language) and empiricism (what programmers manage or don't manage to do) give a lot of qualitative arguments to weigh one way or another when designing a language. But we rarely have any quantitative reason to decide. There is a delay between the time some theory stabilizes enough for an innovation to be usable in a practical programming language, and the time this innovation begins to appear in “mainstream” languages. For example, automatic memory management with garbage collection can be said to have been mature for industrial use in the mid-1960s, but to have only reached mainstream with Java in 1995. Parametric polymorphism was well-understood in the late 1970s, and made it into Java in the mid-200s. On the scale of a researcher's career, 30 years is a long time. Wide-scale industrial adoption of a language is a matter for sociologists to study, and that science is even more in its infancy. Market considerations are an important factor — if Sun, Microsoft or Apple pushes a language, this has a lot more impact than any number of POPL and PLDI papers. Even for a programmer who has a choice, library availability is usually far more important than language design. Which is not to say that language design isn't important: having a well-designed language is a relief! It just usually isn't the deciding factor. Process calculi are still at the stage where the theory hasn't stabilized. We believe that we understand sequential calculations — all the models of things that we like to call sequential calculation are equivalent (that's the Church-Turing thesis). This does not hold for concurrency: different process calculi tend to have subtle differences in expressivity. Process calculi do have practical implications. A lot of computations out there are distributed — they involve clients talking to servers, servers talking to other servers, etc. Even local computations are very often multithreaded to take advantage of parallelism over multiple processors and to react to environmental concurrency (communication with independent programs and with the user). Are research advances needed to make better software? After all there's a billion-dollar industry out there that can't tell the pi calculus from a pie in the sky. Then again, that industry spends billions of dollars fixing bugs. “Will they ever be needed” is never a worthwhile question in research. It is impossible to predict in advance what will have long-term consequences. I would even go further and say that it is a safe assumption that any research will have consequences one day — we just don't know at the time whether that day will come next year or next millennium. 

The order of element can only be determined by comparing two elements. More precisely, the only possible operations on the input elements take two elements and return a result that depends only on their relative order and not on the rest of the input. Typically, the algorithm can contain where and are two elements of the input. Variants with or , etc., are equivalent. Nothing is known beforehand about the input data. In particular, the elements may be all distinct. 

This is true as long as you're only reducing towards a weak head normal form. But if you want to go further, you start reducing under lambdas. If you're looking at the pure lambda-calculus (typed or not), you must be reducing under lambdas if you're reducing closed terms. And when you're reducing under lambdas, you're no longer operating on closed terms. Here's an example that starts with a closed lambda-term where all binding sites use unique names, proceeds to a term with repeated but non-capturing binders, then reaches a term with capture, which requires alpha-conversion to proceed. The initial term is not simply-typed, but you can remedy this by changing it to $(\lambda w. (\lambda z. w z z) (\lambda x. \lambda y. x y))$ which is simply-typed and shows the same binding-related issues. $$ \begin{array}{rl} (\lambda \underline{z}. \underline{z} \underline{z}) \underline{(\lambda x. \lambda y. x y)} \quad \to & (\lambda \underline{x}. \lambda \color{blue}{y}. \underline{x} \color{blue}{y}) \underline{(\lambda x. \lambda \color{red}{y}. x \color{red}{y})} \\ \to & (\lambda \color{blue}{y}. (\lambda \underline{x}. \lambda \color{red}{y}. \underline{x} \color{red}{y}) \underline{\color{blue}{y}}) \\ \to & (\lambda \color{blue}{y}. \lambda \color{red}{y}. \color{blue}{y} \color{red}{y}) \end{array} $$ That last step requires alpha-conversion, to make the blue $y$ and the red $y$ distinct. Your proof sketch is correct as long as you apply the beta rule in a context that does not bind any variable. If you extend the proof to the lambda context case, it fails when $x$ is in the context. 

For more complex cases, or if your tastes run that way, you can use recursion closer to the way it's taught in math courses, building the fixpoint from a step computation and a separate well-foundedness argument, often using an integer measure. You can also make your definition look more like a classical program in a non-total language with a separate termination using the vernacular. 

Coq's fixpoint definitions require that inductive calls receive a structurally smaller argument. Deep down, a fixpoint construct takes a single argument: there's no built-in concept of a recursive definition over two arguments. Fortunately, Coq's definition of structurally smaller includes higher-order types, which is extremely powerful. Your two-argument fixpoint definition follows a simple pattern: either the first argument becomes smaller, or the first argument remains identical and the second argument becomes smaller. This fairly common pattern can be handled by a simple fix-in-fix. 

For the case, we need to show that the hypotheses are contradictory. We have a single hypothesis . Each branch of the disjunction is an impossible equality because the constructors , and are distinct. 

A semantics of a program is a model of its behavior which, like any scientific model, ignores aspects that you don't want to study. An extremely detailed model of the execution of a program would model the physical behavior of the computer that executes it, including the execution time, power consumption, electromagnetic radiation, etc. Such aspects are very rarely taken into account because they are very rarely relevant. Nonetheless they do matter sometimes: a useful model of an airplane autopilot needs to include runtime information, a useful model of a credit card's security needs to include electromagnetic radiation, ... In typical semantics, side effects such as timing and power consumption are ignored. Even if in a mundane setting where you type an expression at a Haskell interpreter prompt, the printing of the result is a side effect (if you try to print out an infinite object, it matters). If the Haskell interpreter runs out of memory, this is also an observable side effect in a “real-world” model, but not in an idealized model of Haskell that effectively allows unbounded computations. An observable side effect is one which is modeled in the semantics. In typical models of programming languages, memory consumption is not modeled, so a computation that requires 1TB of storage can be pure, even though if you try to run it on your PC it would observably fail. Another kind of non-observable side effect is one that is internal to the function. This is, I think, what most semanticists would think of when talking about non-observable side effects. Consider a computation that uses mutable data internally, but does not share this mutable data with any other part of the program. For example, a list sorting function which builds an array with the same elements as the list, sorts the array in place, and returns a list containing the elements as the array in their final order: a semantic model of subexpressions of this function exhibits side effects (modifications of the array), but the function itself has no external side effect, so it is pure. For a more subtle example, consider a function that writes some data to a temporary file and cleans up after itself. In a semantics where there is always enough room for temporary files and programs do not share temporary files, the function has no side effect; the temporary file acts as extra memory used by the function. In a semantics which takes filesystem full conditions into account, the function has a side effect — it may fail due to external circumstances. In a semantics that allows the machine to crash, the function has a side effect: if there is a crash during the execution of the function, the temporary file may be left behind. In a semantics that allows concurrently executed programs to see and maybe modify the temporary file, the function has a side effect. 

Proof irrelevance in general is not implied by the theory behind Coq. Even proof irrelevance for equality is not implied; it is equivalent to Streicher's axiom K. Both can be added as axioms. There are developments where it's useful to reason about proof objects, and proof irrelevance makes this nigh-impossible. Arguably these developments should have all the objects whose structure matters recast in , but with the basic Coq theory the possibility is there. There is an important subcase of proof irrelevance that always holds. Streicher's axiom K always holds on decidable domains, i.e. equality proofs on decidable sets are unique. The general proof is in the module in the Coq standard library. Here's your theorem as a corollary (my proof here is not necessarily the most elegant):