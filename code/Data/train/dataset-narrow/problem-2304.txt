I can show that every $\{+,\times\}$-circuit counting a related Hamiltonian path polynomial HP requires exponential size. Monomials of this polynomial correspond to $1$-to-$n$ paths in $K_n$ containing all nodes. Unfortunately, the reduction of $\#$HP to $\#$PATH by Valiant requires to compute the inverse of the Vandermonde matrix, and hence cannot be implemented by a $\{+,\times\}$-circuit. 

[ADDED 16.04.2016] I just realized that the case $r=1$ is indeed easy. If a boolean function $f$ is homogeneous (all minterms have the same cardinality) then $B_1(f)$ is at least the monotone arithmetic $(+,\times)$ circuit complexity of $f$. [Read-once circuits cannot use the idempotence $x^2=x$.] Let now $h$ be the spanning tree function: minterms are spanning trees of $K_n$. This is a matroid function (graphic matroid). Jerrum and Snir have shown that the $(+,\times)$ complexity of $h$, and hence, also $B_1(h)$ is exponential in $n$. On the other hand, as a boolean function, $h$ is just the graph connectivity function CONN, and a well-known pure DP algorithm of Floyd-Warshall yields $B(h)=O(n^3)$. So, my question actually asks what happens for $r>1$, the first interesting case being $r=2$. 

Adleman has shown in 1978 that $\mathrm{BPP}\subseteq \mathrm{P/poly}$: if a boolean function $f$ of $n$ variables can be computed by a probabilistic boolean circuit of size $M$, then $f$ can be also computed by a deterministic boolean circuit of size polynomial in $M$ and $n$; actually, of size $O(nM)$. To be a bit more specific, a probabilistic circuit $\mathsf{C}$ over a semiring $(S,+,\cdot,0,1)$ uses its "addition" $(+)$ and "multiplication'' $(\cdot)$ operations as gates. Inputs are input variables $x_1,\ldots,x_n$ and possibly some number of additional random variables, which take the values $0$ and $1$ independently with probability $1/2$; here $0$ and $1$ are, respectively, the additive and multiplicative identities of the semiring. Such a circuit $\mathsf{C}$ computes a given function $f:S^n\to S$ if for every $x\in S^n$, $\mathrm{Pr}[\mathsf{C}(x)=f(x)]\geq 2/3$. 

As we know, the $k$-clique function $CLIQUE(n,k)$ takes a (spanning) subgraph $G\subseteq K_n$ of a complete $n$-vertex graph $K_n$, and outputs $1$ iff $G$ contains a $k$-clique. Variables in this case correspond to edges of $K_n$. It is know (Razborov, Alon-Boppana) that, for $3\leq k\leq n/2$, this function requires monotone circuits of size about $n^k$. 

P.S. I will not "accept" this my half-answer (which came only right now after rethinking the connection with monotone arithmetic circuits), because a much more interesting Question 2 remains open: how much worse simple DP can be than Greedy when approximating optimization problems? This question is more interesting, because Greedy has often a fairly good approximation factor. This factor is known to coincide(!) with the so-called "rank quotient" of the underlying family of feasible solutions (see, e.g. this writeup. In the case of Max-Weight Matching problem, this quotient is $2$, and is at most $k$ for any intersection of $k$ matroids. On the other hand, as far as I know, DP based approximation algorithms usually use some kind of "scaling" of input weights, and only apply to "knapsack like" problems or some scheduling problems. A negative answer to Question 2 would confirm this seeming "approximation weakness" of DP. 

But what if we take one fixed graph $G\subseteq K_n$, and consider the monotone boolean function $CLIQUE(G,k)$, which takes a subset $S\subseteq [n]$ of vertices, and outputs $1$ iff some $k$ vertices in $S$ form a clique in $G$. Variables in this case correspond to vertices of $K_n$, and the function is just the standard clique function but restricted to the spanning subgraphs of one fixed graph $G$. 

Remark: I am aware of this paper where the authors claim $\mathrm{BPP}\subseteq \mathrm{P/poly}$ over the real field $(\mathbb{R},+,\cdot,0,1)$. They deal with non-monotone arithmetic circuits, and also arrive (in Theorem 4) to circuits with the voting function $\mathrm{Maj}$ as an output gate. But how to simulate this $\mathrm{Maj}$-gate by an arithmetic circuit (be it monotone or not)? I.e. how to get their Corollary 3? Actually, the following simple argument told to me by Sergey Gashkov (from Moscow University) seems to show that this is impossible (at least for circuits able to compute only polynomials). Suppose we can express $\mathrm{Maj}(x,y,z)$ as a polynomial $f(x,y,z)=ax+by+cz+ h(x,y,z)$. Then $f(x,x,z)=x$ implies $c=0$, $f(x,y,x)=x$ implies $b=0$, and $f(x,y,y)=y$ implies $a=0$. This holds because, over fields of zero characteristic, equality of polynomial-functions means equality of coefficients. Note that in Question 1, the range of probabilistic circuits, and hence, the domain of the $\mathrm{Maj}$-gate is infinite. I therefore have an impression that the linked paper deals only with arithmetic circuits computing functions $f:\mathbb{R}^n\to Y$ with small finite ranges $Y$, like $Y=\{0,1\}$. Then $\mathrm{Maj}:Y^m\to Y$ is indeed easy to compute by an arithmetic circuit. But what if $Y=\mathbb{R}$? 

Now consider the following restricted protocols for such a game: (i) the referees function if just $f(\alpha,\beta)=1$ iff $\alpha \leq \beta$, (ii) the messages of players are restricted to linear ones: in each round, Alice must send the message of the form $m_A(\vec{x})=\vec{c}\cdot \vec{x}$, and Bob the message of the form $m_B(\vec{y})=\vec{d}\cdot \vec{y}$. 

To see that the approximation factor cannot be larger than $r({\cal F})$, take a set $S\subseteq E$ and two its subsets $A,B\subseteq S$ achieving the minimum in the definition of $r({\cal F})$; hence, $r({\cal F})=|A|/|B|$. Give weight $1$ to all elements of $A\cup B$, and weight $0$ to the rest. Then the pessimistic greedy will output $A$, whereas the optimum is at least $|B|$. $\Box$ 

To be a bit more precise, suppose we are given a system of linear inequalities with integer coefficients. We know that the system has no $0$-$1$ solution. The variables are somehow split among the players (in fifty-fifty manner); this is the "worst partition" scenario: the adversary can choose the "worst" partition. Given a $0$-$1$ string, the goal of the players is to find an unsatisfied inequality. That is, the answer is now not a single bit, but the name of one inequality of our system. (This is a Karchmer-Wigderson type communication game.) 

Proof: Construct a random all-$1$ submatrix $R$ by picking each row independently with the same probability $p=1/(d+1)$. Let $I$ be the obtained random subset of rows. Then let $R=I\times J$, where $J$ is the set of all columns of $A$ that have no zeros in the rows in $I$. 

When restricted to $0$-$1$ inputs, every $\{+,\times\}$-circuit $F(x_1,\ldots,x_n)$ computes some function $F:\{0,1\}^n\to \mathbb{N}$. To obtain a boolean function, we can just add one fanin-1 threshold gate as the output gate. On input $a\in\{0,1\}^n$, the resulting threshold $\{+,\times\}$-circuit then outputs $1$ if $F(a)\geq t$, and outputs $0$ if $F(a)\leq t-1$; the threshold $t=t_n$ can be any positive integer, which may dependent on $n$ but not on input values. The resulting circuit computes some (monotone) boolean function $F':\{0,1\}^n\to \{0,1\}$. 

In a randomized DP algorithm, we allow some additional random variables be used as parameters in the recursion equations. Even if very vaguely defined, this model is uniform: we have one DP recurrence for all dimension $n$. But the model is "clearly" much weaker than the (universal) model of Turing machines: no loops, no "crazy" operations - just min or max combined with arithmetic operations. 

would imply a lower bound $\Omega(2^{N/2})$ on the non-monotone circuit complexity of an explicit boolean function $f_G$ of $N$ variables. If $G$ is $n\times m$ graph with $m=o(n)$, then even a lower bound $c(G_n)\geq (2+\epsilon)n$ is enough (again, see, e.g. here on how this happens). Lower bounds $c(G)\geq (2-\epsilon)n$ can be shown for relatively simple graphs. The problem, however, is to do this with "$-\epsilon$" replaced by "$+\epsilon$". More combinatorial measures lower-bounding circuit complexity (including the $ACC$-circuits) can be found in the book. P.S. So, are we by a constant factor of $2+\epsilon$ from showing $P\neq NP$? Of course - not. I mentioned this latter measure $c(G)$ only to show that one should treat "amplification" (or "magnification") of lower bounds with a healthy portion of skepticism: even though the bounds we need look "innocently", are much smaller (linear) than almost all graphs require (quadratic), the inherent difficulty of proving a (weak) lower bound may be even bigger. Of course, having found a combinatorial measure, we can say something about what properties of functions make them computationally hard. This may be useful for proving an indirect lower bound: some complexity class contains a function requiring large circuits or formulas. But the ultimate goal is to come up with an explicit hard function, whose definition does not have an "algorithmic smell", does not have any hidden complexity aspects. 

So, your question "Specifically do any of these problems have more than a linear complexity lower bound?" remains widely open (in the case of circuits). My appeal to all young researchers: go forward, these "barriers" are not unbreakable! But try to think in a "non-natural way", in the sense of Razborov and Rudich. 

Still, this is a "worst case" result: for every input weighting $x$, we have $\mathrm{alg}(x)\geq r({\cal F})\cdot \mathrm{opt}(x)$, and there exist an input weighting $x$ for which $\mathrm{alg}(x)= r({\cal F})\cdot \mathrm{opt}(x)$. But what about the "average case": can greedy achieve a better approximation factor on most inputs $x$? To be more specific, consider a random $m$-weighting $\mathbf{x}:E\to \{0,1,\ldots,m-1\}$ which assigns each element $e\in E$ its weight independently at random with probability $1/m$. The following two "amortized" versions of approximation factor seem to be quite natural: 

Sorry, I came across this 1 year old question only now ... In fact, there are lots of results showing that explicit graphs with some properties imply strong lower bounds for boolean functions. Say, graphs of high affine or projective dimension imply strong lower bounds for formulas and branching programs. There are also "simpler" measures of graphs, good lower bounds on which would have great consequences in computational complexity. Let me sketch some of them. View graphs as sets of edges. Let $s(G)$ be the smallest number $s$ such that $G$ can be written as an intersection of $\leq s$ graphs, each of which is a union of $\leq s$ bicliques (bipartite complete graphs). Easy counting shows that $s(G)\geq n^{1/2}$ for almost all bipartite $n\times n$ graphs. But by Valiant's results, every explicit bipartite graph $G$ (more exactly, a sequence of graphs) with $s(G)\geq n^c$ for a constant $c>0$ would resolve an old problem: would give a boolean function which cannot be computed by a log-depth circuit of linear size. It is conjectured that dense graphs without $K_{2,2}$ have large $s(G)$. Even better, let $Star(G)$ be the smallest number of fanin-$2$ union and intersection operations that are enough to generate $G$ starting with complete stars (graphs of the type $K_{1,n}$ or $K_{n,1}$). Counting shows that most of the graphs have $Star(G)=\Omega(n^2/\log n)$. But any $G$ with $Star(G)\geq (4+c)n$ for a constant $c>0$ would give an explicit boolean function requiring circuits of exponential size! If the graph has dimension $m\times n$ with $m=o(n)$, then even a lower bound $Star(G)\geq (2+c)n$ would have the same consequences. The best we can show so far is $Star(G)\geq 2n-1$. Let $Sym(G)$ be the smallest number $t$ for which there exists a subset $T\subseteq\{0,1,\ldots,t\}$ and a sequence of $t$ bicliques such that $(u,v)\in G$ iff the number of bicliques containing $(u,v)$ belongs to $T$. Again, counting gives $Sym(G)\geq n/2$ for most of the graphs. But by results of Yao, Beigel and Tarui any explicit graph with $Sym(G)$ larger than $2^{poly(\ln\ln n)}$ would give us a boolean function outside $ACC$. Warning: being "combinatorialy complicated" alone does not imply large $Sym(G)$: there exists strongly Ramsey graphs for which $Sym(G)=O(\log n)$, even if $T$ = set of odd integers. More details on how all this happens can be found here. 

Every monotone arithmetic circuit, i.e. a $\{+,\times\}$-circuit, computes some multivariate polynomial $F(x_1,\ldots,x_n)$ with nonnegative integer coefficients. Given a polynomial $f(x_1,\ldots,x_n)$, the circuit 

Given a family ${\cal F}\subset 2^E$ of (feasible solutions), the maximization problem on ${\cal F}$ is, for every weighting $x:E\to \{0,1,\ldots\}$ of ground elements, to compute the maximum weight $x(F)=\sum_{e\in F}x(e)$ of a feasible solution $F\in{\cal F}$. An algorithm has the approximation factor $\alpha\leq 1$ on ${\cal F}$, if it always (for every input $x$) gives a solution at least $\alpha$ times the optimal solution. 

We also have the following upper bound on the OR-rank of dense matrices. The argument is a simple variation of that used by Alon in this paper.