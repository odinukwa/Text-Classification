I suspect that this is evidence that your question, especially within the time bounds you mention, is going to be difficult to answer. 

Here are some points that help give perspective to the new results. The decision tree complexity result is big. One line of attack (and Jeff Erickson can say more on this) was to try and lower bound 3SUM via looking at the decision complexity of the problem (i.e the number of comparisons needed to solve the problem). The hope was that something close to $\Omega(n^2)$ was attainable. This result decisively trashes that argument with a $O(n^{3/2})$ bound. Note that this doesn't say anything about the true complexity of the problem. It says that a decision tree lower bound isn't going to happen. And that (along with other evidence) casts doubt on the basic premise that 3SUM is "morally" close to $n^2$. The algorithmic result is subquadratic unconditionally (i.e not in a word-parallel model). That is a big deal, although I suppose one might quibble about the fact that it's not $O(n^{2-\epsilon})$ for some constant $\epsilon$. As @domotorp says, this could very well be the beginning of a a series of new results. It's really hard to say. The current upper bound comes from "re-implementing" the decision tree algorithm with some magic tricks from Timothy Chan. It's conceivable that this could be pushed further. 

Let $P = (V,\leq_P)$ be a poset, and for each $x \in V$ let $x^P = \{ y \in V : x \leq_P y \}$. A well-known property of certain posets (forests, Young diagrams) is the existence of a simple hook length formula counting their linear extensions - while the problem is $\# P$-hard in general. To make this notion precise, we say that $P$ has a hook length formula if there exists a function $h : V \rightarrow \mathbb{N}$ such that for every $x \in V$, the number of extensions of $P | x^P$ is equal to: 

Are there linear-time recognition algorithms for these classes? By the previous remark, an algorithm for class 2 would immediately yield an algorithm for class 1, although I suspect a more direct algorithm to exist in that case. What is the complexity of the subpattern problem for two skew-merged or two vexillary permutations? The subpattern problem is known to be polynomial for separable and 2-increasing permutations, and these classes seem the next to study. 

It is commonly accepted that matroids provide an abstract setting for which greedy optimization works (although there are more general structures known as 'greedoids'). I was wondering whether there had been attempts to formalize a notion of 'semi-greediness'. Intuitively, it means that we would still construct a solution iteratively, but going from a solution $S$ of cost $i$ to a solution $S'$ of cost $i+1$ would work differently: instead of having $S'$ of the form $S \cup \{x\}$, we could for example have $S'$ of the form $(S \Delta T) \cup \{x\}$ i.e. we would replace $r$ elements present in $S$ by $r+1$ elements exterior to $S$. This could possibly model interesting problems such as bipartite maximum matching, optimizations in $\Delta$-matroids or 2-polymatroids. 

Probably the single most important guide to experimental algorithmic is David Johnson's article on how to do it, which also has lots of references. $URL$ 

Lattices (in this form) show up in theoryCS in (briefly) the theory of submodularity (with the subset lattice) and clustering (the partition lattice), as well as in domain theory (which I don't understand too well) and static analysis. But I'm interested in applications which use metric structures on lattices. A simple example comes from clustering, where any antimonotone submodular function $f : X \rightarrow R$ (antimonotone means that if $x \le y, f(x) \le f(y)$) induces a metric $$d(x,y) = 2f(x \wedge y) - f(x) - f(y)$$ This metric has been used extensively as a way to compare two different clusterings of a data set. Are there other applications of lattices that care about metric structures ? I've been interested in the domain theory/static analysis application, but so far I've not seen any need for metrics. 

The $k$-means algorithm for clustering is provably exponential even in the plane, but it works very well in practice. 

Background I was looking for a formulation of 'free sets' and 'independent sets' from linear algebra that would extend to groups. This question was considered here but I couldn't find a satisfactory definition in the literature so I'd like to suggest the following simple approach. Let $G$ be a finite group, and for each $x \in G$ let $o(x)$ denote its order, i.e. the smallest positive integer $p$ such that $x^p = 1_G$. Core definitions Let $X = \{x_1,\ldots,x_k\}$ be a $k$-subset of $G$ and let $A_k = \{a_1,\ldots,a_k\}$ be the $k$-letter alphabet. Let $\Sigma_k$ denote the free monoid generated by $A_k$, and let $\Pi_k = \prod_{i \in [k]} \mathbb{Z}_{o(x_i)}$ viewed as an abelian group. Consider the homomorphism $\phi_k : \Sigma_k \rightarrow G$ that maps each $a_i$ to $x_i$, and consider the homomorphism $\psi_k : \Sigma_k \rightarrow \Pi_k$ defined by $(\psi_k(w))_i = |w|_{a_i}$. We can then give the following definitions: (1) $X$ is a 'generating set' of $G$ if $\phi_k$ is surjective; (2) $X$ is a 'free set' of $G$ if there exists an homomorphism $\eta_k : Im(\phi_k) \rightarrow \Pi_k$ such that $\psi_k = \eta_k \circ \phi_k$. We observe that some properties from linear algebra carry over to this setting. For instance, any superset of a generating set is also generating, and any subset of a free set is also free. We may then define a 'free generating set' of $G$ the obvious way. Note that such a set may not exist, but when $G$ does admit a fgs we may define $rank(G)$ as the cardinality of this set. Basic remarks Similar to linear algebra, it turns out that two fgs always have the same cardinality. Here is a proof sketch for a special case. Suppose for contradiction that $G$ had two fgs $X,Y$ with $k = |X| < |Y| = l$, and suppose for simplicity that each element of $X \cup Y$ has the same order $p$. Consider the surjective mapping $\phi_k : \Sigma_k \rightarrow G$ obtained from $X$, and the surjective mapping $\phi_l : \Sigma_l \rightarrow G$ obtained from $Y$. As $Y$ is free, we obtain an homomorphism $\eta_l$ by (2). Given an element $x_i \in X$, there is a word $w_i \in \Sigma_l$ such that $\phi_l(w_i) = y_i$. Define the $k \times l$-matrix $M$ by $M_{i,j} = \eta_l(x_i)[j] = |w_i|_{a_j} \mod p$. For every $z \in G$, we can then write $\eta_l(z)$ as a combination of the column vectors of $M$. It follows that the image of the $\eta_l$ must be included in the subspace spanned by $M$ and thus has size at most $p^k$; therefore $\eta_l$ cannot be surjective contradicting the assumption that $Y$ is free. Some examples follow. For a finite abelian group $G$, we know that it is isomorphic to a product $\mathbb{Z}_{n_1} \times \ldots \times \mathbb{Z}_{n_k}$ and then $rank(G) = k$. For the permutation group $S_n$, it can be seen that $rank(S_n) = n-1$ as the transpositions form a free generating set; indeed, for a permutation $\pi \in S_n$ we can define a tuple $\eta(\pi) = (s_1,\ldots,s_{n-1}) \in \mathbb{Z}_2^{n-1}$, where $s_i = 0$ if $\pi(i) < \pi(i+1)$ and $s_i = 1$ otherwise. More generally, if $G \subseteq S_n$ is a permutation group, I conjecture that a free generating set can be obtained by the Schreier-Sims algorithm. Questions 

The LÃ¶wner-John ellipsoid of a convex set $C$ is the minimum-volume ellipsoid (MVE) that encloses it. The ellipsoid can be computed using Khachiyan's method, and there are a number of approximations available if $C$ is (the convex hull of) a set of points. Are there fast (i.e non-ellipsoid-method based) approximations to the MVE of a bounded polyhedron presented only in terms of the halfplanes whose intersections define it ? In particular, I'd be interested in methods that run in time polynomial in the dimension and the inverse error $1/\varepsilon$. 

Alice and Bob have n-bit strings, and want to figure out if they're equal while doing little communication. The standard randomized solution is to treat the n-bit strings as polynomials of degree $n$ and then evaluate the polynomials over a few randomly chosen elements from a field of size larger than $n$. This takes $O(\log |F|)$ communication. Suppose instead that we fix a lexicographic ordering over the strings and want instead to determine which string is "larger", which is equivalent to finding the leftmost bit where the strings differ. 

is it possible to adapt the above proof to handle distinct orders? if so, what part of linear algebra carries over to this setting? if we can compute the rank for permutation groups as suggested above, are there other, presumably infinite, groups for which this is doable efficiently? are there any algorithmic applications of this notion to problems involving graphs or permutations? 

Some structures have a property of closure by a "sum" or "product" operation. Given a family of structures $(S_i)_{i \in I}$, we can then define a new structure denoted by $\sum_{i \in I} S_i$, resp. $\prod_{i \in I} S_i$; structures enjoying this property are vector spaces for instance. I consider these operations as "undirected" as the result does not depend on the order of the summands, up to isomorphism. I am interested in structures for which we can define a "directed sum" / "directed product" operation, meaning that the definition of the sum / product could now depend on the structure of the index set $I$ (e.g. it could be a totally ordered set and we would compute non-commutative sums / products according to this order). To illustrate this, consider the following definitions for posets. Let $I = (G,\leq)$ be a poset and for each $i \in G$ let $S_i = (U_i,\leq_i)$ be a poset. 

It's not clear what an "algorithm" based on natural forces implies. Arguably, a quantum computer already operates based on 'natural principles' (excluding gravity, but including Maxwell's equations). What are the atomic steps in your 'natural algorithm' ? If you're talking about taking an $n$-body system and letting it "evolve" to perform a computation, how would you measure its running time ? Along these lines though, Roger Brockett did some interesting work in the 80s on viewing sorting and linear programming as the solution to a dynamical system. 

This is not a direct answer to your question, but it's related. The probability you want is precisely what is provided by min-wise hashing schemes. In particular, a min-wise hash takes a set and produces a single element, with the property that the probability of the two elements being identical is precisely the Jaccard similarity between the sets (intersection/union). 

A family of counter-intuitive results is the whole "prove an upper bound to prove a lower bound" family of results. The Meyer result that $\mathsf{P} = \mathsf{NP}$ implies $\mathsf{EXP} \not\subseteq \mathsf{P}/poly$ is one example of this, and this came to my mind from both Ketan Mulmuley's GCT work as well as Ryan Williams' recent result that again used an upper bound for CIRCUIT-SAT to prove a lower bound for $\mathsf{NEXP}$ in terms of $\mathsf{ACC}$. 

That is to say, $\sigma_i$ combines the effect of a swap and a conjugation at positions $i$ and $i+1$. It might be possible to solve this problem optimally in polynomial time, which would answer to your question. 

Let $G$ be a complete graph edge-colored with $k$ colors. We say that $G$ is Gallai-colored if no triangle is colored with three distinct colors. Fix a tuple of integers $c = (c_1,\ldots,c_k)$. We may then define the polytope $\mathcal{P}(G) \subseteq \mathbf{R}^V$ by: 

Consider a poset $P = (V,A)$. We may define a path structuring of $P$ as a chain $\Sigma$ of the form $X_0 \subset X_1 \subset \ldots \subset X_n$ where : (i) for every $x \in V$, the set $\{ i \in [n] : x \in V_i \}$ is an interval, (ii) for every $i$ we have $X_{i+1}$ of the form $X_i + \{x\}$ or $X_i - \{x\}$, (iii) for every $i < j$, $P$ contains no backward arcs joining $X_j - X_i$ to $X_i - X_j$. The width of $\Sigma$ is defined as $w(\Sigma) = \max_i |X_i|$, and the path-width of $P$ is defined as the minimum width of a path structuring of $P$. In particular, the path-width is always smaller than the width (defined as the size of a largest antichain). The following questions seem natural in this respect: 

Fano's inequality can be stated in many forms, and one particularly useful one is due (with a minor modification) to Oded Regev: 

While there are numerous individual examples of sharp thresholds (2-coloring vs 3-coloring is another one), probably the best structural result along the lines you're looking for is Schaefer's dichotomy theorem. Roughly speaking, the theorem looks at very general classes of problems (called constraint satisfaction problems) and completely characterizes (merely in terms of the kind of input) when the problem is NP-complete and when it's in P. I haven't read it, but the linked wikipedia page indicates a recent survey by Hubie Chen that presents this result in a more general framework. 

It seems obvious, but from personal experience, the idea that you can estimate the median of a collection of items using a constant number of operations is a little shocking. And if that seems a little too technical, you can always convert it into a statement about polls an elections (you need 1300 people to get a sample with 3% error, regardless of the population size). Related to this is the birthday paradox of course. 

As far as I know, the parameterized complexity of the counting problem is still open. It is known that ILP solving is fixed-parameter tractable in the number of variables ('Integer programming with a fixed number of variables', H.W.Lenstra Jr), although ILP solution counting is expected to be $\# W[1]$-hard. The question was first asked by N. Betzler if my memory is correct. 

It can be seen that this property is preserved by some natural operations: parallel composition, adjonction of a minimal or maximal element. This immediately implies the Knuth hook formula for forests, although it has a number of different proofs (including some of algorithmic nature). I'd like to know if there are examples of other operations preserving/breaking the existence of a hook length formula, in particular is it possible to define a gluing operation extending the above adjonction operation? 

By a graded PSG, we mean a pair $(S,r)$ consisting of a PSG $S = (X,*)$ and of a rank function $r : X \rightarrow \mathbb{N}$ such that whenever $x*y$ is defined, $r(x*y) = r(x)+r(y)$. This holds for instance for the interval algebra of an Eulerian poset (see On the Foundations of Combinatorial Theory I. Theory of Moebius Functions, by G.C. Rota). By analogy with the situation in posets, we may define a 'triangular matrix' $\lambda \in \mathbb{R}^X$ by $\lambda(x) = 1$ for every $x \in X$, and a mapping $\zeta : \mathbb{R}^X \rightarrow \mathbb{R}^X$ by $\zeta(x) = \lambda * x$. It is known that for a partial semigroup obtained from the interval algebra of an Eulerian poset, the function $\lambda$ has an inverse $\mu \in \mathbb{R}^X$ given by $\mu([x,y]) = (-1)^{r(y)-r(x)}$. This suggests a possible extension to certain graded PSGs, which leads me to ask the following. Question: for a graded PSG, under what conditions does $\lambda$ have an inverse $\mu$? When this holds, is it always true that $\mu(x) = (-1)^{r(x)}$? (NOTE: I renamed the rank function $r$ instead of $l$ for greater consistency.) 

Larry Wasserman has a recent post where he talks about the "p-value police". He makes an interesting point (all emphasis mine) (the premise in italics that I added, and his response below it): 

Space filling curves turn out to be useful when building quad trees for search. Sariel's book has more on this. 

One idea that comes to mind to exploit these things is to use a trie. Think of each $V_i$ as encoding a string consisting of all the dimensions where the vector has a nonzero entry. If you now build a trie over this set of strings, it will capture the common substructures that you'd like to exploit. For example, in your example above, $V_1$ becomes $124$ and $V_2$ becomes $1234$, and the trie structure will allow you to compute the $12$ segment simultaneously. But it's not optimal, because the dimensions don't have a natural ordering. If you wanted to couple this with a procedure that reorders the dimensions initially so that in each vector, all the 1s are as close to being consecutive as possible (you can do this heuristically using a hamming-space TSP on the columns of the matrix V), and then build a trie, then you can get even more improvement. Ultimately given your sizes (1024 and 20), simple heuristics are likely to work better than more complicated ones, so even the simple trie idea might help.