You should also point the db_create_file_dest to the default directory for creating datafiles. As long as you have restore the controlfile, then you can just do the following: 

I took some code snippets from a backup script that I use. You can redirect all output with the exec command, then redirect it back. You can also surround everything with braces and redirect that to a log file. Personally I think bash is a better shell to program in, if you don't have bash ksh works too. 

My feeling doing a Merge is the same as doing a select statement, then inserting data that does not yet exist in the target table and updating data where the primary key is already there. It sounds like you are saying that all of the data should be there in advance and you are just updating some columns based on new data. You could do a minus query first and if that returns rows then raise your exception. Otherwise you would do either a merge command or simply an update command. Row level triggers tend not to scale to large volumes of data. if you are doing batch processing, then you probably only have one process changing the data at a time, so worrying about concurrency might not be relevant. Why is it an error to not have data? 

My function is called several times per second (but only once per session) by a web application. The very first thing it does is lock the table (to do an 'insert if not exists'â€”a simple variant of an ). My understanding of the docs is that other calls to should simply queue until all previous calls have finished: 

The earliest compatibility list on the Internet Archive is for 2.1, and that is certified back to 10g only: 

You should probably start with the New Features Guide and in particular the section on 11.2.0.2. Note that: 

now the rule that "Ford" dealers can only stock "Ford" cars is enforced naturally by the model. Note that in the 'composite keys' example, may or may not be unique, according to preference. It does not need to be unique (ie an alternate key), but very little is lost by making it so (perhaps a little storage space) and it can be very handy so that is the way I usually set it up, eg: 

If you are very likely to hit a mis-match early, you may want to encourage the CBO to use (perhaps you could test this with the hint). In the worst case this will run very slowly on big tables especially if there is no index to speed up the lookup on , but the best case will be very quick. 

Then you just need to write the SQL that generates SQL, if you need to connect as a different user you can include a line like: SELECT 'CONNECT user1/cdjkfghljsdg@sidname' text from dual; But you need to figure out how to write the SQL, since you have not given many details. You will spool to a file, then turn on feedback, set pagesize 50 or so, probably set timing on etc, then run the output of the spool file. You may want to spool to a log file during the run phase. There are alot of examples, you just need to query for an example that matches what you want to do. 

You may also want to query dba_segments filtering by table space name, looking for the largest object. You should look specifically at the sys.aud$ table. 

You can define your own exceptions. If you have a package for defining business logic, you can define some exceptions in the package header then raise the proper exception when needed. Which means that you need to keep the exception numbers unique so that you can handle the exceptions from the calling procedures. ssociating a PL/SQL Exception with a Number (EXCEPTION_INIT Pragma) 

Also under sqlplus you can set the number of days for the control files to track the backup information for RMAN. 

On recent versions you can see a timer in the "Task Progress" view (View->Task Progress). Annoyingly it resets to zero the moment the task finishes! : 

This kind of solution has immense practical benefits if you have the luxury of restricting all access to the data through your transactional API. You lose the very important benefit of DRI, which is that integrity is guaranteed by the database, but in any model of sufficient complexity there will be some business rules that cannot be enforced by DRI. I'd advise using DRI where possible to enforce business rules without bending your model too much to make that possible: 

But note that I have arbitrarily chosen to when discarding 'duplicates' - and even more arbitrarily chosen to overlook that if two matching rows also have matching s the database will order them in an undefined and possibly unpredictable way. You will need to adjust that to meet your requirements. 

Instance Recovery is necessary to get the database back to a consistent state after or some other abnormal shutdown event. Oracle is never going to let you open the database in an inconsistent state, so no, there is no possibility that you can disable it. If Oracle is crashing during Instance Recovery you need to get in touch with Oracle Support or revert to a backup. The concepts guide has essential reading about instances and more particularly what Instance Recovery is and why it is sometimes needed: 

You should alter your tables and indexes and specify the amount of parallel that you want. How Parallel Execution Works 

If you have more than one backup of your control file, try using the oldest backup. Essentially the message is telling you that the control file is newer than the rest of the backup. for example if you backup the control file, then backup the data base, then backup the control file. Doing a restore using the second backup of the control file will result in the error that you are getting. Doing a restore using the older or first backup of the control file should work. 

Try changing the tables parameter to this. You only need parentheses when you have more than one table. 

I worked with an insurance company that used active directory on all of their Redhat Linux servers so that their DBA's could log in with their own credentials. The DBA's would then SUDO to Oracle which did not have a password. Hence, the upper management could track when DBA's logged into each server. Using AD can work well in Linux the manage authentication. It also makes for fewer passwords that the DBA's need to remember. 

I need an easy way to force SQLite to materialize the subquery so that is only evaluated once this is a simplified example for demonstration - my real world query does not have but selects from a view 

Ok, that changes everything - Flashback Data Archive is not for you. Instead consider moving to a 2-table solution: one for current data and one for history. 11.2 allows many more DDL operations such as than 11.1 but AFAIK there is no way of splitting the table into two copies retaining the history. 

If you are running a mixed workload (OLTP on the table and analysis on the snapshot), you could consider replication and a hot-standby This reduces the impact on the heavily loaded server to 0 - you need to be on 9.0 though... 

This also does the trick but 'not terribly fast' would be a huge understatement this time unfortunately - I'm posting it anyway in case it is of academic interest to anyone: 

I'd logically do (2) first with a join ("Restaurant has the same location as customer"), and then eliminate the results with a restaurant that sells a pizza that the customer doesn't like: 

I would like to be able to generate random fields of arbitrary length (<1Gb) for populating test data. What is the best way of doing this? 

Try just backing up the tablespace for the new datafile. Then as long as you have all of the archive logs that were generated since the last backup, your clone should work. 

You can't use utl_file, since it has a maximum line length of 32767. It's possible that sqlplus has a similar limit. If you are tying to move data to a different Oracle database you can use data pump. You can eve use datapump on a materialized view. If you want to migrate to a non Oracle database you can use Perl or heterogeneous services to create a database connection to mySQL, SQL Server, etc. Then you won't have to worry about the line length. UTL_FILE 

You won't need downtime just to add additional redo logs to your redo groups. Take a look at this: Oracle Configuration Best Practices And then look at this: Creating Redo Log Groups and Members You will need downtime to add additional control files: Creating Additional Copies, Renaming, and Relocating Control Files 

I have not manually created a database since 7.3.4. You can use dbca in silent mode without x-windows. Installing and Configuring Oracle Database Using Response Files 

I know someone who wrote a Perl program that woke up every few minutes to read the new lines of the alert log and send an email if there are errors. If you have grid you can probably configure that to notify if there are deadlocks. 

If I understand correctly, that is not a justification for table - you can have several tables with 1-to-many relationships with the same table. The only reason for introducing would be if the same comment can be attached to more than one response, as well as more than one comment being attached to each response, ie it is a many-to-many relationship - I don't think that is what you are trying to achieve. This does not answer your main question but may sidestep the problem in your updated trigger. Your trigger then becomes: 

This is what is called a 'skip scan' in Oracle terminology. Skip scans work best when the number of possible values in step (1) is relatively small (that is small compared to the size of the index) 

Timing can be turned on with at the psql prompt (as Caleb already said). If you are on 8.4 or above, you can add an optional on/off argument to , which can be helpful if you want to be able to set timing on in .psqlrc - you can then set explicitly in a script where plain would otherwise toggle it off 

If, on the other hand, the original archivelog files were deleted along with their backups, you do not have a full recovery set and it is a good thing you said "This is not a production database. I'm practising in a staging environment". A full backup run online is not a consistent backup â€” you need the archivelog files containing log records from just before the backup starts until just after it finishes or there is no way to restore the database consistently.