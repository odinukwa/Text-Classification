i don't know of a method to group the mails, but when you add dependencies to your hosts and services you will not get too much mails. 

what about a diff of your directory structures? Generate a directory listing of your remote mirror and your local system and transfer only the new files to the CD/DVD. 

it does not matter if you have a xen guest or a hardware system, you should partition it. The post contains a first idee of how to partition your system. Perhaps you want to go a bit further by adding additional partitions for and , but it depends on your system setup, how much logs are generated and where you install any additional software or where you save e.g. database files. Another good practice is to use LVM when possible. This way your system is more flexible and it is easier to make adjustments when your requirements change. 

but you can't mount a filesystem from the nfs server on the nfs client and you do not get any error message. what is the difference between your and calls? do you use ip adress in one and fqdn in the other? could you please post both commands with output and returncode? 

it is not the absolute exact cache hit ratio, but is good enough in most cases. and some more background info from apache. 

I don't know your distribution, but in SUSE you can edit the file and enter one or multiple proxy servers for system wide usage. Here is an example: 

the communication with ssh protocol is secure, but i think your admins have other reasons for not granting ssh access with the first request. when you gain ssh access to a system, you can normally copy files to the system and execute them (not only your jars). this way, you could upload files gaining root access or compromise the system in a different way. so when granting ssh access, no matter how secure it is setup, you have direct access to the machine and this is another attack vector to the system. the philosophy should be to enable as few services as possible. but i can't imagine your admins are not already using ssh. so there should be no additional risk on enabling ssh, but perhaps on giving away a new account on the server. ask them whether they use public key authentication or password authentication. it is a good security advice to use public keys. another reason for not granting you access, could be that they loose a bit of control over the server, because they do not know in detail what is running on the server. so tell them in detail what you want to upload and what these programs do. they might ask you some more questions or tell you you have to change something, but this way you can work together with your admins to get your jars running on the server. as a windows client, i agree with Chris Kaufmann, putty is a very good client on windows. 

i'm afraid there is no easy answer to this. first of all you have to find where program1 is redirecting its output to. to give you some hints what to search for, take a look at this. when you've found out, let program1 print to stdout. this way the output of program1 will get piped to the input of program2. as you say "gets upset about missing parameters" it could also be, that program2 is not able to read all information from stdin. when you use the output of program1 as parameters to program2, try xargs: 

the error message is complaining about your port. http normally runs on port 80 and https on port 443. you are trying to use https on port 80. change your call from 

a lot people are trying to adopt agile software development principles to system administration. and there has been more written about than i can tell you here. i will mention some resources later on, but first some hints: 

take a look at linux-ha. it is a cluster software which allows you to do what you plan without scripting anything. you would basically define a service in the linux-ha configuration which consists of a virtual ip address and the nagios daemon process. with linux-ha you install some agents on both systems. they exchange heartbeats and will detect a failure of the active node. in this case linux-ha will run shutdown scripts on the failed node for your ip and nagios (default init script) and run a startup on the standby server. the configuration is very well documented and there are some examples which will help you to get your cluster running. 

check your routes. perhaps you have different routes for different ips or not the correct netmask. another way to test your connection is to ping with a specified source interface. 

because your site offers sensitive data as you mentioned, it should be encrypted. Of what type are the design flaws? Is it possible to implement redirect rules for https to overcome the design flaws temporarily? 

when you use a proxy which is redirecting request in case of downtime, you only change your single point of failure (spof) from the webserver to the proxy. so you definitely need a cluster software (e.g. linux-ha or mscs as mentioned by Insanity5902) or cluster hardware (e.g. two load balancers with hot failover). dns has, besides the caching problem you already found out, a distribution delay which can be up to 24 hours. but you can use dns to do loadbalancing be giving an fqdn two ore more ip adresses. but this will not do the trick for every situation. imagine the ip is up, but your webserver is not responding, then the client will timeout. 

These metrics give you a good hint about how the application works and how memory efficient it is. These metrics only make sense for your customers when each application has its own process. But they are definitely very valuable for you. More application specific: 

just put the redirect rule in the vhost config of your deploy domain. this way it will only be active for this host. 

To access your balancer-manager, create an extra management vhost with in it. This way, you have one place where all your balancers are defined and one vhost to access the balancer-manager which shows you all your balncers. 

to get more info, take a look at this directive and the following. after enabling logging try it all again and look at the logs. have you checked whether case of your regex and the case of your files match? 

I want to capture some traffic with tcpdump for troubleshooting. The problem is, the error is not reproducible. To not fill up the hole disks with captures, I would like to capture the traffic with some sort of sliding window. Let's say I write the capture to a file and when the file reaches a size of 1GB it will drop the oldest packets and write the new ones. This way I would only get the traffic for some hours but hopefully enough to have the right packets when the user calls. I couldn't find an option for tcpdump. Has someone an idea how to solve this? 

as already mentioned the connection is not leaving the server. but you should try to avoid using the http request for performance/design/... reasons. when the source is in the same app/webserver, you should access it directly without doing the detour over http. 

you can also call , but this will only change the hostname until you restart your server the next time. as far as i can remember, these are all changes you have to make. 

What about splitting your config. Put your balancer definition outside any virtual host and remove the context: 

You have to add a routing rule for each webserver you want to access with usb0. Find out the IP addresses of the webservers and add 

there are two good tools i know off for measuring website performance: yslow by yahoo and page speed by google. these tools will give you a good overview where your page is spending time and give you some hints how to do it better. here are also some good blogs about page performance: High Scalability High Performance Web Sites in these blogs you might get some new perspectives and ideas about website performance. EDIT: here is an article which discuses performance. EDIT2: it seems it is now becoming even more important, as google calculates page rank also by speed: $URL$ EDIT3: here is a page with numbers correlating speed and business from google, bing, yahoo, mozilla and some others. 

RunDeck could also be descriped as the lightweight ControlTier. These tools not only give you a cluster shell, but also a web frontend and you can save your jobs for future use. 

you have two virtual host configured for 188.40.153.185:443. This confuses apache and he only accepts the first one. You should merge these vhosts or change the ip for the seconds vhost, depending on what you want to achieve. Or if your are using name based virtual hosting (possible with ssl, but not the best way) you need to add a directive as one of the error messages says. 

this sounds like you need a configuration management tool. this would be a central repository where you safe the configuration and clients (your windows vms) can get new configurations. as i'm using linux, i'm not sure how smooth this runs with windows, but there are solutions out there supporting windows. take a look at this wiki article. it has a nice sum up. 

it is not possible to use variables as Keltia already said, but perhaps mod_vhost_alias can help you with your problem. 

is it possible to replicate the mod_jk sticky session information to another apache for an failover setup? the idea behind the question is to setup two apaches with sticky sessions in front off some tomcats. when one apaches fails, the other one should take over the mod_jk session information so he knows which requests to serve to what tomcat. i know an alternative would be session replication at the tomcat level and not to use sticky sessions, but this is not possible at the moment.