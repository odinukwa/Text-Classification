OK, I get that. But then there is also continuous-delivery and continuous-deployment, and that's where I continuously get a bit lost: 

Integration with SCM tools Some time ago I was challenged by a customer to actually "integrate" the BMC alternative in their SCM lifecycle (managed by SERENA ChangeMan ZMF). The idea behind such integration being "Consider our DB2 DBA-team (doing stuff with DDLs) as a special case of a development team, accidentally using some exotic editor (the BMC tool) to produce the code (DDL) to be migrated". About the only (but real!) challenge about this integration was to also facilitate "restartability", which is one of the key benefits of such DBA tool: 

Code Reviews This is about having at least 1 other person look at the code written by somebody, eg to evaluate if it meets some predefined criteria like: 

The above is just a blueprint, which hopefully helps to understand how in the end it is the server that takes care of the segregation of duties ... provided you have the CxO cover you to impose some access rules that not everybody will like. To complete the picture as explained above, the server creates an audit trail (logging) of anything that's happening in the system. So that at any point in time, it is always possible to answer questions like 

The most important thing for DevOps Engineers in this kind of situations, is to get (a) Management Commitment and (b) Required Budgets. Read on for some more details on both ... Get Management Commitment Once that is in place, things become easy for such DevOps engineers. Especially whenever resistance (from all sorts of parties) comes into the game. Trust me, there will be such resistance, which challenges such as: 

Step 2: Configure the workflow (in the SCM system) After the permissions are configured in your security system (as in Step 1), all that's left to do in your SCM system is to configure how the various steps in the lifecyle match with the related security entities in your security system. That is, only those users who have the appropriate access to the required security entity, are allowed to request the server to perform the corresponding step in the workflow. Here are some examples of how you'd configure your SCM system to make some magic happen: 

To not further complicate things, just assume that it is agreed between all parties involved, that the SE-agent does NOT have to do any type of "verifications" about the deposits being done. That is: whatever is deposited is assumed to be complete, up-to-date, documented, etc. About "major new release": assume there are between 1 to 3 every year, which means that the licensed customer only expects to be able to get access (via the SE-agent) to those releases. Even though if there have been intermediate deliveries (like fixes or beta versions) to the licensed customer, those types of deliveries are considered out-of-scope. Even if it was only because: 

When using Jenkins to trigger some process (eg a remote build) at a remote location, why would you ever want to "fire" something, and then just "forget" about the outcome of what got triggered remotely? At least that is what "fire and forget" seems to be about. Such "fire and forget" sounds like not respecting some of the basic rules in IT: in whatever process (or program, etc) you write, try to always be prepared for unexpected conditions (return codes, etc) that may arise. So that you'll also have appropriate error handling in place. To the extend it does make sense, in the (mainframe) SCM tool that I'm most familiar with, you'd start some FTP process from a dev environment to a remote target (eg to distribute and activate executables). And sooner or later (asynchronously) somebody gets some kind of acknowledgement back (from the remote target to the dev site) that indicates "success or failure". And if within a reasonable amount of time no feedback at all is returned, you know that something is not working as it should. Using such scenarios, you'll never forget what got fired. Any examples for which using "fire and forget" in Jenkins does make sense? 

Remark: The same article also mentions "Chaos Gorilla: simulates an outage of an Amazon availability zone", though it could well be that this has been renamed now to "Chaos Kong: simulates an outage of an Amazon region" ... Talking about Chaos! I couldn't find any confirmation/docu on that so far, at least there does not seem to be an issue for it in the issue queue. An undocumented change might have made it to production on github ... Gggggggrrrrrreat! Setup and use your own Monkeys. Head over to github to get in touch with the Simian Army (same link as the very first link in your own answer). Here is a quote of what you'll find there: 

There are quite some questions and answers that mention "artifactory". I wouldn't be surprised if it is somehow related to artifacts. My questions: 

And I bet the reason for the "key prerequisite" (as in your quopted text), is that in the end you want to be able to perform some sort of comparison between 2 versions of such files. E.g. like a patch file (if usig GIT). Think about how a compare of 2 versions of a binary looks like, pretty sure every line is updated (or: all old lines are deleted, and all new lines are inserted). This applies to pretty much any operating system, be it Linux, Windows, ..., or even zOS (aka good old mainframes). 

To answer this question in the context of a mainframe environment, and specific to DB2® databases, there are typically 2 commonly used (not cheap ...) alternatives to pick from: 

In such case, you probably can find the answer to your question at $URL$ . At this very moment, it shows links to these locations: 

It seems like forget in this context is not really what it sounds like. Instead it is rather something like "". An example 'for which using "fire and forget" in Jenkins does make sense' (as in my question) is creating parallel chains to reduce the duration of pipelines, and combine it with using the Join plugin. Some more details about this plugin (from the linked page): 

But, probably the toughest part is to have adequate reporting tools available (and know how to use them). At least to (easily) satisfy requests from IT auditors (their questions can be very challenging). But also to point to relevant log records in your SCM system to answer all sorts of "What happened"-questions in crisis situations where (part of) production is down. 

A technique we've used in the past in similar situations is to get "management commitment" that imposes these rules to every team member: 

The server will ensure that the user trying to make something happen (like 'approve something') will only be able to do so, if the user's permissions are appropriate. That part is easy. But you don't want to use the SCM system to administer all those permissions for all the users involved, that's what belongs in your security system (not the SCM system!), so that you can adapt your workflow (in your SCM system) to go check those permissions whenever appropriate. The steps below provide some more details on that. Step 1: Configure the permissions (in the security system) 

With the above in place, any kind of update to be applied by the server to the library structure, will only be possible via a well defined workflow, which we call the lifecycle of a software change package (SDLC if you prefer). To actually execute the various steps in that workflow, this is what it takes to make it happen: 

Evaluation If you're using an approach as described above, various things may (will!) start to happen: 

However, doing some of those "experiments in production" may damage your reputation (not really your "points", but your real reputation). So as a good SE citizens, I would be happy to do those experiments in an environment that is designed just for that: a none-production environment designed for users like me who want to do such experiments. Voilà, if you were asked to configure test environments for SE sites, what would you do (or recommend SE management) about this DevOps issue? PS: to my knowledge, at this very moment, there is no such SE environment that I can use right now to go check if a user that I "invite right now" for DevOps (while it is in private beta), and which I know did NOT commit to DevOps, will actually be able to login to the DevOps site. So instead of discussing it with mods on some other site, I already ... tested it in production. And dispite what some moderator try to make me believe ... it did work (and the invited user now also has an account on DevOps.SE). 

Bonus: after the SCM integration of the BMC alternative got completed, the customer decided to change their DBA tool to the IBM alternative. And even though both alternatives don't really look the same, the impact of it to the SCM integration was rather minimal: simply a matter of replacing (in some reusable SCM customization) some calls to the BMC alternative by the equivalent calls to the IBM alternative ... which (using DevOps terminology) got implemented by feature-flags / feature-toggles. 

What the purpose of the software is, doesn't really matter. It could by CRM, CMS, BI, Accounting ... even DevOps! Nor does it matter what the hardware is about, or the OS it's running on: a small server (running some sort of Linux, etc), a huge mainframe running zOS, etc. Examples 

PS: From the above quote, it should also be clear what the relationship is with DevOps and Chocolats, Beer, Fries and ... Drupal. 

So at the risk it may not 100% fit the rules in your question, I thought I'd post what, IMO, is "the reference" to get an up-to-date answer if you ever wonder something like 

Let me explain a bit further ... My business / hobby / passion is scm, more specifically in mainframe environments. And wherever I go (to finetune stuff to fit the customers needs), the very first requirement the I impose (in my contract), is that any tuning done to the system we implement, is via that very same system. And by doing so (true, that takes like a few hours, say half a day at max), we get these benefits from it (incomplete list): 

In the SCM-world where I'm mostly familiar with, the above scenario is typically addressed by what's called the "abbreviated-approval list procedure. Here is a blueprint of it: 

Approvals to update some target environments This is about having at least 2 confirmations from some person and/or automated system before it is allowed to update some target environment (which may be live, or may be something like some master file / baseline library). Some examples are: 

Your question doesn't seem to make any assumption about the platform/OS it is about. Which is why it may make sense to add an answer about how part of such toolchain looks like in a mainframe environment. My answer is based on using the SCM product where I'm most familiar with (not sure if it's needed to disclose the product name). Your example about "Plan, Code, Build, Test, Release, Deploy, Operate, Monitor" seems to be missing these crucial phases: 

My question: In the context of DevOps, what's the difference between feature flags and feature toggles (if any)? PS: Looking at $URL$ , I don't get the difference between such flags and toggles. 

Your question doesn't seem to make any assumption about the platform/OS it is about. Which is why it may make sense to add an answer about how this is typically done/addressed in a mainframe environment, where the "engineers" (as in your question title) are actually groups of people were dozens (possibly hundreds) of people are involved. My answer is based on using the SCM product where I'm most familiar with (not sure if it's needed to disclose the product name). 

My take on this (if I was faced with such commandement, or whatever you'd call it), would be something like "". Because, accidently: 

With such solution in place, the call can be closed around 3:23 am ... since there will be no more red flag at 3:21 am ... ggggrrreat, time for a beer to celebrate my fix to get production going again (instead of coffee) ... and fingers crossed the outstanding post approvals will come in soon ... 

Read on for more details about the kind of tasks in each of those 4 groups ... Task descriptions Priority 1 - Operate the helpdesk 

With these procedures in place, all that's left to do is to periodically review each of those reports / reasons why it was required to use such special user ID, and ask the question "Is there anything that can be done to further automate this, to further reduce the need for such special login?". 

Mainframe (zOS) software is still very much in use right now. Many of the most critical applications of banks, insurance companies, airlines, global companies, etc are running on (good old) mainframes. However, for companies that are in the business of developing and supporting mainframe softwares, it may be (terribly) expensive to have the required infrastructure for developing, testing and supporting such software. What are possible solutions for developing and supporting mainframe software without paying fortunes for the required infrastructure (and spending all software revenue to it)? 

Apart from your "this", there is also Compuware Source Code Download for Endevor, PDS, and ISPW Plugin. Some details about it (from the linked page): 

Grouping of tasks An approach we've used in the past in similar situations is to organize the work of a team in 4 major groups of tasks, and allocate the equivalent of 2 FTE (Full Time Equivalents) to (try to) complete those tasks. In our case it was related to running an SCM helpdesk in a mainframe environment, with about 300 developers asking for all sorts of help / interventions from those 2 FTEs. The groups of tasks are organized in 4 possible priorities: 

Some additions to your own answer to this question ... Additional monkeys The article about "How chaos boosts performance" describes a few more of these monkeys, i.e.: 

A common practice for such SE-agent to get involved, is some sort of a legal person/entity, such as a lawyer. But to actually "process the SE-deposits" (by the SE-agents), all sorts of release management and/or software delivery tasks need to be performed by somebody or something (the poor SE-agent) who probably doesn't know at all what the licensed software is supposed to do ... fun guaranteed! My question: How can DevOps help to improve Software Escrow procedures as described above? Like what kind of toolchain-tools would you recommend to be used for the fulfillment of which part of the SE-agreement? And where appropriate using which (preferably open source) software solutions for it? Notes: 

Enforce predictable problems in production to be addressed before they actually have an impact in production (sounds like sales talk, no?). You might think this is just "impact analysis", but that is not it: 

There are typically the environments like unit test, system test, integration test, volume test, user acceptance test, etc (I bet there are yet another dozen of names like those). But they all relate to environments in which something differs from how things currently work in the real production environments. So none of these is what my question is about. To make this a less abstract question, consider this sample below: 

When configuring test environments, the issue often comes up where I ask the question to the customer like in my question title here: 

I'm not really familiar with the topics you mentioned. However I went ahead to go checkout kuleuven.be, i.e its Computer Science departement (with sufficient credibility, trust me). Here are some interesting articles in which DevOps is mentioned: