I am working on the optimization of a SP which contains some business logic using looping. I have removed the looping and converted those piece of code into some simple insert/update statements. Now I've to do benchmarking and compare old and new code in terms of execution time and logical/physical reads. My problem is because of the loop in my old code, how can I determine what is the total no of logical/physical reads. Because in SSMS, I can see thousands of IO stats statements like: "Table 'Employee'. Scan count 1, logical reads 3, physical reads 0, read-ahead reads 0, lob logical reads 43, lob physical reads 0, lob read-ahead reads 0." 

Changing the below setting and uninstalling some add ons for SSMS 2016 helped me - HKEY_CURRENT_USER\Software\Microsoft\SQL Server Management Studio\13.0\UserFeedbackOptIn Thanks @Kin. 

Page write is just an indicator of logical writes as well as physical writes. Logical write is an operation that happens when the page is being written in the buffer(dirty buffer) and physical write is an operation that happens when the same dirty buffer is converted to clean buffer and is being written on the physical disk. Definitely, this is not a single metric to judge memory pressure. You need to combine different metrics for coming to a conclusion. But in your case, it doesn't seem to be a problem as your page life expectancy is quite high which indicates a page will remain in the buffer for around 37k secs. So you don't need to worry about memory pressure I think. 

Just by looking at the query and predicate ("income in (32,33,34) and status_id = 6;"), you can't be sure that SQL will use covering index. It all depends on cost that SQL thinks is the lowest one. Not sure how many rows your predicate will have, but obviously, using Clustered index scan is cheaper than covering index. 

The only way I can think of is, using "Application Name" in the connection string. That is how SQL server recognises different applications. I think, you can append application name in the connection string at runtime using your v$session column. 

I am tuning an SP and re-written that to optimize. Now I am comparing old code and new code in terms of timing using "setting statistics time on/off". While setting time statistics off, my new code is performing well at least 4 times better than old code (Old code is taking 4 secs and new code is taking 1 secs to execute), but when I am setting time statistics On, my new code is taking approx 12 secs and old code is taking around 7 secs. Why new code is performing badly after setting time stats on? Looks like there is some cost of time stats also and in my new code, that cost is very higher in comparison to old code. Am I right? If so what is that? 

I am connecting to a database server(MSSQL 2008R2) using SSMS 2016 and for simple queries even like "USE DBNAME", it's taking 4-5 secs. However time stat shows "CPU time = 0 ms, elapsed time = 0 ms", not sure why? On the other hand, If I connect the same server using SSMS 2008R2, it gets executed instantly. I compared client statistics for both SSMS2008R2 AND 2016 but both shows almost same stats. Why my SSMS2016 is behaving like this? 

I'll always prefer to store that as a single column unless there is some specific business/application demand. Below are my points - 

You could schedule it in the built-in Event Scheduler. First create a stored procedure to do the flush of the binary logs: 

root@localhost allows a root user to connect to mysql locally from the DB server using the mysql socket file. root@127.0.0.1 allows a root user to connect to mysql locally from the DB server using the TCP/IP protocol. (Trust me, you will be needing this one. MySQL has a nasty bug in it. The bug sometimes causes the mysql socket file to disappear rendering root@localhost useless. You will need to connect using root@127.0.0.1 if this ever happens to you) root@server.domain.lan allows a root user to connect to mysql from server.domain.lan using DNS (Dynamic Naming Services) You could create root@'%' but I highly recommend you do not. I would also recommend using a different password for each root user. 

This will generate a file called (usually located in ), containing a map of all tablespace_ids that are in the Buffer Pool at that moment. You could copy this file to some folder and retrieve it later. You could reload by running 

It works as is. UPDATE 2015-01-16 14:46 EST Here are Queries to Report Sizes and Computes the Memory Units On-The-Fly Total MySQL Data and Index Usage By Storage Engine 

In your case, you must have innodb_file_per_table disabled. The actual table was inside the system tablespace file (usually located in ) My Guess 

You can extract the SQL from a specific schema with the Use mysqlbinlog utility against all binary logs that have the data and time ranges. You will have to name the database you are extracting: 

MySQL is capable of doing an INDEX MERGE which allows MySQL to search on both indexes and merge the results. Of course, the MySQL Query Optimizer is the arbitrator of this process. From my perspective, just looking at your query and its WHERE clause, I would create a compound index on your table and an additional index. Which compound index, or ? I would next check the cardinality of each column as follows: 

and then GIVE IT A TRY !!! I am sure there are other methods which involve doing a DELETE JOIN of the contacts table directly against itself. I prefer not to do so. 

and ditch the trigger. I also recommend using MyISAM over InnoDB in this case in order to alleviate additional table overhead. Give it a Try !!! 

You may need to contact your ISP with assistance on finding out your static IP. Once your find out, simple add root@'yourstaticIP' with the necessary grants. You should also use the following options in /etc/my.cnf and restart mysql 

Which index in has the most columns mentioned in the , , and clauses ? Not , but . How does MySQL locate the MAX value ? 

PROBLEM You are starting from the wrong log. You should start with not The timestamp on each binlog shows the last time a binlog event was written to it 

You will have to remember this subtle rule based on the table. If you do not remember this, creating a test database named or a database name whose first 5 characters is will reopen the same type of security hole. The most secure way around having to remember these things is to run these lines after an initial installation: 

Give it a Try !!! CAVEAT Notice that using MIN function helps keep the first pkey entered for fk. If you switch it to MAX function instead, the last pkey entered for fk is kept. 

recently my PC powered off abruptly for no apparent reason, leaving my Windows 7 installation severly damaged. I can still log onto the machine, but tons of Windows system files have been corrupted and I have no other choice than performing a repair install of Windows. At the same time, my MySQL 5.5.31 instance running on this system has been damaged. I am not sure if it's just the MySQL installation that got damaged, or the data itself. What I can say is that while the serve was certainly running during the shutdown, it did not perform any tasks at that time. What happens now is that when I try to run mysqldump from MySQL Workbench, it aborts after a couple of tables saying that it has lost the connection to the server. Most tables are InnoDB, a few MyISAM. How can I repair the installation and recover my data? Should I uninstall and re-install? Appreciate your help. EDIT: These are the two errors I get from the error event log in MySQL Workbench: "InnoDB: Page checksum 3072680322, prior-to-4.0.14-form checksum 1925954319 InnoDB: stored checksum 24388168, prior-to-4.0.14-form stored checksum 1077079361 InnoDB: Page lsn 3225980244 611338381, low 4 bytes of lsn at page end 2304517124 InnoDB: Page number (if stored to page already) 2370756659, InnoDB: space id (if created with >= MySQL-4.1.1 and stored already) 4293596229" "InnoDB: Assertion failure in thread 3988 in file page0page.ic line 745 InnoDB: We intentionally generate a memory trap. InnoDB: Submit a detailed bug report to $URL$ InnoDB: If you get repeated assertion failures or crashes, even InnoDB: immediately after the mysqld startup, there may be InnoDB: corruption in the InnoDB tablespace. Please refer to InnoDB: $URL$ InnoDB: about forcing recovery." 

First tests (running the same query on both machines) showed a definitive improvement in speed for the new one, but the queries still take a lot of time and I had expected more of a boost. The queries in question are fairly well optimized, i.e. all tables have proper key that are also being used as of "explain extended". Now to my current MySQL settings: First I should mention that I moved from MyISAM to Innodb long time ago. Some of my my.ini tweaks (i.e. departures from default settings): 

I'm running a MySQL 5.5 server on my workstation for scientific data analysis and wonder how to configure MySQL in order to get the most out of it performance-wise. The types of query that I typically run involve joins of 10-20 tables and can run for quite long, one to several minutes being no exception at all. Only very few users access the database at the same time (5 being the maximum). I moved the server from a Lenovo Thinkpad T61 with a 2.2 GHz Dual Core and 4 GB of RAM to the following brand-new machine with hand-selected components: 

I'd like to know whether somebody would suggest changes to the above numbers or even further settings that I do not know of. I'd appreciate any helpful remark. Steve EDIT: I have two queries involving joins across 10-20 tables and ran them on my Lenovo notebook and the new PC. Query #1 took 3m36s on the new machine vs 9m11s on the laptop; Query #2 took 22.5s on the workstation vs 48.5s on the laptop. So the execution speed was improved by roughly the factor 2-2.5. On the workstation, not even 50% of the RAM was used. The average CPU load across the four cores (as reported by Windows Task Manager) was only about 13%. The load on a per-core basis (as reported by Core Temp) was about 25-40% for ONE core, while it was <=10% for the others, indicating that MySQL does not make use of multiple cores for a single query. 

In some cases, you could face issues copying .MYD and .MYIs because of a major weakness MyISAM has: Data changes (changes to files) are cached in the OS. (Of course, this would be 100 times worse with MySQL in Windows, so I'll leave Windows out of this answer). If you are using MySQL 5.6, I have a little good news. The command would look all MyISAM tables under a single lock. MySQL 5.6 now has the syntax . The MySQL 5.6 Documentation says 

This will recommend the correct size for the MyISAM Key Cache. This maxs at 4GB for 32-bit. You can go higher on 64-bit machines. Yet, use common sense based on the amount of RAM the DB Server has. This should be a concern for you because only the pages from the .MYI are cached. Any thing from the .MYD must be read from disk over and over again. If the amount of RAM recommended from this query far exceeds the amount of RAM installed by orders of magnitude, just set key_buffer_size to 4G and call it a day on this perspective. PERSPECTIVE #2 : Leveraging Different Row Formats for MyISAM tables Altering MyISAM tables to use a FIXED row format can increases overall performance for SELECTs. Why ? Since CHAR fields require less string manipulation because of fixed field widths, index lookups against CHAR field are on average 20% faster than that of their VARCHAR counterparts. This is not any conjecture on my part. The book MySQL Database Design and Tuning performed something marvelous on a MyISAM table to prove this. The example in the book did something like the following: 

Queries that return lots of rows If you have queries returning lots of rows, tune your queries to return less data, perhaps adding effective WHERE and LIMIT clauses to SELECTs. Lots of queries If you do not have queries returning lots of rows, then it must be lots of queries. You may find this surprising, but MONYog queries mysqld for the global status variables with either 

You will see all columns with the type ENUM('Y','N') for each privilege. Adjust the INSERT query to match all columns and then run that INSERT. To get the exact columns to set to Y for all privileges, user this query: 

This WHERE clause never mentions Column a. Result? automatic full table scan. As far as order of columns in a table goes, defragging tables and making table formats with fixed row lengths could reduce any possible issues with table column order is that is a suspected concern. If anyone knows of issues with Oracle, PostgreSQL, SQL Server, or other RDBMS's concerning table column order, please chime in. 

EPILOGUE Whichever way one choose, make sure all the logs are good. If you apply the logs immediately, you'll know right then and there if the transaction logs are corrupt or not. You also need to take into account how transaction-heavy the production system is. Why? If the backup is recording incoming transactions as fast or faster than the backup process, then you have no choice but to go with PLAN B. 

The direct answer to your question is Yes, but it depends on the version of MySQL you are running. Before MySQL 5.5, replication would operate as follows: 

There was a bug report about this issue concerning "freeing items" and the query cache. Although the bug is closed, there was no mention of innodb_thread_concurrency. Conincidently, I spoke with Ronald Bradford at Percona Live NYC back in May. I told him of a situation where I tweeked innodb_thread_concurrency because MySQL 5.5's multiple InnoDB Buffer Pool produced a tons of thread locking and I suspected that cached data I need most likely had spread amongst the multiple buffer pools. He plainly told me that I should never set values against innodb_thread_concurrency. Let it always be the default value, which is now zero(0). By doing so, you let the InnoDB storage decide how many innodb_concurrency_tickets to generate on its own. This is what infinite concurrency does. 'Freeing items' most likely occurs more often when we impose limits on innodb_thread_concurrency. That should always be zero(0). I would take a risk and raise innodb_concurrency_tickets and see if it helps or not.