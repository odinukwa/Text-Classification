If I create a data driven subscription in SSRS and use the query to return some value for the username and password, how "safe" is this password? Let me explain. I am looking to set up a data source on the SSRS Web Portal that uses Credentials stored securely in the report server and Use as Windows credentials when connecting to the data source. This data source, in turn, would be used in the data driven subscription. The user setting up the data driven subscription would not have access to the data source's credential password. I would then like to permit the user to pull a username and password using this shared data source and then use in their subscription as the user/pass to save the file to a shared file area... without the user knowing what the user/pass is. Make sense? If so, is this a risky thing to do, security-wise? (Ultimately I am thinking about using this as a way to consolidate the password for shared file area subscriptions into one place.) Thanks! 

I am trying to do something that is very simple and am stumped why it is not working. I have a column ( from ) that is a varchar formatted like yyyymmdd. I want to convert that to a datetime. I use the rather straight-forward command but I keep getting the error Arithmetic overflow error converting expression to data type datetime. The three distinct values in this column are 20120802, 20120803, and 20120806. Using that same function above, I was able to successfully convert all three strings. I feel like I must be missing something obvious...? 

Your question is a little vague, but would two queries per rule get you there? For your example, perhaps make sure the count returned without the criteria is greater than 0? Also, the code you have provided is incorrect. Not sure if you meant instead of or quotes around the or what... 

In SSRS, once a report is deleted, the ItemPath no longer appears in the ExecutionLog because the Catalog no longer has the report information. I can track down the ReportID, but I can't find anywhere with the history of the report name. Is there anywhere I could find the name and path of the deleted report? If not, any suggestions on how to best stash the information moving forward? 

I think only you can answer this question by testing out a few things. The SQL Server End recommendation you list are really just good query-writing practice. Whether or not you cache the reports depends on how live your data is and how live your reports must be. When I am trying to improve an SSRS report, I optimize the query (in SSMS) as much as I can (by limiting nested views and adding indexes when possible). 95% of the time, this does the trick. If it doesn't, I work with the user to come up with a caching strategy - or more specifically, we refresh the report execution snapshot overnight and just display that data all day long. This works well for reports we don't mind running overnight when the database load is low. 

I am a replication rookie. We have a very simple transactional replication set up in SQL Server. I am looking at the Replication Monitor on the subscriber and one of my subscriptions show Poor Current Average Performance. My understanding is this is derived from the latency threshold but when I look at the detail of the subscription, Performance is Excellent and Latency is 00:00:00. For what it's worth, Synchronization Status is fine. Why is this current average performance poor? Over what time period is this "average"? This subscription has had no noticeable issues and has been active for some time. (I am asking for my own knowledge as well as ensuring there is not something broken/sub-optimal that should be modified) 

Addendum 3/26 - I like to think the lack of feedback means this is a fantastic idea, but I'd rather get some validation that I'm not treading in dangerous territory here. So let's try a rephrase: Is there any way a user can view the raw results of the query associated with a data driven subscription? 

I need to take some data from Table A, use some logic, and then insert one or more rows into Table B. I have a PLSQL block that brings in data from Table A with a cursor, performs all the logic required, and inserts the required rows. The problem is there are duplicate rows in Table B - it's the nature of the beast. But I need the final result to be a Table B with no duplicates. It's Oracle so temporary tables are bad form - what's the best way to accomplish this? 

I have a table and a view that is a select of the contents of . If I deny on to a user, the user can still use the view . Is there a way that I can force the view to return an error when the user tries to use it? The real world case here is a database with about 20 tables and hundreds of views. User access is logically tied to the tables so we are hoping to stop folks from using the views based on those tables. 

I have a report in a BIDS project that has been deployed to our SSRS Web Portal. On the web, it renders within seconds. In BIDS, it takes upwards of seven minutes. I can see that the query runs against the database quickly - what in the world is BIDS doing that is taking so long? I have seen this happen in a number of different cases... 

I have PL/SQL anonymous block that creates a table (using ) and then inserts data into the table. When the block compiles, it gives me an ORA-00942: table or view does not exist error. Well, no, it doesn't exist, but it will be created before the insert occurs. If I create the table before compiling, it will work. How can I handle this? 

I am sure there are other methods that I haven't listed. So - what's the fastest way to accomplish this task? 

I am looking to improve the performance of a view in SQL Server 2008. This view exists in a reporting database that is widely used by not-very-technical-folk to basically denormalize all of these attributes of a person. It's a really complicated, long running view. We have over 19 million people and there is a lot of logic that goes into each column. For example, there is an indicator as to whether or not a person is deceased and that relies on three CTEs (common table expressions) and a case statement. Basically, it's a nightmare. I need to figure out a way to improve the performance. Changing it to a table is not possible - data must be up-to-the-second accurate. Changing it to an indexed view is right out - it uses data from multiple databases. I can't really modify the column structure as it would break a number of existing reports. Are there any tools in the toolbox that might help? I'm wondering if stored procedures or functions might help. Maybe a table with computed columns? I would be able to pull the person's identifying information on a nightly basis and store that into a table, but the vast majority of columns rely on live data. 

Just to throw an idea out there - in our environment, the report developers have access to the source database but the report users do not. All of our data sources are deployed to a single folder and a service account is used for authentication of that data source. The report developers do not know the service account password. So with this set up, if a report developer deploys a data source somewhere else, the report won't work for report consumers, just other developers. (Technically, they must always use "do not overwrite data source" or all the reports are messed up until the administrator modify the "new" data source with the service account.) Hope this helps! 

I have a table that is the only source for a second table. Currently the second table is created using a CTAS (create table as select) statement that pulls in all the columns from the first table and includes a dozen more columns that are functions that we have written. These functions take a column or two from the source table and transform them into new data elements. In some cases, the columns that are used as parameters for the functions are sparsely populated. The functions are compiled natively and utilize the result_cache. The first thing that happens in each function is a null check on the input parameters and a return of NULL. I have a few questions: 

We are having some discussions about partitioning some application tables in SQL Server 2008 based on their quickly growing size. One is an event table that is typically used for INSERTS (80% of the queries) or SELECTS that look for the primary key of the table (20% of the queries). The other table is a "mapping" table that is all SELECTS looking up the primary key. Would partitions along the primary key be helpful here? I've done a lot of reading and all the classic examples of partitioning seem to be data warehouse tables that are partitioned on dates. It also seems that partitioning can sometimes cause more harm than good. What do you think about these tables?