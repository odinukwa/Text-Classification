It's easy enough to trade off time for space, as follows. Convert the regular expression to an NFA — for concreteness in comparing algorithms, we'll assume that $r$ is the number of NFA states, so that your $O(rs)$ time bound for directly simulating the NFA is valid and your $O(2^r)$ space bound for running the converted DFA are also valid whenever you're working in a RAM that can address that much memory. Now, partition the states of the NFA (arbitrarily) into $k$ subsets $S_i$ of at most $\lceil r/k\rceil$ states each. Within each subset $S_i$, we can index subsets $A_i$ of $S_i$ by numbers from $0$ to $2^{\lceil r/k\rceil}-1$. Build a table $T[i,j,c,A_i]$ where $i$ and $j$ are in the range from 0 to $k-1$, $c$ is an input symbol, and $A_i$ is (the numerical index of) a subset of $S_i$. The value stored in the table is (the numerical index of) a subset of $S_j$: a state $y$ is in $T[i,j,c,A_i]$ if and only if $y$ belongs to $S_j$ and there is a state in $A_i$ that transitions to $y$ on input symbol $c$. To simulate the NFA, maintain $k$ indices, one for each $S_i$, specifying the subset $A_i$ of the states in $S_i$ that can be reached by some prefix of the input. For each input symbol $c$, use the tables to look up, for each pair $i,j$, the set of states in $S_j$ that can be reached from a state in $A_i$ by a transition on $c$, and then use a bitwise binary or operation on the numerical indices of these sets of states to combine them into a single subset of states of $S_j$. So, each step of the simulation takes time $O(k^2)$, and the total time for the simulation is $O(sk^2)$. The space required is the space for all the tables, which is $O(k^2 2^{r/k})$. The time and space analysis is valid on any RAM that can address that much memory and that can do binary operations on words that are large enough to address that much memory. The time-space tradeoff you get from this doesn't perfectly match the NFA simulation, because of the quadratic dependence on $k$. But then, I'm skeptical that $O(rs)$ is the right time bound for the NFA simulation: how do you simulate a single step of the NFA faster than looking at all of the (possibly quadratically many) transitions allowed from a currently active state to another state? Shouldn't it be $O(r^2 s)$? In any case by letting $k$ vary you can get time bounds on a continuum between the DFA and NFA bounds, with less space than the DFA. 

It can be solved in randomized expected time $O(2^km)$ where $k$ is the size of the small directed tree to be found and $m$ is the number of edges of the large directed graph in which to find it. See Theorem 6.1 of Alon, N., Yuster, R., and Zwick, U. (1995). Color-coding. J. ACM 42(4): 844–856. Alon et al. also state that their algorithm can be derandomized but don't give the details for that part; I think the deterministic time may be a little larger, something more like $O(k!\,m)$. 

I tend to use something resembling Python syntax. Python is close enough to pseudocode already that in some cases my pseudocode can shade into being actual working code. 

Your question is kind of vague, and I don't know of a comprehensive listing of necessary conditions for Hamiltonicity (or equivalently sufficient conditions for non-Hamiltonicity). But for one such condition, commonly used to prove non-Hamiltonicity of certain planar graphs, see Grinberg's theorem. Another necessary condition, valid for all graphs (not just planar graphs), is that a Hamiltonian graph must be 1-tough. So if you can find a set of $k$ vertices whose removal disconnects the remaining graph into more than $k$ pieces, it's not Hamiltonian. 

I'm going to assume your input is a directed graph; I don't know how to do this for the undirected case. Make $n$ copies of the vertex set of your graph, where $n$ is the number of vertices in the graph. Replace each edge from $u$ to $v$ in your original graph by edges that go from copy $i$ of $u$ to copy $i+1$ of $v$, for all choices of $i$. Additionally, if $u$ belongs to the specified vertex set but not otherwise, also include an edge that goes from copy $i$ of $u$ to copy $0$ of $v$. The cycles in the expanded graph all project back down to cycles in the original graph, but every cycle in the expanded graph contains one of the specified vertices (otherwise you can't go backwards through the layers of expansion), so the original graph contains a negative cycle containing a specified vertex iff the expanded graph contains any negative cycle. 

One thing ACM brings to a conference is name recognition: if I see that a conference is ACM-sponsored, even if it's one that I haven't heard of before, I can be confident that it's reasonably high quality and well organized, rather than being one of those scam conferences that accepts everything and profits off the registration fees. (I'm less sure of that with IEEE after the Schlangemann scandal.) Of course, there are also good conferences that are not sponsored by these societies. And I wouldn't want to rely on this halo effect for making decisions about tenure or anything important like that — better to solicit opinions from people who know that area better — but it can be useful when deciding where to send a paper. 

The theoretical study of automata theory and formal languages is kind of moribund (meaning, you can probably still find interesting research problems to work on, but getting it published in top-level conferences and convincing someone to hire you once you graduate may be problematic). However, I believe there is also interesting work being done on applying formal language theory to internet threat/intrusion detection , etc., and this area seems much more hot right now. See e.g. Wagner and Dean, Intrusion detection via static analysis, IEEE Symp. Security and Privacy 2001 Wagner and Soto, Mimicry attacks on host-based intrusion detection systems, ACM Conf. Computer and Communications Security 2002 Giffin, Jha, and Miller, Efficient Context-Sensitive Intrusion Detection, NDSS 2004 Feng et al, Formalizing Sensitivity in Static Analysis for Intrusion Detection, IEEE Symposium on Security and Privacy 2004 

Mehlhorn, K.: A faster approximation algorithm for the Steiner problem in graphs. Information Processing Letters 27, 125–128 (1988) Erwig, M.: The graph Voronoi diagram with applications. Networks 36(3), 156–163 (2000) both references copied from Matthew T. Dickerson, Michael T. Goodrich, Thomas D. Dickerson, Ying Daisy Zhuo: Round-Trip Voronoi Diagrams and Doubling Density in Geographic Networks. Transactions on Computational Science 14: 211-238 (2011) 

There are many continuous problems of the form "test whether this combinatorial input can be realized as a geometric structure" that are complete for the existential theory of the reals, a continuous analogue of NP. In particular, this implies that these problems are NP-hard rather than polynomially solvable. Examples include testing whether a given graph is a unit distance graph, whether a given graph can be drawn in the plane with straight line segment edges and at most a given number of crossings, or whether a given pseudoline arrangement can be stretched to form a line arrangement. There are other continuous problems that are even harder: for instance, finding a shortest path among polyhedral obstacles in 3d is PSPACE-complete (Canny & Reif, FOCS'87). 

This was shown to be hard (more precisely $\mathsf{NP}$-hard to approximate to better than exponential in $k$) by Marco Di Summa, Friedrich Eisenbrand, Yuri Faenza, Carsten Moldenhauer, "On largest volume simplices and sub-determinants", arXiv:1406.3512 and SODA 2015. (They talk about the largest volume simplex contained in the convex hull, rather than the one that has the given points as vertices, but it amounts to the same thing since there is always an optimal contained simplex that has the given points as vertices.) They cite earlier hardness results on the same problem by Packer (ref.37 of the arXiv preprint) and Papadimitriou (ref.38). 

A couple of obvious ones are Valiant's proof that 0-1 matrix permanents are $\#P$-complete (also introducing the class #P) and Toda's theorem in the form that $P^{\#P}$ contains the polynomial hierarchy. But I'm really writing here to suggest a less obvious choice: the Immerman–Szelepcsényi theorem that space complexity classes are closed under complementation. The statement of the theorem doesn't involve counting but its proof does (more specifically a technique called "inductive counting" that has been used by others since). 

A mixed graph is a graph that may have both directed and undirected edges. Its underlying undirected graph is obtained by forgetting the orientations of the directed edges, and in the other direction an orientation of a mixed graph is obtained by assigning a direction to each undirected edge. A set of edges forms a cycle in a mixed graph if it can be oriented to form a directed cycle. A mixed graph is acyclic if and only if it has no cycles. This is all standard and there are many published papers mentioning acyclic mixed graphs. So the following algorithm for testing acyclicity of mixed graphs must be known: Repeat the following steps: 

For some more discussion of this particular paper, see this thread on a related Wikipedia talk page. Some of the participants in that discussion found specific bugs, and the paper does not seem to have been updated in response. I tried to read it myself but rather than finding any specific bugs I just got lost in vague descriptions of matrices and matrix manipulations that did not make clear which variables were inputs and which were outputs. Based on that experience I don't think the paper should be taken seriously until it's passed some level of peer review (accepted to one of the usual journals or conferences). More generally, it is easy to define algorithms for graph isomorphism that attempt to amplify some sort of subtle asymmetry in the graph to the point where it is obvious how to match the vertices to each other, and it is hard to find counterexamples for these algorithms, but that is very difficult from having a clear proof of correctness that works for all graphs. 

Because the input size (a description of the group and its generators) can be so much smaller than the graph itself, even standard polynomial-time graph optimization problems can become hard on Cayley graphs. For instance, shortest paths on circulant graphs (a special case of Cayley graphs) are NP-complete; see "On Routing in Circulant Graphs", Cai et al, COCOON 1999, doi:10.1007/3-540-48686-0_36 Of course that doesn't apply to the exact statement of the problem you give (where the group is given as a multiplication table, itself an object of size comparable to the Cayley graph) but it does indicate a need for some care.