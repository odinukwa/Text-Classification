I Currently have a service with a very high traffic (about 1000 connections/second, and this is not reducable with optimization anymore). Until 1 week ago, I was at AWS and had twiched some of my apache/NGNIX configurations to handle that load. There was no issue at all. I now want to change host and I went with OVH ; the new server config is 4x better than the latter (128GO RAM, 24 Core last gen processor with 30mb cache...) Now comes the issue ; on the new server I somehow get 503 errors (by apache) as soon as I pass the 600 connections per second. - First of all : Of course I know I must loadbalance the connections and I intend too ; but I want a clean config before i replicate it. - Apache is configured to handle 4000 concurrent connections and it does when I stress test simple So my Hypothesis : - Either OVH (new host) blocks my internal connections when too often. But they tell me they only block if I go over the 1GB/S bandwith (I don't - far from it) - Either Apache configuration is a bit different and makes server go into 503 faster than before (maby it doesnt like the 0,5 second between connecting to mysql and getting an result). Indeed there is a major difference ; on the new server (Ubuntu) my apache is behind an NGNIX reverse proxy and is in a docker-container whereas before it was a simple LAMP Does someone have an explanation of what is happening? I am totally lost & depressed. Thank you so much in advance. 

"yum install proftpd" should install the ftpd server. I would STRONGLY suggest just using SCP/SFTP instead though - which is part of SSH. Pretty much every app and file transfer program (i.e. filezilla) already supports this anyhow. It will give you encryption - so if someone is sending files from their home, someone cannot snoop the passwords. The easiest thing to do would be to change the owner/permission on the folders to just those specific users chown -R user1 /path/to/user1/dir chown -R user2 /path/to/user2/dir 

We use a service that allots us X number of requests per IP and has allows us to setup 5 IPs with such a limit (I know.. it seems stupid they could not just up the limit 5x on one IP). Pretend I have a linux box with the following address on the internet: 66.249.90.104 - that is an Google IP and not mine... so feel free to try to hack into it :) I setup apache+mod_proxy as a forwarding proxy (ProxyRequests On). i.e. you can setup firefox to use 66.249.90.104:8080 as a proxy, and all firefox traffic comes out as 66.249.90.104. So far so good. Problem: Now I add more alias interfaces so the total looks like this: eth0: 66.249.90.104 eth0:1 66.249.90.105 eth0:2 66.249.90.106 eth0:3 66.249.90.107 eth0:4 66.249.90.108 I run apache+mod_proxy (single apache instance) which binds to all interfaces, but no matter which address I connect to use the forwarding proxy, all traffic goes out to the internet as 66.249.90.104 I have also tried running 5 different apaches, each binding to its own interface only, but that still sends the outbound request through 66.249.90.104. I was hoping to get it to work as follows: I connect to 66.249.90.108 and make a proxy request, and it goes out as 66.249.90.108. I connect to 66.249.90.107 and make a proxy request, and it goes out as 66.249.90.107. etc. Has anyone else had to deal with this issue? The fall back solution would be to just run apache on 5 separate boxes, but I would prefer it to all work on one box. Thanks! 

Our server rack has two 208V 30A dedicated circuits running to it, each to a PDU mounted in the rack. Most of the equipment has redundant power supplies so one goes to each PDU. I'm not sure who had this installed but it seems like overkill for our relatively modest amount of equipment -- this is not a high-density data center. Our building gets frequent power flickers and we'd like to get a UPS in there to keep things running smoothly. The rack currently houses our SAN, so losing power unexpectedly is not ideal. The problem is that I'm having trouble finding a UPS for 208V/30A service. If we want to keep our dual power supply setup we're looking at dual 4kW UPS's at a total of $10k or so, which just seems silly when our power draw is closer to 1kW total. Is this 208V/30A setup standard and I'm just not looking in the right place for power equipment? Should we have an electrician change the L6-30 connectors for L6-20 connectors and put in a 20A breaker instead? Our other server rack runs just fine off a single 120V/20A circuit. 

you can squeeze a bit (4%) of space out of the filesystem by running: tune2fs -m 1 /dev/mdX (if you are using ext2 or ext3) tune4fs -m 1 /dev/mdX (if you are using ext4) This will change the reserved space on the filesystem from 5% to 1%. This can be a lot of space if you are dealing with TB sized luns. 

openvpn works great for this situation. You just generate a cert for each user (with the provided shell scripts). You can either do pem certs that don't need a passwd to connect, or require a passwd). if it is just 10 people, then a radius server is way overblown. Any modern machine would suffice. EDIT: not 100% sure how they could change their passwd on their own. 

$URL$ Depending on which window manager you are using, you can either to to Applications --> Add/Remove Software Or from a shell type "system-config-packages". If you get "command not found", then you would first have to run yum install system-config-packages 

Short Version: Is replication from 5.5 master to a 5.1 slave possible? Long Version: We did a large scale upgrade from 5.0 to 5.5. It was a long process if dumping the 400gb dbs and importing them into 5.5. 5.5 replication seems to be completely broken. The masters consistently hang, the slaves keep dis/reconnecting and leaving stale binlog_dump connections (visible from show processlist). The master hangs on shutdown until I have to kill when shutdown "gives up" after an hour. Lastly it gets this type error daily "110423 13:55:48 [ERROR] Slave SQL: Could not execute Delete_rows event on table prod.site_iplist; Can't find record in 'site_iplist', Error_code: 1032; handler error HA_ERR_KEY_NOT_FOUND; the event's master log mysql-bin.000385, end_log_pos 65644796, Error_code: 1032" which is a bug that is will be fixed in 5.5.12. This has been very disappointing, as our 5.0 setup ran fine for 3 years. Anyhow, I am looking to move to 5.1.56 (which has at least 56 updates to a stable product). The problem is that all my databases are 5.5 now. Is it possible to have a 5.5 master and a 5.1 slave? The migration process being to import the db into 5.1 and then enable replication, fail everyone over to 5.1 once it is synced up, and then downgrade all the other 5.5 servers while everyone is on the 5.1 db. Will 5.5 master --> 5.1 slave work at all? If so, will it work with the current MIXED mode replication? Would I have to change that to be statement only? Thanks! 

We're planning a forklift upgrade of our NAS and were going to use a utility like to move the files over. However we'd like to preserve Windows' "Previous Versions" shadow copies. Any way to do this? Will it happen automatically? I can't find any information about Shadow Copies with Robocopy one way or another. Edit with clarification: Both NAS devices are EMC, although we were not planning on using the EMC upgrade tool. (Maybe we should reconsider that decision, but that's another question.) The shares are CIFS/SMB and are accessed by Windows domain users. End users can access previous versions of files in Windows Explorer by selecting a file and going to "Properties" -> "Previous Versions". So somehow Windows is aware of the alternate versions. My understanding was that previous version of the file was stored in something like a NTFS resource stream, which it why I think of it as basically file metadata. But maybe that's not right, or not how EMC does it. I don't know what filesystem EMC uses under the SMB covers. But if Windows can access the old file versions over SMB, why can't Robocopy? 

I am looking for some help to twich my MYSQL server. I am running a 5.7 MYSQL server on a 64 GB high end processor server (nothing else on it) and I just am unable to set the config for it to use all available ressouces. So first off, I know I should soon replicate the server and have Slave/master Mysql - ill do it as soon as I exploit my first server fully. I also spent weeks in optimizing the queries and different caching (apache side) to limit the amount of queries. Now I need to make some config for it to be nice. The situation; I have a huge amount of traffic and a huge amount of queries - and mysql seems not to be able to handle it anymore. I currently run at around 700 Selects/second (only 20 Updates and 30 Inserts) (My website and apache runs at 3k simultaneous connections). My first thought was MaxClients that I took up to 250; I however noticed that I never have over 7 simultaneous "Connections" (says MysqlWorkbench). I tried understanding things the "Performance Report" tells me but I do not find any wierd numbers. This is why I need your help. This is the current configurations I did for MYSQL : 

Here is a writeup of the capacity service. Essentially it keeps track of things like data growth over time, transactions per second growth, etc. It then tries to come up with a trend: $URL$ Sadly this is more of a "for management" metric though and usually does not mean anything. It just takes one new app/script/etc added into the environment to make all the trends invalid. At least that has been my experience. 

So this appeared to be a kernel bug in 64bit Centos 5.4 AND 64bit Fedora 14. After I installed Centos 5.5, then problem went away. Sorry I dont have a better answer for everyone... 

Add this to your java_opts: "-XX:+UseConcMarkSweepGC" This is a multi threaded GC that works much better under high load. 

in the httpd.conf file, if you have your CustomLog set to "combined" (which includes %I and %O for in/out sizes) then it tells you the size of each request. to get all in/out, run: 

this seems like a silly question... "I need to use a database, but I refuse to use a database..." use one or don't. not really sure why you hate databases.. then are really simple, stable, and work with every language. You could go for something like hadoop and hive/pig, but that is WAY more then you are looking for and far more complicated the mysql. You need to give more info as to: 1) what you are trying to do 2) the data set sizes involved 3) the usage patterns that this will be hit with 

We have a laptop with intermittent wifi connectivity problems. Rebooting often helps. Today while digging around I noticed that its DHCP IP address and gateway are not addresses on our subnet. The DCHP lease it's using hasn't expired but based on the timestamps it was probably obtained while on a different network with a different SSID. For some reason the laptop is trying to use that DHCP lease on our network but has no connectivity because the gateway doesn't match. I suspect if I do a it will get a new IP address correctly, but I want to know why it's trying to use a lease from a different DHCP server. Is this a misconfiguration on the laptop somehow? Or a problem with our office DHCP server? Edit with more info: Our company wifi network has a non-generic SSID and a subnet of . The DHCP and gateway are both on that subnet. Wifi access points are set to "bridge" mode (our Windows domain controllers do DHCP for the LAN including Wifi). The user of the laptop came in today saying he had been using the laptop at a client's office that morning (don't know the SSID but I can pretty well guarantee it wasn't the same as ours). The laptop connects to our wifi network but gives a "no connectivity" error and has no access. reveals that the wireless card has a DHCP-assigned IP address of and a gateway of . Not our network! The lease was issued that morning, presumably by the client's network, and is still valid. But obviously it's the wrong network. Edit 2: The Plot Thickens We disconnected from the company wifi and connected to a different one, and the laptop works. It's given an IP address of (different network, different DHCP range). Wifi works fine. When we bring it back onto the company network it doesn't work again, but this time shows that it's holding onto the IP address!! So something about that laptop or our DHCP server is causing the laptop to just use a random old DHCP lease when connecting.