If you want to allow a student to be on more than 1 team then just add TeamID to the PK. Or you could skip StudentTeam and put TeamID in student table. At least you only enter TeamName once. Student: ID (PK) Name TeamID (FK to Team.ID) allow null 

I don't think you mean contains I think you mean if any code has that value I think this is what you are looking for 

That changes the left join into an inner join but since there is a match in your data you would get the same results. 

To consider Remove words that contain a smaller word There is no purpose to search on 'indent' if you have searched on 'in' 

If you have no option to disable FK or take the database offline you could delete in batches with a wait (sleep) the keep the cpu down. It will take longer but that might not be a problem. 

1000 transaction per minute = 16.67 / second = 480,000 / 8 hr day 16.67 / second is not the fast. I am getting over 100 / second on just a regular active big table. Pick your PK or at least one index that you can sort the incoming data by so you have minimal fragmentation of that index. If you can hold records to insert 100 or 1000 at a time and insert them sorted. One insert of 100 records is much faster than 100 inserts of one record each. Have timer that they are insert at least every x seconds. On the other indexes pick only what you need. Give them a fill factor of like 50. You would be amazed at how much slower fragmentation takes place if you leave some space with a fill factor. Perform index maintenance daily. Yes you may very well need to get more exotic but 1000 / minute is not that big. Even if you do get more exotic index design that minimized fragmentation is still a good thing. 

an index on försystem should help This index on the view should help person_id, termin_fakta, läsår_fakta Do you need to use the view? Try going straight to the tables. You may be doing stuff in the view that you don't really need for the delete. Optimize the select then try it on the delete But the index on [sko].[stage_närvaro] may help the select but hurt the delete. Index adds overhead to the delete. 

This belongs on dba.stackexchange.com but I don't have the rep to comment (and the reason I don't have the points is I gave out a 100 point bounty) You don't give the sql an algorithm. You give it indexes and a good query (some times with query hints). SQL will parallelize on its own. PartyID would be better as an int and then have another table for the text Int or VarChar put and index on it 

But you have an index on TweetID. Is that index not used by the joins? Even a non-clustered index will fragment with insert and update. As that index fragments inserts take longer. How are you managing fill factor and index maintenance on this non-clustered index? An unused PK does nothing. TweetID as PK will save some space. What % of rows get added daily. If only 1% rows get added daily you could use a fill factor of 90% and defrag daily or weekly. Start with like 80% or 90% and see how fast it fragments. If the TweetID are out of order but in the same range then the problem you have there is fill factor gets hammered in that range and unused in the rest of the range. But a defag is faster as it is only cleaning up that part of the range. In that case you may be better off with 100% fill factor and a regular index reorganize. There is no cut and dry answer. You need to manage and monitor the index for your data and your workload. 

If the tables are getting hit 100-200 times a minute then they are (hopefully) in memory. The load on the server is very very low. Unless you have high CPU or memory on the database server this is likely a non-issue. Yes the queries take shared locks but hopefully the are not blocking any update locks nor being blocked by any update lock. Do you you have any update, insert, or delete on these tables. If not I would just let it go - if you are having performance issues there have got to be bigger fish to fry from a database server perspective. I ran a test on 100,000 select count(*) on an empty table and it ran in 32 seconds and the queries were over a network. So 1/3 millisecond. Unless your network is getting overloaded this is not even impacting the client. If you are having major performance issues these 1/3 milliseconds blank queries are not what is killing the app. And these could be just part of a left join grabbing some static type data the is not part of the current application. It could be chained with with other queries so it is not an extra round trip. If so yes it is sloppy but it is not even causing more traffic. So back to look at the actual statements. Are you seeing any updates, adds, or deletes on these tables? Yes many empty tables and queries to empty tables are indication of sloppy coding. But if you are having major performance issues this is not the cause unless you have some really sloppy write operations also going on with these tables. 

For sure you should have an index on PP.OBJECT_CD Make sure it is not fragmented Start with the above and examine (and post) the query plan Consider an index on #TEMP.price but that is going to slow down the insert More important why are you even using #TEMP in the first place? 

This is SQL but I suspect it holds up for mysql A FK saves space and memory Yes there is overhead to a join but if both sides are indexed then a small overhead Consider a FK that is repeated a lot in the referencing table - like an audit table may repeat each user 1000+ times. a1) 

What is your logging level? You do know simple will clear the transaction log between statements? If you want to minimize logging then multiple updates 

Make sure the join columns are indexed. Declare PK and or unique if that is the case. If it is PK/unique then the join can stop when it finds a hit. 

The where negates the left Why make it hard on the optimizer? At 3 or more joins the optimizer will TEND to go defensive and into loop joins as that protects memory An or condition in the join it will also tend to go into a loop join - do I have hard evidence it will happen every time - no - still a reality With multiple joins pull conditions from the where into the join when you can