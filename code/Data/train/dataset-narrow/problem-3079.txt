Actually, there is not much theoretical justification for classical imputation. I think your method makes sense when just very few of the predictors have missing values. For instance, if there is just one predictor that has missing values, you can build a model to imput those missing values. However, as you said, things scale very badly. Moreover, apart from computational issues, if there are lots of missing values in every predictor your data is not very good, and therefore any predictive model won't be very good anyway. This is done sometimes, but I don't think there's any library in R that implements it, you'll have to code it yourself (I don't think it is difficult). If you do it and your final model is a logistic regression, I don't recommend to fit the missing values with another linear model, as you will suffer from a collinearity problem. One other thing that is typically done is that, for every predictor with missing values, create a binary predictor that is $0$ if the other predictor's value is missing and $1$ otherwise. 

Let me give you an example where Andrew's recommendation works better than yours: Let's say that the real gradient is $(0, 0, 0)$ and the gradient you have computed is $(10^{-4}, 10^{-4}, 10^{-4})$. Then your average would return $10^{-8}$, and Andrew's recommendation would return $1$. Your metric could fool you into thinking that your gradient is computed propperly and the error is just due to a numeric issue, while Andrew's cannot fool you into that, due to the fact that it considers the fact that the gradient can be very small. To wrap up, if your gradient doesn't have norm close to zero, it wouldn't really matter. However, when the gradient is close to zero you can be fooled into thinking that your gradient is right when it is not. 

In SGD you just feed an example to your model, compute the gradient of the loss function of that example and update the weights according to the gradient of the loss of that example. In mini-batch gradient descent you feed a batch to your model, compute the gradient of the loss of that batch and update the weights according to the gradient of the loss of that batch. In fact, SGD is mini-batch gradient descent with batch size equal to 1. 

Outliers: In decision tree learning, you do splits based on a metric that depends on the proportions of the classes on the left and right leaves after the split (for instance, Giny Impurity). If there are few outliers (which should be the case: if not, you cannot use any model), then they will not be relevant to these proportions. For this reason, decision trees are robust to outliers. Null values: You have to replace them (unless the software you use already does that for you, which is not generally the case). Edit about outliers: What I have said in outliers is only about classification trees. However, it is certainly not true in regression tress. Regression tree split criterium depends on the averages of the two groups that are splitted, and, as the average is severly affected by outliers, then the regression tree will suffer from outliers. There are two main approaches to solve this problem: either remove the outliers or build your own decision tree algorithm that makes splits based on the median instead of the average, as the median is not affected by outliers. However, basing the tree algorithm on the median will be very slow, as computing the median is way slower than computing the average. 

I want to do KDE on data that are not necessarily normal using Gaussian kernels. In KDE in wikipedia an expression for the bandwidth is given when the underlying distribution of the data is gaussian. I am looking for a similar bandwidth selection for non-normal data. 

When evaluating results using cross-validation, several strategies can be adopted, as using 5 or 10 folds, or doing leave one out cross-validation, as well as doing a 80/20 split. Under which are general conditions should I try one or another? 

I think the Rbf (Radial basis function) kernel (Wikipedia for rbf) is the most suited for this problem, as its contour plots are circles. For instance, this blog solves very similar problems using the same kernel. 

I would guess it is using the one vs all approach. You measure the auc of the ROC on each class adding the probabilities of all other classes to create the other class. Then you average among classes the obtained areas under the curve. 

Tensorflow has much more flexibility to do the things that you want. If you just want to train a standard CNN, then perhaps Theano will do the Job. If you want to do more funny stuff, like creating a new architecture, using a new learning rate schedule or test time augmentation, Theano is too much restrictive. I have also heard that Tensorflow is a bit more performant so it is better to use in production. 

You can think of examples as vectors in $\mathbb{R}^p$, where $p$ is the number of features. Two examples will be very similar if the distance between them is close to $0$ (in the extreme case, if two examples are equal their euclidean distance is $0$). One way to measure the distance is using euclidean distance, but other distances can be used, as cosine distance or $L^p$ metrics. In fact, if $p$ is very high, then Euclidean distance is not a good measure, as it tends to make the distances too uniform (see this paper). Edit: When $p$ is very high: See this magnificient answer to the issues that very high $p$ may have. 

My question is: which is the best practice regarding the combination of these techniques? Can they be applied simultaneously or is it better to use one at a time? 

Your dataset is highly imbalanced. Your optimization process is just minimizing the loss function, and cannot do better than a model that predicts uninteresting regardless of the input, due to the fact that your training set is very imbalanced. Moreover, you are not overfitting, since your training accuracy is lower than your validation accuracy. In order to have a model that learns something less dummy than your model (and you might have to pay the price of having a lower accuracy), I would do the following: when providing a mini-batch to your optimizer, generate a mini-batch that is more balanced, that is, bias the elements you select towards the interesting articles. For instance, if your batch size is 64, ensure that it has 32 interesting elements and 32 uninteresting elements. Using this your network might start learning some features regarding the words in it, and in principle it should help you achieve a not so dummy predictor. 

The penalty of both Lasso and Ridge is proportional to the magnitude of the weight. That is, the penalization added to the cost function is $\lambda ||\omega||_2$ or $\lambda ||\omega||_1$. Wether it is more convenient to apply regularization or feature selection, Lasso already does some feature selection for you, as the estimated weights for Lasso are sparse (there will be many coefficients equal to 0). About multi-colinearity, Ridge tends to eliminate variables that are colinear, while Lasso doesn't. All that I have said is taken from An Introduction to Statistical Learning book, from James, Witten, Hastie and Tibshirani. 

The accuracy given by Keras is the training accuracy. This is not a proper measure of the performance of your classifier, as it is not fair to measure accuracy with the data that has been fed to the NN. On the other hand, the test accuracy is a more fair measure of the real performance. As there is a big gap between them, you are overfitting very badly, and you should regularize your model. Consider using dropout or weight decay. 

Most of the critical points in a neural network are not local minima, as it can be seen in this question. Although it is not impossible to fall into a local minimum, the probability of it happening is so low that in practice it does not happen, except from very special cases as a single-layer perceptron. Being local minima so hard to find, this means that it is highly unlikely to come across the global minimum in your optimization method. All that we do in deep learning is decrease the loss function to find fairly good parameters, but finding local and global minima is extremely unlikely. 

I think it is ok, as long as your training and test data have the same maximum values for every feature, approximately. The idea is that the scaling has to be done with the training set (remember that using the test set for anything that is not testing is illegal, not even for scaling). So, you actually fit $y'$ as a function of $X'$, and you have a model that maps properly $y' = f(X')$. When you get your test data, you just obtain the predictions by doing $f(X_{test}')$. As the paragraph before states, if you have that $scale \approx scale_{test}$, then you can just recover $y_{test}$ by doing $y_{test} = scale \cdot f(X_{test}')$. Edit: Don't worry about nonlinearities Even if the function $f$ is highly nonlinear, it is a function capable of mapping $X'$ to $y'$. If you trust this function and trust the fact that $y = y' \cdot scale$, then there is no need to worry about the way $f$ acts, as function composition makes sense for all kind of functions, both linear and nonlinear. 

Multi class log-loss is used in many data science competitions. Although not so easily interpretable as accuracy, it penalizes based on your confidence on your predictions. If the models you are using output probabilities, it may be a better way than accuracy to compare and select different models on your validation data, as it takes into account the probabilities and not only the amount of correct predictions. 

With weighted linear regression, it is exactly the same, as the expression for the loss function is a sum of weights multiplied by errors in the prediction. This works, of course, for other methods with loss functions, such as logistic regression and neural networks. This is due to the fact that the loss function is linear with respect to the weights. As you save memory, it is totally recommendable. With other methods, you should check if the criteria for choosing parameters or method is linear or not with respect to the weights. If not, you should not do it (to me it doesn't make sense that methods are not linear with respect to weights, but there might be a case where this happens). 

The main difference between Linear Regression and Tree-based methods is that Linear Regression is parametric: it can be writen with a mathematical closed expression depending on some parameters. Therefore, the coefficients are the parameters of the model, and should not be taken as any kind of importances unless the data is normalized. On the other hand, non-parametric methods have very different ways of measure the importance. In layman terms, measuring the importance of variables on a tree can be done by checking how close they appear to the root node. In ensembling methods, like bagging, one can compute the importance of a variable as the average among the ensemble, like in this stackoverflow answer. The main difference is, then, the fact that parametric models have, through their parameters, a way of showing the importance of the variables, while non parametric models need some extra work. 

The fact that there is a sweet spot is a common issue in numerical differentiation. The issue is that using a high epsilon value will give you high error due to the fact that the numerical derivatives have error $O(\epsilon)$ (or $O(\epsilon^2)$ if you use centered difference), and using a very low epsilon will result in cancelling errors (see catastrophic cancellation question) due to the fact that you are substracting two numbers that are very close. It seems that $\epsilon = 0.01$ is a fair trade-off between these two extreme situations in your case. 

In least squares regression, the problem that you solve is: $X^T X \beta = X^T y$ where $\beta$ are the parameters, $X$ the regressors and $y$ the predicted variable. If you want, in your notation $A$ to be constant and $b$ to change, you need to have constant regressors and change the predicted variable $y$. I don't think there are applications that the regressors are not changed but the predictors do change, but if you want to find one it will probably be in online learning. 

As shown in wikipedia for KDE, a rule-of-thumb bandwidth estimator can be given if the underlying density for your data is Gaussian. This estimator is given by: $h = (\frac{4\hat{\sigma}}{3n})^{1/5}$, where $h$ is the bandwidth of your KDE estimation, $n$ the number of data and $\hat{\sigma}$ the estimation of the standard deviation of your sample. If the underlying distribution for your data is not gaussian, you can still try with this bandwidth, but it might smooth everything too much. In that case, you should go for cross-validation with smaller bandwidths. 

The way to proceed depends on if you want to use the labels or not, and how much importance you want them to have. 

As long as your validation accuracy increases, you should keep training. I would stop when the test accuracy starts decreasing (this is known as early stopping). The general advise is always to keep the model that performs the best in your validation set. Although it is right that your model overfits a little since epoch 280, it is not necessarily a bad thing provided that your validation accuracy is high. In general, most machine learning models will have higher training accuracy compared to validation accuracy, but this doesn't have to be bad. 

You are right, increasing the dropout proportion will help. However, this looks like a setting where early stopping will be a very good choice. Although dropout, weight decay and batch-norm can work propperly, the fact that you easily overfit your training set would make it an appropiate scenario to try early stopping. In addition, as the neural network takes very short to be trained you can train many of them (on some subset of the training set, making them weak learners) and create an ensemble to make the final predictions. 

It is not much more sofisticated than what you thought, but you can try training a linear regression (in the online setting, by stochastic gradient descent) whose inputs are the predictions spit by your models and the output is the real value of the predicted variable. In this case, you wouldn't need to retrain with the whole dataset. In fact, I think that all online machine learning techniques can be applied in this setting. 

When you do k-fold cross-validation, you train k models, each one of them leaving the proportion $1/k$ of the data out. For each of the models, you can compute its train error and validation error. The train error will be the error on the data selected to train the model, and the validation error will be the data left out of the training. For this reason, you have k training errors and k validation/test errors, and computing their averages will give you the quantities you are talking about. 

If I wanted to build a classifier that takes short time to be trained, I would rather simplify the classifier than the data. In this case, I would rather train a logistic regression model (probably with Ridge or Lasso regularization), than a very complex architecture with few of the data I have. If I had to simplify the data, I would take the average of the three channels, thus having a black and white image as input. I would not simplify it further (I am very confident that 4 points won't give decent accuracies). I don't really know your circumstances, but Lasso in logistic regression is a method that might allow you to select some pixels. As the coefficients obtained by Lasso are sparse (see this answer), Lasso will select the pixels that are relevant towards the prediction of the class. You can then train a bigger network with those pixels that allows you to capture nonlinearities and more complexity.