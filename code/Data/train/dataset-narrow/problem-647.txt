If you want to return a subset of matching array elements in a MongoDB document, the best approach to do so is using the Aggregation Framework. The Aggregation Framework processes data similar to a Unix pipeline: the output of each aggregation stage is passed as input to the next: 

Writes to the journal are fast append-only operations. As you've noted, these provide durability and crash resiliency. If MongoDB stops unexpectedly, it can recover from the journal and data will be in a consistent state. Without the journal, your data is in an unknown state after an unexpected shutdown and you need to validate it by running repair or (in the case of a replica set node) resync from a known-good copy of the data. 

MongoDB uses flexible schema and you do not have to predeclare your field types in the database. MongoDB (as at 3.0) does not provide any server-side schema validation, so it's perfectly allowable to have a field with different types within the same collection or to have fields vary by document within the same collection. Schema declaration & validation is optional, but if you want to enforce types you would have to do so via your application logic. There are analogous MongoDB BSON field types for most RDBMS column types, but also obvious differences. For example, MongoDB has a field which stores variable length text (up to ~16MB) rather than distinct and types. In general you will not want to use identical schema when exporting data from a relational database to MongoDB: you would instead consider how best to model your data to support common use cases. For a more complete overview including concepts and examples please refer to the Data Modeling section in the MongoDB manual. You have many options to migrate data, for example: 

The difference in outcome will depend on whether the target database had the unique index previously created. 

Roughly translated: "find all documents where it is not the case that the array includes a value other than one in the provided list". 

Later you can scale/tune the deployment starting with options like (eg. faster storage for some databases) or partitioning your data using sharding. 

This procedure is only for backing up the data from a small sharded cluster and does not cover recreating the sharded environment or capturing a point-in-time backup. As you've noticed, there is no mention of backing up the config server data or other essential steps that would be required for a sharded environment (for example, stopping the balancer). This procedure would perhaps be suitable for backing up data from a development or staging environment, but is not recommendable for a typical production environment. For a more complete sharded backup procedure using , see: Back Up a Sharded Cluster with Database Dumps. Please ensure the version of the documentation you are referencing matches your MongoDB release series as there may be notable differences. However, you mentioned using MongoDB Ops Manager which includes a specific feature for backing up sharded clusters. If you choose the option for a manual restore, Ops Manager will provide archive files to restore the config servers and shards. Since Ops Manager licensing is part of a MongoDB Enterprise subscription, I would recommend raising a commercial support case with MongoDB if you need recommendations or clarification on any of the procedures or your requirements. 

The default values for these two fields indeed increase monotonically, which will lead to a hot shard for writes if you shard an collection using the default ObjectID value for . The chunk number is expected to be a sequence, but you can (and should) provide your own custom IDs when uploading files if you want to improve write distribution for GridFS in a sharded deployment. Official MongoDB drivers should provide an API for setting the when creating a new GridFS file. 

The downgrade instructions assume a change of binaries rather than a backup, but the caveats on backward-incompatible features such as newer index and collection options still apply. I suggest using the 3.4 versions of and to test your outcome in a development or staging environment before applying to a production deployment. 

Any documents missing the field in a unique index will have a value of indexed. You cannot have two documents in the same collection missing a field in a unique index unless that index is configured as a sparse or partial index. You can manually drop the problematic index using the shell or an admin tool. For example: 

Ideally this sort of metadata would be maintained by the database server in a standard format that tools could also choose to interpret. There is a relevant open feature request that you can watch/upvote: SERVER-10021: Add timestamps to user documents. If you are using MongoDB Enterprise there are a few extra features that may help you with a workaround in the interim: 

The "maintenance mode tasks" message is referring to a counter of successive calls to the command and (as at MongoDB 3.4) isn't associated with specific queued tasks. The command is used to keep a secondary in RECOVERING state while some maintenance work is done. A RECOVERING member remains online and potentially syncing, but is excluded from normal read operations (eg. using secondary read preferences with a driver). Each invocation of either increases the task counter (if ) or decreases it (if ). When the counter reaches 0 the member will transition from RECOVERING back into SECONDARY state assuming it is healthy. As at MongoDB 3.4, changes in maintenance mode are currently only noted in the MongoDB log. This command is generally only used internally by , but you can invoke it manually as well. Here's an annotated set of log lines and the associated shell commands showing the task counter changing: 

This is correct. In order to elect (and maintain) a primary, a majority of voting members need to be available. 

This is actually not fine, as far as replication goes. If you restart a that is part of a replica set without the parameter and start writing in standalone mode, the standalone data will diverge from the other members of the replica set. Any standalone writes are not noted in the replication oplog, and there is no way to reconcile local changes with other writes that may have happened in the replica set. 

There isn't enough information here to know if the "sync target falling more than 30s behind" was the case, but it seems likely if you are pushing through a lot of activity with all of your replica set nodes on the same machine. There should be some log entries confirming the change; if you're still on 2.6 you should be able to grep for: 

A 2dsphere index in MongoDB 3.0+ will allow additional values in the coordinate array but only the first two values will be indexed and used in a 2dsphere query. This allows GeoJSON objects with additional elements (eg. altitude) to be saved in MongoDB, but interpretation or use of those extra values is up to your application code. The MongoDB manual has information on supported GeoJSON objects. There's also a relevant feature suggestion you can upvote/watch in the MongoDB issue tracker: SERVER-691: n-dimensional geospatial search. 

MongoDB 2.4+ can build multiple indexes in the background. For more information, see: Background Index Builds. You may also wish to upvote/watch SERVER-20960: Default index build option support in config/runtime in the MongoDB issue tracker, which is a feature request to make the default index build type configurable (eg. default to background index builds). 

A snapshot is a full backup of your data captured at a specific interval. Restoring from a stored snapshot is the fastest option because minimal manipulation needs to be done by Ops Manager in order to provide the restore files. The (6 hours in your config) and (3 days in your config) are meant to provide more frequent restore points for recent data as compared to the archived daily/weekly/monthly snapshots. With your current configuration, restores would be possible to: stored snapshots taken on 6 hour intervals in the last 3 days, a point in time within the last 24 hours, or a daily/weekly/monthly snapshot. 

You are not copying the contents of the config servers and effectively creating a new sharded deployment rather than restoring a backup of an existing one. Config servers include essential information about where data lives in a sharded deployment, so you must backup & restore the config server data along with the shards. Note: there have been changes to sharded cluster configuration in successive releases, so make sure you are using documentation for the correct MongoDB release series. The links below are specific to MongoDB 3.0. For supported backup approaches see the MongoDB 3.0 Backup & Restore a Sharded Cluster tutorials. 

As at MongoDB 3.2, the only supported key for a sharded output collection for Map/Reduce is the field (non-hashed). There are known issues with Map/Reduce output to a sharded collection using a hashed shard index; the two features don't play well together yet and this isn't a supported combination. The documentation currently only suggests that can be used, however there could be an explicit note that hashed indexes are not supported yet. There's a relevant Jira which you can watch/upvote: SERVER-16605: Mapreduce into sharded collection with hashed index fails. 

Migrating an existing sharded cluster to new hardware If you want to migrate a sharded cluster to new hardware, there is a tutorial in the MongoDB manual: Migrate a Sharded Cluster to Different Hardware Migrating data in an existing sharded cluster to a new cluster There is no officially supported process to migrate documents between two different MongoDB clusters (at least as at MongoDB 2.6), but you do have a few options: 

If you want to run multiple MongoDB services on the same host, you just need to create separate service definitions and configuration files for each service. As a starting point I would look at the standard script in Github and adapt a version to start your config server. I would also note that for a production system you should always deploy three config servers. If you lose your single config server, you will also lose the metadata for your sharded cluster. 

MongoDB drivers and clients use the command to describe the current role of a instance, monitor round trip time, and verify replica set configuration/status (if applicable). Frequent commands are expected behaviour, although the occurrence will vary depending on how many clients/drivers you have connected as well as the driver versions. For more information you might be interested in the Server Discovery and Monitoring (SDAM) specification which details the behaviour for officially supported MongoDB drivers. 

The context you are missing is that failure of a replica member does not affect the number of configured members the replica set has (or the required voting majority to maintain a primary). Any changes in fault tolerance or replica set election requirements will involve a re-configuration of the replica set (eg. adding/removing members or changing voting members) rather than a replica set member state change (eg. DOWN, RECOVERING, ..). A three node replica set with one down member still has three members (with a strict majority of two). If you think of it as analogous to a RAID configuration, a three node replica set with one member down is running in degraded mode and cannot tolerate the failure of any further voting members. This allows for continued availability, but you will want to recover or replace the unavailable member to return the replica set to a healthy state. You can check the current replica set configuration with and the current state of members with . 

MongoDB 3.2 and earlier require downtime and a coordinated restart of your deployment and application to enable access control: all clients and members of a deployment must use authentication once enabled. There is a localhost exception that allows you to create the first user on the database after authentication is enabled. With planning this can be a relatively quick process, but to minimize potential downtime I strongly recommend first testing in a representative staging environment. Enabling access control and authentication is an obvious security measure but still leaves you vulnerable to other possible attacks. For example, you should also Configure TLS/SSL to secure your network communication and restrict remote network access via firewall or perhaps a VPN/VPC between your application servers and your MongoDB deployment. For a full list of security measures and links to relevant tutorials, see the Security Checklist in the MongoDB manual. There are several steps that you can test in a staging environment to help ensure the transition goes smoothly, including: 

As at MongoDB 3.2, there is no option for the Windows MSI to only install the and command line tools so a default install is the most straightforward option. 

It generally makes sense to have a shard key that supports your common queries so they can be targeted at a subset of shards with relevant data, but this doesn't appear to be possible in your case as both and are optional fields. If your field provides good cardinality (i.e. large number of values) but is monotonically increasing (eg. default ObjectIDs) you could consider a hashed shard index on the field for good write distribution. The hashed index wouldn't support your common read queries (unless by specific values) so you would need a secondary index for your queries on and (i.e. {}). The recommended secondary index(es) and order will depend on your common queries and sort order. For further background information I suggest reviewing: 

Is the collection that you must shard stressing your current deployment? Before moving to a larger deployment with more moving parts I would encourage you to try to understand and tune your existing deployment. There may be some easy wins that could improve performance ... or conversely, unfortunate design choices that won't scale well in a sharded environment. Helpful starting points include the MongoDB Production Notes, Operations Checklist, and Security Checklist. If you have good understanding or estimation of your data model and distribution patterns, I would suggest sharding in a test environment using representative data. There are many helpful tools for generating fake (but probabilistic) data if needed. For an example recipe, see: Stack Overflow: duplicate a collection into itself.