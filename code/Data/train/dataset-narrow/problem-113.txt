In summary, if you connect your PC to interface 0/1, yes, you should be able to ping interface 0/2 or the loopback interface. With the same setup, if you attempted to ping another host on 192.168.2.0/24 (not the router), the ping would fail. I have set this up with Cisco 7200 routers to prove how this works in real life: 

In the past classful subnets were used. See $URL$ for more information. If you convert the address 192.168.8.5 to binary, it starts with 110, so it is a class C network. Class C networks use a 24 bit subnet mask, i.e. 255.255.255.0. Nowadays, classful networks are no longer used as the masks are inflexible and lead to a lot of wasted IPv4 addresses. We need to preserve IPv4 addresses by sizing networks appropriately. CIDR replaced classful networks and allows the designer to allocate any size subnet mask to a network. The only way to determine the subnet mask nowadays is to look at the design documents or check what mask has been entered on the interface configuration. 

The Protocol Identifier is encoded in Octets 1 and 2 . It takes the value 0000 0000 0000 0000. The Protocol Version Identifier is encoded in Octet 3 . It takes the value 0000 0000. The BPDU Type is encoded in Octet 4. This field takes the value 1000 0000 (where bit 8 is shown at the left of the sequence). This denotes a Topology Change Notification BPDU. 

Second, a ping from PC1 (192.168.1.50) to PC2 (192.168.2.50) with TTL of 1 (PING FAILS): Router debug output: 

Yes, as you say backup links will cause an issue in the future unless you provider lets you run OPSF CE-PE and sets up some sham links. One workaround would be to use GRE tunnels between sites and run OSPF over the tunnels. The tunnel endpoints would need to be advertised through BGP. The OSPF routes would arrive over the tunnel. This does lower you MTU though, which may cause issues and obviously there is work to configure this from your side. I would push for the provider to enable OSPF on the PE-CE 

The control plane can be implemented as separate physical hardware or logically as separate processes on a device. There is usually a single control plane (physical or logical) and it hosts both L2 and L3 processes. There is also the concept of distributed control planes where the supervisor handles some tasks and the individual line cards handle other or same tasks. Usually we speak of both L2 and L3 processes belonging to a single control plane on a single device. When switch stacking technology is used (VSS, FlexStack, IRF), only a single control plane is active for the entire stack. Subordinate switches are on standby and will take over from the master if it fails. The data plane is active on all devices, but the control plane is only fully active on the master device and there is only a single active process for each L2 and L3 feature across the entire stack. Nexus switches operate differently to normal switch stacking. With Nexus the control planes on both devices are both fully active. Both devices have active control planes for both L2 and L3. When features such as vPC are active, the L2 control planes (one on each device) cooperate to give the impression of a single switch/control plane. In reality there are two separate control planes active and two separate processes for each L2 process. At L3 the control planes act totally independently for most tasks. 

Yes, this is certainly possible. You can install a E1/PRI VWIC in a Cisco router and the Cisco router can act as a gateway between TDM and SIP. There are of course solutions by other vendors. This is a fairly broad subject, so you will need to read up on it, but it is defiantly possible. 

With a single group, they only provide resilience, the active router will service all traffic for the group, you are correct that you would need multiple groups to achieve crude load-balancing (although this is not a good solution), each group would have its own virtual IP to provide load-balancing, one router would be active for one group, and the other active for the other group, you would then have to make sure that half the hosts on the LAN had one of the virtual IPs as their default gateway and the other half had the other virtual IP. GLBP (Gateway Load Balancing Protocol) was designed to provide first-hop resilience and load-balancing. GLBP responds with multiple virtual MACs when hosts ARP for the GLBP virtual IP, some hosts receive one MAC, others received the other MAC, one router services one MAC and the other router services the other MAC. If one router goes down, the other router services both MACs. Not sure what you are asking, but you can use the same group number on multiple VLANs as the HSRP/VRRP traffic of different VLANs will never interact. There are situations where it would be better to use unique group number such as if you plan on using Q-in-Q in the future. Not sure what you mean, you can use either the same group number on multiple VLANs or different group numbers, or a combination of both. There will be a platform limit to how many total groups (across all VLANs) that the switch can support. HSRP/VRRP are not load balancing protocols though, use GLBP instead 

This example shows how the next hop behaves on a multiaccess network such as Ethernet. Assume that RTC and RTD in AS300 run OSPF. RTC runs BGP with RTA. RTC can reach network 180.20.0.0 via 170.10.20.3. When RTC sends a BGP update to RTA with regard to 180.20.0.0, RTC uses as next hop 170.10.20.3. RTC does not use its own IP address, 170.10.20.2. RTC uses this address because the network between RTA, RTC, and RTD is a multiaccess network. The RTA use of RTD as a next hop to reach 180.20.0.0 is more sensible than the extra hop via RTC. Note: RTC advertises 180.20.0.0 to RTA with a next hop 170.10.20.3. If the common medium to RTA, RTC, and RTD is not multiaccess, but NBMA, further complications occur. 

For completeness, VLANs can be insecure if attackers deploy VLAN hopping and certain conditions are met (from $URL$ 

So each area now has full topology information for all the routers and networks within the areas it is a member of, and it has summarised information for all the other networks within the AS, which are created by the area ABR. The summarised information only shows the ABRs best cost path to the networks in the other areas, it does not allow the router to build up a full picture of the entire network and work out its best path to the network number from the topology graph. The example above is for standard areas, there is the concept of stub areas, totally stubby areas, not so stubby areas etc that all have different rules for how they summarise internal or external routes. 

Yes, only real way is to copy the startup-config file to a PC and edit on there, then copy back. Of course the commands will not take effect until the next reload and you will have to be careful you don't corrupt the config in any way. Make sure you use a proper text editor like Notepad++ and not a word processor when you edit to make sure you don't insert any odd character codes. 

It shows that frame 22746 was reassembled from the contents of frame 22743, 22744 and the original frame 22746. It also shows the length of each of these segments and the total length of the reconstructed segment. The Ethernet/IP/TCP headers are that of frame 22746, but the TCP payload is the full reconstructed HTTP PDU 

You are correct, the IP addresses will be replaced by the sending and receiving SIP proxy server IP address for that leg of session, although this isn't NAT as they are actually separate IP/UDP sessions. SIP packets aren't treated any differently by routers, and if a packet needs to be routed between networks it will be forwarded with the source and destination IPs unchanged. SIP proxy servers are not IP routers though. SIP is just another application layer protocol transported over IP/UDP When a SIP proxy server makes a routing decision it is actually looking at the destination SIP address (5) and not the IP destination address. The client opens a UDP session to the first SIP proxy using its IP address as the source and SIP proxy server's IP address as the destination. The UDP payload is the SIP application layer traffic. The proxy server makes a SIP routing decision (different to IP routing) and opens a UDP session to the next SIP server and forwards the SIP traffic within that session. The IP and UDP layers for each leg are unique, but the SIP session end-to-end has the same SIP session ID and therefore is the same. The proxy function becomes quite useful when making calls between organisations. Without the proxy server each organisation would have to have full routing information for each other's address space to make an end-to-end connection. There will likely be overlap so complicated NATs are required, which would also have to understand the SIP protocol to change any address at the application layer. With SIP proxy servers sat on the edge of each organisation the internal address space can be hidden. Clients only have to route to the SIP proxy server to reach any IP within that organisation. 

There isn't really any signalling of the requested application in TCP. The application is implied by the destination port number. A web server would open a listening TCP socket on port 80. A client wishing to connect to the web server would open a connection to port 80 and then send HTTP commands. If a misconfigured client connected to port 80 expecting an FTP server it would send FTP commands and these would get passed to the web server application the same, it would be up to the web server application to handle the error and to terminate the connection. 

If you look at the difference between PIM Dense Mode and Sparse Mode, that will give you the answer to why the RP is required. In Dense Mode, multicast traffic is flooded from the source across any PIM Dense Mode enabled links. This allows it to eventually reach multicast receivers registered to IGMP queriers. Dense Mode will then prune off PIM links that don't require that multicast group. So, traffic is essentially flooded across the network to the receivers and then unused links are pruned to create a tree from source to receiver. Sparse Mode uses an RP, which is a predefined router on the network that the source and receivers will built a multicast tree to, and traffic can then be send from source to receivers via the RP. The sources send their traffic into the network and the multicast routers build a path to the RP. When receivers register for a group with their IGMP querier (usually their default gateway), that router also builds a path to the RP. Traffic can now flow from source to receiver without being flooded out of every link. Once this path has been formed the receiver's gateway will request a direct path from the source and eventually forwarding will move over to this path, which may bypass the RP. So the RP serves two purposes, to allow IGMP queriers to discover and build trees from the multicast source, and this allows multicast to be forwarded without flooding the network. The source IP of a multicast packet is the sender's unicast IP address. The destination IP is the multicast group address. 

This happens when Cacti is unable to receive a response from the device being polled. This could be due to the device not responding (high CPU, bug in SNMP code etc), data being lost between device and Cacti (routing issue, dodgy link etc) or the Cacti box is not able to process the traffic (high CPU, bug etc) 

Yes, it is possible to run two separate instances of GLBP on different interfaces on the same router. A core/distribution switch may have many GLBP groups configured when serving a large campus. It may be a simpler solution to remain with the first topology and connect both networks back to both R2 and R3. Both routers can then advertise both networks towards the ISP. If you do connect both networks to both routers, make sure there is resilience in the LAN so that you don't end up with a partitioned LAN during a link failure. That would lead to inbound traffic not reaching some hosts. Also, consider whether you need to load balance at all. If you used VRRP instead, it is an open standard and would give you more vendor options in the future. At the end of the day, this helps with single point of failure to the LAN, but there are also single points of failure with R1 and the ISP connection. 

There are two ABRs (ABR1 and ABR2). Both ABRs have an interface in area 0 and an interface in area 1. Both adjacencies on both ABRs are in the FULL state. Each ABR will create the following type 3 summary LSAs: 

If you do show spanning-tree, the port ID is the Nbr element in the Prio.Nbr column. For this example Gi0/0 is port 1, Gi0/1 is port 2, Po1 is port 65. You will find that all the interfaces of a type are grouped and the port IDs are ordered within that group.