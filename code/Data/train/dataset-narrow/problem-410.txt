There doesn't appear to be a problem. The listener is a process that generally runs on the database server, not on the client machine. Doing a client-only install will not install a listener. If you want to have a listener on your machine, you'd need to do a database install (though you can do a software-only install rather than actually creating a database). If you're just trying to connect to a remote database, however, you'd use the listener that is (presumably) running on the database server. It's theoretically possible to install a listener on one machine for a database running on another machine but that isn't particularly common. It would be extremely, extremely unusual to install a listener on the client machine that listens for connections to a database on a remote server. 

Toad cannot directly read Oracle data files, no. You would need to recover the database on a local server. Once you do that, you could connect to Oracle database on the local server from Toad and run whatever queries you'd like. Assuming that what you've been given is a consistent cold backup of your database, you'd need, at a minimum, exactly the same version of Oracle (patchsets included) on exactly the same operating system that the third party vendor uses. Assuming that the vendor is vaguely competent about applying patches, that will likely require that you have a My Oracle Support account which would require that your organization has appropriate Oracle licenses. How you do that will depend on the operating system your Oracle server will be using. Since it sounds like you are not a DBA and you're not particularly familiar with Oracle, assuming there is a reason that you can't simply access the existing database to run whatever queries you need to run, I would tend to suspect that you would be much better off requesting an export of the database (or potentially the tables you're interested in if you really care about a subset of the data) rather than a backup. Toad can't read an export directly either but it is a lot easier to import and export file into a database you create on your local server without worrying about matching the version and operating system that the vendor uses or going through the effort of doing a proper database restore. 

If the query plan involves a full table scan, Oracle has to read every block from the table up to its high water mark (HWM). If there are 800 MB of blocks below the HWM, it would make perfect sense that it would take 30 seconds to read all that data. The number of blocks that actually have data in them, in this case, is irrelevant. If you truncated the table, however, you should have reset the HWM unless you specified which is not the default. The fact that the table is still 800 MB implies that you either explicitly had a in your command or that you didn't actually truncate the table-- perhaps you just did a to remove all the data, for example. A will not reset the HWM. 

You can return a collection, either all at once or pipelined, but that requires more code to create and maintain the objects. I love pipelined table functions if you need to embed PL/SQL logic in the query itself that would be difficult to follow in SQL. But they're overkill if you just need to run a simple query and return the results. 

Please don't combine the various tables. Your future self (and anyone that ends up writing queries against the tables) will thank you. 

I would tend to suspect on my system, therefore, that if I tried to retain 8 days of that Oracle would generally have a problem complying. It would take a week to verify one way or the other but if I had to wager, I would tend to wager that trying to hold more than a week of would fail. Even if you could hold the that long, formulating the flashback query would be a major challenge since you couldn't specify a timestamp. You could potentially maintain your own (more long-term) mapping between SCN and timestamp by writing an automated job that would capture both every few seconds and hold them for weeks or months at a time and then specify in your flashback queries. I'd tend to expect that the older would be gone but it might work. 

If I saw a data model where a table had 2.1 million rows, one of the columns had only 5 distinct values, and I knew that people wanted to get a listing of those distinct values on a somewhat regular basis, I would strongly suspect that the data model was missing a lookup/ dimension table. Rather than trying to tune your current query, I would bet that the better answer would be to create a separate table that has just those 5 distinct values, create a foreign key relationship between your current table and the new lookup table, and then modify your query to hit the new lookup table instead. 

And from or , you'll see only one row for the query that has the hard-coded literal. That row will show that the query was executed 10 times 

Since is the position of a particular block on a particular form, it should be a column in the table, not a column in the table. If is in the table, a block would need to have the same position in every form it was on. Once you move the column to , you can then create a unique constraint on the combination of on the table. 

Yes, if this was to work, it would generate an infinite loop. But more than likely, it will throw a mutating table exception first. What is the problem that you are trying to solve? A trigger that updates other rows in a table when one row changes is unlikely to be a reasonable solution. If you have cross-row dependencies, that almost always indicates that you have a normalization issue that should be fixed in the data model rather than being coded around. 

Each query would need to be parsed separately and each would generate (potentially) a different query plan. When generating the query plan, the optimizer is going to use each of the literal values along with the statistics that have been gathered to guess at how many rows the query will return and which query plan would be most efficient for that estimate. My guess is that some of the literal values cause the optimizer to estimate many more (or many fewer) rows than are really returned which causes the optimizer to produce a poor plan. Simplifying drastically, let's say that the statistics on the table tell you that it has 150,000 rows. Let's say it has a column which is indexed so there is a statistic on it (and it alone). And let's say that this statistic says that the data is uniformly distributed between 1/1/2015 and 3/31/2016 (15 months with 10,000 rows per month). If I run the query 

What do you expect this query to do? is a regular expression version of the statement so it makes sense to use it in the same sorts of places that you would use a . You wouldn't try to directly the result of a statement. You could, however, embed the in a statement. For example 

That depends-- are your data files set to autoextend? If they are, they'll just grow themselves up to whatever limit you may have set (or to the size of the available disk). If not, whenever the database tries to allocate the next extent in the tablespace, the operation will fail and whatever is trying to write data to a table in the tablespace will fail. There are many products that you might install in and any one of them might be the unlucky one that needs to allocate the next extent so you can't really predict what impact that would have. I'd guess that you'd stop being able to write audit information since the audit trail is commonly in and commonly the most prevalent user of space among products that are commonly installed in . You can check whether your data files are set to autoextend by looking at the attribute in . 

In general, extents are allocated in a round-robin fashion among all data files in a tablespace so long as those data files have enough free space to allocate the extent. So if you add a new data file, you would generally expect that half of the new extents would be allocated in the existing data file and half of the new extents would be allocated in the new data file. Assuming the two data files are the same size, you would generally expect that by the time the first data file got to 100% full, the second data file would be roughly 50% full. I'm not sure, though, how this helps you deal with increasing tablespace size without wasting physical space. I'm hard-pressed to imagine in what circumstances knowing how extents are going to be allocated is going to help you use less physical space. It shouldn't matter from that perspective whether you double the size of the existing data file, add a new data file, or add multiple data files and it shouldn't matter whether one data file is filled up before the next one starts being used. In any case, you've allocated the same amount of space at the operating system level, you've allocated the same number of extents in the tablespace, and your segments are all the same size. You would decrease the amount of space allocated at the operating system level by letting the data files autoextend rather than allocating fixed size data files but then you have to monitor the free space at the operating system level to ensure that the data files all have enough space to grow and you may complicate your ability to move data files to different mountpoints. You may also decrease the amount of parallelism you can get in your backups forcing the backup job to run a bit longer. 

Assuming that your database character set is (if there is a difference between the two, this is almost certainly your character set) and that all the data is characters that exist in the character set ("english" might constitute more than that depending on the definitions we're using) there won't be any noticable performance differences between a and a . Based on your database character set, Oracle has to do a tiny bit more work reading a string to figure out that every byte represents a single character rather than being part of a multi-byte character. But checking the length of a string in either characters or bytes is a pretty trivial operation. Unless you're doing something like running a TPC benchmark where you're already doing a crazy number of things that you'd never do in reality to get tiny fractions of performance improvement, it's not something worth worrying about. If you were going to go crazy with a TPC benchmark level of effort to get every last thousandth of a percent, it would probably be slightly more efficient to use a database character set of and declare everything using byte semantics given that you only had to store English. Practically, though, if your application is so efficient that this sort of thing matters, though, you've already solved every performance problem so you should be focusing much more heavily on supportability and maintainability which would argue for an character set and character length semantics. 

You can't partition an existing non-partitioned table. So if you want a partitioned table, you would need to create a partitioned table before inserting any data. Assuming that you are not using interval partitioning (in which case you're telling Oracle how to automatically add new partitions based on the data), you would almost certainly want to create the appropriate partitions in advance. If you are range partitioning the table, you could, in theory, create a MAXVALUE partition for the table, load all the data, and then split the MAXVALUE partition into the individual partitions that you want. But that would involve moving all the data around multiple times which would be much less efficient than simply loading the data into the appropriate partition from the outset. Inserting the data "in the order of the partitions" probably won't make a difference from a performance perspective. Oracle still has to determine for each row which partition to insert it into. Assuming your indexes are local, it may well be more efficient to load a staging table whose structure matches the structure of a partition, create the appropriate indexes on the staging table, and then doing a partition exchange to exchange an empty partition with the staging table.