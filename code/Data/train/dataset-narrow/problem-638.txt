Have never encountered anything remotely like this and suspect something has been lost in translation. I will of course apply the usual caveat for licensing questions, "don't take anything you read here as gospel, check it with your licensing partner, and Microsoft, and call both back to speak to a different individual. There's a good chance you'll get a different answer the second time around". You get a key, you install the software. Any CPU limitation is a product of the physical environment or version of SQL Server. For standard edition that is: 

If you capture sufficient activity from the database and order the ClearTrace report by execution count, youâ€™ll have a better idea of the common/typical query patterns and from this the permissions required. You can then investigate the non-typical activity and establish if these require a different set of permissions and/or are generated by a subset of the user base. 

If you can format the output of this into a readable form and add to your question, someone can make a more educated guess at the best use of your 18 disks. If I was doing this blind, I'd probably start off allocating: 

Appropriate size for the log is of course dependent on your observations of log usage for each database. 

Yes it is. SQL Server (and arguably any relational database engine) has no foresight as to what other batches may be running when it processes a statement and/or batch, so the sequence of lock acquisition does not vary. 

it should depend on the sort order of MyOtherTable.y. If it's ASC as per MyTable.y then the two indexes would be read in index order and a sort applied after the join. If it's desc, in theory a reverse order index scan could be used for the join and an additional sort wouldn't be required to satisfy your order by clause. Edit2: Couldn't recall if this would show up in the execution plan in SQL Server. The icon doesn't indicate this is a reverse scan, nor does the tooltip on hover. Properties however shows 'Scan Direction - Backward' or checking the plan XML reveals 

If you post the execution plans there may be obvious differences that we can highlight. Other than plans it could be: 

The per language noise list (also referred to as stop list) and thesaurus files are in the MSSQL/FTData/ folder. The noise files are plain text, the thesaurus XML, so both can be inspected or changed easily. Configuring Full-Text Linguistic Components in BOL has the details. 

The undo pass is required to revert any dirty pages from in-flight transactions that were flushed to disk by a checkpoint operation. A checkpoint writes ALL dirty pages from the buffer cache to disk, regardless of whether the transaction that generated them has been committed or not. Checkpoints and the Active Portion of the Log covers the gory details. Restore WITH STANDBY allows you to bring the database up in a read-only state between restores (as your quote mentions). For the database to be in a transactionally consistent state, the undo pass has to be run. To continue further restores (which is the purpose of standby), the pages that were modified by UNDO will need to be re-applied, hence the need to store them in the standby file. 

Edit: Following comments. Obviously (as @gbn pointed out) the 1st call, following buffer and query plan flush, is going to be slower than subsequent calls. That said, there must be something wrong with either the queries or the IO capabilities of the server for the difference to be a factor of 10. Can you add to your question the execution plans (XML), output from , along with the database IO stats. Original answer: 

I'm answering this with hesitation as there isn't enough information in your description of the problem to be 100% sure this is the best advice. "Hangs or throws an exception" suggests the source of the issue isn't properly understood, so proceed with caution. The simplest solution to this is probably . determines whether SQL Server will rollback a transaction in the event of a run-time error. The default will rollback only the statement that caused an error, leaving any parent transaction open. The "gotcha" side-effect of the default setting is that a timeout can cause exactly the same problem, an open transaction that is the clients responsibility to handle and rollback. If the client doesn't try/catch/rollback, the transaction will remain open until attended to with (and I quote @gbn) the ultra-violence of . The oft quoted Erland Sommarskog's articles on error handling in SQL Server contain all the background and strategy you need for dealing with these scenarios and more. Edit (following comment): To identify open transactions, sp_whoisactive is probably the most feature complete. 

You may be experiencing an issue with ascending keys, which is leading to a poor execution plan choice. 

That's it, we're done, the transaction is committed. The in-memory record is modified, details of what was modified is stored in the log, negating the need to harden every record change to physical disk. On to your questions: 

Backup the log Shrink the log file with DBCC SHRINKFILE with TRUNCATEONLY to get it as small as possible Run ALTER DATABASE MODIFY FILE to re-size the log to an appropriate size 

I'm going to approach this answer with a different context to the others, not sure if it will work! A very short answer is that no, they are not good guidelines. You are attempting to reduce a broad, complicated topic to a simple yes/no decision tree that will fit into a forum Q&A. It can't be done. Why not? 

Usually, these questions arise from a misunderstanding of what the log file does. Usually, a larger than expected log is caused by a database in full recovery where log backups aren't being taken. If you are taking log backups or your database is in simple recovery, there is probably a very large or long running transaction that has required 60GB of log space. KB317375 is a good starting point for understanding why unexpected log growth may occur and includes steps for identifying transactions that could be the cause. 

I may be wrong but it looks like this has this been overcomplicated/misunderstood or just plain muddled by the CTE in your original question. From the comments you've added to various answers it appears that: 

You can't accurately project future growth without a history of previous growth. You can however cheat and get a rough trend using backup history, as detailed by Erin Stellato in Trending Database Growth From Backups. Plot the output of the following query in Excel: 

In this particular case the addition of a lock to the would indeed prevent anomalies. The addition of isn't necessary as an update lock is held for the duration of the transaction, but I confess to including it myself as a (possibly bad) habit in the past.