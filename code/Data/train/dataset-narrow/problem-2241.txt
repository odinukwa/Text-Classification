You can try a Jackson network, which is well-studied in queueing theory. Jackson networks provide a formalism for modeling systems of servers (ie, hosts) that process messages/items at varying rates and pass messages between one another, and they can handle notions like fixed-capacity caches and linked processing rates. Your analysis will be easier than what's usually needed for these models because you're considering a constant processing speed. I'll also second Aaron's answer about looking at Petri nets, which are another useful formalism that can represent what you're after. What you want isn't just the formalism by itself, which is nothing more than a way to logically express your requirements, but the ability to leverage existing analytical results about the performance of these systems from the Petri net / Jackson network literature. After you've figured out how to model your system as one of these networks, you can try and use known properties of these networks to determine answers to your questions. With Jackson networks, you usually end up with results like, "as hosts become more active, and the cache size drops, the expected processing time changes by X". This question would be a wonderful candidate for crossposting on the Operations Research Exchange, since we're in the territory of stochastic and queueing models. 

I claim that the number of concave vertices in $P$ is at most $2n$. This is because, whenever a stretched rectangle is removed from $P$, there are 3 possibilities: 

A rectangle-preserving partition of $C$ is a partition $C = E_1\cup\dots\cup E_N$, such that $N\geq n$, the $E_i$ are pairwise-interior-disjoint axis-parallel rectangles, and for every $i=1,\dots,n$: $C_i \subseteq E_i$, i.e, each existing rectangle is contained in a unique new rectangle, like this: 

This algorithm is not optimal. E.g, in the above example it gives $N=13$ while the optimal solution has $N=5$. So two questions remain: A. Is this algorithm correct? B. Is there a polynomial-time algorithm for finding the optimal $N$, or at least a better approximation? 

In response to your second question, you are correct -- a 2-state, 3-symbol state TM was shown to be universal by then-undergraduate Alex Smith in 2007 by reduction to a universal cyclic tag system, which makes for a complex encoding. Here's the original proof, which uses Wolfram's nonstandard notation for representing Turing Machines. It's easy to intuit why simpler universal Turing Machines require more complicated encodings -- after all, the information involved in the computation needs to go somewhere. For example, in the encoding scheme implicit in Smith's proof, we would first need to first encode the Turing Machine under simulation within a cyclic tag system, then use Smith's scheme to represent the cyclic tag system within the 2-3 Turing Machine. 

When the $X_i$ are iid for some distribution with non-negative essential support (i.e., the probability of a negative value is zero) and the distribution of $\log(X)$ has a finite variance, then the Central Limit Theorem easily implies that $X^* = \log(X_1 \times \cdots \times X_k)$ approximately has a normal (Gaussian) distribution for sufficiently large $k$. The problem here is that each $X_i$ has a 50% chance of being negative, precluding (easy) analysis with logarithms. Nevertheless, only two things can happen (almost surely, after neglecting the zero chance that any of the $X_i$ equals $0$): either an even number of the $X_i$ are negative, in which case the product is positive, or an odd number of the $X_i$ are negative, in which case the product is negative. Each case has a 50% chance of happening. Whence, the probability density of the product has two parts (symmetric to each other about 0) and each part looks like the product of draws from a half-normal distribution. A calculation shows that the log of a half-normal distribution has finite variance, allowing us to apply the preceding observation and conclude that for sufficiently large $k$, the distribution of $\log(X^*)$ conditional on $X^* \gt 0$ is approximately Gaussian and the distribution of $\log(-X^*)$ conditional on $X^* \lt 0$ is exactly the same. ($X^*$ is an equal mixture of variables $Y$ and $Z$ where $Y$ is approximately lognormal and $-Z$ has the same distribution as $Y$.) In effect, the distribution of $X^*$ has two pieces: the positive piece is approximately lognormal and the negative piece is its reflection about 0. This makes it bimodal; all odd moments are $0$; and small even moments can be approximated from the Central Limit Theorem. In particular, the mean of the positive part of $\log(X^*)$ equals $-(\log(2) + \gamma)k/2$, approximately $-0.635181 k$, and its variance equals $\pi^2 k/8$, approximately $1.2337 k$. Simulations indicate this approximation is good for $k \gt 100$, more or less. Considerable negative skewness in the distribution of $\log(|X^*|)$ is evident for smaller $k$. 

A great and little-recognized example of this phenomenon is graph isomorphism. The best known algorithm takes something like $O(2^{(\sqrt{(n log n)})})$ time, but graph isomorphism tends to be solvable quite quickly in practice. I don't know if there's a formal result on average/smoothed complexity of the problem, but I remember reading that one existed - maybe someone else can chime in pointing out a formal result. Certainly, there's a good deal of experimental evidence and a lot of fast solvers. I'm also curious if this property extends to other members of the GI-complete family. 

The answer is certainly "yes" if we talk about families of graphs, rather than specific graphs. For example, there's a conjecture of Mihail and Vazirani that all 0/1 polytopal graphs are either good or very good edge expanders (ie, that their edge expansion is bounded below by 1/polynomial(degree), or 1). If this is true, then there exist efficient randomized Markov chain Monte Carlo approximation algorithms for a number of open combinatorial and counting problems via a sampling strategy of Alon, Jerrum, and Sinclair. In a similar vein, if there exist families of polytopal graphs whose diameter grows faster than any polynomial in the number of facets and graph degree, then linear programming cannot be solved in strongly polynomial time via edge-following algorithms. 

I hope I haven't misunderstood, because its seems that in some circumstances a winning strategy is easily computed without looking at the entire decision tree. For example, when both stacks have even numbers of cards and the total of the odd-numbered cards (from both stacks) exceeds the total of the even numbers, then the second player has a simple winning strategy of always responding in the stack from which the first player draws. When exactly one stack has an odd number of cards, the first player might have a similar winning strategy after taking the top card from the odd stack. Simple heuristics like these can potentially simplify a dynamic programming approach by quickly identifying obvious wins. 

It sounds like your glass cutting problem is exactly like the 2D guillotine stock cutting problem -- am I correct that there is no difference? I think Macleod et al's O(n^3) approximation algorithm from the 1990s is still the best known result with proven optimality bounds for very large instances of this problem, but for only 100-200 pieces the exact algorithm by Christofides and Hadjiconstantinou should be able to find the optimal solution in reasonable time. There are also good heuristics for this problem via dynamic programming. A survey on software solvers for the cutting stock problem was written by Macedo et al here in case you're looking for practical solutions. 

This (unnormalized) histogram documents 100,000 independent realizations of $\log(|X^*|)$ with $k=100$. Very slight negative skewness is still visible, but the general Gaussian shape is apparent, centered as expected around -63.5 with variance 123.3 (standard deviation 11.1). It is evident from the wide range of highly negative logarithms that an attempt to plot the distribution of $X^*$ itself would be useless: it would show as a unit spike at 0. If we normalize it, though, a plot is informative. Specifically, consider the "signed geometric mean" $\text{Sign}(X^*)\exp(\log(|X^*|)/k)$: 

This histogram displays the same data as the previous one, but normalized as signed geometric means. The shape on the right is a lognormal distribution; the shape on the left is its reflection about 0. 

$C$ is an axis-parallel rectangle. $C_1,\dots,C_n$ are pairwise-interior-disjoint axis-parallel rectangles such that $C_1\cup\dots\cup C_n \subsetneq C$, like this: 

OLD ANSWER: The following algorithm, while not optimal, is apparently sufficient for finding a rectangle-preserving partition with $N=O(n)$ parts. The algorithm works with a rectilinear polygon $P$, which is initialized to the rectangle $C$. Phase 1: Pick a rectangle $C_i$ which is adjacent to a western boundary of $P$ (i.e, there is no other rectangle $C_j$ between the western side of $C_i$ and a western boundary of $P$). Place $C_i$ within $P$ and stretch it until it touches the western boundary of $P$. Let $E_i$ (for $i=1,\dots,n$) be the stretched version of $C_i$. Let $P=P\setminus E_i$. Repeate Phase 1 $n$ times until all $n$ original rectangles are placed and stretched. In the image below, a possible order of placing the rectangles is $C_1,C_2,C_4,C_3$: 

Although it's hard to discern a precise question here, evidently it concerns spatial data structures for efficient local searches of objects. Hanan Samet's books on the subject are classics and almost encyclopedic. He wrote a brief overview of the subject available here. I suspect a simple quadtree would be an attractive solution. In many cases, exploiting a pre-sorting by one of the coordinates provides fine performance and is simple to code. 

This problem is strongly reminiscent of "Optimal stopping" of stochastic processes. These are frequently solved by dynamic programming. References even to this specialized literature are extensive, due to its applications in decision theory and finance.