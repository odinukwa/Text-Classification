In the context of scheduling maintenance jobs on arcs of a flow network I came across the problem to schedule jobs, indexed by $j$, and given by triples $(r_j,d_j,p_j)$ of (integer) release time, due time and processing time such that the total time in which no job is processed is maximized. A schedule is given by start times $x_j\in S_j:=\{r_j,r_j+1,\ldots,d_j-p_j+1\}$. For instance, for five jobs with $(r_j,d_j,p_j)$ equal to \[(1,3,3),\ (20,22,3),\ (10,11,2),\ (1,18,5),\ (8,26,5)\] and a time horizon of 30, the first three jobs are fixed starting at times $x_1=1$, $x_2=20$ and $x_3=10$, and for the last two jobs it is optimal to start both of them at the same time $x_4=x_5\in\{8,9,10\}$. The objective value (number of time periods without any job) is \[30-3-5-3=19.\] The straightforward binary program for $n$ jobs looks as follows ($y_t$ is the indicator variable for the activity of the "plant" machine suggested in the comment by András Salamon) \begin{align*} \text{minimize}\ \sum_{t=1}^Ty_t&\\ \text{subject to}\qquad \sum_{t\in S_j}x_{jt} &=1 && j\in[n], \\ y_t-\sum_{t'\in S_j\cap[t-p_j+1,t]}x_{jt'} &\geqslant 0 && t\in[T],\ j\in J_t,\\ x_{jt}&\in\{0,1\} && j\in[n],\ t\in S_j,\\ y_t &\in\{0,1\} && t\in[T]. \end{align*} Here $J_t=\{j\ :\ r_j\leqslant t\leqslant d_j\}$ denotes the set of jobs that can be "active" at time $t$. Interestingly, the constraint matrix is not totally unimodular in general, but the LP relaxation seems to have always integer optimal solutions. Writing down the dual also yields a problem which can be interpreted combinatorially, but so far I haven't been able to turn this into an integrality proof. One can look at variants of the problem where the number of jobs that can be processed simultaneously is bounded, or there is a given partition of the job set such that no two jobs from the same part can be processed at the same time. Has this kind of objective function been studied in the scheduling literature? Or is there any other related problem? 

I'm interested in a slight generalisation of DFA. As usual we have state-set $Q$, finite alphabet $\Sigma$, a $\Sigma^*$-action defined on $Q$ by $\delta : Q\times\Sigma\rightarrow Q$, and initial state $q_0$; but instead of the usual terminal set, we take a family $(T_i)_{i\in 1..n}$ of subsets of $Q$. A Multi-language DFA $M$ is then the tuple $(Q, \Sigma, \delta, q_0, (T_i))$ and $L \subseteq \Sigma^*$ is recognised by $M$ iff $L = \{s\in\Sigma^*|q_0s\in T_i\}$ for some $i\in 1..n$ . Define $(L_i(M))_{i\in 1..n}$ to be the family of languages recognised by M, if you like. Okay, now for my question: given a family of regular languages $(L_i)_{i\in 1..n}$ , I want to find the minimal Multi-language DFA $M$ as described above such that $L_i = L_i(M)$ for all $i\in 1..n$ , that is, such that $|Q|$ is minimised over all such machines. My question is, are there any known efficient ways of doing this, perhaps analogous to standard DFA minimisation theory? Conversely, is there any evidence that this problem might be hard? 

Generate $n-2$ blank non-terminals, the leftmost one being the separate blank + read-write head non-terminal, and also generate a "boundary" non-terminal on each side. Again, if we generate the wrong number of non-terminals then we fail. Simulate $M$ in the space between the boundary non-terminals. If $M$ ever shifts onto one of the boundary states, we terminate the simulation and assume that $M$ never halts. If $M$ halts, behave like $G_X$. If we had to terminate the simulation, then fail. 

You can do the following. For every $i\in\{1,\dotsc,n\}$, start with the set $I=\{k\,:\,a_k\leqslant a_i\}$. Let $K=\{k\,:\,\exists j\ b_k^j=\max\{b^j_\ell\,:\,\ell\in I\}=a_i\}$. If $K=\emptyset$, then $I$ is a counterexample. If $i\in K$ then there is no counterexample with $i\in I$ and $a_i=\max\{a_k\,:\,k\in I\}$. If $K\neq\emptyset$ and $i\not\in K$ then any counterexample $J$ with $i\in J$ and $a_i=\max\{a_k\,:\,k\in J\}$ will be a subset of $I\setminus K$. So we can replace $I$ by $I\setminus K$, and iterate. It is necessary and sufficient that for every $i$ we end up with $i\in K$. 

I think you want the Edmonds-Gallai decomposition of your graph which can be computed in time $O(n^3)$ (see these notes). 

This is a special case of the precedence-constrained TSP which has been studied quite a lot. For instance, there are a polyhedral analysis by Balas, Fischetti and Pulleyblank, and a branch-and-cut algorithm by Ascheuer, Jünger and Reinelt. 

I think you can solve this as a single shortest path problem with respect to arc weights $$\overline{w}_{ij}=\min\{-w_{ijp}\ :\ p\in\{1,\ldots,m\}\}.$$ For every arc $(i,j)$ fix some $p(i,j)\in\{1,\ldots,m\}$ with $-w_{ijp(i,j)}=\overline{w}_{ij}$. If you have a shortest path $\pi$ with respect to $\overline{w}$, you can put $$y_{ijp}=\begin{cases} m & \text{if }(i,j)\in\pi\text{ and }p=p(i,j),\\ 0 & \text{otherwise.}\end{cases}$$ For the $l$- and $z$-variables you do the following. If $(1,j)\in\pi$ is the arc leaving 1 then $$z_{1q}=\begin{cases}m-1 & \text{for } q=p(1,j)\\ 0 & \text{for } q\neq p(1,j)\end{cases}$$ $$l_{1q}=\begin{cases}1 & \text{for } q\neq p(1,j)\\ 0 & \text{for } q= p(1,j)\end{cases}$$ If $(i,n)\in\pi$ is the arc entering $n$ then $$z_{nq}=\begin{cases}0 & \text{for } q=p(i,n)\\ 1 & \text{for } q\neq p(i,n)\end{cases}$$ $$l_{1q}=\begin{cases}0 & \text{for } q\neq p(i,n)\\ m-1 & \text{for } q= p(i,n)\end{cases}$$ If $(j,i)$, $(i,j')$ are two consecutive arcs of $\pi$ then you put $l_{ip}=z_{ip}=0$ if $p(j,i)=p(i,j')$, and otherwise $$z_{iq}= \begin{cases} m & \text{for } q=p(i,j')\\ 0 & \text{for } q\neq p(i,j') \end{cases}$$ $$l_{iq}= \begin{cases} m & \text{for } q=p(j,i)\\ 0 & \text{for } q\neq p(j,i) \end{cases}$$ And LP duality tells you that this is optimal. 

Nondeterministically generate a string of $n$ "blank" non-terminals, which we think of as the "tape". One of the blank non-terminals should be a separate "blank + read-write head + start state" non-terminal. If the parse string isn't $1^n$ then this derivation will end up failing. We describe the rest of the process in terms of the deterministic computation simulated by the only possible derivation. Print on the tape an encoding of $D_i$ followed by the number $i$ in binary, where $i = n - c$ and $c$ is chosen so that we always have enough space on our tape to do what we need to. (This is possible since the space required to encode both $D_i$ and $i$ grows logarithmically in $i$.) Evaluate $D_i$ on input $1^i$. This doesn't require a representing $D_i$'s tape -- you can just store a single state, which you change according to the transitions of $D_i$ as you decrement $i$. If $D_i$ rejects $1^i$, overwrite the entire tape with non-terminals which produce $1$. Otherwise fail. 

Firstly, we need our non-regular unary CSG. (EDIT: so, this was overkill -- non-regular unary CSLs can easily be exhibited e.g. via the pumping lemma on any language which exhibits the most basic of non-regularity. See the comments for examples. In hindsight, using a diagonal argument was like bringing a nuclear warhead to a knife fight. Peruse this construction if you're curious, otherwise skip to the reduction.) Let $D_1, D_2, ... $ be an enumeration of DFAs over alphabet $\{1\}$, such that the number of states in $D_i$ increases in $i$. We describe a CSG $G_X$ in terms of its behaviour whilst parsing string $1^n \in \{1\}^*$: 

$URL$ studies one such problem. The problem being studied is multi-unit auctions with unknown supply. The setting is a non-strategic setting (i.e each bidder reports truthfully irrespective of the outcome). Each bidder wants a single item among items which arrive online. When an item arrives it must be allocated immediately, else it perishes. At the end of the algorithm designer is allowed to charge a uniform price to each allocated bidder(which is atmost the bidders bid). The paper shows that there is no deterministic algorithm which is constant competitive, while they show a $1/4$ competitive randomized algorithm. 

I am searching for the VC-dimension of the following set system. Universe $U=\{p_1,p_2,\ldots,p_m\}$ such that $U\subseteq \mathbb{R}^3$. In the set system $\mathcal{R}$ each set $S\in \mathcal{R}$ corresponds to a sphere in $\mathbb{R}^3$ such that the set $S$ contains an element in $U$ if and only if the corresponding sphere contains it in $\mathbb{R}^3$. Details which I already know. 

Finding a minimum degree MST. It is np-hard as finding a hamiltonian path is a special case. A local search algorithm gives to within an additive constant 1. Reference Approximating the minimum-degree Steiner tree to within one of optimal 

For k<=2 1) k=0, the bound is $O(1/\epsilon^2+log(n))$ from $URL$ 2) k=1, The paper by Alon et all gives a reference to paper by Morris which takes $\tilde{O}(log(log(n))$ space. 3) k=2, I think the AMS sketch from their paper is optimal 

We take $X = L(G_X)$. Clearly $X \ne L(D_i)$ for any $i$, since $1^{i+c} \in X \Leftrightarrow 1^{i+c} \notin L(D_i)$. 

Alas, your problem is undecidable. The approach I stumbled upon (which might be overwrought, so anyone who has a more expedient approach should step up!) first uses a diagonal argument to demonstrate that there is a unary CSL $X$ which isn't regular (in contrast to the positive result for unary CFLs), and then reduces from the halting problem for Turing machines by, given a TM $M$, constructing a CSG $G$ which simulates $M$ on a length of tape shorter than the parse string $w$, recognising $X$ if $M$ halts without overstepping its bounds and failing to parse otherwise, so that $G$ successfully parses all $w \in X$ that are sufficiently long iff $M$ halts (so that $L(G)$ differs from $X$ on only finitely many strings and therefore cannot be regular), otherwise $G$ recognizes the empty language (which is clearly regular). Key to this approach is the observation that CSGs are not merely concerned with grammatical matters such as phrase structure -- indeed, CSG derivation sequences can carry out arbitrary nondeterministic space-bounded computation (indeed there are $\mathbf{PSPACE}$-complete CSLs) before ever getting to the business of aligning with the parse string. This is most easily observed via standard conversions between CSGs and monotonic grammars (which continues to work when restricted to unary alphabets), and the use of simple monotonic productions to simulate Turing machine transitions on derivation strings which represent stages in a computation history. Throughout this answer I'm going to assume the reader can intuit most of the details when a CSG is required to simulate a given computation. (I assume the asker is comfortable with all of this, but I'm going over it for completeness. Nevertheless, feel free to request clarification in the comments.)