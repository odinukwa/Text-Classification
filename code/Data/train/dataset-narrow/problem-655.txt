Mirroring doesn't work like that. You can't bring multiple databases into a single database. You need to look to Replication, specifically Merge Replication as a solution for this. It can be set up across domains and it's specifically designed to take multiple different databases and bring them back to a single location. Mirroring is for creating a copy, a mirror, of a database. If you have more than one database, you'll have to maintain more than one mirror. 

Microsoft stopped developing functionality for Trace back in 2008. Everything now is focused on Extended Events. The same is true for Azure SQL Database. Extended events capture the rpc_completed event statement text in a very similar manner. As an example, I'm capturing rpc_completed events from a PowerShell script that is using the SQlClient.SqlCommand object to execute a stored procedure like this: 

No. It doesn't affect anything regarding the internal management within SQL Server. You're setting that connection for you, for your queries. SQL Server manages it's own locking it's own way. Why would you turn off page and row locking on an index? You're more likely to see more severe locking than if you let SQL Server manage that index as it sees fit. By setting both those values to OFF, you just told SQL Server to take a table lock out. Considering you're also messing with read_uncommitted, I'll bet you don't want that. If you're asking does read_uncommitted allow you to get dirty reads on indexes, yes. It does. 

The easiest way to do it is going to be to SSIS to migrate the data over. The wizards alone should be able to do something this simple. The most efficient way would be to export everything to Azure blob storage and then use BULK INSERT. I'd do it all using Powershell so that you can control error handling better. It's going to be a lot more work though. If it's not that big and it's a one-time thing, I'd go with SSIS. 

Assuming we're talking about a full or differential backup, a marker is placed at the start of the backup process. At the end of the backup process, any transactions that committed during the process are rolled forward into the backup. Any transactions that are not completed are marked as rolled back within the backup. So, the short answer to your question is, all completed transactions from the beginning to the end of the backup process. During a restore operation, the final step is the cleanup of these transactions. 

After you dig yourself out of this hole, please, read this and learn from it. Logs are an essential part of how SQL Server works. You can't just get rid of them. But you can manage them appropriately. You're probably working off the default settings, which create all databases in Full Recovery mode, meaning, the logs are going to grow. Change this after you create the database. Other than that, @gbn up there has the answer. 

It's exactly what you'd expect if you were running trace, but it's from extended events. Here's documentation to get you started using Extended Events in Azure SQL Database. 

You can either build your own monitoring tool or look to a 3rd party solution that can provide one for you. If you're interested in building your own, it depends on what version of SQL Server you're working with. If it's 2005, you can use the Blocked Process Report trace event. If you're running 2008 or above, I'd suggest using the equivalent extended event, blocked_process_report. Jonathan Kehayias has a good write up on how to use it. If you're looking at 3rd party products, Red Gate software's SQL Monitor has blocked process and long running process alerts built in. 

I went through a pretty thorough 2008R2 training a year ago. It wasn't radically different from 2005, but it was different enough that you're going to face quite a bit of frustration when you can't do things in 2005, or the things you want to do are done differently. SSAS has been changing quite a lot over the last several releases. This is the list of changes between 2005 & 2008, and it's not small. The new designers are the things that are going to cause you the most pain. 

Looks like some sort of heart-beat detector to see if the database is still online. Certainly it's going to return junk data since it has a TOP without an ORDER BY. As far as not seeing it, I don't know that I'd suggest eliminating this. You want to know if something is being called every 15 seconds, especially since that query is probably causing scans on the data. However, if you really want to, just add a filter on the TEXT column and include that text in the filter. 

You're getting a secondary predicate on the key lookup operation. That's going to slow stuff down. It's not doing a full scan, but remember that even seeks are just limited scans. So the key lookup is still doing what it did before, but now it's also adding a filtering step because of that added index. You could look at exploring a covering index, however, because you have a SELECT *, covering means all columns, effectively creating a second clustered index. There'll be some sacrifice of performance on data modification queries and on disk space (depending on where you are on Azure SQL Database, that could bump you to another tier). Instead, I'd start with experimenting with your existing nonclustered index. See if adding this new column to the key of that index helps at all. Keep it as the second column, other wise you're changing your histogram which could cause other issues. You're going to have to experiment to see for sure what works best in your situation. 

You need to run DBCC CHECKDB on the database. That will identify what the issues might be that are causing the database to be marked suspect. If it's something simple, like a non-clustered index with mismatched pages, you can drop & recreate the index. If it's something like a data table, you'll probably have to go to your backups and run a restore operation. 

The only way to properly answer that question is to fire up the debugger and see what choices were made by the optimizer along the way. The costs are not only IO and CPU. There are additional costs associated with a given operator that are reflected in the total cost, but are not reflected in the IO and CPU cost estimates. You can read more about some of the additional costs in this excellent article by Paul White. I don't have a precise answer to your question (I've not doubt, Paul would). However, I'm willing to take a guess. What you're seeing is added overhead for the operation as determined by the optimizer above and beyond what it is displaying as the overhead for the IO and CPU as determined by the estimated rows, etc.. I believe it's a calculation based on what would necessary in terms of IO to create the table and store the 246.492 rows * 9b worth of data on each that is calculated as being in the INSERT statement. 246.492 * 9 / 1024 = 2.1664 is less than an 8k page. However, we have to create at least a page, so when you calculate 8 * the cost of .01, it puts us just a little above the estimated .073832. That's my guess, and it is a guess. However, I do know that there is overhead in the costs that isn't displayed by the strict addition of CPU + IO in all cases. 

Just as when you update a value in an index that's stored in leaf and non-leaf pages, all those pages get updated with the new value. Columns stored at the leaf level only through include are updated when you update values. It's possible for this to lead to spage splits too. 

Not suggesting a better way to run your queries, but hopefully answering the question. You're asking if SQL SErver can ignore the view when you're running queries, but presumably, you're writing the query against the view. In which case, no, SQL Server can't ignore the view. However, there is a process within the optimizer called simplification. Given enough time (more on that in a moment), SQL Server can recognize which parts of a view you're using or not using and then eliminate tables from the execution plan that are not needed to satisfy the query. But, it has to have enough time to do that. Since you're working with nested views, a major coding issue, you're not generally going to have enough time for the optimizer to get a good execution plan, let alone perform simplification and eliminate unnecessary queries. Short answer, no. There's not short cut open to you. You need to rearchitect the queries to eliminate nested views. Any other tuning you do will be, at best, perephiral to the problems you're experiencing.