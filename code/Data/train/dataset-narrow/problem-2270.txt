The former is really a question in program verification and has little to do with typing. Just needs showing that your implementation meets its specification, see Andrej's answer. Let me talk about the later question. As Andrej said, from an abstract point of view, a typing system seems to enforce properties on programs. In practise, your typing system $T$ seeks to prevent errors from happening, meaning that typable programs should not exhibit the class of errors of interest. In order to show that $T$ does what you think it should, you have to do two things. 

Pippenger's (1) from 1996 shows that (under some assumptions) strict (CBV) functional programming languages are asympotically slower than imperative languages. It is open whether Pippenger's result can be generalised to lazy functional languages, as was pointed out in (2). Pippenger imposes two simplifying assumptions (on-line computation, and a certain atomicity of input). It is open whether they can be removed. Pippenger conjectures that it can be done, but warns: "[s]uch a result [...] seems far beyond the reach of currently available methods in computational complexity theory". See also Campbell's answer in (3), and Ben-Amram's notes (4). 

However, we can easily extend the relation $\equiv_{type}$ to typing environments $\Gamma$ pointwise, and also to programs, e.g. $\lambda x^{\alpha}.M \equiv_{type} \lambda x^{\beta}.M$ whenever $\alpha \equiv_{type} \beta$, leading to a rule $$ \frac{ \Gamma \vdash M : \alpha \quad \Gamma \equiv_{type} \Delta \quad M \equiv_{type} N \quad \alpha \equiv_{type} \beta }{ \Delta \vdash N : \beta } $$ I have never seen this done. I assume that it's just a question of convenience and that both approaches are equally expressive, but I am not sure! Is there some problem with the second approach? 

What is here? It's supposed to be a variable that ranges over one-holed contexts. But languages like OCaml, Haskell or Scala don't give programmers variables that range over arbitrary (one-holed) contexts, only variables that range over values. In other words, in such languages you cannot pattern match at an arbitrary position in a term. You always have to specify the path from the root of the pattern to the parts that you are interested in. I guess the key reason for imposing this restriction is that otherwise pattern matching would be non-deterministic, because a term might match a pattern in more than one way. For example the term matches the pattern in two ways, assuming ranged over such contexts. 

F. W. Lawvere, Diagonal arguments and cartesian closed categories. D. Pavlovic, On the structure of paradoxes. N. S. Yanofsky, A Universal Approach to Self-Referential Paradoxes, Incompleteness and Fixed Points. 

where key concepts from logic, such as conservative extension of theories are generalised to programming languages. 

I'm not 100% sure what the question is about, but the title seems to ask about computation that allows failure. There is a lot of work on noisy (erroneous) computation in the sense that I think you are asking about. I don't have time to give a complete overview, but here are some pointers that may be of interest. 

One standard way of defining Bad is to say: a term $M$ has an immediate type error if it is neither a value nor has a reduction step $M \rightarrow N$. (In this case $M$ is often referred to as stuck.) This only works for small-step operational semantics. One standard way of proving the theorem is to show that 

T. Coquand, Pattern matching with dependent types. M. Hofmann, T. Streicher, The groupoid interpretation of type theory. 

I'm not 100% sure if the explanation below is historically accurate. If it isn't, please feel free to edit or remove. Mutation testing was invented by Lipton. Mutation testing can be seen as a way to measure the quality or effectiveness of a test suite. The key idea is to inject faults into the program to be tested (i.e. to mutate the program), preferably the kinds of faults a human programmer is likely to make, and see if the test suite finds the introduced faults. A typical example of the kind of fault mutation testing would introduce could be to replace x > 0 by x < 0, or replace x by x+1 or x-1. The fraction of faults caught by the test suite is the "mutation adequacy score" of a test suite. Speaking very loosely, one can think of this as a Monte-Carlo method for computing the mutation adequacy score. More abstractly one might say that mutation testing brings to the fore a symmetry or duality between a program and its test suites: not only can the test suite be used to become more confident about the correctness of a program, but conversely, a program can be used to gain confidence about the quality of a test suite. In the light of this duality, mutation testing is also conceptually close to fault injection. Both are technically similar but have different purposes. Mutation testing seeks to measure the quality of the test suite, while fault injection seeks to establish the quality of the program, usually the quality of its error handling. Recently, ideas from mutation testing have been used to test (formalisations of) logical theories. To paraphrase the abstract of (4): When developing non-trivial formalizations in a theorem prover, a considerable amount of time is devoted to “debugging” speciﬁcations and theorems. Typically, incorrect speciﬁcations or theorems are discovered during failed proof attempts. This is an expensive form of debugging. Therefore it is often useful to test conjectures before embarking on a proof. A possible way of doing this is to assign random values to the free variables of the conjecture and then evaluate it. (4) uses mutations to test the quality of the used test-case generators. History. From (1): The history of mutation testing can be traced back to 1971 in a student paper by Richard Lipton [...] The birth of the field can also be identified in other papers published in the late 1970s by Lipton et al. (2) as well as Hamlet (3). 

With FSMs the situation is a bit better, because we do have computationally meaningful algebraic theories of automata construction, the most well-know of which might be process algebras like CSP, CCS, $\pi$-calculus, ACP and so on. But none of them is a calculus of tape-head movements, which is what we'd need if we wanted to compose TMs. All in all, we are quite far away form a substantial algebraic / categorical treatment of computational complexity, and we'd need several conceptual advances to get there. 

Not a direct answer to the question but, as DPLL can be seen as a special case of CDCL, hopefully of interest. The problem of finding optimal branching literals for DPLL is (in a way) harder than SAT itself: the paper "On the complexity of choosing the branching literal in DPLL" by Liberatore shows that choosing optimal branching literals in DPLL is both NP-hard and coNP-hard. 

Summary: full completeness means that the interpretation function is not just complete, but also surjective on programs. Full abstraction has no requirement for surjectivity. $\newcommand{\semb}[1]{[\![ #1 ]\!]}$ Details: The detailed meaning of full abstraction and full completeness depends on the nature of what/where/how you are interpreting. Here is a rendition for the interpretation of one typed programming language into another. You have an interpretation function $\semb{.}$ which maps three things. 

One problem of PL research is that there are no clear-cut open problems like the P/NP question where we can immediately say if a proposed solution works or not. 

Progress on such compilers has been much swifter than many (including myself) anticipated, and I expect to see more and more of this work to seep into industrial compilers. The problem with formal semantics is rarely to write down the semantics of a language. That's usually simple. The real issue is how efficiently to prove theorems about programs in a language using the semantics. 

Virtually every programming language employs types in some form or other. Some languages use types only at run-time or only at compile-time, but hybrids are also widespread. Program specification and verification often uses types. The fruitful connection between programming languages and logic (Curry-Howard correspondence), which has led to technology transfer between both fields, is based on types. Many interesting algorithmic questions arise from type-checking and type-inference. All of the above are empirical evidence that types should be a core part of the study of programming languages. 

One possibily path into abstract algebra could be to look at it from point of view of cryptography, which is about algorithms on finite field. Fields are rings, and fields are also two groups coupled by simple laws. Field theory uses vector spaces in prominent position (Galois theory), so this angle should cover a lot of abstract algebra. The book A Computational Introduction to Number Theory and Algebra by V. Shoup could therefore be of interest. My personal recommendation would be to ignore applications, and study a basic undergraduate maths text on abstract algebra. There is no shortage of those. Just trust that all this stuff is useful, and that the use will reveal itself more easily once you have a basic grasp of the material. Most basic algebra is constructive and you can easily implement basic concepts to gain a better understanding, e.g. algorithms that check if a multiplication table is a group, an equation solver in a group, a program that checks if two algebraic structures are isomorphic etc. Most of these problems have brute-force solutions which are easy to implement, but slow. The more you learn about algebra, the more algorithmic shortcuts you can make, to speed up your programs. E.g. the famous Miller-Rabin and AKS primality tests. 

(Traits are a generalisation of Java interfaces). This can be resolved at compile-time, although I don't know how the Scala compiler and the JVM implement this. Other languages with expressive polymorphism should enable similar constructs. Edit following Neel's remarks: here is a Scala implementation that, I think, does exactly what you want, but without a hack like type-parameterisation.