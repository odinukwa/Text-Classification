If I understand your problem correctly, we can formulate it like so: given pairs $(\alpha_k,\beta_k)$ and $\Delta_0$, find $s > 0$ such that $\max_k (s\alpha_k + \beta_k) = \Delta_0$. Here is one way to solve this (this algorithm can be made more efficient). Go over all possible $k$. For each $k$, find the value of $s$ such that $s\alpha_k + \beta_k = \Delta_0$ (namely $s = (\Delta_0 - \beta_k)/\alpha_k$), check that it's positive, and check that for all $l \leq k$, $s\alpha_l + \beta_l \leq \Delta_0$. This runs in $O(m^2)$, where $m$ is the number of pairs (in your case, $m = n(n-1)$); you can probably get the running time down to quasilinear. Your case is more structured, so you may be able to get an even faster algorithm. 

When researchers write grant proposals or introductions to papers, they often claim that their research pertains and informs practice. Whether they believe in it or it's just lip service I don't know (and it may depend on the researcher), but you should be aware that the connection is greatly exaggerated in academic circles, for obvious reasons. One thing that limits us as mathematical researchers is our dogmatic attachment to formal proof. You mention analysis of randomized algorithms fed by simple pseudorandom generators. This kind of analysis cannot be extended to real-world problems, since they are simply too complicated. And yet, in practice people solve even NP-hard problems all the time, with informed methods. Real-world problems are better understood with a more scientific and experimental eye. They are better solved from an engineering perspective. They inspire theoretical research, and occasionally are informed by it. As Dijsktra said, (theoretical) computer science is not really about computers, not any more. 

Johnson graphs are actually easy to recognize. In particular, you can recognize whether an input graph is a Johnson graph in polynomial time, and you can construct an isomorphism between two isomorphic Johnson graphs in polynomial time. Johnson graphs come into the proof in a different way. Very roughly speaking, the proof juggles between group-theoretic reductions and combinatorial reductions based on individualization/refinement. Johnson graphs are an obstacle for combinatorial reductions but amenable to group-theoretic ones. The Johnson graphs in question are not the actual input graphs – in fact the problem solved by the algorithm is more general than graph isomorphism – but rather, they show up after a series of reductions during the algorithm. 

When $m = O(\log n)$, you can use dynamic programming to find the optimum in polynomial time. The table contains Boolean-valued cells $T_{\ell,X}$ for each $\ell \in \{0,\ldots,k\}$ and $X \subseteq \mathcal{U}$, indicating whether there are $\ell$ sets which cover the elements in $X$. When $m = O(\sqrt{n})$, say $m \leq C\sqrt{n}$, the problem remains NP-hard. Given an instance of SET-COVER, add $m$ new elements $x_1,\ldots,x_m$ and $(2C^{-1}m)^2$ new sets, consisting of non-empty subsets of the new elements, including $\{x_1,\ldots,x_m\}$ (when $m$ is large enough, $(2C^{-1}m)^2 < 2^m$). Also increase $k$ by one. The new $m,n$ are $m' = 2m$ and $n' = n + (2C^{-1}m)^2 \geq (C^{-1}m')^2$. 

It is NP-hard to decide whether your problem is soluble. That's bad news from a theoretical point of view, but in practice it is possible that a heuristic algorithm will fare quite well. Also, you're (possibly) dealing with the "promise" variant of the problem, that is, you assume that a solution exists, and want to find it. To see that the problem is NP-hard, we reduce Hamiltonian cycle to it. All the edges are always open, and they always have unit weight. The windows for each non-zero vertex are $[1,n-1]$ and $[n+1,2n-1]$; for vertex zero they are $\{0\}$ and $\{n\}$. These constraints are satisfiable only via a Hamiltonian cycle, and the converse is also true. 

You can (without too much work) find a number $K$ such that if $|x| > K$ then $|a_n x^n| > \sum_{k<n} |a_k x^k|$, and so all roots are inside $(-K,K)$ and the distance between them is smaller than $2K$. 

Any function which has non-zero correlation with parity has degree $n$. That is, if $$\sum_{x \in \{0,1\}^n} (-1)^{\sum_i x_i}f(x) \neq 0$$ then the unique multilinear expansion of $f$ contains the monomial $x_1\cdots x_n$. Indeed, since $(-1)^{x_i} = \frac{1-x_i}{2}$, the Fourier expansion of $f$ (expressed in terms of products of $\frac{1-x_i}{2}$) will contain the term $\prod_i \frac{1-x_i}{2}$, and the corresponding monomial $\prod_i x_i$ doesn't appear in any other term. Nisan and Szegedy proved that functions of degree $d$ depend on at most $d2^d$ variables. For $d = 1$ we can be more exact: the function must depend on at most one coordinate. 

The original paper by Smolensky, On Representations by Low-Degree Polynomials, actually contains a direct lower bound on majority. You can have a look at the original paper (behind a paywall) or at this writeup. For the converse, it is known that an NC$^1$ circuit of size $S$ can be simulated in AC$_0^d$ in size $2^{S^{1/\Omega(d)}}$. Majority has NC$^1$ circuits of polynomial size, and so you can't get a lower bound better than $2^{n^{1/O(d)}}$. The state-of-the-art might well be Amano's paper for the upper bound, O'Donnell and Wimmer for the lower bound; the latter use Håstad's switching lemma. According to Amano, the minimum size of a depth $d$ circuit that $\epsilon$-approximates Majority on $n$ variables is $\exp(\Theta(n^{1/(2d-2)}))$. 

Higher-order discrete derivatives of set functions are explored in Submodularity, supermodularity and higher-order monotonicities of pseudo-boolean functions. According to them, the strict third-order discrete derivative condition is $$ \begin{multline*} f(A \cap B) + f(A \cap C) + f(B \cap C) + f((A \cap B) \cup (A \cap C) \cup (B \cap C)) \geq \\ f(A \cap (B \cup C)) + f(B \cap (A \cup C)) + f(C \cap (A \cup B)) + f(A \cap B \cap C). \end{multline*} $$ The "aggregate" condition is mentioned in the paper "A characterization of a cone of pseudo-boolean functions via supermodularity-type inequalities" by Cramma, Hammer and Holtzman (inequality (4)), which is part of the rare collection "Quantitative Methoden in den Wirtschaftswissenschaften". This condition should be the same as mine. Edit: The actual condition that Cramma, Hammer and Holtzman give is $$ \begin{multline*} f(A) + f(B) + f(C) + f(A \cap B \cap C) \geq \\ f(A \cup B \cup C) + f(A \cap B) + f(A \cap C) + f(B \cap C). \end{multline*} $$ If you put $C = \varnothing$, you get submodularity. 

If you allow an auxiliary work tape, then it is easy to customize the Turing machine model to suit your needs. The Turing machine has two tapes: input/output and work. The work tape operates as usual, while on the input/output tape, all you can do is switch two adjacent values. This is enough since the transpositions $(i \quad i+1)$ generate all of $S_n$. If you don't allow a work tape, then the goal might be impossible. Let $f$ be a recursive function not computable in $O(n\log n)$ space. We can think of $f$ as a reordering function as follows: convert the $n$ input bits into a permutation in $S_{2n}$ conforming to the regular expression $(12+21)(34+43)\cdots$. The output is read, say, from whether the first element is smaller than the second element or vice versa. In many reasonable models, a computation is convertible to a Turing machine computation with space $O(n\log n)$. (Though a "copy" operation might invalidate this.) Finally, you ask whether this class of transducers has been researched. Since you want this class to encompass all partial recursive reorderings, it is fair to say that recursion theory is the study of these reorderings and their non-recursive generalizations. 

Consider again the case of injective $f_w$. We can think of the letters of $w$ as generating a subgroup of $S_n$. Babai and Seress show that the diameter of the corresponding Cayley graph is at most $e^{(1+o(1)) \sqrt{n\log n}}$. This means that every permutation in the subgroup, including $f_w$, can be written as a word over the letters of $w$ and their inverses of length at most $e^{(1+o(1)) \sqrt{n\log n}}$. Since the order of each permutation is at most $e^{(1+o(1))\sqrt{n\log n}}$, we can replace the inverse of each letter by at most $e^{(1+o(1))\sqrt{n\log n}}$ copies of itself, obtaining a word of length $e^{2(1+o(1))\sqrt{n\log n}}$. This bound can perhaps be improved by looking at the construction of Babai and Seress. In the general case, segment $w=x_1\ldots x_k$ as before, where $k \leq n$. A similar argument shows that $|x_i| \leq e^{2(1+o(1))\sqrt{n\log n}}$, and so $|w| \leq e^{2(1+o(1))\sqrt{n\log n}}$. 

(If we want to make the statements completely equivalent, we should ask all hyperedges to have uniformity at most $d$ rather than exactly $d$.) The question has the flavor of Hall's matching theorem and of Beck-Fiala, but so far I was unable to prove the conjecture using these tools. Perhaps I am missing some simple argument, or perhaps there is a simple counterexample. 

The "canonical" answer (not necessarily linear time) is to use what could be termed binomial encoding (this is probably the same solution given by Knuth). The idea is to interpret Pascal's identity $\binom{n}{k} = \binom{n-1}{k} + \binom{n-1}{k-1}$ as stating that the code of those subsets not containing $n$ should fit into $[0,\binom{n-1}{k})$, while the code of those subsets containing $n$ should fit into $[\binom{n-1}{k},\binom{n}{k})$. (The order is arbitrary, of course.) More precisely, we use the following procedure for encoding: $\operatorname{ENCODE}(S,n,k)$: 

Suppose you have $n$ numbers $x_1,\ldots,x_n$ of width $m \geq \log n$. Without loss of generality, all numbers are different (add an extra $\log n$ lower-order bits). Two numbers can be compared in AC0, so in AC0 we can compute, for each $x_i$, a binary vector $v$, defined by $v_j = 1$ if $x_j \geq x_i$. In TC0 we can sort $v$ to $w$, and then locate (in AC0) the position $k$ such that $w_k = 1$ while $w_{k-1} = 0$ (where $w_0=0$, $w_{n+1}=1$). Given these values $k$ for each $i \in [n]$, we can compute the sorted array in AC0. In total, we get a TC0 circuit. 

See also the recent paper of Daniel Kane and Ryan Williams, Super-Linear Gate and Super-Quadratic Wire Lower Bounds for Depth-2 and Depth-3 Threshold Circuits (STOC 2016). Ryan describes the paper as follows (the following description is taken from his homepage): 

Uniquely solvable puzzles A uniquely solvable puzzle (USP) of length $n$ and width $k$ consists of a subset of $\{1,2,3\}^k$ of size $n$, which we also think of as three collections of $n$ "pieces" (corresponding to the places where the vectors are $1$, the places where they are $2$, and the places where they are $3$), satisfying the following property. Suppose we arrange all the $1$-pieces in $n$ lines. Then there must be a unique way to put the other pieces, one of each type in each line, so that they "fit". Let $N(k)$ be the maximum length of a USP of width $k$. The USP capacity is $$ \kappa = \sup_k N(k)^{1/k}. $$ In a USP, each of the pieces needs to be unique - that means that no two lines contain a symbol $c \in \{1,2,3\}$ in exactly the same places. This shows (after a short argument) that $$ N(k) \leq \sum_{a+b+c=k} \min \left\{ \binom{k}{a}, \binom{k}{b}, \binom{k}{c} \right\} \leq \binom{k+2}{2} \binom{k}{k/3}, $$ and so $\kappa \leq 3/2^{2/3}$. Example (a USP of length $4$ and width $4$): $$\begin{align*} 1111 \\ 2131 \\ 1213 \\ 2233 \end{align*}$$ Non-example of length $3$ and width $3$, where the $2$- and $3$-pieces can be arranged in two different ways: $$\begin{align*} 123 && 132 \\ 231 && 321 \\ 312 && 213 \end{align*}$$ Coppersmith-Winograd puzzles A Coppersmith-Winograd puzzle (CWP) of length $n$ and width $k$ consists of a subset $S$ of $\{1,2,3\}^k$ of size $n$ in which the "pieces" are unique - for any two $a \neq b \in S$ and $c \in \{1,2,3\}$, $$ \{ i \in [k] : a_i = c \} \neq \{ i \in [k] : b_i = c \}. $$ (They present it somewhat differently.) Every USP is a CWP (as we commented above), hence the CWP capacity $\lambda$ satisfies $\lambda \geq \kappa$. Above we commented that $\lambda \leq 3/2^{2/3}$. Coppersmith and Winograd showed, using a sophisticated argument, that $\lambda = 3/2^{2/3}$. Their argument was simplified by Strassen (see Algebraic complexity theory). We sketch a simple proof below. Given $k$, let $V$ consist of all vectors containing $k/3$ each of $1$s, $2$s, $3$s. For $c \in \{1,2,3\}$, let $E_c$ consist of all pairs $a,b \in V$ such that $\{ i \in [k] : a_i = c \} = \{ i \in [k] : b_i = c \}$, and put $E = E_1 \cup E_2 \cup E_3$. Every independent set in the graph $G = (V,E)$ is a CWP. It is well-known that every graph has an independent set of size $|V|^2/4|E|$ (proof: select each vertex with probability $|V|/2|E|$, and remove one vertex from each surviving edge). In our case, $$ |V| = \binom{k}{k/3} \binom{2k/3}{k/3}, \quad |E| \leq 3|E_1| = \frac{3}{2}\binom{k}{k/3} \binom{2k/3}{k/3}^2. $$ Hence $$ \frac{|V|^2}{4|E|} = \frac{1}{6} \binom{k}{k/3} \Longrightarrow \lambda \geq \frac{3}{2^{2/3}}. $$