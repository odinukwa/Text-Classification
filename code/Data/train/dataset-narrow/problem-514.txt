I'm speculating that since you were given RDP access and a username/account to the database server, the security credentials in question are Active Directory credentials. DBeaver on Linux likely cannot use AD credentials (unless the environment taps into AD for authentication on Linux, in which case this isn't your issue). I don't know if trying to authenticate via SQL Authentication using AD Credentials will throw the error you've provided, but it may be the cause. I don't use DBeaver, so again this is just speculation. HeidiSQL requires that you check the box next to Use Windows authentication as shown below: 

Rebuilding the clustered index doesn't automatically rebuild any non-clustered indexes associated with the table. To do this, you would need to specify the keyword instead of the clustered index name. Per MS: 

If this doesn't provide notification that the EE has been stopped, you may be running into a situation where the event is failing for whatever reason. At that point review your ERROR logs and see if anything occurred and dig from there. If you can narrow down a specific error code, you could then configure a SQL Agent Alert to email you if/when that specific error happens again. 

Just a guess, but I suspect someone likely only wanted to manage one backup file without thinking of the consequences if it became corrupted. This approach does backup the TLogs, so you can recover from this backup file. However, shrinking the log is a bad idea for many reasons (as explained by Brent Ozar and Aaron Bertrand). 

The key differentiator between the recommended locations (e.g. local storage, a SAN, or an iSCSI-based network) and a network share is quite simply, redundancy. All of the recommended approaches provide an option for redundant paths for I/O to take to persistent storage. For instance, you can RAID local storage, providing redundancy if any disk fails. With either the SAN or iSCSI-based network storage approaches, these technologies use Multipath Input/Output (MPIO) drivers, providing redundancy to the storage. A network drive, in contrast, does not employ or allow for any redundant I/O paths. If a Network Interface Controller (NIC) fails on either end, the share likely disappears. Even if you have multiple NICs, there will still be a brief outage as a different IP address will now host that share, so any data sent to the old/failed IP will timeout and disappear. Basically a network share wasn't designed with this level of redundancy, and a loss of data mid stream may corrupt your database (or worse just get lost without a trace). The whole point of a database is to reliably store data and a network share brings that whole reliable aspect into question. 

If this doesn't release a sufficient amount of space back to the OS, you'll then need to evaluate other approaches such as the prior answer provided by George K which is likely in reference to Paul Randall's article, Why you should not shrink your data files. Finally, another, yet more manual approach, would be to run using the option a number of times during a number of different maintenance windows. After the initial operation, you would disable automatic file growth on whatever data file(s) you feel need further attention. If lucky, your standard maintenance will shuffle data around enough within the file so that you can run additional operations with the option in the future to further reduce the file(s) without having to move data around within it. After the file gets to a more desirable size, re-enable auto growth so you don't run into database halting issues as your data file begins to grow over time again. To disable auto growth on a data file, run the following: 

My initial suspicion is that the initial setup didn't include the fully qualified name, but check for certain with the following statement: 

If you're lucky, a lot of the space you cleared with your cleanup operation will be located at the end of your data files, and specifying the option will release that space back to the OS quicker than any other operation because no data movement will be occurring within the datafile itself. Running this command is as simple as follows: 

I believe the blue question mark indicates that SSMS does not know if the service is running or not, by way of a WMI call. I'm quite certain this icon replaced the blank or white circle icon of prior SSMS versions. After some digging, it looks like you could run into one or more issues causing this behavior, as follows: 

Even the execution plans are the same, so why the different result sets between a temp table and a formally defined table? Finally, a shout out to Joe Obbish as I gratuitously ripped off his CROSS JOIN approach to build large sets of test data as it's quite efficient! 

Now, check the shortcuts took after pressing Ok and your shortcut keys should work as expected. As you can see, I assigned mine to and , but you can use whatever you want: 

Chances are something in the backup history tables got out of sync and the UI is going to the last "consistent" full backup. If you're really interested in why it's doing what it's doing, start a profiler trace limited to your account, walk through the restore steps in the GUI, and review the commands captured in the trace that show what the UI is doing behind the scenes. This will get you the definitive answer you're looking for. If you'd rather just move past this, you can clear your backup history via (depending on the last time you ran this, you may want to clear it out a month at a time) and then take a new full, etc. I would suspect this will reset the GUI to use the proper backups going forward. Finally, another option is to run this script authored by Wayne Sheffield. It may provide some more information regarding any issues with the backup chain. I didn't come across this until after I posted this answer originally, but hopefully it helps someone else out in the future. 

This is a supplemental answer and I'm making some assumptions here, but it looks like you're trying to reduce the time it takes to complete this process overall. In that case you shouldn't limit yourself only to figuring out how to reduce the summary query. I'm not saying you shouldn't prioritize it, but there may be other steps of your process where you can save additional time minimizing how much performance you need to squeeze out of the summary query. I would suggest you upgrade your environment to SQL 2016 SP1 or later if you've not already done so. This opens up a lot of functionality you can use (even with Express edition) that will likely help with optimizations, such as Table Partitioning, Table Compression, and/or Columnstore Indexing. These features can be used individually or in conjunction with one another and should provide some performance improvements so long as you're not currently running up against a CPU bottleneck in your environment. You may also be able to improve your ETL import processes. This article, Guidelines for Optimizing Bulk Import, from Microsoft goes over some concepts that may apply to your scenario. There's a lot of information in it regarding the Bulk Logged Recovery Model, and if you think of using it, I will also point you to Considerations for Switching from the Full or Bulk-Logged Recovery Model which goes over the proper way to switch between the Full and Bulk-Logged Recovery models. There's a lot here, so again, this isn't an answer to your immediate issue so much as an attempt to show you some other areas you can further improve upon down the road. 

Wait, you didn't ask that question.... but you should have. All of the routines identified above encrypt the data at the database layer only. They decrypt the data on the server and send it over the wire. Unless you're forcing SSL connections to SQL, this data will be traversing your network in plain text and anyone with a packet sniffer could potentially grab it in flight. Encrypting/Decrypting data on the client will ensure the data is encrypted across the wire, but not all approaches will allow for this. Always Encrypted will perform the encryption at the client though, so with it your data is also safe in transit. It's really the best option for you and I would suggest you lobby to get SQL 2016 if you don't already have it. 

I'm not a MySQL guy, but maybe this works for you though it will only return records 5 and 2 because records 5, 2, and 1 have a rolling sum of 1.75 instead of the 1.5 you're filtering for. 

The functionality of the delayed start is best described by CoreTech's post from the SuperUser forum: 

Use Kenneth Fisher's scripts to identify which roles have these rights. I assume once you identify the roles in question, you want to get a better idea of what they're doing. To script them out, feel free to use a script I put together, here, which will fully script out the database role definition. 

SQL Server is not the best with String Manipulation. If this functionality will change over time or become complex in any nature, I would suggest you look into CLR as a way to search/replace complex strings. Here's a great article from Phil Factor on Simple Talk that walks through how to use CLR for complex string operations. 

Disaster Recovery is a complex topic and providing anything resembling a complete answer can't be provided on any forum answer. If you're new to this or feel overwhelmed, I heavily suggest reaching out to MS, a MS Partner, a Consulting Firm/Consultant, etc. and ask for help. 

That's the query SSMS runs when you click on the -> . To confirm fire up a quick SQL Profiler Trace using the TSQL template, filter to your login, then right-click on the server name and select , and you should see the same query roll past in the trace. The specifics of it differ between versions of SSMS, but they all pretty much stick to the same basic query. The error is interesting but not a critical thing to be concerned about. Do you have any 3rd party tools installed that integrate with SSMS? Perhaps these are part of the cause as well. 

I can "fix" it by converting the field to a string by wrapping the field value in a or function, but that also treats the output as a string and not a number. Is there a way to export this to Excel so that the rounding doesn't occur but still keeps this value numeric? 

A few things here that have not already been mentioned. First, is that a UNIQUE constraint can also be used for Referential Integrity. Per the BOL article on Unique Constraints and Check Constraints: 

As an account with elevated rights on the domain, open ADSI Edit (adsiedit from the command prompt) Right-Click on ADSI Edit -> Connect to... Connect to Default naming context Navigate to/Create the OU container holding the service accounts you wish to grant SPN rights to Right-Click on the OU -> Properties Click on the Security Tab Click the Advanced button Highlight SELF and click Edit... or if the SELF special user does not show up in the list of Group or User names, click Add... and enter SELF for the object name Click the Properties tab Select Descendant User objects from the drop down list next to Apply to: Note: This is the slight adjustment to the steps outlined in Ryan's blog post for the reasons better outlined by this ServerFault/StackExchange post. Check the Allow box next to the following: 

I fully expect down-votes on this, but deprecated doesn't mean discontinued. When a feature is deprecated it generally means that said feature won't be expanded and will likely be removed in a future release. According to MS documentation RMO doesn't show up under features that were discontinued in either SQL 2014 or SQL 2016, but again I wouldn't count on it being available going forward. The point here is if you've already got something that works using RMO with SQL 2012, see if it works in 2014 and continue using it. No point to reinvent the wheel if you don't have to. Who knows, in 5-years-time you may no longer use this application, this back-end, or even this approach. It's also entirely possible that RMO could turn into something similar to the sysindexes view that was deprecated way back in SQL 2005 but is still in SQL 2017 by the looks of it. In terms of an official "supported" approach, Merge Replication, as stated by Bartosz X in the comments is your best bet, based on what it sounds like you want to accomplish. Some additional searching on DBA.SE or Google should get you going. 

The first point is what I feel is the most important as related to your question because you can limit your parameters' length, type, etc. This makes is incredibly more difficult to inject nasty code. As to provide an even more complete answer, I've updated your sp accordingly. Interesting enough, in your case because you're trying to parameterize column literals, you'll need to nest sp_executesql statements, so the first nested statement sets column names as literals and the second execution passes in the pagination values, as follows: