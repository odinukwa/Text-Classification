In this example the table maintains information about, student, courses and exams, and we want that this information be consistent. Without this is not possible. Note that however the foreign key is checked: 

Just looking at an example of a table is a wrong way of normalizing. You should understand the meaning of the fields to properly normalize. For instance you should know if the salary is the same for all the persons that have a certain role, or if it differs from person to person (in the above example we cannot know, since each role has only a person, so that both interpretations are possible). To know the meaning of the properties you have two routes: either know the meaning of the elements of the domain, through a high level model of it, (for instance through an Entity-Relationship model or a UML model), or to have formalized this knowledge through the definition of the functional dependencies holding between the properties. In your example there are two points that are not clear (that this that are not clear from the example table): is the salary related to a role or to a person? Is the Est. Time related to a project or to the fact that an employee works for a certain project for that estimated time? If the correct anwer to those question is the first one in both cases, your schema is correct (or normalized). Otherwise you must revise it. 

The usual notation is to write separately the grouping attributes from the aggregation functions, writing the attributes on the left of the γ symbol and the aggregation functions on the right, so your query should be something like this: employeeId, date γ MAX(salePrice)→ largetSale(Sales) This means: 

It is efficient in terms of memory and does not require the use of a complex type like a PostgreSQL array (actually it is a bit array), and more, you do not have to pay attention to the difference between false and null (and also you could set the entire field to a null value, if you need to). 

The relation with the above functional dependencies has two different candidate keys: B and G. You can verify this if you compute the closure of both to see if it contains all the attributes: 

Both are in BCNF and the final decomposition is constituted by R1, R3, R4. Finally, it is worth to note that the following dependencies: 

and both the algorithms for BCNF and 3NF produce decompositions that satisfies this property (see the literature on the subject, for instance the demonstration on Chapter 3 of: “Garcia-Molina, Hector. Database Systems: The Complete Book, Pearson Prentice Hall”). What is true, and that differentiate 3NF from BCNF, is that the synthesis algorithm that produces the 3NF always preserves the dependencies of the original relation, while the analysis algorithm for the BCNF does not. 

The following decomposition is both in Third Normal Form and in Boyce-Codd Normal Form and is without loss of data and loss of dependencies. It can be obtained by applying both the “analysis algorithm” to produce the BCNF, and the “synthesis algorithm” to produce the 3NF, but note that in general only the latter is guaranteed to avoid any loss of dependencies: 

Without any knowledge about the workload of the database, is almost impossible to answer to a question like this. How often those “extra” data will be used? What are the most common queries? How frequent they are? and how much it is important the speed of the execution? The general advice is always the best one for me: make things as simple as possible, than optimize if you need to. So my advice is: create a single table, look at the performances, and only if those performances are not adequate to your objectives, split the table. 

No other set of attributes is a candidate key, since the only other attribute on the left side is , but is determined by , so it can be eliminated from that dependency. Actually a canonical cover of the original set of dependencies is: 

A simple check to see if a relation schema is in Third Normal Form is to see if every left hand side of non-trivial functional dependencies is a superkey (and this test can be used also to check if a relation is in Boyce-Codd Normal Form), or, if this is not the case, if every attribute on the right part is a prime attribute (i.e. part of any candidate key). So in you case both the dependencies A → BCD and BC → A (and also BC → D) have a candidate key as left part, so the schema is in Third Normal Form (and in this case also in the more strict Boyce-Codd Normal Form). 

this is because we decompose in two relations, and . The second relation is still not in BCNF, since in E → C the attribute E is not a superkey. So we can apply again this method to decompose R2 in: 

Finally, note that the dependencies 4 and 5, that you have called “Multiple Functional Dependency to the same dependency”, are in effect particular, in the sense that they are transformed in a relation (department) with two primary keys, the number of the department and the number of the manager of the department. And this is managed by the algorithm in the third step by merging the two relations previously obtained (and for this reason the relation has two primary keys). 

In the next step, all the functional dependencies (projected over the subschemas) should be tested to see if they violate the BCNF, first for , then for . But obvioulsy in relation , (which has key , for the way in which it has been build), the dependency does not violate the BCNF (that says that each dependency should have a superkey as determinant), so it is not used anymore to decompose the relation. So, from this example you can see that the dependency used for the decomposition is not used in the later steps of the algorithm: its determinant is the key of the first of the two new relations produced, and for this reason that particular dependency will not used anymore, since it does violate the BCNF of the schema produced (while other dependencies in the schema could still be used to decompose, like in the example linked). 

To find if the functional dependency AB→G is implied by F you should find the closure of the attributes AB under F, i.e. AB+. These are the steps: 

so the relations respects this definition too. For your second question, the relation has two keys, and , and it is already in BCNF as well as in 3NF. Note that the analysis algorithm to decompose a relation in BCNF should return the original relation, since no dependency violates its definition (each determinant is a superkey). 

This can be seen since there is a theorem that says that a simple criterion for checking whether a decomposition (R1,R2) is lossless-join is that either: 

that produce three relations, R1(A, B), R2(A, B, C), R3(B, C), and, following the algorithm, you obtain as result only R2, since the other two have attributes contained in them. So you have two different outputs from the algorithm, depending on the minimal base used (which in turn depends on the order in which you consider the dependencies when calculating the minimal cover). So, the answer to your question: 

is lost. The reason is that the attributes of this dependency are in two different tables, and there is no way to obtain it, since the tables do not have attributes in common. A decomposition that preserves the dependencies can be obtained by applying the analysis algorithm to produce the Boyce-Codd Normal Form, or the synthesis algorithm to produce the Third Normal Form. 

In your question you ask about the “best performance”, but with respect to what kind of queries? It is not clear which is the typical query, or the typical queries, so my answer will be generic. I think that in this case the best schema is the following: 

First, you assumption about the 3NF is correct. Then, in the analysis algorithm to find the BNCF, when you start to remove a dependency X → Y since it violates the BCNF, you should put in the first relation not only , but , while in the second relation you should have . So, in the first step, the two resulting relations are: 

Yes, for instance you have the information on the file (name, size, data creation, etc.) repeated for different accesses to the same file. 

The important thing about the 2NF is that in each (non trivial) dependency the determinant should not be a proper subset of a key. In the example, the determinant of AB->C is the full key, while the determinant of C->D is C, which is no part of any key. So the schema is obviously in 2NF. 

If I have interpreted correctly your question, here is a query that produces the result given the storeid X: 

In the relation , since is a candidate key of the original relation, a minimal cover of the dependencies is (there are no other non-trivial dependencies that hold in it). Then, what happened to the dependencies and ? Well, they are lost in this decomposition (and if you try decomposing starting with dependencies different from you can find that these or other dependencies are lost as well). From the practical point of view, this means that the constraints expressed by the lost dependencies cannot be enforced in the decomposed database. On the other hand, it is a well known fact that the classical decomposition algorithms for BCNF is not guaranteed to preserve the dependencies, differently from the synthesis algorithm for 3NF. 

There are three keys, , and , and the relation is in BCNF. However, if each person can have more than one phone, than the situation is completely different. You have this set of functional dependencies: 

So, at the end, the final decomposition is H11, H12 and H2. Note that this decomposition determines the loss of the dependency , and this happens also if we start by considering first the dependency (obtaining the same decomposition). 

The last two lines are an abbreviated way of solving the problem without recurring to the complete algorithm to check for dependency preservation. In particular the teacher noted that combining the dependencies that you can obtain from R1 and R3 you cannot obtain C, which is essential to get A in the dependency BC → A. This dependency can never be derived by the projection of F over the decomposed relations. Here follows the complete explanation. To check if a decomposition preserves the dependencies, one can use the Ullman algorithm that checks, for each dependency X → Y in F, if Y is contained in the closure of X with respect to the projection of F over the decomposition (formally, calling G the union of the projection of the dependencies over the decomposed relations Ri, that is G = ∪πRi(F), the algorithm checks if X → Y ∈ G+, the closure of G, by checking that Y ⊆ X+G). So, we must calculate X+G for each dependency of F, to check if Y is contained in it. Such a closure is computed with the following algorithm. 

(a part from minor variations on the notation used, like “:” instead of “.”). The idea is that you obtain the result of the query (a well formed formula of a first order logic), by finding the elements of the relations that, substituted to the free variables of the formula, make the formula true. In this case, we have two variables, and , that range on and respectively, a condition that must be true (), and from every pair of tuples of the relations that satisfy this condition, you add to the resulting set a tuple formed by the of and the of (in other words you perform a simple join). Note that the structure of the expression can be easily mapped to the SQL query (and actually this language is the origin of the SQL language, together with other additions). This can be a suggestion on how to write other queries similar to this one. 

The trick to solve this exercise it to repeat as much as possible the elements of the table. Since is the key, you cannot repeat , but you can, for instance, repeat two times and and three times . Here is a very simple and short example (note that the functional dependencies are respected): 

and are exactly the same relation, since the order of the attributes does not matter. In general the synthesis algorithm requires that, after the first phase, you should remove all the relations contained in others. So, the answer to the first question is yes, and of course the relation has two dependencies and , and two candidate keys, and . On the other hand, and are different relations, which should not be merged together (and, moreover, they do not have the same keys, since has key , while has key (and no dependencies)). 

Your decomposition is not correct, since in R2 you still have dependencies that violates the BCNF, for instance ( is not a key of that relation). The problem is that your algorithm is not correct. When you find a dependency that violates the BCNF, you should decompose a relation in two relations, the first with X+, not XA, and the second one with T – X+ + X. Then you should repeat the algorithm, if you find in one of the two decomposed relation some other dependency that violates the BCNF. So, in your example, a correct decomposition is: 

When we decompose for BCNF, we should be prepared to lose some dependency (and, for this reason, in practice the 3NF is used). The analysis algorithm of BCNF requires an exponential task to be performed, since at each step one should check if a decomposed relation is still not in BCNF, and to know this one should know the projection of the dependencies over it (or at least its cover). 

is: both of them are correct, since both of them satisfies the definition of the 3NF. You have simply discovered that the synthesis algorithm for decomposing a relation in 3NF can produce different solutions. A different question is: which is “better”, and of course the solution with a single relation is “better”, since you do not need to join tables when making queries. Of course, if one could check at the beginning if the relation is already in 3NF, than he can avoid to apply the algorithm. But this in general cannot be done, since the check requires the exponential computation of all the keys, to find the prime attributes of the relation. 

If your attribute is unique, you can leave it in your original table even in case it can have null values. In fact the PostgreSQL manual says: 

The decomposition that you have produced is in effect correct, in the sense that the decomposed schemas are in BCNF. However, as you have already noted, it does not preserve the dependencies, in particular the dependency is lost. So you have re-discovered an important point about the decomposition in BCNF: one can always decompose a relation in BCNF, but at the price of sometimes losing one or more dependencies. What does this mean in practice? We can lose (possibly important) constraints. In this case, for instance, the constraint that for each couple of values there is always a single value of cannot be enforced on the resulting schema. Can we do something for this? Well, we could decompose instead in 3NF, because the synthesis algorithm used for 3NF is guaranteed to produce always lossless and functional dependency preserving decompositions. In this case, for instance, it will produce the decomposition and , that maintains all the dependencies. But, wait a moment! we have now a decomposition that does not eliminates all the redundancies eliminated with the BCNF algorithm, since, because of the dependency , we will have the same value of each time we have a certain value for . So, we have now a dilemma: should we prefer the 3NF that preserves the dependencies at the expense of maintaining some redundancy, or should we prefer the BCNF that reduces the redundancies at the expense of losing some “meaning” of the data? The opinion of many is that we should choose the 3NF, since data meaning is considered more important that data redundancy (and not only this, but because the 3NF algorithm is a polynomial algorithm, while the BCNF algorithm is an exponential one).