Jukka's comment reminds me of the following reference. If we look at such a function g as a matrix, the condition is equivalent to one in the definition of a permuted inverse Monge matrix. There is a polynomial-time algorithm to determine if a given matrix is a permuted inverse Monge matrix (and if so, that gives corresponding permutations). The result was found by Deineko and Filonenko (1979) in their Russian paper, and it is explained (in a more general form) in the following survey by Burkard et al. 

Now I describe the definitions of their polygons. Edge-visible polygons Let $e$ be an edge of a simple polygon $P$. Then, $P$ is edge-visible from $e$ if for every point $p$ of $P$ there exists a point $q$ on the relative interior of $e$ such that the relative interior of the line segment connecting $p$ and $q$ completely lies outside of $P$. Externally visible polygons A simple polygon is externally visible if for every point $p$ on the boundary of $P$ there exists a ray (half line) $r$ starting at $p$ that intersects $P$ only at $p$. Monotone polygons, star-shaped polygons and pseudo-triangles are externally visible. 

For 0/1-polytopes (all vertex coordinates are 0 or 1), this is not known to be true. There is a conjecture by Mihail and Vazirani that the edge expansion of the graph of a 0/1-polytope is at least one. More information is described in a paper by Volker Kaibel. I should note two things. (1) For 0/1-polytopes, the Hirsch conjecture is true. (2) When performing a random walk on the vertices of a polytope, we need to take care of possible degeneracy. One vertex can correspond to a lot of bases, and so the walk can stay at the same vertex if we perform a random walk over the bases. If we want to perform a random walk over the vertices, we need to have a procedure that gives a random adjacent vertex. 

Consider the maximum matching problem in bipartite graphs. The family of feasible solutions consists of all matchings, and local search is performed via finding augmenting paths. The problem belongs to PLS since an augmenting path can be found in polynomial time if a current matching is not maximum, and maximality can be checked in polynomial time. Any local optimum is a maximum matching (i.e., global optimum). However, it is #P-hard to compute the number of maximum matchings in a bipartite graph. Since a local optimum can be found in polynomial time, the problem is unlikely to be PLS-complete. So, I'm afraid this is not an intended answer (your question restricts to PLS-complete problems). However, I should point out that counting the number of local optima can be hard even though one local optimum can be found efficiently. 

The class of line graphs can be characterized by a finite family of forbidden induced subgraphs (Beineke). A dominating set in a line graph G corresponds to an edge dominating set of the root graph of G (and vice versa), and the size of minimum edge dominating set can be approximated by factor of 2 in polynomial time. 

Probably, this is not an intended answer. The sparsity constraint can be naturally represented in integer programming. (1) For the binary integer programming (the variables take only values 0 and 1), an extra constraint that "the sum of variables is at most $k$" leads to a solution with at most $k$ non-zeros. (2) If your problem is not binary and each variable takes the value in $\{0,\ldots,M\}$, then for each variable $x_i$ we introduce another 0/1 variable $y_i$, and give an extra constraint as $y_i \leq x_i \leq M y_i$. This means that $y_i=0$ if and only if $x_i=0$, so the sparsity can be described by means of $y$. Namely, we introduce the sparsity constraint $\sum y_i \leq k$ as (1) above. I'm imagining you're to look at an analogue of linear programming (or convex programming) with sparsity constraint to integer programming. However, linear programming with sparsity constraint is no longer linear programming, while integer programming with sparsity constraint is still integer programming (as described above). Their natures are totally different. Or, if you need some "interesting" examples, I would say "any combinatorial optimization problem with cardinality constraint". That's almost equivalent to a binary integer programming with the constraint that the sum of variables is at most a certain number. 

In this question I understand "difficulty" refers not to "difficult to compute," but to "difficult to study." Graph problems are easier (at least for me) to study since some concepts happen to be equivalent. In other words, if you want to generalize questions for graphs to those for hypergraphs, you need to pay attention to the "right" generalization so that the desired consequence can be obtained. For example, consider a tree. For graphs, a graph is a tree if it is connected and contains no cycle. This is equivalent to being connected and having n-1 edges (where n is the number of vertices), and also equivalent to containing no cycle and having n-1 edges. However, for 3-uniform hypergraphs, let's say a 3-uniform hypergraph is a tree if it is connected and contains no cycle. But, this is not equivalent to being connected and having n-1 hyperedges, nor to containing no cycle and having n-1 hyperedges. I've heard a main difficulty to prove the regularity lemma for uniform hypergraphs was to come up with the right definitions of regularity and related concepts. When you want to consider "spectral hypergraph theory," you may try to look into tensors, or into homology if you see a k-uniform hypergraph as a (k-1)-dimensional simplicial complex, from which linear algebra naturally arises. I don't know which is the "right" generalization for your purpose, or it's possible that neither is right. 

Some people in combinatorial optimization use the word "combinatorial algorithm" to mean "algorithm without using ellipsoid method (or interior-point methods)." For example, Schrijver's Fulkerson-prize-winning paper has a title "A combinatorial algorithm minimizing submodular functions in strongly polynomial time." The co-winners (Iwata, Fleischer, and Fujishige) also have "A combinatorial strongly polynomial algorithm for minimizing submodular function." As far as I see the field of combinatorial optimization, people there actually try to give an answer to the posed question, and still struggle. Cunningham's survey "Matching, matroids, and extensions" (Mathematical Programming B91 (2002) 515-542) is a bit old by now, but you can see how generalizations of Edmonds' blossom algorithm and concepts on matroid lead to combinatorial polynomial-time algorithms for other problems. Please note that the field has been advanced since the survey, and good sources are probably some papers from IPCO (Integer Programming and Combinatorial Optimization). I would like to point out that an big open problem is to find a combinatorial algorithm for the maximum independent set problem for perfect graphs running in strongly polynomial time. 

This is not an answer, but rather a personal experience. I'll post it as an answer since it's too long to be a comment. If I would be an author, I wouldn't use the expression like "linear-approximation" since it's confusing. I haven't seen an expression like "linear-approximation" or "logarithmic-approximation" for referring to approximation ratios. For example, the greedy algorithm for the minimum set cover problem is an $H(n)$-approximation algorithm, where $H(n)$ is the $n$-th Harmonic number, but I haven't seen it's called a logarithmic-approximation algorithm. Here, $n$ is the size of a universe, and this should be explained in the definition of the problem. Please note that $n$ is not the input size. There's one exception. In the book "Complexity and Approximation" by Ausiello, Crescenzi, Gambosi, Kann, Marchetti-Spaccamela, and Protasi, I can find expressions like log-APX, poly-APX and exp-APX. For example, log-APX refers to the class of NP optimization problems that have an $O(\log n)$-approximation algorithm, where $n$ is the input size. Some authors use "a linear approximation algorithm" to mean a linear-time approximation algorithm. I feel this is still confusing, and I would write linear-time approximation. People tend to omit "time", and for example when they write "a polynomial algorithm", they often mean a polynomial-time algorithm. 

Graph isomorphism for bipartite graphs is as hard as graph isomorphism for general undirected graphs. Considering bipartite posets (i.e., seeing bipartite graphs as Hasse diagrams of posets), you may see that counting the automorphisms of bipartite posets is as hard as counting the automorphisms of general undirected graphs. 

This is not a solution, but a reference suggestion. The problem should be (a special case of the decision version of) the partially-ordered knapsack problem. For example, you can look at the following paper and the references therein. S.G. Kolliopoulos and G. Steiner: Partially ordered knapsack and applications to scheduling. Discrete Applied Mathematics 155 (2007) 889-897. $URL$ 

There are several output-sensitive algorithms to enumerate all maximal cliques in polynomial time per output. One of the earliest algorithms was developed by Tsukiyama, Ide, Ariyoshi, and Shirakawa (1977). 

The following paper by Hernando, Houle, and Hurtado introduces a few classes of simple polygons in terms of visibility. 

Therefore, unless some complexity-theoretic collapse happens, you cannot get essentially faster algorithm than listing all of them. 

This is not a complete answer, but it's too long to be a comment. I think I found an example for which the bound $\lceil \log_2 N_X \rceil$ is not tight. Consider the following poset. The ground set is $X=\{a_1, a_2, b_1, b_2\}$, and $a_i$ is smaller than $b_j$ for all $i,j\in\{1,2\}$. The other pairs are incomparable. (The Hasse diagram is a $4$-cycle). Let me identify the monotone properties with the upsets of the poset. This poset has seven upsets: $\emptyset$, $\{b_1\}$, $\{b_2\}$, $\{b_1,b_2\}$, $\{a_1,b_1,b_2\}$, $\{a_2,b_1,b_2\}$, $\{a_1,a_2,b_1,b_2\}$, and this poset has seven antichains since the antichains are in one-to-one correspondence with the upsets. So, $\lceil \log_2 N_X \rceil=\lceil \log_2 7 \rceil = 3$ for this poset. Now, by adversary argument I'll show that any strategy needs at least four queries (so needs to query all elements). Let's fix an arbitrary strategy. If the strategy first queries $a_1$, then the adversary answers "$P(a_1)$ doesn't hold." Then, we are left with five possibilities: $\emptyset$, $\{b_1\}$, $\{b_2\}$, $\{b_1,b_2\}$, $\{a_2,b_1,b_2\}$. Thus, to determine which is the case, we need at least $\lceil \log_2 5\rceil = 3$ more queries. In total, we need four queries. The same argument applies if the first query is $a_2$. If the strategy first queries $b_1$, then the adversary answers "$P(b_1)$ holds." Then, we are left with five possibilities: $\{b_1\}$, $\{b_1,b_2\}$, $\{a_1,b_1,b_2\}$, $\{a_2,b_1,b_2\}$, $\{a_1,a_2,b_1,b_2\}$. Therefore, we need at least three more queries as before. In total, we need four queries. The same argument applies when the first query is $b_2$. If we take $k$ parallel copies of this poset, then it has $7^k$ antichains, and thus the proposed bound is $\lceil \log_2 7^k \rceil = 3k$. But, since each of the copies needs four queries, we need at least $4k$ queries. Probably, there is a larger poset with larger gap. But this argument can only improve the coefficient. Here, the problem looks to be a situation where no query partitions the search space evenly. In such a case, the adversary can force the larger half to remain. 

The realizability problem for $d$-dimensional polytopes is a candidate. When $d \le 3$, it's polynomial-time solvable (by Steinitz' theorem), but when $d \ge 4$, this is NP-hard. For further information, please look at "Realization spaces of 4-polytopes are universal" by Richter-Gebert and Ziegler (there is an arxiv version as well), and the book "Lectures on Polytopes" (2nd printing) by Ziegler. 

This is not an answer, but more like a comment. But this is too long to be a comment. There is a related problem: the computation of the Frobenius number in polynomial time, which is known to be NP-hard (Ramirez-Alfonsin, 1996). The Frobenius number is defined as follows. Let $a_1,\ldots,a_n$ be positive integers such that $\text{gcd}(a_1,\ldots,a_n)=1$. Then, it's known that there exists a (unique) number $b$ that cannot be represented as any non-negative integer combination of $a_1,\ldots,a_n$, but every integer larger than $b$ can be represented in such a way. This $b$ is called the Frobenius number of $a_1,\ldots,a_n$. I would expect, if your problem could be solved in polynomial time then we could compute the Frobenius number in polynomial time too. But I have no clue why this should be true for the moment. 

The problem should be solved in polynomial time since it can be reduced to the integral convex quadratic minimum-cost flow problem. The essential part of the objective function is $\sum_{v \in V}m(v)^2$ since $\sum_{v \in V}m(v)$ does not change by a choice of mates. Given a graph $G=(V,E)$ we construct the following network. The vertex set is $V_+ \cup V_- \cup \{s,t\}$, where $V_+ = \{v_+ \mid v \in V\}$ and $V_- = \{v_- \mid v \in V\}$ are copies of $V$, and $s,t \not\in V_+\cup V_-$. The vertex $s$ is a source, and $t$ is a sink. The arcs are constructed as follows. 

Flajolet and Sedgewick published the book "Analytic Combinatorics" $URL$ I don't know much about that topic, but people in the field use tools from complex analysis. So far, their applications seem more on analysis of algorithms, not on computational complexity, as far as I see. 

Bipartite graphs are comparability graphs, and it's known (since Provan and Ball) that counting the number of independent sets in bipartite graphs is #P-complete. J. S. Provan and M. O. Ball. The complexity of Counting Cuts and of Computing the Probability that a Graph is Connected. SIAM J. Comput. 12 (1983) 777-788. If I haven't mistaken, this should answer your question. 

An example from the parameterized complexity is a kernelization for the vertex cover problem using a theorem of Nemhauser and Trotter. In the minimum vertex cover problem, we are given an undirected graph G, and we need to find a vertex cover of G of minimum size. A vertex cover of an undirected graph is a vertex subset that touches all edges. Here is an exact algorithm that uses an approximation at the first phase. Phase 1: Set up the integer linear programming formulation of the minimum vertex cover problem. It's known (or easy to show) that a basic optimal solution of the linear programming relaxation is half-integral (i.e., every coordinate is either 0, 1, or 1/2). Such a basic optimal solution can be found by a usual polynomial-time algorithm for linear programming (or in this special case, we can formulate it as a network flow problem, so we can solve it combinatorially in polynomial time). Having such a basic optimal solution, we round it up to obtain a feasible solution to the original integer linear programming problem. Let S be the corresponding vertex subset. It's good to note that S is a 2-approximation of the given minimum vertex cover instance. Phase 2: Find a minimum vertex cover in the subgraph induced by S (for example by an exhaustive search). A theorem by Nemhauser and Trotter states that this subgraph contains an optimal solution of the original input graph. So, the correctness of this approach follows. You may consult a book by Niedermeier on fixed-parameter algorithms for this algorithm. 

Among other results, he proved that when $G=H=S_n$ and $\Gamma$ is the set of "cyclically adjacent transpositions," the minimum length of such $w$ can be found in polynomial time. 

Now, we inject the $k|V|$ unit of flow to the network from $s$ toward $t$. Then, a feasible integral flow corresponds to a mate assignment, and the total cost is $\sum_{v \in V} m(v)^2$. 

An optimal solution lies on some face. So, we can go through all the faces of the cube, and find all stationary points on each of the faces. Here is a more concrete procedure. A face of the cube can be characterized by two disjoint index sets $I_0$ and $I_1$. For $i \in I_0$, we fix $x_i = 0$, and for $i \in I_1$ we fix $x_i = 1$. Let $\tilde{x}$ consist of the remaining unfixed entries of $x$. This fixing turns the objective function into the following form: $$ \tilde{x}^{\top} \tilde{A} \tilde{x} + \tilde{c}^{\top} \tilde{x} + d, $$ with appropriate $\tilde{A}$ and $\tilde{c}$, and some constant $d$, and we want to find the stationary points of this function under the condition that $0 < \tilde{x} < 1$. To this end, we take the differentiation of the objective function to obtain $$ \frac{1}{2} \tilde{A} \tilde{x} + \tilde{c} = 0. $$ Solving this system of linear equations gives you the stationary points, the candidates for optimal solutions. We go through all of them, check the condition, and choose one with the minimum objective value. The overall time complexity is something like $O(3^n \text{poly}(n))$ since the number of faces of the $n$-cube is $3^n$ and a system of linear equations can be solved in polynomial time. The space complexity is polynomial in $n$. 

The following paper by Mark Jerrum studied the problem you mentioned when $G=H=S_n$ and $G=H=A_n$ (the alternating group): 

(1) Farkas' Lemma should show the equivalence of the infeasibility problem and LP in strongly polynomial time. (2) Solving a linear programming problem (as a computational problem) usually means "detect the problem is either infeasible, unbounded, or having an optimal solution, and if the problem has an optimal solution, find one". In this sense, they are equivalent. 

I don't know which is true, but suppose 1 is true. Let $H(n)$ be the number of Hamiltonian graphs with $n$ vertices. Then $H(n)\geq 2^{\binom{n}{2}}/2$. In this case, listing all the Hamiltonian graphs with $n$ vertices can be easily done in output polynomial time: Look through all the $2^{\binom{n}{2}}$ graphs, check the Hamiltonicity for each of them (in $2^{O(n)}$ time), and output if it is Hamiltonian. The running time is $O(2^{\binom{n}{2}}\times 2^{O(n)}) = O(H(n)^2)$, since $2^{O(n)} = O(H(n))$. If 2 is true, then by the same argument we can list all the non-Hamiltonian graphs with $n$ vertices in output polynomial time. Namely, if the output size is polynomial in $2^{\binom{n}{2}}$, then such a trivial algorithm runs in output polynomial time (assuming that the verification can be done in $\text{poly}(2^{\binom{n}{2}})$ time). So the only interesting cases are when the output sizes are subpolynomial in $2^{\binom{n}{2}}$.