What's more important, reads or writes? If it's reads then ensure the queries you are performing against the database are optimized by building covering indexes where you can - reading the data page is expensive. If it's writes then be careful how many indexes you build because of the shear size of your data, so only build what you have to because rebuilding the index is expensive if there are a lot of transactions. Key constraints. Do not go down the road of not building proper foreign key constraints. When you build a foreign key it will build an index, you do not want to build in the referential integrity yourself, there is no performance bottle-neck with foreign keys. If you are using a database engine where foreign keys are a bottle-neck you should probably consider a different engine. Last but not least, tune your database to your usage. A substantial amount of usage statistics are needed to understand exactly how to tune your database and because of the size of your database it will need tuned regularly. 

This is your best option. It maintains atomicity while at the same time encapsulating and decoupling the data access from your app. Do this. UPDATED: To clarify my point about the Party model. The following are my major beefs with the approach: 

That's a big 'it depends.' Depending on how your statistics have been maintained and the options you specify you could end up running full table/index scans and thrashing your I/O and buffer pool. Depending on the characteristics of your hardware and databases that could be very bad. Also, rebuilding statistics invalidates execution plans, which means you could see a CPU spike and slower performance while SQL Server re-compiles queries. Best practices dictate updating statistics during off-peak hours to minimize impact. Otherwise, take due precautions to minimize load on the system such as rebuilding statistics on only the tables that require it over a period of time. Check books online for more information: $URL$ $URL$ 

I have an app that's local to the SQL Server and thus has a Shared Memory connection to it. I was wondering whether the RAM taken by the connection (including data transfers) counts against the max memory limit set for the SQL Server. The reason I am asking is that the SQL Server is maxed out on memory (e.g. Target Server Memory = Total Server Memory and other metrics). If the RAM taken up by Shared Memory connection counts against it, wouldn't I be better off using TCP connection to the SQL Server? 

With SQL Server 2005, you could look at the Task Manager and, at least, get a cursory look at how much memory is allocated to SQL Server. With SQL Server 2008, the Working Set or Commit Size never really goes above 500 MB, even though the SQLServer:Memory Manager/Total Server Memory (KB) perf counter states 16,732,760. Is there a setting where it will actually show the server memory in the Task Manager? Or is it a result of them changing how memory is used in SQL Server 

How about another answer? Get rid of the Party table entirely. I'm a passionate hater of the Party model as it causes vastly more problems than it solves. However, since it's unlikely you're in the position to do such a thing: 

Denormalization to the 'Vehicle' table is still option, technically, but based on the responses in the comments I would probably sub-class it as well. So, you're at the options I mentioned in my last comment. There's no magic bullet. It's string concatenation for dynamic SQL or separate statements. One way or another you're going to have to change your app code. 

Using mirroring alone won't be sufficient since the mirrored secondary is not available for querying. You have to create and maintain snapshots, which can be annoying. Your options are, in no particular order: 

I am about to split a large database into a bunch of federated instances. Currently, most of the primary keys are auto-generated identity ints. Obviously, that's not going to work in a federated setup. I've read people using auto-generated GUIDs to replace the ints, but that data type has well-known performance sapping problems on its own. What are some of the strategies I can use for having unique values for primary keys across federated members? 

What does Table Scan (HEAP) mean for a partitioned table? Does it indeed use an index, perhaps behind the scenes? Is there anything I need to do to improve efficiency? 

I run on box box and get multiple rows per single SPID. For example, see below. Does this mean that SQL Server has broken the query into 23 parallel sub-queries? If that's the case, why is it ignoring MaxDegreeOfParallelism setting of 8? Or is this something else? 

It's difficult to make a clear recommendation without knowing a lot more information about your environment. I've used most of these methods to varying degrees of success. Note that most places end up building out more robust data marts / data warehouses for reporting and analytics, so you'll probably end up with the ETL route one day. Oh, and make sure you have licensed the secondary server ;) 

There's nothing per-se wrong with this as long as you do your transaction control in your app. It's not what I would suggest though. 

This is nasty from a model perspective. If you go this route you have attributes that apply only to organizations intermingled with attributes that are only for users. It becomes quite a mess and you just 'have to know' what is right and wrong. It is a significant impediment to clarity. 

I've inherited a very volatile table which is a map of who holds what resource in the system. At any given moment, there could be a dozen inserts/deletes/reads going against that table. However, there are never any more than 30-40 rows in the system. The system was written in the SQL 2000 era and the access to the table is serialized via sp_getapplock/sp_releaseapplock system sprocs, so that only 1 request is modifying the table. In addition, the INSERT & DELETE statements execute . Reading the notes from a decade ago, it states that without these restrictions, the system would experience non-stop deadlocks. I've ported the database to SQL Server 2016 Enterprise Edition. Now that the throughput of the system has increased 10 fold, this table is easily the biggest bottleneck. What are my options for a table as volatile as this with SQL 2016? I am looking for fast (hopefully concurrent) access and no deadlocks.