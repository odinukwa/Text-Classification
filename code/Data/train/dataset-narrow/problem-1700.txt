We send Out of Office replies to all emails, but the sender gets stored in a list of already-sent addresses. This ensures that each sender receives a maximum of one reply (per day, but is configurable in lots of software packages). We have a company policy that the content of the vacation message includes the email address and phone number of the person taking your responsibilities during the vacation. This makes sure that clients, vendors, etc are not left hanging while you're tanning on the beach. 

The tiniest windows tool to allow automated uploading is probably FTP. Here's a guide that I found: $URL$ Essentially, you create a text file containing the things you would type by hand: 

You might look at just using apache authentication, and authenticating THAT with your centralized authentication scheme: $URL$ 

If hard links aren't preserved, I imagine you would use excess disk space, because a hard link is a link to an inode, so essentially, you would be copying the data rather than linking to it. Note that this does have effects on restoring from the backup. If your backup solution unlinks the file before restoring it, then you can wind up with multiple copies of the file: Suppose File A and File B are hard links to the same file. You recover File A from backup, so the backup software removes File A then restores from backup. You now have File A matching the backed up data and File B living on disk with the data you probably wanted overwritten. 

In plain english, it sets the date and the backup directory, removes any existing backup for today then creates the new backup in a gzip'd tarball, with the date specified in the name. Being added to this script soon will be an upload method to Amazon's S3 cloud. Don't want to be in a total loss situation like Jeff and Joel were. 

I would like to think that I am the exception, but my research has shown that there's a decent percentage (~10%) of administrators who are the sole admin in a 24x7x365 computing environment who have >50 servers. (See my research and the results of my 334-admin survey here: $URL$ Essentially, your job as an admin is either to support a 24 hour operation or it isn't. You're hired to be on-call at times, then it's acceptable. If you weren't hired to be on-call and your employer wants to make you on-call, then that needs to be a two way street. You need reimbursed in one way or another for your extra investment in the company. Whether you take that as monetary or temporal is between you and your employer. If they don't want to reimburse you more and you disagree, chances are good that it is an at-will employment. Ultimately, you must be the final judge of "fair", but reaching a mutually beneficial agreement should definitely be possible. 

What kind of RAID device do you have? Is it a hardware or software RAID (and if so, what OS?) As MarkM mentioned, there's an excellent chance that the degradation is caused by a failed drive. If you've got a hardware raid, you can probably see the bad drive because there will be a different colored LED. If it's software RAID, then you know the port number (port 1, I guess?), but you can cat /proc/mdstat and learn more details about the status of the array. 

Technically, unless your cable provider is blocking access to the ports that you want to serve (probably 80 and 443 for http/https respectively) then there's nothing stopping you from hosting your own . Seth said as much. As pretty much everyone else said, it's a bad idea. So what would it take to make it a good idea? Well, first, know that you're never going to get as fault tolerant as the expensive colocation facilities. That's why they're expensive. They have things like redundant power sources, redundant generators, redundant network connection, redundant everything. You can't afford this. Instead, a close approximation might be... A home with a section of the basement dedicated to the servers. Get a cheap four post rack, and buy rack mount servers. You can buy 2nd hand or from cheaper places like Supermicro, etc Get a decent quality UPS, and make sure it's enough to power the servers and network equipment for 15 minutes or so. Get a leased line for bandwidth. This is the expensive part. A cable modem that gets you 10Mb/s down and 2Mb/s up might cost $50 a month. A T1 that gives you 1.5Mb/s down and 1.5Mb/s up will probably run you $700 a month, if you can find a provider who will install it in your house. Depending on the area, you might be able to find metro ethernet for $1500 that gives you 10Mb/s both directions. Buy environmental sensors to track the temperature and humidity of the area around your servers. Homes aren't meant for this sort of thing, so you have things like waterlines above servers that can break and dryer vents which cause moisture and heat. Sensors will help you maintain a good working environment. Get insurance. The kind of insurance will depend on if it's a business. Here's a hint: create a business. 

womble's answer would be fine, if you configured dhclient to register the domain name (and your dns server allows dynamic DNS updates). Another easy way would be to register the MAC address for the sheevaplug in the DHCP server so that the address, while being assigned by DHCP, is always the same. You would do this on the router, or whatever is acting as a DHCP server on your network. Once you do that, register the IP in DNS and go to town. 

I just had my minion new admin setup OCS inventory ( $URL$ ) with clients installed and running on user desktops. Server clients have been compiled and run once, though not added to cron (yet). Because the OCS interface is rubbish and looks like a 4 year old drew it with crayons (no offense to any OCS people who read this - the actual software is great), we're using GLPI for the actual interface ( $URL$ ). It runs smoothly on a LAMP stack and we've had no problems with it yet. 

or 111 It should be noted that in the command listed above, there's an echo to prevent you from shooting yourself in the foot. Feel free to take of the safety once you'd nailed the files correctly. EDIT OK, this blows away all shell scripts, too. There is no nice, neat way to do it with find that I've found, due to the relatively primitive regular expression matching. I can find all of the .sh files you want using regular expressions: 

In terms of flexibility, you can't beat the snmp plugin. It's behind nearly every check I run, and if that isn't, the TCP connect is. 

Be careful doing this. Every time a process spawns a child process, that child process inherits the parent process's nice level. This means that if firefox (in my case) goes berserk, the system will probably go with it. That's why the maximum priority a user can change to is 0. Be careful. 

Have you looked at mod_gunzip? I'm too new to link to it, but a google search should point you in the right direction. 

It's not really so shocking as much as self obvious at this point. You're doing tons of stuff for absolutely no benefit. So what should you do? Ask yourself some questions. Why do you want RAID-6? Is it because it's more fault tolerant than RAID-5? If so, you should understand which failure modes RAID-6 protects you against: It has two parities. This comes into play whenever you suffer a drive failure, then replace the drive, then have another drive fail during rebuild. This commonly happens when you get a bad batch of drives, or when you've had drives continually exposed to high heat (probably higher than you think). In addition, there's the possibility of a Unrecoverable Read Error (URE) about once every 10^14 bits read (that's a bit over 11TB of data). If it happens during a RAID rebuild, well, that's not good, and RAID-6 can protect that. Tell us more about the array you want to build. Why are you building it, what does the capacity need to be, and what kind of performance do you need from it? With that information, we can give you recommendations.