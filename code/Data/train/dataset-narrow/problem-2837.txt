You probably want the ImageMagick Convert Command-Line Tool: It is very easy to manipulate images, for example to resize: 

As you can see, if there is no external force, then the velocity will not change, but it is still added to position every frame, thus fulfilling first law. In space, there is no friction slowing the ship down so it would continue along its path forever (unless you want to go through a nebula or something). This update function uses Euler integration, which is the simplest, but also the most inaccurate. Presumably this will be no problem, but if you need more accuracy, you can either decrease the timestep and do more update iterations per frame, or else look into a better integrator, such as Runge-Kutta. You will also need analogous data and calculations to deal with rotational stuff, but I leave that as an exercise for the reader. It's a bit more complicated however. For example, the rotational equivalent for mass is a matrix rather than a scalar. :-/ 

In some ways, rasterisation is the opposite problem to raytracing. In raytracing, you know which pixel you are testing, and you have to find the triangles that are hit by the ray through it. In rasterisation, you have information about a triangle and you need to find which pixels it covers. Basically, the vertices describing the triangle are projected into screen space through a series of matrix transformations and then interpolation is used to find all the pixels inside the 2D triangle. These are then tested by the depth buffer to see if they are closer than anything else that was written, and if so, then the colour at that pixel can be determined, with lighting, texturing and anything else you want to do. Here is a good discussion and code regarding software rasterisation. If you are searching for more information, here are a few terms of interest: "scan conversion" "half spaces" and "perspective correct" 

This is your direction vector. It starts at the origin but is in the direction the goal is to the player. Finally, the length of this vector is the distance between player and goal. Often, you need a normalised vector wich has a length of 1 and is a pure direction. To do this you just divide all components by length. I don't know the library you are using in question, but they will have a 'normalize' or 'unit' method to do that. 

The main problem you are encountering is due to the fact that you are using Euler integration. It is very innacurate and assumes a constant acceleration over the whole timestep (in your case an implicit dt = 1), which is wrong in general, but in the case of a bounce, very very wrong indeed. Also, there is no damping. In the real world, energy is always lost due to friction and collisions. In fact, your code is actually giving free energy to the ball: at the moment of impact, you teleport the ball upwards a short distance, while giving it an impossibly perfect eleastic collision, thereby giving the ball free potential energy. The only reason your ball's bouncing becomes smaller is because of the innacurate Euler integration, not something you want to rely on. To make your code better, I would first introduce an explicit timestep, then you can call your integration function multiple times per frame with smaller timesteps to get better accuracy, or have an adaptive timestep. You could also look into higher order integration functions. 4th Order Runge-Kutta is very accurate, but overkill for what you want, but perhaps a second order leapfrog method. You could also look into Verlet integration. While it is still not very accurate, it is easy to control and very stable. You should also add some damping. The easiest way is to multiply the velocity by a number slightly smaller than 1 (which is equivalent to adding a small force proportional to velocity, in the opposite direction): 

There is absolutely no way of making it perfect because you simply don't have enough information about depth. You have 2D representations of 3D objects. Think about this. If it were a real object, there would be parts of it that the camera can't see, but still block the light and cast a shadow. Also, think about each pixel, there is no way to know how high off the ground the surface at that pixel is. I mean, WE can see that steps are leading up to a treehouse, but an algorithm does not have that kind of semantic information, it just sees a 2D sheet of colours and alphas. I think what you have is the best approach you can hope for. Although, like ashes999 suggested, you can use photoshop to tweak your projection/angle/pivot. Also, you could accumulate some jittered samples to get an antialiased shadow which would look nicer. As for the angle being dependent on proportions, I am not sure what kind of linear algebra system you are using, but you could experiment with making the shear inversely proportional to sprite proportions to counteract the effect. I'll leave it up to you to experiment. 

Another point to mention, is that sometimes complex objects have much more than just geometry. They could also have spatial partitioning data such as kd-trees or bsp-trees for accelerating intersection tests. Imagine a high polygon spaceship fying through space. The spatial partitioning data is constant in the local frame, but not in world space. Transforming rays to object space means that these extra data do not need to be precomputed every frame. 

By anyway, from the sound of it that's not what you need. In your case, you just need a vector (V) to point from your player position (P) towards a goal point (G). All you need to do is construct V every frame. 

I have implemented a lense flare for my game, and it looks great. The first draft used individual sprites each with its own draw call. This was very slow, so I reengineered it to use a single draw call, expecting it to be much faster. It's not. I realise now that it is the call to glReadPixels that is the culprit. I have read that the function is slow, especially for reading the depth buffer, but I am reading only a single pixel. And my drawing goes from about 2ms to 15ms (although I'm not sure I trust my timings). I know that glReadPixels stalls, because it has to wait for all previous operations to finish. Another point is that I would like to port my game to mobile devices at some point, and it's even worse there, perhaps impossible using the depth reading technique. So my question is, is there a way to speed it up. For example, can I refactor things so that I only do the test at the very end, or perhaps use last frame's data. This is tricky because I have a splitscreen game and use offscreen buffers. Or should I just bite the bullet and implement raytracing. I can do this and my geomentry is simple and static, so fairly easy to use a kdtree or something, but I'd rather not if I don't need to. Thanks 

be sure to cull things that are over the horizon, otherwise the whole world will wrap around. Disclaimer: I haven't tested this and I am no mathematics expert, but the answer is something like this. Someone please correct me if I am wrong. 

It depends on what kind of game you are making. For example, a smallish 2D game might require a different approache than a large 3D one. My initial suggestion would be a kind of fluid simulation. A grid with values for density, pressure and velocity for each gas. A very good resource for this is a stable fluid solver from Jos Stam. It even has code. The fluid solver can be 2D or 3D, but I would suggest solving a 2D system even if your game is 3D (if possible). A 'cellular automata' is another approach, similar to a fluid system but can be a quite a bit simpler. Perhaps look into "lattice gas" If your world is particularly large, you might consider and even more simplified system, such as running the fluid simulation on a very simple waypoint like graph. Although again, this highly depends on your game. 

I am not sure this is the actual answer to your problem (although it might be related). I noticed that you are generating UV of the hit in the triangle by the (normalised) weighted sum of distances from the vertices. This is wrong. Think what you get if your ray exactly hits v1: d1=0 therefore w1=0 and you get a divide by zero. What you should do is get the distance to the opposite edge. So d1 = distance to edge created between v2 and v3. Normalise the result: d1,d2,d3 /= d1+d2+d3 Then UV(hit) = UV(1)*d1 + UV(2)*d2 + UV(3)*d3 where d1=1, d2=0, d3=0 which gives exactl y the right answer. This is called barycentric coordinates. It may not solve your immediate bug, but it will fix your uv generation. 

I would say you should feel free to hard code stuff on only the smallest details. If you have even the slightest notion of the element you are working on being used by someone else, growing much larger, or being used in another project, then take the time to do it right, now. On the other hand, I prototype like a madman and hard-coding certain things is quick and easy and leaves you free to concentrate on experimenting. It kind of depends on your style of working. 

You will have to insert your own 2D vector class (or one from a library). If you don't understand vectors, just find a good tutorial and learn the very basics, it will only take 5 minutes. Hope it helps. 

I am trying to play a movie file into an OpenGL texture in a Java application. I am using JOGL and have a basic OpenGL scene, but I have no idea how to play a movie into a texture. The only thing I could find was this: $URL$ It is quite old and uses JOGL 1.1 and Fobs4JMF, which is no longer maintained. I managed to get it to build in eclipse but it wasn't able to read my movie clip. I could probably convert the movie clip to an older codec to see if that works, but I would rather have a modern solution. I am a professional game developer, so the OpenGL part is no problem, but I am new to Java (coming from a C++ background). Is there any modern library that wraps this functionality in an easy to use package? 

Okay, for anyone interested in this topic I will now detail the solution I have chosen. Thanks to every one who replied and gave me ideas. First, for the 'best' tesselation, I will choose the truncated icosahedron as a starting point. Subdividing it leads to a very nice tesselation of hexagons with 12 pentagons providing the curvature. Also, continuing the subdivision on its dual will give me a very good triangular mesh for rendering with nice properties. Regarding the 12 pentagonal cells: I can ignore them, make them special (like the only places bases can be constructed), or I can hide them under scenery. The hexagonal and pentagonal cells will be stored in a half edge data structure for easy access to neighbours and fast traversal. The only tricky part is finding which cell a given world point is in, but that can be done by starting at a random cell and walking towards the point through neighbours. I hope someone finds this information useful. I have learned a lot and am looking forward to getting some results. Edit: Here is an image showing the result of my icosahedron subdivision and dual mesh switching using half-edge data structure. I may do a few iterations of relaxation to get the cell areas even more uniform.