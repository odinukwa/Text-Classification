I don't think there will be a noticeable difference. Query optimization has very little to do with the syntax of your query and a lot to do with the RDBMS query optimizer. The optimizer pulls apart your queries and optimizes it as it sees fit. 

This is likely because your User has status on your system. Users without administrator access should not be able to access these files. If you enable Guest User, you can login and try same thing with Guest. You should not get access to the files as guest or normal user without administrator rights. 

Secondary indexes (non-primary keys) in MongoDB and MySQL are very similar. Secondary indexes declare fields or columns to be sorted separate from the rest of the data, and use row identifiers to reference the rest of the row for a query. 

MySQL has "General Query Log". This logs everything that is going on MySQL server: users connecting, disconnecting, queries etc. This query log is a file on your filesystem or (from 5.1.6 versions) table Control the general query log at server startup as follows: 

MySQL is not cutting it at 233. The problem is likely in your save method which cuts it to 233 before the data even reaches MySQL. Also, don't forget that 233 limit is not character limit, and as some character might need more han 1 byte to be stored, you might see less than 233 characters. Also please make sure that data in MySQL server is really stored as latin1, this can be accomplished with: 

You can try creating a new table according to old one's structure. Not sure if this will realy help as you say that your original table is corrupted: 

Every process under linux runs under specific user privileges. Services (like MySQL) usually need to open ports and access various system resources during startup, so they are required to be started as user. However, it is not safe to have all the processes run under as it is not required for continuous operation of services, thus it is recommended to create a special user, which will be used to run MySQL service. MySQL will only be able to access what special user can, and this is going to be limited to MySQL files on the system. This is usual practice in linux. If you, however, use you distributions built-in package manager to install MySQL, this will be done for you automatically (in most distributions at least). 

Yes this is normal. When RAM is no longer needed it is not freed at the same time. It is kept as cached in case the server would decide that it needs to access it again. This would save you extra time that you would otherwise need for data to appear in RAM. Cached memory is freed only when new applications request more RAM. 

Otherwise you will have to drop the table (with the command above) and recreate it manually or from a backup. 

Which way to go depends on your data, the way you plan to perform queries etc. and is too broad question to answer. You will likely have to apply different method for different tables. 

After starting query log, investigate the file (or table) for further information. MySQL Query Log Documentation: $URL$ 

What I would do, is compare both files with tool. can come in help here as it has ability to diff word by word and if you pass it option don't even have to place them in git repository. This command can help 

Also, it is possible to set timestamp per session. is used to always get timestamp in UTC, no matter what MySQL server's timestamp is configured to. 

I would use for this. - $URL$ - The script imports only a small part of the huge dump and restarts itself. The next session starts where the last was stopped. 

MySQL queries are not case-sensitive by default. It is possible that you have created case sensitive tables when importing data. Check if you have collation, that makes it case-sensitive. Reimport your data then using . Also, if you have collation, it will make queries case sensitive. Your collations changed when re-importing data. 

For inserts, you can use . This lets you update certain fields if primary key is already used. The syntax would be something like: 

If you use , MySQL will use timezone which is configured in . If no value is set in , then MySQL will determine your operating systems' timezone and use that upon startup. You can see how your server is configured with this query: 

Hi I am trying to create a SQL Server Failover cluster. Windows Cluster is configured and working as expected but now when I try to install SQL Server it throws the following error: 

I have a pretty complicated situation at hand, let me try to explain. I have a SQL Server and a Web Server in a separate domain (lets call it domain A). SQL Server has the databases for Reporting services, The web server has reporting services installed. The SSRS also have SSL certificates installed and we are using https protocol to connect to the reports manager. Now I need to give access to users from domain B to connect to the Reports Manager (report server on Domain A). The users from Domain B cannot have any logins in the Domain A where the reporting services are installed, I know to access the reports manager I need to add domain logins/groups to the reports manager and assign them appropriate roles to access the reports. What options do I have (if any) to give access to users from Domain B to connect to a Reports Manager on Domain A? Important Note: The access to reports server is via NLB with external facing IP, the reports server (web server) or the SQL server does not have any external facing IP 

I have sql server 2008 R2 64bit Developer's Edition Installed on my machine. And Microsoft Office 2010 Professional 32bit. I have been trying to import some Excel data from an Excel sheet into a sql server database. I have used the following query to do this: Query 

I have been working on a SSIS package. Package is extracting data from Access Databases , excel sheets and SQL server databases and loading into SQL Server database. Its been working fine but all of a sudden it has started throwing an error, Its not a run-time error, but whenever I open the solution in Visual Studio, the takes a couple of minutes to validate all the package components (which is expected because the package is huge) at the end of the validation it throws the following error: 

Checked the Processor and Memory usage on the sql server (nothing is maxed out) plenty of free resources. Index fragmentation was minimum yet rebuilt the indexes and updated statistics. No changes has been made to the code in application (application code/sql server code) hence poor performance because of the poorly written code is unlikely to be the cause of overall poor performance of the application. Finally got Paul Randalâ€™s script . The result of the script shows that sql server has to wait a lot when writing to log file. The biggest wait type is WRITELOG. 

Since you have made sure that you have SQL Server installed on your machine all is left is to make sure that you follow the following steps to create a database. 1) Go to right click and Select 

which I think indicates that it was standard edition that was installed originally, yet after the installation was finished and the sql server service started for the first time the windows log show the following: 

Current Environment I am in a bit of trouble here, I have a Reporting Services installed on one server and SQL Server database engine installed on another server. The users are connecting to Reports Manager which obviously points to the server where Reporting Services are installed. User use a generic login (a local admin account on the Report Server) to connect to the reports manager. But when the users click on the Report Builder button on the Reports Manager, It initially failed to launch complaining `"Application cannot retrieve the files. Authentication failed." After a lot of research I found out that, even though users entered the credentials of a Local account when connecting to the Reports Manager, but when they clicked on the Report Builder button, the credentials from their windows account were picked up hence the ClickOnce application failed to Authenticate the user and failed to Launch. I changed this behavior by allowing on Reporting Server and Allowing on Reports Builder. Now when the users click on the instead of report builder just picking the credentials from the current user windows account it prompts for the credentials , the user passes the credentials of the Local Admin account on the reporting server and the ClickOnce Application is downloaded as expected. Problem Now the report builder is launched fine and I can right click the The datasources folder in the left pane of the designer, browse for the available data sources , test connection (shows tested successfully) . But as soon as I click on the DataSet and try to add a new dataset it throws an error saying "Authentication failed, cannot connect to the datasources." Even though in the last step the connection test was successful but at this stage it is failing. 

You mention that you already have a piece of sql that does what you want. So i would suggest you modify that to become an update statement and implement it as a sql task. Another option is to save the update within a stored proc and call/execute it from ssis. With a bit of thought you could probably write an update statement that only sets the values for new records or for records that have changed. Or perhaps do it as part of your ETL insert/merge. However going back to my other comments: i don't think it makes sense to have a single key or dimension for such diverse attributes. I suggest you have a look at some sample schemas to see if it could be done differently. 

If you are comfortable with excel you could import the data directly with an odbc connection. If you wanted olap capability you can do that with a tabular model within excel. In general most BI solutions will work with any relational database via an OLE or ODBC connection. If the product doesnt have a native adapter you could use a 3rd party ETL tool to move the raw data to a db that is supported. 

Im not sure if this answers all of your questions but this sounds very similar to what I do with our data warehouse. However the way i interpret your post you would want to have 1 table with your data, and another with your table definition meta data. For the data table add a column to flag is_current. (Either an int or bit datatype) Also have 2 date columns valid_dt_from & valid_dt_to. If a row changes you update the old row is_current = 0, and valid_dt_to =getdate (). Insert the new row with is_current =1 and valid_dt_from = getdate (). You can do the above with a single merge query. To get the current data select * from table where is_current = 1. Add new columns as they are needed. But don't delete old columns. In theory if you have to reconstruct the table to previous point in time all rows from that time should have null values at that time. Create a similar table to define your data table definition. Add a record to show when each field gets added, removed and if it is current. Your select query could be built dynamically. Where is current =1, or reconstructed as at a point in time. 

The scheduled cache refresh at 08:00:19 & 08:10:19 Followed by my report request about 8:15:01 I don't get the report back until about 8:17:06 

For simplicity if this was for a limited time frame, i would go with backup and restores. Unless you are required to respond within minutes you could probably get by with daily backups. You could even set up a job to take a backup and FTP it on demand, after they have reported a defect. I wouldn't even bother restoring daily unless the issue was specific to newly created data. In my experience UAT is more about functionality. So you test with existing data and create new data. Having a db a few days behind theirs means you should be able repeat the test and reproduce the defect. This approach would get slower, more difficult and less feasible the bigger the db is. If you will be providing ongoing support, the db is huge, and you have SLA saying you have to respond within minutes, replication/synchronization of some sort might be worth the effort to set up. The best solution for you may be dictated by client security or technical policies. Work with them to ensure what you are proposing is acceptable. If not keep in mind there are lots of variations on this theme. But it all boils down to: 1. you need something to take a back up, and 2. you need something to FTP the files either on a schedule or on demand. If I was doing this personally I would begin with Ola Hallengren's backup scripts. After installing that I'd create a SQL Agent job which invokes the backup. I would then edit the agent job to add a step to FTP the files. I just had a quick search and found this example of how to use SSIS to FTP a file and I found this example of an FTP script for SQL Server. My recommendation is that you set this job up to run on a schedule, and only run it on demand if truly required. If you don't have remote access and permission to execute tasks, you can request someone on the client site execute the task for you.