In general, it would mean that the instance that was servicing the request failed and is no longer running. In a RAC cluster, you run one instance on each node of the cluster. I wouldn't make sense to move an instance from one node to another in the cluster-- that would imply that one node would be running two instances of the same database. Depending on your environment, the DBA might choose to add a new node (and a new instance) to the RAC cluster at least temporarily in response to the failure of one of the instances. 

You cannot create a foreign key that references an Oracle data dictionary table. So assuming the "users on that database" are Oracle database users that exist in , you can't reference in your constraint. You can, however, create your own copy of the table and create a job (using or or some external scheduler) that will periodically copy the data from to your copy of the table. You can then create a primary key constraint on the column in your copy and create a foreign key constraint that references your copy of the table. You may need to work with the DBA team to ensure that your copy of the table gets refreshed as part of any process that creates new database users depending on how soon after a user is created you would expect rows to be created in the child table. 

In this loop, every row will be read from . And every row will be rejected (assuming is something other than 1) because of the predicate. So you'll incur the cost of reading every row from the table, evaluating the for every row, and you'll never return any data. Your first query appears to be working because it is filtering out rows in the subquery. Of course, since there is no , your first query always returns an arbitrary row. It would be entirely legal for Oracle to return the same row for every value of in your first query. It probably won't, of course, but it is entirely possible that you'd get different rows over time, that some rows would never be returned, that other rows would be returned multiple times, etc. If you want a deterministic result (prior to 12.1 which has some simpler syntax options), you'd need to do two levels of nesting 

A materialized view in Oracle is a combination of a structure to hold the data (a table), a job that refreshes the data (a job), and a process that figures out how to refresh the data based on the specified query. This process would generally involve the creation and maintenance of materialized view logs on the base table to track changes so that the materialized view can be refreshed incrementally though that is technically optional. It is also common to organize multiple materialized views into refresh groups so that the materialized views themselves are transactionally consistent with each other. The Materialized View Concepts and Architecture chapter of the Advanced Replication manual is a good place to start reading up about this sort of thing. 

Why are you doing this in PL/SQL in the first place? The most efficient way to do anything in an Oracle database should be to do it in SQL 

I'm not completely sure that I understand what you mean by "every should be unique for that value of ". If you are trying to say that each has to be unique within each , then it sounds like you just want a composite index on (, ). 

If you do a full import, the import will attempt to create all the tablespaces and data files exactly as they existed in the source system. If you want exactly the same data files on exactly the same locations, you're all set. If you want the data files to be in different locations, you would need to create the tablespaces before doing the import. If you do a full import, the import will create all the users. 

If you actually have different databases (and it sounds like you do), you can't grant permissions for users in one database to access data in another database. You'd need to create a database link in the EmployeeDB database that connects to the ContractorDB database. That database link can either use a fixed username and password in the ContractorDB database (i.e. create an EmployeeUserRemote user in ContractorDB with a password that doesn't change that can be hard-coded into the database link definition) or that database link can be a database link in which case the username and password would need to match in the two databases for every user that wants to use the database link. Queries that use the database link log in to the remote database (ContractorDB) as the specified user and have the privileges of that user in the remote database. 

Query instead (or in the event that you've changed session-level parameters in your current session and want to ignore those changes). is just SQL*Plus syntactic sugar on top of those data dictionary views. 

The other option would be to use explicit scoping prefixes when referring to local variables (i.e. and ) rather than altering the names of your local variables. 

If the is a role, you would then need to look at to see what users (or roles) have been granted that role and follow the chain if you have roles granted to other roles. If you need to account for users that have grants because of the (very dangerous) grants (i.e. ), that would require a separate query. If you want to get more sophisticated than simply doing a straight query against , though, you are probably better off using on of Pete Finnigan's scripts like the (or ). Pete is probably the leading expert on Oracle security so these are much more likely to account for every possible corner case than anything I would attempt to cobble together. 

When you issue an , Oracle merely marks the session as killed and does the actual work of killing the session asynchronously. That may take just a couple seconds, it may take many hours if the session has an uncommitted transaction that did a lot of work that now has to be rolled back or if the session needs to stay around in a killed state in order to notify the client that their session was terminated. It may, therefore, require a substantial amount of waiting before a user could be dropped. You could do something like this where your loop will wait indefinitely for all the sessions to disappear. 

tells the optimizer to assume that the collection aliased to has only 4 elements which should cause it to pick a more appropriate query plan. 

It's realistically a bug in whatever version of 10g you're using that would appear to be fixed in whatever version of 11g you're seeing an error on both statements. I don't have a 10g database in front of me to test this. I would wager, through, that you'd get an error in both cases if you used the old-style join syntax rather than the SQL 99 syntax. That would imply that the problem is that Oracle wasn't translating from SQL 99 syntax to old-style syntax correctly and was missing the fact that the column was ambiguously defined. 

It is certainly possible to delete data from a random access file. Realistically, though, virtually any database will do a soft delete and mark data as deleted rather than physically deleting data. At some point, something else will then reuse the space that the deleted row had been using (what operations reuse the space will depend on the database among other things). And different databases may provide tools to automatically or manually reclaim that space. Practically, this is basically the same thing that the operating system does when you delete a file. The operating system removes the file system's pointer to the file just like the database removes the reference to the row from the index. The actual data for the file still exists on the drive. At some point, some other file system operation will eventually reuse that space. Some operating systems provide a "defrag" utility that will go through and make all the free space contiguous and move the allocated data to the "beginning" of the drive. 

will specify that the refresh happens every day at 2 AM. The expression is evaluated at the conclusion of each refresh so you just need to ensure that the expression evaluates to whatever time you want at that particular instant in time. 

A fast refresh would copy incremental changes over the network but requires that a materialized view log be created on the master site on the source table. That adds some overhead to the inserts happening on the master table but would generally make the refresh more efficient. A complete refresh would copy every row over the network every time the materialized view is refreshed. That is likely to be less efficient from a refresh perspective but there will be no overhead to inserts on the source table and the master site does not need to create a materialized view log. Oracle provides a host of data replication technologies-- materialized views are the oldest and probably the least efficient but are relatively trivial to set up. Streams is a newer technology that has much lower overhead but is quite a bit more complex to set up. Golden Gate is the preferred replication technology today but that has extra licensing costs.