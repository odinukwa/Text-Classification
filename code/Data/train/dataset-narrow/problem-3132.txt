Also check built-in data sets in the open source software Parallel Sets, which is focused on the categorical data visualization: $URL$ 

For this answer, I have assumed that you prefer open source solutions to big data visualization. This assumption is based on budgetary details from your question. However, there is one exclusion to this - below I will add a reference to one commercial product, which I believe might be beneficial in your case (provided that you could afford that). I also assume that browser-based solutions are acceptable (I would even prefer them, unless you have specific contradictory requirements). Naturally, the first candidate as a solution to your problem I would consider D3.js JavaScript library: $URL$ However, despite flexibility and other benefits, I think that this solution is too low-level. Therefore, I would recommend you to take a look at the following open source projects for big data visualization, which are powerful and flexible enough, but operate at a higher level of abstraction (some of them are based on D3.js foundation and sometimes are referred to as D3.js visualization stack). 

I also suggest software ($URL$ which seems to be quite powerful. Some additional information on using with large networks can be found here and, more generally, here. ($URL$ is an alternative to , being an another popular platform for complex network analysis and visualization. If you'd like to work with networks programmatically (including visualization) in R, Python or C/C++, you can check collection of libraries. Speaking of R, you may find interesting the following blog posts: on using R with Cytoscape ($URL$ and on using R with Gephi ($URL$ For extensive lists of network analysis and visualization software, including some comparison and reviews, you might want to check the following pages: 1) $URL$ 2) $URL$ 3) $URL$ 

The following general answer is my uneducated guess, so take it with grain of salt. Hopefully, it makes sense. I think that the best way to describe or analyze experiments (as any other systems, in general) is to build their statistical (multivariate) models and evaluate them. Depending on whether environments for your set of experiments are represented by the same model or different, I see the following approaches: 1) Single model approach. Define experiments' statistical model for all environments (dependent and independent variables, data types, assumptions, constraints). Analyze it (most likely, using regression analysis). Compare results across variables, which determine (influence) different environments. 2) Multiple models approach. The same steps as previous case, but compare results across models, corresponding to different environments. 

Since you have looked at Kolmogorov-Smirnov and Shannon entropy measures, I would like to suggest some other hopefully relevant options. First of all, you could take a look at the so-called approximate entropy $ApEn$. Other potential statistics include block entropy, T-complexity (T-entropy) as well as Tsallis entropy: $URL$ In addition to the above-mentioned potential measures, I would like to suggest to have a look at available statistics in Bayesian inference-based model of stochastic volatility in time series, implemented in package : $URL$ (see detailed vignette). Such statistics of uncertainty include overall level of volatility $\mu$, persistence $\phi$ and volatility of volatility $\sigma$: $URL$ A comprehensive example of using stochastic volatility model approach and package can be found in the excellent blog post "Exactly how volatile is bitcoin?" by Matt Simpson. 

The following great article by Sebastian Raschka on Bayesian approach to text classification should be very helpful for your task. I also highly recommend his excellent blog on data science topics, as an additional general reference: $URL$ You may also check this educational report on text classification: $URL$ It might provide you with some additional ideas. 

Check the Stanford NLP Group's open source software ($URL$ in particular, Stanford Classifier ($URL$ The software is written in , which will likely delight you, but also has bindings for some other languages. Note, the licensing - if you plan to use their code in commercial products, you have to acquire commercial license. Another interesting set of open source libraries, IMHO suitable for this task and much more, is parallel framework for machine learning GraphLab ($URL$ which includes clustering library, implementing various clustering algorithms ($URL$ It is especially suitable for very large volume of data (like you have), as it implements model and, thus, supports multicore and multiprocessor parallel processing. You most likely are aware of the following, but I will mention it just in case. Natural Language Toolkit (NLTK) for ($URL$ contains modules for clustering/classifying/categorizing text. Check the relevant chapter in the : $URL$ UPDATE: Speaking of algorithms, it seems that you've tried most of the ones from , such as illustrated in this topic extraction example: $URL$ However, you may find useful other libraries, which implement a wide variety of clustering algorithms, including Non-Negative Matrix Factorization (NMF). One of such libraries is Python Matrix Factorization (PyMF) with home page at $URL$ and source code at $URL$ Another, even more interesting, library, also Python-based, is NIMFA, which implements various NMF algorithms: $URL$ Here's a research paper, describing : $URL$ Here's an example from its documentation, which presents the solution for very similar text processing problem of topic clustering: $URL$ 

Most likely, SGD is not a limiting factor for you. But, have you considered taking a classification rather than regression approach? (It looks like you're predicting real values as opposed to classes). Since you state that the prediction doesn't have to be perfect, why not try grouping your outcome variable into bins, then predict the bins? You'll have a far less granular solution, but you might find it works. 

This is a controversial subject with no clear best answer and myriad options, some of which are model-specific. You can drop them, replace them with extreme values, interpolate, replace with median, impute with nearest neighbors, etc. In all cases besides dropping them altogether, you make assumptions and create data where no data exists. I can't tell you the best approach, but I can advise you to try multiple approaches and hope the end results are robust to choice. 

In the context of sentiment analysis, removing stop words can be problematic if context is affected. For example suppose your stop word corpus includes ‘not’, which is a negation that can alter the valence of the passage. So you have to be cautious of exactly what is being dropped and what consequences it can have. 

Feature engineering refers to creating new information that was not there previously, often by using domain specific knowledge or by creating new features that are transformations of others you already have, such as adding interaction terms or as you state, moving averages. A model generally cannot 'pick up' on information it doesn't have, and that is where finesse and creativity comes into play. Whether you should one-hot or leave a feature as categorical depends on the modeling approach. Some, like randomForest will do fine with categorical predictors; others prefer recoding. Intuition on these questions comes with practice and experience. There's no substitute for trying out and comparing toy examples to see how your choices affect outcomes. You should take the time to do that, and intuition will follow. 

If you must plot raw values, use a random sampling strategy or some form of decimation (every nth value). Otherwise, computing and plotting summary statistics will be orders of magnitude faster. While lossy, careful attention to variability around the metrics will help you understand the form of the raw data 

This is kind of tough if the only data you have is login/log off (bytes sent/received by time would be better). You surmise that logoffs can't be trusted, because not everyone logs off, and logged in users may equally be someone who stays 10 minutes or several hours. You might consider running some scenarios to get best/worst case estimates. For instance, each login could be considered a singular event convolved with a probability density kernel (positively skewed with some decay rate perhaps), which then gives you 'occupancy over time'. You can then try different parameters to ascertain best case/worst case scenarios. 

If you are new to data science and data munging, this could be kind of a tricky task, but a good one to get your feet wet. Many programming languages have the capability to do this (R, Python, Matlab, etc.). I use R primarily, so I'll give you a brief heuristic for how I'd approach the task in R. Perhaps looking into these steps will get you started. 

I assume you are running classification, and have a binary target variable. If that's the case, it does not make sense to show component ROC curves, because your separation may be based on on combinations of 2, 3, or more predictors that individual ROC curves will not reflect. I would show your overall ROC curve, along with perhaps variable importance measures. If you have a handful of predictors that are clear winners, you could re-run your model including only those, and then show that ROC. Otherwise, I don't see what it buys you. 

I see at least five ways to approach this problem of finding a data scientist position/work specifically at non-profit, non-governmental or similar organizations, as I describe below. I hope that this is helpful. 

Clearly, the only fuzzy fact that connects this book with the subject "Audiology and Speech Pathology" (!) is IMHO the author's last name (Boice), which, is close to the word "voice". If my guess is correct, Amazon's ML engine, for some reason, decided to take into account the book's lexicographical attribute instead of the book's most important and most relevant attributes, such as title, topic and contents. I've seen multiple occurrences of similar absolutely incorrect ML-based decision making on Amazon.com and some other websites. So, hopefully my question makes sense as well as interesting and important enough to spark a discussion: What could be other potential reasons for misclassification and what are the strategies/approaches to avoiding such problems? (Any related thoughts will also be appreciated.) 

This is a very vague question. However, I will try to make sense of it. Considering rules of logic as well as your statement that both entities are "software systems that are based on data and algorithms", it appears that data products are intelligent systems and intelligent systems are, to some degree, data products. Therefore, it can be argued that the difference between the terms "data products" and "intelligent systems" is purely in the focus (source of information or purpose of system dimensions) of each type of systems (data vs. intelligence/algorithms). 

So far, so good. However, let's scroll down a bit further to see the the books ratings in relevant categories. We should expect Amazon to figure out categories, relevant to the book's discipline, topic and contents. How surprised was I (and that's an understatement!) to see the following result of Amazon.com's sophisticated ML engine and algorithms: 

Another idea is to combine OpenStreetMap project map data, for example, using corresponding nice R package ($URL$ with census data (population census data, such as the US data: $URL$ as well as census data in other categories: $URL$ to analyze temporal patterns of geosocial trends. HTH. 

I have a very limited knowledge of game theory, but hope to learn more. However, I think that potential applications of Nash equilibrium in the context of big data environments, implies the need of analyzing a large number of features (representing various strategic pathways or traits) as well as large number of cases (representing significant number of actors). Considering these points, I would think that complexity and, consequently, performance requirements for Nash equilibrium in big data applications grow exponentially. For some examples from the Internet load-balancing domain, see paper by Even-Dar, Kesselman and Mansour (n.d.). The above-mentioned points touch only the volume aspect of 4V big data model (an update of Gartner's original 3V model). If you add to that other aspects (variety, velocity and veracity), the situation seems to become even more complex. Perhaps, people with econometrics background and experience will have some of the most comprehensive opinions on this interesting question. A lot of such people are active on Cross Validated, so I will let them know about this question - hopefully, some of them will be interested to share their view by answering this question. References Even-Dar, E., Kesselman, A., & Mansour, Y. (n.d.). Convergence time to Nash equilibria. Retrieved from $URL$ 

I would like to recommend to check the following open data repositories and meta-repositories (they are not focused on categorical data, but I'm sure that many data sets, listed there, contain such data): 

Let me give you some pointers (assuming that I'm right on this, which might not necessarily be true, so proceed with caution :-). First, I'd figure out the applicable terminology. It seems to me that your case can be categorized as multivariate sampling from a categorical distribution (see this section on categorical distribution sampling). Perhaps, the simplest approach to it is to use R ecosystem's rich functionality. In particular, standard package contains function (link). If you need more complex types of sampling, there are other packages that might be worth exploring, for example (link), (link), offering function (link). If your complex sampling is focused on survey data, consider reading this interesting paper "Complex Sampling and R" by Thomas Lumley. If you use languages other than R, check function from Python's package and, for Stata, this blog post. Finally, if you are interested in Bayesian statistics, the following two documents seems to be relevant: this blog post and this survey paper. Hope this helps.