Postfix provides something called smtp-sink. By default it blackholes all of the email it receives. Later versions can also be configured to capture the email in files. This doesn't technically use postfix (but a utility provided by postfix.) It also doesn't technically redirect each email to a single email address. But it does capture all traffic on port 25 and dumps that to a file that can be parsed. 

Instead of piping email directly into your script, pipe it into procmail first. Procmail can look at the headers and route it accordingly. 

On the Dell R710 (and many other makes/models) you can monitor the power usage yourself with this command: 

Google isn't helping me find out anything about a Sun NPR900R Server Cabinet. What is it? Can you provide a link to the documentation for it? 

It looks like your machine is making an outbound connection to a remote server on port 443. Use your favorite network sniffer (ngrep, tcpdump, etc.) and look at connections to the remote ip and port 443. 

I just did something similar and it had no discernible effect on the remote host. I say: go for it. What I did: 

This seems wrong on so many levels. You are making it more difficult to maintain (by installing from source you lose all upstream QA and updates) and you are moving things around that don't need to move around. Is it valid? Yes. You can make it work. Should you so this? Probably not. 

You need to get readings on each type of server (hopefully you don't have 20 different types, but if you do you need readings on each.) You can get readings with a Kill-A-Watt or a Watts-Up meter (I like the International version (UO) of the Watts-up because it handles 120-240 input voltages and servers are more efficient with higher voltages.) If you trying to size a UPS you might want to read APC White Paper #15. 

I'm going to guess that thus is an ssh key problem. Either backup's private key has a pass phrase or the public key isn't in the remote users keychain. Maybe your previous attempt used a key that was already loaded in your keychain? 

How much fuel does it use per hour? Where are you going to store that fuel? Who will deliver the fuel in the event of an emergency? How many of their customers are in line before you when the natural disaster of your region happens? How often will you test it's operation? Will you test it under full load? Will you be using the live load for that? How long can the generator run before it needs maintenance? And finally: will an on site generator be cheaper than housing those servers in a quality data center? 

If your cooling is from below (raised floor) you want your high power draw servers close to the floor. Reverse for overhead cooling. Make sure to install blanking panels on all unused spaces. I like to mount the switches in the middle, it keeps the cables a bit more manageable. And push back on that front mounting. The switches mount on the rear, IMHO. Can you imagine tracing a cable from front to back? I also like to leave the top U for the connecting patch panel (drops.) Is this a rack they are providing? Do they provide the power strips (PDUs) as well? Do the PDUs require rack units too or are they vertical mount? I'm assuming that you have two PDUs. You should verify that they are powered from two different upstream UPSes. If they are not, that might change your decision on how you are setting up the power. 

If the host is up you can also run: to reboot the BMC which will then do the port test again. If you use the dedicated port, it will then start using it. 

I agree with voretaq7, just install a special 120V circuit (and associated rack PDU) for that load. Alternatively find a dual voltage power supply to replace each el-cheapo power brick you have. Do NOT use a IEC C14 to NEMA 5-15R cable for that. It'll certainly make the copper connection, BUT it won't step down the voltage. Those cables should be banned, IMHO (but I keep one in my bag, just in case.) They are dangerous in the wrong hands as they are a fire hazard. (Since you are asking this question I assume you know those dangers, but I want others to read this.) Others: ALWAYS check the allowed input voltage. Plug in the wrong thing and you'll cause a fire. Which is NEVER good in a DC. 

I would recommend any other configuration management system over puppet. Puppet will re-order the configuration steps on each run. Even on the same host. Proponents will tell you that you can setup your requires appropriately so that it works well. I'll tell you that I'm human and I make mistakes. If you have a puppet recipe of any complexity you'll want to test your work. When you do and it succeeds you'll assume that it works on all of your other hosts. This is not necessarily the case. Any system that assumes I'm perfect is, itself, not. 

If these IPs are certified by Return Path, ask them what you can do. They are very helpful in this sort of situation. 

Setup a second server as a slave. You probably want a master/master setup so either host can take writes. Move the IP over to this second machine. Now you can do your maint on the first machine. This will minimize the downtime for the MySQL service. 

Well, from a high level, this sounds plausible. But, as they say, the devil is in the details. Who would be swapping the drive? Will you ALWAYS verify that the other drive hasn't failed before pulling the backup drive? A backup isn't a backup without a restore procedure. What is yours? 5 years from now will you be able to purchase a RAID card that will read this drive? My suggestion is to find another way. 

I help send millions of emails every day from my employer. I'm not sure the problem is solved by adjusting your instance size. What is your bottleneck? Do you have dedicated IPs from AWS for sending email? Have you asked Amazon to add reverse DNS for your sending IPs? Are the major MBP (MailBox Providers) limiting your sends? Have you signed up for their FBLs (FeedBack Loops)? Do you have a bounce email processing system? Some people don't know how to unsubscribe and some don't trust the links in the email. This is why it is important to have the FBLs. There are also headers you can add in the emails to help people unsubscribe. I look forward to your reply. 

I really like that you're trying to make this repeatable! My rule is this: Kickstart partitions the OS drive (if there are multiple) and installs enough to run the configuration management system of your choosing. Nothing more. Your config mgmt system takes care of adding the required packages. Start with and remove all packages except for your config mgmt system. This way it will install everything necessary for it to run, but nothing more (it may require some trial and error to get it perfect.) This way your 'repeatable step-by-step doc' is actually programmatic. 

Setup a server on your internal network, copy your content over (you might also need to make sure you're running the same CMS internally.) Then figure out how to allow access to that server on your internal network (maybe even put it on a DMZ.) Then change the public DNS record to point to your server. Given how you are asking this question, it sounds like you should hire a consultant for this. I'd also ask this question: Do you need to move the whole website or can you create another sub-domain and use that? www.example.com is your public website, mobileop.example.com runs your internal services. mobileop.example.com could be run on a separate server on your internal network and wouldn't necessarily touch anything on your public one. So many options. Don't tell the consultant exactly what you want done, tell them what your end goal is. 

Both of your virtualhosts are configured for the same servername. Use the desired sub domain for the second. 

You can have a check do anything you can program. The only thing you need to do is craft the output of your check such that Nagios can parse it. In this specific case, I might implement this as a passive check. 

What's the difference between managing 10 servers and managing 1,000? Nothing, if you did it right. This is a job for configuration management. Look into Ansible/Cfengine/Chef/Puppet/etc. 

Email deliverability is a whole industry onto itself. When you start working with a company that helps you with these issues one of the first things they have you do is set yourself up with FBLs for all the major email providers. (Google doesn't participate in that one.) This can significantly improve your inbox percentage (assuming you act on the messages.) Speaking of, do you have a good bounce processing setup? 

Databases are commonly I/O bound. Without knowing anything about your particular application I'd drop 3 of the processors and look at getting a Fusion IO card (or maybe an SSD) for the pgdata partition. I'd also setup the RAID a bit different. The usage pattern of the xlog (sequential) will typically be different than the pgdata (random) partition. For this reason I'd suggest putting them on separate physical devices. 

This sounds like a good case for sharding. If the data in one survey doesn't need immediate access to the data in another survey, then sharding your data will be easy. You'll setup a database that has basically a user ID key which points to a Survey DB. You can then setup multiple Survey DBs. Hopefully you'll also choose to set those up in a replicated tuples as well. Your application will need a bit of re-working. Run your reports and do the joins in software. If that's also an option, sharding is the way to go. 

If a user clicks the 'this is spam' link you can get an email reply from the mailbox provider (except gmail, they have their own thing) that will tell you that the user clicked that button. BUT you'll never receive those messages unless you sign up for feedback loops (FBLs) from the major email box providers. You might also need libraries from certain companies that can easily parse all of the bounce messages From the myriad providers on the Internet. 

I setup passive checks in nagios for various events. Then at the end of the event the passive check is sent to nagios (either via wrapper script or built into the event itself.) If the passive check hasn't been received in freshness_threshold seconds, it will run check_command locally. check_command is setup as a simple shell script which returns critical and the information of the service description. I don't have code examples handy, but if I could if interest is shown. EDIT ONE added code examples: This assumes that you have done the basic setup for NSCA and send_nsca (make sure password and encryption_method is the same in send_nsca.cfg on the client and nsca.cfg on the nagios server. Then start nsca daemon on the nagios server.) First we define a template that other passive checks can use. This goes into services.cfg. 

This option uses no extra disk space on the remote machine. It creates the tar file and immediately pushes it out over the ssh connection and saves it on your Mac. 

Boot from a rescue CD and mount the drive, then you can run your chown. I can't think of a way to do this without a reboot. 

Also note that Dell's fiscal end of quarter trails the calendar year by one month (specifically Jan 31, Apr 30, July 31, Oct 31.) If you are buying servers in any quantity, that's when you can get the best deals. ;) 

They might turn themselves off. Check to see if thermal shutdown is enabled (if you can get to that setting without rebooting.) 

How about two servers acting as your application databases and another server (or two?) acting as your data mining tier. Your two 'application' databases shouldn't get (too far) behind because they aren't handling any of the heavy queries. You can also setup mysql-mmm to handle the failovers. Then you can point your application to the Mysql VIP that you've setup in your load balancer. This VIP has two MMM IPs in it. But MMM might say 'hey, box 2 is too far behind, I'll move that IP to box 1 so it can catch up.' Then you'll have two tiers of databases handling your two different query types. The caches on these machines will be optimized for their query types (in theory.) Also look into a Fusion-IO card or Virident TachIOn. That might solve the problems without adding a bunch of hardware.