For the poset which is the union of $w$ disjoint chains you need at least $w\log n$ evaluations of $P$ by just applying the standard lower bound on the query complexity of binary search to each chain. Since you give comparisons for free, you can compute a chain decomposition of the poset into $w$ chains for free. Do binary search on each chain to identify the first element that satisfies $P$. Then go over the identified elements and remove any dominated ones. Number of evaluations of $P$ is $O(w\log n)$. This identifies all maximal elements, as there can be at most one maximal element per chain. 

Notice that the input size is $n = \lceil \log_2 P \rceil$. The problem can be solved in time near linear in $n$, i.e. in time $n \log^{O(1)} n = (\log P)(\log \log P)^{O(1)}$. The trick is to do binary search on the integers in $[1, 2\log_2 P]$ in order to find the largest $N$ such that $N! \leq P$: call this number $N(P) = \max\{N: N! \leq P\}$. If $N(P) = P$, the answer is 'yes', otherwise it is 'no'. Let us first show that $N(P) \leq 2\log_2 P$ for $P \geq 4$ (for smaller $P$ you can use a lookup table). This follows because for every $N$, $N! \geq (N/2)^{N/2}$, and therefore $$ \log_2 P \geq \frac{N}{2}\log_2 \frac{N}{2} \geq \frac{N}{2}. $$ The complexity is $T(2\log_2 P) \cdot O(\log \log P)$, whete $T(N!)$ is the complexity of computing $N!$. This is because the binary search performs $O(\log \log P)$ steps, and at each step computes $N!$ for some $N \leq 2\log_2 P$. The work by Borwein, referenced in Chad's answer, gives a near-linear in $N$ bound on $T(N)$. 

What about the following. I claim that solving the feasibility problem $\exists? x: Ax \le b$ reduces in strongly polynomial time to finding a linear separator. Then it's easy to reduce linear programming to the feasibility problem. First notice that you can reduce the feasibility problem to the special case $b=0$: just transform the constraints to $$ Ax + tb \le 0, \\ t \le 0 $$ This just adds a row and a column to $A$, and a new variable $t$. Now, after modifying the problem in this way, create a data set which has one point labeled $0$ for every row of $A$, and is equal to that row. It also has the point $0$, labeled $1$. The linear separator, if it exists, will be the solution to your feasibility problem. Another way to say this is that certifying if 0 is in the convex hull of a set of points is equivalent to linear programming, in the sense of a strongly polynomial time reduction. 

The empty set has the fingerprint 0. add(x) and remove(x) update a fingerprint $f$ to $f \oplus h(x)$, where $\oplus$ is xor. 

Hash functions can be represented with $O(\lg n)$ bits of space Hash functions can be evaluated in $O(1)$ time The maximal load is $O(\lg n / \lg \lg n)$ with high probability. 

I think Purely Functional Worst Case Constant Time Catenable Sorted Lists by Brodal et al. supplies what you want. 

One potential issue is when reading from a hash table, the elements should not be read in the order of the slots if all hash tables use the same hash function. This is because those elements, in that order, can cause the insert procedure on a smaller hash table with the same hash function to go quadratic, assuming that the max fill factor is over $1/2$. See: 

The almost-universal family of hash functions $$\{h_a(\vec{x}) = \sum a^i x_i \bmod p: a \in \mathbb{Z}_p\}$$ has a nice property here: $h_a(\vec{x}) + a^{|\vec{x}|}h_a(\vec{y}) = h_a(\vec{x} \circ \:\vec{y})$, where "$\circ$" denotes concatenation. If you cache at the root of each tree both its hash value and $a^{|\vec{x}|}$, you can calculate the hash of the concatenation of two trees in $O(1)$ operations on $\mathbb{Z}_p$. This is both associative and pretty fast. The collision probability of $\vec{x} \neq \vec{y}$ is $O(\min(|\vec{x}|,|\vec{y}|)/p)$. See CLRS or Dietzfelbinger et al.'s in "Polynomial Hash Functions Are Reliable". 

The cleanest introduction to this I have seen is in Tim Griffin's "A formulae-as-types notion of control". 

Update: An idea very similar to this one is described in "Digital Access to Comparison-Based Tree Data Structures and Algorithms". 

Keep your integers in skip lists. Normal skip lists are ordered by key, but we will just use them as a representation of sequences. Additionally, maintain an array of pointers of size $n$. Each element of the array should point to a node in a skip list. I believe this supports $next$ in $O(1)$ and all other operations in $O(\lg n)$. Specifically: 

I'm not sure I understand the question completely. Do you want an algorithm that, given two sorted sequences each containing no duplicates, outputs a new sorted sequence containing every item in either of the input sequences and also no duplicates? If so, you can represent the larger sequence as a special type of tree (a finger tree), then merge the sequences in $O(i \lg (k/i))$ merges, where $i$ is the size of the smaller sequence and $k$ is the size of the larger sequence. I think this is asymptotically optimal in the pointer machine model. 

Let me finish with a sketch of the SDP rounding (which is fairly standard fare). Let $P = \{x: -b \leq Ax \leq b\}$ be a centrally symmetric polytope, where $A$ is $m \times n$. Define the vector program: $\alpha^2 = \max \sum_{i = 1}^n{\|v_i\|_2^2}$ subject to: $\forall 1 \leq i \leq m: \|\sum_{j = 1}^n{A_{ij} v_j}\|_2^2 \leq b_i^2$ Above the $v_i$ range over $n$-dimensional vectors. This can be written as an SDP in the standard way and is a relaxation of the diameter of $P$, i.e $\alpha$ is at least the euclidean diameter of $P$. I now claim that $\alpha \leq O(\sqrt{\log m})\cdot \text{diam}(P)$. To show this, I will give you an algorithm that, given $(v_i)_{i=1}^n$ of value $\alpha$, outputs $x \in P$ of length at least $\frac{\alpha}{O(\sqrt{\log m})}$. The algorithm is just a random projection: pick a random $n$-dimensional vector $g$ where each $g_i$ is a standard gaussian. Set $\tilde{x}_i = g^T v_i$. By standard properties of gaussians: $$ \mathbb{E}\ \|\tilde{x}\|_2^2 = \alpha^2 $$ $$ \forall i \leq m: \mathbb{E}\ |(A\tilde{x})_i|^2 \leq b_i^2 \ \ \Rightarrow \ \ \mathbb{E}\ \max_{i=1}^m{\frac{|(A\tilde{x})_i|}{b_i}} \leq C\sqrt{\log m}. $$ where the last bound holds for large enough $C$ (this is a standard fact about the maximum of $m$ subguassian random variables, and can be proven using the Chernoff bound). The two equations already imply there exists an $x$ such that $x \in P$ and $\|x\|_2^2 \geq \frac{1}{C\sqrt{\log m}}\alpha$. Or, using concentration bounds, you can show that with constant probability $\frac{1}{2C\sqrt{\log m}}\tilde{x} \in P$ and $\|\tilde{x}\|_2\geq \frac{1}{2}\alpha$. 

With respect to Problem II, it is coNP-hard (under Karp reductions) to tell if the number of unreachable strings is 0 or at least $1 - 2^{-\text{poly}(|x|)}$ fraction of all strings. I suspect there is a way to boost this and show that the gap problem is PSPACE-hard, maybe by using IP as a robust characterization of PSPACE. Let $L$ be a coNP-complete language. There exists a polynomial time Turing machine $M$ and a polynomial $p$ such that $x \in L$ if and only if for all $w$, $|w| = p(|x|)$, we have $M(x, w) = 0$. Construct $P$ to be the following program. It takes $z$ where $|z| = p(|x|) + i$ and maps it to 

A typical problem is MaxCut: output a cut in a graph that (approximately) maximizes the number of edges cut. Goemans and Williamson showed an SDP approximates the value of MaxCut to within a factor at least 0.878. Recently, Chan, Lee, Raghavendra, and Steurer showed that for a natural linear encoding of the MaxCut problem, all polynomial size LPs achieve approximation no better than 0.5. It's hard to say concisely what kind of problems usually benefit from an SDP. A systematic approach to constructing SDP relaxations is through hierarchies, the most powerful of which is the Lasserre hierarchy: see Rothvo√ü's survey for a nice introduction. By now there are too many examples of successes of SDPs in optimization to list. Also, Raghavendra showed that one particular SDP gives the best approximation to all MaxCSP problems, if the Unique Games conjecture is true. Check the books of Gaertner and Matousek, chapters 6 and 13 of Willimson and Shmoys' book, Lovasz's survey. 

I think Raman and Rao's "Succinct Dynamic Dictionaries and Trees" meets the bounds you specify. From the abstract: 

What is the complexity of calculating the values of the integers $x_i$, where $0 \leq x_1 < x_2 < \dots < x_k < n$, given only the values $s_m = \sum_{i=1}^k x_i^m$? for $1 \leq m \leq k$? If this can be solved in $f(n,k)$ space, it can be used to solve the following problem: Given $S \subset \mathbb{Z}_n$ where $|S| = n-k$, determine $\mathbb{Z}_n \backslash S$ in $\tilde O(f(n,k))$ space. This is done by calculating $\sum_{s \in S} s^i$ for $1 \leq i \leq k$ and $\sum_{s \in \mathbb{Z}_n} s^i$ for $1 \leq i \leq k$. 

I'm really looking for worst-case or expected bounds on the time, but amortized bounds would also be interesting. Results in the word RAM would be interesting as well. Here are some results I am aware of. In this table, sloppy split is like approximate split, but the bounds are $1/c < (\lg p)/(\lg q) < c$. 

Is there in an integer priority queue that uses $O(n)$ words of space with the following operations, all in worst-case time and without access to randomness: 

"The TAP conference is devoted to the convergence of proofs and tests". Just googling for "proofs and tests" has several good hits above the fold. 

Update: see below for an update on the incorrectness of this join operation Here is a very rough sketch of a possible solution: I think I may have a solution to this problem using a type of randomly-balanced B+-tree. Like treaps, these trees have a unique representation. Unlike treaps, they store some keys multiple times. It might be possible to fix that using a trick from Bent et al's "Biased Search Trees" of storing each key only in the highest (that is, closest-to-the-root) level in which it appears) A tree for an ordered set of unique values is created by first associating each value with a stream of bits, similar to the way each value in a treap is associated with a priority. Each node in the tree contains both a key and a bit stream. Non-leaf nodes contain, in addition, a natural number indicating the height of the tree rooted at that node. Internal nodes may have any non-zero number of children. Like B+-trees, every non-self-intersecting path from the root to a leaf is the same length. Every internal node $v$ contains (like in B+-trees) the largest key $k$ of its descendant leaves. Each one also contains a natural number $i$ indicating the height of the tree rooted at $v$, and the stream of bits associated with $k$ from the $i+1$th bit onward. If every key in the tree rooted at $v$ has the same first bit in its bit stream, every child of $v$ is a leaf and $i$ is $1$. Otherwise, the children of $v$ are internal nodes all of which have the same $i$th bit in the bit stream associated with their key. To make a tree from a sorted list of keys with associated bit streams, first collect the keys into contiguous groups based on the first bit in their streams. For each of these groups, create a parent with the key and bit stream of the largest key in the group, but eliding the first bit of the stream. Now do the same grouping procedure on the new parents to create grandparents. Continue until only one node remains; this is the root of the tree. The following list of keys and (beginning of) bit streams is represented by the tree below it. In the bit stream prefixes, a '.' means any bit. That is, any bit stream for the key A with a 0 in the first place with produce the same tree as any other, assuming no other key's bit stream is diffferent. 

As in my comment, the configuration graph (Chapter 4.1.1. of Arora-Barak) is a simple way to view a Nondeterministic Turing Machine (NTM) as a graph. The vertices of the graph are labeled by configurations: the internal state of the NTM, the contents of the tape, and head positions. There is a directed edge from vertex $c_1$ to vertex $c_2$ if the NTM can move from $c_1$ to $c_2$ in one step. This is especially useful for arguing about space-bounded computation. If the space used by the NTM is bounded by $s(n)$, then on input size $n$ you only need to consider a configuration graph with $O(2^{s(n)})$ vertices. Then a lot of complexity results follow from algorithms for directed $s$-$t$ connectivity: Savitch's theorem (NPSPACE = PSPACE) follows from an algorithm that uses $O(\log^2 |V|)$ space, PSPACE in EXP (or, scaling down, L in P) follows from DFS, etc. Similarly, you can show that directed connectivity is complete for NL under L reductions, so NL is equal to L if and only if there is a logspace algorithm for directed $s$-$t$ connectivity. 

You can also check out Oded Regev's lecture notes which reference more applications. This book chapter by Hanrot gives details about quite a few applications of LLL to constructive versions of results in diophantine approximation. 

If you have a graph with maximum degree $\Delta$, then the greedy algorithm finds a coloring with $\Delta+1$ colors, so for $k = \Delta+1$ the assumption that you are given a proper $k$-coloring does not change the complexity of the problem. It is known that independent set is NP-hard to approximate within factor $\Delta^{1 - o(1)}$, and Unique Games-hard to approximate within factor $\frac{\Delta}{O(\log^2\Delta)}$, which, for $k=\Delta$ is within a $\log \Delta$ factor from the upper bound you cite. (I prefer to write approximation factors as bigger than 1, so I am thinking of the upper bound you cite as $\frac{k}{2}$.) However, it's still interesting to close these logarithmic gaps. This paper by Bansal may contain useful ideas. 

If I am understanding your problem correctly, this is the problem of computing the face containing a given point in an arrangement of line segments. There is a randomized algorithm running in expected time $O(n\alpha(n)\log n)$, where $n$ is the number of line segments, and $\alpha(n)$ is the inverse Ackermann function. The algorithm computes the boundary of the face, and from that you can list the grid points if you want using a sweep algorithm, but the time to do that will of course depend on the number of grid points inside the face. 

The COLAs paper discusses another data structure at this intersection, as does a more recent improvement. Implementation-wise, I only know of one other DBMS that uses cache-oblivious data structures: EaseDB, which is in-memory and appears to no longer be under active development. 

There are a few more that you might be interested in, including buffer trees and Brodal et. al's tunable update/query tradeoff trees. 

There are a number of purely functional deques that support $O(1)$ operations at each end. None that I know of are "uniquely represented" - deques with the same number of items can have different shapes. In the imperative case, doubly-linked lists are uniquely represented deques with constant-time operations. Hoogerwoord, in "A Logarithmic Implementation of Flexible Arrays", showed that Braun trees have logarithmic deque operations, and they are purely functional and uniquely represented. Is there a deterministic uniquely represented purely functional deque with $o(\log n)$ operations? 

Well, you could just use a two-way hash map between $S_k$ to $\{0,1\}^{\lceil\lg{{n}\choose{k}}\rceil}$. That's easily computable in each direction in time corresponding to the length of the input, but it uses a lot of space. Knuth's "Pre-Fascicle 3a: Generating all combinations", as well as TaoCP, 7.2.1.3, Theorem L and exercise 17 explore the "Combinatorial number system", which is an enumeration of $S_k$ that can be reversed without just computing a huge lookup table. No detailed analysis of run-time is given, but it looks $\omega(\lg{{n}\choose{k}})$ to me. 

The shared prefix pairs at A, C, and E all remain valid. The left shared prefix of B and the right shared prefix of D also remain valid. The right shared prefix at B is the left shared prefix at D from before the rotation. The left shared prefix of D is the minimum of the left shared prefixes of D and B from before the rotation. The right rotation and deletion cases are the same, mutatis mutandis. 

What are the known limits of the decidability of the comparison of growth rate of functions from $\mathbb{N} \to \mathbb{N}$? I am here thinking of the decidability of questions like "Is $x^x \sim 2^{\lfloor x \lg (x+2) \rfloor}$?" or "Is $2^{\lg^* x} \in O(\lg \lg x)$?". If we restrict the functions to be polynomials (expressed in the usual manner), then this isn't hard. See also Cantor normal form. How large can we make the class of functions before comparison becomes undecidable? Can we extend it to the functions used in a typical undergraduate algorithms class? As Joshua Grochow explains in the comments, I'm interested really in the set of expressions, not the functions themselves. So, for instance, I'd be interested in decision procedures that could compare "$1$" and "$2$", even if they can't compare "$\ln e$" and "$n^{(\ln n)^{-1}}$". Possibly related question: "Is the theory of asymptotic bounds finitely axiomatizable?"