As your data is already multi dimensional i would create a schema that follows along those lines using a datawarehouse star schema with a central fact table. In your example the fact table would have the lat, long, pressure, timestamp, observed value and attributes. If you wanted to extend your model to use an olap cube you could add a time dimension and others as needed. 

Another reason to use a data warehouse or dedicated reporting server is to avoid placing load on a production OLTP server. Or if you have to write reports which include data from different systems. The way you have described your db I doubt there would be much advantage in denormalising unless you wanted to build OLAP cubes on it. In which case an OLAP friendly schema might have significant benefits. 

I have been given the task of improving the performance of the first report connection each day. Similar to this post. We have a simple SSRS report that acts as a landing page. It has hyperlinks to subsequent reports. I have tried: 

Our organization has an externally hosted website that displays data from a monitoring system. Data is currently provided by FTPing csv files. The data is stored in a SQL Server Database. The exported data sets are only a few hundred rows of data, updated hourly. I would like to replace this with a a web service so that data can be retrieved on demand, and we avoid handling files. I'm a DBA not a programmer, so i'm out of my comfort zone. Is there a conventional approach for doing this? Are there standard Microsoft products that we could configure to achieve this? Or do we need to hire a programmer to develop something custom from scratch? I know SSIS can consume data from a web-service but can it provide data as a web-service? 

When things go horribly wrong all eyes will be on you. Your ability to restore a critical system quickly, quietly and calmly will be important. Practice. Test. Practice some more. Ensure your backup strategy meets business expectations. Learn about RPO & RTO. When things arent going wrong the next most important thing is ensuring that only the people who should have access to your data do. IMO 1 & 2 above are critical. Failure to achieve these responsibilties will not be forgiven lightly. For critical production systems stability is highly desireable. You want it to run smoothly without interruptions and downtime. Dont play around in prod during office hours any more than is necesary. Do your bulk jobs and maintenance during scheduled time slots. Stop others from doing stupid things. Lead by example. Eg don't develop on a prod server, and don't put prod dbs on a dev box. If performance degrades people will start turning to you for help or answers. Learn about query tuning and the relationship with regular maintenance of indexes and statistics. There are some great free resources such as Brent Ozar's and Ola Hallengren's scripts that will give you a big headstart. The more routine tasks you can automate the more time you will have to spend elsewhere. If you do have spare time and want to learn more you can start by reading blogs and forums like DBA Stack Exchange and SQL Server Central, Paul White, Pinal Dave, Brent Ozar, Aaron Bertrand etc. 

Both relations are in BCNF, and the final decomposition is given by R2, R3, and R4. Finally, note that sometimes the algorithm can produce different solutions, depending on the functional dependency chosen at each step. 

and in fact is a key for the relation. Then, supposing that there are no null values, if you want be unique, there exist another dependency: 

the key now is only , and the relation has redundancies (since for each phone related to the same person, you have to repeat the ID, TFN, and Name of that person). The relation is not in BCNF, and to bring it in such form you have to decompose it in the two relations: 

While it is immediate to discover tha is a candidate key since it determines all the other attributes, you can see that this is true also for and by calculating the closure of those attributes: 

Note that this decomposition preserves the functional dependencies. Addition How the decomposition is obtained? Starting from the original relation: 

In the terminology used to to talk about conceptual design of database, this is called inverse relationship. So you could say: 

The relation is actually in 3NF (so that it is also in 1NF and 2NF). The reason is that each attribute of the relation is prime, that is, it belongs to a (candidate) key (the are four keys in this relation: ). The definition of the 2NF (which has only an historical interest), is the following (Database System Concepts, 6th edition, Korth, Silberschatz, Sudharshan): 

R1 is not in BCNF, since the key is , so for instance the dependency violates the normal form. Applying the algorithm, R1 is decomposed in: 

You are correct, the relation is in 2NF, 3NF, BCNF. The reason is that the relation has two keys, and . So the relation is in BCNF (which is a property stronger than 3NF and 2NF) since each determinant of the minimal conver of R1 is a key. Here is one minimal cover: 

The information about the Maker of a certain Model is already contained in the Model record, so storing it inside Vehicle is redundant. Having both attributes in Vehicle is prone to errors, since one could have inconsistent information inside a Vehicle. 

I believe we've found another workaround. I'm posting my answer as I think it may be useful, and it's different enough from wBob's suggestion. We've changed the insert part of the merge statement so that it inserts to a temp table rather than the original target. Once the merge statement executes, we then insert from the #table into the target. It's not ideal, but at least the merge still handles the complexity of the 'upsert' by marking rows that have been retired/expired. We found this to be an acceptable trade-off compared with completely re-writing the merge as separate inserts and updates. 

Yes you can work offline but it will be difficult to develop and test without the input/output connections files/tables your package will use. One option is to install SQL Server on your pc, create a database and copy the source and target tables and some test data to your pc. Then you can develop and test locally. Deployment will be difficult unless you have ssdt on the server. Otherwise you will need access to deploy to the server from another machine. 

I've been given the task of producing a report which will extract data from a web service. The web service provides data from a list of sites including aggregates. Lets say total sales by month. Id like to plot these values on a map and locate them using geometry points. The developer is investigating how he can include the spatial reference in the xml reply. But before i get him change the Web service is there any limitation or restriction in ssrs that would stop this working? If so, what are my alternatives? If i can retrieve the easting and northing in the xml can i convert to a spatial datatype with an expression on the fly? 

Short version is yes its possible to do what you are describing. Your report will have a spatial query to display the shapes you want. Whether that's a map or a floor plan. You can then add analytical data in a second query to colour the map. The two queries needs to be joined by a key. Im not familiar with qgis but ideally you'd want both the spatial data and analytics in sql server. The other option is to have 2 layers. The first layer gives the 'map' shape and the 2nd adds points on top. If you search Google images for "SSRS map" you'll see lots of examples. 

This means that all the attributes are primes, so the relation is by definition already in Third Normal Form. For the BCNF, you have described the “analysis algorithm”, which is the algorithm presented in every good book on databases. But, with this example, you have also discovered that in normalizing, you can have different results, that can depend for instance on the order in which the dependencies that violates the normal form are chosen (not in this case, but possible in other cases with this same algorithm), or by using a different algorithm (there are others, not interesting from a practical point of view). In your example, the second decomposition, which is in BCNF as you correctly stated, cannot be produced by the analysis algorithm: there is nothing wrong about it, in general there is no guarantee that this algorithm will produce the “best” decomposition (for instance a decomposition with a minimum number of relations), and, if this is an exercise, you have done it correctly! What instead is interesting to note, also from a practical point of view, is if the decomposition satisfies all the important properties, i.e. if it preserves the dependencies of the original schema (since the decomposition produced by the algorithm is instead guarantee to be always a lossless decomposition). And you can see that both the decompositions fail under this aspect. The decomposition in does not preserve the dependency , while the decomposition does not preserve the dependency . This means that in practice both of them reduce the redundancies and other anomalies of the data, at the expense of losing an important contraint over them. So, knowing that the relation is already in 3NF, one could be satisfied by this result and leave the relation in this form. This means the preservation of the dependencies, together with a limited amount of redundancy, which in practice can be tolerated. 

Normal forms are used to eliminate or at least reduce redundancy in data, as well as to eliminate the so called insertion/deletion anomalies. The BCNF eliminates redundancies and anomalies, but decomposing a relation in BCNF sometimes has the unpleasant effect of causing the loss of one or more functional dependencies during the process. For this reason the 3NF is used instead of the BCNF in practice, since a decomposition of a relation in this form always mantains data and dependencies, and reduces anomalies and redundancy (even if to a lesser extent that the BCNF). Another reason is that the decomposition in 3NF can be obtained with a polynomial-time algorithm, while the decomposition in BCNF requires an exponential algorithm. 

I have tried with both singleton and non singleton nodes The end result should preferably not be a singleton. i.e something like: 

create a separate db and move your log tables there. set a max file size. create a trigger which checks row count or table size and limits inserts when value exceeded and/or deletes old data. create a foreign key constraint on an id column pointing to a table with a set number of rows. When your log table reaches the maximum it will stop. add a check constraint. 

If that value is unique and is used to identify a single row in a table ( in your example to uniquely identify a single client record) it would typically be considered a unique or primary key. If it has some meaning and you already have a separate primary key it could could also be described as a business key or natural key. Without a constraint to enforce uniqueness you cant guarantee the data is unique. You should be careful to avoid developing with that as an assumption. 

Create a user db with a decent file size, and set the file growth to be 100Mb or 1Gb. Ensure each table has a primary key defined in sql server. Not just a unique index. Access wont allow you to update a sql table without a primary key defined in the linked table definition. The equivalent of an access autonumber is an identity column. If you have staging tables with duplicates its worth adding an identity column as a pkey. 

MDX & DAX are query languages for OLAP cubes. MDX is used on multidimensional OLAP cubes and is used by many vendors. DAX is Microsofts own query language and can only be used in a tabular OLAP cube or within Excel. Assuming you are using Microsoft SSAS you must choose which type of cube you want when you install SSAS. If you (or someone else) has already installed SSAS then you need to find out if its Multidimensional or Tabular, and then use the appropriate query language. 

part, since it does not change the result of the query, but only slows down its execution. Another possibility is to rewrite the query by using a join instead of a IN: 

In your relation schema, there are three candidate keys: , and . Since, for instance, violates the BCNF, we can decompose the original relation in: 

The indexes on Warehouse will be on the primary and foreign keys, and on (idCustomer, warehouseLevel). where the warehouseLevel could be a string like 1, 1.2, 2.1.3, etc. With a scheme like this you could find easily: 1) all the warehouses of a customer, 2) the parent warehouse of a certain warehouse, 3) all the child warehouses of a certain warehouse, 4) all the descendent warehouses of a certain warehouse (with a recursive query). Moreover, if you have the need of finding also all the warehouses of a certain degree of a customer, you should add to Warehouse an attribute degree, integer, with an index on (idCustomer, degree). 

To bring a schema in 3NF, first you have to find a canonical cover of the dependencies. Then, you can apply either the “analysis” algorithm to find the Boyce-Codd Normal Form (which is more strict that the 3NF) or you can apply the “synthesis” algorithm to find the 3NF. In this case, both the algorithms gives the same results. Here is a canonical cover of the dependencies: 

In the book, "Garcia-Molina H., Ullman J., Widom J., Database Systems: The Complete Book. Pearson Prentice Hall, 2009", Chapter 5, "Algebraic and Logic Query Languages", on top of page 217 of the 2nd edition, there is a box titled "δ is a Special Case of γ", that explains how the duplicate removal operation δ is redundant, since it can be replaced by the use of γ. Simply group by all the attributes of the relation, without any aggregation. This will replace each group, consisting only of tuples wich coincide on all the attributes, with a single tuple. And this is exactly the semantics of the duplicate elimination operator. 

Staging: Tables are extracted into our DW exactly as they appear in the source system plus a few columns for meta data as you describe. The key one being the extract time. Transform: These tables have exactly the same schema as the end result dimensions and fact tables. At this point we are denormalising data from an OLTP schema into a DW star schema. Again it has fields for ETL meta data. The original extract datetime can travel with the records. Store: This our end result, a DW star schema which is read by our OLAP cube. To Load our data we compare the transform with the final datastore tables. New rows get inserted. If a row has changed we retire the old and insert the new. To do this we have fields for is_current, valid_from datetime and valid_to datetime. 

Note: it is possible to have multiple tabs open which can be connected to different databases or servers. If you have multiple tabs open and tiled it can get a bit confusing. As an added precaution you could disconnect and close other tabs to prevent any accidents. It is possible that a sql script can contain many sql statements, and could include another use database command which could change the connection part way through the script. You should read the script or search for this before executing scripts supplied by other people. One last warning: it is also possible to connect to multiple database servers simultaneously using registered servers. However unless you deliberately chose to do this its extremely unlikely you would ever encounter this even accidentally. 

I believe this would be possible with a relational database but throughput will be an issue. SQL Server In Memory Optimised Tables could be very useful for this. The best form of storing the data will be trade off between simplicity and storage efficiency. Given the sheer volume of data, If the observations were taken at the same second. I think it would make sense to save each engine in its own column. This would result in 25,000 rows rather than 500,000 rows for 1 seconds worth of data. Edit: However as the observation time will vary then it would make more sense to store each series in its own row. Although this will result in a massive volume of data, Storing each observed value in its own field will make reporting and analysis much easier. However long term I don't think this would be feasible. The next challenge is then being able to make use of this data. Realistically a human can't interpret that volume of raw data. And so Aggregating data would make sense, So for a given sample record the max, min, avg, std dev etc.