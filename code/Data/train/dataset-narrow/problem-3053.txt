It is not dangerous if you have enough data. If you have enough data, you can somehow estimate the distribution of the phenomenon in hand. If you find the distribution of your sample and its parameters, it means that you know everything about your phenomenon under study. If you are familiar with statistics and probability, you may know that whenever you have enough data, you can estimate the expected value of the random variable using the mean of data samples and standard deviation of the sample will be equal to the standard deviation of the random variable, again if you have enough data. If you have enough data, it means that there will be no difference between the value of mean and expected value also standard deviation of the sample and standard deviation of the population. So if you extract enough data, these values may be so close together. Moreover, the reason we are using standardization of data is that we want to have features with same scale. This is the main reason, consequently there is no need to find the exact value of mean and standard deviation. You may see among machine learning and deep learning practitioners that they may not do this operation because it's a bit time consuming. They just usually divide each feature by the greatest value of the corresponding feature among data samples. 

First of all, you have to shuffle your data because it seems that the model has learned a special pattern in the training data which has not occurred in the test data so much. After that, suppose that you get a validation curve like the current one. As you can see, Increasing the value of depth, does not change the learning. The two lines are parallel. In cases which each of the lines may have intersection, the upper line has negative slope and the lower one has positive slope, in the future on seen levels, you may want to increase the number of levels, not in this case. Having same error, means that you are not over-fitting. but as you can see the amount of learning is not too much which means that you are having high bias problem, which means you have not learned the problem so well. In this case means that your current feature space maybe has high Bayes error which means that there are samples which have same features and different labels. Actually the distributions of different classes overlap. There is something to argue about decision trees. If you have numerical features which are continuous, you may not have exactly same input patterns but they have overlap in their range. 

TFlearn is a modular and transparent deep learning library built on top of Tensorflow. It was designed to provide a higher-level API to TensorFlow in order to facilitate and speed-up experimentations, while remaining fully transparent and compatible with it. Even with TensorFlow, however, we face a choice of which “front-end” framework to use. Should we use straight TensorFlow, or TF Learn, or Keras, or the new TF-Slim library that Google released within TensorFlow. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Straight is really verbose while and both seem solid, but the syntax seems a little cleaner. One drawback to Tflearn is the lack of easily integrated pre-trained models. Actually there are so many answers for your question here and here and I quote some of them here. TensorFlow is currently the mainstream of deep learning framework, they are all the wrapper of TF. Whereas, Keras was released at the age of Theano, and therefore having a good support from Theano’s users. While TensorLayer and TFLearn are both released after TensorFlow. A good reason to choose Keras is that you could use TensorFlow backend without actually learning it. Plus Keras tends to wrap up the model deeply, so you don't necessarily need to consider the backend to be Theano or TF, which is a big advantage of Keras. It depend on what do you want to do, fast prototyping or something else? Keras : Many people are using it, easy to find examples on github. Suitable for beginner. Capable of running on top of either TensorFlow or Theano. Tflearn : Why no one discuss it? It is also a famous library, transparent over TensorFlow. High running speed. TensorLayer: Just release (Sep 2016), transparent over TensorFlow. High running speed. Easy to extend, suitable for professional, its tutorial include all modularized implementation of Google TensorFlow Deep Learning tutorial. TF-Silm: Just release (Aug 2016) similar with Tflearn, but no RNN layer at the moment (Sep 2016). The best deep learning framework is the one you know best. 

There is a paper called Spatial Transformer Networks written by Max Jaderberg et al. What it does is trying to find the canonical shape of its input by reducing transformations, like translation and rotation, or even diminishing the distortion of the inputs. It introduces a module which helps convolutional network to be spatial invariant. One of the significant achievements of this module is that it tries to enhance distorted inputs. Take a look at here. 

You have hard coded the output size, which is two, you have to change it to be a variable and during calling, you have to specify the size of your new output, which is four. 

You can also take a look at here for the labels in the imagenet. I guess you are right, there is no label for human in the data-set but there is something to notice. There are labels in imagenet like cowboy or some specific hats and other related things to human like shirt and t-shirt. You can take a look at here and also here. In the latter link Yosinski et al, has tried to show that the popular AlexNet has learned to recognize human faces although there is no label as human face in the imagenet data-set. In their paper, they have investigated that Convolutional neural networks may try to learn things that are distributed among layers or maybe not and they may not have special label in the training data. As an example, the face of cats and humans can be referred to. Moreover, as you can see here maybe the aim was attribute learning in large-scale datasets, as quoted in the last line of the page, as the reference.