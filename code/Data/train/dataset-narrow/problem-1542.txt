Ah I think this can be done with the setting as decribed at the bottom of the limit_req wiki: If delaying excess requests within a burst is not necessary, you should use the option nodelay: 

I am using nginx as a reverse proxy to my backend(s). Configuration is pretty basic, e.g. the core is just: 

My StartCom cert has and alt names for as well as . However after the latest chrome update, Chrome marks the SSL cert as valid for the first site, but not the second. Why is this? Does Chrome no longer allow multiple wildcards on a single cert? 

I am hosting a REST service which is sending appropriate cache-control headers. I use Varnish as a caching server in front of my webserver. However, a limitation of varnish is that it doesn't support caching HTTP POST and HTTP PUT. Is there any alternate caching server that will be able to cache these requests? I understand that caching POST is a bit tricky because you cannot just cache based on the url as a key like for GET; it needs to actually inspect the request body. In case of requests, there should probably be a limit on the size of the request body for it to be cached (so that big file uploads, etc won't be cached). Nevertheless I really want to be able to cache short HTTP POST, or at least the ones. 

in inside of the in either 000-default or default-ssl, it works perfect. However instead I would rather use the Ubuntu convention of putting new sites in a seperate file inside /etc/apache2/sites-enabled/. I have other sites inside of this path using directives. However, when I put the above line in a file in this path, it does not work. I just get a 404 when trying to access the site. Also, when I instead use something like 

One recurring problem with CORS is that the spec prescribes request headers get stripped from the preflight request (HTTP OPTIONS). However if the server requires authentication, this means the preflight request will fail (because the header does not get included) and it will not be able to receive the required header. The only way out seems to be to configure the server to not enforce authentication for HTTP OPTIONS requests. Is there any way in apache 2.4 that i can make conditional on the http method? 

In order to deploy content on tomcat, one has to include a file . Below an example of a that I used to deploy some static content through tomcat. 

Turns out that installing the package will automatically create some self-signed certificates, with an out of the box configuration file 

Turns out nginx has quite a lot of proxy redirect options to deal with this sort of problems. I ended up using something like this: 

In the web application I can simply refer to when connecting to the DB. This works fine in Debian/Ubuntu, but in Fedora it fails. The problem is that SElinux prevents from reading and therefore cannot resolve . When I disable SElinux, the problem disappears. 

My webserver hosts several subdomains (vhosts) of a website, say sub1.example.com and sub2.example.com. The only difference between these vhosts is the documentroot. Everything else is shared across vhosts. Now I would like to do the same for HTTPS, but of course ssl + virtualhost is tricky. The good thing is that my SSL certificate is valid for my complete domain. Hence I don't need to specify per-vhosts certificate. The only thing that I want to specify per vhost is the document root. The FAQ says: 

I had a thorough look at the machine that is running on 192.168.1.111, but I am quite sure there is no malicious software on there. What could be triggering this alarm? The only thing I can think of is utorrent local peer discovery, although I that is not exactly a "port scan". 

Or alternatively, is there a good way to add this script as a hook to the /etc/init.d/apache2 or /etc/init.d/httpd script? I am looking for a solution that works on both CentOS and Ubuntu. If I am totally approaching this the wrong way, any other suggestions are welcome as well. 

I still get a 404. Any ideas why the JkMount does no work outside of the even though other Locations do work like this? 

I am using Ubuntu 11.10 which ships with Apache 2.2.20 and openssl 1.0.0e so I think I should be good. However, I can't get it to work. I already have default and default-ssl sites enabled. If I add a virtualhost like I would do for HTTP: 

I use apache2-mpm-itk to run one of my web applications with a custom uid and gid, both for audit and security reasons. This works nicely, however I am not sure what kind of privileges this user would need. I am currently doing something like but that creates pretty much a regular user. So first a newbie question: how do I add a 'system' user that can that cannot be used for e.g. ssh'in or does not appear in the login screen on Ubuntu Desktop? And what kind of privileges would I need to give this user? E.g. I assume that www-data can write to the Apache log files, so it needs something more than a regular user. But at the same time I don't want to give it more privileges than necessary to run my webapp, definitely not root. 

I am running an Ubuntu server on EC2 ebs, and my application needs a lot of temporary disk space, allocated in /tmp. However, on ec2 the root drive which also contains /tmp is pretty small, around 10GB. All of the remaining disk space is mounted under /mnt. As a result, my application returns 'out of disk space' errors, because /tmp seems to be full. What is the best way to solve this problem? One thing I can think of is create /mnt/tmp and make a symbolic link 

I.e. the name of the upstream backend should have been replaced in the response header. Is this a bug in nginx or am I doing something wrong? 

So the problem appeared to be that the back-end server was not setting any headers for some files due to misconfiguration. When this happens, inserts a based on local configuration like in Apache. I have not been able to figure out what caused the different behavior between OSX and Ubuntu, but after we fixed the back-end server to always send a response header, the problem disappeared. 

This basically replaces the upstream name with the $host and removes the port. This worked in my case because I am hosting nginx on the default ports for HTTP/HTTPS. If nginx is running on a non-default port, something like this is needed: 

Is there a way to discover or specify the username I need to use to ssh in to an ec2 instance? I know how to select my key pair, but depending on the ami, you need to use a different username. For example, the official ubuntu images require you to ssh in with , the debian images with and the fedora images with . However, sometimes I just want to search an ami from the 'community AMI's', but then I don't know which username my keypair was associated with. So how is one supposed to know this? Is there a way to specify this when launching the instance? Am I overlooking something? 

I need an Ubuntu 11.10 server, but for reasons beyond my understanding, our host only preinstalls Ubuntu 10.04 LTS. So I decided to get a 10.04 server and use Ubuntu's do-release-upgrade to get to 11.10. I expected that do-release-upgrade would get me straight from 10.04 to 11.10. However, after running do-release-upgrade, I ended up with Ubuntu 10.10. So after reboot I had to run it again, and then it upgraded to 11.04, so then I had to run it a third time to get to 11.10. Is this expected behavior or am I doing something wrong here? 

I manage an application that contains a filestore in which all the files are stored with the filenames equal to their md5 sums. All files are stored in one directory. Currently there are thousands, but soon their should be millions of files on the server. The current server is running Ubuntu 11.10 on an ext4 filesystem. Someone told me that it is not wise to put many files in a directory, as this will create significant increase in lookup time and reliability (he had a story about max files a single dir could point to, resulting in a big linked list). Instead he suggested to create sub directories with e.g. substrings of the filename. However, this will make some things in my application much more cumbersome. Is this still true, or do modern filesystems (e.g. ext4) have more efficient ways to deal with this and naturally scale? Wikipedia has some details on filesystems, but it doesn't really say anything about max files per directory, or lookup times. 

One of my server daemons uses a lot of space in /tmp. Because I don't want to reboot my machine when the server runs out of disk space, I need to run a CRON script that removes old temporary files. What would be good a way to recursively remove all files and directories under /tmp which are more than 1 hour old from user , say ? Of course it should not resolve symlinks and start removing files elsewhere on the system. I am using Ubuntu 12.04 and will run this cronjob as root. 

However I am a bit reluctant to mess around with something that is used by so many linux programs and tools. I am not sure if every program will correctly resolve the symbolic link, and not sure what it would to to performance. 

I created a simple web application that consists of a dir with html, css, js. No server code. For reasons complicated to explain, my administrator insists on turning it into a .war file, so that it can easily be deployed on tomcat. Again, the application does not contain any Java code at all. I didn't create it with ant or eclipse. I tried creating an archive: 

I am running into an odd problem for which I am not sure if it is a configuration issue or a bug in nginx. My setup is a nginx reverse proxy which has Apache2 backend servers. The load balancer is pretty basic similar to the example from the wiki, e.g. simplified: 

How do I turn this into a script that enables this policy on my CentOS servers? I already enabled booleans for and but that didn't help. 

I would like to set a limit on the number of requests a single client IP address can make to my server, based on their IP address. Nginx has a directive. However, this directive will actually limit the average hits. If I set my limit to , it will actually allow only one hit every 10 seconds. Instead, what I need is an actual limit per minute. E.g. the client should be able to fire 3 or 4 requests quickly after each other or even simultaneously, but not more than e.g. 10 per minute. Is there a way to do this? 

Note the url has a slash in the end to note that it is a directory. Also note that Apache is "smart" by using the incoming request hostname in the redirect header. All fine so far. However, when using nginx with load balancing, this 301 is sent to the client without translating the nginx upstream name back to an actual server/domain. So the client will receive the following: 

Now, a problem arises for example when I try to request a directory in Apache, without the trailing slash at the end of the url. For example, a client requests: 

Using curl on a clean vanilla Fedora 21 to retrieve a site hosted via the cloudflare https service gives an error: 

Our web service software consists of a (precompiled) war file and a mysql database. We provide installation packages to deploy it on Ubuntu or Fedora, in a standard Tomcat7/MySQL/Apache2 setup using . Now some people have asked if they can try our software on windows. It should be possible, but I have no experience with deploying on an OS without a package manager. Is there a way to provide a self contained installer for windows that will include Java, Tomcat7 and MySQL? Something like wamp but then with tomcat and java instead of php? 

I have a complex web application that contains a lot of dynamic Aliases. I created a shellscript that updates the .conf file(s) with these aliases. Is it possible to call this shellscript from within the Apache conf? So something like: 

In Debian/Ubuntu, my packages depend on the package, to provide some self-signed https certificates, if nothing else. From the Debian page: 

Ah I solved the problem. Taking a close look at revealed that the version was hardcoded in the repository url, e.g: baseurl=$URL$ So I replaced "5.0" with "5" in every line and then did a followed by a which updated me straight to CentOS 5.7. 

So I ended up moving over from mod_jk to mod_proxy_ajp, which I should have done a long time ago. It is much easier to configure and works out of the box. When using mod_proxy_ajp, adding a tomcat site is as easy as putting a file insite my /sites-enabled/ containing: 

There are often backports available from more recent Ubuntu releases for the latest server release. For example, this repository seems to have some. Although they don't seen to be updated very recently. $URL$ You can of course also just grab the .deb for Quantal and install it on Precise using , but that is usually not recommended, due to potential changes in dependencies. Although I think in this case you'll be fine. 

The repositories are active and even my EPEL packages are being updated, but it seems that the person who created this instance somehow fixed it at 5.0. However, I really need to update to the latest 5.x. How can I get yum upgrade to work again? 

I purchased some (cheap) HTTPS certificates from StartSSL. They work great on all browsers I tested. According to this post they are even recognized in IE 7 and 8, which good enough for me. However, one user complained that his WordPress RSS client/aggregator stopped working. His website runs the latest version of Wordpress and simplepie, but it does not recognize the StartCom CA: 

How can I modify the policy to allow httpd to read ? Background: My web application connects to a mysql database. Instead of hardcoding the credentials in the code, they are stored in a mysql group in : 

Have a look whats inside these directories, and if you're confident you want to trash it, manually remove the directories. In my case: 

For whatever reason I managed to mess up Apache quite bad (Ubuntu 12.04). The output below complains about configtest failing and a segmentation fault. However, I can't find anything in . How can I debug this "configtest"? 

I would like to remove any packages that I installed from third party repositories. I recently installed some backports from PPA repositories, and I suspect one of them broke my Ubuntu 12.04 server system. Is there any way I can list all currently installed packages in that are not available in the repositories, or, which version is higher than the one available in the current repositories? 

Mostly depends on which Apache modules you want to use. I think worker is generally the default choice, but some (older) modules require forking and depend on prefork. If you have no preferences, I recommend you go with the preferred dependency from your OS distribution. Ubuntu for example will by default install mpm-worker when you install Apache2. 

I have a service daemon that creates a lot of temp files. Recently my server died, because a malicious user managed to flood /tmp and fill up the disk. I have taken some measures to actively clean up the temp dir, but additionally I would like to constrain the max size of this applications temp dir. Is there any way I can create dir, say, that will never be larger than e.g. 10G? I know I can set disk limits by-user, but I just want to limit this tmpdir; the application should always be able to write elsewhere. I am running Ubuntu Linux 12.04. edit: All of this should eventually be wrapped up in an installable Ubuntu package though. So I don't think I want to rely on modifying the partitions, unless I can somehow simulate it. 

Now my service is computationally very heavy, and I would like nginx to limit the number of active parallel (simultaneous) requests to a single upstream backend to e.g. 10. I looked into limit_req module, however this module only seems to focus on incoming requests per minute. I specifically would like to limit the number of active backend connections; i.e. take into account if requests have already returned or not. Is this possible at all? In Varnish this can be done using e.g.