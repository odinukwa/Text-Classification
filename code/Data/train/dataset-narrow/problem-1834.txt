We have a solution for this at work where we use SSH host certificates signed with a known CA to get rid of this problem. If you are not familiar with ssh certificates you can read about it in the ssh-keygen manual and also there is a lot of nice guides out there: 

The cloud part In order to have the ssh host keys unique and generated at the host we have the private host CA key stored in the images we use. When the image boots the first time sshd generates new keys and then we have a "firstboot" script that basically does this: 

Then all you have to do on your side is to trust this single CA key by adding it to your either personal or global known_hosts file. You will now be able to log in to the machines without and having them automatically trusted. This may give you some security concerns because the CA is available inside the image at first boot, but it works for us because we have very good control over the infrastructure and we think its good enough to remove it once the certificates are generated. Possible alternative solution using SSHFP Another solution that just came to mind is the use of SSHFP records. If you have a DNSSEC signed zoned you could have the VMs report their key by some method to your DNS server and then publish their keys in DNS using the SSHFP record type. The OpenSSH client will trust these records if the zone is signed. I have writted a short guide on this: 

But if youÂ´re not used to this kind of stuff I would recommend to have a loo at BeeGFS or NFS. NFS have support of of the box in *nix operating systems and BeeGFS should be pretty easy to set up. 

Yes you should add NS records for all of your DNS servers in your zonefile. You should be able to ask all of your DNS servers: "what are the DNS servers for example.com" For example: 

Like the comment says OpenSSH is pretty strict about what permissions it allows on chroot directories (for good reasons). So the solution is to have a subfolder with the permissions you want to use for your particular case. 

This post is very unclear and not very specific. It also contain several questions. But I will try to give you some pointers where you can start to look. The first one is how you should handle the storage at the storage server. If you want to have a single namespace for the whole amount of disk space it would be easier to have some sort of raid solution. This could also be good for data integrity or data availability (or both) depending on what raid level you choose. There is multiple ways to implement raid (both in hardware and software) so you need to investigate what suits your needs. If you want to have it simple I would recommend some sort of software raid that doesn't require any specific hardware to start with. The other question is how you will share this data among the cluster nodes. The simplest way to do this is probably by setting up a NFS server on the storage server. But there is multiple network file systems out there with different characteristics. Please have a look at: 

The tracert eventually completes at hop 16. The addresses are all unique, and the timed out hop only occurs on hop 5. After the att.net, the hops are all some long hostname at above.net, before finally resolving to another IP, and then, finally, the hostname. 1) What does this timeout effect? 2) Is there anything I can do about this - or it something I've misconfigured? Dedicated CentOS/WHM server. (The other IP's assigned to the same machine works without this timeout...) 

Migrated from Server A to Server B from different providers, with different IP's for hostname, nameservers, etc. I've updated the nameservers for a number of domains, and they seem to be propagating, resolving, etc... for a short period of time, and then later, it looks like some of the domains have "jumped" back to the old nameservers... Any idea what might be causing this? Solutions? 

I am trying to migrate from my old server (Server 1) from provider 1 to a new server (Server B) at provider 2, keeping the process as seamless as possible. One of the first things I noticed in the test folder I migrated is that several PHP functions are not supported with Server 2 -- apache_request_headers(), for example. This is supposedly because PHP was not compiled as an Apache module on Server 2. There might be other differences that may cause fatal script errors, that I haven't yet found. Both servers run CentOS with WHM. Is there a way to configure the new server to be exactly the same as the old, without this ad hoc checking? 

PHP, WHM, and several other services are already installed on a CentOS x64 server I am trying to migrate data to. Many of my existing PHP scripts are dependent on PHP's apache_request_headers() function, which the current server's PHP configuration does not support. Apparently, compiling PHP as an Apache module is one solution, but are there other ways to enable this (without uninstalling PHP, reinstalling, etc., and all dependent services), perhaps as easy as modifying php.ini, somehow? 

For some reason, traceroute to hostname (and any domains hosted on this server) always times out on hop 5, right between the two shown below: 

Is there a way to know for sure that your remote-ly hosted server is actually a dedicated, and not just a virtual, "faking" to be a dedicated? What are some possible shell commands to run to test this? 

Is there a way to migrate from an old dedicated server to a new one without losing any data in-between - and with no downtime? In the past, I've had to lose MySQL data between the time when the new server goes up (i.e., all files transferred, system up and ready), and when I take the old server down (data still transferred to old until new one takes over). There is also a short period where both are down for DNS, etc., to refresh. Is there a way for MySQL/root to easily transfer all data that was updated/inserted between a certain time frame?