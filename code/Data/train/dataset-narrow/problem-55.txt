This effect is called vignetting. The cases you talk about are specifically natural and pixel vignetting as explained on the wiki page. Digital cameras compensate these effects, but optical vignetting has to be done in image software for DSLR's with interchangeable lenses. For example if you import an image to Lightroom, there's "lens correction" option, which removes vignetting, distortion and chromatic aberration caused by the lens based on the lens profile. 

However, in smallPt there's no Fresnel approximation (e.g. Schlick's approximation) but the surface reflectance is simply multiplied with constant term which is wrong. Also 0.999 specular albedo is very high for any real material. Below is a table of specular reflectange of various metals & dielectrics at normal incidence for reference: 

This is all about reducing noise in unbiased manner. There are also denoising techniques to further reduce noise in the rendered images. 

Incidentally I implemented something similar for similar hardware some 20 years ago (: IIRC, I calculated 1D look-up-table using acos for parallel projection of a cylinder and just scaled it for each scanline to reduce computation cost. The LUT just gives you x-coordinate offset to the texture and you add constant offset for each scanline for the rotation. For y-axis the input texture can be preprocessed to have proper spherical distortion applied and you can just linearly scale the texture vertically. Though it's not that much extra computation to do it run-time either. This doesn't give you proper perspective projection but might be enough for your purposes. Edit: Didn't notice you said you can't afford LUT. You could do the table look-up every N pixels and linearly interpolate inbetween. This was a common method in the past to do affordable perspective correct texture mapping for software rasterizers to avoid div/pixel 

You can calculate the world position of the pixel on near plane quite easily by first defining Normalized Device Coordinates (NDC) for the point and then transforming the NDC back to the world space. You can calculate NDC for your point as follows: $$v=[2*(x+0.5)/width-1, 1-2*(y+0.5)/height, 0, 1]$$ I'm using 0 here for the z-component assuming near-plane in NDC is 0, but it could be something else as well, so check with your projection matrix what values comes out at the near-plane. This vector must be then transformed to clip-space by multiplying by near-plane distance: $$v'=np*v$$ Now that you have clip-space coordinates, you just need to transform this back to world space with the standard clip$\rightarrow$world transformation matrix: $$v''=M*v'$$ 

There is spectral rendering, where you can quantize the visible wavelengths from ~390nm to ~700nm to N discrete wavelengths instead of the standard 3 for RGB. Then if you had to model say a prism, you would get more realistic distribution of the spectrum. Light has also property of polarization that you would need to model for increased realism. I don't know if this is being modeled in any existing publicly available rendering engines and how would you represent it exactly. Light is electromagnetic wave with two orthogonal electric and magnetic components, which may have different amplitudes and be also out of phase potentially resulting in elliptical polarization. The polarization would be a relevant propery for example to model multiple specular reflections from dielectric surfaces, or modeling polarizing filters used by photographers on cameras. Both spectral rendering and accounting light polarization would come with the cost of performance and higher memory usage. 

Instead of sampler3D, you should use isampler3D to sample integer formats as described in the GLSL documentation. 

There are different numerical approximations you could use: A simple solution is to use brute-force Monte Carlo integration. Distribute $N$ random points on the polygon and calculate the number of points inside the 3D polyhedron $N_i$ using ray tracing. The area inside the polyhedron is $A_i=A\frac{N_i}{N}$, where $A$ is the area of the polygon. To improve convergence of the above algorithm, you can distribute the points on grid on the polygon and jitter the sample positions within each grid cell. This is called stratified sampling. Further improvement would be to perform the MC integration in 1D instead of 2D. Pick a sample line $L$ that's on the polygon plane and a sample direction $D$ that's on the plane perpendicular to $L$. Select sample line segment $L_s$ that bounds the polygon within $D$-directed lines (min & max of the polygon projection on $L$ for tight fit). For each sample on $L_s$ first calculate the line segment $S$ that intersects the polygon in direction $D$. Next calculate the length of $S$ that's inside the polyhedron $S_i$. You can calculate $S_i$ using ray tracing, by starting from one end of $S$ and marching along $S$ to intersection points of the segment with the polyhedron, summing up the segments of $S$ inside the polyhedron. More specifically, you need to first check if the start point on $S$ is within polyhedron and set the "inside" state accordingly. Then for each intersection flip the state and add the length from previous intersection to $S_i$ if the state changes from inside to outside. The approximation of the polygon area inside the 3D polyhedron is: $$A_i=\frac{|L_s|}{N}\sum_{i=1}^N S_i$$ 

I think it's best to first implement basic brute force Monte Carlo path tracer to serve as unbiased reference before looking into the more advanced techniques. It's quite easy to do mistakes and introduce biasing that goes unnoticed, so having simple implementation is good to have around for reference. You can also get some really nice results by applying path tracing to participating media but that gets slow really fast :D 

Many of the real-time GI solutions have some precomputation involved, which enable real-time lighting changes but limit the geometry changes to be non-real time. E.g. in cone tracing you mentioned there's the non-real time scene voxelization process. Here are few such real-time solutions that come to mind in addition to the ones you listed. 

You can look into alias-free volumetric sampling algorithm by Huw Bowles for potential solution to the ray marching aliasing issues. The basic idea is to snap your samples to planes based on the ray direction that's best explained with this Shadertoy demo. 

In computer graphics the effect is called "atmospheric scattering", that's sun light scattering from atoms and other particles in the atmosphere. In modern computer games atmospheric scattering is commonly modelled with Bruneton sky model by Eric Bruneton and is most prevalent in the clear sky due to long view distances, but also visible for distant objects on ground as you have observed. 

Another improvement to handle the gaps and overlaps is to use bitmask soft shadowing algorithm, where occlusion bitmask determines the occluded areas of the light source. The most recent paper I have seen on the topic is "Real Time Area Lighting. Now and Next" by Sam Martin from 2012, where the cost was several milliseconds for a single light, but it's definitely worth the read if you are interested of the topic, to get general idea about the state-of-the-art real-time shadow techniques. 

I think the mention of "global coordinates" is a lead to the next lecture of learning different coordinate systems (object, camera, etc.), so you are kind of jumping a head I suppose (: The points could indeed be given in local coordinates, which would then require definition of transformation from local to global coordinate system, and understanding how this transformation is performed. This transformation is usually defined as a 4x4 matrix, and the 3D points are defined as homogeneous coordinates [x, y, z, 1], which are multiplied with the matrix to bring them into global coordinate system. 

$E_n$ is illuminance (or irradiance), $L_i$ is luminance (or radiance) in direction $\omega_i$ and $d\omega$ is differential solid angle. TL;DR: $d\omega_i$ is the area of a pixel in the cubemap projected onto the unit sphere. And then a longer explanation (: Solid angle is essentially the projected area of an object on a unit sphere and has unit of steradians (sr). Entire unit sphere has surface area of $4\pi$ and hemisphere has the area of $2\pi$, which comes up a lot in lighting calculations due to integrals over spheres and hemispheres respectively. If you would want to calculate solid angle for an object and point $p$, you would project the object on the unit sphere at $p$ and calculate the area of the projection. Further the object is from $p$ smaller the solid angle gets. As an example, we can calculate solid angle for Sun as seen from the Earth. We know that Sun has about 0.52 degrees subtended angle from Earth, so we can calculate that the solid angle is: $$2\pi*(1-Cos(0.52/2))\simeq0.00006469sr$$ So the differential solid angle is just an infinitesimally small solid angle. If you would numerically integrate over a sphere ($4\pi$ steradians) by taking $N$ uniform samples on the sphere, your $d\omega$ would be $4\pi/N$. With integrals $N$ goes to infinity, while with numerical approximation of the integral $N$ is some "sufficiently large" number. However, in this paper you refer to they perform integration by rendering to a cubemap at $p$ and then iterating through all the rendered pixels. But because these pixels are rendered onto planes (cubemap faces), their projection on the unit sphere isn't constant, so $d\omega$ needs to be weighted by the position in the rendered image, which they explain in your second quote: 

Also Metropolis light transport (MLT) is a more advanced path tracing technique that converges even faster to the ground truth by mutating existing "good" paths: 

It's only more intense if $d\phi$ remains constant. However, for light source with constant luminance the flux is a function of $cos(\theta)$, i.e. depends on the light's angle of incidence with the surface and cancels out the term. Edit: I feel the need to clarify this a bit because you say that: 

Your equation for the absorption-only model is correct: You multiply the luminance $L_x$ at the surface by the transmittance of the participating media between eye and the surface. For homogeneous media you can calculate this analytically using Beer-Lambert law: $$L_y=L_xe^{-\mu|x-y|}$$ For heterogeneous medium you can calculate the transmittance by ray march through the medium using for example Woodcock tracking explained in the paper you referred. 

PBR isn't just a feature you "add to a rendering engine" but an entire philosophy how to approach solving rendering problems. This seems to be a common prevailing misconception when people talk about PBR. Quite often people assume that when they have implemented physically based BRDF (e.g. GGX) into their engines, they are "done implementing PBR". They are not - they are barely getting started. Let's take some of the topics you list and how do you approach implementing them in the "physically based" way. Depth of Field, Lens Flares, Motion Blur and Bloom & HDR for example. For years these effects have been implemented and slapped as post effect on the screen with some adhoc factors exposed to artists. However, as you know these are effects happening in the camera system and they should be exposed and implemented considering how the light interacts with this system. How camera aperture, shutter speed, ISO, camera sensor and lense system influence the effects and how they should be implemented and controlled considering these physical characteristics of the camera. This same principle applies to all the aspects of rendering when you go PBR. For example shadows you mention, how would you implement shadows in physically based way? First of all you would have to consider all lights as area light sources as opposed to infinitesimally small lights that people have been doing for ages. Recently there has been push to implement physically based area lights in real-time which is an important step towards more complete PBR, but they are completely omitting proper area shadowing. This is a difficult problem in real-time rendering and there has been only little research done in the area of area shadows. "Real Time Area Lighting - Now and Next" by Sam Martin is a good thing to read if you are interested in the topic. Talking about dynamic reflections or lighting in general. How should all the different lighting techniques such as SSLR, local/global environment maps, (textured) area lights, SSAO, GI, etc. be implemented and integrated together to form a cohesive lighting system? That you doesn't do double counting and that they are properly accounted in diffuse and specular lighting calculations. When implementing real-time PBR algorithms, you will eventually have to introduce some bias due to performance constraints and resort to some kind of approximations. But the important thing is that you know what the real result should be and are aware of the compromises you are making. PBR is a paradigm shift from the classic artistic observation-based real-time rendering algorithm development towards more scientific way of solving the rendering problems and this shift in mentality and the direction it pushes you helps you to get more realistic results in the end. 

What "ideal" means in this context is that there is no divergence in the direction of light reflection vectors (i.e. no roughness) but that they are all considered to be perfect reflections from an optical flat surface. However, even for optical flat surfaces Fresnel equation is still applied in BRDF evaluation that changes specular reflection and is potentially wavelength dependent, which is a common characteristic with metals. 

You need to convert luminous flux (lumens, $lm$) to luminance ($\frac{lm}{m^2sr}$) for the rendering equation. For that you need to know the physical properties of the light source. For example luminance $L$ of spherical light source of radius $r$ radiating over $4\pi$ solid angle and luminous flux $\phi$ has luminance: $$L = \frac{\phi}{4\pi^2r^2}$$ I think it's better to define lights straight using luminance instead of luminous flux though since you can simply measure luminance with spot meter instead of using cumbersome special equipment or be limited by values provided by manufacturers in illuminant packaging. Regarding distance attenuation, all lights in PBR should be area lights and there's no explicit attenuation formula for lights because the distance attenuation emerges naturally from the rendering equation. It's also better use photometric units instead of radiometric units for RGB-rendering (not spectral/monochromatic rendering). In photometric units the extraterrestrial sun luminance is ~1.9B cd/m^2 (don't try to measure sun with spot meter though!). If you want to use radiometric units, you need to take the spectral power distribution of the sun and integrate that with CIE color matching functions to get any reasonable values rendering.