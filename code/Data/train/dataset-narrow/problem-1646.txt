To the best of my knowledge you cannot change file system mount parameters without re-mounting it. By the way, the article you refer to cites shred man page, which says: "In the case of ext3 file systems, the above disclaimer applies (and shred is thus of limited effectiveness) only in data=journal mode, which journals file data in addition to just metadata. In both the data=ordered (default) and data=writeback modes, shred works as usual." Have a look at (or post) your /etc/fstab. Chances are you don't need to change journaling mode on the filesystem. 

Ad a). The file was deleted, possibly by process 2147 itself. It's a common practice if you need a scratch file (not a log file) to create a file, get a handle to it and then delete it. A delete will remove entry in the directory in which the file was created, so it is not possible for other processes to access it. A handy scratchpad that no-one can look into. When the process closes the file and there are no more handles to it, the file system will mark blocks occupied by the scratchpad file data as unused. If it really was a log file, it could have been deleted (e.g. by accident), so the process still writes to its log, but no one else can read it (think: write only memory ;) ). Ad b) You cannot access the contents of the file in the normal way, i.e. accessing file system in a normal way. You would have to start accessing the data below the filesystem, looking for blocks which hold scratchpad's data. If you want to try that, then mount the filesystem holding the deleted file read only as soon as you kill 2147, or you risk the data being overwriten once 2147 quits and the blocks holding its data will be marked as unused. How exactly find the data blocks you need? Sorry, cant help you there :(. 

I would download necessary package(s) on the installation server, together with the public key it was signed with. Then I'd use in to import the key, and download and install the package(s). Then you can use yum to install , because only now it will have its prerequisites installed. You could also add needed repositories to , to be able to pull updates later. 

etc Can you log in to the NAS and monitor its performance too? One case I encountered was a NAS spawning multiple NFS daemons and dying under the load when a client connected. 

Connect to HMC or ASMI and see what they say. You will get error description there, which may be comprehensible (e.g. a fan died somewhere) or utterly cryptic ("there's no detailed description for error code XXXXX"). If it's not something that you can resolve (e.g. "No AC detected for PSU 1", resolvable by re-plugging a loose cable), then call support. Anyhow, there is no error code on the LCD display, so it seems that machine is currently happily running. Edit: Decoding of the panel hieroglyphs: -- Function Code 01. Function description: 

You try to install package postgresql-libs-8.4.4-2PGDG.el5.x86_64 and postgresql84-libs-8.4.7-1.el5_6.1.x86_64, at the same time. This indicates, that you have incompatible repositories enabled. If it is a RHEL installation, I would recommend disabling everything but the default repositories and once again running . If you want postgresql that's newer/different than the one provided by Red Hat (warning: this makes your config unsupported), then you'd have to play a bit more with disabling some/all RH repos and enabling non-RH repositories one at a time, and run . At one time you'll get combination of enabled repositories with no conflicting postgresql packages and the command will succeed. 

A stab in the dark: Your Samba server is in a Workgroup group. Your XP client is in an AD domain. This may cause problems. Do you have some entries in Samba logs when the client tries to access the share? Does it ever contact the server? At what stage the negotiations between the client and the server break? 

How much RAM do you have in your VM? F15 installer is comfortable at about 1GB. Gnome-shell uses video acceleration for its functionality, but from what I read in this release there's a fail-back mechanism for environments that don't provide hardware acceleration. 

Apache is running multiple processes to have them ready when a client request comes in. Spawning a server process is slow, so it's best to have one waiting for a client. For memory usage, you should take into the account RES size (as displayed by top), which is the amount of physical memory used by the task. Why do you think you have too many apache2 processes using too much memory? What do you expect? Why do you expect so? 

This will omit files with names starting with . (a dot). If you have any (check it with ls -l /var/www/abc) you can do 

If you really insist on having a single system disk, it doesn't change the layout. On (each of) the system disks I'd have a separate partition for /boot filesystem. The rest of the disk(s) I'd made into an LVM physical volume. I like to have OS filesystems that can fill up on separate logical volumes (/tmp, /var, /opt if something writes logs in there). So that would result in /boot (mirrored, no LVM) and /, /tmp, var and possibly /opt filesystems, each on a separate logical volume on a mirrored disk. On each of the other disks I'd create a single partition of the type Linux raid and create appropriate RAID arrays (one RAID 10, one RAID 0). On each of the arrays I'd create a single partition of the type Linux LVM and make two separate volume groups, one for redundant and one for non-redundant data. Then for each file system you plan to make I'd make a logical volume. In each volume group I would recommend to leave some space unused, so that you can do an LVM snapshot and do fsck of a file system without bringing the server down. I would also disable automatic fsck of all filesystems (tune2fs -i 0 -c 0 /device/name). Rationale 1) Mirroring of the OS disks. Failure of the system HDD brings down the whole machine. Your data is protected, but your production stops until you can bring a replacement disk and reinstall / restore the OS. In a production environment it is usually cheaper to have one more disk installed. 2) Partitioning disks for RAID arrays. All the servers I use have partition tables. You may use just whole disks as RAID / LVM volumes, but then you end up with some machines that have partition tables (stuff on /dev/sdX1) and some, which don't (stuff on /dev/sdX). In case of a failure and need for recovery under stress I like to have one variable less in the environment. 3) LVM on the RAID arrays LVM gives two advantages: easy changes of filesystem sizes and ability to fsck filesystems without bringing the whole server down. Silent data corruption is possible and happens. Checking for it may save you a lot of excitement. 4) tune2fs -i 0 -c 0 Having a surprise fsck of a large filesystem after a reboot is a time-consuming and nerve-whacking affair. Disable it and do regular fscks of LVM snapshots of filesystems. A question: /opt/backup is where you plan to keep backups of your production environment? Don't. Have the backups somewhere else from the machine. A malicious program, a mis-spelled command (e.g. ) or some water spilled in / flooding the wrong place will leave you without your system and with no backups. If all else fails, have two external USB disks for backups, still better than a partition inside the same box. 

NAT definition rules; routing rules (if some packets leave via eth0, and some via eth1 things may get strange); 

Pay the fee and have the DNS records updated. Change the provider to one who doesn't charge you for such minor service. Set up your own DNS servers. 

Identifying the repository What method do you use to access the repo? If it's , then look for , the path to the repository will be inside. If it's HTTP/HTTPS, then look into your web server configuration file. If you access it using protocol, then I'm afraid you'll have to figure its location by yourself ;). Exporting the data Stop the server used to give access to the repository and make sure no one is using it locally (via ). You don't want to miss some last-second update to the code. Then you can run or , whichever you prefer, to have a ready-to-move set of data files. Additionally, save the files responsible for authentication to the repository. You may want to re-create corresponding access hierarchy in the new location, or simply know in 2 years, what users had access to which parts of code. If, for reasons incomprehensible, you want to drop the history of the project(s) within the repo, then simply check out the latest version(s) of project(s) and forget you've ever been using a version management system. Removing the repository and Subversion After you migrated the data off the server being decommissioned (at least 2 independent data copies recommended), and, preferably, have it imported into their new home (to verify that you can actually use whatever you exported) you can remove binaries used by it. It means the subversion packages plus whatever was used to access the repository. If you use , then you can simply remove it, using standard procedures for your distribution. For HTTP/HTTPS based access you need to remove modules responsible for Subversion interface, but may as well mean removing the whole server (if its sole role was to let developers access the code). If the web server stays, then remember to comment out/remove section of the configuration, that was responsible for interfacing the server with Subversion. The last step is a simple on the Subversion repository directory.