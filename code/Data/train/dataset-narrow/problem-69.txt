A note on decompression costs Although the compression algorithms for the schemes described above have a moderate to high evaluation cost, the decompression algorithms, especially for hardware implementations, are relatively inexpensive. ETC1, for example, requires little more than a few MUXes and low-precision adders; S3TC effectively slightly more addition units to perform the blending; and PVRTC, slightly more again. In theory, these simple TC schemes could allow a GPU architecture to avoid decompression until just prior to the filtering stage, thus maximising the effectiveness of internal caches. Other Schemes Other common TC modes that should be mentioned are: 

I'm fairly certain your problem lies in not handling so-called degenerate Bezier patches correctly, in particular (as Joojaa noted), the computation of the surface normal. I say "so-called degenerate" because, geometrically, the surfaces are often perfectly well behaved. It's just that some assumptions people frequently make regarding the parametric equations may not hold. Books such as Gerald Farin's, snappily titled, "Curves and Surfaces for CAGD. A Practical Guide" will give more details, but I'll try to summarise two simple cases. Now assuming your Bezier is defined as $\bar{B}(u,v)$ the usual two causes of problems are: Zero derivatives: To compute the normal at $(a,b)$ one normally (pardon the pun) computes the two derivatives, $\frac{\partial }{\partial v}\bar{B}(a,b)$ and $\frac{\partial }{\partial u}\bar{B}(a,b)$ or scaled versions thereof, to obtain two tangents and then take the cross product. (Implementation note: Since we can use a scaled version of the tangent in the calculation, we really don't have to calculate the actual derivative. For example, especially at the corners, differences of control points can yield a scaled tangent. However for brevity in this discussion we will assume the actual derivatives). A common occurrence, at least at the corners, is that the 1st partial derivatives at the location can be zero, which leads to an incorrect normal, i.e. a zero vector. In the case of the tops and bottoms of the teapot, one whole edge of a number of the (bicubic) Bezier patches has been collapsed to a point, i.e. all 4 control points are the same, and thus, say $\bar{B}(0,v)==\bar{B}(1,v)$ and $\frac{\partial }{\partial u}\bar{B}(0,0)=\bar{0}$. The surface, however, is completely well behaved so you can simply choose another derivative that starts at that collapsed point. In this case, say, choosing $\frac{\partial }{\partial v}\bar{B}(1,0)$ for the second tangent. Having said this, you still have to check that your first derivatives are not zero for another reason (e.g 2 or 3 coincident control points), in which case, you can fall back (thanks to L'Hopital's rule) on the second (or if that's zero, even the third!) derivative(s) to obtain valid tangents. Parallel tangents: Another similar problem can arise if your two tangents, are parallel. - Farin has a good example in his book. In this case, I think you may need to look at using something like $\frac{\partial^2 }{\partial u \partial v}\bar{B}$ or, possibly, just fall back to using a small UV offset to approximate a vector. 

In the book Computer Graphics Principles and Practice, they use the term specular reflection when they want to imagine things resembling a mirror and glossy reflection when things like a polished door knob or an orange skin. The charts shows you exactly that. When a material has more specular color, it should have less diffuse color due to the conservation of energy. That is, the sum of the light reflected specularly and light absorbed and emitted in random directions must be less than equal to 100% (the amount of light incident on the surface). Hence when you increase the specular color the material tends to go white or have a slight tint of the color like in metals. Where as glossy surfaces can have more diffuse color but show a specular highlight like the surface of an orange skin. So assuming the CGPP's point of view, we can say in pure specular reflection, the diffuse part is much less than the glossy part. Where as in glossy reflection the diffuse part is usually greater. 

All the books and reference I have read say that the view vector is calculated by subtracting the point where eye is at, from the point where we want to calculate light. But since, eye is at (0, 0, 0) the view vector would be just the negative of the point where eye is at? Is my understanding correct? 

This question perhaps has been asked and answered a thousand times, and yet I haven't found any that satisfy me. The reasons are often these: 1/ You need a 4 dimensional vector to work with 4x4 matrices. 2/ You need to have z stored in some way so that you could later use it for z-buffering, etc. 3/ You need a matrix that is independent of z so that you could multiply matrices together and save computations. Instead of having to multiply each vector with each of the matrix, you could multiply 1 result matrix with each of the vector. Here's the problems I have with each of them. 

I think you are confusing all the binding targets thingy. From what I see your vertex data is coming from compute shader after some processing and now you want to pass it to the vertex shader. You can create a buffer object once, use it as an SSBO for use in a compute shader then use it as a VBO for use in rendering. i.e 

This is the simple case for just a projective transformation. Consider the vector going through 5 or 6 transformations and in the last comes the projective transformation. If we pre-multiply all these transformation to create a single matrix, you will notice that now when we multiply the vector with this combined transformation matrix the division factor isn't just a simple value. The 4th row of the matrix won't be as in the standard projection matrix. It might have changed due to multiplying all the transformations together. Now when you multiply that 4th row with the 4D vector, you will get your value by which you need to divide now. 

You might find the 1983 paper that introduced this**, i.e. Lance Williams' "Pyramidal Parametrics" informative. You can ignore the scheme, in Figure 1, he used for the layout of the MIP maps as I doubt any hardware, at least in the last 20+ years, used that approach. ** (Actually, Williams *may* have described the technique at an earlier SIGGRAPH (1981) as part of tutorial/course but I've never been able to get hold of a copy) 

To align vectors, you are applying a rotation. If you consider a simple rotation matrix, e.g. rotation of angle $\theta$ around Z axis, then you will trivially see that its inverse matrix, a rotation of $-\theta$, is the transpose, since $sin(-\theta)=-sin(\theta)$ and $cos(-\theta)=cos(\theta)$. An arbitrary rotation matrix, R, can be constructed by multiplication of other rotations, e.g $R = A \cdot B$. If the inverse of A and B exist, then $R^{-1} = B^{-1} \cdot A^{-1}$. Similarly we know $(A \cdot B)^T = (B^T \cdot A^T)$. Put these together and you have your answer. 

I can't really answer this without the context so post some links where this is done. What I can tell you is that the general formula for viewport transformation as done by OpenGL is given as (taken from wiki) 

That's what I want to hear about more. Why "non-orthogonal" ? I can't see any problems with making the axes orthogonal? What does making the axes non-orthogonal give us? 2) After the above one has been answered. How is this all tied up to rotational matrices? How can we achieve gimbal lock through the matrices when they only rotate a given point/vector around global axis. For example If i multiply a column vector with a Rotation matrix around X-axis and then with It will rotate around the global X-axis first, then global Y-axis second. So how can I achieve the lock situation using matrices? EDIT:- To make it more clear I've also heard about rotation orders like in the order when Y axis rotates then and rotate with it but when rotates only rotates with it. thus making the axes non-orthogonal. I am assuming that's what it means by non-orthogonal mentioned in the wiki article. Here is a picture. 

I don't even know where to start. So far, the only progress I have gotten is checking the z value of each vertex of each triangle. If all three vertices' z values > absolute value of far or < absolute value of near, then I dont draw that triangle. It seems wrong somehow, though I dont get why. Im using the right hand rule, if that info is necessary. 

I am reading a model from an obj file and draw it using glDrawElements. For some reasons, only a small part of it is drawn, even though I dont even have back-face culling turned on. 

Obj files give pre-calculated normals which are in model coordinates. So I wonder if they have to be transformed into eye coordinate (where I use them to calculate lighting)? I'm thinking yes because it makes sense, but only rotation and scale because translation doesnt really make sense for free vectors. 

I Recently posted this question on SO but didn't got any response so i thought to post it here since it's somewhat related to Raytracing. I am making a real time ray tracer in OpenGL using Compute Shaders for my project and was following this link as a reference. The link tells to first draw a full screen quad, then store all the individual pixel colors gotten through intersections in a texture and render the texture to the quad. However i was thinking can't we use Frame Buffer Objects to display the texture image instead of rendering the quad and save the over head? Like I save all the colors using ImageStore and GlBindImageTexture in a texture, then attach it to a FBO to display it. And since I won't be using any rendering commands I won't be causing a Feedback loop as in writing and reading the same texture? Here is the snippet