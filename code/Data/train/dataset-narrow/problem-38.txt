I am trying to scale and repeat a Cubemap with Latitude-Longitude mapping layout just as you would do with classical UV mapping but without any interesting result. This should be used as a skybox. This comes from the fact that the coordinates are in 3D space and we can't apply this simple formula How would you handle such features : scaling which involves tiling and offsetting. 

I am trying to achieve a special texture stretching effect in my shader. Here is how I define my uv before sampling my texture, nothing really special. 

I am trying to implement the Parallax Refraction effect explained by Jorge Jimenez on this presentation: $URL$ and I am facing some difficulties. Here is a screenshot of the interesting part. 

As I said it works well but when I scale my shape in the X axis the result is not correct. Here is a illustration of the issue. 

This gives the standard tiling/stretching and offset behaviour when you tile/stretche a clamped texture as you can see in the image below. First is normal, second is offset and last is stretching. 

Huge thanks to @MJP who answered this. The aim is to avoid the simplification made when using tangent space normals. Here is the paper : Blending in detail But only implement equation (4) which gives you this. 

Why this background shows through at the triangle edges I do not know. However, the second problem is identified below, so I recommend you fix that first and then the first problem can be analysed in isolation without the additional distractions. 

I am not aware of any technical reason that would prevent this being used to produce arbitrary full colour images. In fact a colourful scene will have less noticeable colour artefacts than black on white text, which makes the colour differences hardest to camouflage. Fonts are rendered on demand The relevant difference between rendering a raytraced scene and rendering fonts is that fonts tend to be rendered on demand, and can take into account the screen being used. Contrasting with this, a raytraced scene is often prerendered and then displayed on many different types of screen (with different pixel geometry). For example, displaying your raytraced image on a webpage will prevent tailoring it to a specific monitor type. If you were designing a realtime raytracing program and you had access to check the pixel geometry of the monitor, then you could raytrace to the specific sub pixel layout. However, offline raytracing that produces a still image can only be tailored to a single type of pixel geometry, which will then make the image look worse on any other pixel geometry. You could work around this by rendering a set of different images and choosing the appropriate one when it is later displayed on a particular type of monitor. There is unlikely to be a long term benefit So there is no reason you couldn't develop sub pixel rendering for a raytracer, but it means taking into account an aspect of the monitor that is not always known. Another thing to bear in mind is that you will be developing this software for a shrinking market. Sub pixel rendering is useful for screens that have relatively low resolution. As more and more screens (even mobile screens) are approaching such high resolution that the human eye cannot detect the difference made by sub pixel rendering, your work is likely to be more of theoretical interest than practical use. 

I am using the Inigo Quilez SDF function to generate a round box shape. The aim is to be able to control the shape scale, smoothness and roundness. I've achieved a pretty good result but I am facing an issue when I change the scale. Here is my current code. 

I am currently trying to implement a specific directional light type. This light type has been used in the game INSIDE and is called orthogonal spotlight (aka local directional light). I assume that this is a directional light which behaves like a spot light and have a squared or rectangular attenuation but I have some difficulties to integrate it in my deferred pipeline and get the general concept of this light type. Classical directional light : 

I am using this Kawase Blur, to apply blur onto my buffer. But I would like to apply this blur in an uniform way, no matter the distance of the objects from the camera. I guess that I need to use the depth buffer but how should I change the offset according to depth ? 

I discovered that some engines use derivative maps instead of tangent space normal maps. After some reading, it seems to be a really awesome way to replace tangent space normals but are there some disadvantage using them ? Why still continue using tangent space normals ? Is it possible to compare both with advantage and disadvantage ? 

It will then move the pixels around one step at a time so they drift into place to give the original image again: 

This will give a zero contribution from the intersection with the light. Note that since I can only see this one function, I can't guess whether this will also make the light appear black in the image. You may or may not need to adjust for rays that start at the camera and hit the light directly. 

A few different approaches I'll consider a few variations on your specific request, since you mention efficiency and I suspect your specific request may be the least efficient. I'll also suggest ways of improving the efficiency without varying from your intended approach, so you can weigh up the alternatives. Blurring the volume instead of the surface If you want the distance metric to be 3D Euclidean distance instead of 2D Euclidean distance within the surface, then you could perform the blur on a regular 3D grid to which the scalar function you have in mind has been applied. Then you can use the final result of your Difference of Gaussians to calculate the scalar values at the vertices of your irregular mesh. This avoids having to take into account the mesh shape for the bulk of the calculation. The 3D grid is likely to have a much larger number of vertices than the 2D mesh, but they will all be equally spaced and the large kernel blur can be achieved by repeated application of a small kernel blur taking into account only the 6 nearest neighbours, which will always be at a constant distance away. This approach involves potentially more calculation, but the ease with which the regular grid could be GPU accelerated may appeal. This will give a different result from performing the blur on the mesh vertices using 3D Euclidean distance. For example, the 3D grid approach will be affected by distinctive regions of the 3D scalar function that are near but not on the mesh. This may be desirable or not depending on your specific purpose. Using 2D distance instead of 3D distance If you find that you need the distance metric to be 2D Euclidean within the surface, then you can get a good approximation to a larger kernel Gaussian blur by repeatedly applying a smaller kernel Gaussian blur. If there is not too much variation in the edge lengths within your mesh you may be able to choose a kernel size which allows for only including vertices one edge away at each iteration. This allows for only using single edge lengths to calculate the size of the contribution of a vertex, rather than calculating a 2D multi-edge distance. 3D distance using the surface without the volume If you need the calculation to be precisely as described in your question, being calculated within the mesh rather than within the surrounding volume, but also using the 3D Euclidean distance, then using nearest neighbours and several iterations will not work. Unless the mesh is near to flat, the repeated application of a nearest neighbour blur will result in an approximation to the 2D Euclidean distance case, since the values will only be able to bleed from vertex to vertex, not directly along the shortest path as they would in a single pass. This will give less spread than would be achieved by a single pass that calculates the 3D distance to a vertex 10 edges away. (I have used 10 edges since you mention a 10-hop in your comment on the question.) Implementing the blur in a single pass will mean calculating the 3D Euclidean distance between every vertex and every other vertex within a 10 edge radius. This will be expensive, but perfectly possible. Since you mention efficiency, consider that there are some redundancies you can eliminate provided you have sufficient available memory. The two blurs that you produce prior to taking the Difference of Gaussians will use the same set of 3D distances up to the edge radius of the smaller kernel blur. If you can save these then you only need to calculate them once, rather than once per blur. Also, each distance will be used twice per blur - once in each direction as the length from vertex A to vertex B is the same as the length from B to A. Caching/memoising these distances will avoid calculating them twice. Effects from arbitrarily many edges away If the surface curves such that some vertices which are many edges away are still near enough to affect each other in 3D distance, then rather than considering vertices within a certain number of edges away, you may need to consider all vertices within a certain 3D radius, regardless of how long the path via edges. In this case you can consider fewer vertices by using space partitioning, choosing a specific method which suits the mesh. If you don't want parts of the surface which approach each other to influence each other, then you probably want a 2D distance metric rather than the 3D one. If you have a wide range of different edge lengths then you may find the same problem of not being able to define a set number of edges to traverse, even if the mesh is fairly flat. Again you may need to define a 3D distance instead of a number of edges, and consider all vertices that lie within that radius. 

The Nine Patches algorithm is a really nice and powerful solution if you are using textures but I ended up by using a fully procedural solution. I am just drawing a round box like this. 

It seems that doesn't allow to interpolate shadow lookup linearly. So I had to do it by hand. Here's an example of interpolated shadowing. 

You can simply avoid the 3rd dimension if you don't need it. This method comes from the Distance function article from Inigo Quilez. 

I would like to blur the content of those 4 spheres using the same offset no mater their position. If I apply the same blur on all objects, far object's content will appear more blurry than ones on the foreground and I want to avoid that. I think that the depth make could help but any precision would help me. If I blur the whole image and apply the result on the sphere, the white background will bleed onto the sphere shape and I want to avoid that. I also don't want that the blue (3) and yellow (4) sphere merges with the red (1) and green (2) ones. But I would like that the green and red ones merges. Again This could be done using the depth but if you have more precision about how to do it it would be interesting.