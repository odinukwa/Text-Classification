If you have the database recovery model set to FULL for those database with 50-100GBs of T-logs, then you have to start doing frequent T-log backups. Remember in Full recovery model, once a log backup chain has been established, even the automatic checkpoints wont cause the log to truncate. As a last resort, you can truncate the log file and then immediately take a full backup and then start taking T-log backups so that you can do point-in-time recovery if a disaster happens. 

Also, you should understand that there is a cost to reinventing the wheel, why not use existing - well tested worldwide Solution Ola's - SQL Server Index and Statistics Maintenance Your script has a major drawback, since it will rebuild all the indexes - irrespective of their fragmentation level, page count, etc. This will lead to a huge transaction log that can impact your mirroring (Disaster recovery). The above solution is much more mature as it is customizable - e.g. rebuilds indexes which have more than 30% fragmentation, between 10% and 30% does a index reorganize and less than 10% - does nothing. There are many more benefits of using Ola's solution that you can read on the link above. 

These files are always kept in \\unc\folder with a timestamp value for the folder name. The folder name is the publication name. So, everytime a snapshot is generated, a new timestamp value is used for the folder name and then the snapshot files are written to that folder. Also, if you see that your distribution agent is not cleaning up the files, you can delete the older snapshot folders. But I would still troubleshoot - why distribution agent is not cleaning it up !! 

Do not copy to root directory as @AaronBertrand mentioned. As a side note, why are you using SQL Server to do filesystem tasks? This should be done by PowerShell e.g. 

Doing HA in a mixed environment is a recipe for disaster because during disaster once you failover from lower (2012) to higher (2014) version, you cannot failback. Since you are using Enterprise edition of SQL Server, you can leverage AlwaysON technology wherein you can configure writes occurring on Primary and reads on the secondary configured as . Thinking a bit more on what you mean by mixed, I presume that transactional replication would be much better from sql server 2014 as publisher and 2012 acting as subscriber. The performance would be dependent on how big is the database, if you are replicating only the tables that you need or the entire database, the latency between the two servers as well as the amount of transactions that are occurring on your publisher. Note that if you go with T-Rep, doing schema changes will require a new snapshot. Also there will be a distribution database created as a part of setting up T-Rep. 

As a side note, you need to take care of migrating jobs, ssis packages, logins, etc once you failover to secondary or mirrored database. 

Be careful when using SQLNCLI10 as described in connect item A workaround can be found here You have to first test the application that is using such SP's from 2012 to 2000. It also depends on where both server's are located and how much data you are pulling out using Linked Servers. Is there a reason that the other referenced databases cannot be moved to 2012 ? 

You can put above statement in a variable and do and and save it in a file and then use sqlcmd to just directly call that file and run it on multiple servers. 

Based on your question .. which you have removed "automate the same script on some of the databases on the instance" Below is the code that will help you 

Ignore that wait type - its a benign wait type to be filtered out as per Wait statistics, or please tell me where it hurts 

Also refer to my answer : Understanding the impact/risk of turning off “verify backup integrity” on SQL backup 

how SQL server knows to replicate only the stored proc execution (i.e. actual exec myupateproc t-sql command) instead of underlying table data that is being updated on the publisher (i.e. actually replicating 25 million update statement)? SQL Server knows that when you set up article property as below : 

I would suggest you to have a separate publication for these 2 tables and rest of the tables in another publication. This way if something goes wrong with these 2 tables, then your entire data set is not affected. 

No tempdb physical size does not matter (you should properly (equally size) you tempdb files (not more than 8) with TF 1118 and 1117 enabled). 

Why, despite the increasingly log_send_queue_size for these two databases, did the log_send_rate keep decreasing? There were no bandwidth issues on the network at this time, and no other databases experienced this issue. If this happens again, is there a recommended fix, apart from having to manually restore the primary database over the secondary to re-synchronize the pair? 

When is set to then a comparison of or returns TRUE or FALSE respectively, instead of an unknown answer . It treats into a value of its own. 

Depending on what you are doing with calling a file, you should look into using PowerShell as an alternative. 

Taking into account a 400GB database, its upto you to choose any of the below routes : Method 1 : Backup and Restore Depending on your hardware and the amount of activity going on, it will be slower than Method 2 - BCP OUT / BCP IN Below is the script that will help you : 

By default, the @replicate_ddl is set to 0 “false”, meaning no schema changes are replicated. Changing the value to 1 “True” will allow changes done on publisher to propogate to subscribers (as shown in below figure). 

You can create a role and then grant / revoke permissions to it. Any user that is a part of the role will inherit the permissions. Below is an example to get you started : 

When you restore the database, you have to use REPLACE option to overwrite the files with the new ones. 

you can also create a generic SP for all your agent jobs to check first in master to check few status e.g. dbo.usp_CheckDBOnline 

This might be due to MAXDOP setting on your server instance. It has been proven that Online Index Rebuild – Can Cause Increased Fragmentation when it is allowed to run with MAX DOP > 1 and ALLOW_PAGE_LOCKS = OFF directives. If you are seeing this behavior, then its better to use () (serial index rebuild) at query level when rebuilding index. e.g. 

Caution: Since you will be using SQL Server 2012 express edition (and sql 2008R2), it has a limit of 10GB per database size (for previous versions, it is still 4GB). So if your table is not going to grow that big with all the archive data for many years, you are good uptill it hits 10GB limit. My first suggestion would be to use Transactional replication. It wont have that much of overhead as you are just replicating 1 table. If you set the agent to run continuously or every 5 mins, then you will have up-to-date data on the destination (subscriber). You just have to remember to not replicate delete statements. Secondly, you can use SSIS and schedule it using SQL Agent Job to incrementally load the data into the destination server. 

IMHO, Replication is your best choice. Alternatively, you can look into SSIS and do an incremental load to your remote database for the tables you need. If they have foreign key dependencies then make sure to load them in order. Also, you can look into logshipping (this is entire database) and delay the restore of transaction logs, so that you can use secondary as stand by for running reports as well. 

I don't see a need to over complicate it. Just test it if it is a preferred replica or not and then take backup. 

Same as above. Now the optimizer has wrong statistics and hence inefficient query plan can be produced. Best is to UPDATE STATISTICS and mark that table for recompile using , so next time the optimizer will generate new plan based on updated stats available. Also read up on : Slow in the Application, Fast in SSMS? 

Complimenting to Tara's answer ... You should tsql / powershell for availablity group failover. T-SQL : PowerShell : You should check in dmv to see if the replica is ready for failover and there wont be any data loss. 

SQL Server 2008 Microsoft Certified Master (MCM) Readiness videos especially Backup Internals. A Look at Backup Internals and How to Track Backup and Restore Throughput (Part 1) - By: Jonathan Kehayias A Look at Backup Internals and How to Track Backup and Restore Throughput (Part 2)- By: Jonathan Kehayias 

You can even invoke Asynchronous procedure execution using service broker. This way if you want to scale up your application or implementation then you can as there will be no external dependencies like sqlcmd being called by windows task scheduler, etc. 

A script can be found here or Indexes Supporting Foreign Keys. Review any index recommendation that the scripts give and make sure you dont end up created a lot of indexes since there is always a cost of maintaining them. Also, make sure that your FK's are trusted. 

No. As I mentioned in the comments section. THe database wont be able to maintain its ACID properties. Either the query fails and returns an error or the lock is held by SQL Server until the transaction is completed and then released. 

You can use 3 part naming . Below is an example to get you started. You can modify as per your needs : 

I wrote about how you can use DBCC CLEANTABLE to reclaim the space. Under the hood, sql server just deallocates the pages and then a background thread called ghost clean up will clean up the data. Another aspect that I am assuming you are asking is physical data security - how can I be sure that my data is not recoverable once I delete/drop it ? To answer that, SQL Server provides - 

Recovery can be done by 2 methods : method 1 : Restore the database by using the most recent full database backup 

Try running DBCC CHECKDB with TABLOCK hint + make sure that the drive tempdb resides has enough space and tempdb is not having restrictive growth (autogrowth OFF). Lastly, 

You can follow any method below : Note: If you are using any new features like new data types, etc then you have to test out as it will throw errors. METHOD 1: Using Native Tools 

Database Main requires service broker enabled for - which is enabled by default unless someone disables it. Refer BOL : 

No there would be no perf issues. But its a good practice to use TO do it for all your user tables, run below code, review it and then run the output : 

Below script will help you setting up T-Rep, just change the databasename, destination server name along with object name. 

For your replication to work, the and should be on same version. The subscriber can be on lower version. From BOL : 

I would suggest you to create a utility database e.g. (or whatever name you like the database to be called) and store the information in that utility database along with the query plan XMLs. Overtime, it would server as a warehouse for doing performance tuning and benchmarking. Create physical tables and store relevant information in them. This way you can backup the database along with all the info you gathered. IMHO, it would become ugly if you go on the route of exporting plan XMLs into folders. Basically, you need to explore the plan cache from your server instance that will allow you to tune your workload and give you below information : 

Complementing @WEI_DBA's answer, I find it highly useful if you have a script that generates exact command to run. sp_RestoreGene is incredibly useful in situation that you are in (provided you have good restore-able backup files). 

I have used both and they are life savers. Highly recommend to test them first. First one is the one that will help in your situation. 

SQL Server Express cannot serve as a Publisher or Distributor. SQL Express can only be Subscriber. Refer to : Replication Considerations (SQL Server Express) for more details. 

This PROC deletes from , , , and and bunch of other tables. This is the entire code for a ready reference : 

(I am not mentioning database mirroring, but you can use that (even though it is marked as deprecated. You can create database snapshot on secondary and every 2-3 hrs, you can generate a new one since your requirement is - data can be stale 2-4hours) When using backup/restore method, make sure that you have enabled Instant file initialization and you are using backup compression. That will cut down the backup/restore time. 

There are couple of options which you can use to Minimize your downtime with your current available hardware : Foremost step is to run Upgrade advisor (if the SQL Server versions will be different from Old to New server). Also, take FULL backups first for all the databases - just in-case if something goes wrong. Method 1 : Log Shippping 

You should not be concerned if the query is just "ONE OFF". If you feel that you will need to run more "AD HOC" queries, then look for turning ON the option. Reference : Plan Caching and Recompilation in SQL Server 2012 

A non cursor approach would be as below. I am assuming that the login already exists and the schema is default to . 

If you want to just see who is connected now then you can use DMVs to look into The NULL in dbid (below query) indicates that those queries are Ad Hoc queries. 

15K databases on one instance of sql server - depending on the activity, overhead of maintenance, backup time, etc sounds to be an overkill. You should instead spin up VMs on one powerful host machine and then balance out your databases on multiple VMs. You are looking at a wrong place for optimization - Auto-Close option and you know that what you are trying to do is not a good practice either ! 

The secondary will be used for read intent requests and will server as a standby server in case the primary fails. 

You can use and in native mode which will be super fast. I just tested below script on and it work with XML data as well. Alternatively, you can use SSIS to do it on a more frequent basis if you want. Below is the script that will help you with BCP OUT and BULK INSERT : BCP OUT 

The answer is classic -"It depends !" Sit down with your stake holders and present then the pros and cons of the 2 options. Being in the business of client hosting, I have faced this situation and below is my view point : 

Never do that without understanding your workload and proper testing the recommendations. Refer to Don’t just blindly create those “missing” indexes! by Aaron Bertrand. 

Also, its important to note that in SQL Server 2016 you dont need TF 1117 since is equivalent of TF1117 

No, not at this time - A foreign key between a disk based table and memory optimized table is not allowed. You will get below error : 

EDIT: Below is an excerpt from msdn /Conf[igFile] filespec (Optional). Specifies a configuration file to extract values from. Using this option, you can set a run-time configuration that differs from the configuration that was specified at design time for the package. You can store different configuration settings in an XML configuration file and then load the settings before package execution by using the /ConfigFile option. You can use the /ConfigFile option to load additional configurations at run time that you did not specify at design time. However, you cannot use the /ConfigFile option to replace configured values that you also specified at design time. To understand how package configurations are applied, see SSIS Package Configurations and Behavior Changes to Integration Services Features in SQL Server 2008 R2. You have asked on How to override SSIS 2008 package config file path? What you are talking about is SET switch -- Overrides the configuration of a variable, property, container, log provider, Foreach enumerator, or connection within a package. I have been dealing with SSIS dev's and I use /CONFIG when deploying it to PROD .. provided the paths are same in the package. EDIT: 2 Agree with OP that the behaviour has changed in 2008 and up : In SQL Server 2008 Integration Services, events occur in the following order: