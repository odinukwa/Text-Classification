As far as I understand it, the Neural Style Transfer uses a content image and a style image, and generate a new image based on the two images. It tries to find a set of pixel values such that the cost function is minimized. It does not have any labels associated in advance, but it has an output (generated image) that should be the target of the learning. However, I'm not sure if this is considered supervised or unsupervised learning. Which does it belong to? 

I wonder whether one epoch using mini-batch gradient descent is slower than one epoch using just batch gradient descent. At least I understand that one iteration of mini-batch gradient descent should be faster than one iteration of batch gradient descent. However, if I understand it correctly, since the mini-batch gradient descent must update the weights by the number of the batch size in one epoch, the training would be slower than the batch gradient descent, which computes and updates the weights only once in one epoch. Is this correct? In that case, is it worth worrying about the loss of the overall training time? 

Bagging and dropout do not achieve quite the same thing, though both are types of model averaging. Bagging is an operation across your entire dataset which trains models on a subset of the training data. Thus some training examples are not shown to a given model. Dropout, by contrast, is applied to features within each training example. It is true that the result is functionally equivalent to training exponentially many networks (with shared weights!) and then equally weighting their outputs. But dropout works on the feature space, causing certain features to be unavailable to the network, not full examples. Because each neuron cannot completely rely on one input, representations in these networks tend to be more distributed and the network is less likely to overfit. 

A linear regression would work, but the real issue here is feature extraction. You have to encode your categorical features somehow, likely by vectorizing them. You can one-hot encode your features, treat them as text and countVectorize them, etc. 

There are multiple factors to consider, but the first thing to realize is that in regression, you don't want to think about whether an example is "correct" or "incorrect" but rather how close it was to the true target value. Therefore you can ignore your original intuition about "80% of predictions are 'correct'." Second remember that RMSE is in the same space as your target values. So it is relative to the variance in your target values. The benchmark of random guessing should get you an . So lower than this, your model is demonstrating some ability to learn; above that number, you haven't even learned to guess the mean correctly. There isn't a cutoff for "my model is doing well" in RMSE space, just like with other metrics. Everything is relative to a naive solution/benchmark or the state-of-the-art. 

Suppose that I train my image dataset on CNN, but the resolution of the image varies significantly on the dataset. In this case, should I scale the images up to the image that has the maximum resolution, or scale down to the lowest resolution? In that case should I scale up/down the whole images even if the highest likelihood of the whole samples are somewhere in the middle of the distribution of the resolution? Or should I use another technique to deal with the varying resolution problem? 

The means the x-axis on the graph above. I understand the derivative is smooth since the line has a curve and in that realm the derivative is no longer equal to . However, why is it the case that if the function is "smooth everywhere, including around z=0", it speeds up Gradient Descent? 

I'm now learning YOLO but I don't understand how the number of grid cells is determined. Some article like this one uses 13 x 13 while others use 19 x 19 (e.g. Andrew Ng's course in Coursera). It seems that the height and width should always be the same, but how is it determined? Is there any general guideline regarding how many grid cells should be picked up over the others? 

Using a word-based metric would explicitly favor word-level retrieval methods. The theory is that (just as you suggest with dwell time), the URL-level metric measures more directly the desired result. More concretely, consider a search of "alcohol from potatoes." Assume we have two pages: 

Generally people perform a grid search, which in its simplest "exhaustive" form is similar to Method 1. However there are also more 'intelligent' ways to choose what to explore, which optimize in parameter space in a fashion similar to how each individual model is optimized. It can be tricky to do greedy optimization in this space, as it is often strongly non-convex. This page describes the basics of optimizing model parameters. 

So for those datasets, you shouldn't need to balance the classes. You might also try using class weights instead of under/oversampling, as this takes care of this decision for you. For you likely want to optimize using whatever metric you will be scored on (if it's a competition). But if that isn't a consideration, all those models are fine choices. F1 may be influenced by the low precision, but you want that to be captured. It's precisely when naive models (like guessing the majority class) can score well by some metrics that scores like F1 are relevant. As for there is nothing wrong with showing whichever metric you end up optimizing on. 

I'm reading a book titled Python Deep Learning, and in Convolutional layers in deep learning on the chapter 5, the following is written: 

It is not a pre-requisite, and you can learn it easily once you encounter something you are not familiar with. Statistics is pretty old and there are many learning resources on the Web, which you can get to whenever you hit the wall while learning about deep learning. As to which field is pre-requisite, I think it is enough to first learn about Gaussian (normal) distribution, linear regression, and logistic regression. Then when you encounter something you don't understand, it is time to invest your time on statistics. The more requisite fields I believe are calculus and linear algebra. If you haven't learned about them (such as partial derivative, matrix transpose, etc), it is very difficult to start to learn deep learning. Also, the prior exposure to some of machine learning algorithms would make your learning faster. But I'm sure you already got it given that you finished Andrew Ng's course on Coursera. Andrew Ng starts deep learning course on Coursera from August 15th, so you can join it and get a grasp of what is required. The book you linked sounds more like focused on machine learning than on statistics, BTW. 

Both the answers from @Emre and @Madison May make good points about the issue at hand. The problem is one of representing your string as a feature vector for input to the NN. First, the problem depends on the size of the string you want to process. Long strings containing may tokens (usually words) are often called documents in this setting. There are separate methods for dealing with individual tokens/words. There are a number of ways to represent documents. Many of them make the bag-of-words assumption. The simplest types represent the document as a vector of the counts of words, or term frequency (tf). In order to eliminate the effects of document length, usually people prefer to normalize by the number of documents a term shows up in, document frequency (tf-idf). Another approach is topic modeling, which learns a latent lower-dimensional representation of the data. LDA and LSI/LSA are typical choices, but it's important to remember this is unsupervised. The representation learned will not necessarily be ideal for whatever supervised learning you're doing with your NN. If you want to do topic modeling, you might also try supervised topic models. For individual words, you can use word2vec, which leverages NNs to embed words into an arbitrary-sized space. Similarity between two word vectors in this learned space tends to correspond to semantic similarity. A more recently pioneered approach is that of paragraph vectors, which first learns a word2vec-like word model, then builds on that representation to learn a distributed representation of sets of words (documents of any size). This has shown state-of-the-art results in many applications. When using NNs in NLP, people often use different architectures, like Recurrent Neural Nets (like Long Short Term Memory networks). In some cases people have even used Convolutional Neural Networks on text. 

I now read a book titled "Hands-on Machine Learning with Scikit-Learn and TensorFlow" and on the chapter 11, it has the following description on the explanation of ELU (Exponential ReLU). 

As far as I know, the width and the height should be a figure divisible by 2, in order to use a pooling layer. However, I don't understand why the depth must be a figure divisible by 2. The pooling layer just operates on a 2-dimensional screen based on width and height, and it operates on each filter (depth) separately, right? Why should the depth also be set to a figure divisible by 2? 

The function is from TensorFlow, FYI. The author explains that the γ parameter should not be set on ReLU activation function. However, I don't understand why on ReLU, the next layer's weights can take care of scaling... I understand that the next layer takes input from the ReLU output, which is . Why does it take care of scaling and thus no need to set γ parameter on ReLU? 

How much help this community can be depends on where in the process you are. If you have a dataset and are looking on ways to determine the most predictive features, then you could include some information as to what kind of data you've collected/have access to. If instead your question is specifically "what data should I collect to predict purchases from banks, and what models should I use," you are unlikely to get a very specific answer. 

You might find it useful to treat n-grams of characters as your feature space. Then you could represent a string as a bag of substrings. With or greater, you would capture things like ".com" in emails, "##.#" in dates, etc. It might also help to encode all single digits as one reserved number-only-character. An easy way to to this might be to create all the n-gram substrings for each string in your dataset, then simply treat that list of words as a document. Then you could use term frequency or tf-idf vectors in your supervised step. For example to create the substring uni-, bi-, and tri-grams for "whatever@gmail.com":