In "Domains and Lambdi Calculi" by Amadio and Curien, in the section on solving recursive domain equations (section 7), they give sufficient conditions on a cpo-enriched category so that the category of embedding-projection pairs has $\omega$-colimits. Specifically they say it is sufficient for the category to have $\omega^{op}$-limits. They then have a theorem that the category of CPOs has all $\omega^{op}$ limits, but the proof is incomplete and I don't see how it could be completed. To be clear in their book the category CPO has as objects directed complete partial orders with a least element and as maps continuous (not neccesarily strict!) functions. Their proof proceeds as follows. Given a diagram ${D_n,f_n}_{n\in\omega}$ ($f_n : D_{n+1} \to D_n)$, their proposed limit is $$D = \{ \alpha : \Pi_{n\in\omega} D_n | \forall n, f_n(\alpha_{n+1}) = \alpha_{n} \}$$ with the pointwise ordering. This is definitely the product of that diagram in DCPO, but I don't see why $D$ must have a least element. In particular, you can't pick $\alpha_n = \bot_n \in D_n$ since the functions $f_n$ are not assumed to be strict. However, restricting to strict continuous functions is all they need anyway because embedding-projection pairs of CPOs are strict anyway (embedding because it's a left adjoint, projection because it's a retract of the embedding). However this doesn't seem quite right either because for example a right adjoint wouldn't necessarily have to be strict, so it seems less general. So to summarize 

For ordered enumeration instead of random generation you are getting into the realm of combinatorics. I don't know of any generic results, but this paper Counting and Generating Lambda Terms describes an enumeration of untyped terms and empirical data on the sieve approach to enumerating typed lambda terms. It looks like they use a hindley-milner type system so no annotations are needed. On the other hand if you want to generate typed terms directly, there are libraries like SciFe (website,paper) and data/enumerate (docs,draft paper) that support "dependent enumeration" where you enumerate one thing and then select what enumeration to use based on that (essentially enumeration of Sigma types), that is essential for enumerating typed terms in non-trivial languages. Dependent enumeration isn't fast either, but it might be faster than a sieve. 

I have recently been working with polynomial functors and monads based mostly on Gambino-Kock. There they define polynomial functors in a Locally Cartesian Closed Category (LCCC) and extensively use dependent type theory to define constructions on polynomials because working in the internal language is much easier than the diagrammtic language. However, I am interested in polynomial monads for their application to defining flavors of multi-category and for that I need to use polynomals in Cat and similar categories, which have pullbacks, but are not locally closed. There you instead require in the polynomial diagrams $$I \leftarrow E \to B \to J$$ that the middle arrow $E \to B$ is exponentiable, since that is the only $\Pi$ you use. The only paper I know of that does this is this which doesn't use the internal language at all and is much more difficult for me at least because of it. Is there some kind of restriction I can put on dependent type theory so that I can use it as an internal language for a category with pullbacks rather than an LCCC? Specifically I want to be able to manipulate exponentiable morphisms as dependent types that I can take $\Pi$ of, but not every dependent type should be exponentiable. Then hopefully the usual proofs using dependent type theory would still be valid, because every use of $\Pi$ would be modeled by an exponentiable morphism. My own idea would be to have dependent types interpreted as exponentiable morphisms and other terms interpreted as arbitrary morphisms, but since you can define from any $x : A \vdash t : B$ (where $\cdot \vdash B$) the dependent type: $$b : B \vdash \sum_{x:A} t = b$$ it seems like the type theory would make every morphism exponentiable. 

I'm trying to solve a particular problem, and I thought I might be able to solve it using automata theory. I'm wondering, what models of automata have containment decidable in polynomial time? i.e. if you have machines $M_1, M_2$ you can test if $L(M_1) \subseteq L(M_2)$ efficiently. The obvious ones that come to mind are DFAs and reversal-bounded counter machines where the number of counters is fixed (see this paper). What other notable classes can be added to this list? The more powerful the automata, the better. For example, DFA's aren't enough to solve my problem, and the counter machines can't do it with a fixed number of counters. (Naturally, if you get too powerful, then containment is either intractible, like for NFA's, or undecidable, for CFG's). 

Context I realize that subtyping often doesn't admit principle types, and that inference in the presence of subtypes is undecidable. I'm working in a context where typechecking should simply fail there is not a single most general solution, and where annotations can be added to aid inference. Suppose that we have a type system where, during typechecking, some types need to be inferred using unification, and may be type variables $\alpha, \beta$ etc. If, at some point, the constraint $\alpha <: T_1 \to T_2$, where $\to$ is the usual function type constructor, then we can decompose this into $\alpha = \alpha_1 \to \alpha_2$, with $T_1 <: \alpha_1$ and $\alpha_2 <: T_2$. The Problem The problem arises if you have constraints $\alpha_2 <: \beta_1 \to \beta_2$, $\beta_2 <: \alpha_1 \to \alpha_2$. In such a case, we get: $\alpha_2 = \alpha_{21} \to \alpha_{22}, \beta_1 <: \alpha_{21}, \alpha_{22} <: \beta_2 $ from the first constraint, and $\beta_2 = \beta_{21} \to \beta_{22}, \alpha_2 <: \beta_{21}, \beta_{22} <: \alpha_2$ from our second problem. If we substitute from our equalities, we then get (among other constraints), $\alpha_{22} <: \beta_{21} \to \beta_{22}$ and $\beta_{22} <: \alpha_{21} \to \alpha_{22}$, which is identical to the form of our original problem. Clearly if we continue solving this way, we will never terminate. When going to write a proof of termination, the problem is that substitution decreases the number of unsolved variables but increases the structural-size of the problems, and solving subtyping decreases the structural size of the problems, but increases the number of unsolved variables, so they don't work in a well-founded ordering. My Solution Attempt With my definition of subtyping, if $\alpha_2 <: \beta_1 \to \beta_2$, $\beta_2 <: \alpha_1 \to \alpha_2$ has no solution. The problem seems to be the "cycle", so if we do a sort of occurs check and fail when cycles are detected (or turn them into equality cycles, which will fail except for $\alpha <: \beta \wedge \beta <: \alpha$). I'm not exactly certain how to formalize a cycle like this, and how such a check would give me something that I can use in a proof of termination. My Question My work is about the specifics of a particular subtyping system, but the problem here seems very general, so I'm wondering if this is a known problem, if there are known solutions to it, and if there is research that either formalizes or disproves my intuition about cycles. What kind of cycles in subtyping constraints do I need to eliminate to avoid this infinite looping? Or is this a deeper problem with no solution? 

I am working on complete metric graph (V,d) where shortest distance is used as metric. The question is how large can be the ratio of the sum of weights of all edges to the weight of the MST (minimum spanning tree). Also if anyone of you can post some lecture note or paper links so that i can study these graphs. 

I am looking for a linear programming formulation for the max-cut problem. My interest is to know about the primal - dual algorithm for max-cut. It would be nice if someone can tell me that what is the best ratio achieved via this approach. Please mention the formulation along with the integrality gap. Any other information/comments are invited. My primary interest includes to know about the alternate ways that lead to 0.8 approximation for max-cut or near to it (preferably via primal dual scheme). 

I am again restating what has been said. The amortized analysis formally introduced by Robert Tarjan involves considering the whole sequence of operation throughout the algorithm. The analysis is based on a simple practical phenomenon that when we run any algorithm there are some operations which are cheap and some are costly but overall the costly operations do not force the algorithm to perform very badly. There is simple charging argument involved that there are enough cheap operations that account for the extra cost paid for the costly operations. An easy example includes when you are simulating a queue using two stacks. A little involved analysis comes in disjoint set union problem using path compression which has inverse ackermann funtion complexity on an average. It is important to note that it is not same as average case complexity analysis or the probabilistic analysis. In this we consider the worst case analysis of the algorithm as a whole (instead of each operation separately). Now since we are talking of worst case complexity, the analysis becomes important to prove the algorithm efficiency as the algorithm's performance will be upper bounded by the amortized complexity. However if we do take each operation's worst case analysis separately then our analysis might be quite loose as a whole. 

Williamson with many of his co-authors had worked on generalized primal dual algorithms on edge weighted graphs considering three types of functions: (1) super-modular functions (2) proper functions (3) uncrossable function All of which is covered in his PhD thesis titled: "On the design of approximation algorithms for a class of graph problems.", 1993. My question is that where is the difficulty coming when we move from 0-1 proper to uncrossable functions?? May be some example would also help. Is there some variant of uncrossable function which is superset of proper functions in general?? Also, why do we not consider sub-modular functions?? Why types of problems would have sub-modular functions. (i know about the trivial example of modular function i.e. cut $\delta(S)$ but nothing much). So where is difficulty coming when we consider sub-modular functions or are they too easy?? This would probably let me know the inspiration to consider the super-modular functions. Also a related question is that sometimes we replace two crossable sets by uncrossing sets, like when we construct the tree corresponding to laminarity concept. Is this always possible?? (in case of every supermodular function?? or only in uncrossable functions??) Interesting references would also help.