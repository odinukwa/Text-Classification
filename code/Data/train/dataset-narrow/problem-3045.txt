It is at this stage that I see strange behaviour and I'm unsure how to proceed. Do I take the .best_estimator_ from the GridSearch and use this as the 'optimal' output from the grid search, and perform prediction using this estimator? If I do this I find that the stage 3 metrics are usually much lower than if I simply train on all training data and test on the test set. Or, do I simply take the output GridSearchCV object as the new estimator? If I do this I get better scores for my stage 3 metrics, but it seems odd using a GridSearchCV object instead of the intended classifier (E.g. a random Forest) ... EDIT: So my question is what is the difference between the returned GridSearchCV object and the .best_estimator_ attribute? Which one of these should I use for calculating further metrics? Can I use this output like a regular classifier (e.g. using predict), or else how should I use it? 

I use Python to run a random forest model on my imbalanced dataset (the target variable was a binary class). When splitting the training and testing dataset, I struggled whether to used stratified sampling (like the code shown) or not. So far, I observed in my project that the stratified case would lead to a higher model performance. But I think if I will use my model to predict the new cases which would highly probably differ in the distribution of target class with my current dataset. So I inclined to loosen this constrain and use the unstratified split. Could anyone can advice to clarify this point? 

My task is to create a model to predict the defaulted rate for each cohort in a time window like 360-day, i.e. how about the defaulted rates of each cohort after 360 days given its input into the model? I have tried some ARIMA models but the AR order is too large, e.g. AR(100) and the computation take long time to run. I wanna try a machine learning method which allow me to create a dataset not only with the time labels but also some other features which can contribute to the prediction of the defaulted rate. Please advice how can I do that or please recommend some papers/textbooks for reference. I know some concepts about the sequential learning but not to the executable level. Many thanks 

I think it makes sense to stick with Classification here, since you already have examples of fraudulent and non-fraudulent calls that you can train on. It might also be beneficial to train several models for different regions based on IP, as well as your 'global' model, and apply the region specific and global models to incoming calls. Just a few ideas. From what I understand, real-time learning would require immediate feedback, which most fraud detection systems can't provide. For example, it may take a few days or weeks to have a case of fraud resolved (labelled fraud/not-fraud), and therefore take some time for the learning system to receive the feedback on it's prediction. 

I suggest training/testing your classifier on separate splits of the original dataset, and then printing a confusion matrix: $URL$ This is a way of seeing how many of the 'true' classifications your classifier predicted correctly or incorrectly, and the same for 'false' classifications. This will give you more information than just 'accuracy', because a model trained on data where most of the classes are 1, for example, will predict 1 most of the time because it will probably report reasonably high accuracy in doing so. A confusion matrix is like a sanity check for this. 

Although denoising autoencoders rely on this aspect to learn a model that ignores noise from a sample, the same paper on AAEs (section 2.3) shows that combining noise with a one-hot encoded vector of classes can be used to incorporate label information about the sample. This information is only provided to the discriminator, but it still influences how the encoder produces $q(z)$. 

The purpose of having a prior distribution $p(z)$ in any generative adversarial network is to be able to smoothly match a latent code $z$ in a known distribution to an input $x$ in the domain and vice versa. The encoder of a simple autoencoder, without any additional measures other than the in typical pipeline $$x \rightarrow E \rightarrow z \rightarrow D \rightarrow x'$$ would only require $x$ to approach $x' = D(E(x))$, and for that purpose the decoder may simply learn to reconstruct $x$ regardless of the distribution obtained from $E$. This means that $p(z)$ can be very irregular, making generation of new samples less feasible. Even with slight changes to the bottleneck vector, we cannot be sure that the encoder would ever be able to produce that code with any $x$. In an adversarial autoencoder (AAE) however, the encoder's job is two-fold: it encodes inputs in $p(x)$ to the respective code in $q(z)$ so that: 

What you want to do is develop your code in IntelliJ, and then package your code and dependencies into an executable jar file using SBT or Maven. When you have your jar stored locally, you can use spark-submit to transfer the jar to your cluster (along with some other parameters) for execution. You might also want to take a sample of your data and store it locally so you can run spark locally and test/debug your code in IntelliJ. This can speed up development considerably, and having access to a debugger is a huge help. 

I'm currently working with Python and Scikit learn for classification purposes, and doing some reading around GridSearch I thought this was a great way for optimising my estimator parameters to get the best results. My methodology is this: 

I'm not a business analyst so I guess you'll have to take what I have to say with a pinch of salt. From my understanding, Business Analysts responsibilities are focused around improving processes within a company, for example how certain technologies could be implemented to improve a workflow, they are expected to understand how these technologies might improve the workflow, or a product etc, and manage these improvement projects. This seems to differ from Data Science on an abstract level in that it is exploring known unknowns ("can our process be improved? what technologies/methods exist that could improve it?"), whereas Data Science is great for exploring unknown unknowns. For example, why is this better for our workflow/product specifically? Data Science is great at throwing up results you don't expect, which is one reason why it is so valuable. I may be wrong in saying this, but Business Analysis seems to be relatively free-form depending on the company and the needs, whereas Data Science has a less subjective methodology. With this in mind, perhaps Data Scientists could be used to better inform your Business Analysts decisions? But the other way around, BAs could perhaps be used to better inform Data Scientists of business processes, or maybe your BA could focus on improving processes to make life easier for your Data Scientists such as data pipelines in a non-automated environment (example: how can we gather more useful data from our vehicle showrooms? What technologies do the DSs need and how can we implement them?). 

I fit my dataset to the random forest classifier and found that the model performance would vary among different sets of train and test data split. As what I have observed, it would jump from 0.67 to 0.75 in AUC under ROC curve (fitted by the same model under same setting of parameters) and the underlying range may be wider than that. So what is the issue behind this phenomena and how to deal with this problem? As my understanding, cross validation is used for a specific split of train and test data. 

When people start to figure the self driving cars will replace some vehicles on the road in the very near future, it somehow means the learner in the robot software could reach a very low empirical error to ensue the safety of passengers. The great deal of driving data collected and the deep reinforcement learning algorithms contribute mostly to the success of the self driving practice as we see it. When it comes to many cases of our own practice, we yet cannot build up a predictive model to such a high accuracy to bring the huge difference to our own business. My question is, when I saw the machine learning techniques can make revolution on the car driving due to its successful algorithms to implement behavioral cloning tasks well, what is the foremost reason for this success, the big training data set or the deep reinforcement learning techniques or any particular reason in self driving problems? Furthermore, can this success be copied in most of the practical problems we solve in machine learning? In other words, if today's machine learning techniques can help a car drive by itself, how can I reach the same success when developing my own predictive models to boost my business? If we cannot, what are the constraints, not enough data in the particular case or not a smart model yet? The reason the self driving thrives can help answer this question. 

Even if the discriminator might not know anything about any of the two distributions at the beginning, it is only a matter of making enough iterations before it does. The ideal encoder will manage to trick the discriminator to the point of having approximately 50% accuracy in the discrimination process. Also note that $p(x)$ may not be just a Gaussian or uniform distribution (as in, some sort of noise). Quoting from Goodfellow's Deep Learning book (chapter 20): 

The same link shows how these features are extracted, with a deep look into the cited article "Image-based recommendations on styles and substitutes": 

The reference neural network that was mentioned is the BAIR Reference CaffeNet at the Caffe Model Zoo, which is a slightly modified version of AlexNet. Since the model was trained over ImageNet, which contains a wide variety of photographs of various categories (1000 of them, if I recall correctly), retrieving the neural codes of one of the layers (obtained just by forward propagation) will give you visual features with a fair representation of the images, even if the network was not specifically trained for Amazon's tasks (such as product recommendation). What these values actually mean is not something that tangible: it is the outcome of multiple 2D convolutions and other normalization and regularization functions, the parameters of which were adjusted specifically for classifying photographs from ImageNet. The FC7 layer has a rectified linear unit activation (ReLU), which means that they are all non-negative numbers (potentially with several zeros). And since it's a fully connected layer that follows several convolutions, there is no intuitive mapping between a feature index and a certain characteristic of the image. You may picture the network as a highly complex function that yields a high-level representation of the image, under the form of a vector of numbers. See also the paper Neural Codes for Image Retrieval, where the authors retrieve features from a pre-trained neural network in this fashion, for retrieving images in a different image domain.