In accounting practice values are never altered once written. Instead secondary compensating transactions are created. In your circumstances this would mean one new row with a value that is the negative old, incorrect value and a second new row with the new, correct value. To work each data row will need its own datetime column so these compensating transactions can be post-processed into the correct buckets. Averages cannot be altered in post processing. Instead the sum and count are stored and average calculated at run time. 

If you intend to transfer money from one user's account to another user's account you want to make absolutely sure that the amounts all balance out and nothing gets lost anywhere, even on a system failure. Mongo does not offer this guarantee since it does not support multi-record transactions. You will have to include additional application programing to provide this guarantee. If money is involved then there will be audit considerations. Ask the authorities in your jurisdiction what their requirements are. Ensure you build these into the system from the start. Anti money laundering is a big thing too. Make sure the system can comply with local regulations. It will never be sufficient to store the balance alone. You should have a complete transaction history of debits and credits for whatever duration your regulator requires - typically seven years. Double entry book-keeping principles will be applicable. Never, ever use floating point for anything. Know where all the rounding errors end up. For security you will need multiple levels of prevention, detection and remediation. No software is perfect. All software has bugs, or relies on software (OS, DBMS, comms) that has bugs. These bugs will be exploited. Isolate each part of the application from others as much as possible. Grant minimum rights to run-time credentials. Make sure the lawyers agree the end-user terms and conditions. Good luck. 

For the simple case shown, with a business rule something like "Each company can be contacted at zero to many email addresses; each email address directs to exactly one company" it would be more usual to have the company id as a foreign key in the email table. What you have shown will work, but there will be a one-to-one relationship between Email and CompanyEmail, which is redundant. For your linked question, where you actually have a Company, a Branch and an Employee, all of which can have many email addresses and any one email address could relate to any or all of Company, Branch and Employee, things are slightly different. In the logical data model I would be inclined to show Email and the intersection entity types explicitly to document the rules fully. In the physical table design, however, I would consider omiting the Email table. This is because its only columns are e_addr and e_id, so e_id is really just a synonym for e_addr and e_addr can replace e_id in the intersection tables CustomerEmail, BranchEmail and EmployeeEmail. You can still answer all the questions about one email address's re-use but won't have the actual table to maintain. It would mean an email address disappeared from your system when it stopped being used. That's probably OK in practice. 

The identity column will keep things in chronological order. @tables' contents survive a rollback so are preferable over #tables or normal tables for this purposes. I haven't profiled this suggestion; I throw it out there for what it's worth. 

SQL Server uses four part names. To reference an object outside the current DB qualify the name more fully or create a synonym. USE provides security context and default for the optional parts in four part names. 

There's a good overview of #Tables and @Tables here. Both are written to TempDB. Both will incur IO costs. #Tables support indexes and statistics on non-key columns. If you have 9,000 rows this could make a significant difference to the query plan and hence performance. A #Table is only visible in the scope where it was created and is cleaned up by the system once out of scope (garbage collected, in effect). This allows concurrent or consecutive executions of the script without fear of one execution mixing data with another execution. Your real alternative is a "proper" table in the database used as a working space. You will write your 9,000 rows here and remove them once your process has finished. The problem is you will have to code garbage collection and data isolation yourself. @Tables' data survives a ROLLBACK which #Table and "proper" tables do not. If your 9,000 rows are logging or audit information that could be a useful feature. Be aware of #Table caching, however, the problems it may cause and the work-arounds. 

If you write your queries properly and avoid then any relational database will allow the addition of further columns or tables without requiring adjustment to the application. You do, of course, have to declare each column to the DBMS before referencing it in SQL. I find the claim that NoSQL is "schema-less" to be slightly misleading. Applications using NoSQL persistance do, indeed, have a schema. The difference is that the schema is held in application code and it is the responsibility of every programmer who touches the code throughout the application's life to enforce that schema. With relational databases the data structure is declared to the service and it then takes the responsibility for enforcing those rules for ever after. The real flexibility of NoSQL is that two rows within the same "bucket" (the definition of which various depending on your DBMS) can have different structure. 

Note specifically that rows 1 and 2 have the same value in column "a". So for row id 1 we generate the following hashes (using my magical hash function): 

The UNION operator combines all the rows from one query with all the rows from another, eliminating duplicates, and forming a single list. The hash operator builds a hash table from the upper input and probes that table with the lower input. I can see how this would work to implement a UNION, given the row-at-a-time pull model the execution engine uses. I imagine it works something like this. The Hash operator is asked for a row. It, in turn, pulls a row from the upper table, hashes it and compares it to its current list. If it is not found in the list it is a new value, it gets added to the hash list and also returned to the calling operator. This continues. Eventually a row is read that has a match in the hash table. That row is rejected (UNION eliminates duplicates) and the next row read. Eventually the upper input is exhausted. Processing continues with the lower input, reading rows, rejecting matches and passing on fresh values, until it, too, is exhausted. Under what conditions would a hash match be used instead of another operator? The trivial answer is because the optimiser has determined that the cost of a hash operator, for the given datasets, is less than the cost of any other operator that could perform this task. More specifically (I'm extrapolating somewhat from joins) hash match typically occur for larger datasets without appropriate sorting. Here's an example that shows the usage. I have a Numbers table, which I've copied to create dbo.Numbers and dbo.Numbers2. The query