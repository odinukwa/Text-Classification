Alternatively you can manage the delay of a replica using the MySQL Delayed Replication, note that this feature is available only on 5.6.x + MySQL version. I think that a good idea where to start (but there are many configuration for your scenario) is to have 2 node attached in replica, one standard replica and one delayed replica: 

UPDATE FOR FREE BLOCK EXPLAIN When you delete a row a free block can occur, if you have free blocks in the "middle" of your insert, for example if you replace deleted rows(index and/or primary key) with your load data, concurrent insert do not work, when free blocks are filled in future insert become concurrent again. 

For MySQL and PostgreSQL you can use the Snapshot Backups in order to obtain full or incremental backups: Some file system implementations enable “snapshots” to be taken. These provide logical copies of the file system at a given point in time, without requiring a physical copy of the entire file system. (For example, the implementation may use copy-on-write techniques so that only parts of the file system modified after the snapshot time need be copied.) It is available through third-party solutions such as Veritas, LVM, or ZFS. 

I have tried a differen variety of solution with a similar data load -over 1B- but the better that I have found is this: From mysql documentation With some extra work, it is possible to make run even faster for a MyISAM table when the table has many indexes. Use the following procedure: 

Another test you can try... "probably" in your table the cardinality of 'domain_id_resource_type' index is lower than 'domain_id', you can try to skip the optimizer choice and declare the usage of 'domain_id_resource_type' instead of 'domain_id' 

UPDATE 2015 FROM MY PREVIOUS POST 2012 :) You can take a look into MySQL Fabric (Official Doc) I have tried this tool only in R&D env for testing a basic HA It supports some sharding scenarios Here some high level pros and cons Pros: 

You can take a look into MySQL Fabric (Official Doc) but it requires more db server I have tried this tool only in R&D env for testing a basic HA It supports some sharding scenarios Here some high level pros and cons Pros: 

I am trying to create a database for a small book library. This is not a professional project, just an exercise of my own. This is my first time to try this ( I am self-taught ) and I am encountering a problem. Before I continue I will provide my version of the ER diagram below ( I made it in Paint, using the following resource for notation ): 

Since this is my very first post here, please leave a comment if anything needs to be changed ( if it doesn't fit this site's question format ) or if you need further info. Thank you. 

I apologize if I drew something wrong, please correct me if that is the case. I don't wish to simply get "free solution" but also to learn how to deal with this problem so I can solve it on my own in the future. The only thing that comes to my mind is to create two separate tables, one for cats and one for dogs. Also, the race attribute in the Animal table would only store cat or a dog value. Something like this: 

I have stumbled upon a problem during database design. Let me explain what it is all about: Contract has some simple attributes that describe him, and may have annexes and dynamics of payment. Therefore I have decided to make main table , and placed simple attributes as columns. Dynamics of payment is a complex attribute of table. It can have none, one or multiple values. Searching through the Internet I have learned that this is called multivalued attribute, and have found this example that seems to illustrate my case very well ( Dynamics of payment is equivalent to the Hobbies table in the linked example ). As for Annexes, it is a complex attribute of . Contract can have many of them, one or none. Annex has exactly the same simple attributes as contract, and can also have Dynamics of payment. Annex' relationship with Dynamics of payment is the same as the relationship between Contract and Dynamics of payment. To sum-it-up, Annex and contract have everything the same, the only difference is that Annex is a complex attribute of Contract. Using this as a reference, I have made the sketch of my ER diagram: 

Try increasing your (probably a default value of 60secs is too small in your scenario of blob and binary values) Ref: net_write_timeout and in general: net_read_timeout 

In my previous response I have not taken into account behavior, so I have tried your test, first using the mysql native client and all works fine, column are automatically converted, and then using a jdbc connector and actually I got the same error... but after some test and some research I have found the solution. You have to add in your jdbc connection url the parameter and set it to 

Antivirus or "Non-Conventional" Backup software has also often caused corruption. Your first step is to try to find out what is changing the MySQL files. Countinuous table repairing can only be a temporary workaround. 

And the list under "If you do encounter a full-table error, there are several reasons why it might have occurred:" 

Collect and graph your results, changing concurrency and requests, and monitor your database and HW status. My file contains about 80.000 query and it is composed by insert, update and delete UPDATE #1 A good starting point that you can try is to start (simply) writing your file like this: 

Source: 5.1 Driver/Datasource Class Names, URL Syntax and Configuration Properties for Connector/J Running my java code using the above connection url the table are correctly converted (I have tried only with an empty table): 

This is only a trace not a complete solution, "your" solution is based on your fantasy (and some work) ;) I have done this kind of test capturing and cleaning query generated by the general query log and using sysbench 0.5 after writing my own file, you can find some samples in the official source code, on a mirror of my database(hw and schema/data -a snapshot so I can restore the original status of data immediately- ). Then you can run a command like this: 

Execute a statement or a command. Use to remove all use of indexes for the table. Insert data into the table with . This does not update any indexes and therefore is very fast. Re-create the indexes with . This creates the index tree in memory before writing it to disk, which is much faster that updating the index during LOAD DATA INFILE because it avoids lots of disk seeks. The resulting index tree is also perfectly balanced. Execute a statement or a command.