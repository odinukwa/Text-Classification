Another reason to use a data warehouse or dedicated reporting server is to avoid placing load on a production OLTP server. Or if you have to write reports which include data from different systems. The way you have described your db I doubt there would be much advantage in denormalising unless you wanted to build OLAP cubes on it. In which case an OLAP friendly schema might have significant benefits. 

This is difficult question to answer. The right solution for you will depend upon several factors such as cost and availablity of resources and expertise. But the solution should be driven by the requirements. Before you try to select a technical solution it is worth considering what the end result should look like. Where is the data going and for what purpose? Do you want reporting and analytics? Who is going to use this data and how? (Executives? Analysts? Marketing? Sales? Customer Service?) What level detail does each group need? (Raw ? Aggregated?) How often? When? How fast? Does it need to be integrated with Salesforce? If you decide the solution is a data warehouse I would seriously consider a cloud based alternative. As you already have cloud based software and you are spread across different countries. 

Considering you have to create a single procedure to deal with three result sets, below is the solution. But as Aaron Bertrand have suggested in comments, if possible try having three different procedures for three different result set. If three different SPs is not accepted due to complex logic or architectural limitation, then 

You can make use of the procedure to get the logical foreign key information. But the limitation is you have to execute is by passing the table name as the input parameter. More info for the same can be found at below MSDN library. sp_fkeys Alternatively you can make use of below query to get all the tables from sys.tables and check if the same table object_id exists in the sys.foreign_key_columns. 

I will suggest to have a table valued function. This function will accept your string as an input parameter and then return a table by splitting the inputs separated by ";" in different rows with an index ID. You can use this function internally with you logic and determine the MAX(Us) from mapping table. Below is the sample example for the function. 

I dont think there is 1 right answer to your question. Best for what? The right answer for you will depend on factors like cost, scalability, platform preference and compatibility. And ultimately what you want to use the data for and how you expect to use it. I have done similar things in the past. In my case i've used SQL Server and MS-Access. But any database should be able to do the same or similar. Ive used powershell to iterate through folders and generate a csv file and then load into SQL. A path and file name can be stored in a single varchar field, or can be split into multiple fields. In my case i wanted the data split into multiple fields. This suited me as it allowed me to report and filter by any level very easily and the data was used by a multiuser app built in access. The important questions to ask yourself now are, what do you want to do with tha data once its within the db? Do you want to query it to produce reports? Do you want 1 row per file? What other meta data do you want to store? Size? date? File owner? Depending upon what you are trying to do you might not need to store the data in a db at all. One issue you might face is trying to keep your data in Synch. As the db will be unaware of new files, file changes, moves or deletes. For this reason you might be better off not using a db but instead reading the data on the fly as required. 

Accept additional input parameter (like an integer with possible values 1,2,3) If passed 1 in the input parameter, at the time of returning result return the first result set. If 2 in input parameter then return 2. In main procedure, call this procedure thrice. Each time with different input parameter number (i.e. 1 , 2, 3). 

Only thing is remaining is to include an exception for the NA or "Input Does not exists" case which can be easily handled. 

What we have understood here is you want to migrate selective data from selective tables from SQL 2008 server into SQL express edition. Have you considered using the import/export data feature on the SQL management studio? Import/Export wizard can be accessed by logging into server using the SQL Management Studio. Here is the link to MSDN library. You can select the source as SQL 2008 server (production) and destination as SQL Express Edition (development). And then map the required tables and columns. If required, truncate the tables from the development environment first if you want to have a clean data. Once data is migrated, you can perform the update queries to remove/mask any personal or confidential data. Let me know if this serves the purpose else we will delete the answer later and will think something else. 

I use SQL Agents tasks to execute my backup, maintenance & ETL tasks. IT would like to have a regular maintenance window to apply server patches. They're suggesting the 3rd Tuesday of every month outside work hours. Can I configure jobs to run on every day except that day? The other option I thought might be possible was to have multiple schedules configured to somehow exclude that day. 

Running SQL 2012 SP2 (11.0.5343) on Win 2008 R2 Standard SP1 on VMWare. Edit: Updated to SQL 2012 SP3 (11.0.6020). Edit: have added more context and clarified. Following suggestion from @TheGameisWar, I queried ssisdb SELECT * FROM [SSISDB].[catalog].[event_messages] WHERE event_name = 'OnError' Returns no error messages for that date. 

I think we solved this or at least found a workaround. I believe the problem is a timeout when connecting to SSAS when it has been inactive for some time. It has nothing to do with the actual processing of the cube. We added a step to our ETL process which queried the OLAP cube before the processing step was triggered. This causes SSAS to 'wake up' before it is needed. Since we added this step we haven't had a repeat of this problem. 

I guess you can try something like below. This is combination of identity column primary key with the computed column. Hope this helps. 

If I understood correctly, if stored procedure is called with @vCol1 = 'AA', @vCol2 = 'BB' and @vCol3 = NULL then you want to update only Col1 and Col2 keeping the Col3 value intact. Then you can go with the following solution. I agree its not the best one but it will get work done in simple manner. 

You can also try to below approach. ISNULL is more convenient that if-else structure. But this approach will only work when you want to compare the result with NULL. 

EDIT: One additional point. You have to very precise while making us the if logic over here. As the same procedure needs to be called thrice, if may have an performance impact depending on what logic you are trying to implement. If possible make use of the same if block to incorporate all logic for the specific return scenario and keep the generic part in the head. 

If we are talking about throughout and the scenario you describe i would consider that your requirement is 10 x 6kb/sec which would be 60kb/sec. So yes assuming you had 10 concurrent requests and you tried to move 10 x 100kb it would take 17 seconds. In theory 1000kb / 60kb per sec = 16.66sec. In the real world if the app required data from the db, the response time would vary depending on whether the db had to parse the query, create an execution plan and then read from disk. This all adds up to delays (latency) before the data starts being returned to the app. If this is a commonly executed query (for example in a stored proc) the query has already been parsed, execution plan created and data already held in memory this means the initial delay would be much lower. I would say it would be rare for an app to consistently consume data with 10 simultaneous connections. I guess a video streaming app could set a constant load for long durations. But more likely as other connections start and stop there would be more or less bandwidth available so transfer times could be lower. I would expect to see fluctuations in transfer rates. Also: watch out for kilobit vs kilobyte, And expect there will be some overhead. I.e It takes more than 6kb to move 6kb worth of data.