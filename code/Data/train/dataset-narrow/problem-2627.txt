There is a lot of misinformation floating around here. Buffer Objects were introduced into core in OpenGL 1.5. They existed in extension form as far back as OpenGL 1.4. Vertex Buffer Objects are basically an evolution of ATi's original Vertex Array Object extension. That extension has the unfortunate honor of sharing its name with a completely different concept by the same name in modern OpenGL. In fact, Buffer Objects now describe a generic source of dedicated server (GPU) storage. You can use this memory for vertices, indices, textures, uniforms, transform feedback, pixel transfer, and so on in modern GL. Prior to Vertex Buffer Objects, vertex memory was transferred from client (CPU)->server (GPU) everytime you called . That is because GL did not own the memory referenced by vertex array pointers, and the CPU and GPU generally operate asynchronously with potentially many frames queued up in advance. Since the CPU could modify vertex memory at any time without the GPU's knowledge, implementations either had to copy data immediately when was called or block the CPU until vertex processing finished. Many attempts were made to address this issue over the years, including things like NVIDIA's Vertex Array Range, which allocated special memory for DMA data transfer. However, ultimately VBOs were the only real way to solve it. VBOs plug that hole by making the only way to access vertex memory by using GL commands. Now GL knows anytime memory is changed and does not have to waste time copying data for every draw command. 

By the way, the extension has one other benefit that does not. You can look for this extension by name (assuming you properly query the extension string) whether you have a debug context or not. A non-debug context created by an implementation of OpenGL that supports debug output is guaranteed to list this extension, whereas it may not list . 

You eliminated the need for an alpha channel in the first half of the blend function, but it still remains in the second half. 

Modern hardware performs early stencil testing using the same sort of tile-based approach as early depth. It can reject large groups of fragments before they make it to the fragment shader stage. That means shader units will not waste time calculating data they are just going to throw out anyway. You add an extra 8-bits of memory per-pixel (pretty much a non-issue on modern hardware) but you can skip the fragment shader stage completely for any fragment that fails the stencil test. The best solution is one that can broadly reject a lot of work. The more complicated your fragment shader, the more you will benefit from using stencil rejection. 

No, will not cause you to lose any state. What it will do is make any OpenGL call in the thread where you did this an invalid operation (until you make a device + render context current again). So if you were to try to setup state during a period in which your calling thread has no current context, then you would have issues with state tracking. But in this code, I see no such situation. 

Since the only thing you are interested in is the magnitude of the distance from the center of your frustum to the extremes, this works quite well. The square of any non-zero displacement will always be greater than a shorter displacement (regardless of direction). Now, given the squared distance to each of the 8 points, all you need to know is which one of them has greater magnitude. The squared distance with greatest magnitude is the only distance you actually have to perform a on. This will be the minimum radius necessary to create a circumsphere for your frustum. Thus, what takes you 6 operations right now can be done with only 1. 

You do not want to use the render meshes exclusively (if at all). Back in the days of Quake 3, its curved surfaces (quadratic bezier patches) were evaluated at load-time using a "Geometric Detail" user setting for tessellation quality. If collision were done against those surfaces it would have killed CPU performance. So instead brush planes were used, they approximated the curvature of the surface several orders of magnitude simpler for the sake of collision. Doom 3 did away with this, and evaluated curved surfaces at level build-time... there were several reasons for this, but it meant that collision geometry was once again relatively static. Modern games are back to the point where patch-based surfaces are evaluated using varying degrees of quality at render-time (DX11/GL4 tessellation). You cannot possibly hope to match your collision geometry against what is being rendered on screen when it is dynamically tessellated (you also would not gain much by implementing such fine-grained collision), so that actually necessitates separate simplified collision geometry. In fact, gameplay often relies on collision against things that are never actually rendered. There are collision meshes to keep AI from taking certain undesirable paths, to prevent players from running off into the distance, and so on. You will probably never get away with simply colliding against the rendered geometry in a reasonably sophisticated game. 

I think you may not entirely understand how destination blending works. You claim that you set your output to and then use for the destination blend factor. Because source alpha is constant (0.0) in this example, that works out to be equivalent to . Likewise, if you apply in your second pass that effectively discards anything already in the color buffer: 

Remember how I mentioned that some GLSL implementations compile your shader twice? Well, after all this active vs. inactive codepath determination, some lines and variables will effectively be stripped from the shader the second time it is compiled. Hypothetical Link-Phase Vertex Shader (Active Uniforms) 

More importantly, however, it is generally quicker to draw a textured quad than to do a blit from a read buffer to a draw buffer. It seems almost counter-intuitive; after all, the only reason was created was to copy data from one framebuffer to another. But if you want the best performance just stick to textured quads. The textured quad approach will also allow you to do linear interpolation of the depth buffer, which is a limitation you cannot work around if you do a blit. The only real reason you would ever consider using is if you want to do MSAA resolve of a multi-sampled renderbuffer. This is the least painful way of accomplishing that particular task, particularly if you are dealing with Shader Model 4.0 (DX10) hardware, which only supports multi-sampled color textures. You can implement MSAA resolve in shaders using but unless you have Shader Model 4.1 (DX10.1) hardware you cannot use multi-sampled depth textures. 

You only have a set of 16 possible textures per-shader stage, so way 1 may get out of hand quickly. You have up to 80 possible unique binding locations in GL4 (16 * 5 stages = 80) and 48 in GL3 (16 * 3), but you can only use 16 of those in a single shader invocation. In truth, bindless textures, array textures and texture atlases have all been designed to make this sort of thing more efficient. Instead of breaking up your draw calls to change something as trivial as a single texture, those each allow you a way of working around that. I listed them in order of hardware requirements, by the way - with texture atlases requiring zero special hardware and bindless textures requiring a late model GPU. 

If you enable linear filtering of your shadow map, instead of returning exactly 1.0 or 0.0, will fetch the 4 nearest depths, perform 4 individual tests against your coordinate and then return the average value. This gets you rudimentary shadowmap anti-aliasing; unfortunately a 2x2 window (like this uses) is not adequate in most cases, but it is a cheap form of anti-aliasing. 

Why is this a versus question? One of those things usually follows from the other. Also, 15 draw calls are not terribly expensive. If you are in a scenario where you would be able to collapse those 15 draw calls into 1 then you obviously do not have (m)any state changes between the draw calls. The states you do have to change are probably just vertex buffer states, and those are very cheap compared to things like changing render targets. In modern graphics APIs it is the states you change between draw calls (particularly the validation that follows) that make them expensive. In D3D9 on Windows XP, each draw call lead to a kernel-mode switch irrespective of the states changed and that added a fixed amount of time to each draw call that is no longer relevant. D3D10 / Windows Vista (WDDM) addressed this problem by creating a user-mode component to the Direct3D driver to more efficiently handle command batching and also by reducing the amount of validation by shifting much of it to resource creation time. You will probably have to profile your solution to come up with any definitive answer, but draw calls themselves are not the enemy. 

You might need some casts to make Java happy, I would not consider this more than pseudo-code to give you an idea of what needs to happen. 

Indirect texture lookups are supported by all hardware that implements GLSL (and even by GL's predecessor assembly shader language). In early shader models there was a limit to the number of indirections you could perform (e.g. at most, you could do 4 indirect lookups in a single invocation) but modern hardware no longer suffers from any such limitation. This is even less of an issue since your question mentions this is a vertex shader that you are doing the texture lookups in. Vertex texture fetches are a Shader Model 3.0 feature. As for doing this efficiently, that is a different story. Generally dependent lookups are not cache/pre-fetch friendly, but sometimes if the access pattern (e.g. monotonic coordinate change) is correct this is less of an issue. An interesting point to note is that any time a texture coordinate is computed (as opposed to taken directly from a per-vertex / per-fragment input) while the shader is being invoked, it is technically a dependent lookup. You do not need to fetch something from another texture in order to create this situation (though this is most often what people associate with dependent). 

You switch to and add a depth value to test to your texture coordinates You remove the line of code mentioned above, and perform shadow comparison yourself. 

Effectively in this packed example, each texture in the atlas uses a 258x258 region of the atlas, but you will generate texture coordinates that map to the visible 256x256 region. The bordering texels are only ever used when texture filtering is done at the edges of textures in the atlas, and the way they are designed mimics behavior. In case you were wondering, you can implement other types of wrap modes using a similar approach -- can be implemented by exchanging the left/right and top/bottom border texels in the texture atlas and a little bit of clever texture coordinate math in a shader. That is a little more complicated, so do not worry about that for now. Since you're only dealing with sprite sheets limit yourself to :) 

Regarding "EBOs" that is the only binding that is important at the time that you call . That is because a non-zero value assigned to means that the pointer you pass to that function will point to memory owned by the buffer object you have bound to . Vertex Buffers are different, you only have to bind a buffer object to prior to calling . Just like , this function will setup a pointer to memory relative to the currently bound . However, after returns it no longer matters what is bound to ; you already established what the source of the vertex pointer's memory is and changing the thing bound to after the fact will not change anything. Long story short, your vertex pointers specify which VBO(s) is/are used and the binding establishes the EBO. You can coalesce multiple buffers into one and then draw specific ranges at a time if you want to do this more efficiently (e.g. fewer buffer object binds). The thing is, buffer object bindings are a really cheap state to change - draw calls become expensive primarily because you change other more complicated things like textures or render targets. 

Side determination is done using the direction the vertices are wound in "window-space." Basically, that means after you project your geometry onto the image plane (window), you either shade the front or the back of a triangle. Which side is being shaded is determined by following the path the vertices that make up each triangle takes. Technically, it is much simpler than that... you can determine winding direction using something called the signed area test. Consider the equation for the area of a right triangle:   1/2 Base x Height Ordinarily, Base and Height are assumed to be positive because in every day life negative lengths are meaningless. But if you include direction in either, suddenly and quite meaningfully, the area of a triangle can be either positive or negative. Visualization of Signed Area Test:            As you can see, triangles are always either arranged in a clockwise or counter-clockwise winding, and graphics APIs let you establish which one is supposed to represent the front. You must arrange your vertices according to this convention. Likewise, APIs allow you to cull either the front or back (or both, as silly as that sounds) of polygons. So you need to define both the front-face orientation and the side(s) to cull before you can say which side of a polygon is going to be visible.