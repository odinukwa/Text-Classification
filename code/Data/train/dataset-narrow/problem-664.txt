Alter source table (or tables - in this case You may create view formed one recordset with all data and use it as a source) and add the field which will reference store for the values of fields which will be moved to separate table with datatype dependent of theoretical limit of new table' records amount (INT or BIGINT usually). Create new table which will store fields data moved from source table, with proper fields (and possibly proper datatypes, if they are not optimal for data stored in source table) and PK-autoincrement with the type of reference field created on the step 1. moved fields from source and the result into new table. Of course, it's one query. PK is filled auto. Remove PK and UNIQUE properties from PK field. Check moved data for "non-equal duplicates" (misprints, synonyms, etc.). Correct "PK" values of poor duplicates to the value of proper one. Update source and fill ref. field with PK from new table (join tables by values of moved fields, solving NULL values problem). Restore PK on PK field. Remove records with "poor dups" (they have no referals). Check JOIN result matched to original state. Drop moved fields. 

How about starting with this article, 'The Curse and Blessings of Dynamic SQL' by Erland Sommarskog? 

Most of the forum and example online always suggest to have both and set to ON whenever someone is asking snapshot, row versioning or similar question. I guess the word SNAPSHOT in both setting get a little confusing. I thought that, in order for database engine to use row versioning instead of locks for READ_COMMITTED default behavior, the database is set to ON regardless of what setting. The setting is set to ON only to allow snapshot isolation when starting a transaction (e.g. SET TRANSACTION ISOLATION LEVEL SNAPSHOT) regardless of setting. The only reason to have these two settings set to ON is when it needs to have READ COMMITTED row versioning AND snapshot isolation. My question is, is my understanding incorrect in some way? And that these two setting have to be always set to ON together (especially for READ COMMITTED row versioning)? 

The delete statement without the where clause delete all rows in the table without change of table structure. If the delete statement is within a transaction, then it can be rollback before the transaction is committed. If the delete transaction has been committed, the deleted transaction can't be rollback. Unless use of third party tool or restore the log backup to previous point if available. 

Create new table + copy data + (lock source table + copy data changes in source made by another clients while first copy) + drop old + rename new + restore access rights, FKs and triggers. 

Your subselect looks like remark to server "take EVENT_IT.WRTime from this query:". But server will never understand the syntax You think up, it has its own one... Maybe You want to perform 

Prepared statement is stored in memory till explicit statement or connection termination and can be reused in the same connection if it was not destroyed - including from within another simple or compound statement. Query result obtained by SELECT type prepared statement is stored in query cache on a universal basis independent of the source type of query and can be reused both by another execution of this or another prepared statement and by direct query when its text is identical to the SQL text of the first prepared statement. 

But there is a problem - tables are not joined, so each COMP_IT record will be compared with each EVENT_IT record, and You'll obtain a lot of copies for each record. I think Your query is to look like 

I am looking for a robust open source tool for monitoring SQL server. Ours is a small organization and we don't have any monitoring tool in place and would like to know if any open source tool is available so that we can start monitoring lower environments to start with. 

We are just entering Azure world and will be migrating SQL server soon.I am trying to test throughput on Azure VM (DS13_V2) which is supposed to give 384MB/Sec as per documentation. Here is the configuration : a) No of cores : 8 b) Attached 2 disks (1TB each formatted with 64K block size) to the VM which are P30 disks(200mb/sec). c) I have striped 2 disks into one and called it F: drive with total 2TB which in theory is capable of giving 400mb/sec. Test: I ran SQLIO (with Test file size - 20GB on the striped drive F:) with multiple combinations and looks like throughput is throttled at 256mb/sec consistently. I even ran diskspd which also gives me the same 256mb/sec throughput. Question: I am wondering where the bottleneck is or anything that i am missing as VM is capable of giving 384mb/sec and my striped disks are capable of giving 400mb/sec (200mb/sec each) wherein i should be seeing a throughput of 384mb/sec. Please let me know if any suggestions would help me in doing a better storage test. 

SQLFiddle Update: Martin's alternative is much efficient. Here is another similar approach to Martin's solution, 

@elijah, SMO does have function to shrink file. The PowerShell script below shows the log.shrink method is used to shrink ONLY the log file. The shrink with default or truncateonly option work for me with full recovery model. 

performs best to be unique, narrow, static and ever-increasing by itself. So in this case, the inclusion of DeletedDate actually result the clustered key becomes non-unique non-static (presuming the DeletedDate value could be changed). The in the non-clustered index is useful to cover the query without having to perform a key lookup to the table. However, as the INCLUDE columns only stored in the index leaf level, it does not help in searching for the values of the query predicate (in this case, NULL) A of DeletedDate allows a more effective search on the range of records that satisfy the predicate (NULL). A filtered non-clustered index could further narrow down the subset (only NULL records) and provide a better performance as well as storage for the non-clustered index. With your description showing that the query returns all columns from the table with a single predicate, You could create a single filtered non-clustered index key for DeletedDate with . Test it and examine the execution plan. 

Standard method to extract some field(s) from denormalized data to separate table with the aim of normalization looks like: 

While creating procedure, function or another compound statement consisted from more than 1 command, You MUST use statement. The reason is simple - command to create procedure is one command. But it consists from a lot of common commands. When You enter its code, server has no any marker, what kind of command is terminated by any delimiter - simple statement or compound statement. To avoid it You must use separate delimiters for that aims. Because standard delimiter will be used when server executes procedure, You must NOT alter default delimiter used for common commands. So You must alter delimiter used for fixed compound statement. You must update Your code: 

I think You can perform update You need using 2 statements. 1) Update all records in decreasing order. Single-table UPDATE allows ordering. 

You have 5 records with I think. Any operator applied to NULL (except and ) returns NULL treated as False. 

No in practice. Because in the second approach the operations execution is one-by-one, not parallel. Cause of this is simple - in theory the next step can be dependent by previous one and so impossible until next step fully performed. 

You could look into changing the database to contained database. Contained database user are authenticated by the database, not at instance level through login. It makes moving database to different instance simpler. If not, you could backup the login information using sp_help_revlogin scripts provided at this Microsoft support KB. And execute the output script on the new instance. 

The script show the log file was initially set at 1MB, change the size to 10MB, and shrink it back down to 1MB. Data file size remains at its initial size, 5MB. 

You can try using the maintenance plans under management folder. There are Execute SQL Server Agent Job task and other task that fit your need. you can design the plan in the way that after a step completion, it run multiple tasks like shown below. 

I am trying to determine the duration of backup restore. I executed a few restore command to restore some backups located on network share drive. Here is command and summary, 

I was doing some test on server name change and encounter some errors. Here is the original setup: Server name - ServerA, SQL Server default instance - ServerA Changes: Server name - ServerB Before I change the SQL Server default instance 'servername' 

I am trying to understand why is my query (update statement) causing clustered index update in the plan? Based on my clustered index (in the where clause), I am just updating columns in my table. What is the need for SQL Server to do clustered index update? I am not updating clustered key for SQL Server to reorganize/order the table according to clustered key, instead it has to update the columns or NC index on them wherein clustered key pointer to the NC rows will be the same (since I am not updating the clustered key). Can someone explain me why such behavior from SQL Server? 

Issue : I was moving a non clustered index on a 3TB table from one file-group to another (userfilegroup to index) and it took 11 hours and never completed and i had to kill the process as it was blocking other processes. Rollback is taking forever. It has been 10 hours since i killed the process and no clue what is happening. Locks are still held and are impacting other processes. I see the percent_complete as 0% and kill with statusonly also shows 0% after so many hours. Can you please suggest me if there is a way that i can check if rollback is still doing anything or just got stuck . I am afraid to restart the server as it has to go through the rollback process anyways. Only positive thing that see is under sp_who2 diskIO moving for this SPID. ****Any help is much appreciated....**** 

I don't know how your report table is currently being designed, but I would think the report table list all the rows from the query results (with predefined number of columns, in your case, 2 columns). As long as the query is correctly developed to return all required rows e.g. all enrolled courses, the report table designing part should be fairly straight forward. 

@cicik, if you are looping through the databases at your local server like you mentioned, assuming you already have the list of remote databases stored somewhere in a local table, you could loop through the databases (e.g. cursor) and execute, SELECT * FROM [YourLinkedServer].[YourRemoteDatabase].[dbo].[view]; This should put the remote database context at the database you specified, and hence it works for FILEPROPERTY function and sys.database_files dmv that return values only for the current database. Also, you might want to consider extracting the list of databases on the remote server and run the script on each execution at the remote server (client side) so you won't have to create the view on each database, and won't miss any new databases created without your knowledge. UPDATE If you want to use script to dynamically extract the data from the linked server without adding the view on every database,