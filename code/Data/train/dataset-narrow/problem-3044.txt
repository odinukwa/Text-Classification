Now the less formal part. In data mining SVD has two main applications: for computations (like matrix inversion, least squares, etc.), and for dimensionality reduction (e.g. to compress the user-item matrix in recommender systems). For computations, only algebraic properties of SVD (shown above) matter. For dimensionality reduction, truncated SVD is used: the small elements of $\Lambda$ are discarded, and only $k$ largest are kept. This operation is equivalent to finding a $k$-dimensional hyperplane such that projection of the data ($M$) on this hyperplane is closest (in Euclidian distance) to the original data. In the last case, the analyst would like the decomposition to generalize well to unseen data, and this problem can be formulated well in probabilistic language. If we state that $M$ consist of $n$ IID $m$-dimensional random variables, then it appears that SVD works best if they are joint normal. That's because: a. multivariate normal is indeed distributed nearly a hyperplane, and b. for multivariate normal, Euclidean distance is tightly connected with probability density. Therefore truncated SVD (or PCA, which is mathematically identical) can be viewed as maximum likelihood estiomation of multivariate normal distribution with $k$ independent components. For more details, see the article by Bishop. 

Update. A navigation engineer noticed that such a model would be most accurate when the angle is close to $\frac{\pi N}{2}$. Indeed, near 0° and 180° the angle $\alpha$ is almost linear in $\cos(\alpha)$, and near 90° and 270° it is almost linear in $\sin(\alpha)$. Thus, it could be beneficial to add two more outputs, like $z=\sin(\alpha+\frac{\pi}{4})$ and $w=\cos(\alpha+\frac{\pi}{4})$, to make model almost-linear near 45° and 135° respectively. In this case, however, restoring the original angle is not so obvious. The best solution may be to extract coordinates $(x,y)$ from both representations (in the second one, we need to rotate $(z,w)$ to get $(x,y)$), average them, and only then calculate . 

Random forest, and tree-based models in general, do not handle trends well. The reason is simple: inside any decision tree, there are discrete rules such as: $$ y = \begin{cases} y_1, & \text{if } x > c \\ y_2, & \text{if } x <= c \end{cases} $$ This is a tree of depth 1 (so-called "stem"), but deeper trees obey the same logic. The variable $x$ and constants, $y_1$, $y_2$, $c$ are fit to the train data. And this is the problem: if in the training data $y$ was never higher than $y_1$, your tree will never predict $y>y_1$, even if $y$ is clearly increasing. On the other hand, linear models (such as XARIMA and its special cases) catch trends very well. But they are poor with non-linearities and feature interplay in your data. In my own experience, the following stacking approach works best: 

Fit a simple time-based linear model to your data. Fit a tree-based model (random forest or boosting) to the residuals of your linear model. 

Generally, preprocessing parameters are fit only on the training subset, because otherwise you could overfit your data and overestimate quality of your model on the test subset. With feature standardization, however, overfitting is not so dangerous, so I assume you can preprocess all your dataset at once safely. The best practice, however, is to do all the processing in one pipeline and apply cross-validation to the whole pipeline: 

The sklearn implementation of Lasso that can force non-negative weights (as in this answer) is based on the coordinate descent algorithm. You can reimplement it, using for example coordinate-wise Newton method. For simplicity, I did not inclide intercept into the model: 

You see that the results are not very close, but with longer sampling (1000 points is too few) they will converge. And the shape of distribution is already well matched: 

Use probabilistic prediction. Maybe what you really need is to estimate the probability that the current example belongs to class 2. If identical observations are sometimes marked as 0 and sometimes as 2, you cannot do much better then say "with probability X this is the second class". Use and increase them for class 2. This will lead to more predictions of 2 and less predictions of 0 and 1. With imbalansed datasets it sometimes helps. 

Your problem could be solved either by direct numeric integration or by MCMC. Numeric integration can be performed most easily by scipy: 

So the first colum of $X$ is always one. Then, your model will look like $$ p(y=1|x) = \frac{1}{1+e^{-(\beta_0 x_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_k x_x)}} $$ where $\beta_0$ is your bias term and $x_0$ always equals $1$. Thus, your model would be equivalent to the one with explicit intercept. 

Scikit-learn indeed does not support stepwise regression. That's because what is commonly known as 'stepwise regression' is an algorithm based on p-values of coefficients of linear regression, and scikit-learn deliberately avoids inferential approach to model learning (significance testing etc). Moreover, pure OLS is only one of numerous regression algorithms, and from the scikit-learn point of view it is neither very important, nor one of the best. There are, however, some pieces of advice for those who still need a good way for feature selection with linear models: 

Decision function of GLM itself does not depend on the choice of "reference level". What can probably depend is your threshold, and I guess it is chosen poorly. For the problem of churn prediction, you probably shouldn't use error rate or confusion matrix at all. You can read here why ROC AUC or other metrics can be preferred to accuracy. Or if you still use error rate, choose your threshold in a wiser way than letting an algorithm maximize F1 metric for a single class. What you need in the end is to decide for each client whether to treat her as ready-to-churn (it would certainly cost you $c_1$) or leave her alone (but if she churns, you lose $c_2$). If it is the case, you will profit from your anti-churn measures iff the probability of churn is higher than $\frac{c_1}{c_2}$. This is the natural threshold for your classification problem, and you can use the corresponding cost function to measure your success. Or if you don't know exact losses $c_1$ and $c_2$ in advance, use ROC AUC, which averages all the possible thresholds. And yes, ROC AUC is not affected by class balance/imbalance. 

There is a blog post with a recursive implementation of piecewise regression. That solution fits discontinuous regression. If you are unsatisfied with discontinuous model and want continuous seting, I would propose to look for your curve in a basis of L-shaped curves, using Lasso for sparsity: 

So you ask how does class imbalance affect classifier performance under different losses? You can make a numeric experiment. I do binary classification by logistic regression. However, the intuition extends on the broader class of models, in particular, neural networks. I measure performance by cross-validated ROC AUC, because it is insensitive to class imbalance. I use an inner loop of cross validation to find the optimal penalties for L1 and L2 regularization on each dataset. 

Your intuition about 'no effect' is true in some sense. But this replacement may be not the best use of the information you have. The choice of missing value treatment depends on your initial problem statement. In all the cases I assume that you have already somehow estimated conditional means $\mu_0$ and $\mu_1$ and the common variance matrix $S$. Problem 1. For your test observation, you know $y$, but know nothing about $x$, and want to estimate $x_n$. If this is the case, $\mu_{y, n}$ is indeed your best guess of $x_n$. Problem 2. For your test observation, you know $x_{-n}$ (that is, $x$ except $x_n$) and $y$, and want to predict $x_n$ as best as possible. If it is the case, you should use expectation conditional not only on $y$, but also on $x_{-n}$: $$ \hat{x_n} = \mathbb{E}(x_n|y, x_{-n}) = \mu_{(y,n)} + S_{(n,-n)}S_{(-n)}^{-1}(x_{-n}-\mu_{(y,-n)}) $$ where $S_{*}$, $\mu_{y,*}$ are corresponding parts of covariance matrix and conditional expectation vector. Problem 3. You know $x_{-n}$ but don't know $y$, and you want to estimate $x_n$. In this case, conditional distribution of $x_n$ is a mixture of two normal distributions, $p(x_n|y=0, x_{-n})$ and $p(x_n|y=1, x_{-n})$, with weights, proportional to $p(y=0|x_{-n})$ and $p(y=1|x_{-n})$ respectively. The mean of this distribution is just the weighted average of two means conditional on $y$. But this distribution may be bimodal, and its mean may be not a very useful statistic. Problem 4. You know only $x_{-n}$, and your final goal is to estimate probability of $y=1$. In this case, you should just forget about $x_n$ (you don't know anything of it anyway), and use the distribution of $x_{-n}$ conditional on $y$ for whatever algorithm you apply. 

If the linear model is specified correctly, it will catch and remove the non-stationarities in the data. Thus, the tree-based model will be predicting stationary residuals and find finer dependencies that the linear model. This Python example illustrates the issue: 

You can go on and plot the predictions, to see that predictions of the sine-cosine model are nearly correct, although may need some further calibration: 

Due to Lasso approach, it is sparse: the model found exactly one breakpoint among 10 possible. Numbers 0.57 and 0.825 correspond to 0.5 and 1.25 in the true DGP. Although they are not very close, the fitted curves are: 

To use a single vector (p1x, p1y, p1z, p2x, p2y....) per person would be the most easy and obvious way. Maybe, your prediction would also improve by including some other transformations of these features. But usually trees are eager to use all the information you feed them efficiently. So just a 9-dimensional vector would do. 

Your pass your into where it defines how the cost function and its derivative are evaluated. Just make the same width as . If you want your code to be fool-proof, check variable sizes within your function like 

This approach does not allow you to estimate the breakpoint exactly. But if your dataset is large enough, you can play with different (maybe tune it by cross-validation) and estimate the breakpoint precisely enough. 

If I understood correctly, by bias you mean the intercept term in your model, that is, $\alpha$ in the equation $$ p(y=1|x) = \frac{1}{1+e^{-(\alpha + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_k x_x)}} $$ If it is the case, you can easily incorporate intercept by adding a colum of ones into your : 

SVD is operation of decomposing a matrix $M$ into the matrix product $M=U\Lambda V$, where $U$ and $V$ are unitary matrices ($U^TU=UU^T=I$, etc.), and $\Lambda$ is diagonal. So the formal answer is: 

The question "how NN can be applied if not in machine learining" is incorrect. It's like "how cats can be applied if not in animals"? Cats are animals. And neural networks are a kind of machine learning. So a meaningful question is "where machine learning can be applied?", or "where statistics can be applied?", or "where math can be applied?". And the answer is: everywhere. 

However, if the distribution is multidimensional, direct integration may be intractable. In this case, MCMC methods (Markov chain Monte Carlo) can help. What they do is just make a correct (in some sense) sample from the distribution for which you know the PDF. When you have a sample, you can calculate all your parameters from it as classical sample statistics, just like from any observed data. MCMC algorithms may be implemented manually, like in the example below. Another option is to use the library or its analogs. One of the best known MCMC algorithms, Metropolis-Hastings, works as follows: