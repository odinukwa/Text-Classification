New A records shouldn't have any propagation delay at all, since they aren't cached anywhere. Any DNS client that can't find your new record in a cache should be going direct to your authoritative servers. It's only changes to existing records that can suffer from a delay. 

The easiest thing you can do to have good options later is to write your software in such a way that images can never keep the same filename when modified -- a change to an image must always change the filename. This means that you can set very long cache lifetimes, either through your own caches or through a content delivery network, which will greatly reduce the number of disk reads that you need. (You may need to have some mechanism to immediately flush a specific file from the cache or CDN, if there might be circumstances where an image has to be deleted completely.) To allow horizontal scaling, break the images up into a large number of groups (100 or more), and prefix the path to each image with the group it's in. You may serve all the groups off the same server now, but at a later date it will be fairly easy to use a load balancer to direct traffic to different servers based on the image group. I wouldn't use a clustered filesystem for images, because it adds an extra layer of complexity, and it's probably easier just to use multiple servers and some load balancer rules to spread the load. 

Depends on the subnet mask on the server. If the subnet mask is set so that the .255 address is the last address in the subnet, then there's no way to connect to it over the network. If not, then change the subnet mask on any other machine on the network (Windows or Linux) to match, and you'll be able to connect to it. 

You don't use a root certificate to generate a CSR. You use the root certificate and the CSR to create a signed certificate. What you need to do is to generate your CSR as if you were buying a third-party SSL certificate, and then sign it yourself instead. Assuming you're on a Unix box and have openssl installed, you then do: 

Amazon now make available a complete list of their IP address blocks in JSON format here: $URL$ You can use that to create rules to block all of those addresses. How you do that will depend on exactly how your website is set up, but it would be a lot easier if you had your own server or virtual server that you could administer yourself. 

While banks of ports often share an ASIC, each has to have its own separate PHY. If the PHY has been damaged it could very have a problem while its neighbors don't. That said, output drops are an odd symptom for a physical problem - not impossible, but not typical. Notwithstanding half duplex links, output drops usually have more to do with buffer exhaustion than physical problems. You may get more information by setting up a packet capture on the other side of the wire. A bad PHY would be expected to manifest with some number of physical layer errors (bad CRC, runt/giant, etc) on one or both sides of the link. All in all it sounds like you've eliminated enough that it may be past the point of diminishing returns. I'd recommend an RMA if you have a contract. 

EIGRP isn't nearly as involved as OSPF in establishing neighbor relationships. Routers discover one another via periodic multicast (and sometimes unicast) hellos. The hello is answered with an exchange of topology information and the adjacency is formed. There's not really an equivalent of a DR election, EXSTART, etc.. You can pull information that might indicate a one-way communication (OutQ building up, null SRTT) but it's otherwise pretty on and off. Apologies if you've already looked at it, but the EIGRP MIB has an EIGRP peer entry - check out 1.3.6.1.4.1.9.9.449.1.4.1.1 (or browse it @ $URL$ 

This is usually accomplished with LUN masking (usually on the array) or some combination of zoning and/or VSAN's on the switch (depending on vendor, topology, etc). Generally it's not a great idea to expose all of your LUN's to all of your hosts. 

Do you expect DDoS attacks to be coming from somewhere other than the Internet? If so, wouldn't this imply that your edge router would pretty much provide the definitive source of information about traffic surges and such? Anyhow - as far as the router is concerned, Netflow/sflow (depending on vendor in use) provides both good analytics about the source, destination and type of traffic in use as well as packets per second, overall volume, AS information, etc. There are commercial tools that specialize in DDoS detection from this information (i.e. the Arbor products) as well as a wide variety of open source options that can be set up (in conjunction with appropriate packages) to alarm when certain thresholds are reached, etc. The added benefit here is also that the information can provide historical records of when attacks began, ended, etc. There's also obviously the other major benefit of being able to provide tons of information for capacity planning, traffic engineering, etc. 

I print some data to a connected printed via the dos COPY command. It works great when the printer is set to "spool" but when using "print directly to printer" windows returns an access denied error. The user is an administrator. I have tried adjusting the Win32/Spool directory to allow full RW permissions. I've had the issue under Windows 7 and Windows 8. The printer is connected via USB and is shared using NET USE as LPT3 via local ip, ex: net use lpt3 \127.0.0.1\printer /persistent:yes When using the spooler and printing large sets of print jobs it seems like the spooler is getting maxed out (I can't find a limit to # of jobs) thus the desire to print directly. 

Just switched from dedicated T1's with analog phone lines to cable modem with 10/2 uplink. We're having some VOIP call quality issues on the outgoing side when bandwidth is stressed and I need to setup QOS or a VLAN on our RVS4000 router. Currently all phone traffic (talkswitch device and ip phones) are on it's own d-link PoE switch, and all workstations are on a LinkSys 1GB switch. Both switches are plugged into ports on the RVS4000. I'd like to set it up so that the dlink port has ~512Mbsp dedicated to it for voice at all times. It's my understanding that with a VLAN or QOS I can set this up. I've got QOS setup already with port 5060 to have high priority but it doesn't seem to make a difference. 

I just finished installing 2008 on a new server and enabled PHP support thru the Web PI Installer. I'm having two issues that I can't seem to get past. I see modules, etc. but there doesn't appear to be a mapping for static files. 

I have a Mac setup using LPD to a remote printer/port and it works great. I'm trying to add the same printer on a Windows server and it fails. I've tried standard TCP/IP port specifying the IP as 9.3.3.3:1234 and also LPR Port. With Standard TCP I've also removed the port and configured as raw with the alternate port #. I've got windows firewall set to allow anything outgoing to port 1234. What am I doing wrong? 

I have a server running Ubuntu 16.0.4 with Apache 2.4.18 which seems to be rejecting requests from certain clients. I have another server with Apache 2.4.7 that accepts the same request w/o error. If I run the request thru a proxy like Runscope I get back a result. If I request just a simple php file I will get the error. I have loglevel set to debug but server is not showing the request at all. I have set LimitRequestLine 100000 and AllowEncodedSlashes On as the URL I'm requesting does include an encoded URL. However, if I remove all passed params it still fails. It appears to be a handshake issue. [MacBook-Pro-2:~] admin% openssl s_client -connect www2.nrgsoft.com:443 CONNECTED(00000003) 3519:error:140790E5:SSL routines:SSL23_WRITE:ssl handshake failure:/BuildRoot/Library/Caches/com.apple.xbs/Sources/OpenSSL098/OpenSSL098-59.60.1/src/ssl/s23_lib.c:185: The cert appears to be OK when I check with a site like SSLLabs. $URL$ My Mac has OpenSSL 0.9.8zh 14 Jan 2016 while the server is OpenSSL 1.0.2g 1 Mar 2016. 

You need to install the appropriate intermediate certificate, chaining your certificate back to a root certificate that's installed in the user's browser. You should go to this page, download the certificate bundle that's appropriate for your certificate, and then install the bundle in your web server -- how you do that depends on what web server it is, which you haven't specified. 

The forbidden image is being cached in your web browser. You need to use mod_headers to set it to never cache. Try: 

Get an SSL certificate that uses subjectAltName and is valid for both example.com and example.co.uk, and then you can run SSL name-based virtual servers and do the 301 redirect all on the same server. Or if your server and all your clients support SNI you can do the same with two different certificates (the most likely clients that won't support it are IE on Windows XP and Chrome on Android versions before 3.0). 

You need to run a cleanup on the old nodes, and perhaps a repair -- it won't move the data around of its own accord. See the documentation here for adding new nodes to a cluster. 

Assuming you've followed the rest of the instructions about creating workers.properties and loading the mod_jk module, you just need lines like: 

PHP-FPM doesn't (as far as I know) speak http, it speaks fastcgi. A web browser therefore can't talk to it directly -- you need to have something like nginx in between to convert http requests to fastcgi requests. 

You have two entirely separate connections. The connection from the web browser to nginx is encrypted with SSL, and so nginx has to be configured for SSL. The connection from nginx to Apache to retrieve the data for nginx to send to the browser should be a regular http connection with no SSL, so Apache doesn't need to have any SSL configuration. 

Safe compared to what? Compared to running your different environments on separate physical hardware, VMs are not as safe, but that would be fantastically expensive. Compared to running your environments as different processes in the same instance of the OS, which is the practical alternative, VMs are pretty safe. 

The netmask tells the device how to contact other IP addresses. Anything in the same network, as defined by the netmask, is contacted directly; anything outside the same network has to be contacted via a router (the default gateway, unless thereâ€™s a local routing table with an applicable route). 

The configuration is pretty straightforward, but two items pop out: 1.) Are you positive that your ISP has instructed you to lock speed/duplex? A mismatch in this area will cause major performance problems. 2.) If the interfaces are OK then try turning off rpf checking, replacing it with an ACL blocking traffic from your local network (really any RFC1918 addresses) inbound on your external interface. 

In theory compiling your own source could take advantage of particular compiler optimizations that leverage your hardware. In practice for most applications it doesn't make a tangible difference - particularly when compared to the issues associated with dependencies, updates, testing, etc. This advantage also presupposes a compiler that can optimize for the features in question which is by no means a given. There's a semi-famous quote by Donald Knuth: "Programmers waste enormous amounts of time thinking about, or worrying about, the speed of noncritical parts of their programs, and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered. We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%." At this point rolling your own binaries for the purposes of performance would definitely fall into this category. There are times when particular features that aren't in distributions might be needed (for example) but these cases should be few and far between. 

It really depends on the implementation and the particular configuration/feature set on the LSR in question. In the event of some sort of path protection mechanism (i.e. fast reroute) an additional label may be applied (or popped) to accomplish a bypass. Normally, however, the LSR should normally simply be swapping the outermost label based on whatever LSP bindings have been previously signaled. This sounds to me like a homework question - in which case the answer (beyond actually doing the assigned reading and research yourself) is that the label space in use on the outer label is normally only locally significant to the outbound interface and that, in practice, there is a separate binding for each LSP and hence a distinct outbound label associated with each inbound label. 

A one second resolution is going to be impractical. I'd suggest no less than 15 seconds and collect for at least an hour. There are a lot of tools out there to gather this kind of performance data - many based around rrd. Take a look at mrtg or cacti.