I am tempted to read into this that even when an abstract operation is implemented by a sequence of multiple rep operations, the rep invariant has to hold after each of those rep operations, and when it holds, then the rep value must have at least one abstract counterpart. Many people have addressed some of the challenges posed in this paper. One of the more interesting and recent developments is R Jagadeesan and J Riely's Quantitative Quiescent Consistency [$URL$ 

$[x > 0, x=x'\land(p\Leftrightarrow x>1\land\forall k(k>1\land \exists j(kj=x)\Rightarrow k=x))]$, and 

Short answer: yes. Long answer: Using process algebra as a witness to the claimed existential is certainly admissible, but the way the question is phrased might warrant are more direct answer. If TMs are used as mathematical model for sequential computation, we can surely come up with a concurrent version, and show that it is no more powerful than the good old sequential ones. There are many ways to do so. We could aim for modelling shared variables as a tape shared by a family of TMs, we could add message passing steps to TMs, we could allow TMs to fork off child TMs with their own little tape, or we could invent TM equivalents of starting and ending a supposedly atomic transaction. Every single one of these extensions is easily shown wo be simulatable by an ordinary good old TM. If you're prepared to elaborate where you see a difficulty, the community may be able to help, even if I'm concerned that none of this qualifies as a research-level question. 

The first thing to notice is that an $n$-vertex polygon polygon $B$ is inside circle $A$ if and only if all of the vertices of $B$ are inside $A$. (1) So the solution to your problem is exactly the area of $n$ arbitrary intersecting circles, each of radius $r$ and centered at a vertex of $B$. (You then divide by $S$ to get the final probability). There's an interesting blog about this problem here which includes some heuristic solutions which may be sufficient for your use case. From a theoretical point of view, the furthest-point Voronoi diagram gives us a good starting point. Let's look at a point $p$ inside a cell of the furthest-point Voronoi diagram. We want to know if the circle $A$ centered at $p$ contains $B$. By the above, this happens if and only if all vertices of $B$ are within distance $r$ of $p$. Now, since we're inside a cell of the furthest-point Voronoi diagram, we can just test whether or not $p$ is within distance $r$ of that furthest point. In particular, the area we want within a given Voronoi cell is the intersection of that Voronoi cell with a single circle of radius $r$. This can be easily calculated in time linear in the number of edges of the cell. Since the furthest-point Voronoi diagram has $O(n)$ cells with $O(n)$ total edges, this takes $O(n)$ time in total. Then the total running time is dominated by the time to calculate the furthest-point Voronoi diagram, which is $O(n \log n)$. A similar strategy gives you an explicit region $C$ in the same amount of time. (1): The "only if" direction is obvious. To see the "if" direction, we can first note that by convexity if all vertices of $B$ are inside $A$, then all edges of $B$ must be as well. The same argument then extends to all interior points of $B$ since they must lie on a line between two vertices or edges. 

$[\mathit{true},\mathit{length}(a)=\mathit{length}(a')\land\mathit{elements}(a)=\mathit{elements}(a')\land\forall i(1\leq i <\mathit{length}(a')\Rightarrow a'[i]\leq a'[i+1])]$. With those ingredients, we have quite some power at our disposal, too much as it turns out. Even the domain of $S$ is undecidable because we would need to say whether a spec $[\mathit{true},\phi]$ is implementable or not, for arbitrary formulas $\phi$ of Peano arithmetic, before worrying about how to synthesize implementations. But that is already undecidable. 

You seem to be looking for the origin of a theorem that allows the simplification of a set of recursive definitions to a single recursive function definition. Apologies if I misread that. On with answering the question I think you asked. The crucial year seems to be 1969. Three teams (where I take the liberty of calling some individuals teams) came up with important contributions to the theory of recursion as it matters to program semantics. As I helped to write elsewhere: "In general, recursion appears in the form of a system of so-called mutually recursive procedures, for instance, procedure $S$ with body $P (S,T)$ and procedure $T$ with body $Q (S,T)$. The semantics of $S$ can now be expressed by $$\mu X.P (X,\mu Y.Q (X,Y))$$ and, similarly, $T$ is characterized by $$ \mu Y.Q (\mu X.P (X,Y),Y)~,$$ a result of [Bekic69][1], [Scott.deBakker69][2], and [Park70][3]." [1]: Hans Bekic. Definable operations in general algebra, and the theory of automata and flow charts. Unpublished notes, IBM Laboratory Vienna, 1969. [2]: Dana S. Scott and Jaco W. deBakker. A theory of programs. Seminar notes, IBM Seminar, Vienna, 1969. [3]: David M. R. Park. Fixpoint induction and proofs of program properties. Machine Intelligence, 5:59â€“78, 1969. 

A practical example of this is the Tic Tac Toe computer made out of Tinker Toys at the Boston Science Museum (originally made by a team of MIT students). Of course, this is much simpler than Microsoft Word. Here is a 1989 article from Scientific American describing it. There have also been Turing machines made out of legos (this cheats a bit because it uses electricity---indeed a computer---for movement, but I think the design could be modified to avoid this) scrap metal, and more. 

Chess endgame techniques have been greatly enhanced by the advent of endgame tablebases. Endgame tablebases are lookup tables that solve chess when there are no more than (currently) seven pieces on the board. Here is an online tablebase I've used in the past that works for up to six pieces. Algorithmically, these tablebases are not very interesting; they are generated mostly by brute force. However, they have contributed to several aspects of endgame theory. Wikipedia has a nice summary of some interesting points here. These discoveries also had implications for the "fifty move rule," which states that after fifty moves without a capture or pawn advance, either player can claim a draw. Even before computer analysis, several endgames were thought to take more than fifty moves, and the rule was slightly extended in those circumstances (probably the most famous is the rook and bishop vs rook endgame). As the number of positions requiring these moves became larger, these extensions were dropped and the normal 50-move rule was reinstated in all cases. Modern analysis has shown that some endgames take several hundred moves . This is another interesting article, summarizing some effects of seven-piece tablebases on endgame theory. I particularly like the mutual zugzwang shown in the last position. 

As you already noted, a single example or any finite number of words in $L$ is not going to be the right notion. So why not look beyond? A somewhat famous paper did so in 1967 and revealed what happens when you try to learn in the limit. An alternative interpretation of your question leads to the field of descriptive complexity theory where one looks for a minimal representation $\langle M,w\rangle$ of a string in terms of a TM $M$ and an input word $w$ to it. Asking whether a single word had been (better?) generated by a CFG or a DFA is indeed ill-posed: either one would be a special case of such a pair $\langle M,w\rangle$. Finally, given $w\in\Sigma^*$, asking for the smallest DFA $A$ and CFG $G$ with $w\in L(A)$ and $w\in L(G)$ is going to disappoint. The simplest DFA (with just one state) and the simplest CFG (with one non-terminal) will do because the universal language $\Sigma^*$ is blatantly regular. To make this intersting you'd need some negative instances, which brings you back to reading said famous article. I recommend it. 

store $x$ and $y$ initialize counter $c$ to $0$ for each of your $n^{2n} 2^n$ DFAs a. simulate it on both words (this step is $\mathcal{O}(|xy|)$) b. increment $c$ if both simulation runs are accepting output $c$ 

Yes, here (this was a class project; beyond that I know almost nothing about that code). The author of that code has a nice writeup of the data structure as well. Some more recent academic work on that data structure has experimental results. You may try looking through those as well (though of course, many are variations on the original concept). 

While this doesn't answer your exact question, CFG parsing is a decision problem that was reduced from matrix multiplication (so it is as hard as matrix multiplication in a sense). Specifically, in [1] it was shown that CFG parsing is as hard as boolean matrix multiplication. In particular, if CFG parsing (a decision problem) can be solved in $O(gn^{3-\epsilon})$ time, boolean matrix multiplication can be solved in $O(n^{3-\epsilon/3})$ time. An interesting aspect is that matrix multiplication can also be used for fast CFG algorithms, so the problems are computationally equivalent in a sense. The reduction has some unusual aspects because boolean matrix multiplication requires $n^2$ output bits, whereas CFG parsing only requires one. To deal with this, the paper assumes that the CFG parser solves certain subproblems when parsing the string (and argues that this is a reasonable assumption to make). The reduction makes $n^2$ queries to these subproblems to obtain the product matrix. Thus CFG parsing is a decision problem that is computationally as hard (in a sense) as matrix multiplication. However, this is not specifically a decision version of matrix multiplication, and furthermore, the reduction relies on the idea that CFG parsing is actually made up of $n^2$ decision subproblems. 

I am coming to this question even later, but I am equally fascinated by it. Why the theory of imperative programming is considered less settled than that of functional programming evades me. It probably started to get serious with Scott and de Bakker in 1969 with their analysis of the meaning of recursion in a simple imperative language [1]. When the imperative language gains features, the story gets a bit messier but that is just the price to pay for being closer to the metal. To name one of the more comprehensive efforts, in 1980, de Bakker, de Bruin, and Zucker wrote a monograph on the subject [2]. Others were mentioned above. These references of course pre-date separation logic but [2] nevertheless tackles arrays and mutually recursive procedures. [1]: unpublished in 1969 but appeared as Jaco W. de Bakker and Dana S. Scott. A Theory of Programs, pages 1-30. In Klop et al. J. W. de Bakker, 25 jaar semantiek. CWI, Amsterdam, 1989. Liber Amoricum. [2]: Jacobus W. de Bakker, Arie de Bruin, Jeffrey Zucker: Mathematical theory of program correctness. Prentice Hall 1980. 

Constant functions have unique fixed points. Another criterion that may be applicable is to compare approximations, $\mu_i = \bigcup_{k<i}f^k(\bot)$ and $\nu_i = \bigcap_{k<i}f^k(\top)$, of the least and the greatest fixed points. Trivially, as soon as $\mu_i = \nu_i$ for some $i$, it has been settled that $f$ has a unique fixed point. The problem with this characterization is that, depending on the lattice and $f$, it is incomplete unless you are prepared to explore transfinite approximations. 

Yes, you can use standard dynamic array methods, like doubling in size when it gets too large. Some data structures like to avoid this, as a full rebuild is a really large cost. But this isn't a big deal for us, as this data structure is highly amortized anyway. In other words, you have to do about $\Theta(\log N)$ rebuilds of the entire data structure every $N$ insertions anyway, just to maintain balance. This just adds an extra one. 

Lee, Lillian. "Learning of context-free languages: A survey of the literature." Techn. Rep. TR-12-96, Harvard University (1996). 

Traveling salesman is open on sold grid graphs, but Hamilton cycle (the unweighted variant) is known to be polynomial. Discussion of both on the open problems project: $URL$ 

I assume you're talking about maintaining keys, in order, in an $O(N)$-sized array, with $O(\log^2 N)$ amortized worst-case update time. 

No, so long as there aren't too many elements. This is fairly easy to see---so long as you don't have too many elements, you can distribute the elements in the whole array evenly, meeting the density bound of the top array. But the arrays have looser requirements as they go down. So if the top array fits in density (and the elements are distributed evenly), the entire data structure is correctly balanced. 

Short answer: no. Long answer: No, because the synthesis problem is in general undecidable once you allow for a modest amount of expressive power in the assertions (pre- and post-conditions). The specification for your synthesizer $S$ would look something like this. Its type would be $S:\mathit{Assn}^2\rightarrow\mathit{Prog}$, that is, given two assertions (a pre- and a post condition) it would return a program. Its functionality would be given by $\{\phi\}S(\phi,\psi)\{\psi\}$, for all $\phi,\psi\in\mathit{Assn}$. I understand your question as "is $S$ computable?" Before we need to get technical, note that we've been a bit sloppy. $S$ can hardly be a total function for we could feed it an infeasible spec such at $[\mathit{true},\mathit{false}]$. Consequently, the best we could hope for is a partial function that works for all implementable specifications. To answer the refined question we need to answer another first. What expressive power would we need in the assertion language? Firstly, assuming that we're living in a simple world where we only deal with integers, we require the usual operations from integer arithmetic. Secondly, we need a mechanism to refer in the post-condition to the value of a program variable in the pre-condition. Without such a mechanism, we cannot even specify an increment-by-one function. Various such mechanisms exist. Let us here use one borrowed from the Z specification language: in the post-condition primed occurrences $x'$ of a variable $x$ refer to its value after the program has terminated whereas unprimed occurrences denote its value in the pre-condition. With this convention the increment-$x$-by-one function is specified by $[true,x'=x+1]$. Thirdly, we'd also want quantifiers so we can at least specify simple examples such as