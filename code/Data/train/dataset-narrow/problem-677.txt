You can configure this behaviour through the Administration Assistant, where you can choose to start (or not to start) the database instance with the Windows service. Alternatively, you can do this with oradim, e.g: 

This is really bad: . You did not bind the variable properly. means char to nchar conversion, so the column type and variable type is different. The database will implicitly convert one type to another, this time you got lucky, but the other way around it would make impossible to use an index, unless you have a function-based index especially for the affected columns - do not do that, bind properly. If the column is an nvarchar2 type column, then bind as an nvarchar2 type variable. If the above still does not help, did you actually measure the steps in the execution plan? Did you identify the steps that take long? Run the below and report back with the output: 

You can read the raw trace file (if you are patient enough and interested in more details), but if you want only the query texts, you can use on the trace file (e.g: orcl_ora_6789.trc): 

But this should list the image copies that would be recovered, the starting and ending SCN, and the logfile used for recovery: 

should be replaced with the name of your instance. After that, restart the listener. One way to find the name of the instance is: , while logged in. Based on your , I guess your instance is called . So: 

and are built-in internal services, they are not exposed to the listener and clients. is your default service, because an Oracle database by default has a service with its , and registers itself to a listener running on the default port (1521) with the as service name, and as instance. The service exists, because you have the XML database option configured, and the parameter is set. You do not need to modify manually, the database handles that parameter based on the service configuration. In my opinion, the use of services should be promoted, not prevented. Using multiple services for a single database has the benefit of being able to manage clients connecting through different services seperately, when troubleshooting (tracing), or prioritizing workload (resource manager). 

Through Oracle Virtual Directory or Oracle Unified Directory. See above. Technically it should be possible to use as standalone, but not supported at all. None for direct connection. See above. 

If the standby site becomes unreachable, the primary site will not delete archivelogs as they will be still needed by the standby site. If you try to backup and delete these archivelogs, you will receive an error, and eventually your archive area will become full and your database will stall. Setting to lifts this "restriction". If the stanbdy database becomes unreachable, you can manually set to , with that, you disable that destination, and the primary site will be able to delete the archivelogs. An other way to work around this is to write your backup scripts in a way that is prepared to handle this situation. By adding the option to a operation, RMAN skips the above check and deletes the archivelogs specified regardless of the state of the standby database. For example adding this line to the end of your backup script: 

No. Depends on the amount of data in your LOB segment. If your segment is 200 GB, but you have only 5 GB data in it (because the rest of the data was deleted), then you do not need 200 GB. 

No, there is not. You have already defined manually the filters for each table, why not use that? Just dynamically generate the parameterfile before running : 

The inner view originally contained DISTINCT, and because of that, it was eligible for Complex View Merging. However, as the DISTINCT was eliminated earlier by another query transformation, this view became a simple SPJ (select-project-join) view, and now is eligible for Simple View Merging. Here are some interesting information about view merging: $URL$ This part is especially important in this case: 

At this point, the procedure becomes INVALID automatically, but you can not query errors related to it yet: 

Unfortunately Oracle does not have anything like for example in MySQL. You can do the above in a PL/SQL block, catch the exception, then do nothing with it. 

or to cache reads only. +1. You have a bigger SGA configured than your database. (Hopefully you configured it at instance level as well). You can force full database caching as: 

Then you create the partitioned version of this table with 1 partition where you can store all your existing data: 

Seems simply a password issue. Different instances may have different passwords, the SYS password is not necessarily the same for all the instances on the same server. Make sure is configured properly in the instances, and the password file exists for instance on the server in (Linux-Unix) or (Windows). If you do not know the correct SYS password, you can recreate the password file, for example: 

Even if you manage to create a database with (from scratch (Custom Database or manually), not seed or template) , the indexes will be still created without these settings and you have to take specify them manually. 

You can have not only a single query, but your whole session execute queries against a specified time using flashback query. You can set this time with . Sample data: 

insert parent row with new PK value update child tables to new value delete parent row with old PK value 

So you want to concatenate multiple rows and columns into a single string, just to parse it later. Overhead 1. Next, you want to write PL/SQL for something that can be easily done in plain SQL. Overhead 2. Something like this (I just used random views for the 7 data source): 

This is incorrect because you need the 32-bit version (x86). Now if you check stage/ext/bin, there is a file vcredist_x86.exe, and obviously no x64, since it is a 32-bit installer. Try editing oraparam.init and fixing the above line to: 

After that you can use this credential when creating an external job. For example calling your convert script with 2 arguments (input and output file) 

An Oracle database keeps track internally the number of rows affected by DML/DDL operations on a table since the last time statistics were gathered on it. I have never used it for anything important, but you can give it try. First flush the monitored data from the memory to the underlying table, so you can query it (this also happens periodically): 

Note that this is a Query Transformation, but NOT a Cost-based Query Transformation. As you can see from the "Final query" part, the optimizer removed DISTINCT from the query. But there is no such optimization for GROUP BY. DISTINCT is used for retrieving distinct values, but GROUP BY is used for producing aggregates, not just distinct values. Even if the optimizer can skip sorting or hashing the data, it can not skip counting, adding, calculating the average, etc, and this is the important difference, so DISTINCT and GROUP BY are not handled in the same way (even if aggregates are not specified). Another case of eliminating the DISTINCT part, when it is obviously unnecessary, for example: 

With the below regexp, you can keep the 2nd part () and add the extra characters around it you need (): 

HASH JOIN - no NESTED LOOPS JOIN - yes, depends on statistics Oracle can not use index range scan for join predicates when performing a hash join. But it can use indexes for other predicates for the ("second" table), so hash join does not automatically result in a full table scan of the probe table. 

Listener listens on port , but you are trying to connect port . Update your with the correct port number (). 

This will not work. First, as the error message states, you are not allowed to use local collection types in SQL statements, you need to define them in the database. Second, that syntax does not exist. So first you define the type: 

It is better to create a role, grant that role to the other user, and maintain the grants to the role when you create new objects. 

I had to try it myself, because I did not believe SQL*Plus can not handle this. Apparently, it can. Script for generating the create statement: 

Removing (commenting out) the filter from the relevant part of the definition of that filters dropped objects: 

The installation of a PSU is not complete without the execution of the postinstall script. Opatch works at file level, it replaces/relinks files in the Oracle home. Changes inside the database are performed by running the postinstall script. Sometimes I see DBAs forget this step. The postinstall script maintains the table (on which the view is built) with the actual patch versions. In 11.2, you run the postinstall script as (stated in the README): 

Unless the application or middle tier supports distinguishing the application sessions from the database sessions (for example setting module, action), this will not happen. The closest you can get to this is querying or , but this is just sampled data, and it requires Enterprise Edition + Diagnostic Pack licensed. 

Depends on the data distribution. I can easily construct examples for both cases (index vs table scan). Just think about the case when the columns are unique or nearly unique (index), or when all the rows are the same (table scan). Technically your index can be used for the above query. Given the general nature of a column named , I would say the index will be chosen, but it is not guaraanteed. The predicate will be processed at the table level, not the index level, even if you make sure that the NULL rows are indexed as well: 

There is no such index/object type as JOIN INDEX in the USER_INDEXES/USER_OBJECTS views. A join index is a BITMAP index, search for that. Also you can query what columns are indexed from USER_IND_COLUMNS view. Thinking backwards, you can filter to one specific column and check whether it is indexed or not (this will also reveal the index name): 

By the way the functions in your list are unnecessary, because you filter data with and , so you can have only 1 possible value, and you could just write: 

If you had flashback enabled or created a restore point before activating the standby database, you can flashback the database to that point/SCN, and the standby database can continue from there like nothing happened. You can do this manually, or if you have Data Guard Broker configured, you can use the command to do this. In case you have not prepared for this, you will need to rebuild (duplicate/restore) the standby database. Next time do not do this, there is a feature called Snapshot Standby designed for testing tasks like this, that works just as I described above. 

Normalized means that the timezone information is not stored. When storing such data, the timestamp value is automatically adjusted to be in the same time zone as the database server. When retreiving such data, the timestamp value is automatically adjusted to be in the same time zone as the client. For example, I simulate that my client is in a different time zone: 

You can not force the creation of a single trace file, but you can consolidate these traces into a single file with , based on your SID, module name, action name, service or client identifier. 

If you want to keep the PDB in the source CDB, you do not need to unplug the PDB, you can just simply clone the PDB to another CDB: 

Yes, and name resolution may be still different. Or more likely the firewall may block connection attempts from host B on the listener port. Just use telnet, nc or whatever tool you have installed to check the host + port combination: 

and are deprecated long ago. LOG_ARCHIVE_DEST_N is available since 8.1 in EE, and 10.1 in SE. You can define up to 31 destinations, but only the first 10 can be local. Remote destinations are not supported in SE. 

The cost of an index depends on its clustering factor. Clustering factor measures how well aligned are the rows in the table - in respect of the columns of the index in question. It is calculated as: inspect all the rows from table in order (order is defined by the index columns). Every time a row is found to be in a different (not the currently inspected) block, increase the clustering factor by 1. If consecutive values in a table are near each other in the same block, the clustering factor of the index will be low. If consecutive values are scattered across the table, the clustering factor will be high. Simply put, the lower the clustering factor is, the cheaper the index becomes. Despite the myth, the clustering factor can not be decreased by rebuilding the index, because it depends on the table data distribution. However, you can set the clustering factor manually by: 

There is no for the instant client, but even if you had the full administrator client, may not return anything useful. 

works, because you did not specify or , and in this case, is the default. does not need to perform comparison. But if you try (which removes duplicates), that will also fail: 

If you set it to 10.2.0.4.0, the downgrade should be still successful to version 10.2.0.4, but not anything lower. 

You should not edit the spfile directly as it contains binary data, but you can create a pure, text-based parameterfile from it: 

PCT fast refresh is not possible because there are no partitions in this example. Verify fast refresh: 

Using the above information, now you are 1 Google search away from finding what you need. Whether you trust those sources, is up to you. 

If you really had in your table, SQL Developer would display in the data grid. Clean your data, then try again. 

I fail to see the point of a requirements such as avoiding , but here it is, without , or hardcoding a constant (still, hardcoding is done by using ): 

As long as you try connecting locally as SYSDBA with a user that is member of the group (defined at install time, typically ), you can type any password, because you will be authenticated based on your group membership, not the password. Successfully logging in with the above does not necessarily means that the SYS password is dolan123. 

You can not store Chinese characters in a Western European characterset (). Define the column as NVARCHAR2 to use and insert the string as below (notice the modifier): 

I see no COMMIT up there in your code. By default SQL*Plus autocommit is off. Data changes are not visible to other sessions until they are not commited. 

That is just the paging in SQL*Plus. By default the paging value is 14, that includes a blank line at the start of each page, the line with column headers, the line with heading separators (hyphens) and the rest (11 rows) is for user data. If you increase the pagesize, the result of your query will fit in to 1 page: 

In order to perform open resetlogs, and incomplete media recovery should be performed (even if it can not do anyhting): 

Notice that the RUID is different, it is the UID of grid user. For example, this can lead to a situation, where file generation from the database with is successful when executed in a scheduled job, but fails when executed from a remote session. With the above setup, server processes forked by the listener inherit the privileges of grid user. Scheduled jobs spawned by the database itself inherit the privileges of oracle user. If they do not have the same privileges, they will behave differently.