For the OP's purposes partitioning isn't likely to achieve much performance benefit for operational queries, but it may be useful for system management. If there is any significant requirement to report aggregates across large volumes of data then an appropriate partitioning scheme may help with that. 

A useful little protip - Visio is quite widely available within corporate I.T. suites and Visio Pro or higher can be used to reverse engineer and document a database. Visio professional has a database reverse engineering tool that will read a database schema and create E-R models from it. Both PostgreSQL and MySQL support ODBC INFORMATION_SCHEMA tables so the ODBC driver for the Visio DB modeller will work with them. Start by creating a database diagram, and then look for 'Reverse Engineer' under the 'Database' menu. As a bonus, if the physical DB has missing foreign key constraints you can add them back into the model for documentation purposes. For a more complex schema you can also create several diagrams showing different subsystems. I find myself using this to document systems at client sites on a semi-regular basis and it's often the only tool available. 

Is B.I. a business or technical project? There are too many variables to answer this categorically; I'm tempted to VTC the question as it doesn't really have a single correct answer. However, on second thoughts I can say a few reasonably meaningful words on the subject. Customer Intimacy Business Intelligence (or more prosaically reporting) is very requirements heavy in comparison to the amount of implementation work involved in the delivery. Getting the requirements right requires nuanced analysis but may not require technically sophisticated solutions - in some cases a spreadsheet is sufficient. I.T. departments may not have a sufficiently close working relationship with individual consumers within the business to effectively gather the requirements for a B.I. solution, or may simply not be responsive enough. In many cases desktop tools such as Excel and Access are sufficient for the job, so reporting teams are often run directly within the individual business units. I have seen a number of sites where B.I., reporting or analysis teams were run within the business precisely because of the issues with responsiveness and working relationship. In many cases they even had their own server hardware, ETL processes and development staff. Strong business sponsorship vs. responsibility without authority On a larger scale, data warehouse or business intelligence projects are middle men. They do not produce data, but aggregate and present it in a form that is easy to report on. This means they are integration projects and have to interact with many little empires within the business - sometimes called 'gatekeepers' in the literature. This interaction - and the attendant real or perceived shifts in power - tend to politicise data warehouse projects and make them heavily dependent on the cooperation of many third parties. These third parties will have responsibilities and agendas of their own and no responsibility for the project success, so they can seriously affect the progress of a project and even derail it without necessarily being held responsible. Building a non-trivial data warehouse system involves putting fingers in a lot of pies, including ones where they may not be particularly welcome. Without a business sponsor who has the political will and the political clout to bang heads together on behalf of the project, this generates a significant risk of failure or limitations on delivery that can't be worked around. Weak business sponsorship is the #1 cause of B.I. project failure. Getting it right The flip side of this is that a larger scale project is much more challenging technically and from a 'software engineering' perspective - keeping the code base well organised. This requires technical skills that may or may not be present in a B.A.U. reporting team. Most reports or other dependent feeds from a data warehouse will be tightly coupled to the data model unless care is exercised in interface and data mart design - and even then a database has a limited range of options for supporting abstractions efficiently. This means that getting the model wrong can be very expensive or even infeasible to fix. Failure to build the system properly can also allow a large class of leaky abstraction problems where the underlying data exposed through the system is too dirty or inconsistent to use through an ad-hoc reporting tool. A project where the brief has aspirations to a self-service facility must be built so that the data presented is clean, consistent and presented in a form that works well with the ad-hoc reporting tools. Leaky abstractions place non-obvious elephant traps in the data and make it easy to report incorrect data from the system. Even if the problem could (in theory) be corrected with user training this can significantly erode end-user confidence in the system. Getting conformed data (where it may behave significantly differently across multiple sources) is a critical success factor for any self-service reporting project and may well be a non-trivial undertaking. In many cases technology is a non-issue, although I have seen projects switch platforms due to technical limitations of the original choice. In some cases data volumes may require the use of certain technologies; in others a ROLAP tool may lack sophistication necessitating the use of dedicated OLAP software. A server consolidation environment may not be the best choice of platform for a TB range data warehouse system (don't laugh - I've seen this attempted on more than one occasion). Platform choice can be politicised and can also have a significant bearing on project success or implementation costs or time scales. Conclusion A large B.I. project will need strong sponsorship, priority treatment from the business and good technical leadership to work effectively. In most cases the actual technology used is less of an issue, but an unsuitable platform can impede the delivery of an effective system. 

When you go to write out the record you filter the write by the timestamp/version so that the write writes nothing if the timestamp/version has changed. This makes the write atomic, e.g. 

Often the data can be profiled by just poking around it with a database query tool such as SSMS and writing SQL queries directly against the source, or a copy of the data loaded into a staging or scratch area. Desktop tools such as spreadsheets (pivot tables can be useful) or database systems such as MS-Access may also be helpful. Purpose built data profiling tools such as Pandora X88 are also available, but these tend to be relatively expensive. Often they are quite a hard sell, even on larger enterprise projects where they represent a tiny fraction of the overall budget. Often your best approach is to simply copy all your source data into a staging database and poke around it. This work can also form the basis of your staqging processes later on. 

Depends on your DBMS platform to some extent. On the SQL Server based development I did back in the late Jurassic period we used timestamps as these are automatically updated when the record is written. You load a record with: 

The simplest way is to just store previous versions of the whole document rather than attempting to calculate deltas. Lots of systems do it that way even though the XML fields use a lot of space. It might be worth evaluating the data storage requirements for doing this before you go to whole lot of trouble to capture differences. 

If you want to do calculations that aren't directly available in the cube script, many OLAP tools such as the late, lamented ProClarity will allow you to formulate queries involving custom MDX based calculations. Unless the cube doesn't have the information you need to do the actual calculations, the custom MDX calculations should be able to support any computation you need. Although OLAP queries are traditionally associated with statistical queries in aggregate, if you have a dimension that allows drill down to detail you need it is certainly possible to formulate queries that will calculate medians, percentiles or histogram queries from which modes can be inferred or computed. For example, this has an example of a pareto analysis query, which is based on rankings. Many cube products can operate in a hybrid or relational OLAP mode where they do not persist the data themselves but query it from an underlying database. In addition, pure ROLAP tools such as Business Objects, Report Builder or Discoverer can query from an underlying database and do work row by row. However, they tend to lack the sophistication of dedicated OLAP products, and they don't have much in the way of statistical analysis capability out of the box. 

Now, put up a cube over the database with Customer and Product dimensions and a measure group for the sales leads. A MDX query like the following would show you which customers wanted to buy Widgets but not Dohickeys: 

So, as an opinion, use timestamping or versioning if you have the option - it's much simpler. You can do field comparisons or hashing if you don't have the option of changing the database, but it's more fiddly. 

Locking Locking can also cause performance issues. If your system is performing badly under load, look at the profiler and perfmon counters related to locks and check whether there is any significant contention. has a 'BlkBy' column in the result set that will show if a query is blocked and what is blocking it. Also, profiles with 'deadlock graph' events (if you have queries deadlocking) and lock related events can be useful to troubleshoot lock issues. 

On the trials and tribulations of making generic B.I. products For the moment I'm assuming that the physical structure of the data model remains the same and you just want to change fields and their definitions to site-specific customisations. If you want to frig the database structure you're really into building bespoke systems. The other assumption that I'm going to make is that you have a star schema. This can be done with metadata for each of the dimensions and fact tables that has the attributes for each table. This metadata allows you to construct a customised view over a generic base structure. You will have to structure the ETL to map to the base data but you can configure a customised view over it. There are a couple of ways you can manifest the base tables: generic columns and an entity-attribute-value structure. In each case, the customer's view is customised by metadata stored in the system. Generic columns A table with generic columns will have columns with names like 'Code1', 'Code2' etc. and groups of columns of particular types (code, money, float, int etc.). You implement a table like this for every dimension in your system, and a table with some integer and money values for each of the fact tables. This is customised by creating a view that links the columns to a business facing name. Te metadata is used to generate the view. If you have cubes and/or a suite of reports sitting on top of this, you can also programatically annotate the dimensions and report fields based on the metadata, allowing them to be productised but customisable. The view is simply a view over the base table that renames the columns. 

What I like about this process This is a bit heavyweight, and was designed for deploying into fairly bureaucratic and opaque production environments. However, it has the following strengths: 

In order to implement what you want you need something functionally equivalent to or on unix. Windows does ship with a utility called that has similar functionality to grep, so you may be able to use that to extract relevant lines from your output file. As far as I am aware there is no equivalent to shipped with base windows builds. This sort of thing is trivial in a unix shell scripting environment but can cause issues on a raw Windows build. If you can't implement what you want using , three basic approaches come to mind: 

At some point in these debates, somebody often chips in with a comment about XML columns ("I know, I'll just use XML"). To paraphrase JWZ, now you have two problems. Around 1998 they installed another circle in hell just for people who defile their systems with XML blob fields. Although it doesn't look slick and high-tech, just having a set of user attribute columns on the table is by far the best solution. It is the most efficient, and there are many ways to make this approach more user friendly at the database level. EDIT: Here is a snippet of T-SQL showing how to create a view that flattens out an EAV structure. Note that you wound have to write a generator for the view based on the application's attribute metadata and re-create it every time this is changed. 

Building temporary indexes for ETL jobs is not necessarily bad practice, as the index builds are fairly quick. Where it might not be so efficient is if you have relatively small incremental updates on very large tables, but it sounds like this is not the case here. The only caveat is if you expect the tables to grow substantially with time. If they are just work tables for the ETL then it may well be oK. If the tables are fact tables that will accumulate large data volumes over the next 5 years then the index rebuilds may get slower with time. For staging data, dropping indexes then loading will make bulk loads to staging much quicker, and you may well need to add indexes to the staging tables in order to support queries supplying the ETL process. 

Similar things can be done with base metrics on the fact tables. A bit of experimentation will show you how to programatically annotate columns onto a cube dimension or report model, so it is possible to make a template and use a metadata based system to synchronise the cubes or front-end artifacts by generating columns. If you build reports in a templatable way (base columns with particular signatures) you may also be able to do something similar with canned reports if you have a significant body of these as a part of your product. This might be getting into diminishing returns, though. One golden rule: Never, ever edit generated code, or make an architecture that relies on subsequent human intervention in generated code. That way lies madness. 

The question is a bit vague, but I think this answer (a) may provide some useful guidance and (b) is too big to fit in a comment. Capacity planning a server is a bit more complex than that. You need to take some basic queuing theory into account. Your model would describe a server at 100% capacity, which is actually likely to cause performance issues. In practice you would want to aim for the server running perhaps 25-50% capacity at peak load if you want your site to have consistent response times. Queuing models state that the average wait time increases hyperbolically as the system approaches saturation. 

To take a different approach, what is driving your read workload - Is it the application, or do you have a bunch of reports hanging off the system that are driving the load? Depending on the nature of your workload you might be able to re-shuffle your hardware or push some of it off onto other machines. A couple of ideas: 

The cube doesn't store medians, modes (or even averages), but you can write queries that calculate them and embed them as calculated measures in the cube. The ability to embed this sort of computation is one of the main unique selling points of OLAP technology. If you have a dimension that can identify individual rows (which could be a degenerate or 'fact' dimension derived from an identifier on the fact table) then you can do a query based on individual rows. However, OLAP works in terms of dimensions and aggregates, so you would need to have a dimension capable of identifying individual rows (with an aggregate composed of one value). Any OLAP tool can do what's described in (2), plus they generally support a mechanism known as 'drill-through' where the cube will return a bordereaux of the transactional data underlying a given slice that you drill through into.