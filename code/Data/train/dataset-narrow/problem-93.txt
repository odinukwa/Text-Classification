Case 2 If the case 1 test fails, you could then check for the "trivial" existence of an intersection. Now there are probably better ways to do this, but the following, relatively cheap, approach occurred to me: Consider just curve A: 

You aren't doing perspective correct texturing. At each pixel, $(X,Y)$, you need to be computing U and V with the equivalent of: $U=\frac{aX+bY+c}{pX+qY+r}$ $V=\frac{dX+eY+f}{pX+qY+r}$ The denominator of both is linearly interpolating 1/W, while the numerators are, respectively, interpolating U/W and V/W. I.E. for each triangle vertex you need to compute 1/W, U/W and V/W and then interpolate these. You can do it via barycentrics if you wish but you must do the division per pixel. 

To align vectors, you are applying a rotation. If you consider a simple rotation matrix, e.g. rotation of angle $\theta$ around Z axis, then you will trivially see that its inverse matrix, a rotation of $-\theta$, is the transpose, since $sin(-\theta)=-sin(\theta)$ and $cos(-\theta)=cos(\theta)$. An arbitrary rotation matrix, R, can be constructed by multiplication of other rotations, e.g $R = A \cdot B$. If the inverse of A and B exist, then $R^{-1} = B^{-1} \cdot A^{-1}$. Similarly we know $(A \cdot B)^T = (B^T \cdot A^T)$. Put these together and you have your answer. 

Write two different rendering classes keep the data and simulation stuff in its own area and just vary the renderer. Take high level functions like createTexture or copyTextureOnto and have an if statement checking which language the device supports and have a custom implementation for it. Coming up with your own graphics objects that are akin to the objects from the framework (such as MTLCommandEncoder or MTLTexture) and use those to build you app and just use the implantation that matches your graphics frameworks. Essentially recode the functions one of your graphics libraries has that you use and just add an if statement to your implementation is called on a OpenGL supporting device. 

I am making a 2d game in opengl es 2.0 Inside are tons of rectangles defined by 4 points and one 4 component color. I am using vertex buffer objects, and I have heard that it is efficent to interlace the data. So like traditionally you would do 

When you have to code a software to support multiple graphics libraries how do you generally do it? Do you loose any efficiency with the technique you use? 

I am running this fragment shader on every pixel on screen. I am trying to make it as efficient as possible. BTW I am running open gl es 2.0. The code segment below is only a sample from the code, there are about 56 different calls to Gaussian() spread across different functions. I am wondering wether it would be valuable to replace the calls to Gaussian() with there appropriate resulting float value. I know a lot of times stuff like this is pre-calculated on compilation of the shader, or calculated only once because the gpu realizes it is the same calculation for all fragments. So would it be worthwhile for me to manually calculate each of these and replace them with their values? 

I have been playing around for a day with Perlin noise, and I am currently stuck. Most of the modifications I have made to the algorithm produce more or less the following: 

As title says. What are the broad, general ideas of voxel based global illumination and what exactly makes it efficient enough to produce real time dynamic shadows with so much photorealism? 

There are 2 problems i am trying to solve involving rasterization of geometry in 3D. The first is, my 3D texture is created with a top down viewd, so triangles that are perfectly orthogonal to the planes represented by the texture are not visible. The second is, due to how rasterization in OpenGL works, if a triangle is visible inside of a voxel, but too small to cover the middle of it, it will be discarded. In general I am looking for a method that guarantees that if a voxel contains a triangle or part of a triangle ti will be marked as occupied. 

Ok so I am trying to understand how to combine these 2 techniques and I am failing. The main goal is to reach the O(lights+fragments) complexity of deferred rendering, instead of O(lights*fragments) of naive/forward rendering. This is how I am currently doing things: Say we have n lights. Then we generate n shadow maps O(n) Now we do one render pass to generate the normal, diffuse color, depth.... information in the geometry buffers. Then we check for every fragment whether it is occluded in ALL lights. and shadow it if it is. This has an O(Lights*Fragments) complexity, which is wrong, so how can you reach the ideal asymptotic complexity? What should you do? 

Right now I am just taking the oscillating float time and passing it in directly, but eventually I will put it on a function so it fades in, temporarily is extra bright, then goes to the source texture. Edit: I solved my problem, to my surprise the GLSL log function is in base e rather then 10. 

Most texture data is stored in a format where the rgb brightness is incorrect because it has not been square rooted (right?) therefore if we want to display it correctly When using textures is the gaussian formula technically incorrect? If so what would be the mathematically correct method. What part of this problem is the monitor responsible for? Does it square or squareroot the values it gets? What part of the problem is the GPU responsible for when you ask it to sample a color at a point? Does it square or squareroot the values it gets? Do people know about this property and just not act on it because square rooting a number is an expensive computation? If so do we have a good way to quickly approximate the squareroot of numbers from 0-1? How do you apply the knowledge from that video in your graphics? 

You might find the 1983 paper that introduced this**, i.e. Lance Williams' "Pyramidal Parametrics" informative. You can ignore the scheme, in Figure 1, he used for the layout of the MIP maps as I doubt any hardware, at least in the last 20+ years, used that approach. ** (Actually, Williams *may* have described the technique at an earlier SIGGRAPH (1981) as part of tutorial/course but I've never been able to get hold of a copy) 

Decoding Speed: You don't want texture compression to be slower (at least not noticeably so) than using uncompressed textures. It should also be relatively simple to decompress since that can help achieve fast decompression without excessive hardware and power costs. Random Access: You can't easily predict which texels will be required during a given render. If some subset, M, of the accessed texels come from, say, the middle of the image, it's essential that you don't have to decode all of the 'previous' lines of the texture in order to determine M; with JPEG and PNG this is necessary as pixel decoding depends on the previously decoded data. Note, having said this, just because you have "random" access, doesn't mean you should try to sample completely arbitrarily Compression Rate and Visual Quality: Beers et al argue (convincingly) that losing some quality in the compressed result in order to improve compression rate is a worthwhile trade-off. In 3D rendering, the data is probably going to be manipulated (e.g. filtered & shaded etc) and so some loss of quality may well be masked. Asymmetric encoding/decoding: Though perhaps slightly more contentious, they argue that it is acceptable to have the encoding process much slower than the decoding. Given that the decoding needs to be at HW fill rates, this is generally acceptable. (I will admit that compression of PVRTC, ETC2 and some others at maximum quality could be faster) 

My goal is to take a point that is inside of a circle with a given radius and put it on the circumference. Recently I have been normalizing the vector between the point and the center of the circle then multiplying that by the radius. However I need (if possible) a less computationally expensive method because the distance formula is expensive. Another thought I had was to use "atan2" to get the angle between the two points and then use sine and cosine multiplied by the radius to get the point on the circumference. Which method do you think would be faster for the computer to process? Can you think of a faster method. Details about the simulation This is an ios application written in swift. Basically there are a bunch of particles moving around randomly. And the user is putting down fingers. Each finger is a circle with a radius that grows as time goes on. The part that is inneficent is that if the dot is ever inside of any of the circles (attached to touchscreen touches) that it goes on the circumference of the circle. 

Although I am not sure it is physically accurate to have those spikes on the edges of the diamond I sure do like the effect that creates. 

Assume we start with an axis aligned cube at known origin/corner Origin with known dimension Dim. This cube is divided into 8 equally big cubes with length Dim/2 and corners Origin, Origin + (0,0,dim/2), origin +(0,dim/2,0)... Until we hit all 8 corners (the origins of the subcubes) Given the corners for one of these cubes, we wish to mapp them such that each maps to an index from 0 to 7 without collisions. How cna this be achieved? 

So I am facing the following issue. Say I have a minecraft like mesh (i.e I have a bunch of cubes on top of one another). I wish to smooth out this mesh in such a way that only the regions where there are edges are smoothed out and such that the smoothing concentrates around the edges (e.g if I have a staircase the overall shape of the cubes that make the staircase should still be close to the staircase but instead of having straight edges I should have something much smoother). I should be able to smooth things out locally, so that if a block is removed from this mesh, I can run the subdivision algorithm only on the newly generated coarse region that is created after the block is removed. Does anybody have any ideas as to which algorithm I could use? 

What it does is draw capsules on the screen. Basically these capsules connect where the particle was to where it is. On the cpu I compute the vertices of this with some linear algebra, but in screen space terms the rectangle we will be shading has a maximum width of and a minimum height of . If the capsule is moving faster then then the capsules geometry will start to get longer to accommodate. Here are some diagrams of what is going on: This is an image of a single capsule. Because the distance between the current and last position is not large enough so the screen-space geometry is a square. You can see that to compensate that lighter area is shaded in as well to connect the two points. 

I am starting to think about how I can make my app (currently written in Metal) available on older device that dont have Metal as well as Android. This is a predicament because while I could write two separate rendering classes that use the different frameworks it doesn’t teach me anything about how to code for multiple graphics libraries. I can only get away with this because my rendering code is fairly short and has few states and draw calls. I want to learn for the future. Writing a renderer that supports multiple frameworks especially becomes difficult when thinking of the two libraries I hope to support — OpenGL and Metal — which are quite different in how they expose the GPU. I am curious how exactly one usually does design their code for apps that can target multiple graphics languages. Here are my theories of what could be done: 

But even this has issues. For example if I want smooth hills I loose jaggy shores, same as if I try to create more valleys. And doing this also reduces the likelihood of generating islands in the middle of the bodies of water, moreover in terms of topography, the mountains look chaotic, they don't really from mountain ranges, they rather spread semi-chaoticly, which is not exactly what I need. 

I am attempting to code a simple spring mass system simulation in which a sphere pendulus hangs from a string (a spring) by the top and then another sphere (controlled by the user) can move and interact (collide) with this hanging sphere. I have already coded the dynamics of the spring part of the simulation, but I do not know enough to add the rigid body dynamics. I would like either an explanation or a source to understand how to algorithmicaly do this simulation. 

I am trying to fetch a given geotery and through (potentially multiple) render passes create a 3D texture that stores the color value of said geometry into an associated texel. In other words there is a 3D box that i am trying to color based on the geometry inside of that box. Currently I am attempting to do so with the following vertex shader: 

So I have a batch draw call I am doing with various squares. I have a 4x4 tile map with the numbers 1-16 going in right/down order. When it gets to the fragment shader their is a varying float "id" that holds the number. In a perfect world if the id was 0, it would sample the top left and display "0", if the id was 4 it would sample the top right and display "4". Thankfully this is mostly working! However the numbers 5, 9, and 13 (which happen to be on the left of the tile map flicker! The values on these squares just change frequently. I have traced it down to being the fault of the sample location. And probably this function here: The goal is to take the id and return the proper row and column to texture map. 

I am trying to recreate a filter that Facebook released back in 2015 when gay marriage was legalized in the US. Unfortunately, that filter no longer works/exists so people are left with only knockoff ones that dont do quite as good as a job. None of them manage to do quite what the actual one did espeically in the red, yellow, and purple stripes. I am trying to figure out what kind of filter they used to achieve this. I have tried a bunch of colors with each of the blend modes available in my photo editing software with no luck. Perhaps it is more complicated than just one math equation? Perhaps they processed the background image before putting on the overlay? I did find in the blog that the algorithm to achieve this was a O(n^2) algorithm but that is about all I know. Any ideas of what could have caused this? Below are some reference photos I used to attempt to find a pattern. Example 1: Mark Zuckerburg (note this isn't exact. I couldn't perfectly recreate the crop they used)