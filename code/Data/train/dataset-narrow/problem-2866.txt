I'm using a slope scaled depth bias to reduce shadow acne with shadow maps. However, it's causing artifacts when used with normal mapped surfaces because the bias varies over the surface, where it should really take the normal of the underlying geometry as a bias for all texels instead. 

Specifically I'm talking about particles as seen (for example) in the UE4 dev video here. They're not just points and seem to have a nice shape to them that seems to follow their movement. Is it possible to create these kinds of particles (efficiently) completely on the GPU (perhaps through something like motion? Or is the only (or most efficient) way to just create a small particle texture and render small quads for each particle? 

This is probably API independent (more dependent on hardware implementation), but just in case, I'm using OpenGL. The question is restricted to PC hardware. I have a couple of questions concerning the output textures that are being rendered to (e.g. in gbuffer geometry pass) 

This is, of course, probably not the fastest way to do it. Question is: What are faster ways to do it or what are some optimizations that can be applied to this technique? 

where shader is a shader constructed from vertex, geometry and fragment shaders. It compiles fine without warnings. Depth and stencil testing are disabled. textureSizeX and textureSizeY are the width and height of the 3D texture. Vertex shader: 

Setup I have an entity-component architecture where Entities can have a set of attributes (which are pure data with no behavior) and there exist systems that run the entity logic which act on that data. Essentially, in somewhat pseudo-code: 

Generally physics engines are split into two major parts: Collision detection and collision resolution. The solver is just responsible for the second part. After your collision detection part determines which pairs of objects collide, and where, the solver is responsible for creating the correct physical response. "Iterative solver" just means that the solver uses an iterative method to calculate those responses. It's iterative because you can, for various reasons, not calculate a completely physically accurate response in physics engines. Therefor these engines perform certain approximations (via numerical integration) to the correct physical response. These approximations are done more than once per simulation frame (hence, "iterative") since it generally gets more accurate the more iterations of these approximations you have. 

From what I've read, with a simple directional light, the color of a point, if you only take diffuse reflection into account (intensity = 1), should be 

As you can see, at the bottom of the function glDrawBuffers is called so that (at least intentionally) first the diffuse texture is drawn to, then the position texture, and then the normal texture. The shaders for them look like this (here worldViewProjection and world matrices are attributes because I use them in instanced rendering. They're actually only advanced once per instance of the mesh that I draw): Vertex: 

The solution is to simply turn around the displacement vector (multiply it by -1) if it's pointing towards the shape from which the object needs to be pushed away. To find out if the displacement vector is pointing towards the shape, you first have to get the general direction from object a to object b by subtracting their centers from each other. After that, you check the dot product between the displacement vector and the direction(a,b) vector that you just created. If it's > 0, the displacement vector and direction(a,b) are pointing in the same direction, hence you need to flip the displacement vector. 

First I'll explain the principle, then the drawing part: In my example I'm assuming that you have a progress value of 0 to 100 (%), even though anything else will do. On a straight line, the current position of progress-bar would simply be that progress value. On a circle you get that position with trigonometry. Any point on a circle is given as: 

So I need to find the axis to rotate around, with the constraint that I give the algorithm an axis in camera space to rotate around. Sounds confusing. For example, let's take a camera that points up (0,1,0), is positioned in the XY plane at (-1,0,0) and is looking at the origin (0,0,0). So it is looking "along the x-axis". Now I want to rotate the camera around the "camera-space-x-axis" (1,0,0). In words: The axis that "points to the right" when looking through the camera. Simply rotating around (1,0,0) doesn't work, since it will rotate the camera around the world-space x-axis. From the point of the camera however, the x-axis is in fact the z-axis in world space (0,0,1); How do I go from the given camera-space axis to the world-space axis? I've tried just applying the inverse transform of the current camera transform to the given camera-space axis using 

As far as I know, the second sample has undefined results, because the value of uv is outside of uniform flow control. Is there any way to efficiently (i.e., not copy back the contents of the texture and upload it as a uniform to the shader or something) do what I want to do? 

(stolen from here). An important property of functions in SH representation is that the integral in the last line boils down to a dot product between the vectors containing the SH coefficients for projected functions L(x,omega_i) and max(N_x dot omega_i,0). For example, for each surface point in our target image that we wish to compute indirect illumination on, L(x,omega_i) is available in a precomputed vector of a 4 elements (for 2 bands SH approximation) containing spherical harmonics coefficients. It's not really important where it came from, just that it's available at this point. From my understanding, to actually evaluate the lighting at a surface point [x] is to project the function max(N_x dot omega_i,0) into spherical harmonics, and dot it with the coefficient vector of the incoming radiance (and then, multiply it by the stuff in front of the integral, surface albedo / pi). The first 4 coefficients (2 bands) for max(N_x dot omega_i,0) projected into SH basis are known, they are: c_0 = 0.88622692545... c_1 = -1.02332670795 * n_y c_2 = 1.02332670795 * n_z c_3 = -1.02332670795 * n_x So the irradiance at that surface point with normal n would just be E = dot(L_sh,n_sh) where L_sh is the coefficient vector for the incoming radiance, and n_sh the coefficient vector for the cosine lobe that we just calculated. Specifically, L_sh is the same max(...) function projected into SH basis [with the normal from which the light originated], just scaled a little by the color its transporting. The above actually looks fine and seems to produce somewhat correct lighting (hard to judge without ground truth reference). What's confusing me now, is that I've come across several sources (for example here, page 17), which, in the final step, when evaluating the lighting, don't actually use the coefficients from above to project max(N_x dot omega_i,0) into SH basis, but instead calculate n_sh as n_sh0 = 0.282094792f n_sh1 = -0.488602512f * -n_y n_sh2 = 0.488602512f * -n_z n_sh3 = -0.488602512f * -n_x These four coefficients are actually just the spherical harmonics basis function coefficients parametrized in cartesian coordinates (here they are for spherical coordinates). What's up with that? Why aren't they projecting the max(...) function into SH basis instead? Am I misunderstanding something? Unfortunately, the usual sources (gritty details and stupid SH tricks) don't seem to go into details of this either. 

I've just now tested rendering a lot of sprites using XNA's SpriteBatch class. What I basically do is this: Somewhere in the code, I have a list which contains all sprites that have to be rendered. I prefilled this list with 2500 Sprites, all of which have the same rotational angle, texture, position etc. I then draw them like this: 

before I start rendering, then draw the geometry using the shader program. The rendered results look correct, the only problem is that the program draws results of the position into the normal texture, and vice versa (weirdly enough, the diffuse texture is correctly drawn to). This is a screenshot of the test draw, where I expect bottom left to be diffuse texture (correct), bottom right position texture and top left normal texture (those two are switched): 

As for #1, you need to take a look at space partitioning algorithms. They divide up space into smaller subspaces in order to make searches faster (in your case, you're searching for every cube that collides with your view frustum). Since most things you have are static, and also cubes, an easy suggestion for you would be the Octree. As for #2: I'm pretty sure that's not going to be a problem. Not drawing faces that you can't see with cubes usually equals not drawing faces that don't face towards you. The process of this is called backface culling and is usually done by the graphics card. So the only really limiting factor for #2 could be bandwidth consumption on the way to your graphics card. But I really doubt that's the case, since your world is static and thus only needs to be sent to the graphics card whenever something in the terrain changes, which will not be very often. Even if you sent everything you need to draw over once every frame, this will most likely be no bottleneck as long as you batch everything together in a reasonable manner (faces with same textures, etc). However, if this really turns out to be a bottleneck, you should search the internet for software backface culling algorithms. Here's an example. 

You have two problems here: First problem: Adding some kind of position offset to your lookat matrix (currently your camera position) is essentially supposed to center the "light camera" (whatever your light source sees from its view) around that position. This is done because the shadow map will only cover a limited range, but if your shadow maps in one directions extends to 10 [whatever distance unit you have], but objects are positioned at 12[whatever distance unit you have], you obviously still want those objects to cast shadows. So you apply this position offset to all objects via the shadow caster view transform, so they're essentially "pushed into" the visible range of whatever the camera sees. The reason this causes artifacts if done in a straight forward way is because due to how rasterization works, the amount of pixels that are rasterized for each object are not the same if you change positions of either the objects or of the visible range that the camera sees. The solution to this is to snap that position offset you apply to multiples of the shadow map texel size. I.e. the offset you apply must be a multiple of the shadow map texel size in each direction. You do this by just taking your normal offset, and snapping it to a multiple (either the nearest, or just floor/ceil, doesn't matter I think) of the texel size. 

You need some kind of gamestate-management system in place. I could write a long answer explaining this, but people who are a lot more experienced with XNA and game programming than me have already done that, so I shall point you towards them. Personally I've found this example to be excellent: $URL$ It shows a gamestate-management system with the ability to easily add as many gamestates as you like, as well as defining transitions in and out of the gamestate. You should give it a read! 

So I have to sample the depth texture completely per each slice, and I also have to go through the processing (at least until to discard;) for all texels in it. It would be much faster if I could rearrange the process to 

In the vertex shader, I need to make a texture fetch, where the texture coordinate itself is read from some other texture. 

I'm trying to implement a free-look third person camera (using glm). I know that the general transformation is 

I'm trying to implement deferred rendering and for now I only have diffuse, normal (in world space) and position (in world space) as textures inside the GBuffer. I create the GBuffer like this: 

Now to the drawing part. I don't know anything about Actionscript3 (or Flash, for that matter), so I can't tell you the exact procedures, but in general there would be two ways to do this: First, and (in my opinion easier of the two): Draw the partial circle manually as simply a connection of colored lines with a certain thickness (if you don't need a texture on it and a simple color suffices), or draw the circle as a connection of textured quads (if you need texturing). Second: You draw a picture which represents your fully-loaded circle and load it as a texture, put it on a quad and render it. Then, you again manually draw a partial circle over that quad and only render the part of the textured quad where the circle is rendered in front of it. In OpenGL, for example, you could easily do this using the stencil buffer. If you want to know how to calculate the different points on your circle to use them to assemble the required polygons/lines, you can take a look at the above equations again: 

From my understanding, spherical harmonics are sometimes used to approximate certain aspects of lighting (depending on the application). For example, it seems like you can approximate the diffuse lighting cause by a directional light source on a surface point, or parts of it, by calculating the SH coefficients for all bands you're using (for whatever accuracy you desire) in the direction of the surface normal and scaling it with whatever you need to scale it with (e.g. light colored intensity, dot(n,l),etc.). What I don't understand yet is what this is supposed to accomplish. What are the actual advantages of doing it this way as opposed to evaluating the diffuse BRDF the normal way. Do you save calculations somewhere? Is there some additional information contained in the SH representation that you can't get out of the scalar results of the normal evaluation? 

However, only the first point is drawn, as expected in the middle of layer 0 of the 3D texture. I can move around the coordinates of the first point and get the expected results. The points after the first point are always ignored, however. What am I doing wrong? Hardware is GTX 770 with driver version 331.82 (latest version). Tested it on GT 555M (Notebook GPU), same results. The OpenGL device context is version 4.3. edit: Tried so far: When I extend the attribute input of the vertex data to a vec4, and change the argument in glVertexAttribPointer accordingly, as expected the z and w components of the vec4 contain the coordinates of the second point, so the positions seem to be in memory as expected. Similarly, if I change the offset using glVertexAttribPointer (e.g. putting (void*)(sizeof(vec2)*3) as the last parameter), the respective point is drawn. Just to test if something may be wrong with the rasterization in general, I tried emitting triangles instead of points in the geometry shader. This works as expected. 

I use this format for all types of meshes. However, 16 bit and the ~65000 possible values are more than enough to represent textures that are mostly smaller than 4096 in size per dimension. Now I'm wondering how to most effectively switch to some 16 bit based texture coordinate format, with respect to maximizing performance, i.e. minimizing the bandwidth and storage used, as well as minimizing ALU operations. Mainly I'm wondering: 

Animation can still perfectly be split between logic and rendering. The abstract data state of animation would be the information that is necessary for your graphics API to render the animation. In 2D games for example,that could be a rectangle area which marks the area that displays the current part of your sprite sheet that needs to be drawn (when you have a sheet consisting of lets say 30 80x80 drawings containing the various steps of your character jumping, sitting down, moving etc.). It can also be any kind of data that you don't need for rendering, but maybe for managing the animation states themselves, like the time left until the current animation step expires or the name of the animation ("walking","standing" etc.) All of that can be represented any way you want. That's the logics part. In the rendering part, you just do it as usual, get that rectangle from your model and use your renderer to actually do the calls to the graphics API. In code (using C++ syntax here): 

You can use rigid body systems to create soft body and fluid dynamics. Physical reason behind it? Well, essentially, atoms are small rigid bodies. This isn't completely true of course, but for simplicity you can view them as such. And soft bodies are obviously made from atoms. So, how do you create "soft bodies" using a rigid body engine like Box2D? The essential technique is to bind points in your body together using joints. For example, if you connect the ends of a list of lines together using distance joints (meaning the distance between two joints must be constant), it will roughly behave like a piece of cloth. But that's just a rough description. A lot of "elementary field work" with Box2D has been done by ewjordan, one of the developers of the Java-port of Box2D. Here's a good starting point: $URL$