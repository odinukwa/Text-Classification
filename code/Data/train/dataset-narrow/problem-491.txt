I am trying to setup a new sql server 2008 cluster, on windows 2012R2, and the installer is failing on the cluster shaed disk availability check. I have verified that there are 5 disks assigned to "Available Storage" when viewed in the fail over cluster manager. Some background, this is my second attempt to install sql server on this cluster. The first time, the cluster object was unable to create the new computer object during the cluster installation. This caused the installer to fail do to lack of permissions. I have since resolved this, and have run "Remove node from cluster" to uninstall sql server from the node. I am now trying again to run the installer. About the environment, OS: windows 2012R2 SQL Version/Edition: 2008/Enterprise I am running the installer from the current cluster host(node1), and all storage is owned by node1. This includes the quorum, as well as the 5 disks assigned to the available storage group. Both cluster nodes are up and available, and accessible either through the node names, or through the windows cluster name. There are no cluster validation warnings that I know of, but I have asked the windows admin to rerun the validation tool to confirm that, that is still the case. SQL 2008 is required by the front end application (I pushed for at least 2012, but was told it was a no go) 

I assumed that the error was being caused by differences in the module path, so I explicitly set the path via $env:PSModulePath to match the path of my powershell session where the code runs fine. I am pretty new to powershell, so any help you could provide would be appreciated I am just doing some testing right now, so everything is running locally on my desktop. I am running SQL Server 2012. 

I am trying to track down a cause for a difference in oracle impdp processing, and I am not finding anything, so I am wondering if anyone here can explain the cause of the differences. At times I have seen times where using the parallel=x parameter in oracle datapump will cause either multiple tables to be inserted at once, up to the value of 'x', or other times will use parallel threads to import a single table. I have not been able to track down what might be causing the difference in performance, and I am wondering if anyone has an explanation, or even just a direction to point me to. It would be helpful to determine why it runs in a given matter. For instance right now I am monitoring a single table, data_only import that was started with parallel=8 in the command, but the import job is only using a single thread to do the import, as shown by the following hint the insert query /+ PARALLEL("XXXXX",1)+/. If the process would use the maximum 8 threads specified to should run much faster. 

if you are running the deletes in batches, do you have an index on column that you are using to chunk up the process? IF there is no index and the query is resorting to full table scans to try and find the data that should be deleted, you could be adding a lot of time to the process. Unfortunately backing up the good data then dropping/truncating the table is probably your best bet. You could always rename the existing table and create a new table to do this rather then trying to extract just the data you need to keep. This way if you found your initial load of data was lacking, you still have everything to go back to for a second look. 

Well, it looks like when the first sql server install ran, it also configured the file server services, which were still trying to hold the disk and the network name. Odd thing is, the disks where all showing 0 dependencies. This probably could have been fixed if we opened a case with microsoft, maybe cleaned up some registry entries, etc, but the guys from out windows team wanted to just rebuild from bare metal, as it would be quicker and easier. I was on board with that, so that is the solution I am implementing 

SQL Server 2016 Enterprise Edition SP1 Database Mail was working fine with no problem until a server patching was performed. I tried to do all the troubleshooting available but still all mails got queued and no records in the log. Finally when I checked the database mail program I didn't find the executable file in the Binn folder or any other folders! My questions are how this happened and how to solve it ? 

I need some clarification regarding Always on avilabilty group in SQL SERVE 2016 connection to the secondary DB. Now i have availability group with 2 nodes. 

we are going to upgrade from SQL server 2012 Standard edition to SQL Server 2016 Enterprise edition. we can't have a downtime more than 30 mins .The new SQL server will use always on availably group 1 primary and 2 read-only secondaries . I am looking for the fastest way to do this upgrade. I am thinking of creating mirroring between the old server and new primary server , then shut down the application update the connection string , fail over the DB to the new server, stop mirroring, start the application. After the application is up, start in configuring the Availability group. IS this a correct way to use 

Update I find this Delete in the Query Store , there were 3 plans and I forced SQL to use different plan. I am wondering Why SQL choose to use the most expensive execution plan? 

note : the counts for IX_CoursePrerequisiteAssignment is not accurate because i just rebuild it These columns are the predicts for the top executed queries 

when the application is busy and there are many deletes coming, the performance become so bad, My questions are. How to make this delete perform better? why the granted memory is so high, it steals the memory from the buffer pool ? Note: there is on delete cascade on the 2 tables Highschoolcourse and highschoolgrade. 

i would like your help in the best way to tune and re-write a stored procedure. it is consuming a lot of time and don't know what should i do to make it faster 

I don't know much about the feature, my question is as now we can send read-only workload to secondary replica . My question is what will perform better single instance with 32 cpu 1 primary and 1 secondary with 16 cpu each or 1 primary and 2 secondaries 12 cpu each Or in other way is the number of worker threads affected by the number of cpu in the primary only or the number of cpu of all members in Availability Groups 

However the trigger is preventing the user from log although the IP exists in the IPAddress table. Any idea where in the problem in the trigger 

we have a SQL Server DB running on Amazon RDS, i need to refresh it. But in RDS there is no option to restore a backup. What other ways i can use to refresh this DB? It is 60 GB, and has 200 tables. Thanks 

If I want another report application to connect to the secondary DB. The connection string should be point to secondary or the listener Also, i when i try to connect to the databases secondary server with a SQL authentication user i failed. 

I see that i don't need all these indexes as they overlapping. What is your recommendation for best performance ? 

On a QA server I need to give access to specific login from just one IP to log to this server. I wrote This login trigger 

If I am going to set an Alwayson Availability group with 3 nodes, with read-only routing. Do I need to configure a Quorum ? 

This table is very active table and i need to avoid blocking. If i need to create indexes on this table should i create 4 indexes. 

If I have database db1 with schema sch1. I want to create another schema sch2 which will be used by different application and the tables in sch2 will be populated from tables in sch1. So my approach is to create triggers on insert, update, delete perform this task So my questions are 1- Is this a good way to do this or there is a better way? 2- I donâ€™t want to affect the performance on db1 so I am want to replicate it to different server and create the triggers on the subscriber DB, so replicated server will have Sch1 and Sch2 on it, Can I do that? This is on SQL Server 2012 Standard Edition Note: tables in sch2 have different names and different columns name. also more than one table in sch2 may get populated from data from sch1 

So, I will use one catalog and one index on the multi-lingual table using a neutral language resource. Then I'll use the appropriate language setting when searching so everything is parsed correctly. 

Just asked a Microsoft Certified Trainer during a developer course for SQL Server. He suggested using a single catalog, and having the two indexes, "title" and "body" in the same catalog. The only thing to watch is with too many indexes in the same catalog, there is a greater chance for fragmentation within the catalog. With two columns though, this should not be a concern. 

Or, should it be designed with an active flag? This would involve a single insert for each setting, followed by several updates. 

Then, when doing a search using FREETEXT or FREETEXTTABLE, once again use the "LANGUAGE" argument. Microsoft defines the "LANGUAGE" argument as follows: 

Is it possible in the foreign key definition to restrict the categories that can be referenced based on another column in the table? For example, say there is a column in the table. I'd like the foreign key constraint to be restricted to those categories where . 

Setting history is not really important to me, so I wouldn't need to see if the active flag was ever used. Is it best to use a smaller table and insert, delete, insert, delete, etc., or is it best to add that extra bit, and insert, update, update, update? I'm running SQL Server 2005. 

I would like to implement full-text search on the article title and body. The complication is that the title and body can be in one of six languages, indicated by the "lang" column. I see there are noise word files for several languages, so depending on the currently set language, I would like to use the appropriate noise file when searching. Does this require one catalog per language? If separate catalogs are used, is it possible to, for example, not include French articles in the English catalog? 

I'm using SQL Server 2005. I have two tables that contain aggregate information. The information is constantly being updated, generating almost 5GB of log data a day. (That's larger than the entire database!) I'd like to disable logging on these tables, as rolling back is not really necessary. I would however like to keep logging on the other tables in the database. Is it possible to disable logging on certain tables within the database? If not, can I place the two tables in the same schema, then disable logging on the schema? Is the only option to move the two tables to a separate database, and disable logging there? Update: I guess I'll explain why I really don't need to log the activity on these tables. The two tables are filled with GPS data, so they get quite large. The first table is capturing raw locations from six Android tables in the field. New data from each of the tablets comes in every 5-10 seconds. That information is then aggregated as locationA, locationB, travelTime. The goal is to ultimately have the shortest travel times between all locations, based on actual driving data. The data is only for a small city, and only accurate to four decimal places, so it is manageable. However, as new raw data comes in, there are slower travel times that need to be updated, and new ones that need to be inserted. Once the raw data is aggregated, it is purged. We're not going backwards to longer travel times, so that is why rolling back does not matter so much in these tables. 

Each user can choose to apply the settings, or not. The interface allows them to quickly toggle the settings on or off. The setting is by default off. I'm looking for best practice on the table. Should it be designed such that only active settings are included in the table? This would involve a lot of inserts and deletes on the table.