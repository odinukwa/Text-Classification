They can handle large variations of inputs because the neurons have weights and those weights get optimized as part of learning a good model. So even though the model might take in a value that ranges from 0 to 100,000, if it isn't relevant to predicting the outcome, it will have a very small effect. 

There might be a fancier way to create dynamic weights but I would probably start with oversampling the subset and see how that goes. So if you've got classes A, B, and C and want to emphasize C, make a duplicate copy of C and insert that into your training data. In other words, assume you have six records to train on: 

The Hadoop stack is difficult to setup and people complain that you can't trust any answers to problems over 6-12 months old. I would recommend getting a pre-configured Hadoop/Spark setup from Cloudera or HortonWorks. Both have free community editions. 

If I had to choose, it would be one of the last three as they are more sophisticated. If you have some time, try them all out, sweep the hyperparameters, and compare the results. This free EdX course has a segment where you do that comparison of different models in Azure ML: $URL$ 

Do you mean 150 million rows and 70 columns? Yeah, it is easy to run out of memory creating the DTMs. This is a similar question to here: Text Mining on Large Dataset Like they talk about, I would create in splits (batches) and save the chunks out to a file or a DB. When it is time to do classification, see how it works if you pull 10,000 randomly sampled rows. 

Note the true negatives are useless - we don't care if the state stays in the same spot (below/above threshold). Therefore, the accuracy is NOT important for this system. Do not try to minimize the wrong metric. This extends to "do not use ROC/AUC" (in your problem). You can use a PR curve, but be careful with those (no interpolation, no AUC PR, these are wrong/useless). Basic metrics that you could use are F-score, precision and recall, or a re-weighting of those (if the false alarm is less important than a missed alarm, for example). We also would like to deal with a huge "lag", trying to predict many time periods before (predicting just before the change is less useful). For this reason, I'd suggest investigating models other than MLP, notably those based on recurrent neural networks, such as LSTM. Also consider using time series prediction instead of classification - the literature on the subject is extensive and matches your problem really well. 

This way of putting it is valuable to some techniques, notably "one-class classification" and "PU learning" (learning from positive and unlabeled examples). These techniques are very relevant when you want to know "does my item follow a given distribution D?", which is exactly what you're looking for. Still, if you have (or can collect) a lot of data that is neither A nor B, you may simply label it as a garbage class C and use a common classifier - however, it may actually be harmful, because you're likely assuming wrong things about class C [Li, Liu and Ng, "Negative Training Data can be Harmful to Text Classification", 2010]. It could work and is much simpler to develop, may be good enough depending on your case. 

Many ETL tools support MongoDB and are commonly used for normalizing and standardizing data. For example: $URL$ 

It depends on how you're one-hot encoding them. Many automated solutions for that will name all the converted booleans with a pattern so that a categorical variable called "letter" with values A-Z would end up like: letter_A, letter_B, letter_C, letter_D,.... If after you've figured out feature importance you've got an array of feature and the associated weight/importance, I would analyze the array and perhaps sum up the feature importance weights for anything starting with "letter%". 

Try out some of the examples in this library, which attempts to use machine learning to understand black box machine learning models in Python: $URL$ 

I'll assume you want an image that you could display on a standard 1920x1200 monitor. If every node was PERFECTLY packed and represented by only one pixel, you would have a white box that covered only 1% (2.3 million) of the nodes. Subset your data to display to something reasonable like 1000 nodes using Neo4j and then you should get more reasonable performance. 

In this case, a picture is a worth a thousand words. They literally mean data whose distribution on X,Y is roughly a sphere. Different clustering algorithms work better on different distributions. For example, K means does poorly on the arrangement in the first two rows but OK on the last row. 

Do you want to forecast sales by day over a series of days? That would be more like ARIMA. Do you want to forecast "How many widgets will we sell in the next month"? That would be more like a regression problem. As an aside, if the retailer you're working with has a large assortment of products that changes over time (e.g. seasonal clothing) then you should also look into forecasting by product category and not an individual product. It will be much more accurate. 

Now, when you use accuracy, you're saying A and D are good, B and C are bad, all with the same weights. But is B really bad? Isn't C way worse than anything else? Or depending on the cost to prevent the disease, maybe B is worse? I don't know, it requires domain knowledge. But point being, using accuracy mindlessly is not a good way of dealing with this situation: adjust your metrics to your problem. 

Let's assume a threshold of 0.99 because I want two equal-sized bins and that's the midpoint between 0.96 and 1.02. The classes on the left are [1, 0, 1, 0] (since it's half/half, entropy is 1.0), and on the right [1, 1, 1, 1] (since it's all equal, entropy is 0.0). To make a table like the one you show here, you need to define N threshold points (2.0, 3.5, ..., 7.5, in your case), which will produce N+1 bins of data, you take the class of each bin and calculate the entropy. A more correct visualization to your table should be changing the "Split point" values to "2.0 or lower", "Between 2.0 and 3.5", "Between 3.5 and 4.5", ..., "7.5 or higher". Note this all applies to classification (decision) trees, and not regression trees, where the "class" is actually a continuous variable. 

Suggestion: use an indicator variable for your "order"+"orderAmount" data. The table would look like this, which seems more suitable for distance metrics: 

You need to do feature extraction or feature engineering to create variables in your training data that "catch" those patterns you boxed in and then have a target variable saying "malice found" or "malice found not found" Take a really simple example: predicting if it is going to rain. You could come up with a reasonably good predictor of rain in the next 30 minutes that checked every 30 minutes if 1. it suddenly got cloudy and 2. barometric pressure dropped. 

Sign up for free account on Algorithmia. Copy/paste resume to the tester box on $URL$ Review results. 

If the encryption is done right without the workaround described in the comments, this is not possible. Do a thought exercise: I have an encrypted file. I do not know the contents. I pass it to a program that can determine that it is an image containing a cat or no cat. Why couldn't the classifier also determine the color of the cat? The location the picture was taken? The text on the sign behind the cat? 

If you have a broad set of testing data, I think this is feasible. I've had luck in getting basic models to identify abstract concepts like someone's politics or real news vs fake news, so I think this could work if people write content marketing different from normal news. I just used this link in another post, but here is a tutorial in Python of taking unstructured articles, passing them through an NLP pipeline, and classifying them into multiple groups. 

The relevant Wikipedia article seems to agree with me and adds the Jaccard index to the mix. The paper that presents the famous Classifier Chains method (READ J. et al, Classifier Chains for Multi-label Classification, 2009) uses four different evaluation methods: a accuracy variation that closely resembles Jaccard distance, a similarly changed F1-score, and a log-loss function. The fourth method they use to evaluate is the area under the Precision-Recall curve, but that's one I'd argue should not be used (see the work on Precision-Recall-Gain curves by Peter Flach). 

How to use cross-validation on regression (assuming 10-fold for example purposes): separate your dataset in 10% and 90%, train on 90%, test your metric (squared error or anything you're modeling) on the remaining 10%. Do that 10 times using different 10% groups. Now you have 10 of your metrics, you can analyze its mean and range to see if the model is robust against over-fitting. How do you pick a model: you train on the whole dataset. Cross-validation is for testing/validating purposes, you don't use the models generated by cross-validation. 

When you build a tree, you need to define some criteria for splitting nodes. These include metrics like Information Gain and Gini Index. Those are heuristic approaches, they are not guaranteed to give the best possible split. Weight in the fact some attributes are less relevant and/or more noisy, and many other problems that happen in real data. In short, you cannot build a perfect tree in a decent computational time (you could of course build all possible trees and test the best, but then you'd have to wait some years for training even in a medium-sized dataset). Since we cannot have the best tree, we have approximations. One approximation is to build many trees (using different data partitions or attribute partitions), since we expect most trees to be somewhat correct, and consider their classifications in a voting system; this should deal with most noise, the vertical partition can deal with irrelevant attributes, the heuristic has less importance, and maybe other advantages. 

I think the premise of your question has a problem. Pandas is not a "datastore" in the way an RDBMS is. Pandas is a Python library for manipulating data that will fit in memory. Disadvantages: 

This is fairly common with scenarios like predicting fraud when overall cases of fraud are uncommon. The model will frequently just say "no fraud!" and be right a large percent of the time. You need to assign a penalty, cost, or weight so that output 1 isn't so easy. This article describes that further in a lot of detail: $URL$ 

That is called a "recommender system" and is related to the associative rules/market basket stuff you said you tried. There is a package for it in R called recommenderlab. Here is a nice walk-through: $URL$ 

Just because you trained the model on a dataset, it does not mean it will have 100% accuracy in predicting on it. I ran a similar test today on another dataset I was working with and only got 90% accuracy when I ran the model on training data. 100% is actually really uncommon and usually means you are overfitting. Try showing the confusion matrix for your training run and see how much Weka claimed it got right. 

If the goal is to predict actual revenue, you want to predict the unadjusted rate. You should include month as a variable so your model has a chance of getting the seasonality right. If the goal was more performance monitoring (e.g. "We did better in March than February"), then you would want to be analyzing the adjusted rate. You see this all the time in the US news when they forecast adjusted home sales, car sales, or unemployment. 

In decision trees, the (Shannon) entropy is not calculated on the actual attributes, but on the class label. If you wanted to find the entropy of a continuous variable, you could use Differential entropy metrics such as KL divergence, but that's not the point about decision trees. When finding the entropy for a splitting decision in a decision tree, you find a threshold (such as midpoint or anything you come up with), and count the amount of each class label on each size of the threshold. For example: 

This kind of data set is trivial for association rule mining. A very simple and well-known algorithm of this kind is the Apriori. I'm certain there are packages for executing this algorithm in R. For the restriction of "discover cross category sales only", you can just post-prune the generated rules, ie. let the algorithm generated inter-category sales and then remove those later, which should be trivial. 

I'd rather challenge the assumption and say the algorithm didn't miss in this case: it correctly identified a high risk case, which was its original purpose. You're not building a model to identify people who will catch the disease, but to prevent the disease of actually happening. Always think hard about which metrics make sense and what they represent. If you build a confusion matrix: 

What you have here is some data that's too specific. I don't know if there's a widely accepted name for that, but I've seen it be called "leaking" or "cheating" data. Using this kind of data is potentially dangerous for the algorithm, as you have seen in your experiments. Here's a quick example: I want to predict whether a costumer prefers to wear white socks or black socks. I have a lot of information from this costumer, like the city he lives or how many TVs he owns (let's pretend there's some relation with preferred sock color). For some reason, the people handling the interview I use as data sorted the 200 interview responses by class - the first 120 responses are for white socks, and the next 80 are for black socks. Each response has an identifier from 1 to 200. Now I run this data through a decision tree learner. It comes up with a stump (a tree with only a single node) containing the rule: 

If all the attributes are highly correlated with the best attribute that you down-selected to, I guess that could happen as having too many correlated variables is known to cause problems in certain cases. The model might have settled on a local maximum and you happened to know the global maximum. 

Think of this problem as a pipeline of steps to automate and re-run, and not just the ML step at the end: 

Interpolation seems like it would make sense in this case: any time you miss a day, take an average of the before and after. As an aside, I don't think you have to give up on the missing weather values so easily. There are a variety of R packages that simplify getting weather for an arbitrary location from someone like Weather Underground with only a couple lines of code. 

I'm not aware of a simple way to compare. I've more read that you want a diverse set across different types of algorithms to reduce "group think", so you would choose an SVM, a NN, a decision tree, etc. If at layer 2 they tend to vote together, that either means they were all fooled for the same reason, or you found predictable data. 

Sounds like pretty orthodox feature importance analysis. Easy option: FeaturePlot from caret It basically creates a correlation matrix for you to see which seem to separate most. Slightly fancier: VarImpPlot from randomForest This uses the random forest ML technique to directly tell you which features were most important. It sounds very similar to what you're looking for. Edit: Here is a more detailed discussion of VarImpPlot. 

Machine learning is the process of automatically discovering (inductively) the formula/rule that models a certain random variable. If you can discover the rule by yourself, you don't need the machine to aid you. You (or your boss), as the domain expert, has written a rule to model the target class. As long as this rule is deemed good enough for the business, there's no need to spend resources to generate another model. 

Consider your problem as a binary classification. We have two kinds of prediction: raise an alarm (state is going to change), do not raise an alarm (state is going to stay the same). 

Machine learning (perceptrons or not) is all about automatically finding generic but correct rules, be it in the form of If-Else-Rules, encoded formulas, closest occurrences, or others. The ML algorithm is just a way to (automatically) find this knowledge, whatever the representation may be. In another words, you use it to find the of your example, based on your data. You don't need ML if you can represent such knowledge yourself. 

If you do so, then I'd also suggest to normalize those indicator variables (make sure they are in the interval [0, 1], otherwise you could have a order of 1000 "cd" completely dominating the maximum of 10 "tv" for example). Another thing to watch out is the possibility of binning. If the time of the day doesn't matter much, you could group all the orders from the same day and customer in a single row. Also, if you can manually label some examples, you could use a semi-supervised algorithm, and maybe it would have better performance than completely unsupervised clustering. Some possible algorithms are HMRF-KMeans, co-training variants, and Spy EM.