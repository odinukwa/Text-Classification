The "standard" only returns records that match in both tables. To return records that only exist in the left or right table you need to use an . This can be a or a depending on whether you want to retrieve all the records from the left or right table; by convention LEFT is more frequently used. $URL$ $URL$ In the above you are selecting from as the first table, and then doing one inner and then several LEFT outer joins on the other tables. So you are only going to get results from tbl_rooms, that is the LEFT table, that is how it works. You need to either move to using RIGHT outer joins on the other tables, or, and I recommend as probably less complicated to conceptualize, to start with the table on the top/LEFT that you want to get all the results from. So you need to start with at the top and then do your left joins down to at the end, reverse the existing order. You'll then get all the results from the table you start with, and only the results that match from the table you are LEFT outer joining on. EDIT: Something like this should give you properties with a room count: 

If the columns you are talking about could be viewed as describing a single logical entity, you could (and perhaps should) move them all to a (new) separate table and establish a primary key/foreign key relationship between the two tables. For instance, if your table was like this: 

In short, the query parameter of OPENQUERY must be a single string literal and there is no getting around that. The only way to resolve this, therefore, is to dynamically build and execute the entire statement containing OPENQUERY. That adds one more level of string nesting, which means that the list must now be passed with items surrounded by pairs of apostrophes rather than single apostrophes. You can still pass them with single apostrophes, though, and do the doubling in your script using the REPLACE function. So, here is an example of a complete script to implement parametrisation in OPENQUERY: 

I was able to find an answer with some more searching in the archive. $URL$ So the SQL I created from that post which works in my case is as follows... 

You could supply the new values as a table (with the help of the VALUES row constructor), so that you could join it with the target table and use the join in the UPDATE statement, like this: 

CONCAT will return NULL if any of the inputs are NULL. This suggests that your is returning NULL in at least one iteration. CONCAT_WS will skip NULLs. Try using that instead and see what it gives you. Note the first argument is the desired separator, so if you want to duplicate CONCAT() but with NULL skipping just set that to an empty string, This should help you debug the problem. Alternately you can also use on your if you want to set a default in cases where the result might be NULL. If you use COALESCE to set a default empty string you shouldn't get NULL back from the subsequent CONCAT. Presuming the is in Thai script, you may very well have collation/character set issues, which could be why it is returning null in the first place, your comparison may be failing. Check that all the columns you are comparing have the same character set/collation. @Rick James is also entirely right that looping in SQL should be avoided, and likely is not necessary in the first place. So ideally rewrite the whole thing without the loop. But the above hopefully will help you get to the bottom of where the problem is. 

As you can see, one of the copies has NULL instead of and the query groups by the copied and rather then the original ones. The effect of this is that one half of the row set effectively gets grouped by alone, and so you get aggregated results for both levels in one go. I have to admit, though, that I have no PostgreSQL instance to test this solution and thus no idea how it will perform compared to the other suggestions posted. 

You can certainly avoid a conditional here, but you also do not need either or conversion to to get the total number of pages if both and are integers (and it seems safe to assume that they are). The following formula will give you the number of pages to accommodate rows given the page size of : 

Since each row's values are aggregated twice, you could also try duplicating each row in a manner that would allow you to perform both level's aggregations in one pass. The query below uses LATERAL for that purpose: 

Use your original query as a derived table and outer-join it to a dummy row, referencing only the query's columns in the main SELECT: 

I have 3 servers, one of which has a linked server configured that points to the other - I'll call this Server A. Server A has over 100 user databases for various purposes. Server B is running SQL 2005 which we're trying to eliminate. Server C has copies of some of the databases from Server B, and we're migrating applications from Server B's database copies to Server C's. When I'm on Server B, I can see connections from Server A to a certain database but I don't know how to tell what procedure, task, or job from Server A is using that linked server connection to Server B. In order to retire the database on Server B, I need to re-point Server A's connections to a database on Server C; but in order to do that, I need to know what procedures, tasks, or jobs on Server A are using that connection so they can be updated. Is there a way to see the dependencies on a linked server without disabling the linked server to see what starts failing? 

Short answer: test your backups, and validate the backup frequency. It's difficult to write a long enough answer to properly address this topic; I'll explain a little on my short answer and give you some links to more information. Testing backups is extremely important; merely taking backups will not ensure that they're useful. Most DBAs will periodically restore the backups to another database or another server - for practice and to test the validity of the backup. The frequency of backups is important to address "minimal loss" - it depends on how frequently the database is used, and more importantly, how much data loss is acceptable. If the users/analysts can lose an entire day of data and be okay with re-entering it, you only need daily backups. If they can only lose 5 minutes, you need to be taking log backups every 5 minutes to satisfy that requirement. Communication with the database stakeholders is key to identifying a proper backup strategy. The other thing they can tell you is how long ago they will need to restore. If there are occasionally mistakes they need "rolled back" up to 3 months ago, your backup plan needs to keep backups around for 90 days so you're able to pull from those. Check out Brent Ozar's site for a more complete discussion of backup practices and why and when to employ them. 

Yours appears to be a "greatest N per group" problem. What you can do is get the maximum seats per engine results: 

The following approach will only work as expected if we can safely assume that rows with the same will have the same where is populated. As the first step, you need to get the list of all distinct / pairs where has a value. If my assumption above is correct, you will get a result set where all values are unique, and each will have a corresponding – a kind of reference table: 

That is assuming that by "empty" you mean NULL. If you mean to exclude empty strings () as well, you could modify the above condition like this: 

However, you could do better than using a correlated subquery here. Since SQL Server 2005, Transact-SQL has started supporting window aggregate functions. They allow you to obtain aggregate results alongside detail results and use both in various calculations – exactly what you need for your problem. Basically, the syntax would go like this: 

There is no such built-in function – or system table, or view – that would give you a list of all styles (formats). Otherwise people would not have been creating custom-made style lists (like this one, for instance) – as a way of a reminder, I guess, because the same information can be found in the online documentation, even if slightly differently arranged. Go ahead and create one for yourself. New styles are sometimes introduced with new major releases, but those do not happen too often, so maintaining such a list should not be much of a burden. Another thing is whether you really need to have such a table. Unless you are still using a pre-2012 version, you can format your datetime values arbitrarily with the FORMAT() function.