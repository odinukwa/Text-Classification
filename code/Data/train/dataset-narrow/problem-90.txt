You could phrase it in terms of elevation away from various planes. So 30° above the x-z plane, and 0° from the y-z plane (or whatever it actually is). It's sort of like spherical coordinates where you'd have theta and phi. 

One way to speed things up if you're doing this on a mobile GPU is to avoid indirect texture look-ups. This is where you calculate the texture coordinates then use the results of that calculation in the call to sample the texture. So it might be natural to write a 3-tap blur fragment shader doing something like this: 

If you look at chapter 8 of the SVG Specification, it describes how to parse a element. The short version is that you'll want to find the attribute of the element. That element should be a string describing the curve. It will contain the following commands: 

I've not done this, but it seems like you could keep a store of a single image and update it with each new image you receive. For example, you capture the first image and project it onto the scene. It likely won't cover the entire scene. When you capture the next image, assuming it's not from exactly the same place as the previous, you can combine it with the first image by mixing them 50/50. When you get the next image, you want to mix the existing image at 66.666% and the new image at 33.333%, etc. So I think you'd need to keep the combined image, and a second image that is the count of how many images have covered each pixel. Then: 

The way I read it, you have a 2D cubic Bézier that defines the projections of the ridge line (or river bed, or whatever) onto the X/Y plane. In Figure 4, these are the dark purple lines. So that gives you the direction that the ridge line moves. Now you have to define the height of the ridge line. This is done by picking some points along the 2D Bézier and setting the following values at each of the chosen points: 

A naive approach is to bundle one of the single components into the alpha (fourth) channel with one of the 3-part vectors, which is my current line of investigation. However, given that four 4-channel full precision floating point render targets isn't small I understand it's common to use half precision and even smaller representations to be more memory conscious. What I'm asking is: which components can I safely cut precision down on without losing quality, and by how much? 

The array is an array 400 items long, each 5 values representing a diagonal vector. The array is a 512 item long array of permutations, the values 0 to 255, in a random order, the second half of the array the same as the first half. The array is also a 512 item long array, but the values of have been modulo'd by 80. (Because there are 80 gradients in ) Currently I'm not sure if the s should actually be as is seen in the 3D & 4D simplex noise functions and SimplexNoise.java by Stefan Gustavson who wrote Simplex noise demystified in which he says the value should be changed to 0.5 (and yet doesn't do it in his own source code?). Ken Perlin uses 0.6 in his paper on Simplex Noise, but it's just a magic number which he doesn't explain the significance or origin of. I am also not sure if the used to "normalise" the sum of the n components when returning the final value is correct, though it's doing a reasonable job of keeping the values within a usable range. I've puzzled over this math stackexchange question which purports that the normalisation value should actually be 67.6953. That value does make the noise closer to 4D noise in its variation in values but causes some values to go beyond -1 and 1. It also makes the diagonal areas more obvious and the diamond artifacts clearer. 

Visible diagonal areas (lines) where values are all very similar, as well as diamond shapes where values don't blend together. (Does that count as discontinuous?) The full 5D Noise function is: 

Note that there are 11 buckets because the range includes both 0 and 1. If you want only 10 buckets and want to put any pixels that have a value of 1 in them into the top bucket, you could do: 

The best way to do proper outlines, is unfortunately, also the hardest. It involves calculating an offset curve, usually from the medial axis or straight skeletons, which are non-trivial to calculate. Once you have those, you can calculate the distance of any point from the input object and decide whether to draw the outline or bolding, or not. A simpler way to do it, which doesn't look quite as nice, is to use a MinMax filter. This is a sliding 2D box filter that calculates either the minimum or maximum value within the box at each input pixel. If you have black text on a white background, you can use the min filter to generate an outline. Use the max if it's white text on a black background. Other shapes, such as a circular area will produce different results that may be better for some fonts. In most cases, you'll end up either rounding off or squaring off corners as the size of the filter increases. 

A motion controlled camera - The camera is on some sort of rig connected to a computer. The computer is programmed to move the camera in a particular way. The camera runs through the moves without the actors, generating a clean set of plates. Then it is run again with the actors, usually wearing green or blue costumes or parts of costumes. For example, in "Forest Gump," one character loses his legs in a war. The actor had working legs, as shown early in the movie, and in some later scenes wore blue stockings that were composited out. (They also used other techniques, such as simply hiding his legs under various things in some scenes. Whatever works!) As you surmise, sometimes a larger scene is filmed with a locked-off camera to get a clean plate, then during editing, they zoom into the scene and pan around in software to create the motion you see on screen. This doesn't work well for anything that involves the camera rotating, but can work for scenes where the camera trucks. Parts of the scenery that the actors are in front of are blocked off with green screens and recreated in post. I've seen outtakes of a few pieces where they did this. If you have the DVD of "The Others" with Nicole Kidman, the "DVD Extras" show how fog was added to scenes that were filmed without it. It basically involved someone walking around behind her holding a large green board. Then, they recreated the scenery in the computer, added fog to it, and replaced the green screen with the foggy scene. I have a vague recollection of them doing something similar in the HBO mini-series John Adams. This would likely involve cameras that record their motion. The motion is then imported into the computer and used to move the virtual camera around the scene they've recreated.