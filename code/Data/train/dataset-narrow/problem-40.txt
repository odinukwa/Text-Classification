I get that in the first image, there is one input (the index of the sample) and two outputs (the x,y coordinate of the point) and that the second image is basically the reverse where there are two inputs (the x,y coordinate of the sample) and one output (the value of the point). I'm curious though, how are these related? If you take the DFT of the second image, you can see that it has more high frequency components than low, but I'm not sure how you'd take the DFT of the first set of data points. I'm wondering if it's possible to take other low discrepancy sequences (say, halton, or jittered grid) and make a texture out of the idea, like the second image? 

Note that you can get an estimated signed distance numerically using finite differences and using the gradient. Check this link for more info on that: $URL$ You can find more info about finite difference methods here: $URL$ With a signed distance estimating function, you can also use the gradient to get the normal, which is required for shading. Below is a ray marched surface that is defined as a function $z=f(x,y)$, and is a screenshot of this webgl2 interactive demo: $URL$ 

Looking at a light probe texture, it looks like a blurry environment map. What's the difference between the two, how is a light probe made, and what is the benefit of it being blurry? 

I believe the issue here is that along the edge of the white ssphere, the values are either very low (background color aprox 0.02) or very high (the sphere has a color of 10,10,10), and that combining them makes a number much larger than 1 which ends up getting clamped to 1, completely destroying the blending at the edges. That makes sense but now I'm not quite sure how to solve it. I tried applying Reinhard tone mapping after step 3, thinking that would help by keeping everything in the [0,1] range, and preserving the smoothness between the colors. That helped only a very small amount. 

If i read you right, you have a triangular patch which is on the surface of a sphere - so the triangle isn't really a true triangle in that it isn't flat, but instead is just made up of 3 points on a sphere connected by lines across the spheres surface. Also if I understand you correctly, you have a point on this triangle that you want to get the normal at. If all that is right, the way to get the normal is to subject subtract the origin from your point on the triangle (since it is also a point on the sphere) and normalize it. The normal at any point on a sphere is parallel to the point from the center of the sphere to that point on the surface. 

Lastly, here's a smaller angle (~35 degrees). You can see that the results of lerp / nlerp are more accurate as the angle between the interpolated vectors gets smaller. 

What you want to do is compare your anti aliased results against a "ground truth" image. The ground truth image is the same image but without aliasing. There are many ways to make an alias-free image, but a straight forward way would be by doing super sampling. Super Sampling When we render, we usually take a single sample at the center of the pixel (rasterize / shader / etc it to make the color). Super sampling works by taking $N$ samples for that pixel, in random parts of the pixel and averaging the result. The more samples you take, the closer you get to the ideal "alias free" image. One simple way to do super sampling is to render the image at a higher resolution and then shrink it down to a smaller image. For instance, if you render at double the height and width and then shrink the image down, that's the same as taking 4 samples per pixel. However, going this route, the samples are evenly spaced on a grid, not randomly placed within each pixel. Making the samples random instead of evenly spaced is really important to this monte carlo integration working, but evenly spacing them on a grid is a cheaper method that is better than nothing (aka useful in realtime situations sometimes). Another way to do super sampling would be to render the image at normal resolution $N$ times, but in each render, you randomly offset the camera by a sub-pixel amount. You then average your $N$ images together. That will give you the random sample points, but of course each pixel has the same random sample points. This is why you will often see sub-pixel jittering of the camera when doing temporal anti aliasing: it allows for better anti aliased results (a better integration) over time when combining multiple frames. If they were all in the same position every frame, you wouldn't get any more information about the pixel. Doing these methods will give you your ground truth alias free image, if using enough samples. Comparing vs Ground Truth The next step would be to compare the other anti aliased techniques against the super sampled ground truth. There are lots and lots of ways that you might want to analyze this data, but I'll give a few ideas. One way could be to make a texture where each pixel is the absolute value of the difference between the images for that pixel. This would let you see the entire image as a whole and get an idea of how different it was from the ground truth. Another way could be to calculate the mean and standard deviation of the difference of the images. This would give you an idea of how much on average pixels differed in the images, while also giving you an idea of how much variance there was in the difference. A high variance means some places are much worse than other places, so isn't very consistent. Ideally, the best AA method will have the lowest mean, and the lowest variance. Lastly, since aliasing is ultimately a "frequency" thing, you might find interesting data in doing a discrete Fourier transform on your images and comparing the frequencies in the AA method images vs the frequencies in the super sampled image. More info on taking DFT of images here: $URL$ 

This projection value represents how far down the line segment from $A$ to $B$ that the closest point to $P$ is. It may be a negative distance, or it might be farther from $A$ than $B$ is, so you next clamp this value to the line segment. 

If you are looking to render smooth / non polygonal objects in general, it's not always easy, desired or even possible to do an analytical solve for ray vs object. For these cases, a useful technique is something called "ray marching" (also called sphere tracing). In ray marching, you take steps down a ray and at each point ask if you are inside or outside of the object (alternately, above or below). When the answer changes, you know that the intersection of the ray occurred between the current and last point you tested on the ray. Once you know the intersection occured between those two points, you can do various things to try and find the actual intersection, depending on quality needs vs computation costs. Some common techniques: 

I have a naive diffuse/emissive material path tracer implementation (pinhole camera) and am seeing it take a very large number of samples per pixel to converge - like probably in the millions - and was wondering if that was normal for "naive" path tracer implementations? Here is 10,000 samples per pixel: 

I know that if you turn of vsync, it synchronizes rendering with the vertical redraw cycle to prevent tearing, and that doing so caps your rendering rate (FPS) at the monitor refresh rate, which is commonly 60hz / 60 fps, although other rates exist as well. However, when you are not running at a full 60fps how does vsync affect your frame rate? I've heard people say that you will be limited to a multiple of 60fps (well, ~16ms to be precise), but from observation, fps can fluctuate wildly. 

I guess your question boils down to: Is there a way to do De Boor's algorithm as an equivalent equation, the same way Bernstein polynomials are an equation form of De Casteljeau's algorithm. I'm not sure, but since there are "branches" (see N_i_1 in the glsl code), it seems like it'd be difficult. Maybe someone else will have a more direct answer to that part of it though. 

I've found that bump mapping when calculating lighting and refraction rays can add a lot to the look of ice. It makes the ice look textured and imperfect, like a melting ice cube would look. I sort of wonder if maybe animating a bump map could help make it look wet, as water sheets / droplets ran down it's surface. The images below look pretty nice, but they would probably look even better with the internal imperfections that other people are talking about. 

It makes me wonder if something about that noise shows the secret on how to generalize (low discrepancy) sample points to 2d? Does anyone know whether there is a way to translate these sample concepts to noise textures for dithering and stippling? 

While those are good for sampling (a 2d quad in this case), here is a blue noise texture which is useful for dithering or stippling (src: $URL$ 

Back to talking about rendering spheres, you can also just rasterize a sphere as a 2d circle (possibly distorted by a view / projection matrix type setup in "2d"), and then shade it as if it was a 3d object. It can also work for non spheres, and you see this a lot with renderings of metaballs. Below is a screenshot of this shadertoy which renders metaballs: $URL$ 

Next, you use dot product to project the point $P$ onto the line segment defined by $A$ and $B$, by first getting the vector from $A$ to $P$ and then dot producting that against the direction vector. 

Slerp - short for "spherical interpolation", this is the most correct way, but is also the costliest. In practice you likely do not need the precision. lerp - short for "linear interpolation", you just do a regular linear interpolation between the vectors and use that as a result. nlerp - short for "normalized linear interpolation" you just normalize the result of a lerp. Useful if you need your interpolated vector to be a normalized vector. 

Apparently bicubic pixel interpolation is good for scaling up or down an image (in real time or not). Is it recommended to use a lowpass filter before downsizing though, or does the bicubic sampling handle aliasing problems at all? 

Can anyone explain why you would use a 4d texture lookup for color grading? Alpha (transparency) lookup doesn't seem like a plausible explanation, but perhaps something like normalizing an HDR color and using the intensity as the fourth component could be? Or perhaps time, to be able to interpolate between two different color gradings, when moving from inside to outside and exposure is changing or something? 

It then explains that it takes that constant color texture and applies dilation and erosion filters to obtain the darker and lighter versions of the (greyscale) texture. My question is this: Let's say that you have a greyscale image of an artist drawing some cross hatching on a white piece of paper using a pencil. How would you convert that image to be constant average color or brightness? The only other relevant info from the paper seems to be this: 

Alternately, here's a shadertoy which ray marches a cubic bezier rectangle - it's cubic on each axis, so is degree(3,3): $URL$ 

I've heard that pre-multiplied alpha gives you order independent transparency but when I sit down and do the math, it doesn't seem to be working. Is that untrue, or am I doing something incorrectly? The formula I'm using is: $out_{rgba} = in_{rgba} + out_{rgba} * (1 - in_a)$ where $in$ is premultiplied alpha. In other words, taking a "normal" color in RGBA, i multiply RGB by a. 30% opaque white would start out as (1, 1, 1, 0.3) but would become (0.3, 0.3, 0.3, 0.3) as premultiplied alpha. After getting the wrong answers when working it out by hand, I wrote the C++ program below and am still getting the wrong results. After execution: $out1 = (0.738, 0.913, 0.3, 1.0)\\ out2 = (0.738, 0.875, 0.113, 1.0)$ Can anyone explain why? 

There are, and I am looking forward to seeing the specifics of other answers, but one way to deal with this is to not have the noise (or as much noise) in the source data to begin with. The noise is coming from the fact that there is high variance in the rendering - the number of samples you've taken haven't converged enough to the actual right answer of the integral, and so some pixels are too high/bright and some are too low/dim (in each color channel). The problem is this: If you use white noise random numbers to do your sampling, you may get samples clumping together like the image below. Given enough samples, it will converge, but it will take a while before it gives good coverage over the sampling space. Find a region of empty space in the image below (like in the lower right) and imagine that there was a small, bright light there and that the scene was dark everywhere else. You can see how not having any samples there is going to make a problem for rendering. 

When cutting the triangle on the x/y plane to make a hole for for the road, you would also want the terrain to take on the z value (height) of the road, for those vertices that touch the road. This will make the terrain rise up to meet the road. At this point, you'd have a terrain mesh which did conform to the road but it would be ugly. Instead of having a nice even looking hill that went up to the road at reasonable slopes, you'd have some triangles that went very sharply up to the road, and others that had a very flat slope. What you would need to do now is do some kind of "relaxation" algorithm to address this. Basically, if something is too steep, because there wasn't enough triangle to make the height adjustment, you need to make your whole triangle closer in height to the road and bleed off some of the height adjustment to neighboring triangles. If something was not steep enough for your liking, because there was a lot of triangle to make a small height adjustment, you may want to break the triangle up into more polygons where it gets closer to the road and have that smaller area handle the change in elevation. During this second step, when you added a vertex to an edge between two triangles you would also need to adjust the triangle that shares the edge to prevent cracks from showing up in the terrain. The details are a bit vague, but hopefully the two step approach will help you tackle the problem.