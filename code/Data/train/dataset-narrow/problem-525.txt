Then any query you enter won't be sent to the binary log, and thus won't be sent across to the slaves. Once you're finished, re-activate binary logging with 

You can alter some values in your /etc/my.cnf file. Some important ones are innodb_buffer_pool_size (assuming you are using InnoDB) and also query_cache_size. You should try to set the innodb_buffer_pool_size to 10% larger than the size of your data if you can. The default of 8MB really is woeful for performance. 

which seems to relate to the authentication protocol. Do I need to completely restart the MySQL master server, with old_passwords=0 in the my.cnf file? 

From my Googling on the subject matter, I'm led to believe this is a network/firewall problem. I've checked that my RDS has a security group allowing all network traffic with the source machine (both private IP and Elastic IP) and the source machine has a security group allowing all network traffic with the RDS instance IP address. The master server has a user set up with replication privileges. I'm sure there must be something easy I'm missing here. I've tried running on the master server. How do I get the replication to proceed successfully? Some additional information: 

I have an SSRS report that can I access from the Web Portal but once I try to access it directly through the URL, I get the following error: . I looked at the logs on the Report Server and the error there says . But, of course, it is in the database - I can access the report from the Web Portal. Any ideas? UPDATE The same link now works, two days later. 

Try using a bind variable. You can declare it as a number and that should prevent a damaging SQL injection. ADDITION: bind variables also increase performance and scalability because the query plan is compiled and stored for re-use. Just something else to add to your argument. :) 

I need to make a decision about deployment and security of some SSRS reports and would like some advice on what the most appropriate method is. We have 40+ sites that have access to information for only their site. Each site has access to a number of reports, say Report A, Report B, etc. For each site, the report name is Report A - Site 1, Report B - Site 1, etc. For each report (Report A), there is a Master report (Report A - Master). The Master report is the report itself - dataset and formatting - and includes a hidden parameter for site name. Each sites' report links to this Master report and passes in the site name. Thus if the report changes, the change is made in one place. Please note that limiting the reports to site-specific information on the database is not possible. Our data source uses a service account and all users access to the information is handled in the Web Portal. Now it is time to grant users access to these reports. Currently it is set up that all of the reports and the master reports are in a single folder. Access is controlled on each report individually and users must have access to the Master report as well as their site reports. I would strongly prefer to set up a folder for each site and control access that way. Obviously that would make it easier as site-specific reports are added, and it's superusers who will be controlling access. My concern is this will make deployment of reports incredibly difficult. Because of the linked mechanism, all 40+ versions (plus Master) are in the same BIDS project - and it seems like a nightmare to deploy to 40 different locations for every change. I would like to know if anyone else has had a similar challenge and found a good way to solve it. I have played around with linked reports, but then the site report can't "find" the Master report, even if a linked copy is placed in the same folder. EDIT: To clarify, and use the exact terminology, the Master report is a sub-report of each site report. 

The rows with "null" IDs are new, the numbered IDs are those already existing in the main, target table. The NaturalID can uniquely identify an entry (in fact, it's multiple columns). I want to set the "null" IDs to incremental values, following the current max ID, here: 5 and 6, increasing when more null IDs are found. Currently, I use a cursor to iterate over the rows with ID null and update each ID with a value, but sincce it's really a very big table, it would take days. I tried to do an update with row_number(), but it gives me an error "Windowed functions can only appear in the SELECT or ORDER BY clauses.": 

Perhaps this problem has been solved meanwhile. Databases need dedicated knowledge and skills, any 10 year old kid can play around with SQL Server Management Studio, but creating and maintaining databases is not that easy. For SQL Server , make sure that Snapshot Isolation level is enabled. It's disabled by default, with the database in data locking mode, where long running queries block write acces, or even certain read accesses. I think Snapshot Isolation was introduced in SQL Server for analysis/OLAP. Also, the OLTP tables should have proper indices. There should be somebody in the company knowing at least the basics of all this. 

Is it possible to set up an alert in SQL Server 2008 that will send an e-mail anytime a job in a specific category fails? I am wondering because I would like to set up an e-mail anytime an SSRS subscription fails - and all of these subscriptions are jobs in the category Report Server. EDIT - it turns out that when an SSRS subscription fails, the job itself does not fail, so my question will not apply to the SSRS subscription monitoring use. However I would still like to know for other jobs that we run in our environment 

Set up a new dataset to pull back the language value (let's say language_dataset into column language_value). Create a parameter, let's say called language. Set it to internal and set its default value -> Get values from a query and use the dataset and value you just set up. In the Report properties, Language -> expression and set the expression to . 

I am considering to apply a fix which will generate constant parameter sizes for (n)varchar parameters. The application is based on NHibernate with the old OracleClientDriver (using Microsoft Oracle driver, ODP.NET can't be used in the near future), and NHibernate runs on top of ADO.NET. This driver creates parameterized SQL with the size of (n)varchar parameters set to the actual string size, which, of course, varies: 

Can a clustered index (or IOT in Oracle) be detrimental, when to be used on a very "broad" table, but only few columns are used? In this case, the "Product" table is used only like a junction table between "ProductCategory" and "Sales". If there was a nonclustered index on pr.ID and pr.CategoryID, the DBMS would do an Index-only-check, which has a very good performance. But, if I am right, a clustered index actually IS the entire table, ordered by the index columns. So, even if the clustered index had pr.ID and pr.CategoryID as it's index columns, the database would still have to load the entire table with all the heavy nvarchar(4000/max) stuff, only for two small columns. 

Is it possible to set up an SSRS e-mail subscription, with an attachment, that is encrypted? POST ANSWER-ACCEPT UPDATE: Does the SMTPUseSSL switch in the RSReportServer.config file help me at all? ONE MORE UPDATE: No, the SMTPUseSSL switch does not get me there. I have validated the answer below here: 

I am using the PIVOT function in Oracle and am curious if I can replace the null values with zeroes? I know I can wrap the entire query in another SELECT and then use COALESCE on the values, but I am curious if there is a shortcut. 

I want to know if it is possible to create a group "internal" to SSRS without having to add the group to the ReportServer box itself. I do not have permissions to the box to create groups and/or add users to existing groups. I know that this is a way (the only way?) to have groups that can be assigned permissions in the SSRS interface. I do have all the permissions I need within the SSRS interface. I have the System Administrator System Role on the ReportServer instance, as well as full sysadmin rights to the ReportServer database. I am trying to simplify our security structure and it would be peachy if I could create a group, assign users to that group, and then permissions to the group. But all I seem to be able to do is assign permissions to the groups set up on the box. Any ideas? EDIT: Active Directory groups are not an option for me. :( 

The first thing you should configure is the innoDB buffer pool size. According to this blog entry by Percona, the ideal buffer pool size is 10% larger than your total data size. Other settings to add: InnoDB log file size, query cache size. 

I am migrating my company's database from an EC2 instance to an RDS instance. I have already migrated a snapshot of data and am now trying to set up replication to get the data that has been added since the snapshot. I am following the instructions at this link: $URL$ When I call , shows the following: 

I've tried setting on the master, creating a new replication user with a new, long hash format, but still can't get replication to start. The error I get is: 

You could quite easily write a simply Perl or PHP script, running on a cron, to periodically check the MySQL server and alert you to any problems. For example, I have the following Perl code in my setup: 

I am looking for the most efficient way to solve this situation: We are an Oracle shop. I need to load data from one table to another. My source table, which contains 30 million rows, has a person identifier that is supposed to be unique, but it is not enforced so there are cases where it is not - about .1% of the time. There is logic I can use to resolve the non-unique cases. I want my target table to "enforce" the uniqueness of the identifier - so I have defined this identifier as my primary key. (Note that the data that is loaded isn't a straight source table -> target table; there are a few look-up tables that are used too) My goal: load this table as quickly as possible. It's a lot of rows, so efficiency is key. I have thought of a number of ways to address this, but I'm not sure what would be the most efficient. My ideas include: 

to clear the cursor cache (an admin had to do it!). There are probably other ways to do this on user or SQL statement level (the above is for the whole database server), which I did not understand the quick way. An alternative may be: 

Doing this with SQL Server would create a huge number of query plans, one for each combination of string sizes (when multiple string parameters), which is of course highly inefficient. I always thought it's the same in Oracle, but now I've got doubt. I applied a fix to the NHibernate driver, which would send always the max parameter size, e.g. if the above Name column was 32 chars wide, it would always send [Type: String (32)]. My code does not use dedicated Prepare statements, but sends parameterized SQL together with the values (similar to calling EXECUTE IMMEDIATE). I then looked at the statement (actually an INSERT) in Oracle Enterprise Manager, and the old version did not appear in the duplicate queries list. The statement itself showed a parse for each call (total of more than 1000 after some testing), but few hard parses. Thus, I could see no difference in performance between the fixed and the variable varchar length. Does this mean the fix is futile, and different query plans for different parameter sizes occur only in SQL Server (and maybe other DBMS)? Does Oracle check only for SQL string equality, but not for equality in parameter size settings? I also noticed that duplicate SQL was almost only found where parameter values were concatenated into the SQL string, instead of using bind parameters. EDIT: cursor_sharing in Oracle is set to EXACT. 

It is a bit kludgey, but here's an idea sketch... instead of merging the cells, could you fake it by placing an A in the left cell aligned to the right and a D in the right cell aligned to the left? You could then use expressions to control value, alignment, color, and borders of the two separate cells. Let me know if you need more details. I think this would give you the visual you want. Good luck! 

I have a number of Oracle stored procedures whose activity I want to be able to audit, so I am having them INSERT some data into an auditing table. It would be swell if I could list the variables and their values as well. Is it possible to get this list in an automated way? I am already using to get my procedure name. 

(I fear what I wish to accomplish shall not be easy, but a lot of my googling around about it yielded rather dated results, so here it goes...) I have a Postgres 8.4.x database with over 100 tables. I need to recreate these tables in SQL Server 2008. I'm not concerned about the data, just the structure. Are there any slick tools, shortcuts or suggestions to accomplish this? Heck, I'll even take the find-and-replace values to run on Postgres CREATE TABLE scripts! Thanks! 

If you're just going to delete this extra data manually on the odd occasion, you can simply turn off the binary logging for the current session. On the MySQL prompt run the following: 

The master server is version 5.5.33 The slave server is version 5.6.13 The master server is using old_passwords=1 The slave server is using old_passwords=0 

Yes, I believe this is the cause of your errors. Your method for introducing the new slave seems to be correct. It is quite strange in my opinion to define a table with a DATETIME field as the Primary Key. As you've quite rightly pointed out, the slave gets the replicated queries from the master and they will use the now() keyword in the queries, which will grab the timestamp from the local server. Really, the table should be defined with some other data type for the PK (such as INT or BIGINT) which can be guaranteed to be unique, unlike a timestamp inserted with now().