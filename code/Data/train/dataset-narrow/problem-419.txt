To normalise this table you must remove the repeating groups and multi-value columns. The resulting table will be 

Add three attributes to the project and asset documents - , and . Populate these in the "real" data. When a record changes copy it but with IsActive flag flipped. Retain the original values for and . The original record is updated with the amending user's id and the time stamp. I do it this way around so any other record holding this record's magic id retains that pointer. When reading only look for records with the right value of IsActive. IsActive can be omitted if the archive records are written to a separate collection / namespace / database / whatever your technology supports. 

One imagines that, as the product matures and gains traction, more features from the current data engine (auto create & auto update statistics) will be ported to PolyBase. On the flip side, do you really want to wait for results while a 5PB store is sampled? 

The functionality you are looking for goes under the name UNPIVOT. I'd suggest you have a good search on that subject and post a new question if you have further, specific question. 

Assuming a yellow background means that column is part of the primary key, I think could use some more attention. First up, and shouldn't be part of any key. It is acceptable (though unlikely) that every singe row have the same values for these two columns. Second, is can be calculated from the home and away teams and their scores. It could be handy to retain it for performance but I'd suggest it be taken out of any key. I can see some candidate keys which could provide uniqueness, depending on some other rules. If you limit to single-concurrent-use places, like a football field where only one game can take place at a time, and becomes a candidate key. If you are including places like squash clubs where many games can be running simultaneously then this won't work. Similarly and will be unique and could be a key. may be redundant to , depending on what questions you will be asking of this system. If you keep it adding joining and leaving dates could prove useful. On a point of style - you use both singular () and plural () names. This is the stuff of religions wars but I'd suggest you pick one and stick with it. 

The FORMAT function accepts a wide variety of input types. The output can be formatted to the culture & locale of the user. It produces an nvarchar result. 

The interesting point here is not that TempDB is full, it is why it is filling up. The reason being the GROUP BY UserID needs to do a sort and that's spilling out of memory and into TempDB. With 1.8 Billion rows I'm not surprised. The fix is to reduce the date range you query to what will fit comfortably in TempDB. Let's say this turns out to be a week. Write the output to a working table. Repeat for the next week and the next until the whole period your user asked for is covered. let's say four week's worth. Then run the same query against the working table. This works because min(week 1 + week 2 + week 3 + week 4) = min(min(week1), min(week2), min(week3), min(week4)). This is an excellent candidate for partitioning, if you haven't done so already. 

Since you can clearly state what task_ids are required for each name I suggest you capture this information in a new table. Let's call it . Now you can include this new table with joins. 

In brief: yes. SQL Server moves data around in 8k chunks known as "pages." This includes disk IO. The more cruft there is in a page, the lower the amount of useful information, since the page size is fixed. Therefore to process a certain number of rows will require more IO in a cruft-filled table than in a cruft-free one. This will be slower. Even if the data is permanently in memory and never updated the management overhead of the extra pages will be fractionally higher. Similar consideration should be given to choosing data types. A tinyint will use less space than int, which is less than bigint. The difference is only a few bytes but may add up to a considerable amount at data warehouse scales. Blindly defining all strings as is just plain bad. 

You can use SQL Server Management Objects (SMO). They are a set of classes and APIs for interaction with almost all aspects of SQL Server. There is support for scripting. The SMO dlls can be invoked from any of the usual programming languages, Powershell or even, if you're desperate enough, from within T-SQL. 

A columnstore arranges the data on disk differently to how a "normal" table does it. The column's values are split into segments of just over one million values. Each segment is compressed. Since a single column's values can show a lot of repitition (think "country code" or "product name") the compression ratio can be significant. Read performance can improve from several factors. First, only the columns required in the query are read off disk. Second, compression means much less IO for a given number of values compared to rowstore. Third, aggregate functions can be performed in what's called "batch mode," which is optimised for CPU cache utilisation. Compression is also available for rowstores. My experience is that CPU utilisation increases but IO drops, for a nett improvement in elapsed query time. This was for moderately large databases performing reporting & analytics. Of course, your mileage may vary. 

Your model has two different relationships between and . They should be modeled separately. They have different cardinalities. One is the "created" relationship which is one-to-many. This is modeled by the foreign key in , as you say in your question. The second is "engaged," which is many-to-many. This can be handled by creating a new table, known as an intersection table, that has foreign keys to both and . This table often acquires further columns as design progresses. An alternative model is to omit the "created" foreign key and instead add a status flag to the intersection table. This flag will have two values - "created" and "engaged." The choice is a compromise between ease of ensuring data integrity and ease of querying. If every trip must always have a created user, that is most easily enforced by a NOT NULL foreign key. I would suppose you would also want to ensure the total number of passengers is within the capacity of the vehicle, which is easier if they are all in one list. Other constraints will emerge as your design evolves. From the information given my choice would be for a foreign key / intersection table design and a view to give all participants in the trip. 

The primary key is a way to distinguish one row in a single table from all other rows in that same table. It is not a way to distinguish one row in the context of its associated rows from other tables. Sometimes a table's primary key consists of a single column. A person's user_id would be an example. Sometimes it is made up of several columns. A location is both latitude and longitude. This is known as a compound key. Sometimes one or more of those columns may also be a foreign key. This is termed a weak entity type. To take your example - could a single row in the Orders table be distinguished from all other rows by the Order Number alone? Typically, yes. The order number is unique across the whole system. So given order number 8765 we know that's for customer A. This makes Order a strong entity type. How about the OrderLine table? Given a single order line number, say "1", could we unambiguously find which Order that relates to? Typically no, because order line numbers start again for each Order. OrderLine is therefore a weak entity because its primary key (order number, order line number) requires the primary key from another related table, viz. Order. So according to the business rules it makes no sense for an Order to exist without the Customer but according to the database rules this is OK. An OrderLine cannot exist without the Order under either set of rules. 

One would have to suppose that any natively-supported data type would be better optimised in the product than anything that could be put together as a client of that product. After that, whatever has the smallest byte count so you get the maximum rows per page. 

Fewer rows are returned (3,423) because of the matching algorithm. The same plan and row count is achieved by changing the base query to . By creating indexed, computed column 

With just the two foreign keys in there I would leave it as it is. If other tables have a foreign key pointing to person_image then the debate becomes a lot more nuanced and you will start considering table size and query (plan) complexity. 

You don't. You restore the backup as a new DB, drop the unwanted table from it, then take a new backup of this working DB. 

The exercise is for m=3, so at most 2 keys per node. The first two keys are easy -- they go into the first page: 

You ask "Are there any methods to avoid performing a full data copy to obtain smaller values for identity columns?" To find if there are gaps in the sequence will require a read of every row (or index on that column). To fill a gap will require an update on every row past the gap. (Since you can't update an IDENTITY this becomes a delete followed by an insert, with some reseed jiggery-pokery in between. This feels like it may be slower than a "bulk table copy" approach, though the justification for that eludes me and is likely dependent on read-ahead and disk access algorithms.) Assuming it is the older data that is being deleted, it will have lower identity values so the bulk of the table will be updated each time. A full copy with sequential reads and writes sounds like it will be more efficient. One approach is to make the table partitioned. Reseed the identity in a working table. Switch out the old data and switch in the working table. Down time will be measured in (milli)seconds. Disclaimer: I haven't tested so am not sure how it will react. There will have to be consideration for writes that happen while the working table is being prepared. Perhaps two active partitions which undergo maintenance on alternate schedules? 

I use as an example. You use whatever's right for you. The list of monitors is more difficult because it may contain many values. For this you have to use a table-valued parameter and for that you will have to define a type: 

So, primary first, then secondary. 2) Are you asking how node 1 is brought back on line? That's what your operations people and DBAs are paid their huge salaries to do. Alerts would be a good idea. Maybe even a very good idea. If node 1 dies AlwaysOn will re-direct connects to node 2, transparently to the application. In-flight work may receive an error. New connections will be routed to node 2. This is the HA and DR in action. When node 1 comes back you can continue running on node 2 as primary and have node 1 as secondary, or organise a managed failback at a time of your choice. If node 2 dies while node 1 is still down, then you have a problem. 3) Active-passive has the secondary as "warm": it is ready to become the primary when required but cannot do any application work until then. Active-active has a "hot" secondary: it can process read-only transaction from the application, and be used for backups and such like. IIRC one license is needed for active-passive and two for active-active. 

The round trip time from the client to the server per query starts to add up with your approach. Even if it is only 1ms per query, you are burning almost half a second for no gain. Combine the statements into one. At the very least submit them as a single batch and process the individual results sets in the application. 

Migrate piece-wise. Rename your table to tblsapdispatch_old. Create a new table called tblsapdispatch_new. This new table has the partitioning you want. Create a view called tblsapdispatch which unions the two together. This way the application is agnostic to the change. Move data from _old to _new in batches. The batch size will be found by testing. Move data one partition at a time. Rebuild that partition's index once full. You may be able to make the historical partitions read-only so you'll never have to rebuild those indexes again. When all's done drop the view and rename _new. You should only require an outage at the beginning and end, when the table names change. 

.. with appropriate error handling and return code checking, of course. This can be packaged as a single batch, a job with one step per triggered job or one step per statement. I prefer the last as restart is simpler. 

Sadly row 4 has been eliminated entirely since it has only NULLs! It can be conveniently re-introduced by injecting a dummy value into the source query: 

Write as many rows as you need; a few hundred thousand should do it. Keep id sequential to avoid fragmentation a this stage. Leave gaps for new values, however. Now INSERT rows to achieve the desired fragmentation. Randomly generate id values between the current min and max. This can be done iteratively, or set-wise by selecting the top n from the current rows and incrementing the id to fit in the gaps reserved. A target row count can be achieved by starting with a smaller initial set or deleting rows randomly. 

It will be the difference between a system function execution to get the date versus the overhead to pass the explicit value through the call chain. In my opinion, not significant. Bugs could be introduced either way. With the explicit value, a programmer will, eventually, forget to populate the parameter and the SQL will fail at run-time. Making the column NOT NULL hides the error further and requires thorough checking on top of thorough testing. It is difficult to enforce sensible, system-enforced checks on the provided value without invoking other system functions or hard-coding values and making the system brittle With a default, the value used is that at the server at the instant of execution. Time zones can play havoc here, especially if the client and server are in different zones. Writing and immediately reading a value can, in theory, produce any result from twenty six hours in the future or past. This is not intuitive. If the server observes daylight saving values may be duplicated, or be in the future immediately after the autumn step backwards. If the value is business-significant (say, the payment date on an invoice) and the user is supposed to enter an historical date (when the invoice was received), but the application fails to send it (a bug) the server will default the current date. That could have big legal and financial implications for the business. To me these approaches have different semantics. Application-provided values hold meaning from the business and user. System-defaulted values have a context internal to the database, data lineage and maintenance. They do not encode business-meaningful information. Use each as needed. The performance differences are negligible but the confusion could be immense.