Suppose we have a set S of graphs (finite graphs, but an infinite number of them) and a group P of permutations that acts on S. 

Such a graph is called a Latin square graph (see e.g. this article by Bailey and Cameron $URL$ We can interpret an autotopism of a Latin square as an automorphism of the Latin square graph. So let S be the set of Latin square graphs formed from the Latin squares of order n. So the question I'm interested in is: 

Suppose you have some software which includes a list of n variables s1,s2,...,sn and suppose each of these variables are able to take on a range of values. Now suppose you want to test the program for 2-way interactions, that is, you want to test the program runs correctly for every pair of variables si and sj and every possible value that these variables can take. To do this by a brute-force -- by processing each pair i and j one-by-one, then testing each allowable value of si and sj (while picking the others arbitrarily) -- would require testing a enormous number of cases. But, we can test for several 2-way interactions in a single test. For example, if we have three variables s1, s2 and s3, and we test when s1=1, s2=2 and s3=0, then we have tested three possible 2-way interactions simultaneously. We can design a test-suite for the program based on a covering array -- the columns represent the variables and the rows represent the specific test to be performed (so fewer rows is better). In a strength 2 covering array, within any two columns i and j, there exists every possible pair of the variables si and sj. There are higher strength covering arrays which can test for t-way interactions (but usually interactions are a result of only a few components). Sets of mutually orthogonal Latin squares (and various other block designs) form particularly efficient covering arrays. 

My feeling is that it is a difficult question to answer in general -- I'm currently writing a 30+ page paper on the matter (with 2 co-authors). Actually most of the time it is easy (most of the time it's "no"), but there are some difficult cases. So I'm interested in finding decision problems that would be related to "symmetry classification". They don't really need to be related to Latin squares, I'm just hoping to use these techniques to answer the question for Latin squares. 

Is this problem NP-complete for some sets S? It would be easy to check that a graph admits the permutation p (i.e. certificate). Moreover, it's easy to find examples of S where the problem is not NP-complete, such as S being the set of complete graphs, whence the answer is always yes. Note: I'm not really interested in what type of graphs they are; if you like they can be non-simple, directed, coloured, etc. ADDENDUM: The problem I'm currently looking at is classifying which isotopisms are autotopisms of Latin squares (which can also be interpreted as a special type of graph automorphism). Given a Latin square L(i,j) we can construct a graph in the following way: 

What are the morphisms of adjunctions in $\textbf{Adj}(C,T)$ and what is meant by ... which are the identity on $C$? 

Most resource regarding categorical notions in programming describe monads, but I've never seen a categorical description of monad transformers. How could monad transformers be described in the terms of category theory? In particular, I'd be interested in: 

so it satisfies the prefix property. It's extension Zot adds input and output, where the input is the sequence of bits that follow the valid program description. By restricting Zot to programs with empty input you get a language that should satisfy your requirements. (Or alternatively adding just output to Iota.) There is Haskell implementation of Zot. 

(This is because if a term is typable in System F then all its subterms are.) Is there a simple proof the other way around? That is, a proof that typability implies type checking in System F? 

When learning about generalized arrows, a question arised to me: Are there any languages (or potential languages) that lack one or more of the structural rules: contraction, weakeing and exchange? Under Curry-Howard isomorphism, these rules, used frequently in logic, map into programming concepts. If we denote $A, B, C \vdash D$ a program (corresponding to a natural deduction proof) that computes a value of type $D$ from inputs of types $A$, $B$ and $C$, we get: 

It depends how you define simplest. One of the simplest languages I know are Iota and Jot. A detailed description can be found here. Both are Turing-complete languages that use just two symbols (no variables etc.). Iota programs can be viewed as binary trees with $*$ at nodes and $i$ at leafs. So the whole program is solely determined by the shape of its binary tree. Any such a binary tree forms a valid program. Jot is similar, but slightly different. Programs in Jot are (arbitrary) sequences of 1's and 0's. Any such a binary sequence forms a grammatically valid program. 

It's not entirely clear what do you mean by a functional programming language without closures. Can you give an example? Functional programming languages are usually based on lambda calculus, whose essential part is that you can have open lambda terms. For example the term for the constant function (the K-combinator) $\lambda x . \lambda y . x$ can be viewed as a function that given $x$ returns a constant function that returns $x$ on any argument - a closure of the open term $\lambda y . x$. However you can use another basis, a combinatory calculus such that it's power is equivalent to the lambda calculus. Then you can take a lambda term and convert it into an equivalent combinator that doesn't use any variables at all, so there are even no closures to talk about - see Completeness of the S-K basis. Which I believe answers yes to your question. This is actually what Haskell compilers do under the hood. Evaluating lambda terms with variables is very inefficient and cumbersome, so they convert the program to a representation without variables and use techniques such as combinator graph reduction. I can recommend two books on the subject which are both available online, the first one more theoretical, the second one focused more on the actual implementation techniques: 

In response to your second question, you are correct -- a 2-state, 3-symbol state TM was shown to be universal by then-undergraduate Alex Smith in 2007 by reduction to a universal cyclic tag system, which makes for a complex encoding. Here's the original proof, which uses Wolfram's nonstandard notation for representing Turing Machines. It's easy to intuit why simpler universal Turing Machines require more complicated encodings -- after all, the information involved in the computation needs to go somewhere. For example, in the encoding scheme implicit in Smith's proof, we would first need to first encode the Turing Machine under simulation within a cyclic tag system, then use Smith's scheme to represent the cyclic tag system within the 2-3 Turing Machine. 

You can try a Jackson network, which is well-studied in queueing theory. Jackson networks provide a formalism for modeling systems of servers (ie, hosts) that process messages/items at varying rates and pass messages between one another, and they can handle notions like fixed-capacity caches and linked processing rates. Your analysis will be easier than what's usually needed for these models because you're considering a constant processing speed. I'll also second Aaron's answer about looking at Petri nets, which are another useful formalism that can represent what you're after. What you want isn't just the formalism by itself, which is nothing more than a way to logically express your requirements, but the ability to leverage existing analytical results about the performance of these systems from the Petri net / Jackson network literature. After you've figured out how to model your system as one of these networks, you can try and use known properties of these networks to determine answers to your questions. With Jackson networks, you usually end up with results like, "as hosts become more active, and the cache size drops, the expected processing time changes by X". This question would be a wonderful candidate for crossposting on the Operations Research Exchange, since we're in the territory of stochastic and queueing models. 

It sounds like your glass cutting problem is exactly like the 2D guillotine stock cutting problem -- am I correct that there is no difference? I think Macleod et al's O(n^3) approximation algorithm from the 1990s is still the best known result with proven optimality bounds for very large instances of this problem, but for only 100-200 pieces the exact algorithm by Christofides and Hadjiconstantinou should be able to find the optimal solution in reasonable time. There are also good heuristics for this problem via dynamic programming. A survey on software solvers for the cutting stock problem was written by Macedo et al here in case you're looking for practical solutions. 

A great and little-recognized example of this phenomenon is graph isomorphism. The best known algorithm takes something like $O(2^{(\sqrt{(n log n)})})$ time, but graph isomorphism tends to be solvable quite quickly in practice. I don't know if there's a formal result on average/smoothed complexity of the problem, but I remember reading that one existed - maybe someone else can chime in pointing out a formal result. Certainly, there's a good deal of experimental evidence and a lot of fast solvers. I'm also curious if this property extends to other members of the GI-complete family.