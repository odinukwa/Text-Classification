I'd be very careful using this flag on a VM, as their memory has an extra level of abstraction. Had more than enough trouble with it on physical servers with lots of RAM dedicated to SQL. Example: with 3 2008R2 instances co-hosted when restarting one of them it took forever to come back because it could not find contiguous memory segments anymore. The performance benefits were neither here not there (lets say 'statistically insignificant overall). I treat it as a 'TPC special'. Also consider that 834 doesn't play nice with columnstores either. 

Just came across this 2-year old link: $URL$ It implies that 64K NTFS cluster size is still recommended for SSDs To improve this answer it would be ideal to hear from real-life experience with latest generation SSDs (FusionIO or SATA-controlled). Maybe 256K is even better for columnstores on SSDs! 

If performance is important, Option1 has the clear edge. Every time a query goes across a linked server, performance takes a hit as what you expect to behave like set-based operations become serialised by OLEDB. They go row-by-row. It helps if you follow best practices for linked server queries (OPENROWSET for example) and ensure all remote processing is done on the other side, but the results will still come across serialised. (Look for Conor Cunningham's presentation on Distributed Queries). If all your remote tables are small lookup tables this may not be a serious issue, but if millions of rows have to come across linked servers then performance will suffer. There used to be an extra layer of problems with invisible statistics (for data readers) over linked servers, but as you're running SQL 2014 this will not affect you. If you can afford the Dev cost of eliminating the linked servers, Just Do It! 

What precisely does the query duration measure when conducting a profiler trace? I have a stored procedure which performs data access (SELECT), and it is taking an average of around 30 seconds to return results to the application. But when I run a trace on it I get an average duration of around 8 seconds, max duration 12 seconds, average CPU 5.5 seconds. What could cause it to take so much longer to return the result set? I am not accumulating large NETWORKIO waits. The result set is only around 270 rows of simple text data, around 50 columns total. 

The service account you use to run SQL server (what you enter in configuration manager) must also be a member of the sysadmin role within SQL server. If you have not done so already, log in using the sa account and grant the Windows service account the sysadmin server role. 

I am trying to troubleshoot locking behavior and the READ_COMMITTED_SNAPSHOT isolation level while attempting to resolve concurrency issues. Background: Assume an online ordering system (ecommerce). Product price changes are calculated minimum monthly, and this results in around 600,000 records that must be changed in the database. When posting the price change updates to the database (SQL Server 2008R2 Web Edition) the site becomes unusable due to the significant levels of locking in the primary ProductDetails table when using READ_COMMITTED transaction isolation level. To resolve this, READ_COMMITTED_SNAPSHOT is enabled, however other transactions are still being blocked during the price updates. Investigation of sys.dm_tran_locks shows the blocked session is caused by a waiting Sch-S lock. As I understand it, Sch-S locks (schema stability) are taken while a schema-dependent query is being compiled and executed (aren't they all schema-dependent?). But sys.dm_tran_locks also shows a series of Sch-M locks (schema modification), which are not compatible with any outside operations per BOL. I assume this is caused by the fact that the 3rd party tool used to replicate data changes drops foreign keys during the update process and recreates them after the update is completed. And so, in spite of READ_COMMITTED_SNAPSHOT, other queries are still blocked, not by the update, but by the Sch-M locks cause by the changes to foreign key relationships. This theory was confirmed by eliminating the setting that dropped/recreated the foreign keys. Now the update process no longer takes Sch-M locks (sys.dm_tran_locks only shows X, IX, S locks), and other transactions are not blocked from using the version store to satisfy their queries. However, when executed using this process, the price changes take approx 1 hour to process (vs. 1-2 minutes) and sys.dm_tran_locks shows the transaction taking almost 90,000 different locks, compared to around 100-150 when foreign keys were being dropped/recreated. Can anyone explain this behavior and offer suggestions on how concurrency could be maintained without exponentially increasing the maintenance time for price changes? 

i have now setup a Master => Slave setup, after hard fightning its now working and this is great. the next step is, how can tell the slave if i create a new table/stroed procured or function on my Master its need to copy that to? right now i need to run my sync script for on the master and the slave, i think if i got a lot of slaves this will be a big problem, so i hope there are a friendly person there can explain what to do now? so what i need is i'm only need to consentrat about my Master setup, and lat the slaves copy every thing from my master and not only the row's i, can i that? and how. 

I use a method where you select 1.000 => 10.000 first, after it i delete every single one, you can delete more of same time, but what i want is not to block eny thing inside your database. i try to remove logs from our database, we got like 450 milions ( 240gb ) of data, if i just use the database block. so i resoved it create a script there select a amount of rows out and delete every single one, we use InnoDB and its take a long time, right now we can remove 1milion a /day becures we get perfomes issues. hope you can use this method to remove all your rows you do not need enymore. remeber if you run a script to do this, remeber to start the loop agin when its hit your row count, to get the next rows out and starting to delete it. 

I have a DDL trigger defined for database- and server-level events in SQL Server 2008R2 (Standard) which exists to log all DDL events into an audit table in an admin database. It's only function is to extract the relevant data from EVENTDATA() and insert into this table. For the purpose of the insert (as only sysadmins can access the admin database), I have created a dedicated SQL login with only INSERT permission to this table, and granted IMPERSONATE on this login to public. This is intended to prevent permissions-related errors from the trigger firing and attempting to insert into the audit table, when the caller does not have the necessary access to the database/table. Here is the trigger definition: 

After a considerable amount of testing, I finally discovered the reason behind this error. The client connection explicitly set ANSI_WARNINGS and CONCAT_NULL_YIELDS_NULL OFF. XML data operations, such as @data.value('(/EVENT_INSTANCE/EventType)[1]', 'nvarchar(100)'), require both to be ON. I had attempted to override these within the trigger, but I may have placed them wrong. The final code below works, even with the explicit SET options in the connections from Great Plains: 

Shrinking log files for user databases might be OK if internal fragmentation is becoming an issue (search on Virtual Log Files for detailed info). Best to avoid it by setting up appropriate autogrowth sizes, avoiding growth in percentages as well. The defaults are inappropriate for any realistic data load. Shrinking tempdb log files may not be a good idea at all, I have even seen data corruption that I believe resulted from a job that was regularly shrinking the tempdb log file. Best to pre-size it according to the workload. 

Checkdb creates a snapshot in the background. Snapshots are supported by sparse files (they look large in Windows but are typically almost empty). Could it be that you are looking at this file? 

Just my 2cents from my own experiments on 1-2 year old hardware: Read-only operations (DW-style scans, sorts etc) on page-compressed tables (~80rows/page) I've found to break-even at compression size reduction of ~ 3x. I.e. if the tables fit into memory anyway, page compression only benefits performance if the data size has shrunk by over 3x. You scan fewer pages in memory, but it takes longer to scan each page. I guess your mileage may vary if your plans are nested-loop and seek-heavy. Among others, this would also be hardware-dependent (foreign NUMA node access penalties, memory speed etc). The above is just a rough rule-of-thumb that I follow, based on my own test runs using my own queries on my own hardware (Dell Poweredge 910 and younger). It is not gospel eh! Edit: Yesterday the excellent SQLBits XI presentation of Thomas Kejser was made available as a video. Quite relevant to this discussion, it shows the 'ugly' face of CPU cost for page compression - updates slowed down by 4x, locks held for quite a bit longer. However, Thomas is using FusionIO storage and he picked a table that is only 'just' eligible for page compression. If storage was on a typical SAN and the data used compressed 3x-4x then the picture might have been less dramatic. 

I have had no issues with this trigger since implemented months ago. However, now it appears to be preventing even a sysadmin from executing an ALTER LOGIN, DROP LOGIN, etc. under certain circumstances as follows: My environment also includes MS Dynamics GP 2010 (Great Plains). Great Plains allows an admin to manage users, and for each new Great Plains user, the software creates a SQL login for that user in SQL Server. Resetting a password in the Great Plains interface resets the SQL password. And so forth... However, even if logged into Great Plains as 'sa' as long as the above trigger is enabled any attempt to alter or drop a login fails with error 15151 (Cannot alter the login 'loginname', because it does not exist or you do not have permission). If I disable the trigger, everything works normally. The same operations executed in SSMS, or through some other interface, are successful, even for non-sysadmins who have some level of DDL permissions. It only fails when performed in Great Plains. A profiler trace of the operation shows that GP is merely submitting a standard T-SQL 'ALTER LOGIN' or 'DROP LOGIN' statement, and that the statement correctly shows as called by the sa account. It does not appear that the session ever switched to a different context, other than for the insert into the audit table (which it never actually got to, as no record was logged for the statement). And just in case the session somehow was maintaining the wrong context after that impersonation, I tried making the dummy-insert login a sysadmin with no success. My question is, are there certain combinations of SET options/connection settings, etc. that could result in this type of behavior or issues with DDL trigger execution that could prevent a sysadmin from performing certain operations? Or is there some other avenue to investigate that I am completely missing? 

Long shot perhaps, but worth checking file \Program Files\Microsoft SQL Server\100\DTS\Binn\MsDtsSrvr.ini or the equivalent on your setup. You may have to manually edit it with the instance name. Otherwise SSIS connections might be looking for an msdb of a default SQL instance that doesn't exist. 

It would probably had been slightly better if you had a version of the database before running checkdb with REPAIR_ALLOW_DATA_LOSS. I feel for you - can't be obsessive enough when it comes to backups... 

There is enough info for you to estimate roughly how many pages/extents were lost (deallocated by the REPAIR_ALLOW_DATA_LOSS option). What good is that though? Without backups there is no natively-supported way to recover the data. What logs are you referring to? Transaction logs or Errorlogs? TLog entries need to be interpreted (not for the faint-hearted) and then applied to a consistent database file (which you haven't got). Errorlogs are useless for data retrieval anyway. 

Trace flag 5004 is useful for starting/stopping TDE. Might be worth giving it a go in case it helps. DBCC TRACEON(5004) GO DBCC TRACEOFF(5004) GO 

I have a question, and its about how mongo will handle if 2 user at same time make a update request to update the data with the $set function like this Update reuqest 1: 

We have a huge database on work, and i think the combinasion of bad hardware and bad indexing are the reason for our backend are not working so perfect as we work on its shut do. So are there are perfect or great way to work with index and query optimizion in MySQL and not only use the slow-log and test every single query out? hope there are way around so we can find all the bad querys and fix them. 

if i using mysql, its will update each field without eny issue, will mongodb do the same? the next case can be i have 3 users and there will be a update like this Update request 1: 

in this case if i do the same in MySQL its will request 2 overwirte the field1 from request 1 and the request 3 will overwirte the field2 data in request 2. and i expect mongo work at the same way? or can i make a version control over a document so i can se changes over time? i need to prevent this case, becures i'm building a huges API layer where evey thing use Mongo and i want to update product catalog on cross from platform to platform. hope eny how can help me to understand this part in MongoDB