Long story short... you need to write your own init.d script to run on run-levels 0(halt) and/or 6(reboot) and perhaps run-level 1 (single-user-mode, typical for recovery situations). For example, in /etc/init.d/virtualbox, create something like this: 

If you can't figure out how to edit the registry (since regedit.exe probably won't load) you probably shouldn't be attempting this. If you're still hell-bent on fixing this yourself, I would suggest looking at burning one of many pre-built recovery isos, like hirens bootcd. If you don't have a 2nd computer to burn the iso to a disk, and don't have a friend nearby to hook you up... you could also try using regedit on a second computer in the same network and open the registry remotely (assuming firewall isn't blocking). If none of these options work for you... well... you're outa luck. 

Sadly... No. The very core of Windows must be replaced with all the components that are designed around 64-bits instead of 32-bits. 

Absolutely. You can setup what Microsoft calls "Hardware Profiles" (which is designed more towards laptops being docked & un-docked). With each "Hardware Profile" you can enable/disable services & also enable/disable hardware. The down-side... is that it requires a reboot to do the transition between each profile. More info can be found here: $URL$ 

First... stop virtualizing. There is a reason the BT framework comes on a bootable ISO. Second... just because you can select your wifi card as a virtual network adapter... does not mean it will work properly. For BT/almost any good wifi-tools out there, you must be able to configure your wifi adapter into some form of promiscuous mode; meaning it will capture and report all traffic... and not just the packets that are destined for that machine. 

I've been tinkering. Don't ask me why... but here is quite an extensive check for your ipaddress/gateway/subnet-mask: 

IE does not have it's own DNS cache. It relys on the OS'es DNS caching system. A simple DNS flush can be done from a command prompt with 

USB is a fickle animal today... most of the time on modern motherboards... the 5v line is used as a reference for the data lines... and simply cutting the trace going to the 5v pins on the header and adding 5v to it won't bring the hardware back to life. Sure, it'l supply power... but if the USB controller chip itself is only getting 1v instead of the necessary 5v or damaged to the point where the output voltage is only 1v... you're going to be out of luck. 

I would advise against removing/disabling the fan... especially in such a small & compact setup. Instead, I would look at pulling the fan & heat-sink assembly out... cleaning it all up/replacing fan if necessary. Also, add a dab of thermal-paste between the heat-sink & chip to ensure good heat transfer. If yours is indeed similar to that picture... you'll notice the little white plastic clips that are spring loaded on either side of the heat-sink. You should be able to pop them up... and then the heat-sink & fan should come right out. 

and voila. Dll copied to workstation... registered as administrator... and the user never had to log off... and you didn't have to sit at their computer either. 

URN does not identify the availability of the resource, merely the type of resource. It leaves the decision of how to handle the resource to the underlying system. When you see URNs that include web-based resources, there really isn't a "standard" for defining them. They're 2 sides of the coin. URN is used to identify what something is... URL is used to define how you get it. $URL$ 

With that knowledge you'll note that your MTU is currently set to 1500. If that's what your ISP works with... you should be able to ping sites like msn.com or google.com with a packet size of 1500 without fragmenting the packet. 

Honestly, there's a few bits as to why this wouldn't work properly. But lets skip that for now. Rather than adding an arbitrary route to an IP address, try adding the route with the "interface" specified. i.e. when connected to the VPN if you do a: 

Lets explain a bit of grub... and then explain whats going on with what you're doing. There are 4 parts to booting a linux system: 

Unfortunately, once the MBP is asleep... it can't wake-up to hibernate. Your best bet is to simply disable sleep on lid-close and enable a hibernate when idle for XXX time. When the lid is closed, power-consumption goes down quite significantly... (the backlight takes quite a bit of power by itself) 

You can actually do this in 2 ways. Cell Formatting, and Formulas. Cell-Formatting (right-click cells -> Format Cells -> Custom) would require a custom format of something like: 

Installing Windows will always overwrite your MBR. The MBR is not stored inside a partition, but rather it is in the first 512 bytes of the disk. If sda was booting using truecrypt's bootloader, you need to re-install truecrypt's bootloader. Windows knows nothing about GRUB, SYSLINUX, LILO or any other non-Microsoft bootloader. It also knows nothing about truecrypt disks/volumes. The MBR will ALWAYS be overwritten to the first bootable disk, because it assumes that the system cannot boot into windows otherwise. True-crypt has it's own super-secure bootloader that can unlock the volume and allow the OS to boot. If this has been damaged/overwritten/tampered with, it needs to be restored or corrected. You may want to try using the TrueCrypt Rescue Disk. 

HTTP is not based on the telnet protocol. There's actually a few things wrong with using telnet as a basis for "testing" a HTTP server. First, telnet sends some extra bytes immediately after establishing a TCP connection to setup the session parameters. Second, HTTP does not echo data sent by the remote peer. Third, good IPS devices can drop connections to the HTTP port when that initial session data is sent, or when protocol violations are encountered. Using the telnet client to test a HTTP server is hack-ish at best. The client will display a blank-screen when the TCP session is established, and will only display what the server sends back. If you're clever, you might be able to retrieve a page on many servers... but I would consider it bad-practice. Read up on the HTTP protocol on how to format a request. A basic http request is done as follows: 

Despite the feasibility of the underlying file-system, you REALLY should NEVER store that many files in one directory. When it comes time to browse the contents of that directory, you'll quickly discover that there is a HUGE amount of lag while the OS tries to build the file listing and such. It really puts a significant amount of strain on the system. Most tools out there that do any sort of "web archiving" will usually build a directory structure similar to the website's layout. Nearly all websites do not base all their contents off of the root directory... i.e. mydomain.com/document-1 ... they will have some logistics behind it all that split it up into several paths (for a variety of reasons) i.e. images go in mydomain.com/images and stuff about goldfish are in mydomain.com/goldfish/ etc... There are several tools out there that can & will build this sort of directory structure for you. even wget has options to download an entire site. Personally, I've used "httrack" in the past, and it worked quite well. There are also command-line options for wget to download an entire site as well. Look at the -r (recursive) option. Just make sure you setup your domain list so you don't download links infinitely across multiple sites. Best do some reading-up on the wget man page.