It is essentially using decision tree model to obtain the lower-bound. There are $n!/H(n)$ leafs in the decision tree, it is a binary tree, therefore the height of the tree is at least $\lg (n!/H(n))$. 

Note that $\mathsf{TC^0} \subseteq \mathsf{NC^1} = \mathsf{ALogTime} = \mathsf{AltTime}(O(\lg n), O(\lg n))$. So the question is essentially asking if we can save a $\lg \lg n$ factor in the depth of circuits when computing threshold gates. 

SOS can be considered as a proof system where lines are of the form $p(\vec{x}) \geq 0$ where $p(\vec{x})$ is a polynomial in variables $\vec{x}$. The inference rules are: 

It is not true that we always look at reduction theorems as hardness statements. For example, in algorithms we often reduce a problem to LP and SDP to solve them. These are not interpreted as hardness results but algorithmic results. However, although they are technically reductions we often don't refer to these as such. What we mean by a reduction is usually a reduction to some (NP-)hard problem. A reduction is a relative hardness result, if we have a reduction from problem $A$ to problem $B$ is means that $A$ is easier than $B$ in a sense, which is the same as $B$ is more difficult than $A$. A lower-bound is an absolute hardness result. Now if we know/conjecture that $A$ is absolutely hard then the reduction from $A$ to $B$ implies the same for $B$. Most researchers find it more likely that P is not equal to NP, and even conjecture that SAT requires exponential time. In other words SAT is believed to be very hard. If you accept these conjectures then it is completely reasonable to look at reductions proving universality of a problem for NP as the problem being hard. (Why researchers find P not equal to NP more likely is a different issue, there has been several blog posts on theory blogs about that.) Part of the reason we replace lower-bound with universality results (i.e. there is a reduction from every problem in a class to the problem) is our lack of success in proving good general lower-bounds (it is consistent with the current state of knowledge that SAT can be solved in linear deterministic time). 

There a number of open problems in proof complexity, I will mention only one which remains open even after some experts spent years on trying to settle it. It is the proof complexity version of the state in circuit complexity. (See [Segerlind07] if you want to see more open problems in proof complexity.) Open 

Edited/Corrected based on the comments When authors talk about real number inputs in linear programming, Nash equilibrium computation, ... in most papers (papers which are not on the topic of computation/complexity over real numbers) they don't really mean real numbers. They are rational numbers and numbers that arise from them due to their manipulations (algebraic numbers). So you can think of them as represented by finite strings. On the other hand, if the paper is on computability and complexity in analysis, then they are not using the usual model of computation, and there are various incompatible models of computation/complexity over real numbers. If the paper does not specify a model of computation over real numbers, you can safely assume that it is the first case, i.e. they are just rational numbers. Computational Geometry is different. In most papers in CG, if the authors does not specify what is the model which with respect to it the correctness and complexity of an algorithm is being discussed, it can be assumed to be the BSS (a.k.a. real-RAM) model. The model is not realistic and therefore the implementation is not straight-forward. (This is one of the reasons that some people in CCA prefer Ko-Friedman/TTE/Domain theoretic models, but the problem with these models is that they are not as fast as floating-point computation in practice.) The correctness and complexity of the algorithm in the BSS model does not necessarily transfer to the correctness of the implemented algorithm. Weihrauch's book contains a comparison between different models (Section 9.8). It is only three pages and worths reading. (There is also a third way, which may be more suitable for CG, you may want to take a look at this paper: 

If you don't know what denotational properties you require from your denotational semantics of your language and have no use for it, what is the point of to providing a denotational semantics? It will not add any confidence that your extended language will behave as you want. On the other hand, if you know how the denotational semantics should be based on your intuitive informal semantics and your intention in extending the language, then you can use that to formally check that your programming language matches your intuitive semantics and will work as you intended (w.r.t. the requirements you had for your denotational semantics). 

The techniques depend on the model and the type of resource we want to get a lower bound on. Note that to prove a lower bound on the complexity of a problem we have to first fix a mathematical model of computation: a lower bound for a problem states is that no algorithm using some amount of resources can solve the problem, i.e. we are quantifying universally over algorithms. We need to have a mathematical definition of the domain of quantification. (This is generally true for impossibility results.) Therefore, lower bound results hold only for particular model of computation. For example, the $\Omega(n \log n)$ lower bound for sorting only works for comparison based sorting algorithms, without this restriction and in more general models of computation it might be possible to solve sorting faster, even linear time. (See Josh's comment below.) Here are a few basic direct methods of proving lower bounds in computational complexity theory for the more general models of computation (Turing machines and circuits). I. Counting: Idea: We show that there are more functions that algorithms. Ex: There are functions which require exponentially large circuits. The problem with this method is that it is an existential argument and doesn't give any explicit function or any upper bound on the complexity of the problem proven to be difficult. II. Combinatorial/Algebraic: Idea: We analyze the circuits and show that they have a particular property, e.g. the functions computed by them can be approximated by some nice class of mathematical object, while the target function does not have that property. Ex: Håstad's switching lemma and its variants use decision tree to approximate $\mathsf{AC^0}$, Razborov-Smolensky use polynomials over fields to approximate functions $\mathsf{AC^0}[p]$, etc. The issue with this method is that in practice it only has worked for small and relatively easy to analyze classes. There is also Razborov-Rudich's Natural Proofs barrier which in a way formalizes why simple properties by themselves are unlikely to be sufficient for proving more general circuit lower bounds. Razborov's paper "On the method of approximation" argues that the approximation method is complete for proving lower bounds in a sense. III. Diagonalization: Idea. We diagonalize against the functions in the smaller class. The idea goes back to Gödel (and even Cantor). Ex. Time hierarchy theorems, Space hierarchy theorem, etc. The main issue with this method is that to get an upper bound we need to have a universal simulator for the smaller class and it is difficult to find good non-trivial simulators. For example, to separate $\mathsf{P}$ from $\mathsf{PSpace}$ we need to have a simulator for $\mathsf{P}$ inside $\mathsf{PSpace}$ and there are results showing that if there are such simulators they are not going to be nice. Therefore, we usually end up with separating classes with same type of resources where using a bit more resources we can universally simulate the smaller class. We also have the relativization barrier (going back to the Baker, Gill, and Solovay) and algebraization barrier (by Aaronson and Wigderson) which state that particular types of diagonalization arguments will transfer to other settings where the result is provably false. Note that these barriers do not apply to more general diagonalization arguments. In fact, by Dexter Kozen's paper "Indexing of subrecursive classes", diagonalization in is complete for proving lower bounds. As you probably have noticed, there is a strong relations between finding good universal simulators for a complexity class and separating that complexity class from larger classes (for a formal statement see Kozen's paper). Recent works For recent advances, check Ryan Williams recent papers. I don't discuss them in this answer as I hope Ryan himself will write an answer. 

This is corollary 13 in Uwe Schöning's paper "A uniform approach to obtain diagonal sets in complexity classes": 

General audience talk by Michael Sipser about $\mathsf{P}$ vs. $\mathsf{NP}$: Michael Sipser, "Beyond Computation: The P vs NP Problem", CMI Public Lecture, 2006. Also available on youtube. 

As babou wrote I think the answer depends on your computation model. Let me explain one case: Consider polynomial size circuits with unbounded fan-in AND, OR, and NOT gates. Time is defined as the depth of the circuit. $F$ is complete for $\mathsf{TC}^0$, so it requires super-constant depth, the best known upper-bound requires roughly depth $\lg n/\lg \lg n$. If we fix $a,b$ then $F_{a,b}$ is in $\mathsf{AC^0}$ and therefore can be computed by a constant depth circuit. But the depth is constant only in $n$, not in $a,b$. Can we obtain a dependence of depth on $a$ and $b$ which is better than the more uniform case? The answer is no. Another model is algebraic circuits with binary plus and times gates and constants. $F$ can be computed in depth $2$. It is easy to see that $F_{a,b}$ cannot be computed in depth $1$. Now assume a modified algebraic model where we have unbounded fan-in gates. Now $F_{a,b}$ can be computed in depth $1$ by applying addition on $a$ copies of $x$ and one copy of $b$ in one step. $F$ still requires depth $2$. 

To understand what this means we have to go back to Frege. Frege's turnstile symbol $\vdash$ is a speech-act. It asserts the content (which follows it and is a judgment). In Martin-Löf's type theories we have the four (five?) judgments listed above. In these theories, propositions are just types. Let's assume that $A$ is a proposition. Then $A$ is a type. Let's assume that $t$ is a term of type $A$. Then $t : A$ is a judgment (you can think of it as $t$ is a proof of $A$). Now we can assert that is the case, in which case we use $\vdash t:A$. I would add Michael Beeson's "Foundations of Constructive Mathematics" to the suggestions in Anthony's answer. Martin-Löf has given several talks which explain his theory very nicely but unfortunately most of them haven't turned into published form by him (but check this website). 

This seems to be true in the context of (some areas of) computer science but not generally. One reasons has to do with the Church's Thesis. The main reason is that some experts like Godel didn't think that the arguments that previous/other models of computation capture exactly the intuitive concept of computation were convincing. There are various arguments, Church had some, but they did not convince Godel. On the other hand Turing's analysis was convincing for Godel so it was accepted as the model for effective computation. The equivalences between different models is proven later (I think by Kleene). The second reason is technical and a later development related to the study of complexity theory. Defining the complexity measures like time, space, and nondeterminism seems to be easier using Turing machines than other models like $\lambda$-calculus and $\mu$-recursive functions. On the other hand, $\mu$-recursive functions were and are still used as the main way of defining computability in logic and computability theory books. They are easier to work with when one only cares about effectiveness and not about complexity. Kleene's book "Metamathematics" was very influential for this development. Also $\lambda$-calculus seems to be more common in CMU/European style computer science like programming languages and type theory. Some authors prefer the RAM and Register Machine models. (It seems to me that for some reason Americans adopted Turing's semantic model and Europeans adopted Church's syntactic model, Chruch was American and Turing was British. This a personal opinion/observation and others have a different view. Also see these papers by Viggo Stoltenberg-Hansen and John V. Tucker I,II.) Some resources for further reading: Robert I. Soare has a number of articles on the history of these developments, I personally like the one in the Handbook of Computability Theory. you can find more by checking the references in that paper. Another good resource is Neil Immerman's computability article on SEP, see also Church-Turing Thesis article by B. Jack Copeland. Godel's collected works contains lots of information on his views. Specially introductions to his articles are extremely well-written. Kleene's "Metamathematics" is a very nice book. Finally, if you are not still satisfied check the archives of the FOM mailing list, and if you cannot find an answer in the archive post a an email to the mailing list. 

I am looking for a book on advanced data structures that goes beyond what is covered in standard textbooks like Cormen, Leiserson, Rivest, and Stein's "Introduction to Algorithms". A book that can be used for teaching a graduate level course on advanced data structures like Erik Demaine and André Schulz's Advanced Data Structures course at MIT. An encyclopedic handbook of data structures would be even nicer. 

The issue is that you are not computing anything, you are simply postponing them for the time that the array items need to be accessed. This is worst in general (regarding the time efficiency of the operations) because every time we need to perform an access operation we need to perform all of these computations again. Even if we are performing the access operation only once on each item, it can be more efficient to perform all similar operations at once. It would only be useful if you are going to access very few elements of the array over time, in which case there is probably no point to perform these operations on the whole array. We can simply compute the index and access it directly. If you are not using the intermediate arrays in whole then don't compute them, only compute those which you need to access a considerable number of times. ps: many pure functional programming language do not have environment so there is simply no way of storing the resulting values. However even there you can do either lazy evaluation or eager evaluation (or other evaluation strategies) and what you are describing seems to be similar to the lazy evaluation: not evaluating until we really need the value. 

Helping with thesis topic is one of the reasons that we have supervisors for graduate students, so you should consult your supervisor about it. The general advice that I have heard is that you should pick proceedings of a number of recent reputable conferences in the area you want to work and have a look at the papers in them till you find something interesting and discuss it with your supervisor to see if it is a reasonable thesis topic. 

It seems the issue is the kind of reductions used for each of them, and they are using different ones: they probably mean "$\mathsf{NP}$-hard w.r.t. Cook reductions" and "$\mathsf{NP}$-complete w.r.t. Karp reductions". Sometimes people use the Cook reduction version of $\mathsf{NP}$-hardness because it is applicable to more general computational problems (not just decision problems). Although the original definition of both $\mathsf{NP}$-hardness and $\mathsf{NP}$-completeness used Cook reductions (polynomial-time Turing reductions) it has become uncommon to use Cook reductions for $\mathsf{NP}$-completeness (unless it is stated explicitly). I don't recall any recent paper that has used $\mathsf{NP}$-complete to mean $\mathsf{NP}$-complete w.r.t. Cook reductions. (As a side note the first problem that to be proven to $\mathsf{NP}$-hard was TAUT not SAT and the completeness for SAT is implicit in that proof.) Now if you look at section 7 of the paper, bottom of page 195, you will see that they mean $\mathsf{NP}$-hardness w.r.t. Turing reductions. So what they mean here is that the problem is in $\mathsf{NP}$, is hard for $\mathsf{NP}$ w.r.t. Cook reductions, but it is unknown to be hard for $\mathsf{NP}$ w.r.t. Karp reductions (polynomial-time many-one reductions). 

No. The equivalence if lambda-computability and Turing-computability is a theorem of Kleene. It is not a thesis. It is considered as evidence supporting Church's thesis. 

I am assuming that you are referring to CDCL SAT solvers on benchmark data sets like those used in the SAT Competition. These programs are based on many heuristics and lots of optimization. There were some very good introductions to how they work at Theoretical Foundations of Applied SAT Solving workshop at Banff in 2014 (videos). These algorithms are based on DPLL backtracking algorithm which tries to find a satisfying assignment by setting values to variables and backtracks when it finds a conflict. People have looked at how much impact these heuristics have. E.g. see 

Let $G$ be a directed weighted graph. In addition each edge $e$ has a time-stamp $t(e)$. A route $\pi$ from $s$ to $t$ is a directed walk from $s$ to $t$ with increasing time-stamps. We are also given two sets of vertices $S$ (source) and $T$ (destination). We associate with each route $\pi$ one of the edges $e$ with the following properties: 

As I mentioned in the comments, you need to first clarify what you exactly mean by a proof system. Josh discusses the case where one uses the original definition of a proof system according to Cook-Reckhow. There is an alternative definition which is also common: a proof system is a binary relation $R$ computable in polynomial time (and satisfying some conditions like soundness) and we say $\pi$ is an $R$-proof of $\varphi$ iff $R(\pi,\varphi)$. With this definition it is easy to show that there can be a proof system where the K-complexity of a formula is much higher than the K-complexity of some proof of it. E.g., let $R$ be some usual proof system modified as follows: we first check if $\varphi$ is of the form $\top \lor \psi$ for some formula $\psi$, if that is the case we accept the empty string as a proof of $\varphi$. Otherwise, we fall back to the original proof system. Since $\psi$ can be any formula the K-complexity of $\varphi$ can be arbitrary high. However the K-compleixty of the empty string which is a proof of $\varphi$ is trivial.