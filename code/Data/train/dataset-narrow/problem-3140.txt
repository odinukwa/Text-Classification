See this answer on Cross Validated for a thorough explanation on how to use the caret package for hyperparameter search on xgboost. How to tune hyperparameters of xgboost trees? 

81.5 is a fair score and more importantly it means that the model, even though not fully optimized, it works. My data is Time Series and the task is binary prediction, the same as the example. And now my problem looks like this: 

Just add weights based on your time labels to your xgb.DMatrix. The following example is written in R but the same principle applies to xgboost on Python or Julia. 

Random forests, GBM or even the newer and fancier xgboost are not the best candidates for binary classification (predicting ups and down) of stocks predictions or forex trading or at least not as the main algorithm. The reason is that, for this particular problem, they require a huge amount of trees (and tree depth in case of GBM or xgboost) to obtain reasonable accuracy (Breiman suggested using at least 5000 trees and to "not be stingy" and in fact his main ML paper on RF he used 50,000 trees per run). However, some quants use random forests as feature selectors while others use it to generate new features. It all depends on the characteristics of the data. I would suggest you read this question and answers on quant.stackexchange where people discuss what methods are the best and when to use them, among them ISOMAP, Laplacian eigenmaps, ANNs, swarm optimization. Check out the machine-learning tag on the same site, there you might find information related to your particular dataset. 

Artificial Neural Networks are algorithms loosely based in how a brain functions so they shouldn't be treated as the equivalent of brain learning. The science that studies all the details of how neurons and brains learn is Computational Neuroscience and the question you ask is pretty much an open question but it has many great hypothesis. The method you described about how artificial neural networks update their "weights" is called backpropagation and some authors included O'Reilly, Randall; Munakata, Yuko (2000) in Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain argue that backpropagation is actually the algorithm that drives learning in the brain. In later publications they propose architectures that include the Hippocampus region to re-create short term memory. However in recent years, with the advent of mainstream deep learning new ideas have arrived along with old ideas with new approaches. The ones that look to replicate the brain and its function or at least approximate it as much as we currently can are: 

Spiking neural networks are computationally expensive and hard to tune, their performance isn't close to what deep learning can do, however natural brains use some form of Spiking neural networks which tells you about the potential of these kind of models. Another interesting mention is Neural Turing Machines. It's an interesting approach on how certain brain inspired processes can be used to compute diverse problems. 

Source: Boruta documentation The package has one parameter getImp that defines the importance to be used, which by default is random forests from the Ranger package. So theoretically one could use xgboost's xgb.importance() function as the source of feature importance but I cannot find an example of how to use such parameter in practice because so far I've been unlucky. Is it possible at all to do this with the Boruta package or do I have implement from scratch the Boruta algorithm again using xgboost? And if it's possible how? 

Random search and Bayesian parameter selection are also possible but I haven't made/found an implementation of them yet. Here is a good primer on bayesian Optimization of hyperparameters by Max Kuhn creator of caret. $URL$ 

The model is basically the same as the IMDB one. Though the result means it's not learning anything. However, when I use a vanilla MLP-NN I don't have the same problem, the model learns and the score increases. I tried increasing the number of epochs and increasing-decreasing the number of LTSM units but the score won't increase. So I would like to know a standard approach to tuning the network because in theory the algorithm should perform better than a multilayer perceptron network specially for this time series data. 

How do you add more importance to some samples than others (sample weights) in Keras? I'm not looking for which is a fix for unbalanced datasets. What I currently have is: which is the desired importance I want to give to each sample. 

Twitter's API is one of the best sources of social network data. You can extract off twitter pretty much everything you can imagine, you just need an account and a developer ID. The documentation is rather big so I will let you navigate it. $URL$ As usual there are wrappers that make your life easier. 

My answer comes from experience more than from experiments or benchmarks published. As far as I know, Spiking Neural Networks do not outperform other algorithms in any task. There have been advances in robotics and reservoir computing but reservoir computing algorithms are as good as other algorithms (like reinforcement learning) according to recent publications. There are rumours that some companies are interested in these algorithms because they've hired a few reservoir computing researchers recently but these are only rumours. Here is one of the most recent publications detailing advances and limitations of reservoir computing in robotics $URL$ I began experimenting with Liquid State Machines in college using Wolfgang Maass' proposed architecture. It looked promising, specially the idea of inhibitory neurons forming part of the circuit. But in reality using these algorithms in real life data applications (language classification, image classification among others) was not enough to get close to the benchmarks like RNNs, ANNs, SVMs. Sometimes even vanilla Multilayer Neural Networks perform better than Liquid State Machines. My understanding is that these kind of models are good for robotics and other autonomous related tasks like sensors and autonomous navigation (but that was not my area of research) but not so much for other types of data. There are a few labs, mainly in Europe working with this algorithm but so far I haven't heard of many advances in this area in the past years. I do believe that brain inspired algorithms are the next big step in AI, and while many companies like Numenta and Deepmind are researching towards this direction, as of today there is still a lot of work to be done in order to have the next breakthrough in AI. 

There are two solutions to this problem, though I'm only going explain the second one because I think is the best option. First option: 10 Scatterplots of speed of vehicle on the x-axis and predicted speed of vehicle on the y-axis. The only scenario I see this plot been useful is for clustering different types of vehicles with their speeds (if there actually are many vehicle models). Naturally you could also aggregate all data points in a single scatterplot and color-code the data set (ex. dataset1 = blue, dataset 2 = red, etc.) Second option: Calculate the difference of vehicle speed and predicted speed for each data point and then bind all the results into a single data table so that each column contains the difference between actual speed and predicted speed for each data set. Then plot each column as a bar in a boxplot, here you will be able to describe all differences including signs for outliers and mean. As you might already know boxes represent standard deviation (variance) and the central line represents the mean. You can find more information about boxplots here: Wikipedia Sample code in R and ggplot2 

Here you can see that you'll mostly need to tune row sampling, column sampling and maybe maximum tree depth. This is how I do a custom row sampling and column sampling search for a problem I am working on at the moment: 

From Keras RNN Tutorial: "RNNs are tricky. Choice of batch size is important, choice of loss and optimizer is critical, etc. Some configurations won't converge." So this is more a general question about tuning the hyperparameters of a LSTM-RNN on Keras. I would like to know about an approach to finding the best parameters for your RNN. I began with the IMDB example on Keras' Github. the main model looks like this: