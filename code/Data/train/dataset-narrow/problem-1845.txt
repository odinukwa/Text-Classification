First off, I will assume that your printer is properly implementing DHCP and DDNS updates, but this is not always true. I am also assuming that the original lease it obtained has not expired yet. If you are getting this message after expiration, then you might be dealing with an improper DHCP implementation. Your printer obtained a lease of 192.168.0.20, and was told that it would be valid for X number of days. Some time before that, you changed the range to 10.10.10.0/24. The printer still has an address lease that is technically valid, although undesirable. During a lease, it is common for a device to request a lease renewal and/or a DDNS update. The lease renewal is obviously declined, since the original address is no longer valid for new leases, but that doesn't mean the printer will stop using it. It might not request a new address until the original lease has fully expired. This is not a bug. The name update seems to be what is triggering the error, though. Depending on your configuration, it could be the printer requesting the name renewal, or the DHCP server requesting the renewal on behalf of the printer. The name renewal is being rejected since the address it's tied to is no longer a valid address on the server. The server parts are smart enough to know this, but the device will hang on until the bitter end before trying again, and probably succeeding. The renewal process is basically the client or its agent asking if it may please continue using this name on that address. The server gets to answer "Yes," or "no," but does not get to say "No, but use this one instead." The shortcut is to get your printer to release its lease and get a new one. You can usually do this through the web interface, or sometimes a power cycle. 

I have a basic 3 member MongoDB replication set. 1 x Primary 2 x Secondary A = Primary B = Secondary C = Secondary The Mongod service on the Primary stopped, and as expected, one of the Secondaries took over as Primary. I restarted Mongod on the former Primary, and it became a Secondary, so I now have: A = Secondary B = Primary C = Secondary Should A automatically become the Primary again, or is this something I have to do manually? I haven't set any priorities on the nodes. 

OK. I can explain this. Apache mod_proxy_balancer doesn't have its own independent healthcheck mechanism. The state of Balancer Members (workers) is determined based on outcome of actual forwarded users requests. Sequence is as follows: 

Amazon Linux (Linux distro used in Amazon EC2) $URL$ Issue Overview: A missing bounds check was found in the way OpenSSL handled TLS heartbeat extension packets. This flaw could be used to reveal up to 64k of memory from a connected client or server. Affected Versions: Any Amazon Linux AMI on which openssl 1.0.1 is installed, which is any Amazon Linux AMI 2013.03 or later, and any Amazon Linux AMI that has upgraded to 2013.03 or later. OpenSSL is installed by default on the Amazon Linux AMI. Affected Packages: openssl Issue Correction: Run yum update openssl to update your system. Once the new package is installed, it is required that you either manually restart all services that are using openssl, or that you reboot your instance. While the new package is still named openssl-1.0.1e, it does contain the fix for CVE-2014-0160. New Packages: i686: 

Is your DFS-R staging area on the same volume as the DFS-R folder? For performance reasons, it should be. If not, then DFS-R is having to copy the file from staging volume to the destination volume rather than doing a straight move. Here's where the speculation kicks in. It may be that during this copy operation, DFS-R is creating a sparse file, and then filling the blocks, and "unsparsing" it when complete. If something interrupts this process (like antivirus, Undelete, or some other file filter driver program scanning the DfsrPrivate folder), then you may end up with a temporary sparse file that doesn't get filled with its contents. You can test for this behavior by using Process Monitor on the files that are replicating properly and seeing if they are marked/unmarked as sparse at any point in the process. I'm not a fan of mixing 2008 and 2003 when it comes to DFS-R. I was soooo glad to get the last 2003 machine off our DFS tree. 

Think of SPF and DKIM as ways to validate the mail path, and think of DMARC as an extension that also validates the message sender. Think of this as delivering a FedEx letter. It's easy to validate where the envelope was shipped from, and that the courier was legitimate, but it doesn't provide a way to prove that the letter inside the envelope is really from the person whose name is printed on it. Your webserver is a valid SMTP server for mywebserver.com and that your Sender address is legitimate, but that's not enough for other servers to trust that you have permission to send as website-user@gmail.com . How does GMail know that your server hasn't been hacked or otherwise used for malicious intent? Gmail's servers aren't going to blindly trust you to send mail as one of their users -- unless maybe you are hosted by them, and then you'd probably have trouble sending to Yahoo. To address your first part of the question, yes, it's very likely that this is why GMail is categorizing it as spam. The oldest forms of spam center around spoofing the "From" address. This is what most users see when they get a message, and is the primary field they want to trust. When a message from a legitimate mail server is sent using a From address that doesn't belong to that mail server, it's still a red flag. As you mentioned, DMARC operates on the From address as part of the specification. Granted, it makes it harder to write web apps that send on someone's behalf, but that's sort of the point. As to why they do it - well, that's up to the designers of the specification, but it's a trade-off. They are taking the high road and making a system that works very well if you stay within that limitation. Perhaps future mechanisms will find a way around this. The unfortunate solution is to only use addresses that you have control of. To address your third question, sign your messages with your domain name, and mention in the body that it was sent on behalf of website-user@gmail.com. Otherwise you will have to request that your recipients add the address to their whitelist. It's not much fun for a legitimate web app developer, but it will protect the sanctity of the recipient's inbox. You might have luck using the Reply-To header with the web user's email address. There is a discussion of this limitation on this DMARC thread. In the mean time, you can try to make sure that your server isn't blacklisted on any RBLs. It could be that you can fail DMARC but still get through some spam filters if you have good enough reputation... but I wouldn't rely on it. 

Loop through your servers in Bash and then set you exit status based on Bash arguments you pass to the script, then set this up as a Nagios command. The beauty of Nagios is that you can create your own monitors, for whatever purpose. 

In this instance, any time a play incldues server1, "user" will be used as the ssh user. For server2, the value of remote_user from your ansible.cfg file will be used (eg user1, user2 etc depending on the local environment). 

I have 3 OpenSSH servers and I want to have only one. I have created a new server, and migrated all the accounts/data from the other 3 onto the new server. I now intend to update the DNS records for the 3 existing servers so that they point to the ip address of the new server. However, when clients who connected to any of the older servers now connect to the new server, they are going to get an error, as the server key in their known hosts file is going to differ from the server key issued by the new server. I can move the server key from one server to the new server, so that there are no issues with one of the three, but there are still going to be problems with the other two. I had thought about attached 2 extra ip addresses to the new server, and running 3 separate SSH daemons on it, with different server keys, but I'd really like to avoid this. Has any one got any other suggestions. 

I have done this with one printer and two different drivers (one PostScript, and one PCL). It should work about the same for using the same driver with the following caveat: Windows may not show both queues if it determines they are using the same port and driver (see the article below). You can sidestep this problem by creating another IP printer port on the server with the same address. I'm not sure if this caveat applies if the print server is a different server. It sounded like you were going to use the same server, but it should work fine either way. MSKB 2015694: Printers installed using the same driver and port on Windows are grouped as one when viewed within Devices and Printers 

I would venture to guess that the users who are affected have this update installed in their workstation or VM pool. If you have clients with this update, they will be unable to apply user settings if their workstation does not have Read (not necessarily Apply) permission to your printer assignment GPO. The default security settings include this, but if you customized the security and removed Authenticated Users from the security, you will experience problems. I have actually seen that practice recommended in some older books as a measure to optimize the GPO, but it's time to rip that page out now. The simplest thing to do is to grant Authenticated Users read permission to the GPO. If you have a lot of them to fix, Microsoft has a PowerShell script which can help you update them. As usual, weigh this information with your particular needs. If you aren't comfortable with adding this permission to all Authenticated Users, then you will need to find or make some other group to assign read permissions to the workstations. 

If you look at the logfile for the client who is having problems, you'll see repeated HTTP 206 response codes. You will see a 206 response when you client is requesting a partial download of a resource, which could arise for various reason eg it has downloaded some of that resource before, it recognises that it is a large binary file, or its attempting some sort of bandwidth control. Where you see a HTTP 304 response, that means that the client has requested the resource only if the cache on that resource hasn't expired. If the cache hasn't expired, the server doesn't send any data; it just tells the client that its OK to load the file from the client cache, as it hasn't changed on the server. As such, these response will always render the content quickly, as the resource is being loaded locally. If the pattern in the shared logs is typical, then the issue is caused by the browser making repeated requests for partial content, and only getting the full content after repeated attempts. That could be to do with proxying, browser configuration or just network latency. If you could provide a live link to one of the image files, more debug would be possible. 

I have an issue with routing. I have 2 public subnets: 172.31.1.0/24 and 172.31.100.0/24 In each of these I have a NAT instance. Each NAT instance is an OpenSwan VPN peer to a remote location. This allows the following VPN connectivity: 

DFS namespaces use some referral magic to find if it is on any of the domain controllers -- not just the ones where the namespace is specifically hosted. Non-DFS shares do not have this feature. Normally, you would make each DC a replica for the somecrap namespace by adding them to the Namespace Servers tab on your DFS Management for the namespace. This does not happen automatically when you add a new DC. Otherwise, it will appear to be blank if your computer happens to resolve \my.dom.com to \DC2, if only \DC1 has the namespace shares defined. Your DNS ordering or site setup may be such that it never defaults to the namespace server, so they would always be blank. 

I know it's an old question, but I encountered this today as well. Contrary to what was assumed, you actually can set up costs for a site that doesn't have a DC. I do this for some remote sites which are within our enterprise network, but not within my local control. In my case, site C includes our VPN users, and I was using it to steer them to a specific DFS server that has more bandwidth. 

... using only one line per IP address, and adding all the FQDNs to the list. Make sure to add the appropriate DNS entries so that other computers can connect to this server (if that is needed). Once you have this in place, you can tweak your Apace VirtualHost entries on how to handle requests for each domain name. You might want to start with a single VirtualHost listening on *:80 just to test your FQDNs out. After that, you can add or replace it with specific VirtualHost entries for each domain name that you want to handle. 

2 x ip addresses with 2 individual certs, with each one attached to an ip address A wildcard certificate that works with both domains 

As per documentation: $URL$ If you want to use ebextensions with a Single Container Docker Elasticbeanstalk environment, you have to: 

I have a farm of production mysql servers that are currently protected by restricted passwords. I would prefer to restrict access to these systems at network level, but have to account for the fact that some members of the dev community require read only access to debug applications issues. I know that I can do this by creating mysql users that can authenticate at different levels from different ip subnets (eg 'user'@'10.0.0.8') but I'd like to avoid having to refactor code to introduce new users into the application (which would require a considerable QA effort). Ideally, I just want to break the network link between where the mysql servers are (in a dedicated subnet) and the developers are (in a dedicated subnet) and allow the application servers continue to access the mysql servers as before. It would seem like some sort of TCP based proxy with authentication would suit this purpose. In that way, I could create user specific accounts on the proxy that would create an audit trail and provide short-term on demand access to developers without having to tinker with the user database on the MySQL servers. However, authentication options in software like HAProxy and MySQL proxy appear quite limited. Has anyone implemented something similar?