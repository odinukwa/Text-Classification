Assuming the data is constrained to avoid duplicates, how about something like this? This is assuming Microsoft SQL Server syntax. 

It looks like you have no traces running on your server. The function mentioned in your post title is: 

REAL and FLOAT are approximate data types. FROM: $URL$ "Approximate-number data types for use with floating point numeric data. Floating point data is approximate; therefore, not all values in the data type range can be represented exactly." 

Based on the collation names I assume that you are using Microsoft SQL Server. COLLATE can be used at the database level or the column level. Since you are trying to UNION two tables, using the column collation on the needed columns will resolve your query. Here is a sample bit of code to help you: 

It seems that FLEXFIELD4 is NVARCHAR. If there is some other conversion involved that your code does not show, then that might clarify the issue. 

I definitely would not say that only 1 row is ever passed into the trigger. Triggers handle every row updated in a single statement. (I see Jon Seigel has made the same point.) The problem is that inside a trigger an error can occur that results in a state where the transaction cannot be committed. Your main issue will be determining why the transaction became uncommittable. See $URL$ In part it says: 

A filegroup restore can be done on a database in FULL recovery model. There are requirements and consequences of the restore. 

But of course. I suspect that you want to upgrade to the SQL Server 2014 Business Intelligence edition, since that is more frugal than Enterprise. But the BI edition does include the SQL Server database also in the license. Are you wanting to upgrade the BI server in order to get the latest BI features? SQL Server 2014 can certainly pull information from earlier editions of SQL Server for processing locally. Or the tools can read directly from the 2008 server. 

Of course, you will also need triggers to maintain Deleting Data From a Materialized View and Update Data In a Materialized View. Samples are available for these triggers as well. AT LAST: How Does That Make Sorting Joined Tables Faster? The Materialized View is being built constantly as the updates are made to it. Therefore you can define the Index (or Indexes) that you want to use for sorting the data in the Materialized View or Table. If the overhead of maintaining the data is not too heavy, then you are spending some resources (CPU/IO/etc) for each relevant data change to keep the Materialized View and thus the index data is up-to-date and readily available. Therefore, the select will be faster, since you: 

First of all full text searches are often slower than searching on indexed columns. But full text searches offer some semantics that column searches do not. However, full text search is primarily valuable in larger sets of text, such as indexing documents, or when the likely order of the text is fluid. (e.g. John Johnson; Johnson, John; John, Johnson; etc) In this constrained task of matching names, I would choose the two columns, both indexed, as likely the better performing option. (But testing is the only way to know for sure.) If you are using standard indexed columns, then you can quickly search the fname column and/or the lname column and when needed join them on a matching identifier. This is more accurate than a full text search, because the search is very bounded. This does require that the search query be made knowing which name is the fname and the lname, of course, or adding additional conditions to check both ways. (Not always so simple in a multi-language world.) The full text search syntax will support "phrases" (words in a certain order) or the softer version of words in any order. In the latter case 'John Johnson' would be satisified by either or both of those names in any order. (Of course, for the exact search syntax options see the mySql documentation.) 

I would use your current design over creating a table for each user role. Why create a new role, when your code may have to cycle through several tables to get an answer that could have come from one place. (Opinion: You would just be making more work for yourself.) Of course, a Team, a TeamManager, and a TeamMember may all have some attributes that are related only to their individual roles. If you are facing that challenge, then a supertype-subtype design can work well for this issue. All the common data is in the supertype table, but each subtype contains data unique to that role/task/etc. (Whatever you are trying to manage.) A Microsoft Tutorial give a basic description at Lesson 5: Supertypes and Subtypes at $URL$ 

For snapshot and transactional replication: Distribution cleanup agent purges the non-latest folders For Merge replication: the snapshot agent itself will purge older folders. 

Yes, you can have several versions of SQL Server involved in a replication set up. You are limited (generally speaking) to the least common denominator of functuality. That may be just fine for you, but it is something to consider. SQL Server 2xxx Express can only be a subscriber in a replication landscape. So as long as data only needs to move to the Express database you can use replication. Microsoft has documented this for SQL Server 2008 R2 at: $URL$ 

A LCK_M_S is a blocking problem not a deadlocking problem. So you are just waiting on something else. You need to determine what process is blocking you. So, "it suddenly disappeared" because that block either completed or failed and rolled back. This is normal. What sounds abnormal from your notes or your perspective is the length of the block. In addition to finding the blocking process you could also consider using a different transaction isolation level. For example, although not without their own problems, you might consider using "READ COMMITTED SNAPSHOT" or "SNAPSHOT" isolation levels for your report. This would allow it to read the data as of the start of the transaction and would avoid most blocking situations. 

This happens because the stored procedure is actually dropped and recreated. When the procedure is dropped it loses all its permissions. Therefore, when the 'altered' version is created there are no permissions attached. Your process to update the procedure needs to include a re-grant of the needed permissions. I don't know your full environment, but here was a 2013 forum discussion for one MySQL tool set: $URL$ 

No index on to support . (But this may not be needed if it is a small table with just a few entries.) Your columns in have check constraints, but no indexes. You should create some indexes since they are used to join to several different tables on their existing primary keys. 

That is a pretty huge question to discuss in a forum. If you search the web for "SQL Server Troubleshooting Performance" you will get quite a few hits. Perhaps one good resource is the book by Jonathan Kehayias and Ted Krueger, "Troubleshooting SQL Server: A Guide for the Accidental DBA". The PDF can be freely downloaded from: $URL$ There are also plenty of knowledgable people posting helpful guidance, including Paul Randal, Joe Sacks, Brent Ozar, Aaron Bertrand, and so forth. 

Basically there is not a single place where ever configuration can be found. The more generally used configurations can be found here Server Configuration Options. But they do not necessarily cover everything that you might want to configure. For example: Enable SSL Encryption for SQL Server using Microsoft Management Console Also you can find some information on size limits and allocations at: Features Supported by the Editions of SQL Server 2014 Perhaps these examples will get you started. 

This suggests that if the server is a heavy consumer of the data network, the NUMA scalability profile could significantly increase the consumption of this data. But once that is accomplished and the data is in cache, I cannot see that this would carry over into other SQL Server operations. Is your environment consuming very heavy levels of data, such that this would benefit your work? Then you might consider the various profiles available to you. 

Restoring the database to another server will not result in the data being unencrypted. However, you do need to: 

Create and configure a single-node SQL Server failover cluster instance. ... On each node to be added to the SQL Server failover cluster, run Setup with Add Node functionality to add that node. 

In this case the OR of candidate tokens provides the list to Full Text Search. The performance is based on the ability of the function to return those values which depends in turn on the size of the total text list. (You can also create a physical table that you periodically repopulate with the contents of the function, so that you can query it more quickly.) 

Obviously when you check in a changeset, you know who the customer is (or was) according to what you describe about your process. Even TFS requires something to be the same, so that it can maintain history. In any case, in multiple versions of the same object, what is constant? Is the "Name" constant? (Probably not.) If the Customers table were that simple I do not see what you are tracking other than a name change. But likely the table is much more complex. But as long as you know this is the same Customer, you should carry a constant value in the new Customer rows. Something like: 

I always thought it had an order, from back when I did more programming than administration. I ran through a few execution plans and double checked my beliefs. Here is what I see: In a multi-step query (such as many of our stored procedures) the order reflects the physical order in which the queries are run. For a particular query, it looks like the statistics IO reflect the execution plan by reporting statistics starting at from the right and working to the left Perhaps this is more of an observation than anything else. 

A Foreign Key constraint is primarily there to preserve integrity, though there are some performance benefits. The good thing about foreign key constraints is that they are always in force in the database, instead of depending on the ORM to enforce it. The software platform for your application can change, gaining or losing features, but if the database is enforcing integrity then it safeguards you from platform changes and programming errors. So, yes, use the foreign key constraints. 

This would start at midnight of 7 days ago, but would not include today's partial data. EDIT: GETDATE() is a SQL Server function. In MySQL is would be something like: 

As of SQL Server 2012, sp_resetstatus is still supported. However, Paul Randal has some excellent counsel, including do not detach that database. The first and best answer is to restore a good backup. If that does not work and you can retrieve the files from somewhere (where?), check out Paul's posts at: $URL$ $URL$ Since these are detailed discussions, I will not rehash them. But read carefully. 

Most SQL Servers are configured for the non-system databases to exist apart from the system databases. That is what is normally meant by the default path. You can lookup some code for finding this at: $URL$ Also, Alex Aza has provided a popular post over at StackOverflow. The key is to get the data from the registry keys where they are stored. Here is a snippet from Alex's longer script: 

Using wildcards for data that "they don't necessarily exactly match" seems likely to cause you a lot of grief. A search that begins with '%' will turn that into a table scan since no index can be used in searching the names. From your comments you apparently recognize that this is a thorny problem and you are unlikely to get perfect results. Since you are matching (roughly) on names, have you considered creating FULL TEXT indices for the tables and columns you are searching? Once the full text index is built and maintained, the likelihood of quickly finding candidate rows goes up. Here is a simple example of working on your problem from one of my earlier posts from a few years ago: $URL$ A sample script using the sys.dm_fts_keywords dynamic management function. 

The database space is freed for the use of other expanding databases and/or for creating further new databases. If you are concerned about data loss you could archive the LegacyDatabases to some backup location, a file share, tape backups, whatever your choice may be. 

The work being done on the readable secondary should NOT affect the primary server. It is a readable but not writeable secondary, so the query ('R', 'TSQL', etc.) cannot leak back up to the primary server. Do you have any reason to expect otherwise? 

There is no global command to do this for you, but you could create a script to create all the ALTER TABLE / ALTER COLUMN commands and then run the script on your server. Having said that, why do you want to change all of the 'description' columns to unlimited size? If a description should only be 100 characters (for example) it would be better to stick to a definition VARCHAR(100) that identifies what the limit should be. 

No SQL Server version mentioned, just in case that makes a difference. However, if you have sufficient rights you can connect to the server using the Object Explorer / Connect drop down and choose "Analysis Services..." rather than "Database Engine...". This will expand to allow you to see some details of the Analysis Services databases. Just by using the Object Explore you can explore some details for the Analysis Services databases. This is quite rudimentary, but may get you started. After that you need to learn how to query for the details. Here is some guidance, based on the soon to be released SQL Server 2016. $URL$ 

What version of MySQL are you running? Are you running InnoDB, MyISAM, or some other engine? If you do not mind some downtime, then go ahead and shutdown MySQL and get the file backups. If you want to avoid downtime, look into MySQL Enterprise Backup as a method of getting a consistent backup of your database. It does a hot backup of InnoDB and a warm backup if using MyISAM or another engine. Here is a synopsis which contains a link for further details. $URL$ Of course, there are probably also some other third-party backup tools that you can investigate. I much prefer not to take down the database in order to get a backup of it since that locks out the users. This might not be a significant problem for a less busy database.