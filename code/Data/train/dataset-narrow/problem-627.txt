only applies to non leave level pages. By default those are filled full. So specifying together with is only useful when you specify a fillfactor smaller then 100%. In your case, specifying is not necessary, since by default your leave pages are how you want them. Full. 

I know it's an old Question but this might still help searchers and it's a problem that pops up every now and then. The main reason why you are hitting a performance ceiling without you seeing any resource bottleneck is because you've reached the limit of what is possible to process within one session single thread. The loop isn't processed in parallel, but all inserts are done serially. In my case, it takes 36 seconds to insert 3 million rows. That means 36/30000000 = 0.000012 seconds per row. That's pretty fast. On my system, it simply takes 0.000012 to go through all the steps that are necessary. The only way to get it done faster is start up a second session in parallel. If I start 2 sessions in parallel both doing 15 million inserts. Both of them finish in 18 seconds. I could scale out more, but my current test setup is hitting 95% cpu with two parallel session, so doing 3 would skew the results since I would hit a CPU bottleneck. If I start 2 parallel session both inserting 3 million rows, they both finish in 39 seconds. so that is now 6 million rows in 39 seconds. Okay, that still leaves us with the NETWORK_IO wait showing up. The NETWORK_IO waits are added by the fact that you are using extended events to trace them. In my case the insert takes 36 seconds (on avg). When using the extended event way (from the link above in the very first comment) this is what is registered: 

3 important pieces of information are missing to pinpoint exactly what went wrong in your particular scenario: 

The basic idea is that you want your pages as full as possible. The more rows are packed into a data page, the less pages you need to read and keep in your buffer pool. So ultimately you want all your pages 100% full with data. However: When all your pages are full you run into a couple of problems: 

Assuming you are talking about data that is encrypted with SQL Server keys, there is way to find these columns. The Function will return the name of the key used for the encryption for that particular value and will return NULL if there isn't anything encrypted with a "known" key (3rd party, or simple not encrypted). With that knowlegde we can test every column to see if it contains at least one row which has a varbinary value that returns a key name functionality of key_name() 

We'll take page 296 for this example. It's a data page (Pagetype=1). Let's look at which 2 records are on this page. (Each record is 4015 Bytes so 2 records per page.) 

start process explorer and find the SQL Server process. right click and select properties look at the thread tab. Sort on the CPU column and note the thread id (TID) that is consuming the most CPU. 

Deadlocks and blocking locks are two different concepts that you need to understand. A deadlock is a situation where process/action 1 is waiting for process/action 2 to finish and at the same time process/action 2 is waiting for process/action 1 to finish. In other words. They would wait forever since they are waiting on each other. In your scenario, something else is happening: Process 1 is doing an action and has taken a lock on a resource to complete that action, Process 2 now wants to start a action that requires a lock on the same resource. Process 2 now has to wait for process 1 to complete and the lock is released. The key here is that at any given moment, none of the processes are waiting for each other (at the same time). One process is just waiting for the other process to finish an action on the same resource. They are not waiting for each other. I hope that's clear. On to how we fix your issue: Can you post the Table definition, the indexes on the table and the delete select statement. We could have a look to see if there are ways to make the likelyhood of blocking locks less. 

If you would now re insert record 3 and 4 on the source database you would get a PK violation error when these rows are replicated to the subscriber. That won't work. And I'm assuming that this is what you mean by "Replication don't work" But more important. If you wouldn't do that, you wouldn't notice anything. But you would have more records on the subscriber. 

Now you know, why this resource wait is showing up. It could be very well that this is not your problem. I'd just wanted to point out the other reasons that make ENCRYPTION_SCAN show up. The reason for your query slowdown might be something else. I'll leave improving your query plan up to the query plan experts on this site ;-) However, could you post the actual execution plan as well instead of just the estimated plan? 

You won't only see ENCRYPTION_SCAN resource in your wait list when Encryption (like TDE) is used. Certain operations will take a shared lock on this resource to make sure the database is not being encrypted during the operation. The moment you would encrypt a user database with TDE, the tempdb will also be encrypted (otherwise, you would have security risk when User data is used in temp db). Therefore, some operations will take a shared lock on ENCRYPTION_SCAN in Tempdb to prevent Tempdb from getting encrypted. Here are two examples: BULK INSERT 

So the NETWORK_IO you were seeing in the extended events log, wasn't related to your insert loop. (If you wouldn't turn nocount on, you would have massive async network IO waits, +1 Martin) However I don't know why the NETWORK_IO show up in the extended event trace. Sure the writing out to a async file target of the events accumulates ASYNC_NETWORK_IO, but surely this is all done on a differenent SPID then the one we are filtering on. I might ask this as a new question myself) 

is only supported when both SQL Servers are the same version. You can only use An alternative if you need read access to the destination databases, is to use replication. 

Make sure to choose a directory where SQL Server service account has write permissions and if you run this from SSMS, run it locally on the SQL Server. Next thing is to start a bulk insert loop. While the loop is running, open a second screen and start running sp_lock untill you see the ENCRYPTION_SCAN shared lock in DB_ID 2 (Which is Tempdb). The bulk import loop: 

"Just" making a snapshot of volume on your Netapp can not be considered the correct way of making a backup of SQL server databases. However, Netapp offers Snap manager for SQL Server It's not a free Netapp option, so you need additional licenses. But with this solution you are able to create backups in the correct way. Since Snap manager interacts with SQL Server to coordinate an transactional consistent backup. Snap manager does have certain demands with the disk layout. You need seperate LUNS for your system databases, your temdb, your user db data files and your user db log files. Additionally you'll need an extra lun for snapinfo. So for existing SQL Servers that migrate to Netapp or for server where you want to use Snap manager as a backup strategy this might be an extra hurdle to take. However, especially when you have large databases, using storage based snapshots can be very benificial in regards to backup and restore times. Also consider scenario's where you need to refresh copies of acceptance and test. This can be a lot faster with snapshot technology. Further reading on how Snap manager works.