Note: I do not wish to create a report for each Store, as that would require 100 reports. Some managers can manage more than one store, so they may be included in multiple Security Groups and need the corresponding permissions. I do not want to split up the underlying table , even though it should be possible to split this up into 100 or so schemas, and give members of the Security Groups access to these scehmas accordingly. A solution I have in mind is to create a table using SSIS which gets updated every day with the appearence: 

(Where I have put italics on the part which says it won't update aggregations, even though I have aggregation which I use). does also seem plausible at a glance, but 

I have two cubes which updates in sequence. These takes very long to update, upwards of 5 hours due to their sizes. I have a SQL agent job which updates the cubes. The job has type with the following command: 

I've tested in SSRS that I can replace 'Domain\User.Name' with my variable. I've tested to create a dataset with this query. This seems to give the correct data, but what it lacks is the StoreKey column which I thought could be . I'd like to be able to have a variable called in SSRS where Available Values from Query would be something like: 

Performance advantages of CCIs are not only space-related: batch execution mode is also there to speed things up (in supported operators). Batch sizes can vary from 64 to 900 rows, so it would be reasonable to expect that using smaller datatypes would lead to 'fuller' batches, closer to the 900 max figure. $URL$ Being economical with datatypes is a good habit anyway - why consider bigint if tinyint (or bit) would do the job? 

SSDs tend to have a natural 4K sector size as far as I know, but would it be beneficial to format Windows cluster size at 64K? This is good practice for spinning rust, but is it relevant on SSDs? 

Also if you want to restrict number of concurrent connections then Resource Governor comes in handy. Can even tailor it to specific logins only. 

I'd be very careful using this flag on a VM, as their memory has an extra level of abstraction. Had more than enough trouble with it on physical servers with lots of RAM dedicated to SQL. Example: with 3 2008R2 instances co-hosted when restarting one of them it took forever to come back because it could not find contiguous memory segments anymore. The performance benefits were neither here not there (lets say 'statistically insignificant overall). I treat it as a 'TPC special'. Also consider that 834 doesn't play nice with columnstores either. 

, but naturally I can't make an EXEC in the FROM clause. I also tried to make a new Dataset in SSRS which would use data from another Dataset within the same Project but that did not work either (and I Googled that it is not possible). Edit: final solution What I did to implement this was to create a table of the appearence: 

I then created a variable called in SSRS where I used and where I had a dataset where I selected StoreKey using a JOIN on the current user's using the Query provided by and my table I had created. Thus I only got the StoreKeys available to certain AD Groups. 

I will create a couple of tables in SQL Server for a client. Of special interest for this question is the table to be created and in which the first table will include all store's sales and include for instance , , , and sales data. The table will use complementary data in regards to Products, i.e. , , etc. This table will be a slowly changing dimension, and store Product history in terms of price etc. for each store. The table will include the columns and which describes when the data was entered, and when it was replaced (with default value year 2999 in until a new update of the Product is inserted) respectively. I Believe this is very common. However, the from the underlying database are very complex strings, such as . My alternatives as I see it, is to use in SQL Server to convert the complex strings to integers, and store these in a separate column and use these columns when doing JOINS. But checksum does not guarantee unique integer values, which would likely cause problems. When I tried Checksum on a Virtual PC and my own PC, doing Checksum on the same values returned different results which is a concern as well. I Believe these two together rules out Checksum, unless it can be manipulated somehow to make it more unique? Another alternative is to use more complex functions to ensure that the string values are converted into integers, such as the one provided on a question of mine here. A solution such as this however also has issues, the values and will for instance get the same result. It is also complicated in the sense that if someone who isn't that savvy in SQL need to try to find any issues with the Query that uses the function, it may be very hard to understand what is happening. My third option seem to be to use the table and update this table first from the SQL Agent, and using an Indexing Key on this table and use this index as the ProductKeyInt in the table (where ProductKeyInt will be some kind of subquery in for instance to fetch the value with the biggest in DimProduct corresponding to the ProductKey. Does anyone have any input? Is there a simpler way? I don't want to have strings as JOIN keys because of for example increased CPU time 

v$instace does not exist, try with v$instance But anyway, it does not smell look, such an internal error points to something serious, maybe a corruption. Try this 

I am hitting the same question as you. When using streaming replication, it is in theory not needed to ship the archived Xlogs to the standby. But the archived Xlogs are also needed for normal recovery and they can also be needed if the standby cannot catch up with the master (for example if it was stopped for a while). Note that I am not 100% sure that archives are needed in this case because there is also the concept of replication slot by which the master is aware of what has been consumed by the slave (my understanding). With replication slots I think the master will accumulate the XLOGS as long as they are needed by the slave (and stop operating if the slave does not come back and the file system is full ?) What I will do is ship them to the slave and even keep a copy on the master. So, on the master 

However, I think I will use a shell script that will copy the file locally and rsync it to the slave. This shell script can also check if it is running on the slave and if yes do nothing (this way the config file can be symetric between the two). On the slave, in recovery.config, 

Yes, but only from SQL2012 onwards, if I remember correctly from Bob Ward's 2013 PASS session (gave me a headache!) 

Checkdb creates a snapshot in the background. Snapshots are supported by sparse files (they look large in Windows but are typically almost empty). Could it be that you are looking at this file? 

Unfortunately, as is the case with other counters, the definition of 'reads' is not identical across the board. If a plan has calls to UDFs, statistics IO may under-report them (or hide them completely) while profiler still displays them. Regarding the PLE counter, if the server has more than one (physical) NUMA nodes, it is important to use the ones under 'SQL Server: Buffer Node'. The PLE figure under 'SQL Server: Buffer Mgr' is an average of the Node PLEs, and can be hiding horrors sometimes (I've seen PLE of 400 on one node and 7000 on the other, with SQL supposedly using both nodes). 

There is enough info for you to estimate roughly how many pages/extents were lost (deallocated by the REPAIR_ALLOW_DATA_LOSS option). What good is that though? Without backups there is no natively-supported way to recover the data. What logs are you referring to? Transaction logs or Errorlogs? TLog entries need to be interpreted (not for the faint-hearted) and then applied to a consistent database file (which you haven't got). Errorlogs are useless for data retrieval anyway.