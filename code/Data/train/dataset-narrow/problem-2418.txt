There are plenty such typing systems. Most work is based on the linear/affine typing system introduced in (1) and generalised in (2). Here are the main works on this subject. In (3) the typing system ensures a precise match with PCF (int its call-by-name variant -- changing to call-by-value is easy). In (4) the typing system gives a precise interpretation of the simply typed λ-calculus. (5) does the same thing for System F, (6) does the same for the λμ-calculus. The typing systems in (3, 4, 5, 6) are precise in the sense that the inhabitants of translations of λ-calculus types "are processes coming from lambda expressions", at least up to $\simeq$ (weak typed bisimulation). More precisely, if $\alpha$ is a type in the λ-calculus, and $\Gamma = x_1 : \beta_1, ..., x_n : \beta_n$ is a corresponding typing environment, then the processes $P$ that are typable as $$ \vdash P \triangleright x_1 : \overline{\langle\beta_1\rangle}, ..., x_n : \overline{\langle\beta_n\rangle}, u : (\langle\alpha\rangle)^{\uparrow} $$ are (up to $\simeq$) the translations of λ-terms $M$ with $$ \Gamma \vdash M : \alpha $$ (Here $\langle \alpha \rangle$ is the translation of a λ-type $\alpha$ into π-types, and $\overline{\tau}$ the dualisation of the π-type $\tau$.) The translations are fully abstract in the sense that $$ M =_{\beta\eta} N \quad\text{iff}\quad \langle M \rangle_u \simeq \langle N \rangle_u $$ (Here $\langle M \rangle_u$ is the translation of $M$, located at $u$.) So the embedding is as precise as one can hope for. (NB, I'm skating over some details above, for ease of presentation.) Note that the systems in (3, 4, 5, 6) are closely related -- indeed they are in some sense the same system, although this viewpoint has not yet been published. If you want a brief summary of how those typing systems ensure confluence, then it is this: they ensure that there is always at most one active output (i.e. an output not under a prefix). This is brutal/syntactic. Session types (7, 8) can also be used for this purpose, but the correspondence is not that precise, in parts because session types are not as constraining as the types deriving from (1). See (9) as an example where sessions are used to model effects (such as state) in PCF. Davide Sangiorgi and others have also worked on this problem, see (10) for a summary. Aside: as the question correctly guesses, λ-calculus can be seen as an especially well-behaved form of message passing, a viewpoint put forward 1976 in Carl Hewitt's pioneering work (11) in the actors tradition. The typing systems above essentially constrain π-calculus so only this well-behaved message passing is typable. Coincidentally, Milner invented the π-calculus in parts to give a formal model of actors. In hindsight, as (12) explains nicely, there is a small mismatch between actors and π-calculi. 

Interestingly, there is a nascent mathematisation of version control systems, although at this point it's only partially applicable to Git. It's called patch theory [1, 2, 3, 4, 5] and arose in the context of the DARCS version control system. It can be seen as an abstract theory of branching and merging. Recently patch theory has been given HoTT [ 6 ] and categorical [ 7 ] treatments. Patch theory is work in progress, and doesn't cover all aspects of version control, but contains a lot of theorems that you could look at. It is a clear example of theory that's applicable to the 'real world' -- not surprising, for patch theory is an abstraction / simplification of something very concrete. At the same time it connects with cutting-edge maths like HoTT. 

Note that not all typing systems have "subject reduction", for example session types. In this case, more sophisticated proof techniques are required. 

One dimension is type inference. System F's type inference for example is not decidable, but some its predicative fragments have decidable (partial) type inference. Another dimension is consistency as a logic. Distinguished thinkers have historically felt a bit queasy about having impredicative foundations of mathematics. After all, it's a form of circular reasoning. I think H. Weyl might have been the first or, one of the first, who tried to reconstruct as much of mathematics as possible in a predicative way ... just to be on the safe side. We have learned that the circularities of impredicativity are not problematic in classical mathematics, in the sense that no contradictions have ever been derived from 'tame' impredicative definitions. Over time, we learned to trust them. Note that this (absence of paradoxa) is an empirical observation! However, much of the development of proof theory, with its weird ordinal constructions has as ultimate goal the wish to build up all of mathematics 'from below', i.e. without impredicative definitions. This programme is not completed. In recent years, interest in predicative foundations of mathematics has shifted from worries about paradoxa to the computational content of proofs, which interesting for various reasons. Turns out that impredicative definitions make it difficult to extract computational content. Another angle in the worry about consistency comes from the Curry-Howard tradition. Martin-Löf's original type theory was impredicative ... and unsound. Following that shock, he proposed only predicative systems, but combined with inductive data types to regain much of impredicativity's power. 

You could write an interpreter for VHDL or Verilog in e.g. Coq or Isabelle/HOL and then prove that translated hardware descriptions do the right thing. I know that people have done this, at least for fragments of the hardware description languages, see e.g. A Formal Executable Semantics of Verilog. Companies like Intel or AMD formally verify (parts of) the design of their chips. Have a look at Roope Kaivola's work. 

Given the prominence of computational complexity as a research field, if they were such natural bedfellows, maybe somebody would have brought out the connection already? Wild speculation. Let me entertain the reader with thoughts about why a categorical rendering of computational complexity is hard. Arguably, the key concept cluster in category theory is centering around universal constructions / properties (with the associated apparatus of functors, natural transformations, adjunctions and so on). If we can show that a mathematical construction has a universal property, that gives a lot of insight. So if we wanted a categorical approach to computational complexity, we'd need to find a convenient category and exhibit how key concepts of complexity theory (e.g. LOGSPACE or NP-hardness) can be given by universal constructions using that category. This has not yet been done, and I think that this is because it's a really difficult problem. I suspect that the reason for this difficulty is that the key object of complexity theory, the Turing machine, is not well understood algebraically. The problem with TMs is that they are not naturally equipped with a nice algebra that allows building up programs in a compositional way. By that I mean that we don't usually program TMs by saying our target program is the TM T which is composed as e.g. $T = T_1 \oplus T_2 \otimes T_3$ where the $T_i$ are 'smaller' TMs and $\oplus, \otimes$ are algebraic operators on TMs: we just don't have (natural) algebraic operations on TMs that enable us to build up TMs in stages in an insightful way1. Instead, we construct TMs by specifying their two components separately: the control (a FSM) and the tape. Neither control nor tape have good algebras themselves. Let's look at tapes first. There are a couple of natural ways to compose tapes, none of which appear to work for a compositional description of TMs. 

Inductive types have been studied heavily and many variants exist. A well-known introduction to inductive definitions is 

(probably) pioneered Hoare logics for purely functional languages. This work is based on Hennessy-Milner logic and Milner's encoding of functions into processes, as described here: 

Of all those proposals, the reasoning about binders with nominal techniques is closest to the intuitive "handwaving" done to handle binders in informal reasoning. That is the key advantage of nominal techniques. This might be best observed in Nominal Isabelle, where most of the reasoning about binders can be hidden from the user. See Nominal Techniques in Isabelle/HOL for a discussion. 

Let me clarify the setting, which has nothing to do with $\pi$-calculus or bisimulation. The first thing you have to realise that it does not make much sense to talk about a programming language without reference to the notion of program equivalence you intend to impose on the language. That's because 

It seems inutitively clear that the latter problem is harder to solve than the former, in a way that is reminiscent of computational complexity. If we had a program solving the first problem, and a program for the second, what could we say about their time and space consumption? Moreover, whatever the intrinsic complexity of the distributions, there should be a connection with conventional complexity theory, for the difficulty of the second problem is largely a consequence of the fact that the set of valid proofs in Peano arithmetic is a complicated subset of the set of all integers, and if we had an oracle for it and / or the number of such proofs, we could solve the second problem quite easily. There is no reason to restrict one's attention to uniform or discrete distributions. My question is this: where has the complexity of probability distribution been investigated? What is a good overview article? Please feel free to close or move this question if it's inappropriate for this forum. I asked on cs.stackexchange but didn't get the kind of answers I was looking for. 

1 Note that this is quite different from other computational formalisms like $\lambda$-calculus and $\pi$-calculus, which are algebraic calculi. Because they are algebraic, it has been easy to develop type-theories for $\lambda$-calculus and $\pi$-calculus as a way to constrain programs. However these calculi have powerful operations like new-name generation, scope extrusion, $\alpha$-conversion and unlimited copying of terms, which make them a priori unsuitable as basic formalisms for complexity theory. Indeed one can easily show e.g. P = NP if one isn't careful. It was only in 2014 that Accattoli and Dal Lago showed (in: Beta Reduction is Invariant, Indeed) that $\lambda$-calculus can be used for defining time-complexity if one is careful. No corresponding results are known for space complexity, or for $\pi$-calculus.