What you have isn't a generic networking device. It's basically a MPEG encoder chip with networking wrapped around it. To reduce jitter to the minimum there is likely one ethernet bus on the ingress side of the encoder chip and another ethernet bus on the egress side of the encoder chip. The management ports run to ethernet ports on the supervising CPU. You may not even be able to arc up a TCP link on the input and output ports. A lot of studio video codecs only support the expected packets on the input (such as UDP containing MJPEG images) and only generate the encoded packets on the output (such as UDP containing MPEG-2). To allow IP to work they'll bridge the ethernet bus into the supervisory CPU so that the CPU can generate ARP and so on. 

shows the involvement of the Route Processor in handling ICMP. It's a useful capacity planning tool for setting filters to avoid the RP being attacked by ICMP floods. The counter can't be zeroed from the command line as it's really the output of FreeBSD's . Because of this will will go back to zero when a RP is booted and may have a discontinuity when RPs hitlessly change between master and slave. For what you are trying to do you could set a firewall filter to count the ICMP packets seen at the interface. 

A zero-hop protocol is one which is designed not to be forwarded by a router. This protocol could be expected to set IP.TTL to 1 (the router won't drop the packet just because IP.PROTOCOL=114). These protocols used to be commonplace in the era before internets (DEC LAT,etc). But the advantages of a protocol able to be globally routed are just too great for zero-hop protocols to be of much modern interest (even if in some deployments you might choose to limit the packets to one subnet). You can see IANA felt the same, calling out all zero-hop protocols, not just a particular one. 

There is no way to discover a ISP's complete topology using any source. The bounding constraints for the exterior-visible topology may be listed in IRR. But there is no requirement to do so beyond mutual agreement of interconnecting parties (aka "convenience"). Private route databases are commonplace. 

Think of it this way: broadcasts are seen by all machines in a subnet. So put a single machine on all subnets and start counting. Handwavingly: 

iperf will attempt to send UDP packets to reach the rate given in . If that parameter is missing, it will attempt to achieve the rate of the transmitting interface. 

Sending a good ten minutes of 0-interval MTU-sized DF pings with contents 0x0000 and a second test with contents 0xffff is an excellent way to apply some stress to simple transmission technologies. Lost packets -- or overly delayed packets after the first few packets -- are a clear indication that further investigation is required. It's also a good moment to check that the reported round-trip time is within reasonableness (it's very easy for a transmission provider to provision a circuit which crosses the country and back rather than crossing the city). Ping is great for finding faults. However, ping alone isn't a great acceptance test for being sure there are no faults. The rest of this answer explains why. As part of the ping test you should connect with each of your network elements on the path (hosts, switches, routers) and record the transmission traffic and errors counters before the start and after the end. Rising error counters of any type require further investigation. Don't ignore small rises in error counters: even a low rate of loss will devastate TCP performance. This still isn't to say that the link is acceptable. Let's take 1000Base-LX, ethernet over single-mode fiber. It's possible that the light levels at the receiver are under the specification for that transceiver's model. But we have an above-average sample of that transceiver so all is well. But then that transceiver has a fault and we replace it with a below-average-but-within-specification sample. The link cannot restore to service even though we have fixed the fault. So as part of the acceptance testing we need to check that light levels are within specification at both ends; and we need to check that there is a viable power budget at the extremes of both transmitting and receiving transceiver's performance (to make this easy, manufacturers will give their SFPs nominal ranges where they have done the power budget calculations, such as 10Km for 1000Base-LX/LH. But for any link longer than 10Km you should do your own power budget: five minutes arithmetic can save you hundreds of dollars in allowing you to safely purchase a lower-power SFP). SFPs often have a feature "DOM" which allows you to check the receive light level from the device's command line. More complex transmission technologies have forward error correction. So the link appears to work under high transmission error rates, but if error rates are higher or more sustained then the FEC is overwhelmed and the transmission passes rubbish. So for these links we are very interested in the counts of error corrections. Interpreting those FEC counters requires understanding the physical transmission, as we're now low enough in the "stack" that we can no longer pretend that media isn't naturally free of errors. But even in these systems a simple ping test is enough stress to give initial results. Finally, you should be aware that PCs are a cheap but not perfect test platform. So sometimes packet drops are because of the end-systems rather than the transmission. This can be simple IP-layer issues (such as a MTU inconsistent with the subnet, always a possibility when backbone links should be running with a MTU > 9000) or host performance issues (particularly >10Gbps). The cost of "real" ethernet test platforms is extraordinarily high, because you're paying for those issues to have been fully sorted via hardware or clever software (eg, running within the NIC). 

A IPv6 /128 is a host route, exactly like a IPv4 /32. A /127 is a point-to-point route, exactly like a /31. In the IP allocation database we'd usually allocate a /127 as the only network in an unadvertised /124 block, for textual representation reasons (point to point links then always end in 0 or 1, rather than requiring engineers to do binary math to work out which address goes on the interface nearer the core). All other subnets must be /64. You must not use a /64 on a point-to-point link, as that allows a DoS. As far as subnet address planning goes, subnet 0 is has a special textual representation and should be reserved for control-plane addresses of networking equipment. It is useful to reserve the lower 12 bits of the subnet addressing for a copy of the VLAN ID, as that gives a rapid and simple addressing plan for most subnets. The 13th bit obviously is 0 for VID-derived subnets and subnet 0; and 1 for others. Depending on your network you could use the highest subnetwork bits as a site identifier, allowing easy site address aggregation, and more effective area design (preventing flaps of non-backbone networks being propagated to other sites.). (Not seeking to be an answer, my comment got too long, vote up Ron's answer.) 

Because they don't connect with a provider which requires IRR to configure the import filters on their BGP connections. 

Within the 802.11 hardware is a 1MHz clock maintained in a 64-bit register. That register is initialised to zero when power is applied. Therefore the value of that clock's register is initially the uptime of the WLAN controller in microseconds. When a Beacon frame or Probe Response frame is transmitted then the value of that clock's register is placed in the Timestamp field. For a WLAN with a single access point your "aircrack-ng" program can print the Beacon frame's Timestamp field and claim the value is the uptime of the access point. However, in a multiple access point system the highest value of the Timestamp in all the Beacons received by an access point is written back into that access point's 1MHz clock register. Using this technique all the access points converge on a common value of the 1MHz clock (well, within 25Âµs or so). In the multiple access point case we can only say that an individual access point's Beacon Timestamp is the upper bound of the access point's uptime -- the actual uptime of any particular access point could be lower. In fact it's simple to imagine a scenario where there is no access point which has the uptime claimed by the Timestamp. For more information Google "802.11 Timestamp Synchronization Function", which is the name of the algorithm I have outlined above. You can download the 802.11 specification for free from the IEEE website. You'll find the TSF explained in depth in that specification. There's a lot more depth, as you can imagine from Adhoc networks which have no access points, multiple-SSID access points, and so on. 

The timestamp in a PCAP file is the time the traffic was observed by the capturing platform. It is meant to be wall-clock time to high accuracy. It is expected that not all platforms implement a high-accuracy wallclock time (eg, the platform may not run even NTP, let alone more accurate technologies). Furthermore it is convention that implementations which use some other zero-hour are permissible PCAP. For example, some of the scrubbers for sharing PCAP files will have the first packet be the zero-hour to avoid leaking the wallclock time the capture was done. Deriving uptime from a PCAP requires knowledge of the access point, its configured protocols, and (in more complex deployments) the coverage strategy of the access point controller. A few minutes of no traffic may not imply that the access point is restarting, or it may, it all depends upon the details. Moreover with radio systems a lack of traffic may indicate a failure to receive by the monitoring platform, not necessarily a failure to transmit by the access point. Someone may have been microwaving their lunch :-) Using SNMP is a more typical approach to collecting device uptime. Device uptime can be retrieved using the SNMP variable sysUpTime. That has SNMP object ID iso(1).identified-organization(3).dod(6).internet(1).mgmt(2).mib-2(1).system(1).sysUpTime(3). It is a key variable and implemented on all platforms which claim SNMP compliance. Edit The poster of the question has clarified their question 

The short answer is "SNMP" from a network management platform (canonically Nagios, but there's a world of alternatives). You'd be interested in monitoring: 

Now the interface counters alone are educational. You could install Net-SNMP and plot those directly. You could follow up the high-broadcast VLANs with wireshark for some short-term wins. For a longer-term issue, install a IPFIX export software on the Linux machine, run that to a IPFIX collector on the same machine, and use the collector's "top talker" feature to generate a regular report. IPFIX will be massively inefficient overkill, so in the longest term you'd write your own multi-VLAN listener in Python which keeps a table of (vlan, mac) and a decaying average of the number of broadcasts/all-stations multicast. You'd take the maximum of those values and report that back via Net-SNMP to give the network management platform (Nagios, etc) a value to alarm upon. 

We usually don't. We used to, since getting a 1.5KB of packet onto the wire took some time at 9600bps. But at the high data rates in modern networks the number is so small as to be safely ignored outside of weird situations. 

A "PHY layer switch" isn't possible. For a start, what's the framing of the traffic? Perhaps you are thinking of a SFP-based regenerator. For example, this ghip CONV 2004. They can be thought of as connecting the two SFP's GMII interfaces back-to-back. As such they have no understanding of the framing (although the CONV2004 is an advanced unit, and so has a electrical tap of the GMII which examines the symbols on the interconnection to do error reporting, and for that reporting it needs to know what the symbols mean, and thus if the traffic is ethernet/SONET/SDH/TV/etc). Or perhaps you are thinking of the similar devices available for interception applications, with an SFP's GMII interfaced to another SFP's GMII, and a tap of that GMII bus running to two additional SFPs to transmitting copies of the east-west and west-east received traffic. Although even these devices are increasingly using ethernet switching, to allow typical ethernet features on the links to the interception host, such as autonegotiation. Other than for 1000Base-T, these devices aren't very popular as a optical 50:50 splitter is more economic, assuming sufficient optical budget is available. Even for 1000Base-T the wide availability of 4 port switches with a "port copy"/"span"/"intercept" feature gives strong competition to a GMII-based device. 

In essence yes, with some small adjustments for the difference between the clocking of the media ("bandwidth") and the actual rate at which packets are accepted onto the media (ie, minus minor overheads like ethernet headers, SDH segmentation, forward-error control, tunnelling like VLANs, MPLS, GRE). We can do "traffic shaping" in the router so that the packet transmission rate presented to the media is substantially less than the available transmission rate of the media. This is how you can order a 200Mbps service over a gigabit ethernet fibre. 

These boxes are obviously bad news. So what you want to do is to place a private link-layer around those boxes. Cisco gives you two tunnelling options on their switching devices: Q-in-Q or TRILL (there's even more possibilities on their IP/MPLS devices). Q-in-Q is probably easier for you to get your head around. Configure Q-in-Q to transport all the client-port BPDUs transparently. Then it looks to your A/V gear like they are neighbours on the same hub. They can happily chat their bastardised protocols between the two of them. Once you get that working, simply implement a protected service for the network-side VLAN (aka "s-tag"), in your usual fashion (which I think was STP and interface costs, but you could also use LAGs or whatever). You can grow Q-in-Q into a proper multi-client subnet, should you have more than two of these nightmare boxes. You might also need to do that to get an Internet service into the client-side VLAN. 

The key difference between and is that stores the password using a reversible format and uses a one-way MD5-based hash. Many configurations have as well as and that makes "cracking" simple. But even if the machine is using only if you have a copy of the one-way hashed key then it's usually trivial to "crack" it due to the poor choice of enable passwords of a typical installation. Many will fall to a simple dictionary-based attack. For this reason it's usually better to have a random enable secret (recorded offline) and use the AAA features to allow RADIUS authentication to login into the machine and againt to enter enable mode. That RADIUS authentication can require an access token such as YubiKey, (or, as a less secure choice, gateway into corporate authentication systems where good password practices are more easily enforced). Cisco do make it easy to zeroise and recover a unit where you no longer have access, and another member has given that procedure in their answer. 

The short answer is, it is software defined :-) Let's unpack your question. An Unknown Unicast in a transparently bridged network is one who's destination MAC address isn't known. In transparent bridging we flood that out all interfaces. That address will reach the MAC address of interest (assuming it exists) and when that computer responds we'll see that MAC address in the source address of a ethernet frame and can record the port it was seen on. Importantly, that Unknown Unicast MAC address requires a entry in the TCAM of the switch. An Unknown Unicast Attack is when we send so many never-before-seen MAC addresses that the Unknown Unicast portion of the TCAM of the switch is filled with these rubbish addresses, and as the TCAM ages out the real MAC addresses then those real MAC addresses are fighting against the odds to be reinstated. We can of course take measures against this in a switch. For example, by preferring established MAC address entries in the TCAM. Now if we are using SDN to implement transparent ethernet bridging then the problem remains. The controller still has a table of MAC addresses, and we are filling the OpenFlow-switch with lots of rubbish (match, action) rules. Furthermore the OpenFlow traffic from the switch to the controller and the rate of change of (match, action) rules might begin to be an issue. But we don't have to use SDN to implement transparent ethernet bridging. We could implement something which has the same effect without using the same mechanism. As a really simple example, the provisioning system could configure the SDN controller with the location of every MAC address. Then there is no Unknown Unicast processing at all, as all Unknown Unicasts are invalid and can be dropped immediately. Or we could do away with MAC addresses altogether, and simply configure the lower bytes of the IP address into the MAC address of the ethernet interfaces. The SDN controller application would then essentially route, with no ARP. Or to take a wide area problem, if we are providing a metro ethernet service then we can simply encapsulate the received packet at the A-end client-facing interface, use our own MAC addressing to deliver it across our network to the B-end client-facing interface, and transmit the packet. Not looking into the customer's MAC addressing at all. You can see that this approach could also be used within data centre networking, and there are a variety of "tunnelling" schemes. Such tunnels mean that whatever the host claims as its MAC addressing is irrelevant, the network uses its own addressing which the host can't influence. So an unknown unicast attack is moot. Finally, an important practical consideration is that the SDN controller can scoff at ISO layering. If someone is having a go at an attack, and that attack can be detected from the upper layers (eg, by an intrusion detection system) then the OpenFlow Controller can be told block that class of flows from ever being switched. It's these "whole of network synergies" which make SDN much more interesting than simply reimplementing transparent ethernet bridging. To date the synergy of interest has been datacentre provisioning, but there's obviously lots of untapped scope in internet networks.