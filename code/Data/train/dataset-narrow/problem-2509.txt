Here's a step in the right direction... I'll argue that for $p=1/2$, you have $1/2 - I_n(1/2) = \Omega(\sqrt{1/2^n})$. (This is not quite as strong as it should be. Maybe someone can strengthen the argument to show $\Omega(\sqrt{n/2^n})$.) Here's a proof sketch. It suffices to show $1/2 - E_f[\min(\text{Inf}_1[f],\text{Inf}_2[f])] = \Omega(\sqrt{1/2^n})$. We do that. Note that if $\text{Inf}_1[f]$ and $\text{Inf}_2[f]$ were completely independent, we'd be done because the expectation of the minimum of the two independent sums is $1/2-\Omega(\sqrt{1/2^n})$. First, we'll argue carefully that the two sums are almost independent. Consider the universe of points $X=\{-1,1\}^n$. Call $x$ and $x'$ in $X$ $i$-neighbors if they differ in just the $i$th coordinate. Say the two neighbors contribute (to $\text{Inf}_i[f]$) if $f(x) \ne f(x')$. (So $\text{Inf}_i[f]$ is the number of contributing $i$-neighbors, divided by $2^{n-1}$.) Note that, if $x$ and $x'$ are $i$-neighbors, and $y$ and $y'$ are $i$-neighbors, then either $\{x,x'\}=\{y,y'\}$ or $\{x,x'\}\cap\{y,y'\}=\emptyset$. Hence, the number of contributing $i$-neighbors is the sum of $2^{n-1}$ independent random variables, each with expectation $1/2$. Partition the universe $X$ into $2^{n-2}$ groups of size four, where $x$ and $x'$ are in the same group iff $x$ and $x'$ agree on all but their first two coordinates. Then for each pair $(x,x')$ of 1-neighbors, and each pair $(x,x')$ of 2-neighbors, $x$ and $x'$ are in the same group. For a given group $g$ and $i\in\{1,2\}$, let r.v. $c^g_i$ be the number of contributing $i$-neighbors in $g$. Then, for example, the number of contributing 1-neighbors overall is $\sum_g c^g_1$, a sum of $2^{n-2}$ independent random variables, each in $\{0,1,2\}$. Note that $c^g_1$ and $c^{g'}_2$ are independent if $g\ne g'$. By a case analysis, if $g=g'$, the joint distribution of $c^g_1$ and $c^g_2$ is $$\begin{array}{c|ccc} & 0 & 1 & 2 \\ \hline 0 & 1/8 & 0 & 1/8 \\ 1 & 0 & 1/2 & 0 \\ 2 & 1/8 & 0 & 1/8 \\ \end{array}$$ Let r.v. $N=\{ g : c^g_1=c^g_2=1\}$ denote the set of neutral groups. (They contribute exactly their expected amount to the 1-influence and the 2-influence.) The number of contributing 1-neighbors is then $$|N| + \sum_{g\in \overline{N}} c^g_1.$$ Conditioned on $N$, for each $g\in \overline N$ r.v.'s $c^g_1$ and $c^g_2$ are independent (by inspection of their joint distribution above), so (conditioned on $N$) all r.v.'s $\{c^g_i : i\in\{1,2\}, g\in\overline N\}$ are i.i.d. uniformly over $\{0,2\}$ so, $$\textstyle E\Big[|\overline N|- \min\big(\sum_{g\in \overline N} c^g_1, \sum_{g\in \overline N} c^g_2\big) ~\Big|~ N\Big] \ge \Theta(\sqrt{|\overline N|}).$$ Finally, note that each group is neutral with probability 1/2, so $\Pr[|\overline N| \le 2^{n-2}/3]$ is extremely small, say $\exp(-\Omega(2^n))$ (and even in that case the left-hand-side above is at least $-2^n$). From this the claimed lower bound follows... 

Initialize all $x_s = 0$. Let $N=\log(n)/\varepsilon$. Repeat until all covering constraints have been deleted: 2.1. Choose $s$ maximizing the partial derivative of Lmin$(Ax)$ w.r.t. $x_s$. (Explicitly, choose $s$ maximizing $\sum_{e\in s} \exp({-\sum_{s'\ni e} x_{s'}})/c_s$.) 2.2. Increase $x_s$ by $\delta$, where $\delta$ is chosen maximally such that, for every remaining covering constraint $e$, the increase in $A_e \cdot x$ is at most $\varepsilon$. 2.3 Delete all covering constraints $e$ such that $A_e\cdot x \ge N$. Return $x/\min_e A_e\cdot x$. 

Proof sketch. We show $2 V(n) \ge f(n)$ where $f(n) = n^{1+c\epsilon(n)}$ for some sufficiently small constant $c.$ We assume WLOG that $n$ is arbitrarily large, because by taking $c>0$ small enough, we can ensure $2 V(n) \ge f(n)$ for any finite set of $n$ (using here that $V(n)\ge n$, say). The lemma will follow inductively from the recurrence as long as, for all sufficiently large $n$, we have $f(n) \le \min_{k<n} f(n-k) + \max(n, 2 f(k))$, that is, $f(n) - f(n-k) \le \max(n, (1+\delta) f(k))$ for $k<n.$ Since $f$ is convex, we have $f(n) - f(n-k) \le k f'(n)$. So it suffices if $k f'(n) \le \max(n, (1+\delta) f(k)).$ By a short calculation (using $f(n)/n = e^{c\sqrt{\log n}}$ and $f'(n)=(f(n)/n)(1+c/(2\sqrt{\log n})),$ and using a change of variables $x = \sqrt{\log k}$ and $y=\sqrt{\log n}$), this inequality is equivalent to the following: for all sufficiently large $y$ and $x\le y$, $e^{cy}(1+c/(2y)) \le \max(e^{y^2 - x^2}, (1+\delta) e^{cx})$. Since $1+z\le e^z$, and $e^z\le 1+2z$ for $z\le 1$, it suffices to show $e^{cy + c/(2y)} \le \max(e^{y^2 - x^2}, e^{2\delta+cx}),$ that is, $$cy + c/(2y) \le \max(y^2 - x^2, 2\delta+cx).$$ If $y \le x + 0.1\delta/c$, then $cy+c/(2y) \le cx + 0.2\delta$ (for large $y$) and we are done, so assume $y\ge x + 0.1\delta/c$. Then $y^2-x^2 \ge 0.1y\delta/c$ (for large $y$), so it suffices to show $$cy + c/(2y) \le 0.1 y\delta/c.$$ This holds for sufficiently small $c$ and large $y.$ QED 

Okay, I'll bite. Isn't your problem a special case of (non-metric) $k$-medians? Given a budget $k$ for the number of centers you want to open, and a bound $D$ on the maximum allowed distance on any city $v$ to its assigned center $c$, define $d'_{vc}$ to be $\infty$ if $d_{vc}>D$ and $P_v d_{vc}$ otherwise. Then your problem is modeled by the standard integer linear program for $k$-medians with (non-metric) distances $d'_{vc}$. $~\displaystyle \min \sum_{vc} d'_{vc} X_{vc}$ subject to 

I think the answer to your first question is "no". In particular, if you take a random bipartite graph $G=([n],[n],E)$ where $Pr[(i,j)\in E] = 1/2$ for each pair $(i,j)$, the answer is no with probability close to 1. I am interpreting your first question as follow: Given any graph $G$, is it always possible to find a collection of subgraphs such that 

                   In the picture, time proceeds from top to bottom. The solution $P(k)$ does not stop at time $N(k)$, instead (for use in the recursion) it continues until time $2\,N(k)$, exactly reversing its moves, so as to return to a single pebble at time $2\,N(k)$. The solid vertical lines partition the $L(k)$ layers of $P(k)$. In the picture, $L(k)$ is five, so $P(k)$ consists of 5 layers. Each of the $L(k)$ layers of $P(k)$ (except the rightmost) has two sub-problems, one at the top of the layer and one at the bottom, connected by a solid vertical line (representing a pebble that exists for that duration). In the picture, there are five layers, so there are nine subproblems. Generally, $P(k)$ is composed of $2\,L(k)-1$ subproblems. Each subproblem of $P(k)$ has solution $P(k-1)$. The crucial observation for bounding the space is that, at any time, only two layers have "active" subproblems. The rest contribute just one pebble each thus we have 

In some cases the recurrence for $T$ does not determine it uniquely. We take the following interpretation of $T$, which gives the minimum $T$ consistent with the recurrence. Repeatedly substitute $T(x) = a(x) + T(h(x))$ to expand $T(x)$ as the (random) sum $$ T(x) = a(x) + a(h(x)) + a(h(h(x))) + \cdots = \sum_{j=1}^\infty a_j $$ where $a_i = a(h^{(i-1)}(x))$, in which $h^{(0)}(x) = x$ and and $h^{(i+1)}(x) = h(h^{(i)}(x))$. lemma. With this definition of $T$, we have $K_r(x) \le \sup_i K^i_r(x)$. Proof. Inspecting the definitions of $K^i_r$ and $K_r$, we have $$ K_r(x) = \Pr[\textstyle\sum_{j=0}^\infty a_j > r] ~~\mbox{and}~~ K^i_r(x) = \Pr[\textstyle\sum_{j=0}^{i-1} a_j \ge r]. $$ To prove the lemma, we will show something slightly stronger, namely $$ \Pr[\textstyle\sum_{j=0}^\infty a_j > r] ~~=~ \sup_i \Pr[\textstyle\sum_{j=0}^{i-1} a_j > r]. $$ Define random variable $T = \min\{i : \sum_{j=0}^{i-1} a_j > r\}$ to be the first step at which the sum of terms exceeds $r$ (with $T=\infty$ if that never happens). Rewriting the desired equation in this notation, we want to show $$ \textstyle \Pr[T < \infty] ~=~ \sup_i \Pr[T \le i]. $$ But that equation follows from basic definitions as follows: $$ \Pr[T < \infty] = \sum_{j=0}^\infty \Pr[T = j] = \lim_{i\rightarrow\infty} \sum_{j=0}^i \Pr[T=j] = \lim_{i\rightarrow\infty} \Pr[T \le i] = \sup_i \Pr[T \le i ].$$ QED 

I believe the problem is NP-hard by the following reduction from Set Cover with sets of size at most 3. Given a collection of sets, each of size at most 3, and an integer $k$, create an instance of your problem where your set $S$ contains the edges of the following DAG: For each element $x$, create a vertex $v(x)$. For each set $s$, create a vertex: $v(s)$. Create a root vertex $r$. Add edges from every vertex (except the root) to the root. For each set $s$ and element $x\in s$, add edge $(v(x), v(s))$. All edges have weight 1, and $M\ge 3$ is the number of sets minus $k$. To see that the reduction is correct, first suppose there is a set cover of size $k$. Then there is a solution to your problem that chooses the following edges. From each $v(x)$, the edge to $v(s)$, where $x\in s$ and $s$ is in the set cover. From each $v(s)$ where $s$ is not in the cover, an edge from $v(s)$ to the root. Then the root has $M$ chosen edges into it, and each element-vertex has an edge leaving it, so the total number of edges chosen is $M+n$, where $n$ is the number of elements. Conversely, suppose there is a way to choose $M+n$ edges. The root has at most $M$ chosen edges into it, so there are at most $M$ edges from set-vertices to the root. There is at most one chosen edge out of each of the $n$ element-vertices. Thus, the only way to choose $M+n$ edges is if none of the chosen edges go from any element-vertex to the root. Hence, there must be $M$ edges from set-vertices to the root, and hence $M$ set-vertices that have no chosen edges into them, and hence the remaining $k$ set-vertices give a set cover of size $k$. Am I missing something? 

If you don't require the cycle to be simple, then break the (directed) graph into its strongly connected components, and for each component containing one of the given vertices $V_i$, check whether the component contains a negative cycle. If no component does, there is no negative cycle containing any $V_i$. But if some component does, you can find a (non-simple) negative cycle containing $V_i$ by taking many copies of the negative cycle, and adding to that paths to and from some vertex in the cycle to $V_i$. (The total time to find an implicit representation of the desired cycle will be the same as the time to find a negative cycle in a directed graph, e.g. $O(nm)$, if I recall.) If you do require the cycle to be simple, then the problem becomes NP-complete, even if only a single vertex $V_1$ is given. (You can reduce Hamiltonian Path to the problem: to find a Hamiltonian path from a given source $S$ to a given sink $T$ in a given graph $G$, give the existing edges weight -1, then add an artificial vertex $V_1$ with two edges of cost $N/2-0.01$ each, one from $V_1$ to $S$ and one from $T$ to $V_1$.) If you allow the cycle to repeat vertices but not edges, I believe it is still NP-complete (by a similar reduction, but splitting each vertex $v$ into a directed edge $(v,v')$ in a standard way). 

Initialize all $x_e = 0$. Let $N=\log(n)/\varepsilon$. While $A x < N$: 2.1. Choose $e$ minimizing the partial derivative of Lmax$(Ax)$ w.r.t. $x_e$. (Explicitly, choose $e$ to minimize $\sum_{v\in e} \exp(\sum_{e'\ni v} x_{e'})$.) 2.2. Increase $x_e$ by $\varepsilon$. Return $x/\max_{v} A_v x$. 

Here's a counter-example showing your desired bound is not possible, unless I am mistaken. It's a simple variant of the example in Roei's comment. Fix any $n$ and $N\ge 4n$. Take $D$ to contain $N/2$ points that are all the same (or all within distance 1 from each other), and $N/2$ points that are all widely separated (at distance at least 1 from every other point in $D$). This can be done even with $D\subset\mathbb{R}^1$. The probability that two uniformly random points in $D$ are within distance 1 from each other is $1/4$. In your notation, $p=1/4$ for $B=1$. The probability that $n$ uniformly random points in $D$ don't contain some pair within distance 1 from each other is (at least) the probability that all $n$ points are in the widely separated set and distinct, which is at least $[(N/2-n)/N]^n \ge 1/4^n$. So, in your notation $1-P \ge 1/4^n \ge (3/4)^{5n} = (1-p)^{5n}$. So, $P \le 1 - (1-p)^{5n}$. So your desired upper bound $P \ge 1-(1-p)^{\omega(n)}$ does not hold. (If you generalize the example above for arbitrary (small) $p>0$, I think you get something like $P \le 1- (1-\sqrt p)^n \approx 1-(1-p)^{n/\sqrt p}$, which is not a counterexample if $p\rightarrow 0$ as $n\rightarrow \infty$.) 

This clearly gives a fractional set cover. (Step 2 is well defined, because we are conditioning on the event that there is a set cover, that is, that each element is in some set.) To finish we bound the expected total weight of $X$, that is, $\sum_s X_s$. Stage 1 contributes exactly $k 2/pk = 2/p$ to the total weight. Stage 2 contributes, in expectation, at most $n\exp(-pk/12)$, because each of the $n$ elements has probability at most $\exp(-pk/12)$ of being covered with weight less than 1 by stage 1. (To verify this, fix any element. The element is left insufficiently covered iff it is contained in fewer than $kp/2$ sets. The expected number of sets covering the element is $kp$, so by a standard Chernoff bound the probability that the number falls below $kp/2$ is at most $$\exp(-(1/2)^2 k p/3) = \exp(-kp/12).$$ This ignores the conditioning, but the conditioning only decreases the chance that the element is contained in fewer than $kp/2$ sets.) QED As an aside, note that if $k$ is large enough, then greedy will almost certainly return a cover of constant size. (E.g. if $k$ is $2n2^n$ and $p=1/2$, then won't all possible subsets be present with high probability? And in that case greedy will return just one set of course.) Lower bound Assuming $p\le 1/2$, $k\le \exp(n^{1-\epsilon})$ for some $\epsilon\in [0,1/2]$, and $s = \min((\epsilon/4p)\ln n, n^{\epsilon/2})$, then the probability that there exists a set cover of size $s$ or less is $o(1)$. Of course this implies that greedy does not return a set cover of size less than $s$. The proof is probabilistic, using direct calculation and the naive union bound. Hopefully there are no mistakes in the calculations. Proof. Fix $p$, $k$, $s$, $\epsilon$ as above. The number of ways of choosing $s$ sets from the $k$ sets available is ${k\choose s}$. For any fixed collection of $s$ sets, the chance that it covers all $n$ elements is $(1-(1-p)^s)^n$. Combining these two observations, the expected number of size-$s$ covers among the $k$ sets is $${k\choose s} (1-(1-p)^s)^n ~\le~ k^s \exp(-n(1-p)^s) ~\le~ \exp\big(s\ln(k) - n e^{-2sp}\big).$$ To complete the proof, one shows that the right-hand side above is $o(1)$ (under the assumptions on $k$ and $s$). To do that, it suffices to show that 

Here's a sketch of the calculations. Consider some two consecutive steps that the bug makes. He goes from some point $a$, to $b$, to $c$. Points $a$ and $c$ are on same circle; point $b$ is on the other circle. Let $o$ be the center of the circle that $a$ is on. Consider the following three triangles, in order of decreasing size: 

These triangles are almost similar (i.e., congruent modulo scaling). More precisely, for $\epsilon = |ap|$, all three have the following property: the ratio of the length of the short leg to the long leg is $\Theta(\epsilon)$. (I won't prove this in any more detail here, but note that $\epsilon\rightarrow 0$ as the bug walks, and by perturbing one vertex in each triangle by a negligible amount, the triangles can be made similar.) The long legs $co$ and $po$ of the first triangle have length 1. Its short leg $|ap|$ has length $\epsilon$. Segment $ap$ is a long leg of the second triangle, so that triangle's short leg $ab$ has length $\Theta(\epsilon^2)$. Segment $ab$ is a long leg of the third triangle, so that triangle's short leg $ac$ has length $\Theta(\epsilon^3)$. Thus, in these two steps that the bug takes: