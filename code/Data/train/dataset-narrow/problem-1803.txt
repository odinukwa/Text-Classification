when I run the manifest with the --debug switch, by looking at notify statements I can see the classes be executed in the following order: 

There are times where, in order to do some debugging or perhaps a manual migration, I need to copy large chunks of data from one EC2 instance to another. When the size of the data is tiny, it's simple enough to just scp from instance A to my machine and then scp to instance B, but for large payloads that's unsustainable. My current approach of logging into AWS instances is through SSH over an OpenVPN tunnel. My OS user on EC2 machines doesn't really have the option of ssh from one machine to another. I have a couple of options here. I could generate an SSH key used just to ssh from one machine to another. Or perhaps I could enable password-based login only within the subnet of the instances (not sure that's even doable). Neither sound particularly appealing or safe. What's the best practice in this situation? I'm sure I'm not the first one to need to enable something like this. 

the process gets stuck on the installation of libssl because it requires the user input to restart some services (ssh ntp exim4) 

I just purchased a license of safelyremove. It has command line support. It's very nice. There is a full trial on the website $URL$ 

What CRC does exactly? Accordingly to wikipedia it should be an integrity check but how does it work? I discovered that setting this parameter to false my disks are finally recognized as sata2 rather than sata1 and speed are really increased. Why? I found this IBM paper in which they say: 'CRC Checkingâ€”(Default: No) Determines whether the controller verifies the accuracy of data transfer on the Serial bus. CRC Checking should be disabled on the controller and all devices if any device supported by the controller does not support CRC Checking." How do I discover if a hdd supports CRC? If CRC is disabled and a breaking event occurs, is there a risk? 

It sounds like you need to make the app be in the root of the site. So create a new directory outside of the inetput folder say C:\myweb. In C:\myweb add your wordpres application Open up the IIS management console and add a new website called www.mydomain.com and point the root of the web to C:\myweb. You can then use host headers to make it so www and any other subdoamin point to the same website in IIS. 

You can set up a Microsoft ISA server as your reverse proxy for your SSL. This then forwards the traffic over port 80 to IIS residing on another server. This will also work for wildcard certs in case you were wanting to use them. I believe you can use this to do load balancing also. 

The orange led doesn't mean that the drive is broken: it means that it's marked as failed. Recently I had the same issue on a ServeRaid 6i: two drives disappeared. The raid was level 5. I put one of the two online and I rebuilt the second. At the end of the process I got my array rebuild. Of course it was not a broken disk but a weird bug into the controller or into the disks. Some disks seem to have broken firmware that cause the disk to deattach from the array randomly. 

I never had experience on that precise model but generally the answer to all your questions is YES for every nas like that. Even the last question should be yes because the nas should be capable to rebuilt the array from the meta information stored in the disk and also because a mirror raid is not that hard to be rebuild. If you extract a running disk you can mount it on another host (the filesystem used by Iomega should be XFS, so every linux can mount it). Bonus question: If I understand well what you mean, the action is SWAP. Hot swap is when you can exctract a disk without turning off the device. This Iomega is not hot swap. Apart this in the recent past (last 3 years) we sold 12 Iomega Storcentre nas. We had 100% warranty emergencies on them: broken disks, bugs in the GUI, broken power units. We stopped to sell Iomega for this reason. I don't mean that this model is affected too but I would suggest you Buffalo $URL$ 

If your under windows you will need to edit the in your my.ini file. You can see the MySQL Ref Manual for help on this as well. 

It is possible to have hundreds of different websites all pointing to one database. You just need to make sure your host will allow your different domains/sites to connect to the same database. After all it's not the domain name that allows communication between the web and data tiers, it's the hosts server configuration and policies. For example I host one website, but have been thru 2 or 3 different domain names over the past few years. All of the domains point to the same website inside of IIS. Or in the case of one sub-domain points to a different website inside IIS, but still talks to the same database. It's al in your connection strings. However you will most likely not be able to have the database server on one host, and sites on a different host. These ports are normally shut down for standard communuication. Your best bet would be to talk to their technical support to see if it's allowed. 

I use rsync v. 3.0.4 and when I need to move something I use it with the --remove-source-files. I prefer rsync than mv. Unfortunately, when I use --remove-source-files, the directories are left on the source side (as said in the man). Is there a way to remove directories too once moved all the files? 

We are working on a project which involves different hardware all hosted in a single rack. The machines are mainly IBM servers: 2 x206 (scsi), 1 x226(scsi), 2 x3400(sata) and another assembled machine with sata controllers. We are using several raid controller. Some machines have only one Serveraid controller, others have one or more controllers not always Adaptec ones. All the firmwares and bios are updated. All the servers and connected devices are under ups. Over the last 4 months we experienced several strange behaviours in our hardware. Suddenly and randomly we loose 2 or 3 drives and the raid volumes stop to work. It can happen once a week but never at the same time of the day or week. Most of the times a rebuild process fixes the problem, sometimes we loose the data. Very often we just need to unplug the raid controllers, restart the server and the problem is fixed. At the beginning we thought it was due to firmware bugs but we performed an accurate update for every machine and raid controller and there is nothing else we can do on the hardware. We have really no hint on what's causing all these troubles. We are starting to think that it's an environmental problem but we don't know if there could be something interfering with our hardware. Have you ever heard of something like that? Do you have any idea on how to investigate the problem? 

It's important to remember that the data you are loading may be 900mb outside of your RDBMS but may be even more in the database with out compression. You also have to accout for database growth if it does it in chunks and transaction log space. So always be sure you have ample disk space when doing that large of an import of data as it seems that code is a direct result of running out of disk space. 

I recently added a new modem (a plain ol consumer grade one) to a fax server thats been up and running for years running on Microsoft Server 2003 fax services. The server currently has two modems, the new one is identical to one of the existing. After installing the new modem it showed up in the Fax Server Manager as a device but was not doing outbound faxes. (The server by default does not handle incoming.) So after a reboot the server no loger sees the modem in the Fax Server Manager but is listed as a device in device manager. I've attempted to restart just the fax service and even the whole box again but to no avail. Any one have any ideas on this one? Or any one with good links to resources for the fax service? 

The problem was that, at least on Ubuntu 12.04, runit services symlinks should be placed under /etc/service, not /service as per the Arch guide on Runit 

Some of my Googling revealed that supposedly rebooting the svscan service might fix this, but killing it and running svscanboot didn't make a difference. Any suggestions? Am I missing a step here somewhere? 

I've been doing some load testing with of my nginx reverse proxy -> my web app setup and I noticed that when I get to 1000+ concurrent connections, nginx starts returning 502s and the following error message: 

Ubuntu Trusty here. I'm having some trouble deciding where in the system I should configure which users are to be allowed to ssh into the machine, and which keys they're allowed to use. I've traditionally just created Linux users with respective home folders, and would place under ~/.ssh folder. It seems that another option is to use sshd's config as well. You can define there and even with all the supported ssh keys. I'm using this article for reference. Now, what's the best practice here? Should I not specify AllowUsers/AuthorizedKeysFile and let the OS user's existence and authorized_keys file decide if the user should be able to log in or not? Should I not use the the user's ~/.ssh/authorized_keys? Should I have both in place? The former makes configuration management (through Ansible in my case) a bit simpler, but I can have it in both places if need be. 

If you alter a table to add an index, what could happen if that thread was killed during processing? Accroding to MySQL: 

Sounds like you need to set up SMTP on the server. I had some issues getting everything working on 2008 so I changed direction and went with hMailServer as my server of choice. It's failry flexable and is free. It's also dead simple to get in and configure. Not to mention lots of documentation to help guide you through the porcess. Once you have it installed you would need to open, and forward, the needed ports from your router to the mail server. Normally they are 25 (SMTP) and 110 (POP3) unless your using SSL then you would need to congifure the ports accordingly. You can also configure it to forward your out bound mail to a relay if desired/needed based on your ISP and their restrictions, if any. You of course also need to set up the proper MX record(s) in DNS and have it pointing to your domain/IP. 

Of course I can work around this by continuing the installation by hand through the virtualbox GUI but this should not happen in unattended installation. How can I force the installation process to restart the services without waiting for the user input? 

Unfortunately this is shown inside the terminal and I couldn't find any way to hit "ok" and then, of course, I get this message. 

I can't answer to your question but I want to point on you on a detail that might be interesting: several sata hdd have a jumper to set them sleeping by default (the controller has to send a sata command to wake up the disk. [not all controllers can do that]). Is the jumper already set? Dam 

In the bios interface of Serveraid controller is possible to set the PHY speed. Beside this there is a parameter called CRC check which can be set to true or false. Knowing this, some questions: 

The memory limit is going to be a huge problem for that many databases. It may also stil be a problem in Web if you are using a low end edition of the Windows Server. The web edition is limited only to the memory resources of the host operating system. You may want to check out the features of all the versions. In the end the Web version may not be the right choice either. 

I've been trying to really figure out what my IOPS are on my DB server array and see if it's just too much. The array is four 72.6gb 15k rpm drives in RAID 5. To calculate IOPS for RAID 5 the following formula is used: . The formula is from MSDN. I also want to calculate the Avg Queue Length but I'm not sure where they are getting the formula from, but i think it reads on that page as . To populate that formula I used the perfmon to gather the needed information. I came up with this, under normal production load: . Also the disk queue lengh of . So to the question, am I wrong in thinking this array has a very high disk IO? Edit I got the chance to review it again this morning under normal/high load. This time with even bigger numbers and IOPS in excess of 600 for about 5 minutes then it died down again. But I also took a look at the , , and . These number were taken when the reads/writes per sec were only 332.997/17.999 respectively. %Disk Time: 219.436 %Idle Time: 0.300 Avg Disk Queue Length: 2.194 Avg Disk sec/Transfer: 0.006 Pages/sec: 2927.802 % Processor Time: 21.877 Edit (again) Looks like I have that issue solved. Thanks for the help. Also for a pretty slick parser I found this: $URL$ It works pretty well for breaking down the data into something usable.