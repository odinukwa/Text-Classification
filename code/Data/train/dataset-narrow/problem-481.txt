Reads data as a usual SNAPSHOT transaction, thus db errors would effect it. Some db erors would manifest it in read errors (like if DBA changed column type in a way incompatible with data), but other might result in some data being "invisible" and skipped. Backup file is stripped of fast-access metadata and geta a lot smaller, which is good for archives (example: 1.1GB of raw database -> 380 MB FBK -> ~700 MB after restore). In FB 2.x GBak is known to work considerably slower via TCP/IP than via Firebird Service Manager connection. It is told to be amended in forecoming FB 3. Restore is basically recreating database, so it is slow. But it optimizes the database internal layout and saves some space (no more half-empty pages in the middle of the file). Due to Firebird being very liberal in online (during active operations of users) scheme change (the safe approach was enforced in 2.0 but undone in 2.1 after uproar), the traditional backup might be "unrestorable", so the attempt at restoring a FBK file into a spare free disk is a must. Until you proven you can restore that backup, you may consider you don't have it. 

OTOH is the main and shadow servers were in different networks (offices, cities, countries) and the link disappears between them, then one of the networks would see it as crash of the main server and another would see it as a crash of shadow server. When the link would be repaired, the databases would have new different conflicting data entered by users. 

PS. additionally you have to learn the difference between SuperServer (targeted at small installations) and Classic/SuperClassic servers. For running 24/7 the second options would be preferable, since frequently the server instances would be shut down after user disconnects. So while "server" as a concept keeps running 24/7, the actual executable programs of it get closed and restarted, easening at potential problems like memory leaks in server or UDFs. OTOH Classic server is more vulnerable to cache synchronization issues like in case of crash during Garbage Collection or attempts at metadata (scheme) changes while users are working. In FB 3.x they promise to integrate those two approaches to make it a kind of sliding scale options in firebird.conf 

Here, you can use sequence to increase value. Please noted if the sequence reach to its , you will face this error . For example 

If not, It causes an error . Secondly, please remember that you cannot have when on the trigger. If not, It raises an error . Finally, please take a look at the example as below: 

I am using nethogs command to tracking WAL send/receive between master and slave. Download: nethogs for centos link Install: Tracking: Ref: 18 commands to monitor network bandwidth on Linux server 

2/ QUESTION: What is about "FATAL: terminating walreceiver due to timeout" problem ? How can I fix it ? 

Question: After researching , I found 4 solutions. Are there any better solution (not using copy data from files) ? 

Description: We have a table on PostgreSQL 9.3 and we migrate it (data & structure) to Oracle 11.0.2.4. Here is our table: 

Problem: I tested over 10 times, each of times when data is inserted about 2,000 rows (2,000 records is commited into machine 2) , with insert time about 3 minutes , I got errors: 

From @Craig Ringer, you should re-format the data. Based on your data, on Linux, I used & to format CSV file before importing. For example: suppose that file as below 

You can use Jailer tool, it will find all rows of child tables that reference to master table. Example: I have 4 tables : employee, employee_detail, phone_address, relationship. I want to delete employee which name="JOHN". With Jailer, it will find rows in employee_detail & relationship which FK to "JOHN" (by id). And because of phone_address reference to employee_detail, so Jailer will find rows in phone_address. 

Description: We are using PostgreSQL 9.3 - Centos 6 x64 . We have a dtsc_search_data table as below: 

In EDB 9.3 , just one query no.4 can cast. And PG 9.3 , all queries can do. Are there still any ways to cast date in EDB ? EDIT: With EDB : 

Back-up is very fast (just dumping of changing pages), can use cascading (database -> monthly large snapshots -> daily deltas from last monthly -> hourls delta form last daily). However it does not optimize (by recreating) the database. And it does not detect database errors other than top-level, errors of pages allocation (like "orphane page") OTOH the pages snapshotted are mostly intact, so in case of partially corrupt database they might still have a manually salvageable data. If the corruption would be noticed quickly. In a sense, that amounts to safe and incremental copying of he database file(s), wit hall the cons and pros. 

Yes, it is possible and it is covered in documentation and in lot of FAQs like the one at iBase.ru For the example at my development box I have co-installed FB 2.1.5 Win32 SuperServer (at default port 3050/tcp) and FB 2.5.2 Win64 Super-Classic (at a custom 3064/tcp) There might be troubles with FB 1.x as it was using registry, but FB 2.x was made isolated and self-dependent. One option is to download ZIPs and unpack them to different folders. Then you have to run text window of Windows Command Prompt "As Administrator", go into "bin" subfolder and there are all those executables like server itself. There also is "inst_svc" tool. Running it with an option like "-?" would show you brief help. Focus on installing main service, not installing Guardian (only needed on Win98, only shipped for legacy uniformity) and giving non-default "instance name". Then go outside "bin" and open "firebird.Conf" with any text editor like notepad. The documentation is within that file how to set non-default TCP port. That's all. Do the same for your another FB folder and you've done it. Another option is to run two installers. The 1st one would do all the described above things automatically. The second one would unpack files - and ask you to do those configuring operations for the second copy manually. Just do it like described above. 

Don't use bare because without SET UNTIL it doesn't work as you'd expect in some scenarios. I wouldn't say any of these two solutions are inherently safe, both have as many pitfalls as any other Oracle's advanced features, so you need to do some research upfront. 

The next thing in terms of cost is an OS (machine) and its IP address. You cannot afford a separate system for every TNS name. So crmdb.mydomain.local is not the only name for the IP address; the same IP address would have more names, like financedb.mydomain.local. Your OS admin would decide how to do this best, and how to determine the main hostname of the OS. They have the same problem with many other systems - multiple names referring to one OS - so they should have a solution at hand. The only people who are confused now are DBAs and OS admins, they see multiple hostnames leading to the same IP address. But users don't care about that and are not confused by that. (By the way this approach is coherent with SCAN. ) The next thing in terms of cost is either one of the two: an Oracle instance or "administrative cost of separating schemas out of instance". The tradeoff is for you to decide. 

In case you specify each block is read once and written two times to backupset copies. The two backupset copies are supposed to be bit-to-bit identical. Both copies have the same backupset key (BS_key) in RMAN. You cannot mix tape and disk copies - either both copies go to or both to . 

In Oracle starts a PL/SQL block. In other words, after you ought to provide a text of a program that is written in a procedural language that is somewhat different than SQL (although it bears some similarities). Your sqlplus had read everything but it was not yet parsing or executing anything, it was waiting for an and a line containing only a slash like this: 

Erwin said "You probably don't want to hear this, but the best option to speed up SELECT DISTINCT is to avoid DISTINCT to begin with. In many cases (not all!) it can be avoided with better database-design or better queries" . I think he's right, we should avoid using "distinct, group by, order by" (if any). I met a situation as Sam's case and I think Sam can use partition on event table by month. It'll reduce your data size when you query, but you need a function (pl/pgsql) to execute instead of query above. The function will find appropriate partitions (depend on conditions) to execute query . 

About errors above, because of the connection between pgpool and slave server, if I change slave's pg_hba.conf for pgpool host from md5 to trust, it work fine. Two ways to fix: 

Here our query and result we want, it means: when deleting id = 1 (parent row), table will automatically set parent_id = null in child rows (first level) . 

PSQL connection is OK. When running ETL, I am sure that client on machine 1 is connected to PG database on machine 2 , I tracked by query below 

Here, as you can see with value it worked (didn't raise error), however others didn't. I'm curious about that. Could someone please explain to me why ? I attach test script 

Format CSV, disable quote (replace multiple spaces to one space then space & to ). is result file. However, please check your input data if it is large because I tested on small data . 

Until now, pgBouncer doesn't support rotate log. Hence, you have to do it by yourself. You can refer to sites below: How to rotate PgBouncer logs in Linux/Windows ? How To Manage Log Files With Logrotate On Ubuntu 12.10 

Master ----> Slave : relpica from Master to Slave by asynchronous method (M send WAL, S receive WAL) 2/ Question: How can I monitor (catch) speed of WAL (ex: 1MB/s) is sent from Master to Slave ? 

What is the most costly thing for you? Prioritize. Usually the most costly thing is confusion of users. Ordinary users very rarely use a TNS name. They usually use the data through a dedicated application, and know just the name of application, like CRM. Power users connect to TNS names (think SQLdeveloper), but they also connect to the ordinary applications. Why would you want application CRM use a TNS name of ORION? Power users see TNS name and all they care is TNS name; not SID, not hostname. They talk to other users in terms of TNS name (get that report from FINANCEDB John!). The state of minimal user confusion is when TNS name is the same as the name of application using it. Either exactly the same of with "db" appended. So TNS name is like CRM or CRMDB. I prefer the latter. The next costly thing is changing anything inside tnsnames.ora. You don't know where it is distributed. You put the file in one's client machine or an application server, and voila three months later it magically appeared on dozen others. Can you prevent it? (No, you cannot.) So in practice this means once you give any user a TNS name, its definition is set in stone. So, don't put SID there, put a SERVICE_NAME. Don't put IP address there, put a hostname. And those two would stick to TNS name "forever", so there is no reason for them to be different than TNS name (Occam razor). 

The "device" in RMAN is a misnomer, it should be really called "storage". The "sbt" (synonym of "sbt_tape") is a misnomer again, as it has NOTHING to do with any tapes, it should be simply called "non-rman". This is just an empty placeholder, to be filled with any "plugin"; the plugin is called by Oracle either the "Media Manager library" or SBT_LIBRARY. This plugin allows rman to store and retrieve files through it, so rman only tells that it needs a file handle "xyz" (file is identified by a string handle) and doesn't need to know how the file is delivered, from tape or anything. The plugin is normally a part of an independent backup software, such as IBM TSM or Symantec NetBackup or many others. Oracle provides a simple emulator SBT_LIBRARY=oracle.disksbt for testing. Since you didn't fill that placeholder with any "Media Manager library", you receive an error message as expected.