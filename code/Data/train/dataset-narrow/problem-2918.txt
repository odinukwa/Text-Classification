depends on the license the song is available under, Creative Commons license has many guises some of which allow for usage so long as you nominate artist. the various licenses are explained quite well at 'open game art' $URL$ 

not sure when you are running Mouse.GetState() as this is required to update the state of the previous and current mouseState objects what i would be looking to do is 

forgive me if i understood your question incorrectly, i have given this some thought in the past and decided that i would create an Enum for each action, and map each action to an enum for the controller method and an enum for the button /key press this would in theory allow you to use both controller and keyboard (and mouse for that matter at the same time) all you need to do is code the listening method to return the correct enum set when asking for the mapping of an action finally when in you update loop that key is pressed you need to fire that event and have any component that is listening for it respond accordingly. personally i would just create a wrapper class to do this for you and have the components talk to the wrapper (therefore maintaining separation of concerns a bit more) 

use Texture2D.getData() to extract the colors array then use this array to create a new Texture and add a pixel on each time increment you should be able to modify the below to your need 

note this is untested and you may need to alter the order in which colorsNew is filled, but this should basically do what you require 

the code doesn't do anything with it i can also wrap the draw method like this also whilst it maynot be the cleanest solution (it leaves a empty component in release mode) it does allow me to ignore any if debug anywhere else in the project. 

I have been debugging this for a while but I can't seem to find the issue. I created a framebuffer for layered rendering like this 

DirectX matrices are right-handed and OpenGL requires left handed matrices, but while OpenGLs matrices are column-major, DirectX's matrices are row-major, so they're layout in memory is the same. 

However, this is confusing for me. For example, if the light direction is parallel to the normal, and if the color of the point is pure red (1,0,0) and the color of the light is pure blue (0,0,1) I'd expect the point to have a color of violet - yet the result will be black. Similarly, if I have a completely white point, or a completely black point, I wouldn't intuitively assume that the result is completely white (regardless of light color) or completely black either. I guess I'm fundamentally misunderstanding something here. Can someone explain? 

Now these points will all be centered on the origin of your coordinate system, so you need to move them to whatever position where you want your circle to be. But I think you can figure that one out yourself :P 

First of all, what you're looking for isn't really an engine. You're looking for a graphics API or a graphics API Wrapper to do your drawing. I can recommend SFML. It's a C++ library that manages most of the basic things that everyone needs for you. It features a relatively powerful graphics part which wraps around OpenGL, but also includes basic support for window-management, input (keyboard, mouse, joystick/gamepad), audio (playing sounds and music) and networking. Since it's C++ and entirely object-oriented, it will be very simple to understand. For example, loading an image and displaying it more or less looks as simple as this (in SFML2.0): 

or have RGB packed and A stored in a separate buffer. This is uncommon in live data in games, because modern graphics cards are often designed around RGBA pixels. RGB can be handled uniformly, and with A adjacent quick calculations can be done that allow compositing of a RGBA pixel on top of a pre-painted background. The alpha channel is often used for transparency information. Annoyingly, the max value of alpha is sometimes treated as transparent, and sometimes as opaque, by different code bases. This again is less common nowadays. Alpha channel has also been used for z-buffer ordering (how far the pixel is from the viewer) to allow unordered drawing to a scene, and traditionally mip mapping was done by shoving the 2x 2x (or 4x fewer pixels) lower resolution image into the alpha channel of the higher resolution image, recursively. Even in a system that uses RGBA pixels, the meaning of these pixels can disagree. Does R measure the how much red a human sees, or how many photons of red should be emitted? This is the difference between linear and non-linear color spaces, and it impacts the correct way to composite said pixels with a transparency channel. 

and there are formats with different number of bits per pixel component (3,5,8,16,32), and some have non-uniform bits per pixel component. The channel becomes the "virtual" array of only one pixel component. (I say virtual, because there is a stride between each element, while arrays traditionally do not have such stride) The RGB portion is not fixed either -- you could have a greyscale image with or without an alpha channel, you can have a CMYK image (Cyan Magenta Yellow blacK, usually in a subtractive color space), you can have an image that has a whole myriad of different color channels created by a specular raytracer or a scientific instrument. While I have treated pixels as if they are always contiguous, it is also possible to have the channels be split over multiple locations, like this: 

where _time is the time since last frame. although down side of this is that it will not map actual path accurately just be sure the ball is not so close as to confuse it with the current ball 

store the location of the ball last frame and draw that, or if that is too close store the location 5 frames ago alternatively if you know the direction of the ball use 

there is certainly a change in thought process when working with game development. so some practice there would be a requirement but ultimately like any form of creative work a portfolio would be your strongest asset 

I performed something recently where i ripped every word out of war and peace whilst it does not contain every single word in the dictionary it does have the added benefit of being able to tally word usage to get an idea of usage distribution you will also find slang words and names however, although these can be filtered rather simply 

taking a different approach i would look to do something similar to $URL$ essentially find what is below the line and generate a color 

articles generally are not liked in Stack exchange but this is my go to blog post when trying to remember this $URL$ 

what you code seems to currently be doing is trying to compare two states (that are at the same state) and comparing opposing booleans but you should post more code for us to be sure 

as a quick response to a very open question you need to decide why you want to learn these things if it is to purely make games in your spare time why not have a look at unity3d if it is to become a programmer / gamedeveloper you should first learn the very basics of c++ $URL$ or any other language you may choose. after that you need to start thinking about which platforms you want to target and types of games you would like to make. then i would start to consider your options in line with DirectX or openGL but always keep the options open to things like XNA, monogame or other libraries there are thousands of tutorials out there microsoft has plenty of DirectX tutorials, and reimers.net has some also 

With the availability of compute shaders for both DirectX and OpenGL it's now possible to implement many algorithms without going through the rasterization pipeline and instead use general purpose computing on the GPU to solve the problem. For some algorithms this seems to become the intuitive canonical solution because they're inherently not rasterization based, and rasterization-based shaders seemed to be a workaround to harness GPU power (simple example: creating a noise texture. No quad needs to be rasterized here). Given an algorithm that can be implemented both ways, are there general (potential) performance benefits over using compute shaders vs. going the normal route? Are there drawbacks that we should watch out for (for example, is there some kind of unusual overhead to switching from/to compute shaders at runtime)? Are there perhaps other benefits or drawbacks to consider when choosing between the two? 

Let's say I have some surface rasterized and stored to a texture. How do I calculate the actual surface area that is visible in one of the rasterized pixels, from some other location? So far, I'm assuming it's A_visible = A * cos(theta) / d^2 where d = distance from viewer, theta = angle between normal and viewer-surface direction, A = ... I'm not even sure how to express it. I guess you could call it the "world space area" of the pixel. Is the above equation even correct? How do I determine "A? Notice that I'm not only looking for the area strictly as visible from the (e.g. perspective) camera. I'd also like to compute this area as visible from other surfaces, i.e. other surface areas that receive light from all around its hemisphere above the surface. 

etc. As a bonus, since you're using vector I'm strongly assuming that you have access to a C++ compiler. Please use std::ifstream and getline. Thanks :-) 

The spiders are coming after you, but cooperatively. One gets ahead, then it sends silk back to the other ones. Which are pulled up. They advance exponentially. How close the horde is then becomes a measure of how many spiders are close, instead of how close the front-edge of the spiders are. The handful of spiders that get close also send webs at the player. At below the critical threshold, these just annoy the player in some way. Above the threshold, they are fatal. If there is one lone spider pulling a buddy up, they are far behind. If there are 7 spiders all pulling buddies, they are closer. If there are 20 spiders, they are almost ready to overtake you. If there are 50 spiders, they catch you in their web. These spiders also advance in leaps and bounds. So even if you outrun the spiders, one will sometimes "catch up". But then you'll outrun that one for a short bit. The length of the "gap" grows (logarithmicly), and the chance that a second spider "pulled" by first one before it leaves the screen also shows up goes down. Now, even when the spiders are far away, sometimes a huge clump will catch up and attempt to attack with webs. The frequency of this also goes down as you get further and further ahead (again, logarithmicly; so it never never happens). 

A pixel is a collection of components. Traditionally these components are red, green and blue, with each of them taking 8 bits of data. As it is often advantageous to have memory aligned certain ways, these where sometimes packed into 32 bits of space each. This leaves 8 bits of extra data at the end of each pixel. As this is neither red, nor green, or blue, it needs a name -- and alpha was a relatively meaningless name. Now, when you have an array of pixels arranged into a rectangle, you get this: 

How can I get around this problem, when I don't have geometry information available while applying the shadow map (e.g. during lighting pass of deferred shader)? 

A script is usually a piece of code that runs outside your core engine. It is usually contained within text files wherever you like to keep them. Then it is usually loaded by the engine, parsed, and executed at runtime. What generally happens is that whatever language you use (Lua, Angelscript for example), this language usually has some facilities that enable the engine programmer to expose engine-functions or even entire classes to the instance of the "scripting engine" that is currently running. For example (totally stupid example, but just to get the point across) your game code might have a public function that spawns zombies somewhere: 

Basically, you use collision detection to detect a collision between whatever your objects are, and a rectangle which is the current view of the camera. If the two collide, the object needs to be rendered. In order to not have to check this for every single entity on the screen, you should use spatial partitioning structures such as quad trees to reduce the number of collision tests that you have to do. 

Simple question: In GLSL, is there a way to share functions across multiple shaders, or do I have to define all functions in every shader that needs them? 

I'm looking for implementations of this because I'm having an especially hard time understanding how data is usually handled. As I understand it, simply using critical sections and locking code regions where modification of shared resources happens is a no-go, because locking and unlocking imposes quite severe performance penalties on the program. So another approach would be to simply copy all relevant data when I create the task and pass it to a thread and overwrite the original data at the end or at the beginning of the following frame with whatever the worker thread returns to me... which results in the problem that if two tasks modify the same resource... it's unclear to me how to meaningfully resolve that "result collision". I guess it's theoretically possible to apply only deltas of incoming changes: Let's say each entity, at the beginning of a frame, checks the messaging system for modifications to itself. If there are n modifications to the entities component e, then the new e would probably just be the old e plus the sum of all "old_e - new_e" for each n. But that seems kind of ugly and I'm not sure if it even works for things which are not just simple changes to single numbers. Are there any open source examples which implement this system that I could actually look at?