Use two private keys Set up H2 using the same process (but not the same private key) as you did when you set up H1: 

Bruce Schneier has given a recommendation for storing passphrases in a way that cannot be read or erased by any virus, trojan, or other malware. 

to install that library where gphoto2 can find it. IMPORTANT: Turn the mode dial on the top of the camera to "Superior Auto". It definitely won't take snapshots with the mode dial in the "Movie" position. Then 

p.s.: On a machine in a different city from my server, I occasionally fire up TortoiseHg workbench and click the little button that, behind the scenes, runs 

What is the difference between rsnapshot and rdiffbackup? Which is better for backing up a new snapshot of my laptop (over the Internet from some remote location) to my server (which already has the previous backup) ? As far as I can tell, they both store (on the backup server) the latest backup and many previous versions of backed-up files (in the form of compressed reverse-incremental deltas). They both use rsync to make backups over the internet faster. 

Unison is a nice graphical front-end to rsync. I've used Unison on a Windows box to talk to rsync on a Linux file server in another city to synchronize a folder at both ends. 

Today I noticed a few files on my Ubuntu and Mac machines that seem to be missing in the Mercurial working directory on my Windows machine. I'm using Mercurial to keep a folder synchronized between several machines. Whenever anyone improves any of the files in that folder or adds more files into that folder, I want every one of these machines to be updated with the new and improved files. I did a "push" and a "pull" and an "update" on every laptop to synchronize them with the server (and a "update" on the server), and I checked that the working directory was "clean". So I expected every sub-folder in the working directory to at least have the same number of files on every machine. But what I see is that the Mac and Ubuntu have a dozen files in one particular sub-folder in their working directory -- so apparently Mercurial successfully updated the new files when they were added to one of those machines and copied those new files to the other machine -- but the Windows machine has only 3 files in that sub-folder. In the Window 7 file manager "Folder Options >> View", I have "Show hidden files, folders, and drives" enabled. 

(*) By "supports snapshots", I mean either (a) run inside a virtual machine that supports system snapshots, or (b) use ext3cow, btrfs, ZFS, or some other file system that support snapshots -- all the ones I know of are indicated by the "snapshot" column in the "comparison of file systems" article. 

type: (but use your actual username on S1 and S1's hostname, and later type in your password on S1 when it asks for it). This installs the public key of your workstation into the file for that user on the server. 

which tells me exactly which files have changed, no matter how deeply nested in some sub-sub-folder, then I type 

with a comment that hopefully describes why I made these changes. (There are ways to revert only some of the files, and to commit only some of the files, and even ways to commit only some of the many changes that were made to a single file -- see the documentation for details). Perhaps you would rather have a cron job that daily does something like 

What about it? When a laptop is playing streaming audio (or streaming video; roughly 5 Mbit/s for 1080p video streams), the sender sends it as a series of small packets. Modern communication hardware is fast (802.11a transmits at over 20 Mbit/s, and more recent Wi-Fi standards are even faster), so there are (relatively) long gaps between one packet to that laptop, and the next packet to the same laptop. Between each packet there's plenty of time for the wireless access point to send packets to several other laptops, listening for the occasional packets from those laptops that ACK the previous transmissions and request the next piece of the stream, and even more occasionally re-sending packets when there is a collision or some other error. If all of these laptops are talking to the same access point, then the access point coordinates the transmissions so only one device at a time is transmitting. If some of these laptops are talking to one access point, and others are talking to some other (uncoordinated) access point, and all those devices are located close enough that they can easily overhear each other, then there will be frequent collisions. Any packets that collide are re-transmitted. Typically the re-transmitted packet eventually gets through, long before it's time to play it, so no human even notices there was a problem. So (typically) every human gets the illusion of their own "continuous" dedicated stream. 

(and asks me for Mr Backup's password) to get an off-site backup of everything that has been committed to the repository. Rather than make edits live to the production server, it's usually better to make the edits on some other machine, then commit them and 

speed Often people find it is much faster to (a) download a large file from some untrusted "nearby" content delivery network (CDN), mirror site, torrent peers, etc. and also download the corresponding short checksum file (often SHA256; older software often used MD5) from a few trusted sources. They find it unbearably slow to (b) download the entire large file directly from a trusted source. validation Often that person finds that everything validates -- the sources (trusted and untrusted) agree on the same checksum, and running shasum (or md5sum) with any of those short checksum files (it doesn't matter which one, when they are all identical) indicates that the large file has a matching checksum. modification You are right that when Mallory maliciously alters a large file sitting on some download site, it would be easy for Mallory to also maliciously the checksum for that file on the same download site so that shasum (or md5sum) run on that malicious checksum file would seem to validate the large file. But that checksum file is not the (only) one the downloader should use for validation. When the downloader compares that malicious checksum file to the checksum files downloaded from trusted sources, if the original checksum slips through even one time, then the downloader will see that everything does not validate, and will know that something has gone wrong. As cpast has said before, if a good cryptographic checksum is transmitted over a trusted connection, it can provide security (which is derived from the trusted connection). As supercat has said before, the checksum files one site don't help people who download large files from the same site and in the same way that they download the checksum files -- they help people who want to download files from some other site. 

The immediate consequence is that some of the overlapping bits are corrupted. Often so many bits are corrupted that the receiver sees that the frame check sequence doesn't match up, and the receiver simply throws the bad data out and otherwise acts as if it didn't hear either packet. Later, higher-level protocols eventually notice that a packet hasn't been acknowledged (ACK'ed) and re-send the packet. However, the WiFi standards include many different ways of preventing such collisions, including: 

Most computer chassis put the PSU at the back, because many PSUs have a noisy fan. The system sounds quieter if that fan is as far away as possible from the user, who is usually closer to the front of the chassis. Also, some users don't like hot air blowing directly on them. Have you considered: 

A compressed snapshot of every version that has ever been committed stays in the ".hg/" folder. There's a "hg update" command to revert to any committed version. There's a "hg diff -r 1:2" command to see exactly what changed between the first commit and the second commit. more complex situations (*) Often there is only one folder I want to back up ( "/var/www/" ). However, sometimes I have a more complex situation -- the files I want to back up are scattered in a bunch of different folders, and the only common folder between them is the root folder "/", and I don't want to put the ".hg/" repository in the root folder "/.hg/" . There's probably a better way to handle it, but what I'm doing right now is: 

If you are using (or don't mind switching to) a file system that supports snapshots(*) then backing up 1 GB per day is pretty simple: 

to revert all changes back to the most recent commit. If I do like what I see -- I've tweaked something that actually makes it better -- I type something like 

Some people use a "browser on a USB stick" ( a b ). Since all the cookies, etc. are stored on the stick, when you unplug the "stick I use for banking" and plug in the "stick I use for browsing Youtube", there's no way either site can access the data stored in the other unplugged stick. Other people recommend booting an entire operating system from a USB stick when doing online banking. I have Ubuntu on one USB stick and Fedora on another USB stick. I hear that other people like Puppy Linux or Chrome OS for their "OS on a USB stick". In particular, the Fedora "Live USB creator" tool allows you to set how much of the USB stick to reserve for "persistent storage". If that is set to zero, then you can be sure that no cookies, etc. are stored on the stick. 

them to the production server. The ".hg/" folder keeps growing -- very slowly, because it only saves files that change from one commit to the next, and even those relatively small changesets are compressed before they are stored. There's probably a better way to deal with this slow growth, but what I currently do is: After I "hg commit" the current version and then "hg pull" onto my off-site backup machine, once a year I delete the server's ".hg/" folder and use "hg init" to create a fresh, new, empty ".hg/" folder, and then I commit the current version. (The last version of the 2014 off-site backup repository should be identical to the first version of the 2015 off-site backup repository). 

The "init" line creates a ".hg/" folder that, someday, will store compressed snapshots. (hence the name of a popular Mercurial tutorial, $URL$ ). The "add" line and the "commit" line, by default, scans every file every file in that folder, no matter how deeply nested in sub-sub folders, and puts a (compressed) copy into that ".hg/" folder. When I suspect damage or other change to the working files (and assuming the ".hg" folder, containing all the snapshots, is undamaged), I type 

After you have the first snapshot backed up, you could keep creating fresh new snapshots in exactly the same way. But I think you would get exactly the same results in less time by 

Yes, any one of the many revision control software tools that will do exactly that. I use Mercurial ("hg"), often with the pretty gui TortoiseHg frontend. On many of my servers, there exists one folder(*), perhaps "/var/www/", that contains everything I want to back up -- settings files, templates, custom server-side cgi-bin scripts, custom browser-side .js scripts, .html content, etc. (Everything else on the machine is boilerplate operating system and application stuff. If that stuff gets damaged, I'd probably wipe it and install the latest version of it, rather than try to revert to the old, obsolete version I was using). When I first set things up, I cd to that folder and do the one-time setup 

which tells me exactly what changed in each file. If I don't like what I see -- it's malicious modifications, or more commonly, it's my own silly edits that I now regret making, I type 

When I start the TortoiseHg Workbench, nearly always I want to see what changed since the last commit. So I wait for it to start up, then double-click the repository in the far left "repository registry" widget, then look at the "Revision History view" widget on the right and click the top line "* Working directory *". Then finally I can see a list that shows files that changed (if any) etc., and in the "diff" pane the actual changes in a changed file. I hear rumors that it is possible to change some setting to get TortoiseHg workbench on my computer to do that automatically. How do I do that? How do I set things up so when I start the TortoiseHg workbench, it automatically opens my favorite repository and show files in the working directory that have changed but not yet been committed (if any), etc., without any more clicking on my part? (Is Stack Overflow a better place for this question?) 

People have it hammered into their head that one account on a server has a single username and, of course, a single authorized password. Public-key systems like ssh are better than the password system: One account on a server has a single username and any number of authorized public keys, all of them listed in the file. (more details). 

A standard USB hub will not work in reverse. I suspect it may be possible to adapt the Synergy software to do what you want to do, entirely in software. It may be possible to splice together pieces of several USB hardware projects to do what you want to do -- connect USB keyboard to Arduino; have Teensy Arduino send keypress information to a host PC ( a and b and c ); and somehow combine them ( d ).