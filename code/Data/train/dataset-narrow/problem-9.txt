So still not benchmarked much, but after a quick code inspection and a bunch of reading about netty and logstash in version 5 the input is not the bottleneck to worry about. Logstash team did put a bunch of work in the way the filters and outputs plugins are run in parallel, the beats input plugin wait for a batch of events, and the performances problem have indeed been solved in version 3.1.0 by the rewrite I quoted in the question. 

Unverified as it sounds brittle to me to start a container outside of k8s supervision, but you should be able to mount with a hostPath volume. Example variation from the documentation: 

A node in chef terminology is a machine managed with chef-client. Chef-client is available for various OS, windows, Linux, Aix, Mac etc. However the chef-server is only available for linux. 

Well, the extra bells and whistles is called process isolation, a container gets its own namespace from the host kernel, that means the program in the container can't try to read kernel memory or eat more RAM than allowed. It also isolate network stacks, so two process can listen on port 8080 for exemple, you'll have to handle the routing at host level, there's no magic here, but this allow handling the routing at one place and avoid modifying the process configuration to listen to a free port. Secondly a chroot is still read/write, any change is permanent, a docker container using will start from a clean filesystem each time you launch the container (changes are kept if you stop/start it IIRC). So well a container could be thinked about as process namespace + chroot, but the reality is a little more complex. 

There's one possibility I can think of, if when the dev are working on their own workstation, with sometimes images baked for virtual box to run on their workstation where your infrastructure doesn't use the exact same image. The dev will need, while developing a feature, need to add a JVM parameter or whatever change to the middleware early in its work and forget it. Before commiting, all unit/integration tests run on its workstation works great, as the baked image is shared, it works on every develloper system. But when going through CI, it fails because the change to the middle-ware wasn't implemented, either because the dev forgot to ask for it, or just because the team in charge of updating the base images/provisioning system didn't had the time or did forget to update the system. That's a good thing it break in CI, because it tells early before going into production that the system won't work as expected, but sometimes it becomes a hell to find the missing parameter. This last point advocate to avoid rejecting commits, and just break on CI on a feature branch, thus it won't block anyone else, and let the dev fix the problem early, when the change is needed and prevent this change to be forget in the flow. FWIW, we did exactly this here, developers had whole access to development machines and releases in Q/A were failing because a parameter change has been forget, we did move to chef to handle the configuration of the middleware (tomcat now) so each needed change to the infrastructure has to be coded somewhere and will be reproduced in all environment. 

Mainly DevOps is not a role (when used as such it's more a buzzword than a real role). DevOps is roughly an organization pattern aiming at breaking the silo between developers and sysadmins. The main goal is to build teams with devs and sysadmins (along with testers usually) responsible for a product (application) from its definition, architecture decisions up to the maintenance in run of this product. Each member of the team will be part of the decision on the whole life-cycle of the product, a dev will do some sysadmin tasks in production, and a sysadmin will participate in the design phase of the product to avoid caveats from the infrastructure perspective for example. At ideal, a sysadmin would also be part of the development team for the product, in real world sysadmin code more on the configuration around the product and monitoring solutions, but being able to voice concerns to other members of the team avoid a lot of misunderstanding on the deployment process. 

The keyword will be translated to command (builin of the shell usually), if you remove the multi-line declaration you end up with this equivalent: 

You will need a reverse proxy (usually a WAF), routing /.well-know/acme to a machine and answering the challenges (from anywhere) and doing the IP filtering before forwarding to your application all other requests. 

For the more generic computed metric question I would go with a lambda running priodically, gathering metrics from cloudwatch, doing the maths and pushing back to cloudwatch. Here is an example lambda gathering data from Cloudwatch to push to elasticsearch. With the examples on the documentation you should be able to push metrics back from the lambda. Be aware that calling cloudwatch API to gather and push metrics has a cost when you got over 1M request per month, there's other limits to be aware of to avoid breaking your budget/workflow. 

PHP with nginx is usually done using php-fpm which is a separate processus. Keeping the core idea of docker of one process (see end of answer for more details on this point) per container this makes sense to have the nginx process and php-fpm process in separate containers. As the communication between nginx and php-fpm arise through fastcgi the php-fpm container can also be on a separated host and this allows using a cluster of php-fpm containers behind nginx. After the wall of comment here's a little more background, docker documentation have paragraph about the idea that a container should have only one concern. The main idea of a linux container (lxc) is to run a process in an isolated namespace at the cpu and memory level, docker add on top of this an isolation at the filesystem level. The advantage is that the compromission of a process within this namespace won't allow to read memory of other processes and as such should prevent other compromission on the host. While talking about nginx and php-fpm, they work in pair but each has it's own concern, nginx will do the HTTP part, routing, headers validation, etc. and php-fpm will do the code interpretation and return the html part to nginx. While it's usual to have both together serving a single application that's not mandatory. Depending on context it may be easier to have a container including the whole stack for an application, on a developer workstation for exemple. But ideally for production use, try to keep the fewer interaction inside the container, having separated processes in the same container with supervisord brings its share of problem in term of zombie process and log handling (exemple story here for illustration purpose only). So finally I'll quote the docker page with some emphasis: 

I hope the script itself is commented enough. Default usage (no-params) will list delete commands of orphaned snapshots for the current account and region eu-west-1, extract: 

that's a little introduction to jq, that's probably not enough to solve your problem at all but as you didn't specify your problem I can't help more than that. 

Now if someone comes to connect to the machine and do a manual change to any of the configuration, the goal of a SCM is to keep the machine in its desired state, at next run, the change will be detected and the file changed will be put back to its desired state. This has the advantage of enforcing any change to be put in the desired state description and not be forget between environments. Usually chef or puppet are set up to run on a periodic schedule, default for Chef is 30 minutes. Now the big difference with docker is that you will do the same thing, installing you middleware, libs, forcing a templatized configuration, etc. Once in the runtime phase, if there's a breach in your app and an attacker use it to tweak your config or modifying your webserver configuration it can stay unnoticed for a while if it doesn't break the app. A SCM will revert the changes if you use it properly it will even remove unwanted sites in the directory of apache and thus will raise the complexity for an attacker. Likewise if your webserer dies, the container will stop and unless you have a system to monitor it an relaunch it you're down. A SCM will notice the service is stopped and restart it at its next run, less immediate than runit but it will correct a rotate script which didn't properly restart the service (like the default apache logrotate conf on Ubuntu which may time out and let your apache stopped) A SCM may be interesting even to create your docker containers, you know you'll have the same behavior on a centos or ubuntu machine (most community code handle using the proper package names if you go this way) and so it will remove problem of edit to package names in a dockerfile. 

Auto Scaling groups have a useful feature for this, named lifecycle hooks. Worflow taken from the documentation above: 

Basically, adding a repository, installing package, enabling and starting the service, render a config and restart the service when it changes. On ubuntu the package set the service scripts so I don't have to manage it here. The template is my default configuration using some node attributes depending on environment. See $URL$ for more details on how to use them. According to your question I would strongly sugest you to follow the tutorial paths at $URL$ to grasp the basics about chef. 

Largely inspired by the documentation the idea is to create a terraform datasource from a filter of the available resources and then use this as entry point in your ami resource: 

The root naming comes linux containers (lxc) whose goal was to isolate a process from its host system, the first goal was to avoid compromission of the process to take over the host system. Now they are used in a wider scope. In a modern 'container' definition, you'll more or less release a package for runtime which already include your application, it's underlying middle-ware if needed and all necessary libraries and be sure it will run on any compatible system. The second advantage is that it allows to use multiple applications with the same dependency at different version without having to heavily tweak its environment variables so it load the correct one. Unlikely to a VM system like virtual box virtual machine or an EC2 instance on AWS, containers are virtual only on the file system level and isolated only on the memory stack. They still share the same host and the operating system under them will arbitrate cpu ticks. A virtual machine is virtual at the hardware level, and you run an operating system within, a container is virtual at the OS level, and your run a process within. 

The software declare its dependencies and is released with each dependency at the correct version independently of others. This may create problems if two version of the same dependency are not compatible for database access for example and need a system above to ensure only compatibles version are deployed toward an environment, the way to achieve it may vary depending on the tooling and is quite hard to get it right. The software declare its dependencies, and if current available version of one dependency on the target environment doesn't match, the deploy stop and keep actual version the software. The team has then to coordinate with the dependency team to know when they'll be able to deploy. Main drawback is the delay this can bring in the release train. Here again the way to achieve it may vary depending on the tooling. Cross teams made from members of each team, to handle the coordination of versions toward release. This help getting things at the right time but doesn't enforce any control. 

There's no "silver bullet rule" which apply to everything, it's always a balance between the complexity within the container and the complexity orchestrating the containers themselves. 

First of all, removing ssh on an immutable server doesn't guarantee there'll be no change, it's more that as there should be no need to change something you reduce the attack surface by removing a remote access channel. One way to keep a sort of post-mortem is log centralisation. There's a myriad of methods to achieve it, ELK stack, Splunk, syslog... Another more crude way to keep a post mortem for an immutable server is to have a script on the shutdown process (an immutable server failing would be shutdown and a new one spin up to replace it) to gather a core dump of the program, a memory dump and send them to a remote system for analysis along with most of the logs. Main advantage of this solution is that you get back only failing system information at time of problem, allowing to gather larger informations than getting them periodically. It's hard to be more specific on how to achieve this, each distribution has some way to get things and I've no generic example. 

What I do when I've a command to execute and wish to get the log only in case of failure is as follow (prefixed by a shell comamnd like in case the initiator doesn't use a call or execute the command directly without shell): 

You, Sauron, forged this One Server, in the Darkness of Mount Doom your Datacenter in a desire to rule All applications. Hopefully the Fellowship of Devops did unite to tell you: 

I still don't get how your could launch ansible without a step part. Here is my findings, but I'm unsure it is what you're after, but that was too long for a comment. According to the documentation here you have to add a with the fingerprint of your key as seen in the UI. 

So clearly you can, I wonder why you want the VPN server to run within docker as this add a routing complexity you should avoid if you're not comfortable with the overall routing involved before. Anyway the generic scheme for a request would be: 

Truth would be that should not happen in a DevOps environment . Your "project" will probably use Agile concepts, but there's nothing in DevOps telling how a project has to be run because DevOps is product centric (more details on the blog post and with search on this term). "DevOps" is about breaking silos and have a team composed of developers, architects, ops, exploitation peoples who handle a software on its lifetime, that is antagonistic with the waterfall project approach. To quote here Jez Humble: 

Ok, let's assume you'll pull the latest version of branch prod from a git repo, here's what it would take with chef on a basic illustrative purpose: 

After some researches around the input plugin and specially this rewrite I wonder if I should use only one beat input or multiples to handle multiples entry types. I'll have events coming from roughly 500 machines, with a 20/80 windows/linux distribution. I plan to use multiples beats shipper, filebeat, metricbeat and maybe packetbeat. Is there an interest at using one input per type/os couple or would just one input and "if type=..." in the filter pipeline be enough ? 

For more advanced use cases you can have a look at the poise application cookbooks, it includes a javascript plugin for node.js application. chef is fully open source, either client or server side (out of the fancy UI) and on all OSes including windows. 

will return the length of the array flattened by the selection operator (more details on JMESPath documentation). As we filter the slection for non encrypted volumes () this should allow to demonstrate to the auditor the number is 0 not encrypted volumes. Same filter can be applied in the console, in the ec2 page, under 'Elastic Block Store' => 'Volumes', type to filter the view to non encrypted volumes only. you may add to list only attached volumes. 

So first of all, Chef is just a manner of upgrading/configuring, it will help you keep a reproducible state and keeping configuration in line but it won't do black magic and translate your jobs for you. Now for Jenkins 1 to Jenkins 2, Jenkins-ci site says Jenkins 2 is Backward Compatible. While this is true, when you update the pipeline plugin, not all plugins are compatible yet. There's a compatibility list on the pipeline plugin github page. I would recommend installing a fresh Jenkins 2 and porting your jobs from Jenkins 1 to Jenkins 2. This will spot breaking plugins in Jenkins 2, allow you to strip up not needed anymore plugins and refactor your jobs. This way you can also take advantage of defining the pipelines with a Jenkinsfile along with the code of your application. 

Your python problem is the after the commands, already append a newline, so your script does the following: