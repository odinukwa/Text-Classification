Is it possible to prevent such duplication? An attempt has been made (see first code snippet), but that does not seem to work, i.e. a docker login is required by every step. 

In summary, what is the cause of this merging conflict hassle? Is git causing the issue? This has nothing to do with git, but looks like a workflow issue. My favorite flow is the GitHub flow. When a feature has to be created, a new branch will be created from the master, when development has been completed, a PR will be created, when the review is completed the branch could be merged into master. There are other flows, but what I do not like about gitflow is that I have seen in multiple companies that developers were applying a kind of "through it over the wall" approach as they merge multiple features to the develop branch without releasing. When after a couple of months software has to be released from the master then a lot of issues could occur. I personally like the following quote from this answer. 

If it is possible to switch https to http temporarly then gatling can record the clicks. When the recording was done, https was activated again and the test could be run against https as well. In summary, recording works if the site is http and running it works against https. Note that the recorder file contains http and this need to be changed to https. 

I use docker to deploy a lot of apps since a couple of years. Today I ask myself, should I copy my app to a different location in the docker image? I used to copy the file to the root, e.g. , but I wonder whether this is secure. In order to find an answer to this question, this tutorial was read, but it does not clarify where to copy a app inside a docker image and why. 

that it is possible to check who signed the image if every collaborator creates their own key and each key is trusted individually. As notary and dtr were already mentioned, an alternative is Portus. Portus is able to restrict access to docker images, e.g. some users are only allowed to view images, while others may pull them. It is possible to revoke access as well. 

A couple of months ago I gave a presentation about the pros and cons about docker. My expectation was that the developers would start to use tool immediately after the presentation, but it took a couple of months. It turned out that a (huge) problem was required in order to get it accepted by the team members: 

When docker-compose up is run the override overwrite the overwrites the . When a new volume is added to docker-compose, the override will try to mount is as well. docker-compose.override 

One could run such a snippet by codefresh at the end of the build using a script to notify bitbucket regarding the build status of a commit. 

The Production Hardening document indicates how to harden Vault for production purposes, but what if Vault is run on for example Digital Ocean and the platform is unavailable or the disk gets corrupted? 

Discussion The assumption was made that the keys would have been stored in ~/.ssh, but this seems to be empty. 

One could also try (I did not try it yet) whether the works in combination with script sections: $URL$ Last remark: I would avoid creating such scripts (the ones you included in the question) as most CI tools contain condition checks. 

One could use conditional builds $URL$ If code is merged into master one could decide to deploy code to production, but I personally prefer a human intervention by a Product Owner. 

Background: There are multiple Bitbucket Repository Issues (BRI). If an issue needs to be edited then an edit button could be clicked and several items, e.g. title and priority could be changed. This works well when some issues need to be changed, but when more than 250 issues need to be changed this approach is tedious. Aim: to update all BRI at once. Method: The current method is to update the issues manually, but when multiple issues need to be updated then it is time consuming. Another approach could be using the API. Consulting the API: did not return any issues. In order to update all the issue the following was tried: 

When a container cluster was created on GCP in the morning it took less than 10 minutes until it was created. Now, after more than half an hour the cluster is still not created. It turned out that the resources in a certain zone were insufficient. How to check whether there are enough resources in a GCP zone before creating for example a kubernetes cluster? 

When one follows a Hashicorp's Vault demo, one has to issue at a certain moment that generates a and : 

Everytime docker compose was run again, three anonymous volumes were created by the docker image. Let's replace influxdb with nginx and revert the grafana to nginx: 

There is a comprehensive document about k8s' DNS. According to this document one could validate whether the DNS is working by running: busybox.yaml 

git repositories, like github, bitbucket and gitlab check whether an email account already exists if a new user is created to clarify that the user is a bot and not a human 

What is Git-LFS? Git Large File Storage (LFS) prevents that large files need to be stored in git itself. $URL$ 

Continuous delivery: test the application automatically when the code changes Continuous deployment: deploy the application to production automatically when the code changes 

Making a distinction between Ops, Dev and DevOps promotes isolation and enforces a "throw it over the wall" mindset. To increase cooperation between teams one should put everything in a repository that is required to build and deploy the project. Having said that, the answer to the question: 

Table 1: Ratio of build speed between different Operating System and hardware. Do other people from other organizations experience this as well? 

First impression is that you should either choose one repository or check the API of both products to investigate whether the tools could be integrated. From a DevOps perspective, repetitive processes like this that are done manually over and over again should be automated. 

Less time to be technical. Most of the chiefs told me that they only can spend a maximum of 10% of their time to be technical. 

As it is unclear what command and subcommands you used to run the docker container it could be possible that you run the container without using the subcommand. In this case could solve the issue. 

and restarting the docker systemctl service, but how to do that on CircleCI? It seems impossible as the build output indicates that a connection is made to a remote docker that reside on the CircleCI systems. 

More example could be found in the supermarket. In order to write chef books one could investigate how some recipes are structured like mysql. 

Another analogy was found here $URL$ I think that this on is also applicable as there are still developers that continue throwing things over the wall. 

The last couple of years I am/was part of couple of DevOps teams that are/were creating microservices. Some of the projects failed miserably. Reasons: 

If one adds a component from the catalog in Rancher and one would like to add another component than the list seems to be reduced. Why do some components vanish from the catalog list if one component has been installed in Rancher? 

What are best practices to read the output and check whether it is acceptable, i.e. if a certain value is larger and smaller than X? Should one create a custom script or does Gatling itself provide such a functionality? 

I agree with @Tensibai and rather using a tool I would suggest to talk with your team members about the tools they use rather than sniffing their system. If one would use a tool to spy other systems then that is not aligned with DevOps. DevOps is about getting rid of silos. Using a tool for "secretly" monitoring colleague system is a contradiction. Imagine that the developers would know that such a tool is used, then silos will be created again and that is not DevOps. DevOps is about building bridges between team members and departments, e.g. Ops, Dev and QA. 

Lets investigate whether that would be possible. According to $URL$ one could update an existing ingress yaml and subsequently run . From a microservice perspective, one could store a snippet in every git repo, e.g.: 

I have experienced issues with other plugins as well. I was enabling the build status notification plugin and the build status was not reported. I was checking the log and it turned out that the repository should have been cloned. I would suggest to inspect the Jenkins log when a Pull Request is created in Bitbucket. 

systemd will run inside the docker container. It is also possible to run multiple services using systemd. According to the creator of this blog it would be possible to run both mariadb and http inside the same container. Based on this blog and other articles I read my conclusion is that it is technically possible to run systemd inside a docker container, but I would recommend to avoid running systemd inside a container. First of all, if systemd is able to run inside a container then this means that it is possible to run multiple services like the author did. From a docker perspective this is not recommended as docker is meant to scale horizontally, i.e. if the load increases of http then addtional docker images should be started. Second, if such a systemd container will be deployed on an orchestration platform like docker swarm. Will that work? I have some doubts whether that will work. Third, Running systemd by mounting the cgroup in a privileged container does not look very secure. In conclusion, although you indicated that the script requires systemd, either rewrite the code or use something else. Running systemd inside a docker container should be avoided in my opinion. 

One could leave the package.json in every feature directory and let the CI read the package.json when building the app. One could define the versions of dependencies in the package.json to get control about the app. If the app works with version A.B.C of a certain library then one could define this version to prevent that an update will break the app. 

To me it is not completely clear what you are trying to achieve and why the attempts are not a solution for the problem. If I would need to solve this issue I would like @Tensibai and other Q&As indicated, do a docker pull first on a system with internet connectivity, save the docker image, copy it to the machine without internet connectivity, load the image and run it. Demonstration There are no images on system A: 

If is defined as a volume then the content that reside in the container's will be stored in the folder on the host. It could be possible that it is not possible to mount the as SElinux or app armor is preventing this. $URL$ 

If less then 3 nodes will be used then it is basically more safe to run 1 master, but if one goes down there will be an issue as well. So it is better to run at least three masters. 

I do not understand why a docker container should perform the checkout as this could be done using Jenkins. One of the advantages of docker is that it is immutable, but the way you use it right now is mutable. 

In order to copy files recursively, all files could be put in a directory and when this folder is copied to the pod, all files were copied: 

Currently, certbot and nginx are used to create a trusted webpage. Recently, an attempt was made to move the images to a kubernetes cluster on google cloud platform. A guide was found to configure an SSL loadbalancer. It was tried and it shows multiple textboxes to insert a key, cert and chain. An attempt was made to see whether Google offers wildcard certs as well, but no information was found. Certbot itself announced last year that they will support wildcard certs, but that does not seem to be the case as well. First impression is that using certbot in the google loadbalancer will not be possible or will be cumbersome as the certs have to be renewed every three months. When one Googles: then a lot of results are shown. The questions are which of these providers can be considered as safe, what are the costs and what providers do you use and why?