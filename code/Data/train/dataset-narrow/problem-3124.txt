we store the result of in the variable , which is in this example because our target output (), is equal to the actual output () of the neuron, we return and consider the neuron output correct. 

If "variables" refers to training examples: You can use Stochastic Gradient Descent (SGD) where each iteration uses one training example. Or you could use Mini-Batch Gradient Descent where each iteration uses a partition of the training set. SGD is Mini-Batch Gradient Descent where the partition size is one training example. 

Moreover, we could generate another hypothetical dataset with a bad training/testing split where our set ranges are identical. 

example where the neuron is incorrect: Let's say we have the following parameters: input vector () = target output () = The method executes as follows: 

When training accuracy increases while validation accuracy remains constant or decreases, then the model is most likely overfitting, or it may be saturated. You can try the following methods to increase accuracy: 

Now we are factoring the previous weight delta in our weight update equation. In this form, it is easier to see that the learning rate and momentum are effectively independent terms. However, without a learning rate, our weight delta would still be zero. 

It mostly depends on the amount of data you have available for training. However, unless you have an extremely limited dataset and you are unable to generate synthetic data, CNNs are currently the best approach for solving your task. With sufficient data, they are very robust. To improve their performance, you can augment your dataset by randomly rotating, flipping, brightening, contrasting, scaling, cropping, blurring, and adding noise to the images throughout training. It also depends on your speed requirements and computational resources. Traditional methods have the potential to be quite fast, but if you only care about accuracy, a deep network would probably perform better. Note that some CNNs are also quite fast, such as MobileNets. There are a few reasons you might go "old school" and use template matching and more traditional approaches as suggested by @TheDjentleman: 

I recommend going through this high level introduction to neural networks. It is important to gain some intuition about the models you are using and the options you have available before tackling your problem. 

Using a multi-class network would probably be the most efficient approach. Training two networks (as suggested in the comments: one network to detect the presence of a person and one to detect the state of the knee) is unnecessary and overly complex. You can train the network on three classes: bent knee, straight knee, absent knee. For the absent knee, you do not need to train on all possible images, just images you expect to see. For instance, these negative data could be the background of your intended images; take a picture with the knee and one without. If speed is a major requirement, the model architecture becomes critical. I suggest exploring MobileNets as they are quite fast and do not sacrifice too much accuracy. 

Your question is quite broad so it is not easy to give a direct answer. However, a simple approach and a good introduction to machine learning would be to use a Multilayer Perceptron (MLP). This is one of the most basic and fundamental neural network architectures. You will also want to read about Backpropagation, which is the standard method of training such algorithms. Here are some code examples/libraries (in no particular order) that can get you started: 

Should every learner have an equal representation in the final vote? If you are using the arithmetic mean, the weak learners are treated equally. This is generally desired as you do not know which learner is closer to the correct answer. Using the geometric or harmonic mean would imply that you want to "weight" the set of weak learner outputs according to their respective additive structures. As a side-note, you can (and probably should) use machine learning algorithms, such as a MLP or SVM, to combine the outputs of your ensemble, which would probably yield superior results to simple averaging. 

If "variables" refers to features: You should use dimensionality reduction to reduce your number of features. For instance, you can use Principal Component Analysis (PCA) to reduce your feature vector size while maintaining high variance. This would also help your models train significantly faster. 

This is a very primitive approach. I am by no means an expert on self-driving cars, but after some Googling, deep drive appears to be the premier simulated self-driving car development platform. If you are going to develop the GTA network, I suggest checking out it out. 

Although in some situations recall may be more important than precision (or vice versa), you need both to get a more interpretable assessment. For instance, as noted by @SmallChess, in the medical community, a false negative is usually more disastrous than a false positive for preliminary diagnoses. Therefore, one might consider recall to be a more important measurement. However, you could have 100% recall yet have a useless model: if your model always outputs a positive prediction, it would have 100% recall but be completely uninformative. This is why we look at multiple metrics: 

As for the variable length feature vector, it is difficult to give an exact answer without knowing more details about your problem, but here are general possible solutions: 

Normalizing the input images is widely used and often considered an essential preprocessing step. However, making an absolute statement such as, "normalization is always beneficial" or "normalization is never beneficial" would be misguided. In the scenario you presented, normalizing the data would not benefit the CNN, but the network would still be able to learn the data well. However, this is a very primitive situation, and most real-world problems have significantly more noise and variance for which to account. In general, normalizing the input image will improve SGD convergence time. 

We can answer this overarching question by exploring a couple sub-questions: What are the properties of popular averaging formulae? Geometric: $ \space $ $ \space $ $ \begin{equation} \bigg(\displaystyle \prod^N_{i=1} x_{i}\bigg)^{\frac{1}{N}} \end{equation} $ We can see that any set that contains a zero has a geometric mean of zero. Also, processing negative numbers is awkward as they can become imaginary. Even if negative numbers do not result in an imaginary mean (e.g. the set has an odd number of elements), the result is often not what we expect an "average" to be. For instance, given a set, let's take some averages: $ \{-1,2,4\} $ Arithmetic mean: $ 1.\overline{6} $ Geometric mean: $ -2 $ The geometric mean is clearly not what we would expect for an "average". This is because the geometric mean is primarily used for understanding the multiplicative differences in a set, which is useful for averaging data on differing scales. Let's say we only consider the absolute value of the elements. Then, given a set below, we still get undesirable results: $ \{2^0,2^1,...2^{10}\} $ Arithmetic mean: $ 186.\overline{09} $ Geometric mean: $ 32 $ From this example we can see that where the arithmetic mean is linear, the geometric mean is logarithmic. 

Generate a feature set per image Train a model on your feature set to output a score for a movie. The score is a number in an arbitrary domain. Repeat step 2 for all of your candidate movies to create a score vector. Feed the score vector into the softmax function to squash the scores into the domain [0,1] and make them add to 1 

Implementing the NEAT algorithm for playing video games is quite realistic, especially considering that it is not novel, so there are extensive resources and tutorials to help you. In fact, there are publications that will walk you through exactly what you want to do, such as this paper on using the NEAT algorithm for playing Pong. Of course designing a self-driving car would be significantly more advanced than playing the 2D games you mentioned, but it appears that much of the work for developing AI in GTA V exists. The person you mentioned, Sentdex, has a github page for developing GTA V code. However, note that: 

As @Emre noted in the comments, you will want to use the softmax function. After acquiring a set of scores per film, the function will squash the scores into the range [0,1] and the scores will add up to 1. 

We first center our data by subtracting the mean of the batch. We also divide by the standard deviation, so our formula becomes: $ z = \frac{x - \mu}{\sigma} $ where: $ x $ is the pixel value $ \mu $ is the arithmetic mean of the channel distribution $ \sigma $ is the standard deviation of the channel, calculated as such: $ \sigma = \sqrt{\frac{\sum^N_{i=1} |x_i-\mu|^2}{N}} $ By standardizing the data in this fashion, we obtain a mean ($\mu$) = 0 and a standard deviation ($\sigma$) = 1. This allows SGD to converge faster. It is easier to see how feature scaling can benefit simpler models such as SVMs, but CNNs benefit similarly. (Mini-)Batch normalization makes every pixel have a similar distribution, which helps SGD converge faster. This is because networks generally have shared hyperparameters. For instance, the learning rate is a multiplier for all weights in a network, rather than having a learning rate for every weight; therefore, we want a single learning rate to have proportional influence in all regions. Otherwise, the learning rate could make some weights over-correct and other weights under-correct. 

A CNN automatically extracts features, so hand-crafting features has become unnecessary for most applications. It learns what features to extract via backpropagation. You can introduce handcrafted features in a few ways to boost the performance of a CNN: 

To answer the first question about why we need the learning rate even if we have momentum, let's consider an example in which we are not using the momentum term. The weight update is therefore: $ \Delta w_{ij} = \frac{\partial E}{\partial w_{ij}} \cdot l $ where: 

you are standardizing your features so that every dimension is on the same scale from [-1,1] This is common practice as it helps many models converge faster. It would not prevent your models from generalizing the data, so here are some other possible causes for your problem: 

If none of those criteria fit your situation, go with a CNN. Lastly, you do not necessarily need a generic binary classification CNN. You can use an object detection network or even a semantic segmentation network and just ignore the location data if you do not need it. 

This article does a great job of explaining CNN preprocessing. I'll try to highlight the key points (the following images are pulled from it) Let's consider a face recognition challenge: 

These techniques are not mutually exclusive; combining dropout with weight decay has become pretty standard for deep learning. However, where weight decay applies a linear penalty, dropout can cause the penalty to grow exponentially. This property of dropout can lead to hypothetical failures as proposed and proven in section 4.2 of this paper. In general, research has consistently shown the benefits of dropout (with and without weight decay) for training deep networks. A practical scenario in which weight decay is exclusively preferred over dropout would be quite the anomaly. 

If you were to directly convert categorical values to numerical values, that would imply that one color is greater than another, so we need to use a boolean array instead. 

(red=training, blue=testing) Here we can see that despite the large range discrepancy, our sets are relatively balanced. We can observe a problematic training/testing split in this example where we shift the training mean from 11.625 to 100: 

An easier way of thinking about the averages is like this: Where arithmetic is linear, geometric is logarithmic, and harmonic is reciprocal. 

first passes the input vector through our neuron using the method. The result of is stored in a variable named . The variable is either or . Depending on the pattern that we provide we want the target output () to be either or . If is equal to ( or ), then we return Otherwise, the neuron outputted a value we do not want ( or ), so we update the weights accordingly, then return 

For reference, the softmax function is defined: $ \sigma(z_i) = \frac{e^{z_i}}{\displaystyle\sum^N_{n=1}e^{z_n}} $ 

TL;DR: Do not evaluate the set distribution by range; instead use mean/median or histogram/normal curve. Also normalize your data. 

Here is how the code appears to break down: method parameters: The method takes an input vector called along with the target output called and a weight update coefficient (learning rate) called . 

And an example: Our feature set will include the following: critic/audience ratings, revenue, cost, total tickets sold, etc. Let's say you have historical data of the Oscars that includes the feature sets and a score that we will use for the ground truth metric. You define the score. For instance, if a movie was not even nominated for the Oscar award, it might have a score of zero, whereas an Oscar winning movie might have a score of 1. A movie that was nominated but did not receive many votes might have a score of 0.50. You train a model on your historical data so that, given a movie's feature set, it will output a score, similar to your training set. Now you are considering three movies for prediction: Avengers Infinity War, Deadpool 2, and Venom. You acquire their feature sets (the same categories that you used for training: critic/audience ratings, revenue, etc.) Then you pass each feature set through your model and acquire a vector of scores: \begin{array}{|l|c|} \hline Movie & Score\\ \hline Avengers\ Infinity\ War & 0.98\\ Deadpool\ 2 & 0.82\\ Venom & 0.24\\ \hline \end{array} We can interpret the score results as a probability by using the softmax function: The denominator of the function is given: $ \displaystyle\sum^N_{n=1} e^{z_n} = e^{0.98} + e^{0.82} + e^{0.24} \ \ \ where: z=\{0.98,0.82,0.24\} $ We calculate the softmax of a given score $ x $ as such: $ \Large\frac{e^x}{e^{0.98} + e^{0.82} + e^{0.24}} $ Therefore the softmax scores are: \begin{array}{|l|c|c|} \hline Movie & Score & Softmax\\ \hline Avengers\ Infinity\ War & 0.98 & 0.42932\\ Deadpool\ 2 & 0.82 & 0.36584\\ Venom & 0.24 & 0.20484\\ \hline \end{array} and we can see that: $ 0.42932 + 0.36584 + 0.20484 = 1 $