It seems that our first issue is to model the graph before visualizing it. I would recommend Neo4j and its Cypher query language. For the data modeling, states can be the nodes with a population property (or even population for each year), relationship could be the directed migration with the number of refugees and a year property. Now with such a graph at hand you can easily retrieve the total number of refugees per year by using the year property from the nodes or the edges. 

It seems that Embedding vector is the best solution here. However, you may consider a variant of the one-hot encoding called 'one-hot hashing trick". In this variant, when the number of unique words is too large to be assigned a unique index in a dictionary, one may hash words of into vector of fixed size. One advantage in your use case is that you may perform online encoding. If you have not encountered every vocabulary words yet, you may still assign a hash. Then later, new words may be added to the vocabulary. One pitfall though is "hash collisions". Indeed there is a probability that two different words end up with the same hash. I found an example of this hashing trick in the excellent "Deep learning with Python" from Fran√ßois Chollet. 

You can observe that common words between the two sentences are given the same index: in position 0 of the two sentences (positions 0 and 6 of dimension 2). 

I would still stick with using a CNN for that specific application. Think about CNNs being used to detect various types of cancer in noisy images with an insane precision (Stanford, Google). This type of input is actually very similar to yours with cancer cells hiding in a cluster of healthy ones. And yet the models are performing as good as cancer experts in some cases. CNN have shown to work best when trained with a HUGE amount of data. If possible try to provide more training data with a decent class distribution (roughly the same number of positive and negative examples). Moreover, apart from tuning hyperparameters you could also experiment with different CNN architectures. You will fin plenty of inspiration in the litterature. 

You can try two different approaches: 1) Kalman filter, the method is battle-tested and has proven useful in many areas. Resources: 

Here is a beginner tutorial to do just that with Tensorflow: $URL$ If you need extra data to train your model, take a look at the MNIST dataset: $URL$ 

The approach you use to do dimensions reduction is agnostic to the method you use for classification. You can use PCA to preprocess your data before to train any type of classifier, including artificial neural networks if that's what you want to use. 

Once you installed the GPU version of Tensorflow, you don't have anything to do in Keras. As written in the Keras documentation, "If you are running on the TensorFlow backend, your code will automatically run on GPU if any available GPU is detected." And if you want to check that the GPU is correctly detected, start your script with: 

Elasticsearch is the right tool to use if you don't want to code this yourself. Indeed, you need an indexing algorithm that is able to efficiently retrieve pieces of texts in a big database, and SQL isn't particularly good at it. Moreover, Elasticsearch is quite user friendly, so it won't be an overkill to actually install it and use it. You might discover in the process that finding most similar articles isn't that easy and that Elasticsearch is of a great. Here is the documentation for a Python client: 

This is how you should get your topics and corresponding words. You first get the NMF transformation of your new input and sort its components. The biggest components tell you which features to look at in nmf.components_, which links topics to words (your tfidf features). Finally, you print the k most important words of the topics. Topics and words scores are just calculated by dividing their value by the sum of all values. 

As Sean Owen said, it's a wide problem so I can't be generic on this one. I'll just give you an example on how you could tackle this: Let say you sell some products on a website. If you have 100 different products, you can represent each sale by a 100 dimensional vector, where each component is the count of ordered product. Example: sale1 = (2,0,1,0,...,0) means 2 item1 and 1 item3 sold. We use the same technique with words, and it is called bag of words. If you stop here, you only can perform statistical counts on individual product sales. So let's use matrix factorization. If you sold products to 1000 people, then you can concat all your sales in a matrix of dimension 1000 X 100 where each row represent one sale and each column represent one product. Using decomposition algorithm such as SVD, you can find out singular values in your data that will show you latent factors in your sales (basically trends or connections between your products). 

2) Recurrent Neural Networks, the LSTM and GRU architectures are particularly interesting for time series predictions. Resources: 

RNNs are not designed to do language modeling exclusively, they are designed to process time series data, and language happened to be representable as time series. There is plenty of papers demonstrating how to use RNNs to do classification and regression on time series (awesome list of papers). One-hot encoding is often used in cases where the input is discrete and not a number that can be directly fed into a model. However, one-hot encoding is actually not always the norm for language-modeling, some research actually map each character (or each word depending on how one wants to model the problem) to a unique numerical identifier (e.g. with $id\in \left\{ 0\ldots n \right\}$; with $n$ the size of the vocabulary) and the model map that identifier to a vector representation. This is particularly useful when the size of the vocabulary is huge and you want to avoid having to deal with big one-encoded vectors. Take a look at word2vec and word2vec Tensorflow for more details about this. In your case, you want to process sensor data. There is no need for one-hot encoded because your data are continuous and numerical. In other words, you can input the recorded EEG data directly; although it's usually better to clean them up and normalize them beforehand. There is actually plenty of papers about EEG data processing with RNNs. Regardless of the type of data, the idea to fed them into a RNN remains the same: provide a data sample $x$ for each timestep $t$. For EEG data recorded for 5 timesteps with 1 electrode $a$, you would have a 1-dimensional vector with one sample per timestep: $a = [0.12, 0.44, 0.134, 0.39, 0.23]$ $inputvector = [0.12, 0.44, 0.134, 0.39, 0.23]$ For EEG data recorded for 5 timesteps with 3 electrodes $a, b, c$, you would have a 3-dimensional vector with one sample per timestep: $a = [0.12, 0.44, 0.134, 0.39, 0.23]$ $b = [0.43, 0.92, 0.3, 0.37, 0.4]$ $c = [0.13, 0.1, 0.4, 0.21, 0.14]$ $inputvector = [[0.12, 0.43, 0.13], [0.44, 0.92, 0.1], ...]$ I highly recommend you to read some of the literature I linked above. 

Anonymized dumps of the stack exchange data are available here. Do you know projects or article that have been using these data (for social network analysis or information retrieval) ? My little research on the subject on Google Scholar & co seems to indicate that this dataset has been very seldom used. 

Isn't this solution good enough ? You can protect the access with ssh, and the hosted files could be the git repository you want, with different linux (or whatever) user access. You'll need your own server. 

This is a basic property of convolutional networks. First layers identify simple features and as you go deeper in the CNN each layer uses the features of the previous layer to build more complex ones. I would say the state-of-the-art article about this is Visualizing and understanding Convolutional Networks by Matthew D. Zeiler and Rob Fergus. You can download it here 

Word2vec and GloVe are the two most known words embedding methods. Many works pointed that these two models are actually very close to each other and that under some assumptions, they perform a matrix factorization of the ppmi of the co-occurrences of the words in the corpus. Still, I can't understand why we actually need two matrices (and not one) for these models. Couldn't we use the same one for U and V ? Is it a problem with the gradient descent or is there another reason ? Someone told me it might be because the embeddings u and v of one word should be far enough to represent the fact that a word rarely appears in its own context. But it is not clear to me.