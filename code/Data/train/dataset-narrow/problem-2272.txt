Well, is it only one missing event you want to detect? It you need to be able to detect the presence or absence of all 50 events, this means distinguishing between $2^{50}$ possibilities which obviously cannot be done with 32 bits. However if you wish to just detect a single missing event, here is a way that will work. Assume you have m events. 

Basically, the computation is reversible, so you are not actually using energy. However, the rate at which a computation progresses is generally proportional to the energy gap within the system. In the case of adiabatic quantum computation, the rate at which you can transition between ground states of the initial and final system is determined by the energy gap between the ground state and the low lying excited levels of the system. Thus the energy gap in the Hamiltonian (the total energy operator) is what determines the rate of computation, and so scaling up the energy levels involved allows the computation to be performed more quickly, even though the process does not actually consume the energy. You might be interested in looking at the Margolus-Levitin theorem, which gives limits for the rate of computation by imposing a lower bound on the time taken for a system to transition to an orthogonal state in terms of the energy gap present in the system. This applies more generally than simply to adiabatic quantum computing. In general, you tend to get a linear trade-off between energy scale and time. One way to see where this comes from is simply to think about the case of a constant Hamiltonian. In that case, the time evolution operator is $e^{-iHt}$. Clearly multiplying $H$ by some constant $k$ and dividing $t$ by $k$ leads to the same operator, and hence the same evolution of the system. You can do this more generally, with a potentially time dependent Hamiltonian by looking at the Schroedinger equation: $i \hbar \frac{\partial}{\partial t} \mid \Psi \rangle = H \mid \Psi \rangle$. Replacing $H$ with $kH$ and $t$ with $\frac{t}{k}$ we obtain the same equation. 

There are certain classes of quantum gates which can be simulated efficiently with a classical computer. If no entanglement is present, a computation with pure states (i.e. not random states) can be simulated efficiently. Classical gates reversible gates are a subset of quantum gates, and so can obviously be simulated efficiently. These two examples are pretty trivial, however there are a number of non-trivial gate sets known. 

Well if $L=\oplus L$ then simulation of stabilizer circuits is in $L$, since Aaronson and Gottesman (Physical Review A 70, 052328) proved such simulation is complete for $\oplus L$ under log space reductions, or more weakly that simulating CNOT networks is in $L$. Equivalently, if the simulation of such circuits is in $L$ then $L = \oplus L$. Personally, I would find this surprising, but not in the fall off my chair way I would find $P=NP$ surprising. 

I think part of the problem you are having finding a solution is that you are using the wrong name for this concept. While it is indeed closely related to the idea of a zero knowledge proof, this type of protocol, accepting input from multiple parties, is known as either Secure Function Evaluation or Secure Multi-Party Computation. Let's first simplify the game and assume it is played on a lattice, so that the points are discrete. It seems to me that you simply wish to compute $|J \cup K|$. Trivially, this is equal to $|J \cup K| = |J| + |K| - |J \cap K|$. Now, I assume you do not mind each party learning the number of locations the other party holds. In this case, $|J|$ and $|K|$ are public. In this case all that is required to determine $|J \cup K|$ is $|J \cap K|$. Now, if we think of the inputs of Player 1 (henceforth Alice) and Player 2 (henceforth Bob) as being a vector of length equal to the total number of locations possible, with zeros everywhere except for the location that player holds (which is set to 1). In this case $|J \cap K|$ is simply the inner product of their two vectors. Fortunately there are SFE protocols for exactly this inner product computation (see here for example). So Alice and Bob simply calculate $|J \cup K|$. Now, what about the case you give, where the locations are not discrete, but are rather parameterised by a pair of real numbers which have some range associated with them? In this case it is not sufficient to determine whether two points exactly coincide, but we need to also determine which points are close enough to one another to overlap in the area covered. This can be done fairly easily: The first step is to discretize the set of possible locations, hence imposing some fixed resolution on the problem (in practice you only need the range covered by each location to extend a few pixels in each direction). Once you have done this, the protocol would proceed as follows: Bob prepares a register equal in size to the total number of possible discretized locations. Everywhere is zero, unless that location is within distance $D=2E$ of one of his locations in which case it is set to 1. Alice prepares her vector as before, with a 1 at only her discretized locations. Then Alice and Bob run the secure dot product protocol. The number returned by this is simply the number of locations held by Alice such that the area covered by that point overlaps with the area covered by some location held by Bob. (Adjusting the $D$ parameter can be done to set some minimum overlap if desired). In this way, Alice and Bob can count the number of points where the overlap exceeds some threshold, and the total number of distinct locations they would hold between them. 

Disclaimer: I can only vouch for my research fields, namely formal methods, semantics and programming language theory. The situation is possibly different in other parts of the discipline. It seems that TCS has become rather conference-oriented. Researchers aim at publishing in the next conference. Sometimes a journal version appears. Sometimes it doesn't. In other disciplines (biology, mathematics, and most others I guess) this is unheard of. The effort put into writing the conference papers is a lot lesser, but in turn, the conference papers count a lot less. The "real deal" is the journal publication. Arguing whether this situation is good or bad could result in a flame war, and doesn't have a precise answer. Instead, let's try a more factual question: How did we become so conference-oriented? How did conference papers gain so much weight? 

Also, sometimes properties don't commute on the nose, and the structure under consideration is of higher-dimensional (i.e., 2-categorical). As to your question 3: you can define a category without mentioning objects at all (though it's conceptually clearer if we do mention the objects). 

I want to strengthen Alexey's answer, and claim that the reason is that the first definition suffers from technical difficulties, and not just that the second (standard) way is more natural. Alexy's point is that the first approach, i.e.: $M \models \forall x . \phi \iff$ for all $d \in M$: $M \models \phi[x\mapsto d]$ mixes syntax and semantics. For example, let's take Alexey's example: ${(0,\infty)} \models x > 2$ Then in order to show that, one of the things we have to show is: $(0,\infty) \models \pi > 2$ The entity $\pi > 2$ is not a formula, unless our language includes the symbol $\pi$, that is interpreted in the model $M$ as the mathematical constant $\pi \approx 3.141\ldots$. A more extreme case would be to show that $M\models\sqrt[15]{15,000,000} > 2$, and again, the right hand side is a valid formula only if our language contains a binary radical symbol $\sqrt{}$, that is interpreted as the radical, and number constants $15$ and $15,000,000$. To ram the point home, consider what happens when the model we present has a more complicated structure. For example, instead of taking real numbers, take Dedekind cuts (a particular implementation of the real numbers). Then the elements of your model are not just "numbers". They are pairs of sets of rational numbers $(A,B)$ that form a Dedkind cut. Now, look at the object $({q \in \mathbb Q | q < 0 \vee q^2 < 5}, {q \in \mathbb Q | 0 \leq q \wedge q^2 > 5}) > 2$" (which is what we get when we "substitute" the Dedekind cut describing $\sqrt{5}$ in the formula $x > 2$. What is this object? It's not a formula --- it has sets, and pairs and who knows what in it. It's potentially infinite. So in order for this approach to work well, you need to extend your notion of "formula" to include such mixed entities of semantic and syntactic objects. Then you need to define operations such as substitutions on them. But now substitutions would no longer be syntactic functions: $[ x \mapsto t]: Terms \to Terms$. They would be operations on very very large collections of these generalised, semantically mixed terms. It's possible you will be able to overcome these technicalities, but I guess you will have to work very hard. The standard approach keeps the distinction between syntax and semantics. What we change is the valuation, a semantic entity, and keep formulae syntactic. 

Take any distribution $D$ on $\{0,1\}^n$. Sample $k(n)$ points $x_1, \cdots, x_{k(n)}$ independently from $D$ and let $\tilde D$ be the uniform distribution that gives a random $x_i$. Then, if $k(n)$ is super-polynomially large e.g. $k=n^{\log n}$, you cannot distinguish $D$ and $\tilde D$ using only $\mathrm{poly}(n)$ samples. Hence $D$ is computationally indistinguishable from a distribution $\tilde D$ with entropy $\mathrm{poly}\log(n)$. 

Suppose we try to solve this using the same binary search strategy as before (but with noisy answers). It is reasonably easy to show that this achieves $\mathbb{E}[v_{i_*}] \geq \max_i v_i - O(\log n)$ and that this is tight in the worst case. We can reduce the error to the desired $1$ by repeating each query $O(\log^2 n)$ times and using the average (which drives down the variance). This gives an algorithm using $O(\log^3 n)$ queries. Is there a better algorithm? I conjecture that $O(\log^2 n)$ queries suffice. And I believe I can prove a $\Omega(\log^2 n)$ lower bound. Also, the problem becomes easy -- i.e. $\tilde{O}(\log n)$ queries via binary search -- under the promise that there is a $\Omega(1)$ gap between the largest value and the second-largest value. If it helps, you can assume all the values are between $0$ and $O(\log n)$. 

I have the following conjecture about bounded functions on the hypercube. Any help resolving it (proof, counterexample, some ideas) is much appreciated. 

Here $f(x) = \sum_{S \subseteq [n]} \widehat{f}(S) \prod_{i \in S} x_i$ for all $x \in \{ \pm 1\}^n$ defines the usual Fourier transform. It's clear that $$\mathrm{Var}\left[ f \right] = \sum_{S \ne \emptyset} \widehat{f}(S)^2 \leq \sum_S p^{2|S|+2} = p^2 (1+p^2)^n,$$ but, unless $p \leq \tilde{O}(1/\sqrt{n})$, this is worse than the trivial $\mathrm{Var}\left[ f \right] \leq 1$. I'm interested in $p = (1/\log n)^{O(1)}$. I don't know exactly what the right bound on the variance of $f$ should be. My motivation comes from the area of pseudorandomness, but this seems to be an interesting question about discrete Fourier analysis in its own right. In particular, this seems related to the majority is stablest theorem: Suppose $f = T_p(g)$ for some balanced $g : \{\pm 1\}^n \to \{\pm 1\}$ with $\mathrm{Inf}_i(g) \leq p^2$ for all $i$, where $T_p$ is the noise operator. Then $f$ satisfies the hypotheses of the conjecture and the variance of $f$ is the noise stability of $g$, which is bounded by $O(p)$ as a consequence of majority is stablest. 

We're interested in additive approximations to #3SAT. i.e. given a 3CNF $\phi$ on $n$ variables count the number of satisfying assignments (call this $a$) up to additive error $k$. Here are some basic results for this: Case 1: $k=2^{n-1}-\mathrm{poly}(n)$ Here there is a deterministic poly-time algorithm: Let $m=2^n-2k = \mathrm{poly}(n)$. Now evaluate $\phi$ on $m$ arbitrary inputs (e.g. the lexicographically first $m$ inputs). Suppose $\ell$ of these inputs satisfy $\phi$. Then we know $a \geq \ell$ as there are at least $\ell$ satisfying assignments and $a \leq 2^n - (m-\ell)$ as there are at least $m-\ell$ unsatisfying assignments. The length of this interval is $2^n - (m-\ell) - \ell = 2k$. So if we output the midpoint $2^{n-1} -m/2 + \ell$ this is within $k$ of the correct answer, as required. Case 2: $k=2^n/\mathrm{poly}(n)$ Here we have a randomized poly-time algorithm: Evaluate $\phi$ at $m$ random points $X_1, \cdots, X_m \in \{0,1\}^n$. Let $\alpha = \frac{1}{m} \sum_{i=1}^m \phi(X_i)$ and $\varepsilon = k/2^n$. We output $2^n \cdot \alpha$. For this to have error at most $k$ we need $$k \geq |2^n \alpha - a| = 2^n |\alpha - a/2^n|,$$ which is equivalent to $|\alpha - a/2^n| \leq \varepsilon.$ By a Chernoff bound, $$\mathbb{P}[|\alpha - a/2^n| > \varepsilon] \leq 2^{-\Omega(m \varepsilon^2)},$$ as $\mathbb{E}[\phi(X_i)]=\mathbb{E}[\alpha]=a/2^n$. This implies that, if we choose $m=O(1/\varepsilon^2) = \mathrm{poly}(n)$ (and ensure $m$ is a power of $2$), then with probability at least $0.99$, the error is at most $k$. Case 3: $k=2^{cn + o(n)}$ for $c < 1$ In this case the problem is #P-hard: We will do a reduction from #3SAT. Take a 3CNF $\psi$ on $m$ variables. Pick $n \geq m$ such that $k < 2^{n-m-1}$ -- this requires $n = O(m/(1-c))$. Let $\phi=\psi$ except $\phi$ is now on $n$ variables, rather than $m$. If $\psi$ has $b$ satisfying assignments, then $\phi$ has $b \cdot 2^{n-m}$ satisfying assignments, as the $n-m$ "free" variables can take any value in a satisfying assignment. Now suppose we have $\hat{a}$ such that $|\hat{a}-a| \leq k$ -- that is $\hat{a}$ is an approximation to the number of satisfying assignments of $\phi$ with additive error $k$. Then $$|b-\hat{a}/2^{n-m}| = \left| \frac{a - \hat{a}}{2^{n-m}}\right| \leq \frac{k}{2^{n-m}} < 1/2.$$ Since $b$ is an integer, this means we can determine the exact value of $b$ from $\hat{a}$. Algorithmically determining the exact value of $b$ entails solving the #P-complete problem #3SAT. This means that it is #P-hard to compute $\hat{a}$. 

There is a paper in this year's ICFP, refinement types for Haskell. The paper deals with termination checking rather than full Hoare logic, but hopefully that's a start in this direction. The related work section in that paper contains some pointers, such as Xu, Peyton-Jones, and Claessen's static contract checking for Haskell, and Sonnex, Drossopoulou, and Eisenbach's Zeno and Vytiniotis, Peyton-Jones, Claessen, and Rosen's Halo. 

Note that most of the categories you considered are 'abstract', i.e., they require structure or properties of an abstract category. As computer scientists we should also be familiar several concrete categories which turn out to be useful: Concrete categories 

Augmenting Andrej's answer: There is still no widespread agreement on the appropriate interface monad transformers should support in the functional programming context. Haskell's MTL is the de-facto interface, but Jaskelioff's Monatron is an alternative. One of the earlier technical reports by Moggi, an abstract view of programming languages, discusses what should be the right notion of transformer to some extent (section 4.1). In particular, he discusses the notion of an operation for a monad, which he (20 years later) revisits with Jaskelioff in monad transformers as monoid transformers. (This notion of operation is different from Plotkin and Power's notion of an algebraic operation for a monad, which amounts to a Kleisli arrow.) 

First, the property "having first-class functions" is a property of a programming language, not of particular functions in it. Either your language allows for functions to be passed around and created at will, or it doesn't. Functions that accept or return other functions as arguments are called higher-order functions. (The terminology comes from logic.) Functions that don't are called first-order functions. Perhaps this latter notion is what you wanted. 

My knowledge is a bit stale, as I haven't actively researched this field in the last couple of years. None of these is probably the state of the art, but a good place to start looking backwards (i.e., chase references) and forwards (i.e., see who cites it). If you're looking into information flow (making sure classified information doesn't leak to untrusted roles), a reasonable place to start is Martin Abadi et al's Dependency Core Calculus. I think it's reasonable enough that anyone who does formal methods in the area would refer to it (directly, or once removed). If you're looking into access control/authorisation (role A says role B controls the data, role B says you can access the data, etc.), Abadi recently published a tutorial book chapter on the subject, so might be a good place to start. If you're looking into authentication (whether the agent saying he is A is indeed A), I defer to someone else. I'll try to have a look later. 

Augmenting Noam's answer: Removing the implicit currying, $f : A \to B \to C$ is the same thing as $uncurry( f) : A \times B \to C$. Strong monads $T$ give a map (two, actually!): $dblstr : T A \times T B \to T (A\times B)$. We therefore have a map: $ T A \times T B \xrightarrow {dblstr} T(A\times B) \xrightarrow{uncurry(f)} TC $ If we instantiate this to the continuation monad, we obtain your construction. Generalizing to $n$-variables, the following should work (I didn't check all the details through). Once we choose a permutation $\pi$ over $n$, we have a $\pi$-strength morphism $str_{\pi} : T A_1 \times \cdots \times T A_n \to T(A_1 \times \cdots \times A_n)$. (The monad laws should guarantee that it doesn't matter how we associate this permutation.) Therefore, for every $n$-ary morphism $f : A_1 \times \cdots \times A_n \to C$, we can construct: $\gamma f : TA_1 \times \cdots \times TA_n \xrightarrow{str_{\pi}} T(A_1 \times \cdots \times A_n) \xrightarrow{Tf} TC$. But I still don't think this really gives you the answer you're looking for...