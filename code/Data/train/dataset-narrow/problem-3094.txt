Similar discrepancies can be found in terms such as or for instance. Additionally, you need to consider what is a token for your task at hand. In my previous example make sense either with or without . However without the hash is just a poorly written word. Maybe you want to keep the hashtags in your corpus or maybe not, but this depends on what you want to do with it. So first you need to know how your tokenizer handles these cases and decide if you want to remove them beforehand or later to keep hashtags in your corpus or only (sometimes weird) words. In the case of the is exactly the same, you can keep it, remove it or maybe delete the whole instance as you don't want to keep user names in your corpus. As I said, it all depends on your task. PS: In case you want to play around with different tokenizers, try this. 

As you mentioned, the paper doesn't clarify. However, my guess is that this is not due to concatenating 2 previous layers (I don't really see an specific reason to do this here) but because of concatenating the ResNet shorcut. Generally, all conv layers have a number of filters, thus determining the output size (num_filters, size) regardless the inputs. On the other hand, MaxPooling does keep the input num_filters (though in this case reducing the size). In the paper, note that the num_filters is doubled at the output of all convlayer except for the one that does not keep the ResNet shorcut (last 512 conv layer). So my guess is that they are concatenating the output of the conv layer and the shorcut which would explain the output size. Hope this helps! 

Have a look at this question. There is a nice discussion about how to implement this. The idea in to create a separated Input to your model and concatenate it AFTER the recurrent layer(s). Also in the Keras documentation, there is an example on how to build such models with a few lines of code. 

I would normalise the values between 0-1 (). Regarding the size of the files I don't think this should be a problem. For example, Cifar-10 are 32x32 or the average image resolution on ImageNet is 469x387 pixels although most approaches resize them to 256x256. I think your size should be fine. 

Here is where I actually think we have some problems. First, your dataset is quite small. 3816 reduced to 1908 when splitting train/test... This is not good especially with such structure. In the paper you mentioned, they are using a network with around 2.5M parameters and they used ~137K samples. Your 1908 dataset seems tiny compared and you are using the same model structure... In my opinion your model is not able to do better, simply put. It doesn't matter what parameters you choose you need (a lot) more data. You might try reducing the size of your network, also create more data using noise addition, mirror samples, etc... and see if these help somehow. Finally, apart from the size, you have no way to tell if your data is representative enough so that any model can learn from it. Therefore, 35% accuracy is as good as any other value I am afraid. Anyway, your mission seemed to be able to complete your experiment and you did. And you learnt a lot about limitations of deep models, so I'd say: good work! 

I am trying to do some analysis about user behaviour when typing (keystroke biometrics). Ideally, it will include traits extracted when people are writing code. Although not technically Natural Language, code also has some structured characteristics as language and I wanted to leverage that. I was wondering if there has been some research about performing language analysis focusing on programming languages instead of traditional spoken languages. Mainly, I am interested in having a comprehensive list of stopwords for as many languages as possible. For example, stopwords will include: for, while, return, break, string, if, else, and so on. Although it would be nice to have them separated by languages, I wouldn't mind a list comprising several languages. I know this could be done for example by getting some sample code and retrieving the most frequent terms, but I also wanted to know if there has been some research towards this direction. Any ideas, papers, methods would be welcome. Thanks! 

One Hot encode the categorical features Use PCA to reduce the dimensionality of the data Scale the data (subtract the mean, divide by the standard deviation) Train the regression model on the reduced scaled dataset 

What you could do is apply a dimensionality reduction technique such as tSNE to visualise all your features at once in 2 or 3 dimensions. Scikit learn has an excellent implementation of tSNE. However, tSNE is used more to find local structure and clusters in high dimensions, rather than relationships between features. Furthermore, distances between clusters and the reduced X-Y axes mean nothing in tSNE. If you know your data has some relationship between features, it may be best to use seaborns pairplot, as suggested by M Sef, to plot several features at once. 

If you want to use an unsupervised method i.e if your data is not labelled with classes, then something like k-means clustering may be your best bet to find patterns in the data. Alternatively, if you want to do anomaly detection, 2 possible options are 

Intuitively, the correlation matrix is symmetric because every variable pair has to have the same relationship (correlation) whether their correlation is in the upper right or lower left triangle. It doesn’t make sense to say the correlation between variables $X_1$ and $X_2$ is $\rho$, but the correlation between $X_2$ and $X_1$ is $\rho’\neq \rho$ if calculating a Pearson correlation (so correlation is symmetric). Mathematically, correlation between two variables, $X$ and $Y$, is commutative: $Corr(X,Y)=Corr(Y,X)$. In OP’s case, the correlation between Q1 and Q2 is the same calculation and therefore the same result as the correlation between Q2 and Q1. Therefore the correlation matrix will be symmetric. There are more mathematical reasons and proofs why a correlation matrix of real valued variables has to be symmetric and positive semi-definite, but I’ve excluded them from this answer. 

As you mentioned, each decision tree is trained on p (sometimes sqrt(p) random features). This ensures that each tree is “grown” (trained) differently so that the model 1. Does not overfit the training data (reduces variance) and 2. Generalizes better to new data (reduces bias). Therefore we don’t weigh the trees differently, as this would be similar to having all tress trained on the same features. You can change the voting threshold however from the standard 50% to anything you want (e.g 40%, 70%, 90%) which will change the precision and recall accuracies of the model. EDIT: changing the voting threshold means changing the number of trees needed to make a classification. For example, in binary classification most standard random forests require 50% or more of the trees to vote for a class for that class to “win” (be predicted to be that class). But if you change this threshold, to say 70%, then 70% or more of the trees need to vote for the same class for that class to win. 

PCA is a dimensionality reduction algorithm - it projects your high dimensional data onto a lower dimensional plane. This is useful for either visualisation (if you reduce to 2 or 3 dimensions to plot), or for training machine learning models. You say you want to find patterns in your data - I’m not quite sure what you mean by this. Do you want to visualise your data, train a model on it and make some prediction, or something else? To visualise high dimensional data, you could use either the tSNE (t-stochastic neighbour embedding) algorithm, or PCA. Depending on the type of data you have, you can “find patterns” in different ways. If your data is unlabelled (you don’t know the classes of each sample or there is no dependent variable) you can use unsupervised learning algorithms such as K-means clustering, K nearest neighbours, Gaussian mixture model. If your data has dependent variables, depending on whether your dependent variable is categorical or continuous, you could use classification algorithms for the former and regression algorithms for the latter. Classification algorithms include logistic regression or decision trees. Regression models include linear regression. 

It seems the Adaptive Moment Estimation (Adam) optimizer nearly always works better (faster and more reliably reaching a global minimum) when minimising the cost function in training neural nets. Why not always use Adam? Why even bother using RMSProp or momentum optimizers? 

It will likely help to have a look at the documentation for predict function of the model objects. There is an argument: , which defaults to 32 if not fixed by the model itself, which you can see from the . If you set this equal to 1, perhaps you will get a prediction. Below is a modified version of your code that I would expect to return a prediction. 

Note: In your model specification, you don't need to add the final linear , as the preceding Dense layer by default includes a linear activation. See the excellent documentation here. This is a big topic and there are many things that you could try out. I agree with the comments on your question, that you will need a lot more data to allow an RNN to make a meaning representation of the model. If you are not just doing this to learn about LSTMs etc., another practical approach might be to look into simpler time-series models such as an ARIMA model (do not be intimidated by the complicated name - it is much simpler than an LSTM). Such models can be constructed quite easily with Python, using the statsmodels package, which has a nice implementation. 

You could trying running the same experiment for 15 epochs, then plotting the training and validation losses (as they evolve perhaps, using the TensorBoard callback alongside your others). Do they follow any patterns or converge after some time? You could try using different initilisation methods, or even gradient clipping, in order to make training a little smoother - constraining the size of the updates to weights during backpropagation. Finally, another (brand-new!) result from research into GAN models, shows that progressively increasing the size of the inputs to your models might help to smooth learning and also extract a more robust set of features, which generalise better. Have a read of this section of an article from FastAI on their experience. EDIT: information regarding the ELU activation Using this activation my help learning, as it has a little more flexibility than e.g. the ELU, because it may also assume negative values. Here is an image from the original paper: 

Images If you have a set of images in a folder somewhere and you want to get them into a matrix, you can simply read them from disk using . Here is an example for a colour image: 

This sounds a lot like a linear optimisation problem: given certain resources and constraints, maximise profit. If you are new to this, check out some of these resources: 

Theano While still open source (and so able to be further developed by the community), the team behind Theano announced that they will no longer actively develop it. This means it will now likely fall even further behind other leading frameworks, and new functionality coming from ongoing research are not likely to make it into the library. We can see that many people admire Theano, but given that it is basically the oldest DL framework, the star count tells a tale: 

using your covariance matrix from step 1, compute which of these two variables in step 2 has the highest sum of covariances with the rest of the variables remove the variable with the highest sum of covariances Repeat steps 1-4 until you arrive at your desired number of variables, or a threshold is reached in terms of covariances or individual variable variances. 

define your function where all the other activation functions are defined add it to that dictionary make equal to your custom function (or even a new parameter in cross your fingers it doesn't break something somewhere else run it and solve the inevitable small adaptations that will be necessary in a few places 

One could approach this in two general ways: 1) bottom up: thinking about unifying the data somehow to begin with 2) top down: deciding how the data needs to look based on the final model you wish to use Do you already know which model you will use? If that is fixed (for whatever reason), you already know you need to get your data into the correct form, be it numerical or categorical. As you pinned your question with the tag , I can tell you that you need to make your data all numerical, so regression can work.