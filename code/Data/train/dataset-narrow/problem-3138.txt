If the data you show are the only data that you have, then the Markov Chain is really boring: it is a linear chain, going from Round A to Round B to Round C, and all those states are connected to a base state (which is Death, or something). You can directly calculate the transition probabilities from the data you have, since the number of companies that reached round N are all companies that could have reached round N (there is no alternative path). The death probability at the previous stage is (1 - $P_{reaching N}$) 

However, if you had some more features of each company, like the amount of money they received at each stage, or the profit they were making, then you could train a decision tree, e.g. with this implementation in sklearn, that told you, in simple words, "if a company arrived at round X with at least Y dollars raised and at least Z dollars of profit, then they are passing to the next round with 0.XX probability". Which is, I think, what you are aiming at. 

Let me assume you intend to use Python libraries to analyze the data, since you are using Scrapy to gather the data. If this is true, then a factor to consider for storage would be compatibility with other Python libraries. Of course, plain text is compatible with anything. But e.g. Pandas has a host of IO tools that simplifies reading from certain formats. If you intend to use for modeling, then Pandas can still read the data in for you, if you then cast it from a DataFrame to a Numpy array as an intermediate step. These tools allow you to read CSV and JSON, but also HDF5 ... particularly, I would draw your attention to the experimental support for msgpack, which seems to be a binary version of JSON. Binary means here that the stored files will be smaller and therefore faster to read and write. A somewhat similar alternative is BSON, which has a Python implementation — no Pandas or Numpy involved. Considering these formats only makes sense if you intend to give at least some formatting to the stored text, e.g. storing the post title separately from the post content, or storing all posts in a thread in order, or storing the timestamp ... If you considered JSON at all, then I suppose this is what you intended. If you just intend to store the plain post contents, then use plain text. 

Right now, I only have time for a very brief answer, but I'll try to expand on it later on. What you want to do is a clustering, since you want to discover some labels for your data. (As opposed to a classification, where you would have labels for at least some of the data and you would like to label the rest). In order to perform a clustering on your users, you need to have them as some kind of points in an abstract space. Then you will measure distances between points, and say that points that are "near" are "similar", and label them according to their place in that space. You need to transform your data into something that looks like a user profile, i.e.: a user ID, followed by a vector of numbers that represent the features of this user. In your case, each feature could be a "category of website" or a "category of product", and the number could be the amount of dollars spent in that feature. Or a feature could be a combination of web and product, of course. As an example, let us imagine the user profile with just three features: 

What you are asking about is, in my view, the main problem of implementing a lambda architecture. Here are some suggestions on how to solve it. The combination of Spark and Spark Streaming largely supersedes the original lambda architecture (which usually involved Hadoop and Storm). Read here an example of how to use a and a separate to produce different s, one for batch processed results and another for real-time results. Once you have replicated that in your system, you still have to think about how to query both kind of s. The trivial case would be to just both of them: 

You do not need a wrapper for DBPedia, you need a library that can issue a SPARQL query to its SPARQL endpoint. Here is an option for the library and here is the URL to point it to: $URL$ You need to issue a DESCRIBE query on the United_States resource page: 

Is installing Python locally a good practice? Yes, if you are going to develop in Python, it is always a good idea to have a local environment where you can break things safely. Is there value in setting up a Python "server"? Yes, but before doing so, be sure to be able to share your code with your colleagues using a version control system. My reasoning would be that, before you move things to a server, you can move a great deal forward by being able to test several different versions in the local environment mentioned above. Examples of VCS are git, svn, and for the deep nerds, darcs. Furthermore, a "Python server" where you can deploy your software once it is integrated into a releasable version is something usually called "staging server". There is a whole philosophy in software engineering — Continuous Integration — that advocates staging whatever you have in VCS daily or even on each change. In the end, this means that some automated program, running on the staging server, checks out your code, sees that it compiles, runs all defined tests and maybe outputs a package with a version number. Examples of such programs are Jenkins, Buildbot (this one is Python-specific), and Travis (for cloud-hosted projects). What are the hardware requirements for such a box? None, as far as I can tell. Whenever it runs out of disk space, you will have to clean up. Having more CPU speed and memory will make concurrent builds easier, but there is no real minimum. Do I need to be concerned about any specific packages or conflicts between projects? Yes, this has been identified as a problem, not only in Python, but in many other systems (see Dependency hell). The established practice is to keep projects isolated from each other as far as their dependencies are concerned. This means, avoid installing dependencies on the system Python interpreter, even locally; always define a virtual environment and install dependencies there. Many of the aforementioned CI servers will do that for you anyway. 

The convolution operation, simply put, is combination of element-wise product of two matrices. So long as these two matrices agree in dimensions, there shouldn't be a problem, and so I can understand the motivation behind your query. A.1. However, the intent of convolution is to encode source data matrix (entire image) in terms of a filter or kernel. More specifically, we are trying to encode the pixels in the neighborhood of anchor/source pixels. Have a look at the figure below: 

The error measure in the loss function is a 'statistical distance'; in contrast to the popular and preliminary understanding of distance between two vectors in Euclidean space. With 'statistical distance' we are attempting to map the 'dis-similarity' between estimated model and optimal model to Euclidean space. There is no constricting rule regarding the formulation of this 'statistical distance', but if the choice is appropriate then a progressive reduction in this 'distance' during optimization translates to a progressively improving model estimation. Consequently, the choice of 'statistical distance' or error measure is related to the underlying data distribution. In fact, there are several well defined distance/error measures for different classes of statistical distributions. It is advisable to select the error measure based on the distribution of the data in hand. It just so happens that the Gaussian distribution is ubiquitous, and consequently its associated distance measure, the L2-norm is the most popular error measure. However, this is not a rule and there exist real world data for which an 'efficient'* optimization implementation would adopt a different error measure than the L2-norm. Consider the set of Bregman divergences. The canonical representation of this divergence measure is the L2-norm (squared error). It also includes relative entropy (Kullback-Liebler divergence), generalized Euclidean distance (Mahalanobis metric), and Itakura-Saito function. You can read more about it in this paper on Functional Bregman Divergence and Bayesian Estimation of Distributions. Take-away: The L2-norm has an interesting set of properties which makes it a popular choice for error measure (other answers here have mentioned some of these, sufficient to the scope of this question), and the squared error will be the appropriate choice most of the time. Nevertheless, when the data distribution requires it, there are alternate error measures to choose from, and the choice depends in large part on the formulation of the optimization routine. *The 'appropriate' error measure would make the loss function convex for the optimization, which is very helpful, as opposed to some other error measure where the loss function is non-convex and thereby notoriously difficult. 

A1. What is dimensionality reduction: If you think of data in a matrix, where rows are instances and columns are attributes (or features), then dimensionality reduction is mapping this data matrix to a new matrix with fewer columns. For visualization, if you think of each matrix-column (attribute) as a dimension in feature space, then dimensionality reduction is projection of instances from the higher dimensional space (more columns) to a lower dimensional sub-space (fewer columns). 

Is it possible to construct a CNN for this task? How do I deal with the imbalance in training samples for each ordinal level? 

A preliminary: there are three attributes of a function that are relevant here: continuous, monotonic, and differentiable. The RELU is continuous and monotonic nut not differentiable at z=0. The exponential relu or ELU is all three of those attributes. The differential or gradient gives you a direction. When the derivative of a function is undefined at a point, then the direction of the gradient is indeterminate at that point. When applying gradient descent, we wish to continuously modify parameters such that the loss function steadily decreases, which is the same as saying we wish to keep moving down towards minimum. When the derivative of a loss function is undefined at some point, the gradient is indeterminate. This means the gradient descent could potentially move in the wrong direction. The magnitude of delay caused by this indeterminacy depends on the learning rate and other hyper-parameters. Regardless of the hyper-parameters, statistically, the undefined derivative in RELU at z=0, does contribute to slowing convergence of gradient descent. 

You are erroneously conflating two different entities: (1) bias-variance and (2) model complexity. (1) Over-fitting is bad in machine learning because it is impossible to collect a truly unbiased sample of population of any data. The over-fitted model results in parameters that are biased to the sample instead of properly estimating the parameters for the entire population. This means there will remain a difference between the estimated parameters $\hat{\phi}$ and the optimal parameters $\phi^{*}$, regardless of the number of training epochs $n$. $|\phi^{*} - \hat{\phi}| \rightarrow e_{\phi} \mbox{ as }n\rightarrow \infty$, where $e_{\phi}$ is some bounding value (2) Model complexity is in simplistic terms the number of parameters in $\phi$. If the model complexity is low, then there will remain a regression error regardless of the number of training epochs, even when $\hat{\phi}$ is approximately equal to $\phi^{*}$. Simplest example would be learning to fit a line (y=mx+c), where $\phi = \{m,c\}$ to data on a curve (quadratic polynomial). $E[|y-M(\hat{\phi})|] \rightarrow e_{M} \mbox{ as } n \rightarrow \infty$, where $e_{M}$ is some regression fit error bounding value Summary: Yes, both sample bias and model complexity contribute to the 'quality' of the learnt model, but they don't directly affect each other. If you have biased data, then regardless of having the correct number of parameters and infinite training, the final learnt model would have error. Similarly, if you had fewer than the required number of parameters, then regardless of perfectly unbiased sampling and infinite training, the final learnt model would have error.