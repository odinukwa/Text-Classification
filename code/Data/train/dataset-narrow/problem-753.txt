I have two cubes which updates in sequence. These takes very long to update, upwards of 5 hours due to their sizes. I have a SQL agent job which updates the cubes. The job has type with the following command: 

I'd like to create a SSRS report which utilizes data from our Data Warehouse, DW. This DW has summarized, for example, the sales of 100 or so stores into one table FactStoreSale. In the report I've managed to see which AD user who has started the report, by using the expression as an default value in a parameter. Each AD user is included in a AD Security Group. This Group typically includes the manager and the employees shared account . The name of the Security Group is typically (for StoreKey 1000). Is it possible to allow users in a Security Group to only see data corresponding to that StoreKey, i.e. if some user included in the Security Group uses the report, the resulting set will be: 

I will create a couple of tables in SQL Server for a client. Of special interest for this question is the table to be created and in which the first table will include all store's sales and include for instance , , , and sales data. The table will use complementary data in regards to Products, i.e. , , etc. This table will be a slowly changing dimension, and store Product history in terms of price etc. for each store. The table will include the columns and which describes when the data was entered, and when it was replaced (with default value year 2999 in until a new update of the Product is inserted) respectively. I Believe this is very common. However, the from the underlying database are very complex strings, such as . My alternatives as I see it, is to use in SQL Server to convert the complex strings to integers, and store these in a separate column and use these columns when doing JOINS. But checksum does not guarantee unique integer values, which would likely cause problems. When I tried Checksum on a Virtual PC and my own PC, doing Checksum on the same values returned different results which is a concern as well. I Believe these two together rules out Checksum, unless it can be manipulated somehow to make it more unique? Another alternative is to use more complex functions to ensure that the string values are converted into integers, such as the one provided on a question of mine here. A solution such as this however also has issues, the values and will for instance get the same result. It is also complicated in the sense that if someone who isn't that savvy in SQL need to try to find any issues with the Query that uses the function, it may be very hard to understand what is happening. My third option seem to be to use the table and update this table first from the SQL Agent, and using an Indexing Key on this table and use this index as the ProductKeyInt in the table (where ProductKeyInt will be some kind of subquery in for instance to fetch the value with the biggest in DimProduct corresponding to the ProductKey. Does anyone have any input? Is there a simpler way? I don't want to have strings as JOIN keys because of for example increased CPU time 

A trigger is the proper way to do this. We have a simliar process and when someone sets the value to 1, any existing records that are set to 1 are reset to 0. WHen someone deletes the one that is set to 1, it goes through a chain of rules to determine which remaining record to set to 1 (we must always have one and one one record set to 1). This process is more complicated than a check constraint to handle, thus it is in a trigger (this is the main purpose for triggers even existing). In a SQl server trigger, you need to be careful not to try to handle only one record at atime. Triggers operate on the entier set of data so make sure to write your trigger to do so as well. A 1,000,000 records might be changed in one update, you don't want to lock up your system while it gores through row by row. So no cursors and no scalar functions and no correlated subqueries. 

Merge joins and sort are notoriously slow. Is it possible to write a query for the OLE Db source instead that has joins? 

I like the idea of using a generic linked server name. However, in many environments this may not be possible, In this case, you can use dynamic SQl in your sp. 

Is it possible to get the file as a .csv file which can be imported like a text file and will not have this problem? I generally kick back all Excel files and ask for another format because SSIS and Excel do not play well together on many levels. 

I think any of the major databases can handle the load if designed well. Sadly I would estimate that less than 1% of all databases are designed well. (I have personally dealt with data from literally thousands of different databases performing a wide variety of functions, so I think I have a good idea of the lack of quality that is out there inteh real world.) I would strongly suggest that you get some books on performance tuning for the database you choose and read them thorughly before begining to design. There are many things that will help your database perform better that should be designed in from the start. Just knowing how to write performant queries and design indexes is critical to getting a good design. This type of study and designed-in performance is not premature optimization. There is no reason at all to use known performance killing techniques in the design. Databases need to be designed for performance from the start. 

(Where I once again have put my main concern in italics). What does that mean? Can someone shed some light into this? Both cubes are used in production, and they take up so much time to process during the night, that I am very constrained in terms of testing different options myself, and I am of course concerned that a faulty option may cause a cascade of errors or the like given the complexity of the cube structure. Has anyone found a way to update a cube, without having to process the cube using , and still maintain aggregations etc.? 

(Where I have put italics on the part which says it won't update aggregations, even though I have aggregation which I use). does also seem plausible at a glance, but 

I've tried to replace with but this operation were completed in 0 seconds, and did not update any data at all (even though the underlying SQL tables were updated prior to when the cube job were executed). What I want to achieve is to find an update option which updates the data, keeping the underlying cube structure intact, and which is faster than . Conferring the docs.microsoft documentation here I gather that there are other options available, but they all seem to have a backside. seem plausible at a glance, but reading the description it claims that 

I then created a variable called in SSRS where I used and where I had a dataset where I selected StoreKey using a JOIN on the current user's using the Query provided by and my table I had created. Thus I only got the StoreKeys available to certain AD Groups. 

I've currently had a SQL Server 2008 server, which we now need to migrate to a newer edition. We have a couple of linked servers which I have not set up personally which utilizes a provider called in . It appears as if the previous collegue responsible for this set up has used the link here in order to set up these connections. However, when I try to run the from the installation I get an immediate error telling me that I have a edition which is not supported, and I can see from the documentation that only is supported. How can I set up a OLE DB Provider for (I need to get this working with a ISerie Connection)? EDIT I've tried to install from Microsoft, and it installed successfully. It was even included in my list in Management Studio, but when I tried to set up a Connection using the same name for provider etc. as the previous Connection had, I got the error that . The same server used earlier. I thought it was "included" in the DB2 OLE DB provider On my previous installation I had the following settings: Provider: IBM DB2 UDB for iSeries IBMDASQL OLE DB Provider Product name: ISeries Data Source: Provider string: Location: Catalog: the only difference I have in the new Connection is that Provider: Microsoft OLE DB Provider for DB2 is used instead. Do I miss any steps? 

The only time I would ever consider a delimted list is if there is no chance you will want to look at the data separately. Then indeed it is faster. This is a very rare case, however. And in what you are doing, there is an approximately 100% chance you will want to look at individual payments separately. Delimited lists are harder to query (and updating if someone made a typo, ugh) for individual elements (and generally slower for this than a related table would be) and harder to maintain the data integrity. Databases can easily handle many millions of records in tables that join with correct indexing and design. If you need more performance at that point typically, you partition the database. Sit down and read some books on database design and performance tuning. The rules for good design are different in databases than applications. Please take the time to learn them. 

Deploying the stuff you need to move a job is pretty much just like packaging the stuff you need to move application code assuming that you correcly put all the new items and changes in source control. This includes your SSIS package, any object creation or alteration scripts such as tables, views, UDFs, stored procs, CLRS (Never ever create database objects using the GUI if you want to deploy later), any scripts to populate tables (such as lookup tables). You may need to number the items in the deplyment folder to ensure they are run in the correct order. Usually I write a deployment document as well because some of what we are deploying will go to differnt servers (Our ssis server is is differnt from our database server). You can also script out the job, but you will need to review and change the script for the new environment. I often find it just as easy to set up the job on the other server manaully (but I have dba rights to all servers, if you do not, you will probaly need to this). 

, but naturally I can't make an EXEC in the FROM clause. I also tried to make a new Dataset in SSRS which would use data from another Dataset within the same Project but that did not work either (and I Googled that it is not possible). Edit: final solution What I did to implement this was to create a table of the appearence: 

and then use this table to reduce the amount of stores a user can see data from. I do not know how I can utilize the Security Group in SSRS though, seeing as I have only found in the Built-in Fields in SSRS, but nothing about their respective Security Groups. I am also not sure if this is the optimal way to go, as it requires additional tables and manually updating the table (because I do not wish to automate this procedure yet to have a higher Control over this). EDIT I found a way to get the permission path, i.e. Group belonging to a user using SQL server and the Query: 

I've tested in SSRS that I can replace 'Domain\User.Name' with my variable. I've tested to create a dataset with this query. This seems to give the correct data, but what it lacks is the StoreKey column which I thought could be . I'd like to be able to have a variable called in SSRS where Available Values from Query would be something like: 

Note: I do not wish to create a report for each Store, as that would require 100 reports. Some managers can manage more than one store, so they may be included in multiple Security Groups and need the corresponding permissions. I do not want to split up the underlying table , even though it should be possible to split this up into 100 or so schemas, and give members of the Security Groups access to these scehmas accordingly. A solution I have in mind is to create a table using SSIS which gets updated every day with the appearence: 

Likely you have some data that doesn't fit the datetime column somehwere in the file. Insert to a staging table with a varchar or navachar filed for the column and look at the data. You may need to adjust some of it (or null out bad records) before loading to the real table. 

Alternative approach if you do not need to disguise the data for privacy reasons. All database changes and values for lookup tables should be in source control. We simply restore the last prod backup and then use source control scripts to add whatever dev changes haven't made it to Prod yet. If you have multiple databases, all should be refreshed at the same time. One advantage of this is that you have roughly the same number of records as prod which makes a huge differnce in testing your database code for performance. It also helps keep your devs using source control for changes and helps practice deployments before you move them to other environments. 

One of the biggest performance hits is updating the string data as it changes and you have to update all the child records (which depending on the application could be in the millions!) But in general yes an int is the fastest thing to join on although the difference may be negligible if the string is small. If the stringvalues are large, performance will be slower will be slower and indexes will be greatly expanded as the PK is in all other indexes and thus a slower PK could slow down index use for other things as well. Making child tables wider due to using a natural key as the FK (especially if it is a long one or a compound key) can also cause performance issues if it makes the table becomes too wide when using natural key as the Fk instead of the surrogate int key. There is an argument that there is a lot of time saved from not having to make joins for selecting, but the performance hit for updating is usually not considered by those people. Since most string values are subject to be being changed, you have to account for that in your planning. Further, the addition of the natural key will only help some select queries, you will often still need to make the join to get all the field data that you need. And if the FKs are properly indexed, you often don't save that much time anyway. Saving developer time writing queries is, of course, irrelevant when discussing database performance. So of a string key is both small and unchanging and if you will often be querying where the only thing you want from the parent table is the string value, then yes a string PK is fine. If you don't have those conditions, I personally would use the surrogate key with a unique index on the natural key.