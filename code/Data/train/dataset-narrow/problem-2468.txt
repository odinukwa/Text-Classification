They show how to build a public-key encryption algorithm $E(\cdot)$ with the following two useful properties: 

There's a straightforward way to construct a function $f_z:\{0,1\}^n \to \mathbb{R}$ that is zero at only a single point $z=(z_1,\dots,z_n)$ and strictly positive everywhere else: namely, $$f_z(x_1,\dots,x_n) = (x_1-z_1)^2 + (x_2-z_2)^2 + \dots + (x_n-z_n)^2.$$ Based on this, we can easily construct a function $g : \{0,1\}^n \to \mathbb{R}$ that is zero at only the points $z[1],\dots,z[k]$ and strictly positive elsewhere: namely, $$g(x) = f_{z[1]}(x) \cdot f_{z[2]}(x) \cdots f_{z[n]}(x).$$ It follows immediately that the pseudo-Boolean function $g$ has global minima at exactly the points $z[1],\dots,z[k]$ and no others, so it meets all your criteria. This "algorithm" is simple and efficient. 

I'm going to be given a positive integer $z$, and I want to find an optimal basis $B$ that is good for $z$. A basis $B$ is a multiset of positive integers. The basis $B$ is considered good for $z$ if, for all non-negative integers $x,y$ such that $x+y=z$, we have the following: there exist multisets $B_x,B_y$ such that $x=\sum_{i \in B_x} i$, $y=\sum_{j \in B_y} j$, and $B_x \cup B_y \subseteq B$ (counting multiplicities: since these are multisets, the union operation adds up multiplicities of common elements, and the subset operation checks that multiplicities are pointwise smaller). In other words, for every possible way of decomposing $z$ into a sum of two non-negative integers, each of those integers has to be expressible as a sum of elements from the basis, and there have to be enough basis elements in $B$ that you don't "run out" of elements. When you use a basis element from $B$ to help express $x$, then it gets used up and you can't use that basis element for expressing $y$ (but if there's another copy in $B$, you're free to use the second copy for $y$). I hope this makes sense. The size of a basis $B$ is the number of elements in the multiset $B$ (counting multiplicities). Smaller is better. An optimal basis is one that is as small as possible. My question: is there a feasible algorithm that, given $z$, computes an optimal basis $B$ that's good for $z$? Or, an approximation algorithm that computes a basis that's reasonably close to optimal? How small can we make the basis, on average? A naive algorithm: one simple approach is to use the basis $B=\{1,1,2,2,4,4,8,8,\dots,2^k,2^k\}$ (for $k=\lfloor \lg z \rfloor$), so the basis contains two copies of each power of 2 smaller than $z$. This is certainly good for $z$: we can express $x$ in binary, express $y$ in binary, and since we have two copies of each power of two, we won't run out of basis elements (even if some power of two is common to both binary expressions, that's OK, since we have two copies, one for $x$ and one for $y$). Its size is approximately $2 \lg z$, so it is pretty small. However, it is not optimal in general: this basis could be up to a factor of two larger than optimal, depending upon $z$. Can you do better? If it helps, in my application I would expect $z$ to be no larger than 1000 or so. 

There's the idea of quantum annealing being used to solve optimization problems in terms of a QUBO problem for D-Wave's quantum algorithm. I understand that the advantage of quantum annealing as opposed to classical simulated annealing is that quantum annealing allows the particle/search point to tunnel through high barriers with probability as a function of barrier width, instead of having to climb all the way over the barrier (which in some cases wouldn't be possible because there wouldn't be enough energy). This is my understanding from here: $URL$ If quantum annealing is better than simulated annealing in this fundamental way, would it not be faster to implement QA instead of SA or GA's for solving optimization problems on a classical computer? If so, why aren't people using it? Or are they, and I'm just unaware (in which case I'd love to see references)? D-Wave seems to be banking on the practicality of their quantum computer, not so much insane accuracy or other more "scientific" pursuits. If it just so happens that D-Wave's computers aren't really quantum, shouldn't we be able to find a fast classically implemented quantum annealing algorithm to compete with the quantumly implemented version also? 

At this point the constants are likely to matter, so you might need to try out some implementations to see if my approach is truly better than a binary search tree in practice. You might also want to compare to storing the elements of $S$ in sorted order and answering queries using binary search over that list. 

Renyi entropy (of order 2) is useful in cryptography for analyzing the probability of collisions. Recall that the Renyi entropy of order 2 of a random variable $X$ is given by $$H_2(X) = - \log_2 \sum_x \Pr[X=x]^2.$$ It turns out that $H_2(X)$ lets us measure of the probability that two values drawn i.i.d. according to the distribution of $X$ happen to be the same ("collide"): this probability is exactly $2^{-H_2(X)}$. After drawing $n$ times from this distribution, the expected number of collisions among these $n$ draws is $C(n,2) 2^{-H_2(X)}$. These facts are useful in cryptography, where collisions can sometimes be problematic and enable attacks. For some analysis of other uses in cryptography, I recommend the following PhD dissertation: Christian Cachin. Entropy Measures and Unconditional Security in Cryptography. PhD dissertation, ETH Zurich, May 1997. 

[*] As a matter of courtesy, I thought I should include a brief description of the quantum annealing problem that I used in my other question: In adiabatic quantum computation (AQC), one encodes the solution to an optimization problem in the ground state of a [problem] Hamiltonian $H_p$. To get to this ground state, you start in an easily coolable initial (ground) state with Hamiltonian $H_i$ and "anneal" (adiabatically perturb) towards $H_p$, i.e. $$ H(s) = s H_i + (1-s) H_p $$ where $s \in [0,1]$. Details about AQC: $URL$ 

In adiabatic quantum computation (AQC), one encodes the solution to an optimization problem in the ground state of a [problem] Hamiltonian $H_p$. To get to this ground state, you start in an easily coolable initial (ground) state with Hamiltonian $H_i$ and "anneal" (adiabatically perturb) towards $H_p$, i.e. $$ H(s) = s H_i + (1-s) H_p $$ where $s \in [0,1]$. Details about AQC: $URL$ The interesting thing about this problem is to try to understand the gap between the ground state eigenvalue and the first excited state, as this determines the problem complexity. One interesting thing to do would be to try and say something about the behavior of certain types of Hamiltonians. One can analyze the energy spectrum of small qubit cases by simulation to understand the problem complexity, but this becomes infeasible very quickly. What I'd like to know is if there is a geometrical or topological way of looking at how certain Hamiltonians behave. Someone mentioned that the above form could be looked at as a homotopy (if the scalar functions were generalized to operators), but I'm not well-versed in higher level mathematics so I'm not sure what this implies or what I could do with it. It might help to mention that the Hamiltonians are usually Ising spin-glass Hamiltonians (at least, that is what $H_p$ is). I'm not well-read on advanced statistical mechanics literature either, so this may be another avenue. I wondered if anyone could provide some explanation on this, or at least provide some interesting references, keywords, etc. 

One straightforward approach is to first compute a $d \times k-d$ matrix $M$ such that $Mx=0$ iff $x \in K$. Then you can determine which points from $A$ lie on $K$ by computing $Mx$ for each $x \in A$. The running time for that will be $O(d^3 + k(d-k)n)$ or so, i.e., $O(d^3 + d^2 n)$. Since the input has size $dn$, any solution has to take $\Omega(dn)$ time. Thus this solution is at most a $d$ times factor slower than the best possible. You mention in the comments that you are interested in the case $k,d \ll n$. This might suggest counting the running time as a function of $n$, i.e., focus primarily on the dependence on $n$, and largely ignore the dependence on $k,d$. As a function of $n$ (treating $k,d$ as constants), the solution above runs in $O(n)$ time. 

I've been reading about sketches for processing streaming data (the CountMin sketch, the Count sketch, the tug-of-war sketch, FM sketches, etc.). They use hash functions that are required to be 2-independent or 4-independent or $k$-independent for some small $k$. However from cryptography we have fast hash functions that are very good candidates to essentially perfect: $k$-independent for every value of $k$ you could ever wish for, and so on. We can essentially treat them as uniformly chosen from the set of all functions: as far as we know, no polynomial-time computation can tell that this is not the case. I suspect many practitioners might be happy to use these cryptographic hash functions, if it led to improved performance or better parameters for the sketches. Does it change anything about the design and analysis of sketches, if we assume we have perfect hash functions like this? Can we get asymptotically better parameters if we assumed our hash function was perfect? Or, can we get simpler sketch constructions? (For instance, I've noticed that the analysis of existing sketches is often somewhat conservative: e.g., it uses the Markov bound or Chebyshev bound. If we had a perfect hash function we could use a Chernoff-style bound. Are there any sketches where this makes a significant difference, or is it just a small constant factor in the bound?) 

Some context: there is a current debate in adiabatic quantum computing over whether a particular machine, the D-Wave quantum annealer, can outperform a classical algorithm [*]. Earlier this year, a comparison between quantum Monte Carlo simulation and D-Wave machines were conducted, showing that the scaling behavior of the D-Wave chip did not indicate anything special. Around the same time, Katzgraber and coauthors argue that random instances are not too useful for testing quantum speedup. They go on to propose some things specifically for Chimera spin-glasses (the Chimera graph is the name of the hardware graph used in the D-Wave chips). What Katzgraber says makes a lot of sense--we should focus on the hard problems and see if there's anything interesting. But how to come up with these instances? There is an isomorphism between the Ising spin-glass model and quadratic unconstrained binary optimization problems (QUBOs), so you could ask the question both ways. I am open to: any (possibly small) insights, corrections of my misconceptions, keywords, ideas, or anything else you can contribute. Also I wonder if there are ideas in the AQC context for this that I've just simply missed or haven't understood. 

All of these requirements can be encoded as SAT clauses. As before, you'd use binary search on $k$ to find the smallest $k$ for which such a DFA exists. 

Check for success: If the resulting sets $A,B$ each have size 6, test whether they are a valid solution to the problem. If they are, stop. If not, continue with the loop over candidate values of $z$. 

Let $\mathbb{F}$ be a field. Suppose $p(x,y,z,\cdots) \in \mathbb{F}[x,y,z,\cdots]$ is a multivariate quadratic polynomial with coefficients in the field $\mathbb{F}$. Factor out factors of $x$, to get $$p(x,y,z,\cdots) = c \cdot x^2 + q(y,z,\cdots) \cdot x + r(y,z,\cdots).$$ Notice that $q$ must be linear/affine, $r$ must be quadratic, and $c$ must be a constant ($c \in \mathbb{F}$). We want to find an assignment of values to $x,y,z,\cdots$ that makes $$c \cdot x^2 + q(y,z,\cdots) \cdot x + r(y,z,\cdots) = 0.$$ Now let $\Delta$ denote the discriminant of this quadratic equation (in $x$), i.e., $$\Delta(y,z,\cdots) = q(y,z,\cdots)^2 - 4c \cdot r(y,z,\cdots).$$ The original polynomial is satisfiable if and only if you can find an assignment of values to $y,z,\cdots$ that makes $\Delta(y,z,\cdots)$ be a square (i.e., a quadratic residue) in $\mathbb{F}$. Notice that $\Delta(y,z,\cdots)$ is itself a multivariate quadratic polynomial (since $q$ is affine and $r$ is quadratic). At the point, our solution approach is going to branch, based upon whether $c=0$ (an easy case) or $c\ne 0$ (the harder case).