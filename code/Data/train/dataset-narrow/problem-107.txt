WiMax operates in spectra that needs to be licensed from the FCC (or the corresponding body if you're not in the US). Depending on the frequency in use the licensing process may- or may not- be an issue, but it's not a situation where you can simply buy the gear and start using it the same day. In contrast, the 802.11 standards operate in unlicensed frequencies and don't require a special license. There are plenty of Wifi links operating at- and well beyond- the 10KM mark, with the caveat that the link needs to be properly surveyed and engineered and appropriate antennae (read: high gain / highly directional) employed. In general I would say that there's a lot more activity happening with 802.11 than 802.16. This means that there's more (and cheaper) solutions and a lot more innovation and development happening in the industry. 

So there's a decision tree that just about any router goes through when determining routes. The respective routing protocols (RIP, BGP, OSPF, IS-IS, static) run their own algorithms to determine the ideal path to a given prefix. If the router is presented the same route (i.e. identical network and prefix length) then the administrative distance is consulted. This is the relative priority of the various routing protocols on the box, and in most implementations the lowest value wins. So - my understanding from Quagga is that it treats RIP routes as having a default administrative distance of 120. If you don't specify a different AD on a static route it will end up with a value of 1. As such, a static route will normally take precedent over a RIP route. In turn, if you set up a static route with an AD of 200 (for example) it will only be used if the equivalent RIP route leaves the table (this is known as a floating static route). In your example the situation is that routes received via BGP may have one of two different AD's - one for internal BGP routes (this is 200 by default in Cisco - would assume something similar for Quagga) and another for externals (usually much lower - 20 in Cisco). You can adjust the AD in Quagga manually. Take a look at the manual and specifically consider the command under each routing protocol definition. You can see these rules in effect when you look at the routing table, where the format is usually something like x.y.z.q/nn [AA/MMMM] where AA = administrative distance and MMMM = the metric within the protocol. As an aside - and I hope this is obvious from what I've written above - the longest prefix match is always the most important factor. A /32 received in RIP with an AD of 120 will trump an overlapping /24 from eBGP with an AD of 20. This is just basic routing, though. Also - in almost every properly designed networking scenario there is no need to adjust AD. If this seems like the only way to approach the design I would strongly consider re-approaching the design altogether as there's lots of potential for operational issues, routing loops and the like when AD is not consistently applied. Hope this helps. 

As mentioned, ECN provides an explicit means @ L3 for a signaling of link congestion. On certain L2 transports there are also mechanisms that can be engaged when buffers fill - in frame relay there's the BECN/FECN mechanism. In certain flavors of ATM (read: ABR) there's a live feedback mechanism to both signal congestion and even adjust provisioned PVC levels. There are also flow control mechanisms in traditional Ethernet - 802.3x. This is a MAC-layer mechanism that can be issued that will halt all traffic until the pause status is lifted. This is most commonly seen nowadays in certain storage appliances. As above, though, this is strictly a L2 construct and only operates on a hop-by-hop basis. In modern Ethernet - very specifically the suite of protocols known as DCB (Data Center Bridging) there are mechanisms to both segregate different types of traffic into different forwarding queues (most crucially to identify lossless traffic - generally FCoE) as well as specific control messages sent at MAC layer to turn on flow control on a per-queue basis. The idea here is to proactively pause traffic for queues that can tolerate loss, thus protecting lossless traffic. The mechanism by which this operates is similar to 802.3x but is, in practice, massively more useful. Finally - at L4 - TCP is probably overwhelmingly the most common point at which flow control and back-pressure occurs. The mechanism here, of course, is sliding window. When congestion occurs and packets are dropped TCP will adapt the rate at which packets are retransmitted (and, in turn, the rate at which subsequent data is sent). 

VACL's are a completely different mechanism and provide some measure of per-packet (and usually protocol based) control of traffic bridged within a given VLAN. You might, for instance, block traffic on TCP/80 between all hosts within the VLAN while allowing all other traffic to pass. It's possible to approximate the effects of PVLAN's by using a VACL but this tends to be somewhat fragile, difficult to manage and there are often inherent hardware limitations with which to contend (...highly dependent on platform). 

There are four pairs of connectors in that Cat5e cable. The T1 uses two pairs. If the high speed Internet service is provided via Ethernet then it will use either two or four pairs depending on whether it's 100 megabit or 1 gigabit. If your Internet connection is DSL-based then it uses only one pair. So... the next issue is that these different types of transmission mechanisms can potentially interfere with each other. Technically speaking it's bad practice to mix them inside of a single sheath, but I can't say I haven't seen it done successfully in some cases. I probably wouldn't be all that concerned with mixing DSL and T1. They were both designed to run in these kinds of conditions. I'd honestly be really hesitant to run either with Ethernet, though. Again - it can work, but the potential for things being flaky is high. Bear in mind that someone is going to have to come up with the means on both sides to split out these pairs. This is going to require an adapter that comes out of the jack you're being provided into two other jacks. Leaving aside the baseline signal quality questions I would more or less expect the first technician dispatched to check either service to end up breaking both when (not if) they pull the wrong plug to perform testing. Let me suggest something else, though - the odds are excellent that the building is wired for traditional voice services (i.e. old phone jacks). The T1 can happily run on this wiring, thus leaving the Cat5e behind to run high-speed data safely and simply...and even potentially supporting gigabit speeds. 

It's hard to completely generalize any of the above. Particularly toward the higher end of the spectrum routers will move massively more data than any firewall. That said, as the speed of the router increases the sophistication and completeness of security features will tend to decrease. So - for example - one can buy a 36-port 100GE switch for less than a mid-range firewall. Such a switch will be able to move more packets in less time than racks full of high-end firewalls. At the same time, though, the switch will only be capable of enforcing a relatively small number of stateless rules (in the form of ACL's). Similarly there are a whole bunch of things that routers do really well that firewalls frankly don't. If you're trying to home to multiple ISP's with BGP and want to gracefully and sanely control policy, buy a router/switch. If you want to build out a user access environment with dozens of downstream devices? Router/switch. Fancy QoS? Router. Lots of port density? Switch. Firewalls are really, really good at security functions. Some routers have firewall functionality but, ultimately, such features tend to be add-ons intended for small installations. Full stateful analysis? Firewalls. Add in IDS/IPS? Firewalls (well, some of them). Lots of app-level analytics to intelligently filter traffic based on user identity, app heuristics, etc? Firewall. Hopefully you get the idea. The place where the two options tend to run together are VPN's - and, honestly, cases can be made for either device depending on the context. User access VPN's are almost always better served by firewalls nowadays but most of the really heavy site-to-site work (..especially over public MPLS) seem to be performed by routers. Even so, there are plenty of valid counterexamples to both of my generalizations. So... Routers can also provide firewalling and firewalls can do limited amounts of routing. They both have their place and part of the value of experienced architects and engineers is understanding both where the pieces could fit and where they ought to fit. 

So - depending on the switch platform in use there are a bunch of things that can be used to control load and otherwise make the problem a lot more approachable. As Ron mentions, something like Netflow or sFlow can be a great way to go but won't provide a packet-by-packet level of analysis or allow diving into packet payloads. Being careful with the application of mirroring through the use of ACL's can explicitly exclude/include traffic based on the problem being solved. Some platforms also allow for the truncation of mirrored packets, which can also vastly reduce the sheer volume of information being managed. Another interesting approach is the use of ACL's to copy/redirect interesting traffic on an in-line basis. This is very different from port mirroring and ends up looking a lot more like policy routing but, again, like much of the above is incredibly hardware dependent. There are also item like in-line taps that can physically copy the traffic on a given link to your analyzer. This is almost invariably a better solution to a permanent requirement for full traffic visibility but is obviously a lot more expensive and a lot less flexible. In terms of actual use I tend to see larger organizations focusing first on natural choke points - switch uplinks, routers, firewalls, etc and then adding a capability to look at edge ports on an as-needed basis. Obviously use-cases vary but the sheer volume of data that can be generated can be overwhelming. 

Each test bench will have a physical connection to the router/switch and will be assigned to its own VLAN. The L3 interfaces in the VLAN will be configured with identical IP's but will each be placed in a separate vrf. An additional link from the router will connect to your internal network. This interface will be in vrf "production" and have an IP on the existing internal network. Within each vrf you'll need a default route via the production interface.