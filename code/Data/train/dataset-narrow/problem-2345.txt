This is just an extended comment on Marcos's answer, using his notation. I am not quite able to follow the details of his argument, and the one below is pretty short and easy. By averaging, $$\sum_A{q(A)\sum_x{d(x)\epsilon(A, x)}} = \sum_x{d(x)\sum_A{q(A)\epsilon(A, x)}} \leq \lambda.$$ The fact above and Markov's inequality imply $\sum_{A \in \beta(2\lambda)}{q(A)} \geq 1/2$. So we get: \begin{align*} \max_x \sum_A{q(A)r(A,x)} &\geq \sum_x{d(x)\sum_A{q(A)r(A, x)}}\\ &= \sum_A{q(A)\sum_x{d(x)r(A, x)}}\\ &\geq \sum_{A \in \beta(2\lambda)}{q(A)\sum_x{d(x)r(A, x)}}\\ &\geq \left(\sum_{A \in \beta(2\lambda)}{q(A)}\right) \min_{A \in \beta(2\lambda)}{\sum_x{d(x)r(A, x)}}\\ &\geq \frac{1}{2}\min_{A \in \beta(2\lambda)}{\sum_x{d(x)r(A, x)}} \end{align*} 

To complement the other answer: Costello, Shapira and Tetali showed that the expected approximation ration achieved by Johnson's algorithm on a random permutation of the variables is strictly better than $\frac{2}{3}$. Poloczek and Schnitger showed that another randomized version of the algorithm has expected approximation ratio $\frac{3}{4}$, and that the random permutation version does worse than $\frac{3}{4}$. By the way do not feel bad about getting stuck on this. Proving an approximation ratio better than $\frac{1}{2}$ for this algorithm was an open problem for a while. 

Let me see if I can clarify this, on a high level. Assume the UG instance is a bipartite graph $G = (V \cup W, E)$, bijections $\{\pi_e\}_{e \in E}$, where $\pi_e\colon \Sigma \to \Sigma$, and $|\Sigma| = m$. You want to construct a new graph $H$ so that if the UG instance is $1-\delta$ satisfiable, then $H$ has a large cut, and if the UG instance is not even $\delta$-satisfiable, then $H$ has only very small cuts. The graph $H$ contains, for each vertex in $W$, a cloud of $2^m$ points, each labeled by some $x \in \{-1, 1\}^\Sigma$. The intention is that you should be able to interpret a long code encoding of the labels of $W$ as a cut of $H$. Recall that to encode some $\sigma \in \Sigma$ with the long code, you use a boolean function $f\colon \{-1, 1\}^\Sigma \to \{-1, 1\}$; in particular it is the dictator function $f(x) = x_\sigma$. Let's produce a cut $S\cup T$ (i.e. bi-partition of the vertices) from the long code encoding as follows. If $w \in W$ has a label encoded by the boolean function $f$, go to the the cloud of vertices in $H$ corresponding to $w$, and put in $S$ all vertices in the cloud that are labeled by some $x$ for which $f(x) = 1$. All others go to $T$. You can do this backwards to assign boolean functions to all $w \in W$ based on a cut of $H$. In order for the reduction to work, you need to be able to tell only by looking at the value of a cut $S\cup T$ whether the boolean functions corresponding to the cut are close to a long code encoding of some assignment of labels to $W$ that satisfies a lot of the UG constraints of $G$. So the question is what information do we get from the value of a cut $S \cup T$. Consider any two vertices $a$ with label $x$ in the cloud corresponding to $w$ and $b$ with label $y$ in the cloud corresponding to $w'$ (in the reduction we only look at $w$, $w'$ in different clouds). We said that the cut can be used to derive boolean functions $f_w$ and $f_{w'}$. Now if there is an edge $(a,b)$ in $H$, then $(a, b)$ is cut if and only if $f_w(x) \neq f_{w'}(y)$. Therefore, using only the value of a cut to tell if the boolean functions it induces are "good" is the same as having a test that, given boolean functions $\{f_w\}_{w \in W}$, only asks for what fraction of some specified list of pairs $((w, x), (w', y))$ we have $f_w(x) \neq f_{w'}(y)$. In other words, whenever Ryan says in the notes "test if $f_w(x) \neq f_{w'}(y)$", what he really means is "in $H$, add an edge between the vertex in the cloud of $w$ labeled by $x$ and the vertex in the cloud of $w'$ labeled by $y$". I.e. for every $v\in V$, every two of its neighbors $w, w'$, and every $x,y \in \{-1, 1\}^n$, include the edge between the vertex in the cloud of $w$ labeled by $x\circ \pi_{v,w}$ and the vertex in the cloud of $w'$ labeled by $y \circ \pi_{v,w'}$, and assign the edge weight $(({1-\rho})/{2})^d(({1+\rho})/{2})^{n-d}$ where $d$ is the Hamming distance between $x$ and $y$. In this way the value of a cut divided by the total edge weight is exactly equal to the success probability of the test. 

Your access to $A$ (which is $P$ in the paper) is through an oracle, and you're charged for each query. But in terms of the query count, your accesses to $A'$ (which is $P'$ in the paper and is an "induced sub-poset" of $A$) are free. So the answer is that a chain decomposition of $A'$ is built without asking any new queries to $A$, only based on the relations in $A'$, which are already known. Then a new element is inserted by performing a binary search on each chain of the chain decomposition, and this is where $w\log n$ queries are performed. Note that Theorem 6 relates to Figure 2 and not Figure 1. There the number of queries is decreased by querying asymptotically optimally, rather than blindly doing a binary search independently on each chain (which would ignore relations going between chains). 

More statistics than computer science, but still interesting: In chapter 8 in Diaconis' monograph on Group Gepresentations in Probability and Statistics, spectral analysis techniques for data associated with a group $G$ are developed. This extends more classical spectral analysis of say time series data where the natural $G$ is the reals or the integers under addition. It makes sense to take $G$ to be $S_n$ when data is given by rankings. The monograph goes into interpreting the Fourier coefficients of ranking data. In that case the data set is represented by a sparse $f:S_n \rightarrow \mathbb{R}^+$ which maps rankings (given by a permutation) to the fraction of the population that prefers the ranking. Also in the same chapter, Fourier analysis over the symmetric and other groups is used to derive ANOVA models and tests. A natural extension of this would be statistical learning theory for ranking problems that benefits from representation theoretic techniques in a way similar to the way learning theory for binary classification under the uniform distribution has benefited from Fourier analysis on the boolean cube. 

The obvious problem is that if you use a cryptographic pseudorandom number generator (PRNG), the correctness of your algorithm is conditional on a complexity conjecture. However, usually this can be avoided, because the full strength of cryptographic pseudorandmness is usually a huge overkill for streaming. If your streaming algorithm uses a small amount of space, then what you need from your PRNG is that its output cannot be distinguished from a stream of random bits by a small space algorithm. Such a PRNG was constructed by Nisan, and the proof of pseudorandomness is unconditional. A seminal paper in streaming algorithms that used Nisan's generator is Indyk's work on sketches of $L_1$. Since the sketch uses Cauchy random variables for which the mean and the variance do not exist, the usual moment calculations do not work, so it is hard to imagine how to use $k$-wise independence. That's why the heavy hammer of Nisan's generator comes handy. A problem with these heavy hammers is that they come at a cost in space. The seed needs to be stored in memory, and for Nisan's generator the seed length is roughly $S\log R$ where $R$ is the number of random bits and $S$ is the space complexity of the algorithm. So you end up multiplying the space complexity by roughly the logarithm of the stream length $n$. If you can use $k$-wise independence you usually add a term like $O(k\log n)$ to the space complexity.