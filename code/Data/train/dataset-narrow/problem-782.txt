Now, speaking of inserts: this is where it gets sticky. There is an easy solution (very simple triggers), the problem is DB support. There is a bug in MySQL v5.6 (& in MariaDB) where they were checking NULL constraints before running triggers. This prevented using triggers to populate these fields. However, this bug was fixed in MySQL v5.7 (& MariaDB v10.1.21 & v10.2.4). If you use MySQL v5.7+ (MariaDB v10.1.21+ / v10.2.4+), or any other DB which complies with the SQL standard, and runs triggers before checking NULL constraints, you’ll be fine. Here’s a simple code example implementing the fix, and I also made a SQL Fiddle: 

Create a session variable which holds the accountGroupId. Spring @Transactional & Hibernate isolate DB sessions on a per request/thread basis (in Spring requests & threads are 1-to-1; i.e. 1 isolated thread per request), so there’s no risk of concurrent parameter bleed-through. Create DB stored functions which take in a value and store it in the session variable. Although, if you wanted to, you could just run an update query instead of creating a set() function, but you MUST have a get function. MySQL won't let you put a session variable in a view definition. Create restricted views which compare against the session var in the where clause. Since this variable is an account-wide single value (& won’t be changing from select to select within the same user), this works fine. Each request/user will have their own DB session & their own instance of the session var; all completely isolated from each other. They just call the set() function to initialize their session var before running their queries / inserts, and everything works perfectly. 

I am trying to track down a cause for a difference in oracle impdp processing, and I am not finding anything, so I am wondering if anyone here can explain the cause of the differences. At times I have seen times where using the parallel=x parameter in oracle datapump will cause either multiple tables to be inserted at once, up to the value of 'x', or other times will use parallel threads to import a single table. I have not been able to track down what might be causing the difference in performance, and I am wondering if anyone has an explanation, or even just a direction to point me to. It would be helpful to determine why it runs in a given matter. For instance right now I am monitoring a single table, data_only import that was started with parallel=8 in the command, but the import job is only using a single thread to do the import, as shown by the following hint the insert query /+ PARALLEL("XXXXX",1)+/. If the process would use the maximum 8 threads specified to should run much faster. 

Well, it looks like when the first sql server install ran, it also configured the file server services, which were still trying to hold the disk and the network name. Odd thing is, the disks where all showing 0 dependencies. This probably could have been fixed if we opened a case with microsoft, maybe cleaned up some registry entries, etc, but the guys from out windows team wanted to just rebuild from bare metal, as it would be quicker and easier. I was on board with that, so that is the solution I am implementing 

I think my problem might be caused by this old MySQL bug (and this one: /bug.php?id=6295 ), in which they were checking the column constraints before running triggers. But, according to that bug, they fixed it years ago. I was originally running on MariaDB v10.1.19; but according to this MariaDB ticket: jira.mariadb .org/browse/MDEV-10002, they also fixed this in v10.1.6. I have tried it on: 

Is there perhaps a config problem with my setup that’s causing this to not work in MariaDB? Or is it more likely that the bug got broken again since its original fix? I don’t want to bother the MariaDB devs if it turns out to be my fault. The Problem I have a restricted view which filters on a base table using a discriminator column. The view does not contain the discriminator column. In the base table, this column does not allow NULL values. I created a BEFORE INSERT trigger to populate this column when inserting data through the view, but as far as I can tell, the trigger is not firing. When inserting a record into the view, the DB throws the error: 

I’ve tried doing several insert tests on the base table, and it looks like the trigger is just not firing at all. I searched around online for other people with this problem, but they all seemed to just be syntax errors. I don’t have much experience with this kind of “deep” SQL, but I think mine’s correct. Here’s an example implementation, I also made a SQL Fiddle: P.S. Sorry about the 2 links with spaces in them. StackExchange won't let me post more than 2 links; apparently, I'm 4 reputation short. 

if you are running the deletes in batches, do you have an index on column that you are using to chunk up the process? IF there is no index and the query is resorting to full table scans to try and find the data that should be deleted, you could be adding a lot of time to the process. Unfortunately backing up the good data then dropping/truncating the table is probably your best bet. You could always rename the existing table and create a new table to do this rather then trying to extract just the data you need to keep. This way if you found your initial load of data was lacking, you still have everything to go back to for a second look. 

I did some reading, and opening the master key was only part of what I had to do. I had to completely configure the new server for SSIS. I found the following blog post helpful, $URL$ The following microsoft documentation was also a good second source of information, as a check against the blog posting. 

I assumed that the error was being caused by differences in the module path, so I explicitly set the path via $env:PSModulePath to match the path of my powershell session where the code runs fine. I am pretty new to powershell, so any help you could provide would be appreciated I am just doing some testing right now, so everything is running locally on my desktop. I am running SQL Server 2012. 

I don't know much about the feature, my question is as now we can send read-only workload to secondary replica . My question is what will perform better single instance with 32 cpu 1 primary and 1 secondary with 16 cpu each or 1 primary and 2 secondaries 12 cpu each Or in other way is the number of worker threads affected by the number of cpu in the primary only or the number of cpu of all members in Availability Groups 

I need some clarification regarding Always on avilabilty group in SQL SERVE 2016 connection to the secondary DB. Now i have availability group with 2 nodes. 

Update I find this Delete in the Query Store , there were 3 plans and I forced SQL to use different plan. I am wondering Why SQL choose to use the most expensive execution plan? 

i would like your help in the best way to tune and re-write a stored procedure. it is consuming a lot of time and don't know what should i do to make it faster 

If I have database db1 with schema sch1. I want to create another schema sch2 which will be used by different application and the tables in sch2 will be populated from tables in sch1. So my approach is to create triggers on insert, update, delete perform this task So my questions are 1- Is this a good way to do this or there is a better way? 2- I don’t want to affect the performance on db1 so I am want to replicate it to different server and create the triggers on the subscriber DB, so replicated server will have Sch1 and Sch2 on it, Can I do that? This is on SQL Server 2012 Standard Edition Note: tables in sch2 have different names and different columns name. also more than one table in sch2 may get populated from data from sch1 

I have a powershell script that I am trying to add in to a sql agent job, but the job step is failing with the following error 

I am hoping someone can shed some light on this. When running an import into an oracle database, I sometimes see different behavior based on the parallel option. Some times, I will see multiple data pump workers all running insert commands, with (parallel 1) query hint in them. Other times I have seen a single, or just a few data pump workers, running insert commands with (parallel X) [where x is more then 1] table hint in the queries. I have seen this when running imports that essentially identical. The imports are using different dump files, but where created from the same nightly job, just done on different days. I am using the following options SCHEMAS=XXXXXXXX parallel=32 cluster=y DIRECTORY= DUMPFILE=XXXXXXX_%U.dpdmp CONTENT=DATA_ONLY TABLE_EXISTS_ACTION=APPEND DATA_OPTIONS=SKIP_CONSTRAINT_ERRORS LOGFILE=XXXXXXXXXX.log 

I have noticed differences in executions between data pump imports lately, and I am trying to determine what is causing the differences. I am using impdp to move data between databases, and remap the schemas. It seems like I have seen two different things happen when I run the import in parallel, and I am not sure what is causing the differences. Sometimes I see the job running and it is importing a bunch of tables side by side, other times the import process is only importing a few tables at a time, but using parallel inserts for each table. Using Parallel inserts on just a few tables at a time seems to cut my overall run time substantially, but I can't figure out why it only runs like that sometimes and not others.