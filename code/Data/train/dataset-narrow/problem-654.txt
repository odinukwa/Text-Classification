I want to log the duration and time of user defined stored procedures each time they are invoked. I believe using extended events would be the way to go to achieve this. Can you please help me in defining the session and in querying the results? I am not quite sure about which event to add (sqlserver.rpc_completed?) and which target to choose (synchronous_event_counter,asynchronous_file_target or ring_buffer?). I also need help in querying the results (For example: grouping the result set so that each sp will show from top execution duration and execution time.) We use SQL Server 2008 R2. 

We are on SQL Server 2008 R2 Standard Edition. Some tables are highly fragmented. I want to see if defragmentation will improve performance and is worth the effort and tables/indexes being locked during the process. Therefore I want to restore the full backup on a test environment and simulate a live environment. What is the best way to follow? How can I capture the events going on in a live environment for some period? What tools are available for this? Thank you 

I'd like to know if it is possible to have a single passive node for two SQL Servers in a failover clustering deployment. For example, both active servers A and B will use server C as their contingent server. If it is apt to do so, will server C run two instances of SQL Server; one for A and one for B? As a side question; what is the benefit of "dynamic quorum" and "dynamic witness"? 

Record count is the same but there is a big gap between the two in terms of space used. sp_spaceused shows 7.691.344 KB as reserved while the report shows 4.340.216 KB. Which one is correct? 

I want to dynamically back up all the databases on a given SSAS instance using a SQL Agent job (which would most likely involve executing an SSIS package). It is imperative that this is a dynamic process - if users add databases or cubes, I want to set up a job one time that can automatically detect all existing SSAS metadata. Unfortunately, I don't see anything out there that tells me how I can automatically and dynamically back up all of the databases on an SSAS instance in a clean way. By "clean", I mean: 

The operator utilizes the operator. The specifications for the IN operator (screenshot below) indicate that both the (in this case, on the left of the ) and each (on the right side of the ) must be the same data type. Thanks to the transitive property of equality, each expression must be of the same data type as well. 

Create an Execute Script Task Set the script to use Visual Basic. Set the ReadOnlyVariables and ReadWriteVariables as follows: 

A SQL Agent job with a step for each instance that needs backed up (i.e. A step for the development server, the qa server, and for production). One dynamic SSIS package that is called in each step of the job. An that uses the Analysis Management Objects (AMO). 

Run the following query in the context of the database in question in order to get the Change Tracking table names used on the database. (Query via Kendra Little on BrentOzar.com) 

However, when executed with the embedded script, the job errors out quickly with the following response: 

One of the SQL Server 2008 R2 instances I administer is running on a two node Windows Cluster (Windows Server 2008 R2 Enterprise) When I run the DMV sys.dm_server_services, SQL Server Service Account shows as Y under the 'is_clustered' column. However, Agent Service Account is an 'N' for the same field 'is_clustered'. Is it something that I should be worried about and what might be causing this? Does this mean that Agent will not fail over? Thank you 

I set up a virtual lab environment simulating a clustered network where I unticked one of the nodes as the preferred owner. I can confirm that whenever the ticked node goes offline, the unticked one comes into play and the system failbacks onto the ticked node once it comes online again. 

It was tempdb contention. Adding more tempdb data files and enabling trace flag 1118 solved the issue. 

I'm experiencing high Signal Wait percent while CPU utilisation is very low at one of our servers. I have gone through numerous articles online about Signal Waits and understand that it is the time spent on the runnable queue however I'm still having difficult time to understand why CPU usage is staying so low (25%) while Signal Waits' percent is high (band of 75-90%.) If a session is waiting for an available CPU to run it, why is CPU utilised so low? 

We experience hundreds of sessions going in suspended state on rare occasions. When it happens there is no increase in terms of resource usage - CPU or memory. Rather there is a small amount of decrease 5-10% on CPU. There is no single error on the SQL log. I know this will be a blind question but what might be causing this? Any suggestions to check in SQL Server like plan cache etc..? 

Add a step for each SSAS instance that will be backed up. Each step should be configured to execute the SSIS package. For each step, click the Set values tab and set the value of to the instance name for the step. 

This can in fact be done. There are probably a few ways to do it, and here is a fairly straightforward example. For this solution, you will use a combination of: 

Create a new ADO.NET connection manager that uses the Microsoft OLE DB Provider for Analysis Service Using the Property Expressions Editor, set the ConnectionString property as follows: 

Create the Foreach Container Here, you will create a Foreach based on the Catalogs schema rowset. This will get us the for each database in the instance and the will be put into its corresponding variable. 

Why can't I reference the module from an embedded script, but doing so in a ps1 file works just fine? 

SQL Server 2008R2 PowerShell 2.1 I am trying to create a SQL Agent job that dynamically backs up all non-corrupted SSAS databases on an instance without the use of SSIS. In my SQL Agent job, when I create a CmdExec step and point to a PowerShell script file (.ps1) like this: 

Create a new Analysis Services connection Using the Property Expressions Editor, set the ConnectionString property as follows: 

Create a new OLEDB connection manager that uses the Microsoft OLE DB Provider for Analysis Service Using the Property Expressions Editor, set the ConnectionString property as follows: 

Find the create_date for the change_tracking table(s) found in the first result set by querying sys.internal_tables in the context of the change tracking database. 

My first attempt to delete the data simply took far too long - it's historical data so there are a few million rows. After that failed, I asked around and someone suggested disabling check constraints, deleting, and then re-enabling the constraints for those specific tables (which I will NEVER do again, terrible idea). The disable and delete ran fairly quickly, I had to leave the enable running all night but it did succeed. From then on, the database has been at 100% CPU, and one of the queries occasionally never completes. It's the same query every time: 

If I want to query the logs, in order of date, I have been told I can use the partition number in order to avoid a sort when the results from each partition are joined back together. 

I have a table with a non-clustered index on the id, and a clustered index on some other fields. The fields of the clustered index are non-sequential, and are frequently modified. I'd like to change the table to use the auto-increment id column for the clustered index. 

I would have expected this to result in a simple clustered index scan. However, looking at the live query statistics, and the actual execution plan, the majority of the query's execution time comes from sorting the results after the index scan. Why is the ordered data being sorted again? $URL$ 

I have a table with an id column which is auto-incremented, and various other informational columns. Rows are inserted into this table very frequently. When the data is read back, the majority of queries filter by a foreign key and a date range. Currently, there is a clustered index on the id column, and a non-clustered index on the two columns of importance (TrackerId and DateRecorded). If I were to swap the indexes, our queries will run a lot faster. Would this negatively affect insert times?