That is, the job involves maintaining CI, but it also involves monitoring production servers and applications using TSDBs, and various other activities. 

Essentially what you have designed is a caching system - the service containers have a local copy of the data presumably so that for reads they don't have to make an extra network trip. As you've pointed out, a more standard approach is to have a cluster of read replicas that all of the containers can read from. This allows you to scale them separately from the application servers, which is good, because they generally need different things (do you really want to allocate large amounts of RAM to every application container?). This will add network calls for database reads, but until that's proven to be an issue I wouldn't complicate the architecture to solve it. If it does become an issue, a much more lightweight way of handling the problem is to run an actual cache locally, like memcache or redis. You can tune the TTLs on individual objects to be appropriate, and it will automatically drop off rarely-requested data to keep the application server light. 

You have to check out the entire repo. You have to fetch the entire history (generally - shallow clones are an option, but usually not useful in actual development work). Natively, everyone has read+write access to every directory if they have it at all. 

When code review prompts changes, testing has to be redone. This encourages the building of automated tests over manual tests, as then this can happen automatically without any human involvement. When testing prompts changes, code review has to be redone. Code review is ideally primarily about a 10,000-foot view of how to approach the problem, but it certainly does cover implementation specifics as well. But this is a problem mostly solved by review tools - they'll show you the changes since you last reviewed so you don't have to review everything all over again. Some tools handle this well with rebasing, but if you're using one that doesn't, stick to adding new commits as you work, and then squash them all together at the end if that's what you want. 

In his world, developers and ops engineers had very similar high-level skill sets and responsibilities; where they differed was in their expertise. Their differing specialties encouraged them to work together to solve problems, and their common base-level skills gave them a language in which to do that. This is generally the definition of web operations that I land on for most cases. So it's the one we're going to continue along with. 

One additional way to think about this is to consider alternate approaches, and whether they are suitable for your needs. One of the axis I break down performance metrics on is whether they are real-user metrics or synthetic metrics. Your performance test is presumably synthetic: it runs the same set of actions every time in the same environment, so as to make it easy to compare runs against each other. However, synthetic tests don't show the actual effect a change will have on users, which is what RUMs are all about. You can do canary rollouts, and if performance decreases in the canary past an acceptable threshold, roll it back and investigate (I saw a talk from Facebook about how they gather full debug traces on their canary servers and so can find which code was responsible for the slowness and automatically backtrace that to a set of commits and authors). This is more realistic in the user impact, but can be difficult to do without large amounts of traffic, as there is normal variation in what real users do. Ideally, you'll be using both of these methods of tracking performance. But since we don't live in an ideal world, you may be able to retain a focus on performance at your company by shifting more emphasis on one of these versus the other, to free up resources for other needs. In your particular example, is it an ok trade-off to have slower deploys if it means reducing the CI feedback cycle? Maybe - but that depends on your company. 

Run through the wizard process once. Track down the files it generates, which isn't straightforward and may require a few tries. Pull those configuration files into your app configuration, and try a fresh container that just uses those. That is, reproduce what the installation wizard is doing in your own configuration tool. This sounds difficult, but often the wizard is complex because it needs to address many situations, and you are fine hard-coding many things for your specific situation. As a final note, since we're talking about Docker, presumably the database resides on another machine. Therefore, you only need to set it up once, and that process will be part of its provisioning, not this container's. 

This works nicely with variable precedence, for instance by defining different sets of servers per environment. It also works when you need to perform various other non-looped tasks in-between the two loops. 

Konstantin gave a good answer; here is an additional flavor on it. I will commonly define the lists as variables, and just write two separate loops over the same variable: 

In very traditional shops, developers and sysadmins are very siloed from each other. The devs build an app, then consider their job complete as soon as their code has been committed. The sysadmins take the build artifacts (which may be just the code, if it's an interpreted language) and deploy it to production servers. It's the sysadmins' job to keep the application running smoothly, and in general manage the production environment. However, often performance problems come from architecture issues in the app; the sysadmins don't have the programming knowledge to know what the app is doing, and the developers don't know how the app acts in the production topology with production traffic, so no one is equipped by themselves to solve the problem. Additionally, the developers are usually judged on how quickly they can produce new features, while the sysadmins are judged on how infrequent the app breaks in production. Since change is one of the leading causes of breakage, this puts the two departments at odds with each other - an old rivalry that hurts the business and the people involved. At some point, some developer-centric companies got so annoyed at this that they began practicing "NoOps" - they eliminated their operations departments and the perceived roadblocks that came with them. In reality, this meant that developers took on operations roles, but maintained their old titles. In a discussion surrounding NoOps, John Allspaw, then VP of Technical Operations at Etsy and an editor of the well-respected Web Operations book, defined roles at Etsy this way: 

If an extra variable is truly required, you can add statements at the top of the role to stop it from executing if they're not defined: 

Rather than solving the immediate problem you're having, let me suggest an alternative solution to your actual problem: 

This gives you far more flexibility for re-use. And if you find yourself not wanting to use relative paths for roles, you can override in an in the root directory where you run your Ansible commands from. 

Why are you navigating up three directories? I'd expect a layout that's like the official one, but with one subfolder for playbooks; then you'd end up with just 

I have a server where my user is able to sudo to the root user without a password, and then the root user can sudo to a third user without a password. However, my user cannot sudo directly to the third user without a password. Ansible's become directive uses sudo in the traditional manner, i.e. the one that requires prompting for my password. I've tried putting on a block and on a task inside that block, but it appears Ansible overrides the block's definition, rather than nesting the sudo calls as I hoped. Note: this question has also been asked on StackOverflow, but since there's no solution I figured I would repost it here, which I think is the better site for it. 

Modern configuration management tools can do a wide variety of things, so you can certainly use them to manage a PaaS. But it makes more sense to use a tool like Terraform to handle provisioning, since that's its specialty. 

You've already found the answer: centralized authentication. If you use a tool like Okta, you can indirectly tie GSuite to AD (or an LDAP server). And then of course you can tie OpenSSH there as well. 

That's not true; uses a variety of methods for privilege escalation, and defaults to using . Are you perhaps running into one of the known limitations with the become module, like trying to limit sudo access to certain commands? 

Thankfully, since Site Reliability Engineering developed internally at Google and only recently has started to make its way into the broader community, it is fairly well-defined. What isn't, though, is web operations (or "systems administration" - as an example of the lack of clarity, you use both in your question). It's difficult to discuss the differences between two things when you're not altogether sure what one of them is. But I'm an adventurous fellow, so I'll give it a shot. 

Using task schedulers like Bamboo or Airflow have a lot of advantages over cron, particularly in their ability to recognize errors and process dependent tasks accordingly. By far the most common reason not to use them is that you simply haven't set up the infrastructure yet: some form of cron is installed on every Linux machine, and it's easy to add a new job in, whereas these more advanced schedulers require setting up fleets of machines, installing and configuring various pieces of software, and learning how to write new tasks. Assuming that you already have this in place, though, there are a few usecases that cron handles better: 

Personally, I think it's best to have as little as possible in your playbooks, but delegate everything out to roles. Here's an example entire playbook in your setup: 

The problem seems to be that QA rotates shared laptops instead of just checking out a particular branch the same way devs do. 

I'm going to talk about general process here, rather than addressing Magneto specifically, so that it's useful to more people and situations. 

by default does recursive copies when given a directory, although it seems to be picky about trailing slashes. If is the directory you'd like to copy, simply run 

The most elegant way would be to use dynamic inventory, rather than static inventory and dynamic group variables. However, if for some reason you don't want to do that, one other option is to use custom facts; these are scripts that live in on the client machines. Like dynamic inventory, they be executable scripts (and therefore, query redis or whatever else), but in this case they define variables just for that particular host. This isn't quite what you were looking for, since you'd have to query the database on each individual server instead of once for the entire group, but it can have a similar effect. 

So then, what is Site Reliability Engineering? The Google SRE book opens with a definition of SRE... and then another one... and then spends a chapter continuing to define the role and an entire book covering the specifics. Even when developed in one organization, it seems that it's difficult to condense the job down to one single agreed-upon definition. To start with, we need to walk back to 2003, when Ben Traynor joined Google and founded what came to be the first Site Reliability Engineering team. Recall that a few paragraphs ago we were in the early 2010s; but in 2003, the industry was still pretty set on the sysadmin/developer divide as the natural way of things. So when Ben says that SRE was what would happen if a software engineer created an operations team, this was a much more radical melding of the two worlds than it appears now. The definition given in the preface emphasizes each of the three words individually: 

Nothing in the context of that job description. CI and time series databases are separate entries in the list. To put it another way: 

This then allows you to write individual configuration files from different places, and let the client software concatenate them. However, as far as I can tell from a quick search, Tomcat doesn't support this, so you would need to operate with a single template file. is a hacky solution that's very fragile, and is almost always a better option. However, in this case, the node that you're adding is a self-closing one, so as long as it is top-level, it can go anywhere and will work (I know you say it won't, but I'll get back to that). is a similar option that's a bit less fragile. Those two you've ruled out because they can't delete the comment you have in the file. However, I'll argue that that's an improper requirement (in most cases, and probably yours). When using a configuration management tool, the source of truth shifts from the files themselves on servers to the configuration that's checked in to master. If someone wants to edit the file, they'll do so by editing the configuration management scripts, not by editing the file directly on the server. Therefore, there is no need for comments in the resultant file that lives on the server, except perhaps one at the top that says "this is auto-generated, go look at source control instead". The comment should be in your templating code - and as long as the result compiles and works correctly, you shouldn't care at all what it looks like. 

If you're trying to motivate change in your organization, then you shouldn't be trying to prove anything, but provide evidence of what other organizations say about their own transitions. If you're trying to measure the processes that you already have in place and justify their continued existence, then you should be tracking the standard reliability metrics, like mean time to repair (MTTR). But devops principles are not merely about increasing reliability. Even site reliability engineering is not merely about increasing reliability. Rather, you want to get to an appropriate level of reliability - something that benefits the business but doesn't hinder development. And that brings up the real motivator in devops, which is to empower change. You want to allow the business to respond quicker to market stimuli, which happens by decreasing developer friction, increasing the rate of deploys, automating manual processes, etc. while staying within an acceptable bound of reliability. This means you need to measure reliability, but you also need to measure velocity, because your goal is to increase the latter while keeping the former relatively static.